 John Langford jl@y ahoo-inc.com Alexander Strehl strehl@y ahoo-inc.com Yaho o! Researc h, 111 W. 40th Street, New York, New York 10018 Jennifer Wortman wortmanj@seas.upenn.edu The k -armed bandit problem (Lai &amp; Robbins, 1985; Berry &amp; Fristedt, 1985; Auer et al., 2002; Even-Dar et al., 2006) has been studied in great detail, primar-ily because it can be view ed as a minimal formalization of the exploration problem faced by any autonomous agen t. Unfortunately , while its minimalism admits tractabilit y and insigh t, it misses details that are nec-essary for application to man y realistic problems. For instance, the problem of internet adv ertising can be view ed as a type of bandit problem in whic h choosing an ad or set of ads to displa y corresp onds to choos-ing an arm to pull. However, this formalization is in-adequate in practice, as vital information is ignored. In particular, a successful ad placemen t policy migh t choose ads based on the con ten t of the web page on whic h the ads are displa yed. The standard k -armed bandit form ulation ignores this useful information. This shortcoming can be rectified by mo deling the problem as an instance of the contextual bandit prob-lem (Langford &amp; Zhang, 2007), a generalization of the k -armed bandit problem that allo ws an agen t to first observ e an input or side information before choosing an arm. This problem has been studied under differ-ent names, including asso ciativ e reinforcemen t learn-ing (Kaelbling, 1994), bandits with side information (W ang et al., 2005), and bandits with exp erts (Auer et al., 1995), yet its analysis is far from complete. In this pap er, we study policy evaluation in the con tex-tual bandit setting. Policy evaluation is the problem of evaluating a new strategy for beha vior, or policy , using only observ ations collected during the execution of another policy . The difficult y of this problem stems from the lack of con trol over available data. Giv en complete freedom, an algorithm could evaluate a pol-icy simply by executing it for a sufficien t num ber of trials. However, in real-w orld applications, we often do not have the luxury of executing arbitrary policies, or we may want to distinguish or searc h among man y more policies than we could evaluate indep enden tly. We begin by pro viding imp ossibilit y results character-izing situations in whic h policy evaluation is not possi-ble. In particular, we sho w that policy evaluation can be imp ossible when the exploration policy dep ends on the curren t input. We then pro vide and pro ve the cor-rectness of a principled metho d for policy evaluation when this is not the case. This technique, whic h we call  X  X xploration scavenging, X  can be used to accu-rately estimate the value of any new policy as long as the exploration policy does not dep end on the curren t input and chooses eac h action sufficien tly often, even if the exploration policy is deterministic. The abilit y to dep end on deterministic policies mak es this approac h more applicable than previous techniques based upon kno wn and con trolled randomization in the exploring policy . We also sho w that exploration scavenging can be applied if we wish to choose between multiple poli-cies, even when these policies dep end on the input, whic h is a prop erty shared by most real ad-serving policies. This tric k allo ws exploration scavenging to be applied to a broader set of real-life problems. The motiv ating application for our work is internet advertising . Eac h time a user visits a web page, an adv ertising engine places a limited num ber of ads in a slate on the page. The ad compan y receiv es a paymen t for every ad clic ked by the user. Exploration scaveng-ing is well-suited for this application for a few reasons. First, an adv ertising compan y may want to evaluate a new metho d for placing ads without incurring the risk and cost of actually using the new metho d. Second, there exist logs con taining huge amoun ts of historical clic k data resulting from the execution of existing ad-serving policies. It is economically sensible to use this data, if possible, when evaluating new policies. In Section 4, we discuss the application of our meth-ods to the ad displa y problem, and presen t empirical results on data pro vided by Yaho o!, a web searc h com-pan y. Although this application actually violates the requiremen t that the exploration policy be indep en-den t of the curren t input, the techniques sho w promise, leading us to believ e that exploration scavenging can be useful in practice even when the strong assumptions necessary for the theoretical results do not hold. To our kno wledge, the only similar application work that has been published is that of Dupret et al. (2007) who tackle a similar problem from a Bayesian persp ec-tive using differen t assumptions whic h lead to differen t solution techniques. Our approac h has the adv antage that the estimated value is the output of a simple func-tion rather than an EM optimization, facilitating in-terpretation of the evaluation metho d. Let X be an arbitrary input space, and A = { 1 ,  X  X  X  , k } be a set of actions. An instance of the con textual ban-dit problem is specified by a distribution D over tuples ( x, ~r ) where x  X  X  is an input and ~r  X  [0 , 1] k is a vec-tor of rew ards. Events occur on a round by round basis where on eac h round t : 1. The world dra ws ( x t , ~r t )  X  D and announces x t 2. The algorithm chooses an action a t  X  X  , possibly 3. The world announces the rew ard r t,a The algorithm does not learn what rew ard it would have receiv ed if it had chosen an action a 6 = a t . The standard goal in this setting is to maximize the sum of rew ards r t,a imp ortan t subgoal, whic h is the focus of this pap er, is policy evaluation. Here, we assume that we are given a data set S  X  ( X X A X  [0 , 1]) T , whic h is generated by follo wing some fixed policy  X  for T steps. Now, given a differen t policy h : X  X  X  , we would like to estimate the value of policy h , that is, The standard k -armed bandit is a special case of the con textual bandit setting in whic h |X| = 1. In this section, we characterize situations in whic h pol-icy evaluation may not be possible, and pro vide tech-niques for estimating the value of a policy when it is. To start, we sho w that when the exploration pol-icy  X  dep ends on the input x , policy evaluation can be imp ossible. Later, we sho w that when the explo-ration policy  X  has no dep endence on the curren t in-put, there exists a technique for accurately estimating the value of a new policy h as long as the exploration policy chooses eac h action sufficien tly often. Finally , we sho w that exploration scavenging can be applied in the situation in whic h we are choosing between mul-tiple exploration policies, even when the exploration policies themselv es dep end on the curren t input. 3.1. Imp ossibilit y Results First, note that policy evaluation is not possible when the exploration policy  X  chooses some action a with zero probabilit y. This is true even in the standard k -armed bandit setting. If the exploration policy alw ays chooses action 1, and the policy to evaluate alw ays chooses action 2, then policy evaluation is hop eless. It is natural to ask if it is possible to build a policy evaluation pro cedure that is guaran teed to accurately evaluate a new policy given data collected using an ar-bitrary exploration policy  X  as long as  X  chooses eac h action sufficien tly often. The follo wing theorem sho ws that this goal is unac hiev able. In particular, it sho ws that if the exploration policy  X  dep ends on the curren t input, then there are cases in whic h new policies h can-not be evaluated using observ ations gathered under  X  , even if  X  chooses eac h action frequen tly. Specifically , there can exist two con textual bandit distributions D and D 0 that result in indistinguishable observ ation se-quences even though V D ( h ) and V D 0 ( h ) are far apart. Later we sho w that in the same con text, if we disal-low input-dep enden t exploration policies, policy eval-uation becomes possible Theorem 1 Ther e exist contextual bandit problems D and D 0 with k = 2 actions, a hyp othesis h , and a policy  X  dep endent on the curr ent observation x t with each action visite d with probability 1 / 2 , such that observa-tions of  X  on D are statistic ally indistinguishable from observations of  X  on D 0 , yet | V D ( h )  X  V D 0 ( h ) | = 1 . Pro of: The pro of is by construction. Supp ose x t tak es on the values 0 and 1, eac h with probabilit y 0.5 under both D and D 0 . Let  X  ( x ) = x be the exploration policy , and let h ( x ) = 1  X  x be the policy we wish to evaluate. Supp ose that rew ards are deterministic given x , as summarized in the follo wing table.
 Then V D ( h ) = 0, while V D 0 ( h ) = 1, but observ ations collected using exploration policy  X  are indistinguish-able for D and D 0 . 3.2. Techniques for Policy Evaluation We have seen that policy evaluation can be imp ossible in general if the exploration policy  X  dep ends on the curren t input or fails to choose eac h action sufficien tly often. We now discuss techniques for policy evaluation when this is not the case. Theorem 2 sho ws that in some very special circumstances, it is possible to create an unbiased estimator for the value of a policy h using exploration data from another policy . The main result of this section, Theorem 3, sho ws that this estimator is often close to the value of the policy , even when the stringen t conditions in the Theorem 2 are not satisfied. Theorem 2 For any contextual bandit distribution D over ( x, ~r ) , any policy h , any explor ation policy  X  such that (1) for each action a , ther e is a constant T a &gt; 0 for which |{ t : a t = a }| = T a with probability 1 , and (2)  X  cho oses a t indep endent of x t , Pro of: The first equalit y is a reordering of the sum. The sec-ond and fourth follo w from linearit y of exp ectation. The third equalit y is more subtle. Consider a fixed action a . The term P { t : a volves dra wing T bandit samples ( x t , ~r t ) and summing the term r t,a I ( h ( x t ) = a ) /T a over only the times t for whic h the exploration policy chose action a . There are precisely T a suc h trials. The equalit y then follo ws from the fact that the quan tity r t,a I ( h ( x t ) = a ) /T iden tically distributed for all t suc h that a t = a . It is critical that the exploration policy  X  chooses a t inde-penden t of x t (to mak e the numerator iden tical) and that T a is fixed (to mak e the denominator iden tical). If a t dep ends on x t , then these values are no longer iden tically distributed and the equalit y does not hold. This is imp ortan t, as we have seen that evaluation is not possible in general if a t can dep end on x t . Conditions (1) and (2) in the theorem are satisfied, for example, by any policy whic h visits eac h action and chooses actions indep enden t of observ ations. This theorem represen ts the limit of what we kno w how to achiev e with a strict equalit y. It can replace the sample selection bias (Hec kman, 1979) lemma used in the analysis of the Epoch-Greedy algorithm (Langford &amp; Zhang, 2007), but cannot replace the analysis used in EXP4 (Auer et al., 1995) without weak ening their theorem statemen t to hold only in IID settings. The next theorem, whic h is the main theoretical re-sult of this pap er, sho ws that in a much broader set of circumstances, the estimator in the previous lemma is useful for estimating V D ( h ). Specifically , as long as the exploration does not dep end on the curren t input and chooses eac h action sufficien tly frequen tly, the es-timator can be used for policy evaluation.
 Theorem 3 For every contextual bandits distribution D over ( x, ~r ) with rewar ds r a  X  [0 , 1] , for every se-quenc e of T actions a t chosen by an explor ation policy  X  that may be a function of history but does not dep end on x t , for every hyp othesis h , then for any  X   X  (0 , 1) , with probability 1  X   X  , V D ( h )  X  wher e T a = |{ t : a t = a }| .
 Pro of: First notice that Fix an action a . Let t i denote the i th time step that action a was chosen, with t i = 0 if i &gt; T a . Note that t is a random variable. For i = 1 , . . . , T define Note that Z i  X  [  X  1 , 1] and E [ Z i ] = 0 for all i . Now fix a positiv e integer t  X  { 1 , . . . , T } . We apply Azuma X  X  inequalit y (see, for example, Alon and Spencer (2000)) to sho w that for any  X  0  X  (0 , 1), with probabilit y 1  X   X  and so if t  X  T a , is upp er bounded by p 2 ln(2 / X  0 ) /t . Applying the union bound with  X  0 =  X / ( T k ), we see that Equa-tion 2 holds for all t  X  X  1 , . . . , T } and thus for t = T with probabilit y  X /k . Applying the union bound again yields a bound that holds for all actions. Sum-ming over actions and applying Equation 1 yields the lemma.
 Note that the coun ter-example given in Theorem 1 satisfies all conditions of Theorem 3 except for the assumption on  X  . Thus, we cannot solv e the policy exploration problem, in general, unless we mak e as-sumptions that limit  X   X  X  dep endence on input. Corollary 4 For every contextual bandit distribution D over ( x, ~r ) , for every explor ation policy  X  cho osing action a t indep endent of the curr ent input, for every policy h , if every action a  X  { 1 ,  X  X  X  , k } is guar ante ed to be chosen by  X  at least a constant fraction of the time, then as T  X  X  X  , the estimator grows arbitr arily close to V D ( h ) with probability 1. These observ ations can be utilized in practice in a sim-ple way. Giv en a data set S of observ ations ( x t , a t for t = { 1 ,  X  X  X  , T } , we can calculate  X  V D ( h ) as above and use this as an estimator of V D ( h ). For sufficien tly large data sets S , as long as eac h action is chosen suf-ficien tly often, this estimator is accurate. 3.3. Tigh ter Bounds in Special Cases In some special cases when there is sufficien t random-ization in the exploration policy or the policy h , it is possible to achiev e tigh ter bounds using a sligh tly mo dified estimator. Theorem 5 sho ws that the dep en-dence on the num ber of actions k can be impro ved in the special case in whic h Pr( h ( x ) = a t ) = 1 /k inde-penden t of x . This is true, for instance, when either the exploration policy  X  or the policy h chooses actions uniformly at random. We susp ect that tigh ter bounds can be achiev ed in other special cases as well. Theorem 5 For every contextual bandits distribution D over x, ~r with rewar ds r a  X  [0 , 1] , for every sequenc e of actions a t chosen by an explor ation policy  X  that may be a function of history but does not dep end on x t and every hyp othesis h , if Pr( h ( x ) = a t ) = 1 /k indep endent of x and if |{ t : a t = a }| &gt; 0 for all a , then for any  X   X  (0 , 1) , with probability 1  X   X  , V D ( h )  X  Pro of: Since we have assumed that Pr( h ( x t ) = a t ) = 1 /k indep enden t of x t , For all t , define Z Z t  X  [  X  k, k ] and E [ Z t,a ] = 0. Applying Azuma X  X  in-equalit y and the union bound yields the lemma. 3.4. Multiple Exploration Policies So far, all of our positiv e theoretical results have re-quired that the exploration policy choose actions in-dep enden t of the curren t input. There do exist special cases in whic h exploration data is pro vably useful for policy evaluation even when the exploration policy de-pends on con text. We briefly describ e one suc h case. Supp ose we have collected data from a system that has rotated through K kno wn exploration policies  X  ,  X  2 ,  X  X  X  ,  X  K over time. For example, we may have logs of historical ad displa y data from a compan y that has used K differen t ad displa y policies. Eac h individ-ual exploration policy  X  i may dep end on con text, but we assume that the choice of whic h policy was used by the system at any given time does not.
 We can redefine the action of the bandit problem as a choice of one of the K base policies to follo w; action a now corresp onds to choosing the ad chosen by policy  X  . Since historically the decision about whic h policy to use was made indep enden t of the con text x , we can view the exploration policy as oblivious with resp ect to x . Theorem 3 then implies that we can accurately esti-mate the value of any policy  X  whic h chooses from the set of actions chosen by the K base policies. This can be more powerful than comp eting with eac h historic policy , because  X  can mak e con text-dep enden t choices about whic h policy to follo w, poten tially achieving bet-ter performance than any single policy . Technology companies are interested in finding bet-ter ways to searc h both over the myriad pages of the internet and over the increasingly large selection of poten tial ads to displa y. However, given a candidate algorithm (or ad-serving policy in the case of online adv ertising), a compan y faces a real-life  X  X xploration-exploitation X  dilemma. The new algorithm could be better than existing ones, but it could be worse. To evaluate the performance of an algorithm, the com-pan y migh t decide to adopt it for a short time on a subset of web traffic. This metho d pro duces accurate estimates of performance, but the evaluation phase can be costly in terms of lost rev enue if the candidate algo-rithm performs poorly , and this cost gro ws linearly in the num ber of candidate algorithms that the compan y would like to evaluate. Clearly , a metho d of deter-mining the strengths or weaknesses of an algorithm without adopting it would be highly useful.
 In this section, we tackle the problem of evaluating a new ad-serving policy using data logged from an exist-ing system. We state our results in terms of the online adv ertising problem, but everything we discuss can be applied to web searc h with little or no mo dification. We begin by sho wing how to directly apply exploration scavenging techniques to the problem, and discuss the primary dra wbac ks of this simple approac h. Instead, we consider a standard simplifying assumption whose adoption leads to a more realistic metho d for policy evaluation. This assumption, that clic k-through rates are factorable, leads to another interesting theoretical problem, estimating the atten tion deca y coefficien ts of the clic k-through rates, whic h can also be accom-plished using techniques from Section 3. 4.1. The Direct Approac h The online adv ertising problem can be directly mapp ed to the con textual bandit problem, allo wing us to apply results from Section 3. Here the input space is the univ erse of all possible web pages and the action space con tains all slates of ads. The rew ard is a bit vector that iden tifies whether or not eac h returned ad was clic ked. 1 This bit vector can be con verted to a single real-v alued rew ard r in a num ber of ways, for instance, by simply summing the comp onen ts, yielding the total num ber of clic ks receiv ed, and normalizing. The example would then be used to compute a num-ber r  X  I ( h ( x ) = s ) / Coun t( s ), where Coun t( s ) is the num ber of times the slate s was displa yed during all trials. According to Theorem 3, summing this quan-tity over all trials yields a good estimator of the value of the new policy h .
 There is a significan t dra wbac k to this approac h. Due to the indicator variable, the con tribution to the sum for a single example is zero unless h ( x ) = s , whic h means that the slate chosen by the candidate algo-rithm h is exactly the same as the slate pro duced by the curren t system  X  . With a large set of ads and a large slate size, it is very unlik ely that the same slate is chosen man y times, and thus the resulting estimator for the value of h has an extremely high variance and may not exist for most slates. In the next section, we sho w how a standard assumption in the online adv er-tising comm unit y can be used to reduce the variance. 4.2. The Factoring Assumption The problem describ ed above can be avoided by mak-ing a factoring assumption . Specifically , we assume that the probabilit y of clic king an ad can be decom-posed or factored into two terms, an intrinsic click-through rate (CTR) that dep ends only on the web page x and the ad a , and a position-dep enden t mul-tiplier C i for position i , called the attention decay co-efficient (ADC). This assumption is commonly used in the sponsored searc h literature (Borgs et al., 2007; Lahaie &amp; Penno ck, 2007).
 Formally , let P ( x, a, i ) be the probabilit y that ad a is clic ked when placed in position i on web page x . We assume that P ( x, a, i ) = C i  X P ( x, a ), where P ( x, a ) is the intrinsic (position indep enden t) clic k-through rate for ad a given input x , and C i is a position-dep enden t constan t. Here C 1 = 1, so P ( x, a ) = P ( x, a, 1). A key observ ation is that this assumption allo ws us to transition from dealing directly with slates of ads to focusing on single ads. Let ` be the num ber of ads sho wn in a slate. Giv en an example ( x, s, ~r ), we can form ` transformed examples of the form ( x, a i , r 0 where a i is the i th ad in the slate and r 0 i = r i /C i other words, r 0 i is 1 /C i if the i th ad was clic ked, and 0 otherwise; the division by the ADC puts the rew ards on the same scale, so the exp ected value of the rew ard for a fixed pair ( x, a i ) is P ( x, a i ).
 Let  X  ( a, x ) be the slot in whic h the evaluation policy h places ad a on input x ; if h does not displa y a on input x , then  X  ( a, x ) = 0. For con venience, define C 0 = 0. We define a new estimator of the value of h as where T a is the total num ber of impressions receiv ed by a (i.e., the total num ber of times add a is displa yed). Here C i tak es the place of the indicator function used in the estimates in Section 3, giving higher weigh ts to the rew ards of ads that h places in better slots. Using the results from Section 3, it is straigh t-forw ard to sho w that this estimator is consisten t as long as the curren t ad-serving policy does not dep end on the input webpage x and every ad is displa yed. However, to apply this transformation, we require kno wledge of the ADCs. In the next section we sho w how to estimate them, again using nonrandom exploration. 4.3. Estimating Atten tion Deca y Coefficien ts Assume that a data set S is available from the execu-tion of an ad-serving policy  X  that chooses the t th slate of ads to displa y indep enden t of the input x t (though possibly dep enden t on history). As before, S includes observ ations ( x t , ~a t , ~r t,a is the slate of ads displa yed at time t and ~r t,a rew ard vector. Our goal is to use this data to estimate the atten tion deca y coefficien ts C 2 , . . . , C ` . We first discuss a naiv e ADC estimator, and then go on to sho w how it can be impro ved. In the follo wing sections, let C ( a, i ) be the num ber of clic ks on ad a observ ed during rounds in whic h ad a is displa yed in position i . Let M ( a, i ) be the num ber of impressions of ad a in slot i , i.e., the num ber of times that the ex-ploration policy chooses to place ad a in slot i . Finally , let CTR ( a, i ) = C ( a, i ) /M ( a, i ) be the observ ed clic k-through rate of ad a in slot i , with CTR( a, i ) defined to be 0 when M ( a, i ) = 0. 4.3.1. The Naive Estima tor Initially , one migh t think that the ADCs can be calcu-lated by taking the ratio between the global empirical clic k-through rate for eac h position i and the global empirical clic k-through rate for position 1. Formally , Unfortunately , as we will see in Section 4.4, this metho d has a bias whic h is often quite large in prac-tice. In particular, it often underestimates the ratios C i due to the fact that existing ad-serving policies gen-erally already place better ads (with higher P ( x, a )) in the better slots. To overcome this bias, we must design a new estimator. 4.3.2. A New Estima tor Consider a fixed ad a and a fixed position i &gt; 1. Clearly if a is placed in position i sufficien tly man y times, it is possible to estimate the probabilit y of a being clic ked in position i fairly accurately . If we also estimate the corresp onding clic k-through rate for ad a in position 1, we may estimate C i using a ratio of these two clic k-through rates, since C i = E x  X  D [ P ( x, a, i )] /E x  X  D [ P ( x, a, 1)]. If we perform this pro cedure for all ads, we can average the resulting es-timates to form a single, typically very accurate, esti-mate. Formally , we prop ose an estimator of the form where ~  X  is a vector of nonnegativ e constan ts  X  a for eac h ad a  X  X  .
 Theorem 6 If the ad-display policy cho oses slates in-dep endent of input and ~  X  has all positive entries, then the estimator Est ~  X  in Equation 4 is consistent. Pro of: Consider any fixed ad a and position i , and supp ose that we are only interested in rev enue gener-ated by position i . Let h be the constan t hypothesis that alw ays places ad a in position i . V D ( h ) is then E x  X  D P ( x, a, i ). From Corollary 4, it is clear that so CTR ( a, i ) con verges to E x  X  D P ( x, a, i ) for all a and i . This implies that Est ~  X  ( p ) con verges to Theorem 6 leaves open the question of how to choose ~  X  . If every comp onen t of ~  X  is set to the same value, then the estimate for C i can be view ed as the mean of all estimates of C i for eac h ad a . However, it may be the case that the estimates for certain ads are more accurate than others, in whic h case we X  X  like to weigh t those more hea vily . In particular, we may want to pick ~  X  to minimize the variance of our final estimator. Since it is difficult to analytically compute the variance of a quotien t, we appro ximate it by the variance of the sum of the numerator and denominator, as this tends to reduce the variance of the quotien t. The pro of of the follo wing theorem is omitted due to lack of space. Theorem 7 The varianc e of the expr ession subje ct to P a  X  a = 1 is minimize d when wher e  X  2 a,i is the varianc e of the indic ator random vari-able that is 1 when ad a is clicke d given that ad a is plac ed in position i .
 It is undesirable that  X  is required to have no dep en-dence on the curren t web page x t when choosing the slate of ads to displa y, since most curren t ad-serving algorithms violate this assumption. However, as we have seen in Section 3.1, when this assumption is vio-lated, exploration scavenging is no longer guaran teed to work. In the worst case, we cannot trust our esti-mated ADCs from data generated by an x -dep enden t  X  . Luc kily , in practice, it is generally not the case that extreme scenarios like the coun terexample in the pro of of Theorem 1 arise. It is more likely that the existing ad-serving algorithm and the new algorithm choose among the same small set of ads to displa y for any given con text (for example, the set of ads for whic h adv ertisers have placed bids for the curren t searc h term in the sponsored searc h setting) and the primary dif-ference between policies is the order in whic h these ads are displa yed. In suc h settings it is also the case that additional opp ortunities for exploration arise nat-urally . For example, sometimes ads run out of budget, remo ving them from consideration and forcing the ad-serving algorithm to displa y an alternate slate of ads. 4.4. Empirical comparison We are interested in comparing the metho ds dev elop ed in this work to standard metho ds used in practice. A common technique for estimating ADCs borro wed from the information retriev al literature is discoun ted cum ulativ e gain (J  X arv elin &amp; Kek  X  al  X ainen, 2002). In re-lation to our work, discoun ted cum ulativ e gain (DCG) can be view ed as a particular way to specify the ADCs that is not data-dep enden t. In particular, given a parameter b , DCG would suggest defining C i = 1 /log b ( b + i ) for all i . As sho wn next, when we estimated the ADCs using our new metho d on a large set of data we get values that are very close to those calculated using DCG with b = 2.
 We presen t coefficien ts that were computed from train-ing on about 20 million examples obtained from the logs of  X  X on ten t Matc h X , Yaho o! X  X  online adv ertise-men t engine. Since we don X  X  kno w the true variances  X  a,p for the distributions over clic ks, we heuristically assume they are all equal and use the estimator defined by  X  a = M ( a, p )  X  M ( a, 1) / ( M ( a, p ) + M ( a, 1)). The follo wing table summarizes the coefficien ts computed for the first four slots using the naiv e estimator and the new estimator, along with the DCG coefficien ts. As susp ected, the coefficien ts for the new estimator are larger than the old, suggesting a reduction in bias. 4.5. Toward A Realistic Application To reduce the high variance of the direct application of exploration scavenging to internet adv ertising, we made use of the factoring assumption and deriv ed the estimator given in Equation 3. Unfortunately this new estimator may still have an unacceptably large vari-ance. By examining Equation 3, we observ e that the metho d only benefits from examples in whic h the ex-ploration policy and the new policy h choose overlap-ping sets of ads to displa y. When ads are dra wn from a large database, this may be too rare of an event. Instead of considering policies whic h rank from the set of all ads, we can consider policies h  X  reordering the ads whic h  X  chooses to displa y. A good reordering policy plausibly pro vides useful information to guide the choice of a new ranking policy .
 We define an alternate estimator where  X  0 ( a i , x ) is the slot that h  X  would assign to ad a i in this new mo del. This metho d gives us an (un-normalized) estimate of the value of first using  X  to choose k ads to displa y in a slate and then using h  X  to reorder them. This approac h has small variance and quic kly con verges.
 To illustrate our metho d we used a training set of 20 million examples gathered using Yaho o! X  X  curren t ad-serving algorithm  X  . We let the policy h  X  be the pol-icy that reorders ads to displa y those with the highest empirical clic k-through rate first, ignoring the con text x . We used r = C j 0 /C i , (with coefficien ts given by the new unbiased metho d) to compute the num ber of clic ks we exp ect the new policy (using h  X  to reorder  X   X  X  slate) to receiv e per clic k of the old policy  X  . Here j is the relativ e position of ad a i when the ads in the slate sho wn by  X  are reordered (in descending order) by h  X  . This num ber, whic h was computed using a test set of about two million examples, turned out to be 1 . 086. When we computed the same quan tity for the policy h 0  X  that reorders ads at random, we obtained 1 . 016. Thus, exploration scavenging strongly suggests using policy h  X  over h 0  X  , matc hing our intuition. We study the pro cess of  X  X xploration scavenging, X  reusing information from one policy to evaluate a new policy , and pro vide pro cedures that work without ran-domized exploration, as is commonly required. This new abilit y opens up the possibilit y of using mac hine learning techniques in new domains whic h were previ-ously inaccessible.
 Using the derandomized exploration techniques de-scrib ed here, we sho w how to estimate the value of a policy reordering displa yed ads on logged data with-out any information about random choices made in the past. There are sev eral caveats to this approac h, but the results app ear to be quite reasonable. Note that this metho dology is simply imp ossible with-out considering metho ds for derandomized explo-ration, so the techniques discussed here open up new possibilities for solving problem.
 We are grateful to Mic hael Kearns for a useful discus-sion of the theoretical results, and to our anon ymous review ers for their though t-pro voking questions.
