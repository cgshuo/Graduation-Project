 Samaneh Azadi SAZADI 156@ GMAIL . COM UC Berkeley, Berkeley, CA School of ECE, Shiraz University, Shiraz, Iran Carnegie Mellon University, Pittsburgh We study stochastic optimization problems of the form where  X  follows some distribution over a space  X  , so that f ( x ) = is closed and convex. The function h is assumed to be closed and convex, while X and Y are compact convex sets. Problem ( 1 ) enjoys great importance in machine learn-ing: the function f ( x ) typically represents a loss over all data, while h ( y ) enforces structure or regularizes the learn-ing model and aids generalization ( Srebro &amp; Tewari , 2010 ). The linear constraints in ( 1 ) allow us to decouple f and h , and thereby consider sophisticated regularizers without having to rely on carefully tuned proximity operators X  this can greatly simplify the overall optimization algorithm and is a substantial gain ( Ouyang et al. , 2013 ; Boyd et al. , 2011 ).
 Using linear constraints to  X  X plit variables X  is an idea that has been most impressively exploited by the alternating direction method of multipliers (ADMM). As a conse-quence, ADMM leads to methods that are easy to imple-ment, scale well, and are widely applicable X  X hese bene-fits are eminently advocated in recent survey ( Boyd et al. , 2011 ), and is also the primary motivation of ( Ouyang et al. , 2013 ; Suzuki , 2013 ). Indeed, these benefits have also borne through in applications such large-scale lasso ( Boyd et al. , 2011 ), constrained image deblurring ( Chan et al. , 2013 ), and matrix completion ( Goldfarb et al. , 2012 ), to name a few.
 But despite their broad applicability, traditional ADMM methods cannot handle stochastic optimization, a draw-back recently circumvented by Ouyang et al. ( 2013 ) and ( Suzuki , 2013 ), who approached ( 1 ) using an ADMM strategy combined with ideas from ordinary stochastic convex optimization. Both ( Ouyang et al. , 2013 ; Suzuki , 2013 ) showed some experiments that suggested benefits of combining stochastic ideas with an ADMM strategy. A key benefit of such a combination is that it allows one to tackle stochastic problems with sophisticated reg-ularization penalties such as graph-structured norms and overlapping group norms ( Parikh &amp; Boyd , 2013 ), more easily than either batch or online proximal splitting methods ( Ghadimi &amp; Lan , 2012 ; Duchi &amp; Singer , 2009 ; Beck &amp; Teboulle , 2009 ).
 We remark that (deterministic) ADMM family of meth-ods are now widely used and substantial engineering ef-fort has been invested into deploying them, both in re-search as well as industry (see e.g., ( Boyd et al. , 2011 ); and also ( Kraska et al. , 2013 )). Thus, enriching ADMM to han-dle stochastic optimization problems may be of great prac-tical value.
 Contributions. The main contributions of this paper are:  X 
A new SADMM algorithm (Alg. 2 ) for strongly convex stochastic optimization that achieves the minmax opti-mal O (1 /k ) convergence rate; this improves on the previ-ously shown suboptimal O (log k/k ) rates ( Ouyang et al. , 2013 ; Suzuki , 2013 ).  X 
A new SADMM algorithm (Alg. 3 ) for stochastic convex problems with Lipschitz continuous gradients; this method achieves an O (1 /k 2 ) rate in the smooth component, improving on the previous O (1 /k ) rates of Ouyang et al. ( 2013 ); Suzuki ( 2013 ).
 Empirical results (  X  4 ) indicate that when f is strongly convex or smooth, our new methods outperform basic SADMM. We note in passing that our analysis extends to yield high-probability bounds assuming light-tailed on the errors; this extension is fairly routine, so we omit it for lack of space (see  X  5 also for other possible extensions). ADMM is a special case of the Douglas-Rachford ( Douglas &amp; Rachford , 1956 ) splitting method, which itself may be viewed as an instance of the proximal-point algorithm ( Rockafellar , 1976 ; Eckstein &amp; Bertsekas , 1992 ). But before discussing SADMM, let us first recall some material about ADMM.
 Consider the classic convex optimization problem where f , h are closed convex functions and X is a closed convex set. Introducing y = Ax  X  b , problem ( 2 ) becomes min f ( x )+ h ( y ) , x  X  X  , Ax  X  y  X  b = 0 , y  X  dom h. ADMM considers the slightly more general problem to solve which it introduces an augmented Lagrangian
L ( x, y,  X  ) := f ( x ) + h ( y )  X  X  X   X , Ax + By  X  b  X  Here  X  is the dual variable, and  X  is a penalty parameter. ADMM minimizes ( 4 ) over x and y in a Gauss-Seidel man-ner, followed by a step that updates the dual variable. The resulting method is summarized as Alg. 1 .
 For stochastic problems with f ( x ) = over a potentially unknown distribution P , the standard ADMM scheme is no longer applicable. Ouyang et al. ( 2013 ) suggest linearizing f ( x ) by considering a modified augmented Lagrangian that now depends on subgradients of f . Specifically, the augmented Lagrangian they use is
L k ( x, y,  X  ) := f ( x k )+  X  g k , x  X  + h ( y )  X  X  X   X , Ax + By where g k is a stochastic (sub)gradient of f , i.e., E [ g  X  X  ( x k ) , where g k  X   X  X  ( x,  X  k +1 ) . Replacing L by this iteration dependent L k in Alg. 1 one obtains the SADMM method of ( Ouyang et al. , 2013 ). The  X  x  X  x k  X  2 2 prox-term ensures that ( 5 ) has a unique solution, even if the aug-mented Lagrangian (AL) fails to be strictly convex; it also aids the convergence analysis. 1 Initialize: x 0 , y 0 , and  X  0 . 2 for k  X  0 do 6 end Since SADMM borrows techniques from stochastic subgra-dient methods, it is natural to expect similarities in conver-gence guarantees. Previous authors ( Ouyang et al. , 2013 ; Suzuki , 2013 ) showed that for uniformly averaged iterates (  X  x bound Exploiting properties of the stochastic part f , we can obtain more refined rates. Specifically, if f is strongly convex, the E [  X  ] = O ( 1 ( 2013 ) and Suzuki ( 2013 ) (though for the RDA-ADMM variant in ( Suzuki , 2013 ), no refined rate for strongly con-vex losses was proved).
 However, these rates are suboptimal. It is of great interest to obtain optimal rates X  X ee the long line of work in stochastic optimization ( Nemirovski et al. , 2009 ; 2012 ), where impressive effort has been expended to obtain optimal rates; this effort can even translate into improved empirical performance. For deterministic set-tings, notable examples of optimal methods are given by ( Beck &amp; Teboulle , 2009 ; Nesterov , 2007 ), which often substantially outperform their non-optimal counterparts. In light of this background, we are now ready to present new SADMM methods, which achieve the minimax opti-mal O ( 1 k ) rate for strongly convex losses. Without strong convexity, the rate for the nonsmooth and stochastic parts is O ( 1 k )+ O ( 1  X  contribution from the smooth part of the objective. We begin by stating our key structural assumptions: 1. Bounded subgradients: E [  X  g k  X  2 2 ]  X  G 2 (for the 2. Bounded noise variance: E [  X  g k  X  X  X  f ( x )  X  2 2 ]  X  3. Compactness of X , Y ; bounded dual variables. Unfortunately, we must impose somewhat stricter assump-tions than ordinary SADMM X  X his seems to be the price that we have to pay for faster convergence X  X he discussion in ( Chambolle &amp; Pock , 2011 ) sheds more light on these as-pects (especially Assumption 3).
 Following He &amp; Yuan ( 2012 ) we introduce the notation The operator F (  X  ) satisfies a simple but useful property Moreover, for an optimal w  X   X   X  , and any w  X   X  , we have f ( x )  X  f ( x  X  )+ h ( y )  X  h ( y  X  )+  X  w  X  w  X  , F ( w Therefore, a vector  X  w  X   X  is  X  -optimal for the deterministic ADMM problem (for  X  &gt; 0 ) if it satisfies f (  X  x )  X  f ( x )+ h (  X  y )  X  h ( y )+  X   X  w  X  w, F ( w ) As in ( He &amp; Yuan , 2012 ), Ouyang et al. ( 2013 ) also use this variational characterization of optimality and seek to bound it in expectation. We too use this characterization; we first estimate it after one step of our SADMM algorithm to eventually bound it in expectation. We are now ready to describe the our SADMM algorithms (  X  3.1 and  X  3.2 ). 3.1. SADMM for strongly convex f When f is  X  -strongly convex, we use essentially the same SADMM method as in ( Ouyang et al. , 2013 ) (shown as Alg. 2 ). The key difference lies in how the iterates gen-erated by the Alg. 2 are averaged to obtain an optimal rate. 1 Initialize: x 0 , y 0 , and  X  0 2 for k  X  0 do 3 Obtain stochastic gradient g k ; build L k via ( 5 ) 7 end Algorithm 2: Stochastic ADMM (strongly convex) We begin our analysis by introducing some more notation Here,  X  k is related to the diameter of the primal variable x  X  X , A satisfied, L k measures distance between dual variables, D measures a diameter-like term for the primal variable y  X  Y , while  X  is a parameter that bounds the dual variables. Lemma 1 is a key result that describes progress made at one step. Upon taking suitable expectations, it leads to Thm. 2 . Lemma 1. Let f be  X  -strongly convex, and let x k +1 , y k +1 and  X  k +1 be computed as per Alg. 2 . For all x  X  X and y  X  X  , and w  X   X  , it holds for k  X  0 that f ( x k )  X  f ( x ) + h ( y k +1 )  X  h ( y ) +  X  w k +1  X  To use Lemma 1 to obtain an optimal SADMM method, we use an idea that has also found success for the stochastic subgradient method X  X ee e.g., ( Lacoste-Julien et al. , 2012 ; Shamir &amp; Zhang , 2013 ) X  X he idea is to use nonuniform av-eraging of the iterates where more recent iterates are given higher weight. For SADMM, some care is required to en-sure that the nonuniform weighting does not conflict with the augmented Lagrangian (AL) terms.
 We propose to use the following weighted iterates: 1  X  x It is important to note that these weighted averages can be maintained in an online manner. Indeed, given  X  x k  X  1 , we can update the weighted average as where  X  k = 2 / ( k +2) ; similar updates apply for  X  y k These weighted averages in combination with Lemma 1 help prove our main theorem on SADMM.
 Theorem 2. Let f be  X  -strongly convex. Let  X  k = 2 ( k +2) let x ; y j ,  X  j be generated by Alg. 2 , and  X  x k ,  X  y puted by ( 9 ) . Let x  X  , y  X  be the optimal; then for k E [ f (  X  x k )  X  f ( x  X  ) + h (  X  y k )  X  h ( y  X  ) +  X   X  3.2. SADMM for smooth f Obtaining an optimal version of SADMM for Lipschitz-smooth f  X  C 1 L proves considerably harder.

Input : Sequence (  X  k ) of interpolation parameters; 1 Initialize: x 0 = z 0 , y 0 . 2 for k  X  0 do 3 p k  X  (1  X   X  k ) x k +  X  k z k 4 Get stochastic gradient g k s.t. E [ g k ] =  X  f ( p k ) 9 end Alg. 3 depends on several careful modifications to the ba-sic SADMM scheme. First, it uses interpolatory sequences ( p k ) and ( z k ) , as well as  X  X tepsizes X   X  k (this is inspired by techniques from fast-gradient methods ( Tseng , 2008 ; Nesterov , 2004 )). Second, x is updated ( cf. Line 4 in Alg. 2 ) by first computing z k +1 , which in turn uses a weighted prox-term that enforces proximity to z k instead of to x k . Third, the update to y uses an AL term that depends on z k +1 instead of x k +1  X  X his change is for simplifying the analysis; one could continue to use an AL term based on x k +1 , but at the expense of much more tedious analy-sis. Finally, an important modification is to the augmented Lagrangian, which is now defined as  X 
L k ( x, y,  X  ) := f ( x k )+  X  g k , x  X  + h ( y )  X   X  k  X  for suitable parameters (  X  k ,  X  k ) .
 We begin our analysis by again stating a key lemma that measures per-step progress; here we use slightly different notation by redefining in w and w k ( 6 ) as Lemma 3. Let x k +1 , y k +1 , z k +1 be generated by Alg. 3. For x  X  X  , y  X  X  and w  X   X  , and with  X  k = ( L +  X  k )  X  1 the following bound holds for all k  X  0 : The proof of this inequality is lengthy and tedious, so we leave it in the supplement. Lemma 3 proves crucial for doing the induction to obtain the next main step towards our convergence proof. Let R := sup x  X  X   X  x  X  x  X   X  2 . Then, Lemma 4. Using the notation of Lemma 3 , for 1  X  k +1 2 As before, to state our convergence theorem, we average the iterates generated by Alg. 3 non-uniformly. This tech-nique is borrowed from the analysis of accelerated methods, see e.g., ( Ghadimi &amp; Lan , 2012 ). To use Lemma 4 to obtain our main convergence result, we introduce weighted candi-date solution vectors. For k  X  0 , we define the weighted iterates (it is important to note that these weighted aver-ages can be easily maintained in an online manner ( cf. for-mula ( 10 )): where  X  j = 2( j +1) / ( k +1)( k +2) . Since f ( x ) is smooth, it turns out that in ( 12 ) we do not need to average over x thereby obtaining  X  X on-ergodic X  convergence in expecta-tion for the smooth part. This is an interesting technical difference from nonsmooth f ( x ) , where one needs to aver-age over the x iterates too unless one is willing to pay an additional log k penalty ( Shamir &amp; Zhang , 2013 ). Finally, we have the following theorem.
 Then for  X  j = 1 and k  X  0 , An immediate corollary is our refined result on the conver-gence rate of SADMM with a smooth stochastic objective (notice that h is assumed to be nonsmooth): and  X  j = 2 / ( j + 1) ; then E [ f (  X  x k )  X  f ( x  X  ) + h (  X  y k )  X  h ( y  X  ) +  X   X   X  2 LR 2 Observe that when there is no noise (  X  = 0 ), our analysis can be slightly modified to yield the bound E [ f (  X  x k )  X  f ( x  X  ) + h (  X  y k )  X  h ( y  X  ) +  X   X   X  2 LR 2 In this section we present experiments that illustrate perfor-mance of our SADMM variants. The results indicate that our methods converge faster (on the generalization error) than previous SADMM approaches. We note that for all all experiments, we set the AL parameter  X  = 1 , as also done in Ouyang et al. ( 2013 ). 4.1. GFLasso with smooth loss Our first experiment follows Ouyang et al. ( 2013 ), wherein we consider the Graph-guided fused lasso (GFlasso). This problem uses a graph-based regularizer where variables are considered as vertices of the graph and the difference be-tween two adjacent variables is penalized according to the edge weight. This leads to the optimization problem: min E [ L ( x,  X  )] +  X   X  x  X  1 +  X  where E is the set of edges in the graph, and w ij is the weight for the edge between x i and x j . To verify perfor-mance of Alg. 3 we consider the following  X  X arge-margin X  modification to ( 13 ): min E [ L ( x,  X  )] + 2  X  x  X  2 2 +  X   X  y  X  1 , s.t. F x  X  where F ij = w ij , F ji =  X  w ij for all edges { i, j } X  X  training sample  X  . This formulation is a special case of ( 1 ) with A = F , B =  X  I and b = 0 . The corresponding steps of Alg. 3 assume the form z x y  X  where S ( x ) denotes the standard soft-thresholding opera-tor As in Ouyang et al. ( 2013 ), we obtain F by sparse in-verse covariance selection Banerjee et al. ( 2008 ) to deter-mine the adjacency matrix of the graph by thresholding the sparsity pattern of the inverse covariance matrix. We compare the following methods: SADMM ( Ouyang et al. , 2013 ), Alg. 3 (called Optimal-SADMM 2 ), ordinary stochastic gradient descent (SGD), proximal-SGD (aka FOBOS ( Duchi &amp; Singer , 2009 )), and online RDA ( Xiao , 2010 ). We compare these methods on a version of the well-known 20newsgroups dataset 3 . This dataset consists of binary occurrence data of 100 words for 16,242 instances, and the samples are labeled into four categories for which one can do classification by one-vs rest scheme multiclass classification. In Fig. 1 , we show prediction accuracy on test data (20% of samples) and the training performance as measured by the objective function value.
 To implement proximal-SGD and online-RDA, the two methods that require computing the proximity operator  X  x  X  y  X  2 2 +  X   X  F x  X  1 , we implemented an inexact QP-solver that solves the corresponding dual problem 4 : If u  X  is the optimal dual solution, we can recover the primal solution by setting x  X  = y  X  f T u  X  .
 Fig. 1 shows that on the training data SADMM and Optimal-SADMM converge faster than the other methods. The classification performance of all methods is similar, except Optimal-SADMM which achieves higher test accu-racy (notice #-iterations refers to number of training data points seen). Also, once we made a single pass through the training data we terminate all the methods. 4.2. Overlapped group lasso In our second experiment, we present overlapped group lasso results, as explained in ( Suzuki , 2013 ). Here, where G is a set of groups of indices. Feature selection us-ing non-overlapping groups of features by the Lasso can be extended to the group Lasso. But using only non-overlapping groups limits the discoverable structures in practice. One of the solutions to handle this problem is to allow overlapping groups accompanied with the fol-lowing settings. We divide G into m non-overlapping subsets G 1 ,  X  X  X  , G m , and let Ax be a concatenation of m -repetitions of x . Thus, h ([ x ;  X  X  X  ; x ]) = h ( Ax ) = C  X  the optimization problem using a proximal operation for each subset; see ( Qin &amp; Goldfarb , 2012 ) for more details. We applied our optimal SADMM on a dataset for a binary where L ( x,  X  j ) = log(1 + e  X  l j s T j x ) ,  X  x  X  block  X  sion of x as a square matrix; observe that L ( x,  X  ) is a logis-tic loss and h ( y ) is the overlapping group lasso regularizer. We used the dataset  X  X dult X  5 which contains 123 dimen-sional feature vectors. Following Suzuki ( 2013 ) we also augmented the feature space by taking products of features resulting in (123+123 2 ) dimensions. Vector x (1) in ( 15 ) is related to the 123-first elements of x , while x (2) represents the rest of x . Hyperparameter C is set to 0.01. Moreover, we used mini-batches of size 10 for each iteration. We present plots on the test data classification accuracy as well as the training data objective functions. We compare ordinary SADMM, Optimal-SADMM, and RDA-ADMM.
 On this task, the difference between SADMM and Optimal-SADMM is not remarkable, but both substantially outper-form RDA-SADMM, as seen from Fig. 2 . 4.3. Strongly Convex Loss Functions Now we show a more detailed comparisons between SGD, Proximal SGD, strongly convex SADMM from ( Ouyang et al. , 2013 ), and our Optimal-SADMM version of Alg. 2 . In this case, we compare the men-tioned algorithms on a nonsmooth but strongly convex GFLasso and Group Lasso problems, which use the hinge loss for L ( x,  X  ) in ( 4.1 ) and ( 15 ), respectively ( L ( x,  X  ) = max { 0 , 1  X  ls T x } ). Other terms remain the same. The closed form updates are similar to those in the previous section, except that x k is used instead of z k ; also  X  x k is computed as per ( 10 ). Step size equal to 1 k is used for the SGD and proximal SGD methods. The classification accuracy on hold out test data and the objective function value on the training data are plotted in Figures 3 and 4 . The plots indicate that the proposed algorithm significantly outperforms other methods, both in terms of training objective value and classification accuracy except for the objective function value on the training data in Fig. 3 in which our optimal-SADMM training performance dips a bit in comparison with SGD and proximal-SGD methods. 4.4. Experiment on Synthetic Data Here, we intend to explore the behavior of smooth SADMM variants as the number of features in the data varies. For this experiment, we generated a synthetic dataset, and performed classification using the smooth for-mulation ( 4.1 ). The data is generated as follows: we sam-ple a matrix of m samples with n features following a multivariate Gaussian with a random covariance matrix. The true weight vector x  X  is chosen from an i.i.d. stan-dard normal; the labels are defined according to l m = sgn( s T m x  X  +  X  m ) in which  X  m is a mean zero Gaussian noise with standard deviation of 2.
 Fig. 5 reports percentage improvement of Optimal-SADMM over SADMM in terms of classification accuracy as a function of number of features. This experiment sug-gests that our optimal SADMM may use features more ef-ficiently, especially with increasing feature dimension. Ex-ploring this phenomenon more closely is ongoing work. We presented two new accelerated versions the stochastic ADMM ( Ouyang et al. , 2013 ). In particular, we presented a variant that attains the theoretically optimal O (1 /k ) conver-gence rate for strongly convex stochastic problems. When the stochastic part is smooth, we showed another SADMM algorithm that has an optimal O (1 /k 2 ) dependence on the smooth part.
 Our initial experiments reveal that our accelerated vari-ants do exhibit notable performance gains over their non-accelerated counterparts (see  X  4 , also Fig. 3 ) X  though, obviously as also seen from the experiments in ( Lacoste-Julien et al. , 2012 ; Shamir &amp; Zhang , 2013 ), gains in stochastic settings are less dramatic than in the deterministic case ( Beck &amp; Teboulle , 2009 ). This is not surprising, since accelerated methods are more sen-sitive to stochastic noise than their deterministic counter-parts ( Devolder et al. , 2011 ).
 There will be more results and details available in the longer arXiv version of the paper. We mention below a list of extensions to the present paper:  X  Transfer the O (log k/k ) convergence rate of the last  X  Obtaining high-probability bounds under light-tailed  X  Incorporate the impact of sampling multiple stochas- X  Derive a mirror-descent version.  X  Improve rate dependence of the augmented La-Most, except the last, of these extensions are easy (though tedious) and follow by invoking standard techniques from the analysis of stochastic convex optimization. We hope to address these in a longer version of the present paper. We conclude by highlighting that our empirical results are encouraging and suggest that for strongly convex or smooth losses, our accelerated SADMM variants outperform the other known SADMM methods.
 Banerjee, O., Ghaoui, L. E., and d X  X spremont, A. Model selection through sparse maximum likelihood estimation for multivarite Gaussian or binary data. JMLR , 9:485 X  516, 2008.
 Beck, A. and Teboulle, M. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM J. Imaging Sciences , 2(1):183 X 202, 2009.
 Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statisti-cal learning via the alternating direction method of multi-pliers. Foundations and Trends R  X  in Machine Learning , 3(1):1 X 122, 2011.
 Chambolle, A. and Pock, T. A First-Order Primal-Dual Al-gorithm for Convex Problems with Applications to Imag-ing. J. Math. Imaging Vis. , 40(1):120 X 145, 2011. Chan, R. H., Tao, M., and Yuan, X. Constrained total vari-ation models and fast algorithms based on alternating di-rection method of multipliers. SIAM J. Imaging Sci. , 6 (1), 2013.
 Chen, Xi, Lin, Qihang, and Pena, Javier. Optimal regular-ized dual averaging methods for stochastic optimization.
In Advances in Neural Information Processing Systems (NIPS-12) , pp. 404 X 412, 2012.
 Devolder, O., Glineur, F., and Nesterov, Yu. First-order methods of smooth convex optimization with inexact or-acle. Technical Report 2011/17, UCL, 2011.
 Douglas, J. and Rachford, H. H. On the numerical solution of the heat conduction problem in 2 and 3 space vari-ables. Tran. Amer. Math. Soc. , 82:421 X 439, 1956. Duchi, J. and Singer, Y. Online and Batch Learning us-ing Forward-Backward Splitting. J. Mach. Learning Res. (JMLR) , 2009.
 Eckstein, J. and Bertsekas, D. P. On the Douglas-
Rachoford splitting method and the proximal point al-gorithm for maximal monotone operators. Math. Prog. , 55(3):292 X 318, 1992.
 Ghadimi, S. and Lan, G. Optimal stochastic approximation algorithms for strongly convex stochastic composite op-timization, i: a generic algorithmic framework. SIAM J. Optimization , 22:1469 X 1492, 2012.
 Goldfarb, D., Ma, S., and Scheinberg, K. Fast alternat-ing linearization methods for minimizing the sum of two convex functions. Math. Prog. Ser. A , 2012.
 He, B. and Yuan, X. On non-ergodic convergence rate of
Douglas-Rachford alternating direction method of multi-pliers. Unpublished , 2012.
 Kraska, T., Talwalkar, A., J.Duchi, Griffith, R., Franklin, M., and Jordan, M.I. MLbase:A Distributed Machine
Learning System. In Conf. Innovative Data Systems Re-search , 2013.
 Lacoste-Julien, S., Schmidt, M., and Bach, F. A simpler approach to obtaining an O (1 /t ) convergence rate for projected subgradient descent. arXiv:1212.2002 , 2012. Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Ro-bust stochastic approximation approach to stochastic pro-gramming. SIAM Journal on Optimization , 19(4):1574 X  1609, 2009.
 Nesterov, Y. Introductory Lectures on Convex Optimiza-tion: A Basic Course . Springer, 2004.
 Nesterov, Yu. Gradient methods for minimizing composite objective function. Technical Report 2007/76, Universit  X  e catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2007.
 Ouyang, Hua, He, Niao, Tran, Long, and Gray, Alexan-der G. Stochastic alternating direction method of multi-pliers. In Proceedings of the 30th International Confer-ence on Machine Learning (ICML-13) , pp. 80 X 88, 2013. Parikh, N. and Boyd, S. Proximal Algorithms , volume 1. NOW, 2013.
 Qin, Z. and Goldfarb, D. Structured sparsity via alternating direction methods. J. Machine Learning Research , 13: 1435 X 1468, 2012.
 Rockafellar, R. T. Monotone Operators and the Proximal
Point Algorithm. SIAM J. Control Optim. , 14(5):877 X  989, 1976.
 Shamir, Ohad and Zhang, Tong. Stochastic gradient de-scent for non-smooth optimization: Convergence results and optimal averaging schemes. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) , pp. 71 X 79, 2013.
 Srebro, N. and Tewari, A. Stochastic Optimization for Ma-chine Learning. ICML 2010 Tutorial, 2010.
 Suzuki, Taiji. Dual averaging and proximal gradient de-scent for online alternating direction multiplier method. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) , pp. 392 X 400, 2013.
 Tseng, P. On accelerated proximal gradient methods for convex-concave optimization. Unpublished , 2008. Xiao, L. Dual averaging methods for regularized stochastic learning and online optimization. JMLR , 11:2543 X 2596,
