 Johns Hopkins University
I am very grateful for the award you have bestowed on me. To understand your generosity I have to assume that you are honoring the leadership of three innovative groups that I headed in the last 47 years :at Cornell, IBM, and now at Johns Hopkins. You know my co-workers in the last two teams. The Cornell group was in Information
Theory and included Toby Berger, Terrence Fine, and Neil J. A. Sloane (earlier my Ph.D. student), all of whom earned their own laurels.
 ideas. So I will tell you about my beginnings and motivations and then focus on the contributions of my IBM team. In this way the text will have some historical value and may clear up certain widely held misconceptions. 1. Beginnings
Information Theory seemed to be one of the most prestigious disciplines during my years as a student at MIT (1954 X 1962). The faculty included the founders of the field X  proach Shannon himself, I asked Professor Fano to be my thesis adviser. I was making slow progress when in 1961, after three years of trying, I succeeded in extricating my future wife Milena from communist Czechoslovakia (how this was accomplished is another story) and married her. One problem we needed to solve was how she should occupy herself during the long hours I was spending in the underground stacks of the
University Professor at Harvard and an Institute Professor at MIT. Russian by origin, the founders of the Prague Linguistic Circle. He had a Czech wife, the anthropologist
Svatava Pirkova. He continued to maintain his connections with Czechs, and even young Czechs. I was invited to dinner at his house several times, once also with the newly arrived Milena. Jacobson was well known to have an eye for beautiful young women and he was reputed to enjoy exercising his influence. When my wife asked him for advice as to what to do, he suggested that she take up a fellowship at MIT which he would arrange for her to get. As promised, she got the fellowship and enrolled in the
Ph.D. program of the Linguistics department. It should be appreciated that Jacobson did not interview her in a non-social setting and was aware that her previous schooling in Prague consisted only of one year of Slavic studies followed by two years at the Film Academy.
 sat in with her and got the crazy notion that I should switch from Information Theory to
Linguistics. I went so far as to explore this notion with Professor Chomsky. Of course, word got around to my adviser Fano, whom it really upset. He declared that I could contemplate switching only after I had received my doctorate in Information Theory. I had no choice other than to obey. Soon thereafter, my thesis almost finished, I started interviewing at universities for a faculty position. After my job talk at Cornell I was would accept the Cornell offer and help develop his ideas on how to apply Information
Theory to Linguistics. That decided me. Surprisingly, when I took up my post in the fall of 1962, there was no sign of Hockett. After several months I summoned my courage and went to ask him when he wanted to start working with me. He answered that he was no longer interested, that he now concentrated on composing operas.
 was the golden period of government support for science and technology. It seemed easy to get grants. Perhaps that was the reason I neglected to make any arrangements for work during the coming summer of 1972. I phoned Joe Raviv (whom I knew as a colleague from my sabbatical at IBM in 1968 X 69) to ask if I could spend three months in his group in Yorktown Heights. His answer was  X  X ertainly, the sooner you arrive the better. We are starting to work on speech recognition. X  of the IBM Scientific Center in Haifa, and IBM was negotiating with Professor Jonathan
Allen of MIT to take over the speech group. These negotiations were not successful, and several weeks later the job was offered to me. I requested Cornell to grant me a leave of absence; they did, and I joined IBM (the following year I asked for and got another year, but when I tried to carry out the same maneuver in 1974, I was turned down). when all the need for further improvements would disappear, and IBM business would dry up. Somebody came up with the suggestion that speech recognition would require
The group included John Cocke (inventor of RISC machines), Herman Goldstine (right hand of von Neumann in research leading to ENIAC) and others. It recommended that a Continuous Speech Recognition group be established in the Research Division. At the time IBM had a small speech group in one of its development laboratories in Raleigh, NC. (Actually, IBM  X  X ad X  speech recognition even earlier. At the 1964 World X  X 
Fair in New York, Ernest Nassimbene demonstrated an isolated digits recognizer  X  X n a shoe box. X ) Its three main members, Das, Dixon, and Tappert, were transferred from
Raleigh to the Research Division in Yorktown. High management concluded that to get
Fred Damerau, Stan Petrick, and Jane Robinson from linguistics to CSR. The staffing of the group was then completed by volunteers from the Computer Sciences Department:
Lalit Bahl, Raimo Bakis, George Nagy, and others (later Jim and Janet Baker joined as well). But at the time only Bakis, Das, Dixon, and Tappert knew anything about speech.
Towards the end of the summer I took over the direction of the group and received a gift from heaven :the freshly graduated physicist Robert Mercer, who in the spring accepted an IBM job in a group that was abolished before he arrived in September. 484 2. The Competition In 1971, parallel to the work of the IBM task force, ARPA established a project in Speech
Understanding. I don X  X  know what led to that decision, but the main forces behind it were Allen Newell and J. C. R. Licklinder. Funds were provided to Carnegie Mellon, Systems Development Corporation, Bolt Beranek &amp; Newman, and probably SRI,
Sperry-Univac, University of Pennsylvania, UC Berkeley, and UCLA. Not all of these institutions were to field complete systems. For instance, Ohio, UCLA, and Berkeley provided consulting by linguists (Peter Ladefoged, Vicky Fromkin, John Ohala, Michael O X  X alley, and June Shoup).
 Bonnie Nash-Webber, George White, Fil Alleva, Wayne Ward, Don Walker, Victor Zue,
Stephanie Sennef, Bill Woods, John Makhoul, Wayne Lea, Beatrice Oshika, and Janet and Jim Baker. IBM was invited to attend the meetings, but we did not compete. ARPA was a six-year project which was supposed to recognize (and interpret?) sentences from a  X  X esource Management X  grammar; for an example of the sentences generated, see
Table 1. At the end of the six-year period the project was declared a success because it  X  X et its goals. X  implemented by the Bakers, graduate students at CMU. It used Hidden Markov models (HMMs) whereas the rest of the ARPA participants based their work on templates,
Dynamic Time Warping (DTW), and hand-written rules. The best of these latter systems was Harpy, developed by another CMU graduate student, Bruce Lowerry. 3. IBM X  X  Initial Formulation For our first task we decided to recognize sentences generated by the so-called New grammar is actually a Hidden Markov Model. State transitions generate words and are taken with uniform probability. Generation starts in the initial state, a transition is taken, and a word from the list associated with that transition is selected with uniform probability; then one of the transitions out of the new state is taken (again with uniform probability), a word corresponding to that transition is again selected at random, a new grammar generates such bizarre sentences as are shown in Table 2. 486 speakers and were recorded in a special sound-proofed room. The recorded speech signal was transformed into a string A of symbols (one symbol per centisecond) chosen from an  X  X lphabet X  A of size 200. The alphabet A itself was extracted by vector quantization from a training sample of the speech signal.
 ure 2. The entire operation can be regarded as transmission through a noisy channel, the basic problem of Information Theory. 2 Its corresponding mathematical formula is where W denotes a string of words, A denotes the acoustic signal observed by the recognizer, and S is the set of sentences that can be generated by the grammar. The formula calls for a statistical approach. The noisy channel of Figure 2 has input W and must attempt to estimate the channel probabilities P ( A |
P ( W ) that the speaker will utter the word string W . Once the models P ( A are established, the recognizer when it observes A can conduct a search leading to the maximizing word string W .
 P ( W ) is in fact equal to the actual probability P ( W ) . Furthermore, because the set word strings W over which we maximize can be listed, the difficulty of the task can be measured approximately (because the acoustic similarity of words is not taken into account) by entropy:
However, because the participants in the ARPA project introduced the false measure  X  X ranching factor X  (which was the arithmetic mean of the out-of-state branching of their finite X  X tate grammar), we replaced H ( W ) as a measure of difficulty by perplexity , defined by It turned out that the New Raleigh grammar had approximate perplexity 7 whereas the Resource Management grammar had 2.
 (1975), Bahl and Jelinek (1975), and Jelinek (1976). 4. The Tangora recognizing  X  X atural X  speech. We settled on a 5,000-word vocabulary and set ourselves
Our ambition was to use a combination of IBM array processors to achieve essentially real-time performance. We fulfilled a promise to the management to achieve it by 1984. discrete speech, where sentences were spoken with pauses between words. We got rid of the sound room and recorded the readings on close-talking microphones. The system was to be speaker-sensitive, that is, acoustic models were trained for each individual reader separately.

Jim and Janet Baker, who were not to receive their Ph.D.s from Carnegie Mellon until a year later. At the same time,  X  X ur X  three linguist helpers returned to their original group.
 approach ought to be somehow related to English grammar. The linguist Stan Petrick, course he never did, and the phrase acquired a mythical status in the manner of  X  X amous last words. X  488 used the approximation
But this decision did not dispose of the problem. How should we estimate the basic provided us with the read speech). But relative frequencies would not suffice. Indeed, the speech would frequently involve trigrams w whose count C ( w k  X  2 , w k  X  1 , w k ) in the training corpus equalled 0. Then if w were uttered by the speaker (reader), the recognizer would necessarily make an interpolation: where  X  j s would be non-negative, would satisfy  X  3 +  X  optimally chosen (we knew how).
 recognizer were a string of centisecond symbols chosen from an alphabet itself was derived by vector quantization. This discretization of speech allowed for an easier estimate of parameters defining the acoustic processor model P ( A pronunciation lexicon) is shown in Figure 3.
 the acoustic and language models P ( A | W )and P ( W ), it remains to discuss the search for the recognizer output W implicit in the formula
We used the appropriately modified Viterbi algorithm version of dynamic program-ming (Viterbi 1967), a decision we took in spite of the fact that the algorithm would carry out a sub-optimal search.
 nonstop typing, a rate of 147 words per minute. Incredibly, it was estimated that Tangora executed an average of twelve-and-a-half strokes per second! 5. Some ASR Firsts
During my time at IBM, my colleagues and I pioneered various techniques that were later taken over by the entire field. Here are some examples: 490 6. IBM Influence on the Speech and Language Field invitations to share our methods with a wider audience. We presented several courses teaching our data-centric approach. In 1980 we gave a course in Udine, Italy, organized at MIT; and finally in 1986 a course in Oberlech, Austria, organized by IBM Scientific
Centers. Furthermore, I was invited to give a keynote speech at the 1990 ACL Meeting in Pittsburgh, PA.
 were  X  X ure X  that an appropriate grammatical approach would be better. Because we wanted to stick to our data-centric philosophy, we thought that what was needed as training material was a large collection of parses of English sentences. We found out that researchers at the University of Lancaster had hand-constructed a  X  X reebank X  under the guidance of Professors Geoff Leach and Geoff Sampson (Garside, Leech, and Sampson 1987). Because we wanted more of this annotation, we commissioned above all was quantity, possibly at some expense of quality :We wanted to extract the grammatical language model statistically and so a large amount of data was required.
Another belief of ours was that the parse annotation should be carried out by intelligent native speakers of English, not by linguistic experts, and that any inaccuracies would naturally cancel each other. Indeed, the work was done by Lancaster housewives led by a high school drop-out.
 and I explained to him what was needed and why. He immediately entrusted Charles with the creation of the appropriate organization. One of the problems was where the eventual corpus should reside. Deep-pocketed IBM would be unsuitable :Possessors of desirable corpora would charge immoderate sums for the acquisition of rights. I thought that only a university would do. So I inquired of Aravind Joshi and Mitch Marcus (and perhaps even Mark Liberman) at the 1988 Conference of Applied Natural Language Processing in Austin whether the required site could be the University of Pennsylvania.
My colleagues were interested, and Charles Wayne invited appropriate people to a meeting at the Lake Mohunk Mountain House to discuss the matter. That is how the Linguistic Data Consortium was born.

NSF grant for grammatical development. The eventual result was SPATTER, imple-and grammar derivation. Ezra Black of IBM organized a committee whose aim was the specification of a metric suitable for evaluation of parser performance. The result was the PARSEVAL measure. 7. Machine Translation of us started to wonder in the mid 1980s whether our ASR methods could be success-fully applied to new fields. Bob Mercer and I spent many of our after-lunch  X  X eriphery X  walks discussing possible candidates. We soon came up with two :machine transla-tion and stock market modeling. It is probably only coincidence that Bob eventually ended up investigating the possibilities of stock value prediction. Indeed, he and Peter Brown departed IBM in 1993 to work for the phenomenally successful hedge fund
Renaissance Technologies. Eventually at least 10 former members of the IBM CSR group were to be employed by that same company. The performance of the Renaissance fund is legendary, but I have no idea whether any methods we pioneered at IBM have operation! in Figure 4, is practically identical to that of ASR. In fact, even the basic formulas are identical except for a change in the letters that designate the variables: translator is to ferret out the speaker X  X  original thought E .
 of the task, but because our knowledge of English would allow us to judge the quality 492 of the translation. We wanted to make the problem real, yet as easy as possible. So we looked for a language F that was relatively close to English. The answer was French.
Because we wanted the process to be data-centric, we searched for a pair of corpora F and E that would be translations of each other. Luck was with us :The Canadian par-liament Hansards (proceedings) were maintained in English and French. language translation was born (Brown et al. 1990), and the descendants of our original methods are being continually improved.
 a small institute whose location was opposite New York Grand Central Station on 42nd Street. The institute advertised that it would teach French to anybody in two (!) weeks of intensive immersion. We didn X  X  believe it, of course, but because the costs and location were convenient, we started on our daily commute. I will not go into the semi-fraudulent aspects of the operation, but Lalit Bahl, Peter Brown, Bob Mercer, and I had a lot of fun and did advance considerably our knowledge of French.
 the rejection review we received: 8. Conclusion Research in both ASR and MT continues. The statistical approach is clearly dominant. significant progress, we are very far from solving the problems. That is a good thing:
We can continue accepting new students into our field without any worry that they will have to search, in the middle of their careers, for new fields of action. References
