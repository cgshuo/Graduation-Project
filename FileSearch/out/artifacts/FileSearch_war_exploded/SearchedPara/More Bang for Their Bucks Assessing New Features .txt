 Online search systems that display ads continually offer new features that advertisers can use to fine-tune and enhance their ad campaigns. An important question is whether a new feature actually helps advertisers. In an ideal world for statisticians, we would answer this question by running a statistically designed experiment. But that would require randomly choosing a set of advertisers and forcing them to use the feature, which is not realistic. Accordingly, in the real world, new features for advertisers are seldom evaluated with a traditional experimental protocol. Instead, customer service representatives select advertisers who are invited to be among the first to test a new feature (i.e., white-listed), and then each white-listed advertiser chooses whether or not to use the new feature. Neither the customer service representative nor the advertiser chooses at random. This paper addresses the problem of drawing valid infer-ences from whitelist trials about the effects of new features on advertiser happiness. We are guided by three principles. First, statistical procedures for whitelist trials are likely to be applied in an automated way, so they should be robust to violations of modeling assumptions. Second, standard anal-ysis tools should be preferred over custom-built ones, both for clarity and for robustness. Standard tools have with-stood the test of time and have been thoroughly debugged. Finally, it should be easy to compute reliable confidence intervals for the estimator. We review an estimator that has all these attributes, allowing us to make valid inferences about the effects of a new feature on advertiser happiness. H.4 [ Information Systems Applications ]: Miscellaneous; G.3 [ Mathematics of Computing ]: Probability and Statis-tics X  Experimental Design Statistical Inference, Biased Sampling, Propensity Scores, Causal Modeling Randomized experiments are commonplace in the search en-gine (SE) industry. They are used to evaluate new ranking functions, changes to the user interface, and new algorithms for ad placement. These changes are typically tested on a sample of users that are chosen by randomly directing each query or cookie (depending on the study design) to the new conditions or the standard operating conditions.
 Advertisers provide the revenue that allows search engines to provide free services to their users. SEs do their best to ensure that advertisers, like users, are happy by providing tools to manage and tune ads campaigns. These tools are continually improved and expanded according to changing business needs and advertiser feedback. When a new feature is introduced in the ads system front-end, it is often tested on a selected (i.e., white-listed) subset of advertisers before it is introduced to the entire advertiser base. Developers have limited control over which advertisers are white-listed, as this largely depends on customer service representatives (CSRs). CSRs whitelist advertisers for a variety of reasons, including the need to test functionality for both large and small advertisers and the desire to target advertisers that have requested the feature in the past. Indeed, these lat-ter advertisers are especially important as they are likely to exercise the new feature because they are interested in it. The purpose of the whitelist trial is largely to ensure that the feature is bug-free and meets the desired performance criteria. But it is also of interest to learn if the feature in-creases customer satisfaction. Will advertisers be happier if they use the feature than if not? We could ask advertisers directly if they are happy with the new feature, but self-reported satisfaction is often unreliable. Advertisers often tell you one thing, but their behavior in-dicates otherwise. Conventional thinking is that advertiser happiness is better reflected by their retention and spending. If advertisers feel that their return on investment is high, they will direct more money to their ad campaigns. Other-wise they will continue to spend at their current level, or, even worse, decrease their spending. We propose to address advertiser happiness in whitelist studies through metrics like retention and comparisons of pre-feature and post-feature spending behavior, correcting for the biases introduced by the whitelist selection process.
 This paper is organized as follows. Section 2 introduces the application and some of the complexities due to its non-random nature. Since the feature is not yet launched, we are not able to identify its exact nature, but the details are not germane to understanding the methodology. Section 3 then describes the basic statistical model for estimating ef-fects in observational (non-randomized) studies. Section 4 reviews the notion of propensity scores, which measure selec-tion bias. Incorporating propensity scores into the analysis leads to unbiased estimates of the effects of a new feature. In Section 5 we combine propensity scoring with outcome mod-eling to obtain better, doubly robust , estimates, so-called be-cause inferences are valid even if only the propensity model or only the outcome model is correctly specified. (Lunce-ford and Davidian [7] give an excellent introduction to dou-bly robust estimators.) We argue that double robustness is extremely important because the analysis of non-random advertiser studies within a large SE company is likely to be automated. In Section 6, we apply the doubly robust esti-mator to our example, highlighting the data analysis steps along the way. Section 7 gives a high-level view of the lit-erature that guided our thinking. Finally, in Section 8 we discuss other applications of the methods in the SE business. The format of whitelist trials is similar across all new fea-tures. The CSRs choose a set of advertisers who are first offered the new feature in the ads front-end. Some adver-tisers are chosen for their willingness to test new features, some because they have asked for the new feature, some be-cause the CSR believes that the new feature will benefit the advertiser, some because the advertiser is not entirely happy and the CSR is hoping to change that, and some for reasons that are perhaps not so obvious. Accordingly, the first users of a new feature are  X  X elected X  in two steps. 1. A CSR selects a whitelist of advertisers that will have 2. White-listed advertisers choose to use the new feature. A new feature could both enlarge the advertiser base and the spend of current advertisers, so any early indications that bear on these questions are important to detect, despite the complications that ensue from the nature of advertiser selection.
 The set of advertisers on the whitelist is not a random subset of all advertisers, and the set of advertisers on the whitelist that choose to use a new feature is not a random subset of the white-listed advertisers. Advertisers who believe that they may benefit greatly from the new feature may be more likely to participate, or those who would benefit more may not want to be an early adopter, for example. So, neither the CSR selection nor the decision of a white-listed adver-tiser to use the feature can be considered to lead to random sampling.
 The two step selection process results in a 3-way partition: 1. Advertisers on the whitelist that use the feature. 2. Advertisers on the whitelist that do not use the fea-3. Advertisers not on the whitelist.
 To make the main ideas clearer, this paper considers only the first group of white-listed advertisers, which we call users , and a random sample of the third group of advertisers, which we call controls . (Note that the third group is not the same as a random sample of all advertisers, because all advertisers in the first two groups are excluded from controls.) The second group of advertisers would be needed to estimate the adoption rate of the feature or to estimate the effect of merely offering a new feature to advertisers, even if they do not use it. Since we do not consider these estimation problems in this paper, we ignore this group.
 The challenge of assessing whether a new feature makes ad-vertisers happier is exacerbated by irregularities in adver-tiser behavior that largely depend on business conditions outside an SEs control and occur whether or not a new fea-ture is introduced. Figure 1 shows the variability in ad-vertiser spending over an 18 week period for a small sam-ple of advertisers. Each panel of the plot corresponds to a single randomly chosen advertiser, and each point is the amount the advertiser spent on that day relative to its max-imum daily spend over the 18 weeks. The curve is a smooth through the points and the light grey vertical line delineates the introduction of the new feature.
 Figure 1 shows that there is no  X  X anonical X  advertiser, and for many advertisers there is no canonical spending amount. Advertisers have nearly constant spend (like H), increasing spend (like J), decreasing spend (like G), and cyclical ups and downs (like B). While seasonality is apparent (and ex-pected for this time of year), each advertiser has its own seasonal spending pattern. Teasing out effects in these data would be challenging even with random samples.
 Finally, we acknowledge possible confusion in the term  X  X ea-ture X  that has one interpretation in software engineering and another in machine learning. We try to avoid this confusion in the sequel by using  X  X odule X  in reference to the new fea-ture in the ads front-end system and  X  X dvertiser character-istic X  in reference to features of advertisers that are related to usage and outcome. We are interested in estimating the effect of a new ads front-end module on an outcome Y (e.g., advertiser spend) for a population of advertisers. We use the term user to denote white-listed advertisers that used the module and the term control for the comparison advertisers randomly chosen from those not on the whitelist. It is convenient to think of both users and controls as being part of the trial, although the controls are unaware of the trial while it is ongoing and unaffected by it. Usually there are also variables X that consist of both static characteristics of the advertiser, like tenure and country, and summaries of daily activity, like pre-trial spend. The only restriction on the variables X is that they should depend only on information that could be collected before the trial starts. It is also convenient to introduce a binary variable Z that denotes whether an advertiser was on the whitelist and used the module ( Z = 1) or an advertiser was not on the whitelist ( Z = 0). Thus, for each advertiser in the trial, the observed data consist of ( Y, Z, X ).
 The problem with observational studies is that characteris-tics of advertisers that might affect the outcome Y might also affect whether the advertisers were on the whitelist and used the module. For example, advertisers with long tenure might be both more willing to experiment with new mod-ules and have the financial resources to use them effectively. Differences in outcome then capture not only the effect of using the new model but also the uninteresting difference in tenure between users and controls. Such confounding of outcome and selection implies that the effects of using the new module on advertiser spend and retention cannot be estimated correctly (e.g., without bias) unless the sampling bias is taken into account.
 The methodology for removing selection bias is best under-stood through the concept of counterfactuals, which are re-sponses under conditions different from those used in the experiment. Each advertiser has a pair of potential out-comes Y : the outcome we would observe if the advertiser used the Y : the outcome we would observe if the advertiser did not Of course, we cannot observe both outcomes for an adver-tiser. We can observe Y 1 for an advertiser that used the module, but we cannot observe Y 0 for a user, and we can observe Y 0 (but not Y 1 ) for a control. The unobservable outcomes are termed counterfactuals because we did not in fact observe them. (Holland [4] provides an insightful dis-cussion of counterfactual reasoning in statistics.) Using the binary indicator variable Z we introduced earlier allows us to express our observed outcome as Equation (1) hints that there is a connection between mak-ing inferences in observational studies (whitelist trials, in our case) and missing data problems because Z indicates which potential outcome is observed.
 The difference Y 1  X  Y 0 for any advertiser is the effect that using the module has on that advertiser. The distribution of Y 1  X  Y 0 over all advertisers describes the distribution of the effect of module usage across advertisers. The mean difference over the population of advertisers is then the average effect of using the module .
 The difficulty facing observational studies concerns the ex-tent to which the observed data can be used to estimate  X . Note that we cannot observe Y 1  X  Y 0 for any advertiser because we can never observe both of Y 0 and Y 1 . We shall see that our ability to use the observed data to estimate  X  depends on the relationship between the variables ( Y, Z, X ). The mean outcome for the white-listed advertisers that used the module is E ( Y | Z = 1) = E ( Y 1 | Z = 1), which is not the same as E ( Y 1 ) in Equation (2) unless Y 1 and Z are independent. Similarly, the mean outcome of the controls is E ( Y | Z = 0) = E ( Y 0 | Z = 0), which is not necessarily the same as E ( Y 0 ) when the controls are chosen from the set of advertisers that are not on the whitelist rather than from the set of all advertisers. We need a way to estimate  X  from E ( Y | Z = 1) and E ( Y | Z = 0).
 One case is simple. Random assignment of subjects to user and control groups ensures that selection ( Z ) is independent of potential outcome ( Y 0 , Y 1 ). This justifies comparing the observed average differences of user and control groups, as is typically done in analyses of randomized experiments. That is, if the outcome Y is independent of selection Z , then E ( Y | Z = 1) = E ( Y 1 ) and E ( Y | Z = 0) = E ( Y 0 ) and the average effect of using the module can be expressed by the difference of the average of users and controls, so In non-randomized experiments like the whitelist trials of interest to us, progress can be made by exploiting the ad-vertiser characteristics X that are associated with both se-lection Z and outcome Y . These characteristics are called confounders . If all the relevant confounders are known, then conditional on X we have the necessary independence of hy-pothetical outcomes and selection into the user or control group. Precisely, X includes all confounders if where  X  indicates independence, here conditional on X . The notions of potential outcomes and confounders are pow-erful enough to provide consistent (i.e., asymptotically unbi-ased) estimates of the effect  X . We next review some meth-ods for removing confounding that build on these ideas. Knowledge of all the variables that affect both selection and the outcome, as in Equation (4), is not quite enough to get valid estimates of the average effect of using the module. We must also assume that every advertiser has a non-zero probability of being a user and a nonzero probability of being a control. If so, we can obtain valid estimates of the effect of using the module by partitioning X into level sets such that the values of all characteristics in the set are fixed, and then computing a difference  X  k in the mean of user and control groups within each partition k . The separate estimates  X  can be combined to yield a consistent estimate of  X . In an important paper, Rosenbaum and Rubin [12] define the propensity score p ( x ) as the conditional probability that an advertiser is in the group of white-listed users given it has characteristics x : They prove that if ( Y 0 , Y 1 , Z, X ) satisfies equation (4) and if so that every advertiser has a nonzero chance of being a control or a user, then partitioning on p ( x ) is as good as partitioning on x in the sense that Partitioning on p ( X ) instead of X itself can dramatically re-duce the number of partitions in which separate estimates,  X  k , need to be computed because p ( X ) is one dimensional regardless of the dimension of X . Rosenbaum and Rubin argue that the method can be applied using an estimate of the propensity score and much of the following work in the area has concerned diagnostics that suggest whether the estimated propensity score model should be trusted. Paren-thetically, subsequent work, reviewed in [6], concludes that estimators based on an estimated propensity  X  p ( x ) perform better than those based on the true (unknown) p ( x ). The work of Rosenbaum and Rubin has led to numerous variations on the theme of matching. User and control ad-vertisers can be paired by matching their propensity scores, or subclasses of users and control advertisers can be formed based on quantiles of estimated propensity scores and an av-erage computed for each subclass, or outcome models that estimate  X  k as a function of X can be fit within each sub-class to capture any residual dependence that the propen-sity score model missed. Propensity score matching is used extensively in medical and social science applications, and until recently it was our preferred method of analysis for whitelist trials. Our main reservation about all variants of matching is the degree of care required in building the propensity score model and the degree to which the matched sets must balance the advertiser characteristics. We require an estimator that has good performance and that can be applied routinely by non-statisticians. The challenge of making causal inferences from observa-tional data is well-studied in statistics and there are many ways to proceed. The established methods all rely on as-sumptions (like no confounders beyond X ) that are diffi-cult to validate with sample diagnostics. Instead of giving a laundry list of methods for estimation for whitelist trials, we discuss two of the most commonly used methods and then a variant that combines both methods into a new method that is more attractive than either of the basic two meth-ods alone. The hybrid estimator, called the  X  X oubly robust X  estimator, has certain advantages in our application, inas-much as it protects against the constituent models being incorrectly specified. Since the doubly robust estimator can be built from simple logistic and ordinary regression models, for example, it can be applied without specialized software. Finally the doubly robust estimator has an estimated stan-dard error that is both easy to compute and accurate [7]. Suppose that we knew the true relationship between the outcome Y and the pre-experiment variables X , and that we could represent the relationship with the pair of models m 1 ( X ) = E ( Y | X, Z = 1) , m 0 ( X ) = E ( Y | X, Z = 0) . (7) Note that where the outer expectation averages over the distribution of X .
 We can then estimate the average effect of using the mod-ule in an asymptotically unbiased way by fitting separate regression models m 1 ( x ) to the users and m 0 ( x ) to the con-trols. Each of the fitted models yields predictions for all advertisers, users and controls, and the average difference in these direct outcome models is an unbiased estimate of  X .
 Estimating  X  by the direct outcomes method is both simple and dangerous. If we misspecify the outcome model (7), our estimate and test statistic do not capture the effect of using the module. If X contains enough information to remove selection bias (as assumption (4) requires), then the observed outcomes for the users on the whitelist satisfy Similarly, Together, these last two equations lead to the inverse propen-sity weighted estimator where n is the total number of users and control advertisers in the whitelist study and  X  p ( x ) is an estimate of the propen-sity score P ( Z = 1 | X = x ). Note that  X   X  IP W is asymp-totically unbiased if  X  p ( x ) is asymptotically unbiased; e.g.., if the correct propensity model is fit by logistic regression. In fact, [11] shows that any estimator of  X  that is asymptoti-cally unbiased must involve inverse propensity weighting no matter what the distribution of ( Y, Z, X ) is.
 Direct outcome estimates of  X  are valid (asymptotically un-biased) if the fitted outcome model for Y is correct. In prac-tice this is often addressed by fitting separate outcome mod-els, m 1 ( x ) , m 0 ( x ), to user ( Z = 1) and control ( Z = 0) ad-vertisers. IPW estimates of  X  are valid if the fitted propen-sity model is correct. Surprisingly, there is a simple combi-nation of the two methods of estimation that is asymptot-ically unbiased even if either the form of the assumed out-come models or the form of the assumed propensity model (but not both) is wrong.
 The doubly robust estimate  X   X  DR can be written in terms of the estimated propensities  X  p ( x i ) and the predictions  X  m and  X  m 0 ( x i ) under the direct outcome mean models for the users and controls respectively. Note that there is a predic-tion for each advertiser in the trial under both the model  X  m 1 for the users and the model  X  m 0 for the controls. There are two expressions for  X  DR that we find convenient. First,  X   X  which shows that  X   X  DR adjusts the inverse propensity weighted estimate with residuals of the Z i from their fitted values  X  p ( x i ). Second,  X   X  which shows that  X   X  DR also adjusts the direct outcome esti-mate with residuals of the Y i from their fitted values  X  m and  X  m 1 ( x i ). Robins, Rotnitzky and Zhao [11] show that  X   X 
DR has the smallest asymptotic variance among all asymp-totically unbiased estimates of  X  that are based on  X   X  IP W Lunceford and Davidian [7] give a simple estimate of the standard error of  X   X  DR that can be used to give confidence intervals for  X . We rewrite the expression for the DR esti-mate as  X   X  DR = 1 /n P  X  i where  X  =  X  m 1 ( x i )  X   X  m 0 ( x i )+ Z i ( Y i .
 Then the variance of  X   X  DR can be estimated by Using simulation studies, Lunceford and Davidian [7] show that this variance estimate is remarkably accurate, yielding confidence intervals of correct size. The module we study was made available to 600 advertisers over a period of 11 weeks. Advertisers were added in  X  X aves X  roughly every week. For each wave we sampled advertisers not on the whitelist at a 6:1 ratio to form a control group that shared the same wave start date. Although there is no single start date, we use the term  X  X re-trial X  to mean the period before an advertiser was added to the study. The advertisers that the CSRs put on the whitelist are far from a random sample of all advertisers. For example, each advertiser has an assigned  X  X ier X  that governs the level of customer service that it receives. The highest tier is over-represented on the whitelist and the lowest tiers are under-represented. Our allocation of controls to weekly cohorts maintained the whitelist tier distribution, at least approx-imately. The main reason for doing this is to help ensure that the overlap (in advertiser characteristics) between user and control advertisers is adequate for propensity models to be applied (as required by condition (5)). In the analysis that follows, we sometimes distinguish between the top tier and others.
 Of the 600 white-listed advertisers, 284 used the new mod-ule; in what follows they constitute the users. We omit the 316 other white-listed advertisers that did not use the module. As stated in Section 2, the white-listed non-users provide information about the adoption rate of the module, but that is not the focus of the present analyses. We ran database queries to extract advertiser variables for each advertiser. The variables can be broken into two cat-egories: static advertiser demographics and time-varying metrics like daily click-through-rate that describe the per-formance of advertisers X  ad campaigns. The demograph-ics are pre-trial conditions and as such are suitable predic-tors (and hence possible confounders) for our propensity and outcome models. The time-varying metrics were limited to eight weeks pre-trial and eight weeks post-trial. The pre-trial metrics are suitable predictors for the propensity and outcome models, while the post-trial metrics can be out-comes. For example, the daily spend before the trial cap-tures how advertisers manage their campaigns before they used the new module. Once the trial starts for an advertiser, daily spend is an outcome that we want to track. It would be possible to model the daily time series of performance directly but rather define functions, f k ( x t ) , k = 1 , ...K , that capture salient features relevant to either the decision to use the module or the value of the post-trial outcome. Table 1 displays the advertiser characteristics that we use in our models. The first five are static demographic charac-teristics that are thought to influence use of the module and associated spend. The remaining advertiser characteristics are functions of time series that capture the daily perfor-mance of an ad campaign. We consider two outcomes, Retention and LogSpendRatio . Retention is a binary variable that is one if an advertiser that was active in the eight weeks prior to its trial start remained active in the eight weeks following and zero otherwise. By active we mean that an advertiser had at least one ad shown in that period (i.e., an ad impression). LogSpendRatio is based on the relative change in average daily spend (AveDai-lySpend) for an advertiser. Define S post as the average daily spend for an advertiser in the eight weeks following trial start and S pre as its average daily spend in the prior eight weeks. LogSpendRatio is the logarithm of the ratio of post-trial spend to pre-trial spend.
 The doubly robust estimator estimates the mean (causal) effect of using the new module. That is, it estimates the mean change in the outcome if an advertiser previously not using the new module starts to use it where the mean aver-ages over all advertisers. Define  X   X  DR ( Retention ) to be the doubly robust estimate of the mean change in Retention if an advertiser starts using the new module. Similarly define  X   X 
DR ( Spend ) to be the doubly robust estimate of the mean difference in LogSpendRatio when an advertiser starts using the new module. Note that by construction the Spend out-come compares the change pre-and post-trial for the users to the controls.
 The doubly robust method combines three separate mod-els for each outcome under consideration. In our experi-ence, good performance is obtained by using the same large set of advertiser characteristics as predictors in both the propensity score and the outcome models and using vari-able selection techniques (or L1-regularization) to mitigate over-fitting. Even though the theory of the doubly robust es-timator indicates that we need not get these models exactly right, it behooves us to make the best possible attempt. The propensity score model captures how advertiser charac-teristics relate to the probability that an advertiser uses the module. It is possible the propensity model changes over waves but we found negligible evidence of this in our data. [This is easily assessed by including wave as a variable in the model.] All variables in Table 1 were included in a standard logistic regression model and Table 2 summarizes their im-portance in the models. A centered dot indicates that the advertiser characteristic was not important at the 1% level. The column headed  X  X rop Score X  indicates which of these were important in distinguishing between white-listed users and the controls. The fact that so many variables are sta-tistically significant in the propensity score model highlights the fact that there is much selection bias in this study. Figure 2 describes the quality of the fitted propensity mod-els. The ROC curve on the left-hand side plot shows the trade-off between true positive (a true user predicted to be a user) and false positive probabilities. The area under the curve (AUC) is a measure that captures the ability of the model to rank users and non-users appropriately; for our propensity score model we observe AUC=0.88.
 The right-hand side panel of Figure 2 shows the agreement between the observed data and the fitted propensity model by tabulating the fraction of users within bands defined by the quantiles of fitted propensities. Perfect agreement would be obtained if all points fell on the 45  X  line through the ori-gin. Our fitted propensity model fits reasonably well across Table 2: Roles of advertiser characteristics in the propensity and outcome models. Variables were transformed as appro-priate (e.g. log odds of CTR rather than raw CTR were used.) * denotes statistical significance at the 1% level. Advertiser Prop Retention SpendRatio Characteristic Score User Cntl User Cntl Tenure * * *  X  * Tier *  X   X   X  * Channel *  X   X   X  * Country *  X   X   X  * ConversionTrk *  X   X   X   X  Reports *  X   X   X   X  Impressions  X   X   X  * * CTR  X   X  *  X   X  AveDailySpend *  X  * * * SpendVariation *  X   X   X   X 
SpendPer1000 *  X   X  * * the whole range of estimated propensities. We base our outcome model for Retention on a logistic re-gression of the observed fraction of retained advertisers us-ing the pre-trial advertiser characteristics listed in Table 1 as predictors. The columns in Table 2 headed by  X  X eten-tion X  indicate which of these characteristics demonstrated significant association with retention in the user and control groups. In contrast to self-selection, only a few variables af-fect retention when controls and users are directly modeled. The outcome model for LogSpendRatio is a linear regression on the logarithm of SpendRatio. Mean daily number of ad impressions, mean daily spend, and mean daily spend per 1000 impressions are important in both the user and control models, but advertiser demographics are also important in the controls model. This is not surprising; the controls may be more heterogeneous than the users and the control sample is much larger than the user sample.
 To assess the quality of the models, recall that the expression for the doubly robust estimator requires an estimate of the outcome for the user group of advertisers had they not used the module, and an estimate of the outcome for the control group of advertisers had they used the module. Thus, we need to use models fitted to one group of advertisers to pre-dict outcomes for the complementary group of advertisers. Our trust in these models is guided by the extent to which prediction is interpolation rather than extrapolation. Figure 3 conveys the degree to which the Retention models are appropriate for prediction. Each subpanel contains a kernel density estimate of the Mahalanobis distance (based on the in-model covariance) of the out-of-model instances from the in-model mean. In the left hand panel we show the distribution of distances for the model fitted to the controls. The solid curve corresponds to the controls (i.e., the in-model data) and the dashed curve corresponds to the users (i.e., the out-of-model data). The shapes of the distribu-tions are quite different and we observe a bimodality in the distances for the in-model control set. In the right hand panel we show the distribution of distances for the model fitted to the users. The solid curve corresponds to the users (i.e., the in-model data) and the dashed curve corresponds to the controls (i.e., the out-of-model data). Here we observe less pronounced bimodality in the distances for the in-model (users) data and an excess of small distances for the out-of-model control data. In both cases, it appears that there is sufficient overlap in the distribution of distances to mitigate the concern with extrapolation. The associated plots for LogSpendRatio are similar but not shown.
 Figure 3: Plot illustrating the degree of extrapolation of the Retention outcome model. Given our satisfaction with the components of the doubly robust estimator, we are ready to compute  X   X  DR ( Retention ) and  X   X  DR ( Spend ) from the component models  X  p ( x ),  X  m and  X  m 0 ( x ). The effect of tier is pronounced in the propensity score model, so we compute a separate mean effect (and associated standard error) for tier 1 for both Retention and SpendRatio. For Retention, we obtain In words this says that, on average, there is a +4.4% dif-ference in retention for tier 1 advertisers who use the new module, while for non-tier 1 advertisers who use the mod-ule the difference in retention is +9.5%. Both estimated differences are statistically significant at the 0.05 level. For the log of SpendRatio, we find In words, the module does not have a statistically signifi-cant effect on LogSpendRatio for tier 1 users. For non-tier 1 advertisers, the difference is 1.09 and it is statistically signif-icant. Moving from the differences of log spend to the ratio of spend, the estimated effect corresponds to an increase in relative spend ratio of 2.13 (2 1 . 09 ) for users. Thus, users have a post-trial to pre-trial spend ratio that is more than doubled if they use the new module. Evidently, in addition to the retention gain we saw earlier, the new module has an upside in spend for non-tier 1 advertisers. The field of statistics has long been associated with meth-ods and models for the analysis of data from randomized experiments and observational studies. The papers cited in this section are meant as exemplars as each of these au-thor has multiple papers (even books) in the area thereby demonstrating their longterm contributions to the area. Fisher [3] is credited with the foundation of experimental de-sign and the central role of randomization. Rubin [13] cred-its Fisher with the device of counterfactuals in developing the framework for causal inference for non-randomized stud-ies, though Holland [4] calls the counterfactual framework  X  X ubin X  X  model. X  Horvitz and Thompson [5] were pioneers in the area of sample surveys where the notion of correcting for non-representative samples first arose.
 Our exposure to causal inference from observational data is through the work of Rosenbaum and Rubin [12] who intro-duced propensity score matching as a means of succinctly capturing differences in the selected users group and the controls. Robins et al. [10],[11] chart a more mathematical course and are responsible for the theoretical basis that un-derlies the doubly robust estimator. Imbens [6] provides a review of the field up to roughly five years ago. He mentions the doubly robust estimator in passing but concentrates on propensity score and outcome modeling separately, largely from the viewpoint of econometric studies. Our apprecia-tion of the doubly robust estimator is due to the work of Lunceford and Davidian [7]. They complement the theory introduced by Robins and co-workers with empirical studies that convincingly demonstrate that the asymptotic prop-erties of the doubly robust method apply to samples that occur in practice. They also demonstrate the accuracy of the sandwich variance estimator (9).
 Our analysis of the Spend outcome focused on the differ-ence in log SpendRatios, or the difference-in-differences in LogSpend. A related approach is the difference-in-differences linear model (see Ashenfelter[1] and Ashenfelter and Card[2]) for LogSpendRatio that results in an estimate of  X ( Spend ). We prefer the robustness properties of  X   X  DR ( Spend ) whereby we fit separate models for users and controls and accommo-date selection bias with the propensity score weighting. McCaffrey, Ridgeway, and Morral [8] provide a link between statistics and machine learning as they apply boosting to the propensity score model. Smith and Elkan [14] provide an-other bridge between machine learning and statistics focus-ing on Bayesian networks as a means to formalize the condi-tional independence relationships that occur when training a model on a different population than the model will be ap-plied to. Of course, Pearl [9] is the father of the foundation of causal inference in AI. Advertisers are an important component of the eco-system of modern search engines: advertisers come to a SE because it has users, users come because the SE provides high-quality search results, and publishers come because the SE has a large inventory of ads that allows publishers to monetize their content. Improving the  X  X ser-experience X  for all three of these groups is key to the viability and growth of a SE. In this paper we introduced a new method for analyzing a common form of advertiser trial and illustrated the ideas on a specific study. Propensity score matching and the doubly robust estimator are broadly applicable within the enter-prise, specifically to all three participants in the eco-system. Consider the application to studying whether a new self-selected service leads end-users to search more. In this sce-nario we have exactly the same problem  X  are there con-founders that affect both the probability of self-selection and search frequency? The methods we propose can be applied once pre-experiment characteristics of users are extracted from logs. Some likely candidates are browser type, geo-location, use of other services, visit frequency, average num-ber of searches per week, etc. [1] O. Ashenfelter. Using the longitudinal structure of [2] O. Ashenfelter and D. Card. Estimating the effect of [3] R. Fisher. The Design of Experiments . Hafner Publish-[4] P. Holland. Statistics and causal inference (with discus-[5] D. Horvitz and D. Thompson. A generalization of sam-[6] G. Imbens. Nonparametric estimation of average treat-[7] J. K. Lunceford and M. Davidian. Stratification and [8] D. McCaffrey, G. Ridgeway, and A. Morral. Propensity [9] J. Pearl. Causality: Models, Reasoning, and Inference . [10] J. Robins and A. Rotnitzky. Semiparametric effi-[11] J. Robins, A. Rotnitzky, and L. Zhao. Estimation of [12] P. Rosenbaum and D. Rubin. The central role of the [13] D. Rubin. Estimating causal effects of treatments in [14] A. Smith and C. Elkan. A bayesian network framework
