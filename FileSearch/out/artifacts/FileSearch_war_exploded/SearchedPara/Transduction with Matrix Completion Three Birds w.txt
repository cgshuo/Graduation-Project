 learning process, such as the manifold assumption (data lies on a low-dimensional manifold) and experimental results on a range of synthetic and real-world data. Let x feature matrix whose columns are the items. Let there be t binary classification tasks, y be missing at random. Let  X  and only if x is to predict the missing labels y learning when t = 1 , |  X  and de-noising the observed features, in X . 2.1 Model Assumptions The above problem is in general ill-posed. We now describe our assumptions to make it a well-defined problem. In a nutshell, we assume that X and Y are jointly produced by an underlying using a modified method of matrix completion. Specifically, we assume the following generative where produced by y 0 Y rank too: rank( Y 0 ; X 0 )  X  rank( X 0 ) + 1 . The actual label y via a sigmoid function: P ( y well on synthetic and real datasets, outperforming several baselines with linear classifiers. 2.2 Matrix Completion for Heterogeneous Matrix Entries observed features and labels as specified by X , Y ,  X  sign( Y 0 ) will contain the completed and correct labels.
 from a  X  X ard X  formulation that is illustrative but impractical, then relax it. features and labels. Note the indices ( i, j )  X   X  to skip the part for Y 0 , and hence the constraints z issues with formulation (1), and we handle them as follows: alternative matrix completion formulations below.

Y 0 ; X 0 , making it potentially lower rank. The optimization problem is found, we recover the task-i label of item j by sign( z all-1 vector. Under the same label assumption y 0 Y be This is a constrained convex optimization problem. Once the optimal Z is found, we recover the task-i label of item j by sign( z is inferior in our preliminary experiments, and we do not explore it further in this paper. We solve MC-b and MC-1 using modifications of the Fixed Point Continuation (FPC) method of Ma, programming (SDP) problem [2], current SDP solvers are severely limited in the size of problems which provably converges to the globally optimal solution and has been shown to outperform SDP solvers in terms of matrix recoverability. 3.1 Fixed Point Continuation for MC-b of (2) consists of two alternating steps for each iteration k : In the gradient step,  X  loss terms in (2) (i.e., excluding the nuclear norm term): Note for g ( z element in Z back to the item x In the shrinkage step, S A k . Then S  X  the nuclear norm.
 tinuation or homotopy method to improve the speed. This involves beginning with a large value  X  1 &gt;  X   X  k +1 = max {  X  k  X   X  ,  X  } , k = 1 , . . . , L  X  1 of rounds of continuation. The complete FPC algorithm for MC-b is listed in Algorithm 1. sizes satisfying  X  the gradient step is non-expansive in the sense that k b for all b 3.2 Fixed Point Continuation for MC-1 Nonetheless, in our empirical experiments, Algorithm 2 always converges and tends to outperform MC-b. The two algorithms have about the same convergence speed. two real-world datasets: music emotions and yeast microarray. In each experiments, we compare MC-b and MC-1 against four other baseline algorithms. Our results show that MC-1 consistently outperforms other methods, and MC-b follows closely.
 Parameter Tuning and Other Settings for MC-b and MC-1: To tune the parameters  X  and  X  , divide  X  using 4 under consideration can be evaluated in one run of the continuation method. We set  X  as in [10], consider  X  values starting at  X  The range of  X  values considered was { 10  X  3 , 10  X  2 , 10  X  1 , 1 } . We initialized b Z  X 
Z = min( functions (2)(3) smaller than 10  X  5 .
 Baselines: We compare to the following baselines, each consisting of some missing feature impu-tation step on X first, then using a standard SVM to predict the labels: [FPC+SVM] Matrix com-pletion on X alone using FPC [10]. [EM( k )+SVM] Expectation Maximization algorithm to impute which are estimated in an iterative manner to maximize the likelihood of the data. [Mean+SVM] Impute each missing feature by the mean of the observed entries for that feature. [Zero+SVM] Impute missing features by filling in zeros.
 After imputation, an SVM is trained using the available (noisy) labels in  X  of parameter values considered was { 10  X  8 , 10  X  7 , . . . , 10 7 , 10 8 } .
P level  X  = 0 . 05 , are marked in bold. 4.1 Synthetic Data Experiments Synthetic Data Generation: We generate a family of synthetic datasets to systematically explore the random  X  we generate 10 trials, where the randomness is in the data and mask.
 relative feature imputation errors, on the synthetic datasets. We make several observations. Observation 1: MC-b and MC-1 are the best for feature imputation, as Table 2 shows. However, is the simple average over all parameter settings and all trials. imputation error, both MC-b and MC-1 did achieve perfect feature imputation. Also, FPC+SVM is X . We believe the fact that MC-b and MC-1 can use information in Y to enhance feature imputation in X made them better than FPC+SVM.
 FPC+SVM took second place.
 Beneficial combination of these factors (the 6 th row) produces the lowest label errors. Matrix completion benefits from more tasks. We performed one additional synthetic data exper-iment examining the effect of t (the number of tasks) on MC-b and MC-1, with the remaining data 4.2 Music Emotions Data Experiments rhythmic, 64 timbre-based) automatically extracted from a 30-second sound clip. not shown because it by definition has relative feature imputation error 1. We vary the percentage of observed entries  X  = 40% , 60% , 80% . For each  X  , we run 10 random trials with different masks  X  best with only 40%.
 baseline, we give 100% features (i.e., no indices are missing from  X  in cally indistinguishable from the oracle baseline. 4.3 Yeast Microarray Data Experiments This dataset comes from a biological domain and involves the problem of Yeast gene functional examples (Yeast genes) with d = 103 input features (results from microarray experiments). 4 We this larger dataset, we omitted the computationally expensive EM4+SVM methods, and tuned only  X  for matrix completion while fixing  X  = 1 .
 FPC+SVM. However, it seems simultaneously predicting the missing labels and features appears to 0.0009 for MC-1, 0.0011 for FPC+SVM, and 0.0001 for Mean+SVM.
 Again, we compared these algorithms to an oracle SVM baseline with 100% observed entries in  X  60%, and 80% observed labels, respectively. Both MC-b and MC-1 significantly outperform this of multi-label learning and transduction that is intrinsic to our matrix completion methods. on missing labels and features [6], but the underlying model assumption is quite different. matrix, and apply our methods there. Though such mapping proliferates the missing entries, we functions of the original features.
 [2] Emmanuel J. Cand ` es and Benjamin Recht. Exact matrix completion via convex optimization. [3] Emmanuel J. Cand ` es and Terence Tao. The power of convex relaxation: Near-optimal matrix [7] Daniel Hsu, Sham Kakade, John Langford, and Tong Zhang. Multi-label prediction via com-[9] Roderick J. A. Little and Donald B. Rubin. Statistical Analysis with Missing Data . Wiley-[10] Shiqian Ma, Donald Goldfarb, and Lifeng Chen. Fixed point and Bregman iterative methods [13] Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Proceedings of the
