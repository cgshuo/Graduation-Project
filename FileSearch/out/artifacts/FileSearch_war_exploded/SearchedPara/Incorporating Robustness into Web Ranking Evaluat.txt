 In many Web search engines, a ranking function is selected for deployment mainly by comparing the relevance measure-ments over candidates. Due to the dynamical nature of the Web, the ranking features and the query and URL distribu-tion on which the ranking functions are built, may change dramatically over time. The actual relevance of the function may degrade, and thus the previous function selection con-clusions become invalid. In this work we suggest to select Web ranking functions according to both their relevance and robustness to the changes that may lead to relevance degra-dation over time. We argue that the ranking robustness can be effectively measured by taking into account the ranking score distribution across search results. We then propose two alternatives to the NDCG metric that both incorporate ranking robustness into ranking function evaluation and se-lection. A machine learning approach is developed to learn the parameters that control the metric sensitivity to score turbulence, from human-judged preference data.
 H.4 [ Information Systems Applications ]: Miscellaneous Measurement, Reliability Web ranking, ranking robustness, NDCG
In Web search, a ranking function usually ranks the search result pages for a query, by first assigning a score that mea-sures the relevance of each page to the query, and then ranking pages in the descendent order of the score. Many popular learning-to-rank algorithms, such as RankSVM [5], RankNet [3], GBrank [7], and SoftRank [6], belong to this category. For function selection purpose, metrics such as Mean Average Precision (MAP) and NDCG [4] have been developed and are commonly used to measure the search re-sult relevance. The underlying assumption for this strategy is that the relevance of the selected function will be con-sistently better over time after deployment, compared with other candidates.

However, this assumption usually does not hold for Web search. As the Web size expands, Web content updates, Web link structure evolves, and user search behaviors change, the rank features that the function was trained may get updated, and the query and search result page distribution it is ap-plied to may drift too. This brings practical issues to the ranking quality in this dynamical Web search environment. On one hand, a deployed ranking function may not be so ro-bust in the sense that its relevance may degrade with these changes. On the other hand, the ranking function itself, can not always be retrained so timely to capture the changes. Consequently, the relevance comparison previously done for function selection may not be valid any longer.

In this paper we claim that, selecting web ranking func-tions purely based on relevance is inadequate, and ranking robustness should also be measured and taken into account in ranking function evaluation. Rather than functions with only higher static relevance measurement, those whose rel-evance measurement are more robust to potential changes, should be considered for deployment. One observation we made is: the impact from potential change in rank fea-tures and the distribution of user queries and search result pages on a ranking function, is mostly reflected to the rank-ing scores assigned by the function. Consider two score lists with the same ranking order: (1) { 2 . 1 , 2 . 0 , 1 . 0 { 2 . 1 , 1 . 5 , 1 . 0 } . The human-judged relevance grades of the three pages are {  X  X xcellent X ,  X  X ad X ,  X  X ad X  } . Although the two lists have identical NDCG values, apparently (2) is bet-ter than (1) in terms of ranking robustness with regard to potential turbulence in scores. As rank features update, the first two pages in (1) can be easily swaped, and the corre-sponding NDCG measurement can drop dramatically. This indicates the inadequacy of NDCG as the only criteria in function selection when score turbulence can happen. Therefore in this work, we propose two alternatives to NDCG, that measure both relevance and ranking robustness for ranking function selection. We design the first robust-ness metric, namely rNDCG, to add in the probability that neighboring pairs of search results in the original ranking order may switch positions when ranking score turbulence happens. The second metric is SoftNDCG, first proposed in [6]. SoftNDCG calculates the expected NDCG of a ranked list according to a probability distribution that a result page takes each rank, which is also defined over the ranking scores across pages. Both of the proposed metrics are sensitive to the possible ranking score turbulence as we will demon-strate in this paper. In addition to these two manually de-fined metrics, we also develop a machine learning approach to automatically learn a robustness metric that maximizes its consistency with user preference data. For doing that, we first parameterize the SoftNDCG metric with parame-ters that control the sensitiveness of the measurement to ranking scores. Then we collect training data by providing randomly permutated search results to human editors and collect their preference ove r the ranked lists. A maximum likelihood optimization is then applied for the purpose of parameter learning.

How to evaluate the proposed metrics with respect to their measurements of robustness is a key issue in this study. In the empirical study, we compare different metrics with re-gard to their reliability in function selection, according to three criterions. A metric tha t can better measure the rank-ing robustness should be 1) more stable (i.e. having less variance) to the noise that causes ranking score turbulence; 2) more reliable in their function selection across different data samplings, better measuring the robustness to the pos-sible drifting in the distribution of queries and result pages; and 3) more stable in their function selection when the eval-uation data is updated over time. Our experimental results indeed demonstrate the superiority of the proposed metrics over the standard NDCG metric, based on these criterions.
There are some related works that attempt to address the ranking robustness problem from different angles. Some works [1, 2] address the problem from the perspective of spamming, and attempt to improve the ranking relevance in a noisy environment by actively adding noise to train-ing data. [9] defines a ranking robustness metric to predict query search performance, which applies a noise model to perturbable documents and then measure the search result similarity before and after perturbation. The calculation of designed metric highly relies on a specific retrieval func-tion. SoftRank [6] defines SoftNDCG as some optimiza-tion loss function and applies it directly to function training. We first apply it directly as a ranking evaluation metric in function selection. From the perspective of learning ranking metrics, [8] has conducted similar experiments in automat-ically learning the gain value and rank discounting factors for NDCG. The learned metric is a static relevance measure-ment, and does not measure robustness to changes. Figure 1: Evolutions of NDCG and SoftNDCG for one
In the real Web search environment, changes are happen-ing all the time, and these changes may cause the quality of a ranking function to degrade over time. For example: 1) Page content update  X  the web pages are not static and may get updated frequently; 2) Web scale expansion  X  more pages, more link structures and more users, may lead to drifting of the feature distribution; 3) User search behavior change  X  evolution in search query distribution may cause the data distribution used for evaluation to drift. The basic assumption for ranking robustness is that, the less relevance degradation a ranking function may suffer, the higher ro-bustness the function has. Therefore we focus on measuring the expected relevance of a ranking function over possible changes, and predict its potential relevance over time.
Can the future relevance of a ranking function be pre-dicted with existing data ? Our answer is yes, at least par-tially. We notice that the dynamical changes are reflected on the value and distribution of the ranking scores assigned by a function. If we take into account the ranking scores in the relevance evaluation, the ranking robustness can be at least partially measured. Let X  X  use a real Web query and its search results as an example to demonstrate the relation-ship between the ranking scores and the potential relevance change. The relevance scores for Web pages in a ranked list and the relevance label for each page (Perfect, Excellent, Good, Fair and Bad) are known and used to compute the NDCG value. We incrementally add larger Gaussian white noise to the scores. Figure 1 presents the evolution of the NDCG values as the intensity of the additive noise increases. As seen, the NDCG curve is unstable with respect to the additive noise: at some points NDCG is insensitive to the additive noise, while at some points a small increase in the noise results in a dramatic change in the NDCG value. This is because the NDCG computation only relies on the order of a ranked list, but is regardless to the ranking scores. As one of the advocated metrics for robust ranking function selec-tion, the SoftNDCG curve (to be discussed next) is smooth in response to the additive noise.
In order to select ranking function according to both rel-evance and robustness, two metrics, namely rNDCG and SoftNDCG, are proposed in this section as alternatives to NDCG. Both of the metrics go beyond NDCG which only considers the absolute ranks, and take into account the prob-ability that a search result page belongs to a rank position. The probability, although calculated differently, is defined over the normalized relevance scores of the pages.
The first metric rNDCG (namely robust NDCG) consid-ers the potential ranking order changes in the neighbor-ing positions of a ranked list. Consider a ranked list of N Web pages in the decreasing order of their relevance score: L = { &lt;g 1 ,s 1 &gt;, &lt; g 2 ,s 2 &gt;,...,&lt; g N ,s ( r =1 , 2 ,...,N ) represents the human-judged grade and the relevance score of the document at rank r . Without loss of generality, we assume s 1 &gt;s 2 &gt;  X  X  X  &gt;s N .Whenwe compute the relevance gain from each ranking position r , three neighboring result pages at rank r  X  1, r and r +1 in the original order, may contribute their relevance gain with probabilities p r  X  1 ,1  X  p r  X  1  X  p r ,and p r , respectively. The rNDCG of the ranked list is then defined as follows: G where p r is the probability that the page at rank r may take rank r + 1, due to score turbulence. D ( r ) is the same rank-discounting function as in NDCG that is often chosen in the form of D ( r )=1 / log(2 + r ), and G max is the maximum possible value of N r =1 D ( r )[ p r  X  1 g ( r  X  1) + p r p r  X  1  X  p r ) g ( r )] achieved when pages are optimally ordered. The probability p r is defined as p r = 1 1 , 2 ,...,N  X  1; 0 otherwise. As noted, the difference between the relevance scores of two neighboring pages decides the probability: the closer the two relevance scores are, the more likely the two pages may switch ranks in the future. Here  X  is a positive normalizer that controls sensitive of the rank probability to the score difference.
The second metric that considers the ranking score distri-bution across documents is SoftNDCG, introduced in [6]. SoftNDCG calculates the expected NDCG with regard to a distribution that defines the probability of a result page tak-ing each specific rank. Consider the same ranked list of Web result pages: L = { &lt;g 1 ,s 1 &gt;, &lt; g 2 ,s 2 &gt;,...,&lt; g where p j ( r ) is the probability of document j ranked at po-sition r , given a score list { s 1 ,s 2 ,  X  X  X  ,s N } .InSoftNDCG,a ranking form of a Binomial distribution is used to approxi-mate the distribution of ranks. In order to estimate the rank distribution of document j ,itfirstdirectlyestimates  X  ij probability that document i out-ranks documents j ,forev-ery other document i = j . It then computes the probability p ( r ) that document j has rank r basedonthem.

In the computation of SoftNDCG, one parameter  X  is re-quired to be pre-determined. This parameter controls how to measure the closeness among ranking scores. As  X   X  X  X  , the rank distribution p j ( r ) becomes a uniform distribution, i.e., no score differences are differentiated, while as  X  SoftNDCG degenerates to the normal NDCG.

Our approach to determining  X  is to learn from preference pair, e.g., L 1 L 2 , that signifies the ranking generated by score list L 1 is preferred to that generated by L 2 .Wemodel this pairwise preference in a probability model: By adjusting parameter  X  , we attempt to maximize the like-lihood to favor users X  and/or hu man editors X  preferences.
Assume we have preference lists L = { L i 1 L i 2 } M i =1 optimal  X   X  can be sought by maximizing the following log-likelihood function: In this study, a simple gradient descent procedure is applied to searching for the optimal  X   X  , with the gradient computed as follows: We executed the experiment of learning  X  using M = 2183 Web queries. The queries are randomly sampled from the query log of a commercial search engine, and the top five search results are also scraped from the same search engine. For each query, we randomly permutate the search results to get two different ranked lists L ( i ) 1 and L ( i ) 2 Then we ask human editors to give their preference over the optimal  X   X  that maximizes the log-likelihood as in Equa-tion 4 converges to around 0 . 2. Figure 2: Evolutions of NDCG and SoftNDCG as the Figure 3: The value and the variance of the Binomial
In this section, we compare the three metrics: NDCG, rNDCG, SoftNDCG 1 in three experiments: 1) Study how the relevance of a ranking function evaluated in those met-rics degrade with artificial noise added to the ranking scores. 2) Study how function selection results evaluated by different metrics are affected by the size of the test data. This exper-iment simulates the situation when the distribution of eval-uation data could change over time. 3) Study how function selection results are affected over time. For this purpose, we train two ranking functions with data from an earlier time,
All metrics are calculated only for top 5 rank positions. and then evaluate the variance of function comparison re-sults with data sampled data from different time points. The ranking functions for all these experiments are trained with GBrank [7] a state-of-art ranking algorithm. The features used for training can be roughly divided into four categories: link-based features, content-based features, click-based fea-tures and others. The calculation of the rank probability in rNDCG and SoftNDCG both exploit function-dependent ranking scores. We normalize raw scores into their percentile positions in a large score sampling for each function, making them comparable across functions.
The data set used for evaluating the three metrics contains 4 , 000 queries, sampled from a large commercial web search engine. All the query and result page pairs have human labels, with five relevance levels (Perfect, Excellent, Good, Fair and Bad). We compute NDCG and SoftNDCG over the ranking scores given by a ranking function, but under additive noise from a Gaussian noise model, while the NDCG and the SoftNDCG values here are averaged over all the 4 , 000 queries. To demonstrate the instability of NDCG, we implement the experiment ten times with the additive noise generated by ten different random seeds. The results reported in Figure 2, show that the SoftNDCG measurement has smaller variances. It indicates that compared with the standard NDCG, SoftNDCG is more stable with regard to potential score turbulence, because it has already considered the impact of score turbulence in the measurement. This experiment is to study whether rNDCG and Soft-NDCG are more reliable than NDCG in ranking function selection, with respect to different data samplings. For this purpose, We first train two ranking functions (named as f and f 2 ) using GBRank [7] with differ ent learning parameters and feature sets. For SoftNDCG, we learned its parameter using the methods described in Section 3.2. The  X  in rDCG is simply set to 0 . 5. We randomly sample 1 , 000 subsets (each contains K queries, we also study the impact of dif-ferent K ) from our test data (which is different from our holdout set), and calculate the average NDCG, rNDCG and SoftNDCG over each subset. A fter that, we construct a Bi-nomial distribution for each metric, using the following rule: For each subset, a binary random variable is assigned 1 if the metric shows a higher and equal value (tied values are very rare) for f 1 than f 2 averaged over the queries in the subset, and 0 otherwise. This way, we have a Binomial distribution for each metric over the 1 , 000 subsets.

Figure 3 plots the value and variance curve for the evolu-tions of the three Binomial distributions as the subset size K increases. The Binomial distribution with K =4 , 000 is treated our ground truth result, where all three met-rics agree that f 1 is better than f 2 . As seen in the graph, the value curves of three Binomial distributions converge to the ground truth as the subset size increases, while the variances converge to zero. However, as test sets become smaller, rNDCG and SoftNDCG outperform NDCG in the sense they are more consistent with ground truth results, and with much smaller variances. In this experiment, we sample latest news queries and Web pages (including the rank features) from nine individ-ual days over three months after training, 300 queries each day. The two ranking functions f 1 and f 2 were also trained at an earlier time. As we did in the Section 4.2, for each test set, we randomly sample 1 , 000 subsets, with 50 queries each, and construct a similar Binomial distribution for each metric. Figure 4.3 plots the value and variance curves of three Binomial distributions across different dates. As seen, the NDCG metric cannot distinguish f 1 and f 2 very well. In contrast, based on rNDCG and SoftNDCG, it is clear that f outperforms f 2 constantly across time. Figure 4: The value and variance of Binomial distribu-
In this work, we argue that not only relevance, but also ranking robustness should be considered in Web ranking function selection. We further model the ranking robust-ness as the expected relevance of a ranking function with regard to potential ranking score turbulence.
