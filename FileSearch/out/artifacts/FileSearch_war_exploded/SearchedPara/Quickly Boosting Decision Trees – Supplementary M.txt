 the data, given as Proposition 1: regression trees as well as binary or multi-class classification trees. is Z  X  and the sum of the weights of the elements in  X  with class y is Z y  X  proportionally weighted by the total mass of samples belonging to that leaf: in  X  that are not in the m -subset, where m  X  n : Accordingly, in the following sections, it is enough to show that Z  X   X   X   X  Z u  X  u since: that these proofs apply to trees of any depth, not just stumps. Information Gain The proof for Information Gain Ratio is a trivial adaptation of the proof above. Gini Impurity Variance Minimization Inequalities For positive scalars a,b  X  0 and  X , X  &gt; 0, the following inequality holds: | a + b | 2  X  +  X 
