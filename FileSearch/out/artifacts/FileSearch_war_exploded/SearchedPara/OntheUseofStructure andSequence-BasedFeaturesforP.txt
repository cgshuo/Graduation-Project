
The need to retrie ve or classify protein molecules using structur e or sequence-based similarity measur es underlies a wide rang e of biomedical applications. In drug disco very , resear cher s sear ch for proteins that shar e specific chemical properties as possible sour ces for new treatment. With fold-ing simulations, similar intermediate structur es might be in-dicative of a common folding pathway . To derive any type of similarity , howe ver , one must have an effective model of the protein that allows for easy comparison. In this work, we present two normalized, stand-alone repr esentations of proteins that enable fast and efficient object retrie val based on sequence or structur e.

To create our sequence-based repr esentation, we tak e the frequency and scoring matrices returned by the PSI-BLAST alignment algorithm and create a normalized summary us-ing a discr ete wavelet transform. Our structur al descrip-tor is constructed using an algorithm we developed previ-ously . First, we transform eac h 3D structur e into a 2D dis-tance matrix by calculating the pair -wise distance between the amino acids of a protein. We normalize this matrix and apply a 2D wavelet decomposition to gener ate a set of ap-proximation coef ficients, whic h serve as our featur e vector . We also concatenate the sequence and structur al descrip-tor s together to create a hybrid solution.

We evaluate the gener ality of our models by using them as database indices for near est-neighbor and rang e-based retrie val experiments as well as featur e vector s for clas-sification using support vector mac hines. We find that our methods provide excellent performance when compar ed with the curr ent state-of-the-art tec hniques of eac h task. Our results show that the sequence-based repr esentation is on par with, or out-performs, the structur e-based repr esen-tation. Mor eover , we find that in the classification conte xt, the hybrid str ate gy affor ds a significant impr ovement over sequence or structur e.
Given a protein, the ability to effecti vely retrie ve simi-lar molecules, or to identify the functional group to which it belongs, is needed in man y biomedical applications. A key requirement in this process is defining a suitable met-ric of similarity . Protein molecules are highly comple x and a number of dif ferent measures can be used to determine similarity . One could calculate a geometric similarity be-tween the structure of proteins, for instance, or use a mea-sure based on certain sequence-related properties. In man y cases, the application or the available data will dictate the type of similarity required. For instance, if a protein is found to pre vent a disease based on a key chemical prop-erty , researchers may look for sequentially-similar proteins in the hope that the y can also be used for pre vention. On the other hand, if a protein is designed as part of a drug deli v-ery system, structurally-similar proteins might also be used to effecti vely deli ver a medicinal payload to sites within the body . In folding simulations, similar structures between proteins could be indicati ve of a common folding pathw ay. Structural representations are particularly critical for pro-cessing the results of folding simulations, as sequences are fix ed throughout. In this work, we present two protein rep-resentations that can be emplo yed to quickly and efficiently retrie ve molecules using similarity based on either sequence or structure.

Man y researchers have look ed to create protein models for use in lar ge-scale structure-based retrie val [2, 4, 7]. Pro-posed solutions have represented proteins as sets of 3D sub-structures [21], or as triplets deri ved from secondary struc-tures [6]. One dra wback to these particular solutions is that the y rely on the pair -wise alignment of an entire database, which can be very expensi ve for lar ge-scale retrie val tasks. Other algorithms are tied to a specific data structure such as a suf fix tree or inverted file inde x [2, 7]. Unfortunately , these data structures can be memory-intensi ve and do not scale well when faced with thousands of proteins.
There are also a number of approaches that determine similarity between proteins using sequence-based proper -ties. In contrast to structure-based techniques, which typ-ically focus on database retrie val, much of the work using sequence-based information has involv ed predicting the fi-nal structure of an input sequence. Some of the more popu-lar methods include Hidden Mark ov Models (HMMs) [11], profile-based alignment algorithms such as PSI-BLAST [1] support vector machine (SVM) classification [8, 10, 12, 20].
In the protein domain, SVM-based structure predic-tion generally falls into two cate gories. The first con-sists of those methods that tak e a protein and compute a representation encompassing a lar ge number of sequence-based properties [8]. Once the properties have been deter -mined, a feature vector is created by calculating the pair -wise similarity between them. The feature vectors are then used to train the SVM. The second form of SVM struc-ture prediction involv es the use of specialized kernel func-tions [10, 12, 20]. Kernel functions are designed to com-pute a high-dimensional similarity between objects in the dataset. A hyperplane can be constructed in this high-dimensional feature space, partitioning the object space. While kernel functions can be used on pair -wise feature vec-tors generated from several dif ferent properties (as in the former), the y generally rely on just one. These functions have been sho wn to be quite effecti ve, but the y can be com-putationally expensi ve. While some users might be willing to trade speed for accurac y, there is another dra wback to these kernels in that the y are essentially a  X  X lack-box X  so-lution, leading to a loss in the interpretability of both the results and the decision-making process. Moreo ver, there is no intermediate representation that one can leverage for other purposes such as retrie val. The closest thing to an in-dependent feature vector is a pair -wise matrix that illustrates the relationship between every object in the dataset. As the size of the dataset increases, any attempts at manipulating this matrix or using it as a representation quickly become infeasible.

Ev en alignment algorithms such as PSI-BLAST cannot be used to directly compare between a lar ge number of proteins. PSI-BLAST alignments are based on the overall length of the protein, which mak e the widespread compar -ison of results impossible. Thus, it is imperati ve to have a normalized, stand-alone sequence-based representation. Such feature vectors could be used by alternati ve classifica-tion algorithms or in applications such as nearest-neighbor or range-based similarity search. Of course, it would also be desirable if this representation could be used for tasks such as SVM-based classification while still pro viding re-sults that are comparable to the state-of-the-art.
In this paper , we present two representations, one based on sequence, the other on structure, that are designed to ad-dress the challenges listed abo ve. Our sequence-based ap-proach, lik e man y existing solutions, is deri ved from the re-sults of multiple sequence alignments. Using wavelets, we construct a summary of the alignment profile that results in a compact, stand-alone feature vector that can be emplo yed in man y applications. Our structure-based technique, devel-oped pre viously [15], results in a compact descriptor that outperforms the state-of-the-art. We begin by computing the pair -wise distance between the amino acid residues of a protein to transform each 3D structure into a 2D distance matrix. We apply a 2D wavelet decomposition on this ma-trix to generate a set of approximation coef ficients, which serv e as our feature vector . We also combine the two rep-resentations into a hybrid variant when both structural and sequence information is available.

We test the general applicability of our representations by utilizing them as database indices for a series of nearest-neighbor and range-based retrie val experiments and as fea-ture vectors for classification by support vector machine. We find that both our methods outperform existing ap-proaches, but that our sequence-based representation either outperforms, or is on par with, that of our structure-based version. In our nearest-neighbor retrie val tests, we find that the sequence-based approach is comparable to our struc-tural descriptor but outperforms the curr ent state-of-the-art in structur e-based retrie val by 4-13% . In terms of classi-fication accurac y, we find that the sequence and structure-based approaches yield roughly the same performance. In the task of classification, howe ver, we find that the hybrid str ate gy affor ds a significant impr ovement over either of the base ver sions, from 6% to 14% over sequence and struc-ture, respecti vely , suggesting that both representations can be effecti vely combined for such tasks.

The rest of the paper is organized as follo ws. We pro vide background on proteins and protein structure databases in Section 2. Section 3 covers the basics of the wavelet de-composition as well as the procedures used to create our representations. Our datasets are covered in Section 4 and our experiments and results are detailed in Section 5. We conclude with a discussion of those results and possible fu-ture work in Section 6.
Proteins are comprised of a varying sequence of 20 dif ferent amino acids. Indi vidual amino acids are called residues. Each amino acid type is composed of a number of dif ferent atoms, but all contain a central Carbon atom de-noted as C . The position of this atom is often used as an abstraction for the position of the entire residue. The se-quential connection of the C atoms is called the protein backbone. Amino acids combine to form interacting sub-units called secondary structures. Two of the most common secondary structures are -helices and -sheets. The inter -actions between secondary structures give a protein its over-all shape, which is often called the protein X  s fold . Several structural databases have arisen to group proteins based on their fold. One of the most popular is the Structural Classi-fication of Proteins (SCOP) Database [17].
 SCOP arranges proteins into several hierarchical levels. The first four are Class , Fold , SuperF amily and Family . Pro-teins in the same Class share similar secondary structure information, while proteins within the same Fold have sim-ilar secondary structures that are arranged in the same topo-logical configuration. Proteins in the same SuperF amily sho w clear structural homology and proteins belonging to the same Family exhibit a great deal of sequence similarity and are thought to be evolutionarily related.
In this section we detail the steps needed to create our wavelet-based representations. We begin with an overvie w of the wavelet decomposition. We follo w with the proce-dure for computing our sequence-based summary and then cover the process used to create our structural descriptor . We conclude with a description of our hybrid protein model.
The use of wavelets is natural in applications that require a high degree of compression without a corresponding loss of detail, or where the detection of subtle distortions and discontinuities is crucial [14]. Wavelet decompositions fall into two separate cate gories: continuous ( cwt ) and discrete ( dwt ). In this work, we deal with the discrete transform.
Given a decomposition level L and a one-dimensional signal of length N , where N is divisible by 2 L , the dwt consists of L stages. At each stage in the decomposition, two sets of coef ficients are produced, the Approximation and the Detail. The Approximation coef ficients are gener -ated by con volving the input signal with a low-pass filter and down-sampling the results by a factor of two. The De-tail coef ficients are similarly generated, con volving the in-put with a high-pass filter and down-sampling by a factor of two. If the final stage has not been reached, the Approx-imation coef ficients are treated as the new input signal and the process is repeated. In man y cases, once the wavelet transformation is complete, the original signal will be rep-resented using combinations of Approximation and Detail coef ficients. In our applications, howe ver, we only use the final level of Appr oximation coef ficients and ignor e the rest .
To operate on a two-dimensional signal of size N x N , the decomposition proceeds as follo ws: First, the rows are con volv ed with a low-pass filter and downsampled by a fac-tor of two, resulting in matrix L ( N= 2 x N ). The process is repeated on the original signal using a high-pass filter , which leads to matrix H ( N= 2 x N ). The columns of L are con volv ed two separate times, once with a low-pass fil-ter and again with a high-pass filter . After passing through the filters, the signals are downsampled by a factor of two. This results in a matrix of approximation ( LL ) and horizon-tal detail coef ficients ( LH ), respecti vely (both of size x
N= 2 ). These steps are executed once more, this time on the columns of H , resulting in a matrix of diagonal ( HH and vertical ( HL ) detail coef ficients (ag ain of size N= 2 N= 2 ). The whole procedure can then be repeated on the ap-proximation coef ficients contained in matrix LL . As in the one-dimensional case, we only deal with the final level of approximation coef ficients.
Here we pro vide an overvie w of the method used to create our wavelet-based sequence representation. We start with a discussion of PSI-BLAST and follo w with the steps used to normalize the results. We conclude with specific details of the wavelet transformation.
 Generation of PSI-BLAST Pr ofiles: PSI-BLAST is one of the more popular methods used to determine the similar -ity of a protein to a database of sequences. This similar -ity is returned in the form of a profile, or scoring matrix. In PSI-BLAST , a sequence is tested against a database to identify conserv ed patterns, or motifs. For each position in each of these conserv ed regions, the algorithm computes a score for each amino acid type. In highly conserv ed re-gions, those amino acids that are highly conserv ed recei ve a high positi ve score, while the others recei ve high negatives. In weakly conserv ed regions, residues recei ve scores near zero. These scores are calculated based on amino acid fre-quenc y information. Ev olution-based substitution matrices such as BLOSUM [9] can also be used when calculating the scores. This process can be iterati ve, running a profile against the database to refine the results. The final output of the algorithm is a profile that includes a position-specific scoring matrix (PSSM) and a position-specific amino acid frequenc y matrix (PSFM), as well as a sequence of Z-scores or E-v alues that denote the statistical significance of the alignment. When discussing both the PSFM and PSSM matrices, we will refer to them as the PSI-BLAST profile matrices (PBPM).

The first step in creating our sequence-based repre-sentation involv es generating a PSI-BLAST profile for each protein. Using version 2.2.13 of the algorithm, we compute a multi-w ay alignment against the non-redundant ( nr ) protein database. Do wnloaded in March 2006, this database contains almost 3.5 million sequences. We run PSI-BLAST for five iterations with the parameter set to 0.001. Since our representation is deri ved partly from the position-specific frequenc y matrix (PSFM), we set the program to output both that and the position-specific scor -ing matrix (PSSM), in addition to the standard alignment information. We pro vide a graphical illustration of the matrices for protein 1CCR in the left-most plots of Figure 1. Each row corresponds to an amino acid, while each column represents a position in the query sequence. These images are false color representations, so lower values have dark er colors, while higher values appear lighter . Once the profile has been generated, we create a summary of the frequenc y information by applying a 1D wavelet decomposition to the PSFM matrix. We create a similar summary of the profile scores by applying a wavelet decomposition to the PSSM matrix. Pr ofile Normalization: Before we can apply the wavelet decomposition, howe ver, we must normalize the size of the matrix to ensure that we are left with the same number of coef ficients for each protein. This will allo w us freely com-pare between the resulting feature vectors. The PSFM and PSSM matrices are of size n x 20 , where n refers to the num-ber of amino acids in the protein sequence and 20 corre-sponds to one of the dif ferent amino acid types. We fix n be 128 and normalize each PBPM matrix to 128 x 20 . The normalization occurs either through interpolation or extrap-olation, depending on whether the input protein is shorter or longer than 128 residues, respecti vely . We choose a value of 128 because wavelet transformations are most effecti ve on signals whose length is a power of 2. We prefer to interpo-late and smooth or average the excess points, over extrap-olation, where we would be forced to generate additional data. Most of the proteins in our datasets are shorter than 256 residues, thus our choice of 128.

Once the matrix has been normalized we transpose it ( 20 rows x 128 columns) and apply a 4th level Haar 1D decomposition to each row. We only use the final level of approximation coef ficients, so each decomposition will produce 1 coef ficient for every 16 input values ( 2 4 ), or 8 coef ficients per row/amino acid. This results in feature vectors of size 160, which we use when conducting our experiments. These representations can also be combined together into a single profile summary of 320 attrib utes. An example of the wavelet-based representations can be seen in the middle of Figure 1. The PSSM representation is given on the top, while the PSFM version lies on the bottom. Transf ormation Details: The wavelet decomposition cre-ates a frequenc y signature for each amino acid. Using a 4th level Haar decomposition results in 8 coef ficients. The Haar wavelet filter is an averaging filter , so each of these coef ficients will represent the average frequenc y value for 12.5% of the sequence. One could vary the decomposition or the normalization factor to change the number of coef-ficients per amino acid, which would change the represen-tation percentage, but a tradeof f must be made between the total number of coef ficients and the overall accurac y of the representation. As sho wn in Table 2, we varied the decom-position level from 2-7 and found that a 4th level decompo-sition gave us the best overall results.

Since the Haar wavelet filter is orthonormal, each co-efficient represents a non-o verlapping segment of the pro-tein sequence. If one wanted to create a representation that includes such an overlap, this could be done by manually calculating the average using a sliding windo w, or through other techniques such as n-grams. If n were set to 16, one would start at position one, and then compute an n-gram for the first 16 points, slide to the right one position and com-pute a second n-gram. This process would repeat down the profile. Such an approach would increase the total number of coef ficients, howe ver. We use wavelets instead of man-ually computing the summary because of their speed and relati ve ease of implementation.
 Man y SVM kernels that are based on the results of a PSI-BLAST alignment focus solely on values deri ved from the PSSM matrix [10, 12], though there are approaches that in-corporate both matrices [20]. Our own testing on the matter has been inconclusi ve. As we sho w in Section 5, it is not en-tirely clear whether either representation is preferable over the other , though in most cases, the y can be combined to-gether for superior performance.
 We surmise that the PSFM summary outperforms the PSSM-based measure in certain cases because the entries in the scoring matrix represent log-odd ratios, which scales the data and remo ves some of the variance that may help distinguish between the dif ferent groups. As an example, for one of our datasets, all of the PSSM values fall between -7 and 13. The PSFM matrices, on the other hand, contain values between 0 and 100. This information is pro vided in the histograms on the left of Figure 1. The wider variance of the PSFM matrix is at times preferable to the tighter range of the PSSM values. This is analogous to often preferring the covariance matrix over correlation for EM-based miss-ing value analysis [18].
We now cover the steps needed to calculate our wavelet-based structural descriptors. This algorithm has been published pre viously [15], but we repeat it here for the sak e of completeness. We begin by describing the process needed to transform each 3D structure into a 2D distance matrix, follo wed by the procedure used to normalize those matrices. We conclude with details of the wavelet decomposition.
 Generation of Distance Matrices: To generate our structure-based feature vector , we first con vert the protein structure into a distance matrix. This process occurs in the follo wing manner: First, we obtain the 3D coordinates of the protein from the PDB and calculate the distance between the C atoms of each residue. We place these values into an n x n matrix D , where n represents the number of residues in the protein and D(i,j) represents the distance between the C atoms of residues i and j . Figure 3 pro vides a graphical depiction of this matrix (for protein 1CCR), with higher ele vations, or lar ger distances, having a lighter color . In these matrices, secondary structures such as -helices and parallel -sheets emer ge as dark bands along (or parallel to) the main diagonal, while anti-parallel -sheets appear perpendicular to it. The top image in Figure 3 represents a pix elated version of the distance matrix. In contrast to this discretized approach, or to the binary  X  X ontact map X  representation, where distances belo w a certain threshold are set to 1 (else 0), we operate on the actual distance values.
 Matrix Normalization: Ha ving con verted the 3D struc-ture, we apply a 2D decomposition to the distance matrix. As with our sequence-based representation, in order to use the results of this transformation as a feature vector , the final number of coef ficients must be the same for every protein. This can be achie ved either by normalizing the size of the input (the distance matrix), or the output (the approximation coef ficients). It is not immediately clear how to normalize a variable-size coef ficient matrix while still preserving the necessary spatial correlations. Thus, we again normalize the input signal, fixing the size of the distance matrix to 128x128.
 Transf ormation Details: We perform a 2D Haar decom-position on this normalized matrix and use the final level of approximation coef ficients as our feature vector . Examples of the wavelet decomposition can be seen in Figure 3. The top figure illustrates a normalized distance matrix, while the figure on the bottom corresponds to the approximation coef-ficients produced from a 3rd level decomposition. We only focus on the approximation values produced by the final level of the wavelet decomposition, so as the decomposition level increases, the number of coef ficients we must consider decreases by a factor of 4. Despite this lar ge reduction in data, important features such as secondary structures, are still present in the figures. Since the matrix is symmetric across the diagonal, we only need to keep the coef ficients in the upper (or lower) triangle, plus those that fall on the di-agonal itself. Of all the decomposition levels that we tested (see Table 2), we found that the 3rd level affords the best performance between accurac y and feature vector size. This decomposition lea ves us with a structural descriptor of 136 coef ficients per protein.
To create the hybrid variant of our representations, we simply concatenate the PBPM-summaries to the end of the structural descriptors. This results in an overall feature vec-tor with 456 attrib utes that characterizes both the sequence and structure of a protein in a concise, normalized man-ner . One can also create a hybrid version of the indi vidual sequence summaries. In these cases, we concatenate the structure descriptors to the end of the PSSM and PSFM-based representations. Each of these feature vectors con-tains a total of 296 attrib utes.
We validate our representations using two dif ferent sub-sets of the SCOP database. One subset was selected to gauge the performance of our representations in a database conte xt, the other to evaluate classification accurac y.
To evaluate database performance, we focus on a dataset that has been examined in a number of inde xing publica-tions [4, 6], most recently by Gao and Zaki [7]. In this set, a total of 181 SuperF amilies were selected from SCOP , with each SuperF amily having at least ten members. Ten pro-teins were selected from each SuperF amily , resulting in a total dataset size of 1810 proteins. To construct a query set, one protein was selected at random from each of the Super -Families, yielding a query subset of 181 proteins. We refer to this dataset as the 181 SF set.

To evaluate the classification performance of our repre-sentations, we rely on a subset of the ASTRAL database [5]. ASTRAL is a repository deri ved from SCOP that filters pro-teins based on their shared sequence similarity . One version of the ASTRAL database consists of proteins with less than 40% sequence identity , the other with those that share less than 95%. Both versions have been examined in pre vious classification experiments [8, 10, 12], but we choose the ver-sion containing proteins with less than 40% sequence iden-tity , as we feel that poses a greater classification challenge. Using version 1.65 of the ASTRAL database, we tak e all the proteins that share less than 40% sequence identity , which results in a set of approximately 5600 proteins. From there, we select all the proteins that belong to Folds with at least 20 members, remo ving the rest. After this pruning step, we are left with a total of 2915 proteins that belong to 63 dif fer -ent Folds. This dataset, which we call the 63 Fold dataset, is essentially the same as the one used in the SVM exper -iments of Han et al. [8]. Table 1 pro vides the number of unique groups for the 63 Fold and 181 SF datasets over the dif ferent levels of the SCOP hierarch y.

Taking the 63 Fold dataset, we classify proteins using their SCOP labels as our gold standard. We initially focus on the Fold level, but we are also interested in determining the performance of our representations at the other levels of the SCOP hierarch y. If a representation is truly effecti ve, there should be little change in the overall performance as one progresses down the hierarch y. This is despite the fact that the classification challenge itself is increased, with the lower levels containing a lar ger number of potential classes with fewer members per class.

As it is originally constructed, the 63 Fold dataset is not suited for classification at the lower levels of the SCOP hi-erarch y. While each Fold was chosen because it contains at least 20 members, there is no such requirement on the un-derlying Families and SuperF amilies. Some Families, for instance, contain a single member , which mak es classifica-tion at this level impossible.

Therefore, we construct two additional subsets of our 63 Fold dataset to classify at the SuperF amily and Family levels. For the first subset, we select the proteins belonging to those SuperF amilies that contain more than 10 members. We classify this subset, denoted SF &gt; 10 , at the SuperF am-ily level. The second subset contains all the proteins that belong to Families with more than 10 members. This sub-set is classified at the Family level and denoted Fam &gt; We pro vide the membership information for these datasets (as well as the original 63 Fold dataset) in Table 2. We could classify the SuperF amily subset at the Fold level and the Family subset at the SuperF amily and Fold levels but do not. Classifying the smaller Family subset at the Fold level, for instance, will simply result in impro ved performance over the more general 63 Fold dataset. The original 63 Fold dataset presents the greatest classification challenge at the Fold level, and the SuperF amily and Family subsets pose the greatest challenges at their respecti ve levels. All of our experiments were conducted on a 2.4 GHz Pentium 4 PC with 1.5 GB RAM running Ub untu Linux on a 2.6.12 kernel. When referring to our dif ferent repre-sentations, we use PSFM and PSSM to denote the PSFM and PSSM sequence summaries, respecti vely , and 2D to re-fer to our 2D structural decomposition. We also combine the structural descriptor with the indi vidual sequence sum-maries ( 2D &amp; PSFM , 2D &amp; PSSM ). We denote the com-bined PSSM and PSFM descriptors as PBPM and finally , the sequence-structure hybrid as 2D &amp; PBPM .
To illustrate the effecti veness of our representations as an inde xing method, we validate our approach using two dif ferent query retrie val tests, nearest-neighbor and range. For the nearest-neighbor tests, given a query , we retrie ve the k nearest-neighbors from the tar get database and report the number of correct matches. For the range queries, we specify a range value r and retrie ve from the tar get database those objects that lie within a distance r to the query . In both tests, distance refers to the standard Euclidean distance.
We use a k-d tree as our tar get data structure [3]. The k-d tree is a generalization of the binary search tree. Both start with a root node and place the data into left and right subtrees based on the value of a particular attrib ute, or key. This process is recursi vely repeated until the data is divided into mutually exclusi ve subsets. While a binary search tree uses the same key on all nodes of all subtrees, a k-d tree will use a dif ferent key at each level. This leads to a data structure that effecti vely partitions the data but still allo ws the user to search for best matches without having to mak e a lar ge number of comparisons. For our tests, we use k-d tree code obtained from the Auton Lab 1 .

We also compare our techniques with PSIST , one of the more recently-published inde xing methods [7]. In PSIST , a feature vector is generated for each protein based on the distances and angles between residues. These vectors are placed into a suf fix-tree, which serv es as the inde xing structure. Using the PSIST source code 2 , we generate the input features required by the program and test our datasets. The PSIST algorithm allo ws the user to tune a number of parameters, all of which affect both the accurac y and computation time. We test a number of parameter values but find that the program def aults pro vide the best overall performance.
 Near est-Neighbor Retrie val: For our nearest-neighbor re-trie val tests, we use the SCOP labels at the SuperF amily level to determine whether a neighbor is correct. We exam-ine the number of correct matches for k equal to 4, 10, 50 and 100. Similar values have been used in pre vious retrie val experiments [7]. We evaluate our structure and sequence-based representations on the 181 SF dataset and compare against the values returned by PSIST .
 The results of our retrie val tests are presented in Table 3. Sho wn in the table are the average number of nearest-neighbors that belong to the same SuperF amily as the query protein for dif ferent values of k . On the 181 SF dataset, we see that both our repr esentations return a lar ger number of corr ect neighbor s than PSIST for all the tested values of k and that the sequence-based summaries outperform those deri ved from structure. There is little performance gain for values of k lar ger than 10, but that is because the tar get database only contains 10 members for each SuperF amily . We find that PSSM pro vides the best overall performance, though PSFM is comparable. The structure-based values are slightly worse, and we see no impro vement in combining sequence with structure. Range Queries: We also test our representations with short, medium and long range queries using the 181 SF dataset. We empirically define a query as short, medium or long based on the number of objects returned. If the range value is too small, very few objects will be returned. Too lar ge, and the query will return the entire database. Since the choice of the actual range value is dependent on the rep-resentation, we cannot use the same value of r for every test. Therefore, we list the range values used for each representa-tion along with our results. We report the average number of objects retrie ved (total number of objects retrie ved divided by the total number of queries) as well as the average num-ber of matches among those objects. A match occurs when the object and query belong to the same SuperF amily . The results of this experiment are pro vided in Table 4.
As we can see, range queries are highly accurate, but the number of retrie vals is variable. For instance, for the short range values, we retrie ve 3-5 objects with 100% accurac y. Ev en at the long range values, our accurac y remains very high. We see an accurac y of almost 100% with the the 2D features, and 84-98% with the sequence-based representa-tions. Each SuperF amily in the 181 SF dataset contains only 10 members. As we from the table, we can retrie ve half of the possible total with no err or .
For our classification experiments, we use the SVM pro-vided by WEKA, version 3.4 [23]. WEKA uses Platt X  s se-quential minimal optimization (SMO) algorithm for SVM training [19]. We use a linear kernel with the def ault com-ple xity parameter (1.0) and train each classifier in a one-vs-rest fashion. As with our database experiments, we test each representation separately , as well as combined into a single feature vector . We report classification perfor -mance in terms of the average accurac y, with accurac y val-ues reported as the number of true positi ves divided by the number of true positi ves plus the number of true negatives (TP/(TP+TN)), returned as a percentage.

We perform two dif ferent classification tests. For the first set of tests, we train an SVM using 10-fold cross-v alidation and report the average classification accurac y. In these ex-periments, there is at least one member from every class
Features short med: long 2D Match 3.71 4.54 5.90 r = (75 ; 125 ; 250) Total 3.71 4.54 5.93
PSFM Match 3.73 5.58 6.96 r = (25 ; 75 ; 110) Total 3.73 5.58 8.27
PSSM Match 3.78 5.33 7.33 r = (10 ; 20 ; 35) Total 3.78 5.33 7.88
PBPM Match 3.69 5.35 6.76 r = (25 ; 75 ; 110) Total 3.69 5.35 6.88 2D &amp; PSFM Match 4.60 5.62 7.20 r = (150 ; 250 ; 450) Total 4.60 5.62 7.47 2D &amp; PSSM Match 3.61 4.52 5.89 r = (75 ; 125 ; 250) Total 3.61 4.52 5.92 2D &amp; PBPM Match 4.56 5.61 7.20 r = (150 ; 250 ; 450) Total 4.56 5.61 7.46 included in the training data. We vie w such tests as reco g-nition experiments. This is in contrast to our second set of tests, which we refer to as homolo gy detection .
In these experiments, we tak e the SF &gt; 10 and Fam &gt; datasets and set aside 9 SuperF amilies and 8 Families, re-specti vely . The proteins in these groups are completely un-touched during the training phase. We tak e the remaining members of each dataset and train a classifier one level  X  X p X  the hierarch y. In other words, we train an SVM on the SF &gt; 10 dataset using Fold labels and the Fam &gt; 10 dataset using SuperF amily labels. After the classifier is trained (us-ing 10-fold cross-v alidation), we test the independent vali-dation sets and if the classifier can identify the correct Fold of the held-out SuperF amilies and the correct SuperF amily for the held-out Families, we state that the classification has been successful. Similar homology tests have been reported else where [10, 20] and are considered to be a more challeng-ing exercise in structure prediction.

The SuperF amilies that were set aside were selected because the y belong to Folds with more than one Su-perF amily , giving us an opportunity to train a classifier to recognize that Fold. The same process was used for selecting the Families, except that the y were required to belong to a SuperF amily with more than one Family . After dividing the SF &gt; 10 and Fam &gt; 10 datasets using the abo ve criteria, we are left with training sets of 1911 and 877 proteins and testing sets of 203 and 137 proteins, respecti vely .
 Recognition: We present the results of our recognition ex-periments in Table 5. As one can see, Fold-le vel classifica-tion presents the greatest challenge, where we report accu-rac y values of around 66%, 62% and 65% for the 2D, PSFM and PSSM representations, respecti vely . At the SuperF am-ily level, the accurac y for the PSSM and PSFM representa-tions impro ves to roughly 72%, while the performance of our structure-based representation is essentially constant. We see additional impro vement progressing down to the Family level, with the accurac y of the 2D representation in-creasing to 81% and the PSSM and PSFM summaries to almost 85%. While the PSSM summary outperforms the PSFM representation at the Fold level, that edge is erased at the other levels of the hierarch y. Combining the PSSM and PSFM representations together results in an impro vement of approximately 3%.

Table 5 also sho ws the accurac y of our hybrid sequence-structure representation. It is quite surprising how much one can impro ve the overall performance by combining se-quence with structure. At the Family level, the stand-alone PSFM and PSSM summaries yields an accur acy of around 84%. Combined with the structur al featur es, that value incr eases to 89%, an impr ovement of almost 6% . Ev en though the PBPM representation has an accurac y of 87% at the Family level, that value can be increased to more than 90% by adding structure. One can see even greater increases at the SuperF amily and Fold levels; 10 and 14%, respecti vely . While structural information would not be available to a researcher trying to predict the global fold of a protein, our results do indicate that the information contained in our 2D representation could be used in a semi-supervised manner to refine the sequence-based classification results, potentially impro ving the prediction process [22]. In addition, there may be instances where a researcher has access to information on both the sequence and the structure of a protein. In such cases, our methods can be combined to impro ve the results over any indi vidual representation.
 Homology Detection: The results of our homology detec-tion experiments are pro vided in Table 6. While it is un-lik ely that a researcher would use structural information to detect homologs, we include the values for the sak e of completeness, but do not discuss them further . As we see from the results, homology detection is a harder classifi-cation challenge, with accurac y values 10-20% lower than those of the recognition experiments. There is little dif fer -ence between the PSSM and PSFM representations, how-ever. With the combined PBPM representation, we see an increase in Fold-le vel accurac y, but that gain is erased at the SuperF amily level. Our results are comparable to homology results reported else where on similar , though not identical, datasets [20]. We see these results without having to resort to a task-specific kernel function or repr esentation, whic h highlights the gener al applicability of our appr oac h .
In this paper , we present two methods of protein rep-resentation, one based on sequence information, the other deri ved from the solv ed structure. We find that the PSSM and PSFM representations pro vide roughly the same overall performance, though there are instances where the PSSM summary yields a higher accurac y. We also find that even though the PSSM scores are deri ved from the PSFM values, combining the summaries can result in a fairly dramatic in-crease in accurac y. We belie ve this is because the combined PBPM representation mer ges information about the overall amino acid frequenc y counts contained in the PSFM sum-mary as well as the PSSM scoring information pertaining to the overall conserv ed-ness of each region. The PBPM sum-maries are comparable to our 2D structural descriptor . It is also possible, as evident in our classification experiments, to combine information on both sequence and structure into a hybrid repr esentation that can pro vide an impro vement over any indi vidual variant. Such a technique can be useful in ap-plications where the researcher is not limited to sequence or structure and further emphasizes the general applicability of our approach.

We find that the PBPM-based summaries pro vide com-parable performance to our structural representation and that both techniques have applications where one is more suited than the other . If one were interested in structure prediction, for instance, then the sequence-based approach would be an appropriate choice. It is possible, howe ver, to include structure-based information to impro ve the per -formance in fold classification, which implies that the struc-tural representation could be leveraged in a semi-supervised manner to increase the overall accurac y. In some circum-stances, howe ver, a sequence-based representation is simply not useful. If a researcher were examining the results of a protein folding simulation, the need for a structure-based representation is critical. An y attempts to successfully characterize the intermediate structures or analyze common folding pathw ays, either between multiple runs of a sin-gle protein or among the results of several proteins, would hinge on an effecti ve structural representation. In fact, since a protein X  s sequence is static throughout the course of the simulation, it is not possible to use a sequence-based repre-sentation in such settings. While most of the folding simu-lations to date have been relati vely small, focusing on runs of short, engineered proteins, lar ge-scale simulations such as Folding@Home [13] have come online and are expected to generate a tremendous amount of data. In order to have any hope of efficiently processing these results, it is crucial that one emplo y an effecti ve method of representation.
Throughout this work, we have presented our structural descriptors as a way to represent protein molecules. There is nothing that limits our technique to proteins, howe ver. One could easily extend our approach to work with other do-mains, such as defect tracking in molecular dynamics (MD) simulations [16]. MD simulations are used to model the be-havior of spurious atoms as the y mo ve through a lattice of a base material such as silicon. Given a set of simulation frames, one could apply our algorithm to create a sequence describing that set, which could then be compared against other sets. Being able to characterize the beha vior of these defects would be a boon to those who work in the manuf ac-ture of semiconductors.

