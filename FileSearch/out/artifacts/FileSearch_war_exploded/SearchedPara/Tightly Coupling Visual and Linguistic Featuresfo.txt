 People who are blind use screen readers for browsing web pages. Since screen readers read out content serially, a naive readout tends to mix irrelevant and relevant content thereby disrupting the coherency of the material being read out and confusing the listener. To address this problem we can parti-tion web pages into coherent segments and narrate each such piece separately. Extant methods to do segmentation use vi-sual and structural cues without taking the semantics into account and consequently create segments containing irrel-evant material. In this paper, we describe a new technique for creating coherent segments by tightly coupling visual, structural, and linguistic features present in the content. A notable aspect of the technique is that it produces segments with little irrelevant content. Preliminary experiments indi-cate that the technique is effective in creating highly coher-ent segments and the experiences of an early adopter who is blind suggest that it enriches the overall browsing experi-ence.
 H.5.2 [ Information interfaces and presentation ]: User Interfaces; H.3.3 [ Information storage and retrieval ]: Search and Retrieval Algorithms, Design, Human Factors Singular Value Decomposition, Clustering, Segmentation, Screen Reader, Blind
Blind people listen to digital content by having it read out aloud by screen readers. This read out is an inherently serial process. So a naive read out of a web page, especially multi-themed content-rich web pages such as on-line news and e-commerce web sites, will mix relevant content with irrelevant items during the serial read out. This kind of intermixed read outs can confuse and disorient the listener.
Let X  X  suppose that the web page fragment shown in Figure 1 has been organized into four segments labeled S1 through S4. Each segment represents one dominant theme: taxon-omy dominates S1; the dominant theme in S2 is a narrative about gadgetry; S3 X  X  theme is referrals to some popular top-ics; S4 is advertisement.

Observe that S2 consists of content pieces labeled 1 through 4. Screen readers such as JAWS [6] and Apple X  X  VoiceOver [1] will narrate S2 X  X  content by reading out the leaf nodes of its corresponding DOM (sub)tree in left-to-right order which is: piece labeled 1 followed by 2, 3, 4. Pieces labeled 1 and 4 are consecutive paragraphs making up the dominant theme in S2, namely, a narrative of  X  X sing gadgetry X . The other pieces 2 and 3 in S2 are irrelevant to this main narrative. Thus the read out of a segment such as S2 that consists of several irrelevant content items disrupts the flow of the main narrative in several places causing confusion and difficulty in following it.

Segments such as S1 through S4 can be identified using web page segmentation methods. Many such methods have appeared in the literature (e.g., [3, 5, 4]). Segmentation technology is beginning to be embraced in commercial prod-ucts. In fact the segments S1 through S4 in Figure 1 were produced by Apple X  X  auto web spot in VoiceOver.

By and large the reported segmentation methods (includ-ing auto web spot) exploit the idea that content elements making up a segment exhibit consistency in presentation style (captured by visual cues) and spatial locality (captured by structural cues). The problem of relying solely on such visual and structural clues for segmenting is that the result-ing segments consist of content elements that are coherent in a  X  X resentational X  but not necessarily in a semantic sense. For example, observe in S2 that although pieces 1 through 4 exhibit presentation consistency and spatial locality to be included in the same segment, pieces 2 and 3 bear no se-mantic similarity to the pieces 1 and 4 that constitute the main narrative.

Eliminating such extraneous items is an interesting prob-lem that is addressed in this paper. The idea is to exploit linguistic, visual and structural features in the segmenta-tion process. The effect of such a tight coupling will create segments whose elements are spatially local, as well as lin-guistically and presentationally similar. Applying this idea will further refine S2 into 3 smaller segments as shown in Figure 1. The para pieces 1 and 4 get into one segment since they are all semantically similar whereas each of the remaining non-para pieces -2 and 3 -form a separate seg-ment by themselves since they bear no semantic similarity to any other piece.
 Overview of our Approach
We adapt Latent Semantic Analysis (LSA) [8] for this problem. LSA is a technique that computes the linguistic similarity of text documents. It will, for instance, compute a high degree of similarity for the para pieces in S2 and a low similarity value for the others. However, LSA relies only on words as features for computing linguistic similar-ity. On the other hand, we also want to utilize visual and structural similarities. So we expand the feature sets used in LSA to include both visual features, as well as words as features. LSA with this expanded feature computes a com-posite similarity score among the web elements based on their visual/structural/linguistic features combined. This adaptation of LSA is a fully automatic online process with no a priori training. Following the LSA step we utilize the composite score to automatically cluster the web elements bottom-up into coherent thematic segments.

The paper is organized as follows: Section 2 gives a brief overview our algorithm; experimental results and prelimi-nary user experience with an early adopter (who is blind) are presented in Sections 3; Section 4 discusses related works; Conclusions appear in Section 5.
Latent Semantic Analysis (LSA) [8] is a powerful tech-nique for analyzing relationships in a set of text documents based on the terms (i.e., words) present in those documents. Let W be a m  X  n term-document matrix in which cell ( i, j ) is associated with the occurrence of term i in document j .
LSA uses Singular Value Decomposition (SVD) [8] to project terms onto a reduced vector space. SVD of W is defined as: where U and V denote the left and right singular matrices respectively and  X  is the diagonal matrix whose diagonal elements are singular values arranged in descending order. Note that each singular value represents the importance of each dimension. By setting less important dimensions to zero LSA retains only the important information and gets rid of irrelevant details. Suppose we keep only the k (  X  min ( m, n )) largest singular values and set others to zero. Then the rank-k approximation to W is:
This reduced LSA is a relatively more compact representa-tion of similarity and maps synonyms to the same dimension in the reduced vector space.
Traditionally LSA has been primarily used for analyzing text documents based on linguistic features in the context of NLP and IR applications. For the problem of identifying coherent segments that is addressed in this paper, we pro-pose extending LSA with visual and structural features of web elements. In our formulation each web element is con-sidered as a document and each term represents either a vi-sual, structural or linguistic attribute of a web element. For our problem the matrix W is a m  X  n attribute-web element matrix; w i,j is 1(0) whenever attribute i is present(absent) in web element j . Each column vector ( e j ) represents a web element and each row vector ( a i ) represents an attribute.
The idea behind using visual, structural and linguistic at-tributes together is to identify more coherent segments with very little irrelevant material.
Algorithm CoherentSeg is a high-level algorithmic sketch of our ideas for identifying segments with high degree of co-herency. Given a web page P it first computes a matrix M representing the similarity among the web elements of P based on visual/structural/linguistic features. The com-putation of the matrix is based on LSA with the extended feature set. Following this step the similarity information in matrix M is utilized to cluster the web elements into assorted coherent segments. We point out that Algorithm CoherentSeg operates on web pages in real-time.
 Algorithm CoherentSeg Input: P : A web page Output: S : A set of coherent segments of P 1. M  X  ComputeSimilarityM atrix ( P ). 2. S  X  ClusterSegments ( M ).
Given a web page P we compute a matrix M consisting of similarity values between every pair of web elements. What follows is a description of the steps of similarity matrix com-putation.

Feature Extraction: We traverse the DOM tree of the web page to collect attribute value data of the web elements in the page. The attribute data of a web page element in-cludes text-attributes, x-position, y-position, width, height, bgcolor, font, parent-id, div-id, table-id, form-id, p-tag-id, UL-id, LI-id .

Now we construct an attribute-element matrix W where columns represent web page elements and rows represent attributes. As mentioned earlier w i,j is 1 if the i th attribute is present in the j th element, otherwise it is 0.

Singular Value Decomposition: In this step we de-compose the original matrix W into three components: the term/attribute matrix U , the document/web element ma-trix V and the singular matrix  X . Each singular value in  X  represents the importance of a dimension. Unimportant dimensions are characterized by singular values which are close to zero. These are set to 0 and we get the matrix  X  The product of these three matrices W k gives a low rank approximation to W .

Similarity Matrix Construction: W k is used to com-pute the element-element similarity matrix M . The (i,j) entry in M denotes the similarity between elements i and j . Note that, column vectors of W k correspond to the web elements. Given two column vectors e i , e j in W k we com-pute the cosine similarity between elements i and j . M will now be used to cluster the web page elements into coherent segments.

Figure 2: Accuracy of the segmentation algorithm
We iteratively cluster the web elements bottom-up into larger coherent segments based on similarity matrix M . Ini-tially each web element is a separate segment by itself. Two maximally similar segments are merged together to get a new segment. This process continues until the clustering quality is less than a threshold value. The threshold is set to the average of all the similarity values in matrix M . Our empirical study suggests that this approach gives high accu-racy across different web sites.

Matrix M also comes into play in the computation of intra and inter cluster similarities of segments. Intra-cluster simi-larity is the measure of similarity of the elements in a cluster or segment whereas Inter-cluster similarity is the measure of similarity between two clusters or segments.

We compute intra and inter-cluster similarity and cluster-ing quality using [10]. We determine the pair of segments ( S i , S j ) which has the highest inter-cluster similarity. All the web page elements of this segment pair are merged to-gether to construct a new segment. The algorithm iterates as long as the quality of clustering after replacing S i , S their merged segment is above the threshold. Otherwise it terminates declaring the current segments as the final out-put.

It is worthwhile mentioning that by restricting the feature set to include only the visual and structural attributes of web elements Algorithm CoherentSeg will identify segments that are spatially local and have presentational consistency (such as segments S 1 to S 4 in Figure 1) akin to the algorithms in [3, 1].
To evaluate our algorithm X  X  performance, we collected a dataset of 300 web pages from different domains (e.g., news, shopping, education, email, encyclopedia, weather forecast, etc.). A total of 5,478 segments were identified and labeled manually in those web pages. We then ran our segmenta-tion algorithm on the dataset. Since the algorithm did not require any training, the entire dataset was used for testing purposes.

We tested our segmentation algorithm, first, considering only visual and structural features of web elements. We then repeated the experiment using visual, structural, and linguistic features. Figure 2 illustrates the average recall, Figure 3: Mean and st. dev. of task completion time and keyboard shortcuts precision, and F-measure for the two experiments. As ex-pected, using the combination of linguistic, visual and struc-tural features, we were able to achieve higher (by 10%) over-all accuracy with more coherent segments.

To get a sense of the user experience we conducted another experiment with a early adopter who is blind. We used the HearSay [2] aural web browser which provides standard screen-reading functionalities similar to JAWS [6]. For the study, we extended HearSay with an interface for navigating segments. The interface shortcuts allowed users to navigate back and forward among the coherent segments produced by CoherentSeg . We will refer to this extended version as the segmentation system.

We asked the blind user to perform various browsing tasks using both systems. The tasks included finding specific in-formation in news articles, Wiki, weather reports, shopping sites, emails, etc. During the study we measured the time and the number of shortcuts used to complete each task. Figure 3 presents the summary of the results. On the av-erage the user was able to complete the tasks 1.61 times faster with segmentation system compared to the base sys-tem, and pressed fewer shortcuts. The ability to quickly navigate among the identified segments helped the subject to skip irrelevant information. The user agreed that the segmentation system enhanced the browsing experience.
VoiceOver(VO) [1] is a screen reader with a number of useful features for web navigation. Its  X  X uto web spot X  does web-segmentation based on visual and structural informa-tion of the web page. The key advantage of using this fea-ture is that users can jump from one web spot to another using shortcuts. However, as VO primarily considers pre-sentational features of web elements, it is frequently unable to identify coherent segments.

The Readability [9] tool converts any web page into a clean reading view. Given an online article page, readability extracts the main article, ignoring all the irrelevant parts (e.g., advertisements, menus, headers, footers, etc.). How-ever it is not able to handle web pages with more than one article. Note that, the segmentation approach proposed in this paper is also able to extract main content.

VIPS [3] employs rules to analyze structural information to segment web pages. Consequently it does not work well on web content where linguistic attributes are important.
Guo et. al. [5] describe a general-purpose approach to partitioning web pages. Instead of using any domain-specific knowledge they use visual layout information and presenta-tion style similarity to determine the spatial locality of the web elements. The idea of contextual browsing is introduced in [7] where visual and structural information was used for web page segmentation.
In this paper, we described an algorithm for generating clutter-free thematic segments from web pages by adapting LSA, a technique pioneered for IR and NLP applications. The segments generated by our algorithm are amenable for distraction-free narration by TTS engines. This is very im-portant to Web users who are blind because now they can listen to web content without becoming confused and disori-ented.

A novel aspect of our algorithm is the incorporation of visual and structural features of web elements in addition to the  X  X ords as features, X  traditionally used in LSA. This facilitates the generation of segments that are spatially local as well as thematic in  X  X orm and content X . Another interest-ing aspect is that our algorithm operates in real-time and neither uses prior training data nor does it do any parameter learning.

Quantitative evaluation showed that our algorithm has high recall and precision. The results of a preliminary user study with a blind individual have been encouraging.
Acknowledgements: This work was supported in part by NSF grants CNS-0751083 and IIS-0808678. [1] Apple. Voiceover, screen reader from apple [2] Y. Borodin, F. Ahmed, M. A. Islam, Y. Puzis, [3] D. Cai, S. Yu, J.-R. Wen, and W.-Y. Ma. VIPS: a [4] D. Chakrabarti, R. Kumar, and K. Punera. A [5] H.-F. Guo, J. Mahmud, Y. Borodin, A. Stent, and [6] JAWS. (http://www.freedomscientific.com). 2010. [7] J. Mahmud, Y. Borodin, and I. V. Ramakrishnan. [8] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [9] Readability. (https://www.readability.com). 2010. [10] A. Strehl. Relationship-based clustering and cluster
