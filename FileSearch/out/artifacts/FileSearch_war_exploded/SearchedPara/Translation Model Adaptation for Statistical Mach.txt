 In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put in-to practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems of-ten suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely approximate the empirical distributions of the train-ing data, which typically consist of bilingual sen-tences and monolingual target language sentences. When the translated texts and the training data come from the same domain, SMT systems can achieve good performance, otherwise the translation quality degrades dramatically. Therefore, it is of significant importance to develop translation systems which can be effectively transferred from one domain to anoth-er, for example, from newswire to weblog .

According to adaptation emphases, domain adap-tation in SMT can be classified into translation mod-el adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilin-gual corpus, for domain-specific translation task, leaving others for future work. In this aspect, pre-vious methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel cor-pus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ig-noring the monolingual corpora, therefore limiting the potential of further translation quality improve-ment.

In this paper, we propose a novel adaptation method to adapt the translation model for domain-specific translation task by utilizing in-domain monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some spe-cific topical contexts, and the topical context infor-mation has a great effect on translation selection. For example,  X  bank  X  often occurs in the sentences related to the economy topic when translated into  X  y  X  i nh  X  a ng  X , and occurs in the sentences related to the geography topic when translated to  X  h  X  e ` a n  X . There-fore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if  X  bank  X  occurs more often in the sentences related to the economy topic than the ones related to the geography topic, it is more likely that  X  bank  X  out-of-domain bilingual corpus, we first incorporate the topic information into translation probability es-timation, aiming to quantify the effect of the topical context information on translation selection. Then, we rescore all phrase pairs according to the phrase-topic and the word-topic posterior distributions of the additional in-domain monolingual corpora. As compared to the previous works, our method takes advantage of both the in-domain monolingual cor-pora and the out-of-domain bilingual corpus to in-corporate the topic information into our translation model, thus breaking down the corpus barrier for translation quality improvement. The experimental results on the NIST data set demonstrate the effec-tiveness of our method.

The reminder of this paper is organized as fol-lows: Section 2 provides a brief description of trans-lation probability estimation. Section 3 introduces the adaptation method which incorporates the top-ic information into the translation model; Section 4 describes and discusses the experimental results; Section 5 briefly summarizes the recent related work about translation model adaptation. Finally, we end with a conclusion and the future work in Section 6. The statistical translation model, which contains phrase pairs with bi-directional phrase probabilities and bi-directional lexical probabilities , has a great effect on the performance of SMT system. Phrase probability measures the co-occurrence frequency of a phrase pair, and lexical probability is used to vali-date the quality of the phrase pair by checking how well its words are translated to each other.

According to the definition proposed by (Koehn et al., 2003), given a source sentence f = f J 1 = f ,...,f j ,...,f J , a target sentence e = e I 1 = e ,...,e i ,...,e I , and its word alignment a which is a subset of the Cartesian product of word position-s: a  X  ( j,i ) : j = 1 ,...,J ; i = 1 ,...,I , the phrase pair (  X  f,  X  e ) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word inside one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase. After all consistent phrase pairs are extracted from training corpus, the phrase probabil-ities are estimated as relative frequencies (Och and Ney, 2004): Here count (  X  f,  X  e ) indicates how often the phrase pair (  X  f,  X  e ) occurs in the training corpus.

To obtain the corresponding lexical weight, we first estimate a lexical translation probability distri-bution w ( e | f ) by relative frequency from the train-ing corpus: Retaining the alignment  X  a between the phrase pair (  X  f,  X  e ) , the corresponding lexical weight is calculated as p w (  X  e |
However, the above-mentioned method only counts the co-occurrence frequency of bilingual phrases, assuming that the translation probability is independent of the context information. Thus, the statistical model estimated from the training data is not suitable for text translation in different domains, resulting in a significant drop in translation quality. In this section, we first briefly review the principle of Hidden Topic Markov Model(HTMM) which is the basis of our method, then describe our approach to translation model adaptation in detail. 3.1 Hidden Topic Markov Model During the last couple of years, topic models such as Probabilistic Latent Semantic Analysis (Hof-mann, 1999) and Latent Dirichlet Allocation mod-el (Blei, 2003), have drawn more and more attention and been applied successfully in NLP community. Based on the  X  X ag-of-words X  assumption that the or-der of words can be ignored, these methods model the text corpus by using a co-occurrence matrix of words and documents, and build generative model-s to infer the latent aspects or topics. Using these models, the words can be clustered into the derived topics with a probability distribution, and the corre-lation between words can be automatically captured via topics.

However, the  X  X ag-of-words X  assumption is an unrealistic oversimplification because it ignores the order of words. To remedy this problem, Gruber et al.(2007) propose HTMM, which models the topics of words in the document as a Markov chain. Based on the assumption that all words in the same sen-tence have the same topic and the successive sen-tences are more likely to have the same topic, HTM-M incorporates the local dependency between words by Hidden Markov Model for better topic estima-tion.

HTMM can also be viewed as a soft clustering tool for words in training corpus. That is, HT-MM can estimate the probability distribution of a topic over words, i.e. the topic-word distribution P ( word | topic ) during training. Besides, HTMM derives inherent topics in sentences rather than in documents, so we can easily obtain the sentence-topic distribution P ( topic | sentence ) in training corpus. Adopting maximum likelihood estima-tion(MLE), this posterior distribution makes it pos-sible to effectively calculate the word-topic distri-bution P ( topic | word ) and the phrase-topic distribu-tion P ( topic | phrase ) both of which are very impor-tant in our method. 3.2 Adapted Phrase Probability Estimation We utilize the additional in-domain monolingual corpora to adapt the out-of-domain translation mod-el for domain-specific translation task. In detail, we build an adapted translation model in the following steps:  X  Build a topic-specific translation model to  X  Estimate the topic posterior distributions of  X  Score the phrase pairs according to the prede-
Formally, we incorporate monolingual topic in-formation into translation probability estimation, and decompose the phrase probability  X  (  X  e |  X  f ) follows: where  X  (  X  e |  X  f,t f ) indicates the probability of trans-lating  X  f into  X  e given the source-side topic t f , P ( t f |  X  f ) denotes the phrase-topic distribution of
To compute  X  (  X  e |  X  f ) , we first apply HTMM to re-spectively train two monolingual topic models with the following corpora: one is the source part of the out-of-domain bilingual corpus C f out , the oth-er is the in-domain monolingual corpus C f in in the source language. Then, we respectively estimate avoid confusion, we further refine  X  (  X  e |  X  f,t f P ( t f |  X  f ) with  X  (  X  e |  X  f,t f out ) and P ( t f in tively. Here, t f out is the topic clustered from the corpus C f out , and t f in represents the topic derived from the corpus C f in .

However, the two above-mentioned probabilities can not be directly multiplied in formula (4) be-cause they are related to different topic spaces from different corpora. Besides, their topic dimension-s are not assured to be the same. To solve this problem, we introduce the topic mapping probabili-ty P ( t f out | t f in ) to map the in-domain phrase-topic distribution into the one in the out-domain topic s-pace. To be specific, we obtain the out-of-domain phrase-topic distribution P ( t f out |  X  f ) as follows: P ( t f out |  X  f ) = X
Thus formula (4) can be further refined as the fol-lowing formula:
Next we will give detailed descriptions of the cal-culation methods for the three probability distribu-tions mentioned in formula (6). 3.2.1 Topic-Specific Phrase Translation
We follow the common practice (Koehn et al., 2003) to calculate the topic-specific phrase trans-lation probability, and the only difference is that our method takes the topical context information in-to account when collecting the fractional counts of phrase pairs. With the sentence-topic distribution P ( t f out | f ) from the relevant topic model of C f out the conditional probability  X  (  X  e |  X  f,t f out ) can be eas-ily obtained by MLE method: where C out is the out-of-domain bilingual training corpus, and count  X  f , e  X  (  X  f,  X  e ) denotes the number of the phrase pair (  X  f,  X  e ) in sentence pair  X  f , e  X  . 3.2.2 Topic Mapping Probability P ( t f out | t f in )
Based on the two monolingual topic models re-spectively trained from C f in and C f out , we com-pute the topic mapping probability by using source word f as the pivot variable. Noticing that there are some words occurring in one corpus only, we use the words belonging to both corpora during the mapping procedure. Specifically, we decompose Here we first get P ( f | t f in ) directly from the top-ic model related to C f in . Then, considering the sentence-topic distribution P ( t f out | f ) from the rel-evant topic model of C f out , we define the word-topic distribution P ( t f out | f ) as: where count f ( f ) denotes the number of the word f in sentence f . 3.2.3 Phrase-Topic Distribution P ( t f in |  X  f )
A simple way to compute the phrase-topic distri-bution is to take the fractional counts from C f in and then adopt MLE to obtain relative probability. However, it is infeasible in our model because some phrases occur in C f out while being absent in C f in . To solve this problem, we further compute this pos-terior distribution by the interpolation of two model-s: where P mle ( t f in |  X  f ) indicates the phrase-topic dis-tribution by MLE, P word ( t f in |  X  f ) denotes the phrase-topic distribution which is decomposed into the topic posterior distribution at the word level, and  X  is the interpolation weight that can be optimized over the development data.

Given the number of the phrase  X  f in sentence f denoted as count f (  X  f ) , we compute the in-domain phrase-topic distribution in the following way:
Under the assumption that the topics of all word-s in the same phrase are independent, we consid-er two methods to calculate P word ( t f in |  X  f ) . One is a  X  X oisy-OR X  combination method (Zens and Ney, 2004) which has shown good performance in calcu-lating similarities between bags-of-words in differ-ent languages. Using this method, P word ( t f in |  X  f ) is defined as: where P word (  X  t f in |  X  f ) represents the probability that t f in is not the topic of the phrase P (  X  t f in | f j ) indicates the probability that t f in the topic of the word f j .

The other method is an  X  X veraging X  combination one. With the assumption that t f in is the topic of  X  f if at least one of the words in  X  f belongs to this topic, we derive P word ( t f in |  X  f ) as follows: where |  X  f | denotes the number of words in phrase  X  f . 3.3 Adapted Lexical Probability Estimation Now we briefly describe how to estimate the adapted lexical weight for phrase pairs, which can be adjust-ed in a similar way to the phrase probability.
Specifically, adopting our method, each word is considered as one phrase consisting of only one word, so Here we obtain w ( e | f,t f out ) with a simi-lar approach to  X  (  X  e |  X  f,t f out ) , and calculate P ( t f out | t f in ) and P ( t f in | f ) by resorting to formulas (8) and (9).

With the adjusted lexical translation probability, we resort to formula (4) to update the lexical weight for the phrase pair (  X  f,  X  e ) . We evaluate our method on the Chinese-to-English translation task for the weblog text. After a brief de-scription of the experimental setup, we investigate the effects of various factors on the translation sys-tem performance. 4.1 Experimental setup In our experiments, the out-of-domain training cor-pus comes from the FBIS corpus and the Hansard-s part of LDC2004T07 corpus ( 54 . 6 K documents with 1 M parallel sentences, 25 . 2 M Chinese words and 29 M English words). We use the Chinese Sohu weblog in 2009 1 and the English Blog Authorship corpus 2 (Schler et al., 2006) as the in-domain mono-lingual corpora in the source language and target language, respectively. To obtain more accurate top-ic information by HTMM, we firstly filter the noisy blog documents and the ones consisting of short sen-tences. After filtering, there are totally 85 K Chinese blog documents with 2 . 1 M sentences and 277 K En-glish blog documents with 4 . 3 M sentences used in our experiments. Then, we sample equal numbers of documents from the in-domain monolingual corpo-ra in the source language and the target language to respectively train two in-domain topic models. The web part of the 2006 NIST MT evaluation test da-ta, consisting of 27 documents with 1048 sentences, is used as the development set, and the weblog part of the 2008 NIST MT test data, including 33 docu-ments with 666 sentences, is our test set.

To obtain various topic distributions for the out-of-domain training corpus and the in-domain mono-lingual corpora in the source language and the tar-get language respectively, we use HTMM tool devel-oped by Gruber et al.(2007) to conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of d-ifferent corpora: topics = 50 ,  X  = 1 . 5 ,  X  = 1 . 01 , iters = 100 . See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight  X  in formula (10) to 0 . 5 by ob-serving the results on development set in the addi-tional experiments.

We choose MOSES, a famous open-source phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics  X  X row-diag-final-and X  are used to generate a word-aligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram lan-guage models on the filtered English Blog Author-ship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and per-form minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sam-pling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods
Our first experiments investigate the effect of dif-ferent smoothing methods for the in-domain phrase-topic distribution:  X  X oisy-OR X  and  X  X veraging X . We build adapted phrase tables with these two meth-ods, and then respectively use them in place of the out-of-domain phrase table to test the system perfor-mance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes of in-domain monolingual corpora: 5 K and 40 K .
 Noisy-OR ( 5 K ) 31 . 16 20 . 45 Averaging ( 5 K ) 31 . 51 20 . 54 Noisy-OR ( 40 K ) 31 . 87 20 . 76 Averaging ( 40 K ) 31 . 89 21 . 11
Table 1 reports the BLEU scores of the translation system under various conditions. Using the out-of-domain phrase table, the baseline system achieves a BLEU score of 20 . 22 . In the experiments with the small-scale in-domain monolingual corpora, the BLEU scores acquired by two methods are 20 . 45 and 20 . 54 , achieving absolute improvements of 0 . 23 and 0 . 32 on the test set, respectively. In the exper-iments with the large-scale monolingual in-domain corpora, similar results are obtained, with absolute improvements of 0 . 54 and 0 . 89 over the baseline system.

From the above experimental results, we know that both  X  X oisy-OR X  and  X  X veraging X  combination methods improve the performance over the base-line, and  X  X veraging X  method seems to be slight-ly better. This finding fails to echo the promis-ing results in the previous study (Zens and Ney, 2004). This is because the  X  X oisy-OR X  method in-volves the multiplication of the word-topic distribu-tion (shown in formula (12)), which leads to much sharper phrase-topic distribution than  X  X veraging X  method, and is more likely to introduce bias to the translation probability estimation. Due to this rea-son, all the following experiments only consider the  X  X veraging X  X ethod. 4.2.2 Effect of Combining Two Phrase Tables
In the above experiments, we replace the out-of-domain phrase table with the adapted phrase table. Here we combine these two phrase tables in a log-linear framework to see if we could obtain further improvement. To offer a clear description, we repre-sent the out-of-domain phrase table and the adapted phrase table with  X  X utBP X  and  X  X dapBP X , respec-tively.

Table 2 shows the results of experiments using d-ifferent phrase tables. Applying our adaptation ap-proach, both  X  X dapBP X  and  X  X utBP + AdapBP X  consistently outperform the baseline, and the lat-ter produces further improvements over the former. Specifically, the BLEU scores of the  X  X utBP + AdapBP X  method are 20 . 70 and 21 . 20 , which ob-tain 0 . 48 and 0 . 98 points higher than the baseline method, and 0 . 16 and 0 . 09 points higher than the  X  X dapBP X  method. The underlying reason is that the probability distribution of each in-domain sentence often converges on some topics in the  X  X dapBP X  method and some translation probabilities are over-estimated, which leads to negative effects on the translation quality. By using two tables together, our approach reduces the bias introduced by  X  X dapBP X , therefore further improving the translation quality. 4.2.3 Effect of In-domain Monolingual Corpus
Finally, we investigate the effect of in-domain monolingual corpus size on translation quality. In the experiment, we try different sizes of in-domain documents to train different monolingual topic mod-els: from 5 K to 80 K with an increment of 5 K each time. Note that here we only focus on the exper-iments using the  X  X utBP + AdapBP X  method, be-cause this method performs better in the previous experiments.

Figure 1 shows the BLEU scores of the transla-tion system on the test set. It can be seen that the more data, the better translation quality when the corpus size is less than 30 K . The overall BLEU scores corresponding to the range of great N val-ues are generally higher than the ones correspond-ing to the range of small N values. For example, the BLEU scores under the condition within the range [25 K, 80 K ] are all higher than the ones within the range [5 K, 20 K ] . When N is set to 55 K , the BLEU score of our system is 21.40 , with 1.18 gains on the baseline system. This difference is statistically sig-nificant at P &lt; 0.01 using the significance test tool developed by Zhang et al.(2004). For this experi-mental result, we speculate that with the increment of in-domain monolingual data, the corresponding topic models provide more accurate topic informa-tion to improve the translation system. However, this effect weakens when the monolingual corpora continue to increase. Most previous researches about translation model adaptation focused on parallel data collection . For example, Hildebrand et al.(2005) employed infor-mation retrieval technology to gather the bilingual sentences, which are similar to the test set, from available in-domain and out-of-domain training da-ta to build an adaptive translation model. With the same motivation, Munteanu and Marcu (2005) extracted in-domain bilingual sentence pairs from comparable corpora. Since large-scale monolin-gual corpus is easier to obtain than parallel corpus, there have been some studies on how to generate parallel sentences with monolingual sentences. In this respect, Ueffing et al. (2008) explored semi-supervised learning to obtain synthetic parallel sen-tences, and Wu et al. (2008) used an in-domain translation dictionary and monolingual corpora to adapt an out-of-domain translation model for the in-domain text.

Differing from the above-mentioned works on the acquirement of bilingual resource, several stud-ies (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) adopted mixture modeling frame-work to exploit the full potential of the existing par-allel corpus. Under this framework, the training cor-pus is first divided into different parts, each of which is used to train a sub translation model, then these sub models are used together with different weights during decoding. In addition, discriminative weight-ing methods were proposed to assign appropriate weights to the sentences from training corpus (Mat-soukas et al., 2009) or the phrase pairs of phrase ta-ble (Foster et al., 2010). Final experimental result-s show that without using any additional resources, these approaches all improve SMT performance sig-nificantly.

Our method deals with translation model adap-tation by making use of the topical context, so let us take a look at the recent research developmen-t on the application of topic models in SMT. As-suming each bilingual sentence constitutes a mix-ture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical ad-mixture formalism to improve word alignment by capturing topic sharing at different levels of linguis-tic granularity. Tam et al.(2007) proposed a bilin-gual LSA, which enforces one-to-one topic corre-spondence and enables latent topic distributions to be efficiently transferred across languages, to cross-lingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose re-lated target phrases according to the specific topic of the source phrase.

Besides, our approach is also related to context-dependent translation. Recent studies have shown that SMT systems can benefit from the utiliza-tion of context information. For example, trigger-based lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selec-tion (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former gener-ated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich con-text information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks.

As compared to the above-mentioned works, our work has the following differences.  X  We focus on how to adapt a translation mod- X  In addition to the utilization of in-domain  X  Different from trigger-based lexicon model and This paper presents a novel method for SMT sys-tem adaptation by making use of the monolingual corpora in new domains. Our approach first esti-mates the translation probabilities from the out-of-domain bilingual corpus given the topic information, and then rescores the phrase pairs via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental results show that our method achieves better perfor-mance than the baseline system, without increasing the burden of the translation system.

In the future, we will verify our method on oth-er language pairs, for example, Chinese to Japanese. Furthermore, since the in-domain phrase-topic dis-tribution is currently estimated with simple smooth-ing interpolations, we expect that the translation sys-tem could benefit from other sophisticated smooth-ing methods. Finally, the reasonable estimation of topic number for better translation model adaptation will also become our study emphasis.
 The authors were supported by 863 State Key Project (Grant No. 2011AA01A207), National Natural Science Foundation of China (Grant Nos. 61005052 and 61103101), Key Technologies R &amp; D Program of China (Grant No. 2012BAH14F03). We thank the anonymous reviewers for their insightful comments. We are also grateful to Ruiyu Fang and Jinming Hu for their kind help in data processing.
