 set of objects into a more complete set. A well-known example system that does set expansion using the web is Google Sets. In this paper, we propose a novel method for expanding sets of named entities. The approach can be applied to se mi-structured documents written in any markup language and in any human language. We present experimental results on 36 benchmark sets in three languages, showing that our system is superior to Google Sets in terms of mean average precision. constellations, or US presidents, but were only able to remember the names of a few of them? In this paper we consider the problem of set expansion using the web as a resource. In set expansion, the user issues a query consisting of a small number of seeds x 1 , x 2 , ..., x k (e.g.,  X  X rsa major X ,  X  X rion X ) where each x i is a member of some target set S t . The answer to the query is a listing of other probable elements of S t (e.g.,  X  X rsa minor X ,  X  X ancer X ,  X  X anis major X , etc). based set expansion system 1 . Google Sets has been used for a number of purposes in the research community, including deriving features for named-entity recognition [1], a nd evaluation of question answering systems [2]; unfortunately, however, Google Sets is a proprietary method that may be changed at any time, so research results based on Google Sets cannot be reliably replicated. Set expansion using the web is also closely related to the problem of unsupervised relation learning from the web [3, 4], and set-expans ion-like techniques have been used to derive featur es for concept-learning [5], to construct  X  X seudo-users X  for collaborative filtering [6], and to compute similarity between attribute values in autonomous databases [7]. Set Expander for Any Language (SEAL). As we will detail below, SEAL works by automatically finding semi-structured web pages that contain  X  X ists X  of items, and then aggregating these  X  X ists X  so that the  X  X ost promising X  items are ranked higher. Unlike earlier systems, the SEAL method is simple enough to be easily described and replicat ed, and is independent of the human language from which the seeds are taken. SEAL is also independent of the markup language used to annotate the semi-structured documents. Extensive experiments have been conducted with SEAL, based on 36 benchmark problems from three languages, each of which consists of a moderate-sized set of entities that is semantically well-defined (e.g., the constellations, or the major-league baseball teams). With randomly constructed three-seed queries from these domains, SEAL obtains a mean average precision (MAP) of more than 94% for English-language queries, more than 93% for Japanese queries, and nearly 95% for Chinese queries. MAP performance on the English-language queries is more than double that of Google Sets X . (Google Sets cannot be used for non-English queries). research contributions. To find  X  X ists X  of items on semi-structured pages, SEAL uses a novel technique to automatically construct wrappers (i.e., page-specific extraction rules) for each page that contains the seeds. Every wrapper is defined by two character strings, which specify the left-context and right-context necessary for an entity to be extracted from a page. These strings are chosen to be maximally-long contexts that bracket at l east one occurrence of every seed string on a page. The use of character-level wrapper definitions means that SEAL is completely language-independent: it is not even necessary to be able to tokenize the target language.  X  X oisy X   X  i.e., they will extract some entities not in the user X  X  target set S t . Thus, it is important to rank entities, so that the entities most likely to be in the target set are ranked higher. To rank entities, SEAL uses another novel approach: a graph is built containing all seeds, all c onstructed wrappers, and all extracted candidate entities. Candidates are then ranked by  X  X imilarity X  to th e seed entities, according to a certain measure of simila rity in the graph. The similarity metric is defined by aggregating the results of many randomly-selected walks through the graph, where each walk is defined by a particular random process. illustrates the architecture of SEAL system. Section 3 describes how wrappers ar e constructed. Section 4 explains our ranking sche me based on graph walk. Section 5 presents our evaluation dataset. Section 6 illustrates our experimental results. The last section summarizes this paper and describes our future work. Fetcher, the Extractor, and the Ranker. The Fetcher focuses on fetching web pages from the World Wide Web. The URLs of the web pages come from top results provided by Google, and the query is simply the concatenation of all seeds (each seed is quoted, to require that it occur as an exact phrase). The Extractor then learns one or more wrappers for each page, and then executes the wrappers , to extract additional candidate entities (see Section 3). Finally, the Ranker builds a graph, and then ranks the extracted mentions globally based on the weights computed in the graph walk (see Section 4). automatically from only a fe w training examples (the seeds). In this section, we explain the semi-structured characteristics of web documents that SEAL requires, and describe an unsupervised approach for automatic construction of wrappers. The wrappers that are constructed are page-dependent  X  i.e., they are intended to be applied only to a single web page. However, the approach that we use to learn wrappers is both domain-and language-independent. will be formatted quite differently on different pages, but fairly consistently within a single page. For example, each movie name in a list of classic Disney movies might be be embedded with  X  &lt;tr&gt;&lt;td&gt; the left) and  X  &lt;/td&gt;&lt;/tr&gt;  X  (to the right) in one page, and  X  &lt;ul&gt;Disney:  X  and  X  &lt;/ul&gt;  X  in another. This observation suggests that entities belonging to the same class (i.e. movies) will be linked by appearing in similar contexts (formatti ng structures) on the same page. documents can be exploited for expanding some set of given seeds. Suppose initially, a couple of seeds are provided from a reliable source (i.e. a human), and web documents that contain all of these seeds are retrieved. Then it is very likely that each of these documents will contain other entities that are embedded in the same contexts as the seeds, and also belong to the same semantic class as the seeds. The next section explains in detail the algorithm for constructing wrappers utilizing the semi-structured characteristics of web documents. Relations seeds as the query, are downloaded from the web. All instances of seeds are identified from web documents D by simple string matc hing. For each document d  X  D , context l i,j  X  L be the part of d preceding s i,j right context r i,j  X  R be the part of d following s each d  X  D , all possible suffixes of some left context from L and all prefixes of some right context from R that embed at least one instance of every seed are (conceptually) extracted; th ese are referred to as full suffixes and full prefixes respectively. Within these full suffixes, ones that are suffixes of other full suffixes are removed, keeping only the longest full suffixes. To suffixes, and each node is marked with the number of unique seeds that the suffix precedes. For each of those longest suffixes, it is easy to find its corresponding longest right context (and vice versa). An extraction pattern, or wrapper, is then constructed for each of those pairs. When extracting candidate entities using left and right contexts L and R, we consider only substrings between L and R which do not contain both L and R . consistently within the same page but not across multiple pages, the wrappers derived from a particular document d are used to extract from d only. We will call each such extracted string a (candidate) entity mention. The complete pseudo-code for building these wrappers is described in Figure 2. based and does not assume any language or domain. Also, unlike prior approaches [4, 8], we do not impose any limit on the length of the contextual strings in L and R nor do we require any parser (i.e. HTML). This also implies that our algorithm will apply not only on HTML pages, but also on other documents semi-structured by any kind of mark-up language (i.e. XML, SGML, TeX, Wiki Markup Language, etc.). Definition: 1. s i,j = j th occurrence of i th seed, } , , { 2. 3. } , , { 4. } some of suffix a is : { ) , , ( FullSuffix
L l x i x L L  X   X  = K 5. = ) , , lSuffix( LongestFul 6. ) , , Prefix( l LongestFul Pseudo code:  X   X   X  X oyota X  were provided as seeds, wrappers can be automatically constructed for each document by using the proposed wrapper construction algorithm. Figure 3 shows an example source page (of  X  X urryauto.com X ) and Table 1 shows the contexts that the algorithm selected for constructing wrappers from  X  X urryauto.com X , with the symbol  X  X ...] X  representing the placeholder for an extracted entity. The entities extracted by the wrappers are also shown in Table 1. Here the boldfaced mentions are the seeds themselves. contain noisy entities, or entities that are rarely associated with the seeds by popular consensus on the web. For example,  X  X onda atlanta X  and  X  X onda yorktown X  extracted by Wrapper #3 in Table 1 are such entities; these are unlikely to be members of the user X  X  target set. Since it is extremely difficult for machines to perfectly understand the information needs of users, we choose to rank the extracted entity mentions in the set presented to the users. In this section, we first analyze the problem of finding similarity between seeds a nd extracted mentions. We then propose a graph walk for ranking extracted mentions. Wrapper #1: \n&lt;li class="[...]"&gt;&lt;a href="http://www. Wrapper #3: &lt;span class="dName"&gt;Curry [...]&lt;/span&gt; extracted mentions and seeds (or the likelihood that they all belong to the same class based on contextual information), we need to first understand how they are related globally. We know that seeds were used as a query to find documents online. We also know that the same wrapper may be derived from more than one document, and the same entity can be extracted by more than one wrapper. Also, we have observed that noisy entities are usually extracted less frequently than non-noisy entities. Intuitively, the more non-noisy entities extracted by a wrapper, the better quality the wrapper (and vise versa), and the more high-quality wrappers derived from a document, the better quality the document (and vise versa). use a graph which contains all the objects of interest  X  seeds, web pages, wrappers, and extracted entity mentions. Similarity in the graph will then be used to rank entity mentions. labeled directed edges. We will use letters such as x , y , and z to denote nodes, and we will denote an edge from x to y with labeled relation r as y x r  X  X   X  node x has a type and we will assume that there is a fixed set of possible node ty pes. We will also assume, for convenience, that there are no edges from a node to itself; however, this assumption can be easily relaxed. holds. The graph edges are di rected. We also create an will certainly be cyclic. The first column of Table 2 shows all possible source entity types, and the middle column shows each of thei r possible relations with some target entity types in the last column. lazy walk process , similar to PageRank with decay. To walk away from a source node x , one first picks an edge relation r ; then given r , one picks a target node y such that y x r  X  X   X  . We assume that, given a source node x , the probability of picking an edge relation r is uniformly distributed among the set of all r , where there exist a target node y such that y x r  X  X   X  specifically, We also assume that once an edge relation r is picked, a target node y is chosen uniformly from the set of all y such that y x r  X  X   X  . That is, At each step in a lazy gra ph walk, there is also some probability  X  of staying at x . Putting everything together, the probability of reaching any node z from x is computed recursively as follows: where I( x = z ) is a binary function that returns 1 if node x and node z are the same, and 0 otherwise. we sample the graph by taking 10000 walks randomly, 
Source Type Edge Relation Target Type document derive find -1 wrapper seeds 
Figure 4. Example of a constructed graph each walk consists of up to 10 steps starting from the node with type  X  X eeds X . At the end of the graph walk, each node will have a probab ility, or weight, assigned, and we rank all nodes of type  X  X ention X  by their assigned weights. 4. A graph walk has been performed on this graph where each node is assigned a weight due to the walk. As expected, a walk on the graph in Figure 4 would weigh  X  X mw pittsburgh X  and  X  X olvo chicago X  the least among the extracted mentions because these nodes have fewer incoming edges; thus they are harder to reach. The weights assigned to these mentions are shown on the example graph as well. constructed evenly over three languages: English, Chinese, and Japanese; thus there are 12 datasets per language. The datasets consist of 18 classes, where half were constructed in all three languages and the other half in one language onl y, as illustrated in Table 3. The intention is to diversify the datasets such that some are culture-specific while some are not. Each dataset is a plain text file that represents a particular class C , and each entity e  X  C is represented by a list of true mentions, or synonyms, of that particular e . The statistics of classes, entities, and entity mentions for each language are shown in Table 4. alternative methods we attempted to use, evaluation metric and procedure, experimental results, and finally, comparisons of our results with those published by other researchers. mainly because it is well-known and publicly available. However, since Google Sets does not handle languages other than English, it is only directly comparable to SEAL on the English evaluation dataset. To our knowledge there is no set expansion system that can handle Chinese and/or Japanese, with which we could compare our evaluation results. methods for set expansion. The extraction approach described in section 3.2 was simplified. In the definition of full suffix of the wrapper construction algorithm, instead of finding all possible common suffixes of left context L and prefixes of right context R that embed at least one instance of every seed , it finds common suffixes of L and prefixes of R that embed all seed instances . More specifically, we let: This simple extractor, referred to as E1, is compared to the proposed extractor, referred to as E2, in the experimental results section. also simplified: instead of a graph walk, it ranks entity mentions by their frequency counts of being extracted by any wrapper from any document. This simple ranking scheme is referred to as EF (extracted frequency) and the proposed method is referred to as GW (graph walk) in the experimental results section. extracted mentions, we choose mean average precision (MAP) as the evaluation metric. MAP has been commonly used for evaluating ranked lists in the field of Information Retrieval. It contains both recall and precision-oriented aspects, and it is sensitive to the entire ranking. For evaluati ng a system that produces multiple ranked lists, MAP is simply the mean value of average precisions computed for each ranked list separately. Average precision of a single ranked list is defined as: where L is a ranked list of extracted mentions, r is the rank, Prec( r ) is the precision at rank r . NewEntity( r ) is a binary function, which returns 1 if a) the extracted mention at r matches any true mention, and b) there exist no other extracted mention at rank less than r that is of the same entity as the one at r . It returns 0 otherwise. dataset: 1. Randomly select three true entities and use their 2. Expand the three seeds obtained from step 1. 3. Repeat steps 1 and 2 five times. 4. Compute MAP for the five resulting ranked lists. baseline Google Sets (G.Sets) performed the worst. Even our simplest approach, E1+EF, beats Google Sets by a substantial amount. In our first set of experiments, we requested only top 100 URLs per expansion from Google. Our simplest approach, E1+EF, achieved an overall average of around 82%. After improving E1 to E2, we observed a 6.34% improvement on the overall average. We then enhanced EF to GW, and observed a 6.30% improvement. We decided to increase our corpus size to see if any improvements can still be made. Rather than requesting only top 100 URLs, we increase the number to 200 and 300, and we observed a slight improvement of 0.97% and 0.16% respectively. expansion results published by other researchers and obtained by SEAL. Table 6 illustrates the top 42 set expansion results on watch brand names using 17 seeds as presented in Talukdar et al [8]. As comparison, we present our top 42 results in the right column using only the first three of thei r 17 seeds (namely Corum, Longines, and Lorus). SEAL achieved a precision of 100% on those results; whereas Talukdar X  X  system returned noisy entities (bol dfaced mentions) towards the bottom of their list and achieved a precision of 85.7%. We also tried the three seeds on Google Sets but obtained no results other than the seeds themselves. Table 7 shows top 10 set expansion results on children X  X  movies from Bayesian Sets, as presented in Ghahramani et al [9]. We provide results from Google Sets and SEAL as comparisons. Note that Bayesian Sets used a movie-specific dataset: EachMovie. As illustrated, both Bayesian Sets and SEAL systems perform well on finding children movies. Similar to Table 7, Table 8 shows top 10 results on NIPS authors from Bayesian Sets as presented in Ghahramani et al [9]. We provide results from SEAL as comparisons but not from Google Sets since it failed to return any result. Note that Bayesian Sets, again, used a domain-specific dataset: the NIPS dataset. been published. The KnowItAll system [4] contains a List Extractor (LE) component that is functionally similar to Google Sets. It uses an HTML parser for identifying sub-trees of a parsed web page. For each selected sub-tree, it finds one contextual pattern that maximally matches all of the seeds. However, SEAL does not require any parsing, and it finds all contextual patterns in the whole document that maximally match at least one instance of every seed. variants of the LE component, but it is not clear which variant was used in their experiment. The KnowItAll system achieved precisions of 23~79% on four sample problems. using free text rather than semi-structured Web documents; for instance, Talukdar et al [8] present a method for automatically selecting trigger words to mark the beginning of a pattern, which is then used for bootstrapping from free text. algorithm that solves a particular sub-problem of set expansion, in which candidate sets are given, rather than a corpus of web documents. We intend to compare their ranking method with graph-walks in future experiments. effective approach for expanding sets of named entities in an unsupervised, domain and language independent fashion. We have shown that our system, SEAL, performs better than Google Sets in terms of mean average precision for the dataset tested. We have also shown that our system is capable of handling various languages such as English, Chinese, and Japanese which Google Sets does not. are currently considering. First, we will investigate bootstrapping of named entitie s by performing several rounds of expansions, where each round uses the top extracted mentions from the previous round as seeds. Second, we will explore re -ranking of web documents based on their contained set of mentions extracted from previous round of expans ion. Third, we will look into automatic identification of possible class names for expanded sets. Lastly, we will study the possibility of hierarchical clustering on the expanded sets and graph learning for ranking candidate entities. Nyberg for proof-reading this paper. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view s of the Defense Advanced Research Projects Agency (DARPA). 
