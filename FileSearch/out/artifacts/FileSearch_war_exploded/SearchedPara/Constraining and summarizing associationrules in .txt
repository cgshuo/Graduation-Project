 REGULAR PAPER Carlos Ordonez  X  Norberto Ezquerra  X  Cesar A. Santana Abstract Association rules are a data mining technique used to discover frequent patterns in a data set. In this work, association rules are used in the medical do-main, where data sets are generally high dimensional and small. The chief dis-advantage about mining association rules in a high dimensional data set is the huge number of patterns that are discovered, most of which are irrelevant or re-dundant. Several constraints are proposed for filtering purposes, since our aim is to discover only significant association rules and accelerate the search process. A greedy algorithm is introduced to compute rule covers in order to summarize rules having the same consequent. The significance of association rules is evaluated us-ing three metrics: support, confidence and lift. Experiments focus on discovering association rules on a real data set to predict absence or existence of heart disease. Constraints are shown to significantly reduce the number of discovered rules and improve running time. Rule covers summarize a large number of rules by produc-ing a succinct set of rules with high-quality metrics.
 Keywords Association rules  X  Search constraint  X  Cover  X  Lift 1 Introduction Association rules [ 1 , 2 ] are a data mining technique used to discover frequent patterns in a large data set [ 27 ]. In particular, association rules are suitable for discovering predictive rules relating subsets of attributes from a data set with nu-meric and categorical attributes [ 38 ], since interesting patterns may exist in subsets of the attributes, but not on all attributes taken together. Our goal is to find signif-icant association rules with high predictive power. We have identified three main issues. First, in the case of a high dimensional data set, a significant fraction of association rules is not useful because they are irrelevant or redundant. Second, most relevant rules with high-quality metrics appear only at low frequency levels. Third, the number of discovered rules becomes extremely large at low frequen-cies. Therefore, we focus on defining constraints to reduce the number of rules and summarizing rules to solve these issues.
 work comes from a long-term project to apply data mining, image processing and artificial intelligence techniques, to enhance automated heart disease diagnosis [6, 12 X 14, 38, 39]. Association rules are proposed and preliminary results are reportedin[ 12 ]. Neural networks are used to predict reversibility (heart response) images, based on stress and myocardial (heart muscle) thickening images [ 6 ]. In [ 39 ] we explore the idea of constraining association rules in binary data for the first time and report preliminary findings from a data mining perspective. The knowledge base rules from an expert system used for heart disease diagnosis are validated using association rules [ 13 ]. Some theoretical results about constrained association rules are presented in [ 38 ]. 1.1 Contributions and article outline Association rules are constrained to reduce the number of discovered patterns and they are summarized to get a concise set of rules. The significance of as-sociation rules is evaluated using support, confidence and lift. The proposed con-straints include maximum association size, an attribute grouping constraint and an antecedent/consequent rule filtering constraint. Association rules are sum-marized using rule covers with a greedy algorithm. We show that the prob-lem of mining association rules in a high dimensional data set with numeric and categorical attributes is challenging, due to the large number of patterns rather than data set size. On the other hand a large number of patterns im-pacts the running time of the data mining algorithm, making result interpretation difficult.
 the definition of association rules, as well as the metrics, support, confidence and lift. Section 3 describes the main problems encountered using association rules, introduces filtering and search constraints and explains how to summarize asso-ciation rules with covers. Section 4 presents three sets of experiments with a real data set. The first set of experiments studies the impact of each constraint on the number of rules and performance, varying program parameters. The second set of experiments focuses on finding predictive rules with high confidence and high lift. The third set of experiments presents covers summarizing a large number of discovered rules. Section 5 discusses related research work. Section 6 presents conclusions and directions for future work.
 2 Definitions 2.1 Association rules Let D ={ T 1 , T 2 ,..., T n } beadatasetof n transactions and let I be a set of items, I ={ i Let X and Y be two itemsets s.t. X  X  I , Y  X  I and X  X  Y = X  . An association rule is an implication of the form X  X  Y ,where X is called the antecedent and Y is called the consequent of the rule. In the context of this work, a data record consists of numeric and categorical attribute values. A data record is transformed into a binary vector and then such vector is transformed into a transaction (itemset), where the presence of item i j indicates the j th coordinate is 1; when item i j is not present then the j th binary coordinate is zero. The itemset representation is used for efficiency purposes because transactions represent vectors with most coordinates equal to zero. 2.2 Support, confidence and lift Let P ( X ) be the probability of appearance of X in D and let P ( Y | X ) be the conditional probability of appearance of Y ,given X appears. For an itemset X , support ( X ) is defined as the fraction of transactions T i  X  D such that X  X  T i . Thus, P ( X ) can be estimated as P ( X ) = support ( X ) . The support of a rule X  X  Y is defined as support ( X  X  Y ) = P ( X  X  Y ) . An asso-ciation rule X  X  Y has a measure of reliability called confidence, defined as confidence ( X  X  Y ) = support ( X  X  Y )/ support ( X ) . Confidence can be used to estimate P ( Y | X ) : P ( Y | X ) = P ( X  X  Y )/ P ( X ) = confidence ( X  X  Y ) .View-ing support and confidence as probabilistic metrics is explained in more detail in [ 26 , 28 ]. The standard problem of mining association rules [ 1 ]istofindall rules whose metrics are equal to or greater than some specified minimum sup-port threshold  X  and minimum confidence threshold  X  .A k -itemset with sup-port above the minimum threshold is called frequent. We use a third significance lift ( X  X  Y ) = P ( Y | X )/ P ( Y ) = confidence ( X  X  Y )/ support ( Y ) . Lift mea-sures how much the presence of Y depends on the presence or absence of X ,or vice versa. Lift values greater than 1 indicate that the presence of Y depends on the presence of X . 3 Constraining and summarizing association rules We briefly analyze the problem of mapping information from categorical and nu-merical attribute values to binary format. We introduce constraints to find predic-tive association rules with preferred attribute combinations and to accelerate the rule discovery process. We explain how to summarize association rules having the same consequent using rule covers. 3.1 Mapping attributes A data set with numeric and categorical attributes must be transformed into trans-actions in order to apply association rules. Numeric attributes are binned into intervals and each interval is mapped to an item. Categorical attributes are trans-formed into items, mapping each categorical value to one item. If an attribute has negation there are additional items corresponding to each negated categorical value or corresponding to each negated numeric range. In short, each transaction is a set of items and each item corresponds to the presence or absence of one categorical value or one numeric interval.
 ing items. Let A 1 , A 2 ,..., A p be all the attributes to be analyzed. Let R = { r dom ( A 1 )  X  dom ( A 2 )  X  X  X  X  X  dom ( A p ) ,where dom ( A i ) is either a categorical or numerical domain. The data set size is n and its dimensionality is p . We transform R into a data set D ={ T 1 , T 2 ,..., T n } of n transactions containing subsets of m items, by means of the transformation process to be explained later. Items are identified by consecutive integers starting with one, i.e. 1 , 2 ,..., m . values into binary data, where each binary dimension corresponds to one item, as defined in Sect. 2 . Each binary dimension corresponds to one numerical range or one categorical value. The transformation process is divided into two phases. In the first phase, a transformation table M is constructed based on user X  X  require-ments. In the second phase, attribute values in each tuple r j are mapped to items based on M . Each tuple r j becomes a transaction T j , suitable for association rule mining. A transaction only includes the subscripts (items) of binary dimensions equal to 1 (for instance, in T 1 ={ 1 , 5 , 10 } , it is assumed items 2 and 8 are not present in T 1 ). For a categorical attribute A i each categorical value is mapped to one item. If negation is desired for a categorical attribute A i with more than two distinct values, then each negated value is mapped to an item. So if a categorical attribute A i has h i distinct values ( h i  X  3) the transformation process produces h i items without negation or 2 h i items with negation. The domain expert spec-ifies N i cutoff points corresponding to numerical attribute A i producing N i + 1 intervals. Then each interval is mapped to one item producing N i + 1 items. If negation is desired the number of items is doubled, producing in total 2 ( N i + 1 ) items. Negation on numerical attributes significantly increases the potential num-ber of associations. Therefore, it must be used on an individual attribute basis after careful consideration. Once table M has been constructed the second phase is straightforward. There are two important points about M . First, constraints are specified on attributes, but constraints are actually used on items. Second, it is not possible to apply the standard association rule [ 2 ] directly on a data set with alphanumeric records without a transformation into a binary data set. In general, R can be a high dimensional data set and the transformation process produces an even higher dimensional data set D . 3.2 Constraining and summarizing association rules Several problems come up when trying to discover association rules in a high dimensional data set. For each problem, we propose a solution that is in the form of a constraint, using a metric or summarizing patterns. Our primary goal is to discover significant rules for predictive purposes, but also making the data mining process faster.
 ize, are hard to interpret and can potentially generate a large number of redundant rules. Therefore, there should be a default threshold for association size. Most ap-proaches are exhaustive in the sense that they find all rules above the user-specified thresholds, but in general that produces a huge amount of rules as it will be shown in Sect. 4. The largest size of discovered associations is a practical bottleneck for algorithm performance. Another reason to limit size is that if there are two rules X because it is simpler and it is likely to have higher support. However, the second rule may be more interesting if it summarizes several rules similar to the first rule. This aspect will be explained later in more detail.
 an item can appear in the antecedent or in the consequent of a rule. But from a practical standpoint we would like items corresponding to predicted attributes to appear in the consequent and conditions, causes, independent attributes to appear in the antecedent. Given the rule X  X  Y , the association X  X  Y must be a fre-quent itemset because support ( X  X  Y ) defines rule support. But there are many rules derived from X  X  Y that are irrelevant. In other words, minimum support helps pruning the search space for significant associations, but minimum confi-dence is not enough to filter out irrelevant rules because there may be rules having high confidence containing forbidden items in the antecedent or in the consequent. Therefore, items, derived from attributes, need to be constrained to appear either on the antecedent or the consequent of each rule.
 the case where certain itemsets have such high support or represent well-known attribute X  X alue combinations, that they do not really say anything new. Consider ready known that i j and i j commonly appear together, then any other association X 2 s.t. X 1 than X 1 because in general it will have lower support and it will be more complex since it has more items. That is, simpler association rules with higher support are preferred. Therefore, many of the items (if not all) can be carefully grouped by the domain expert to discard uninteresting associations. If no grouping is done then item i j is always relevant, no matter which other items i j appear together with it. Small groups of uninteresting item combinations can be identified either automat-ically by running a straight association rules algorithm or by domain knowledge. This constraint has the purpose of reducing the number of redundant association rules which grow in a combinatorial manner as the minimum support threshold is lowered.
 performance bottleneck for association rules [ 37 ]. It is desirable to run the algo-rithm once with a very low support, thus avoiding repeated runs with decreasing supports. However, rules with very low support may be interesting for exploratory purposes, but they may not be reliable. In particular, rules involving only one transaction should be filtered out.
 consequent may not be necessarily linked to the appearance of the antecedent. Recall the definition of lift ( X  X  Y ) from Sect. 2 .If lift ( X  X  Y )  X  1then P ( X  X  Y )  X  P ( X ) P ( Y ) . Then this is evidence that X and Y are independent. On the other hand, if lift ( X  X  Y )  X   X &gt; 1, then P ( X  X  Y )  X   X  P ( X ) P ( Y ) . This is saying that the actual probability of X and Y appearing together is much higher that the product of their respective probabilities if they were independent. That is, X and Y are dependent on each other. Notice that lift ( X  X  Y ) is sym-metric (i.e. lift ( X  X  Y ) = lift ( Y  X  X ) ), but confidence ( X  X  Y ) is not. Lift can be used to identify rules where the dependence between X and Y is weak or strong and to further reduce the number of rules. Recall from Sect. 2 we are using a minimum confidence threshold  X  to filter rules. So if a rule X  X  Y has confidence ( X  X  Y )  X   X  then lift ( X  X  Y )  X   X / support ( Y )  X   X  because support ( Y )  X  1. Therefore, if  X   X  1then lift ( X  X  Y )  X  1. For instance, if we are using a minimum confidence threshold  X  = 0 . 8 then all rules will have lift metrics starting at 0.8, which is fairly close to 1. Such rules will not be considered interesting since they indicate X and Y are independent. By a similar reasoning, lift numbers close to zero or below  X  correspond to low confidence rules. These facts support the idea of introducing a minimum lift threshold above 1 to filter out rules with independent itemsets as well as low confidence rules. Lift is a well-known measure used for predictive purposes, but it is not sufficient by itself to evaluate the implication strength of a rule, because it is symmetric and it does not provide a probability. In this work, lift is used in combination with confidence to evaluate the significance of association rules. One of the goals for discovered asso-ciation rules is to enrich the knowledge base of an expert system [ 14 ]. Confidence is a measure that is directly used as the confidence factor (CF) of a prediction rule used by the expert system.
 even incorporating filtering constraints, interpretation becomes difficult. It is desir-able to get a few representative rules so that many rules can be derived from them. We propose to generate a cover of association rules with the same consequent (predicted disease) in order to generate a rule summary that can be interpreted by the domain expert in less time. Given two rules with the same consequent, r : A we can include r 2 and omit r 1 . When there are several rules covered by r 2 this will produce a concise summary. We will apply this approach to each set of rules with the same consequent, but different antecedent itemsets. 3.2.1 Search and filtering constraints Based on the difficulties outlined in the previous sections, we introduce the follow-ing improvements to the standard association rule search algorithm [ 2 ], which has two phases. Phase 1 finds all associations having minimum support, proceeding bottom X  X p, generating frequent 1-itemsets, 2-itemsets and so on, until there are no frequent itemsets. Phase 2 produces all rules having minimum support, minimum confidence and minimum lift. Two of our constraints work on Phase 1 and one works on Phase 2.
 ciations are generated up to size  X  , eliminating the search for associations of size  X  + 1,  X  + 2 and so on. This constraint is simple, but essential to get simple rules and reduce output size. Notice that each discovered rule will also have at most  X  items.
 Input attributes are extended with two constraints. Let I ={ i 1 , i 2 ,... i m } be the set of items to be mined, obtained by the transformation process from the attributes A ={ A corresponding to one item.
 each attribute A j as motivated earlier. Observe that constraints are specified on attributes and not on items. Each constraint c j can have one out of three values: 1 if item A j can only appear in the antecedent of a rule, 2 if it can only appear in the consequent and 0 if it can appear in either place. We define the function antecedent/consequent ac : A  X  C as ac ( A j ) = c j to make reference to one such constraint. Let X be a k -itemset; X is said to be antecedent-interesting if for all i j  X  X then ac ( attribute ( i j )) = 2; X is said to be consequent-interesting if for all i j  X  X then ac ( attribute ( i j )) = 1.
 A j ; g j is a positive integer if A j is constrained to belong to some group or 0 if A j is not group-constrained at all. We define the function g g . Since each attribute belongs to one group, then the group numbers induce a partition on the attributes. Note that if g j &gt; 0 then there should be two or more attributes with the same group value of g j . Otherwise that would be equivalent to having g j = 0. The itemset X is said to be group-interesting if for each item pair { i , i 3.3 Algorithm to compute rule covers The algorithm to compute a cover for a set of rules with the same consequent is as follows. Rule generation in Phase 2 is modified to generate sets of rules having the same consequent. We then apply the following greedy algorithm to quickly find a cover. For a set of rules R ={ r 1 : A 1  X  B , r 2 : A 2  X  B , ... , r we say r i covers r j .Let co v er ( r i ) be the number or rules covered by r i .Let therulecover R be initially empty. Then if rule r i has maximum co v er ( r i ) it is added to the rule cover and all its covered rules are marked and discarded. Ties are broken in favor of the rule with more items. The process is recursively repeated for the rest of uncovered rules. For completeness, a rule can cover itself if it has not been covered yet. Then by the aforementioned algorithm, the rule cover will have at most the original number of rules and it is guaranteed that every rule will be covered. This algorithm is fast, but possibly not optimal with respect to the number of rules selected, but for our purposes this is acceptable. The cover for a set of rules with the same consequent R represents a subset of rules, R  X  R , R ={ r 1 : A 1  X  B , r 2 : A 2  X  B ,..., r c : A c  X  B } , s.t. co v er ( r algorithm works in a post-processing fashion and not in Phase 1 or Phase 2 for sev-eral reasons. Each cover rule has minimum-quality metrics, but we are interested in particular rules that have high confidence and a cover rule cannot provide such information. As explained earlier, the antecedent/consequent constraint cannot be pushed into Phase 1. Cover rules tend to be too specific, with very low support levels.
 itemsets. A maximal frequent itemset is an itemset in which adding an item will make it fall below the minimum support threshold [ 23 , 34 ]. By the downward clo-sure property, all proper subsets of a maximal frequent set are frequent and not maximal. Cover rules may correspond to maximal frequent itemsets when there is no threshold on rule size and their consequent is not a subset of the consequent of other cover rules. Cover rules do not correspond to maximal frequent item-sets when their consequent is a subset of the consequent of longer rules. Covered rules do not correspond to maximal frequent itemsets because the itemset from the covering rule is a superset. In general, itemsets from cover rules are a subset of maximal frequent itemsets. Therefore, mining all maximal frequent itemsets would produce a larger set of rules than required. Even further, computing sup-port for their subsets would be required anyway to get all metrics and a filtering phase would be required to eliminate rules not complying with the group con-straints and minimum interestingness metrics. An itemset is closed [ 41 ]ifthere is no superset with the same support; closed itemsets represent a more restricted pattern than maximal frequent itemsets. But closed itemsets are a more concise representation than all frequent itemsets, but less concise and more precise than maximal frequent itemsets. In general, cover rules correspond to closed item-sets if there is no threshold on rule size; the exception happens when the cover rule itemset is a proper subset of the itemset from another cover rule and has the same support. Many covered rules may correspond to closed itemsets. Min-ing all closed itemsets instead of frequent itemsets would lead to a similar algo-rithm, but all rules derivable from subsets of closed itemsets need to be generated anyway. 3.4 Algorithm to search and summarize constrained association rules Based on the search constraints and rule filters introduced in Sect. 3.2 ,andrule covers introduced in Sect. 3.3 , we assemble all pieces together into one algorithm that goes from converting input records to computing rule covers. The transfor-mation process using the given cutoffs for numeric attributes and desired negated attributes, produces the input data set for Phase 1. Each data record becomes a transaction T i ; refer to definitions from Sect. 2 . After the data set is transformed, certain items are further filtered out depending on the prediction goal. Notice items can only be filtered after attributes are transformed because they depend on the numeric cutoffs and negation. This will be explained in more detail in Sect. 4 .In Phase1weusethe g () constraint to avoid searching for irrelevant itemsets. Phase 1 generates all associations up to size  X  . Phase 2 generates all rules filtering us-ing the ac () constraint. Phase 3 summarizes association rules using covers. The main input parameters are  X  , as well as the support threshold  X  , the confidence threshold  X  and minimum lift  X  . The major algorithm steps are as follows:  X  Preprocessing : Transform n records with numeric and categorical attributes into a transaction data set to get D ={ T 1 ,..., T n } , based on specified cutoffs and negation for numeric attributes. Filter items depending on predictive goal to generate 1-itemsets.  X  Phase 1 : Search for frequent itemsets from size 1 up to size  X  using  X  and the g () constraint.  X  Phase 2 : Generate rules in sets of rules having the same consequent, using the ac () constraint having confidence at least  X  and lift above  X  . Each set of rules is of the form: R ={ A 1  X  B , A 2  X  B ,..., A d  X  B } .  X  Phase 3 : Summarize results computing a set of cover rules R for each set of rules R having the same consequent. The output is a set of rules: R ={ r 1 :
A 1  X  B , r 2 : A 2  X  B ,..., r c : A c  X  B } such that co v er ( r i )  X  co v er ( r j ) for i &lt; j and A i  X  X  A 1 ,..., A d } ,where co v er ( r i ) is the number of covered rules by rule r i .
 constraints and items to be discarded. Phases 1 and 2 are automatic requiring the user to specify the minimum support, confidence and lift thresholds. Phase 3 is automatic and has no parameters.
 be found in [ 38 ]. We provide a summary of them. Group constraints produce an overall speedup in the frequent itemset generation because they reduce both num-ber of associations in Phase 1 and the number of rules in Phase 2. As it has been noted in the literature, in general, Phase 1 is the most time demanding. The algo-rithm cannot take advantage of ac ( i ) constraints in Phase 1, only in Phase 2. That is, an association involving items that appear either in the antecedent or the con-sequent must be generated anyway. This causes Phase 1 to be slow even though the number of generated rules is reduced. Maximum association size  X  , signifi-cantly improves performance, given the large number of subsets in a long itemset. Assume given the minimum support there is a frequent k -itemset X s.t.  X &lt; k . Then there are 2 k  X  k  X  2  X  pruned associations by the proposed algorithm. That is, the number of pruned associations is large when the X has many items. The size constraint eliminates the need of searching many subsets of X . 4 Experiments Our experiments were run on a Sun computer running at 800 MHz with 256 MB of main memory and 100 GB of disk space. The algorithm was implemented in the C++ language. Performance experiments were repeated five times and the average is reported. 4.1 Real data set description Our experiments were based on a real data set obtained from a hospital, shown in Ta b l e 1 . The data set contained the profiles of several patients being treated for heart disease. There were 655 patients having 113 attributes, combining numeric, categorical and image data. Medical records were collected during the course of several years. Records were created when the patient was admitted to the hospi-tal for treatment and were subsequently updated as the patient received medical treatment. We selected 25 medical attributes corresponding to all numeric and cat-egorical attributes listed in Table 1 . The remaining attributes that were not used in our experiments, involved image data for the patient during stress (exercise), rest (no exercise) and reversibility for the heart (referred to a condensed map of nine regions from the heart of a normal person), and attributes with highly skewed dis-tributions. For instance, race was not used because 82% of patients had race = W and 10% had missing race information. Only global cholesterol levels were avail-able, with most patients having no measurements on LDL (bad) or HDL (good) cholesterol levels. Image attributes were already summarized into the nine per-fusion measurements that were used in the experiments, which in the context of this work were the basic and most important criteria to evaluate the patient X  X  heart health. Therefore, in our experiments, p = 25 and n = 655. These attributes provide a complete picture about the patient based on perfusion measurements (obtained with a medical procedure to digitize the pattern of a colored substance given to the patient) for specific regions of the heart, known risk factors for heart disease (if available) and the degree of disease in four arteries (artery narrow-ing in percentage terms). For each attribute, we give its usual abbreviation in the medical domain, its data type, what type of medical information (MI) it contains and a complete description. Attributes are classified into three types according to the information they contain;  X  X  X  attributes correspond to perfusion measurements on specific regions of the heart;  X  X  X  attributes correspond to risk factors and  X  X  X  attributes correspond to heart disease (artery narrowing) measurements.
 surements. The image data represents the local degree of blood distribution (per-fusion) in the heart muscle (myocardium). Heart disease measurements include four numeric attributes that store the percentage of heart disease caused by a spe-cific artery of the heart ( LM , LAD , LCX ,and RCA ) and nine numeric attributes ( AL , IS , SA , AP , AS , SI , LI , IL , LA ) that store a perfusion measurement, which is a value in the range [ X  1 , 1 ] . Each of the four artery measurements has a value between 0 and 100% that represents the percentage of vessel narrowing. A per-fusion measurement closer to 1 indicates a more severe defect, whereas a value closer to  X  1 indicates absence of a perfusion defect. An abbreviation that is used in several column names is CAD, which stands for Coronary Artery Disease. Risk factors are self-explanatory. The right part of Table 1 contains the default values of constraints for association rule mining. Constraints are explained in Sect. 4.2 . 4.2 Default parameter settings In this section, we explain the default settings for program parameters that were based on the domain expert opinion. To understand the meaning of attribute names given later please refer to Table 1 . 4.2.1 Transformation parameters In order to set the transformation parameters we must discuss attributes corre-sponding to heart vessels. The LAD , RCA , LCX and LM numbers represent the percentage of vessel narrowing (or blockage) compared to a healthy artery. At-tributes LAD , LCX and RCA were partitioned by cutoff points at 50 and 70%. In the cardiology field, a 70% value or higher indicates significant coronary disease and a 50% value indicates borderline disease. A value lower than 50% means the patient is healthy. The most common cutoff value used by the cardiology com-munity to distinguish healthy from sick patients is 50%. The LM artery is treated different because it poses higher risk than the other three arteries. Attribute LM was partitioned at 30 and 50%. The reason behind these numbers is both the LAD and the LCX arteries branch from the LM artery and then a defect in LM is more likely to cause a larger diseased heart region. That is, narrowing (blockage) in the LM artery is likely to produce more disease than blockages on the other arteries. That is why its cutoff values are set 20% lower than the other vessels. The nine heart regions ( AL , IL , IS , AS , SI , SA , LI , LA , AP ) were partitioned into two ranges at a cutoff point of 0.2, meaning a perfusion measurement greater or equal than 0.2 indicated a severe defect. CHOL was partitioned with cutoff points 200 (warn-ing) and 250 (high). These values correspond to known medical settings. Missing values were assigned one item during the transformation process, but they were filtered out before rule generation. This is important, since we did not use any medical field that had missing information. Also, we did not substitute any missing value for some imputed value. In our data set, missing information was indicated with a question mark ( X ? X ). The question mark for each attribute was mapped to an item. Items in the mapped data set corresponding to missing information, were ignored during the association search process. That is, no associations containing items corresponding to missing information were generated. All discovered as-sociations referred to records without missing information in the corresponding attributes. Finally, only the four artery measurements had negation to find rules referring to healthy patients and sick patients. The rest of attributes did not re-quire negation. Since most risk factors were binary and perfusion measurements were divided into two ranges negation on them was eliminated. Negation was not considered useful for age and cholesterol level. 4.2.2 Search and filtering constraints The maximum association size  X  varied from 1 to 5 to study the impact of con-straints. In the second set of experiments, to get simple rules,  X  = 4. For the last set of experiments, computing rules covers, we increased  X  to 10. A lower  X  pro-duces fewer and simpler rules. A higher  X  produces many more rules that are also more complex, but which can be used to derive simpler rules using covers. threshold was fixed at  X  = 1%  X  7. That is, rules involving six or fewer patients were discarded. This eliminated rules that were statistically unreliable or were probably particular to our data set. From a medical point of view, rules with high confidence are desirable, but unfortunately, they are infrequent. Based on previ-ous experiments [ 38 , 39 ] and the domain expert opinion, minimum confidence was  X  = 70%. From the medical point of view, rules with 90% or higher confi-dence are preferable, but they are infrequent and generally do not involve all risk factors. Rules in the 80 X 90% range may be acceptable depending on which mea-surements, arteries and risk factors are involved. Rules in the 70 X 80% range have borderline reliability and they indicate potential patterns to be discovered in sub-sets of patients. Rules with confidence lower than 70% are not medically reliable; in particular, rules with confidence 50% or lower are never considered interesting. For the first set of experiments, we use a minimum lift threshold equal to zero to effectively use only support and confidence for filtering purposes. Then for the second set, we use a lift threshold slightly higher than 1 to filter out rules where X and Y are very likely to be independent. All rules with lift ( X  X  Y  X  1 turned out to have lift ( X  X  Y )  X  0 . 9. This is explained by  X  = 0 . 7 as explained in Sect. 3.2 . Finally, we use a high lift threshold equal to 2 to get rules where there is a strong dependence between X and Y , to explore with higher  X  and to reduce the number of rules. In short, for the set of experiments measuring number of patterns and time minimum lift was 0, to get simple predictive rules minimum lift was 1.2, and to get rule covers minimum lift was 2.0.
 lows. This set of constraints is by no means definitive or optimal, but it represents what our experience has shown to be most useful. Refer to Table 1 to understand the attribute meanings. The four main coronary arteries LM , LAD , LCX ,and RCA were constrained to appear in the consequent of the rule; that is, ac ( i ) =2.All the other attributes were constrained to appear in the antecedent, i.e. ac ( i ) = 1. In other words, R (risk factors) and P (perfusion measurements) appear in the antecedent, whereas D (disease) medical attributes, appear in the consequent of a rule. It is evident that from a medical point view, we are not interested in deter-mining the likelihood of presenting a risk factor based on artery disease. The nine regions of the heart ( AL , IS , SA , AP , AS , SI , LI , IL , LA ) were constrained to be in the same group (group 1). The group settings for the rest of attributes varied depending on the type of rules being mined. Recall that combinations of items in the same group are not considered interesting and are eliminated from further analysis. The nine heart regions were constrained to be on the same group, since we were interested in finding their interaction with risk factors and not among them. Group constraints were used for certain risk factors to decrease the num-ber of rules predicting absence of heart disease. Group constraints on risk factors were not necessary for rules predicting existence of heart disease. The default constraints are summarized in Table 1 . 4.3 Impact of constraints on the number of patterns We carried out experiments to evaluate the impact of constraints on the number of patterns found and on running time at the lowest possible support level. Min-imum support was set at  X  = 0 . 2% in order to capture all potential associations involving at least two patients, and the maximum rule size was  X  = 4. Minimum confidence was  X  = 0 . 7and  X  = 0 (for lift). This support level will show the out-put size of all potentially interesting association rules. However, later in the article we will focus on prediction rules involving at least 1% of patients. The reason behind such higher setting is that those rules will be more reliable from a medical point of view. The experiments show the change in the number or rules and elapsed time when negation, antecedent/consequent constraints and group constraints are turned on and off.
 ning time. Notice that our data set is small, but the problem of finding significant association rules is difficult given the large number of patterns. The first part of Ta b l e 2 , where there is no negation, shows that removing the ac () constraint pro-duces an order of magnitude increase in the number of rules when g () constraints are used. But when group constraints are not used, removing the ac () constraint increases the number of rules by two orders of magnitude. Times are affected accordingly. The last run took more than 3 hours to finish. The second part of Ta b l e 2 introduces negation for the arteries. The rest of numeric attributes are not negated. The number of rules increases an order of magnitude on average. The last run, where ac () and g () constraints are not used, could not be completed in less than 1 day. Only Phase 1 (association generation) was finished. The num-ber of rules was probably in the order of 1 million. We emphasize again that rules pruned by constraints are not medically interesting. The third part of Table 2 shows the combinatorial explosion on the number of rules as association size in-creases even with all constraints turned on. Some association rules involving five attributes may provide valuable information, but it would require significant ef-fort to analyze 60,000 rules. In short, these experiments prove that constraining is essential to get a manageable number of rules and to have acceptable running times. 4.4 Mining prediction rules The goal of the experiments was to relate perfusion measurements and risk fac-tors to vessel disease (also known as stenosis), to validate and improve actual diagnosis rules used by an expert system [ 19 ]. Some rules were expected, con-firming valid medical knowledge, and some rules were surprising, having the po-tential to enrich the expert system knowledge base. Discovered rules were clas-measurement or no risk factor, then there is no heart disease; second, those that express that if there exists a risk factor or a high perfusion measurement, then there is heart disease. The maximum association size  X  was4.Thatis,wemined rules involving at most four attributes; that way we get simpler rules with higher support.
 sensitivity and specificity. Sensitivity refers to the probability of correctly identi-fying sick patients. Specificity is the probability of correctly identifying healthy individuals. These measures rely on a standard that can be trusted by a medical doctor. That is, a measurement that tells with very high accuracy if the person is sick or not. Getting such ideal measurement may involve doing invasive (danger-ous) medical procedures on the patient. In the context of this paper, the medical standard was catheterization. Unfortunately, in a few cases, a clinician reading was taken, but in general it was not available. The general guideline we used was to increase sensitivity and decrease specificity. That is, we preferred to increase false positives at the risk of including people with borderline heart disease. We used lift, in addition to confidence, as a measure to balance sensitivity and specificity. So our experiments show a bias towards finding rules predicting disease with higher lift and rules predicting no disease with lower lift.
 imum lift in this case was  X  = 1 . 2. Support was used to discard low probability patterns. Confidence was used to look for reliable prediction rules. Lift was used to compare similar rules with the same consequent and to select rules with higher predictive value. Confidence, combined with lift, was used to evaluate the sig-nificance of each rule. Rules with confidence  X  90% were considered significant. Rules with lift  X  2 were considered significant. Rules with two or more items in the consequent were considered significant. Rules with high support, only risk factors, low lift or borderline confidence were considered interesting, but not significant. Rules with borderline cases were discarded, but are potentially interesting. Rules with artery figures in wide intervals (more than 70% of the attribute range) were not considered interesting like rules having a measurement in the 30 X 100 range for LM . 4.4.1 Rules predicting absence of heart disease The default program parameter settings are described in Sect. 4.2 . Refer to Table 1 to understand the meaning of abbreviations for attribute names. Perfusion mea-surements for the nine regions were in the same group (group 1). Rules relating no risk factors (equal to  X  X  X ) with healthy arteries were considered interesting. Risk factors HTA , DIAB , HYPLPD , FHCAD , CLAUDI were in the same group (group 2). Risk factors describing previous conditions for disease ( PA N G I O , PSTROKE , PCARSUR ) were in the same group (group 3). The rest of the risk factor attributes did not have any group constraints. Since we were after rules relating negative risk factors and low perfusion measurements to healthy arteries, several items were fil-tered out to reduce the number of patterns. The discarded items involved arteries perfusion measurements in [ 0 . 2 , 1 ] (no perfusion defect), and risk factors equal to  X  X  X  for the patient (person presenting risk factor). Minimum support was  X  = 1% and minimum confidence was  X  = 70%.
 over 1 min. Although most of these rules provided valuable knowledge, we only describe some of the most surprising ones. Figure 1 shows rules predicting no heart disease in groups. These rules have the potential to improve the expert sys-tem. The group with confidence = 1 shows some of the few rules that had 100% confidence. It was surprising that some rules referred to young patients, but not older patients. The rules involving LAD had high lift with localized perfusion de-fects. The rules with LM had low lift, confirming that other risk factors may imply a healthy artery. The group with two items shows the only rules predicting absence of disease in two arteries. They include combinations of all the arteries and have high lift. These rules highlight low cholesterol level, female gender and young patients. It turned out all of them refer to the same patients. The 90% confidence group shows fairly reliable rules. Unfortunately, their lift is not high. The group with only risk factors shows rules that do not involve any perfusion measurements. These rules highlight the importance of smoking habits, diabetes, low cholesterol, gender and age in having no heart disease. The last group describes rules with high support. Most of them involve the LCX artery, the IL region and some risk factors. These rules had low lift stressing the importance of many other factors to have healthy arteries. Summarizing, these experiments show LCX is more likely to be healthy given absence of risk factors and low perfusion measurements. Lower perfusion measurements appeared in heart regions IL and LI . Some risk factors have less importance because they appear less frequently in the rules. But age, sex, diabetes and cholesterol level appear frequently, a clear indication of their importance. 4.4.2 Rules predicting existence of heart disease The default program parameter settings are described in Sect. 4.2 . Refer to Table 1 to understand the meaning of abbreviations for attribute names. The four arteries ( LAD , LCX , RCA , LM ) had negation. Rules relating presence of risk factors (equal to  X  X  X ) with diseased arteries were considered interesting. There were no group constraints for any of the attributes, except for the nine regions of the heart (group 1). This allowed finding rules combining any risk factors with any perfusion de-fects. Since we were after rules relating risk factors and high perfusion measure-ments indicating heart defect to diseased arteries, several unneeded items were filtered out to reduce the number of mined patterns. The filtered items involved perfusion measurements in [ X  1 , 0 . 2 ) (no perfusion defect), and risk factors equal to  X  X  X  for the patient (person not presenting risk factor). In a similar manner to predicting absence of disease,  X  = 1% and  X  = 70%.
 1 min. Most of these rules were considered important and about one third were medically significant. Most rules refer to patients with localized perfusion defects in specific heart regions and particular risk factors with the LAD and RCA arteries. It was surprising there were no rules involving LM and only nine with LCX .To-mography or coronary catheterization are the most common ways to detect heart disease. Tomography corresponds to myocardial perfusion studies. Catheteriza-tion involves inserting a tube into the coronary artery and injecting a substance to measure which regions are not well irrigated. These rules characterize the pa-tient with coronary disease. There are three basic elements for analysis: perfusion defect, risk factors and coronary stenosis.
 betes, previous cardiac surgery and male sex constitute higher risk factors. The 100% confidence group shows some of the only 22 rules with 100% confidence. They show a clear relationship of perfusion defects in the IS , SA regions, certain risk factors and both the RC A and LAD arteries. The rules having RCA have very high lift pointing to specific relationships between this artery and cholesterol level and the IS region. It was interesting the rule with LAD  X  70 also had very high lift, but referred to different risk factors and region SA . The group of rules with two items in the consequent, shows the only rules involving two arteries. They show a clear link between LAD and RCA . It is interesting these rules only involve a previous surgery as a risk factor. These four rules are surprising and extremely valuable. This is confirmed by the fact that two of these rules had the highest lift among all discovered rules (above 4). The 90% confidence group shows some outstanding rules out of the 35 rules that had confidence 90 X 99%. All of these rules have very high lift with a narrow range for LAD and RCA . These rules show that older patients of male gender, high cholesterol levels and localized perfusion measurements, are likely to have disease on the LAD and RCA arteries. The group involving only risk factors in the antecedent shows several risk factors and dis-ease on three arteries. Unfortunately, their support is relatively low, but they are valuable as they confirm medical knowledge. The rule with lift = 2.2 confirms that gender and high cholesterol levels may lead to disease in the LCX artery. The group with support above 0.15 shows the rules with highest support. All of them involved the LAD artery and a variety of risk factors. Their lift was low-medium, confirming more factors are needed to have a more accurate prediction. There were no high-support rules involving LCX , RCA or LM arteries, confirming they have a lower probability of being diseased. 4.5 Summarizing association rules using rule covers The following experiments focus on summarizing results with rule covers and have two goals. The first goal is to discover a few representative rules from which simpler ones, like the ones discussed in Sect. 4.4 , can be derived. The second goal is to get a summarized representation of a large set of rules, that has a sig-nificantly lower number of rules, but which comprises all discovered rules. We applied the greedy algorithm introduced in Sect. 3 , along with parameter settings that will generate a significantly higher number of patterns than previous exper-iments. Minimum support was 0.01 and minimum confidence was 0.7, like in previous experiments. Minimum lift was 2.0 in order to include only rules with high predictive value and to produce a manageable number of rules. This setting allowed us to increase the maximum association size  X  because many rules with low lift were automatically filtered out. So the maximum association size  X  was 10 in order to generate a large number of subrules to be covered. All attributes used the antecedent/consequent constraint (Sect. 4.2 ) and the group constraints (Sect. 4.4 ).
 ters, but some items had to change in the pre-processing step from the algorithm from Sect. 3.4 , depending on the prediction goal. The first run involved rules pre-dicting absence of disease (disease = N). The second run was for rules predicting existence of disease (disease = Y). There is a rule cover computation for each different consequent. Results are summarized in Table 3 . It is clear that increasing  X  produces a big impact on running time compared to  X  = 4, used in previous experiments. There are more distinct consequents for rules predicting existence of disease, but fewer rules, compared to their counterpart. The data set contains many more patterns predicting absence of disease. The reduction in output size is significant for the covers of rules predicting absence of disease, where the number of rules decreases by 62%. The decrease for the covers of rules predicting disease is 52%. It is outstanding that rule covers produce a reduction in size because of the constraints we are using. Reduction would be higher if constraints were not used since patterns would be more redundant. Thus, rules predicting existence of heart disease are more complex and based on their covers, have less redundancy. The table indicates that the number of rules is the main factor impacting the total running time; the number of associations plays a smaller role.
 and predicting existence of disease, respectively. These rules have high lift, high confidence and a high number of rules covered. In general, we picked one of the first top rules in each cover. Notice that each cover rule also has minimum met-rics above the thresholds specified in Sect. 4.2 , since they are themselves rules produced by our algorithm.
 average than those predicting disease. In fact, there were many cover rules predict-ing absence of disease with confidence equal to 1. Most of them had lift between 2 and 3. We found rules predicting no disease for all arteries and their combinations with most of them having middle age, female gender and low cholesterol as fac-tors for no disease. It was interesting there were 2,508 rules with three items in the consequent. The highest coverage (i.e. c ( r i ) ) rules were found in rules predicting no disease. These numbers confirm such rules have higher redundancy.
 Most of them had lift between 2 and 3. Most rules with high coverage had confi-dence less than 1. The sets of cover rules with more items involved the LAD and the RC A artery. There were fewer cover rules with LC X in the consequent. There were no rules involving the LM artery. Most cover rules involved older age, high cholesterol, hyper-tension, diabetes, hyperlipoidemia and being male. In compar-ison to rules predicting no disease, there were very few cover rules having three items in the consequent: only three such rules were discovered. Given their high lift metric they were considered valuable, even though they only covered them-selves and had borderline confidence.
 dicting absence of disease because cover rules have high confidence and high lift, and also there is a higher reduction in output size. On the other hand, cover rules predicting existence of disease tend to have lower confidence figures. Therefore, some of the simpler (covered) rules predicting existence of disease, presented in Sect. 4.4 , are preferred because they have both high confidence and high lift. In short, rule covers were more useful to provide a summary for association rules predicting absence of disease. 5 Related work The following articles are related to using data mining with medical data. Apply-ing data mining to medical data creates unique challenges [ 47 ]. Some particular issues include fragmented data collection, stricter privacy concerns, rich attribute types (image, numeric, categorical, missing information), complex hierarchies be-hind attributes and an already rich and complex knowledge base. A well-known program to help heart disease diagnosis (called HDP) is described in [ 20 , 35 , 36 ]. This research work shows a computer program can improve differential diagno-sis made by medical doctors for cardiovascular disease. The program is based on Bayesian networks combined with artificial intelligence inference mechanisms [ 35 ]. Association rules have been used to help infection detection and monitoring [ 8 , 9 ], to understand what drugs are co-prescribed with antacids [ 11 ], to discover frequent patterns in gene data [ 5 , 15 ], to understand interaction between proteins [ 40 ] and to detect common risk factors in pediatric diseases [ 18 ]. Association rules have also been extended with fuzzy sets and alternative metrics to improve their application [ 17 ]. The problem of significantly reducing the number of association rules by using constraints was not studied before in the medical domain. To our knowledge, there is no previous work on using association rules to find prediction rules for artery disease, relating risk factors and perfusion measurements. mining Quantitative Association Rules (QAR) [ 49 ], but they have not been shown to be superior to decision trees. The I/O complexity of early association rule al-gorithms as well as some statistical support metrics are studied in [ 24 ]. Clustering association rules, rather than transactions, once they are mined, is analyzed in [ 33 ]. The output is a summary of association rules. Both [ 49 ]and[ 33 ]usedif-ferent approaches to automatically bin numeric attributes. In our project, it was preferred to use well-known medical cutoffs for binning numeric attributes, to improve result interpretation and validation. Mining association rules combining numeric and categorical attributes is studied in [ 46 ]; those rules are allowed to contain disjunctions, which are not useful for our purposes because they do not provide a conjunction of risk factors and measurements of sick patients; the opti-mized confidence approach is interesting, but it may not produce medically mean-ingful ranges for arteries. Incorporating constraints into the proposed optimized confidence problem is an issue for future research.
 plates to understand association rules in a post-processing phase is proposed in [ 29 ]. Two types of templates are introduced: inclusive and restrictive templates. Inclusive templates are similar to the ac constraint, but there is an important dif-ference: the ac constraint is specified on original attributes and then it is converted into constraints on items (mapped numeric and categorical attributes); this map-ping of constraints is dynamic given different numeric cutoffs and different se-lected attributes. Restrictive templates are similar to the item filtering performed on the preprocessing phase. Our group constraint has no equivalent with inclu-sive or restrictive templates. A further difference is that we incorporate the group constraint into Phase 1 to accelerate association generation and we show that the ac constraint can only be used as a filtering process during rule generation. An optimized algorithm that incorporates constraints into the mining process is pro-posed in [ 25 ], but it does not consider group constraints. Our association rule constraints are related to [ 32 , 37 , 50 ]. Constraining association rules is studied in depth in [ 37 ], where constraints are defined as item boolean expressions involving two variables. This approach was later improved to incorporate queries as predi-cates on itemsets [ 32 ]. In [ 50 ] there is a proposal of algorithms that can incorporate constraints to include or exclude certain items in the association generation phase; they focus only in two types of constraints: items constrained by a taxonomy [ 48 ] or associations which include certain items. We do not use hierarchies, and even though we perform filtering on the preprocessing phase, excluding/including items is not enough to filter medically relevant rules. However, taxonomies could be use-ful classifying attributes into types of risk factors and types of perfusion measure-ments. This approach is based on succinctness (simple item selection predicates) and two-variable constraints (boolean expressions); it is different from ours be-cause it does not deal with predictive rules, attribute transformation, rule size and only uses support as the fundamental interestingness metric. It is well known that simple constraints on support can be used for pruning the search space in Phase 1 [ 52 ]. Several association rule constraints are classified and studied in [ 42 ]. We dis-cuss how these constraints relate to ours. Item constraints are related to our group constraint, but item constraints do not produce a partition of attributes as group constraints do. The length constraint is similar to the rule size constraint, with the difference that we only consider rules with a maximum size. Model-based constraints are somewhat similar to the ac constraint, but they are specified on itemsets and not on raw attributes. Aggregate constraints are not applicable since we are dealing with a non-uniform set of items, produced by the pre-processing phase. Our item filtering can be considered a succinct constraint. Further work on constrained association rules is discussed in [ 26 ]. Association rules and prediction rules from a decision tree are contrasted in [ 21 ]. The lift measure was introduced in [ 7 ] and was called interest. This measure is now commonly called lift and can be used to evaluate association rules for predictive purposes [ 4 ].
 sets [ 22 , 41 , 43 ] and basis [ 3 , 10 , 44 , 51 ] represent alternatives to get condensed representations of association rules. Maximal frequent itemsets and closed sets represent itemsets, where adding an item makes them infrequent or makes them decrease their support. As explained in Sect. 3.3 , our rule covers are related to them. The general idea is to find frequent closed itemsets or sets of representative associations from which the rest can be easily generated. An inference rule to gen-erate a small set of non-redundant rules is introduced in [ 16 ]. The set of rules in a cover is minimized by allowing small errors in support [ 45 ]; in this manner more rules are covered by fewer closed itemsets. The representation discussed in [ 30 ] involves a set of rules derived from rules having a form with a disjunction in the consequent. Such rules are not good for predictive purposes because they provide a complex profile of the attributes appearing in the antecedent. Our approach is different in several aspects from previous work. We use covers as a way to sum-marize and classify sets of association rules with the same consequent, whereas several approaches generate covers of rules having different consequents. Covers help understanding many rules, but they are not a substitute for each covered rule because we are more interested in rules having higher confidence and as shown in the experiments, some cover rules have lower confidence than individual rules. Our cover rules can be used to derive all mined rules, but the cover may derive some rules that may not be interesting based on the group constraint. 6 Conclusions We studied the problem of constraining and summarizing association rules. We focused on using association rules for predicting combinations of several target attributes. The main reason behind using association rules for predictive purposes, instead of other data mining techniques, is that they are adequate to discover com-binatorial patterns that exist in subsets of the data set attributes. Association rules are filtered using support, confidence and lift. Lift helps selecting rules with high predictive power and is used in conjunction with confidence to evaluate the sig-nificance of discovered rules. Several computational problems related to associa-tion rule mining were presented to motivate the introduction of search and filter-ing constraints. Such constraints had three main purposes: finding only predictive rules, reducing the number of discovered rules down to a manageable size and im-proving running time. Thus, attributes are constrained to belong to groups, in order to avoid uninteresting combinations among them and to reduce the combinatorial explosion of patterns. They are also constrained to appear either in the antecedent or in the consequent to discover predictive rules. Associations are constrained to have a maximum number of attributes to produce fewer and simpler rules. In our case, constraints are not enough to produce a small set of rules. Therefore, we use covers to produce a summary. We introduce a greedy algorithm that computes a cover for each set of rules having the same consequent. Covers include rules whose antecedent is a superset of the antecedent of simpler rules. This results in a smaller set of representative rules. Experiments with a real data set, studied the proposed constraints and the summarization with rule covers. Constraints were shown to significantly reduce both the number of discovered rules and running time. We were able to show interesting association rules predicting absence or existence of disease in four specific heart arteries, with high confidence and high lift. We used rule covers to summarize rules with a higher number of attributes and higher lift. We showed some representative rules selected from the rule cov-ers, predicting absence or existence of disease as well, that have high-quality metrics. Our proposed framework can be applied in other domains where pre-diction based on subsets of attributes is required on combinations of several target attributes.
 straints and summarization with covers needs to be understood in more depth. The number of rules grows as support, confidence or lift are lowered. The data set may be post-processed with a clustering algorithm to explain why certain rules have low or high confidence. We intend to validate prediction rules with a different medical data set, but collecting such information is a difficult task given privacy regulations. Association rules and decision trees can be compared in the context of heart disease prediction with respect to their reliability, usefulness and simplic-ity. Finally, we need to study the impact on performance of each constraint from a theoretical point of view.
 References
