 Data stream processing is an emerging domain of applications where data are modeled not as persistent relations but rather as transient data streams. Exam-ples of such applications include network monitoring, financial applications, telecommunications data management, sensor networks, web applications, and so on. The data from these sources are often continuous, rapid, time-varying, pos-sibly unpredictable and unbounded in nature. Such applications cannot afford storing or revisiting the data and often require fast and real-time response. Month, Hour, Minute, Second), Geographic Locations (Continent, Country, State, City), IP addresses (192.*, 192.168.*, 192.168.1.*, 192.168.1.1). Analyzing such data at multiple aggregation levels simultaneously X  when data is arriving in a stream X  is much more challenging (and meaningful) than analyzing flat data. This is known as the Hierarchical Heavy Hitters (HHH) problem. Formally, we compute a  X  -HHH summary as follows: for a given threshold  X  , we report only those nodes (prefixes of source or destination IPs) as heavy hitters with fre-quency exceeding  X N , after removing the frequency of all its descendant nodes (descendant prefixes of source or destination IPs) that are also heavy hitters. In other words, a reported  X  -HHH element does not contain the frequency of any other descendant  X  -HHH element, but may contain the frequencies of non-HHH elements.
 monitoring team because it may reveal important patterns in the underlying data. For example, the network traffic may be composed of numerous peer-to-peer traffic connections (such as torrents or online gaming) that can be identified as one generalized traffic connection (composed of many lower level packets having a common pattern) in the HHH-summary.
 portional to at least the number of input elements [ 1 , 2 ], therefore the paradigm of approximation is adopted in a resource constrained environment such as data streams. Consequently, researchers have recently focused on efficient computa-tion (by minimizing memory requirement) of HHH with an acceptable preci-sion, where is a given error tolerance between 0 and 1. The algorithms for com-puting HHH are compared in terms of space usage and update cost that is often bounded using the error induced by the proposed solution in the estimation. 1.1 Our Contribution The contributions of this paper are as follows; we have proposed a Counter-based Hierarchical (CHS)  X  X pproximation algorithm that only requires memory bounded in terms of a user supplied parameter &lt; 1. We provide theoretical proof of accuracy, update cost, and memory requirement. The proposed aperi-odic (CHS-A) and periodic (CHS-P) compression strategies offer improved space complexities of O (  X  )and O (  X  log N ), respectively. Apart from the space com-plexities and theoretical update costs, experimental results demonstrate that the proposed algorithm requires fewer updates per element of data, and uses a moderate amount of bounded memory. Moreover, precision-recall analysis show that CHS algorithm provides a high quality (e.g., fewer false positives) output compared to existing benchmark techniques on widely compared datasets. Computing one-dimensional HHH using the Space Saving algorithm [ 3 ]ispro-posed in [ 4 ] using space O (  X  2 ) which is improved by [ 5 ] that require space O ( Similarly, another algorithm is proposed in [ 6 ] for computing two-dimensional HHH that requires space O (  X  3 / 2 ). Also, in [ 7 ] an algorithm is proposed for find-ing HHH which is well-suited to commodity hardware such as Ternary Content-Addressable Memories (TCAM). More recently, the technique proposed in [ 7 ] is extended in [ 8 ] by combining the information provided by the TCAM packet counters with an on-demand sampling mechanism. The sampling mechanism is implemented by a collector device, running on a separate machine. Also, resources allocation for TCAM counters using HHH detection is proposed in [ 9 ]. Recently, it has been shown in [ 10 ] that the massive graphs can be modeled in a form of hierarchy to extract bicliques using HHH technique. The approach used in [ 10 ], however, does not follow the streaming model (i.e., requires multiple passes over the graph) and is just an application of the HHH technique used in graph mining. Also, computing HHH has been used in detection of distributed denial of service attacks [ 11 ] and providing customized information to a user based on identifying trends in data [ 12 ].
 Finally, the propose CHS algorithm is similar to the work from Cormode et al X  X  [ 13 ] Full Ancestry (FA) algorithm and Partial Ancestry (PA) algorithm. The space required by their algorithms is O (  X  log( N )). Our proposed algorithms has improved on these bounds. Let S = { R 1 ,R 2 ,R 3  X  X  X } be a continuous stream of records, where each record is characterized by a hierarchical attribute. Let the number of levels or the height of the hierarchy of attribute is denoted by  X  , which is numbered 0 to  X  As an example, consider a record from network IP data with an IP address 1.2.3.4/32, where the notation x.x.x.x/b is often used to represent IP addresses, and b indicates the number of bits important to distinguish a given IP address. For example, if b = 32, then all the 4  X  8 = 32 bits are significant. If b = 24, then only the first 24 bits (the 24 MSB bits) are significant and the remaining 8-bits are disregarded.
 Definition 1. Hierarchical Heavy Hitters: Let  X  be the support and N is the current count of records processed so far, F H is the set of all HHH, F set of HHH at level l of the lattice where 0  X  l  X   X   X  1 . Let the symbol the descendant  X  ancestor relation, such that R  X  R a means that record R generalization of record R ,and ( R  X  R a )  X  ( R = R a )  X  F
H 0 the HHH at level 0 are simply the heavy hitters of S ,and  X  F
H l are HHH at level l of lattice, such that, node must contain the frequencies of all its descendant nodes except the nodes that are heavy hitters themselves. At each level it is sufficient to take care of its immediate descendants because the immediate descendants include the fre-quency of their immediate descendants and so on. This definition require space linearly proportional to the size of input for computing HHH with exact frequen-cies. In the data stream model with constraint on the resources, HHH detection problem is as follows; Definition 2.  X  approximate HHH detection problem: Given a data stream S , and user define parameters and  X   X  [ , 1] , then  X  approximate HHH identifi-cation problem is to output a set F H of prefixes and approximate bounds on the frequency of each prefix, such that the following conditions are satisfied; 1. Accuracy:  X  f ( R a )  X  f ( R a )+ N 2. Coverage:  X  R a /  X  F H  X  ( f ( R a ) &lt; X N ) level of the hierarchy. Therefore, the precision guarantee at an upper level is not N . However, by an appropriate rescaling of one can compute the frequencies of the ancestors accurately, nonetheless, with the price of higher space require-ments. It had been shown by Hershberger et al. [ 14 ] that this factor is unavoidable for computations of HHH, therefore our focus is on providing a methodology to compute the count of HHH from the data structure we maintain. In this Section, we provide details of  X  X pproximation CHS algorithm for com-puting HHH. 4.1 Counter-Based Hierarchical Summarization (CHS) Algorithm In this section, we explain counter-based algorithm for computing HHH. The cen-tral idea behind the CHS algorithm is to maintain  X  number of data structures; one at each level l of the generalization hierarchy of data. The CHS algorithm inserts records from stream into a data structure (i.e., DataStructures[ ] distinguished by an array index l =0 , 1 ,  X  X  X   X   X  1) at the lowest level of the hierarchy. To keep the space required by the data structure at each level bounded, the algorithm uses a compression strategy, which can either be periodic (every t secs or n records) or after every incoming record R i . For the periodic compression strategy we use Lossy Counting (LC) algorithm [ 15 ], and for the aperiodic compres-sion strategy we use Space Saving (SS) algorithm [ 3 ]. The deleted records during compression from lower level data structure are then generalized and inserted at the higher level data structure. When a user query is issued, the algorithm generates the output by starting from the lowest level of the hierarchy and progressing towards Algorithm 1. CHS: An Algorithm for Computing HHH the top. At each level, the output contains all the records whose count exceeds  X N , after discounting the count of their descendant HHH.
 The CHS algorithm (see Algorithm 1 ) has two simple operations, the first one is P rocessStream () which processes each incoming record from stream S , and the second one is a compression operation ( CompressP () periodic com-pression or CompressA () aperiodic compression). First, each incoming record R from stream S is compared with D [0]; if R i exist in D [0], its count is incre-mented. Otherwise, R i is passed to one of the compress sub routines, where it is processed for insertion into the data structure D [ l ]. Each entry in representation of R i , f D [ l ] ( R i ) is the true frequency of the record R the error relative to insertion of R i . The estimated frequency for a given record CompressP(): With this option, first, CHS-P inserts 1 records into the lowest level of D [ l ], and then perform a compression. During the compression infrequent records are deleted from each level, generalized and inserted into the next higher level of D [ l ]. CHS-P first computes cw = rc w (where rc is the number of records counted thus far, and w =  X  1 ), and then inserts each incoming Algorithm 2. CHS (Continue) record into D [0]. Second, after every 1 new insertions into D [ l ] sequentially from l =0to l =  X   X  1. At any level l of to the next level and inserts it into D [ l + 1]. This step, which delete any record whose count is less than cw , is repeated for all the levels of compression steps keep the data structures D [ l ] at each level l under O ( CompressA(): With this option, CHS-A inserts each incoming record into low-est level of D [ l ], and then if this insertion causes a record to be deleted from lower level of D [ l ], it is generalized and inserted into next higher level of CHS-A inserts incoming record R i to a container T (that contains only one record and its frequency), and iterate through D [ l ] sequentially from l =0to l =  X   X  1, such that T is not empty. CHS-A removes record R checks if R x exists in D [ l ], if it exist CHS-A just updates its frequency using f D [ l ] ( R x )+ = f T ( R x ) and returns. Second, if the record does not exist in CHS-A checks if the number of records in D [ l ] (denoted by sizeOf) is less than , and then inserts the R x into D [ l ] with its frequency f wise, CHS-A finds a record R s with smallest count in D [ l ], replaces it with R and sets the error of  X  R x equal to f ( R s ). Finally, CHS-A generalizes the deleted record R s to its upper level, and inserts it into T with frequency f for processing in the next iteration that follows the same steps explained above. Thus, just like CompressP (), the compression strategy CompressA () keeps the data structures D [ l ] at each level under O ( 1 ).
 Finally, at any time to output HHH for a given threshold  X  , the CHS algo-rithm scans D [ l ] sequentially from l =0to l =  X   X  1. The algorithm outputs the record if  X  f D [ l ] ( R i )  X   X N . Otherwise, if  X  f is generalized one level up and inserted into the data structure way it extracts all the HHH of the stream at any level of the hierarchy whose frequencies have exceeded the threshold  X  .
 Next, we provide some theoretical observations of CHS algorithm including proof of accuracy and coverage properties of the algorithm.
 Theorem 1. The CHS algorithm satisfies the accuracy and coverage properties from Definition 2 .
 Proof. Accuracy: From the theoretical proofs of the compression strategies (LC [ 15 ], SS [ 3 ]), we are guaranteed to find records from error N and requires memory O (  X  log N )or O (  X  ) (for CHS-P and CHS-A, see Lemma 2 ).
 Coverage: Next we prove the coverage requirements satisfied by CHS. The algorithm scans D [ l ] sequentially from l =0to l =  X   X  1, and for each l it performs either of the following two operations, (1) if  X  f D [ l ] outputs this record, or (2) if  X  f D ( R i ) &lt; X N : R i record to its upper level D [ l + 1]. Since operation 1 and 2 are mutually exclusive, this follows that the records generalized by operation 2 do not contain the count from records output (i.e., HHH) by operation 1, i.e., the frequency of HHH is computed as follows: Which completes the proof of the coverage requirements of the CHS algorithm. Lemma 1. The CHS algorithm performs O (  X  ) updates per packet in the worst case.
 Proof. The number of updates in D [ l ] depends on the number of compressed records in the previous level data structure D [ l  X  1]. Consider that a record comes into the lowest level of the data structure (i.e., l = 0) and gets deleted from here (e.g., because it is not frequent with respect to the current window). This generalized record requires another insertion at the upper level of the data structure (i.e., l = 1). In the worst case a record may get deleted at every level of D [ l ], except the root, so there can be a maximum of l updates. Hence, the worst case update cost by a single record can be O (  X  ). Lemma 2. The CHS-P algorithm uses O (  X  log N ) memory, while CHS-A uses O (  X  ) memory.
 Proof. The proof of Lemma 2 is simple; since  X  is the total number of levels in
D [ l ], and each level requires O ( 1 log N )or O ( 1 ) space depending on CSH-P and CHS-A. Therefore, the space required by CHS-P is O (  X  space required by CHS-A is O (  X  ). We have compared the CHS-P and CHS-A algorithms with existing FA (termed Full ) and PA (termed Partial ) algorithms for a range of parameters (i.e., ,  X  , and varying stream length N ). We have implemented our proposed algorithms using the Java programming language, and compared them with the open source version of PA [ 13 ] and FA [ 13 ] algorithms 1 . The data structures used are based on hashing techniques, which require one hashing function to lookup a particular record in the data structure.
 Datasets: In our experiments, we have used real Internet traffic datasets [ 16 ] that are openly available from the WAND Network Research Group from the University of Waikato, New Zealand. Each of these datasets contain 30 min trace of network traffic in tcpdump 2 format. We used Wireshark format data, and to extract the source IPs from each network traffic traces. Evaluation Criteria: To measure the efficiency and effectiveness proposed algorithms, we have considered different factors including space usage, update efficiency, and quality of the approximate output compared to the exact answer. The space usage is compared using the maximum number of tuples main-tained by the algorithms in their respective data structures. The update efficiency is compared using the total number of updates performed during insertion and compression by each algorithm after processing a stream of around 8 Millions records. The quality of output is evaluated using a number of measures, for exam-ple, we have used Precision, Recall and Dice Coefficient which is the harmonic mean of precision and recall, to evaluate our algorithms. Moreover, we have used hierarchical measures such as Optimistic Genealogy Measure (OGM) [ 17 ]to compare the output quality of the algorithms. 5.1 Update Efficiency Table 1 compares the number of updates from all four algorithms for =0 . 01 and =0 . 001. It can be seen that for higher values of (e.g., =0 . 01) all the algorithms performs more updates than for lower values of (e.g., =0 . 001). The reason is that, for lower values of , the algorithms have higher space (because space is an inverse function of ), and therefore they delete less num-ber of records compared to higher values of . The update efficiency of both CHS-P and CHS-A algorithms are better than the update efficiency of both Full and Partial . Particularly, CHS-A is the best algorithm in terms of update cost as it requires the lowest number of updates. The update cost of CHS-P is little higher than CHS-A but it is still better than both Full and Partial . The Full algorithm requires the highest number of updates of all the four algo-rithms due to the fact that it has to perform O (  X  ) updates for each incoming record, while the remaining three algorithms perform only an O (1) operation for each incoming record. Both Full ,and Partial also perform updates during the periodic compression operation. CHS-P and CHS-A algorithms perform an O (1) update for each deleted record into the upper level. We observe from Table 1 that, Partial roughly performs 1.5 times more updates than CHS-P and CHS-A algorithms, and Full roughly perform 5 times more updates than the CHS-P and CHS-A algorithms. This demonstrates that the CHS algorithms are much more efficient than Full and Partial in terms of update performance. 5.2 Output Quality One of the most important issues with approximate algorithms, such as in the case of HHH, is that these techniques are not guaranteed to find the exact set of HHH (hence they are -approximate). It is expected that the output may miss some HHH and potentially might include some extra (i.e., non-HHH). Thus to evaluate the proposed algorithms, we have compared them using both flat (e.g., Recall, Precision and Dice coefficient) and hierarchical (e.g., OGM) measures using the exact answer as a frame of reference. All the four measures were plotted against the stream length, in order to observe the performance of the algorithms for varying values of N . We have compared our algorithms for a range of and  X  values for different measures, but due to space limitation, we have reported results for = 0.01, and  X  = 0.02 which reflects the overall behavior across the broad range of these parameters.
 show that in terms of not missing true HHH records, both CHS-P and CHS-A perform better than the Full and Partial . Figure 1 b compares the precision scores of various algorithms. It can be seen that, both Full and Partial have higher precision than CHS-P and CHS-A algorithms. The high precision of Full and Partial suggest that they almost do not output any non-HHH record, however, their very low recall values shows that they miss a lot of true HHH records. On the other hand, on average both CHS-P and CHS-A may not miss any true HHH records but might output about less than 5 % additional non-HHH records. Moreover, we used other measures (described next) to show the tradeoff between precision and recall.
 rithms against the Full and Partial algorithms in terms of Dice and OGM scores. It can be seen that both CHS-P and CHS-A algorithms outperform both the Full and Partial for the two measures in majority of the cases. In general, the Dice scores of all the algorithms are lower than their corresponding OGM scores. This is because the Dice score shows the exact matching of the actual answer and approximate answer, whereas the OGM measure, in addition to exact matching also take into account the ancestor-descendant relations between an approximate answer and the exact answer. In the case of approximate HHH solu-tions, if an algorithm does not output an exact HHH, it is highly likely that the algorithm will output its ancestor HHH, because the deleted descendant HHH are generalized and combined at ancestor HHH. Therefore, overall the OGM score of the algorithms are higher than their corresponding Dice score. In general, our proposed algorithms outperform Partial and is consistently better than Full for a range of experiments. Thus, we emphasize that even in terms of trade off between precision and recall our proposed algorithms are consistently better than both the Full and Partial algorithms. 5.3 Space Usage Table 1 provides the comparison of space usage for Full , Partial , CHS-P and CHS-A algorithms for =0 . 01 and =0 . 001. Notice that the memory usage of the algorithms is independent of  X  , i.e., it only depends on , hence Table 1 does not specify any  X  values. It can be seen that the memory usage of the Full algorithm is highest as it inserts and stores every ancestor of each incoming record. Notice that, the memory usage of CHS-P is almost identical to Partial , and the memory usage of CHS-A is identical to Full . The memory usage of CHS-P and CHS-A is higher than the memory usage of Partial , however, this higher memory usage is a tradeoff for better quality results compared to Partial and Full . Partial uses the lowest memory, but its better memory usage comes at the cost of missing a larger fraction of true HHH records and significantly decreased output quality. We emphasize that the space usage of the CHS algorithm is not a major disadvantage in the situations where high quality output is required as it consumes just a little more memory than Partial but produces significantly better results.
 We conclude that, Full uses largest memory and has highest update cost but it is better than Partial in terms of output quality. The Partial has low output quality but it is better than Full in terms of memory and update performance. On the other hand, CHS is better than both Full and Partial in terms of update cost and output quality, and uses moderate amount of memory like Partial . This paper presented efficient techniques to compute hierarchical heavy hitters from data streams. The theoretical analysis proved that the worst case update cost and space requirements for the CHS algorithm are better than the existing algorithms. The experimental results demonstrated that the output produced by the CHS algorithm for one dimensional data is identical to the exact answer sizes in majority of the cases. The Precision-Recall analysis also suggested that the output produced by the CHS algorithm is of high quality and very close to the exact answers. Finally, accuracy and coverage of the algorithms are theoretically proven to have a bound which can be controlled using user supplied parameter . This provides the flexibility to make trade-off between the space used and the precision required in the estimation.

