 For many multi-dimensional data applications, tensor oper-ations as well as relational operations need to be supported throughout the data lifecycle. Although tensor decomposi-tion is shown to be effective for multi-dimensional data anal-ysis, the cost of tensor decomposition is often very high. We propose a novel decomposition-by-normalization scheme that first normalizes the given relation into smaller ten-sors based on the functional dependencies of the rela-tion and then performs the decomposition using these smaller tensors. The decomposition and recombination steps of the decomposition-by-normalization scheme fit nat-urally in settings with multiple cores. This leads to a highly efficient, effective, and parallelized decomposition-by-normalization algorithm for both dense and sparse ten-sors. Experiments confirm the efficiency and effectiveness of the proposed decomposition-by-normalization scheme compared to the conventional nonnegative CP decomposi-tion approach.
 H.2.4 [ Database Management ]: Systems X  Query pro-cessing, Relational databases ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clus-tering Algorithms, Experimentation Tensor Decomposition, Tensor-based Relational Data Model  X 
This work is supported by the NSF Grant #1043583 - X  X iNC: NSDL Middleware for Network-and Context-aware Recommendations X  and the NSF Grant #1116394  X  X an-Kloud: Data Partitioning and Resource Allocation Strate-gies for Scalable Multimedia and Social Media Analysis X 
For many multi-dimensional data applications, tensor op-erations as well as relational operations need to be supported throughout the data lifecycle (from collection to analysis). Tensors are high-dimensional arrays. Due to the convenience they provide when representing relationships among differ-ent types of entities, tensor representation has been increas-ingly used for relational data in various fields from scientific data management to social network data analysis. Tensor based representations have proven to be useful for multi-aspect data analysis and tensor decomposition has been an important tool to capture high-order structures in multi-dimensional data [2, 11, 12, 17, 20, 23].

Although tensor decomposition is shown to be effective for multi-dimensional data analysis, the cost of tensor decom-position is often very high, especially in dense tensor repre-sentations where the cost increases exponentially with the number of modes of the tensor. While decomposition cost in-creases more slowly (linearly with the number of nonzero en-tries in the tensor) for sparse tensors, the operation can still be very expensive for large data sets. In fact, in many data intensive systems, data is commonly high-dimensional and large-scale and, consequently, of all the operations that need to be supported to manage the data, tensor decompositions tend to be the costliest ones. Recent attempts to parellelize tensor decomposition [2, 17, 23] face difficulties, including large synchronization and data exchange overheads.
Our goal is to tackle the high computational cost of tensor decomposition process. Since, especially for dense data sets, the number of modes of the tensor data is one of the main factors contributing to the cost of tensor operations, we focus on how we reduce the dimensionality and the size of the input tensor. In particular, we argue that if then the resulting partial decompositions can be efficiently combined to obtain the decomposition of the original data set (tensor). We refer to this as the decomposition-by-normalization (DBN) scheme. (b) decomposition-by-normalization (DBN) process Example 1.1. Consider the 5-attribute relation, R (workclass, education, ID, occupation, income) , in Figure 1(a) and assume that we want to decompose the corresponding tensor for multi-dimensional analysis.
Since high-dimensional tensor decomposition is expensive, we argue that a better processing scheme would involve first normalizing this relation into the smaller tensors (based on the functional dependencies of the relation) and then per-forming the decompositions of these smaller tensors. These normalized tensors are lower-dimensional than the original input tensor, therefore these decompositions are likely to be much less expensive than the decomposition of the origi-nal input tensor. Figure 1(a) illustrates an example nor-malization which divides the 5-attribute relations into two smaller relations with 3 attributes, R 1 (workclass, educa-tion, ID) and R 2 (ID, occupation, income) respectively.
Figure 1(b) illustrates the proposed DBN scheme: Once the two partitions are decomposed, we combine the resulting core and factor matrices to obtain the decomposition of the original tensor corresponding to the relation R .

In the above example, if the relation R is dense , we expect that decompositions of relations R 1 and R 2 will be much faster than that of the relation R and the gain will more than compensate for the normalization cost of the initial step and the combination cost of the last step of DBN.
If the relation R is sparse , on the other hand, the de-composition cost is not only determined by the number of modes, but also the number of nonzero entries in the tensor. As a result, unless the partitioning provides smaller number of tuples for both relations, DBN may not provide suffi-cient savings to compensate the normalization and combi-nation overheads. However, as we will experimentally verify in Section 6, the decomposition and recombination steps of the DBN scheme fit naturally in multiple cores. This leads to a highly efficient, effective, and parallelized DBN strategy under both dense and sparse tensors.

In general, a relation can be vertically partitioned into two in many ways: we need to first select a join attribute and then, we need to decide which attributes to include in which vertical partition. The join attribute ( ID in the above example) around which we partition the data is important for two different reasons. First of all, we need to ensure that the join attribute is selected in such a way that the normal-ization (i.e., the vertical partitioning) process does not lead to spurious tuples. Secondly, the join attribute need to par-tition the data in such a way that the later step in which de-compositions of the individual partitions are combined into an overall decomposition does not introduce errors.
Task 1.1. One way to prevent the normalization pro-cess from introducing spurious data is to select an attribute which functionally determines the attributes that will be moved to the second partition. This requires an efficient method to determine functional dependencies in the data. This is difficult because the total number of functional de-pendencies in the data can be exponential.

Task 1.2. A second difficulty is that many data sets may not have perfect functional dependencies to leverage for normalization. In that case, we need to identify and rely on approximate functional dependencies in the data . In this paper, we argue that we can rely on pairwise approximate functional dependencies that are incomplete, yet very effi-cient to compute.

Task 1.3. Once the approximate functional dependencies are identified, we need to partition the data into two parti-tions in such a way that will lead to least amount of errors during later stages. In this paper, we argue that partition-ing the attributes in a way that minimizes inter-partition functional dependencies and maximizes intra-partition de-pendencies will lead to least amount of errors in recombina-tion step. After data is vertically partitioned and individual partitions are decomposed, the individual decompositions need to be recombined to obtain the decomposition of the original relation. This process needs to be done in a way that is efficient and parallelizable.
The organization of the paper is as follows: We conclude the paper in Section 7. Tensor Decomposition. The two most popular ten-sor decompositions are the Tucker [21] and the CANDE-COMP/PARAFAC [7, 4] decompositions. The Tucker de-composition generalizes singular value matrix decomposition (SVD) to higher-dimensional matrices. CANDECOMP [4] and PARAFAC [7] decompositions (together known as the CP decomposition) take a different approach and decompose the input tensor into a sum of component rank-one tensors. More specifically, the CP Decomposition of P is a weighted sum of rank-one tensors as the following (an alternative way to view the decomposition is in the form of a diagonal tensor and a set of factor matrices of the input tensor): where  X  is a vector of size r and each U ( n ) is a matrix of size I n  X  r , for n = 1 ,  X  X  X  ,N .

Many of the algorithms for decomposing tensors are based on an iterative process that approximates the best solution until a convergence condition is reached. The alternating least squares (ALS) method is relatively old and has been successfully applied to the problem of tensor decomposi-tion [4, 7]. ALS estimates, at each iteration, one factor matrix, maintaining other matrices fixed; this process is re-peated for each factor matrix associated to the dimensions of the input tensor.
 Nonnegative Tensor Decomposition (NTF). Note that these decompositions can be interpreted probabilistically, if additional constraints (nonnegativity and summation to 1) are imposed. In the case of the CP decomposition, for ex-ample, each nonzero element in the core and the values of entries of the factor matrices can be thought of as a cluster and the conditional probabilities of the entries given clusters, respectively. The N-way Toolbox for MATLAB [1] provides both CP and Tucker decomposition with nonnegativity con-straints.
 Scalable Tensor Decomposition. Tensor decomposition is a costly process. In dense tensor representation where the cost increases exponentially with the number of modes of the tensor. While decomposition cost increases more slowly (linearly with the number of nonzero entries in the tensor) for sparse tensors, the operation can still be very expen-sive for large data sets due to the high computational cost and memory requirement to build up the approximate ten-sor [17]. A modified ALS algorithm proposed in [17] com-putes Hadamard products instead of Khatri-Rao products for efficient PARAFAC for large-scale tensors. Kolda et al. [11] developed a greedy PARAFAC algorithm for large-scale, sparse tensors in MATLAB.

The ALS in Tucker decompositions involves SVD which is the most computationally challenging part [20]. To address this, randomized sampling is used in [20]. MET decomposi-tion proposed in [12] addresses intermediate blowup problem in Tucker decomposition.
 Parallel Tensor Decomposition. Phan and Cichocki [17] proposed a modified ALS PARAFAC algorithm called grid PARAFAC for large scale tensor data. The grid PARAFAC divides a large tensor into sub-tensors that can be factor-ized using any available PARAFAC algorithm in a parallel manner and iteratively combined into the final decomposi-tion. The grid PARAFAC can be converted to grid NTF by enforcing nonnegativity.

Zhang et al. [23] parallelized NTF for mining global cli-mate data in such a way that the original 3-mode tensor is divided into three semi-NMF sub-problems based on ALS approach and these matrices are distributed to independent processors to facilitate parallelization. Antikainen et al. [2] presented an algorithm for NTF that is specialized for Com-pute Uniform Device Architecture (CUDA) framework that provides a parallel running environment.

Note that since these block-based parallel algorithms are based on an ALS approach where one variable can be op-timized given that the other variables are fixed, the com-munication cost among each block is not avoidable. In the parallelized DBN strategy, on the other hand, each block is completely separable and run independently.
 Functional Dependency (FD). A functional dependency (FD) is a constraint between two sets of attributes X and Y in a relation denoted by X  X  Y , which specifies that the values of the X component of a tuple uniquely determine the values of the Y component.

The discovery of FDs in a data set is a challenging problem since the complexity increases exponentially in the number of attributes [15]. Many algorithms for FD and approximate FD discovery exist [8, 13, 15, 22]. TANE proposed in [8] used the definition of approximate FDs based on the minimum fraction of tuples that should be removed from the relation to hold the exact FDs.

The computation of FDs in TANE and Dep-Miner [13] is based on levelwise search [16]. Dep-Miner finds the minimal FD cover of a hypergraph using a levelwise search. Simi-larly, FastFD [22] finds the minimal cover, however, differ-ently from Dep-Miner, it uses a depth-first search to address the problem in the levelwise search that the cost increases exponentially in the number of attributes. The main factor in the cost of FastFD is the input size. FastFD works well when the number of attributes is large. TANE takes linear time with respect to the size of the input whereas FastFD takes more than linear time of the input size.

CORDS [9] generalized FDs to determine statistical de-pendencies, which is referred to as soft FD. In a soft FD, a value of an attribute determines a value of another attribute with high probability. CORDS only discovers pairwise cor-relations reducing a great amount of complexity that nev-ertheless can remove most of correlation-induced selectivity error. In this paper, we also leverage pairwise FDs to mea-sure dependency between partitions (interFD) and within a partition (intraFD).
As we discussed earlier, our goal in this paper is to tackle the high computational cost of decomposition process. Since, especially for dense data sets, the number of modes of the tensor data is one of the main factors contributing to the cost of tensor operations, we focus on how we reduce the di-mensionality and the size of the input tensor. In particular, we argue that a large relation can be vertically partitioned into multiple relations relying on the approximate functional dependencies in the data, and the decompositions of these relations can be combined to obtain the overall decomposi-tion. Without loss of generality, in this paper, we consider 2-way partitioning of the input data. The process can easily be generalized for multiple partitions.

In this section, we provide an overview of this DBN pro-cess. We first introduce the relevant notations and provide background on key concepts.
Without loss of generality, we assume that relations are represented in the form of occurrence tensors .
Let A 1 ,..., A n be a set of attributes in the schema of a relation, R, and D 1 ,..., D n be the attribute domains. Let the relation instance R be a finite multi-set of tuples, where each tuple t  X  D 1  X  ...  X  D n .

Definition 3.1 (Occurrence Tensor). An occur-rence tensor R o corresponding to the relation instance R is an n -mode tensor, where each attribute A 1 ,..., A represented by a mode. For the i th mode, which corresponds to A i , let D 0 i  X  D i be the (finite) subset of the elements such that and let idx ( v ) denote the rank of v among the values in D relative to an (arbitrary) total order, &lt; i , defined over the elements of the domain, D i . The cells of the occurrence tensor R o are such that
R o [ u 1 ,...,u n ] = 1  X  X  X  t  X  X  s.t.  X  1  X  j  X  n idx ( t. A and 0 otherwise. Intuitively, each cell indicates whether the corresponding tuple exists in the multi-set corresponding to the relation or not.
First introduced by Codd [5], the normalization process of the relational model evaluates relations based on the un-derlying functional dependencies of the data and vertically partitions a large relation into smaller relations (with lesser number of attributes) to minimize redundancy and insertion, deletion, and update anomalies.
 Functional Dependency (FD). The functional depen-dency, between two sets of attributes X and Y , in a rela-tional model is defined as the following.

Definition 3.2 (Functional dependency). A func-tional dependency (FD), denoted by X  X  X  , holds for rela-tion instance R , if and only if for any two tuples t 1 and t in R that have t 1 [ X ] = t 2 [ X ] , t 1 [ Y ] = t 2 [ Y ] also holds.
The key idea of the normalization process is that if A = { A 1 ,..., A n } is a set of attributes in the schema of a rela-tion, R , and X , Y  X  X  are two subsets of attributes such that X  X  X  , then the relation instance R can be vertically parti-tioned into two relation instances R 1 , with attributes A\Y , and R 2 , with attributes X  X  X  , such that R = R 1 1 R 2 ; in other words the set of attributes X serves as a foreign key and joining vertical partitions R 1 and R 2 on X gives back the relation instance R without any missing or spurious tu-ples. Note that, while for some data sets, FDs are known at the database design time, for many data sets they need to be discovered by analyzing the available data sets. Approximate FD. As described above, the search for ex-act FDs is a costly process. Moreover, in many data sets, attributes may not have perfect FDs due to exceptions and outliers in the data. In such cases, we may search for ap-proximate FDs instead of exact FDs.
 Definition 3.3 (Approximate FD). An approximate FD (aFD), denoted by X  X   X  Y holds for relation instance R , if and only if We refer to the value of  X  as the support of the aFD X  X   X  X  .
As described above, the concept of relational normaliza-tion (vertical partitioning) relies on the relational join opera-tion. Since in this paper we represent relations as occurrence tensors, we also need to define a corresponding tensor join operation that operates in the domain of tensors.
Let P and Q be two tensors, representing relation in-stances P and Q , with attribute sets, A P = { A P 1 ,..., A and A Q = { A Q 1 ,..., A Q m } , respectively. In the rest of this section, we denote the index of each cell of ( i ,i 2 ,...,i n ); similarly, the index of each cell of Q is denoted as ( j 1 ,j 2 ,...,j m ). The cell indexed as ( i 1 ,...,i noted by P [ i 1 ,...,i n ] and the cell indexed as ( j 1 Q is denoted by Q [ j 1 ,...,j m ].

Definition 3.4 (Join ( 1 )). In relational algebra, given two relations P and Q , and a condition  X  , the join operation is defined as a cartesian product of the input relations followed by the selection operation. Therefore, given two relational tensors P and Q , and a condition  X  , we can define their join as Given two relations P and Q , with attribute sets, A { A 1 ,..., A P n } and A Q = { A Q 1 ,..., A Q m } , and a set of at-tributes A  X  A P and A  X  A Q , the equi-join operation, 1 = , A , is defined as the join operation, with the condition that matching attributes in the two relations will have the same values, followed by a projection operation that elimi-nates one instance of A from the resulting relation.
The pseudo-code of the DBN algorithm is shown in Fig-ure 2. In this subsection, we provide an overview of the steps of the algorithm. Then, in the following sections, we study in detail the key steps of the process.

In its first step, DBN evaluates the pairwise FDs among the attributes of the input relation. For this purpose, we employ and extend TANE [8], an efficient algorithm for the discovery of FDs. Our modification of the TANE algorithm returns a set of (approximate) FDs between the attribute pairs and, for each candidate dependency, A i  X  A j , it pro-vides the corresponding support,  X  i,j .

The next steps involves selecting the attribute, X , that will serve as the foreign key and partitioning the input rela-tion R into R 1 and R 2 around X . Note that if the selected join attribute X does not perfectly determine the attributes of R 1 , then to prevent introduction of spurious tuples, we will need to remove sufficient number of outlier tuples from R to enforce the FDs between the attribute, X , and the attributes selected to be moved to R 1 .

Desideratum 1: Therefore, to prevent over-thinning of the relation R , the considered approximate FDs need to have high support; i.e.,  X  i,j  X   X  support , for a sufficiently large support lower-bound,  X  support .

We next consider various criteria for vertical partitioning of the input relation: We elaborate on these desiderata and their implications on the vertical partitioning of R into R 1 and R 2 in Section 4.
Once R 1 and R 2 are obtained, the final steps of the DBN process include decomposition of the tensors corresponding to R 1 and R 2 , joining of the decompositions to obtain can-didate combined decompositions, and selecting the one that is likely to provide the best fit to R . For these, we rely on the join-by-decomposition scheme proposed in [10]: in order to construct a rank-r decomposition of a joined tensor, join-by-decomposition first finds all rank-r 1 rank-r 2 decompositions of the two input tensors, such that r  X  r 2 = r . The rank-r 1 and rank-r 2 decompositions of the input decompositions are then combined along the given factor matrix, which corresponds to the join attribute in the equi-join operation. Finally the algorithm finds the r 1 pair which provides the least approximation error, given all possible factorizations, r 1  X  r 2 of r . Note that join-by-decomposition involves creation of multiple alternative join pairs (corresponding to different r 1  X  r 2 factorizations of r ), which are independently evaluated for accuracy and the one that is predicted to provide the best accuracy is used for obtaining the final result. This provides a natural way to parallelize the entire operation by associating each pair of rank decompositions (and the computation of the cor-responding pair selection measure) to a different processor core. In Section 6, we also evaluate the applicability of such parallelizations in the context of the DBN operation.
Based on the desiderata discussed in Section 3.2, here we discuss the vertical partitioning strategies for sparse and dense tensors. For each situation, we consider the cases: (Case 1) the join attribute (approximately) determines a subset of attributes and (Case 2) it determines all attributes of the input relation.
For the first case (i.e., the join attribute X (approxi-mately) determines a subset of the attributes of R ), we cre-ate a partition R 1 with all the attributes determined with a support higher than the threshold (  X  support ) by the join attribute. This helps us satisfy Desiderata 1 and 2. The second partition, R 2 consists of the join attribute X and all the remaining attributes. Since inter-partition FDs of the form X  X   X  are all less than  X  support , this also reflects Desideratum 5.

Note that by construction, the size of R 2 is equal to the number of tuples in R independent of which attributes are included in it. The size of R 1 , on the other hand, can be minimized (to satisfy Desideratum 4) down to the number of unique values of X by eliminating all the duplicate tuples.
For the second case, where the join attribute X determines all attributes of R , we apply the interFD-based vertical par-titioning strategy detailed later in Section 4.3.
For the first case, similarly to sparse tensors, we create partitions, R 1 and R 2 , by considering the FDs of the form X  X   X  , where X is the join attribute. Differently from the sparse tensors, in this case we also consider Desideratum 3, which prefers balanced partitions:
For the second case, as in the sparse tensors, we apply the interFD-based vertical partitioning strategy detailed later in Section 4.3. The major difference is that in this case, we also promote balance.
Desideratum 5 implies that the vertical partitioning strat-egy should minimize inter-partition FDs. As discussed ear-lier, this helps minimize the likelihood of error when the individual partitions are decomposed and the resulting de-compositions are combined to obtain the overall decompo-sition. Kim and Candan [10] have shown that, given a join operation, R = R 1 1 A R 2 , it is possible to obtain a rank-r decomposition of R by combining rank-r 1 and rank-r 2 de-compositions of R 1 and R 2 , as long as r = r 1  X  r 2 and that rank r 1 and r 2 decompositions of the input tensors lead to clusters that are independent relative to the join attribute, A . Authors have also argued theoretically and experimen-tally that the accuracy of the decomposition is especially high if the other attributes of the two relations R 1 and R are independent from each other. Relying on this obser-vation (which we also validate in Section 6), DBN tries to partition the input relational tensor R in such a way that the resulting partitions, R 1 and R 2 , are as independent from each other as possible.

Remember that the support of an approximate FD is de-fined as the percentage of tuples in the data set for which the FD holds. Thus, in order to quantify the dependence of pairwise attributes, we rely on the supports of pairwise FDs. Since we have two possible FDs ( X  X  Y and Y  X  X ) for each pair of attributes, we use the average of the two as the overall support of the pair of attributes X and Y . Given these pairwise supports we approximate the overall dependency between two partitions R 1 and R 2 using the average support of the pairwise FDs (excluding the pairwise FDs involving the join attribute) crossing the two partitions. We refer to this as interFD and the partitioning based on interFD as the interFD-based partitioning strategy.
We formulate this interFD-based partitioning problem as a graph partitioning problem: Let the pairwise FD graph, G pfd ( V,E ), be a complete, weighted, and undirected graph, where each vertex v  X  V represents an attribute and the weights of the edge between nodes v i and v j is the average support of the approximate FDs v i  X  v j and v j  X  v i . The problem is then to locate a cut on G pfd with the minimum average weight.

This graph partitioning problem is similar to the mini-mum cut problem [19], with some key differences. The ma-jor difference is that we do not seek a cut with minimum total weight, but a cut with minimum average weight. Also, depending on whether we are operating on dense or sparse networks, we may or may not seek to impose a balance crite-rion on the partitions: for sparse tensors, the cost of tensor decomposition increases linearly with the number of modes and since the total number of modes of the two partitions is constant, we do not need to seek balance.

In DBN, we use a modified version of the minimum cut algorithm proposed in [19] to seek minimum average cuts. Given an undirected graph G pfd ( V,E ), for each ver-tex v  X  V , the algorithm finds a vertex with the minimum average cut that separates it from the rest of the graph and creates a subset of vertices V 0 where the vertex is merged with its neighbor vertex, connected to the rest of the graph with the least average weight; the edges from these two ver-tices are replaced by a new edge weighted by the average of the weights of the original edges. The process is repeated while V 0 6 = V . The minimum of the minimum average cuts at each step of the algorithm is returned as the overall mini-mum average cut. When balance of the number of attributes is needed, the minimum is selected among the steps that lead to similar number of attributes. The complexity of this minimum average cut algorithm is O ( | V || E | + | V | 2 Figure 4 shows an example of the process.
Given a partitioning of R into of R 1 and R 2 , to obtain a rank-r decomposition of R , we need to consider rank-r and rank-r 2 decompositions of R 1 and R 2 , such that r = r  X  r 2 and pick the  X  r 1 ,r 2  X  pair which is likely to minimize recombination errors.

We note, however, that we can rely on the supports of the dependencies that make up the partitions R 1 and R to prune  X  r 1 ,r 2  X  pairs which are not likely to give good fits. In particular, we observe that the higher the overall depen-dency between the attributes that make up a partition, the more likely the data in the partition can be described with a smaller number of clusters. Since the number of clusters of a data set is related to the rank of the decomposition, this leads to the observation that Thus, given R 1 and R 2 , we need to consider only those rank pairs  X  r 1 ,r 2  X  , where if the average intra-partition FD support for R 1 is larger than the support for R 2 , then r r and vice versa. We refer to this as the intraFD criterion for rank pruning. Similarly to interFD, given the supports of FDs, we define intraFD as the average support of the pairwise FDs (excluding the pairwise FDs involving the join attribute) within each partition.

In the next section, we evaluate the effect of the interFD-based partitioning and intraFD-based rank pruning strategy of DBN for both dense and sparse tensor decomposition in terms of the efficiency and the accuracy.
Here, we present experimental results assessing the effi-ciency and effectiveness of the proposed DBN scheme rela-tive to the conventional implementation of the tensor decom-position in both stand-alone and parallelized versions. We used various data sets from UCI Machine Learning Reposi-tory [6]. We consider two partitioning cases discussed in Section 4: Case 1. Firstly, we evaluate DBN for the case where the join attribute X determines only a subset of the attributes of the relation R . In this case, as long as one partition R has X and the determined attributes of X , the number of nonzero entries of R 1 is less than or equal to that of R and the number of nonzero entries of the other partition R 2 is same as that of R . For dense tensor decomposition, we make the number of attributes of R 1 and R 2 similar.

For this case, we used a 5-mode relational tensor of di-mensions 118  X  90  X  20263  X  5  X  2 of the Adult data set (D1). We ran TANE [8] to find FDs on this data set. TANE identified almost exact FDs A 3  X  A 9 and A 3  X  A 10 (with 99% and 98% confidence respectively) where A n is the n th attribute. Now we take A 3 as the join attribute and remove unmatched tuples (1 . 72%) for these two FDs to get the exact FDs. Next the tensor is normalized into two 3-mode tensors R 1 { A 3 ,A 9 ,A 10 } and R 2 { A 3 ,A 11 ,A 12 } . We then create relational tensors corresponding to different table sizes by randomly selecting entries from the data.
 Case 2. Secondly, we evaluate DBN in the case where the join attribute X determines all attributes of the relation R . In this case, all partitioning cases generate as many tuple (nonzero entries) as the relation R and thus, it is not pos-sible to select a configuration which better minimizes the total number of nonzero entries than the rest. In this case, for each pairwise FD support above a support threshold,  X  support = 75%, we take an attribute with the highest total pairwise FD support among all attributes of the input rela-tion as the join attribute. For these experiments, we took 15 different data sets (D1-D15) with different sizes and dif-ferent attribute sets (see Table 1) and we experimented all partitioning cases with the same number of tuples for R , R , and R 2 for each data set.

All tensors were encoded as occurrence tensors (see Def-inition 3.1), where each entry is set to 1 or 0, indicating whether the corresponding tuple exists or not. Therefore, we report the number of nonzero entries. We also report the tensor size for the dense tensor decomposition since the decomposition cost of dense tensor decomposition depends on the size of dense tensors.

We experimented with rank-12 decompositions. DBN uses 6 combinations (1  X  12, 2  X  6, 3  X  4, 4  X  3, 6  X  2, and 12  X  1) for rank-12 decomposition for each relation in Table 1. Discovery of FDs. We employed TANE [8] and made an extension to detect approximate FDs for all pairs of at-tributes for pairwise FDs and remove all unmatched tuples to get the exact FDs for the normalization process based on the supports of the approximate FDs.

The supports of the approximate FDs for each attribute set of different relational data sets are shown in Table 2. The table also shows the execution times to discover FDs for each data set. The modified TANE algorithm is effi-cient when the number of attributes is small (5 attributes in these experiments). The overall cost increases linearly in the size of the input [8]. Since the execution times for find-ing approximate FDs are negligible compared to the tensor decomposition time, we focus on the decomposition times. Tensor Decomposition. We experimented with alterna-tive algorithms for both nonnegative CP tensor decompo-sition on the original tensors (NNCP) and DBN. Table 3 shows the various algorithms we use in our experiments.
The first decomposition algorithm we considered is the N-way PARAFAC algorithm with nonnegativity constraint (we call this N-way PARAFAC in the rest of the paper) which is available in the N-way Toolbox for MATLAB [1]. We refer to DBN and NNCP using N-way PARAFAC implementation as DBN-NWAY and NNCP-NWAY respectively.

Since MATLAB X  X  N-way PARAFAC implementation uses a dense tensor (multi-dimensional array) representation, it is too costly to be practical, especially for sparse ten-sors. Another PARAFAC implementation, the CP-ALS al-gorithm [3], on the other hand, can run with both sparse and dense tensors. In the sparse tensor model, the cost increases linearly as the number of nonzero entries of the tensor increases. The CP-ALS, however, does not support nonnegative constraints. Therefore, we implemented a vari-ant of the single grid NTF [17] using CP-ALS as the base PARAFAC algorithm. We refer to DBN and NNCP based on CP-ALS as DBN-CP and NNCP-CP respectively.

For the parallel version of the NNCP, we implemented the grid NTF algorithm [17] with two different partition strate-gies (2 and 6 grid cells along the join mode) using N-way PARAFAC and CP-ALS as the base PARAFAC algorithms.
 Each grid is run with the base PARAFAC algorithm sepa-rately in parallel and results are iteratively combined into the final decomposition based on an ALS-like approach. We refer to the grid NTF algorithm for the parallel NNCP using N-way PARAFAC with 2 and 6 grid cells as NNCP-NWAY-GRID2, and NNCP-NWAY-GRID6 respectively. Similarly, we refer to CP-ALS based implementations as NNCP-CP-GRID2 and NNCP-CP-GRID6.

For the parallel version of DBN, we implemented pairwise parallel DBN-NWAY and DBN-CP, referred to as pp-DBN-NWAY and pp-DBN-CP, respectively.

Finally, for DBN with a subset of pairs, DBN with 2 and 3 pairs selected based on the intraFD-based rank pruning strategy are referred to as DBN2 and DBN3 respectively. For all alternative DBN strategies can be DBN2 or DBN3 according to the number of pairs selected (e.g., DBN2-CP for DBN-CP with 2 pairs selected).
 We ran our experiments on an 6 cores Intel(R) Xeon(R) CPU X5355 @ 2.66GHz with 24GB of RAM. We used MAT-LAB Version 7.11.0.584 (R2010b) 64-bit (glnxa64) for the general implementation and MATLAB Parallel Computing Toolbox for the parallel implementation of DBN and NNCP.
We use the following fit function to measure tensor de-composition accuracy: The fit measure is defined as where k X k is the Frobenius norm of a tensor X . The fit is a normalized measure of how accurate a tensor decomposition of
X ,  X  X with respect to a tensor X . k  X  X k is also used as an approximate fit measure of X to substitute for fit computation for large data sets. The intu-ition behind this measure is as follows: For any W it can be shown that k W  X   X  W k X k W k X  X   X  W k . Therefore, as long as k
W k X k  X  W k holds, we can minimize the term k W  X   X  W k by maximizing k  X  W k .

Our evaluation criteria also include fit ratio which indi-cates how accurate one strategy compared with the other strategy in terms of fit to the input tensor. For example, the fit ratio of DBN to NNCP is defined as where X is the input tensor,  X  X nncp is the tensor obtained by re-composing the NNCP tensor, and  X  X dbn is the tensor obtained by re-composing the DBN tensor.
As discussed in Section 6.1, we evaluate the execution times of DBN vs. the conventional nonnegative CP (NNCP) algorithms (Table 3) for two partitioning cases: in the first case, the join attribute X determines only a subset of the attributes of the relation R ; that is, nnz( R 1 )  X  nnz( R ) and nnz( R 2 ) = nnz( R ) where nnz( X ) denotes the number of nonzero entries of X . In the second case, the join attribute X determines all attributes of the relation R ; that is nnz( R = nnz( R 2 ) = nnz( R ).
 Case 1: X does not determine all attributes of R .

First we present the execution times for dense tensor de-composition. As seen in Figure 5, for NNCP-NWAY, as the tensor size increases, the execution time increases fast. DBN-NWAY, running on smaller number of modes than NNCP, saves significant amount of time.
 Figure 5 (b) shows the running time of NNCP-NWAY-GRID2 and NNCP-NWAY-GRID6 vs. pp-DBN-NWAY on 6 cores. Comparing the execution times in the figure against the single core execution times in Figure 5 (a) shows that the DBN-NWAY benefits more from parallelization. Since the grid NTF divides a larger tensor into smaller sub-tensors but with the same number of modes as the original tensor, paral-lelization does not save as much as in pp-DBN-NWAY where a larger tensor is divided into sub-tensors with a smaller number of modes.

Figure 6 presents results for sparse tensor decomposi-tions, NNCP-CP vs. DBN-CP with different pair selections on single-core and NNCP-CP-GRID vs. pp-DBN-CP on 6 cores. Since we select a foreign key for normalizing the input relation, one of the normalized relations in DBN-CP has the same number of nonzero entries as that of the original rela-tion. As a result, when using a single core, the sparse tensor DBN time (which is determined by the number of tuples) exceeds the time required to decompose the original rela-tion. As discussed in Section 5, we address this using the intraFD-based rank pruning strategy. Figure 6 (a) shows that we achieve the better performance by choosing 2 pairs or 3 pairs in DBN based on intraFD.

Furthermore, as shown in Figure 6 (b), when using multi-ple cores, the proposed pp-DBN-CP achieves greater execu-tion time savings and, therefore, significantly outperforms parallelized versions of NNCP. As seen in the figure, pp-DBN-CP has the lowest execution time. The grid based parallelization of NNCP does not improve the running time much, since the underlying ALS-based combining approach involves significant communication overheads.
 Case 2: X determines all attributes of R .

Since in this case, selection of a configuration is especially difficult, we also consider intraFD-based rank pruning strat-egy that can maximize the performance of DBN. We exper-iment NNCP-NWAY-GRID (avg. of NNCP-NWAY-GRID2 and NNCP-NWAY-GRID6) vs. pp-DBN3-NWAY for dense tensor decomposition and NNCP-CP-GRID (avg. of NNCP-CP-GRID2 and NNCP-CP-GRID6) vs. pp-DBN3-CP for sparse tensor decomposition for D1-D15 on 6 cores. We use the average of all different partitioning cases for each data set. As shown in Figure 7, in most cases of both experi-ments, DBN outperforms NNCP, especially for dense tensor decomposition. In few cases, the running times are too small to make a significant difference between NNCP and DBN. We compare the accuracy of DBN against standard NNCP. Note that we include results for DBN-CP and NNCP-CP (results for DBN-NWAY and NNCP-NWAY are similar). We measured the fit ratios for all different parti-tioning cases of D1-D15 (Table 1). Note that fit computation requires a lot of memory (the computation is not feasible for some larger datasets). Thus, for large datasets with the size of the join mode bigger than 1,000, we created subsets of each tensor by sampling random 1,000 entries from the join mode and compute fit for NNCP-CP and DBN-CP.

As shown in Figure 8 (a), for an overwhelming majority of the experimented data sets, fit ratio falls in the range between 0 . 8 and 1 indicating that the proposed DBN scheme is, in addition to being efficient, also highly effective.
As discussed in Section 4.3, the proposed DBN strategy first identifies alternative normalized partitions of the rela-tional tensor and then selects the most promising pair of partitions to compute the final decomposition. More specif-ically, the algorithm picks the most independent partitions according to the interFD.
 In these experiments, we use 15 different configurations in Table 1. In order to quantify the benefits of the interFD-based partitioning strategy, we measure the ratio of the dif-ference between the approximate fits of interFD-based par-titions and the minimum approximate fits against the differ-ence between the maximum and minimum approximate fits among all partitioning alternatives of each data set.
Results shown in Figure 8 (b) indicates that the interFD-based partitioning strategy results in fits very close to the optimal partitioning strategy. The few cases where the interFD-based strategy provides only 70  X  80% of the op-timal attribute partitioning is largely due to the pairwise nature of interFD. This tends to miss multi-attribute de-pendencies of the form AB  X  C with high support, e.g., in D3, we miss A 1 A 3  X  A 15 whose support is 94%.
We compare the approximate fit of DBN-CP with 2 and 3 pairs selected based on the intraFD-based rank pruning strategy against that of DBN-CP using the original 6 pairs. Note that we use only the cases (41 cases) where each sub-tensor has more than 2 attributes (only the join attribute and a determined attribute) for each data set since the in-traFD does not include pairwise FDs involving the join at-tribute. As shown in Figure 8 (c), the intraFD-based rank pruning strategy of DBN-CP has a good accuracy. The ap-proximate fit ratios in the most of cases of DBN3-CP are close to 1; in other words, one of the 3 pairs in DBN3-CP gives the best approximate fit. Note that the data set D4 (with partitions { A 3 ,A 7 ,A 15 } and { A 3 ,A 8 ,A 13 } ) gives the worst approximate fit for both DBN2-CP and DBN3-CP, with fit ratios 0.63 and 0.80 respectively. This is because, the intraFD strategy with only pairwise dependencies may on occasion fail to take into account critical dependencies, such as A 3 A 7  X  A 15 with 92% support.
Lifecycle of most data includes various operations, from capture, integration, projection, to data decomposition and analysis. To address the high cost of tensor decompsi-tion, which is highest among these operations, we proposed a highly efficient, effective, and parallelized DBN strategy for approximately evaluating decompositions by normaliz-ing a large relation into the smaller tensors based on the FDs of the relation and then performing the decompositions of these smaller tensors. We also proposed interFD-based partitioning and intraFD-based rank pruning strategies for DBN based on pairwise FDs across the normalized partitions and within each normalized partition, respectively.
Experimental results confirmed the efficiency and effec-tiveness of the proposed DBN scheme, and its interFD and intraFD based optimization strategies, compared to the con-ventional NNCP approach.
