 Many programs and tools have been develo ped to formulate and execute queries based on the visual content and to help browsing large multimedia repositories. Still, no general breakthrough has been achieved with respect to large varied databases with exogenous documents. Many questions with respect to speed, semantic descriptors or objective image interpretations are still unanswered. In the medical field, digital images are produced in ever-increasing quantities and used for diagnostics and therapy. With digital imaging and communications in medicine (DICOM), a standard for image communication has been set and pa-tient information can be stored with the actual image(s), although still a few problems prevail with respect to the standardization. In several articles, content-based access to medical images for supporting clinical decision-making has been proposed that would ease the management of clinical data and scenarios for the integration of content-based access methods into picture archiving and commu-nication systems (PACS) have been created [16].

There are several reasons why there is a need for additional, alternative image retrieval methods apart from the steadily growing rate of image production. For the clinical decision making process it can be beneficial or even important to find other images of the same modality, the same anatomic region of the same disease. Although part of this information is normally contained in the DICOM headers and many imaging devices are DICOM-compliant at this time, there are still some problems. DICOM headers contain a high rate of errors: error rates of 16% have been reported by [9] for the field anatomical region. This can hinder the correct retrieval of all wanted images. Clinical decision support techniques such as case-based reasoning [12] or evidence-based medicine [4] can even produce a stronger need to retrieve images that can be valuable for supporting certain diagnoses. It could even be imagined to have Image-Based Reasoning (IBR) as a new discipline for diagnostic aid. Decision support systems in radiology [10] and computer-aided diagnostics for radiological practice as demonstrated at the RSNA (Radiological Society of North America) are on the rise and create a need for powerful data and meta-data management and retrieval [1].

It needs to be stated that the purely visual image queries as they are executed in the computer vision domain will most likely not be able to ever replace text-based methods as there will always be queries for all images of a certain patient, but they have the potential to be a very good complement to text-based search based on their characteristics. Still, the problems and advantages of the technol-ogy have to be stressed to obtain acceptance and use of visual and text-based access methods up to their full potential.

Besides diagnostics, teaching and research especially are expected to improve through the use of visual access methods as visually interesting images can be chosen and can actually be found in the exi sting large repositories. The inclusion of visual features into medical studies is another interesting point for several medical research domains. Visual features do not only allow the retrieval of cases with patients having similar diagnoses but also cases with visual similarity but different diagnoses. In teaching it can help lecturers as well as students to browse educational image repositories and visually inspect the results found.
According to those remarks, this study introduces a semantic indexing ap-proach of the medical report and the medical image, according to the concepts associated to an unified medical modeling system. This approach give a comple-mentary description of the textual and visual characteristic of a medical case, using a balanced weighted vectorial norm fusion method at the conceptual level, by taking into account the confidence, the localization and the frequency of the associated concepts.

The remainder of this paper is organized as follows. Section 2 provides a brief state of the art in the content-based image retrieval, focusing on text and image fusion. In section 3, we introduce the semantic indexing approach, by presenting the main ideas used to close the semantic gap and to develop a complementary text-image fusion approach. Finally, some operational approaches and results are synthesized in the section 4 in CLEF 1 benchmark context, by extracting the main points for the conclusions and the future developments. Although access methods in image databases already existed in the beginning of the 1980s [5], the content-based image retrieval (CBIR) started in the 1990s [17],[8] and has been an extremely active research area over the last 10 years [16]. There is growing interest in CBIR because of the limitations inherent in metadata-based systems. Textual information about images can be easily searched using existing technology, but requires humans to personally describe every image in the database. Moreover, it is possible to miss images that use different synonyms in their descriptions.

Content-based image retrieval (CBIR) is the application of computer vision to the image retrieval problem, that is, the problem of searching for digital images in large databases.  X  X ontent-based X  means that the search makes use of the contents of the images themselves, rather than relying on human-imputed metadata such as captions or keywords.

The visual features, used for indexing and retrieval, are classified in [11] into three classes:  X  primitive features that are low-level features such as color, shape, and tex- X  logical features that are medium-level features describing the image by a  X  abstract feature that are semantic/contextual features.

Current CBIR systems generally make use of primitive features [17], [18]. How-ever, some general semantic layers are insufficient to model medical knowledge. Consequently results are poor when common algorithms are general system of-fers interpretation of images or even medium level concepts as they can easily be captured with text. This loss of information from image data to a representation by features is called the semantic gap [21]. To reduce this semantic gap, special-ized retrieval systems have been proposed in literature [11],[20],[6]. Indeed, the more a retrieval application is specialized for a limited domain, the smaller the gap can be made by using domain knowledge.

Some interesting initiative like Image Retrieval in Medical Applications (IRMA) [13] and medGIFT [16] propose a general content-based medical im-age retrieval system. Even if the results are considerably improved with those systems in the general content-based retrieval framework, still remain the chal-lenge to bridge the semantic gap and the fusion between heterogeneous medical multimedia sources.
Some recent studies [3], [2] associate explicitly the image and the text. Statistic methods are used for modeling the occurrence of document keywords and visual characteristics. This system is very sensitive to the quality of the segmentation of the images.
 Some published works [19] propose to find a semantic of the text using Latent Semantic Analysis. Visual information are extracted using color histograms and edges,and next clustered using the Principal Component Analysis method. In this approach, the two modalities are not combined using the LSA.
 Other more recent initiatives study the use of LSA techniques. [22] apply the LSA method to color and texture features extracted from the two media. The conclusion of this study is that combining the image and the text through the LSA method is not always efficient. [23] use the LSA method on the textual and visual combined information. The tests are not really consistent, like the database contain only few documents.

Interesting recent initiatives using LSA for combining text and image are pre-sented in [7] for general images retrieval by the combination of different sources (text and image) at the features level, after clustering. This approach, even promising, is still subject to some empiric thresholds and parameters like the number of visual clusters associated to the image indexes. The database -more consistent that in [23] -, contains nevertheless only 4500 documents.
In our approach, we initiate a semantic level indexing and fusion, according to a well known medical metathesaurus: the Unified Medical Language Sys-tem. Even if this ontology is still perfectible (some incoherences and inconsis-tencies still remain), this approach allows us to fuse the medical report and image at the homogeneous medical conceptual level and the tests give us some reliable indicators about its efficiency (the tests are operated on Clef Medical Image Database containing 50 000 medical images). Moreover, the conceptual level indexing and fusion will facilitate all future works concerning the seman-tic query and case expansion, the context-aware navigation and query and the data-mining. The main idea of our approach is to take deeply into account existing struc-tured medical ontology/knowledge to reduce the indexing semantic gap and improve the efficiency of the current general CBIR systems for medical im-ages. The semantic indexing framework proposed consists of three main modules: 1. Medical image analysis and conceptualization 2. Medical report analysis and conceptualization 3. Medical conceptual pro cessing and balanced fusion
This article proposes a semantic approach by focusing on the medical image and report fusion. This approach uses concepts from the Unified Medical Lan-guage System (UMLS) to index both images and medical reports (Fig. 1). After an introduction to UMLS in section 3.1, each module of our system is described in detail in the following sections. 3.1 Use of the UMLS Ontology for Improving the Fusion and The purpose of NLM X  X  2 Unified Medical Language System (UMLS) 3 is to facili-tate the development of computer systems that behave as if they  X  X nderstand X  the meaning of the language of biomedicine and health.

The UMLS Metathesaurus is a very large, multi-purpose, and multi-lingual vocabulary database that contains information about biomedical and health re-lated concepts, their various names, and the relationships among them. The Metathesaurus is organized by concept or meaning. All concepts in the Metathe-saurus are assigned to at least one semantic type from the Semantic Network. This provides consistent categorization of all concepts in the Metathesaurus at the relatively general level represented in the Semantic Network.

In order to filter the UMLS concepts and relationships needed for the fusion, medical case semantic indexing and query expansion, the Metathesaurus UMLS Knowledge Source has been used. This Metathesaurus is composed by medical concepts and is distributed with several tools (programs) that facilitate their use, including the MetamorphoSys install and customization program. Meta-morphoSys is the UMLS installation wizard and customization tool included in each UMLS release. It may be used to exclude vocabularies that are not required or licensed for use in local applications and to select from a variety of data output options and filters.

Using MetamorphoSys, we have extracted all the Concept Unique Identifiers (CUI) from the UMLS Metathesaurus UMLS file in order to build the concept layer used for medical cases (image+report) indexing. 3.2 Medical Image Analysis and Conceptualization Concerning the image indexing part, our aim is to associate to each image, or to each image region, a semantic label that corresponds to a combination of UMLS concepts and visual percepts. We define three types of UMLS concepts that could be associated to one image or one region:  X  modality concepts that belong to the following UMLS semantic type:  X  X i- X  anatomy concepts that belong to the following UMLS semantic types:  X  X ody  X  pathology concepts that belong to the following UMLS semantic types:  X  X c-
We propose a structured learning framework based on Support Vector Ma-chines (SVMs) to facilitate modular design and extract medical semantics from images (Fig. 2). We developed two complementary indexing approaches within this statistical learning framework:  X  a global indexing to access image modality (chest X-ray, gross photography  X  a local indexing to access semantic local features that are related to one
After presenting our general learning framework, we detail both approaches hereafter.
 General Statistical Learning Framework: Firstly, a set of disjoint semantic tokens with visual appearance in medical images is selected to define a Visual and Medical vocabulary. This notion of using a visual and semantic vocabulary to represent and index image has been applied to consumer images in [15,14]. Here, we use UMLS concepts to represent each token in the medical domain.
Secondly, low-level features are extracted from image region instances to rep-resent each token in terms of color, texture, shape, etc.

Thirdly, these low-level features are used as training examples to build a semantic classifier according the Visual and Medical Vocabulary. We use a hier-archical classification based on SVMs. First, a tree whose leaves are the Visual-Medical terms is constructed. The upper levels of the tree consist to auxiliary classes that cluster similar terms with respect to their visual appearance. A learning process is performed at each node in the following way. If a node corre-sponding to a cluster C has N C direct children, N C SVM classifiers are learned to classify in one class against the N C  X  1 other classes. The positive and neg-ative examples for a class c  X  X  is respectively given by the instances of the term(s) associated to the class c and the instances of the terms associated to the N
C  X  1 other classes. This is a conditional learning in a sense as the classifiers are learned given that a class belongs to a given cluster.

The classifier according the Visual and Medical vocabulary is finally designed from the tree of SVM conditional classifiers in the following way. The conditional probability that an example z belong to a class c given that the class belong to the cluster C is first computed using the softmax function: where D c is the signed distance to the SVM hyperplane that separate class c from the other classes of the cluster C . The probability of a Visual-Medical term VMT i (i.e.aleaveofthetree)foranexample z is finally given by: where L is the number of hierarchical levels, C 1 ( VMT i )) denotes the cluster to which VMT i belongs, and C l ( VMT i ) the cluster to which C l  X  1 ( VMT i ) belongs. C ( VMT i ) is the cluster containing all the defined classes (it corresponds to the tree root).
 Global UMLS Indexing According Modality: The global UMLS indexing is based on a two level hierarchical classifier according modality concepts. This modality classifier has been learned from 4000 images separated in 32 classes: 22 grey level modalities, and 10 color modalities. This images come from the CLEF database (2500 examples), from the IRMA database (300 examples), and from the web (1200 examples). The training images from CLEF database was obtained from modality concept extraction using medical report. A manual filtering on this extraction to remove irrelevant examples had to be performed. We plan to automatize this filtering in the near future.

The first level corresponds to a classification according grey level versus color images. Indeed, some ambiguity can appear due to the presence of colored images, or the slightly blue or green appearance of X-ray images. This first classifier uses HSV three first moments computed on the entire image. The sec-ond level corresponds to the classification according Modality UMLS concepts given that the image is in the grey or the color cluster. For the grey level cluster, we use grey level histogram (32 bins), texture features (mean and variance of Gabor filtering for 5 scales and 6 orientations), and thumbnails (grey values of 16x16 resized image). For the color cluster, we have adopted HSV histogram (125 bins), Gabor texture features, and thumbnails. Zero-mean normalization is applied to each feature. For each SVM classifier, we adopted a RBF kernel with modified city-block distance between feature vectors y and x that equally takes into account each type of feature: where F isthenumberoftypeoffeatures( F = 1 for the grey versus color clas-sifier, F = 3 for the conditional modality classifiers: color, texture, thumbnails), x f is the feature vector of type f, and x = The probability of a modality MOD i for an image z is given by equation 2. More precisely, we have: where C and G respectively denote the color and the grey level clusters.
A modality concept can thus be assigned to an image z using the following formula:
The classifier has been first learned on the half on the training dataset to evaluate its performance in the other half of the training dataset. The error rate on the test set is about 18%, with recall and precision rates larger than 70% for a large majority of the classes. The classification is quite good given the intra-variability of some classes with respect to the class inter-variability. For example, differentiate a brain MRI and a brain CT can be a hard task, even for a human operator.

After learning (using the entire learning set), each database image is indexed according modality given its low-level features. The index values are the proba-bility values given by equation (4).
 Local UMLS Indexing: To better capture the medical medical image con-tent, we propose to extend this first modeling for local patch classification in local visual and semantic tokens (LocVisMed terms). Each LocVisMed term is expressed as a combination of Unified Medical Language System (UMLS) con-cepts from Modality, Anatomy, and Pathology UMLS semantic types. In these experiments, we have adopted color and texture features from patches (i. e. small image blocks) and a non hierarchical classifier based on SVMs and the softmax function given by equation (1). A Semantic Patch Classifier was finally designed to classify a patch according 64 LocVi sMed terms. The color features are the three first moments of the Hue, the Saturation, and the Value of the patch. The texture features are the mean and variance of Gabor filtering using 5 scales and 6 orientations. Zero-mean normalization is applied to both the color and texture features. We adopted a RBF kernel with modified city-block distance given by equation (3).

The training set is composed of 3631 patches extracted from images mostly coming from the web (1182 images coming from the web and 158 images from the CLEF collection). The classifier has been first learned on a first half of this training set to be evaluated on a second half. The error rate of this classifier is about 30%.

After learning, the LocVisMed terms are detected during image indexing from image patches without region segmentation to form semantic local histograms. Essentially, an image is tessellated into image overlapping blocks of size 40x40 pixels after area standardization. Each patch is then classified in 64 semantic classes using the Semantic Patch Classifier. An image containing P overlapping patches is then characterized by the set of P LocVisMed histograms and their respective location in the image. An histogram aggregation per block gives the final image index : M  X  N LocVisMed histograms (each bin corresponding to the probability of a LocVisMed term presence). 3.3 Medical Report Analysis and Conceptualization In order to improve conventional text Information Retrieval approaches, we pass form the text syntactic level to the semantic one. In the medical field, this step requires the use of a specialized concepts from available thesaurus and metathe-saurus. In this sense, the Unified Medical Language System help us to acquire this pertinent higher level indexing, using a specific UMLS concepts extractor like MetaMap , provided by the National Library of Medicine (NLM). A con-cept is then an abstraction of this synonymous set of terms. Thus, conceptual text indexing consists of associating a set of concepts to a document and uses it as index. This set of concepts should cover the document theme. Conceptual indexing naturally solves the term mismatch and the multilingual problem. 3.4 Medical Conceptual Processing and Balanced Fusion Even if from the medical point of view, a medical case can have mode than one associated medical image, for the retrieval purpose, we consider one case c composed by one medical report and one medical image.

The main idea of our approach is to use the medical rapport and medical image conceptual and/or visual-concept indexing in order to build an homogeneous high level fusion approach for improve the retrieval system performances.
Such a medical case c will bring thus an indexing from the associated image and respectively medical report: where: CUI is the notation for an UMLS concepts,
VC are visual concepts, mix between UMLS concepts and perception concepts,  X  means the indexing confidence degree,  X  is associated to the local relative frequency of the concept,  X  corresponds to the spatial localization fuzzy weight,  X  represents the semantic tree (modality, anatomy, biology, pathology and di-
For the medical case c (medical image and associated medical report docu-ment) and the corresponding UMLS extracted concept CUI i , we obtain the next fuzzy confidence indexing vector:  X  i =  X 
If the concept is a visual concept VC s ,wehave:
As the semantic medical report and image use the same homogeneous ontol-ogy, the fusion takes into account the norm of the so obtained global credibility vector  X  c i as a projection of the c medical case to each concept CUI i : Obviously, we will find the same kind of projection of the case c to each associated visual concept VC s of the corresponding medical image:
A medical case c will be then characterized by the vector  X  c of the global credibilities of all associated UMLS and visual concepts:
The parameters used for the projection of a medical case on CUI or VC are obtained by direct image and text pre-computation (indexing) -e.g. fuzzy indexing confidence degree  X  and the local relative frequency  X  -or by fuzzy-fication -for the spatial localization  X  and semantic tree membership  X  fuzzy weights.

The indexing confidence degree  X  is a fuzzy result directly given by the classifiers (equation (4) used for the medical image indexing. For the text pre-processing, a text indexing software (in our case, MetaMap ) has been used for calculate the local relative frequency  X  c txt medical report. If a patch extraction method is used for the image, the local rel-ative frequency  X  c img versus all the patches of the image. The spatial localization  X  corresponds -for the text indexing -to the importance of the medical report XML tag or the sec-tion from which the concept comes. For example, the &lt; Diagnosis &gt; paragraph of the medical report, the physician synthesized the most important keywords describing the disease (pathology) and the anatomic part. This tag will thus be more important than (par example) the &lt;Description&gt; tag. In order to fuzzy this subjective parameter, we propose the fuzzy membership sets presented in the Fig. 3.

For the image indexing, the spatial localization corresponds to a special weight accorded to a particular place in the image (e.g. the central patches for a CT medical image or a circle sector shape object for a Doppler indexing).
The semantic tree membership  X  intent to give a particular weight to a concept belonging to a particular semantic tree (modality, anatomy, pathology, biology, direction) according to the indexing source (image or text) of this concept. Par example, a modality concept coming from the medical image indexing will have more importance that a modality extracted from the medical report. Opposite, a pathology concept will have more  X  X onfidence X  if extracted by the medical report indexing service (Fig.4), with: where  X  corresponds to the size of the fuzzy membership function associated to the influence zone of each semantic type.
 We apply our approach on the medical image collection of CLEF 4 Cross Lan-guage Image Retrieval track. This database consists of four public datasets (CASImage 5 ,MIR 6 ,PathoPic 7 , PEIR 8 ) containing 50000 medical images with the associated medical report in three different languages. In 2005, 134 runs were evaluated on 25 queries containing at least one of the following axes: anatomy (ex: heart), modality (ex: X-ray), pathology or disease (ex: pneumonia), abnormal visual observation (ex: enlarged heart).

We test five approaches on these 25 queries to evaluate the benefit of using a UMLS indexing, especially in a fusion framework. First, three UMLS image indexing were tested on the visual queries: 1. Global UMLS image indexing presented in section 3.2 / Retrieval based on the 2. Local UMLS image indexing presented in section 3.2 / Retrieval based on 3. Late fusion of the two visual indexing approaches (1) and (2) The two last tests respectively concerns textual indexing and retrieval using UMLS and the fusion between this textual indexing and the Global UMLS image indexing (i.e. modality indexing). The integration of local information is under developmen. The text indexing uses MetaMap software and give us the UMLS associated concepts CUI with the corresponding relative frequencies  X  .The medical image indexing uses a global image approach giving essentially UMLS modalities concepts CUI MOD , the associated frequency  X  and SVM confidence degree  X  .

With these partial indexing information, we build the global confidence degree  X  such as:
Comparative results are given in table 1. For the visual indexing and re-trieval without textual information, our results are quite good with respect to the best 2005 results, especially when local and global UMLS indexes are mixed. Our textual approach is on the first 2005 results (between 9% and 20%) but significantly under the approach proposed by Jean-Pierre Chevallet (IPAL) in 2005 that uses a textual filtering on MeSH terms according three dimensions: Modality, Anatomy, and Pathology. The high average precision is principally due to this textual filtering. We have to notice that the associa-tion between MeSH terms and a dimension had to be done manually. With the use to UMLS metha-thesaurus, we have the advantage to have access to these dimension thanks to the semantic type associated to each UMLS concept.

We can verify the benefit of the fusion between image and text : from 16% for the text only and 10% for the image only, we reach the 24% with a mixed indexing and retrieval. We are slightly superior in average precision than the mixed approaches that do not use dimension filtering. The presented researches constitutes a very promising approach of semantic indexing and late balanced fusion, in the medical cases retrieval framework.
One important contribution of this study is the use of a web-based up to date medical metathesaurus (UMLS) in order to  X  X tandardize X  the semantic indexing of the text and the medical image. This allows to work on the same level for both medical media and to have thus an homogeneous complementary point of view about the medical case.

The introduction of a late semantic fusion for each common UMLS concept, using multiple criteria weighted norm involving the frequency of a concept, the indexing confidence degree, the spatial localization weight and the semantic tree belonging, constitutes another important point to be underlined. The so obtained vectorial norm represents a balanced projection of the medical case (document and image) to the given UMLS concept, a consistent information enabling a reliable and robust semantic retrieval.

Future developments become very promising using this homogeneous balanced semantic fusion. Appropriate clustering methods should be able to bridge to med-ical multimedia data-mining, opening the way to the evidence-based medicine and other advanced medical research applications and studies.

In future evolutions of this application, our fusion approach will be improved using the local visual information derived from the proposed local patch classifier. Indeed, this method is complementary to the global medical image analysis and will certainly improve the global retrieval results.

Otherwise, the use of the incremental learning based on the initial database clustering, should be able to facilitate the development of an efficient real-time medical case-based reasoning.

Finally, a semantic query and case expansion policy can be deployed using the symbolic and statistic relation available in UMLS at the first time, and the contextual behavior information extracted from the real use of the retrieval system in the second time.

