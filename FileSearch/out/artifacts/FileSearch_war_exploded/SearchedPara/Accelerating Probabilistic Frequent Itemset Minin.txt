 Data uncertainty is inherent in emerging applications such as location-based services, sensor monitoring systems, and data integration. To handle a large amount of imprecise information, uncertain databases have been recently devel-oped. In this paper, we study how to efficiently discover fre-quent itemsets from large uncertain databases, interpreted under the Possible World Semantics . This is technically challenging, since an uncertain database induces an expo-nential number of possible worlds. To tackle this problem, we propose a novel method to capture the itemset mining process as a Poisson binomial distribution. This model-based approach extracts frequent itemsets with a high degree of accuracy, and supports large databases. We apply our tech-niques to improve the performance of the algorithms for: (1) finding itemsets whose frequentness probabilities are larger than some threshold; and (2) mining itemsets with the k highest frequentness probabilities. Our approaches support both tuple and attribute uncertainty models, which are com-monly used to represent uncertain databases. Extensive evaluation on real and synthetic datasets shows that our methods are highly accurate. Moreover, they are orders of magnitudes faster than previous approaches.
 H.2.8 [ Database management ]: Data applications X  data mining ; G.3 [ Probability and statistics ]: Distribution functions Algorithms
In many applications, the underlying databases are un-certain. The locations of users obtained through RFID and GPS systems, for instance, are not precise due to measure-ment errors [22, 16]. In habitat monitoring systems, data collected from sensors like temperature and humidity are noisy [1]. Customer purchase behaviors, as captured in su-permarket basket databases, contain statistical information for predicting what a customer will buy in the future [3, 25]. Integration and record linkage tools associate confi-dence values to the output tuples according to the quality of matching [11]. In structured information extractors, con-fidence values are appended to rules for extracting patterns from unstructured data [26]. Recently, uncertain databases have been proposed to offer a better support for handling imprecise data in these applications [8, 11, 20, 15, 13].
As an example of uncertain data, consider an online mar-ketplace application (Table 1). Here, the purchase behav-ior details of customers Jack and Mary are recorded. The value associated with each item represents the chance that a customer may buy that item in the near future. These probability values may be obtained by analyzing the users X  browsing histories. For instance, if Jack visited the mar-ketplace ten times in the previous week, out of which video products were clicked five times, the marketplace may con-clude that Jack has a 50% chance of buying videos . This attribute-uncertainty model, which is well-studied in the lit-erature [22, 8, 15, 25], associates confidence values with data attributes. It is also used to model location and sensor un-certainty in GPS and RFID systems.

To interpret uncertain databases, the Possible World Se-mantics (or PWS in short) is often used [11]. Conceptually, a database is viewed as a set of deterministic instances (called possible worlds ), each of which contains a set of tuples. A possible world w for Table 1 consists of two tuples, { food } and { clothing } , for Jack and Mary respectively. Since { food } occurs with a probability of (1  X  1 2 )  X  1 = 1 2 , and { clothing } has a probability of 1  X  (1  X  1 3 )  X  (1  X  2 3 ) = 2 9 , the prob-ability that w exists is 1 2  X  2 9 , or 1 9 . Any query evaluation algorithm for an uncertain database has to be correct under PWS. That is, the results produced by the algorithm should be the same as if the query is evaluated on every possible world [11].

Although PWS is intuitive and useful, querying or min-ing under this notion is costly. This is because an uncertain database has an exponential number of possible worlds. For example, the database in Table 1 has 2 3 = 8 possible worlds. Performing data mining under PWS can thus be technically challenging. In fact, the mining of uncertain data has re-cently attracted research attention [3]. For example, in [17], efficient clustering algorithms were developed for uncertain objects; in [14] and [27], na  X   X ve Bayes and decision tree classi-fiers designed for uncertain data were studied. Here, we de-velop scalable algorithms for finding frequent itemsets (i.e., sets of attribute values that appear together frequently in tuples) for uncertain databases. Our algorithms can be ap-plied to two important uncertainty models: attribute uncer-tainty (e.g., Table 1); and tuple uncertainty , where every tuple is associated with a probability to indicate whether it exists [11, 20, 13, 10, 21].
The frequent itemsets discovered from uncertain data are naturally probabilistic, in order to reflect the confidence placed on the mining results. Figure 1 shows a Probabilis-tic Frequent Itemset (or PFI ) extracted from the table in Table 1. A PFI is a set of attribute values that occur fre-quently with sufficiently-high probabilities. In Figure 1, the support probability mass function (or s-pmf in short) for the PFI { video } is shown. This is the pmf for the number of tuples (or support count ) that contain an itemset. Under PWS, a database induces a set of possible worlds, each giv-ing a (different) support count for a given itemset. Hence, the support of a frequent itemset is described by a pmf. In Figure 1, if we consider all possible worlds where itemset { video } occurs twice, the corresponding probability is 1
A simple way of finding PFIs is to mine frequent patterns from every possible world, and then record the probabilities of the occurrences of these patterns. This is impractical, due to the exponential number of possible worlds. To rem-edy this, some algorithms have been recently developed to successfully retrieve PFIs without instantiating all possible worlds [28, 25]. These algorithms are able to find a PFI in O ( n 2 ) time (where n is the number of tuples contained in the database). However, our experimental results reveal that they require a long time to complete (e.g., with a 300k real dataset, the dynamic programming algorithm in [25] needs 30.1 hours to complete). We observe that the s-pmf of a PFI can be modeled by a Poisson binomial distribution, for both attribute-and tuple-uncertain data. We make use of this in-tuition to propose a method for approximating a PFI X  X  pmf with a Poisson distribution, which can be efficiently and ac-curately estimated. This model-based algorithm runs in O ( n ) time, and is thus more scalable to large datasets. In fact, our algorithm only needs 9.2 seconds to find all PFIs, which is four orders of magnitudes faster.

We demonstrate how the model-based algorithm can work under two semantics of PFI, proposed in [25]: (1) threshold-based , where PFIs with probabilities larger than some user-defined threshold are returned; and (2) rank-based , where PFIs with the k highest probabilities are returned. As we will show, these algorithms can be adapted to the attribute and tuple uncertainty models. For mining threshold-based PFIs, we demonstrate how to reduce the amount of effort of scanning the database. For mining rank-based PFIs, we optimize the previous algorithm to improve the perfor-mance. We derive the time and space complexities of our ap-proaches. As our experiments show, model-based algorithms can significantly improve the performance of PFI discovery, with a high degree of accuracy. To summarize, our contri-butions are:
This paper is organized as follows. In Section 2, we review the related works. Section 3 discusses the problem defini-tion. Section 4 describes efficient and accurate methods for computing s-pmf. Then, in Sections 5 and 6, we present our algorithms for discovering threshold-and rank-based PFIs respectively. Section 7 presents our experimental results. We conclude in Section 8.
Mining frequent itemsets is often regarded as an impor-tant first step of deriving association rules [4]. Many efficient algorithms have been proposed to retrieve frequent itemsets, such as Apriori [4] and FP-growth [12]. While these algo-rithms work well for databases with precise and exact values, it is interesting to extend them to support uncertain data. Our algorithms are based on the Apriori. We believe that they can be used by other algorithms (e.g., FP-growth) to support uncertain data.

For uncertain databases, [9, 2] developed efficient frequent pattern mining algorithms based on the expected support counts of the patterns. However, [28, 25] found that the use of expected support may render important patterns miss-ing. Hence, they proposed to compute the probability that a pattern is frequent, and introduced the notion of PFI. In [25], dynamic-programming-based solutions were devel-oped to retrieve PFIs from attribute-uncertain databases, for both threshold-and rank-based PFIs. However, their al-gorithms have to compute exact probabilities, and find a PFI in O ( n 2 ) time. By using probability models, our algorithms avoid the use of dynamic programming, and can find a PFI much faster (in O ( n ) time). In [28], approximate algorithms for deriving threshold-based PFIs from tuple-uncertain data streams were developed. While [28] only considered the ex-traction of singletons (i.e., sets of single items), our solution discovers patterns with more than one item. More recently, [24] developed an exact threshold-based PFI mining algo-rithm. However, it does not support rank-based PFI dis-covery. Here we also study the retrieval of rank-based PFIs from tuple uncertain data. To our best knowledge, this has not been examined before. Table 2 summarizes the major work done in PFI mining.
Other works on the retrieval of frequent patterns from imprecise data include: [7], which studied approximate fre-quent patterns on noisy data; [18], which examined associa-tion rules on fuzzy sets; and [19], which proposed the notion of a  X  X ague association rule X . However, none of these solu-tions are developed on the uncertainty models studied here.
We now discuss the uncertainty models used in this paper, in Section 3.1. Then, we describe the notions of threshold-and rank-based PFIs in Section 3.2.
Let V be a set of items. In the attribute uncertainty model [22, 8, 15, 25], each attribute value carries some uncertain information. Here we adopt the following vari-ant [25]: a database D contains n tuples, or transactions. Each transaction, t j is associated with a set of items taken from V . Each item v  X  V exists in t j with an existential probability Pr ( v  X  t j )  X  (0 , 1], which denotes the chance that v belongs to t j . In Table 1, for instance, the existential probability of video in t Jack is Pr ( video Jack ) = 1 / 2. This model can also be used to describe uncertainty in binary attributes. For instance, the item video can be considered as an attribute, whose value is one, for Jack X  X  tuple, with probability 1 2 , in tuple t Jack .

Under the Possible World Semantics (PWS), D generates a set of possible worlds W . Table 3 lists all possible worlds for Table 1. Each world w i  X  X  , which consists of a subset of attributes from each transaction, occurs with probability Pr ( w i ). For example, Pr ( w 2 ) is the product of: (1) the probability that Jack purchases food but not video (equal to 1 2 ); and (2) the probability that Mary buys clothing and video only (equal to 1 9 ). As shown in Table 3, the sum of possible world probabilities is one, and the number of possible worlds is exponentially large. Our goal is to discover frequent patterns without expanding D into possible worlds.
In the tuple uncertainty model , each tuple or trans-action is associated with a probability value. We assume the following variant [10, 21]: each transaction t j associated with a set of items and an existential probabil-ity Pr ( t j )  X  (0 , 1], which indicates that t j exists in D with probability Pr ( t j ). Again, the number of possible worlds for this model is exponentially large. Table 4 summarizes the symbols used in this paper. Let I  X  V be a set of items, or an itemset . The support of I , denoted by s ( I ), is the number of transactions in which I appears in a transaction database [4]. In precise databases, s ( I ) is a single value. This is no longer true in uncertain databases, because in different possible worlds, s ( I ) can have different values. Let S ( w j ,I ) be the support count of I in possible world w j . Then, the probability that s ( I ) has a value of i , denoted by Pr I ( i ), is: Hence, Pr I ( i )( i = 1 ,...,n ) form a probability mass function (or pmf ) of s ( I ). We call Pr I the support pmf (or s-pmf ) of I . In Table 3, for instance, Pr video (2) = Pr ( w 6 ) + Pr ( w since s ( I ) = 2 in possible worlds w 6 and w 8 . Figure 1 shows the s-pmf of { video } .

Now, let minsup  X  (0 ,n ] be an integer. An itemset I is said to be frequent if s ( I )  X  minsup [4]. For uncer-tain databases, the frequentness probability of I , denoted by Pr freq ( I ), is the probability that an itemset is frequent [25]. Notice that Pr freq ( I ) can be expressed as: In Figure 1, if minsup = 1, then Pr freq ( { video } ) = Pr Pr
Using frequentness probabilities, we can determine whether an itemset is frequent. We identify two classes of Probabilis-tic Frequent Itemsets (or PFI ) below:
Before we move on, we would like to mention the following theorem, which was discussed in [25]:
THEOREM 1. Anti-Monotonicity: Let S and I be two itemsets. If S  X  I , then Pr freq ( S )  X  Pr freq ( I ). This theorem will be used in our discussions.
 We next derive efficient s-pmf computation methods in Section 4. Based on these methods, we present algorithms for retrieving threshold-based and rank-based PFIs, in Sec-tions 5 and 6 respectively.
From the last section, we can see that the s-pmf s ( I ) of itemset I plays an important role in determining whether I is a PFI. However, directly computing s ( I ) (e.g., using the dynamic programming approaches of [25, 28]) can be expen-sive. We now investigate an alternative way of computing s ( I ). In Section 4.1 we study some statistical properties of s ( I ). Section 4.2 exploits these results by approximating s ( I ) in a computationally efficient way.
An interesting observation about s ( I ) is that it is essen-tially the number of successful Poisson trials [23]. To ex-plain, we let X I j be a random variable, which is equal to one if I is a subset of the items associated with transaction t (i.e., I  X  t j ), or zero otherwise. Notice that Pr ( I  X  t be easily calculated in our uncertainty models:
Given a database of size n , each I is associated with ran-dom variables X I 1 ,X I 2 ,...,X I n . In both uncertainty models considered in this paper, all tuples are independent. There-fore, these n variables are independent, and they represent n Poisson trials. Moreover, X I = P n j =1 X I j follows a Poisson binomial distribution.

Next, we observe an important relationship between X I and Pr I ( i ) (i.e., the probability that the support of I is i ): This is simply because X I is the number of times that I exists in the database. Hence, the s-pmf of I , i.e., Pr the pmf of X I , a Poisson binomial distribution.

Using Equation 5, we can rewrite Equation 2, which com-putes the frequentness probability of I , as: Therefore, if the cumulative distribution function (cdf) of X
I is known, Pr freq ( I ) can also be evaluated. Next, we discuss an approach to approximate this cdf, in order to compute Pr freq ( I ) efficiently. From Equation 7, we can express Pr freq ( I ) as: For notational convenience, let p I j be Pr ( I  X  t j ). Then, the expected value of X I , denoted by  X  I , can be computed by: Since a Poisson binomial distribution can be well-approximated by a Poisson distribution [6], Equation 8 can be written as: where F is the cdf of the Poisson distribution with mean  X  R tion quality is shown in Appendix A. The upper bound of the error is small. According to our experimental results, the approximation is quite accurate.

To estimate Pr freq ( I ), we can first compute  X  I by scan-ning D once and summing up p I j  X  X  for all tuples t Then, F ( minsup  X  1 , X  I ) is evaluated, and Equation 10 is used to approximate Pr freq ( I ).

We have also observed an important property of the fre-quentness probability:
THEOREM 2. Pr freq ( I ), if approximated by Equation 10, increases monotonically with  X  I .

Proof. The cdf of a Poisson distribution, F ( i, X  ), can be
Since minsup is fixed and independent of  X  , let us examine the partial derivative w.r.t.  X  .
Thus, the cdf of the Poisson distribution F ( i, X  ) is mono-tonically decreasing w.r.t.  X  , when i is fixed. Consequently, 1  X  F ( i  X  1 , X  ) increases monotonically with  X  . Theorem 2 follows immediately by substituting i = minsup .

Intuitively, Theorem 2 states that the higher value of  X  I the higher is the chance that I is a PFI. Next, we will illus-trate how this theorem avoids the costly computations of F , and improves the efficiency of finding both threshold-and rank-based PFIs.
Can we quickly determine whether an itemset I is a threshold-based PFI? Answering this question is crucial, since in typi-cal PFI mining algorithms (e.g.,[25]), candidate itemsets are first generated, before they are tested on whether they are PFI X  X . In Section 5.1, we develop a simple method of testing whether I is a threshold-based PFI, without computing its frequentness probability. We then enhance this method in Section 5.2. We demonstrate an adaptation of these tech-niques in an existing PFI-mining algorithm, in Section 5.3.
Given the values of minsup and minprob , we can test whether I is a threshold-based PFI, in three steps: Step 1. Find a real number  X  m satisfying the equation: The above Equation can be solved efficiently by employing numerical methods, thanks to Theorem 2.
 Step 2. Use Equation 9 to compute  X  I . Notice that the database D has to be scanned once.
 Step 3. If  X  I  X   X  m , we conclude that I is a PFI. Otherwise, I must not be a PFI.

To understand why this works, first notice that the right side of Equation 11 is the same as that of Equation 10, an expression of frequentness probability. Essentially, Step 1 finds out the value of  X  m that corresponds to the fre-quentness probability threshold (i.e., minprob ). In Steps 2 and 3, if  X  I  X   X  m , Theorem 2 allows us to deduce that Pr freq ( I ) &gt; minprob . Hence, these steps together can test whether an itemset is a PFI.

In order to verify whether I is a PFI, once  X  m is found, we do not have to evaluate Pr freq ( I ). Instead, we compute  X  in Step 2, which can be done in O ( n ) time. This is a more scalable method compared with solutions in [28, 25], which evaluate Pr freq ( I ) in O ( n 2 ) time. Next, we study how this method can be further improved.
In Step 2 of the last section, D has to be scanned once to obtain  X  I , for every itemset I . This can be costly if D is large, and if many itemsets need to be tested. For example, in the Apriori algorithm [25], many candidate itemsets are generated first before testing whether they are PFIs. We now explain how the PFI testing can still be carried out without scanning the whole database.
 Let  X  I l = P l j =1 p j , where l  X  (0 ,n ]. Essentially,  X  the  X  X artial value X  of  X  I , which is obtained after scanning l tuples. Notice that  X  I n =  X  I . Suppose that  X  m has been obtained from Equation 11, we first claim the following:
LEMMA 1. Let i  X  (0 ,n ]. If  X  I i  X   X  m , then I is a threshold-based PFI.

Proof. Notice that  X  I i monotonically increases with i . If there exists a value of i such that  X  I i  X   X  m , we must have  X  I =  X  I n  X   X  I i  X   X  m , implying that I is a PFI.
Using Lemma 1, a PFI can be verified by scanning only a part of the database. We next show the following.

LEMMA 2. If I is a threshold-based PFI, then:
Proof. Let D l be a set of tuples { t 1 ,...,t l } . Then,
Since Pr ( I  X  t j )  X  [0 , 1], based on the above equations, we have:
If itemset I is a PFI, then  X  I  X   X  m . In addition,  X  I n  X  i Therefore, This lemma leads to the following corollary.

COROLLARY 1. An itemset I cannot be a PFI if there exists i  X  (0 , b  X  m c ] such that:
We use an example to illustrate Corollary 1. Suppose that  X  m = 1 . 1 for the database in Table 1. Also, let I = { clothing, video } . Using Corollary 1, we do not have to scan the whole database. Instead, only the tuple t Jack needs to be read. This is because: Since Equation 14 is satisfied, we confirm that I is not a PFI without scanning the whole database.

We use the above results to improve the speed of the PFI testing process. Specifically, after a tuple has been scanned, we check whether Lemma 1 is satisfied; if so, we immediately conclude that I is a PFI. After scanning n  X  X   X  m c or more tuples, we examine whether I is not a PFI, by using Corol-lary 1. These testing procedures continue until the whole database is scanned, yielding  X  I . Then, we execute Step 3 (Section 5.1) to test whether I is a PFI.
The testing techniques just mentioned are not associated with any specific threshold-based PFI mining algorithms. Moreover, these methods support both attribute-and tuple-uncertainty models. Hence, they can be easily adopted by existing algorithms. We now explain how to incorporate our techniques to enhance the Apriori [25] algorithm, an important PFI mining algorithms.

The resulting procedure (Algorithm 1) uses the  X  X ottom-up X  framework of the Apriori: starting from k = 1, size-k PFIs (called k -PFIs) are first generated. Then, using Theo-rem 1, size-( k + 1) candidate itemsets are derived from the k -PFIs, based on which the k -PFIs are found. The process goes on with larger k , until no larger candidate itemsets can be discovered.
 The main difference of Algorithm 1 compared with that of Apriori [25] is that all steps that require frequentness proba-bility computation are replaced by our PFI testing methods. In particular, Algorithm 1 first computes  X  m (Line 2). Then, for each candidate itemset I generated on Line 3 and Line 17, we scan D and compute its  X  I i (Line 10). If Lemma 1 is satisfied, then I is put to the result (Lines 11-13). However, Algorithm 1 : Apriori-based PFI Mining if Corollary 1 is satisfied, I is pruned from the candidate itemsets (Lines 14-16). This process goes on until no more candidates itemsets are found.
 Complexity. In Algorithm 1, each candidate item needs O ( n ) time to test whether it is a PFI. This is much faster than the Apriori [25], which verifies a PFI in O ( n Moreover, since D is scanned once for all k -PFI candidates C , at most a total of n tuples is retrieved for each C k (in-stead of | C k | X  n ). The space complexity is O ( | C k candidate set C k , in order to maintain  X  I for each candidate.
Besides mining threshold-based PFIs, the s-pmf approx-imation methods presented in Section 4 can also facilitate the discovery of rank-based PFIs (i.e., PFIs returned based on their relative rankings). In this section we investigate an adaptation of our methods for finding top-k PFIs, a kind of rank-based PFIs which orders PFIs according to their fre-quentness probabilities.
 Our solution (Algorithm 2) follows the framework in [25]: A bounded priority queue, Q , is maintained to store candi-date itemsets that can be top-k PFIs, in descending order of their frequentness probabilities. Initially, Q has a capacity of k itemsets, and single items with the k highest proba-bilities are deposited into it. Then, the algorithm iterates itself k times. During each iteration, the first itemset I is popped from Q , which has the highest frequentness proba-bility among those in Q . Based on Theorem 1, I must also rank higher than itemsets that are supersets of I . Hence, I is one of the top-k PFIs. A generation step is then performed, which produces candidate itemsets based on I . Next, a test-ing step is carried out, to determine which of these itemsets should be put to Q , with Q  X  X  capacity decremented. The top-k PFIs are produced after k iterations.

We now explain how the generation and testing steps in [25] can be redesigned, in respectively Sections 6.1 and 6.2. Given an itemset I retrieved from Q , in [25] a candidate Algorithm 2 : top-k PFI Mining itemset is generated from I by unioning I with every single item in V . Hence, the maximum number of candidate item-sets generated is | V | , which is the number of single items.
We argue that the number of candidate itemsets can ac-tually be fewer, if the contents of Q are also considered. To illustrate this, suppose Q = {{ abc } , { bcd } , { efg }} , and I = { abc } has the highest frequentness probability. If the gen-eration step of [25] is used, then four candidates are gen-erated for I : {{ abcd } , { abce } , { abcf } , { abcg }} ). However, Pr freq ( { abce } )  X  Pr freq ( { bce } ), according to Theorem 1. Since { bce } is not in Q , { bce } must also be not a top-k PFI. Using Theorem 1, we can immediately conclude that { abce } , a superset of { bce } , cannot be a top-k PFI. Hence, { abce } cannot be a top-k PFI candidate. Using similar arguments, { abcf } and { abcg } do not need to be generated, either. In this example, only { abcd } should be a top-k PFI candidate.
Algorithm. Based on this observation, we redesign the generation step (Line 9 of Algorithm 2) as follows: for any itemsets I 0  X  Q , if I and I 0 contain the same number of items, and there is only one item that differentiates I from I , we generate a candidate itemset I  X  I 0 . This guarantees that I  X  I 0 contains items from both I and I 0 .

Since Q has at most k itemsets, the new algorithm gener-ates at most k candidates. In practice, the number of single items ( | V | ) is large compared with k . For example, in the ac-cidents Dataset used in our experiments, | V | = 572. Hence, our algorithm generates fewer candidates, and significantly improves the performance of the solution in [25], as shown in our experimental results.
After the set of candidate itemsets, C , is generated, [25] performs testing on them by first computing their exact fre-quentness probabilities, then comparing with the minimal frequentness probability, Pr min , for all itemsets in Q . Only those in C with probabilities larger than Pr min are added to Q . As explained before, computing frequentness probabili-ties can be costly. Here we propose a faster solution based on the results in Section 4. The main idea is that instead of ranking the itemsets in Q based on their frequentness probabilities, we order them according to their  X  I Also, for each I 0  X  C , instead of computing Pr freq ( I evaluate  X  I 0 . These values are then compared with  X  the minimum value of  X  I among the itemsets stored in Q . Only candidates whose  X  I 0 s are not smaller than  X  min are placed into Q . An itemset I 0 selected in this way has a good chance to satisfy Pr freq ( I 0 )  X  Pr min , according to Theo-rem 2, as well as being a top-k PFI. Hence, by comparing the  X  I values, we avoid computing any frequentness proba-bility values.

Complexity. The details of the above discussions are shown in Algorithm 2. As we can see,  X  I 0 , computed for every itemset I 0  X  C , is used to determine whether I 0 should be put into Q . During each iteration, the time complexity is O ( n + k ). The overall complexity of algorithm is O ( kn + k ). This is generally faster than the solution in [25], whose complexity is O ( kn 2 + k | V | ).

We conclude this section with an example. Suppose we wish to find top-2 PFIs from the dataset in Table 1. There are four single items, and the initial capacity of Q is k = 2. Suppose that after computing the  X  I s of single items, { food } and { clothing } have the highest values. Hence, they are put to Q (Table 5). In the first iteration, { food } , the PFI with the highest  X  I is returned as the top-1 PFI. Based on our generation step, { food, clothing } is the only candidate. How-to the minimum  X  I among itemsets in Q . Hence, { food, clothing } is not put to Q . Moreover, the capacity of Q is reduced to one. Finally, { clothing } is popped from Q and returned.
 We now present the experimental results on two datasets. The first one, called accidents , comes from the Frequent Itemset Mining (FIMI) Dataset Repository 1 . This dataset is obtained from the National Institute of Statistics (NIS) for the region of Flanders (Belgium), for the period of 1991 X  2000. The data are obtained from the  X  X elgian Analysis Form for Traffic Accidents X , which are filled out by a police officer for each traffic accident occurring on a public road in Belgium. The dataset contains 340,184 accident records, with a total of 572 attribute values. On average, each record has 45 attributes.

We use the first 10k tuples as our default dataset. The default value of minsup is 20% of the database size n .
The second dataset, called T10I4D100k , is produced by the IBM data generator 2 . The dataset has a size n of 100k transactions. On average, each transaction has 10 items, and a frequent itemset has four items. Since this dataset is relatively sparse, we set minsup to 1% of n .

For both datasets, we consider both attribute and tuple uncertainty models. For attribute uncertainty, the existen-tial probability of each attribute is drawn from a Gaussian distribution with mean 0.5 and standard deviation 0.125. This same distribution is also used to characterize the ex-istential probability of each tuple, for the tuple uncertainty model. The default values of minprob and k are 0.4 and 10 respectively. In the results presented, minsup is shown as http://fimi.cs.helsinki.fi/ http://www.almaden.ibm.com/cs/disciplines/iis/ a percentage of the dataset size n . Notice that when the values of minsup or minprob are large, no PFIs can be re-turned; we do not show the results for these values. Our experiments were carried out on the Windows XP operating system, on a machine with a 2.66 GHz Intel Core 2 Duo processor and 2GB memory. The programs were written in C and compiled with Microsoft Visual Studio 2008.

We first present the results on the real dataset. Sec-tions 7.1 and 7.2 describe respectively the results for min-ing threshold-and rank-based PFIs, on attribute-uncertain data. We summarize the results for tuple-uncertain data and synthetic data, in Section 7.3. We now compare the performance of three threshold-based PFI mining algorithms mentioned in this paper: (1) DP , the Apriori algorithm used in [25]; (2) MB , the modified Apri-ori algorithm that employs the PFI testing method (Sec-tion 5.1); and (3) MBP , the algorithm that uses the improved version of the PFI testing method (Section 5.2). (i) Accuracy. Since MB approximates s-pmf by a Poisson distribution, we first examine its accuracy with respect to DP , which yields PFIs based on exact frequentness probabilities. Here, we use the standard recall and precision measures [5], which quantify the number of negatives and false positives. Specifically, let F DP be the set of PFIs generated by DP , and F
MB be the set of PFIs produced by MB . Then, the recall and the precision of MB , relative to DP , can be defined as follows: In these formulas, both recall and precision have values be-tween 0 and 1. Also, a higher value reflects a better accuracy.
Table 6 shows the recall and the precision of MB , for a wide range of minsup , n and minprob values. As we can see, the precision and recall values are always higher than 98%. Hence, the PFIs returned by MB are highly similar to those returned by DP . Since MBP returns the same PFIs as MB , it is also highly accurate. (ii) MB vs. DP . Next, we compare the performance (in log scale) of MB and DP , in Figure 2(a). Observe that MB is about two orders of magnitude faster than DP , over a wide range of minsup . This is because MB does not compute exact frequent-ness probabilities as DP does; instead, MB only computes the  X 
I values, which can be obtained faster. We also notice that the running times of both algorithms decrease with a higher minsup . This is explained by Figure 2(b), which shows that the number of PFIs generated by the two algorithms, | PFI | , decreases as minsup increases. Thus, the time required to compute the frequentness probabilities of these itemsets de-creases. We can also see that | PFI | is almost the same for the two algorithms, reflecting that the results returned by MB closely resemble those of DP.

Figure 2(c) examines the performance of MB and DP (in log scale) over different minprob values. Their execution times drop by about 6% when minprob changes from 0.1 to 0.9. We see that MB is faster than DP . For instance, at minprob = 0 . 5, MB needs 0.3 seconds, while DP requires 118 seconds, delivering an almost 400-fold performance improvement. (iii) MB vs. MBP . We then examine the benefit of using the improved PFI testing method ( MBP ) over the basic one ( MB ). Figure 3(a) shows that MBP runs faster than MB over differ-ent minsup values. For instance, when minsup = 0 . 5, MBP has about 25% of performance improvement. Moreover, as minsup increases, the performance gap increases. This can be explained by Figure 3(b), which presents the fraction of the database scanned by the two algorithms. When minsup increases, MBP examines a smaller fraction of the database. For instance, at minsup = 0 . 5, MBP scans about 80% of the database. This reduces the I/O cost and the effort for in-terpreting the retrieved tuples. Thus, MBP performs better than MB . (iv) Scalability. Figure 4(a) examines the scalability of the three algorithms. Both MB and MBP scale well with n . The performance gap between MB / MBP and DP also increases with n . At n = 20 k , MB and DP need 0.62 seconds and 657.7 seconds respectively; at n = 100 k , MB finishes in 3.1 seconds while DP spends 10 hours. Hence, the scalability of our approaches is better than that of DP . (v) Existential probability. We also examine the effect of using different distributions to characterize an attribute X  X  probability, in Figure 4(b). We use Un to denote a uniform distribution, and G i ( i = 1 ,..., 4) to represent a Gaussian Figure 4: Other Results for Threshold-Based PFIs distribution. The details of these distributions are shown in Table 7. We observe that MB and MBP perform better than DP over different distributions. All algorithms run compara-tively slower on G 1 . This is because G 1 has high mean (0.8) and low standard deviation (0.125), which generates high existential probability values. As a result, many candidates and PFIs are generated. Also note that G 3 and Un , which have the same mean and standard deviation, yield similar performance. Lastly, we found that the precision and the re-call of MB and MBP over these distributions are the same, and are close to 1. Hence, the PFIs retrieved by our methods closely resemble those returned by DP .

Table 7: Existential Probability (Experiment (v))
We now compare the first top-k solution proposed in [25] (called top-k ) and our enhanced algorithm (called E-top-k ). (vi) Accuracy. We measure the recall and precision of E-top-k relative to that of top-k , by using formulas similar to Equation 16 and 17. Table 8 shows the recall and the precision of E-top-k for a wide range of n , k and minsup values. We observe that the recall values are equal to the precision values. This is because number of PFIs returned by top-k and E-top-k are the same. As We can also see the recall and precision values are always higher than 98%. Hence, E-top-k can accurately return top-k PFIs. (vii) Performance. Figures 5(a), (b), and (c) com-pare the two algorithms under different values of n , k , and minsup . Similar to the case of threshold-based PFIs, our solution runs much faster than top-k . When n = 10 k , for Top-k
E-Top-k example, E-top-k finishes in only 0.2 seconds, giving an al-most 20000-fold improvement over that of top-k , which com-pletes in 1.1 hours. In Figure 5(a), we see that the runtime of top-k increases faster than that of E-top-k with a bigger database. In Figure 5(b), observe that the E-top-k is about four orders of magnitude faster than top-k .In Figure 5(c), with the increase of minsup , top-k needs more time, but the runtime of E-top-k only slightly increases. This is because (1) fewer candidates are produced by our generation step; and (2) the testing step is significantly improved by using our model-based methods.
We have also performed experiments on the tuple uncer-tainty model and the synthetic dataset. Since they are simi-lar to the results presented above, we only describe the most representative ones. For the accuracy aspect, the recall and precision values of approximate results on these datasets are still higher than 98%. Thus, our approaches can return ac-curate results.
 Tuple uncertainty. We compare the performance of DP , TODIS , MB and MBP in Figure 6(a). TODIS [24] is pro-posed to retrieve threshold-based PFIs from tuple-uncertain databases. It shows that both MB and MBP perform much better than DP and TODIS , under different minsup values. For example, when minsup = 0 . 3, MB needs 1.6 seconds, but DP and TODIS complete in 581 and 81 seconds respectively. In Figure 6(b), we also see that E-top-k consistently out-performs top-k by about two orders of magnitude. This means that our approach, which avoids computing frequent-ness probabilities, also works well for the tuple uncertainty model.

Synthetic Dataset. Finally, we run the experiments on the synthetic dataset. Figure 7(a) compares the performance of MB , MBP and DP , for attribute uncertainty model. We found that MB and MBP still outperform DP . Figure 7(b) tests the performance of top-k and E-top-k , for tuple uncertainty model. Again, E-top-k runs faster than top-k , by around two orders of magnitude. Thus, our approach also works well for this dataset.
We propose a model-based approach to discover PFIs from uncertain databases. Our method represents the s-pmf of a PFI as some probability models, in order to find PFIs quickly. This method can efficiently extract threshold-and rank-based PFIs. It also supports attribute and tuple un-certainty models, which are two common data models. We develop new approximation methods to evaluate frequent-ness probabilities efficiently. As shown theoretically and ex-perimentally, our algorithms are more efficient and scalable than existing ones. They are also highly accurate. In the future, we will examine how the model-based approach can be used to handle other mining problems (e.g., clustering) in uncertain databases. We will also study how other approx-imation techniques, such as sampling, can be used to mine PFIs.
This work was supported by the Research Grants Coun-cil of Hong Kong (GRF Projects 513508 and 711309E). We would like to thank the anonymous reviewers for their in-sightful comments. [1] A. Deshpande et al. Model-driven data acquisition in [2] C. Aggarwal, Y. Li, J. Wang, and J. Wang. Frequent [3] C. Aggarwal and P. Yu. A survey of uncertain data [4] R. Agrawal, T. Imieli  X nski, and A. Swami. Mining [5] C. J. van Rijsbergen. Information Retrieval .
 [6] L. L. Cam. An approximation theorem for the Poisson [7] H. Cheng, P. Yu, and J. Han. Approximate frequent [8] R. Cheng, D. Kalashnikov, and S. Prabhakar.
 [9] C. K. Chui, B. Kao, and E. Hung. Mining frequent [10] G. Cormode and M. Garofalakis. Sketching [11] N. Dalvi and D. Suciu. Efficient query evaluation on [12] J. Han, J. Pei, and Y. Yin. Mining frequent patterns [13] J. Huang et al. MayBMS: A Probabilistic Database [14] J. Ren and S. Lee and X. Chen and B. Kao and R. [15] R. Jampani, L. Perez, M. Wu, F. Xu, C. Jermaine, [16] N. Khoussainova, M. Balazinska, and D. Suciu. [17] H. Kriegel and M. Pfeifle. Density-based clustering of [18] C. Kuok, A. Fu, and M. Wong. Mining fuzzy [19] A. Lu, Y. Ke, J. Cheng, and W. Ng. Mining vague [20] M. Mutsuzaki et al. Trio-one: Layering uncertainty [21] M. Yiu et al. Efficient evaluation of probabilistic [22] P. Sistla et al. Querying the uncertain position of [23] C. Stein. Approximate Computation of Expectations. [24] L. Sun, R. Cheng, D. W. Cheung, and J. Cheng. [25] T. Bernecker et al. Probabilistic frequent itemset [26] T. Jayram et al. Avatar information extraction [27] S. Tsang, B. Kao, K. Y. Yip, W. Ho, and S. Lee. [28] Q. Zhang, F. Li, and K. Yi. Finding frequent items in In Section 4.2, we use Poisson distribution to approximate Poisson binomial distribution. Here, we summarize the re-sults of this approximation quality, discussed in [23]. Let X 1 ,X 2 ,...,X n be a set of Poisson trials such that Pr ( X j = 1) = p j and X = P n j =1 X j . Then X follows a Pois-son binomial distribution. Suppose  X  = E [ X ] = P n j =1 The probability of X = i and X  X  i can be approximated by probability distribution function ( pdf ) and cumulative distribution function ( cdf ) of Poisson distribution, [23] gives an upper bound of the error of the approxima-tion: for i = 0 , 1 , 2 ,...,n where  X   X  1  X  1 = min (  X   X  1 , 1).
Now, we want to compute a bound on expression on the right hand side. Since  X  = P n j =1 p j ,
Obviously the above expression is greater than or equal to 0.

When 0  X  P n j =1 p j  X  1,
So, in either case:
The upper bound of the error is very small. As also ver-ified by our experiments, the Poisson binomial distribution can be approximated well.
