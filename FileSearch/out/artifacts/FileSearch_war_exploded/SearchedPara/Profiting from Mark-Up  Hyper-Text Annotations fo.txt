 Unsupervised learning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question an-swering to speech recognition and machine trans-lation. A restricted version of this problem that tar-gets dependencies and assumes partial annotation  X  sentence boundaries and part-of-speech (POS) tagging  X  has received much attention. Klein and Manning (2004) were the first to beat a sim-ple parsing heuristic, the right-branching baseline; today X  X  state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM.
Pereira and Schabes (1992) outlined three ma-jor problems with classic EM, applied to a related problem, constituent parsing. They extended clas-sic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdetermined by unannotated text; (ii) fewer iterations needed to reach a good grammar, coun-tering convergence properties that sharply deterio-rate with the number of non-terminal symbols, due to a proliferation of local maxima; and (iii) better (in the best case, linear) time complexity per it-eration, versus running time that is ordinarily cu-bic in both sentence length and the total num-ber of non-terminals, rendering sufficiently large grammars computationally impractical. Their al-gorithm sometimes found good solutions from bracketed corpora but not from raw text, sup-porting the view that purely unsupervised, self-organizing inference methods can miss the trees for the forest of distributional regularities. This was a promising break-through, but the problem of whence to get partial bracketings was left open.
We suggest mining partial bracketings from a cheap and abundant natural language resource: the hyper-text mark-up that annotates web-pages. For example, consider that anchor text can match lin-guistic constituents, such as verb phrases, exactly: To validate this idea, we created a new data set, novel in combining a real blog X  X  raw HTML with tree-bank-like constituent structure parses, gener-ated automatically. Our linguistic analysis of the most prevalent tags (anchors, bold, italics and un-derlines) over its 1M + words reveals a strong con-nection between syntax and mark-up (all of our examples draw from this corpus), inspiring several simple techniques for automatically deriving pars-ing constraints. Experiments with both hard and more flexible constraints, as well as with different styles and quantities of annotated training data  X  the blog, web news and the web itself, confirm that mark-up-induced constraints consistently improve (otherwise unsupervised) dependency parsing. It is natural to expect hidden structure to seep through when a person annotates a sentence. As it happens, a non-trivial fraction of the world X  X  pop-ulation routinely annotates text diligently, if only vary font sizes, and toggle colors and styles, using mark-up technologies such as HTML and XML.

As noted, web annotations can be indicative of phrase boundaries, e.g., in a complicated sentence: In doing so, mark-up sometimes offers useful cues even for low-level tokenization decisions: Above, a backward quote in an Arabic name con-with the broken noun phrase, signals cohesion, and moreover sheds light on the internal structure of a compound. As Vadas and Curran (2007) point out, such details are frequently omitted even from manually compiled tree-banks that err on the side of flat annotations of base-NPs.

Admittedly, not all boundaries between HTML tags and syntactic constituents match up nicely: Combining parsing with mark-up may not be straight-forward, but there is hope: even above, one of each nested tag X  X  boundaries aligns; and Toronto Star  X  X  neglected determiner could be for-given, certainly within a dependency formulation. Our idea is to implement the DMV (Klein and Manning, 2004)  X  a standard unsupervised gram-mar inducer. But instead of learning the unan-notated test set, we train with text that contains web mark-up, using various ways of converting HTML into parsing constraints. We still test on WSJ (Marcus et al., 1993), in the standard way, and also check generalization against a hidden data set  X  the Brown corpus (Francis and Kucera, 1979). Our parsing constraints come from a blog  X  a new corpus we created, the web and news (see Table 1 for corpora X  X  sentence and token counts).
To facilitate future work, we make the final models and our manually-constructed blog data to share larger-scale resources, our main results should be reproducible, as both linguistic analysis and our best model rely exclusively on the blog. Table 1: Sizes of corpora derived from WSJ and Brown, as well as those we collected from the web. The appeal of unsupervised parsing lies in its abil-ity to learn from surface text alone; but (intrinsic) evaluation still requires parsed sentences. Follow-ing Klein and Manning (2004), we begin with ref-erence constituent parses and compare against de-terministically derived dependencies: after prun-ing out all empty subtrees, punctuation and ter-minals (tagged # and $ ) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic  X  X ead-percolation X  rules (Collins, 1999) to convert the rest, as is standard practice. Our primary reference sets are derived from the Penn English Treebank X  X  Wall Street Journal por-tion (Marcus et al., 1993): WSJ45 (sentences with fewer than 46 tokens) and Section 23 of WSJ  X  (all sentence lengths). We also evaluate on Brown100, similarly derived from the parsed portion of the Brown corpus (Francis and Kucera, 1979). While we use WSJ45 and WSJ15 to train baseline mod-els, the bulk of our experiments is with web data. 4.1 A News-Style Blog: Daniel Pipes Since there was no corpus overlaying syntactic structure with mark-up, we began constructing a style blog. Although limited to a single genre  X  political opinion, danielpipes.org is clean, consis-tently formatted, carefully edited and larger than WSJ (see Table 1). Spanning decades, Pipes X  editorials are mostly in-domain for POS taggers and tree-bank-trained parsers; his recent (internet-era) entries are thoroughly cross-referenced, con-veniently providing just the mark-up we hoped to
After extracting moderately clean text and mark-up locations, we used MxTerminator (Rey-nar and Ratnaparkhi, 1997) to detect sentence boundaries. This initial automated pass begot mul-tiple rounds of various semi-automated clean-ups that involved fixing sentence breaking, modifying parser-unfriendly tokens, converting HTML enti-ties and non-ASCII text, correcting typos, and so on. After throwing away annotations of fractional &lt;i&gt; Sesame Street &lt;/i&gt; -like), we broke up all mark-up that crossed sentence boundaries (i.e., loosely tags left covering entire sentences.

We finalized two versions of the data: BLOG tagged with the Stanford tagger (Toutanova and BLOG son for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (and non-standard AUX [ G ]) POS sequences from interfer-4.2 Scaled up Quantity : The (English) Web We built a large (see Table 1) but messy data set, WEB  X  English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system. We kept only sentence-like runs of words (satisfying punctuation and capitalization constraints), POS-tagged with TnT (Brants, 2000). 4.3 Scaled up Quality : (English) Web News In an effort to trade quantity for quality, we con-structed a smaller, potentially cleaner data set, NEWS. We reckoned editorialized content would lead to fewer extracted non-sentences. Perhaps surprisingly, NEWS is less than an order of magni-tude smaller than WEB (see Table 1); in part, this is due to less aggressive filtering  X  we trust sites In all other respects, our pre-processing of NEWS pages was identical to our handling of WEB data. Is there a connection between mark-up and syn-tactic structure? Previous work (Barr et al., 2008) has only examined search engine queries, show-ing that they consist predominantly of short noun phrases. If web mark-up shared a similar char-acteristic, it might not provide sufficiently dis-ambiguating cues to syntactic structure: HTML tags could be too short (e.g., singletons like  X  X lick &lt;a&gt; here &lt;/a&gt;  X ) or otherwise unhelpful in re-solving truly difficult ambiguities (such as PP-attachment). We began simply by counting vari-ous basic events in BLOG Table 3: Top 50% of marked POS tag sequences.
Table 4: Top 99% of dominating non-terminals. 5.1 Surface Text Statistics Out of 57,809 sentences, 6,047 (10.5%) are anno-tated (see Table 2); and 4,934 (8.5%) have multi-token bracketings. We do not distinguish HTML tags and track only unique bracketing end-points within a sentence. Of these, 6,015 are multi-token
As expected, many of the annotated words are nouns, but there are adjectives, verbs and other parts of speech too (see Table 3). Mark-up is short, typically under five words, yet (by far) the most frequently marked sequence of POS tags is a pair. 5.2 Common Syntactic Subtrees For three-quarters of all mark-up, the lowest domi-nating non-terminal is a noun phrase (see Table 4); there are also non-trace quantities of verb phrases (12.9%) and other phrases, clauses and fragments.
Of the top fifteen  X  35.2% of all  X  annotated productions, only one is not a noun phrase (see Ta-ble 5, left). Four of the fifteen lowest dominating non-terminals do not match the entire bracketing  X  all four miss the leading determiner, as we saw earlier. In such cases, we recursively split internal nodes until the bracketing aligned, as follows:
We can summarize productions more compactly by using a dependency framework and clipping off any dependents whose subtrees do not cross a bracketing boundary, relative to the parent. Thus, Viewed this way, the top fifteen (now collapsed) productions cover 59.4% of all cases and include four verb heads, in addition to a preposition and an adjective (see Table 5, right). This exposes five cases of inexact matches, three of which involve neglected determiners or adjectives to the left of the head. In fact, the only case that cannot be ex-plained by dropped dependents is #8, where the daughters are marked but the parent is left out. Most instances contributing to this pattern are flat NPs that end with a noun, incorrectly assumed to be the head of all other words in the phrase, e.g., As this example shows, disagreements (as well as agreements) between mark-up and machine-generated parse trees with automatically perco-5.3 Proposed Parsing Constraints The straight-forward approach  X  forcing mark-up to correspond to constituents  X  agrees with Char-niak X  X  parse trees only 48.0 % of the time, e.g., This number should be higher, as the vast major-ity of disagreements are due to tree-bank idiosyn-crasies (e.g., bare NPs). Earlier examples of in-complete constituents (e.g., legitimately missing determiners) would also be fine in many linguistic theories (e.g., as N-bars). A dependency formula-tion is less sensitive to such stylistic differences.
We begin with the hardest possible constraint on dependencies, then slowly relax it. Every example used to demonstrate a softer constraint doubles as a counter-example against all previous versions.  X  strict  X  seals mark-up into attachments, i.e., inside a bracketing, enforces exactly one external arc  X  into the overall head. This agrees with head-percolated trees just 35.6 % of the time, e.g.,  X  loose  X  same as strict , but allows the bracket-ing X  X  head word to have external dependents. This relaxation already agrees with head-percolated de-pendencies 87.5 % of the time, catching many (though far from all) dropped dependents, e.g.,  X  sprawl  X  same as loose , but now allows all words inside a bracketing to attach external de-percolated trees to 95.1 %, handling new cases, e.g., where  X  Toronto Star  X  is embedded in longer mark-up that includes its own parent  X  a verb:  X  tear  X  allows mark-up to fracture after all, requiring only that the external heads attaching the pieces lie to the same side of the bracketing. This propels agreement with percolated dependencies to 98.9 %, fixing previously broken PP-attachment ambiguities, e.g., a fused phrase like  X  X ox News in Canada X  that detached a preposition from its verb: Most of the remaining 1.1% of disagreements are due to parser errors. Nevertheless, it is possible for mark-up to be torn apart by external heads from both sides. We leave this section with a (very rare) true negative example. Below,  X  X SA X  modifies  X  X uthority X  (to its left), appositively, while  X  X l-We implemented the DMV (Klein and Manning, 2004), consulting the details of (Spitkovsky et al., 2010a). Crucially, we swapped out inside-outside re-estimation in favor of Viterbi training. Not only is it better-suited to the general problem (see  X  7.1), but it also admits a trivial implementation of (most Figure 1: Sentence-level cross-entropy on WSJ15 for Ad-Hoc  X  initializers of WSJ { 1 Six settings parameterized each run:  X  INIT : 0  X  default, uniform initialization; or  X  a high quality initializer, pre-trained using Ad-Hoc  X  (Spitkovsky et al., 2010a): we chose the Laplace-smoothed model trained at WSJ15 (the  X  X weet spot X  data gradation) but initialized off WSJ8, since that ad-hoc harmonic initializer has the best cross-entropy on WSJ15 (see Figure 1).  X  GENRE : 0  X  default, baseline training on WSJ; else, uses 1  X  BLOG  X  SCOPE : 0  X  default, uses all sentences up to length 45; if 1 , trains using sentences up to length 15; if 2 , re-trains on sentences up to length 45, starting from the solution to sentences up to length 15, as recommended by Spitkovsky et al. (2010a).  X  CONSTR : if 4 , strict ; if 3 , loose ; and if 2 , sprawl . We did not implement level 1 , tear . Over-constrained sentences are re-attempted at succes-sively lower levels until they become possible to parse, if necessary at the lowest (default) level 0 . 15  X  TRIM : if 1 , discards any sentence without a sin-gle multi-token mark-up (shorter than its length).  X  ADAPT : if 1 , upon convergence, initializes re-training on WSJ45 using the solution to &lt;GENRE&gt; , attempting domain adaptation (Lee et al., 1991). These make for 294 meaningful combinations. We judged each one by its accuracy on WSJ45, using standard directed scoring  X  the fraction of correct dependencies over randomized  X  X est X  parse trees. Evaluation on Section 23 of WSJ and Brown re-veals that blog-training beats all published state-of-the-art numbers in every traditionally-reported length cutoff category, with news-training not far behind. Here is a mini-preview of these results, for Section 23 of WSJ10 and WSJ  X  (from Table 8): Table 6: Directed accuracies on Section 23 of WSJ { 10 tems and our best runs (as judged against WSJ45) for NEWS and BLOG Since our experimental setup involved testing nearly three hundred models simultaneously, we must take extreme care in analyzing and interpret-ing these results, to avoid falling prey to any loom-large pool of models, where each is trained using a randomized and/or chaotic procedure (such as ours), the best may look good due to pure chance. We appealed to three separate diagnostics to con-vince ourselves that our best results are not noise. The most radical approach would be to write off WSJ as a development set and to focus only on the results from the held-out Brown corpus. It was ini-tially intended as a test of out-of-domain general-ization, but since Brown was in no way involved in selecting the best models, it also qualifies as a blind evaluation set. We observe that our best models perform even better (and gain more  X  see Table 8) on Brown than on WSJ  X  a strong indi-cation that our selection process has not overfitted. Our second diagnostic is a closer look at WSJ. Since we cannot graph the full (six-dimensional) set of results, we begin with a simple linear re-gression, using accuracy on WSJ45 as the depen-dent variable. We prefer this full factorial design to the more traditional ablation studies because it allows us to account for and to incorporate every single experimental data point incurred along the way. Its output is a coarse, high-level summary of our runs, showing which factors significantly con-tribute to changes in error rate on WSJ45: The default training mode (all parameters zero) is estimated to score 39.9%. A good initializer gives the biggest (double-digit) gain; both domain adap-tation and constraints also make a positive impact. Throwing away unannotated data hurts, as does training out-of-domain (the blog is least bad; the web is worst). Of course, this overview should not be taken too seriously. Overly simplistic, a first order model ignores interactions between parame-ters. Furthermore, a least squares fit aims to cap-ture central tendencies, whereas we are more in-terested in outliers  X  the best-performing runs.
A major imperfection of the simple regression model is that helpful factors that require an in-teraction to  X  X ick in X  may not, on their own, ap-pear statistically significant. Our third diagnostic is to examine parameter settings that give rise to the best-performing models, looking out for com-binations that consistently deliver superior results. 7.1 WSJ Baselines Just two parameters apply to learning from WSJ. Five of their six combinations are state-of-the-art, demonstrating the power of Viterbi training; only the default run scores worse than 45.0%, attained by Leapfrog (Spitkovsky et al., 2010a), on WSJ45: 7.2 Blog Simply training on BLOG The best runs use a good initializer, discard unan-notated sentences, enforce the loose constraint on the rest, follow up with domain adaptation and benefit from re-training  X  GENRE = TRIM = ADAPT = 1 : The contrast between unconstrained learning and annotation-guided parsing is higher for the default initializer, still using trimmed data sets (just over a thousand sentences for BLOG  X  Above, we see a clearer benefit to our constraints. 7.3 News Training on WSJ is also better than using NEWS: As with the blog, the best runs use the good initial-izer, discard unannotated sentences, enforce the loose constraint and follow up with domain adap-With all the extra training data, the best new score is just 49.5%. On the one hand, we are disap-pointed by the lack of dividends to orders of mag-nitude more data. On the other, we are comforted that the system arrives within 1% of its best result  X  50.4%, obtained with a manually cleaned up corpus  X  now using an auto-generated data set. 7.4 Web The WEB-side story is more discouraging: Our best run again uses a good initializer, keeps all sentences, still enforces the loose constraint and follows up with domain adaptation, but per-forms worse than all well-initialized WSJ base-lines, scoring only 45.9% (trained at WEB15).
We suspect that the web is just too messy for us. On top of the challenges of language iden-tification and sentence-breaking, there is a lot of boiler-plate; furthermore, web text can be difficult for news-trained POS taggers. For example, note that the verb  X  X ign X  is twice mistagged as a noun and that  X  X ouTube X  is classified as a verb, in the 7.5 The State of the Art Our best model gains more than 5% over previ-ous state-of-the-art accuracy across all sentences of WSJ X  X  Section 23, more than 8% on WSJ20 and rivals the oracle skyline (Spitkovsky et al., 2010a) on WSJ10; these gains generalize to Brown100, where it improves by nearly 10% (see Table 8).
We take solace in the fact that our best mod-els agree in using loose constraints. Of these, the models trained with less data perform better, with the best two using trimmed data sets, echo-ing that  X  X ess is more X  (Spitkovsky et al., 2010a), pace Halevy et al. (2009). We note that orders of magnitude more data did not improve parsing per-formance further and suspect a different outcome from lexicalized models: The primary benefit of additional lower-quality data is in improved cover-age. But with only 35 unique POS tags, data spar-sity is hardly an issue. Extra examples of lexical items help little and hurt when they are mistagged. The wealth of new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, anchor text and link data enhanced a variety of traditional NLP tech-niques: cross-lingual information retrieval (Nie and Chen, 2002), translation (Lu et al., 2004), both named-entity recognition (Mihalcea and Csomai, 2007) and categorization (Watanabe et al., 2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambigua-tion (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, can-didate core NLP tasks  X  tokenization, CJK seg-mentation, noun-phrase chunking, and (until now) parsing  X  remained conspicuously uninvolved.
Approaches related to ours arise in applications that combine parsing with named-entity recogni-tion (NER). For example, constraining a parser to respect the boundaries of known entities is stan-dard practice not only in joint modeling of (con-stituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) parsing. Although restricted to proper noun phrases, dates, times and quantities, we suspect that constituents identified by trained (supervised) NER systems would also Table 8: Accuracies on Section 23 of WSJ { 10 ,  X  } and Brown100 for three recent state-of-the-art be helpful in constraining grammar induction.
Following Pereira and Schabes X  (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Rie-zler et al., 2002) and also proved useful in mod-eling (Japanese) dependencies (Sassano, 2005). There was demand for partially bracketed corpora. Chen and Lee (1995) constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used
We combine the two intuitions, using the web to build a partially parsed corpus. Our approach could be called lightly-supervised , since it does not require manual annotation of a single complete parse tree. In contrast, traditional semi-supervised We explored novel ways of training dependency parsing models, the best of which attains 50.4% accuracy on Section 23 (all sentences) of WSJ, beating all previous unsupervised state-of-the-art by more than 5%. Extra gains stem from guid-ing Viterbi training with web mark-up, the loose constraint consistently delivering best results. Our linguistic analysis of a blog reveals that web an-notations can be converted into accurate parsing constraints ( loose : 88%; sprawl : 95%; tear : 99%) that could be helpful to supervised methods, e.g., by boosting an initial parser via self-training (Mc-Closky et al., 2006) on sentences with mark-up. Similar techniques may apply to standard word-processing annotations, such as font changes, and to certain (balanced) punctuation (Briscoe, 1994).
We make our blog data set, overlaying mark-up and syntax, publicly available. Its annotations are 75% noun phrases, 13% verb phrases, 7% simple declarative clauses and 2% prepositional phrases, with traces of other phrases, clauses and frag-ments. The type of mark-up, combined with POS tags, could make for valuable features in discrimi-native models of parsing (Ratnaparkhi, 1999).
A logical next step would be to explore the con-nection between syntax and mark-up for genres other than a news-style blog and for languages other than English. We are excited by the possi-bilities, as unsupervised parsers are on the cusp of becoming useful in their own right  X  re-cently, Davidov et al. (2009) successfully applied Seginer X  X  (2007) fully unsupervised grammar in-ducer to the problems of pattern-acquisition and extraction of semantic data. If the strength of the connection between web mark-up and syntactic structure is universal across languages and genres, this fact could have broad implications for NLP, with applications extending well beyond parsing.
