 Speakers rarely have difficulty correctly ordering modifiers such as adjectives, adverbs, or gerunds when describing some noun. The phrase  X  X eau-tiful blue Macedonian vase X  sounds very natural, whereas changing the modifier ordering to  X  X lue Macedonian beautiful vase X  is awkward (see Table 1 for more examples). In this work, we consider the task of ordering an unordered set of prenomi-nal modifiers so that they sound fluent to native lan-guage speakers. This is an important task for natural language generation systems.

Much linguistic research has investigated the se-mantic constraints behind prenominal modifier or-derings. One common line of research suggests that modifiers can be organized by the underlying semantic property they describe and that there is an ordering on semantic properties which in turn restricts modifier orderings. For instance, Sproat and Shih (1991) contend that the size property pre-cedes the color property and thus  X  X mall black cat X  sounds more fluent than  X  X lack small cat X . Using &gt; to denote precedence of semantic groups, some commonly proposed orderings are: quality &gt; size &gt; shape &gt; color &gt; provenance (Sproat and Shih, 1991), age &gt; color &gt; participle &gt; provenance &gt; noun &gt; denominal (Quirk et al., 1974), and value &gt; dimension &gt; physical property &gt; speed &gt; human propensity &gt; age &gt; color (Dixon, 1977). However, correctly classifying modifiers into these groups can be difficult and may be domain dependent or con-strained by the context in which the modifier is being used. In addition, these methods do not specify how to order modifiers within the same class or modifiers that do not fit into any of the specified groups.
There have also been a variety of corpus-based, computational approaches. Mitchell (2009) uses a class-based approach in which modifiers are grouped into classes based on which positions they prefer in the training corpus, with a predefined or-dering imposed on these classes. Shaw and Hatzi-vassiloglou (1999) developed three different ap-proaches to the problem that use counting methods and clustering algorithms, and Malouf (2000) ex-pands upon Shaw and Hatzivassiloglou X  X  work.
This paper describes a computational solution to the problem that uses relevant features to model the modifier ordering process. By mapping a set of features across the training data and using a maxi-mum entropy reranking model, we can learn optimal weights for these features and then order each set of modifiers in the test data according to our features and the learned weights. This approach has not been used before to solve the prenominal modifier order-ing problem, and as we demonstrate, vastly outper-forms the state-of-the-art, especially for sequences of longer lengths.

Section 2 of this paper describes previous compu-tational approaches. In Section 3 we present the de-tails of our maximum entropy reranking approach. Section 4 covers the evaluation methods we used, and Section 5 presents our results. In Section 6 we compare our approach to previous methods, and in Section 7 we discuss future work and improvements that could be made to our system. Mitchell (2009) orders sequences of at most 4 mod-ifiers and defines nine classes that express the broad positional preferences of modifiers, where position 1 is closest to the noun phrase (NP) head and posi-tion 4 is farthest from it. Classes 1 through 4 com-prise those modifiers that prefer only to be in posi-tions 1 through 4, respectively. Class 5 through 7 modifiers prefer positions 1-2, 2-3, and 3-4, respec-tively, while class 8 modifiers prefer positions 1-3, and finally, class 9 modifiers prefer positions 2-4. Mitchell counts how often each word type appears in each of these positions in the training corpus. If any modifier X  X  probability of taking a certain position is greater than a uniform distribution would allow, then it is said to prefer that position. Each word type is then assigned a class, with a global ordering defined over the nine classes.

Given a set of modifiers to order, if the entire set has been seen at training time, Mitchell X  X  sys-tem looks up the class of each modifier and then or-ders the sequence based on the predefined ordering for the classes. When two modifiers have the same class, the system picks between the possibilities ran-domly. If a modifier was not seen at training time and thus cannot be said to belong to a specific class, the system favors orderings where modifiers whose classes are known are as close to their classes X  pre-ferred positions as possible.

Shaw and Hatzivassiloglou (1999) use corpus-based counting methods as well. For a corpus with w word types, they define a w  X  w matrix where Count [ A, B ] indicates how often modifier A pre-cedes modifier B . Given two modifiers a and b to order, they compare Count [ a, b ] and Count [ b, a ] in their training data. Assuming a null hypothesis that the probability of either ordering is 0.5, they use a binomial distribution to compute the probability of seeing the ordering &lt;a,b&gt; for Count [ a, b ] num-ber of times. If this probability is above a certain threshold then they say that a precedes b . Shaw and Hatzivassiloglou also use a transitivity method to fill out parts of the Count table where bigrams are not actually seen in the training data but their counts can be inferred from other entries in the table, and they use a clustering method to group together modifiers with similar positional preferences.

These methods have proven to work well, but they also suffer from sparsity issues in the training data. Mitchell reports a prediction accuracy of 78.59% for NPs of all lengths, but the accuracy of her ap-proach is greatly reduced when two modifiers fall into the same class, since the system cannot make an informed decision in those cases. In addition, if a modifier is not seen in the training data, the system is unable to assign it a class, which also limits accu-racy. Shaw and Hatzivassiloglou report a highest ac-curacy of 94.93% and a lowest accuracy of 65.93%, but since their methods depend heavily on bigram counts in the training corpus, they are also limited in how informed their decisions can be if modifiers in the test data are not present at training time.
In this next section, we describe our maximum entropy reranking approach that tries to develop a more comprehensive model of the modifier ordering process to avoid the sparsity issues that previous ap-proaches have faced. We treat the problem of prenominal modifier or-dering as a reranking problem. Given a set B of prenominal modifiers and a noun phrase head H which B modifies, we define  X  ( B ) to be the set of all possible permutations, or orderings, of B . We sup-pose that for a set B there is some x  X   X   X  ( B ) which represents a  X  X orrect X  natural-sounding ordering of the modifiers in B .

At test time, we choose an ordering x  X   X  ( B ) us-ing a maximum entropy reranking approach (Collins and Koo, 2005). Our distribution over orderings x  X   X  ( B ) is given by: P ( x | H, B, W )= where  X  ( B, H, x ) is a feature vector over a particu-lar ordering of B and W is a learned weight vector over features. We describe the set of features in sec-tion 3.1, but note that we are free under this formu-lation to use arbitrary features on the full ordering x of B as well as the head noun H , which we implic-itly condition on throughout. Since the size of the set of prenominal modifiers B is typically less than six, enumerating  X  ( B ) is not expensive.

At training time, our data consists of sequences of prenominal orderings and their corresponding nom-inal heads. We treat each sequence as a training ex-ample where the labeled ordering x  X   X   X  ( B ) is the one we observe. This allows us to extract any num-ber of  X  X abeled X  examples from part-of-speech text. Concretely, at training time, we select W to maxi-mize:
L ( W )= where the first term represents our observed data likelihood and the second the 2 regularization, of  X  2 to 0 . 5 throughout. We optimize this objective using standard L-BFGS optimization techniques.
The key to the success of our approach is us-ing the flexibility afforded by having arbitrary fea-tures  X  ( B, H, x ) to capture all the salient elements of the prenominal ordering data. These features can be used to create a richer model of the modifier or-dering process than previous corpus-based counting approaches. In addition, we can encapsulate previ-ous approaches in terms of features in our model. Mitchell X  X  class-based approach can be expressed as a binary feature that tells us whether a given permu-ation satisfies the class ordering constraints in her model. Previous counting approaches can be ex-pressed as a real-valued feature that, given all n -grams generated by a permutation of modifiers, re-turns the count of all these n -grams in the original training data. 3.1 Feature Selection Our features are of the form  X  ( B, H, x ) as expressed in the model above, and we include both indica-tor features and real-valued numeric features in our model. We attempt to capture aspects of the modifier permutations that may be significant in the ordering process. For instance, perhaps the majority of words that end with -ly are adverbs and should usually be positioned farthest from the head noun, so we can define an indicator function that captures this feature as follows:  X  ( B, H, x )= We create a feature of this form for every possible modifier position i from 1 to 4.

We might also expect permutations that contain n -grams previously seen in the training data to be more natural sounding than other permutations that gener-ate n -grams that have not been seen before. We can express this as a real-valued feature:
See Table 2 for a summary of our features. Many of the features we use are similar to those in Dunlop et al. (2010), which uses a feature-based multiple se-quence alignment approach to order modifiers. 4.1 Data Preprocessing and Selection We extracted all noun phrases from four corpora: the Brown, Switchboard, and Wall Street Journal cor-pora from the Penn Treebank, and the North Amer-ican Newswire corpus (NANC). Since there were very few NPs with more than 5 modifiers, we kept those with 2-5 modifiers and with tags NN or NNS for the head noun. We also kept NPs with only 1 modifier to be used for generating &lt; modifier, head noun &gt; bigram counts at training time. We then fil-tered all these NPs as follows: If the NP contained a PRP , IN , CD , or DT tag and the corresponding modifier was farthest away from the head noun, we removed this modifier and kept the rest of the NP. If the modifier was not the farthest away from the head noun, we discarded the NP. If the NP contained a POS tag we only kept the part of the phrase up to this tag. Our final set of NPs had tags from the following list: JJ , NN , NNP , NNS , JJS , JJR , VBG , VBN , RB , NNPS , RBS . See Table 3 for a summary of the num-ber of NPs of lengths 1-5 extracted from the four corpora.

Our system makes several passes over the data during the training process. In the first pass, we collect statistics about the data, to be used later on when calculating our numeric features. To collect the statistics, we take each NP in the training data and consider all possible 2 -gms through 5 -gms that are present in the NP X  X  modifier sequence, allowing for non-consecutive n -grams. For example, the NP  X  X he beautiful blue Macedonian vase X  generates the following bi-grams: &lt; beautiful blue &gt; , &lt; blue Macedonian &gt; and &lt; beautiful Macedonian &gt; , along with the 3 -gram &lt; beautiful blue Macedonian &gt; . We keep a table mapping each unique n -gram to the number of times it has been seen in the training data. In addition, we also store a table that keeps track of bigram counts for &lt;M,H&gt; , where H is the head noun of an NP and M is the modifier clos-est to it. In the example  X  X he beautiful blue Mace-donian vase, X  we would increment the count of &lt; Macedonian , vase &gt; in the table. The n -gram and &lt;M,H&gt; counts are used to compute numeric fea-ture values. 4.2 Google n -gram Baseline The Google n -gram corpus is a collection of n -gram counts drawn from public webpages with a total of one trillion tokens  X  around 1 billion each of unique 3 -grams, 4 -grams, and 5 -grams, and around 300,000 unique bigrams. We created a Google n -gram base-line that takes a set of modifiers B , determines the Google n -gram count for each possible permutation in  X  ( B ) , and selects the permutation with the high-est n -gram count as the winning ordering x  X  .We will refer to this baseline as G OOGLE N -GRAM . 4.3 Mitchell X  X  Class-Based Ordering of Mitchell X  X  original system was evaluated using only three corpora for both training and testing data: Brown, Switchboard, and WSJ. In addition, the evaluation presented by Mitchell X  X  work considers a prediction to be correct if the ordering of classes in that prediction is the same as the ordering of classes in the original test data sequence, where a class refers to the positional preference groupings defined in the model. We use a more stringent evaluation as described in the next section.

We implemented our own version of Mitchell X  X  system that duplicates the model and methods but allows us to scale up to a larger training set and to apply our own evaluation techniques. We will refer to this baseline as C LASS B ASED . 4.4 Evaluation To evaluate our system (M AX E NT ) and our base-lines, we partitioned the corpora into training and testing data. For each NP in the test data, we gener-ated a set of modifiers and looked at the predicted orderings of the M AX E NT ,C LASS B ASED , and G
OOGLE N -GRAM methods. We considered a pre-dicted sequence ordering to be correct if it matches the original ordering of the modifiers in the corpus. We ran four trials, the first holding out the Brown corpus and using it as the test set, the second hold-ing out the WSJ corpus, the third holding out the Switchboard corpus, and the fourth holding out a randomly selected tenth of the NANC. For each trial we used the rest of the data as our training set. The M AX E NT model consistently outperforms C
LASS B ASED across all test corpora and sequence lengths for both tokens and types, except when test-ing on the Brown and Switchboard corpora for mod-ifier sequences of length 5, for which neither ap-proach is able to make any correct predictions. How-ever, there are only 3 sequences total of length 5 in the Brown and Swichboard corpora combined. M
AX E NT also outperforms the G OOGLE N -GRAM baseline for almost all test corpora and sequence lengths. For the Switchboard test corpus token and type accuracies, the G OOGLE N -GRAM base-line is more accurate than M AX E NT for sequences of length 2 and overall, but the accuracy of M AX -E NT is competitive with that of G OOGLE N -GRAM . If we examine the error reduction between M AX -E
NT and C LASS B ASED , we attain a maximum error reduction of 69.8% for the WSJ test corpus across modifier sequence tokens, and an average error re-duction of 59.1% across all test corpora for tokens. M
AX E NT also attains a maximum error reduction of 68.4% for the WSJ test corpus and an average error reduction of 41.8% when compared to G OOGLE N -It should also be noted that on average the M AX -E
NT model takes three hours to train with several hundred thousand features mapped across the train-ing data (the exact number used during each test run is listed in Table 4)  X  this tradeoff is well worth the increase we attain in system performance. M
AX E NT seems to outperform the C LASS B ASED baseline because it learns more from the training data. The C LASS B ASED model classifies each modifier in the training data into one of nine broad categories, with each category representing a differ-ent set of positional preferences. However, many of the modifiers in the training data get classified to the same category, and C LASS B ASED makes a random choice when faced with orderings of modifiers all in the same category. When applying C LASS B ASED to WSJ as the test data and training on the other cor-pora, 74.7% of the incorrect predictions contained at least 2 modifiers that were of the same positional preferences class. In contrast, M AX E NT allows us to learn much more from the training data. As a re-sult, we see much higher numbers when trained and tested on the same data as C LASS B ASED .

The G OOGLE N -GRAM method does better than the C LASS B ASED approach because it contains n -gram counts for more data than the WSJ, Brown, Switchboard, and NANC corpora combined. How-ever, G OOGLE N -GRAM suffers from sparsity issues as well when testing on less common modifier com-binations. For example, our data contains rarely heard sequences such as  X  X talian, state-owned, hold-ing company X  or  X  X rmed Namibian nationalist guer-rillas. X  While M AX E NT determines the correct or-dering for both of these examples, none of the per-mutations of either example show up in the Google n -gram corpus, so the G OOGLE N -GRAM method is forced to randomly select from the six possibilities. In addition, the Google n -gram corpus is composed of sentence fragments that may not necessarily be NPs, so we may be overcounting certain modifier permutations that can function as different parts of a sentence.

We also compared the effect that increasing the amount of training data has when using the C LASS B
ASED and M AX E NT methods by initially train-ing each system with just the Brown and Switch-board corpora and testing on WSJ. Then we incre-mentally added portions of NANC, one tenth at a time, until the training set included all of it. The re-sults (see Figure 1) show that we are able to benefit from the additional data much more than the C LASS B
ASED approach can, since we do not have a fixed set of classes limiting the amount of information the model can learn. In addition, adding the first tenth of NANC made the biggest difference in increasing accuracy for both approaches. The straightforward maximum entropy reranking approach is able to significantly outperform previous computational approaches by allowing for a richer model of the prenominal modifier ordering process. Future work could include adding more features to the model and conducting ablation testing. In addi-tion, while many sets of modifiers have stringent or-dering requirements, some variations on orderings, such as  X  X ormer famous actor X  vs.  X  X amous former actor, X  are acceptable in both forms and have dif-ferent meanings. It may be beneficial to extend the model to discover these ambiguities.

