 Kohei Ogawa ogawa.mllab.nit@gmail.com Motoki Imamura imamura.mllab.nit@gmail.com Ichiro Takeuchi takeuchi.ichiro@nitech.ac.jp Nagoya Institute of Technology, Nagoya, Japan Masashi Sugiyama sugi@cs.titech.ac.jp Tokyo Institute of Technology, Tokyo, Japan Semi-supervised learning, the paradigm of learning from labeled and unlabeled data, has been extensively studied in the last decade (Chapelle et al., 2006). The semi-supervised support vector machine (S 3 VM) or transductive SVM (Vapnik &amp; Sterin, 1977; Joachims, 1999) is one of the popular semi-supervised classifica-tion algorithms that inherits the large-margin concept of supervised SVMs (Boser et al., 1992; Vapnik, 1996; Cortes &amp; Vapnik, 1995). The basic idea of S 3 VM is to improve the supervised SVM solution (obtained only from labeled data) with the help of unlabeled data (Joachims, 1999). A key challenge for the success of S
VM is to optimally control how strongly the effect of unlabeled data is incorporated into a classifier. From an algorithmic point of view, S 3 VM tries to maximize the margin over both labeled and unlabeled data, and this is cast as either a combinatorial op-timization problem of assigning labels to unlabeled data (Joachims, 1999) or a non-convex optimization problem of maximizing the margin for unlabeled data (Collobert et al., 2006). As described in Chapelle et al. (2007), it is practically difficult to find the global optimal solution for large problems. For that rea-son, a great deal of effort has been made to obtain good sub-optimal solutions efficiently (Joachims, 1999; Sindhwani et al., 2006; Chapelle, 2007).
 It was pointed out in Chapelle et al. (2008) that most of the successful S 3 VM training algorithms proposed so far can actually be viewed as annealing (Korte &amp; Vygen, 2000; Hromkovic, 2001; Kirkpatrick &amp; Gelatt, 1983; Colorni et al., 1991). That is, starting from the original supervised SVM formulation, a sequence of sub-problems where the effect of unlabeled data is in-creasingly strengthned, is solved to obtain a final so-lution. However, in this annealing procedure, there is a trade-off between the number of annealing steps and the computation cost. Thus, enhancement of the annealing resolution is possible only at the expense of increasing the computation cost, which is a critical limitation in the current implementations of S 3 VM. The goal of this paper is to go beyond this trade-off: We propose a new training algorithm for S 3 VM that ef-ficiently performs annealing with an infinitesimal res-olution. Technically, our algorithm can be regarded as a non-trivial extension of the parametric programming (Allgower &amp; George, 1993; Best, 1996; Ritter, 1984; Efron &amp; Tibshirani, 2004; Hastie et al., 2004; Takeuchi et al., 2009; Karasuyama et al., 2012), and it gives a path of local optimal solutions when the effect of un-labeled data is continuously increased. Interestingly, through the analysis of necessary and sufficient con-ditions for the local optimality of S 3 VM, we find that the local solution path followed by the infinitesimal annealing steps is not continuous; a solution path ac-tually contains a finite number of abrupt jumps . Our algorithm can exactly identify such jumps and trace the entire path of local optimal solutions. To the best of our knowledge, this is technically a novel contribu-tion to the parametric programming community. Through experiments, we demonstrate that our in-finitesimal annealing algorithm tends to produce bet-ter solutions with less computation time than existing approaches. We review S 3 VM (Joachims, 1999) here. Suppose that we are given labeled instances f ( x i ; y i ) g i 2L and unla-beled instances f x i g i 2U , where x i 2 R d is an input vector and y i 2 f 1 ; 1 g is a class label. The decision function is learned in S 3 VM, where is a feature map, w are the parameters to learn, and &gt; denotes the transpose. In S 3 VM, the bias term b is usually fixed as b = 2 r 1 in order to satisfy a class-balance constraint of unla-beled instances, where r = 1 jLj Chapelle &amp; Zien (2005) for details.
 The problem of S 3 VM training is interpreted as a com-binatorial optimization problem (Joachims, 1999) or a non-convex optimization problem (Collobert et al., 2006). Below, we review both interpretations. S
VM as Combinatorial Problem: The basic idea is to learn the decision function and labels of unlabeled instances simultaneously to maximize the margin: min where  X  y 2 f 1 ; 1 g jUj is a vector of predicted labels of unlabeled instances and [1 z ] + is so-called the hinge loss function (see the left panel of Figure 1). C and C are regularization parameters for labeled and unlabeled instances, respectively. Because labeled instances are more reliable than unlabeled ones, they are chosen to satisfy C C .
 At the optimal solution of the minimization problem (1), the predicted labels should satisfy because, if one of the predicted labels,  X  y i , violates this condition, the objective function J ( f;  X  y ) can be strictly decreased by flipping it.
 Introducing this condition, we can rewrite the S 3 VM training criterion (1) as This formulation can be interpreted as a combinato-rial optimization problem of finding the best predicted label vector  X  y that minimizes J ( f;  X  y ) from all possible 2 jUj candidates in f 1 ; 1 g jUj .
 S
VM as Non-Convex Problem: If a predicted label  X  y i ; i 2 U is chosen to satisfy (2), we can elimi-problem (1) can be rewritten as Because the loss for unlabeled instances, [1 j f ( x ) j ] is non-convex as plotted in the right panel of Figure 1, (4) is a non-convex optimization problem. As explained in the previous section, S 3 VM has either a combinatorial or a non-convex nature. Thus, the practical goal of existing S 3 VM studies has been to develop an algorithm that can find a good local opti-mal solution (Joachims, 1999; Sindhwani et al., 2006; Chapelle, 2007).
 As pointed out in Chapelle et al. (2008), these exist-ing S 3 VM algorithms utilize the concept of annealing either explicitly or implicitly to find a local optimal so-lution: Starting from the supervised SVM ( C = 0), a sequence of sub-problems with increasing C is solved. In this annealing procedure, there is a trade-off be-tween the number of annealing steps and the compu-tation cost. This means that the annealing resolution can be enhanced only at the expense of increasing the computation cost, which is a critical limitation in the current implementations of S 3 VM.
 Our goal is to go beyond this limitation by developing an infinitesimal annealing algorithm for S 3 VM named S
VM path . The basic idea of S 3 VM path is to use a convex parametric programming technique (Allgower &amp; George, 1993; Gal, 1995; Best, 1996) for comput-ing the entire solution path of S 3 VM for C 2 [0 ; C ]. However, since S 3 VM is non-convex, our target is to compute a path of local optimal solutions.
 To this end, we need to characterize the properties of S
VM local optimal solutions. Given that we have a convex optimization problem defined in a convex poly-tope for fixed predicted labels  X  y (the inner optimiza-tion problem in (3)), we define the following notion: De nition 1 (Conditionally optimal solution) For a given  X  y 2 f 1 ; 1 g jUj , we refer to the optimal solution of the convex problem as the conditionally optimal solution in pol( X  y ) , where is the convex polytope defined by the constraints in (2). Roughly speaking, the solution space of S 3 VM consists of many such convex polytopes. As we will show in the next section, the S 3 VM solution space possesses the following two important properties: These two properties indicate that a path of S 3 VM lo-cal optimal solutions is inevitably discontinuous and contains a finite number of abrupt jumps . To cope with the discontinuity, the proposed S 3 VM path algo-rithm consists of the continuous path (CP) step and the discrete jump (DJ) step. More specifically, start-ing from C = 0, the S 3 VM path algorithm iterates the CP step (following the local optimal solution path in a polytope) and the DJ step (once the path reaches a boundary of the polytope, find another local opti-mal solution in the adjacent polytope) until C = C . Figure 4 illustrates the behavior of the algorithm, in which the red ones indicate the CP step, while the blue ones indicate the DJ step.
 In the next section, we formally discuss the properties of S 3 VM local optimal solutions stated above. Then we describe implementation details of the S 3 VM path algorithm in Section 5. In this section, we formally discuss the properties of S VM local optimal solutions.
 To begin with, the following proposition clarifies the relationship between conditionally optimal solutions and local optimal solutions: Proposition 2 Any local optimal solution f of S 3 VM is the conditionally optimal solution in pol( X  y ) , where  X  y satisfies  X  y i f ( x i ) 0 ; i 2U .
 This proposition is clear because, if a solution f is not conditionally optimal for  X  y , there exists a strictly bet-ter feasible solution in the neighborhood of f . Note that the converse is not always true, i.e., every con-ditionally optimal solution is not necessarily a local optimal solution of (4). Therefore, we need to clarify which conditionally optimal solutions are local opti-mal.
 Since each local optimal solution corresponds to one of the conditionally optimal solutions, the Lagrangian multiplier theory (Boyd &amp; Vandenberghe, 2004) im-plies that a local optimal solution can be written in the dual form as where K is the kernel function and f i g i 2L[U are the Lagrange multipliers. Note that, in the standard SVM, the 2nd term is usually written as Here, we augment i to include y i for notational sim-plicity.
 Then we have the following necessary and sufficient conditions for the local optimality of S 3 VM: Lemma 3 For C 2 (0 ; C ] , necessary and sufficient conditions for f to be local optimal are and all the constraints (2) are non-active , i.e., The proof of Lemma 3 is given in the supplementary. A non-trivial part of the local optimality conditions is the non-activeness condition (9). To clarify this, we rephrase Lemma 3 as follows: Theorem 4 A conditionally optimal solution in pol( X  y ) for a certain  X  y is a local optimal solution if and only if it is strictly in the interior of pol( X  y ) . This theorem is directly deduced from Lemma 3 be-cause (8) is a part of the KKT optimality conditions of the conditionally optimal solutions in pol( X  y ), while (9) indicates that the solution cannot be at the boundary of pol( X  y ) (see the proof of Lemma 3 in the supplemen-tary for details). Theorem 4 indicates that the path of f ^ y s is guaranteed to be local optimal as long as it stays in the interior of the convex polytope pol( X  y ), and the solution is not local optimal anymore when the path arrives at a boundary of the convex polytope (see Figure 2).
 In order to explain why a conditionally optimal solu-tion f ( X  y ) at the boundary cannot be a local optimal solution, let us consider the  X  X djacent X  convex poly-tope pol( X  y 0 ), where  X  y Then, as stated in the following theorem, the condi-tionally optimal solution f ^ y 0 in pol( X  y 0 ) is guaranteed to be a strictly better S 3 VM solution than f ^ y : Theorem 5 Let f ^ y be the conditionally optimal solu-tion in pol( X  y ) , and suppose that  X  y i f ^ y ( x i ) = 0 ; i holds for a non-empty set S U . If we define a new label vector  X  y 0 by (10), i.e., the labels  X  y i for i 2 S flipped, then the conditionally optimal solution f ^ y 0 on pol( X  y 0 ) satisfies i.e., f ^ y 0 is a strictly better S 3 VM solution than f This theorem can be proved by comparing the KKT optimality conditions of the two solutions f ^ y and f ^ y and showing that the former cannot be optimal in pol( X  y 0 ). Its detailed proof is given in the supplemen-tary.
 Theorem 5 shows that, after we flip the label as in (10), the new conditionally optimal solution f ^ y 0 in the adja-cent convex polytope pol( X  y 0 ) is strictly better than the previous one. This means that, once a solution path reaches a boundary of the current polytope, we can al-ways find a better feasible solution by computing the conditionally optimal solution in the polytope pol( X  y 0 ) (see Figure 3).
 Actually, Theorem 5 proves the  X  X nly if X  part of Theo-rem 4. That is, the strict improvement of the solution in Theorem 5 indicates that, if f ^ y is at the boundary of pol( X  y ), then it cannot be local optimal because there exists a strictly better feasible solution in the adjacent polytope pol( X  y 0 ). In this section, we describe implementation details of the S 3 VM path algorithm.
 The pseudo-code of the S 3 VM path algorithm is sum-marized in Algorithms 1, 2, and 3. In these pseudo-codes, we denote f [( C )] and  X  y [ C ] to represent a local optimal solution and the corresponding label vector at C , respectively. In addition, a path of local optimal solutions for C 2 [ C 0 ; C 1 ] is written as f [ C Entire Procedure (Algorithm 1): The S 3 VM path algorithm is initialized at C = 0 with the stan-dard supervised SVM trained only on labeled in-stances f ( x i ; y i ) g i 2L . The predicted labels  X  y unlabeled instances are initialized based on the sign function obtained by the initial SVM. After the ini-tialization, the algorithm enters the CP-step, where the path of conditionally optimal solutions f ^ y s is com-puted with increasing C by using a convex parametric programming technique.
 If the path arrives at a boundary of the convex poly-tope pol( X  y ), then, it exits from the CP-step, and enters the DJ-step. In the DJ-step, C is fixed, and a bet-ter feasible solution is sought for by computing the conditionally optimal solution f ^ y 0 after the predicted labels for i 2 f i 2 Uj f [ C ] ( x i ) = 0 g are flipped as in (10). This process is repeated until the newly com-puted conditionally optimal solution f ^ y 0 is in the strict interior of the convex polytope pol( X  y 0 ). In that case, the algorithm exits from the DJ-step, and enters the CP-step again.
 By this procedure we can eventually find a local opti-mal solution at the C because the objective function J ( X  y; f ) is bounded below. Computational complexity of S 3 VM path algorithm is discussed in the supplemen-tary.
 Algorithm 1 Entire procedure of S 3 VM path 1: Input : f ( x i ; y i ) g i 2L , f x i g i 2U , K and C ; 3: f [0] Train a SVM with f ( x i ; y i ) g i 2L ; 5: C 0; 6: while C C do 7: C 0 C ; 9: C C 1 ; 10: S f i 2Uj f [ C ] ( x i ) = 0 g ; 12: end while CP Step (Algorithm 2): The CP-step is imple-mented with a convex parametric programming tech-nique. Since the optimization problem (5) for com-puting conditionally optimal solutions is a convex quadratic program, we can use piecewise-linear para-metric programming (Best, 1996) in the same way as the famous SVM regularization path algorithm (Hastie et al., 2004).
 In piecewise-linear parametric programming, we com-pute the sensitivity of the optimal solutions to the pa-rameter C based on the KKT optimality conditions. Here, we consider the following four index sets:
M := f i 2L[Uj y
I If we know that the members of these four index sets are unchanged in a short range of C , then we can easily observe that the optimal solutions (i.e., the set of Lagrange multipliers f i g i 2L[U ) are linear in C . It suggests that the entire path of the conditionally optimal solutions is a piecewise-linear function of C because whenever one of the members in the above four index sets changes, the linearity (slope) of the solution path also changes. See Hastie et al. (2004) for the detail of piecewise-linear parametric programming in the context of SVMs.
 The algorithm exits from the CP-step when the piecewise-linear path of f ^ y s reaches a boundary of the convex polytope pol( X  y ), i.e., any one of the unlabeled instances satisfies f ( x i ) = 0 ; i 2 U . This moment can be exactly and easily identified by exploiting the piece-wise linearity of the solution path, and it does not sig-nificantly increase the computational cost of piecewise-linear parametric programming.
 Algorithm 2 CP-step 1: Input : C 0 ; f;  X  y ; 2: Output : C 1 ; f [ C 4: C C 0 5: while  X  y i f ( x i ) &gt; 0 8 i 2U and C C do 6: Compute the path of f ^ y s with increasing C ; 7: end while 8: C 1 C ; DJ Step (Algorithm 3): If the path reaches a boundary of a convex polytope at a certain C , the algorithm exits the CP-step and enters the DJ-step. In the DJ-step, the parameter C is fixed until a local optimal solution at that C is found. In this step, we exploit Theorem 5. If the predicted labels  X  y are flipped to  X  y 0 as in (10), then the conditionally optimal solution defined in the new convex polytope pol( X  y 0 ) is strictly better than the previous solution. By this strict im-provement property and the fact that the number of possible  X  y 2 f 1 ; 1 g jLj is finite, we can always find a local optimal solution at that C by repeating this process.
 Although any convex optimization solver can be used in this step, we note that this step can be carried out very efficiently: The two conditionally optimal solu-tions f ^ y and f ^ y 0 should not be much different (the difference is only in a few constraints corresponding to i 2 f i 2 Uj f ^ y ( x i ) = 0 g ). If we use the former solution as the initial starting point of the latter opti-mization problem, it should be solved very efficiently. For the experiments in the next section, we have devel-oped an active set method-type solver for the DJ-step. In our experience, new conditionally optimal solutions are usually obtained within tens of iterations. Algorithm 3 DJ-step 1: Input : C ;  X  y; S ; 2: Output : f;  X  y ; 3: while S is not empty do 4:  X  y 0 i  X  y i ; i 2S ; 5: Compute f ^ y ; 6: S f i 2Uj f ^ y 0 ( x i ) = 0 g ; 7:  X  y  X  y 0 ; 8: end while In this section, we report experimental results. Setup: The proposed S 3 VM path is compared with the supervised SVM (Vapnik, 1996) and two exist-ing S 3 VM algorithms in terms of generalization per-formance, optimization performance, and computation time: S 3 VM light (Joachims, 1999) which is also an an-nealing algorithm that computes a sequence of solu-tions with increasing C , and an algorithm (Collobert et al., 2006) based on a general non-convex solver called the convex-concave procedure (CCCP) (Yuille &amp; Rangarajan, 2002).
 We use the 10 benchmark datasets listed in Table 1. In each data set, a half of the instances are used for training. We set the number of labeled instances at jLj = min f 30 ; 10% of the training set size g , and the rest of the training instances are used as unlabeled instances. From the remaining half of the data set, we choose validation instances of size equal to jLj , which are used only for tuning hyper-parameters. The rest of instances are used for evaluating the generalization performance.
 Generalization Performance: First, we compare the generalization performance for (i) unlabeled in-stances used for training (i.e., the transduction error), and (ii) test instances which are not used for train-ing (i.e., the generalization error). We use the Gaus-sian kernel: K ( x i ; x j ) = exp( k x i x j k ). Model selection is carried out by finding the best parameter combination that minimizes the validation error from C = f 1 ; 10 ; 100 ; 1000 g and = f 1 4 d ; 1 2 d ; 1 d ; d is the input dimensionality. The best C is also se-lected based on the validation performance. In CCCP, C was chosen from f C 8 ; C 4 ; C 2 ; C g . For S 3 VM light S
VM path , C was selected from those computed in the annealing steps: In S 3 VM light , C was selected from f 2 9 C; 2 8 C; : : : ; C g following the suggestion in Chapelle et al. (2008); in S 3 VM path , C that exactly minimizes the validation error in the entire range of [0 ; C ] was selected. Note that this is possible be-cause S 3 VM path performs annealing with an infinites-imal resolution.
 The results reported in Table 2 are the average and the standard deviation for 10 different random data splits. The table shows that the proposed S 3 VM path gives the smallest generalization errors in many cases. This gain may be brought by the infinitesimal annealing effect. Optimization Performance: Next, we compare the optimization performance of the three S 3 VM algo-rithms. Since S 3 VM involves a non-convex optimiza-tion problem, it is interesting to see how good local optimal solutions can be found by each algorithm. In-deed, as pointed out in Chapelle et al. (2008), finding good local optimal solutions of the problem (1) or (4) leads to good S 3 VM generalization performances. The optimization performance of each algorithm is compared in terms of the objective function value J ( f;  X  y ) defined in (1). After we obtained predicted la-bels  X  y from each of the three algorithms, we computed Figure 5 plots the experimental results, where = 1 d and C = C = f 1 ; 10 ; 100 ; 1000 g . The results show that the proposed S 3 VM almost consistently outper-forms S 3 VM light and CCCP. We can also observe posi-tive correlation between objective function values and generalization errors. We conjecture that S 3 VM path yields better generalization performances in Table 2 partly because it finds better local optimal solutions. Computation Time: Finally, we compare the com-putation time of each algorithm. Figure 6 plots the entire computation time for training S 3 VM, where the horizontal axis indicates the number of annealing steps in S 3 VM light , and the number of candidates of C s in CCCP. The results show that the computation time grows as the number of annealing steps and the num-ber of C -candidates increase. Although more anneal-ing steps and more C -candidates are preferable for better generalization performance, Figure 6 indicates that increasing the numbers of annealing steps and C -candidates is possible only at the expense of increasing the computational costs in S 3 VM light and CCCP. We can interpret the proposed S 3 VM path as comput-ing solutions with infinitely many C -candidates by infinitesimal-step annealing. However, as Figure 6 shows, its computation time is usually much smaller than CCCP, and it is comparable to S 3 VM light that has a much smaller number of annealing steps. This means that S 3 VM path can go beyond the trade-off be-tween the resolution of annealing steps and the com-putation cost.
 Overall, our proposed method, S 3 VM path , is shown to be a promising alternative to existing S 3 VM training algorithms.
 In this paper, we proposed a novel training algo-rithm for S 3 VM based on infinitesimal annealing. Our method efficiently tracks a path of local optimal solu-tions when the effect of unlabeled data is gradually increased. A notable difference from existing solu-tion path algorithms is that our solution path includes jumps , due to non-convexity and non-smoothness of the loss function for unlabeled instances. Through ex-periments, we demonstrated that our algorithm, called S
VM path , is promising in generalization performance, optimization performance, and computation time. IT was supported by MEXT KAKENHI 23700165 and the Hori sciences and arts foundation. MS was sup-ported by MEXT KAKENHI 23300069.
 Allgower, E. L. and George, K. Continuation and path following. Acta Numerica , 2:1 X 63, 1993.
 Best, M. J. An algorithm for the solution of the parametric quadratic programming problem. Ap-plied Mathemetics and Parallel Computing , pp. 57 X  76, 1996.
 Boser, B. E., Guyon, I. M., and Vapnik, V. N. A train-ing algorithm for optimal margin classifiers. Proceed-ings of the Fifth Annual ACM Workshop on Com-putational Learning Theory , pp. 144 X 152, 1992. Boyd, S. and Vandenberghe, L. Convex Optimization . Cambridge University Press, 2004.
 Chapelle, O. Training a support vector machine in the primal. Neural Computation , 19(5):1155 X 1178, 2007.
 Chapelle, O. and Zien, A. Semi-supervised classifica-tion by low density separation. Tenth International
Workshop on Artificial Intelligence and Statistics , 2005.
 Chapelle, O., Sch  X olkopf, B., and Zien, A. (eds.). Semi-
Supervised Learning . MIT Press, Cambridge, MA 2006.
 Chapelle, O., Sindhwani, V., and Keerthi, S. Branch and bound for semi-supervised support vector ma-chines. Advances in Neural Information Processing Sysrtems , 2007.
 Chapelle, O., Sindhwani, V., and Keerthi, S. S. Op-timization techniques for semi-supervised support vector machines. J. Mach. Learning Res. , 9:202 X  233, Feb. 2008.
 Collobert, R., Sinz, F., Weston, J., and Bottou, L. Large scale transductive SVMs. Journal of Machine Learning Research , 7:1687 X 1712, 2006.
 Colorni, A., Dorigo, M., and Maniezzo, V. Distributed optimization by ant colonies. Proc. European Con-ference on Artificial Life , pp. 134 X 142, 1991. Cortes, C. and Vapnik, V. Support-vector networks. Machine Learning , 20:273 X 297, 1995.
 Efron, B. and Tibshirani, R. Least angle regression. Annals of Statistics , 32(2):407 X 499, 2004.
 Gal, T. Postoptimal Analysis, Parametric Program-ming, and Related Topics . Walter de Gruyter, 1995. Hastie, T., Rosset, S., Tibshirani, R., and Zhu, J. The entire regularization path for the support vector ma-chine. Journal of Machine Learning Research , 5: 1391 X 415, 2004.
 Hromkovic, J. Algorithmics for Hard Problems . Springer, 2001.
 Joachims, T. Transductive inference for text classifi-cation using support vector machines. International Conference on Machine Learning , 1999.
 Karasuyama, M., Harada, N., Sugiyama, M., and
Takeuchi, I. Multi-parametric solution-path al-gorithm for instance-weighted support vector ma-chines. Machine Learning , 88(3):297 X 330, 2012. Kirkpatrick, S. and Gelatt, C. D. Optimization by simulated annealing. Science , 220:671 X 680, 1983. Korte, B. and Vygen, J. Combinatorial Optimization:
Theory and Algorithms . Springer-Verlag, Berlin, 2000.
 Ritter, K. On parametric linear and quadratic pro-gramming problems. mathematical Programming:
Proceedings of the International Congress on Math-ematical Programming , pp. 307 X 335, 1984.
 Sindhwani, V., Keerthi, S., and Chapelle, O. Deter-ministic annealing for semi-supervised kernel ma-chines. International Conference on Machine Learn-ing , 2006.
 Takeuchi, I., Nomura, K., and Kanamori, T. Non-parametric conditional density estimation using piecewise-linear solution path of kernel quantile re-gression. Neural Computation , 21(2):539 X 559, 2009. Vapnik, V. N. The Nature of Statistical Learning The-ory . Springer, 1996.
 Vapnik, V. N. and Sterin, A. On structural risk min-imization or overall risk in a problem of pattern recognition. Automation and Remote Control , 1977. Yuille, A. L. and Rangarajan, A. The concave-convex procedure (cccp). In Advances in Neural Informa-
