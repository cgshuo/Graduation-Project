 At present, multimedia data have evolved into our lives, where we increasingly have higher expectations in exploiting these data at hands. Typical manipulation usually has been demonstrated that time series representation could be more efficient and effective in several domains, including science, bioinformatics, economics, and espe-cially in multimedia [1]. For example, in a query-by-humming system, we can just extract a sequence of pitch from a sung query [2-6] to retrieve an intended song from the database. In motion retrieval, we can extract a sequence of motion in each video frame from a centroid of the object of interest in X, Y, and Z axes [7-9]. Similarly, in a content-based image search, image X  X  shape can also be transformed into time series data for an efficient retrieval [10]. 
For the past decade, the most widely used distance measure in time series data has been Euclidean distance. It has been used fo r data retrieval, classification, clustering, etc. However, Euclidean distance appears to be unsuitable for many applications be-cause of its high sensitivity to variability in the time axis, and the superiority in accu-racy of Dynamic Time Warping (DTW) over the Euclidean distance, which have been noted in various domains [7-9, 11, 12]. DTW, nonetheless, can handle only local variations in the data. Thus, it appears to be inappropriate for many applications, especially for multimedia or human-related domains where uniform stretching and shrinking are very typical [8, 12]; for example, in a query-by-humming system, most users tend to sing slower or faster than the original song. Similar problems also arise in other applications, such as motion capture and image retrievals [7, 9]. Recently, the use of Uniform Scaling (US) together with DTW has been introduced to mitigate this problem since US allows global scaling of time series before DTW distance calcula-tion. However, this combination comes at a cost and cannot scale well with large databases. Fortunately, we have a lower-bounding function of this distance measure [12], which can efficiently prune most of the dissimilar candidate sequences to achieve significant speedup over these calculations. The Importance of Normalization Even though we now have a relatively efficient technique to speed up US with DTW curate and meaningful similarity measurements. In addition, z-normalization and mean normalization are typically used when our primary interest is the time series X  shapes. 
In query by humming, for instance, we want to search the song database for the one whose segment is most similar to the sung query. However, most users may not sing queries in the same key or the same pitch level as the songs stored in the database. Thus, normalization of both the query and candidate sequences is needed to remove any exist-illustrated in Fig.1, where Q is the extracted sung query segment of the popular  X  X appy Birthday X  song, and C is one of the candidate sequences stored in the database. We can quickly see that the shape of Q and a prefix of C are quite similar. Nevertheless, if we measure the similarity of these sequences directly using either Euclidean distance or DTW distance without any normalization nor rescaling, the distance will become exces-Therefore, normalization and rescaling of both sequences before distance calculation are crucial steps to achieve an accurate retrieval (see Fig. 1 b) and c)). 
In addition, the current lower-bounding method to prune off a large number of candidate sequences is developed without realization of the importance nor effects of data normalization. Hence, we propose a lower-bounding function to deal with this normalization problem efficiently and to calculate a distance under the US with DTW, where no false dismissals are guaranteed. 
The rest of this paper is organized as follows. In section 2, we describe related re-search work and our motivation behind solving this normalization problem. Section 3 covers necessary background. In section 4, we describe our proposed method with a proof of no false dismissals. Section 5 verifies correctness of our method with a set of experiments to confirm the large pruning power in massive databases. Finally, section 6 gives some conclusions and offers possible future work. For the past decade, DTW has attracted many researchers because of its superiority in accuracy over the ubiquitous Euclidean distance, which has been widely known in a variety of domains and applications [1-3, 5, 7-12, 14, 15]. However, lack of ability to globally stretch or shrink a time series of DTW in dealing with tempo variations has been known in music retrieval community [6, 16]. A straightforward solution is to generate every possible scaled version of the query or the candidates to be used in the measurement; it is, however, impractical for large databases. Thus, some researchers have proposed the methods to address and resolve this concern efficiently. Keogh has proposed a lower-bounding function for the US that can speed up the calculation by two to three orders of magnitude [8]. In 2005, Fu et al. have extended Keogh X  X  method providing a solution for a lower-bounding distance calculation under US with DTW [12]. At this point, although there have been relatively efficient solutions to deal with both US and DTW, practically none of the researchers has realized the im-portance and effects of normalization under US, and this is a primary cause of flaws need in normalization as mentioned earlier, it has been neglected and in turn has blocked up both US and DTW to achieve high accuracy and efficient retrieval. Our contribution is to propose an efficient lower-bounding function for US with DTW distance calculation under normalization requirement, which can prune a sig-nificant number of unqualified candidate sequences. Nonetheless, we would like to reemphasize that normalization is a crucial step to achieve a meaningful distance calculation, especially in multimedia applications, as well as for efficient retrieval of the time series data. We begin with a formal problem definition as well as reviews of necessary back-ground. q sfmax are minimum and maximum scaling factors respectively, i.e., we can shrink or stretch a query sequence from lengths sfmin*m to sfmax*m , where sfmax  X  1 and 0 &lt; stored in a database D . For simplicity, here, we define n  X  sfmax * m . Finally, we want to find the most similar-shaped candidate sequence C in the database D to the query sequence Q , which is also scalable in arbitrary lengths between sfmin * m and sfmax * m . Definition 1. Squared Euclidean distance : We define a squared Euclidean distance measure in eq.(1), which calculates distance between two sequences of equal length m (query X  X  length). Note that the square root from the original Euclidean distance has been removed for an optimization purpose since the rankings of the results from both of these approaches are identical [8]. Definition 2. Uniform Scaling : Uniform Scaling is a technique that uniformly stretches or shrinks a time series. In this approach, if we want to stretch a prefix of a candidate sequence C of length l to length m , we can use the Uniform Scaling func-tion in eq.(2); shrinking of a candidate is done similarly to a stretching process. 
We can formally define the Uniform Scaling function as follows. 
For US distance calculation, prefixes of a candidate sequence C of length l , where  X  X  X   X  X  X  ) , min( n m sfmax l m sfmin  X   X   X   X  , are rescaled to length m (query X  X  length). Then we use a squared Euclidean distance function to calculate distance between a query sequence and all rescaled prefix sequences in order to find a minimum distance value ranging from sfmin to sfmax . 
The formal definition of a Uniform Scaling distance function (US) is provided in eq.(3) and eq.(4), where RP( C , m,l ) is a Rescaled Prefix function that returns a prefix of a candidate sequence of length l rescaled to length m . Definition 3. Lower bounding of Uniform Scaling [8, 12]: Lower bounding of Uni-form Scaling is a distance approximation function, which can quickly compute a lower-bounding distance between a query and a candidate sequences; however, this lower bound value must not exceed the true distance value in order to be a valid lower-bounding function. To illustrate the idea, two new sequences are created, an upper envelope sequence UY and a lower envelope sequence LY , which enclose a candidate sequence. This envelope represen ts all scaled candidate sequences for a lower-bounding distance calculation. 
UY and LY are formally defined in eq.(5), which was proposed in [8]. Note that a lower-bounding distance can simply be a squared Euclidean distance between a query sequence and the candidate X  X  envelope, as defined in eq.(6). 
Interest readers may consult [12] for further details about US and DTW. Because the existing lower-bounding functions are not designed for distance calcula-tion under normalization requirement, they are flawed and do not give correct and meaningful calculation. Consequently, this paper is highly motivated to fix this flaw by proposing US with DTW function under normalization condition as well as their efficient lower-bounding functions. Furthermore, our proposed lower-bounding func-tion is able to prune a large number of candidate sequences without undergoing such costly distance calculation, primarily to speed up the computation with no false dismissals. 
The failure in lower-bounding distance calculation under z-normalization condition of the previous work is illustrated in Fig. 2 b) and c). The shown query in Fig. 2 a) is a rescaled version of the candidate X  X  prefix (scaling factor = 1.2). Then we normalize both sequences by using z-normalization as presented in Fig. 2 b) to e). However, in Fig. 2 b) and c) being the previously proposed lower-bounding functions [8, 12], it is apparent that their results (lower-bounding distance) are not zero, i.e., the normalized query is not fully contained within the lower-bounding envelopes, as illustrated by the shaded regions. This phenomenon definitely violates the lower-bounding rule because distance should in fact be zero. Therefore, these existing lower-bounding functions could cause some false dismissals in normalization scheme. Actually, this phenome-non is not surprising since both of the previously proposed lower-bounding functions lower-bounding function that satisfy all the lower bounding conditions. 
In this section, we begin with solutions for US and US with DTW distance measure, which satisfy normalization requirement, followed by the proof of no false dismissals. Definition 4. Uniform Scaling with Normalization : The formal definition of a US with z-normalization is shown in eq.(7) and eq.(8), where Q  X  is a z-normalized query, length l , respectively. Although different scalings of the same sequence through in-terpolation may yield different mean and standard deviation values, our observation discovers no statistically significant diff erence of mean and standard deviation be-tween normalization before rescaling the sequences and rescaling the sequences be-fore normalization. Definition 5. Lower bound of Uniform Scaling with Normalization : We develop a bounding envelope as expressed in eq.(9) and eq.(10), where UZ  X  i and LZ  X  i are upper and lower envelope sequences respectively. The corresponding distance calculation function is shown in eq.(11). 
In case of US with DTW distance measure with normalization, we can simply change distance calculation function D in eq.(8) from the squared Euclidean function to DTW function. Additionally, its lower-bounding function is also quite straightfor-ward that we can apply lower bounding of DTW over our envelope from eq.(9) and distance calculation. The calculation of this lower-bounding function is similar to other functions as stated earlier. Due to space limitations, we omit full details of this distance function and its proof for brevity. 
Lastly, to validate the correctness of the proposed method, we complete this sec-tion with a proof of our lower bounding properties. Proposition 1. Let Q  X  be a normalized query sequence of length m , and C be a candi-date sequence of length n . In addition, a minimum scaling factor and a maximum LBZ( Q  X  , C ) is a lower-bounding distance of US norm ( Q  X  , C , sfmin , sfmax ). US norm ( Q  X  , C , sfmin , sfmax ). 
If a query is stretched to length sf * m , where sf is a scaling factor, a prefix of a can-its mean and standard deviation, as shown in eq.(13). From eq.(9) and eq.(10), the upper and lower envelopes are defined as follows. From eq.(13) and (14), it follows that In the previous section, we introduce our proposed lower-bounding function as well as justification of its correctness in preser ving the lower-bounding properties. In this section, we carefully evaluate the efficiency of our proposed method by conducting sets of experiments to observe the pruning power [11, 12]. Note that the pruning power is the fraction of the total candidate objects that can be discarded from further calculation, as defined in eq.(15). 
In these experiments, we develop a simple Query-by-Humming system based on one-nearest-neighbor time series matching technique [3, 4, 6, 12, 17] in order to dem-onstrate the quality and utilities of our proposed method in multimedia database. We use 100 to 2,000 different international songs in MIDI file format and generate candi-date sequences from this MIDI songs by using sliding windows of length 150 * sfmax data points, where sfmax = 1.4, and then store it in a database. For query sequences, we collect 55 sung queries from 12 subjects of both genders with various singing abilities and then extract sequences of pitch from these sung queries by using autocor-relation algorithm [18]. 
To carefully evaluate each factor that affects quality of the lower-bounding func-tion, we conduct three experiments. In the first experiment, we examine an effect of different lengths of sequences with different ranges of scaling factors under 22441 sequences (generated from 100 songs), where a size of the global constraint is set to 4 percent of the sequences X  length (see Fig. 3 a)). In the second experiment, we investigate an effect of different lengths of the sequences with different sizes of global constraint under 22441 sequences, wh ere range of scaling factor is between 0.8 and 1.2 (see Fig. 3 b)). In the last experiment, we use 22441, 55595, 107993, 220378, and 442366 subsequences from 100, 250, 500, 1000, and 2000 songs to construct different-sized databases in order to observe their pruning powers, as shown in Fig. 4.
According to these experiment results, our proposed lower-bounding function can prune a great number of candidate sequences in every parameter setting, as shown in Figs. 3 and 4. However, from these experiments, we found that there are several fac-Fig. 3 a), increases in range of scaling f actors are likely to decrease the pruning power directly. Besides, the pruning power slight ly decreases as the sizes of global con-straint increase (see Fig. 3 b)). The rationale behind these results is that both increases in range of scaling factors and in size of globa l constraint definitely enlarge the size of the lower-bounding envelopes, causing a reduction in the lower bounding distance, and hence its pruning power. In addition, longer sequences appear to give smaller Fig. 4), which is a highly desirable property for lower-bounding functions. 
Regardless of a few factors that decrease the pruning power, we discover no sig-nificant improvement in accuracy when we increase scaling factor range over 0.6-1.4 constraint do not imply higher accuracy. In fact, in most cases, the size of the global constraint of less than 10 percent often yields optimal accuracy. 
Notice that the normalization does affect the pruning power because the distances between the normalized query and the normalized candidate sequences are greatly reduced, comparing with the distances among unnormalized sequences. However, we would like to reemphasize that normalization is essential in many applications. We have shown that this proposed lower-bounding function of US with DTW under normalization requirement can efficiently prune a large number of candidates in the database, significantly reducing the time complexity in the data retrieval, especially for multimedia retrieval, while no false dismissals are also guaranteed. Furthermore, our approach can work well with other types of normalization. Nonetheless, we would like to reemphasize the importance and necessity of normalization, especially in mul-timedia applications. 
Besides dramatically speeding up the calcula tions by pruning almost all candidates, this lower-bounding function is possible to utilize dimensionality reduction and in-dexing techniques [9] in order to be scalable to truly massive databases. Acknowledgments. We would like to thank Dr. Boonserm Kijsirikul for valuable comments and enlightening discussions. This work is partially supported by the Thai-land Research Fund (Grant No. MRG5080246). 
