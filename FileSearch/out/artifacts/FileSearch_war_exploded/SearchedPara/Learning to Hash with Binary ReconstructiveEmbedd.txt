 of data is huge X  X arge image repositories, video sequences, a nd others X  X aving fast techniques for dimensional binary (Hamming) space. Unlike standard dimen sionality-reduction techniques from Hamming space, or one can use data structures for finding appr oximate nearest neighbors in the Hamming space which have running times that are sublinear in the number of total objects [1, 2]. Since the Hamming distance between two objects can be comput ed via an xor operation and a bit input dimensionality is very high, hashing methods lead to e normous computational savings. the Hamming space. One of the basic but most widely-employed methods, locality-sensitive hashing (LSH) [1, 2], generates embeddings via random projections a nd has been used for many large-scale observed that the number of hash bits required may be large in some cases to faithfully maintain the distances. On the other hand, several recent techniques  X  X ost notably semantic hashing [3] and spectral hashing [4] X  X ttempt to overcome this problem by designing hashing techniques that objective. Both methods have shown advantages over LSH in te rms of the number of bits required restrictive assumption in some cases.
 we minimize a squared loss over the error between the input di stances and the reconstructed Ham-our method on the very large Tiny Image data set of 80 million i mages [5], to qualitatively show some example retrieval results obtained by our proposed met hod. 1.1 Related Work this paper is on hashing-based methods, which map the data to a low-dimensional Hamming space. accommodating distances such as  X  Several recent methods have explored ways to improve upon th e random projection techniques used and boosting-based hashing methods [14]. descent algorithm used to minimize the objective function, and discuss extensions of the proposed approach. 2.1 Setup Let our data set be represented by a set of n vectors, given by X = [ x that these vectors are normalized to have unit  X  kernel function over the data be denoted as  X  ( x standard inner product to emphasize that the algorithm can b e expressed purely in kernel form. we will compute the b -dimensional binary embedding by projecting our data using a set of b hash functions h binary reconstruction can be represented as  X  x 0 and 1. 2.2 Parameterization and Objective In standard random hyperplane locality-sensitive hashing (e.g. [1]), each hash function h ated independently by selecting a random vector r and identity covariance. Then the hash function is given as h propose to generate a sequence of hash functions that are dep endent on one another, in the same size b  X  n , and we parameterize the hash functions h Note that the data points x function (and so we can represent all weights via the b  X  s matrix W ). Though we are not aware Furthermore, the form of each hash function X  X he sign of a line ar combination of kernel function values X  X s the same as several kernel-based learning algorit hms such as support vector machines. Rather than simply choosing the matrix W based on random hyperplanes, we will specifically con-following objective with respect to the weight matrix W : size of N will be nk . 2.3 Coordinate-Descent Algorithm and gradient  X  X  can both be computed in O ( nkb ) time. However, our experience with minimizing O with such an approach using a quasi-Newton L-BFGS algorithm typically resulted in poor local optima; we need an alternative method.
 Instead of the continuous relaxation, we will consider fixin g all but one weight W the original objective O with respect to W hash function h optimum of the objective function O . We sketch out the details of our coordinate-descent scheme b elow. We begin with a simple lemma characterizing how the objective function changes when we u pdate a single hash function. Lemma 1. Let  X  D (where  X  d uses h the hash function can be expressed as Proof. For notational convenience in this proof, let  X  D distances using h and new hash bits, respectively. Also, let e all ones. Note that H updated. We can express  X  D where  X  of squared norms of the rows of H vectors are binary-valued. Therefore we may write where we have used the fact that H obtain O = X since h The lemma above demonstrates that, when updating a hash func tion, the new objective function can be computed in O ( nk ) time, assuming that we have computed and stored the values of  X  D we show that we can compute an optimal weight update in time O ( nk + n log n ) . Consider choosing some hash function h W except W pq , which corresponds to the one weight updated during this ite ration of coordinate-descent. Modifying the value of W for every point x , there is a hashing threshold : a new value of W that Observe that, if c We first compute the thresholds for all n data points: once we have the values of c computing t we can update the values of c thresholds t hash function h need only compute the objective function at each of the n + 1 intervals, and choose the interval that minimizes the objective function. We choose a value W thresholds, the total cost of an update to a single weight W Lemma 2. Consider updating a single hash function. Suppose we have a s equence of hash vectors h for all n + 1 hash functions can be computed in O ( nk ) time.
 Proof. The objective function may be computed in O ( nk ) time for the hash function h sponding to the smallest interval. Consider the case when go ing from h some 1  X  j  X  n . Let the index of the bit that changes in h the objective that change are ones of the form ( a, j )  X  X  and ( i, a )  X  X  . Let f to as given in Lemma 1 may be written as: n + 1 hash functions results in a total of O ( nk ) time.
 Putting everything together, we have shown the following re sult: Theorem 3. Fix all but one entry W to minimize (1) may be computed in O ( nk + n log n ) time.
 of possible hash configurations are possible. 2.4 Extensions the algorithm we developed is completely unsupervised. One could easily extend the method to (say, 1).
 option would be to use an  X  addition of an  X  be worth additional study. We now present results comparing our proposed approach to th e relevant existing methods X  X ocality sensitive hashing, semantic hashing (RBM), and spectral ha shing. We also compared against the Boosting SSC algorithm [14] but were unable to find parameter s to yield competitive performance, and so we do not present those results here. We implemented ou r binary reconstructive embedding method (BRE) and LSH, and used the same code for spectral hash ing and RBM that was employed in [4]. We further present some qualitative results over the Tiny Image data set to show example retrieval results obtained by our method. 3.1 Data Sets and Methodology We applied the hashing algorithms to a number of important la rge-scale data sets from the com-of approximately 300,000 image patches, processed using SI FT to form 128-dimensional vectors; the Caltech-101 [16], a standard benchmark for object recog nition in the vision community; and LabelMe and Peekaboom [17], two image data set on top of which global Gist descriptors have Nursery, one of the larger UCI data sets.
 applying spectral hashing or BRE X  X he results of the RBM metho d and LSH were better without using 1000 randomly selected data points. For training the B RE method, we select nearest neigh-same hash key. The spectral hashing and RBM parameters are se t as in [4, 17]. After construct-ing the hash functions for each method, we randomly generate 3000 hashing queries (except for Caltech-101, which has fewer than 4000 data points; in this c ase we choose the remainder of the data as queries).
 normalized Hamming distance using the constructed hash fun ctions is less than or equal to three. would correspond to nearest neighbors in the original data e mbedding. 3.2 Quantitative Results method performs comparably to or outperforms the other meth ods on all data sets. Observe that both RBM and spectral hashing underperform all other method s on at least one data set. On some Figure 1: Results over Photo Tourism, Caltech-101, LabelMe , Peekaboom, MNIST, and Nursery. The plots show how well the nearest neighbors in the Hamming s pace (pairs of data points with unnormalized Hamming distance less than or equal to 3) corre spond to the nearest neighbors (top comparably to, existing methods. See text for further detai ls. data sets, RBM appears to require significantly more than 100 0 training images to achieve good One surprising outcome of these results is that LSH performs well in comparison to the other ex-isting methods (and outperforms some of them for some data se ts) X  X his stands in contrast to the results of [4], where LSH showed significantly poorer perfor mance (we also evaluated our LSH performance in our tests may be due to our implementation of L SH; we use Charikar X  X  random projection method [1] to construct hash tables.
 In terms of training time, the BRE method typically converge s in 50 X 100 iterations of updating RBM but slower than spectral hashing and LSH. Search times in the binary space are uniform across 3.3 Qualitative Results and we employ the global Gist descriptors that have been extr acted for each image. We ran our reconstructive hashing algorithm on the Gist desc riptors for the Tiny Image data set to the results of the linear scan. similar to the linear scan results but are significantly fast er to obtain. descent algorithm for finding a local optimum, and demonstra ted improved performance on several may fall into poor local optima in some cases. Second, we woul d like to explore the use of our algorithm in the supervised setting for large-scale k -NN tasks.
 Acknowledgments This work was supported in part by DARPA, Google, and NSF gran ts IIS-0905647 and IIS-0819984. We thank Rob Fergus for the spectral hashing and RBM code, and Greg Shakhnarovich for the Boosting SSC code.

