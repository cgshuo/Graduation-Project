 Relevation! is a system for performing relevance judgements for information retrieval evaluation. Relevation! is web-based, fully configurable and expandable; it allows researchers to ef-fectively collect assessments and additional qualitative data. The system is easily deployed allowing assessors to smoothly perform their relevance judging tasks, even remotely. Rele-vation! is available as an open source project at: http://ielab.github.io/relevation .
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval] General Terms: Measurement, Experimentation.
An integral part of information retrieval (IR) evaluation is the use of standard test collections. Relevance assessments are critical to the quality of test collections and obtaining as-sessments is often a long and laborious task, which, in many cases, involves a large number of documents to be judged by multiple assessors. Performing relevance assessments is often a one-off task for the creators of test collections and is usually under tight time and budget constraints. As a result, collection creators often have little resources to in-vest in a high quality tool to aid judges in performing rel-evance assessments, even though such a tool could greatly aid this process. With this in mind, we have developed Relevation! , an adaptable, open source tool for performing relevance judgements.

Relevation! allows users to upload documents and queries that are then browsed by judges through a web-based inter-face; for each document, judges can assign a relevance label. The system also allows judges to provide qualitative feed-back about the judgement process, both at query level and at document level. Relevation! is open source and uses the Model-View-Controller design pattern, making it customis-able to specific user requirements. The system is written in Python using the Django Web framework, making it eas-ily deployed and remotely available. Relevation! has already been used to collect additional relevance assessments for the TREC Medical Records Track [1] and for the CLEF eHealth (2013 &amp; 2014) evaluation campaigns [ 2].

Early work by Hawking et al. [3 ] developed a tool for collecting relevance assessment. However, this tool is out-dated, does not provide web-based deployments and is no longer available. RAT is a more recent web-based system for obtaining relevance assessments [4 ] but is specific to web search engines and assumes documents are screen scraped from the web. In addition, RAT is not open source so can-not be extended or adapted to specific needs.
The architecture of Relevation! is shown in Fig 1. The system comprises four main modules: (i) the setup module, (ii) the queries module, (iii) the documents module, and (iv) the judgements module.

Setup Module. This module allows users to upload their queries and the document judging pool to Relevation! . Two files are required: 1) Query file in tab separated for-mat QueryId [tab] QueryText , and 2) Document judging pool using the standard TREC results file format. Note that the standard setup module can be extended with additional parsers for query files and document judging pool; for ex-ample, the deployment of Relevation! for the CLEF eHealth 2013 was extended by implementing a query file parser which accepted additional query fields like  X  X escription X ,  X  X arra-tive X  and  X  X rofile X .

Queries Module. Judges are presented with a list of queries currently in the system (screenshot of Fig 2(a) ). For each query, the number of documents assigned to that query is displayed, along with the number of unjudged documents, giving an indication of the overall judging progress. In the screenshot, each query has a QueryId column which identi-(a) The list of queries currently in the system (query module). (b) List of docs. assigned to single query (documents module). Figure 2: Screenshots from the CLEF eHealth 2013 deployment of Relevation! . fies it and an associated Text column, which is the actual keywords for that query.

Documents Module. Clicking on the Text field entry takes the judge to the next page: the list of documents asso-ciated with that query (Fig 2(b)). For each document, the assessment Status column shows the relevance label assigned to the documents (e.g., highly relevant , somewhat relevant , etc.). The status field provides the judge with a quick and easy overview of the progress of this query, as well as the collection creators with an overview of the relevance assess-ments for that query (e.g., distribution of relevant/irrelevant documents). The Documents# field is the filename of the particular document and is a link to the judgements page.
Once all the documents for a query are judged, the as-sessors can optionally provide some qualitative feedback on the particular query. A short questionnaire is presented at the bottom of the list of documents. The questionnaire can be removed or customised to suit the particular needs of the relevance assessment task.

Judgements Module. This is where judges can read a document and enter their assessment (Fig 2(c) ). The top of the page gives the query keywords and description. The document contents are displayed on the lefthand side. 1 On the right hand side panel is a choice for the relevance as-sessment ( X  X ot Judged X  is the default judgement). Judges can also select portions of the document content and the selected text will automatically be added to the Supporting Evidence field. This information can be used for qualitative analysis after judging is complete or for passage based re-trieval evaluation. Once assessment is complete, judges have to press the Save&amp;Next button (not shown in screenshot), which saves the assessment and loads the next document for judging. The judgement module is customisable and dif-ferent deployments of Relevation! may implement different judgements modules. For example the judgement module for CLEF eHealth 2013 also displayed narrative and profiles for the queries and used a different relevance assessment scale.
Other functionality. A script is provided for export-ing relevance judgments in the standard TREC qrel format from the SQL database of Relevation! In addition, the SQL database can easily be queried to export other data, e.g., the qualitative questionnaires provided by the judges.
In its current version, Relevation! provides an open-source, modular, customisable system for collecting relevance as-sessments. In future versions of Relevation! , we plan to inte-grate the qrel creation tools and database querying methods within the core modules. The Setup module will be extended to allow users to configure custom relevance grades and qual-itative questionnaires. Additional document parsers for the main TREC collections will also be added to this mod-ule. The Judging module will be extend to allow config-urable placement of documents within the judgement in-terface, e.g., to support the visualisation of two documents at the time for preference judgements. Finally, we plan to incorporate a new Crowdsourcing module that allows Rele-vation! to outsource relevance assessment collection to work-force platforms such as Amazon Mechanical Turk. This will be available at http://ielab.github.io/relevation
