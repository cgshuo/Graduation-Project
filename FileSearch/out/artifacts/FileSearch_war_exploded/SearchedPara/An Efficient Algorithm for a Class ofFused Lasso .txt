 The fused Lasso penalty enforces sparsity in both the coefficients and their successive differences, which is desirable for applications with features ordered in some meaningful way. The resulting prob-lem is, however, challenging to solve, as the fused Lasso penalty is both non-smooth and non-separable. Existing algorithms have high computational complexity and do not scale to large-size prob-lems. In this paper, we propose an Efficient Fused Lasso Algorithm (EFLA) for optimizing this class of problems. One key building block in the proposed EFLA is the Fused Lasso Signal Approxima-tor (FLSA). To efficiently solve FLSA, we propose to reformulate it as the problem of finding an  X  X ppropriate" subgradient of the fused penalty at the minimizer, and develop a Subgradient Finding Algo-rithm (SFA). We further design a restart technique to accelerate the convergence of SFA, by exploiting the special  X  X tructures" of both the original and the reformulated FLSA problems. Our empirical evaluations show that, both SFA and EFLA significantly outper-form existing solvers. We also demonstrate several applications of the fused Lasso.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing Algorithms Fused Lasso, 1 regularization, restart, subgradient
The fused Lasso penalty introduced in [30] can yield a solution that has sparsity in both the coefficients and their successive differ-ences. It has found applications in comparative genomic hybridiza-tion [25, 31], prostate cancer analysis [30], image denoising [8], and time-varying networks [1], where features can be ordered in some meaningful way. Some properties of the fused Lasso have been established in [26].

In this paper, we focus on optimizing the following class of op-timization problems with the fused Lasso penalty: where loss( x ) is a given smooth and convex loss (e.g., the least squares loss) defined on a set of training samples, and is the fused Lasso penalty with the nonnegative  X  1 and  X  problem in (1) is challenging to solve, as the fused Lasso penalty is both non-smooth and non-separable.

Existing algorithms reformulate (1) as the equivalent constrained smooth optimization problem by introducing additional variables and constraints, and then apply the standard solver for optimiza-tion. Let n denote the sample dimensionality. Tibshirani et al. proposed the fused Lasso with the least squares loss [30]. They derived a smooth reformulation by introducing 4 n auxiliary vari-ables, linear constraints of the type: a  X  A y  X  b (the matrix A is of size (2 n +2)  X  5 n with 11 n  X  1 non-zero elements), and 4 n non-negative constraints, and then solved the reformulated problem by the SQOPT 1 package. Ahmed and Xing proposed to solve the fused Lasso penalized logistic regression by introducing 2 n aux-iliary variables and 4 n inequality constraints [1], and then solved the reformulated problem by the CVX 2 optimization package [10]. However, the constrained smooth reformulation usually does not scale well with n , due to the large number of auxiliary variables and inequality constraints intr oduced. Indeed, it was pointed out in [30] that,  X  X ne difficulty in using the fused Lasso is the compu-tational speed", and  X  X hen n&gt; 2000 and m&gt; 200 ( m denotes the number of samples), speed could become a practical limitation".
In this paper, we develop an Efficient Fused Lasso Algorithm (EFLA) by treating the objective function of (1) as a composite function with the smooth part loss(  X  ) and the other non-smooth part fl(  X  ) . One appealing feature of EFLA is that it makes use of the spe-cial structure of (1) for achieving a convergence rate of O (1 /k k iterations, which is optimal for the first-order black-box methods. Note that, when directly applying the black-box first-order method for solving the non-smooth problem (1), one can only achieve a convergence rate of O (1 /
In the proposed EFLA, a key building block (in each iteration) is the proximal operator [19] associated with the nonsmooth fused Lasso penalty fl(  X  ) (corresponding to a pair of  X  1 and  X  tomopt.com/tomlab/products/snopt/solvers/SQOPT.php stanford.edu/  X  boyd/cvx is also called the Fused Lasso Signal Approximator [8, FLSA]. Ex-isting approaches [8, 12] for solving FLSA usually employ a path technique that requires exactly following the path for computing the solution corresponding to the desired pair of  X  1 and  X  spite their advantage in obtaining the whole path solutions, they might not be efficient for our proposed EFLA, which needs the computation of FLSA corresponding to a pair of  X  1 and  X  2 in each iteration. In addition, it is hard for them to incorporate the  X  X arm" start technique for further improving the efficiency.
To efficiently solve FLSA corresponding to a pair of  X  1 and  X  we propose to reformulate it as the problem of finding an  X  X ppropri-ate" subgradient of the fused penalty at the minimizer, and develop a Subgradient Finding Algorithm (SFA). We further design a restart technique for accelerating the convergence of SFA, by exploiting the special  X  X tructures" of the original and the reformulated FLSA problems. When used as a building block in EFLA, SFA is shown to converge within dozens of iterations for problems of size up to 10 7 using the  X  X old" start; with the  X  X arm" start, SFA usually con-verges within 10 iterations. Our empirical evaluations show that, both SFA and EFLA significantly outperform the existing solvers. We also demonstrate several applications of the fused Lasso. Notations:  X  1 ,  X  ,and  X   X  denote the 1 -, 2 -, and  X  -norm, respectively. R  X  R ( n  X  1)  X  n isasparsematrixdefinedas: We can rewrite the fused Lasso penalty in (2) as Let sgn (  X  ) and SGN (  X  ) be the operators defined in the componen-twise fashion: if t&gt; 0 ,sgn ( t )=1 ,SGN ( t )= { 1 } ;if t&lt; 0 , sgn ( t )=  X  1 ,SGN ( t )= { X  1 } ;andif t =0 ,sgn ( t )=0 , ement of x onto the interval [  X   X  2 , X  2 ] .Let e  X  R n be a vector composed of 1 X  X . Denote [1 : n ] as the set of indices from 1 to n .
We review several categories of first-order methods that can be applied to optimizing the composite function (1) in Section 2.1, present our proposed algorithm in Section 2.2, and discuss the key building block X  X LSA in Section 2.3. Subgradient Descent (SD) When treating h ( x ) as the general non-smooth convex function, we can apply the subgradient de-scent [20, 21], which can achieve a convergence rate of O (1 / for k iterations. However, SD has the following two disadvantages: 1) the convergence is slow; and 2) the iterates of SD are very rarely at the points of non-differentia bility [6], thus it might not achieve the desirable sparse solution (which is usually at the point of non-differentiability) within a limited number of iterations. Coordinate Descent (CD) Coordinate descent [33] and its recent extension X  X oordinate gradient descent [34] are applicable for op-timizing the non-differentiable composite function. Convergence results have been established, when the non-differentiable part is separable [33, 34]; and CD was applied for solving Lasso in [8]. However, when applied for solving (1), CD may not converge to the desirable solution, as the fused Lasso penalty is non-separable. Friedman et al. [8] derived a modified CD for solving FLSA X  a special case of (1), and discussed its extension for solving the general fused Lasso; however, as explicitly mentioned in [8, Sec-tion 3, page 310], the resulting algorithm is not guaranteed to give the exact solution.
 Nesterov X  X  Method Nesterov X  X  method [20, 21] is an optimal first-order black-box method for smooth convex optimization, achiev-ing a convergence rate of O (1 /k 2 ) . In the recent studies [2, 22], the Nesterov X  X  method is extended to solve the composite function composed of one smooth part and the other non-smooth part. The resulting algorithm can achieve the optimal convergence rate of O (1 /k 2 ) , at the expense that the proximal operator [19] associated with the non-smooth part needs to be solved at each iteration. For the problem in (1), the associated proximal operator is the FLSA. The Nesterov X  X  method has been applied to solve various sparse learning formulations [2, 13, 16, 17, 18, 22, 32].
 Forward Looking Subgradient (FOLOS) The FOrward-LOoking Subgradient [6] was proposed for optimizing the composite func-tion. FOLOS is a forward-b ackward splitting m ethod. It can be applied for both online and batch learning. For bach learning, the convergence rates of O (1 / the general convex and the smooth convex loss functions. Regularized Dual Averaging (RDA) The Regularized Dual Av-eraging [35] was proposed for solving the regularized composite function, based on the dual averaging method proposed in [23]. RDA was designed for stochastic learning and online learning, and the convergence rates of O (1 / for the general convex and the strongly convex regularization.
In this paper, we consider solving (1) in the batch learning set-ting, and propose to apply the Nesterov X  X  method due to its fast convergence rate. Note that, one can develop the online learning al-gorithms for (1) using algorithms such as FOLOS and RDA, where FLSA is also a key building block. The efficient computation of FLSA will be discussed in Section 3.
We first construct the following model for approximating the composite function h (  X  ) at the point x : h L, x ( y )=[ loss ( x )+ loss ( x ) , y  X  x ]+fl( y )+ where L&gt; 0 . In the model h L, x ( y ) , we apply the first-order Taylor expansion at the point x (including all terms in the square bracket) for the smooth function loss (  X  ) , and directly put the non-smooth penalty fl (  X  ) into the model. The regularization term x 2 prevents y from walking far away from x , thus the model can be a good approximation to h ( y ) in the neighborhood of x .
With the model (4), we can develop the following gradient de-scent like method for solving (1): for some properly chosen { L i } . It has been shown in [2, 21] that, the scheme in (5) can yield a convergence rate of O (1 /k ) , and can be further accelerated to O (1 /k 2 ) .

The accelerated method can be derived using the  X  X stimate se-quence" [21, 22], which is quite involved. To make the presentation relatively easy to follow, we use the scheme provided in [2, 20] to present the Nesterov X  X  method for solving (1).
The Nesterov X  X  method is based on two sequences { x i } and in which { x i } is the sequence of approximate solutions, and is the sequence of search points. The search point s i is the affine combination of x i  X  1 and x i as where  X  i is a properly chosen coefficient. The approximate solu-tion x i +1 is computed as the minimizer of h L i , s i ( y ) : where L i is determined by the line search according to the Armijo-Goldsteinrulesothat L i should be appropriate for s i .
The algorithm for solving (1) is presented in Algorithm 1. Fol-lowing the proof given in [20, 2], we can establish the following global convergence result: where x  X  is an optimal solution to (1), and  X  L is the Lipschitz con-tinuous gradient of the smooth convex loss function loss( Algorithm 1 The Efficient Fused Lasso Algorithm (EFLA) Input:  X  1  X  0 , X  2  X  0 ,L 0 &gt; 0 , x 0 ,k 1: Initialize x 1 = x 0 ,  X   X  1 =0 ,  X  0 =1 ,and L = L 0 . 2: for i =1 to k do 4: Find the smallest L = L i  X  1 , 2 L i  X  1 ,... such that 5: Set L i = L and  X  i +1 = 1+ 6: end for
In Algorithm 1, a key building block is the problem (7), which is the fused Lasso signal approximator to be discussed in the next subsection.
The Fused Lasso Signal Approximator (FLSA) solves the fol-lowing problem: which is a special case of (1) by setting loss( x )= 1 2 x Note that, FLSA in (9) is essentially the proximal operator [11, 14, 19] associated with the fused Lasso penalty fl( x ) .

Let We can easily verify that Thus the building block (7) in Algorithm 1 can be solved by FLSA in (9). In the sequel, we present its efficient computation. a unique minimizer, denoted by x  X  . The optimality condition [21, Theorem 3.15, Chapter 3] requires that where  X  X   X  1  X  2 ( x  X  ) denotes the subdifferential of f subdifferential of f  X  1  X  2 (  X  ) can be computed as:
Using the subgradient technique, it has been shown in [8] that, the minimizer of the problem (9) for any value of (  X  1 , X  obtained by a simple soft-thresholding of the solution obtained for (0 , X  2 ), as stated in Theorem 1. We provide an alternative and sim-plified proof using the technique of subdifferential; and this simpli-fied proof also motivates our proposed method in Section 3.
T HEOREM 1. For any  X  1 , X  2  X  0 , we have Proof: We first analyze the optimality condition for the solution  X  2 ( v ) . According to (12) and (13), there exists such that  X  0  X  2 ( v )= v  X  R T z  X  .Let We can easily verify x  X  v + g + R T z  X  = 0 and g  X   X  1 SGN ( x ) . Utilizing the definition of x , the special structure of R that each of its row has two nonzero elements  X  1 and 1 , and (15), we can verify z  X   X   X  2 SGN ( R x ) . Therefore, It follows from the optimality c ondition ( 12) that (14) holds.
Theorem 1 implies that, it suffices to solve (9) with  X  1 For discussion convenience, we omit the superscript to indicate that  X  1 =0 . The proof of Theorem 1 implies that,  X   X  2 ( v ) can be analytically solved as  X   X  2 ( v )= v  X  R T z  X  , provided that an appropriate z  X   X   X  2 SGN ( R X   X  2 ( v )) can be found. Interestingly, z is unique, as shown in the following analysis. It follows from  X  2 ( v )= v RR T z = R v  X  R X   X  2 ( v ) .Since RR T is positive definite (see the discussion in Section 3.1) and  X   X  2 ( v ) is unique, we conclude that z  X  is unique. The above discussion motivates us to solve (9) via finding the appropriate and unique z  X  .

In the next section, we shall show that z  X  can be efficiently com-puted by a special quadratic programming problem with the bound constraint. As R T z  X  is a subgradient of the fused penalty  X  at the minimizer, we term our proposed method as the Subgradi-ent Finding Algorithm (SFA). Note that, SFA is our main technical contribution in this paper.
In this section, we discuss solving (9) with  X  1 =0 , i.e., Introducing the dual variable z  X  R n  X  1 , we can reformulate (16) as the following equivalent min-max problem: This is a saddle-point problem, and the existence of the saddle point is ensured by the well-known Von Neumann Lemma [20], as  X  ( x , z ) is differentiable, convex in x , and concave in z .
Exchanging min and max and setting the derivative of  X  ( x , z ) with regard to x to zero, we have Plugging (18) into (17), we obtain the following optimization prob-lem with regard to z : It follows from (18) that, once z  X  , the minimizer of (19), is found, lationship  X   X  2 ( v )= v  X  R T z  X  shown in the proof of Theorem 1.
In the sequel, we discuss the efficient optimization of the bound constrained quadratic programming problem (19). To this end, we exploit the special  X  X tructures" of (16), (17) and (19), and develop a novel restart technique for the fast convergence.

The rest of this section is organized as follows: we present the optimality condition for (19) in S ection 3.1, derive the maximal value of  X  2 in Section 3.2, present the proposed SFA in Section 3.3, compute the duality gap of the solution in Section 3.4, develop a restart technique for accelerating the convergence in Section 3.5, and provide further discussions in Section 3.6. The Hessian of  X  (  X  ) can be computed as which is an ( n  X  1)  X  ( n  X  1) tridiagonal matrix. The n eigenvalues of the Hessian RR T can be analytically computed as 2  X  2cos( i X /n ) ,i =1 , 2 ,...,n  X  1 . Therefore, the Hessian is positive definite, and the minimizer of (19) is unique .
According to [21, Theorem 2.2.5, Chapter 2], we have that z satisfying z  X   X   X   X  2 , is a minimizer of (19) if and only if The optimality condition leads to an i mportant relationship between the minimizer and its gradient, as summarized in the following lemma (this lemma shall help derive the restart technique to be dis-cussed in Section 3.5):
L EMMA 1. Let g  X  =  X  ( z  X  ) . We have: 1) if g  X  i z i =  X   X  2 ;2)if g  X  i &lt; 0 ,then z  X  i =  X  2 ; and 3) if g =0 .
When  X  2  X  X  X  , the problem (19) becomes an unconstrained optimization problem. Intuitively, there exists a  X  max 2 value for  X  2 ), over which the problem (19) has the same solution. The following theorem shows how to compute  X  max 2 .
 T HEOREM 2. The linear system has a unique solution, denoted by  X  z .Let (16) can be analytically computed as: system (22) has a unique solution, denoted by  X  z .Forany  X   X  2 , we can easily verify that  X  z  X  =  X  max 2  X   X  2 and  X  (  X  z )= RR T  X  z  X  R v = 0 . It follows from the op timality condition (21) that  X  z is the optimal solution of (19) for any  X  2  X   X  max
When  X  2  X   X  max 2 ,wehave  X   X  2 ( v )= v  X  R T  X  z from (18). It e as R e = 0 . Thus (24) holds for any  X  2  X   X  max 2 .

The linear system (22) can be efficiently computed in O ( n ) time by using the special tridiagonal structure of the matrix RR well known algorithm for solving the general tridiagonal systems of equations is the Thomas algorithm [29], which consumes approxi-mately 3 n additions and 5 n multiplications. In addition, when con-sidering the special form of RR T in (20), we can apply the Rose algorithm [27], which costs only n multiplications and 3 n ditions, as can be easily observed from Algorithm 2. Evans [7] presented an algorithm similar to the Rose algorithm, and proved that its rounding error is bounded.
 Algorithm 2 The Rose Algorithm Output:  X  z  X  R ( n  X  1)  X  1 satisfying RR T  X  z = u 1: Compute the scalar s =  X  n  X  1 2: Compute  X  z j sequentially using  X  z n  X  1 = u n  X  1 + s and  X  z 3: Obtain  X  z j sequentially using  X  z j = X  z j + X  z j  X  1
To solve (19), we can first compute  X  z , the solution to the linear system (22) by Algorithm 2, and obtain  X  max 2 .If  X  2  X   X   X  z is the solution of (19); otherwise, we apply the algorithm to be discussed in the subsequent subsections for 0  X   X  2 &lt; X  note that, Algorithm 2 shall also be used in the restart technique to be discussed in Section 3.5.
In the literature, there have been quite a few algorithms for solv-ing the bound constrained optimization problem like (19); e.g., [4, 5, 15] and the references therein. However, these algorithms are either for the general quadratic programming or the general opti-mization.

In this paper, we propose to apply the gradient descent [21], and present the algorithm in Algorithm 3. According to [21, Chap-ter 2.2.4], Algorithm 3 converges linearly as where L =2  X  2cos(  X  ( n  X  1) /n ) and  X  =2  X  2cos(  X /n ) are the largest and the smallest eigenvalues of the Hessian RR T , respec-tively. Algorithm 3 can be further accelerated with the Nesterov X  X  method; and we denote the resulting algorithm as SFA N .
Our proposed SFA G can be significantly accelerated with the restart technique (to be discussed in Section 3.5), by exploiting the special  X  X tructures" of the original and the reformulated problems. Before presenting the restart technique, we show in the next subsec-tion how to compute the duality gap for checking the convergence of the algorithm. Algorithm 3 SFA via Gradient Descent (SFA G ) 1: Set L =2  X  2cos(  X  ( n  X  1) /n ) 2: for i =1 to k do 3: Compute g i =  X  ( z i )= RR T z i  X  R v 4: Set z i +1 = P  X  2 ( z i  X  g i /L ) 5: end for
In optimizing (19) via SFA G (see Algorithm 3), SFA N (the ac-celerated version of SFA G via the Nesterov X  X  method), and SFA (SFA with the restart technique to be discussed in the next subsec-tion), it would be desirable that we can check the convergence of the algorithm based on the  X  X oodness" of the approximate solu-tion. To this end, we propose to compute the duality gap for the min-max optimization problem (17), as both (16) and (19) are its resulting problems by eliminating the variable z or x . Let  X  z be an appropriate solution computed by SFA G (or SFA and SFA R ). Note that, we have  X  z  X   X   X  2 from Step 4 of Algo-rithm 3. Let  X  x = v  X  R T  X  z be the appropriate solution computed by (18). We can define the duality gap for (17) at (  X  x ,  X  z ) as:
The following theorem shows that, gap (  X  x ,  X  z ) can be computed  X  x for  X  (  X  ) and f  X  2 (  X  ) , respectively.

T HEOREM 3. The duality gap in (26) can be computed as: In addition, we have Proof: From (16-19), we can establish the following relationships:  X  ( x  X  ,  X  z )  X  max From (26-33), we can write the duality gap as: where the last equality follows from  X  (  X  z )= RR T  X  z using (26-33). This completes the proof.
Although Algorithm 3 has a linear convergence rate, it may con-verge slowly for large n , due to the high condition number For example, when n =10 2 , 10 3 , 10 4 and 10 5 ,wehave L 4  X  10 3 , 4  X  10 5 , 4  X  10 7 and 4  X  10 9 , respectively. In this sub-section, we propose to make use of the special structures of (16), (17), and (19) to restart SFA G for fast convergence; and we call the resulting method as SFA R (SFA via the restart technique). Fig-ure 1 illustrates SFA G ,SFA N and SFA R for solving (19). From this figure, we can clearly observe that, the restart technique can signifi-cantly accelerate the convergence and yield the exact solution using much fewer iterations than SFA G and SFA N .

Our proposed restart technique is based on the so-called support set 3 , motivated by Lemma 1. Specifically, for any z  X   X   X  2 define its support set as: When S is nonempty, we denote the j -th largest element in the set S by s j ,j =1 , 2 ,..., | S | . It is clear that 1  X  s 1 For discussion convenience, we let s 0 =0 and s | S | +1 = n .We note that the following discussion also holds for the case when S [1 : n ] into | S | +1 non-overlapping groups: Let e G j and v G j denote the j -th group of e and v corresponding to the indices in G j , respectively.

Basedonthe support set S , we define the mapping x =  X  ( z ) as: where j =1 , 2 ,..., | S | +1 and we have assumed z 0 = z for presentation convenience.

L EMMA 2. For any z  X   X   X  2 and i  X  S ( z ) , we have: 1) if z =  X  2 ,then v i +1 &gt;v i ; and 2) if z i =  X   X  2 ,then v Proof Let g =  X  ( z ) . For discussion convenience, we add z z n =0 into the ( n  X  1 )-dimensional vector z .Wehave g i =  X  z S ( z ) in (35) that, 1) if z i =  X  2 ,then g i =  X  z i  X  1 ( v i +1  X  v i ) &lt; 0 , which leads to v i +1 &gt;v i ; and 2) if z v
Lemma 2 shows that, for any i  X  S ( z ) , the sign of z i determined by v i +1  X  v i . Next, we show that the optimal solution to (16) can be exactly recovered using the support set S ( z
T HEOREM 4. Let z  X  be the minimizer of (19). Then x minimizer of (16), satisfies Proof Let g  X  =  X  ( z  X  ) . It follows from Lemma 1 and the defini-tion of S in (35) that, g  X  i =0 ,  X  i/  X  S . Based on the relationship R x  X  =  X   X  ( z  X  )=  X  g  X  ,wehave x  X  i = x  X  i ,  X  i, i the matrix R be partitioned into | S | +1 non-overlapping blocks as We can easily get (38) by left multiplying e T G j to both sides of (39), using (37), and incorporating the fact that x  X  i = x i, i  X  G j . This completes the proof.
We call S ( z ) the support set due to the following two reasons: 1) as shall be shown in Theorem 4, S ( z  X  ) supports the exact recovery of x  X  for (16), and 2) S ( z ) directly induces the mapping  X  : z in (37).
Theorem 4 offers an alternative way for computing x  X  from z which is quite different from (18). More specifically, (18) requires that all the entries in z  X  are known; while (38) says that, x be exactly computed, if the support set S ( z  X  ) is known (we have used Lemma 2). In other words, if we have an appropriate solution solution  X  z = z  X  , x =  X  (  X  z ) can be a much better approximate solution than  X  x = v  X  R T  X  z for optimizing f  X  2 (  X  x =  X  (  X  z ) , we compute a restart point z 0 using the relationship x = v  X  R T z 0 . Here, z 0 can be easily computed by solving the linear system RR T z 0 = R v  X  R x . We present the proposed restart technique in Algorithm 4, where Step 4 ensures that z 0 is feasible for (19).
 Algorithm 4 The Restart Technique 1: Compute the support set S (  X  z ) according to (35) 2: Compute x =  X  (  X  z ) according to (37) 3: Calculate z 0 as the solution to RR T z 0 = R v  X  R x 4: Set z 0 = P  X  2 ( z 0 )
We illustrate the proposed Subgr adient Finding Algorithm via the restart technique (SFA R ) in Figure 2, from which we can see that, SFA R recursively calls SFA G (Algorithm 3) and the restart technique (Algorithm 4). For the SFA G block in the proposed SFA R , we set the number of iteration(s) k =1 , as it yields the best performance in our experiments. It is clear that the per itera-tion cost of SFA R is O ( n ) .
We summarize our methodology for solving (16) as follows. We first make use of the dual of the 1 -norm to rewrite the primal prob-lem (16) as the equivalent saddle point problem (17). By using the relationship between the primal and dual variables in (18), we ob-tain the dual problem (19), which is a bound constrained quadratic programming problem and can be solved in linear time by the first-order methods such as gradient descent. To further accelerate the optimization of (19), we propose a restart technique. The underly-obtain a better appropriate point z 0 , the optimization can be greatly accelerated with the restart of z 0 . For the problem discussed in this paper, z 0 is obtained by exploiting the  X  X tructures" of the primal problem (16), the saddle point problem (17), and the dual prob-lem (19). Such a restart technique can potentially be used for other problems, when utilizing the problem  X  X tructures" in a nice way.
Next, we compare our proposed SFA with the modified CD (mCD) proposed in [8] and the path algorithm (pathFLSA) proposed in [12]. First, both mCD and pathFLSA focus on solving the original for-mulation (16); while our proposed SFA focuses on solving the dual problem (19) utilizing the special structures of (16), (17) and (19). Second, in solving  X   X  2 ( v ) , both mCD and pathFLSA need to start from  X  =0 and then increase  X  according to certain strategies until  X  =  X  2 to obtain the path solutions; while our proposed SFA di-rectly solves (19) corresponding to the given  X  2 . We note that, for EFLA in Algorithm 1, we need the efficient computation of  X  corresponding to a given  X  2 rather than the path solutions; and this is also the case for the the online learning algorithms (e.g., FO-LOS [6] and RDA [35]) for solving (1). Third, the starting point of our proposed SFA is quite flexible, thus it can benefit from the  X  X arm" start technique, when used as a building block in EFLA (note that, the solution of FLSA in the previous EFLA iteration can potentially be close to that of FLSA in the next iteration; and Fig-ure 4 illustrates the benefit of the X  X arm" start); while in solving  X  2 ( v ) , both mCD and pathFLSA need to exactly follow the solu-tion path thus they cannot benefit from the  X  X arm" start technique (note that the solution of FLSA in the previous EFLA iteration is not necessarily on the path of the next one).
We first demonstrate the efficiency of the proposed SFA for solv-ing FLSA in Section 4.1, and then the proposed EFLA in Sec-tion 4.2. All experiments were carried out on an Intel(R)Core(TM)2 Duo CPU (E6850) 2.99GHZ processor. The source codes, included in the SLEP package [18], are available online 4 . Illustration of the Proposed SFA We generate a vector v  X  R n with n =10 5 . The entries in v are randomly drawn from the stan-dard normal distribution. By applying Theorem 2, we get  X  300 . 2 with Algorithm 2. All the algorithms start from the origin.
We first compare SFA R with SFA G and SFA N , and present the results in Figure 1. For SFA G (Algorithm 3) and its accelerated version via the Nesterov X  X  method X  X FA N , we run them for 200 iterations; and for SFA R (depicted in Figure 2), we terminate the algorithm until the duality gap is zero. From Figure 1, we can observe that: 1) SFA N converges faster than SFA G ; 2) the duality gaps of SFA G and SFA N are not very small after 200 iterations, especially for large  X  2 ; and 3) SFA R achieves the exact solution within 8 and 9 iterations for  X  2 =0 . 5 and 1, respectively, and thus significantly outperforms both SFA G and SFA N . We attribute the superior performance of SFA R to the restart technique using the special structures of (16), (17) and (19).

Next, we further explore the performance of SFA R under differ-ent values of  X  2 =0.1, 0.5, 1, 2, 3, 5, 10, 20, 200. In Figure 3, we report the duality gap and the number of elements in the support set during the iterations. We can observe from this figure that: 1) SFA R converges within dozens of iterations, and 2) the number of elements in the support set decreases with an increasing  X  Comparison with the Other Algorithms Before comparing the proposed SFA with the other algorithms, we first discuss the effi-ciency of the existing solvers for FLSA. Friedman et al. [8] showed that their proposed modified CD (mCD) outperforms the general solver SQOPT by factors of 50 up to 300 or more. H X efling [12] showed that his pathFLSA 5 is over 100 times faster than the gen-eral solver CVX. In addition, pathFLSA is significantly faster than mCD for problems with size up to 10 6 . Therefore, in this paper, we compare our proposed SFA R with pathFLSA.

We try problems of sizes n =10 2 , 10 3 , 10 4 , 10 5 , 10 For each n , we generate 100 input vectors v  X  R n , whose entries are randomly drawn from the standard normal distribution, and set  X  2 = r  X   X  max 2 ,where  X  max 2 is computed according to Theorem 2, and r =10  X  3 , 10  X  2 , 10  X  1 and 1. For our proposed SFA set the origin as the starting point. We report the average results over 100 runs in Table 1, from which we can observe that, 1) our proposed SFA R is much more efficient than pathFLSA for solving FLSA corresponding to a given parameter  X  2 , 2) our proposed al-gorithm converges within dozens of iterations even for problems of size 10 7 , and 3) with an increasing value of  X  2 = r  X   X  the number of iterations and the computational time for SFA crease, as the starting point (the origin) is much farther away from the solution for the larger  X  2 than that of the small ones.
We would like to emphasize the following two points. First, www.public.asu.edu/~jye02/Software/SLEP cran.r-project.org/web/packages/flsa when the objective is to compute the path solutions for FLSA, pathFLSA should be a better choice than SFA R , as it is special-ized for the path solutions; however, when used as a building block in EFLA, we only need to solve FLSA corresponding to a given  X  , and thus SFA R is a better choice. Second, when used as the building block in EFLA, SFA R can achieve much better practical performance than what reported in Table 1, as we can apply the  X  X arm" start technique, i.e., using the solution obtained from the previous EFLA iteration as the  X  X arm" start for the latter; and we have observed in our experiments that, the average SFA R iterations is usually within 10 (see the following experiments). However, we note that, neither mCD nor pathFLSA can benefit from the  X  X arm" start technique (see the discussion in Section 3.6).
 SFA R as A Building Block for EFLA with the  X  X arm" Start To evaluate the efficiency of SFA R in EFLA, we apply it for solving the following fused Lasso problem: Here, A  X  R m  X  n is a random matrix whose entries are drawn from the normal distribution with zero mean and unit variance, b = A  X  x + is the response,  X  x  X  R n  X  1 is a vector whose en-2.1  X  10  X  3 4.8  X  10  X  2 0.72 9.2 6.2  X  10  X  2 0.76 9.6 -4.2  X  10  X  3 5.9  X  10  X  2 1.1 14 6.1  X  10  X  2 0.76 9.6 -5.1  X  10  X  3 8.7  X  10  X  2 1.5 20 6.1  X  10  X  2 0.76 9.7 -3.5 6.2  X  10  X  2 0.77 9.7 -Ta b l e 2 : Computational time (seconds) and average number of iter-tries are drawn from the normal distribution with zero mean and unit variance, and is the noise vector whose entries are drawn from the normal distribution with zero mean and variance equal to 0.01. We set m = 100 ,  X  1 =  X  2 =0 . 01 , and try different values of n =10 3 , 10 4 and 10 5 .

We run EFLA for 1,000 iterations, and report the results in Ta-ble 2. We can observe from the second row of this table that, the av-erage number of iterations consumed by SFA R is very small (within 10). In Figure 4 , we further report the number of SFA R iterations with increasing EFLA iterations. We observe a similar trend. We attribute the small number of iterations to the usage of the  X  X arm" start, i.e., to solve SFA R , we use the solution z in the previous EFLA iteration as the  X  X arm" start for the successive one.
In rows 3 and 4 of Table 2, we report the the computational time consumed by EFLA and SFA R , from which we can observe that SFA R consumes about 1/3 of the total computational time for the above experiments. If we apply pathFLSA [12] for fulfilling the same task as SFA R , it consumes much more computational time, as shown in the last row of Table 2; and this shall make the EFLA much slower than that with SFA R . From the results in Tables 1 and 2, we can conclude that our proposed SFA R is much more efficient than pathFLSA for solving FLSA (which acts as a building block in the proposed EFLA) corresponding to a given parameter, especially when the  X  X arm" start technique is applied.
We apply the proposed EFLA to several real world applications, with the least squares loss. Specifically, we solve the fused Lasso (40), where each row of A denotes a sample, and each row of b contains the corresponding class label information. Data Description We conduct experiments on the following three data sets: ArrayCGH [28], Prostate Cancer [24], and Leukemias [9].
The ArrayCGH (Array comparative genomic hybridization) [28] is a micro-array based technology that can detect genomic copy number variation (CNV) at different locations along the genome. Each measurement (feature) is the log ratio of CNV, and adjacent features correspond to adjacent locations along the genome. This data set contains ArrayCGH profiles of 57 bladder tumor samples, and each profile contains 2,385 m easurements. Here we consider the tumor grade classification problem, with 12 samples of Grade 1 and 45 samples of higher grades (2 or 3).

The Prostate Cancer [24] data set used in our experiments is based on the protein mass spectro metry, where the features are in-dexed by many time-of-flight values. Time of flight is related to the mass over charge ratio m/z of the constituent proteins in the blood. The data set contains 15,154 measurements of 132 patients, including 63 healthy and 69 with prostate cancer.

The Leukemias [3, 9] is a DNA microarray data set. It contains 7,129 genes and 72 samples: 47 of acute lymphocytic leukaemia and 25 of acute myelogenous leukaemia. Unlike the ArrayCGH and Prostate Cancer data sets, the features in Leukemias have no prespecified order [30]. We follow [30] to reorder the features in the data, using the binary hierarchical clustering; and we call the resulting data set as  X  X eukemias Reordered". Figure 5: Computational time (seconds) on ArrayCGH (top left), Computational Efficiency of EFLA We compare our proposed EFLA with the CVX optimization package [10], for solving the fused Lasso (40). In the comparison, we terminate EFLA till it achieves an objective function value less than or equal to that of CVX. The parameters  X  1 and  X  2 are specified by a 9  X  9 grid sam-pled using the logarithmic scale from the parameter space. We re-port the computational time (seconds) in Figure 5, where the x-axis denotes the different values of  X  2 , and the y-axis represents the total computational time (seconds) corresponding to the given  X  For a given  X  2 , we can solve (40) by applying the solution corre-sponding to the large  X  1 as the  X  X arm"-start to the smaller one; and this is the so-called  X  X arm" start technique widely employed in the literature [8, 16]. From Figure 5, we can observe that EFLA is one or two orders of magnitude faster than the standard solver CVX, es-pecially for larger  X  2 . In addition, the X  X arm" start helps improve the efficiency. The superior efficiency of EFLA attributes to the following reasons: 1) EFLA directly solves the composite func-tion (40) utilizing the composite structure, while CVX is a general solver optimizing the smooth reformulation of (40) by introducing many additional variables and constraints; 2) EFLA enjoys the op-timal convergence rate for the first-order black-box methods; 3) the SFA R developed in Section 3 can efficiently solve FLSA, the key building block of the proposed EFLA (see Tables 1 &amp; 2). Classification Performance We follow [25] to report the leave-one-out performance of the fused Lasso (via EFLA). The param-eters  X  1 and  X  2 are specified by a 9  X  9 grid sampled using the logarithmic scale from the parameter space. The classification er-Table 3: The best leave-one-out accuracy (%) on different data sets by Fused Lasso and Lasso.
 rors corresponding to different parameter values are visualized by a heat map in Figure 6. We also report in Table 3 the best leave-one-out accuracy (%) on different data sets by Fused Lasso and Lasso; and we can observe that Fused Lasso can achieve compa-rable or better classification performance than Lasso, benefited by the additional fused penalty R x 1 . We refer the readers to [30] for detailed comparison between Lasso and Fused Lasso, and [25, 31] for the biological interpretation.
In this paper, we consider solving the class of problems with the fused Lasso penalty, leading to a class of non-smooth and non-separable optimization problems. By treating its objective function as the composite function (composed of one smooth part and the other non-smooth part), we propose to apply the Nesterov X  X  method to develop the Efficient Fused Lasso Algorithm (EFLA), achieving the optimal convergence rate of O (1 /k 2 ) . In the proposed EFLA, a key building block in each iteration is FLSA, for which we pro-pose an efficient Subgradient Finding Algorithm (SFA), equipped with a restart technique for fast convergence. When used as a build-ing block in EFLA, SFA is shown to converge within 10 iterations with the  X  X arm" start. Our empirical evaluations show that, both SFA and EFLA significantly outperform the existing solvers, thus making it applicable for large-scale problems.

We plan to apply the proposed EFLA for the construction of time-varying networks [1], and other applications with either spa-tially or temporally ordered features. In addition, we plan to de-velop the online and stochastic versions of EFLA using FOLOS [6] and RDA [35], where the proposed SFA again acts as a building block. Finally, we plan to extend our methodology to multidimen-sional fused Lasso, where the features form more complex graph structures, e.g., the two-dimensional fused Lasso [8]. This work was supported by NSF IIS-0612069, IIS-0812551, CCF-0811790, NGA HM1582-08-1-0016, NSFC 60905035, and the Office of the Director of National Intelligence (ODNI), Intelli-gence Advanced Research Projects Activity (IARPA), through the US Army.
