 The exploitation of search engines today is a serious issue, but, like it or not, most busi-nesses see it as something that must be done X  X n online business imperative. For many commercial web sites, an increase in search e ngine referrals translates to an increase in sales, revenue and profits. The practices of crafting web pages for the sole purpose of increasing the ranking of these or some affi liated pages, without improving the utility to the viewer, are called spamdexing(web spam) [6]. Web spam seriously deteriorates search engine ranking results, makes it harder to satisfy information need, and frustrates users X  search experience. Combating spamd exing has become one of the top challenges for web search.
 The relation of spamdexing and spam de tection has been likened to an  X  X rm race X . Once an anti-spam technique is developed, new spamdexing techniques will be imple-mented to confuse search engine. Machine learning based methods have shown their superiority for being easy to adapt to newly developed spam techniques [2] [17] . In most machine learning based spam detection a lgorithms, spam identification is treated as a binary classification problem. After extracting content based or hyperlink relevant features, classification algorithms are employed to detect web spam.

In this paper, our focus is placed on finding more effective learning strategy by ex-ploiting the hyperlinks depende nt relation. Generally, more training samples will pro-vide more discriminant information for classification. Based on such consideration, a two-stage classification strategy based on predicted spamicity( PS ) is proposed. Self-learning and label propagation through hyperlink graph are used to expand the training set in this strategy. The samples with high classification confidence or propagation con-fidence will be put into the training set for the next stage learning. The strategy provides a well-founded mechanism to integrate the existing learning algorithms for spam detec-tion. Experiments on WEBSPAM-UK2006 show that the strategy can improve the web spam detection performance effectively.

The remainder of this paper is organized as follows. We review the previous research work in Section 2. In section 3, we describe the proposed two-stage classification strat-egy in detail. Experimentally evaluation of the proposed strategy on the standard col-lection is presented in section 4. At last, we give the concluding remarks and future research directions. Generally, search engine ranks a web page acco rding to the content relevance and the page importance which is computed with link analysis algorithm, such as PageRank. Accordingly, web spam can be broadly classified into two categories: content spam and link spam[11]. Previous work on web spam identification mostly focused on these two types of spam.

Content spam is created for obtaining a h igh relevance score, which refers to the deliberate changes in the content of the pages, including inserting a large number of keywords, content hiding and cloaking. Similar with the methods used in fighting email spam, [5] detected content spam via a number of statistical content-based attributes. These attributes are parts of the features in our experiments.

Link spam tries to unfairly gain a high ranking on search engines for a web page without improving the user experience, by mean of trickily manipulating the link graph [14]. Based on the assumption that good pages seldom point to spam pages, but spam pages may very likely point to good pages. Gyongyi et al. [7] proposed a method to separate reputable pages from spam. The met hod consists of the following processes, firstly, a small set of seed pages were evaluated by experts; then, the link structure of the web was used to discover other pages that are likely to be good.

In recent years, machine learning based detection methods have received much at-tention, which shows its superiority for being easy to adapt to newly developed spam techniques. According to the characteristic of web spam, many link-based features have been extracted to construct automatic classifiers [1] [2] [8], such as transformed PageR-ank [3],TrustRank [7], and Truncated PageRank[1], etc. [19] used a number of features for Web spam filtering based on the occurrence of keywords that are either of high advertisement value or highly spammed. [17] designed several heuristics to decide if a node should be relabeled based on the preclassified result and knowledge about the neighborhood.

Many studies show that properties of neighboring nodes are often correlated with those of a node itself [2] [8] [17]. Literature [2] computed the topological dependencies of spam nodes on standard WEBSPAM-UK2006, the experiment shows that:  X  X on-spam nodes tend to be linked by very few spam nodes, and usually link to no spam nodes X  and  X  X pam nodes are mainly linked by spam nodes X . Based on these facts, we plan to increase the samples in the train set through link topology learning.
In our previous work[9][18], we proposed a predicted spamicity-based ensemble under-sampling strategy for spamdexing detection. Within this strategy, many existing learning algorithms, such as C4.5, bagging and adaboost, can be applied in this strat-egy; distinguishing information involved in the massive reputable websites are fully explored and solves the class-imbalance problem well. Based on the strategy, we will probe new classification method to improve spamdexing detection performance in this paper. A two-stage classification strategy is proposed, which integrates the Self-learning and Link-learning. This method takes full advantage of the information involved in the web topology and expands the training dat a sets for the ultimate spam detection. In this section, we will propose an two-stage classification strategy, which is based on the predicted spamicity( PS )[18]. PS is defined as the probability of a website belong-ing to web spam, which is based on the predicted confidence of classifiers.
The flow chart of the proposed two-stage classification strategy is depicted in Figure 1. The arrows indicate the direction of data flow, and the figures on the line represent the order of the execution process.

Figure 1 illustrates the general bootstrappi ng process. The bootstrapping algorithm aims to improve the classification performance, by integrating examples from the un-labeled data into the labeled data set. In t he first stage, we impl ement a preliminary classification, then converts the most confide ntly predicted examples of each class into a labeled training example. After that, lin k learning is carried out, and the samples with high propagation confidence are converted into labeled ones, too. In the second stage, a new classifier is trained with the expanded train set, then the the remaining unlabeled examples are tested with the learned machin e. In the next part, we will describe the algorithm processes in detail.

The strategy starts with a set of labeled data(train set), and builds a classifier, which then applied on the test set. The instances with a PS value exceeding a certain threshold are added to the labeled set. Taking into account the dependence between samples, we use link learning to further expand the labeled samples set.

As the intention of link learning is to label more unlabeled samples for next step learning, we adopt a more stri ngent restrictions, ie. both i nlink and outlink nodes prop-erties are taking into account. In order to remove the noise hyperlinks, not all the hy-perlinks are taken into account. If the number of hyperlinks from one node(website) to another one is more than 5, we think that there exist a valid link relation between the two nodes. The propagated confidence of node is computed as follows: where out s ( i ) and in s ( i ) are the spam nodes number am ong inlink and outlink of node i , respectively. Correspondingly, out n ( i ) and in n ( i ) are the non-spam nodes number among inlink and outlink of node i , respectively. , is the smoothing factor, whose value belongs to interval (0 , 1) .Thevalueof K 1 and K 2 are decided by the following algorithm: Algorithm 1. Computing the Value of K 1 and K 2
The condition C1 is the boolean conditions fo r selecting the samples with high pre-dicted confidence and condition C2 is the c onditions for selecting the instances with high propagated confidence. For keeping consistency with original data, selected sam-ples must have the same class ratio as the train set.

Algorithm 2 is the detailed implementation of our proposed two-stage classification strategy. Algorithm 2. Two-stage classification strategy for web spam detection 4.1 Data Collection The WEBSPAM-UK2006 [6] collection is used in our experiments, which is a pub-licly available data set. The collection i ncludes 77.9 million pag es, corresponding to roughly 11400 hosts, among which over 8000 hosts have been labeled as  X  X pam X ,  X  X on-spam(normal) X  or  X  X orderline X .

We use all the labeled data with their homepage in the summarized samples[6], where 3810 hosts are marked normal and 553 hosts are marked spam 1 . 4.2 Features The features used for classification involve transformed link-based features[2] and content-based features[5]. All the link-based features are computed with the whole web graph; and the content based features used in the rest of the paper are extracted from the summarized samples containing 3.3 million pages[2].

Similar with [2][18], most of the link-based features are computed for the home page and page in each host with the maximum Page Rank. The content-based features include the number of words in page, amount of anchor text, average length of words, fraction of visible content and compressibility etc [2][5].

The following experiments were run with a combination of all the features mentioned above, and preprocessed with in formation gain. After feature selection, each sample has 198 dimensions attributes. 4.3 Classification Algorithms Most of the existing classification algorithms are applicable in our proposed two-stage strategy. In [18], we proposed an ensemble random under-sampling classification strat-tegy, which exploits the information involved in the large number of reputable websites to full advantage. In the ensemble method, several subsets S 1 ,S 2 , ..., S n are indepen-dently sampled from spam set S . For each subset S i ( i  X  N ) , a classifier C i is trained using S i and non-spam set M . All the results generated by the sub classifiers are com-bined for the final decision. Experimental results on standard WEBSPAM-UK2006 col-lection show that the proposed learning strategy is robust, and can improve the web spam detection performance effectively. Especially when choose adaboost as the base classifier, system achieved the best results. Thereby, we chose ensemble adaboost as base classifier for our two-stage classification test. Decision stump is used as the weak classifier for adaboost. 4.4 Experimental Results 5 times 2-fold cross-validation is run on the data set. The precision, recall, true positive rate(TP), false positive rate(FP), area under ROC curve (AUC) and F-measure were used to measure the performance of the learning algorithms.

We use the usual definition for ROC Area (AUC). An ROC plot is a plot of true positive rate vs. false positive rate as the prediction threshold sweeps through all the possible values. AUC is the area under this curve. AUC of 1 is perfect prediction  X  all positive cases sorted above all negative cases. AUC of 0.5 is random prediction  X  there is no relationship between the predicted values and truth.
 All the results were obtained with M 1 =3 &gt;M 2 =2 &gt;M 3 =1 , N 1 =1 &gt; N under-sampling classification.

Figure. 2 shows the performance of AU C with different feedback sample ratio R in step 4 and 7. The X-axis represents the ratio of annotating unlabeled samples ratio in test set, and the Y-axis is the AUC values. The baselines of AUC were computed with one-stage classification.
From figure. 2, we can find that two-stage classification boosts the detection perfor-mance. The classification strategy is robust, especially, when R  X  [0 . 1 , 0 . 7] ,theim-provement is notable. It can be seen from figure. 2 that the AUC is poor with R&gt; 0 . 8 . Actually, the phenomenon is understandable because bigger R value means more un-labeled samples are put into train set accord ing to the predicted confidence, then there will be more samples are incorrectly labeled. When the ratio of wrong samples becomes more and more in the new train set, the performance decline is inevitable.
To gain more understanding of the performance comparison, we further list in Table 1 all the assessment criteria with two-stage classification and one-stage classification. All the data is computed with R =0 . 5 .

From the table, we can find that our proposed method can boost the detection per-formance effective, especially TP(Recall) , F-measure and AUC. For more conservative spam filtering application, cost sensitive cl assification can be employed. The false pos-itive changes with different cost matrix. In this paper, a two-stage classification strategy is proposed. Self-learning and link learning through hyperlink graph are used to expand the training set in this strategy. The samples with high classification confi dence or propagation confidence are put into training set for the next stage learning. Experimental results on standard WEBSPAM-UK2006 collection show that the proposed learning strategy is feasible and effective.
Future work involves testing more learning algorithms with the proposed strategy, taking the weight information of hyperli nks into account and improving the strategy from two-stage classification to multi-stage classification.
 This work is supported by the National High Technology Research and Development Program of China (863 Program) under Grant No 2006AA010106.

