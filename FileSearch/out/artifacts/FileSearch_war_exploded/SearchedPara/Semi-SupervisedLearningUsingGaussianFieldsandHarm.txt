 In man y traditional approaches to machine learning, a tar -get function is estimated using labeled data, which can be thought of as examples given by a  X  X eacher X  to a  X  X tudent.  X  Labeled examples are often, howe ver, very time consum-ing and expensi ve to obtain, as the y require the efforts of human annotators, who must often be quite skilled. For in-stance, obtaining a single labeled example for protein shape classification, which is one of the grand challenges of bio-logical and computational science, requires months of ex-pensi ve analysis by expert crystallographers. The problem of effecti vely combining unlabeled data with labeled data is therefore of central importance in machine learning. The semi-supervised learning problem has attracted an in-creasing amount of interest recently , and several novel ap-proaches have been proposed; we refer to (See ger , 2001) for an overvie w. Among these methods is a promising fam-ily of techniques that exploit the  X  X anifold structure X  of the data; such methods are generally based upon an assumption that similar unlabeled examples should be given the same classification. In this paper we introduce a new approach to semi-supervised learning that is based on a random field model defined on a weighted graph over the unlabeled and labeled data, where the weights are given in terms of a sim-ilarity function between instances.
 Unlik e other recent work based on ener gy minimization and random fields in machine learning (Blum &amp; Cha wla, 2001) and image processing (Bo ykov et al., 2001), we adopt Gaussian fields over a continuous state space rather than random fields over the discrete label set. This  X  X e-laxation X  to a continuous rather than discrete sample space probable configuration of the field is unique, is character -ized in terms of harmonic functions, and has a closed form solution that can be computed using matrix methods or loop y belief propagation (W eiss et al., 2001). In contrast, for multi-label discrete random fields, computing the low-est ener gy configuration is typically NP-hard, and approxi-mation algorithms or other heuristics must be used (Bo ykov et al., 2001). The resulting classification algorithms for Gaussian fields can be vie wed as a form of nearest neigh-bor approach, where the nearest labeled examples are com-puted in terms of a random walk on the graph. The learning methods introduced here have intimate connections with random walks, electric netw orks, and spectral graph the-ory , in particular heat kernels and normalized cuts. In our basic approach the solution is solely based on the structure of the data manifold, which is deri ved from data features. In practice, howe ver, this deri ved manifold struc-ture may be insuf ficient for accurate classification. We sho w how the extra evidence of class priors can help classi-fication in Section 4. Alternati vely , we may combine exter -nal classifiers using verte x weights or  X  X ssignment costs,  X  as described in Section 5. Encouraging experimental re-sults for synthetic data, digit classification, and text clas-sification tasks are presented in Section 7. One dif ficulty with the random field approach is that the right choice of graph is often not entirely clear , and it may be desirable to learn it from data. In Section 6 we propose a method for learning these weights by entrop y minimization, and sho w the algorithm X  s ability to perform feature selection to better characterize the data manifold. We suppose there are labeled points and unlabeled points ; typically .
 Let gin, we assume the labels are binary: a connected graph ing to the data points, with nodes sponding to the labeled points with labels , and nodes beled points. Our task is to assign labels to nodes assume an of the graph is given. For example, when weight matrix can be where as a vector hyperparameters for each dimension. Thus, nearby points in Euclidean space are assigned lar ge edge weight. Other weightings are possible, of course, and may be more appro-priate when is discrete or symbolic. For our purposes the matrix Figure 1).
 Our strate gy is to first compute a real-valued function then assign labels based on ues Intuiti vely , we want unlabeled points that are nearby in the graph to have similar labels. This moti vates the choice of the quadratic ener gy function To assign a probability distrib ution on functions the Gaussian field temperature X  parameter , and all functions constrained to It is not dif ficult to sho w that the minimum ener gy function on the labeled data points Laplacian , given in matrix form as diag and The harmonic property means that the value of unlabeled data point is the average of points: which is consistent with our prior notion of smoothness of with respect to the graph. Expressed slightly dif ferently , principle of harmonic functions (Do yle &amp; Snell, 1984), unique and is either a constant or it satisfies for To compute the harmonic solution explicitly in terms of matrix operations, we split the weight matrix ilarly Letting labeled data points, the harmonic solution to In this paper we focus on the abo ve harmonic function as a basis for semi-supervised classification. Ho we ver, we em-phasize that the Gaussian random field model from which this function is deri ved pro vides the learning frame work with a consistent probabilistic semantics.
 In the follo wing, we refer to the procedure described abo ve as harmonic ener gy minimization , to underscore the har -monic property (3) as well as the objecti ve function being minimized. Figure 2 demonstrates the use of harmonic en-ergy minimization on two synthetic datasets. The left figure sho ws that the data has three bands, with and ener gy minimization clearly follo ws the structure of data, while obviously methods such as kNN would fail to do so. As outlined briefly in this section, the basic frame work pre-sented in the pre vious section can be vie wed in several fun-damentally dif ferent ways, and these dif ferent vie wpoints pro vide a rich and complementary set of techniques for rea-soning about this approach to the semi-supervised learning problem. 3.1. Random Walks and Electric Netw orks Imagine a particle walking along the graph from an unlabeled node bility ticle hits a labeled node. Then the particle, starting from node label 1. Here the labeled data is vie wed as an  X  X bsorbing boundary X  for the random walk.
 This vie w of the harmonic solution indicates that it is closely related to the random walk approach of Szummer and Jaakk ola (2001), howe ver there are two major dif fer -ences. First, we fix the value of and second, our solution is an equilibrium state, expressed in terms of a hitting time, while in (Szummer &amp; Jaakk ola, 2001) the walk crucially depends on the time parameter . We will return to this point when discussing heat kernels. An electrical netw ork interpretation is given in (Do yle &amp; Snell, 1984). Imagine the edges of conductance voltage source, and points labeled is the voltage in the resulting electric netw ork on each of the unlabeled nodes. Furthermore dissipation of the electric netw ork harmonic property here follo ws from Kirchof f X  X  and Ohm X  s laws, and the maximum principle then sho ws that this is precisely the same solution obtained in (5) . 3.2. Graph Kernels The solution tral graph theory . The heat kernel with time parameter on the graph the solution to the heat equation on the graph with initial conditions being a point source at and Laf ferty (2002) propose this as an appropriate kernel for machine learning with cate gorical data. When used in a kernel method such as a support vector machine, the kernel classifier solution to the heat equation with initial heat sources on the labeled data. The time parameter must, howe ver, be chosen using an auxiliary technique, for example cross-validation.
 Our algorithm uses a dif ferent approach which is indepen-dent of , the dif fusion time. Let Laplacian restricted to the unlabeled nodes in the heat kernel on this submatrix: Dirichlet boundary conditions on the labeled nodes. The Gr een X  s function is the inverse operator of the restricted Laplacian, the inte gral over time of the heat kernel : The harmonic solution (5) can then be written as Expression (7) sho ws that this approach can be vie wed as a kernel classifier with the kernel and a specific form of kernel machine. (See also (Chung &amp; Yau, 2000), where a normalized Laplacian is used instead of the combinatorial Laplacian.) From (6) we also see that the spectrum of is a connection to the work of Chapelle et al. (2002), who ma-nipulate the eigen values of the Laplacian to create various kernels. A related approach is given by Belkin and Niyogi (2002), who propose to regularize functions on ing the top to the smallest eigen values, thus obtaining the best fit to in the least squares sense. We remark that our labeled data exactly , while the order not. 3.3. Spectral Clustering and Graph Mincuts The normalized cut approach of Shi and Malik (2000) has as its objecti ve function the minimization of the Raleigh quotient subject to the constraint smallest eigen vector of the generalized eigen value problem the normalized cut to specify which points should be in the same group. Since labeled data can be encoded into such pairwise grouping constraints, this technique can be applied to semi-supervised learning as well. In general, when data points are tightly clustered in the eigenspace spanned by the first few eigen vectors of &amp; Shi, 2001), leading to various spectral clustering algo-rithms.
 Perhaps the most interesting and substantial connection to the methods we propose here is the graph mincut approach proposed by Blum and Cha wla (2001). The starting point for this work is also a weighted graph supervised learning problem is cast as one of finding a minimum (with lar ge weight) to a special source node , and positi ve labeled data is connected to a special sink node . A mini-mum -cut, which is not necessarily unique, minimizes the and corresponds to a function solutions can be obtained using linear programming. The corresponding random field model is a  X  X raditional X  field over the label space the labeled entries. Because of this constraint, approxima-tion methods based on rapidly mixing Mark ov chains that apply to the ferromagnetic Ising model unfortunately can-not be used. Moreo ver, multi-label extensions are generally NP-hard in this frame work. In contrast, the harmonic so-lution can be computed efficiently using matrix methods, even in the multi-label case, and inference for the Gaussian random field can be efficiently and accurately carried out using loop y belief propagation (W eiss et al., 2001). To go from assign label 1 to node wise. We call this rule the harmonic thr eshold (abbre viated  X  X hresh X  belo w). In terms of the random walk interpreta-tion, if more lik ely to reach a positi vely labeled point before a neg-atively labeled point. This decision rule works well when the classes are well separated. Ho we ver in real datasets, classes are often not ideally separated, and using tends to produce severely unbalanced classification. The problem stems from the fact that the data manifold, is often poorly estimated in practice and does not reflect the classification goal. In other words, we should not  X  X ully trust X  the graph structure. The class priors are a valuable piece of complementary information. Let X  s assume the desirable proportions for classes 1 and 0 are and by an  X  X racle X  or estimated from labeled data. We adopt a simple procedure called class mass normalization (CMN) to adjust the class distrib utions to match the priors. Define the mass of class 1 to be to be masses so that an unlabeled point iff This method extends naturally to the general multi-label case. Often we have an external classifier at hand, which is con-structed on labeled data alone. In this section we suggest how this can be combined with harmonic ener gy minimiza-tion. Assume the external classifier produces labels the unlabeled data; can be 0/1 or soft labels in combine with harmonic ener gy minimization by a sim-ple modification of the graph. For each unlabeled node the original graph, we attach a  X  X ongle X  node which is a la-beled node with value to its dongle be , and discount all other transitions from by on this augmented graph. Thus, the external classifier in-troduces  X  X ssignment costs X  to the ener gy function, which play the role of verte x potentials in the random field. It is not dif ficult to sho w that the harmonic solution on the augmented graph is, in the random walk vie w, We note that throughout the paper we have assumed the labeled data to be noise free, and so clamping their values mak es sense. If there is reason to doubt this assumption, it would be reasonable to attach dongles to labeled nodes as well, and to mo ve the labels to these new nodes. Pre viously we assumed that the weight matrix and fix ed. In this section, we investigate learning weight functions of the form given by equation (1) . We will learn the sho wn to be useful as a feature selection mechanism which better aligns the graph structure with the data. The usual parameter learning criterion is to maximize the lik elihood of labeled data. Ho we ver, the lik elihood crite-rion is not appropriate in this case because the labeled data are fix ed during training, and moreo ver lik eli-hood doesn X  t mak e sense for the unlabeled data because we do not have a generati ve model. We propose instead to use aver age label entr opy as a heuristic criterion for parameter learning. The average label entrop y defined as where is the entrop y of the field at the indi vidual unlabeled data point relying on the maximum principle of harmonic functions which guarantees that entrop y implies that the intuition that a good perparameters There are of course man y arbitrary labelings of the data that have low entrop y, which might suggest that this criterion will not work. Ho we ver, it is important to point out that we are constraining arbitrary low entrop y labelings are inconsistent with this constraint. In fact, we find that the space of low entrop y labelings achie vable by harmonic ener gy minimization is small and lends itself well to tuning the There is a complication, howe ver, which is that has a minimum at 0 as zero, the tail of the weight function (1) is increasingly sen-siti ve to the distance. In the end, the label predicted for an unlabeled example is dominated by its nearest neighbor X  s label, which results in the follo wing equi valent labeling procedure: (1) starting from the labeled data set, find the unlabeled point that is closest to some labeled point ; (2) label with  X  X  label, put in the labeled set and re-peat. Since these are hard labels, the entrop y is zero. This solution is desirable only when the classes are extremely well separated, and can be expected to be inferior other -wise.
 This complication can be avoided by smoothing the tran-sition matrix. Inspired by analysis of the PageRank algo-rithm in (Ng et al., 2001b), we replace matrix with entries We use gradient descent to find the hyperparameters minimize . The gradient is computed as where the values using the fact that tained by normalizing the weight matrix Finally , In the abo ve deri vation we use rectly; that is, rate class prior information, or combine harmonic ener gy minimization with other classifiers, it mak es sense to min-imize entrop y on the combined probabilities. For instance, if we incorporate a class prior using CMN, the probability is given by and we use this probability in place of deri vation of the gradient descent rule is a straightforw ard extension of the abo ve analysis. We first evaluate harmonic ener gy minimization on a hand-written digits dataset, originally from the Cedar Buf falo binary digits database (Hull, 1994). The digits were pre-processed to reduce the size of each image down to a ing, with pix el values ranging from 0 to 255 (Le Cun et al., 1990). Each image is thus represented by a 256-dimensional vector . We compute the weight matrix (1) with 10 trials. In each trial we randomly sample labeled data from the entire dataset, and use the rest of the images as unlabeled data. If any class is absent from the sampled la-beled set, we redo the sampling. For methods that incorpo-rate class priors , we estimate from the labeled set with Laplace ( X  X dd one X ) smoothing.
 We consider the binary problem of classifying digits  X 1 X  vs.  X 2,  X  with 1100 images in each class. We report aver-age accurac y of the follo wing methods on unlabeled data: thresh, CMN, 1NN, and a radial basis function classifier (RBF) which classifies to class 1 iff RBF and 1NN are used simply as baselines. The results are sho wn in Figure 3. Clearly thresh performs poorly , because the values of ity of examples are classified as digit  X 1 X . This sho ws the inadequac y of the weight function (1) based on pix el-wise Euclidean distance. Ho we ver the relati ve rankings of are useful, and when coupled with class prior information significantly impro ved accurac y is obtained. The greatest impro vement is achie ved by the simple method CMN. We could also have adjusted the decision threshold on thresh X  s solution method is inferior to CMN due to the error in estimating , and it is not sho wn in the plot. These same observ ations are also true for the experiments we performed on several other binary digit classification problems.
 We also consider the 10-w ay problem of classifying digits  X 0 X  through  X 9 X . We report the results on a dataset with in-tentionally unbalanced class sizes, with 455, 213, 129, 100, 754, 970, 275, 585, 166, 353 examples per class, respec-tively (noting that the results on a balanced dataset are sim-ilar). We report the average accurac y of thresh, CMN, RBF, and 1NN. These methods can handle multi-w ay classifica-fashion. As the results in Figure 3 sho w, CMN again im-pro ves performance by incorporating class priors. Ne xt we report the results of document cate gorization ex-periments using the 20 newsgroups dataset. We pick three binary problems: PC (number of documents: 982) vs. MA C (961), MS-W indo ws (958) vs. MA C, and base-ball (994) vs. hock ey (999). Each document is minimally processed into a  X  X f.idf  X  vector , without applying header re-mo val, frequenc y cutof f, stemming, or a stopw ord list. Two documents are connected by an edge if is among  X  X  10 nearest neighbors or if is among  X  X  10 nearest neigh-bors, as measured by cosine similarity . We use the follo w-ing weight function on the edges: We use one-nearest neighbor and the voted perceptron al-gorithm (Freund &amp; Schapire, 1999) (10 epochs with a lin-ear kernel) as baselines X  X ur results with support vector ma-chines are comparable. The results are sho wn in Figure 4. As before, each point is the average of 10 random tri-als. For this data, harmonic ener gy minimization performs much better than the baselines. The impro vement from the class prior , howe ver, is less significant. An explanation for why this approach to semi-supervised learning is so effec-tive on the newsgroups data may lie in the common use of quotations within a topic thread: document of document , although documents far apart in the thread may be quite dif ferent, the y are link ed by edges in the graphical repre-sentation of the data, and these links are exploited by the learning algorithm. 7.1. Incor porating Exter nal Classifiers We use the voted-perceptron as our external classifier . For each random trial, we train a voted-perceptron on the la-beled set, and apply it to the unlabeled set. We then use the 0/1 hard labels for dongle values , and perform harmonic ener gy minimization with (10). We use We evaluate on the artificial but dif ficult binary problem of classifying odd digits vs. even digits; that is, we group  X 1,3,5,7,9 X  and  X 2,4,6,8,0 X  into two classes. There are 400 images per digit. We use second order polynomial kernel in the voted-perceptron, and train for 10 epochs. Figure 3 sho ws the results. The accurac y of the voted-perceptron on unlabeled data, averaged over trials, is mark ed VP in the plot. Independently , we run thresh and CMN. Ne xt we combine thresh with the voted-perceptron, and the result is mark ed thresh+VP. Finally , we perform class mass nor -malization on the combined result and get CMN+VP. The combination results in higher accurac y than either method alone, suggesting there is complementary information used by each. 7.2. Lear ning the Weight Matrix To demonstrate the effects of estimating dataset are sho wn in Figure 5. The upper grid is slightly tighter than the lower grid, and the y are connected by a few data points. There are two labeled examples, mark ed with lar ge symbols. We learn the optimal length scales for this dataset by minimizing entrop y on unlabeled data. To simplify the problem, we first tie the length scales in the two dimensions, so there is only a single parameter to learn. As noted earlier , without smoothing, the entrop y approaches the minimum at 0 as ditions, the results of harmonic ener gy minimization are usually undesirable, and for this dataset the tighter grid  X  X n vades X  the sparser one as sho wn in Figure 5(a). With smoothing, the  X  X uisance minimum X  at 0 gradually disap-pears as the smoothing factor gro ws, as sho wn in Figure 5(c). When we set bits at length scale is sho wn in Figure 5(b), which is able to dis-tinguish the structure of the two grids.
 If we allo w a separate learning is more dramatic. With the same smoothing of and we reach a minimum entrop y of 0.619 bits. In this case gorithm has identified the -direction as irrele vant, based on both the labeled and unlabeled data. Harmonic ener gy minimization under these parameters gives the same clas-sification as sho wn in Figure 5(b).
 Ne xt we learn digits dataset. For this problem we minimize the entrop y with CMN probabilities (15). We randomly pick a split of 92 labeled and 2108 unlabeled examples, and start with all dimensions sharing the same periments. Then we compute the deri vatives of dimension separately , and perform gradient descent to min-imize the entrop y. The result is sho wn in Table 1. As entrop y decreases, the accurac y of CMN and thresh both increase. The learned Figure 6 range from 181 (black) to 465 (white). A small (black) indicates that the weight is more sensiti ve to varia-tions in that dimension, while the opposite is true for lar ge a white  X 2 X  in this figure; that is, the learned parameters start 0.6931 97.25 0.73 % 94.70 1.19 % exaggerate variations within class  X 1 X  while suppressing variations within class  X 2 X . We have observ ed that with the def ault parameters, class  X 1 X  has much less variation than class  X 2 X ; thus, the learned parameters are, in effect, compensating for the relati ve tightness of the two classes in feature space. We have introduced an approach to semi-supervised learn-ing based on a Gaussian random field model defined with respect to a weighted graph representing labeled and unla-beled data. Promising experimental results have been pre-sented for text and digit classification, demonstrating that the frame work has the potential to effecti vely exploit the structure of unlabeled data to impro ve classification accu-rac y. The underlying random field gives a coherent proba-bilistic semantics to our approach, but this paper has con-centrated on the use of only the mean of the field, which is characterized in terms of harmonic functions and spectral graph theory . The fully probabilistic frame work is closely related to Gaussian process classification, and this connec-tion suggests principled ways of incorporating class priors and learning hyperparameters; in particular , it is natural to apply evidence maximization or the generalization er-ror bounds that have been studied for Gaussian processes (See ger , 2002). Our work in this direction will be reported in a future publication.
 Belkin, M., &amp; Niyogi, P. (2002). Using manifold structure for partially labelled classification. Advances in Neur al Information Processing Systems, 15 .
 Blum, A., &amp; Cha wla, S. (2001). Learning from labeled and unlabeled data using graph mincuts. Proc. 18th Interna-tional Conf . on Mac hine Learning .
 Bo ykov, Y., Veksler , O., &amp; Zabih, R. (2001). Fast approx-imate ener gy minimization via graph cuts. IEEE Trans. on Pattern Analysis and Mac hine Intellig ence , 23 . kernels for semi-supervised learning. Advances in Neu-ral Information Processing Systems, 15 .
 Chung, F., &amp; Yau, S. (2000). Discrete Green X  s functions. Journal of Combinatorial Theory (A) (pp. 191 X 214). Do yle, P., &amp; Snell, J. (1984). Random walks and electric networks . Mathematical Assoc. of America.
 fication using the perceptron algorithm. Mac hine Learn-ing , 37(3) , 277 X 296.
 Hull, J. J. (1994). A database for handwritten text recog-nition research. IEEE Transactions on Pattern Analysis and Mac hine Intellig ence , 16 .
 graphs and other discrete input spaces. Proc. 19th Inter -national Conf . on Mac hine Learning .
 Le Cun, Y., Boser , B., Denk er, J. S., Henderson, D., Ho ward, R. E., Ho ward, W., &amp; Jack el, L. D. (1990).
Handwritten digit recognition with a back-propagation netw ork. Advances in Neur al Information Processing Systems, 2 .
 Meila, M., &amp; Shi, J. (2001). A random walks vie w of spec-tral segmentation. AIST ATS .
 Ng, A., Jordan, M., &amp; Weiss, Y. (2001a). On spectral clus-tering: Analysis and an algorithm. Advances in Neur al Information Processing Systems, 14 .
 Ng, A. Y., Zheng, A. X., &amp; Jordan, M. I. (2001b). Link analysis, eigen vectors and stability . International Joint Confer ence on Artificial Intellig ence (IJCAI) . See ger , M. (2001). Learning with labeled and unlabeled data (Technical Report). Uni versity of Edinb urgh. See ger , M. (2002). PAC-Bayesian generalization error bounds for Gaussian process classification. Journal of Mac hine Learning Resear ch , 3 , 233 X 269.
 Shi, J., &amp; Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Mac hine Intellig ence , 22 , 888 X 905.
 sification with Mark ov random walks. Advances in Neu-ral Information Processing Systems, 14 .
 Weiss, Y., , &amp; Freeman, W. T. (2001). Correctness of belief propagation in Gaussian graphical models of arbitrary topology . Neur al Computation , 13 , 2173 X 2200. Yu, S. X., &amp; Shi, J. (2001). Grouping with bias. Advances
