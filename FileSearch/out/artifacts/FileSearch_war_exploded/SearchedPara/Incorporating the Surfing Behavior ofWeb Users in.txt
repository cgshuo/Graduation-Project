 In large-scale commercial web search engines, estimating the importance of a web page is a crucial ingredient in rank-ing web search results. So far, to assess the importance of web pages, two different types of feedback have been taken into account, independent of each other: the feedback ob-tained from the hyperlink structure among the web pages (e.g., PageRank) or the web browsing patterns of users (e.g., BrowseRank). Unfortunately, both types of feedback have certain drawbacks. While the former lacks the user prefer-ences and is vulnerable to malicious intent, the latter suffers from sparsity and hence low web coverage. In this work, we combine these two types of feedback under a hybrid page ranking model in order to alleviate the above-mentioned drawbacks. Our empirical results indicate that the proposed model leads to better estimation of page importance accord-ing to an evaluation metric that relies on user click feedback obtained from web search query logs. We conduct all of our experiments in a realistic setting, using a very large scale web page collection (around 6.5 billion web pages) and web browsing data (around two billion web page visits). H.3.3 [ Information Storage Systems ]: Information Re-trieval Systems Algorithms, Performance, Experimentation, Human Factors Page quality, web search, ranking, PageRank, BrowseRank
Query-dependent features, such as BM25, are successfully used in IR systems to estimate the degree of relevance be-tween a given query and a document. In the context of large-scale web search engines, however, quantifying only the rel-evance is not adequate. The large size of the Web and high variation in content quality require distinguishing the impor-tance of web pages independent of the query. Hence, most web search engines incorporate query-independent page im-portance scores into their ranking algorithms.

PageRank [22] is perhaps the most well-known and widely used technique for computing web page importance. The ba-sic idea behind this technique is to compute the importance of a web page based on the quantity of the links received from other pages as well as the quality of those referring pages. Although PageRank has found many important use cases, it has two serious drawbacks. First, PageRank solely relies on the hyperlink structure of the Web without incor-porating any kind of feedback from the real users surfing the Web. Therefore, all pages are treated equally, ignoring their importance for end users or the likelihood of being visited by a web surfer [9]. Second, since the hyperlink structure is mainly created by the web site owners, it is subject to manipulation. As an example, link farms can be created to artificially boost the importance of certain web pages, mak-ing PageRank vulnerable to link spam [12].

An interesting alternative to PageRank is to exploit the web surfing behavior of users to assess the importance of web pages (e.g., BrowseRank [21]). In this approach, a vir-tual link structure is created between web pages based on the web browsing patterns of users, i.e., the transitions they make between different pages when surfing the Web. Such patterns can be obtained by mining navigational user activ-ity that is tracked by the toolbar applications, commonly installed in web browsers. This approach provides better quality feedback about page importance and also solves the previously mentioned spam problem associated with Page-Rank. However, the web browsing patterns extracted from the toolbar logs are very sparse. Hence, many web pages (especially, the less popular web pages) are not covered and their scores cannot be computed.

One of the main objectives of this work is to investigate whether exploiting web and user feedback at the same time (i.e., using both web data and browsing data) improves the quality of page rankings over using only one type of feedback. To this end, we define a discrete-time Markov chain con-structed by aggregating web and browsing data with prop-erly scaled page transition probabilities. Importance scores of pages are estimated using the standard procedure fol-lowed in PageRank computations. We refer to the proposed technique as PBRank (PageBrowseRank). All of our exper-iments are conducted in a large scale and realistic setting.
Our contributions can be summarized as follows:
The following are the selected findings of our work:
The rest of the paper is organized as follows. In Sec-tion 2, we provide some background on the PageRank and BrowseRank techniques. The proposed hybrid rank-ing model, PBRank, is presented in Section 3. We present our performance evaluation metrics in Section 4. Then, in Section 5, we provide the characteristics of our data. The results of conducted experiments are presented in Section 6. A brief survey of related work is given in Section 7. Finally, we conclude the paper in Section 8.
Pagerank. PageRank [22] is motivated by the aca-demic citation literature. The main idea in this technique is to assign higher scores to pages that receive many links from other important pages which have relatively few out-links [2]. The computation of scores relies on a probabilistic model known as the random surfer model, where the score of a page is defined by the steady-state probability that the surfer will be at that particular page at some time step in the future. This model consists of a Markov chain induced by a random walk on a web graph having n vertices. Each state of the chain corresponds to a different vertex in the web graph. A transition matrix P = ( p ij ) is associated with this chain such that where L i denotes the set of out-links of page i . Given this transition matrix, the PageRank vector p = ( p i ), where p indicates the score of page i , can be computed by find-ing the Markov chain X  X  stationary distribution that satisfies p = P T p , i.e., the principal eigenvector of the chain. The solution can be obtained through a series of iterations of the form p k +1 = P T p k using the power method [10]. The existence of a solution, i.e., the convergence of iterations, requires the P matrix to be stochastic, irreducible, and ape-riodic, neither of which are guaranteed for P .

The reason behind matrix P for not being stochastic is the presence of sink (or so-called dangling) pages with no out-links. Although there are other possibilities [3, 16, 17], the common solution to this problem is to add artificial links from such pages to every other page in the Web [22]. This results in a stochastic transition matrix P 0 , computed as where d = ( d i ) is a dangling page vector (if i is a dangling page, d i = 1; otherwise, d i = 0) and v = ( v i ) is a vector, where v indicates the transition probability from dangling pages to a specific page i . Typically, the transition probabilities are set to 1 /n for all pages. The resulting matrix P stochastic, but not irreducible. Applying a similar technique on P 0 , an irreducible stochastic transition matrix P 00 obtained, also guaranteeing aperiodicity as Here, e n is a vector of size n containing all ones.  X  denotes the probability that the surfer will follow one of the links in the current page while (1  X   X  ) is the probability that the surfer will jump to a page that is not necessarily linked by the current page. In practice,  X  values between 0.85 and 0.9 are used. The t = ( t i ) vector is referred to as the teleportation vector, where t i indicates the probability of jumping to page i . Typically, this probability is set to 1 /n for all pages. In case of personalized or topical teleportation vectors, non-uniform jump probabilities can also be used [13].

BrowseRank. BrowseRank [21] mainly relies on the same principles with PageRank. However, it is based on a continuous-time Markov process, which exploits the stay-ing times of users on pages. In this technique, the browsing graph is constructed as G = ( V , W ), where V is the set of vertices representing web pages visited by users and W is a set of directed edges indicating the visit patterns between pages. Vertices are associated with staying times of users on respective pages and the teleportation probabilities of those pages. Each edge is weighted with the number of visits be-tween the two pages corresponding to its end vertices.
Given the above-mentioned browsing graph, a continuous-time Markov process is defined. This model is then con-verted into a discrete-time Markov process whose station-ary distribution is estimated using the power method. The details of this conversion are too technical to be discussed here and we refer the reader to [21] for further details. In-stead, herein, we briefly discuss how the transitions of users between different pages are obtained since this step is some-what different than the procedure we adopted in our work.
In BrowseRank, the transitions that users make between different pages are obtained by relying on the page visits observed in individual web browsing sessions of users. A browsing session is composed of a series of page visits that are sorted in increasing order of timestamps. Initially, an edge ( v i ,v j ) is added to W for every pair ( i , j ) of pages such that there is no other page visited between i and j . Afterwards, an edge ( v i ,v j ) is removed from W if the user visited page j by typing its URL in the browser X  X  navigation bar or if page j is visited more than 30 minutes after page i is visited.
The main idea behind PBRank is to combine two dif-ferent types of feedback, i.e., those provided by the web data and browsing data in a meaningful way. Our goal is to come up with a simple extension to the standard procedure summarized in Section 2, leaving the theoretical foundations unchanged. To this end, we use a transition matrix X corre-sponding to the pages in the union of the web and browsing data. X is a square matrix of size m  X  m and is expressed as a linear combination of two other matrices of the same size: Here, P 00 is an m  X  m version of the final PageRank matrix used in the power method iterations (see Eq. 3), i.e., this matrix is created based on the web feedback. In addition, using the user feedback, we define another matrix B 00 , which we will describe next.  X  is a constant in the [0 , 1] range and is used to adjust the influence of one type of feedback over the other. The page importance scores can be obtained by finding the principal eigenvector of X using the power method as usual.
 We form the B 00 matrix in a similar fashion to Eq. 3: where  X  and r = ( r i ) are the counterparts of the  X  con-stant and the t vector in Eq. 3, respectively. We use biased teleportation probabilities in r , instead of uniformly setting them to 1 /n as in t . The teleportation probability r i particular page i is computed as where T i denotes the number of visits to page i by means other than following a link in a page. This way, the jumping behavior of the surfer is biased towards more popular pages. Here, we add one to visit counts for smoothing purposes.
Following the idea in [9],  X  can be computed as where V j denotes the total visit count of page j . The  X  con-stant reflects the users X  tendency to reach a page by following the hyperlinks in web pages.
 The B 0 matrix is computed by the following equation: where d and v are defined as before (see Eq. 2). The prob-abilities in the page transition matrix B = ( b ij ) are set de-pending on the likelihood of a hyperlink being followed by users. Therefore, the links within a page are not treated equally as in Eq. 1. Instead, the transition probability from page i to page j is computed in a biased manner by taking into account the share of the click volume of page j in the overall click volume observed on page i as where V ij is the click volume from page i towards page j .
PBRank can be considered as a variant of BrowseRank since both techniques use page visit probabilities extracted from browsing data. In practice, one may prefer PBRank to BrowseRank because of the following reasons. First, as we will show later in Section 6, PBRank achieves a bet-ter coverage of web pages than BrowseRank due to the use of web data in scoring computations, i.e., a larger number of pages receive non-zero scores. Second, PBRank X  X  imple-mentation is easier than the implementation of BrowseRank, which employs a relatively more sophisticated continuous-time Markov model. Finally, the transition probabilities computed in PBRank are accurate values computed over actual user clicks on links. The transition probabilities com-puted in BrowseRank, however, are only approximations be-cause they are computed based on a timestamp-sorted se-quence of page visits in user sessions, not the links that are actually followed by users. Given that many users browse the Web by opening multiple browser tabs [14] and concur-rently following links in different tabs, a time-ordered se-quence of page visits may not be sufficient to obtain the ac-tual transitions between pages. Hence, the transition prob-abilities computed in BrowseRank may not reflect the true surfing patterns of users.

We note that the existence of a solution is guaranteed since the X matrix is irreducible and aperiodic because both summation terms in Eq. 4 already have these properties. When  X  = 0 or  X  = 1, X may not be row-stochastic, but this does not prevent the convergence of iterations. If  X  is set to zero or one in Eq. 4, PBRank reduces to a discrete-time variant of BrowseRank or PageRank, respectively. As we will see in Section 6, the best ranking quality will be obtained for  X  values close to zero.
Given different ranking techniques for estimating page im-portance, we would like to quantify two different aspects of those techniques: page coverage and ranking quality. The former aspect refers to the ability of the ranker to compute a (non-zero) score for many pages. The second aspect refers to the ability of the technique to place important pages at higher ranks with respect to a ground-truth ranking.
Herein, we define two separate metrics to quantify the two aspects mentioned above. In both metrics, to repre-sent the actual page importance, we rely on a ground-truth data obtained from web search click logs. Since our focus in this work is on the impact of the generated page rankings on web search, the user clicks issued on web search results form a natural ground-truth. We note that, at the level of individual queries, the click likelihood of a page in search results is largely affected by the relevance of the page to the query. However, as the click information is aggregated over many different queries, the click volume of a page in search results stands as a fairly reasonable ground-truth indicating the importance of the page for users.
 We now introduce some notation and define our metrics. Let  X  be a given page ranking technique and R  X  be the set of pages that are accessible by this technique. Every page in R  X  when ranked by  X  receives a positive importance score, higher scores indicating more important pages. Let  X  an oracle ranker that can access to the ground-truth impor-tance values for a set R  X  of pages. Given these definitions, the coverage  X   X  of a ranking technique  X  is defined as the fraction of ground-truth pages for which a score can be com-puted by  X  .

Next, we devise a metric to quantify the quality of the ranking generated by a given ranker  X  with respect to the oracle ranker  X   X  . In literature, there are various metrics readily available for measuring the correlation between two given rankings (e.g., Kendall X  X  tau [18] and Spearman X  X  footrule [24]). Unfortunately, those metrics operate on fully ranked lists. In our case, we have partially ranked lists (i.e., some pages in the ground-truth set are not available to ranker  X  and vice versa). In literature, there are variants that can handle partial rankings [7]. Nevertheless, we prefer not to use those metrics here because neither of them take into account the popularity of the ranked items (i.e., in our setting, the click volumes of pages would be omitted). In our setting, the commonly used IR metrics such as DCG or NDCG [15] are not very useful either because, in such metrics, the top ranked items are heavily weighted. Our rankings are very long and having such a strong bias only at the top ranks is not very meaningful.

Due to the above-mentioned reasons, we devise a quality metric that can capture items X  estimated rank and the im-portance in the ground-truth data at the same time while being able to yield meaningful results for rankings with a large number of items. We define the relative quality  X   X  of a given ranking R  X  at rank k with respect to a ground-truth ranking R  X  as Here,  X  R  X  ( k ) is a normalization factor representing the qual-ity of the best possible ranking that can be achieved by the oracle ranker  X   X  . Note that the best possible ranking is achieved when pages in R  X  are ranked in decreasing order of their importance. We define C R ( k ) (the sum of importances of top k pages in R ) and  X   X  through recursive functions as
C R ( k ) =  X  R ( k ) = where R k denotes the k -th ranked page in a given ranking R of pages and I ( p ) denotes the importance of page p , in-ferred from the ground-truth data. In Section 6, we will consider two different alternatives to compute I ( p ), one as-suming unit page importance values and another where the page importance is represented by the click volume of the page in search results. In either case, we will assume that I ( p ) = 0 if p 6 X  X   X  . We note that PageRank, BrowseRank, and PBRank yield only positive scores when ranking pages.
The devised  X  metric emphasizes accumulation of impor-tance at early ranks, as the total page importance attained at rank k continues to contribute to the value of the metric at all ranks following k . In this sense, the functioning of this metric resembles the ROC analysis and the area under the curve metric [8].
Web page collection. We use a snapshot of the Web (crawled in late 2011), which contains around 6.5 billion web pages. Due to the difficulties involved in parsing web pages written in the CJK languages, we exclude such pages from further consideration. Moreover, self-links are removed and identical out-links in a page are contracted into a single out-link. We convert the remaining pages and links into a web graph and further compress this graph to obtain a host-level graph of the Web. In the rest of the paper, we use this host-level graph, which includes about 230 million unique web hosts with 1.5 billion inter-host links.

Browsing data. We obtain the web browsing data through a commercial toolbar application deployed at the web scale on a large number of web browsers. Our experi-ments use only the browsing data acquired from users who explicitly gave permission for their page views to be logged. For each visited page, the toolbar log contains information about the time at which the page is visited and how the page is reached. In particular, a page has a referrer URL if the user reached the page by clicking a link in another page, otherwise, i.e., if the user manually typed the URL or clicked a bookmark link, the referrer URL is not available. In total, our data contains around two billion page visits, performed by users all around the world. The browsing data is obtained in a period right after the web collection is obtained.
Click data. To serve as a ground-truth in evaluation of the generated page rankings, we use a large (random) sample of clicks obtained from the query logs of a commer-cial web search engine. The query log contains information about the query string, and the URLs clicked by the user who submitted the query. The data includes around 170,000 unique URLs and over 700,000 clicks. The click sample is obtained in a time period that follows the acquisition of the browsing data. The scatter plot in Fig. 1 shows the correla-tion between the visit counts of URLs in the browsing data and their click counts. According to this figure, there is a large number of URLs that are highly visited by web surfers, but not received many clicks from search engine users when displayed in web search results. However, the reverse state-ment is not true, i.e., highly clicked web pages tend to be visited by many web surfers. This observation provides us enough motivation to use the click data as an alternative ground-truth for representing page importance.
Optimizing  X  . An important parameter that needs to be tuned before PBRank computations is the  X  constant. Tuning this constant requires measuring the ratio between the number of visits initiated by following an out-link in a page and the total number of visits in the browsing data (see Eq. 7). In our data, we found this value to be  X  = 0 . 62, i.e., URLs are slightly more likely to be visited by following links. The obtained number is somewhat consistent with the earlier observation in [9]. Fig. 2 displays the distribution of URL visit counts in the browsing data. We observe a power-law distribution with slight distortion at the head.
Overlap among data sources and Coverage. Fig. 3 displays the overlap between the three different data sources. As expected, the web data is considerably larger than the browsing and click data. Based on these numbers, we can compute the coverage metric (  X  ), defined in Section 4. While using only the browsing data provides a coverage of 80.1%, using only the web data gives a coverage of 95.8%. On the other hand, using both of the feedbacks results in a coverage of 97.2% and provides a coverage increase of 17.1% over using only the browsing data. This result indicates that PBRank can produce non-zero importance scores for a larger number of URLs than both BrowseRank and PageRank.

Optimizing  X  . We next aim to find the  X  value that optimizes the ranking quality metric defined in Eq. 12. To this end, we compute the value of this metric for different PBRank rankings that are obtained by varying  X  through parameter sweeping. Fig. 4 shows the values of the metric with  X  increased between zero and one at increments of 0 . 1. Figure 1: Visit count of a URL in the browsing data versus its click count in search results. Figure 4: The variation in ranking quality (  X  ) for different values of  X  . According to the figure, any  X  value between zero and one yields a superior ranking performance than either baseline. We observe better performance as  X  is closer to zero. Hence, we perform another parameter sweep for  X  values near zero. The results of this experiment are displayed in Table 1. We observe that the optimum  X  value is somewhere between 0.005 and 0.015. In the rest of the experiments, we set  X  to 0.01, where we observe the best ranking quality (when the page importance is weighted by the click count). Accord-ing to the ratio 0 . 99 / 0 . 01, the feedback obtained from the browsing data has 99 times more influence on the ranking quality than the feedback coming from the web data.
Comparison of rankings. Fig. 5 shows the distribution of URL importance scores generated by PBRank for three different values of  X  . All distributions are heavily skewed. As expected, the score distributions for  X  = 0 and  X  = 0 . 01 are very similar to each other and somewhat different than the score distribution in case of  X  = 1. The curve representing  X  = 0 is shorter than the other two because fewer URLs (only those in the web browsing data) are ranked.

Contribution of data sources to top k ranks. As illustrated in Fig. 6, the main contribution to the top k = 100 URLs in the ranking generated by PBRank (  X  = 0 . 01) comes from the URLs that are present in both web and browsing data. As k increases to 10K, we observe some URLs that are available only in the browsing data to enter the rankings. The URLs that are available only in the web data become visible after the first 10K ranks. This result indicates that the URLs in the very top ranks are mainly determined by the feedback obtained from the browsing data.
Overview. PageRank is originally proposed in [22]. The technique finds application in a variety of problems from dif-ferent domains including bibliometrics [6], web crawling [5], and spam detection [11]. HITS [19] is a technique closely related to PageRank. Interested reader may refer to [2] and [20] for a survey of further issues.

Customizing PageRank. A large effort is spent to cus-tomize PageRank computations depending on the interests of users. This is mainly achieved by either adjusting the  X  constant, which shows the probability of following a link in the current page, or by customizing the teleportation vector v (see Eq. 3). Regarding the first possibility (customizing the random jump probability), several works investigated the effect of  X  on the quality of the final rankings [1, 4, Table 1: The ranking quality metric (  X  ) for varying values of  X  9, 23]. The order of pages in the final PageRank vector is found to be heavily affected by the  X  constant used [23]. The results reported in [4] show that  X  values close to 1 do not yield accurate rankings. Two latter works suggest using  X  values around 0.5 [1] or in the 0.6 X 0.725 range [9]. The approach proposed in [9] is relevant to ours in that it relies on the web browsing data to set the  X  constant. Regarding the second possibility (customizing the teleportation vector), several attempts were made [13, 16, 17]. In topic-sensitive PageRank [13], several topic-specific PageRank vectors are computed for a fixed number of topics. The PageRank com-putation is biased to yield higher scores for pages belonging to a certain topic by simply adjusting the jump probabil-ities in the teleportation vector. In [17], a similar idea is described, restricting personalization preferences to blocks of web domains instead of topics. In [16], an approximate personalized PageRank vector is computed based on precom-puted basis vectors. The BrowseRank approach [21] relies on web browsing data to customize the teleportation vector. Our work goes beyond these works in different aspects. First, we use web browsing data of users to customize the probabilities in the transition matrix, instead of adapting only the  X  constant as in [9] or adjusting the probabilities in the teleportation vector as in [21]. In this respect, our model can accurately capture the variation in the quality of the links within web pages, unlike the above-mentioned two works, which assume a uniform probability for following a link in a page. Moreover, we conduct our experiments in a very large setting, orders of magnitude larger than the settings in most previous work.
We proposed a novel model for computing web page im-portance scores by using a mixture of the feedback extracted from the hyperlink structure of the Web and the feedback obtained from the web browsing patterns of users. Accord-ing to a quality metric using user clicks on web search results mined from a query log, the proposed hybrid model exploit-ing both the web structure and the navigation patterns of users lead to a better performance than using only a single type of feedback. We found that the optimum mixture is achieved when 99% of the score comes from the browsing feedback, and only 1% from the web feedback.
