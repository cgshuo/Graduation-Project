 One issue related to some data mining techniques is the lack of comprehensibility. Although several learning techniques have been tested as useful in the way that they offer good predictions, they do not give a description, pattern or generalisation which that a given molecule belongs to a cluster according to a certain distance measure, but it is even more interesting to know what the chemical properties shared by all the molecules in that cluster are. techniques based on distances. The source of this problem is the dichotomy between distances and generalisations. It is well known that distances and generalisations give rise to two different approaches in data mining and machine learning. On the one hand we have distance-based techniques, where we only need to count on a distance function for the data we are working with. However, distance-based techniques (such as [11, 12, 13]) do not provide patterns or explanations justifying the decisions made. On the other hand we have symbolic techniques [7, 8, 9, 10] that, unlike distance-based methods, are founded on the idea that a generalisation or pattern discovered from old data can be used to describe new data covered by this pattern. patterns discovered for each cluster by a distance-based technique are consistent with the underlying distance used to construct the clusters. Inconsistencies can arise when the notion of distance and generalisation are considered independently. That is, given a set of examples and a generalisation of them, it is expected that those examples that are close in a metric space ac cording to its distance are covered by the generalisation, coverage. This problem has been extensively treated in [6]. In the present work we focus on the relationship between distances and generalisations in the context of HDCC [1], a general approach for agglomerative hierarchical clustering [2, 3]. HDCC, that stands for Hierarchical Distance-based Conceptual Clustering, constructs a cluster hierarchy by using a distance at the same time that in produces a hierarchy of patterns resulting in an extended dendrogram referred as conceptual dendrogram. The main aspect considered in [1] and that has been ignored by other conceptual clustering methods that use distances is knowing a priori whether the hierarchy of clusters induced by the underlying distance is consistent with the discovered patterns, i.e. how much the cluster elements covered by a given pattern reproduce the distribution of the elements in the metric space. Accordingly, in [1] three different levels of consistency between a distance and a generalisation operator have been defined. general framework presented in [1]. Here, we give the results of a formal analysis carried out for a set of distances and generalisation operators useful for propositional clustering, where we prove that intervals and absolute difference distance for real numbers, and the union set and discrete distance for nominal data work well together in HDCC. More importantly, we have also shown that it is also the case when using them as generalisation operators and distances for tuples of real numbers and nominal data. This rounds up the approach for propositional learning. But, additionally, this composability result for tuples is obtained independently from the base data types. The property of composability allows our framework to be directly extended to tuples of any complex data type provided that the generalisation operators associated to the component data types satisfy the property wanted for tuples. For instance, we can assert properties of tuples of graphs, strings and numbers provided we know the properties for the underlying data types. Besides these theoretical results we also present some experiments. 
The paper is organised as follows. Due to space limitations, all necessary preliminary concepts about the HDCC approach can be found in [1] and the generalisation operators and distances for numerical and nominal data, which are used in turn to define generalisation operators and distances for tuples. In Section 3 we present some experiments by applying the operators and distances proposed in Section 2, and we also compare the results obtained in HDCC wrt. traditional hierarchical clustering. Finally, Section 4 closes the paper with the conclusions and future work. In this section, we present an instantiation of HDCC for propositional clustering where flat data are expressed in terms of attributes and instances. We propose generalisation operators for numerical and categorical data and also for tuples, which levels of consistency defined in [1] between the proposed operators and distances have been verified through a satisfability analysis of the strong and weak boundedness and acceptability properties given in [1] . 2.1 Nominal Data A nominal data type, also referred as enumeration or categorical data type denotes a colours, etc. A Boolean data type is a special case where there are only two possibilities. The metric space for nominal data type is composed of a set X , which is just a finite set of symbolic values, and a distance d . used distances are the discrete distance  X  X hat returns 0 when both values match and 1 otherwise X  and the VDM (Value Difference Metric) distance [4], among others. In d (XXL, XL) = 1, d (XXL, L) = 2, d (XXL, M) = 3, d (XXL, S) = 4, d (XXL, XS) = 5, d (XXL, XXS) = 6, d (XL, L) = 1, d (XL, M) = 2, d (XL, S) = 3, d (XL, XS) = 4, d (XL, XXS) = 5, d (L, M) = 1, d (L, S) = 2, d (L, XS) = 3, d (L, XXS) = 4, d (M, S) = 1, d (M, XS) = 2, d (M, XXS) = 3, d (S, XS) = 1, d (S, XXS) = 2, d (XS, XXS) = 1 organizes the points into a line where XXL and XXS are the extreme points. attributes, e.g. attributeName = XL or attributeName  X  XL. However, since X is finite, the coverages 1 of the possible patterns are also finite and they can be expressed extensionally as subsets of X . Thus, the pattern language L for nominal data can reduce to 2 X . both values. binary generalisation operator 2 for nominal data.
 generalisation of two patterns. pattern binary generalisation operator 3 wrt. 2 X .
 Proposition 3 gives the properties satisfied by the proposed operators  X  and  X  * . Proposition 3. Let  X  and  X  * be the generalisation operators given in and Proposition strongly bounded 4 by d and d L , respectively; (ii) weakly bounded 5 by d and d L , respectively; (iii) acceptable 6 .
 The example in Fig (left) shows the use of HDCC for the evidence E = {XXS, S, M, XXL}. We have used the discrete distance, and the generalisation operators given in and Proposition 2 to compute the patterns. Note that applying the user-defined distance given above, the dendrogram changes to that shown in Fig (right).We can dendrograms are equivalent to the corresponding traditional dendrograms. 2.2 Numerical Data Numerical data are widely used to express amounts and measures and many attributes of usual generalisation for a set of numbers is the minimal interval whose extreme values are the least and the greatest values in the set. Thus the pattern language L we consider here is the set of all the finite closed intervals in  X  . We propose for the generalisation in L of two elements in  X  the minimal interval that includes both elements.  X  binary generalisation operator for real numbers. Next we propose for the generalisation of two intervals the minimal interval that covers both. Proposition 5 . Let L be the set of all the finite closed intervals in  X  . The function  X  * : L  X  L  X  L defined by  X  * ([e {e operator wrt. L . Proposition 5, d the absolute d ifference between numbers and d L a linkage distance.  X  and d L , respectively; (iii) acceptable.
 Fig 2 shows a simple application of HDCC under single linkage using the proposed operators and distance for real numbers. By Proposition 1 in [1] and by Proposition 6 the conceptual dendrogram is equivalent to the traditional one.
 2.3 Tuples A tuple is a widely-used structure for knowledge representation in propositional learning since examples are represented as tuples of nominal and numerical data.To define a generalisation operator for tuples, unlike the previous data types, we base it on they are embedded in metric spaces, therefore we can use the distances defined over each space to define distances between tuples. Analogously, to define the pattern language for tuples, we also use the pattern languages defined for each space. in Table 1 are distance functions in X . In that follows, we denote as d T any of them. (formalized by Proposition 7 below) can be defined as the tuple whose components are the generalisations of the respective components in x and y, while the coverage of a pattern in L is given by Definition 1. Definition 1. Given p = (p 1 ,..., p n )  X  L , the coverage Set(p) of the pattern p over L is defined as {(x 1 ,..., x n )  X  X | x i  X  Set(p i ), i = 1,..., n}. For example, given the pattern p = ([34, 54], {XXL, XL, XS, XXS}, [0, 130]), the examples e 1 = (54, XXL, 100) and e 2 = (36, XS, 60) are covered by the pattern. However, the tuple (40, M, 70) is not covered by p since M  X  Set ({XXL, XL, XS, XXS}). ( L  X  X. formalised in Proposition 8. (
 X  In HDCC, generalisations of unitary sets are computed as the generalisation of the element with itself. Therefore, the pattern associated to a cluster with only one tuple Proposition 9. (Composability of  X  ) The binary generalisation operator  X  for tuples given by Proposition 7 when applied to tuples in the space X = X 1  X  ...  X  X n , where (X i ,d i ) (i = 1,...,n) is a metric space equipped with a binary generalisation operator  X  i. is: ( i ) Strongly bounded by d T if  X  i is strongly bounded by d i ,  X  i: i = 1,...,n. ( ii ) Weakly bounded by d T if  X  i is strongly bounded by d i ,  X  i: i = 1,...,n. ( iii ) Acceptable if  X  i is acceptable,  X  i: i = 1,...,n. propositional clustering in X =  X  X  X  X  for the evidence given in Fig. (a). We have used the language of closed intervals in  X  as pattern language for each dimension in X , and the absolute difference as the distance between real numbers. Note that a tuple pattern, in this case, describes an axis-parallel rectangle. Fig. (d) shows the conceptual dendrogram resulting from th e application of HDCC using the single see that the conceptual dendrogram is not equivalent to the traditional one under single linkage given that although the binary generalisation operator  X  for tuples clusters C 1 and C 2 is a rectangle p that cover points that can fall outside the balls with instance with { i } which is covered by p4 (see Fig. 3 (b)). X are instantiated to  X  . Let us consider the following example. C 1 ={(0, 0, x 3 ,..., x n ), C , d T ) = 1.55 where d T is the Euclidean distance. However, there exists x = (4.5, 0.5, x 4.63 &gt; 1.55. linkage distance d c L , as the next proposition establishes. Proposition 10. (Composability of  X  *) The pattern binary generalisation operator  X  * for tuples in the space X = X 1  X  ...  X  X n given by Proposition 8 when applied to patterns operator  X  * i , is: ( i ) Strongly bounded by d c L if  X  * i is strongly bounded by d c L ,  X  i: i = 1,...,n. ( ii ) Weakly bounded by d c L if  X  * i is strongly bounded by d c L ,  X  i: i = 1,...,n. ( iii ) Acceptable if  X  * i is acceptable,  X  i: i = 1,...,n. linkage produces a conceptual dendrogram that is equivalent to the traditional dendrogram as Proposition 1 in [1] establishes given that Proposition 10 (i) and Proposition 9 (i) hold. In the previous section we proposed a set of generalisation operators and distances for tuples that applied to HDCC under complete linkage distance produces equivalent conceptual dendrograms with the additional advantage of providing a description of each cluster in the hierarchy. We have also seen through an example that the same operators and distances when used under single linkage distance can produce dendrograms that are not equivalent. The experiments described in this section are aimed to (i) empirically illustrate the first result with a real dataset and (ii) show that the new conceptual clustering, coming from the on-line re-arrangement of the dendrogram, although not equivalent to the traditional dendrogram does not undermine cluster quality when applied under single linkage. three classes, 50 instances each and four numeric attributes. Each class refers to a type of iris plant namely Iris Setosa, Iris Versicolor and Iris Virginica. The numeric attributes refers to the sepal and petal lengths and widths in cms. One internal measure, called S, which reflects the mean scattering over k clusters with n ( i = 1, ..., k ) instances each. This measure is given by eq. (1) where d denotes the Euclidean distance. The lower S is the better the clustering is. (ii) One external measure, the purity P given by eq. (2), where k is the number of clusters, n is the total number of instances and n i j the number of instances in cluster i of class j . Purity can be interpreted as classification accuracy un der the assumption that all the objects of a the class was considered for obtaining purities, it was removed from the dataset to build the clusters. linkage. Each pattern is a 4-tuple where the component i is also a pattern that provides a description of attribute i . C3 was Iris Virginica. 
In fact, each of these patterns can be seen as a rule. For instance the discovered pattern for C1 under complete linkage and single linkage is ([4.3, 5.8], [2.3, 4.4], [1.0, 1.9], [0.1, 0.6]) that can be interpreted as the rule where sepallength , sepalwidth , petallength and petalwidth are the 1 to 4 th attributes in the dataset, respectively. 
Table 3 shows the values of S and P for HDCC and the traditional hierarchical clustering algorithm under complete distance d c L and single linkage distance d s L for k quality of the conceptual clustering does not differ from that of traditional hierarchical clustering even under single linkage and it provides useful descriptions that allow preserved by HDCC, was confirmed by four experiments carried out on 100 artificial datasets each. Datasets were formed by 600 points drawn from 3 Gaussian were set to the values reported in Table 4. In these experiments the average values of S over the 100 experiments were obtained for HDCC and the traditional algorithm under single and complete linkage. These values are also reported in Table 4. Hierarchical distance-based conceptual clusteri ng provides an integration of hierarchical distance-based clustering and conceptual clustering. It can be easily seen that for complex datatypes (sequences, graphs, etc.) the original dendrograms are usually different to the dendrograms obtained by applying the generalisation operators. In order to cope with these (negative) results, the notion of conceptual dendrogram and three generalisation operator have been proposed. Some pairs of distances and generalisation operators are compatible at some degree resulting in equivalent, order-preserving or acceptable conceptual dendrograms while some other pairs are not, so showing that some distances and generalisation operators should not be used together. propositional world, and using the most common distances and generalisation operators for nominal data, numerical data and tuples, we have found out that the strongest properties (in fact all of them) hold. From these results, we can affirm that the integration of hierarchical distance-based clustering and conceptual clustering for applications) is feasible, congruent and relatively straightforward. 
Additionally, the composability result obtained with the tuple datatype and several distances, allow the handling of more elaborate information in the form of tables, where some attributes can have structure, provided that the distance and generalisation operators used for every attribute have some degree of consistency. 
In this regard, our immediate future work is focussed on finding operative pairs of distances and generalisation operators for common datatypes in data mining applications, such as sequences, graphs and multimedia objects.

