 As a popular heuristic to the matrix rank minimization prob-lem, nuclear norm minimization attracts intensive research attentions. Matrix factorization based algorithms can re-duce the expensive computation cost of SVD for nuclear norm minimization . However, most matrix factorization based algorithms fail to provide the theoretical guarantee for convergence caused by their non-unique factorizations. This paper proposes an efficient and accurate Linearized Grass-mannian Optimization (Lingo) algorithm, which adopts ma-trix factorization and Grassmann manifold structure to al-ternatively minimize the subproblems. More specially, lin-earization strategy makes the auxiliary variables unneces-sary and guarantees the close-form solution for low per-iteration complexity. Lingo then converts linearized objec-tive function into a nuclear norm minimization over Grass-mannian manifold , which could remedy the non-unique of solution for the low-rank matrix factorization. Extensive comparison experiments demonstrate the accuracy and effi-ciency of Lingo algorithm. The global convergence of Lingo is guaranteed with theoretical proof, which also verifies the effectiveness of Lingo.
 G.1.6 [ Mathematics of Computing ]: Optimization; H.2.8 [ Database Management ]: Database Applications X  Data Mining  X 
Corresponding author.  X  Corresponding author.
 c  X  Low-rank; nuclear norm; matrix completion; Grassmannian
In the past few years, matrix rank minimization has re-ceived considerable attention from machine learning, com-puter vision, recommender systems, and a wide range of problems can be represented under this framework [7,11,14]. However, minimizing the rank of a matrix subject to a con-vex constraint is NP-hard, and no polynomial time algo-rithm yields exact solution for most practical problems. A popular heuristic that is widely used to solve the matrix rank minimization problem, is to relax the objective function by minimize the nuclear norm where f ( X ) is any smooth convex function, the nuclear norm k X k  X  can be also represented by the sum the singular values of X , and  X  &gt; 0 is a given regularization parameter. Theo-retic research also confirms that the nuclear norm minimiza-tion leads to the exact solution to matrix rank minimization when certain conditions are satisfied [8, 9].

Despite its strong theoretical guarantee, the nuclear norm minimization has been found incapable to handle large-scale problems in real world applications. For example, the rating matrix in Netflix usually has more than 10 8 entries [5], and existing nuclear norm minimization solvers are usually not efficient enough.

Existing attempts in scalable and practical nuclear norm minimization solutions can be categorized as rank minimiza-tion and matrix factorization. Semidefinite Program (SDP) solvers [14] and First-order methods (FOMs) [4, 15, 19] are two representatives in rank minimization. Among them, the SDP solvers such as Interior-point method (IPM) require the second order information that is usually unavailable in large-scale problems [14], while FOMs require only the first-order information, and hence are usually preferable for large-scale problems [15, 17]. Nevertheless, FOMs X  X  soft-thresholding operator still requires computing the SVD, for which the speed is still an issue in many real applications. Instead of directly minimizing the rank, Riemannian Optimization , such as OptSpace [18], GROUSE [3] and SET [10], factor-izes the matrix and explores their geometric structures, so that the searching space is limited to manifold. However, although SET reduces the higher-dimensional search space caused by jointly minimization in OptSpace, it has been reported as unstable as more than one solution can be gen-erated [6]. GROUSE [3] empirically outperforms OptSpace and SET, which however is not guaranteed with theoreti-cal convergence. Although most existing Riemannian Opti-mization based algorithms have superior performance than classical heuristics (e.g.,SDP and FOMs) with numerical ex-periments, they lack the theoretical analysis to prove the stability of their superior performance. Consequently, how to develop an efficient and accurate Riemannian Optimiza-tion method with theoretical convergence guarantee remains an open issue.

In this paper, we attempt to develop an efficient and ac-curate Linearized Grassmannian Optimization (Lingo) algo-rithm for a general nuclear norm minimization (1.1). The main contributions of this paper are in the following three aspects:
Nuclear norm minimization aims to find the low rank ma-trix solution via replacing the rank with the nuclear norm, and then to construct a tractable convex problem in (1.1) over the affine subset. The smooth convex functions f ( X ) is usually referred to as the penalty term to characterize specific applications, such as Multivariate Regression , Multi-task Learning and Matrix Completion .
 Multivariate Regression Given an observation data ma-Multi-task Learning Finds the low-dimensional represen-Matrix Completion Given a partially low rank matrix A
According to the strategies in modeling the low-rank prior, existing approaches to nuclear norm minimization can be categories into rank minimization and matrix factorization .
A straightforward approach to the nuclear norm mini-mization is to directly minimize the nuclear norm in (1.1). Representative methods include interior-point method (IPM) and First-order method (FOMs). Among them, IPM can achieve exact solution but inefficient for large-scale prob-lems, because of its dependency on the second-order infor-mation [14]. IPM X  X  high computation cost has inspired an ef-ficient method FOMs that requires only the first-order infor-mation of the matrix. One FOMs algorithm is the Iterative soft-thresholding algorithm (IST), which has been shown to be a slow but with asymptotic convergence rate [4]. Acceler-ated Proximal Gradient (APG) algorithm is then proposed to reduce the number of iterations in IST [17]. Although the convergence rate has been improved in APG compared with IPM and IST, APG produces an approximate solu-tion [19]. To achieve exact optimal solution while main-taining a satisfying convergence rate, Exact Augmented La-grangian method (EALM) is proposed, which improves the convergence rate from the sub-linear in APG to Q-linear [19]. It is worthy to mention that both APG and EALM need to use the full singular value decomposition . Inexact Aug-mented Lagrangian method (IALM) adopts partial SVD by calculating only the dominated singular values [19]. No mat-ter for full or partial SVD, singular value decomposition at each iteration leads to extensive computation cost especially for the large-scale problems.

Matrix factorization is another category of approach to the nuclear norm minimization , which factorizes matrix X in problem (1.1) into smaller factor matrices to reduce the computation burden. As one representative method in this category, LMaFit [24] adopts the alternating strategy for each factor matrix and integrates a nonlinear successive over-relaxation scheme to accelerate the convergence. Rieman-nian Optimization is also based on low-rank matrix factor-ization approach by constraining the searching space over manifolds, such as OptSpace [18], GROUSE [3] and SET [10]. Both OptSpace and SET utilize some special structural prop-erties of the low-rank matrices. OptSpace iteratively esti-mates the factor matrices and updates them simultaneously by gradient descent over the Grassmannian, so this further introduces numerical and analytical difficulties for theoreti-cal guarantees. Substantially differs from simultaneous up-dating in OptSpace, SET minimizes over only one space (the column or the row space) but requires a priori on the rank of the matrix k X k  X  , which is sometimes unreliable via rank estimation thus affects the performance of SET. GROUSE is an on-line incremental gradient descent algorithm on the Grassmannian, which accelerates OptSpace and SET. Yet, the convergence of GROUSE is not theoretical guaranteed.
To alleviate those issues mentioned above, this paper pro-poses an efficient and robust method for the nuclear norm minimization in (1.1) with less computational cost and sta-ble accuracy. In this section, we propose the Linearized Grassmannian Optimization (Lingo) algorithm to solve the nuclear norm minimization via Grassmann optimization, and will also prove that Lingo algorithm can converge with low complexity.
Manifolds can be considered as low dimensional smooth  X  X urfaces X  embedded in a higher dimensional Euclidean space. Stiefel manifold and Grassmann manifold are two differen-tiable manifolds that have been widely used in computer vi-sion [12, 23] and signal processing [21]. Among them, Stiefel manifold refers to an embedded sub-manifold in R n  X  r , which defines the set of n  X  r semi-orthogonal matrices, S n,r
U  X  R n  X  r : U T U = I r ; while Grassmann manifold refers to a matrix quotient space of Stiefel manifold with respect to U  X  S n,r } , where O ( r ) defines a set of the orthogonal ma-trices.

In order to obtain Grassmann geometries of the result-ing search space, SVD decomposes matrix X in (1.1) and get: X = USV T . However, such SVD decomposition is not unique, since the X remains unchanged under the following transformation.
 where U  X  S m,r and V  X  S n,r . Consequently, the optimal matrix X , when rotated by matrix Q  X  X  ( r ), can form a set of equivalent class: [ X ] = [( U,S,V )] = n ( UQ,Q T SQ,V Q ) : Q  X  X  ( r ) o (3.1) Let [ U ] = { UQ : UQ  X  X  m,r } and [ V ] = { V Q : V Q  X  X  construct two equivalent classes over Stiefel manifold . As G m,r ' S m,r / O ( r ), these equivalent classes satisfy [ U ]  X  G m,r and [ V ]  X  G n,r . The quotient geometry of Grassmann manifold in Lingo algorithm guarantees the uniqueness of matrix factorization. Since X = USV T , from the definition of nuclear norm , we have k X k  X  = k S k  X  .

Based on the discussion above, we can reformulate the general nuclear norm minimization (1.1), into a non-convex problem as follows, by applying SVD to solve U and V over the Grassmann manifold searching space. In general, solving U , S and V simultaneously is very time consuming, so alternating direction method (ADM) can be adopted to split the optimization objective function (3.2) into several subproblems with respect to each variable. How-ever, some subproblems may have no closed-form solution in ADM [20]. Moreover, the ADM constraints related to quadratic penalty involve unnecessary auxiliary variables, and this usually results in inefficiency for ADM.

Through linearizing the optimization objective f ( USV T ) at iterate X j = U j S j V T j by adding a proximal term, the optimization objective can be approximated as: min where  X  is a proximal parameter,  X  ,  X  is the inner product.
After expanding k USV T  X  U j S j V T j k 2 F , the optimization objective can be further simplified as the quadratic form: where P j = USV T  X  1  X   X  f ( U j S j V T j ).

This minimization problem can then be decomposed to three subproblems about U , V and S , respectively:  X   X   X   X   X   X   X   X   X   X   X 
In conclusion, the nuclear norm minimization can be ad-dressed by solving three subproblems in (3.2), where U and V is iteratively updated over the Grassmann mainifold by gradient decent algorithm, S can be obtained with close-form solution after linearization strategy. In the following, we discuss how to solve (3.5) in detail.
We adopt ADM to alternatively solve each subproblem while fixing the other two variables. Specifically, U and V can be obtained by iteratively minimizing the subproblem on Grassmann manifold with gradient descent and S can achieve close-form solution with the guarantees from lin-earized (3.4). The following part describes the updating processes for U , V and S respectively.
The updating processes for U and V are similar, since they have similar corresponding subproblem, which can be solved by optimizing on Grassmann manifold . In the following, we will explain the updating on U . Based on the analysis above, the solution for U j +1 is Similarly, the solution for V j +1 is
Before discussing how to update S , we first introduce a useful theorem mentioned in [8].

Theorem 1. Consider the SVD of a matrix X = U  X  V T , for each  X   X  0 and Y  X  R m  X  n , the singular value shrinkage operator of Y satisfies that
When U and V are fixed, the optimal S should be updated by minimizing the subproblems: where P j = U j S j V T j  X  1  X   X  f ( U j S j V T j ).
According to Theorem 1, the solution of S at j + 1 iterate for (3.12) can be yielded by the singular value shrinkage operator: where G  X  H T is the SVD of U T P j V , D  X  singular values  X  below the threshold  X   X  .

Since S is small-scale r  X  r matrix, the SVD in above shrinkage operator has significantly low complexity. Con-sequently, Lingo can update S efficiently and obtains the close-form solution.
Following the above analysis, the pseudo code of Lingo al-gorithm can be summarized in Alg. 1. The numerical com-plexity of Lingo algorithm is mainly determined by the com-putational costs of QR decomposition for solving U and V, the SVD decomposition on small scale matrices r  X  r for solving R. The total complexity is O (( n + m ) r 2 + r 3 O ( n + m ) r 2 is the time complexity of QR decomposition, O ( r 3 ) is the time complexity by SVD.
Although most existing Riemannian Optimization algo-rithms can achieve satisfactory accuracy in numerical ex-periments, many have no theoretical guarantee in conver-gence. In this section, a complete convergence proof is given to provide theoretical guarantee for Lingo to achieve high accuracy.

Theorem 2. The accumulation point of { U j ,S j ,V j } pro-stationary point for F in (3.4).
 Algorithm 1 Lingo:Linearized Grassmannian Optimization for Nuclear Norm Minimization
Require: The tolerance parameter err , the rank r , the total number of iterations n and the parameters  X  and  X  .
Initialize: ( U 0 ,S 0 ,V 0 ) = SV D ( X 0 ) while k X j +1  X  X j k F &lt; err and j &lt; = n do end while Output: X = USV T
Proof . Let us assume the contrary statement: there is a subsequence { U j 0 } j 0  X  X  converging to the stationary point U j +1 with grad F ( U j +1 ) 6 = 0.

According to U j +1 calculated by (3.9), the step size starts at  X   X  and decreases by Armijo Rule. There exists a update  X  satisfying
F ( U j )  X  F (qf( U j +  X  j lated as where  X  F U is a real-valued function on the tangent space.
Let  X   X   X  j  X  F U j be the directional derivative in the direction of  X   X  j , then
Because F in (3.4) is non-increasing, we have F ( U j F ( U j +1 )  X  0 and {  X  j }  X  0. Consequently, according to t  X  (0 ,  X   X  j ), we have t  X  0. Hence, (5.3) can be reformulated as (5.4)
Based on the local rigidity property in Retraction , (5.2) can be reformulated as
When f converges to iterate U j +1 and the compact set {  X   X  j }  X  J X  J achieves the accumulation point  X   X   X  , (5.5) can be reformulated as Considering the condition c &lt; 1, it can be concluded that  X  grad F ( U j +1 ) ,  X   X   X   X   X  0. Since  X  grad F ( U j +1 quently, the Grassmann gradient grad F ( U j +1 ) = 0 is true. Similar conclusion can also be obtained for grad F ( V j +1 0. According to (3.13), the closed-form solution S j +1 is op-timal, so we have grad F ( S j +1 ) 3 0.
Consequently, grad F ( U j +1 ) = 0, grad F ( V j +1 ) = 0 and grad F ( S j +1 ) 3 0, these make { U  X  ,S  X  ,V  X  } the stationary point for optimization problem (3.4).

Theorem 3. Let G = f ( USV T ) +  X Tr ( S ) denote the ob-jective function in (3.2). Then the stationary point for F in (3.4) is also the stationary point for G .

Proof . The first-order optimality conditions of F with
According to Theorem 3, the sequence { U j ,S j ,V j verges to ( U  X  ,S  X  ,V  X  ). Thus we have . Then (5.6), (5.6) and (5.8) can be reformulated as which are the first-order optimality condition of G with re-spect to U j +1 ,V j +1 and S j +1 respectively. When j  X   X  , then { U  X  ,S  X  ,V  X  } is the stationary point for the original problem (3.2).
Matrix completion has attracted significant recent atten-tion [10, 19, 24], which satisfies the form of the nuclear norm minimization in (1.1). Matrix completion (MC) recovers the low rank matrices with missing elements and can be ad-dressed by our Lingo algorithm. In this section, we evaluate both the effectiveness and efficiency of Lingo algorithm for solving a collection of matrix completion problems, such as image restoration, denoising and collaborative filtering. For a systematic comparison, we compare the proposed Lingo model with three state-of-the-art matrix completion algorithms, including IALM [19], OptSpace [18] and LMaFit [24]. All algorithms were implemented in Matlab and tested on a desktop computer with a 3.20 GHz CPU and 4.00 GB of memory.
 IALM Inexact Augmented Lagrange Multiplier Method [19], OptSpace [18] correctly estimates the rank of the original LMaFit [24] Through applying the matrix factorization, Lingo The algorithm proposed in this paper, linearized the
We first compare the Lingo algorithm with state-of-the-art matrix completion algorithms on synthetic data. The datasets are synthesized in the following way: We randomly sample a few entries from the synthetic matrices and com-pare the recovery results. A set of ground-truth m  X  n matri-ces X 0 = U 0 S 0 V T 0 of rank r are randomly generated, where U 0 and V 0 are two orthonormal matrices with m  X  r and n  X  r , S is a diagonal matrix with size r  X  r . m and n are both assigned with 5000 and rank r adopts two values with 5 and 10. The sampling ratios s defining percentage of uniformly observed entries are set with 1 . 5% and 5%.
We use Root Mean Squared Error (RMSE) to measure the matrix recovery accuracy: , where X 0 refers to the ground-truth matrix and X is re-constructed matrix by the matrix completion algorithms. Fig. 1 shows RMSE versus computation time for IALM, OptSpace, LMaFit and Lingo. It is observed that the recon-structed errors of all algorithms decrease as their computa-tion time increases. Noted that, our Lingo obtains faster convergence speed as well as faster decreasing of RMSE under all four scenarios in Fig. 1. OptSpace performs the worst under the same settings, since the requirement of it-eratively computing the SVD of the trimmed matrix leads to the deteriorated performance. IALM achieves a relatively similar RMSE as OptSpace but converges much faster than OptSpace. Although LMaFit obtains lower RMSE within less time, it still performs worse than Lingo.

Note that the performance of matrix algorithms often de-pends on two significant factors: the rank and the sampling ratio. We further test parameter sensitivity of Lingo when varying the rank or sampling ratio and their influences to RMSE. Fig. 2(b) plots the RMSE obtained by four meth-ods for varying rank r from 5 to 10 with fixed sampling ratio s = 1% (i.e. 99% of entries are missing). Apparently, Lingo algorithm still performs substantially better in RMSE than IALM, OptSpace, LMaFit. Moreover, when the rank is fixed as 5 in 2(a), the RMSE from Lingo algorithm is still much lower than compared algorithms. Note that here again IALM performs similar to OptSpace and is worse than LMaFit and Lingo, as shown in Fig. 2(a) and Fig. 2(b).
A popular application of matrix completion is image restora-tion. Here we conduct the task of recovering images from (b) rank r = 10, sampling ratio s = 1 . 5% Figure 2: (a) RMSE vesus rank with sampling ratio s = 1% . (b) RMSE vesus sampling ratio with rank r = 5 random incomplete observations, and investigate the recov-ery performance of IALM, OptSpace, LMaFit and Lingo. Four 512  X  512 images are corrupted by randomly remov-ing 50% of pixels. Specifically, IALM, OptSpace, LMaFit and Lingo are required to fill in the missing pixel values to reconstruct the images. We present both the visual re-constructed results and the numerical Peak Signal-to-Noise Ratio (PSNR) for all compared algorithms.
 Table 1: The PSNR values corresponding to four images in Fig 3.
 Fig. 3 shows the recovered images from the IALM, OptSpace, LMaFit and Lingo, along with the masked images. We test all possible rank values from 5 to 20 and choose the best as that our Lingo algorithm yields the most visually pleasant recovery results. LMaFit recovers most parts of images with good visual results except that the left parts of these images remain corrupted in some sense. OptSpace and IALM can merely recover the shapes of the objects in corrupted images, which both fails to reconstruct the detailed pixels.
Table 1 qualifies the visual quality using PSNR, from which we further verify the superior visual results of Lingo with the highest PSNR values among all compared algo-rithms. (b), image (c) and image (d) respectively.

When compared with IALM, LMaFit can recover the cor-rupted images with more clear visualized results, but their PNSR are close, since the left part of LMaFit degrades the overall recover quality. On the contrary, OptSpace has the lowest PSNR value, and this is consistent with the visual results in Fig. 3. Consequently, both the visual results and PSNR results verify that our Lingo can better capture the structure of low rank matrices, and is more robust to noise.
In many applications, it is desired to reconstruct the cor-rupted parts of an image caused by texts or logos. Image denoising task aims to remove the noises from an image, by assuming that the corrupted regions might be unknown. We further conduct image denoising task to test our Lingo algo-rithm. There are 4 ground-truth images with size 300  X  300, which are all masked by some text. In the following, we try all possible rank values (i.e., from 5 to 20) to choose the best as their results.

Fig. 4 shows the recovered results of four images indexed as image (e) to image (h), which also demonstrates the sat-isfying visualized results of Lingo algorithm. OptSpace pro-duces some black blocks at the position of the text, as evi-dent in the recovered image (e) and image (f). IALM some-how improves the OptSpace, but it still produce some black blosks especially in recovered image (f). Moreover, the vi-sual quality of recovery by LMaFit is comparable to Lingo algorithm. To verify the performance, we further quantize these visual quality and takes the computation time into consideration.
 Table 6.4 shows the PSNR and the computation time of IALM, OptSpace, LMaFit and Lingo to recover the four im-ages shown in Fig. 4. Lingo can achieve comparable PSNR values with other algorithms, and this is consistent with the visualized results in Fig. 4. Noted that, Lingo con-sumes much less computation time to achieve the compa-rable PSNR as LMaFit. Based analysis above, Lingo algo-rithm is more preferable than other three methods because of the satisfying recovery visual quality as well as the con-vergence speed.
Collaborative filtering aims to predict users X  preferences on various items based on existing ratings, and it can be considered as a matrix completion application. The datasets applied in this experiment are two common benchmarks for collaborative filtering: the Jester joke dataset [1] and the MovieLens dataset [2]. Among them, the Jester-all data set contains all 4 . 1  X  10 6 rating matrices for 100 jokes from 73421 users; and the MovieLens dataset contains three sub-sets, from which we select movie-1M characterized by 1 mil-lion rating matrices for 3900 movies by 6040 users. In this experiment, we randomly select about 50% of known entries in the corresponding M ij of Jester-all or movie-1M dataset as the test set  X .
 The experiment results are evaluated by the Normalized Mean Absolute Error (NMAE) [22, 24]: (f ), image (g) and image (h) respectively.
 where X 0 refers to the rating matrix,  X  X is the reconstructed matrix, r max and r min are the upper and the lower bounds for the ratings. For the jester -all dataset, we have r max and r min =  X  10; while for the movie -1M dataset, we have r
We run the compared algorithms with rank =5 and rank=10 on both data sets. The experiments repeated 20 times over the test data  X  for each algorithm, the parameters of the baselines are tuned for the best averaged performance shown in Table 6.5. It is evident that Lingo algorithm generates most often the better or comparable NMAE values, when compared with IALM, OptSpace and LMaFit. More specif-ically, when we take the computation time into considera-tion, Lingo algorithm is much more superior to other three methods with less computation time. In contrast, it takes the most computation time for OptSpace algorithm to gen-erate the comparable NMAE values with Lingo. The slow convergence of OptSpace might be caused by the repeated larg-scale SVD. These results indicate that Lingo algorithm provides a computational efficient solution to collaborative filtering with high accuracy.
Various algorithms have been proposed for nuclear norm minimization , but most rank minimization based algorithms depend on large-scale SVDs which results in extra compu-tational burden. To alleviate this issue, matrix factoriza-tion factors the larger matrix into small factor matrices and scales the searching space to reduce the computational bur-den. Through this measure, most matrix factorization algo-rithms achieve satisfying accuracy, but without theoretical convergence guarantee, and it might be very slow to achieve satisfactory accuracy.

In this work, we propose an efficient and accurate Lin-earized Grassmannian Optimization (Lingo) algorithm with theoretical convergence guarantee. Lingo adopts matrix fac-torization and fully explores the underlying quotient Grass-mann nature of the factor matrices. By constraining the search space over manifolds, Lingo designs the gradient-based optimization to alternatively minimize the subprob-lems with Grassmann manifold structure. Moreover, to guarantee the close-form solution with low complexity in each iteration, Lingo adopts linearization to the objective function. The convergence of Lingo is guaranteed with the-oretical proof, and the effectiveness is validated in extensive comparison experiments with state-of-art algorithms.
This research is partially supported by the Strategic Pri-ority Research Program of the Chinese Academy of Sci-ences Grant (XDA06030200), Beijing Key Lab of Intelligent Telecommunication Software and Multimedia (ITSM201502), Guangxi Key Laboratory of Trusted Software (KX201418), the National Natural Science Foundation of China (No.61403369). [1] Jester jokes. [2] Movielens. http://www.movielens.org. [3] Laura Balzano, Robert Nowak, and Benjamin Recht. [4] Amir Beck and Marc Teboulle. A fast iterative [5] James Bennett and Stan Lanning. The netflix prize. In [6] Nicolas Boumal and Pierre-antoine Absil. Rtrmc: A [7] Christos Boutsidis, Petros Drineas, and Malik [8] Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. [9] Emmanuel J Cand`es and Benjamin Recht. Exact [10] Wei Dai, Olgica Milenkovic, and Ely Kerman.
 [11] Petros Drineas, Alan M Frieze, Ravi Kannan, Santosh [12] Alan Edelman, Tom  X as A Arias, and Steven T Smith. [13] A Evgeniou and Massimiliano Pontil. Multi-task [14] Maryam Fazel. Matrix rank minimization with [15] Massimo Fornasier and Holger Rauhut. Iterative [16] AJ Izenman. Modern multivariate statistical [17] Shuiwang Ji and Jieping Ye. An accelerated gradient [18] Raghunandan H Keshavan and Sewoong Oh. A [19] Zhouchen Lin, Minming Chen, and Yi Ma. The [20] Zhouchen Lin, Risheng Liu, and Zhixun Su. Linearized [21] Jonathan H Manton. Optimization algorithms [22] Kim-Chuan Toh and Sangwoon Yun. An accelerated [23] Pavan Turaga, Ashok Veeraraghavan, and Rama [24] Zaiwen Wen, Wotao Yin, and Yin Zhang. Solving a
