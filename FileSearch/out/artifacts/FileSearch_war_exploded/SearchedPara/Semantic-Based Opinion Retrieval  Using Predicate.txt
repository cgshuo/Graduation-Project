 Opinionated documents (documents that contain opinion) in opinion-driven sources such as collection of blogs called blogosphere (e.g. Google Blogs Search 1 , Technorati used in different semantic contexts. However, current opinion retrieval techniques are only concerned with retrieving opinionated documents as long as they contain some opinion words. Usually, the opinion words may be within a proximity window [1] (a certain distance of opinion words to a query word) or randomly located within the document. For example, consider an opinion finding scenario below: User query topic : Why Apple made huge sales on iPhones? If we assume a unified opinion retrieval model that combines topical relevance and opinion relevance to retrieve the above document [1], it is quite obvious that the result iPhone, sales) used in the topical relevance and then the proximity or presence of opinion words (i.e. interesting, worst, enemy, fell) used in the opinion relevance . However, if we consider the semantic context of the query topic based on human judgment, it could be observed that the query topic and the retrieved document are not semantically relevant. Consequentially, opinions retrieved for the qualitative analysis of products, performance measurement for companies, and public reactions to political decisions are more likely to be largely biased. 
Few approaches have made attempts to perform sentence-level opinion retrieval, for example, by merging opinionated nature of individual sentences (sentiment flow) with the degree of relatedness of individual sentences to the given query (topic relevance flow). The result forms a sentiment-relevance flow (SRF) [2] that is still largely based on frequency of opinion words within sentences. 
Considering these challenges, our work introduces a semantic-based approach for identifying semantic and subjective relationships between the given query topic and the sentences in opinionated documents. The proposed technique employs the semantic similarities between predicate-ar gument structures of a given query topic and the sentences in a given opinionated document. The predicate-argument structures are derived from the output parse tree of a syntactic parser. In addition, each predicate-argument structure is checked to contain a subjective component identified by subjective adjectives. This ensures semantically relevant sentences are also subjective. To overcome the efficiency pr oblem due to syntactic parsing, predicate-argument structures are derived offline and used to index the opinionated documents. We summarize our contributions as follows:  X  Rather than proximity-based or keyword-based opinion retrieval approaches, we  X  Our technique employs the predicate-argument structures output of syntactic The rest of this paper is organized as follows. Section 2 presents related work. Section 3 presents the overview of the proposed t echnique. Section 4 explains the approaches used in our model. Section 5 explains model formations. Section 6 discusses experiments and results. Finally, section 7 presents conclusions and future work. Opinion polarity detection techniques have recorded some level of success [3]. In opinion polarity detection, specific keywords within a document are labeled with a particular polarity (e.g. positive or negative). However, determining an effective way to differentiate between what is positive and what is negative is still an unsolved problem[4][10][16]. For example in [4], the presence of ironic phrases and inverted polarity in opinionated documents led to lower precision for positive opinions with just 77% accuracy. As a result of this, the choice of individual words for polarity detection in opinionated documents is still a big challenge. 
Subjectivity detection shows if a sentence contains opinion or not [5]. Words or sentences that contain opinions are systema tically categorized according to the degree of subjectivity shown by their lexicons. However, an automatic and effective way to detect hidden and inverted polarities in phrases and sentences is still a huge research challenge [4]. According to Pang and Lee [5 ], subjectivity is a two-state task and can be interpreted differently in some cases. It is still very challenging to effectively determine appropriate subjectivity state in many documents that contain opinion. 
Lexicon-based approaches consider domain-specific ev idences to form lexicons for opinion retrieval [6]. For generating lexicons, individual opinionated keywords are selected from each sentence in a document. However, we believe opinionated words alone cannot completely and independently express the overall opinion contained in a document, without taking into considera tion the grammatical dependencies between words. We suggest opinion should be syntactically retrieved from a complete sentence rather than individual words. Individual keywords in the lexicon might have been selected from varying grammatical contexts. 
Probabilistic approaches are commonly used to evaluate and explain theoretical foundations for information retrieval models [7-8]. In many cases, estimates or assumptions made in probabilistic approaches may not be practically applicable to real scenarios. It is only effective where there are high chances of frequency of observations as applicable to BM25 [9] and Divergence from Randomness probabilistic models. Some probabilistic models use proximity density information to calculate probabilistic opinion score for individual query terms [10]. However, proximity of words to some of the query terms may not reflect the semantic context at which opinion is required. 
Language model approach combines prediction of occurrence for natural language words and then shows a probabilistic interpretation of such occurrences. A common model a document to give a higher probability of relevance to user query. For example, [11] addressed the parameter tuning or optimization problem in higher order language models by incorporating different component models as features. A common approach is to perform smoothing at varying levels to ensure high chance of document retrieval. However, this approach usually leads to having higher number of model parameters, where all of such parameters would also require optimization at different levels. As shown in Figure 1, given a document from a collection, sentences are transformed to grammatical trees. Predicate-argument structures are derived from the trees and then annotated semantically for indexing. In the online process, the given query topic is semantically annotated as done for each sentence in the offline process. The index is then searched for relevant documents based on a scoring scheme that compensates each relevant annotation containing a subjective adjective with an empirical score. 4.1 Grammatical Tree Derivations We use a context-dependent syntactic parser, specifically, Categorial Combinatory Grammar (CCG) [12]. This does one-time complete natural language processing (NLP) tasks to show the underlying meaning of each query topic or well formed sentence. CCG has a relatively straightforward way of providing a compositional semantics for the intending grammar by providing completely transparent interface between syntax and semantics [13]. We chose CCG because of its distinct ability to integrate syntactic and semantic information in its single parse process even for long-range dependencies in natural language sentences. It also has speed advantage over other parsing models such as Collins parser and Charniak parser [13]. Derivation examples are shown below: 
The focus of this paper is beyond the deep understanding of the CCG parsing processes as it has been thoroughly discoursed in existing literatures [12-13]. For the purpose of our experiment, we use a log-linear CCG syntactic parser by Clark and Curran [13]. The model is described in [14] as part of the popular C&amp;C tools 4 . 4.2 Predicate-Argument Structures Predicate-argument structures are derived from the output parse tree of a syntactic local and long-range word-word dependencies structures for a sentence. These structures can be retrieved with F-measure accuracy of about 83% labeled recovery and 91% unlabeled recovery [15]. Usually, a transitive verb category denoted as (S\NP)/NP , contains two predicate-argument relations. One is the object NP argument and the other is the subject NP argument. To differentiate between argument slots, the arguments are indexed from left to right. Indices can be used to number the argument slots of the categories. For example, the transitive verb can be represented as \ X  X   X   X  word  X   X  ,  X   X  X  X  X  X  is the index of the argument slot, and  X   X  is the head word that shows predicate-argument semantic structure as tuples can be found in [15,13]. 
In Table 1 and Table 2, both sentences have the same predicate-argument semantic sentences, the subject China holds index 1 of the argument slots of the predicate purpose of our experiment, we are interested in the  X   X  column above. Thus we use the Boxer 5 component of the C&amp;C tools [14], to get the predicate-argument semantic structure from the CCG output parse trees. 5.1 Subjective Component Identification Adjectives play significant role in expression of feelings in English language. We use a list of 1336 subjective adjectives reported by Hatzivassiloglou and Mckeown [24] to identify subjectivity from the predicate-argument structure of each sentence. Given a structure, the subjectivity score  X  X  X  X  X   X  X  X  X  X  of a sentence is computed as follows: where  X  X  X  X   X  X  X  X  X  X  X  is the predicate-argument structure,  X  is an adjective element in list of  X  X  X  X   X  X  X  X  X  X  X  , and the constant 0.5 can vary empirically while 0 is assigned to a non-subjective structure. For example, if the predicate-argument structure of a sentence contains five terms, at least one or more of these terms must be present in the list of subjective adjectives. 5.2 Semantic Similarity between Structures Having derived predicate-argument structures that represent underlying meaning of sentences, we propose to identify semantic similarities between predicate-argument structures. Given that  X  X  X  X  X   X   X , ...,  X   X  is a set of well formed sentences in document d , the semantic similarity function  X  X  X   X  is used to estimate the similarity score between a given natural language query q and each well formed sentence s, i.e., derived from q ,  X   X  X  X  X  X  X  X  X  X  X  X   X  X  X  X   X   X , ...,  X   X  is a set of predicate-argument terms derived from s . Since the semantic similarity between q and s can be shown using the ( q not similar to s ). However, it is often difficult to determine when  X  X  X   X   X  X , X  X  must be 1 or 0. One straightforward way is to model similarity between each term in  X   X  X  X  X  X  X  X  X  X  X  X  and  X   X  X  X  X  X  X  X  X  X  X  X  . For example, by using topic models such as LSA [16] and PLSA [17] to measure term co-occurrences ( t,s ) between  X   X  X  X  X  X  X  X  X  X  X  X  and  X   X  X  X  X  X  X  X  X  X  X  X  . We use PLSA because of its advantages over the conventional LSA. argument structure, and z is an un-observed class variable which acts as a bottleneck variable in predicting terms for each obser vation. Intuitively, similar predicate-argument structures would show a similarity score that tends towards 1. However, this process suffered a particular set back. We observed direct word overlap between and 1. Whereas, a similarity score of 0.85 does not necessarily indicate similarity. For example, a single dissimilarity among terms in the predicate-argument structures may change the semantic meaning of the concerned sentence. Consider q and s in the following word overlap problem as shown by their predicate-argument structures. has word overlap (i.e. designed/show ) with the predicate-argument structure for  X   X  (i.e. 1 X   X  X  X  X  X  X  X  X  X  X  X   X  . Although, the two predicate-argument structures share the same words like  X  X mazing X  and  X  X wards X , but q and  X   X  turned out to have different semantic meaning. Thus, we propose a more intuitive similarity function that can help model semantic relevance such that the relations score of  X   X  X  X  X  X  X  X  X  X  X  X  to  X   X  X  X  X  X  X  X  X  X  X  X  transformed term-term similarity matrix on which Jaccard Similarity Coefficient used to measure word overlaps between the attributes of paired sets. Ideally, the best JSC value is 1. For each document, we then consider linear combinations of the JSCs and the relevance scores given by a popular relevance model . The chosen relevance model was proposed by Lavrenko and Croft [18], and it has shown good performance for effectively detecting topical relevance in sentences [19]. We give the reason for using this relevant model in section 5.4. 5.3 Transformed Terms Similarity (TTS) Since we can no longer apply PLSA because of its word overlap problem, our model may be prone to implicit relevance problem resulting from synonymy and polysemy . These problems are without doubt solved in LSA and PLSA respectively. We understand that two terms with the same implicit meaning (e.g. clothes and dresses) can exist among  X   X  X  X  X  X  X  X  X  X  X  X  and  X   X  X  X  X  X  X  X  X  X  X  X  . Thus, we propose a straightforward transformation mechanism that uses synonyms and hyponyms of individual terms in both  X   X  X  X  X  X  X  X  X  X  X  X  and  X   X  X  X  X  X  X  X  X  X  X  X  . Arguably, we use the method to determine implicit similarity between terms, such that two paired terms  X  and  X  have implicit meaning if either term is the synonym or hyponym of the other. Thus, we form a similarity matrix distribution over terms to measure implicit relationship between  X   X  X  X  X  X  X  X  X  X  X  X  and  X   X  X  X  X  X  X  X  X  X  X  X  . This approach is used within LSA, termed as LSA term-term matrix. 
We form a similarity matrix where each  X  X  X   X  X  X   X  X  X  X 0 X   X  X  X   X 1 X  is the similarity between  X  and  X  . The similarity  X  X  X   X  X  X  is calculated as the absolute observation of  X   X   X  Independent Model (BIM) [20], to construct the term-term similarity matrix. The BIM method avoids using frequency of terms but takes each term as binary vector over the vocabulary space of  X   X  X  X  and  X   X  X  X  . That is,  X  X  X   X  X  X  shows the similarity between term  X  present in  X   X  X  X  . Conversely,  X  X  X   X  X  X  can also show dissimilarity between term  X  and  X  as is not present in  X   X  X  X  . Synonyms in  X   X  X  X  and hyponyms in  X   X  X  X  were derived using Wiktionary [22]. More importantly, the binary attributes help in calculating the JSC for the term-term similarity matrix which can only contain asymmetric binary  X   X  X  X  X  X  X  X  X  X  X  X   X  X  X  X   X   X ,...,  X   X  be the pair predicate-argument structures respectively. Thus, using BIM, we calculate  X  X  X   X  X  X  as the probability of term incidence vectors  X  X , X | X  X  X  . probabilities that term  X  is  X  and term  X  is not  X  respectively. However, since the actual  X   X   X 1 X  |  X   X  and  X   X   X 0 X  |  X   X  respectively. Since  X  can either be present or not present in Consider the following term-term similarity matrix with asymmetric binary attributes:  X   X  X  X  X  X  X  X  X  X  X  X  and  X   X  X  X  X  X  X  X  X  X  X  X  by using BIM to compute the binary attributes. It is therefore straightforward to compute the JSC value for the term-term similarity matrix. matrix is filled with 1 (i.e. all  X  X  elements must be identical to all  X  X  elements). where  X   X  X  is the total number of attributes where  X  and  X  both have a value of 1,  X   X  X  is the total number of attributes where the attribute of  X  is 0 and the attribute of  X  is 1,  X   X  X  is the total number of attributes where the attribute of  X  is 1 and the attribute of  X  semantically similar to sentence  X  unlike Table 3 which has a JSC value of 0 because of the word overlap problem. 5.4 Linear Relevance Model (LRM) The Transformed Term Similarity (TTS) approach that is described above is efficient based on the fundamental principle of term-term similarity matrix which we adopted. However, in some cases,  X   X  X  X  X  X  X  X  X  X  X  X  and  X   X  X  X  X  X  X  X  X  X  X  X  may not have the same number of rows and columns since there may be variations in the number of terms in each predicate-argument structure. For example, a given query may have just three terms in its predicate-argument structure, while another sentence with much longer grammatical dependencies may have five or more terms in its predicate-argument structure. Which is why we use the relevance model proposed by [18] to estimate from  X   X  X  X  X  X  X  X  X  X  X  X   X   X   X  X  X  X  X  X  X  X  X  X  X  [18]. Consider, for example, the following scenario: Each predicate-argument structure above has different number of terms. The chosen  X   X  X  X  X  X  X  X  X  X  X  X   X   X   X  X  X  X  X  X  X  X  X  X  X  are sampled independently and identically to each other. By this way, the relevance model would be appropriate to determine when the predicate-argument structure  X   X  X  X  X  X  X  X  X  X  X  X  is independently present as part of the predicate-argument structure of some long range sentences such as shown in 1 X   X  X  X  X  X  X  X  X  X  X  X  and 2 X   X  X  X  X  X  X  X  X  X  X  X  above. The relevance model is computed below: each document, we do a linear combination of the absolute sum of subjectivity scores in equation (1) and the results of TTS in equation (6) and the relevant model in equation (7). Thus, the predicate-argument structure of a given query can be compared with the predicate-argument struct ures of sentences with either short-range or long-range grammatical dependencies. While the subjectivity score ensures a document is subjective, we believe the TTS model and the relevance model would solve the short-range and long-range dependencies efficiently and respectively. The linear relevance score for ranking a document is computed as follows: where  X   X   X  X , X  X  is the linear relevance model that takes input as query q and document d satisfying a linear combination expression  X  X  X  X  X  X  X  X  X  X  , where a and b can be number of sentences with short-range dependencies that satisfy TTS, N is the total number of sentences with long-range dependencies that satisfy  X  X   X  X  X  X  X  X  X  in equation 7. We perform experiments on TREC Blog 08 [21] and compared our results with Blog 08 best run and one of the TREC Blog 08 best runs that used title and description fields for opinion finding task (KLEDocOpinTD) [23]. 50,000 English TREC Blog posts were extracted using our ad-hoc blog posts extraction algorithm with optimized sentence model (BlogTEX) 6 . At least 90% of the extracted posts have well formed sentences and are transformed to their equivalent syntactic parse trees and then predicate-argument structures. Again, we use the log-linear CCG parsing model and the Boxer tool[14]. The description fields of 50 query topics (TREC 1001-1050) are also transformed to their equivalent syntactic parse trees and predicate-argument structures. We use Lucene 7 for indexing with focus on high precision at top-ranked documents. Thus, we use a re-ranking technique by initially retrieving top 20 documents using BM25 popular ranking model with empirical parameters k =1.2 and b= 0.75. These top 20 documents are then re-ranked by our model. The IR evaluation metrics used include Mean Average Precision (MAP), R-Prec, and precision at ten (P@10). A comparison of opinion MAPs with the increased number of top-K blog documents is shown in Figure 3. Our model shows improved performance over Blog 08 best run and significantly outperforms Blog 08 X  X  KLEDocOpinTD. Performance improvement in precision and recall curves upon the re-ranking technique is shown in Figure 4. We observed improved performance with and without our re-ranking technique with marginal difference between the two performances. This shows the effectiveness of our model and the possibility of good performance without a re-ranking mechanism. In Table 5 below, the best significant improvements over Blog 08 best run is indicated with *. 
In terms of MAP, our model has significant performance more than Blog 08 best run and Blog 08 X  X  KLEDocOpinTD. Our model also shows significant improvement in terms of R-prec in all cases. This shows the effectiveness of our model in terms of retrieving semantically relevant opinionated documents. We proposed semantic-based opinion retrieval using predicate-argument structures and subjective adjectives. The predicate-argument structures were derived from the output of a syntactic parser. We developed a linear relevance model that is based on semantic similarity between a pair of structures for a query topic and each subjective sentence. We performed experiments on TREC Blog 08 and compared the results with Blog 08 best run and Blog 08 X  X  KLEDocOpinTD. Our model outperforms the baselines in the results shown above. We observed that relevant document to not only contain opinionated query words but opinionated sentences that lexically combine synonyms and hyponyms of query words. This implies that frequency of keywords may not be helpful to context-dependent opinion retrieval afterall. In future, we will consider semantic and opinion dependencies between multiples sentences in the same document. We will also consider using sentence-level subjective component to detect and assign weight to opinionated sentences rather than using only subjective adjectives. 
