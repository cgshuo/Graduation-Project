 Michael Kipp  X  Michael Neff  X  Irene Albrecht Abstract The empirical investigation of human gesture stands at the center of multiple research disciplines, and various gesture annotation schemes exist, with varying degrees of precision and required annotation effort. We present a gesture annotation scheme for the specific purpose of automatically generating and ani-mating character-specific hand/arm gestures, but with potential general value. We focus on how to capture temporal structure and locational information with rela-tively little annotation effort. The scheme is evaluated in terms of how accurately it captures the original gestures by re-creating those gestures on an animated character using the annotated data. This paper presents our scheme in detail and compares it to other approaches.
 Keywords Multimodal corpora Embodied conversational agents Gesture generation Human X  X omputer interaction 1 Introduction Animated characters are useful in a wide range of applications such as interfaces, education, games, movies, and accessibility (sign language) (cf. Rist et al. 2003 ). Generating nonverbal behavior for artificial bodies remains a challenging research task. One important technique for reproducing human-like gestures is to analyze original human behavior (Kipp et al. 2007 ; Neff et al. 2008 ; Kipp 2004 ; Kopp et al. 2004 ; Martin et al. 2006 ). This can be done using motion capture or by manually annotating video data. While motion capture has unequalled precision, it requires special conditions (laboratory, special hardware/software) and the resulting data comes without semantic meaning. In contrast, when manually annotating the video the resulting data is encoded on an abstract level that can be understood and analyzed by conversational analysts, linguists, ethologists, and computer animators alike, whereas motion captured data can only be interpreted with significant computational and human effort. Moreover, manual video annotation is an unobtrusive observation method where people are less aware or unaware of the observation and thus, can be captured performing spontaneous behaviors. Further-more, non-laboratory material (e.g., TV shows) can be analyzed, even of people otherwise unavailable.

If the annotated data is to be used with an animation system that can create arbitrary motions for a humanoid character, the need for positional data becomes highly important, especially if the specific style of a speaker is to be captured. Speakers do not only differ in what and when they gesture, but also where they gesture (Kipp 2004 ). For instance, the  X  X  X aised index finger X  X  can be displayed quite shyly near the chest or dominantly above the head. We believe that such locational variation is integral to personal style.

When transcribing gestures, especially in manual annotation, a lot of information is lost compared to the complexity of the original movement. Consequently, the question arises as to how faithfully the encoding reflects the original movement. However, successfully re-creating the original motion from the encoded data with a virtual character, as we have done (see Fig. 1 ), proves that something essential must have been captured by the annotation.

Existing annotation schemes for human movement can be classified according to the amount of detail they capture, where high detail seems to be proportional to high annotation cost and a low level of abstraction. On one side of the spectrum lies the Bern system (Frey 1999 ; Frey et al. 1983 ), where a large number of degrees of freedom are manually annotated, thus resembling modern motion capture techniques in the type of detail recorded. While this process results in fine grained, purely descriptive and reliably coded data which can be reproduced easily on a synthetic character, the annotation effort is immense. In addition, the resulting data is hard to interpret. It does not abstract away even minor variations and the amount of data is so massive that it is hard to put it in relation to the accumulated knowledge about gesture structure and form found in the literature. On the other end of the spectrum lies conversational analysis and psycholinguistic gesture research (McNeill 1992 ), where the written speech transcription is used as a basis and gestures are annotated by inserting brackets in the text for the beginning and end of a gesture (Kendon 2004 ). Gesture form is captured by either a free-form written account or by gestural categories which describe one prototypical form of the gesture. Such information is too informal or too imprecise for automatic character animation. Thus, a key decision in annotation is: how much does one abstract? Usually, gesture variants are packaged together to form equivalence classes like  X  X  X aised indexfinger X  X  or  X  X  X ictory sign. X  X  So another decision is: how large are these equivalence classes?
We propose a scheme that makes a conscious compromise between purely descriptive, high-resolution approaches and abstract interpretative approaches. For a start, we restrict ourselves to hand/arm movement to identify the most essential features of a gesture before moving to other body parts. Our scheme encodes positional data but relies on an intelligent  X  X  X ime slicing, X  X  based on the concept of gesture phases (cf. Kendon 2004 ), to determine the most relevant time points for position encoding. It is based on the observation that transition points between phases can be mapped to keyframes as used in traditional animation. Moreover, we use the concept of a gesture lexicon, well known in Conversational Analysis, where each lexeme contains some generalized information about form. Lexemes can be taken as prototypes of recurring gesture patterns where certain formational features remain constant over instances and need not be annotated for every single occurrence. When encoding lexeme type for an annotated gesture in the video material all this general data is implicitly encoded as well. For instance, if a coder labels a gesture to be the lexeme  X  X  X aised indexfinger, X  X  the lexeme implies a hand shape (all fingers closed, index finger sticking out) and a hand orientation (index finger pointing up), whereas the precise position of the hand must still be encoded.
In this article, we provide a detailed account of our coding scheme. We extend the scheme described in Kipp ( 2004 ) by adding spatial information and a new speaker. While in previous publications we have focused on the gesture generation and animation system (Neff et al. 2008 , Kipp et al. 2007 ), this article deals with all manual annotation aspects in much more detail. Our scheme was implemented and used within the ANVIL annotation tool (Kipp 2001 ) to transcribe 420 gestures of two speakers, found in a total of 18 min of video footage from TV shows. The latter figure refers to the net amount of video material that was used for gesture transcription after removing unsuitable pieces (hands/arms not visible in the frame or too far away, speaker turned away from the camera).

In the next section we will first motivate our approach by comparing it to related work, and clarify the distinctions of our method.
 2 Related work The method of visualizing hand-annotated data with an animated virtual character can also be employed for the systematic study of the perception of nonverbal behavior (cf. Kra  X  mer et al. 2003 ). Buisine et al. ( 2006 ) have analyzed blended emotions in perception tests by replaying them from manual annotations. A virtual character allows one to switch on or off particular aspects of the annotation and thus explore which parts of the annotation carry the biggest effect, an approach that the authors call copy-synthesis.

The need for a standardization of gesture form has recently been formulated by an international initiative who are developing a unified XML language called BML (Behavior Markup Language) for sending descriptions of form and timing of nonverbal behaviors to an animation engine that controls a virtual character (Vilhjalmsson et al. 2007 ; Kopp et al. 2006 ). Our scheme shares with BML the insight that transition points between movement phases represent a key abstraction for animation. However, we impose a stronger restriction by choosing to focus on the stroke phase, showing that this is sufficient to re-create the observed gesture. In terms of gesture form description, BML is currently under development, but is aiming for a complete set of descriptors where the question remains which components are best suitable for annotation and re-creation.

Chafai et al. ( 2006 ) use an annotation scheme based on gesture phase structure to examine modulations in movement expressivity. They focus on transcribing the expressivity of the gesture on a relatively small corpus. Expressivity refers to how forceful, expansive, smooth etc. a gesture is. To achieve this, they use the expressivity parameters defined by Hartmann et al. ( 2005 ) and encode these for each gesture phase . In our scheme we do not explicitly describe qualitative parameters of the movement. However, the spatial extent of a gesture is captured by our positional data and is in that respect more precise than Chafai et al. X  X  work, whereas in terms of temporal dynamics our scheme carries less information. We will discuss the issue of possible extensions to our scheme in Sect. 6 . 3 Target scenario Our annotation scheme aims at the specific application of generating gestures for an animated character. However, we think that the annotation scheme will be of general interest in the interdisciplinary fields of multimodal and gesture research (Wegener Knudsen 2002 ), because the needs that arise from animating gestures on the basis of manual annotation provide good guidance on the essential descriptive parameters of human gestures.

The generation approach we aim at  X  X  X mitates X  X  a human speaker X  X  gesture behavior using statistical models and a database of annotation data for sample gestures, both of which are extracted from video annotations (Kipp et al. 2007 ; Neff et al. 2008 ; Kipp 2004 ). For this application, the annotation scheme must capture the temporal and spatial structure of a gesture, and its relation to speech. Since the original gesture samples are used as building blocks in the generation process, the annotated data should be rich enough to allow the re-creation of the original gesture in the final synthetic animation. On the other hand, the annotation should be as economical as possible in terms of annotation effort.

Our video corpus consists of 18 min of suitable video passages of two different speakers from two TV talk shows. 4 Annotation scheme While gestures appear to be quite arbitrary in form at first glance various researchers found them to have fairly stable form, even when they are clearly not emblems (Kendon 1996 ). Conversational gestures have no clear meaning and may even be a byproduct of speech. However, there seem to be shared lexica or inventories of conversational gestures (Webb 1997 ). For instance, the metaphoric gesture  X  X  X rogressive X  X  (McNeill 1992 ), where a speaker makes a circular movement with the hands (see Fig. 8 ), seems to occur when talking about progress, movement or the future (Calbris 1990 ). Another universal gesture is the  X  X  X pen hand, X  X  that we call  X  X  X up X  X  (see Fig. 7 ), where the speaker holds the open hand in front of the body, showing the palm (Kendon 2004 ; McNeill 1992 ). While such forms appear to be universal, there is still much inter-speaker and intra-speaker variation in terms of the exact position of the hands and their ensuing trajectory. To investigate and capture these variations was one driving force behind our work.

We use the Anvil video annotation tool (Kipp 2001 ) for our purposes, which allows the annotation of temporal events on multiple tracks (see Fig. 2 ). The tracks can be used to encode the various signal types (e.g., gestures, words, head movements, torso rotations), sometimes several tracks are necessary to describe a signal (e.g., we use three tracks for gestures). Coding consists of adding annotation elements to a track (the colored boxes in Fig. 2 ). The coder describes each element with a set of attributes. The attributes are defined before the whole coding process starts, and the definitions are kept in a separate document: the annotation scheme . For instance, our annotation scheme specifies that in the gesture phrase track we want the coder to fill in the following attributes for a single gesture: lexeme, handedness, trajectory, lexical affiliate, several attributes for positional data, and others. 4.1 Capturing temporal structure We capture the temporal structure of a gesture by first identifying the basic movement phases (Kendon 2004 ; Kita et al. 1998 ; McNeill 1992 , 2005 ): where the stroke is the most energetic part of the gesture. The preparation moves the hands to the stroke X  X  starting position. Holds are optional still phases which can occur before and/or after the stroke. Kita et al. ( 1998 ) identified independent holds which can occur instead of a stroke (recently termed stroke holds by McNeill 2005 ). In this article we distinguish between stroke gestures , which have a stroke, and hold gestures , which have an independent hold instead of a stroke. The retraction returns the arms to a rest pose (e.g., arms hanging down, resting in lap, or arms folded). Kita et al. refined the notion of stroke by defining a multiple stroke that includes repetitions of the first stroke that belong to the same gesture. In our scheme, a stroke contains a  X  X  X umber X  X  attribute to capture the number of within-stroke movements.
To annotate phases in Anvil, the coder specifies beginning and end times of a phase as well as phase type (prep, stroke, etc.) and stroke number. On a second track, the coder combines phases into gestures, also called gesture phrases (Fig. 3 ). In this way, we store the gesture X  X  internal temporal structure, most importantly begin/end times of the stroke or independent hold. On a third track, we combine gestures into gesture units (g-units). A gesture unit is a sequence of contiguous gesture (Kendon 2004 ). This allows us to examine a speaker X  X  g-unit structure, for instance, the average number of gestures, patterns of recurring lexeme sequences, etc. The g-unit only contains a label for the final rest pose that the hands arrive in at the end of the unit. At the moment, we only have a small set of possible rest poses for a standing character, listed in Table 1 . 4.2 Capturing spatial form In order to capture the spatial form we aimed at the best compromise between exactness and economy. For the sake of economy we make two important assumptions: (1) the most  X  X  X nteresting X  X  configurations occur exactly at the beginning and at the end of a stroke, and (2) bihanded gestures are symmetrical. Although many gestures are actually slightly asymmetrical, most of them can be approximated quite well with symmetrical versions.

The first two parameters encoded are handedness and whether the trajectory of the hand(s) in the stroke phase is straight or curved. Next, we have to capture the start and end positions of the hands/arms for the stroke. For a single position we encode three dimensions for hand location (Fig. 4 ) and encode arm swivel as a fourth dimension (Fig. 5 ). These dimensions and the value labels were chosen according to two aims: 1. to have sufficient granularity for later animation and 2. to make manual annotation quick and reliable, which explains the selection of
For bihanded (2H) gestures, the distance between the hands can be inferred from the  X  X  X adial orientation X  X  dimension. However, this yields very coarse results (for instance, one cannot tell from the value  X  X  X ront X  X  whether the hands touch each other or not). Since we deemed it of special importance that there is a high precision in terms of hand-to-hand distance we introduced a special distance encoding for 2H gestures. We extended the Anvil annotation tool to handle  X  X  X patial annotation X  X  where two points can be marked directly on the video screen (see Fig. 6 ). The hand-to-hand distance is normalized by dividing it by the shoulder width which must be encoded each time the size of the displayed speaker changes due to camera movement. One could think of the hand-to-hand/shoulder width ratio as a speaker-independent measure to capture the  X  X  X ize X  X  of a gesture but this remains to be investigated in the future. We are aware of the fact that this parameter could have been encoded without spatial annotation, using a limited set of granular values (e.g.,  X  X  X mall distance, X  X   X  X  X ormal distance, X  X   X  X  X arge distance X  X ), however the spatial coding turned out to be fast and intuitive, providing much more fine-grained results.
In summary, for each stroke-based gesture we encode two positions where each position is expressed by five attributes. Adding handedness and trajectory gives us 12 attributes to code for the spatial form of a gesture. Hold gestures only require one position, for the beginning of the hold (see Table 2 ).
 4.3 Capturing membership to lexical category A number of parameters are predetermined by the gesture X  X  lexeme , including hand shape and palm orientation. For each lexeme, these parameters can be either fixed (definitional parameter), restricted to a range of values, or arbitrary. To annotate lexemes on the phrase track (middle track in Fig. 3 ), we rely on a simplified version of the gesture lexicon collected in (Kipp 2004 ) where 79% agreement in lexeme coding experiments is reported. Typical lexemes include (see Figs. 7 and 8 ): Calm (gently pressing downward, palms pointing downward), Cup (an open hand), Wipe (wiping motion with the flat hand), and Progressive (circular movement). We found 31 and 35 different lexemes for our two speakers respectively with an overlap of 27 lexemes between the two. 4.4 Capturing the relationship between gesture and speech Once shape and lexeme are determined, the gesture must be connected to speech. This is done by annotating the lexical affiliate of each gesture which is the word or words deemed to correspond most closely to a gesture in meaning (Schegloff 1984 ). In ANVIL the coder adds a logical link from a gesture (phrase track) to the word(s) in the speech track. When annotating real data, we found that gesture stroke and lexical affiliate did not always co-occur. However, in the literature and related work, a gesture stroke is traditionally treated as tightly synchronized with (or slightly anticipating)  X  X  X he word that the gesture illustrates X  X  (Cassell et al. 2001 ). Recently, McNeill ( 2005 , p. 37) clarified this issue by calling the words synchronized with gesture  X  X  X o-expressive speech X  X  and says that a  X  X  X exical affiliate does not automatically correspond to the co-expressive speech segment. X  X  We also found this distinction and encode both lexical affiliate and what we call co-occurrence in different attributes. Coding co-occurrence is not trivial since the gesture stroke has a temporal extension and may overlap with many co-occurring words. Choosing every overlapping word does not reflect our intuition of gesture-word co-occurrence. Instead, we were able to come up with a semi-automatic co-occurrence annotation process by using the following heuristics: From the words overlapping with the stroke, choose (1) the word carrying the emphasis, if present, or else (2) the last word. An ANVIL plugin did a first automatic annotation (detecting the last overlapping word), and the human coder corrected it based on the heuristics.
Lexical affiliation is a more difficult task and must be done by hand. We rely on the gesture literature and sometimes intuition when it comes to connecting gestures to the speech X  X  semantics (cf. Kipp 2004 ). The lexeme usually gives some indication: for pointing gestures look for personal pronouns like  X  X  X ou, X  X   X  X  X is X  X  etc., for the metaphoric  X  X  X up X  X  gesture, look for the closest noun, for the metaphoric  X  X  X rogressive X  X  gesture, look for the closest verb or noun that expresses movement or temporal relation. In Fig. 2 one can see that for the abstract pointing gesture  X  X  X ointingHere X  X  (highlighted with a blue frame in the phrase track) the lexical word track) which also happens to co-occur with the stroke. 5 Evaluation by re-creation Any transcription scheme must be measured by three factors. First, how well the annotation reflects the original motion (usually dependent on application or experiment). Second, how reliably the annotation can be performed by human coders. Third, how expensive the annotation is in terms of annotation effort (how much time does it take to encode 1 min of source material?).

We propose a method for evaluating the first criterion: re-creating the gestures with an animated agent (Frey 1999 ). Apart from the annotation information of the corpus we use a lexicon where animation-relevant data (hand shape/orientation, trajectory) is stored for each lexeme so that it does not have to be encoded each time a lexeme is encountered. In our re-creation, the produced animations matched the original motions to a satisfactory degree, depending on the complexity of each original motion. See Fig. 1 for an impression of our re-creation experiments (Neff et al. 2008 ). Although this evaluation method is purely subjective (i.e., it does not yield quantifiable measures of quality), we found it a highly useful tool, especially when watching original video and animated agent side by side. This allowed us (1) to identify specific animation flaws that had to be corrected, and (2) to control the overall quality of the annotation.

For the reliability of the annotation scheme we rely on Kita et al. X  X  ( 1998 ) finding that gesture phases can be coded with 72% reliability and on a similar study by Loehr ( 2004 ) yielding 68% agreement. For the task of coding the gesture lexeme (i.e., assigning the correct lexicon entry to a g-phrase) we showed in (Kipp 2004 ) that a reliability of 79% ( j = 0.78) can be achieved. 1 Future studies must confirm the reliability for the task of position coding.

With respect to the third aspect, annotation effort, our scheme performs reasonably well: we estimate it takes 90 min to encode 1 min of source material for a coder with a short training period. In comparison, for the FORM scheme the authors report a ratio of 20 h per 1 min (Martell 2002 ). For the Bern system there are no such estimates reported, however, their scheme requires approximately 63 data inputs per gesture (Frey 1999 ), compared to 12 data inputs needed in our scheme. This comparison shows how dramatic a reduction in cost can be made by carefully adapting the coding scheme to the target application. 6 Possible extensions In our scheme we made several simplifying assumptions to make the manual annotation as fast and efficient as possible while providing sufficient data for our task. However, in other domains and future extensions of our work the need for more precision will arise. The following extensions would be straightforward to implement without breaking the consistency of the scheme: 6.1 Encoding hand shape Hand shape can be added to the phrase track using an inventory of hand shape labels. Two inventories frequently used are the ASL hand shapes (McNeill 1992 ) and the HamNoSys inventory (Prillwitz et al. 1989 ), both originating from the sign language community. Although we did not encode hand shape for every gesture occurrence we did specify a list of legal hand shapes for each lexeme in the lexicon. These hand shapes were taken from a list of nine hand shapes that seemed to be sufficient for gesture animation in our domain (e.g., open-flat, open-relaxed, finger-ring, fist, etc.). However, the range of shapes required might be a question of how Sophisticated hand animations will require a more precise encoding beyond simple labels to also record the change in hand shape over time during a gesture. 6.2 Encoding the gestures for each hand on separate tracks The current scheme has limitations when both hands are active. It cannot encode the following cases: (1) two separate 1-hand gestures, performed at the same time, and (2) a single but asymmetrical 2-hand gesture. An example for case (1) is a classroom situation where the teacher points at something written on the blackboard with her left hand while making a  X  X  X top X  X  sign gesture with the other hand (i.e., showing the flat palm to the audience). This could be accompanied by saying  X  X  X on X  X  do this. X  X  Such cases rarely occur, at least in our corpus (well under 1%). What occurs more frequently are two gestures performed in sequence with different hands but with a slight overlap: a left handed gesture starting while the right hand is being retracted. To accommodate these situations, our scheme can be extended by creating gesture tracks (i.e., phase + phrase track) for each hand (LH, RH), plus additional tracks (phase/phrase) for symmetrical 2H gestures. For case (2), i.e., asymmetrical 2-hand gestures, one would annotate them on the two separate LH and RH tracks, where an additional attribute must signal that the two movements are part of the same gesture. 6.3 Encoding dynamics For each gesture one might want to capture the velocity profile of the stroke as an indication for abruptness, smoothness, forcefulness, etc. For relatively linear gestures this can be achieved by specifying hand positions on the screen (spatial annotation) at regular time intervals. The simplest form would be encoding hand position at three time points: the beginning, middle, and end of the stroke. This would only work for movement that mainly takes place in the plane parallel to the screen. However, for many cases it might be a good approximation of gesture dynamics. A more abstract and discrete approach is to encode expressivity parameters (Chafai et al. 2006 ) or movement quality (Chi et al. 2000 ) on either the phase level or the phrase level.

Again, we want to point out that an annotation scheme should be tailored to the end application and thus, only display features that can be exploited in later stages of the processing. 7 Conclusion We presented an effective gesture annotation scheme for gesture generation that appears to be a good compromise between detail and economy. Re-creating animations showed that the scheme captures the original motions quite well. We consciously restricted the project to arm/hand movement, ignoring the rest of the body for the sake of simplicity. However, other body parts should be included in the future. Another future issue is to test coding reliability.

We think that the main reason why our annotation so successfully captures gestures in an economic way is that it consciously focuses the annotation effort by exploiting the concept of gesture phases. The coder first identifies those time points most worth investing annotation work in and only then encodes the time-consuming positional data. Another useful practice is to move recurring patterns to a lexicon of gestures. By deciding on the lexeme of a gesture during coding, the coder implicitly encodes a number of lexeme-specific features that need not be explicitly transcribed. While our annotation scheme has obvious drawbacks in what it does not capture (hand shape, asymmetry, etc.) it is straightforward to extend if necessary. However, part of our intent in creating this scheme was to find the most economical solution for descriptive gesture annotation.
 References
