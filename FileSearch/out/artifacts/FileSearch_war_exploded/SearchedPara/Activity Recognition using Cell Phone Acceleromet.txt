 Mobile devices are becoming incr easingly sophisticated and the latest generation of smart ce ll phones now incorporates many diverse and powerful sensors. Thes e sensors include GPS sensors, vision sensors (i.e., cameras), audio sensors (i.e., microphones), light sensors, temperature sensor s, direction sensors (i.e., mag-netic compasses), and acceleration sensors (i.e., accelerometers). The availability of these sensors in mass-marketed communica-tion devices creates exciting new opportunities for data mining and data mining applications. In th is paper we describe and evalu-ate a system that uses phone-based accelerometers to perform activity recognition , a task which involves identifying the physi-cal activity a user is performing. To implement our system we collected labeled accelerometer data from twenty-nine users as they performed daily activities such as walking, jogging, climbing stairs, sitting, and standing, and th en aggregated this time series data into examples that summarize the user activity over 10-second intervals. We then used the resulting training data to in-duce a predictive model for activity recognition. This work is significant because the activity recognition model permits us to gain useful knowledge about the habits of millions of users pas-sively X  X ust by having them carry cell phones in their pockets. Our work has a wide range of applications, including automatic customization of the mobile device X  X  behavior based upon a user X  X  activity (e.g., sending calls directly to voicemail if a user is jogging) and generating a daily/weekly activity profile to deter-mine if a user (perhaps an obese child) is performing a healthy amount of exercise. I.2.6 [ Artificial Intelligence ]: Learning-induction Algorithms, Design, Experime ntation, Human Factors Sensor mining, activity recognition, induction, cell phone, accel-erometer, sensors Mobile devices, such as cellula r phones and music players, have recently begun to incorporate dive rse and powerful sensors. These sensors include GPS sensors, audio sensors (i.e., microphones), image sensors (i.e., cameras), light sensors, temperature sensors, direction sensors (i.e., compasses) and acceleration sensors (i.e., accelerometers). Because of the small size of these  X  X mart X  mo-bile devices, their substantial computing power, their ability to send and receive data, and their nearly ubiquitous use in our soci-ety, these devices open up exciting new areas for data mining research and data mining applications. The goal of our WISDM (Wireless Sensor Data Mining) proj ect [19] is to explore the re-search issues related to mining sensor data from these powerful mobile devices and to build useful applications. In this paper we explore the use of one of these sensors, the accelerometer, in or-der to identify the activity that a user is performing X  X  task we refer to as activity recognition. We have chosen Android-based cell phones as the platform for our WISDM project because the Android operating system is free, open-source, easy to program, a nd expected to become a domi-nant entry in the cell phone marketplace (this is clearly happen-ing). Our project currently employs several types of Android phones, including the Nexus One, HTC Hero, and Motorola Back-flip. These phones utilize different cellular carriers, although this is irrelevant for our purposes si nce all of the phones can send data over the Internet to our server using a standard interface. How-ever, much of the data in this work was collected directly from files stored on the phones via a US B connection, but we expect this mode of data collection to become much less common in future work. All of these Android phones, as well as virtually all new smart phones and smart music players, including the iPhone and iPod Touch [2], contain tri-axial accelerometers that measure accelera-tion in all three spatial dimensions. These accelerometers are also capable of detecting the orientation of the device (helped by the fact that they can detect the direction of Earth X  X  gravity), which can provide useful information for activity recognition. Acceler-ometers were initially included in these devices to support ad-vanced game play and to enable automatic screen rotation but they clearly have many other applications. In fact, there are many useful applications that can be built if accelerometers can be used to recognize a user X  X  activity. For example, we can automatically monitor a user X  X  activity level and generate daily, weekly, and monthly activity reports, which could be automatically emailed to the user. These reports would i ndicate an overall activity level, which could be used to gauge if the user is getting an adequate amount of exercise and estimat e the number of daily calories expended. These reports could be used to encourage healthy prac-tices and might alert some users to how sedentary they or their children actually are. The activity information can also be used to automatically customize the beha vior of the mobile phone. For example, music could automatically be selected to match the ac-tivity (e.g.,  X  X pbeat X  music when th e user is running) or send calls directly to voicemail when the user is exercising. There are un-doubtedly numerous other instances where it would be helpful to modify the behavior of the phone based on the user activity and we expect that many such applications will become available over the next decade. In order to address the activity recognition task using supervised learning, we first collected accelerometer data from twenty-nine users as they performed activities such as walking, jogging, as-cending stairs, descending stairs, sitting, and standing. We then aggregated this raw time series accelerometer data into examples, as described in Section 2.2, where each example is labeled with the activity that occurred while that data was being collected. We then built predictive models for activity recognition using three classification algorithms. The topic of accelerometer-based activity recognition is not new. Bao &amp; Intille [3] developed an activity recognition system to identify twenty activities using bi-axial accelerometers placed in five locations on the user X  X  body . Additional studies have simi-larly focused on how one can use a variety of accelerometer-based devices to identify a range of user activities [4-7, 9-16, 21]. Other work has focused on the app lications that can be built based on accelerometer-based activity recognition. This work includes identifying a user X  X  activity level and predicting their energy con-sumption [8], detecting a fall and the movements of user after the fall [12], and monitoring user activ ity levels in order to promote health and fitness [1]. Our work differs from most prior work in that we use a commercial mass-marketed device rather than a research-only device, we use a single device conveniently kept in the user X  X  pocket rather than multiple devices distributed across the body, and we require no additional actions by the user. Also, we have generated and tested our models using more users (twenty-nine) than most previous studies and expect this number to grow substantially since we ar e continuing to collect data. The few studies that have involved co mmercial devices such as smart phones have focused either on a very small set of users [21] or have trained models for particular users [4] rather than creating a universal model that can be applied to any user. Our work makes several contribu tions. One contribution is the data that we have collected and continue to collect, which we plan to make public in the future. This data can serve as a resource to other researchers, since we were unable to find such publically available data ourselves. We al so demonstrate how raw time se-ries accelerometer data can be transformed into examples that can be used by conventional cla ssification algorithms. We demon-strate that it is possible to perform activity recognition with com-monly available (nearly ubiquit ous) equipment and yet achieve highly accurate results. Finally, we believe that our work will help bring attention to the opportunitie s available for mining wireless sensor data and will stimulate a dditional work in this area. The remainder of this paper is structured as follows. Section 2 describes the process for addre ssing the activity recognition task, including data collection, data pr eprocessing, and data transfor-mation. Section 3 describes our experiments and results. Related work is described in Section 4 and Section 5 summarizes our conclusions and discusses areas for future research. In this section we describe the activity recognition task and the process for performing this task. In Section 2.1 we describe our protocol for collecting the raw accelerometer data, in Section 2.2 we describe how we preprocess a nd transform the raw data into examples, and in Section 2.3 we describe the activities that will be predicted/identified. In order to collect data for our supervised learning task, it was necessary to have a large number of users carry an Android-based smart phone while performing certain everyday activities. Before collecting this data, we obtaine d approval from the Fordham Uni-versity IRB (Institutional Review Board) since the study involved  X  X xperimenting X  on human subjects and there was some risk of harm (e.g., the subject could trip while jogging or climbing stairs). We then enlisted the he lp of twenty-nine volunteer sub-jects to carry a smart phone while performing a specific set of activities. These subjects carried the Android phone in their front pants leg pocket and were asked to walk, jog, ascend stairs, de-scend stairs, sit, and stand for specific periods of time. The data collection was controlled by an application we created that executed on the phone. This application, through a simple graphical user interface, permitted us to record the user X  X  name, start and stop the data collection, and label the activity being per-formed. The application permitted us to control what sensor data (e.g., GPS, accelerometer) was collected and how frequently it was collected. In all cases we collected the accelerometer data every 50ms, so we had 20 samples per second. The data collection was supervised by one of the WISDM team members to ensure the quality of the data. Standard classification algorithms cannot be directly applied to raw time-series accelerometer data. Instead, we first must trans-form the raw time series data into examples [18]. To accomplish this we divided the data into 10-second segments and then gener-ated features that were based on the 200 readings contained within each 10-second segment. We refer to the duration of each segment as the example duration (ED). We chose a 10-second ED because we felt that it provided sufficient time to capture several repetitions of the (repetitive) motions involved in some of the six activities. Although we have not pe rformed experiments to deter-mine the optimal example duration value, we did compare the results for a 10-second and 20-s econd ED and the 10-second ED yielded slightly better results (as well as twice as many training examples). Next we generated informativ e features based on the 200 raw accelerometer readings, where each reading contained an x, y, and z value corresponding to the three axes/dimensions (see Figure 1). We generated a total of forty-three summary features, although these are all variants of just six basic features. The features are described below, with the number of features generated for each feature-type noted in brackets:  X  Average [3]: Average acceleration (for each axis)  X  Standard Deviation [3]: Standard deviation (for each axis)  X  Average Absolute Difference [3]: Average absolute  X  Average Resultant Acceleration [1]: Average of the square  X  Time Between Peaks [3]: Time in milliseconds between  X  Binned Distribution [30]: We determine the range of values The  X  X ime between peaks X  feature requires further explanation. The repetitive activities, like walking, tend to generate repeating waves for each axis and this feature tries to measure the time between successive peaks. To estimate this value, for each example we first identify all of the peaks in the wave using a heuristic method and then identify the highest peak for each axis. We then set a threshold based on a percentage of this value and find the other peaks that met or exceed this threshold; if no peaks meet this criterion then the threshold is lowered until we find at least three peaks. We then m easure the time between successive peaks and calculate the average. For samples where at least three peaks could not be found, the tim e between peaks is marked as unknown. This method was able to accurately find the time between peaks for the activities that had a clear repetitive pattern, like walking and jogging. Certainly more sophisticated schemes will be tried in the future. The number of examples generated per user for each activity var-ies. These differences are due to the time limitations that some users may have or physical limitations that impact the time they spend on each activity. Our data set is summarized in Section 3.1. In this study we consider six activities: walking, jogging, ascend-ing stairs, descending stairs, sitti ng, and standing. We selected these activities because they are performed regularly by many people in their daily routines. The activities also involve motions that often occur for substantial time periods, thus making them easier to recognize. Furthermore, most of these activities involve repetitive motions and we believe this should also make the ac-tivities easier to recognize. When we record data for each of these activities, we record acceleration in three axes. For our purposes, the z-axis captures the forward movement of the leg and the y-axis captures the upward and dow nward motion. The x-axis cap-tures horizontal movement of th e user X  X  leg. Figure 1 demon-strates these axes relative to a user. Figure 2 plots the accelerometer data for a typical user, for all three axes and for each of the six activities. It is clear that sitting and standing (Figure 2e,f) do not exhibit periodic behavior but do have distinctive patterns, based on the relative magnitudes of the x, y, and z, values, while the four other activities (Figure 2a-d), which involve repetitive motions, do exhibit periodic behavior. Note that for most activities the y values have the largest accel-erations. This is a consequen ce of Earth X  X  gravitational pull, which causes the accelerometer to measure a value of 9.8 m/s the direction of the Earth X  X  center. For all activities except sitting this direction corresponds to the y axis (see Figure 1). The periodic patterns for walking, jogging, ascending stairs, and descending stairs (Figure 2a-d) can be described in terms of the time between peaks and by the relative magnitudes of the acceleration values. The plot for walking, shown in Figure 2a, demonstrates a series of high peaks for the y-axis, spaced out at approximately  X  second intervals. The peaks for the z-axis acceleration data echo these peaks but with a lower magnitude. The distance between the peaks of the z-axis and y-axis data represent the time of one stride. The x-axis values (side to side) have an even lower magnitude but nonetheless mimic the peaks associated with the other axes. For jogging, similar trends are seen for the z-axis and y-axis data, but the time between peaks is less (~ X  second), as one would expect. As one might expect, the range of y-axis acceleration values for jogging is greater than for walking, although the shift is more noticeable in the negative direction. For descending stairs, one observes a series of small peaks for y-axis acceleration that take place every ~ X  second. Each small peak represents movement down a single stair. The z-axis values show a similar trend with negative acceleration, reflecting the regular movement down each stair. The x-axis data shows a series of semi-regular small peaks, with acceleration vacillating again between positive and negative valu es. For ascending stairs, there are a series of regular peaks for the z-axis data and y-axis data as well; these are spaced approximately ~ X  seconds apart, reflecting the longer time it takes to climb up stairs. Acceleration As one would expect, sitting and standing do not exhibit any regular periodic behavior and all of the acceleration values are relatively constant. As mentioned earlier, the primary differences between these activities is the re lative magnitudes of values for each axis, due to the different orientations of the device with respect to the Earth when the user is sitting and standing Thus it appears easy to differentiate between sitting and standing, even though neither involves much movement. Note that because the accelerometers are themselves able to determine orientation with respect to the Earth X  X  gravitational field, it would be relatively straightforward to compensate/correct for any changes in the cell phone X  X  orientation due to the phone shifting position in a user X  X  pocket. We plan to implement th is correction in future work. In this section we describe our experiments and then present and discuss our results for the activity recognition task. Our experiments first require us to collect the labeled raw accel-erometer data and then transform that data into examples. This process was described in Secti on 2. The resulting examples con-tain 43 features and cover twenty-n ine users. This forms the data set, described in Table 1, which is subsequently used for training and testing. The last row in Table 1 shows the percentage of the total examples associated with each activity. Note that certain activities contain fewer examples than others, mainly because the users were not asked to perform strenuous activities (e.g., jogging, climbing st airs) for very long and because we thought that the patterns in other activities (e.g., standing) would become apparent quickly so that there would be no need to waste the users time literally  X  X tanding around. X  Furthermore, certain activities, like standing a nd sitting, were only added after the study began, so we have no data for these activities for some users. Once the data set was prepared, we used three classification tech-niques from the WEKA data mining suite [20] to induce models for predicting the user activities: decision trees (J48), logistic regression and multilayer neural networks. In each case we used the default settings. We used ten-fold cross validation for all ex-periments and all results are based on these ten runs. The summary results for our activity recognition experiments are presented in Table 2. This table specifies the predictive accuracy associated with each of the activ ities, for each of the three learn-ing algorithms and for a simple  X  X  traw man X  strategy. The straw man strategy always predicts the specified activity (i.e., walking for the first row in Table 2 and jogging for the second row of Table 2) or, when assessing the overall performance of the classi-fier (i.e., the last row of Table 2), always predicts the most fre-quently occurring activity, which happens to be walking. The baseline straw man strategy allows us to consider the degree of class imbalance when evaluating the performance of the activity recognition system. Walking 89.9 93.6 91.7 37.2 Jogging 96.5 98.0 98.3 29.2 Upstairs 59.3 27.5 61.5 12.2 Downstairs 55.5 12.3 44.3 10.0 Standing 93.3 87.0 91.9 5.0 Overall 85.1 78.1 91.7 37.2 Table 2 demonstrates that in most cases we can achieve high lev-els of accuracy. For the two most common activities, walking and jogging, we generally achieve accuracies above 90%. Jogging appears easier to identify than walking, which seems to make sense, since jogging involves more extreme changes in accelera-tion. It appears much more difficult to identify the two stair climbing activities, but as we sha ll see shortly, that is because those two similar activities are ofte n confused with one another. Note that although there are very few examples of sitting and standing, we can still identify th ese activities quite well, because, as noted earlier, the two activities cause the device to change orientation and this is easily detected from the accelerometer data. Our results indicate that none of the three learning algorithms consistently performs best, but the multilayer perceptron does perform best overall. More detailed results are presented in Tables 3-5, which show the confusion matrices associated with each of the three learning algorithms. The most important activities to analyze are the climbing-up and climbing-down stair activities, since these were the only activities that that were difficult to recognize. The confusion matrices indi-cate that many of the prediction errors are due to confusion be-tween these two activities. If we focus on the results for the J48 decision tree model in Table 3, we see that when we are climbing up stairs the most common incorrect classification occurs when we predict  X  X ownstairs, X  which occurs 107 times and accounts for a decrease in accuracy of 19.6% (107 errors out of 545). When the actual activity is climbing dow nstairs, walking slightly out-paces  X  X pstairs X  in terms of the total number of errors (99 vs. 92), but this is only because walking occurs more than three times as often as climbing upstairs in our da taset. If we look at Figures 2a, 2c, and 2d, we see that the patterns in acceleration data between  X  X alking X ,  X  X scending stairs X  and  X  X escending stairs X  are some-what similar. To limit the conf usion between the ascending and descending stair activities, we ra n another set of experiments where we combine ascending stairs and descending stairs into one activity. The resulting confusion matrix for the J48 algorithm is shown in Table 6 (in the interest of space we do not show them for the other two algorithms). We see that the results are substan-tially improved, although stair climbing is still the hardest activity to recognize. Table 6: Confusion Matrix for J48 Model (Stairs Combined) Actual Class Stand 3 1 10 0 209 93.7 Activity recognition has recently gained attention as a research topic because of the increasing availability of accelerometers in consumer products, like cell phones, and because of the many potential applications. Some of the earliest work in accelerometer-based activity recognition focused on the use of multiple acceler-ometers placed on several parts of the user X  X  body. In one of the accelerometers worn on the user X  X  right hip, dominant wrist, non-dominant upper arm, dominant ankl e, and non-dominant thigh in order to collect data from 20 users. Using decision tables, in-stance-based learning, C4.5 and Na X ve Bayes classifiers, they created models to recognize twenty daily activities. Their results indicated that the accelerometer placed on the thigh was most powerful for distinguishing between activities. This finding sup-ports our decision to have our test subjects carry the phone in the most convenient location X  X heir pants pocket. Other researchers have, like Bao &amp; Intille, used multiple acceler-ometers for activity recognition. Krishnan et. al. [9] collected data from three users using two accelerometers to recognize five ac-tivities X  X alking, sitting, standing, running, and lying down. This paper claimed that data from a thigh accelerometer was insuffi-cient for classifying activities such as sitting, lying down, walk-ing, and running, and thus multiple accelerometers were neces-sary (a claim our research contra dicts). In another paper, Krishnan et. al. [10] examined seven lower body activities using data col-lected from ten subjects wearing three accelerometers. This method was tested in supervised and semi-naturalistic settings. Tapia et. al. [16] collected data from five accelerometers placed on various body locations for twenty -one users and used this data to implement a real-time system to recognize thirty gymnasium activities. A slight increase in performance was made by incorpo-rating data from a heart monitor in addition to the accelerometer data. Mannini and Sabitini [23] used five tri-axial accelerometers attached to the hip, wrist, arm, ankle, and thigh in order to recog-nize twenty activities from thirteen users. Various learning meth-ods were used to recognize three  X  X ostures X  (lying, sitting, and standing) and five  X  X  ovements X  (walking, stair climbing, run-ning, and cycling). Foerster and Fahrenberg [28] used data from five accelerometers in one set of experiments and from two of those accelerometers in another for activity recognition. Thirty-one male subjects participated in the study and a hierarchical classification model was built in order to distinguish between postures such as sitting and lying at specific angles, and motions such as walking and climbing stairs at different speeds. Researchers have used a combination of accelerometers and other sensors to achieve activity recognition. Parkka et. al. [27] created a system using twenty different types of sensors (including an accelerometer worn on the chest and one worn on the wrist) in order to recognize activities such as lying, standing, walking, running, football, swinging, croque t, playing ball, and using the toilet in specific locations. Lee and Mase [25] created a system to recognize a user X  X  location and ac tivities, including sitting, stand-ing, walking on level ground, wa lking upstairs, and walking downstairs using a sensor module that consisted of a biaxial ac-celerometer and an angular velocity sensor worn in the pocket combined with a digital compa ss worn at the user X  X  waist. Subramayana et. al. [26] addressed similar activities by building a model using data from a tri-axial accelerometer, two micro-phones, phototransistors, temperat ure and barometric pressure sensors, and GPS to distinguish be tween a stationary state, walk-ing, jogging, driving a vehicle, and climbing up and down stairs. While these systems using multiple accelerometers or a combina-tion of accelerometers and other sensors were capable of identify-ing a wide range of activities, they are not very practical because they involve the user wearing multiple sensors distributed across their body. This could work for some short term, small scale, highly specialized applications (e.g., in a hospital setting) but would certainly not work for the applications that we envision. Some studies have also focused on combining multiple types of sensors in addition to accelerometers for activity recognition. Maurer et al. [13] used  X  X Watch X  devices placed on the belt, shirt pocket, trouser pocket, backpack, and neck to recognize the same six activities that we consider in our study. Each  X  X Watch X  con-sisted of a biaxial accelerometer and a light sensor. Decision trees, k-Nearest Neighbor, Na X ve Ba yes, and Bayes Net classifiers with five-fold cross validation were used for learning. Choudhury et. al [6] used a multimodal sensor device consisting of seven different types of sensors (tri-axial accelerometer, microphone, visible light phototransitor, barome ter, visible+IR light sensor, humidity/temperature reader, and compass) to recognize activities such as walking, sitting, standing, ascending stairs, descending stairs, elevator moving up and dow n, and brushing one X  X  teeth. Cho et. al. [5] used a single tri-axial accelerometer, along with an embedded image sensor worn at the user X  X  waist, to identify nine activities. Although these multi-sens or approaches do indicate the great potential of mobile sensor da ta as more types of sensors are being incorporated into devices, our approach shows that only one type of sensor X  X n accelerometer X  X s needed to recognize most daily activities. Thus our method offers a straightforward and easily-implementable approach to accomplish this task. Other studies, like our own, have focused on the use of a single accelerometer for activity recogniti on. Long, Yin, and Aarts [22] collected accelerometer data from twenty-four users using a tri-axial accelerometer worn without regard for orientation at the user X  X  waist. Data was collect ed naturalistically, and decision trees as well as a Bayes classifier combined with a Parzen win-dow estimator were used to recognize walking, jogging, running, cycling, and sports. Lee et. al. [24] used a single accelerometer attached to the left waists of five users. Standing, sitting, walking, lying, and running were all recognized with high accuracies using fuzzy c-means classification. However unlike these studies, which use devices specifically made fo r research purposes, our method utilizes commercial devices that are widely-available without any additional specialized equipment. This approach enables make practical real-world appli cations for our models. Several researchers have considered the use of widely-available mobile devices such as cell phones to address the activity recogni-tion problem. However the earlier approaches did not take advan-tage of the sensors incorporated into the mobile devices them-selves. For example, Gyorbiro et. al. [7] used  X  X otionBands X  attached to the dominant wrist, hip, and ankle of each subject to distinguished between six differe nt motion patterns. Each Mo-tionBand contained a tri-axial accelerometer, magnetometer, and gyroscope. As the MotionBand collected data, the data was then transmitted to a smart phone carried by the user to be stored. Ravi et. al. [15] collected data from two users wearing a single acceler-ometer-based device and then transmitted this data to the HP iPAQ mobile device carried by the user. Using this data for activ-ity recognition, researchers compared the performance of eighteen different classifiers. Lester et. al. [11] used accelerometer data, along with audio and barometric sensor data, to recognize eight daily activities from a small set of users. While these studies could have used a cell phone to generate the accelerometer data, they did not do this. Instead, the data was generated using distinct accelerometer-based devices worn by the user and then sent to a cell phone for storage. A few studies, like ours, did us e an actual commercial mobile device to collect data for activity recognition. Such systems offer an advantage over other accelerometer-based systems because they are unobtrusive and do not require any additional equipment for data collection and accurate recognition. Miluzzo et. al. [14] explored the use of various se nsors (such as a microphone, accel-erometer, GPS, and camera) available on commercial smart phones for activity recognition and mobile social networking applications. In order to address the activity recognition task, they collected accelerometer data from ten users to build an activ-ity recognition model for walking, running, sitting, and standing using J48. This model had par ticular difficulty distinguishing between the sitting and standing activities, a task that our models easily achieve. Yang [21] developed an activity recognition sys-tem using the Nokia N95 phone to distinguish between sitting, standing, walking, running, driving, and bicycling. This work also explored the use of an activity recognition model to construct physical activity diaries for the users. Although the study achieved relatively high accuracies of prediction, stair climbing was not considered and the system was trained and tested using data from only four users. Brezmes et. al. [4] also used the Nokia N95 phone to develop a real-time system for recognizing six user activities. In their system, an activity recognition model is trained for each user, meaning that there is no universal model that can be applied to new users, for whom no training data exists. Our mod-els do not have this limitation. In this paper we described how a smart phone can be used to per-form activity recognition, simply by keeping it in ones pocket. We further showed that activity recognition can be highly accu-rate, with most activities being recognized correctly over 90% of the time. In addition, these activities can be recognized quickly, since each example is generated from only 10 seconds worth of data. We have several interesting applications in mind for activity recognition and plan to implement some of these applications in the near future. Our work would not have been possible without establishing our WISDM Android-based data coll ection platform, and we view this software and hardware archit ecture, where data is transmitted by the phone to our Internet-based server, as a key resource pro-duced as a consequence of this work. By having this in place we will be able to mine other mobile sensor data much more quickly. This platform, as well as the da ta that we collected, will ulti-mately be made public. We plan to improve our activity recognition in several ways. The straightforward improvements i nvolve: 1) learning to recognize additional activities, such as bicy cling and car-riding, 2) obtaining training data from more users with the expectation that this will improve our results, 3) generati ng additional and more sophisti-cated features when aggregating the raw time-series data, and 4) evaluating the impact of carrying the cell phone in different loca-tions, such as on a belt loop. In addition, in the near future we plan to significantly enhance our WISDM platform so that we can generate results in real-time, whereas currently our results are generated off-line and are not reported back to the mobile phone and the user. We plan to provide real-time results in two ways. The first way minimizes the intelligence required on the phone by having the phone transmit the data to the Internet-based sever over the cellular connection, as usual, with the server applying the activity recognition model and transmitting the results back to the phone. In one variant, the phone will send the raw accelerometer data and in a second variant the phone will perform the data trans-formation step and only transmit the data when an example is generated. The second method i nvolves implementing the activity recognition model directly on the cell phone. Given the computa-tional power of these devices, this is certainly a feasible option. One key advantage of this method is that it removes the need for a server, which makes the solution perfectly scalable, and ensures the user X  X  privacy, since the sens or data is kept locally on the device. The work described in this paper is part of a larger effort to mine sensor data from wireless devices . We plan to continue our WISDM project, applying the accelerometer data to other tasks besides activity recognition and collecting and mining other sen-sor data, especially GPS data. We believe that mobile sensor data provides tremendous opportunities fo r data mining and we intend to leverage our Android-based da ta collection/data mining plat-form to the fullest extent possible. [1] Anderson, I., Maitland, J., Sherwood, S., Barkhuus, L., [2] Apple iPhone and Apple iPod Touch. 2009. Apple Inc. [3] Bao, L. and Intille, S. 2004. Activity Recognition from User-[4] Brezmes, T., Gorricho, J.L., and Cotrina, J. 2009. Activity [5] Cho, Y., Nam, Y., Choi, Y-J ., and Cho, W-D. 2008. Smart-[6] Choudhury, T., Consolvo, S., Ha rrison, B., LaMarca, A., [7] Gyorbiro, N., Fabian, A., and Homanyi, G. 2008. An activity [8] Inooka, H., Ohtaki, Y. Hayasaka, H. Suzuki, A., and Na-[9] Krishnan, N., Colbry, D., Juilla rd, C., and Panchanathan, S. [10] Krishnan, N. and Panchanathan , S. 2008. Analysis of Low [11] Lester, J., Choudhury, T. and Borriello, G. 2006. A practical [12] Mathie, M., Celler B., Lovell N., and Coster A. 2004. Classi-[13] Maurer, U., Smailagic, A., Siewiorek, D., &amp; Deisher, M. [14] Miluzzo, E., Lane, N., Fodor, K., Peterson, R., Lu, H., Mu-[15] Ravi, N., Dandekar, N. 2005. Activity recognition from ac-[16] Tapia, E.M., Intille, S.S. et al. 2007. Real-Time recognition [17] Unwired View.com. 2009. Googl e wants to make your [18] Weiss, G. M., and Hirsh, H. 1998. Learning to predict rare [19] WISDM (Wireless Sensor Data Mining) Project. Fordham [20] Witten, I. H. and Frank, E. Data Mining: Practical Machine [21] Yang, J. 2009. Toward physical activity diary: Motion rec-[22] Long, X., Yin, B., and Aarts, R.M. 2009. Single accelerome-[23] Mannini, A. and Sabatini A.M. 2010. Machine learning [24] Lee, M., Kim, J., Kim, K., Lee, I., Jee, S.H., and Yoo, S.K. [25] Lee, S.-W. and Mase, K. 2002. Activity and location recog-[26] Subramanya, A., Raj, A., Bilmes, J., a nd Fox, D. 2006. Rec-[27] Parkka, J., Ermes, M., Korpipaa, P., Mantyjarvi, J., Peltola, [28] Foerster F. and Fahrenberg J. 2000. Motion pattern and pos-
