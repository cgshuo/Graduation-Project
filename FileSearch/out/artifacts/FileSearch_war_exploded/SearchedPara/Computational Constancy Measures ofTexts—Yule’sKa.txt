 Kyushu University and JST-PRESTO Gunosy Inc.

We examine various measures proposed since Yule and reconsider reports made so far, thus overviewing the study of constancy measures. We then explain how K is essentially equiva-within language science. We then empirically examine constancy measure candidates within this new, broader context. The approximated higher-order entropy exhibits stable convergence across different languages and kinds of text. We also show, however, that it cannot identify authors, contrary to Yule X  X  intention. Lastly, we apply K to two unknown scripts, the Voynich scripts. 1. Introduction
A constancy measure for a natural language text is defined, in this article, as a com-putational measure that converges to a value for a certain amount of text and remains invariant for any larger size. Because such a measure exhibits the same value for any size the form of his measure K . Since Yule, there has been a continuous quest for such measures, and various formulae have been proposed. They can be broadly categorized into three types, namely, those measuring (1 ) repetitiveness, (2) power law character, and (3) complexity. would differ for texts written by different authors. State-of-the-art multivariate machine learning techniques are powerful, however, for solving such language engineering tasks, in which Yule X  X  K is used only as one variable among many, as reported in Stamatatos, Fakotakis, and Kokkinakis (2001) and Stein, Lipka, and Prettenhofer (2010). understanding the mathematical nature of language. Although mathematical models of language have been studied in the computational linguistics milieu, via Markov models (Manning and Schuetze 1999), Zipf X  X  law and its modifications (Mandelbrot 1953; Zipf 1965; Bell, Cleary, and Witten 1990), and Pitman-Yor models (Teh 2006) more recently, the true mathematical model of linguistic processes is ultimately unknown. Therefore, the convergence of a constancy measure must be examined through empirical verifi-cation. Because some constancy measures have a mathematical theory of convergence for a known process, discrepancies in the be havior of real linguistic data from such a theory would shed light on the nature of linguistic processes and give hints towards improving the mathematical models. Furth ermore, as one application, a convergent measure would allow for comparison of different texts through a common, stable norm, provided that the measure converges for a s ufficiently small amount of text. One of our goals is to discover a non-trivial measure with a certain convergence speed that distinguishes the different natures of texts.
 study of constancy measures over 70 years has been about, by answering the three following questions mathematically and empirically: Question 1 Does a measure exhibit constancy? Question 2 If so, how fast is the convergence speed? Question 3 How discriminatory is the measure?
We seek answers by first showing the meaning of Yule X  X  K in relation to the R  X  enyi higher-order entropy, and by then empirically examining constancy across large-scale texts of different kinds. We finally provide an application by considering the natures of two unknown scripts, the Voynich manuscript and Rongorongo, in order to show the possible utility of a constancy measure.
 (1998), the first paper to have examined the empirical behavior of constancy measures on real texts. The authors used English literary texts to test constancy measure candi-dates proposed prior to their work. Today, the coverage and abundance of language corpora allow us to conduct a larger-scale i nvestigation across multiple languages. Recently, Golcher (2007) tested his measure V (discussed later in this paper) with Indo-
European languages and also programming language sources. Our papers (Kimura and Tanaka-Ishii 2011, 2014) also precede this one, presenting results preliminary to this article but with only part of our data, and neither of those provides mathematical analysis with respect to the R  X  enyi entropy. Compared with these previous reports, our contribution here can be summarized as follows: 482
We start by summarizing the potential constancy measures proposed so far. 2. Constancy Measures
The measures proposed so far can broadly be categorized into three types, calculating the repetitiveness, power-law distributio n, or complexity of text. This section mathe-matically analyzes these measures and summarizes them. 2.1 Measures Based on Repetitiveness
The study of text constancy started with proposals for simple text measures of vocab-ulary repetitiveness. The representative example is Yule X  X  K (Yule 1944), while Golcher recently proposed V as another candidate (Golcher 2007). 2.1.1 Yule X  X  K. To the best of our knowledge, the oldest mention of constancy values was made by Yule with his notion of K (Yule 1944). Let N be the total number of words in a text, V ( N ) be the number of distinct words, V ( m , N ) be the number of words appearing m times in the text, and m max be the largest frequency of a word. Yule X  X  K is then defined as follows, through the first and second moments of the vocabulary population distribution of V ( m , N ), where S 1 = N = m mV ( m , N ), and S 1944; Herdan 1964): where C is a constant enlarging of the value of K ,definedbyYuleas C designed to measure the vocabulary richness of a text: The larger Yule X  X  K ,thelessrich the vocabulary is. The formula can be intuitively understood from the main term of the sum in the formula. Because the square of ( m N ) 2 indicates the degree of recurrence of a word, the sum of such degrees for all words is small if the vocabulary is rich, or large in the opposite case. Another simple example can be given in terms of S Suppose a text is 10 words long: if each of the 10 tokens is distinct (high diversity), then
S
S appeared here and there. For example, Herdan defined V m as follows (Herdan 1964, pp. 67, 79):
Likewise, Simpson (1949) derived the following formula as a measure to capture the diversity of a population: which is equivalent to Yule X  X  K ,asSimpsonnoted. 2.1.2 Other Measures Based on Simple Text Statistics. Apart from Yule X  X  K , various mea-sures have been proposed from simple statistical observation of text, as detailed in
Tweedie and Baayen (1998). One genre is based on the so-called token-type relation (i.e., the ratio of the vocabulary size V ( N )andthetextsize N , in log) as formulated by
Guiraud (1954) and Herdan (1964) as a law. Because this simple ratio is not stable, the measure was modified numerous t imes to formulate Herdan X  X  C (Herdan 1964),
Dugast X  X  k and U (Dugast 1979), Maas X  a 2 (Maas 1972), Tuldava X  X  LN (Tuldava 1977), and Brunet X  X  W (Brunet 1978).
 vocabulary size V ( N )(Honor  X  e 1979). Another ratio, of V (2, N )to V ( N ), was proposed as a text characteristic by Sichel (1975) and Maas (1972).
 extensive study conducted by Tweedie and Baayen (1998). In common with Yule X  X  inten-tion to apply such measures for author identification, they examined all of the measures discussed here, in addition to two measures explained later: Orlov X  X  Z , and the Shannon entropy upper bound obtained from the relative frequencies of unigrams. They exam-ined these measures with English novels (such as Alice X  X  Adventures in Wonderland )and empirically found that only Yule X  X  K and Orlov X  X  Z were convergent. Given their report, we consider K the only true candidate among the constancy measures examined so far. 2.1.3 Golcher X  X  V. Golcher X  X  V is a string-based measure calculated on the suffix tree of a text (Golcher 2007). Letting the length of the string be N and the number of inner nodes of the (Patricia) suffix tree (Gusfield 1997) be k , V is defined as:
Golcher empirically showed how this mea sure converges to almost the same value across Indo-European languages for about 30 megabytes of data. He also showed how the convergent values differ from those calculated for programming language texts. have mathematical grounding and has only been shown empirically. He does not report values for texts larger than about 30 megabytes nor for those of non -Indo-European languages. A simple conjecture on this measure is that because a suffix tree for a string of length N has at most N  X  1 inner nodes, V must end up at some value 0 any given text.
 stancy measure, although we admitted to observing a gradual increase (Kimura and
Tanaka-Ishii 2014). Because V requires further verification on larger-scale data before ruling it out, we include it as a constancy measure candidate. 484 2.2 Measures Based on Power Law Distributions
Since Zipf (1965), power laws have been reported as an underlying statistical character-istic of text. The famous Zipf X  X  law is defined as: where  X   X  1, and f ( n ) is the frequency of the n th most frequent word in a text. Various studies have sought to explain mathematic ally how the exponent could differ depend-ing on the kind of text. To the best of our know ledge, however, there has been a limited number of reports related to text constancy.

Chitashvili attempted to obtain e xplicit mathematical forms for V ( N )and V ( m , N )by more finely considering the long tails of vocabulary distributions for which Zipf X  X  law does not hold. They obtained these forms through a parameter Z , defined as the potential text length minimizing the square error of the estimated V ( m , N ), with its actual value as follows: Thus defining Z , they mathematically deduced for V ( N ) the following formula: Two ways to obtain Z can be formulated through approximation: one through Good-Turing smoothing (Good 1953), which assumes Zipf X  X  law to hold, and the other using
Newton X  X  method. Tweedie and Baayen showed how the value of Z is stable at the size of an English novel by a single author and thus suggested that it could form a text characteristic. The empirical results, however, were not significantly convergent with respect to text size, and, moreover, Tweedie and Baayen provided their results without giving an estimation method (Tweedie and Baayen 1998). Calculation using Good-Turing smoothing, which is derived directly from Zipf X  X  law, would cause Z to converge, but this does not take Orlov X  X  original intention into consideration.
Alternatively, our group (Kimura and Tanaka-Ishii 2014) verified Z through Newton X  X  method by setting g ( Z ) = 0, where g ( Z ) is the following function:
We also showed how the value of Z increases rapidly when the text size is larger than 10 megabytes.
 and tail of the vocabulary population distribu tion. Because these exceptions constitute important parts of the population, parameter estimation by fitting to Equation (3) is sensitive to the estimation method. For example, the estimated value of the exponent for Zipf X  X  law depends on the method used for dealing with these exceptions. We tested several simple methods of estimating the Zipf law X  X  exponent ways of handling the head and tail of a distribution. There were settings that led to convergence, but the convergence depended on the settings. Such difficulty could be one reason why there has been no direct proposal for  X  as a text constancy measure.
Hence, due care must be taken in relating text constancy to a power law. We chose another path by considering text constancy through a random Zipf distribution, as described later in the experimental section. 2.3 Measures Based on Complexity
With respect to measures based on complexity, multiple reports have already examined the Shannon entropy (Shannon 1948; Cover and Thomas 2006). In addition, we intro-duce the R  X  enyi higher-order entropy (R  X  enyi 1960) as another possible measure. 2.3.1 Shannon Entropy Upper Bound. Let X be the random variable of a sequence X
X , X 2 , ... , X i , ... ,where X i represents the i th element of X : X represents a given set (e.g., a set of words or characters) whose members constitute the sequence. Let X j i ( i &lt; j ) denote the random variable indicating its subsequence
X Shannon entropy is then defined as: of the relative frequencies (for P )ofunigrams(for X ), and they concluded that the measure would continue increasing with respect to text size and would not converge for short, literary texts (Tweedie and Baayen 1998). Because we are interested in the measure X  X  behavior on a larger scale, we rep licated their experiment, as discussed later in the section on empirical constancy. We denote this measure as H
Theoretically, the behavior of the entropy rate with respect to text size has been contro-versial. On the one hand, there have been indications of entropy rate constancy (Genzel and Charniak 2002; Levy and Jaeger 2007). These reports argue that the entropy rate of natural language could be constant. Due to the inherent difficulty in obtaining the true value of h  X  from a text, however, these arguments are based only on indirect clues with respect to convergence. On the other hand, Hilberg conjectured a decrease in the human conditional entropy, as follows (Hilberg 1990):
He obtained this through an examination of Shannon X  X  original experimental data and suggested that  X   X  0 . 5. From this formula, De  X bowski inducesthat H ( X the entropy rate can be formulated generally as follows (De  X bowski 2014): 486
Note that at the limit of n  X  X  X  ,thisrategoesto h  X  , a constant, provided that
Hilberg X  X  conjecture is deemed compatible wi th entropy rate constancy at its asymptotic limit, provided that h  X  &gt; 0holds. 1 We are therefore interested in whether this h a text characteristic, and if so, whether h  X  &gt; 0.
 rate. Brown X  X  report (Brown et al. 1992) is representative in showing a good estimation of the entropy rate for English from texts, as compared with values obtained from humans (Cover and King 1978). Subsequently, there have been important studies on calculating the entropy rate, as reported thoroughly in Sch  X  umann and Grassberger (1996). The questions related to h  X  , however, remain unsolved. Recently, De  X bowski used a Lempel-Ziv compressor and examined Hilberg X  X  conjecture for texts by single authors (De  X bowski 2013). He showed an exponential decrease in the entropy rate with respect to text size, supporting the validity of Equation (9). Following these previous works, we examine the entropy rate by using an algorithm proposed by
Grassberger (1989) and later on by Farach et al. (1995). This method is based on universal coding. The algorithm has a theor etical background of convergence to the true h  X  , provided the sequence is stationary, but has been proved by Shields (1992) to be inconsistent X  X hat is, it does not converge to the entropy rate for certain non-
Markovian processes. We still chose to apply this method, because it requires no arbi-trary parameters for calculation and is applicable to large-scale data within a reasonable time.
 rized as follows. Consider a sequence X of length N . The maximum matching length L is defined as: for j  X  X  1, ... , i  X  1 } ,1  X  j  X  j + k  X  i  X  1. In other words, L subsequence before and after i .If  X  L is the average length of L then the method obtains the entropy rate h 1 as
Given the true entropy rate h  X  , convergence has been mathematically proven for a stationary process, such that | h  X   X  h 1 | = O (1) when N this entropy rate h 1 as a constancy measure candidate. 2.3.2 Approximation of R  X  enyi Entropy H  X  . The R  X  enyi entropy is a generalization of the Shannon entropy, defined as follows (R  X  enyi 1960; R  X  enyi 1970; Cover and Thomas 2006; Bromiley, Thacker, and Bouhova-Thacker 2010): where  X   X  0,  X  = 1. H  X  ( X ) represents different ideas of sequence complexity for differ-ent  X  . For example:
The formula for  X  = 0 becomes equivalent to the so-called topological entropy (hence, it is another notion of entropy ) for certain probability fun ctions (Kitchens 1998) (Cover and Thomas 2006). Note that the number of distinct tokens (i.e., the cardinality of a set) has been used widely as a rough approximation of complexity in computational linguistics. Indeed, in Section 2.1.2, we saw how some candidate constancy measures are based on a token-type relation, such that the number of types is related to the complexity of a text. For texts, note also that the value grows with respect to the text size, unless X is considered, for example, in terms of u nigrams of a phonographic alphabet. section. Such difficulty in convergence for these  X  values lies in the nature of linguistic processes, in which the vocabulary set evolves.
 complexity by considering linguistic hapax legomena to a lesser degree, thus giving the possibility of convergence. In fact, an approximation of the probability by the relative frequencies of unigrams at  X  = 2 immediately shows the essential equivalence to Yule X  X 
K ,since K from Equation (1) can be rewritten as follows: where freq ( x ) is the frequency of x  X  X . Therefore, Yule X  X  K has significance within the context of complexity.
 the best of our knowledge. This mathemat ical relation clarifies both why Yule X  X  K should converge and what the convergent value means; specifically, the value represents the gross complexity underlying the language system. As noted earlier, the higher-order entropy considers hapax legomena to a lesser degree and calculates the gross entropy only from the representative vocabulary pop ulation. This simple argument shows that
Yule X  X  K captures not only the simple repetitiveness of vocabulary but also the more profound signification of its equivalence with the approximated second-order entropy.
Because K has been previously reported as a stable text constancy measure, we consider it here once again, but this time within the broader context of H 488 2.4 Summary of Constancy Measure Candidates
Based on the previous reports (Tweedie and Baayen 1998; Kimura and Tanaka-Ishii 2014) and the discussion so far, we consider the following four measures as candidates for text constancy measures.

In addition, we empirically consider ho w these measures can be understood in the context of the power-law feature of language. As noted in the Introduction, for the convergent measures the speed of attaining convergence with respect to text size is examined as well. Among the candidates, K and H 1 have been previously applied in a word-based manner, whereas V is string based. The Shannon entropy rate h considered in both ways. Because we should be able to consider a text in terms of both words and characters, we examine the constancy of each measure in both ways.
H , in the following we only consider H 2 .Asfor H  X  ,weconsider comparison with H 2 .Because H 1 is based on relative frequencies and can be considered together with H 2 , we first focus on the convergence of the three measures V , h and then we consider H 1 in comparison with H 2 , H 3 ,and H 3. Data 3.1 Real Texts
Table 1 lists the data used in our experimental examination. The table indicates the data identifier (by which we refer to the data in the rest of the article), language, source, number of distinct tokens, data length by total number of tokens, and size in bytes. The first block contains relatively large-scale na tural language corpora consisting of texts written by multiple authors, and the second block contains smaller corpora consisting of texts by single authors. The third block con tains programming language corpora, and the fourth block contains corpora of unknown scripts, which we examine at the end of this article in Section 4.3.

Japanese, Chinese, Arabic, and Thai. These languages were chosen to represent different language families and writing systems. The large-scale corpora in English, Japanese, and Chinese consist of newspapers in chronological order, and the Thai and Arabic corpora include other kinds of texts. The m arkers  X  X  X ,  X  X  X , and  X  X r X  appearing at the end of every identifier in Table 1 (e.g., Enews-c, Enews-w, and Jnews-cr) indicate text processed through words, characters, and transliterated Roman characters, respectively. in terms of words, since verification via char acters produced findings consistent with those obtained with the large-scale corpora. The texts were chosen because each was written by a single author but is relatively large. 490
NECTEC corpus, texts were tokenized according to the annotation. The preprocessing methods for the other corpora were as follows: All the other natural language corpora were tokenized simply using spaces. languages, we also collected program sources from different languages (third block in Table 1). The programs were also considered solely in terms of words, not characters.
C++ and Python were chosen to represent different abstraction levels, and Lisp was chosen because of its different ordering for function arguments. Source code was col-lected from language libraries. The programming language texts were preprocessed as follows. Comments in natural language were eliminated (although strings remained in the programs, where each was a literal token). Identical files and copies of sources in large chunks were carefully eliminated, although this process did not completely eliminate redundancy since most programs reuse some previous code. Finally, the programs were tokenized according to the language specifications. scripts at the end of this article in Section 4.3, through Figure 5, to show one possible application of the text constancy measures. The first unknown script is that of the
Voynich manuscript, a famous text that is undeciphered but hypothesized to have been written in natural language. This corpus is considered in terms of both characters and words, where words were defined via the white space separation in the original text.
Given the common understanding that the manuscript seems to have two different parts (Reddy and Knight 2011), we separated it into two parts according to the Currier annotation (identified as A and B, respectively). The second corpus of unknown text consists of the Rongorongo script of Easter Island (Daniels and Bright 1996, Section 13;
Orliac 2005; Barthel 2013). This script X  X  status as natural language is debatable, but if so, it is considered to possess characteristics of both phonographs and ideograms (Pozdniakov and Pozdniakov 2007). Because there are several ways to consider what constitutes a character in this script (Barthel 2013), we calculate values for the two most extreme cases as follows. For corpus RongoA-c, we consider a character inclusive of all adjoining parts (i.e., including accents and ornamental parts). On the other hand, for corpus RongoB-c, we separate parts as reasonably as possible, among multiple possible separation methods. Because the unit of word in this script is unknown, the Rongorongo script is only considered in terms of characters. 3.2 Random Data
The empirical verification of convergence for real data is controversial. We must first note that it does not conform with the stand ard approach to statistical testing. In the domain of statistics, it is a common understanding that  X  X onvergence X  cannot be tested.
A statistical test raises two contrasting hy potheses X  X alled the null and alternative hypotheses X  X nd calculates a p-value indica ting the probability of the null hypothesis to occur. When this p-value is smaller than a certain value, the null hypothesis is considered unable to occur and is thus rejected. For convergence, the null hypothesis corresponds to  X  X ot converging, X  and the alt ernative hypothesis, to  X  X onverging. X  The problem here is that the null hypothesis is al ways related to the alternative hypothesis to a certain extent, because the difference between convergence and non-convergence is merely a matter of degree. In other words, the notion of convergence for a constancy measure does not conform with the philosoph y of statistical testing. Convergence is therefore considered in terms of the distance from convergent values, or in terms of the error with respect to some parameter (such as data size). Such a distance cannot be calculated for real data, however, since the underlying mathematical model is unknown. other means. Our proposal is to consider convergence in comparison to a set of random data whose process is known. For this random data, we considered two kinds. data was generated from real data by shuffling the original text with respect to certain linguistic units. Tweedie and Baayen (1998) presented results by shuffling words, where the original texts were literary texts by si ngle authors. Here, we generated random data by shuffling (1) words/characters, (2) sentences, or (3) documents. Because these options greatly increased the number of combinations of results, we mainly present the
Convergence must be verified especially at la rge scale; the most important convergence findings for randomized small-scale data were already reported in Tweedie and Baayen (1998); and the results for options (2) and (3) were situated within the range of option (1) and the original texts.
 linguistic characteristics, such as n -grams and long-range correlation. The convergence properties of the three measures V , h 1 ,and H 2 are as follows. The convergence of V is unknown, because it lacks a mathematical background. Even if the value of V did converge, the convergent value for randomized data would differ from that of the original text, since the measure is based on repeated n -grams in the text. h the entropy rate of the randomized text, if th edatasizesuffices.Thisissupportedbythe mathematical background of the algorithm, wh ich converges to the true entropy rate for stationary data. Even when h 1 converges for random data, the convergent value will be larger than that of the original text, because h 1 considers the probabilities of n -grams.
Lastly, H 2 converges to the same point for a randomized text and the original text, because it is the approximated higher-order entropy, such that words and characters are considered to occur independently.
 different texts for a constancy measure, as considered in Section 4.2. Random corpora 492 were generated according to four different distributions: one uniform, and the other three following Zipf distributions with exponents of  X  = 0.8, 1.0, and 1.3, respectively, for Equation (3). Because each set of real data consists of different numbers of distinct tokens, ranging from tens to billions, random data sets consisting of 2 for every n = 4 ... 19, were randomly generated for ea ch of the four distributions. We only consider the measures H 2 and H 0 for these data sets. Both of these measures have convergent values, given a sufficient data size. 4. Experimental Results
From the previous discussion, we applied the three measures V , h large-scale and eight small-scale natural language corpora, three programming lan-guage corpora, and two unknown script corpora, in terms of words and characters.
Because there were many results for different combinations of measure, data, and token (word or character), this section is structured so that it best highlights our findings. 4.1 Empirical Constancy
Figures 1, 2, and 3 in this section can be examined in the following manner. The hori-zontal axis indicates the text size of each corpus, in terms of the number of tokens, on a log scale. Chunks of different text sizes were always taken from the head of the corpus.
The vertical axis indicates the values of the different measures: V , h contains multiple lines, each correspondi ng to a corpus, as indicated in the legends. measures for words (left three graphs) and characters (right three graphs). We can see that V increased for both words and characters (top two graphs). Golcher tested his measure on up to 30 megabytes of text in terms of characters (Golcher 2007). We also observed a stable tendency up to around 10 7 characters. The increase in V became apparent, however, for larger text sizes. Thus, it is difficult to consider V as a constancy measure.
 tendency was clearer for words than for characters. For some corpora, especially for characters, it was possible to obser ve some values converging towards h tendency, however, could not be concluded as converging. This result suggests the difficulty in attaining convergence of the entropy rate, even with gigabyte-scale data.
From the theoretical background of the Gra ssberger algorithm, the values would pos-sibly converge with larger-scale data. The continued decrease could be due to multiple reasons, including the possibility of requiring far larger data than that used here, or a discrepancy between linguistic processes and the mathematical model assumed for the Grassberger algorithm.
 all of the estimated values were larger than zero, but many of the results could not be fitted easily, and the estimated values were unstable due to fluctuation of the lines.
Whether a value for h  X  is reached asymptotically and also whether h tant questions requiring separate, more ext ensive mathematical and empirical studies. level of 10 5 tokens, for both words and characters. From the previous verification of
Yule X  X  K , we can conclude that H 2 is convergent. The final convergent values, however, differed for the various writing systems. We return to this issue in the next section. responding randomized data. As mentioned in Section 3.2, the original texts were 494 randomized by shuffling words and characters for the data examined by words and characters, respectively. Therefore, all n -gram characteristics existing in the text were destroyed, and what remained were the different words and characters appearing in a random order. Here, we see how the random data X  X  behavior has some of the theoretical properties of convergence, as summarized in Section 3.2.
 even for uniform random data is unknown, and even if it converged, the convergent value would be smaller than that of the or iginal text. The top two graphs in Figure 2 exhibit some oscillation, especially for randomized Chinese ( oscillation was already reported by Golc her himself (Golcher 2007) for uniformly random data. This was easy to replicate, as reported in Kimura and Tanaka-Ishii (2014), for uniformly random data with the number of distinct tokens up to a hundred.
Because the word distribution almost follows Zipf X  X  law, the vocabulary is not uniformly distributed, yet oscillating results occur for some randomized data in the top left figure. Moreover, the values seem to increase for Japanese and English for words at a larger scale. Although the plots for some scripts seem convergent (top right graph), these convergent values are theoretically di fferent from those of the original texts, if they exist, and this stability is not universal across the different data sets. Given this result, it is doubtful that V is convergent across languages.
 randomized data, but to larger values than those of the original texts, as mentioned in
Section 3.2. The middle two graphs of Figure 2 show the results for h the plots do not reach convergence even at the largest data sizes, but for certain results with characters, especially in the Roman alphabet, the plots seem to go to a convergent value (middle right). All the plots can be extrapolated to converge to a certain entropy rate above zero, although these values are larger than the convergent values X  X f they ever exist X  X f the real data. These results confirm the difficulty of judging whether the entropy rates of the original texts are convergent and whether they remain above zero. graphs), and the convergent values are the same for the cases with and without ran-domization. In fact, the plots converge to exactly the same points faster and more stably, which shows the effect of randomization.
 findings X  X oth the tendencies of the lines and the changes in the values X  X an be situated in the middle of what we have seen so far. The plots should increasingly fluctuate more like the real data because of the incomplete randomization, in the order of sentences and then documents.
 in terms of words for the small-scale corpora (left column) and for the programming language texts (right column). For the sma ll-scale corpora, in general, the plots are bunched together, and the results shared the tendencies noted previously for the large-scale corpora. V again showed an increase, while h 1 showed a tendency to decrease. H converged rapidly and was already almost stable at 10 4 tokens. This again shows how
H exhibits stable constancy, especia lly with texts written by single authors. natural language texts because of the redundancy within the program sources. Still, the global tendencies noted so far were just discernible. V had relatively larger values but h 1 and H 2 had smaller values for programs, as compared to the natural language texts. The differences in value indicate the larger degree of repetitiveness in programs. in unigrams (Enews-w). The horizontal axis indicates the corpus size, and the vertical axis indicates the approximated entropy value. The different lines represent the results for H  X  with  X  = 1, 2, 3, 4. The two H 1 plots represent calculations with and without Laplace smoothing (Manning and Schuetze 1999). We can see that without smoothing,
H increased, as Tweedie and Baayen (1998) reported, but in contrast to their conclusion, we observe a tendency of convergence for larger-scale data. The increase was due to the influence of low-frequency vocabulary pushing up the entropy. The opposite tendency to decrease was observed for the smoothe d probabilities, with the plot eventually converging to the same point as that for the unsmoothed H 496 was by far slower for H 1 as compared with that for H 2 , H attained convergence already at 10 2 tokens. The convergence values naturally decreased for larger  X  , although the amount of decrease itself rapidly decreased with larger constancy, with sufficient convergence sp eed X  X he empirical conclusion from our data is that H  X  with  X &gt; 1 showed stable constancy when the values were approximated using relative frequencies. For H 1 , the convergence was much slower because of the strong influence of low-frequency words. Consequently, the constancy of H is attained by representing the gro ss complexity underlying a text. 4.2 Discriminatory Power of H 2
Now we turn to Question 3 raised in the Introduction and examine the discriminatory power of H 2 . As Yule intended, does H 2 identify authors? Given the influence of differ-ent writing systems, as seen previously in Figure 1, we examine the relation between H and the number of distinct tokens (the alphabet/vocabulary size). Note that because this number corresponds to H 0 in Equation (11), this analysis effectively considers texts on the H 0 -H 2 plane. Since H 0 grows according to the text size, unlike H size must be used for all corpora in order to meaningfully compare H that H 2 converges fast, we chose a size of 10 4 tokens to handle all of the small-and large-scale corpora.
 explained at the end of Section 3.2, Figure 5 plots the values of H the number of distinct tokens H 0 (horizontal) measured for each corpus at a size of 10 tokens. The three large circles are groupings of points. The leftmost group represents news sources in alphabetic characters. All of the romanized Chinese, Japanese, and
Arabic texts are located almost at the same vertical location as the English text. This indicates the difficulty for H 2 to distinguish natural languages if measured in terms of alphabetic characters. The middle group represents the programming language texts in terms of words. This group is located separately (vertically lower than the natural language corpora in terms of words), so H 2 is likely to distinguish between natural languages and programming languages. The rightmost group represents the small-scale corpora. Considering the proximity of these points despite the variety of the content, it is unlikely that H 2 can distinguish authors, in contrast to Yule X  X  hope. Still, these points are located lower than those for news text. Therefore, H 2 genre or maybe writing style. 498 those of the non-alphabetic writing systems. 10 Note that Chinese characters have morphological features, and the Arabic and Thai languages also have flexibility in terms of which units are considered words and morphemes. In other words, the plots closer to the random data with a smaller Zipf exponent are for language corpora of morphemic sequences. The group of plots measured for phonographic scripts is located near the line for a Zipf exponent of 1.0 (the gr ouping of points in the leftmost circle), which could suggest that morphemes are more randomized units than words. 4.3 Application to Unknown Scripts: Voynich Manuscript and Rongorongo Script
The nature of unknown scripts can also be considered through our understanding thus far. Figure 5 includes plots for the Voynich man uscript in terms of words and characters, and for the Rongorongo script in terms of characters. Like all the data seen in this figure, the points are placed at the H 2 values (vertically) for the number of distinct tokens (horizontally) at the specified size of 10 4 tokens, with the exception of Voynich-A in terms of words. Because this corpus consists of fewer than 10 length by tokens listed for VoynichA-w in Ta ble 1), its point is located horizontally at the vocabulary size corresponding to the corpus X  maximum size.

Arabic corpus for words (Abook-w). For characters, on the other hand, the plots are at the leftmost end of the figure. This was due to overestimation of the total number of characters for the alphabetic texts (e.g. , both English and other, romanized language texts), since all ASCII characters, such as colons, periods, and question marks, are counted. Still, the H 2 values are located almost at the same position as for the other romanized texts, indicating that the Voinich manuscript has approximately similar com-plexity. These results suggest the possib ility that the Voynich manuscript could have been generated from a source in natural language, possibly written in some script of the abjad type. This supports previous findings (Reddy and Knight 2011; Montemurro and Zanette 2013), which reported the possib ility of the Voynich manuscript being in a natural language and the coincidence of its word length distribution with that of Arabic.
Zipf exponent of 0.8, with RongoA near Arabic in terms of words but RongoB somewhat further down from Japanese in terms of characters. The status of Rongorongo as natural language has been controversial (Pozdniakov and Pozdniakov 2007). Both points in the graph, however, are near many other natural language texts (and not widely separated), making it reasonable to hypothesize that Rongorongo is indeed natural language. The characters can be deemed morphologically ri ch, because both plots are close to the line for a Zipf exponent of 0.8. In the case of RongoA, for which a character was considered inclusive of all parts (i.e., including accents and ornamental parts), the morphological richness is comparable to that of the wor ds of an abjad script. On the other hand, when considering the different character parts as distinct (RongoB), the location drifts towards the plot for Thai, a phonographic script, in terms of characters. Therefore, the
Rongorongo script could be considered basic ally morphemic, with some parts function-ing phonographically. This conclusion aga in supports a previous hypothesis proposed by a domain specialist (Pozdniakov and Pozdniakov 2007).
 however, only add a small bit of evidence to those conjectures; clearly, reaching a reasonable conclusion would require further study. Moreover, the analysis of unknown scripts introduced here could provide ano ther possible application of text constancy measures, from a broader context. 5. Conclusion
We have discussed text constancy measures, whose values are invariant across different sizes of text, for a given text. Such measures h ave a 70-year history, since Yule originally proposed K as a text characteristic, potentially with language engineering utility for problems such as author identification. We consider text constancy measures today to have scientific importance in understanding language universals from a computational view.
 stancy, we explained how K essentially has a mathematical equivalence to the R  X  enyi higher-order entropy. We then empirically examined various measures across different languages and kinds of corpora. Our results showed that only the a pproximated higher-order R  X  enyi entropy exhibits stable, rapid constancy. Examining the nature of the con-vergent values revealed that K does not possess the discriminatory power of author identification as Yule had hoped. We also applied our understanding to two unknown scripts, the Voynich manuscript and Rongorongo, and showed how our constancy results support previous hypotheses about each of these scripts.
 language. There, too, we will consider the questions raised in the Introduction, of 500 whether K converges and of how discriminatory i t is. We are especially interested in considering the relation between the value of K and the meaningfulness of data. Acknowledgments References
