 SPECIAL ISSUE PAPER Muhammad Fraz  X  M. Saquib Sarfraz  X  Eran A. Edirisinghe Abstract This paper presents an approach for text detec-tion and recognition in scene images. The main contribution of this paper is to demonstrate that the colour information within the images if efficiently exploited is good enough to identify text regions from the surrounding noise. In the same way, the colour information present in character and word images can be used to achieve significant performance improvement in the recognition of characters and words. The proposed pipeline makes use of the colour information and low-level image processing operations to enhance text infor-mation that improves the overall performance of text detec-tion and recognition in the wild. The proposed method offers two main advantages. First, it enhances the text regions up to a level of clarity where a simple off-the-shelf feature repre-sentation and classification method achieves state-of-the-art recognition performance. Second, the proposed framework is computationally fast as compared to other text detection and recognition techniques that offer good accuracy at the cost of significantly high latency. We performed extensive experi-mentation to evaluate our method on challenging benchmark datasets (Chars74K, ICDAR03, ICDAR11 and SVT), and the results show a considerable performance improvement. Keywords Character recognition  X  Word recognition  X  Colour constancy  X  Bilateral regression  X  Histogram of oriented gradients  X  Support vector machine 1 Introduction Text detection and recognition are two major but interlinked problems with numerous applications [ 1  X  4 ]. Text serves as an important parameter for image and video analysis. It is used for indexing and understanding of images and videos for text-based search. A number of methods (see Sect. 2 ) have been recently proposed that explore various theoretical and practical aspects of the field to solve the problems of text detection and recognition. In this work, we paid significant attention to both problems individually in a framework that detects and recognizes the text in outdoor images. The pro-posed framework makes efficient use of colour information to detect text regions in the images. In word recognition sce-nario, it accurately segments and recognizes the characters to understand the word present in the image.

Text carries an important characteristic, that is, its colour in comparison with its background. It stands out to its back-ground to an extent where humans can identify it. Another important trait of the text is that the characters in a word image usually possess similar colour which helps the reader to identify the alphabets associated with a particular word. We intelligently exploit these characteristics of the text to solve the problem of text detection and recognition in the wild. First, we perform text detection that involves the local-ization of words in the images. Next, we identify and recog-nize characters in the words. Finally, we understand the words by verifying the combination of recognized characters against a lexicon to remove errors that occur in the character recognition stage.
For the task of text detection, we propose a novel con-nectedcomponents (CC)-basedstrategythat usecolour infor-mation present in the image to extract candidate regions. All the existing CC-based techniques [ 5  X  12 ] rely on the edge or gradient information in the image to extract candidate regions. These techniques fail in scenarios where image is blurred, text regions have low contrast with their surround-ings, and the intensity information is non-uniform because of surrounding light. To deal with these problems for candidate regions extraction, we pre-process the input image using a colour constancy technique. The purpose of colour constancy is to reduce the effect of illumination and surroundings reflec-tions. The colour-corrected image is then passed through a noise reduction stage where the colour information in small areas is enhanced while maintaining a sharp boundary between the regions of different colour. This is done in such a way that all the pixels in significant colour regions maintain the same colour. The enhanced image is fed into a colour quantization stage, and the CC in the binary map of each quantization level are used to finalize the candidate regions. The candidate regions are classified into text and non-text regions using shape features and pre-trained classifier.
For the task of text recognition, we combine the region grouping method with the object recognition strategy to achieve the advantages of both techniques. First, we binarize the word image using colour information and perform fore-ground segmentation using the proposed modified bilateral regression to separate characters from the background. After that, we extract shape features on binary images of characters and perform classification using a pre-trained classifier. The combinations of recognized characters are fed into a string similarity matching stage where lexicon-based search is per-formed to find the closest matching word.

The proposed character recognition pipeline outperforms the current state-of-the-art methods by a significant mar-gin on Chars74k [ 13 ] and ICDAR03-CH [ 14 ] benchmark datasets. Further to that the proposed word recognition pipeline achieved high recognition accuracy on challenging ICDAR03-Word [ 14 ] and SVT [ 15 ] benchmark datasets.
The rest of this paper are organized as follows: in Sect. 2 , we give a detailed overview of the recent work for text detec-tion, character and word recognition. The proposed frame-work is explained in detail in Sect. 3 . The experimental eval-uation of the proposed method and the discussion about the results is presented in Sect. 4 followed by the conclusion in Sect. 5 . 2 Related work 2.1 Text detection A significant volume of literature exists on detection of text from images and videos. Existing text detection approaches canbeclassifiedintothreebroadcategories:(1)texture-based approaches, (2) connected component-based approaches and (3) object detection-based methods. This section dis-cusses some well-known techniques belonging to these categories.

In texture-based techniques, Jain and Zhong [ 16 ]usedthe distinguishing texture present in text to determine and sep-arate text, graphics and half-tone image regions in scanned greyscale document images. Further in [ 17 ], they utilized the texture characteristics of text lines to extract text in greyscale images. Wu et al. [ 18 , 19 ] proposed the method to segment the input images using a multi-scale texture segmentation scheme and extract potential text regions using nine second-order Gaussian derivatives. Sin et al. [ 20 ] used frequency fea-tures such as the number of edge pixels in the horizontal and vertical directions and fourier spectrum to detect text regions in real scene images. Mao et al. [ 21 ] proposed a texture-based text localization method using wavelet transform. Lim et al. [ 22 ] make a simple assumption that the text regions usually have higher intensity as compared to the background. The pixels that are lighter than a pre-defined threshold and that exhibit a significant colour difference relative to their neigh-bourhood are counted, and the region that possesses a large number of such pixels is classified as a text region. Lee et al. [ 23 ] proposed the use of support vector machine (SVM) and spatio-temporal restoration for text detection in videos. Chen and Yuille [ 24 ] employ the AdaBoost algorithm and the joint probabilities of the features ( X and Y derivatives, histogram of intensity and edge linking) to detect text in natural scenes for visually impaired people. Ye et al. [ 25 ] use multi-scale wavelet features to locate text line in the presence of complex background.

The advantage of texture-based schemes is their good per-formance in the presence of noise and their ability to cope with illumination inconsistencies in the image. However, a big disadvantage of texture-based detection methods is their computational complexity. This disadvantage makes these techniques less suitable for real-time applications and prac-tical systems.

In CC-based techniques, Ezaki et al. [ 7 ] combined edge image, reverse edge image and colour-based analysis for CCs extraction. The top scoring contestant in ICDAR05 [ 8 ] challenge applied an adaptive binarization method to find CCs. Shivakumara et al. [ 10 ]used k -means clustering in the Fourier X  X aplacian domain for CC extraction. Epshtein et al. [ 9 ] proposed a novel image operator named as stroke width transform (SWT) that computes the value of stroke width for each image pixel. They demonstrated its use for the problem of text detection in natural scene images. The proposed oper-ator showed promising results and acquired significant atten-tion from research community mainly because of its simplic-ity and robustness to detect text in many fonts and languages. However, the SWT method relies on canny edge detection and performs poorly in images with high noise and illumina-tion variation. Yao et al. [ 26 ] proposed a two-level classifi-cation scheme and two sets of features (component level and chain level) for capturing both intrinsic characteristics of the text regions. The proposed framework used SWT for finding the potential candidate regions in the images. Mosleh et al. [ 27 ] proposed a novel bandlet-based edge detector to enhance the accuracy of SWT that originally uses canny edge detector. Neumann and Matas [ 11 , 28 ] propose the use of maximally stable extremal regions (MSERs) to detect potential candi-date regions. They use trained SVM to classify text and non-text regions. Chen et al. [ 12 ] use edge-enhanced MSERs to find letter candidates. In recently proposed methods, Neuman and Matas [ 6 ] used various properties of strokes to localize and recognize characters. Huang et al. [ 5 ] proposed a stroke feature transform filter that extends the capability of SWT by incorporating colour cues of the pixels.

A considerable advantage of CC-based techniques is their significantly better computational performance as compared to texture-based techniques. On the other hand, the disad-vantage of CC-based techniques is their inability to per-form well in the presence of noise. This is due to the fact that these methods use gradient and edge images to extract candidate regions. However, most of the modern text detec-tion approaches use CC-based candidate extraction mainly because of their simplicity. The text detection framework proposed in this paper belongs to the category of CC-based techniques.

Apart from texture and CC-based methods, a new direc-tion has recently emerged that applies the object detection-based techniques to locate text in the images. The text is considered as an object and is located in an object detec-tion framework. Wang et al. [ 15 ] proposed the use of famous multi-scale sliding window-based technique to localize text regions in the image. The technique is adopted in a number of recently proposed methods [ 29  X  31 ] for detection of text in images. 2.2 Character and word recognition A significant volume of literature exists that deals with the problem of character and word recognition in natural scene images. Specialized feature representations, binariza-tion techniques, segmentation methods and word models have been proposed to date, yet the problem of text recog-nition is open, the reason being the diversified nature of the text and the presence of high inter-class similarity as well as the high intra-class variation among characters. In this section, we briefly cover the most recent work for character recognition and word understanding.

In the area of character recognition, Campos et al. [ 13 ] introduced Chars74k dataset of characters collected from natural scene images. They showed that commercial OCR engines do not achieve good accuracy for natural scene character images; therefore, they proposed a multiple ker-nel learning-based method. Wang et al. [ 32 ] propose the use of histogram of oriented gradients (HOG) [ 33 ] together with nearest neighbour classifier and show the improved charac-ter recognition performance. They enhance their work in [ 15 ] where they use a Bayesian inference-based method and show a considerable performance improvement on ICDAR03-CH dataset. Sheshdari et al. [ 34 ] use HOG features with exem-plar SVMs and affine warping to demonstrate improvement in character recognition performance. Yi et al. [ 35 ] present a comparative study about the performance of local and global HOG features for character recognition. A few word recognition and end-to-end scene text recognition methods [ 29 , 36 ] also report character recognition scores separately. The most recent work in character recognition is presented by Lee et al. [ 36 ]. They use discriminative region-based feature pooling to learn the most informative sub-regions of each character within a multi-class classification framework. They have reported the state-of-the-art recognition performance for scene character classification.

In the area of word recognition, a number of approaches have been emerged recently that focus on specialized mod-ules for word recognition. For instance, Smith et al. [ 37 ] proposed a similarity expert algorithm to remove the logical inconsistencies in an equivalence graph and perform search for the maximum-likelihood interpretation of a sign as an integer programming. The work in [ 29  X  31 ] builds condi-tional random field (CRF) models on the potential charac-ter locations in a sliding window-based search and adds the linguistic knowledge and spatial constraints to compute pair-wise costs for word recognition. The work in [ 15 , 32 ]uses pictorial structures to detect words in the image. The picto-rial structures find an optimal configuration of a particular word using the scores and locations of the detected charac-ters. Weinmann et al. [ 38 ] formulate Markovian model score on the segmented candidates on the basis of appearance, geo-metric and linguistic characteristics. Neuman et al. [ 28 ]pro-posed a text recognition system where they extract potential candidate characters using a set of extremal regions (ERs) and then perform exhaustive search with feedback loops to group ERs into words and recognize them in an OCR stage that is trained on synthetic fonts. Bissacco et al. [ 39 ]pro-posed an end-to-end system for text detection and recogni-tion in outdoor images. They presented anisotropic Gaussian filter-based detector and combined it with two other detectors [ 40 , 41 ] to locate text in the images. A deep neural network is trained on raw pixels and HOG features to perform charac-ter classification. As a language model, they use a two-stage standard n-gram approach. Recently, Yao et al. [ 42 ] proposed a new feature representation technique named as  X  X trokelets X  that captures the essential substructures of characters at different granularities.
One important challenge in word recognition frameworks is the identification of characters in cropped word images. A number of approaches are available in the literature to deal with this challenge. The proposed methods involve CCs [ 43 ], image binarization [ 38 , 44 ], extremal regions [ 28 ], graph cuts [ 45 ], sliding windows [ 15 ] and k -means clustering [ 46 ]. Most recently, Field et al. [ 47 ] proposed bilateral regression for character identification. It uses colour clustering as a start-ing point to fit a regression model for each image and sepa-rate foreground pixels from background ones using an error threshold. The method reports a superior character segmen-tation performance in comparison with other techniques. We adopt this method with a proposed modification to achieve better character identification performance. 3 Proposed framework 3.1 Text detection The important stages of proposed text detection framework are shown in Fig. 1 . All the stages are explained separately as a subsection below.

Colour-based connected components generation The colour-based CCs generation initiates with an enhancement stage that improves the colour in the input image. Since we are aiming towards the extraction of candidate regions using colour difference of text from its surrounding, therefore, it is important to eliminate the effect of surrounding illumi-nation. The colour enhancement aims at reducing the effect of illumination and surrounding reflection using colour con-stancy mechanism. In [ 48 ], the authors compared a number of well-known and widely used colour constancy algorithms that are based on colour image statistics and showed on large datasets of both synthetic and real images that the best and the worst algorithms do not exist. This leads us to select a technique that offers less computational complexity. Based on this criterion, we use grey-world (GW) [ 49 ] algorithm for illuminant estimation as it offers the fastest computation of surrounding illumination within an image. A comparison of the average computational time of various colour constancy techniques is given in Table 1 .
 Noise reduction After colour constancy, the output image is passed through a noise reduction stage. This stage aims at normalizing the pixels of similar colour while maintaining a sufficient contrast between background and foreground pix-els by preserving the information at edges. The noise reduc-tion is important as it reduces the inconsistencies between the pixels of the similar colour and resulting in compact pixel clusters at colour quantization stage.

The most commonly used averaging filter works well for image smoothing but it does not preserves edges. On the other hand, the specialized edge-preserving image smoothing tech-niques(bilateralfiltering,guidedfilteringandanisotropicdif-fusion) are computationally expensive. For instance, guided filter is the fastest among these techniques, yet it takes an average 2.75s to process an image of 800-by-1,200 pixels. This results in a considerably slow detection process. To find the middle ground between two requirements (edge preserv-ing and fast computation), we turned towards median filter as it offers the edge-preserving capability for small and medium level of Gaussian noise and is computationally faster than specialized edge-preserving methods.
 Colour clustering In order to acquire the clusters of pixels having similar colour, the enhanced image is fed into a colour clustering stage where the full range of colours is reduced to  X  N  X . For the purpose of clustering, the most commonly used k -means technique has several disadvantages in this scenario. First, it is computationally expensive and requires a fairly long convergence time particularly in images where the pixel values are distributed in non-globular manner. Second, the k -means algorithm randomly selects the initial partitions that may result in different final clusters on the same data in various iterations. On the other hand, the uniform quanti-zation offers extremely fast reduction of colour levels in the image, but it is not suitable here because of its non-dynamic nature.

Heckbert [ 50 ] proposed an efficient and extremely fast nonlinear quantization technique named as minimum-variance quantization. The minimum-variance quantization cuts the colour cube in red, green and blue directions until a pre-specified number of non-empty regions are obtained; it then uses the average colour in each region to create the new reduced palette in an iterative manner. The pixels are clus-tered together on the basis of the variance between their val-ues. For example, a set of red pixels may be grouped together because they possess small variance from the centre pixel of the group. We use minimum-variance quantization due to its dynamic capability and extremely fast computation per-formance as compared to k -means clustering. The quantiza-tion process merges all the noisy areas together at different quantization levels while separately clusters the pixels of text regions from surrounding background pixels. The process is fast and accumulates similar coloured regions with good accuracy even in the presence of noise.

The binary map of each quantization level is computed by assigning the value  X 1 X  to the pixels that fall into that quanti-zation level while setting the values of the remaining pixels to  X 0 X . We get  X  N  X  binary maps for  X  N  X  quantization levels. The value of N is important as it has the direct impact on the quality of candidate regions. A small value of  X  N  X  X ay result in the text regions to merge with background pixels, whereas a considerably large value of  X  N  X  may result in the text strokes to break into different quantization levels. We empirically chose the value ( N = 8) in our experiments. Figure 2 shows the output of the quantization and binariza-tion stage on an image for  X  N = 8 X . Note, the text is sepa-rated from background in a single binary maps (see Fig. 2 e), while the background pixels are separately clustered in other binary maps. The background regions are eliminated in the classification stage explained below.

Extraction of candidate regions The CCs in the binary map of each quantization level are extracted. These CCs are passed through an initial screening where the regions are analysed on the basis of geometric properties (width, height, aspect ratio). The regions that do not fulfil the geometric thresholds are eliminated. The empirically computed upper thresholds th u 1 , th u 2 and lower threshold th l can be mathematically expressed as follows: th u 1 = 2 3  X  im h , th u 2 = 2 3  X  th width, respectively, of the input image in terms of pixels. The coordinates of the bounding boxes of the remaining CCs are used to crop respective regions from the original (greyscale) image as the candidate regions. These candidate regions are fed into the feature extraction and classification stage where the regions are classified into text and non-text regions.
Feature extraction We resize each candidate window to n -by-n pixelspriortotheextractionoffeatures.Thisisimpor-tant because the classifier has been trained using the same-sized training examples. We empirically chose the value ( N = 32) in our experiments as it provides a reasonable middle point between the large and the small candidate win-dows. Next, we compute HOG feature descriptor for each candidate image. Each 32-by-32 image is divided into 16 sub-blocks of 8-by-8 pixels, and HOG is computed on each sub-block. Finally, the histograms from all the sub-blocks are concatenated to form a single HOG vector. The images are passed through a difference of Gaussians (DoG) filter prior to HOG computation to enhance edges and other details that may be affected due to resizing operation.

Classification We train an SVM classifier to discriminate text windows from non-text windows. We extract training images from ICDAR11 dataset using the colour-based processing and candidate region extraction scheme explained in the pre-ceding paragraphs. A total of 10,000 images were extracted for training (5,000 positive and 5,000 negative). We use non-linear SVM classifier with radial basis function (RBF) ker-nel. The soft margin parameter  X  C  X  and  X  for SVM model is computed through fivefold cross-validation on the train-ing data that yielded C = 18 . 3401,  X  = 0 . 5587 in our experiments.

Word formation The final step in the detection process is the formation of words from detected text windows. In most images, the characters are detected separately because each character forms a single connected component and is sepa-rately extracted as a text region. The word formation step is important for performance comparison on benchmark datasets as the ground truths in these datasets are available in the form of word windows. The detected text regions are analysed using colour and location cues. The text present in a linear alignment is expected to have similarities in terms of colour and size. These minor cues serve as a valuable refer-ence to combine letters into chains.

All the detected text regions are analysed with respect to their adjacent text regions. If two nearby regions possess similarities in colour and size, then the bounding boxes of these text regions are merged into a single bounding box. The process is repeated in an iterative manner until all the similar regions are merged into single bounding box. How-ever, there is a chance that two adjacent words merge into a single word. To deal with this, a post-processing step is added where the words are separated from each other on the basis of horizontal distance. The geometric distance between the letters of each word is computed using horizontal projection of that bounding box. If the distance between letters is found to be greater than a particular threshold, then it is an indi-cation for breaking the chain into two words and so on. The threshold for separating the letters is empirically computed using the window sizes. The threshold varies for different sentences and images. Figure 3 shows the detected regions before and after the word formation stage. 3.2 Character identification The text recognition stage starts with character identifica-tion. Here, we are dealing with the recognition of text in cropped words. The reason for involving this step is to enable the system to work as a stand-alone word recognition sys-tem. The major stages of word recognition framework are showninFig. 4 . The key requirement for character identifi-cation framework is the accurate segment of characters from background in such a fine way that even the closely located characters remain separated from each other. The gradient or edge-based connected component extraction methods do not perform satisfactorily for character identification. Most of the recent methods use a sliding window-based approach for character identification. This, however, generates a large number of candidate regions that require large number of evaluations. Following [ 47 ], we use the bilateral regression to segment the characters. However, our approach is different than the original method in that we only use it to estimate the horizontal location of each character in the word image. The objective here is the estimation of the starting column and width of each character in word image.

The bilateral regression technique models the foreground pixels by using a weighted regression that assigns weight to each pixel according to its location with respect to the fore-ground in the feature space. The pixels of foreground region get higher weights in comparison with the pixels present in background region. In this case, the regression model in Eq. 1 represents the quadratic surface that best models the image as a function of pixels location. z = ax 2 + by 2 + cxy + dx + ey + f (1)
The error between each pixel in the image and the model is computed. The pixels with error value higher than a particular threshold are excluded as background, while the remaining pixels are labelled as foreground pixels.

The bilateral regression models top  X  N  X  colours in each image separately. In a post-processing procedure, it chooses the segmentation that is most likely to contain the fore-groundtextbycomparingtheshapedescriptorsofforeground regions with a training set. This makes the overall process complex and leads to false segmentation results.

We have devised a pre-processing step to enhance the operation of bilateral regression where the foreground colour is estimated a priori. We apply N -level colour quantization to word image and compute binary map of each quantiza-tion level. Here, keeping in view the relatively smaller vari-ance of colours in the cropped word as opposed to the whole scene image, we quantize each word image into three colours ( N = 3). The main motivation is the observation that in the word image the background (having the uniform colour) can be captured in one large colour cluster, while most of the foreground character pixels are captured in another cluster. The small variations typically present along the edges of the characters due to noise and illuminations can be captured in another small cluster. The respective binary map for each quantization level is separately analysed to decide about the foreground binary map.

The binary map that contains the highest number of white pixels along the border is classified as the background binary map and is dropped straight away. From the remaining two binary maps, we check the number of white pixels along the borders as well as the total number of white pixels present in that binary map. The binary map that contains less white pixels is dropped, and the third binary map is characterized as the foreground map. The average colour value of the pixels belonging to the foreground region is then used to perform bilateral regression.

The characters are cropped from the original (coloured) word image using the estimated horizontal location, while the height is kept same as the height of the original word image. In this way, we might get some background infor-mation, but the chances of segmenting only a part of char-acter are reduced. The segmented characters from original word image are fed into the character recognition pipeline explained next. Figure 5 depicts the improvement achieved in character identification using the proposed pre-processing procedure in bilateral regression. 3.3 Character recognition Accurate character recognition is important for accurate word recognition. The word recognition process needs to be robust with regards to the noise, illumination and perspective distortions. Figure 6 shows the proposed character recogni-tion framework. This section describes each step in detail. Foreground segmentation Consider an image containing a character along with background noise. Similar to the charac-ter identification stage, we use colour quantization to separate character pixels from background. We found on the basis of extensive experimentation that for a character image 2-level ( N = 2) colour quantization is appropriate to separate the character pixels from background. Please note that the fore-ground binary mask of characters obtained in the previous stage cannot be used directly for recognition task. Although it gives a good separation in terms of character identification, the character itself may not be well represented because of the missing edge pixels.

We generate two binary images corresponding to two quantization levels by assigning the pixels for each colour cluster a value 1 (white). Similar as previous stage, one of the two binary maps is categorized as foreground character map. This is done by computing the density of white pixels along the borders of each binary map. The binary map of the background tends to have more white pixels along the bor-ders as compared to the binary map of the foreground. The foreground binary map comprises of mostly the pixels asso-ciated with the character region; therefore, it possesses very low density of white pixel around the borders, especially in thecorners.Weexploitthispropertytoclassifythetwobinary maps into foreground and background. A 5-by-5 pixels win-dow from each corners of the binary map is selected, and the total number of white corner pixels is counted in each binary map. The binary map that possesses the higher number of corner white pixels is classified as the background, whereas the other binary map is categorized as the character map. Noise reduction and enhancement The foreground binary map is passed through an enhancement and noise reduc-tion pipeline where the unwanted pixels are eliminated. Mor-phological closing, spur removal and dilation operations are applied to remove noisy pixels and enhancement of character pixels. The CCs are computed in the foreground binary map, and the biggest CC is selected as the character. The binary map of the character is resized to m-by-n pixels and padded with an array of five black pixels on all sides resulting in a binary image of size ( m + 10)-by-( n + 10) pixels with char-acter map perfectly centred in it. The value of  X  m  X  and  X  n  X  is empirically selected as  X 64 X  and  X 48 X , respectively, in our experiments. It is observed that the characters are slightly taller than their width and the selected size approximately maintains the original aspect ratio of the characters. Figure 7 shows a few images where the proposed colour-based bina-rization scheme accurately extracts and enhances the char-acter binary maps.
 Features extraction and classification We use HOG features and multi-class SVM to classify the characters. The shape features are computed directly on the binary maps of the characters extracted in the previous stage. We consider Digits (10 classes) and English letters (52 Classes) i.e. the alphabets  X  =0,...,9;A,...,Z;a,...,zand |  X  |= 62. Hence, a 62-class nonlinear SVM is trained in a one-vs-all manner. The best parameters for training the SVM model have been estimated using RBF kernel and fivefold cross-validation. 3.4 Word recognition The word recognition stage requires the accurate recogni-tion of characters identified in a cropped word image. The inaccuracies in character identification and recognition may easily lead to false alarms. We rely on a simple lexicon-based alignment procedure to remove the errors that occur in char-acter recognition stage. Some errors in character recognition are inevitable because of high inter-class similarity between various characters i.e.  X  X  X ,  X 1 X  and  X  X  X ,  X 0 X ,  X  X  X  and  X  X  X . Alignment with lexicon The character recognition pipeline predicts the character label  X  X  X  on the basis of highest prob-ability estimate. In order to deal with wrong recognitions, we cannot rely on only the first predicted label, especially if the estimated probability for that recognition is significantly low. To deal with this, we select top  X  predicted labels based on a confidence score S c . The confidence score is the sum of probability of top  X  predicted labels. The value of  X  varies in every test case. When S c approaches a threshold  X   X   X , the char-acter labels corresponding to those probability estimates are included in the predicted word combination. We experimen-tally found the value  X  = 0 . 35 for our evaluation process. One potential problem that may occur in this set-up is the formation of a large number of character combinations for certain images. In order to avoid that we limit the total num-ber of words by selecting only 30 words having the highest sum of probability estimates of characters. The procedure is precisely expressed in the algorithm 1 .
Algorithm 1: Computing words using the combination of the characters with high recognition probabilities.
Next, we find the correct word from all the predicted words. This is done by the alignment of predicted words with a pre-stored lexicon using a string similarity measure. The closest matching word in the lexicon is finalized as the word presents in the image. We use Lavenshtein distance [ 51 ] to compare the predicted word with the words in the lexicon. It basically computes the total number of opera-tions (insertion, deletion and replacement) required to align a string with the other. Mathematically, the Lavenshtien dis-tance between two strings s 1 and s 2 can be computed using Eq. 2 . The accuracy of word recognition framework directly depends upon the performance of the character recognition framework. The proposed character recognition framework (in Sect. 3.3 ) works exceptionally well as a result of which the lexicon alignment strategy achieves good word recognition performance. 4 Experiments and results The experimental set-up and results for each stage of the pro-posed framework are discussed in detail in each subsection. 4.1 Text detection The proposed text detection method is evaluated on two benchmark datasets ICDAR11 [ 52 ] and extremely challeng-ing Street View Text (SVT) dataset [ 15 ].
 ICDAR11 dataset The ICDAR11 dataset contains 229 train-ing images and 225 test images with sizes varying between 626-by-179 and 3888-by-2592 pixels. The proposed frame-work achieves a precision of 80.90% which is in par with existing state of the art; however, the recall rate of 63.57% is slightly poor resulting in the F -measure score of 71.26% which is marginally low as compared to the existing state of the art. This is justifiable due to the fact that proposed technique is simple and computationally less expensive as explained in Sect. 4.5 . Table 2 presents the performance com-parison of the proposed method with other techniques on ICDAR11 dataset.
 SVTdataset TheSVTdatasetiscollectedfromGoogleStreet View and comprises of 101 training and 249 test images. The dataset is extremely challenging because the images contain significant background information with similar pattern as text regions. Also, the words are not in pure horizontal ori-entation. Another challenge is that the dataset is not fully annotated. The proposed method has achieved a recall rate of 51.55% and superseded the other techniques by a consid-erable margin. Table 3 presents the performance comparison of proposed method with other techniques on SVT dataset. Note that the recall score is only used to compare the per-formance. This is due to the above-mentioned reason that the SVT dataset is not fully annotated. Hence, the precision measure cannot give a meaningful performance comparison. Performance evaluation To evaluate the performance of the proposed text detection pipeline, we computed the standard performance measures (precision, recall and F -measure). Following [ 53 ], the object-level precision and recall are com-puted first. In the next step, the harmonic mean ( F -measure) is computed by taking the mean value of object measures (precision and recall) over all possible constraint values. The values of the constraints while computing these performance measures have been used the same as given in [ 53 ]. 4.2 Character recognition To evaluate the performance of the proposed character recog-nition method, we use two benchmark datasets for scene character classification task: Chars74K-15 and ICDAR03-CH dataset.
 Chars74K dataset contains 12,505 images of characters extracted from scene images. The images are divided into 62 classes (52 alphabets and 10 number digits). The authors have provided various training and test splits of dataset for the research community to perform fair comparison of results. Chars74K-15 is one subset that contains 930 (15 per class) training images and another 930 (15 per class) test images from Chars74K dataset.
 ICDAR03-CH dataset contains 11,482 images of charac-ters. This does not include non-alphanumeric characters. The dataset comprises of 6,113 training and 5,369 test images.
Table 4 lists the character classification results. The pro-posed framework superseded other techniques by achiev-ing the classification accuracy of 81.34% on ICDAR03-CH dataset and 72.03% on Chars74K-15 dataset. It is evident from results that the non-informative regions around the characters have significant degradation impact on recogni-tion accuracy. Also, the intensity variation within the strokes of characters causes a negative impact on recognition accu-racy. The proposed framework not only removes the noisy background regions to achieve a nicely centred binary map but also attenuates the effect of intensity variation within the strokes of characters. This results in a strongly discriminative shape feature extraction along the contour of the characters.
In order to evaluate the importance of noise reduction and enhancement stage, we perform the experiments where we binarize the character images without applying any noise reduction and enhancement procedure. The remain-ing character recognition pipeline is followed as explained in Sect. 3.3 . Table 5 shows the character recognition perfor-mance with and without applying the proposed noise reduc-tion and enhancement stage. Note the significant perfor-mance degradation in the absence of noise reduction and enhancement stage.

The proposed methods in [ 15 , 29 , 36 ] merge similar char-acter classes from 62 to 49 to reduce the confusion among similar looking characters, for example,  X 0 X ,  X  X  X ,  X  X  X  and  X 1 X , X  X  X , X  X  X . We have also evaluated our proposed character classification approach in 49-classes set-up. Table 6 com-pares the character classification performance in 49-classes set-up where our proposed methodology outperforms the other techniques with a substantial margin, especially on Chars74k-15 dataset. Figure 8 shows the rank-wise scores for top five probabilities of character recognition. The recogni-tion rates for top five candidates approach 94% for 49 classes and 92.5% for all (62) classes on ICDAR03-CH dataset which indicates that the use of top five character recogni-tions shall enhance the word recognition performance. 4.3 Word recognition To evaluate the performance of proposed word recognition frame work, we use ICDAR03-WD, ICDAR11-WD and SVT-WD benchmark datasets. For a fair comparison, we use same training and testing splits and the lexicons as [ 15 ]. A number of different-sized lexicons have been provided in [ 15 ]. We use  X  X ULL X  and  X 50 X  lexicons to evaluate and com-pare the proposed framework.  X  X ULL X  lexicon contains all the words from test set of ICDAR03-WD dataset, whereas in  X 50 X  there are 50 distracting words. Note that we use ICDAR03-CH training set to train the classifier. Following [ 29 , 30 , 36 , 42 , 43 ], we skipped the words with two or fewer charactersaswellasthosewithnon-alphanumericcharacters.
The proposed framework achieves 89.37 and 88.94% wordrecognitionaccuracyonICDAR03-WDandICDAR11-WD, respectively, in small (50 words) lexicon set-up and out-performed existing techniques. The performance in large lex-icon (all words) set-up is 77.47 and 78.58%, respectively, on both datasets which is slightly less than the existing state-of-the-art techniques. The performance degradation indicates the weakness of the word recognition method in the pres-ence of large number of distracting words. The recognition performance on SVT dataset is 78.66% which is third high-est recognition accuracy reported to date. Considering the simplicity of the word recognition pipeline, the performance of the proposed method is promising. Table 7 compares the cropped word recognition performance of all the techniques that use the same test set-up as [ 15 ]. Figure 9 shows a few examples where the proposed word recognition framework successfully recognizes the word in the image. More results for end-to-end system are given in Fig. 11 .

A drawback of the proposed character identification mod-ule is its inability to deal with scenarios where the characters are connected with their neighbourhoods either because of font or because of lighting and viewing angle. Figure 10 shows a few images where proposed character identification technique failed that resulted in wrong word recognition out-put Fig. 11 . 4.4 Evaluation of the modified bilateral regression-based We also evaluate the performance of the proposed modified bilateral regression-based character identification scheme. The experiments were performed on ICDAR03 dataset using both (50 and Full) lexicons. Table 8 shows the word recogni-tion performance in two scenarios: (1) when character iden-tification is performed using only the bilateral regression-based segmentation and (2) when the character identifica-tion is performed using the proposed pre-processing stage prior to bilateral regression-based character identification. The results clearly depict the significance of the proposed pre-processing stage in bilateral regression-based character identification. 4.5 Computational performance All the components of the proposed framework have been implemented in MATLAB. The average execution time of each module on a standard PC is given in Table 9 .Note that the code is unoptimized and the execution time can be reduced further with the inclusion of code optimization and parallel processing techniques. Table 10 compares the aver-age execution time with other techniques (where available). Our proposed text detection is fastest, whereas the text recog-nition is slightly slower than the method proposed in [ 43 ]. The average execution time of the end-to-end system per image is considerably low (3.3s) as compared to [ 6 ] where the reported execution time is 35s. 5 Conclusion We propose an end-to-end scene text recognition system. We show that the efficient exploitation of colour informa-tion within the images achieves promising performance for the challenging task of text detection and recognition. We propose a novel pipeline of colour enhancement operations to improve the separation of text regions from noisy back-ground. For text recognition, we combine region grouping (i.e. binarization) method with object recognition strategy to achieve state-of-the-art results on Chars74k and ICDAR03-CH. We further demonstrate with extensive experimentation that a simple but effective colour exploitation and enhance-ment procedure leads to accurate character identification and recognition. This reduced the need for a complex word model and replaced by a simple string similarity measurement tech-nique that achieved exceptional word recognition perfor-mance on ICDAR03-WD, ICDAR11-WD and SVT bench-mark datasets. Conclusively, we developed a fairly simple, fast and practically exploitable text recognition scheme. References
