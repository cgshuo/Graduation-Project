 Keith Marsolo  X  Srinivasan Parthasarathy Abstract The need to retrieve or classify proteins using structure or sequence-based similarity underlies many biomedical applications. In drug discovery, researchers search for proteins that share specific chemical properties as sources for new treatment. With folding simulations, similar intermediate structures might be indicative of a common folding path-way. Here we present two normalized, stand-alone representations of proteins that enable fast and efficient object retrieval based on sequence or structure. To create our sequence-based representation, we take the profiles returned by the PSI-BLAST alignment algorithm and create a normalized summary using a discrete wavelet transform. For our structural repre-sentation, we transform each 3D structure into a normalized 2D distance matrix and apply a 2D wavelet decomposition to generate our descriptor. We also create a hybrid representation by concatenating together the above descriptors. We evaluate the generality of our models by using them as indices for database retrieval experiments as well as feature vectors for classification. We find that our methods provide excellent performance when compared with the state-of-the-art for each task. Our results show that the sequence-based representation is generally superior to the structure-based representation and that in the classification context, the hybrid strategy affords a significant improvement over sequence or structure. Keywords Bioinformatics  X  Protein indexing  X  Protein retrieval  X  Sequence and structure-based protein representations 1 Introduction Given a protein, the ability to effectively retrieve similar molecules, or to identify the functional group to which it belongs, is needed in many biomedical applications. A key requirement in this process is defining a suitable metric of similarity. Protein molecules are highly complex and a number of different measures can be used to define similarity. One could calculate the geometric similarity between the structure of proteins, for instance, or use a measure based on certain sequence-related properties. In many cases, the application or the available data will dictate the type of similarity required. For instance, if a protein is found to prevent a disease based on a key chemical property, researchers may look for proteins that have a similar sequence in the hope that they can also be used for prevention. On the other hand, if a protein is designed to be part of a drug delivery system, it might also be possible to use proteins with similar structural features to effectively deliver medicinal payloads to sites within the body. In folding simulations, similar structures between proteins could be indicative of a common folding pathway. Structural representations are particularly critical for processing the results of folding simulations, where the sequence of a protein is essen-tially unchanged. In this work, we present two protein representations that can be employed to quickly and efficiently retrieve molecules using similarity based on sequence, structure or both.

Many researchers have looked to create protein models for use in large-scale structure-structures [ 22 ], or as triplets derived from secondary structures [ 6 ]. One drawback to these particular solutions is that they rely on the pair-wise alignment of an entire database, which can be very expensive for large-scale retrieval tasks. Other algorithms are tied to a specific data structure such as a suffix tree or inverted file index [ 2 , 8 ]. Unfortunately, these data struc-tures can be memory-intensive and do not scale well when faced with thousands of proteins. In addition, none of these methods have the capability of using sequence-based similarity measures.

There are also a number of approaches that define similarity between proteins using sequence-based properties. In contrast to structure-based techniques, which typically focus on database retrieval, much of the work using sequence-based information has involved predicting the final structure of an input sequence. Some of the more popular methods in-clude Hidden Markov Models (HMMs) [ 12 ], profile-based alignment algorithms such as PSI-BLAST [ 1 ] support vector machine (SVM) classification [ 9 , 11 , 13 , 21 ].
In the protein domain, SVM-based structure prediction generally falls into two catego-ries. The first consists of those methods that take a protein and compute a representation encompassing a large number of sequence-based properties [ 9 ]. Once the properties have been determined, a feature vector is created by calculating the pair-wise similarity between them. The feature vectors are then used to train the SVM. The second form of SVM struc-ture prediction involves the use of specialized kernel functions [ 11 , 13 , 21 ]. Kernel functions are designed to compute a high-dimensional similarity between objects in the dataset. A hyperplane can be constructed in this high-dimensional feature space, partitioning the object space. While kernel functions can be used on pair-wise feature vectors generated from se-veral different properties (as in the former), they generally rely on just one. These functions have been shown to be quite effective, but they can be computationally expensive. While some users might be willing to trade speed for accuracy, there is another drawback to these kernels in that they are essentially a  X  X lack-box X  solution, leading to a loss in the interpreta-bility of both the results and the decision-making process. Moreover, there is no intermediate representation that one can leverage for other purposes such as retrieval. The closest thing to an independent feature vector is a pair-wise matrix that illustrates the relationship between every object in the dataset. As the size of the dataset increases, any attempts at manipulating this matrix quickly becomes infeasible. In addition, any updates to the source dataset requires the recomputation of this matrix.

Even alignment algorithms such as PSI-BLAST cannot be used to directly compare bet-ween a large number of proteins. PSI-BLAST alignments are based on the overall length of the protein, which make the widespread comparison of results impossible. Thus, it is imperative to have a normalized, stand-alone sequence-based representation. Such feature vectors could be used by alternative classification algorithms or in applications such as nearest-neighbor or range-based similarity search. Of course, it would also be desirable if this representation could be used for tasks such as SVM-based classification while still providing results that are comparable to the state-of-the-art.

In this paper, we present two base representations, one derived from sequence, the other from structure, that are designed to address the challenges listed above. Our sequence-based approach, like many existing solutions, is derived from the results of multiple sequence alignments. Using wavelets, we construct a summary of the alignment profile that results in a compact, stand-alone feature vector that can be employed in many applications. Our structure-based technique, developed previously [ 16 ], results in a compact descriptor that outperforms the state-of-the-art. We begin by computing the pair-wise distance between the amino acid residues of a protein to transform each 3D structure into a 2D distance matrix. We apply a 2D wavelet decomposition on this matrix to generate a set of approximation coefficients, which serve as our feature vector. We also combine the two representations into a hybrid variant when both structural and sequence information is available.

We test the general applicability of our representations by utilizing them as database indices for a series of nearest-neighbor and range-based retrieval experiments and as feature vectors for classification by support vector machine. We find that both our methods outper-form existing approaches, but that our sequence-based representation either outperforms, or is on par with, that of our structure-based version. In our nearest-neighbor retrieval tests, we find that the sequence-based approach is comparable to our structural descriptor but outperforms the current state-of-the-art in structure-based retrieval by 4 X 13 % . In terms of classification accuracy, we find that the sequence and structure-based approaches yield roughly the same performance. In the task of classification, however, we find that the hybrid strategy affords a significant improvement over either of the base versions, from 6to14 % over sequence and structure, respectively, suggesting that both representations can be effectively combined for such tasks. 2 Biological background Proteins are comprised of a varying sequence of 20 different amino acids. Individual amino acids are called residues. Each amino acid type is composed of a number of different atoms, but all contain a central Carbon atom denoted as C  X  . The position of this atom is often used as an abstraction for the position of the entire residue. The sequential connection of the C  X  atoms is called the protein backbone. Amino acids combine to form interacting subunits called secondary structures. Two of the most common secondary structures are  X  -helices and  X  -sheets. The interactions between secondary structures give a protein its overall shape, which is often called the protein X  X  fold . Several structural databases have arisen to group proteins based on their fold. One of the most popular is the Structural Classification of Proteins (SCOP) Database [ 18 ]. Structural classification of proteins arranges proteins into several hierarchical levels. The first four are Class , Fo l d , SuperFamily and Family . Proteins in the same Class share similar secondary structure information, while proteins within the same Fold have simi-lar secondary structures that are arranged in the same topological configuration. Proteins in the same SuperFamily show clear structural homology and proteins belonging to the same Family exhibit a great deal of sequence similarity and are thought to be evolutionarily related. 3 Approach In this section we detail the steps needed to create our wavelet-based representations. We begin with an overview of the wavelet decomposition. We follow with the procedure for com-puting our sequence-based summary and then cover the process used to create our structural descriptor. We conclude with a description of our hybrid protein model. 3.1 Wavelet-based compression The use of wavelets is natural in applications that require a high degree of compression without a corresponding loss of detail, or where the detection of subtle distortions and discontinuities is crucial [ 15 ]. Wavelet decompositions fall into two separate categories: continuous ( cwt ) and discrete ( dwt ). In this work, we deal with the discrete transform.

Given a decomposition level L and a 1-D signal of length N ,where N is divisible by 2 L , the dwt consists of L stages. At each stage in the decomposition, two sets of coefficients are produced, the approximation and the detail . The approximation coefficients are generated by convolving the input signal with a low-pass filter and down-sampling the results by a factor of two. The detail coefficients are similarly generated, convolving the input with a high-pass filter and down-sampling by a factor of two. If the final stage has not been reached, the approximation coefficients are treated as the new input signal and the process is repeated. In many cases, once the wavelet transformation is complete, the original signal will be represented using combinations of approximation and detail coefficients. In our applications, however, we only use the final level of approximation coefficients and ignore the rest .
To operate on a 2-D signal of size N  X  N , the decomposition proceeds as follows: first, the rows are convolved with a low-pass filter and downsampled by a factor of two, resulting in matrix L ( N / 2  X  N ). The process is repeated on the original signal using a high-pass filter, which leads to matrix H ( N / 2  X  N ). The columns of L are convolved two separate times, once with a low-pass filter and again with a high-pass filter. After passing through the filters, the signals are downsampled by a factor of two. This results in a matrix of approximation ( LL ) and horizontal detail coefficients ( LH ), respectively (both of size N / 2  X  N / 2). These steps are executed once more, this time on the columns of H , resulting in a matrix of diagonal ( HH ) and vertical ( HL ) detail coefficients (again of size N / 2  X  N / 2). The whole procedure can then be repeated on the approximation coefficients contained in matrix LL .Asinthe 1-D case, we only deal with the final level of approximation coefficients. 3.2 Sequence-based representation Here we provide an overview of the method used to create our wavelet-based sequence representation. We start with a discussion of PSI-BLAST and follow with the steps used to normalize the results. We conclude with specific details of the wavelet transformation. Generation of PSI-BLAST Profiles PSI-BLAST is one of the more popular methods used to determine the similarity of a protein to a database of sequences. This similarity is returned in the form of a profile, or scoring matrix. In PSI-BLAST, a sequence is tested against a database to identify conserved patterns, or motifs. For each position in each of these conserved regions, the algorithm computes a score for each amino acid type. In highly conserved regions, those amino acids that are highly conserved receive a high positive score, while the others receive high negatives. In weakly conserved regions, residues receive scores near zero. These scores are calculated based on amino acid frequency information. Evolution-based substitution matrices such as BLOSUM [ 10 ] can also be used when calculating the scores. This process can be iterative, running a profile against the database to refine the results. The final output of the algorithm is a profile that includes a position-specific scoring matrix (PSSM) and a position-specific amino acid frequency matrix (PSFM), as well as a sequence of Z -scores or E -values that denote the statistical significance of the alignment. When discussing both the PSFM and PSSM matrices, we will refer to them as the PSI-BLAST profile matrices (PBPM).
 The first step in creating our sequence-based representation involves generating a PSI-BLAST profile for each protein. Using version 2.2.13 of the algorithm, we compute a multi-way alignment against the non-redundant ( nr ) protein database. Downloaded in March 2006, this database contains almost 3.5 million sequences. We run PSI-BLAST for five iterations with the parameter set to 0.001. Since our representation is derived partly from the PSFM, we set the program to output both that and the PSSM, in addition to the standard alignment information. We provide a graphical illustration of the matrices for protein 1CCR in the left-most plots of Fig. 1 . Each row corresponds to an amino acid, while each column represents a position in the query sequence. The PSSM representation is given on the top, while the PSFM version lies on the bottom. These images are false color representations, so lower values have darker colors, while higher values appear lighter. Once the profile has been generated, we create a summary of the frequency information by applying a 1D wavelet decomposition to the PSFM matrix. We create a similar summary of the profile scores from the PSSM matrix. Profile normalization Before we can apply the wavelet decomposition, however, we must normalize the size of the matrix to ensure that we are left with the same number of coefficients for each protein. This will allow us freely compare between the resulting feature vectors. The PSFM and PSSM matrices are of size n  X  20, where n refers to the number of amino acids in the protein sequence and 20 corresponds to one of the different amino acid types. We fix n to be 128 and normalize each PBPM matrix to 128  X  20. The normalization occurs either through interpolation or extrapolation, depending on whether the input protein is shorter or longer than 128 residues, respectively. We choose a value of 128 for our signal length because the wavelet decomposition requires that the total length be divisible by 2 L . Since the optimal value for L is unknown, using a length that is a power of 2 gives us the greatest flexibility in choosing an appropriate decomposition level. Using the next higher power of 2, 256, is also an option, but most of the proteins in our dataset are shorter than 256 residues. We prefer to interpolate and smooth or average the excess points, over extrapolation, where we would be forced to generate additional data. Thus our choice of 128 for our normalized signal length. We provide a more thorough examination of signal length and decomposition level in Sect. 5 .
Once the matrix has been normalized we transpose it (20 rows  X  128 columns) and apply a 4th level Haar 1D decomposition to each row. We only use the final level of approximation coefficients, so each decomposition will produce 1 coefficient for every 16 input values (2 4 ), or 8 coefficients per row/amino acid. This results in feature vectors of size 160. These representations can also be combined together into a single profile summary of 320 attributes. An example of the wavelet-based representations can be seen in the middle of Fig. 1 . Transformation details The wavelet decomposition creates a frequency signature for each amino acid. Using a 4th level Haar decomposition results in 8 coefficients. The Haar wavelet filter is an averaging filter, so each of these coefficients will represent the average frequency value for 12.5% of the sequence. One could vary the decomposition or the normalization factor to change the number of coefficients per amino acid, which would change the representation percentage, but a tradeoff must be made between the total number of coefficients and the overall accuracy of the representation. As shown in Fig. 2 , we varied the decomposition level from 2 to 7 and found that a fourth level decomposition gave us the best overall results.
Since the Haar wavelet filter is orthonormal, each coefficient represents a non-overlapping segment of the protein sequence. If one wanted to create a representation that includes such an overlap, this could be done by manually calculating the average using a sliding window, or through other techniques such as n -grams. If n were set to 16, one would start at position one, and then compute an n -gram for the first 16 points, slide to the right one position and compute a second n -gram. This process would repeat down the profile. Such an approach would increase the total number of coefficients, however. We use wavelets instead of manually computing the summary because of their speed and relative ease of implementation.
Many SVM kernels that are based on the results of a PSI-BLAST alignment focus solely on values derived from the PSSM matrix [ 11 , 13 ], though there are approaches that incorporate both matrices [ 21 ]. Our own testing on the matter has been inconclusive. As we show in Sect. 5 , it is not entirely clear whether either representation is preferable over the other, though in most cases, they can be combined together for superior performance.

We surmise that the PSFM summary outperforms the PSSM-based measure in certain cases because the entries in the scoring matrix represent log-odd ratios, which scales the data and removes some of the variance that may help distinguish between the different groups. As an example, for one of our datasets, all of the PSSM values fall between  X  7 and 13. The PSFM matrices, on the other hand, contain values between 0 and 100. This information is provided in the histograms on the left of Fig. 1 . The wider variance of the PSFM matrix is at times preferable to the tighter range of the PSSM values. This is analogous to often preferring the covariance matrix over correlation for EM-based missing value analysis [ 19 ]. 3.3 Structure-based representation We now cover the steps needed to calculate our wavelet-based structural descriptors. An earlier, reduced version of this algorithm was published previously [ 16 ]. We present the expanded version here. We begin by describing the process needed to transform each 3D structure into a 2D distance matrix, followed by the procedure used to normalize those matrices. We conclude with details of the wavelet decomposition.
 Generation of distance matrices To generate our structure-based feature vector, we first convert the protein structure into a distance matrix. This process occurs in the following manner: First, we obtain the 3D coordinates of the protein from the PDB and calculate the distance between the C  X  atoms of each residue. We place these values into an n  X  n matrix D ,where n represents the number of residues in the protein and D(i,j) represents the distance between the C  X  atoms of residues i and j . Figure 3 provides a graphical depiction of this matrix (for protein 1CCR), with higher elevations, or larger distances, having a lighter color. In these matrices, secondary structures such as  X  -helices and parallel  X  -sheets emerge as dark bands along (or parallel to) the main diagonal, while anti-parallel  X  -sheets appear perpendicular to it. The top image in Fig. 3 represents a pixelated version of the distance matrix. In contrast to this discretized approach, or to the binary  X  X ontact map X  representation, where distances below a certain threshold are set to 1 (else 0), we operate on the actual distance values.
 Matrix normalization Having converted the 3D structure, we apply a 2D decomposition to the distance matrix. As with our sequence-based representation, in order to use the results of this transformation as a feature vector, the final number of coefficients must be the same for every protein. This can be achieved either by normalizing the size of the input (the distance matrix), or the output (the approximation coefficients). It is not immediately clear how to normalize a variable-size coefficient matrix while still preserving the necessary spatial correlations. Thus, we again normalize the input signal, fixing the size of the distance matrix to 128  X  128. Transformation details We perform a 2D Haar decomposition on this normalized matrix and use the final level of approximation coefficients as our feature vector. Examples of the wavelet decomposition can be seen in Fig. 3 . The top figure illustrates a normalized distance matrix, while the figure on the bottom corresponds to the approximation coefficients produced from a third level decomposition. We only focus on the approximation values produced by the final level of the wavelet decomposition, so as the decomposition level increases, the number of coefficients we must consider decreases by a factor of 4. Despite this large reduction in data, important features such as secondary structures, are still present in the figures. Since the matrix is symmetric across the diagonal, we only need to keep the coefficients in the upper (or lower) triangle, plus those that fall on the diagonal itself. Of all the decomposition levels that we tested (see Fig. 2 ), we found that the third level affords the best performance between accuracy and feature vector size. This decomposition leaves us with a structural descriptor of 136 coefficients per protein.

A summary workflow for the above transformations is presented in Fig. 4 .Tocreatea structural descriptor for a protein, we transform the 3D coordinates into a 2D distance matrix that is then normalized. After normalization, we decompose the matrix using a 2D wavelet decomposition. We extract the final level of approximation coefficients, which serve as our feature vector. To create the sequence-based representations for a given protein, we begin by generating a PSI-BLAST alignment profile. We extract the frequency and scoring matrices and normalize them. A 1D wavelet decomposition is applied to each matrix and we again extract the final level of approximation coefficients. As with the structural descriptor, these coefficients serve as our sequence-based feature vectors. 3.4 Hybrid representation To create the hybrid variant of our representations, we simply concatenate the PBPM-summaries to the end of the structural descriptors. This results in an overall feature vector with 456 attributes that characterizes both the sequence and structure of a protein in a concise, normalized manner. One can also create a hybrid version of the individual sequence sum-maries. In these cases, we concatenate the structure descriptors to the end of the PSSM and PSFM-based representations. Each of these feature vectors contains a total of 296 attributes. 4 Datasets We validate our representations using two different subsets of the SCOP database. One subset was selected to gauge the performance of our representations in a database context, the other to evaluate classification accuracy.

To evaluate database performance, we focus on a dataset that has been examined in a number of indexing publications [ 4 , 6 ], most recently by Gao and Zaki [ 8 ]. In this set, a total of 181 SuperFamilies were selected from SCOP, with each SuperFamily having at least ten members. Ten proteins were selected from each SuperFamily, resulting in a total dataset size of 1,810 proteins. To construct a query set, one protein was selected at random from each of the SuperFamilies, yielding a query subset of 181 proteins. We refer to this dataset as the 181_SF set.

To evaluate the classification performance of our representations, we rely on a subset of the ASTRAL database [ 5 ]. ASTRAL is a repository derived from SCOP that filters proteins based on their shared sequence similarity. One version of the ASTRAL database consists of proteins with less than 40% sequence identity, the other with those that share less than 95%. Both versions have been examined in previous classification experiments [ 9 , 11 , 13 ], but we choose the version containing proteins with less than 40% sequence identity, as we feel that poses a greater classification challenge. Using version 1.65 of the ASTRAL database, we take all the proteins that share less than 40% sequence identity, which results in a set of approximately 5,600 proteins. From there, we select all the proteins that belong to Folds with at least 20 members, removing the rest. After this pruning step, we are left with a total of 2,915 proteins that belong to 63 different Folds. This dataset, which we call the 63_Fold dataset, is essentially the same as the one used in the SVM experiments of Han et al. [ 9 ]. Ta b l e 1 provides the number of unique groups for the 63_Fold and 181_SF datasets over the different levels of the SCOP hierarchy.

Taking the 63_Fold dataset, we classify proteins using their SCOP labels as our gold standard. We initially focus on the Fold level, but we are also interested in determining the performance of our representations at the other levels of the SCOP hierarchy. If a representation is truly effective, there should be little change in the overall performance as one progresses down the hierarchy. This is despite the fact that the classification challenge itself is increased, with the lower levels containing a larger number of potential classes with fewer members per class.

As it is originally constructed, the 63_Fold dataset is not suited for classification at the lower levels of the SCOP hierarchy. While each Fold was chosen because it contains at least 20 members, there is no such requirement on the underlying Families and SuperFamilies. Some Families, for instance, contain a single member, which makes classification at this level impossible.
 Therefore, we construct two additional subsets of our 63_Fold dataset to classify at the SuperFamily and Family levels. For the first subset, we select the proteins belonging to those SuperFamilies that contain more than ten members. We classify this subset, denoted SF &gt; 10, at the SuperFamily level. The second subset contains all the proteins that belong to Families with more than ten members. This subset is classified at the Family level and denoted Fam &gt; 10. We provide the membership information for these datasets (as well as the original 63_Fold dataset) in Table 2 . We could classify the SuperFamily subset at the Fold level and the Family subset at the SuperFamily and Fold levels but do not. Classifying the smaller Family subset at the Fold level, for instance, will simply result in improved performance over the more general 63_Fold dataset. The original 63_Fold dataset presents the greatest classification challenge at the Fold level, and the SuperFamily and Family subsets pose the greatest challenges at their respective levels. 5 Experiments All of our experiments were conducted on a 2.4 GHz Pentium 4 PC with 1.5 GB RAM running Ubuntu Linux on a 2.6.12 kernel. When referring to our different representations, we use PSFM and PSSM to denote the PSFM and PSSM sequence summaries, respectively, and 2D to refer to our 2D structural decomposition. We also combine the structural descriptor with the individual sequence summaries ( 2D and PSFM , 2D and PSSM ). We denote the combined PSSM and PSFM descriptors as PBPM and finally, the sequence-structure hybrid as 2D and PBPM . 5.1 Database indexing To illustrate the effectiveness of our representations as an indexing method, we validate our approach using two different query retrieval tests, nearest-neighbor and range. For the nearest-neighbor tests, given a query, we retrieve the k nearest-neighbors from the target database and report the number of correct matches. For the range queries, we specify a range value r and retrieve from the target database those objects that lie within a distance r to the query. In both tests, distance refers to the standard Euclidean distance.

We u s e a k X  X  tree as our target data structure [ 3 ]. The k  X  d tree is a generalization of the binary search tree. Both start with a root node and place the data into left and right subtrees based on the value of a particular attribute, or key. This process is recursively repeated until the data is divided into mutually exclusive subsets. While a binary search tree uses the same key on all nodes of all subtrees, a k X  X  tree will use a different key at each level. This leads to a data structure that effectively partitions the data but still allows the user to search for best matches without having to make a large number of comparisons. For our tests, we use k  X  d tree code obtained from the Auton Lab 1 .

We also compare our techniques with PSIST, one of the more recently-published indexing methods [ 8 ]. In PSIST, a feature vector is generated for each protein based on the distances and angles between residues. These vectors are placed into a suffix-tree, which serves as the indexing structure. Using the PSIST source code 2 , we generate the input features required by the program and test our datasets. The PSIST algorithm allows the user to tune a number of parameters, all of which affect both the accuracy and computation time. We test a number of parameter values but find that the program defaults provide the best overall performance. Nearest-neighbor retrieval For our nearest-neighbor retrieval tests, we use the SCOP labels at the SuperFamily level to determine whether a neighbor is correct. We examine the number of correct matches for k equal to 4, 10, 50 and 100. Similar values have been used in previous retrieval experiments [ 8 ]. We evaluate our structure and sequence-based representations on the 181_SF dataset and compare against the values returned by PSIST.

The results of our retrieval tests are presented in Table 3 . Shown in the table are the average number of nearest-neighbors that belong to the same SuperFamily as the query protein for different values of k . On the 181_SF dataset, we see that both our representations return a larger number of correct neighbors than PSIST for all the tested values of k and that the sequence-based summaries outperform those derived from structure. There is little performance gain for values of k larger than 10, but that is because the target database only contains ten members for each SuperFamily. We find that PSSM provides the best overall performance, though PSFM is comparable. The structure-based values are slightly worse, and we see no improvement in combining sequence with structure.
 Range queries We also test our representations with short, medium and long range queries using the 181_SF dataset. We empirically define a query as short, medium or long based on the number of objects returned. If the range value is too small, very few objects will be returned. Too large, and the query will return the entire database. Since the choice of the actual range value is dependent on the representation, we cannot use the same value of r for every test. Therefore, we list the range values used for each representation along with our results. We report the average number of objects retrieved (total number of objects retrieved divided by the total number of queries) as well as the average number of matches among those objects. A match occurs when the object and query belong to the same SuperFamily. The results of this experiment are provided in Table 4 .
 As we can see, range queries are highly accurate, but the number of retrievals is variable. For instance, for the short range values, we retrieve three to five objects with 100% accuracy. Even at the long range values, our accuracy remains very high. We see an accuracy of almost 100% with the the 2D features, and 84 X 98% with the sequence-based representations. Each SuperFamily in the 181_SF dataset contains only ten members. As we from the table, we can retrieve half of the possible total with no error .

Finally, while we do not report the values here, we find that our indexing approach is far superior than PSIST in terms of both query execution time and memory usage. 5.2 Classification For our classification experiments, we use the SVM provided by WEKA, version 3.4 [ 24 ]. WEKA uses Platt X  X  sequential minimal optimization (SMO) algorithm for SVM training [ 20 ]. We use a linear kernel with the default complexity parameter (1.0) and train each classifier in a one-vs-rest fashion. As with our database experiments, we test each representation separately, as well as combined into a single feature vector. We report classification performance in terms of the average accuracy, with accuracy values reported as the number of true positives divided by the number of true positives plus the number of true negatives (TP/(TP + TN)), returned as a percentage.

We perform two different classification tests. For the first set of tests, we train an SVM using ten-fold cross-validation and report the average classification accuracy. In these experiments, there is at least one member from every class included in the training data. We view such tests as recognition experiments. This is in contrast to our second set of tests, which we refer to as homology detection .
 In these experiments, we take the SF &gt; 10 and Fam &gt; 10 datasets and set aside nine SuperFamilies and eight Families, respectively. The proteins in these groups are completely untouched during the training phase. We take the remaining members of each dataset and train a classifier one level  X  X p X  the hierarchy. In other words, we train an SVM on the SF &gt; 10 dataset using Fold labels and the Fam &gt; 10 dataset using SuperFamily labels. After the classifier is trained (using ten-fold cross-validation), we test the independent validation sets and if the classifier can identify the correct Fold of the held-out SuperFamilies and the correct SuperFamily for the held-out Families, we state that the classification has been successful. Similar homology tests have been reported elsewhere [ 11 , 21 ] and are considered to be a more challenging exercise in structure prediction.

The SuperFamilies that were set aside were selected because they belong to Folds with more than one SuperFamily, giving us an opportunity to train a classifier to recognize that Fold. The same process was used for selecting the Families, except that they were required to belong to a SuperFamily with more than one Family. After dividing the SF &gt; 10 and Fam &gt; 10 datasets using the above criteria, we are left with training sets of 1911 and 877 proteins and testing sets of 203 and 137 proteins, respectively.
 Recognition We present the results of our recognition experiments in Table 5 . As one can see, Fold-level classification presents the greatest challenge, where we report accuracy values of around 66, 62 and 65% for the 2D, PSFM and PSSM representations, respectively. At the SuperFamily level, the accuracy for the PSSM and PSFM representations improves to roughly 72%, while the performance of our structure-based representation is essentially constant. We see additional improvement progressing down to the Family level, with the accuracy of the 2D representation increasing to 81% and the PSSM and PSFM summaries to almost 85%. While the PSSM summary outperforms the PSFM representation at the Fold level, that edge is erased at the other levels of the hierarchy. Combining the PSSM and PSFM representations together results in an improvement of approximately 3%.

Ta b l e 5 also shows the accuracy of our hybrid sequence-structure representation. It is quite surprising how much one can improve the overall performance by combining sequence with structure. At the Family level, the stand-alone PSFM and PSSM summaries yields an accuracy of around 84%. Combined with the structural features, that value increases to 89%, an improvement of almost 6%. Even though the PBPM representation has an accuracy of 87% at the Family level, that value can be increased to more than 90% by adding structure. One can see even greater increases at the SuperFamily and Fold levels; 10 and 14%, respectively. While structural information would not be available to a researcher trying to predict the global fold of a protein, our results do indicate that the information contained in our 2D representation could be used in a semi-supervised manner to refine the sequence-based classification results, potentially improving the prediction process [ 23 ]. In addition, there may be instances where a researcher has access to information on both the sequence and the structure of a protein. In such cases, our methods can be combined to improve the results over any individual representation.
 Homology detection The results of our homology detection experiments are provided in Ta b l e 6 . While it is unlikely that a researcher would use structural information to detect homologs, we include the values for the sake of completeness, but do not discuss them further. As we see from the results, homology detection is a harder classification challenge, with accuracy values 10 X 20% lower than those of the recognition experiments. There is little difference between the PSSM and PSFM representations, however. With the combined PBPM representation, we see an increase in Fold-level accuracy, but that gain is erased at the SuperFamily level. Our results are comparable to homology results reported elsewhere on similar, though not identical, datasets [ 21 ]. We see these results without having to resort to a task-specific kernel function or representation, which highlights the general applicability of our approach. 5.3 Transformation parameters In our original experiments, we elected to normalize the length of each protein to 128 and use a fourth and third level wavelet decomposition for the sequence and structure-based represen-tations, respectively (see Sect. 3 ). As a follow up to that work, we conducted a more robust set of experiments where we evaluate the performance of several different protein lengths (128, 160, 192, 224, 256) over a range of decomposition levels (2 X 7). However, we do not repeat all of our experiments for every possible combination of length and decomposition level. We only repeat the full set of experiments on those combinations that appear most promising. To determine those combinations, we repeat the process detailed in the  X  X ransformation details X  subsections of Sect. 3 .

First, for each normalized protein length, we generate feature vectors for the sequence and structure-based representations for every wavelet decomposition level between 2 and 7 (inclusive). This leaves us with a total of 15 feature vectors for each candidate length (3 representations  X  5 decomposition levels). We take these feature vectors and use them to train an SVM to classify the 63_Fold dataset with ten-fold cross-validation.

We use the accuracy values from the classification experiments to determine the suita-bility of each combination of level and length. For the two combinations that we deem the most promising, we generate the additional hybrid representations and repeat our full set of experiments to determine how these alternatives compare against our original results. While accuracy is the primary criteria used to just the suitability of of each combination, we also feel it is necessary to take other details, such as the overall number of attributes, into account. The size of each feature vector will affect the classification training time and the amount of storage and memory needed to process our database queries.

In Fig. 5 , we present the SVM classification accuracies on the 63_Fold dataset for each combination of level and length. The PSFM, PSSM and structural values are presented in Fig. 5 a X  X , respectively. The accuracy values are listed on the y -axis and the decomposition levels on the x -axis. Each line in the figures corresponds to a different length. As one can see, the trends are very similar to those presented in Fig. 2 . The highest values for the sequence-based representations are seen at levels 4 or 5, while the best performance for the structural descriptors occurs at levels 2 or 3. For each representation, the difference in accuracy between the two levels is typically marginal, so when selecting a value for the final decomposition level, we choose the one that will result in a smaller number of total attributes: level 4 for the sequence-based representations and 3 for the structure-based descriptor. These were also the decomposition values used in our original experiments.

After determining the decomposition level for each transformation, it becomes necessary to select a normalized length for the representations. While we could use a different length for each individual representation, we feel that in order to make a valid comparison, the same value should be used for all the transformations. Figure 6 provides a closer view of the classification accuracy for each length and representation at a single decomposition level (4 for sequence, 3 for structure). Unlike Fig. 5 , here each line in the plot corresponds to a representation while the lengths are listed on the x -axis. The y -axis again provides the accuracy values, though at a different scale than in Fig. 5 . As one can see from the figure, using a longer length generally results in a higher accuracy, though the difference may be small. Based on the graphs, we elect to repeat our experiments using length values of 160 and 224. We choose 160 as that seems to be the optimal value for the structural descriptor, in terms of accuracy and the overall number of attributes. Our selection of 224 was motivated by the performance of the sequence-based representations at that value. The number of attributes in each representation are listed in Table 7 . We now repeat our original tests using these new lengths to see how they compare to the representations generated using a length of 128. We begin with a re-evaluation of our recognition and remote homology detection classification experiments.
 Classification Figure 7 contains the results of our recognition experiments, with the Fold, SuperFamily and Family-level results given in Fig. 7 a X  X , respectively. As one can see, with the sequence, structure and combined sequence representations (PSFM, PSSM, 2D, PBPM), a longer length generally results in a higher accuracy. When combining sequence with structure (2D and PSSM/PSSM/PBPM), the reverse trend appears to hold.

While these trends tend to hold across the Fold, SuperFamily and Family levels, we should note that at the Family level, there is little difference in accuracy for the different lengths with the PSSM and PSFM representations. In addition, the PBPM representation present behavior opposite that seen at the Fold and SuperFamily level. The smaller-length representations yield a higher accuracy than those based on the larger values.

Unfortunately, the experiments in homology detection do not produce trends that are as clear-cut as those seen in our recognition tests. Figure 8 provides the accuracy results for our attempts at homology detection. In general, with Fold Detection (Fig. 8 a), a shorter length yields a higher accuracy. The same holds true at the SuperFamily level (Fig. 8 b) for the 2D and sequence-structure hybrids, but not for the purely sequence-based representations. For those descriptors, a longer length is more effective.
 Database indexing We also repeated our nearest-neighbor and range-based indexing experiments, but found that the overall performance of the longer lengths was very simi-lar to that of the length 128 representations. Therefore, we choose not to repeat those values here. However, since the parameters for our range queries are dependent on the represen-tation, they will change based on the length and decomposition level. Thus, in Table 8 ,we list the range parameters needed to execute a short, medium and long range query for each representation.
 Information loss One may have concerns with the amount of information loss that occurs with our transformations. There are two primary areas that contribute to this loss: when normalizing the protein lengths and when applying the wavelet decomposition. Here we attempt to allay some of those concerns by first addressing the latter and then focusing on the former.

For the sequence-based representations, after normalizing the length of the protein, applying a forth level wavelet decomposition reduces the total number of attributes or amount of information by a factor of 16 (2 4 ). With the structural descriptors, the total amount of infor-mation is approximately reduced by a factor of 60. For example, given a normalized matrix of size 128  X  128, we can immediately remove half the points since it is symmetric across the diagonal. In this case, we would be left with a total of 8,192 points. A third level 2D wavelet decomposition yields 136 coefficients, a reduction of approximately 60-fold. This is about four times greater than the reduction seen with the sequence-based representations. Since each level of the 2D wavelet decomposition reduces the total number of points by a factor of 4, one could make the information loss between the different methods equal by either reducing the final level of the structural descriptor from 3 to 2, or by increasing the final level for the sequence-based representations from 4 to 6. However, since there is little difference in accuracy between levels 2 and 3 for the structural descriptors (see Fig. 5 c), we do not feel that such a change is necessary.

When normalizing the length of the protein, it would appear that the amount of information loss is much greater for the structural descriptors, since we are reducing an N  X  N distance matrix to size L  X  L (where L is the normalization length) compared with the sequence-based profile matrix that is being reduced from N  X  20 to L  X  20. However, since the distance matrix is computed from the 3D coordinates of the amino acids, we really are reducing the total amount of information from N  X  3(the xyz coordinates of each amino acid) to L  X  3. This is similar to the amount of information loss in our sequence-based representation, but we are currently looking at several information-theoretic measures to better characterize these values. 6 Conclusions In this paper, we present two methods of protein representation, one based on sequence information, the other derived from the solved structure. We find that the PSSM and PSFM representations provide roughly the same overall performance, though there are instances where the PSSM summary yields a higher accuracy. We also find that even though the PSSM scores are derived from the PSFM values, combining the summaries can result in a fairly dramatic increase in accuracy. We believe this is because the combined PBPM representation merges information about the overall amino acid frequency counts contained in the PSFM summary as well as the PSSM scoring information pertaining to the overall conserved-ness of each region. The PBPM summaries are comparable to our 2D structural descriptor. It is also possible, as evident in our classification experiments, to combine information on both sequence and structure into a hybrid representation that can provide an improvement over any individual variant. Such a technique can be useful in applications where the researcher is not limited to sequence or structure and further emphasizes the general applicability of our approach.

Based on the results of our experiments using representations based on different normali-zed length values, it can be difficult to state whether any particular length or decomposition level is superior to the rest. In general, it seems that a longer length will result in superior classification performance. However, there are cases where that such trends do not hold, such as when classifying at the Family level or with the sequence/structure hybrid descriptor. As a result, there is no one representation that is superior in all areas. Thus, the selection should be based on the task at hand.

We find that the PBPM-based summaries provide comparable performance to our structu-ral representation and that both techniques have applications where one is more suited than the other. If one were interested in structure prediction, for instance, then the sequence-based approach would be an appropriate choice. It is possible, however, to include structure-based information to improve the performance in fold classification, which implies that the struc-tural representation could be leveraged in a semi-supervised manner to increase the overall accuracy. In some circumstances, however, a sequence-based representation is simply not useful. If a researcher were examining the results of a protein folding simulation, the need for a structure-based representation is critical. Any attempts to successfully characterize the intermediate structures or analyze common folding pathways, either between multiple runs of a single protein or among the results of several proteins, would hinge on an effective struc-tural representation [ 25 ]. In fact, since a protein X  X  sequence is static throughout the course of the simulation, it is not possible to use a sequence-based representation in such settings. While most of the folding simulations to date have been relatively small, focusing on runs of short, engineered proteins, large-scale simulations such as Folding@Home [ 14 ]havecome online and are expected to generate a tremendous amount of data. In order to have any hope of efficiently processing these results, it is crucial that one employ an effective method of representation. In a similar context, as part of future efforts, we are also exploring the use of alternative structure descriptors such us 3-D motifs[ 7 ].

Throughout this work, we have presented our structural descriptors as a way to represent protein molecules. There is nothing that limits our technique to proteins, however. One could easily extend our approach to work with other domains, such as defect tracking in molecular dynamics (MD) simulations [ 17 ]. MD simulations are used to model the beha-vior of spurious atoms as they move through a lattice of a base material such as silicon. Given a set of simulation frames, one could apply our algorithm to create a sequence des-cribing that set, which could then be compared against other sets. Being able to characterize the behavior of these defects would be a boon to those who work in the manufacture of semiconductors.
 References Authors Biography
