 The analysis, design and maintenance of Web sites involves two significant challenges: managing the services and con-tent available, and secondly, making the site dynamically a d-equate to user X  X  needs. These challenges can be addressed by automating several of the management activities of a Web site. In this work we propose to develop a platform that can be used for that purpose, which is independent of the Web site. We started by developing a data warehouse that stores data about the usage, content and structure of a Web site. We have also developed a tool that uses the data in the data warehouse and provides information to the editor to monitor the activity on the Web site as well as the site itself. We have recently begun to develop multidimensional recommender systems that take advantage of the wealth of the data in the data warehouse. Both simulated and live tests are carried out to test the platform.
 I.2.6 [ Artificial Intelligence ]: Learning X  Induction ; H.2.8 [ Database Management ]: Database Applications X  Data mining Design, Measurement, Experimentation, Algorithms Web site automation, Recommender system, Web mining, Data warehousing
The management of Web sites has become a constant de-mand for new information and timely updates due to the volume of services and content that site owners must pro-vide to satisfy the complex and diverse needs and behaviors of the users. Currently, site editors must not only keep the contents of the site up-to-date, but also permanently choos e services and navigational structure that best helps achiev e the aims of both the user and the owner of the site. Such constant labor intensive effort implies high personnel cost s.
Several of the management activities of a Web site may be automated, such as the retrieval of new and relevant content, monitoring of existing content and structure, au-tomatic recommendation and personalization. One of the goals of automation is the reduction of the site editor X  X  ef-fort, and consequently of the costs for the owner. The other one is that the site can adapt itself to the behavior of the user, improving the browsing experience and helping the user in achieving his/her own goals.

The objective of this work is to develop a platform that can be used for that purpose. This platform should be, as much as possible, independent of the Web site and, in par-allel, adjustable to different sites. It is based on a data warehouse that stores data about the usage, content and structure of a Web site (Section 3.1). The data in that data warehouse is used by the EdMate tool, to provide informa-tion to the editor to monitor the activity on the Web site as well as the site itself (Section 3.2). Two common rec-ommender systems have been implemented and integrated in the platform (Section 3.3). We are currently developing multidimensional recommender systems that take advantage of the wealth of the data in the data warehouse (Section 4).
The automation of a Web site involves different technolo-gies to gather, provide access to, and analyse data and infor -mation. Among the various technologies, data warehousing, Web mining and recommender systems can enable organiza-tions to gather, access, and analyse Web data for improving their Web sites/businesses.

A data warehouse is an integrated repository of data that employs several methods to transform raw data into mean-ingful business information, identifying trends and patte rns to be used as a foundation for making decisions [8].
Web mining is the use of data mining techniques to auto-matically discover and extract information from Web docu-ments and services [9].

The most traditional recommender systems deal with ap-plications that have two dimensions, user and item. These systems receive information about users and their behav-ior and recommend items (hyperlinks) that are likely to fit his/her needs [7, 10]. More advanced recommender systems use contextual information, added by other dimensions such as time and place, to improve their recommendations [1].
The architecture of the platform for Web adaptation [4, 5] is presented in Figure 1. In this platform, adaptation con-sists of automatically and dynamically introducing change s in the content that is displayed in the browser. For example, if the adaptation is a list of recommended hyperlinks to be shown to a particular user at a given moment, these hyper-links are added to the original Web page on the fly, taking into account what the system knows about the user and the content. To implement an adaptation mechanism using this platform it is only necessary to include in the Web page (or is executed on the client side. The script generates adapta-tion requests to be handled by the adapters and renders the output of the adaptation on the Web page. Additionally, the script collects usage data that can be used to monitor the site and the adaptations.
 Figure 1: Architecture for the Site-O-Matic plat-form.

Communication between the Client and the Adapter is mediated by a Broker and uses XML based messages, that separate Web adaptation from the site X  X  specific HTML lay-out. The Broker is responsible both for storing usage data in the Logger database and routing adaptation messages to the appropriate Web adapters. Logs about Web site usage and adaptation (including positive and negative user feedback , relative position of the adaptation and the adapter used) ar e recorded in the Logger database.

Adapters, such as a recommender system, are pluggable modules that use information from the Site Activity Infor-mation Service component. This component consists of a data warehouse that stores information about the Web site activity (page views, hyperlinks, access logs, events, ada pta-tions). The data warehouse is also used by the EdMate tool to generate reports about the Web site, which are accessed by the editor/owner using a Web browser.
The Site Activity Information Service (SAIS), which col-lects and compiles information regarding activity in the We b site, is responsible for providing information for Web site adaptation and analysis. In [4] the SAIS has been designed as a data warehouse whose fact and dimension tables store information about accesses, content and structure. In Fig-ure 2 we present the snowflake schema of the data ware-house. A detailed discussion about its tables is given in [4] .
With the proposed schema we can provide richer data to the adapter and analyse effect of adaptations in more de-Figure 2: Snowflake schema of the data warehouse. tail. We can, for instance, analyse for each Web access which adaptations were inserted in the page and how the user reacted to them.
EdMate is a Web based tool that we have developed to monitor the quality of Web sites. This tool enables the site X  s editor/owner to detect and correct problems in a Web site, thus improving the quality of the site and the satisfaction o f its users.

Using pre-processed data stored in the SAIS, the EdMate tool provides different types of reports. These include very simple statistics, about the usage of the site, and more com-plex analysis, such as computing the quality of content de-scriptors and adaptations in Web sites [6, 5].

With the reports provided by the EdMate, we can mea-sure, for example, the impact of the adaptations when they are shown to actual users. We can assess what users ac-tually do when interacting with the site and, in particular, how they react to personalization. This enables us to better understand the behavior of users, identify usage patterns, problems with the content and structure of the site and un-met needs. It also enables us to identify issues that would not be evident in a laboratorial study. Additionally, the EdMate tool also provides an easy to use SQL interface to selection and exportation of different data sets that can be used for laboratorial analysis.
In our Web adaptation platform (Figure 1), adapters are pluggable modules that process adaptation requests and pro -vide a response that is delivered to the Client component. The platform specifies only the role of the adapters (not thei r design), and the interface they should provide. Therefore, adapters may run on different hosts and be implemented in different languages and operating systems. Some adapters may be simple programs prototyping a new algorithm while others may be full-fledged components optimized for high performance.

To validate the platform we have implemented two simple adapters. They are recommender systems, the first based on item-based collaborative filtering and the second based on association rules.
 A recommender system, whose model is based on an item-based collaborative filtering (CF) technique, analyses his tor-ical information to identify relations between the items [7 ]. The recommendation model is a matrix representing the similarities between all the pairs of items, according to a similarity measure. In our case, an item is an accessed page and the similarity measure is the cosine angle, defined by where  X  X  X  i and  X  X  X  j are binary vectors representing the users that accessed the page/item i and j , and  X . X  denotes the dot-product of the two vectors.

If we want the N best recommendation (top N ), we use the recommendation model to output a set with the N items more similar to a given set of observable items.
 An association rule (AR) A  X  B represents a relationship between the sets of items A and B . Each item is an atom representing a particular object. The relation is characte r-ized by two measures: support and confidence. The support of a rule within a data set D , where D itself is a collection of sets of items (or itemsets), is the number of transaction in D that contain all the elements in A  X  B . The confidence of the rule is the proportion of transactions that contain A  X  B with respect to the number of transactions that contain A . The most common algorithm for discovering AR is Apriori [2].
A recommendation model M , based on association rules, is a set of rules R with support and confidence. The model outputs a set of items/pages as recommendations Rec , given a set of observable items O . To produce the recommenda-tions, we build the set Rec as follows: Rec = { consequent ( r i ) | r i  X  M and If we want the top N recommendations, we select from Rec the distinct recommendations corresponding to the rules with highest confidence.
In this section we present results of a preliminary em-pirical study where we evaluate how multidimensional data can improve the accuracy of the recommendation algorithms described in Section 3.3. The study was carried out using three data sets, but for sake of space we will present only the results obtained with E-Government , a Web site of a govern-mental program to support the development of Portuguese human resources. This data set contains accesses to infor-mation and application forms (in portable document format -pdf) of the supports provided by the program. It contains 80142 accesses, 175 different items and 38371 different users .
To carry out the experiment, we followed the All But One protocol described in [3]. The data set was divided 80% for training set and 20% for test set. For each user in the test set we moved one item to the hidden set ( Hidden ). The test set with the remaining data is called the observable set. The model is evaluated by comparing the set of recommendations it makes ( Rec ), given the observable set, against the items in the hidden set. Once we have calculated Hidden and Rec , we measure Recall, Precision and the F1 metric [10]: Recall corresponds to the proportion of relevant recommen-dations. It tends to increase with N . Precision gives us the quality of each individual recommendation. As N increases, average precision decreases. F1 is a measure that combines Recall and Precision with an equal weight. It ranges from 0 to 1 and higher values indicate better recommendations. Global recall, precision and F1 are obtained by averaging the respective individual test user values.

To evaluate the addition of a new dimension in the rec-ommendation algorithms, we simply include for each user in the data set its data representing the new dimension. We have selected some data fields in Figure 2 as additional di-mensions. They were chosen based on their potential for improving the accuracy of a recommendation. A short dis-cussion of these data fields is given in Table 1. Notice that though the algorithms use the additional dimension in their models, they will recommend only the top N items. Table 1: Short description of the selected data fields.
In Figures 3 and 4 we see the F1 measure for the tradi-tional model (user  X  item) and for this one with an addi-tional dimension (for instance, + day ).

On the charts we see that in most cases the value of F1 de-creases when we increase N . Here this occurs because when we increase the number of recommendations, the values of recall increase faster than the values of precision decreas e.
Contrarily to [1], in Figure 3 we see that our direct multi-dimensional approach does not improve the accuracy of CF models by adding dimensions. The only exception is the uri referer dimension, with an F1 average gain of 36% com-pared to the same model without any additional dimension.
For the recommendation model based on AR (Figure 4), we have the highest gain for the F1 measure, 309%. This gain is obtained with the value 0.542 (top 1 with uri referer dimension). In this experiment, we chose a minimum sup-port value keeping at least 50% of the items of the data sets. The minimum confidence values were defined as being the support value of the third most frequent item.
 Figure 3: CF technique in E-Government data set.
 Figure 4: AR technique in E-Government data set (min. sup.=0.0013, min. conf.=0.04).
In this abstract we presented a platform for exploiting multidimensional data in Web site monitoring and automa-tion.

The basis of the platform is SAIS (Site Activity Informa-tion Service), which stores relevant information about usa ge, content and structure of the Web site in a data warehouse. We have described one tool, EdMate, which uses the SAIS for monitoring site quality, and assisting editors in maint ain-ing that quality. The SAIS is also being used for modeling the users so that the Web pages can be adapted according to automatically predicted users X  preferences. We have recen tly started to develop multidimensional recommender systems to take advantage of the information stored in the SAIS. Empirical results have shown that adding extra dimensions to collaborative filtering and mainly to association rule re c-ommender algorithms improves predictive performance. As developed, the SAIS is a service that can be used by other tools for improving the Web site X  X  quality automatically or for providing assistance to Web site maintainers.
We are currently experimenting the effect of multidimen-sional data on the performance of recommender systems based on Markov chains. It is our aim to further explore the benefits of multidimensional data for supporting Web site management. The thesis X  supervisors, professors Al  X  X pio M. Jorge and Carlos Soares. Funda  X c  X ao para Ci X encia e Tecnologia (SFRH / BD/22516/2005) and POSC/EIA/58367/2004/Site-o-Matic Project (FCT) co-financed by FEDER. [1] G. Adomavicius, R. Sankaranarayanan, S. Sen, and [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] J. S. Breese, D. Heckerman, and C. M. Kadie.
 [4] M. A. Domingues, A. M. Jorge, C. Soares, J. P. Leal, [5] M. A. Domingues, A. M. Jorge, C. Soares, J. P. Leal, [6] M. A. Domingues, C. Soares, and A. M. Jorge. A [7] G. Karypis. Evaluation of item-based top-n [8] R. Kimball and R. Merz. The Data Webhouse Toolkit: [9] R. Kosala and H. Blockeel. Web mining research: A [10] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.
