 where I n is the identity matrix of size n . Optimization (5) can be rewritten as and then the second term of (12) is convex since K i,j  X  0. B.1. Definitions as and the inductive Rademacher complexity is defined as the definition in Koltchinskii (2001) uses while the definition in Meir &amp; Zhang (2003) adopt  X  (0) = 0, then for El-Yaniv &amp; Pechyony (2009) and Zhang, 2003) are more natural and powerful.
 B.2. Proof of Theorem 2 Define the class of functions F as Lemma 22 of Bartlett &amp; Mendelson (2002), we get Applying Lemma 22 of Bartlett &amp; Mendelson (2002) again gives us derived by the exactly same way based on inequality (14). Let B.2.1. Step 1 sponding empirical Rademacher complexity.
 probability at least 1  X   X / 2 , we have inequality (McDiarmid, 1989) implies that by the same argument as above.
 (2009) by setting p = 1 / 2.
 Lemma 4 (Comparison Lemma) . Let and  X , X  0 : R 7 X  R be real-valued functions. If for all h , h 0  X  X  and i = 1 ,...,n , then lemma.
 Lemma 5 (Contraction Lemma) . For any  X  &gt; 0 , we have Proof. Note that `  X  ( z ) satisfies the Lipschitz condition Let  X  ( h i ) = `  X  ( y i f ( x i )) and  X  0 ( h i ) = y i f ( x i ) / X  , then of each  X  i y i and  X  i . This completes the proof.
 since `  X  maps to the interval [0 , 1]. On the other hand, for any f  X  X  , b R n ( F ) before contract R n ( F ), we can obtain Combining inequalities (15) and (16) finalizes the first step of the proof, that is, B.2.2. Step 2 change of sup  X   X  ` or equivalently, with probability at least 1  X   X / 2, It remains to bound the expectation E ( x Suppose that is a ghost sample for symmetrization, then
