 1. Introduction
The main quality features of a cold rolled steel strip are the accurate thickness, good flatness and superficial finishing. The high interaction degree between these and other controlled and manipulated variables, in a multistand environment, charac-terizes a cold tandem mill as a multiple-input, multiple-output (MIMO) system, which requires a very complex control strategy.
In the context of a growing demand for continuously stricter quality levels, the accuracy in the control of such a process becomes an essential characteristic, only achievable by means of proper automation architecture.

Cho et al. (1997) divide the tandem mill control strategy into three phases: pre-calculation, real-time control and post-calcula-tion. Such a format, schematized in Fig. 1 , is fully compatible with the control system used in the tandem mill focused in this work.
The pre-calculation, or static phase, consists of the determina-tion of optimum values for the reference needed by the real-time controllers, usually by means of a mathematical model. This model uses as inputs the characteristics of raw material (a pickled hot rolled coil) and the final product (a cold rolled coil) desired characteristics. The set of calculated references is called preset and the downloading of it to real-time controllers is called mill setup. Preset generation is typically a function assigned to the so-called Level 2 of automation architecture.
 starts with the effective strip processing. It consists of manipula-tion of many process variables in order to keep the target values face to existing disturbances, so that exit thickness, shape and superficial finishing can be achieved without overload in any mill stand. This function is included among Level 1 automation tasks. matical model parameters, based on actual variable values, measured during strip processing. This is necessary since a phenomenological model capable of perfectly representing the rolling process is not feasible. Moreover, the raw material properties are not fully known, and the measurement of process variables are only partial and liable to multiple kinds of disturbances (noise, calibration errors, etc.). Thus, a periodic adjustment of model parameters, taking into account the results actually obtained after material processing, is required in order to keep the model accuracy.
 and dynamic phases by stating that they are complementary functions, the former establishing the optimum reduction stan-dard for mill stands and the latter keeping the mill in that standard. Therefore, the ideal operational practice demands correct accomplishment for both the phases.
 (programmable logic controllers), during the dynamic phase, and taking into account the typical figures of volume and financial value of a steel mill production, it is not reasonable to interrupt plant operation for the sake of failures restricted to Level 2. For this reason, an emergency operation mode must be foreseen, to permit the static phase execution in the event of a Level 2 malfunction, thus assuring plant operational continuity.
The most common standby procedure is the use of tables of preset values. Each row of these tables is composed of the primary data (entry thickness, chemical composition, etc.), followed by the target values for final product (exit thickness, surface quality, etc.), ending with the fittest values for multiple machine adjustments (roll forces and stand speeds, for example) allowing to meet quality requirements, which constitute the presets proper.

To be effectual, such tables must contain a huge number of rows so that many types of commonly processed materials can be covered. As a consequence, their application is cumbersome and their results are inferior to those obtained in normal mode. The objective of this work is to introduce a novel approach to this problem (use of artificial neural networks) and prove that this new method can outperform lookup tables as a steel mill preset generation backup mode.

Text organization is as follows: the next section outlines the normal operation mode of the surveyed plant as well as the description and analysis of the o riginal backup working method, relating the reasons that motivated the development of a better one.
The third section summarizes the pondered options to cope with the problem and explains the theoretical basis for the choice of artificial neural networks. The following section details the development steps in the implementation of the proposed solution and then, in the fifth section, the results are related and discussed. The last section brings the concluding remarks and suggestions for further work. 2. Plant normal operation mode and its original standby counterpart
Prior to effective material processing, the optimum mill adjustment is obtained by means of a mathematical model, which uses incoming coil data as input. The aim is to get the maximum throughput, employing minimum energy and meeting quality requirements. Preset generation is essentially a problem with multiple degrees of freedom, and there are several combinations of plant adjustments fitted to lead to the desired goal. Thus, finding the best solution demands the use of some optimization technique. In this specific model, the utilized tool is the Simplex algorithm, developed by Nelder and Mead (1965). A cost function is defined based on total power, roll force and strip tension, and the optimization algorithm seeks after the minimum of this function. On completion of this calculation, the operator enables downloading of preset values to Level 1 controllers. So, when the strip reaches mill entry, real-time processing is started.
During rolling, process data are sent from Level 1 to 2 at a 100 ms rate. Level 2 gathers these data and stores them for a given period of time, allowing investigation of eventual abnormal situations and the generation of graphics and reports. These process data are the main input for the adaptation of mathema-tical model parameters (post-calculation phase).

The contents of the lookup table originally conceived to keep plant operation in case of a Level 2 failure calculation were obtained by means of an off-line interaction with the same mathematical model used in the normal operation mode. The table records consisted of primary data values (that served as query indexes) and the corresponding reference values, which must be manually transferred to the Level 1 controllers, in an operator-driven action.

This method had evident shortcomings. The main one was related with the fact that steel cold rolling is a batch process, with a virtually unlimited number of recipes. As a consequence, the processing of a material with completely novel characteristics, different from any previously produced, is very likely. For this reason, the more detailed the lookup table could be, by no means would embrace all the range of possibilities. In cases where there was no match between the characteristics of the strip to be produced and the table records, the operator had to choose among the various records the one that, according to his discretion, was the closest to the target. As a result, the presets applied were not always the best suited.

Moreover, as the table queries were manual, require typing, the wasted time was significantly longer than in normal operation mode. Eventual typing mistakes could lead to considerable preset errors, affecting product quality. In extreme cases, scraps were generated, resulting in production losses due to temporary plant operation breaks.

Anotherproblemwasthatthetablewasfilledwhenpro-grammed production stops, becau se these were the only opportu-nities in which the off-line interaction with the mathematical model was possible, in order to obtain t he preset values. Under these conditions, as the plant was not in operation, the so-generated preset values were somewhat different from those produced during normal run, due to a number of r easons: mill temperature below usual values, absence of the adaptation phase, etc.

Finally, the lookup table mode provided only 52 of the 125 preset values calculated under normal operation. This resulted in a less accurate adjustment of Level 1 controllers, rendering their work more difficult and less efficient and prone to potential quality problems. In short, the original backup operation mode was quite limited and this was the main motivation for the development of a new system. 3. Search for a better backup operation mode Many alternatives were foreseen to solve the problem, such as: Use of a more detailed lookup table.

Adaptation of the existing mathematical model to be executed in another CPU.

Development of a new mathematical model to be executed in another CPU.
 Implementation of redundancy in the critical hardware.
Use of artificial neural networks (ANN) for preset off-line generation.

As stated before, a more detailed lookup table would still be unable to cope with multiple production recipes, with the additional drawback of longer query times. The adaptation of the mathematical model for execution in another CPU, besides technical difficulties, would raise legal issues related with the model supplier intellectual property. The development of a brand new model would be a huge and time-consuming task, demand-ing a highly specialized team, and the implementation of a redundant system, besides significant expenses, would require deep modifications in the system architecture.

The complexity of model equations, the high number of variables and the abundance of historical data suggest the possibility of an approach of the problem using artificial neural networks (ANN). According to Tsoukalas and Uhrig (1997), ANN and other soft computing tools (fuzzy logic, expert systems, genetic algorithms, etc.), used individually or in a combined way, are specially suited to solve complex engineering problems, in which inexactness, uncertainty and non-linearity are involved. Steel cold rolling falls under this category of problem.
Several convergence points were found between the particu-larity of the problem and ANN characteristics, namely
Availability of historical data to serve as training patterns. The plant had been operating for six years and within this period the relevant information for the development of an ANN-based system, covering the full range of plant capability, had been collected and stored.
 ANN capacity to generalize from the training patterns. So, an
ANN-based system can get right presets even for coils of totally new characteristics, granted that these lie into the interval covered by the training data, that is, do not require dealing with extrapolation. Since, as explained above, available data wholly encompass plant and process limits, this condition is met.
ANN aptness to model continuous functions, which are the cases of the functions used in the preset generation process.
Record of many ANN applications in correlated areas, such as the works of Cho et al. (1997), Larkiola et al. (1998) and Pican et al. (1998)
ANN could bring forth a royalty-free system, using in-company personal and material resources, requiring a minimum involvement of process experts.
 the problem, notwithstanding the position of many researchers, such as Pichler and Pfaffermayr (1996) and specially, Jansen et al. (1999) , asserting that ANN should never be used to replace physical or analytical models, being imperative the joint and balanced application of both methods. The standby nature of the proposed system, designed to be used only during relatively short lapses of time, renders its operation safe. Just for information, the license for using the mathematical model software costs h 55,000 (about US$ 77,500), much more expensive than a neural network development tool that has the additional advantage of being multipurpose. 4. Development methodology
Zhang et al. (1998), ANN  X  X  X re well suited for problems whose solutions require knowledge that is difficult to specify but for which there are enough data or observations X  X . Both the quantity and the quality of these data are determinant factors to the feasibility of an ANN solution as well as to guide the choice of the best ANN topology.
 tion system there is the storage of production data, containing information on the coils produced in the last two years (about 100,000 coils). The data collected in this way have the advantage of representing real production circumstances, in opposition to the model off-line operation used to fill in the lookup table. Since operational experience showed that the whole product mix is processed within a four-month period, the database query was used to retrieve information about coils produced from October (2005) to January (2006), totaling 17,455 records.
 set of 30 input values, the treatment of the problem by means of a single ANN would lead to large and unfeasible dimensions. It was thus necessary to conduct a deeper analysis of the problem in order to reduce its dimensionality. The first conclusion was related to the effectiveness of input variables. After an accurate examination of model source code, it was found that only 10 of the input variables were directly used for model calculus, the others serving primarily as information to the operators as well as a base for later generation of reports and graphics. This fact allowed for a significant reduction in input dimensions. referent to the second stand through ANN. Since this is the mill pivot stand, it is possible to derive the value of the other stands speeds from second stand value using mass flow balance equations. Since the force control strategy employed is the imposing of a force value at the fourth stand, the imposed value was utilized as a standard. Finally, it was decided to get coiler tension values by means of the same algorithm used by the mathematical model.
 output variables, the next step was to split them into four distinct subsets, directly related with a natural division of the cold rolling process: those related with thread phase, normal run phase, tail-out phase and automatic gauge control (AGC) gains.

Each one of the four subsets, that share the same 10 input variables, was modeled by means of a dedicated ANN. So, the problem was solved by the combination of four ANNs in conjunction with some model equations. The target output values for ANNs (the presets) were received from the best results brought forth by the model for each available combination of input values.
With this arrangement, the system provided the same 125 preset values calculated by the model, 108 being received through the ANN and the remaining 17 by means of model equations.
A noteworthy feature of this development phase was the fact that data preprocessing, which consists of removing redundant and conflicting records and data transformation (in order to put them in an adequate range), was greatly simplified using a structured query language (SQL). The preprocessing stage was achieved with a single SQL clause (for each one of the four ANNs).
This clause selected from the plant database only the records having all fields lying within operational limits (rejecting in this manner the spurious records) and, among the several presets ensembles existing for a given input pattern, selected only the best suited (the one with the lowest value for the cost function).
At the end of the preprocessing, 16,194 records remained for the run phase ANN and 17,408 records remained for the thread, tail-out and AGC ANN. All these sets were afterwards subdivided into training, test and validation subsets.

The next step in the system buildup was the determination of the best ANN topology to face the problem. Many authors, such as
Basheer and Hajmeer (2000) , state that among the various ANN topologies, there are two specially suited to deal with the problem of function approximation: the multilayer perceptron (MLP) and the radial basis function (RBF) networks. Researchers such as
Ferrari and Stengel (2005) and Lang (2005) have shown that both types have the property of universal function approximators, that is, they are capable of uniformly approximating any continuous multivariable function to any desired degree of accuracy. Comparing the performance of these two topologies in system identification Park et al. (2002) found similar figures.
Another guiding criterion was consulting published works on similar fields, to serve as a reference. This line of investigation showed that, indeed, MLP and RBF are the most commonly used ANN topologies in cold rolling related applications. For these reasons, the tentative networks were built up accordingly.
The ANN training software tool, chosen after a careful market research, had the capability of automatically seeking the best network topology, after performing an user-configurable number of attempts, with different number of hidden units, activation functions and training strategies. After a series of experiments, the final choice fell upon the MLP topology, using a single hidden layer, due to its greater simplicity and better training results in full compliance with research purposes. The experiments showed that, in this specific case, given the dimensions of the ANN, the RBF topology was too much memory-consuming, and only MLP was feasible. The obtained results showed that this architecture fits properly to research purposes.
 Fig. 2 shows the distribution of weight values for the trained ANNs, and Table 1 exhibits the main features of their architecture and training. As stated above, all networks have the same 10 inputs. In all cases, it was employed the backpropagation training algorithm, with the BFGS quasi-Newton optimization method.
After establishing the acceptable ANN topologies and their respective weights, the implementation of the final product of this work followed a software application named NeuraLTF (the Portuguese acronym for  X  X  Neural Tandem Cold Mill X  X  ). The software was developed using the C# programming language (under NET framework), based on the ANSI C code generated by the ANN training tool and following the object-oriented paradigm, in order to ease system maintenance and enhance its flexibility, as it must remain open to eventual changes due to either new operational conditions or additional research outcomes.
 100 200 300 400 500 600 700 frequency 200 400 600 800 1000 1200 frequency
The system was installed in one of the Level 1 operator stations, which was also endowed with a client X  X erver link to the corporative server, in order to enable the access to the production schedules X  database. The main application functions are: preset calculation, checking of value limits, communication with Level 1 (to send the calculated presets) and with Level 3 (to get production schedules), temporary data storage and operator interface. Fig. 3 illustrates the placement of the system in the overall context of plant automation architecture and Fig. 4 shows the main screen of the application. 5. Results
Keeping in mind that NeuraLTF was developed with the specific goal of replacing the lookup table operation mode, the first benchmark test to assess its performance was a raw comparison between the reference values obtained from each of these strategies and those received from the standard mode (model generated reference values). For this purpose, two typical coils were chosen and the corresponding preset values were calculated using each of the three techniques: mathematical model (taken as reference),
NeuraLTF and lookup table. Only the 52 preset values delivered by both NeuraLTF and the lookup table were checked against the analogous values supplied by the model. The results proved NeuraLTF to have higher accuracy with regard to lookup table. Besides this improvement, another important benefit from
NeuraLTF is the production of the complete array of preset values in contrast with the lookup table partial set. Other advantageous
NeuraLTF features, specially from the operators X  viewpoint, are the reduction by more than 90% in the need of typing as well the fact that only one reference set is produced, eliminating the require-ment of choosing one out the many suggestions provided by lookup table. It was also found that NeuraLTF operation was simpler and faster than lookup table. Table 2 briefly compares the attributes of the two emergency modes.
 tool was directly compared with the model it seeks to emulate.
Hence, data were gathered referent to the preset values calculated by the model for 1 out of each 20 coils over a whole week of plant production amounting to 61 coils. Then, NeuraLTF was used to get the preset values for these coils and a statistical analysis was performed on the percent difference between the set of values.
The largest differences were found in the values of bending forces, which was already expected, since these variables have a very complex modeling.
 difference as high as 250% was found between NeuraLTF and model-generated values. In order to demonstrate that such an apparently large difference figure is fully congruent with the nature of cold rolling process, the Level 2 database was searched for coils with exactly the same values of input variables.
Seventeen records were found related to coils with the following input data 7 2.76 mm for the entry thickness, 0.77 mm for the exit thickness, 1205 mm for the width, 0.07 mm for the entry crown, low steel hardness, 8% for the equivalent carbon, chemical composition code equal to 365, 800 tons per meter for the imposed force on last stand, stamping code equal to  X  X  X M X  X  and tension mode  X  X  X N X  X . Fig. 5 shows the maximum and minimum run phase preset values calculated by the model as well the corresponding values received by means of NeuraLTF . Since the variables have different ranges, the data were normalized taking the average model values as a reference (100%).

As can be noted in the figure, there are huge differences between the maximum and minimum preset values calculated by the model, notwithstanding the 17 coils having the same input values. Reminding the multiple degrees-of-freedom essence of the process, as explained in Section 1 of this paper, the reason for such a wide range can be readily grasped. The point is that all the 17 coils were properly processed with different preset values lying into the interval between the gray and black bars of the graph. As can be seen in Fig. 5 , in almost every cases, the single preset results obtained through NeuraLTF were notably close to the average model results, indicating a sound preset calculation. This fact corroborated NeuraLTF interpolation capability, giving further evidence of its efficacy.

The major handicap of NeuraLTF , in comparison with the mathematical model, is not having adaptive features. The importance of this disadvantage, however, can be lessened once
NeuraLTF is merely a backup operation tool to be employed, as stated before, only during narrow time gaps, while normal mode is being brought back to life. The procedure to deal with the unavoidable changes both in product and plant characteristics is to periodically repeat the ANN training process. In the current practice, a new data gathering and networks training are performed at each six months. The retraining process, however, is much more straightforward than the development of the ANN from scratch.

Further research aims to bestow some adaptation mechanism to the system. One line of quest is to include the work rolls rolled length among the inputs of the neural networks so that operation time and thermal crown of the rolls can be taken into account. One promising tack is the on-line training of the ANN, as proposed by Lee and Choi (2004) , among others. Although this technique cannot effectively turn the NeuraLTF into an adaptive system (since during its operation Level 2 is out of service), it will guarantee the continuous updating of weights of the networks, reducing the need for periodical retraining. Another suggested additional work is to employ fuzzy logic to put categorical variables used as inputs for the ANN into a numerical form, specifically the steel hardness (classified as  X  X  X igh X  X ,  X  X  X edium X  X  or  X  X  X ow X  X ) and the stamping code (1 code out of 6). This expedient will certainly improve ANN performance, given the synergetic relationship between both techniques, emphasized in works such as those by Lazzerine et al. (1999) and by Tsoukalas and Uhrig (1997) .

As an extra benefit, the NeuraLTF development motivated a more thorough scrutiny of the fundamentals and characteristics of the cold rolling mathematical model, providing process and automation engineers with a deeper understanding about it. As a consequence some adjustments were made in the productive process, meeting the continuous improvement philosophy that characterizes the enterprise X  X  quality policy.

Good NeuraLTF results eventually lead to the deactivation of the former supplementary operation strategy (lookup table). The first coils were successfully rolled under the fresh schema on May 2006, substantiating the adequacy of the new method.
 6. Concluding remarks
As shown, the operation and results of NeuraLTF were clearly superior to lookup table mode, and even comparable to normal operation method. The main innovative note of the developed system lies in attaining nearly the entirety of the needed presets for cold rolling a steel strip by employing the neural network technique. Since all authors know that NeuraLTF is the only ANN-based system with such a characteristic. On the other hand, the current literature registers works such as those by Cho et al. (1997), Dixit and Chandra (2003) , Pican et al. (1998), and Zhao et al. (2005) , to quote only a few, where the ANN approach is used to receive only a small number of preset values (normally a single one). Proving the suitability of an ANN-based system for supplying virtually the whole set of references needed by a rolling mill controller is the chief contribution of the present work.

Notwithstanding the specificity of the problem exposed in this paper, the phases of data collecting, data processing, network training and deployment described here constitute a standard perfectly suitable for other applications comprising various realms. A few examples of such applications are those related by Hussain (1999), in the chemical industry; by Edwards et al. (1999), in the pulp and paper area; by Kalogirou (2000) , in the field of energy systems; and by Wu and Chow (2004) dealing with induction machines fault detection. For a more general review of successful ANN industrial applications, the reader can refer to Meireles et al. (2003) .

The route traced in this paper can be easily extended to virtually any field of industrial processes in which enough amounts of data are available, since the solution pointed out is essentially a data-driven one. So, in tackling complex and ill-formulated problems, artificial neural networks are a strong candidate for solution, given the variety, the ease of use and relatively low costs of neural networks development tools, the ever-growing processing power of personal computers along with wide dissemination and good results obtained by using this technique.

The whole development process confirmed the extraordinary applicability of neural networks as an alternative approach to complex engineering problems as well as corroborated the view of Curry and Morgan (1997) and Morris and Martin (1998), demystifying the naive idea that neural networks are a panacea to solve that sort of problems based only on plenty of data. In summary, the availability of data cannot replace or exempt from a sound knowledge about the issue dealt with.
 References
