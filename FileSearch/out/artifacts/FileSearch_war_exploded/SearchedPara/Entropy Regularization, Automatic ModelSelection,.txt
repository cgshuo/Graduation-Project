 Image segmentation is one of the basic problems of image processing. In general, there are two approaches to do such a task, i.e., region growing [1] and boundary detection [2]. For the region growing approach, each pixel is assigned to one homogeneous region with respect to some features such as gray level, color and texture, while for boundary detection, discontinuity of those features is regarded as an edge and a boundary consists of such edges. In this paper, only the first kind of image segmentation is considered.

Our study on unsupervised image segmentation was motivated by require-ments and constraints in the context of image retrieval by content [3,4]. Most approaches use the query-by-example principle, performing queries such as  X  X how me more images that look like this one X . However, the user is often more par-ticularly interested in specifying an o bject (or region) and in retrieving more images with similar objects (or regions), which is opposed to similar images as a whole. Our aim is to allow the user to perform a query on some parts (objects of interest) of an image. In this paper, we focus on the problem of clustering based segmentation of each image in the database to allow partial queries.
Though there have been various clustering methods, such as the EM algorithm [5] for maximum likelihood (ML) [6] and k-means algorithm [7], the number k of clusters in the data set is usually assume d to be pre-known. However, since the image databases for image retrieval are often huge, the prior setting of cluster number for each image is no longer feasible. Such requirement then motivates our interest to the idea of selecting clu ster number automatically before or dur-ing clustering. Actually, we can solve this model selection problem with some statistical criteria such as the minimum description length (MDL) [8] through implementing the EM algorithm [9], but the process of evaluating these criteria may incur a large computational cost. Some more efficient approaches such as rival penalized competitive learnin g (RPCL) [10] have also been proposed to make automatical model sel ection during clustering. Though great improvement can be made as compared with k-means algorithm, the segmentation results are not satisfactory and the object of interest may be merged with other regions, since the RPCL algorithm is sensitive to the rival learning rate.

Under regularization theory [11], we present an iterative algorithm for entropy regularized likelihood (ERL) learning [12,13] to solve such problems, through in-troducing entropy regularization into ML estimation on finite mixture model for clustering based unsupervised image segmentation. This kind of entropy reg-ularization [14] has already been successfully applied to parameter estimation on mixtures of experts for time series prediction and curve detection, and some promising results have been obtained due to automatic model selection for mix-tures of experts. In this paper, we further utilize entropy regularization to make model selection on finite mixture for unsupervised image segmentation, that is, to determine the number of regions of an image automatically.

Finally, we conducted image segmentation experiments to test our algorithm on the Corel image database used as benchmark in [15]. Several experiments have demonstrated that the iterative ERL learning algorithm can automatically select the number of regions for each image in the databases during parameter learning. Moreover, since the object of interest ev en can be successfully detected from the confusing background, the iterative ERL learning algorithm then performs much better than the MDL based EM (MDL-EM) algorithm and the RPCL algorithm with much less computational cost in the mean time. We consider the following finite mixture model for cluster analysis: where p ( x |  X  l )( l =1 , ..., k ) are densities from the same parametric family, and k is the number of mixture components.

Given a sample data set S = { x t } N t =1 generated from a finite mixture model with k  X  true clusters and k  X  k  X  , the negative log-likelihood function on the finite mixture model p ( x |  X  )isgivenby The well-known ML learning is just implemented by minimizing L (  X  ).
With the posterior probability that x t arises from the l -th component in the finite mixture we have the discrete Shannon entropy of these posterior probabilities for the sample x t which can be globally minimized by that is, the sample x t is classified into the l 0 -th cluster.
 When we consider the mean entropy over the sample set S: all the samples can be classified into some cluster determinedly by minimiz-ing E (  X  ), and some extra clusters are then discarded with mixing proportions reduced to zero.

Hence, the parameter learning on the finite mixture model p ( x |  X  )canthenbe implemented by minimizing the following entropy regularized likelihood function where  X &gt; 0 is the regularization factor. Here, E (  X  ) is the regularization term which determines the model complexity, and the mixture model can be made as simple as possible by minimizing E (  X  ). Moreover, L (  X  ) is the empirical error of learning on the data set S , and the ML learning by minimizing L (  X  )isonly a special case of the ERL learning with no regularization term. In this section, we apply the above ERL learning to unsupervised image seg-mentation via developing an iterative algorithm. For a N 1  X  N 2 color image to be segmented, we consider an 8-dimensional vector consisting of color, texture, and position features for each pixel just t he same as [9]. The thr ee color features are the coordinates in the L*a*b* color space, and we smooth these features of the image to avoid over-segmentation arising from local color variations due to texture. The three texture features are co ntrast, anisotropy, and polarity, which are extracted at an automati cally selected scale. The pos ition features are simply the (x, y) position of the pixel, and incl uding the position gen erally decreases over-segmentation and leads to smoother regions. Finally, we can get a sample set S of N = N 1  X  N 2 samples for each image in the database.

In the following, we only consider the well-known Gaussian mixture model for unsupervised image segmentation, that is, where n is the dimensionality of x ,and  X  l =( m l , X  l ) ,l =1 , ..., k are the mean vectors and covariance matrices of the Gaussian distributions.

We now derive an iterative algorithm to solve the minimum of H (  X  )asfol-lows. Firstly, we aim to make the above minimum problem without constraint conditions by implementing a substitution:  X  l = exp (  X  l ) /  X  X  X  &lt; X  are then led to the following series of equations: where  X  jl is the Kronecker function. Then, the solution of those equations can be given explicitly as follows:
These explicit expressions give us an iterative algorithm for minimum H (  X  ): during each iteration, we first update P and U according to (3) and (12), re-spectively, and then update  X  with newly estimated U according to (13) X (15). Hence, this iterative algorithm seems very similar to the EM algorithm on Gaus-sian mixture. Actually, the iterative ERL learning algorithm just degrades into the EM algorithm when the regularization factor  X  is reduced to zero. However, it is different from the EM algorithm in that the mechanism of entropy regular-ization is implemented on the mixing proportions during the iterations, which leads to the automatic model selection.

Once the iterative ERL learning algor ithm has converged to a reasonable solution  X   X  , all the samples (i.e., pixels) from a color image can then be divided into k clusters or regions by Due to the regularization mechanism introduced in the iteration process, some clusters may be forced to have no samples and then the desired k  X  ,thatis,the true number of regions in an image, can be selected automatically.

As compared with the gradient imple mentations for the ERL learning in [12,13], the above iterative algorithm has the following two advantages. On one hand, there is no need to select so many parameters for the iterative algorithm, which makes the implementation much more easy. In fact, for the gradient algo-rithm, we must select an appropriate learning rate on a sample data set, which is generally a difficult task. On the other hand, just like the EM algorithm, the iterative algorithm is generally faster than the gradient algorithm, which is specially appropriate for image processing.

Though we originally introduce entropy regularization into the maximum like-lihood estimation (implemented by EM algorithm) for automatic model selection on the Gaussian mixture, it can also be observed that the minimization of the ERL function H (  X  ) is robust with respect to initialization and the drawbacks of EM algorithm may be avoided. That is, when local minima of the negative likelihood L (  X  ) arise during minimizing the ERL function, the average entropy E (  X  ) may still keep large and we can then g o across these local minima. Hence, some better segmentation results may be obtained by minimum H (  X  ).
For example, the standard EM algorithm may not escape one type of local minima of the negative likelihood when two or more components in the Gaussian mixture have similar parameters, and then share the same data. As for image segmentation, it means that the object of interest in an image may be split into two or more regions. However, the iterative ERL learning algorithm can promote the competition among these components by minimum E (  X  k )asshownin[12], and then only one of them will  X  X in X  and the other will be discarded. We further applied the iterative ERL learning algorithm to unsupervised image segmentation, and also made comparison with MDL-EM algorithm and RPCL algorithm on the Corel image database used as benchmark in [15]. We carried out a large number of trials on the database, and only eight images were randomly selected (see Fig. 1(a) and Fig. 2(a)) to show the segmentation results.
In all the segmentation experiments, the parameters of the three learning al-gorithms can be set as follows. The iterative ERL learning algorithm is always implemented with k  X  k  X  and  X   X  [0 . 2 , 0 . 5], while the centers and widths of the Gaussian units are initialized by some clustering algorithms such as the k-means algorithm. In the segmentation, we actually set k a relatively larger value (e.g., k = 6), and select  X  in the empirical range which is obtained by a large number of segmentation trials. Since we can not adaptively select this model selection parameter for each image in the database, we simply set  X  =0 . 4 for all the images uniformly. Moreover, the ERL learning is always stopped RPCL algorithm also fixes k at 6, while the MDL-EM algorithm selects k in the range [2 , 5]. Finally, the learning rates for winner and rival units during RPCL clustering are set as  X  w =0 . 05 and  X  r =0 . 005, respectively.

Once a segmentation model is selected after the clustering is stopped, the next step is to perform spatial grouping of those pixels belonging to the same color/texture cluster. We first produce a k  X  -level image (i.e., the color vision of segmentation results for each algorithm in Fig. 1 and Fig. 2) which encodes pixel-cluster memberships by replacing each pixel with the label of the cluster for which it attains the highest likelih ood, and then run a connected-components algorithm to find image regions (i.e., the gray vision of segmentation results for each algorithm in Fig. 1 and Fig. 2). Note that there may be more than k  X  of these regions for each image. Finally, to enforce a minimal amount of spatial smoothness in the final segmentation, we apply a 3  X  3 maximum-vote filter to the output of each clustering algorithm. This filter assigns its output value as the value that occurs most often in the 3  X  3 window.

From the segmentation results shown in Fig. 1 and Fig. 2, we can find that the iterative ERL learning algorithm su ccessfully detects the object of interest from the confusing background and performs generally better than the other two algorithms. That is, the MDL-EM algorithm may converge at local minima and the object of interest may be split into two regions (see brown bear and horse) when two or more Gaussian centers are i nitialized in the region of it, while the RPCL algorithm is sensitive to the rival learning rate  X  r and the object of interest may be merged with other regions (see brown bear and sparrow).
Moreover, the average seconds per im age taken by the three learning algo-rithms for segmentation of the eight ra ndomly selected images are also listed in Table 1. Note that we just recorded the computational cost by the clustering for grouping pixels into regions, and the postprocessing of the segmentation results such as searching connected components is not included. In all the segmenta-tions, we process the images by the three learning algorithms offline on a 3.0GHz Pentium IV computer. As expected, the iterative ERL learning algorithm runs much faster than the MDL-EM algorithm since the process of evaluating the MDL criterion incurs a larger computational cost. As compared with the RPCL algorithm, the iterative ERL learning algorithm also keeps more efficient to make unsupervised image segmentation.

The further experiments on the other images in the database have also been made successfully for segmentation in the similar cases. Actually, in many ex-periments, the iterative ERL learning algorithm can automatically detect the number of regions for a color image in the database and maintain the edges of the object of interest well even in the confusing background. Note that the it-erative ERL learning algorithm is just compared with the MDL-EM algorithm, and the comparison results should be the same when some other model selec-tion criteria are taken into acco unt to determine the model scale k for the EM algorithm. Additionally, once the color/texture features are assigned to those connected components of the color images, we can then implement region-based image retrieval on the Corel image database. In the future work, we will evaluate the iterative ERL learning algorithm using the precision-recall measure in the context of image retrieval. We have proposed an iterative ERL learning algorithm for unsupervised image segmentation with application to content based image retrieval. Through intro-ducing a mechanism of entropy regularization into the likelihood learning on the finite mixture model, the iterative ERL learning algorithm can make model selection automatically with a good estimation of the true parameters in the mixture. When applied to unsupervised image segmentation, the iterative ERL learning algorithm can even successfully detect the object of interest from the confusing background, and then performs much better than the MDL based EM (MDL-EM) algorithm and the RPCL algorithm with much less computational cost in the mean time.

