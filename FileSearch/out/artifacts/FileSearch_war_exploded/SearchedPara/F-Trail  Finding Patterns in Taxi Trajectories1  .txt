 What patterns can we derive, using trajectory data from a whole fleet of taxis? What is the normal pattern of activity, and which (if any) outliers exist? We seek to discover such patterns, so that we can spot anomalies, and help the taxi operating company understand general trends, with the ultimate goals of maximizing fuel efficiency, profits, and passenger satisfaction. Trajectory anal-ysis has attracted a lot of interest, including trajectories of (migrating) ani-mals [21,8,7,10], of hurricane paths [4,2], as well as from an indexing point of view [6,16]. The latter studied indexing, but not pattern discovery; among the former, the emphasis was on clustering and distance functions on trajectories. Motivation and Challenges. Uncovering rules governing collective taxi be-havior is a challenging task because of the myriad factors that influence an individual X  X  decision to take a particular action. In this work, we study 10,000 trajectories generated by anonymous taxi drivers, with the aim of measuring social and economic activity. Intuitively, the task of this paper is as follows: status (i.e.,  X  X ull X  or  X  X mpty X ), find the general trends of the fleet of taxis, and spot outliers.

Here, we present a novel method, F-Trail , for finding meaningful patterns and anomalies in a huge number of trajectories. Our approach can scalably and automatically identify typical pattern s of taxi behavior, and actually  X  X ee X  such patterns from the point of view of topology. More specifically, we propose using the fractal dimension as a characteristic for trend a nalysis and extreme detection.
Figure 1 shows a snapshot of our discoveries: Namely, taxi drivers follow linear-like paths when  X  X ull X ; and convoluted,  X  X andering X  paths when empty during the day, but clearly different behavior w hen  X  X mpty X  at night. More specifically Figure 1(a) is the probability density function (PDF) of the  X  X ractal dimension X  ( FD ) of all trajectories with a passenger (say,  X  X ull taxi trails X ). Since FD  X  1, for any time of the day, we conclude that, when  X  X ull X , taxi drivers follow linear, piece-wise linear, and in general, smooth p aths. Conversely, Figure 1(b) shows that, when  X  X mpty X , they follow short, wandering paths, creating a bursty-like pattern (see blue dots in Figure 2(b)), with a much lower FD .
 The only exception is during the night (solid blue and dotted blue curves in Figure 1(b)): the drivers abandon their  X  X andering X  behavior, since they don X  X  expect to find nearby customers, and instead go almost straight to taxi plazas or the airport (green dot in Figures 2(a-b)). Notice the linear-like blue paths ( X  X mpty trails X ) in Figure 2(a). We present several more observations, later. Contributions. In this paper we propose a new approach, namely F-Trail , which includes fast and effective techniques that can learn the key trends of a large collection of taxi trails. The contributions of this paper are as follows: 1. Effective : We apply F-Trail to a real trajectory dataset, which allows us to 2. Adaptive: F-Trail describes the common behavior and anomalies of taxi 3. Scalable : The careful design of F-Trail makes it linear on the number of Outline. The rest of the paper is organized as follows: Section 2 discusses related work. Section 3 introduces our approach and describes how to analyze individual taxi behavior. We conclude in Section 4. The previous work on mining trajectory data can be grouped into the following categories: (1) design of distance and similarity scores and (2) indexing methods for spatial-temporal databases. For trajectory similarity functions, Vlachos et al. [21] use the longest common sub-sequ ence, while [8,7] use minimal descrip-tion language. Given a similarity score, [21] study the trajectories of marine mammals, while [8,7] use it to find clusters and outlier trajectories. Gaffney et al. [4,2] use generative models to group the trajectories of moving objects such as hurricanes. Giannotti et al. [5] study the trajectory pattern mining problem. Very recently, Yuan et al. [24,23,11] study a large number of taxi trajectories in Beijing, and present new and sophisticated models to find the different functional regions, and optimal driving route.

Remotely related is the work on indexing and searching moving objects: The work in [16] builds index on moving both spatial and temporal dimensions with pre-aggregation to support OLAP operations. In related work [6,3] also propose various solutions for answering region retrieval queries, predicting the past and future positions. Similarity search and pattern discovery in time sequences have also attracted huge interest [13,18,15].

One of the contributions of F-Trail is that it uses fractal concepts to spot patterns in trajectories. Fractals and sel f-similarity have been used in numerous settings, like financial time series analysis [12], modeling ethernet traffic [22], social network analysis [14,9], and numerous other settings (see, e.g., [19,17]). Given several thousands of trajectorie s, we need to find commonalities and ex-tremes. What do the trips have in common? Can we extract features from such trajectories, to help us understand the dataset? We propose extracting the fractal dimension of every such trip. The fractal dimension has several desirable prop-erties: (a) it is invariant to affine transformations, (b) it is fast to compute, and (c) it captures the complexity of the trajectory. Next we give brief background and the intuition behind our proposed solution. 3.1 Preliminaries (Hausdorff) Fractal Dimension. There exist many fractal dimensions (Haus-dorff, Minkowski, correlation, information, etc). Among them, we use only the first one, which is formally defined as follows:
The (Hausdorff) fractal dimension , or simply the fractal dimension for a range of scales ( r 1 ,r 2 ) and for a given, self-similar, point-set in an n -dimensional ad-dress space, is defined as the exponent of the law [19]: where ( r 1 ,r 2 ) is a suitable range of scales, and N ( r ) is the number of non-empty cells, when we impose a grid of side r , on our dataset. Intuitively, if we plot N ( r )vs r in log-log scales, the plot will be linear for the range of scales ( r 1 ,r 2 ). We refer to such plots as Hausdorff plots , and we report the slope and constant (= intercept), for each taxi trajectory.
 Computational Complexity. Linear. More specifically, we have: Lemma 1. The computation time for the fractal dimension is O ( N ) , that is, linear on the number of points N .
 Proof: Using the so-called box-counting method [19], we only need to go over the data points a few times. QED Basic Properties for Trajectory Data. Our dataset consists of 10,000 trajec-tories taken from anonymized taxi drivers in the large cit y, where each trajectory corresponds to the trail of each taxi for an entire day. The dataset has the fol-lowing attributes: GPS coordinates (i.e., longitude, latitude), a timestamp, and the passenger status (i.e.,  X  X ull X  or  X  X mpty X ).

Figure 3 shows an example of a taxi trajectory of an entire day. The horizon-tal and vertical axes show the longitude and latitude of the GPS points respec-tively, where red and blue lines show the trails of the taxi for each status (full and empty). Note that we anonymize latitude and longitude information due to privacy concerns. The red and blue dots indicate the locations of  X  X ick-up X  and  X  X rop-off X  points. The green dot at (0.3391, 0.1168) is the location of the interna-tional airport, and (0.15 -0.25, 0.06) is a downtown area. In this figure, we can see that most trajectory lines are between th e airport and downtown via highways.
Here we define some terminology. We will refer to each set of trail points with passengers (shown as red lines in Figure 3), as a  X  X ull X  trail ,andrefertoasetof points without passengers (i.e., empty) as an  X  X mpty X  trail . That is, the entire trajectory consists of a combination of  X  X ull X  and  X  X mpty X  trails. 3.2 Intuition -Fractal Dimension as a Feature Our goal is to analyze the trajectories, an d specifically to characterize the under-lying behavior of taxi drivers, and gain insight into how and why the observed characteristics arise.

The taxi drivers have social and economic priorities and follow their own strategies for success. We want to extract detailed information on their behavior, especially regarding their mobility patterns. There exist numerous time-series analysis methods, including FFT and wavelets, but they depend strongly on the locations of trajectories, which makes it hard to find the economic strategies and social behavior. How can we characterize the shape of trajectories, while ignoring their location? Approach 1. We propose to use the fractal dimension of each taxi trajectory, as a feature for finding patterns and groups.
 Several real datasets are self-similar, and thus have an intrinsic, or fractal dimen-sion: the peripheries of clouds and rain patches ( D =1 . 3), coast-lines ( D =1 . 1 to 1 . 58 for Norway), and many more [1]. Are our taxi trajectories self-similar? The answer is  X  X es X , for the vast majority of them. The ones that don X  X , are either too short, or deserve further examinati on, being the exceptions. Next we give the intuition and necessary definitions.

Figure 4 shows some trajectories as well as the tools to measure their fractal dimension: the odd columns (i.e., Figure (a,c,e)) are synthetic, and the even ones (i.e., Figure (b,d)) are real trajectories. Intuitively, a set of points (like our taxis X  (x,y) coordinates per unit time) is a fractal if it exhibits self-similarity over all scales. The way to interpret the value of the fractal dimension is as follows:  X  FD = 1: This happens when the trajectory has iso-spaced points, along a  X  FD &gt; 1: This happens when the taxi does twists and turns, like the real  X  FD&lt; 1, This happens when the taxi does many stops, like the real trajectory  X  FD = 0, if the taxi is completely static, in which case the trajectory reduces The two extremes in Figure 4(a,e) corresp ond to the so-called  X  X antor dust X , and the  X  X ilbert curve X . The former is deriv ed from a line segment, by recursively deleting the middle third, and has fractal dimension log(2) / log(3)  X  0 . 63. The latter is a spacefillingcurve , with fractal dimension = 2, covering the whole 2-d space, in the limit.

Thus far we have introduced the fractal dimension for individual trajectory analysis in order to understand the taxi behavior. However, the behavior of each taxi could vary in a month, a week , or even in a single day since the dynamic strategy typically beats the static strategy. Actually, each empty taxi exhibits distinct behavior in different time ranges according to the distribution of passengers (see Figure 1).
 Approach 2. We propose to apply a short-window approach to the fractal di-mension, which is more flexible for trajectory analysis.
 Instead of handling the entire trajector y, we locally analyze the fractal dimen-sion of each snapshots to obtain a better understanding of time-varying social behavior. 3.3 F-Trail Analysis We now introduce our approach and describe how to analyze individual taxi behavior.
 Fractal Trajectories -Are There Clusters? For a few trajectories, a human could eye-ball them, and derive the above patterns. But, how can we accomplish this automatically for thousands of trajectories? Our first idea is to compute the fractal dimensions for individual trajectory analysis. We begin by investigating the sociability of taxi movement by measuring the fractal dimensions of trajec-tories of two statuses (i.e.,  X  X ull X  and  X  X mpty X  trails). We compute the fractal dimension of each trail, which help us to understand how the taxi drivers find the passengers and how they pick them up.

Do taxi drivers take their passengers over direct paths? Are their trajectories different, when they are empty, looking for passengers? It turns out that F-Trail can answer both questions, and the answers are  X  X es X , with a few twists. Let X  X  see the details.

Figure 5 shows the fractal dimension ( FD ) versus the intercept of full and empty trails. For most  X  X ull X  trails, the fractal dimension is between 1 . 1  X  1 . 3, while for  X  X mpty X  trails, it is between 0 . 8  X  1 . 1. Here, FD = 0 is the burstiest (i.e., static taxi), FD = 1 corresponds to a taxi moving uniformly on a line or smooth curve, and FD = 2 (maximum) would be for a taxi that is uniformly distributed over the whole address space. This figure shows remarkable differences in the behaviors of each status. Thus, we have: Observation 1 (Typical behavior). Typical taxi behavior over an entire day is to have fractal dimensions between FD =1 . 1  X  1 . 3 for full trails and FD = 0 . 8  X  1 . 1 for empty trails.
 Notice that the above observation is invariant to affine transformations. More-over, it helps us spot a clear distinctio n between  X  X ull X  and  X  X mpty X  trail sets (see Figure 5, FD vs. intercept plot). This is because, unlike many full taxis that head straight for their destinations, empty taxis frequently do many stops and turns, to find a new passenger, which leads to low fractal dimension.

A further observation that we can derive from the fractal dimensions of tra-jectories shown in the same figure is that there are several extremes for each of the full and empty trail sets. For instance, the trajectory #7575 is the one extreme, due to its low intercept, which i mplies that the particular trajectory covers a very small area. In fact, the tax i driver adopted a different strategy and focused only on a highly-populated area (see Figure 6(a)). The trajectory #9710 is an extreme example of the empty status and shows a high fractal dimension. Actually the trajectory includes many line-like trips (long blue segments), for his/her long  X  X assenger search X  (see Fig ure 6(b)), in contrast to the majority of taxi drivers, who wander locally, looking for their next passenger.

Thus we have almost answered the first q uestion: taxis seem take their pas-sengers to smooth, line-like curves. The n ext natural question is: are these the shortest paths? We address this question next.
 Trip Length vs. Crow X  X  Flight -Any Waste? So far we have examined the dynamics of the taxi behaviors at an individual level, and proposed a simple model capturing the fractal dimensions. To check whether  X  X ull X  taxis actually use the shortest path, we do another scatter-plot: For every trip, we plot the  X  X row X  X  flight X  length (= Euclidean distance between pick-up point and drop-off point) and the reported length (= sum of lengths between successive location snapshots). Figure 7 shows these sca tter plots, where every dot is a trip. Of course, nothing is below the diagonal, and there is heavy over-plotting. The bottom row contains the density plots.

Notice that the  X  X ull X  trips are almost on the diagonal (slope=1, intercept  X  0), which means that the taxi drivers are efficient, in the shortest path sense. Observation 2 (Taxi driver efficiency). Most taxi drivers take their passen-gers via the shortest path (or very close to it).
 By contrast, when  X  X mpty X , the slope is still 1, but the intercept is higher (0.16, in log-log scales), which means that the drivers  X  X ander X , with many turns and returns, until they find a new passenger. Mathematically: l =10 0 . 16  X  s 1 or where l is the reported length of the trip, and s is the straight ( X  X row X  X  flight X ) length.
 Observation 3 (40 percent wandering). Drivers on  X  X mpty X  status have more convoluted trajectories than on  X  X ull X  status, driving about 40% longer than necessary.
 This observation agrees with the intuition: Drop-offs are typically in residential areas, with many, narrow roads, and the taxi drivers have to turn and loop, until they find another passenger. Thus, the trajectory is more convoluted. In contrast,  X  X ull X  taxis typically go on highways, which are straight or with a few smooth turns and thus the trajectories are simpler and more efficient.
 Fractal Dimension -Any Changes with the Hour of the Day? So far we have seen that most empty taxis are likely to pick up new passengers geo-graphically close to the last drop-off point, and thus we would expect that this strategy to minimize their effort (e.g., ma ximize fuel efficiency, profits). However, as described in the introduction, this is not always the case. We thus introduce a short-window approach to the fractal dimension (FD), which enable us to char-acterize the taxi behavior for each time interval (i.e., snapshot). Specifically, instead of analyzing all the (x,y) snapshots of, say, taxi i , we study separately the snapshots at 0-3am, 3-6am, etc.

As we have seen in Figure 1, the behavior of  X  X ull X  taxis is the same for all hours of the day, but the  X  X mpty X  ones vary in a very interesting way: Observation 4 ( X  X mpty X  at night: differ). There is a clearly distinct pattern of empty taxis at night time: instead of convoluted,  X  X andering X  paths, drivers choose line-like paths.
 In retrospect, the observation above makes sense: During the night (0-6am), the drop-off place is probably at a residential area, and there is slim chance to find another passenger nearby. Thus, taxi drivers choose to drive straight to places with higher chances of demand (airport, down-town, etc).
 Power Law in the Trip-Length Distribution? Whenever there is self-similarity and fractals, we often have power laws and scale-free distributions. Does this hold for the trip-lengths, in our case? Surprisingly, it turns out that the double Pareto lognormal (DPLN) distribution yields good fits to our data.
The DPLN distribution generalizes the power-law and lognormal distribu-tions, and is expressed by the following equation, where  X  and  X  c are the cdf and complementary cdf of N (0 , 1), and for further details (e.g., parameters  X  ,  X  ,  X  ,  X  ), see [20].
 Figure 8 shows the DPLN fitting results of trip-length vs count distribution. The figure shows the PDF for the (a) full (red circles) and (b) empty (blue circles) rides, in log-log scales. There is a power-law behavior up to the  X  X nee X  (at about 20 km), and then a sharp (power-law) drop after that. Notice that the knee is at roughly the radius of the city of study. Consequently, we have: Observation 5 (Trip length: DPLN behavior). The trip length distribution is skewed, for both  X  X ull X  and  X  X mpty X  rides, with a power-law that has a  X  X nee X  at  X  20 km. This is exactly the so-called  X  X oubly Pareto lognormal X  (DPLN).
 From Figure 8 (a), we observe two types of customers: the  X  X elow-city-radius X  ones, that take short trips, or airport-to-downtown; and the (much more rare)  X  X bove-city-radius X  ones, that maybe hire a taxi for sightseeing, or for several days. Such results (i.e., trip-length DPLN fitting) could be used to analyze the tradeoff between the cost of finding passengers (Figure 8 (b)) and the fares received from them (Figure 8 (a)) to desig n pricing structure, which would max-imize the total revenue. We investigated patterns of human mobility on a large collection of taxi tra-jectories, and proposed a new method, F-Trail , to find meaningful patterns and anomalies. Our approach is (a) linear on the input size, and (b) able to spot meaningful general trends, as well as outliers. In more detail, our main discoveries are as follows:  X  Typical for taxi trajectories is to hav e fractal dimension between 1.1 to 1.3  X  Most surprisingly, we found that most taxi drivers change their strategies in Acknowledgement. We would like to thank the taxi company that provides us the data. This work was sponsored by the Defense Threat Reduction Agency and was accomplished under contract No. HDTRA1-10-1-0120 and by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-09-2-0053 .
 Recently, concerns over global climate changes have motivated significant efforts in reducing the emissions of CO 2 and other GHGs (greenhouse gases). Many researchers focus on the reduction of electricity usage in the residential sector because it is a sig-nificant contributor of greenhouse gas emissions. Nevertheless, electricity conserva-tion is an arduous task for the residential users due to the lack of detailed electricity usage. If an electricity bill is high, we usually can tell that it is expensive rather than "why" it is expensive. We argue that if representative patterns of appliance electricity usages are available, residents can adapt their appliance usage behaviors to conserve the energy effectively. 
Due to the great advent of sensor technology, the electricity usage data of all ap-pliances in a house can be collected easily. In particular, power meters are deployed to collect appliance usage log data. With the appliance usage log data, one could vi-sualize how the appliances are used. With a huge amount of appliance usage log data collected, it is necessary to propose mining algorithms to discover appliance usage patterns that capture representative usage behavior of appliances. Appliance usage patterns are able to help users understand how they use their appliances and further-more, appliance usage patterns could be used to detect abnormal usages of appliances or utilize to design an intelligent control for appliances. 
However, extracting meaningful usage information is a complex issue. Our appliance usage behaviors usually vary at different time and season, e.g., many behaviors of the same appliances in summer and in winter are totally different. For example, the air-conditioner may be used everyday in summers, but hardly turned on in other seasons. Many appliances may also have unique usage patterns, e.g., some appliances are sea-sonal-type and some ones belong to daily-type. For example, while the air-conditioner usually is turned on and off frequently everyday and thus belongs to daily-type. Ob-viously, how to mine representative usage patterns which can describe diversified ap-pliance usage effectively and efficiently is a challenging problem. 
Previous researches of usage patterns mainly focused on energy disaggregation [2, 3, 4, 8, 9, 12] and appliance recognition [1, 5, 6, 7, 10, 11]. To the best of our knowledge, very few studies facilitate the energy saving with the discovery and utili-zation of appliance usage patterns. The contributions of this paper are: (1) from ana-lyzing appliance usage data, two types of usage patterns, daily behavior-based usage pattern (DBUP) and clustered-based statistical usage pattern (CSUP), are proposed to represent and describe the complex behaviors of appliance usage effectively; (2) two mining algorithms are developed to discover appliance usage patterns efficiently by employing some effective pruning strategies to reduce the search space; (3) the pro-posed methods have been applied to a real dataset to validate its practicability and show the support for residents to adapt their appliance usage behaviors for conserve the energy. 
The rest of the paper is organized as follows. Section 2 and Section 3 provide the Finally, Section 6 concludes the paper. In this section, we discuss some previous works utilized usage patterns for energy such as number of O N-OFF switches, to disaggregate the whole-house electricity consumption into a number of major end-uses. Suzuki et al. [12] used a new NIALM technique based on integer programming to disaggregate residential power use. Lin et al. [9] used a dynamic Bayesian network and filter to disaggregate the data online. Kim et al. [8] investigated the effectiveness of several unsupervised disaggregation methods on low frequency power measurements collected in real homes. They also proposed a usage pattern which consists of on-duration distribution and dependency between appliances. Goncalves et al. [4] explored an unsupervised approach to deter-mine the number of appliances in the household, including their power consumption from smart meters into specific usage that associated with human activities. They proposed a novel statistical framework for disaggregation on coarse granular smart meter readings by modeling fixture characteristic, household behavior, and activity correlations. 
Prudenzi [11] utilized an artificial neural network based procedure for identifying the electrical signatures of residential appliances. Ito et al. [5] extracted features from the current (e.g., amplitude, form, timing) to develop appliance signatures. For ap-pliance recognition, Kato et al. [6] u sed Principal Component Analysis to ex tract features from electric signals and classified them by Support Vector Machine. Aritoni et al. [1] proposed a software prototype which can be used to understand the household ap-pliances behavior. Some of these works and the characteristics of workable solutions were discussed by Matthews et al. [10]. Definition 1 (usage-point and usage-point log). Let S = { ON , OFF } be the set of states. Without loss of generality, we define a set of uniformly spaced time points based on the natural number N . We say the pair ( s i , t i )  X  S  X  N is an usage-point, Definition 2 ( ON -switch and OFF -switch). Given a dail y usage-point sequence D ( s ( s , t ik ), where 1  X  k  X  m , s ik = OFF and s i ( k -1) = ON . Definition 3 (usage-interval and usage-interval log). Given a daily usage-point of i.e., I i =  X  (1, t i1 , d i1 ), (2, t i2 , d i2 ), ..., (  X  , t i  X  , d i  X  )  X  . We use Fig. 1 as an example to explain Definition 2 and Definition 3. Given a daily usage-point sequence of light as shown in Fig. 1, obviously, the first ON -switch and OFF -switch are ( ON , 6:00) and ( OFF , 8:00), respectively; the second ON -switch and OFF -switch are ( ON , 14:00) and ( OFF , 20:00), respectively. Hence, the usage interval sequence of 14 April, 2013 is  X  (1, 6:00, 2 hrs), (2, 14:00, 6 hrs)  X  . In this section, we will define two types of appliance usage patterns, daily behavior-based usage pattern ( DBUP ) and clustered-based statistical usage pattern ( CSUP ). For each type of appliance usage patterns, we will first define its representation and then propose our algorithms to mine appliance usage patterns. 4.1 Algorithm of Mining Daily Behavior-Based Usage Pattern DBUP mainly focuses on extracting representative behaviors of daily appliance usage. The approach is shown in Fig. 2. For ex tracting DBUP from an usage point log, first, we treat a dail y usage-point sequence as a daily behavior of an appliance. Then, similar daily usage behaviors are clustered in the same group. A hierarchical cluster method is proposed to group similar behaviors together. Finally, we evaluate the centroid behaviors of those clusters and output them as the daily behavior-based usage patterns. 
An efficient method, DBUP-Miner, is developed for mining daily behavior-based usage patterns. The pseudo code is shown in Algorithm 1. DBUP-Miner first calls usage-point log sub-procedure Hierarchical_Clustering to en umerate the clusters of all daily usage-point sequences (line 1, algorithm 1). Hierarchical_Clustering is designed to cluster the similar daily usage behavior. At first, each daily usage-point sequence is consi-dered as a cluster (line 4, algorithm 1). To cluster usage-point sequences (i.e., 0 and 1 time series sequences), we need to measure the similarity among all input time series sequences. In the clustering process, each piece of time series data can be viewed as a point located in an abstract space, and the distances between these points are usually between the sequences of time series data as points in the clustering space. One im-considered, i.e., the range of local time shift should be limited. In this paper, we adopt EDR as the similarity function that can deal with local time shifting under a time shifting constrain, and can deal with noises without being compromised with too much amplitude shifting. 
For each two clusters, if their distance is smaller than or equal to the threshold  X  and is minimum, DBUP-Miner merges two clusters to a new cluster and updates the similarity between new cluster and other clusters (Lines 5-9, algorithm 1). The setting of threshold  X  is usually heuristic. We will discuss in detail in experimental results and show the effect upon the mining results. Finally, after clustering, we evaluate the (lines 2-3, algorithm 1). We compute mean of each cluster as the representative cen-troid. With DBUP-Miner, we can obtain the daily behavior-based usage patterns efficiently. 4.2 Algorithm of Mining Cluster-Based Statistical Usage Pattern For an appliance, the starting time and the duration of the j th ON-OFF switch in each day are also informative. We propose to employ CSUP, to extract the representative usage-interval from the daily log. CSUP clusters each starting time and duration of the j th usage-interval on an appliance in a given usage-interval log. Notice that, dif-quences instead of usage-point sequence, since we want to discover some general behavior to express the starting time and duration of each j th usage-interval. CSUP Finally, we evaluate the centroid usage-intervals of the clusters and output them as a clustered-based statistical usage pattern. The approach of mining CSUP is as shown in Fig. 3. 
Algorithm 2 illustrates the main framework which includes the necessary processing steps of CSUP-Miner. Given usage-interval log, minimum support  X  , CSUP Miner first accumulates the occurrences of the j th usage-interval in each usage-interval sequence and store it into a matrix number[ j ] ( j =1,2,...) (lines 1-3, algorithm 2). Then, we check whether the count in number[ j ] is larger than or equal to minimum since it is nonrepresentative to describe a usage behavior (line 4, algorithm 2). For the frequent j th usage-interval, CSUP-Miner ca lls the sub-procedure Hierarchic-al_Clustering to cluster the similar usage-intervals and stores the result of the cluster-ing into CG j (lines 5-6, algorithm 2). 
The clustering method is similar to the one used in DBUP-Miner. The hierarchical clustering method utilizes two attributes, starting time and duration, for clustering all j algorithm 2) and we group two closest usage-intervals which have the closest distance and the distance is smaller than or equal to threshold  X  (lines 12-13, algorithm 2). The the similarity between new cluster and other clusters (line 14, algorithm 2) and recur-sively run above steps until the distances among all clusters are not smaller than or equal to  X  (lines 10-15, algorithm 2) . 
To find the representative usage-interval to describe the j th time for turning on the appliance, we select the maximum cluster from CG j and evaluate the centroid (mean starting time and mean duration) of all usage-intervals in this cluster to represent the j th usage-interval (line 7, algorithm 2). Finally, we output all representative usage-intervals as the clustered-based statistical usage patterns (line 8, algorithm 2). All algorithms were implemented in C++ language and tested on a Pentium D 3.0 GHz with 2 GB of main memory running Windows XP system. This performance smart home environment, which consists six appliances (microwave, dish-washer, wash-dryer, light, oven and air-conditioner) for 45 days. We transfer the power con-22,550 usage points. To show the practicability of proposed two types of usage pat-terns, we perform two kinds of experiments. First, we extract DBUP and CSUP from a real dataset. From discovered patterns, the characteristic of each appliance is elabo-discussed in detail. We also indicate the influence of the minimum threshold. 5.1 DBUP and CSUP from Real Dataset Fig. 4 shows the discovered daily behavior-based usage pattern of six appliances with similarity threshold  X  = 800. The setting of threshold  X  is set empirically which will be discussed in details in next section. The x axis is daily time (from 00:00:00 to 23:59:59) and the y axis is the states of ON and OFF . The curves with different colors represent different daily behaviors. DBUP reveals the usage information of an ap-pliance, such as how many times the appliance is turned on, the starting time of usage, and duration of usage. For example, Fig. 4(a) shows the DBUP of a microwave which exhibits eight representative behaviors (different colors of curve). The third behavior (black curve) describes three times for turning on the microwave. The first turn-on time is about 08:45 and the duration is about 50 seconds; the second turn-on time is about 19:00 and the duration is also about 2 minutes; the last turn-on time is about 19:30 and the duration is about 45 seconds. 
We also can observe that different appliance has different usage characteristic, i.e., different distribution of using time and duration of turn-on time. For example, in Fig. 4(a), the microwave, in general, is used evenly in a day and has short usage duration; however, oven usually is turned on at a fixed time and has longer usage duration as shown in Fig. 4(b). Hence, with DBUP, residents can know their representative beha-viors of daily appliance usage. 
Table 1 l ists the discovered clustered-based statistical usage pattern of s ix ap-pliances (with minimum support  X  = 10%). The setting of threshold  X  will be discussed in next section. For an appliance, CSUP can provide residents three infor-mation, how many times the appliance is turned on, mean of starting time, and mean of the usage duration. We use the microwave in Table 1 for discussion. There are six usage-intervals for the microwave. The mean starting time and mean duration of first usage interval are 02:57:37 and 1.2 minutes, respectively. In other words, it expresses that residents usually use the microwave six times in a day and turn on the microwave at about 02:57 and use it about 1.2 minutes in average at first usage. 
Different appliances have different usage frequencies and turn-on times. For ex-ample, as shown in Table 1, the microwave usually is turned on/off six times a day, but the dish washer usually is used only once a day. The usage durations of two ap-pliances are also different, e.g., oven usuall y is used for a long time due to the nature of its usage. From the discovered CSUP, residents can know, in general, how many times they use an appliance in one day, and each usage exists for how long. In sum-mary, DBUP and CSUP provide residents their representative patterns of appliance usage. With this useful information, resident may make appropriate adaption for electricity conservation more easily. 5.2 The Influence of Minimum Thresholds on Mining Results In the following experiments, we examine the effects of the threshold setting. Appro-priate similarity threshold  X  and minimum support  X  are very critical f or mining DBUP and CSUP. Fig. 5 and Fig. 6 show the number of generated daily behavior-Section 4.1, DBUP Miner uses EDR as the similarity function for hierarchical cluster-ing. EDR computes the total edit distances between two input usage-point sequences (i.e., 0 and 1 time series sequences); and the similarity threshold decide how closed the sequences are grouped into the same cluster. Obviously, the smaller the similarity threshold, the larger the number of DBUP patterns. As shown in Fig 5, when similari-ty threshold is 400, 62 DBUPs are extracted from the collected real dataset. As can be seen from Fig. 6, we can obtain an amount of usage patterns for all appliances with  X  = 400. When the similarity threshold is greater than 1,600, most of the generated usage patterns are with length one or two. 
Finally, we discuss the relation between the length of generated CSUP and the minimum support  X  . In Fig. 7, we show the length of usage-interval sequence (CSUP) on our real dataset with varying minimum support thresholds which is from 10% to 90%. Obviously, when the minimum support value increases, the length of generated patterns decreases. As shown in the figure, we can generate longer CSUP, especially for microwave and light, if the minimum support  X  is larger than 30%. However, when the minimum support reaches to 50%, some appliances do not have any CSUP patterns (dish-washer, air-conditioner, wash-dryer and oven). Consequently, from the experiments, we can observe that the threshold setting is very critical for the process of mining DBUP and CSUP. Recently, considerable concern has arisen ov er the electricity conservation due to the issue of greenhouse gas emissions. If representative behaviors of appliance usages are available, residents may adapt th eir applia nce usage behaviors to con serve energy effectively. However, how to extract representative usage patterns which can describe diversified appliance usage effectively and efficiently is a challenging issue. In this paper, from analyzing appliance usage data, two types of usage patterns, daily beha-vior-based usage pattern (DBUP) and clustered-based statistical usage pattern (CSUP), are proposed to represent and describe the complex behaviors of appliance usage. The mining algorithms also have b een developed to discover DBUP and CSUP efficiently. Finally, the proposed methods have been applied to real dataset to validate the practicability and showed the support for residents to adapt their appliance usage behaviors for electricity conservation. Acknowledgement. Wen-Chih Peng was supported in part by the National Science Council, Project No. 100-2218-E-009-016-MY3 and 100-2218-E-009-013-MY3, by Taiwan MoE ATU Program, by D-Link and by Microsoft. Stress is part of everyday life and it has been widely accepted that stress which leads people and society. The term, stress , was coined by Hans Selye. He defined it as  X  X he non-specific response of the body to any demand for change X  [1]. Stress is the body X  X  reaction or response to the imbalance caused between demands and resources availa-ble to a person. Stress is seen as a natural alarm, resistance and exhaustion [2] system for the body to prepare for a fight or flight response to protect the body from threats and changes. When experienced for longer periods of time without being managed, stress has been widely recognized as a major growing concern. It has the potential to cause chronic illnesses (e.g. cardiovascular diseases, diabetes and some forms of can-cer) and increase economic costs in societies, especially in developed countries [3, 4]. Benefits of stress research range from improving day-to-day activities, through making it a beneficial area of research and posing technical challenges in Computer Science. Various computational methods have been used to objectively define and classify stress to differentiate conditions causing stress from other conditions. The methods developed have used simplistic models formed from techniques like Baye-sian networks [5], decision trees [6] and support vector machines [7]. These models have been built from a relatively smaller set of stress features than the sets used in the models in this paper. 
The human body X  X  response signals obtained from non-invasive methods that re-flect reactions of individuals and their bodies to stressful situations have been used to interpret stress levels. These measures have provided a basis for defining stress objec-tively. Stress response signals used in this paper fall into two categories  X  physiologi-(GSR), electrocardiogram (ECG) and blood pressure (BP). Unlike these signals, we define physical signals as signals where changes by the human body can be seen by humans without the need for equipment and tools that need to be attached to individu-als to detect general fluctuations. However, sophisticated equipment and sensors us-ing vision technologies are still needed to obtain physical signals at sampling rates sufficient for data analysis and modeling like the ones used in this paper. Physical signals include eye gaze and pupil dilation signals. GSR, ECG, BP, eye gaze and pupil dilation signals have been used to detect stress in literature [5, 8, 9] but this combination has not been reported in literature so far. We use this combination of sensor signals in this paper and refer to them as primary signals for stress. 
Hundreds of stress features can be derived from primary signals for stress to classi-fy stress classes. However, this set of features may include redundant and irrelevant features which may outweigh the more effective features showing stress patterns. This Since this paper is dealing with sensor data, some features may suffer from corruption as well. In order to achieve a good classification model that is robust to such potential features that may reduce the performance of classifications, appropriate feature selec-tion methods must be dev eloped and adopted by classifiers. A feature selection method that selects features in order to redu ce redundancy using correlation based analysis could be used [10]. In addition, a genetic algorithm (GA) could also be used to select subsets of features for optimizing stress classifications. A GA is a g lobal search algorithm and has been commonly used to solve optimization problems [11]. The search algorithm is based on the concept of natural evolution. It evolves a popula-tion of candidate solutions using crossover, mutation and selection methods in search for a population of a better quality. GAs have been successfully used to select features derived from physiological signals [12, 13]. 
This paper describes the method for collecting and developing computational mod-els for recognizing stress patterns in response signals observed from individuals while reading stressed and non-stressed labeled text validated by participants. It details an experiment conducted to collect s ensor an d participant-reported data where experi-ment participants read text with stressful and non-stressful content during which mul-tiple response signals were recorded. Several approaches for stress recognition in reading are developed, compared and disc ussed including methods for selecting fea-tures from hundreds of features derived from the response signals. The paper concludes with a summary of the findings and suggests directions for future work. Thirty-five undergraduate students were recruited as experiment participants. The participant cohort was made up of 25 males and 10 females over the age of 18 years old. Each participant had to understand the requirements of the experiment from a written set of experiment instructions with the guidance of the experiment instructor before they provided their consent to take part in the experiment. Afterwards, physio-logical stress sensors were attached to the participant and physical stress sensors were calibrated. The instructor notified the participant to start reading, which triggered a assessment based on the reading. An outline of the process of the experiment for an experiment participant is shown in Fig. 1. 
Each participant had physiological and physical measurements taken over the 12 minutes reading time period. During the reading period, a participant read stressed and non-stressed types of text validated by participants. Stressed text had stressful content in the direction towards distress, fear and tension. Each participant read three stressed and three non-stressed text. Each text had approximately 360 words and was displayed on a computer monitor for particip ants to read. F or consistency, each text was displayed on a 1050 x 1680 pixel Dell monitor, displayed for 60 seconds and positioned at the same location of the computer screen for each participant. Each line of the paragraph had 70 characters including spaces. 
Results from the experiment survey validated the text classes. This is a common method used in literature to validate stress classes for tasks [14]. Participants found the paragraphs that were labeled stressed stressful and text labeled non-stressed as not stressful with a statistical significance of p &lt; 0.001 according to the Wilcoxon test. 
The physiological and physical sensor signals (which we refer to as primary stress signals) captured during the experiment were GSR, ECG, BP, eye gaze and pupil diameter signals. Biopac ECG100C, Biopac GSR100C and Finapres Finger Cuff sys-tems were used to take ECG, GSR and blood pressure recordings at a sampling rate of 1000 Hz. Eye gaze and pupil dilation signals were obtained using Seeing Machines FaceLAB system with a pair of infrared cameras at 60 Hz. There were other signals that were derived from the primary stress signals to form other stress response signals. These signals included the heart rate variability signal, which was calculated from consecutive ECG peaks and another popular signal used for stress detection [15, 16]. 
Features were derived from the primary stress signals. Statistics (e.g. mean and standard deviation) were calculated for the signal measurements for each 5 second interval during the stressed and non-stressed reading. Measures such as the number of peaks for periodic signals, the distance an eye covered, the number of forward and backward tracking fixations, and the proportion of the time the eye fixated on different regions of the computer screen over 5 second intervals were also obtained. The statistic and measure values formed the stress feature set. There were 215 features altogether. Features used for developing models for cl assification had an effect on the perfor-mance of classifiers. Selecting effective features by reducing redundant and irrelevant features have been known to improve the quality of pattern recognition [17] because it generalizes the patterns in the data better and helps develop a generalized model that captures necessary data patterns. In turn, this improves the quality for classifications. In this paper, a feature selection method based on correlation of features and a genetic algorithm (GA) approach were developed and used as feature selection methods for stress classification. 
The features derived from the stress primary signals may have had redundant data so a feature selection approach based on correlation coefficients for features was de-veloped. Correlation analysis using correlatio n coefficients has been reported to detect some redundancies in data [10]. It also took into account the time-varying nature of features and enabled comparison of features on this basis. 
A correlation coefficient is a measure for the strength of the linear relationship be-tween features. Consider two features, X and Y, with x t and y t values at time-step t in for X and Y, then the correlation coefficient r XY is defined by The values for r XY fulfill the following equation: If the value for r XY = 0, then features X and Y are independent otherwise the features are correlated. However, stress features may have noise originating from data collec-tion and the human body so as a result. In addition, the definition for independence of features for stress classification may be too strict. This motivated the use of different degrees for feature independence. In order to distinguish from independence defined by the strict criteria, we coin the term pseudo-independence to mean independence at a certain degree. Suppose the degree of independence is set at r XY  X  0.1 and r XY is found to be 0.05, then features X and Y are pseudo-independent. On the other hand if r XY is found to be 0.25, then features X and Y are not pseudo-independent. 
The pseudo-independent based feature selection method, pseudo-independent fea-ture selection algorithm (PISA), was used to select stress features that a classifier was provided to detect stress patterns. Given a set of features, each feature was compared with each of the other features to deter mine whether they were not pseudo-independent features. Features and their not pseudo-independent features were found and used to g enerate a s et of pseudo-independent features for classification. The process by which a set of pseudo-independent features were obtained was by PISA. PISA is defined in Fig. 3. 
Given a set of features and a set of features that are not pseudo-independent to each feature, PISA firstly finds features that are pseudo-independent to every other feature. Then it selects one feature from a cluster of not pseudo-independent features. The feature selected from a cluster is based on its not pseudo-independent features  X  the number of features and whether the features have been already selected. Features with a higher number of not pseudo-independent features have a greater chance of being selected. This characteristic of PISA minimi zes the impact of highly correlated noisy features on classification. 
To illustrate PISA, consider an example input set comprising features and the fea-tures that are not pseudo-independent to the features presented in Fig. 4. Firstly, PISA determines features that do not have any not pseudo-independent features. A set {f4} is the result at this point. Now, the rest of the pseudo-independent features need to be appended to the set. There are multiple possibilities and they are {f1, f3, f4} and {f2, f4}. After applying the algorithm the result will be {f2, f4}, which is the smaller fea-ture set of the multiple feature sets with pseudo-independent features. This approach reduces the risk of selecting features that were derived from corrupted sensor data and negatively affect the performance for stress classification. Suppose the feature set {f1, f2, f3} were derived from corrupted sensor data and if {f1, f3, f4} was selected as a model to capture better stress patterns would be lower than if {f2, f4} was chosen instead. 
Another feature selection method used for stress classification in reading was based on a GA. A GA is a global search algorithm that was used to select features to improve the quality of stress classifications. The GA search evolved a population of subsets of features using crossover, mutation and selection methods in search for a population of subset of features that produced a better quality stress classification. A subset of features is referred to as an individual or chromosome . The quality for each chromosome in the population was defined by the quality of classifications produced when a classifier was provided with the features in the chromosome. 
The initial population for the GAs was set up to have all the features. The number of features in the chromosomes varied but the chromosome length was fixed. The length of a chromosome was equal to the number of features in the feature space. A chromosome was a binary string where the index for a bit represented a feature and the bit value indicated whether the feature was used in the classification. The parameters for the GAs implemented were set as provided in Table 1. Classification models developed for stress pattern recognition from primary stress signals were based on an artificial neural network (ANN) and a support vector ma-chine (SVM). The models were extended to incorporate feature selection phases either using the PISA or GA approaches. Each classification model was defined to capture individual-independent stress patterns. The accuracy and F-Score were calculated for each approach to determine the quality of the classification. 
The stress reading data set was divided up into 3 subsets  X  training, validation and test sets  X  where 50% of the data samples were used for training a classification mod-el and the rest of the data set was divided up equally for validating and testing the model. MATLAB was used to implement and test the models. 4.1 Artificial Neural Network Based Stress Classifiers ANNs, inspired by biological neural networks, have capabilities for learning patterns to recognize characteristics in input tuples by classes. An ANN is made up of inter-connected processors, known as artificial neurons , which are connected by weighted links that pass signals between neurons. In this paper, feed-forward ANNs trained using backpropagation were used. Three topologies were used to classify stress in reading. Each of the ANNs was provided stress features as inputs based on a selection method. Therefore, the ANNs differed only on the number of inputs. The ANN based stress classification models were:  X 
ANN: an artificial neural network classification model that was provided with all the features in the stress feature set as input to recognize stress patterns  X 
PISA+ANN: ANN with inputs as features produced by PISA  X  GA+ANN: ANN with inputs as features produced by a GA The MATLAB adapt fun ction was used for training the ANNs on an in cremental basis. Each network was trained for 1000 epoch s using the Levenberg-Marquardt algorithm. The network had 7 hidden neurons and one neuron in the output layer. Future work could investigate optimizing th e topology of the ANN for stress classifi-cation on the reading data set. 4.2 Support Vector Machine Based Stress Classifiers SVMs have been widely used in literature for classification problems including classi-fications based on physiological data [18]. Provided a set of training samples, a SVM transforms the data samples using a nonlinear mapping to a higher dimension with the aim to determine a hyperplane that partitions data by class or labels. A hyperplane is chosen based on support vectors , which are training data samples that define maxi-mum margins from the support vectors to the hyperplane to form the best decision boundary. This contributes to the resistance to data overfitting and helps to generalize classifications well. 
Despite the useful characteristics, SVMs are still not robust to feature sets with re-dundant and irrelevant features. As a consequence, hybrids of SVM with PISA or GA were used to deal with ineffective featur es to investigate whether the hybrids with feature selection methods improved the quality of the classification. 
The SVM based stress classification models developed were:  X 
SVM: a support vector machine classification model that was provided with all the features in the stress feature set as input similar to the ANN  X 
PISA+SVM: SVM with inputs as features produced by PISA  X 
GA+SVM: SVM with inputs as features produced by a GA The six ANN and SVM based techniques were implemented and tested on the reading cross-validation. The results are presented in Table 2. Classifiers with feature selec-tion methods performed better than classifiers that used all stress features to model stress patterns. The hybrid classifiers had stress recognition rates and F-score values that were at least 8% and 12% better respectively. Classifiers with a GA as the feature selection method produced the highest stress recognition rates with GA+SVM as the best performing technique. 
Performance measures for the classification techniques show that it was beneficial to use the feature selection methods to model stress patterns in reading. The stress features would have had redundant features and PISA would have reduced it. PISA compared every feature to every other feature in a pair-wise fashion and took a greedy approach in selecting features. Unlike PISA, the GA took a global view of the fea-tures and would have managed to reduce more redundant, irrelevant and corrupted features. 
In terms of execution time, the GA based approaches, GA+ANN and GA+SVM, took longer times than the other techniques to produce solutions. It took PISA less than one second to select the features for the classification models whereas the execu-tion times for the GA based approaches were in the order of hours. Classification without a feature selection method or with PISA took relatively a similar amount of time. Empirical execution times for the different approaches are shown in Table 3. 
The execution times for GA based approaches were recorded af ter the search reached convergence except for GA+ANN, which took a lot longer to e xecute. GA+ANN took at lea st 3 days and it took the other techniques not more than a few hours to produce a solution. Therefore, it was not practical to let the GA+ANN search execute for longer. Table 2 and Table 3 have a * with the values to show results at the point when the GA+ANN search was terminated. 
Due to the relative long execution times fo r the GA based approaches to search for a better stress classification result, other feature selection approaches can be investi-gated in the future. With shorter execution times and classification performance re-sults for classifiers using PISA, PISA has the potential to increase the performance for classifications. In future, PISA based classifiers could be extended to have a more complex definition for pseudo-independence. 
ANN and SVM classification results using PISA over a range of degrees of pseu-do-independence were also obtained to determine the effect of the different degrees of pseudo-independence on the classification results. The results are displayed in graphs shown in Fig. 5. The graphs show the stress recognition rates and F-score along with the number of features for the different degrees of pseudo-independence. For both, ANN and SVM, the plot for the classification results show a positive overall rate of change and then it becomes negative after 0.5 degree of pseudo-independence. The best classification results are produced when the degree of pseudo-independence is 0.5. At this degree, every feature is pseudo-independent at the degree of 0.5 to every other feature in the set of features used as input to develop th e ANN and SVM classification models. Classification models were developed to recognize individual-independent stress patterns in physiological and physical data fo r reading. The use of a feature selection method that dealt with redundant features improved the quality of the classification. However, classification models based on a genetic algorithm provided better recogni-tion rates for stress than a correlation based feature selection method. On the other hand, genetic algorithm based approaches required much longer execution times but the correlation based feature selection method hardly had any impact on the execution time. In future, the correlation based feature selection method could be extended to have a wider definition for feature independence. Further, a hybrid of the two feature selection methods presented in this work could be developed to reduce the execution time for the genetic algorithm based search. Moreover in this work, features were selected from an individual-independent viewpoint. Future work could investigate feature selection based on relationships of features for each individual and analyze their effect of the approach on classifications. 
