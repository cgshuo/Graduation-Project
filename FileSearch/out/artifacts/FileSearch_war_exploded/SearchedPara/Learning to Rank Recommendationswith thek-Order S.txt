 Making recommendations by learning to rank is becoming an increasingly studied area. Approaches that use stochas-tic gradient descent scale well to large collaborative filtering datasets, and it has been shown how to approximately opti-mize the mean rank, or more recently the top of the ranked list. In this work we present a family of loss functions, the k -order statistic loss, that includes these previous approaches as special cases, and also derives new ones that we show to be useful. In particular, we present (i) a new variant that more accurately optimizes precision at k , and (ii) a novel procedure of optimizing the mean maximum rank, which we hypothesize is useful to more accurately cover all of the user X  X  tastes. The general approach works by sampling N positive items, ordering them by the score assigned by the model, and then weighting the example as a function of this ordered set. Our approach is studied in two real-world sys-tems, Google Music and YouTube video recommendations, where we obtain improvements for computable metrics, and in the YouTube case, increased user click through and watch duration when deployed live on www.youtube.com.
 H.4 [ Information Systems Applications ]: Miscellaneous learning to rank, loss functions, stochastic gradient, collab-orative filtering, matrix factorization
While low-rank factorizations have been a standard tool for recommendation for a number of years [2] optimizing them using a ranking criterion is a relatively recent and increasingly popular trend amongst researchers and prac-ticioners alike. Methods like CofiRank [7], CLiMF [5], or items. Experiments on real-world datasets indicate the use-fulness of our approach.
We consider the general recommendation task of ranking a set of items D for a given user, the returned list should have the most relevant items at the top. To solve this task, we are given a training set of users U each with a set of known ratings. We consider the case where each user has purchased / watched / liked a set of items, which are con-sidered as positive ratings. No negative ratings are given. All non-positive rated items are thus considered as having an unknown rating 1 . We define the set D u to be the positive items for user u . We consider factorized models of the form where V , which is an m  X |D| matrix, one vector for each item, contains the parameters to be learnt. We can further define f ( u ) to be the vector of all item scores 1 , . . . , |D| for the user u . To learn f one typically minimizes an objective function of the following form: where L is the loss function, which measures the discrepency between the known ratings D u and the predictions for user u . The well-known AUC loss (sometimes known as the margin ranking loss) [4, 3] is defined as: L
AUC ( f ( u ) , D u ) = X To optimize it by stochastic gradient descent, one selects a user, a positive item and a negative item at random, and makes a gradient step, corresponding to one term in the dou-ble sum of the equation above. Repeated updates gradually visit all the terms.

The AUC loss is known to not optimize well the top of the rank list. Another set of loss functions called the OWPC loss [6] and its SGD counterpart, WARP loss [8], attempt to focus on the top of the list. The loss is defined as: where  X (  X  ) converts the rank of a positive item d to a weight. Here, the rank of d is defined as where I is the indicator function.

Choosing  X (  X  ) = C X  for any positive constant C is equiv-alent to the AUC loss. However, a weighting such as  X (  X  ) = P i =1 1 /i pays more attention to optimizing the top of the ranked list. Unfortunately, training such an objective by SGD directly is not tractable as eq. 2 sums over all items, which is too slow to compute per gradient update. The
The binary rating case described is rather common in many real world recommendation tasks, especially for those where ratings are harvested from implicit feeback.
 The k -OS loss is then defined as: L where Z = P i P  X  i |D P . P ( j 100 ) is the weight assigned to the j th percentile of the ordered positive items. Different choices of P result in different loss functions. P ( j ) = C for all j and any positive constant C results in the original WARP or AUC formula-tions. Choices where P ( i ) &gt; P ( j ) for i &lt; j result in paying more attention to positive items that are at the top of the ranked list, and tends to ignore the lower ranked positives. This should have the effect of improving precision and recall at the top whilst sacrificing some of the user X  X  taste prefer-ences. Conversely, choosing P ( i ) &lt; P ( j ) for i &lt; j should focus more on improving the worst ranked positives in the user X  X  rating set. We hypothesize that this may more accu-rately cover all of the user X  X  tastes, and try to measure this in our experiments using the mean maximum rank metric.
To optimize k -OS easily via SGD we make the following simplification. During each SGD step we draw, for a random user, K random positives and order them by f ( u ). Then the P distribution only takes on K possible values. The overall method is detailed in Algorithms 1, 2 and 3, for both AUC and WARP generalizations. In the majority of our experiments we use P ( j ) = 1 if j = k/N , and 0 otherwise, and leave k as a hyperparemater. That is, we simply always select the positive in the k th position in the list.
We conducted experiments on three large scale, real world tasks: artist recommendation and track recommendation using proprietary data from Google Play Music ( http:// music.google.com ), and video recommendation from YouTube ( http://www.youtube.com ). In all cases, the datasets con-sist of a large set of anonymized users, where for each user there is a set of associated items based on their watch/listen history. The user-item matrix is hence a sparse binary ma-trix. The approximate dataset sizes are given in Table 1.
To construct evaluation data, we randomly selected 5 items for testing per user, and kept them apart from training. At prediction time for the set of test users we then ranked all un-rated items (i.e. items that they have not watched/listened to that are present in the training set) and observe where the 5 test items are in the ranked list of recommendations. We then evaluate the following metrics: mean rank (the position in the ranked list, averaged over all test items and users), mean maximum rank (the position of the lowest ranked item out of the 5 test items, i.e. the furthest from the top, av-eraged over all test users), precision at 1 and 10 (P@1 and P@10), and recall at 1 and 10 (R@1 and R@10).

Hyperparameters ( C , learning rate) were chosen using a portion of the training set for validation, although for mem-ory and speed reasons we limited the embedding dimension to be m = 64. As we trained our model, K-os , with a rank-ing criteria which includes the WARP loss and AUC losses as special cases, we consider those as our baselines, and re-port relative changes in metrics compared to them. For K-os in all cases we used K = 5 in Algorithm 1, i.e we sample 5 positive items. After ordering them by score, we then select the item in the k th position. We report results for different values of k to show its effect. On YouTube and the Google Music artist recommendation task we also compare to SVD (factorization for the complete matrix with log-odds weight-ing on the columns which downweights the popular features as that worked better than uniform weights). Results on the three datasets are given in Tables 2, 3, 4 and 5.
For the first dataset, Google Music artist recommenda-tion, we report two sets of results. Table 2 gives performance of
K-os using AUC (Algorithm 3) relative to standard AUC training. Table 3 gives performance of K-os using WARP (Algorithm 2) relative to standard WARP training. We also compare to SVD, which is outperformed by both AUC and WARP ranking losses, presumably because they are better at optimizing these ranking metrics as has been observed before [9]. Note that a strongly performing model has a small mean/max rank, and large values of precision/recall, hence we are looking for negative percentage changes in rank but positive changes in the other metrics. In both the AUC and WARP cases the choice of k in K-os gives clear control over the loss function. Small values of k tend to optimize precision and recall metrics as they focus on the top ranked positives in the set. Larger values of k tend to optimize mean maximum rank as they focus on the bottom ranked positives in the set. For example, the choice of k = 5 in Ta-ble 2 gives improved rank metrics over the AUC baseline, at the expense of decreases in precision and recall. Conversely, choices of k  X  4 give improved precision and recall metrics over the AUC baseline at the expense of larger rank metrics. Note that k = 1 does not give the best precision improve-ments as you might at first expect ( k = 2 is better). We hypothesize that this is because concentrating too much on only the top ranked positive makes the overall model suffer from not seeing enough training data with varying labels. (The same effect appears in the next dataset too.)
The second dataset, Google Music track recommendation, is comprised of the same set of anonymized users, but with items represented at the track rather than the artist level. That means there are more items to rank (  X  700k rather than  X  75k) so one could expect bigger differences between the methods as the task is more difficult. Table 4 shows larger improvements over the WARP baseline both in rank metrics ( k  X  4) and precision and recall metrics ( k  X  4). In this case k = 4 is actually a sweet spot which gives improvements in all metrics compared to the baseline.

The third dataset, YouTube video recommendation, also shows improvements in metrics for various choices of k . Again, we see smooth transitions from optimizing max or mean rank metrics versus precision or recall at the top as we vary k . In these experiments as well as showing results for single values of k we also report distributions P where we sam-ple uniformly at random different values of k . For example, K-os k &lt; 4 in the table means that we select uniformly at random one of the top 3 positives after ordering the 5 sam-pled positives. The conclusions are similar to those of the experiments in the previous datasets.
 We next tried our method using the K-os loss (using WARP and k = 5, N = 5) in the live YouTube video rec-ommendation system where we attempted to improve an already strong baseline machine learning system [1]. In our experiments above we measured rank, precision and recall. However, all these metrics are merely a proxy for the online
