 REGULAR PAPER Hansheng Lei  X  Venu Govindaraju regression models the linear relation of two sequences faithfully. However, if a the NASDAQ market mined interesting clusters of stock trends efficiently. Keywords Similarity measure  X  Sequence matching  X  Sequence clustering  X  Generalized regression model  X  Eigenvalue and eigenvector 1 Introduction sure. Several methods exist for measuring the (dis)similarity of two sequences. We briefly describe four commonly used methods.
 1.1 Lp norms the commonly used Euclidean distance. Lp norms are straightforward to compute. However, in many cases such as in shifting and scaling, the Lp distance of two sequences cannot reflect the  X  X eal X  dissimilarity between them. Suppose sequence by 100, i.e., adding 100 to each element of X . Although the Lp distance between The same is true in the case of scaling. For example, let Y =  X  X ,where  X  is mean-deviation normalization ( and X have a perfect linear relation of the form Y =  X  0 +  X  1 X . 1.2 Transforms Fourier transform and wavelet transform are commonly used for sequence analysis of the frequency domain, allowing for the processes to be carried out in a small region with few coefficients, thus reducing dimension and saving computational time. The transforms are used primarily for feature extraction and therefore the task of defining some type of similarity measure remains. 1.3 Time warping Dynamic time warping (DTW) defines the distance of two sequences Xi = [ x 1 , using dynamic programming. It has the advantage that it can tolerate some local non-alignment in the time phase so that the two sequences do not have to begin with the same length. DTW generally is more robust and flexible than Lp norms, because the DTW alignment between two sequences is a many-to-many match, while the Lp norms only allow one X  X ne match. DTW is more robust and flexible sensitive to shifting and scaling. 1.4 Linear relation Linear relation is of the form Y  X  0 +  X  1 X . Linearity can be considered as a  X  1 can be determined so that the distance Lp geometrical standpoint but do not give an intuitive measure of linearity [ 7 ]. ery of sequences. In this paper, we propose generalized regression model (GRM) to match and cluster sequences based on linearity. GRM gives a new measure, algorithm is developed to cluster massive sequences efficiently and accurately. provides a basic background of the traditional regression model. Section 3 de-scribes GRM in detail. Matching and clustering massive sequences by GRM are conclusions are drawn in Sect. 7. 2 Regression model background sumption Y and income X , the regression model is established as: Y =[ y y [ 22 , 29 ],  X  0 and  X  1 are solved as follows: tively.
 as: It can be further derived: the regression line fits the points. Thus, R 2 isameasureofthe Goodness-of-fit . than what we can obtain from the mean-deviation normalization. Only when Y application domains.
 involves only one independent variable X and one dependent variable Y . Multiple independent variables can be added to the model: estimated using the first-order conditions. 3 Generalizing simple linear regression The SLR is well suited for testing the linear relation of two sequences and R 2 R 2 is the same, i.e., R 2 is symmetric. The property of symmetry qualifies R 2 to be a similarity measure.
 X 1 and X 2 are similar with R 2 ( X 1 line, while points in Fig. 2 f scatter.
 However, when more than two sequences are involved in the task of clustering, regression must be performed between each pair of sequences. We cannot directly compute R 2 from multiple sequences, because R 2 in MLR means the linear rela-among multiple sequences.
 address the issues described above. 3.1 GRM: Generalized regression model Given K ( K  X  2 ) sequences X 1 , X 2 ,..., X K and We first organize them into N points in the K -dimensional space: in the sense of minimum-sum-of-squared-error.
 anymore, as no sequence is special.
 dimensional space. For now, we use the conclusion that the regression line is: sponding to the maximum eigenvalue of the scatter matrix S (see Appendix) and x such that p i = p j ( i = j ). This assumption guarantees that the N points de-same in practice.
 fit: implying that the K sequences have high degree of linearity with each other. GRM.
 Lemma 1 Two sequences X 1 and X 2 are linear to each other  X  Two sequences X 1 and X 2 have a linear relation as  X ( X ) denotes the standard deviation of sequence X .
 is linear to X 1 , then: So, we have That is Subtract ( 10 )by( 12 ), we get:  X  Assumption 1,  X ( X 1 ) = 0and  X ( X 2 ) = 0.
 Lemma 2 K sequences X 1 , X 2 ,..., X K are linear to each other  X  K sequences X 1 , 2 ,..., N ) ,where  X ( X ) denotes the standard deviation of sequence X . Proof From Lemma 1, this is straightforward.
 p , p is the regression line.
 Proof From right to left is obvious. Let us prove from left to right. relation can be expressed as: as: that p 1 , p 2 ,..., p N are all distributed on a line in K -dimensional space. p ,..., p On the other hand, our regression line guarantees that the sum of squared-error squared-error is 0 (minimized). 2. GR 2 = 1 means the K sequences have exact linear relation to each other. Proof 1. According to Appendix, we have: 3. This is obvious.
 With the properties above, we define GR 2 as a measure of linearity (similarity) confidence. 4 Matching multiple sequences using GRM The procedure of applying GRM to measure the linear relation of multiple se-quences is described by algorithm GRM.1.
 3. Calculate GR 2 according to property 1 of GR 2 . 4. If GR 2  X  C (confidence threshold C is a user-specified parameter), the K is the actually the first principle component by PCA analysis. However, the es-sential difference between GRM.1 and PCA is that GRM.1 first organizes the K sequences into N points and then applies PCA to determine the generalized re-the overall similarity of multiple sequences. 4.1 Apply GRM.1 to two sequences: An example Suppose we want to test two sequences X 1 and X 2 and as shown in Fig. 3 a and b, respectively.
 in the X 1  X  X 2 space, their distribution is as shown in Fig. 3 c. [ 8 . e =[ 0 . 4553 , 0 . 8904 ] t .
 other. So the conclusion based on GR 2 makes intuitive sense. 4.2 Apply GRM.1 to multiple sequences: An example give an example for testing three sequences at a time.
 of the three sequences is linear to others. This example once again demonstrates that GR 2 is an intuitive measure. 5 Apply GRM to clustering multiple sequences GRM provides heuristic support for efficient clustering based on the following theorem.
 Proof According to Theorem 1, if K sequences are linear to each other, then we to formula ( 8 ), we have relation p ( 1 )  X  x 1 e the two equal relations concludes the proof.
 hint that they may be linear. To infer reversely, if  X ( X i ) e then X i and X j cannot be linear. We call  X ( X i ) e Using the feature value as heuristic information, clustering algorithm by GRM works as follows.
 1. Given sequence set H ={ X i | i = 1 , 2 ,..., K } . Apply Algorithm GRM.1 to artificial data. Figure 5 b shows the sequence number and corresponding feature close to each other. For instance, sequence #2, #3, #5 have close feature values Fig. 5 a.
 mum eigenvalue and corresponding eigenvector of scatter matrix S whose size is K  X  K . The time complexity of computing the eigenvalues of a S is O ( K 2 ) on of eigenvalue problem. The time complexity is O ( log 2 K ) with K 2 processors and GRM.2 is high. Even on a single processor, there exists a fast algorithm to determine the eigenvalues of a symmetric matrix with high efficiency [ 13 ]. number of clusters is unknown, as in case of stock clustering. While the hierar-than GRM.2. 6 Experiments Our experiments will answer this question: What is the accuracy and performance of GRM in clustering massive sequences? We tested the GRM clustering algo-rithm with the real stocks in the NASDAQ market.
 05-08-2001 through 05-08-2003. It has over 4000 stocks. We used the daily clos-quences and removing short sequences. Finally, we have 3140 sequences in the database with the same length 365.
 the accuracy. Noting that neither DTW nor Lp norms handles the linearity of se-quences, we have only two choices. The first is GRM.2 and the second is to cal-venience, we name the latter BFR (Brute-Force R 2 ). As we discussed in Sect. 3, the SLR can also match two sequences with invariance to scaling and shifting. Intuitively, BFR is more accurate but slower, while GRM.2 is faster with at the Given a set of sequences H ={ X i | i = 1 , 2 , K } , BFR works as follows: (1) Take an arbitrary sequence out from H ,say X i . Find all sequences from H (3) Repeat (1) until H becomes empty.
 by BFR and CM 1 , CM 2 ,..., CM l by GRM.2 are determined. The relative accu-racy of GRM.2 to BFR is defined as follows: where Sim ( CM i , CR j ) = 2 RAM on the platform of Matlab 6.0.
 stays above 99%. It should be noted that no algorithm can be considered as 100% ambiguous. Figure 7 compares the running time of GRM and BFR. We can see that GRM.2 is significantly faster than BFR, no matter how the number of sequences or length of sequence varies. a cluster of stocks mined using GRM.2 as an example. The stocks of four compa-nies, A (Agile Software Corporation), M (Microsoft Corporation), P (Parametric fit each other very well with some shifting. T fits others with both scaling and shifting. The linear relation is consistent with our observation.
 2-dimensional space confirms that the two stocks have strong linear relation. 7Conclusion GRM is a generalization of the traditional simple regression model. GRM gives a measure GR 2 , which is a novel measure of linearity (similarity) for multiple sequences. Based on GR 2 , algorithm GRM.1 matches the linearity of multiple sequences at a time and GRM.2 clusters massive sequences with high accuracy and efficiency.
 Appendix References
