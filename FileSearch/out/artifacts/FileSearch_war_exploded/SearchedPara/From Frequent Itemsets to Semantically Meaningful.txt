 } Data mining techniques that are successful in transaction and text data may not be simply applied to image data that contain high-dimensional features and have spatial struc-tures. It is not a trivial task to discover meaningful visual patterns in image databases, because the content variations and spatial dependency in the visual data greatly challenge most existing methods. This paper presents a novel ap-proach to coping with these difficulties for mining meaning-ful visual patterns. Specifically, the novelty of this work lies in the following new contributions: (1) a principled solution to the discovery of meaningful itemsets based on frequent itemset mining; (2) a self-supervised clustering scheme of the high-dimensional visual features by feeding back discov-ered patterns to tune the similarity measure through met-ric learning; and (3) a pattern summarization method that deals with the measurement noises brought by the image data. The experimental results in the real images show that our method can discover semantically meaningful patterns efficiently and effectively.
 H.2.8 [ Database Management ]: Database Applications X  Data mining, Image databases ; I.5.3 [ Pattern Recogni-tion ]: Clustering X  Algorithms Algorithms image data mining, meaningful itemset mining, pattern sum-marization, self-supervised clustering
Meaningful patterns can be those that appear frequently, thus an important task for data mining and pattern dis-covery is to identify repetitive patterns. Frequent itemset Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. mining (FIM) and its extensions [9] [21] [7] have been ex-tensively studied. However, a highly repetitive pattern may not be informative or semantically meaningful. Therefore a more important task is to extract informative and po-tentially interesting patterns (e.g. semantically meaningful patterns) in possibly noisy data. This can be done by min-ing meaningful patterns either through post-processing the FIM results or proposing new data mining criteria, including mining compressed patterns [3] [19] [23], approximate pat-terns [29] [1] [14] and pattern summarization [28] [26] [27]. These data mining techniques may discover meaningful fre-quent itemsets and represent them in a compact way.
Such research in structured data (e.g., transaction data) and semi-structured data (e.g., text) has aroused our curios-ity in finding meaningful patterns in non-structured multi-media data like images and videos [20] [24] [31] [10]. For ex-ample, once we can extract some invariant visual primitives such as interest points [15] or salient regions [17] from the images, we can represent each image as a collection of such visual primitives characterized by high-dimensional feature vectors. By further quantizing those visual primitives to dis-crete  X  X isual items X  through clustering the high-dimensional features [24] [30], each image is represented by a set of trans-action records, with each transaction corresponds to a local image patch and describes its composition of visual primi-tive classes (items). After that, data mining techniques like FIM can be applied to such a transaction database induced from images for discovering meaningful visual patterns.
Although this idea appears to be quite exciting, the leap from transaction data to images is not trivial, because of two fundamental differences between them. Above all, un-like transaction and text data that are composed of discrete elements without ambiguity ( i.e. predefined items and vo-cabularies), visual patterns generally exhibit large variabili-ties in their visual appearances. A same visual pattern may look very different under different views, scales, lighting con-ditions, not to mention partial occlusion. It is very difficult, if not possible, to obtain invariant visual features that are insensitive to these variations such that they can uniquely characterize visual primitives. Therefore although a discrete item codebook can be forcefully obtained by clustering high-dimensional visual features (e.g., by vector quantization [18] or k -means clustering [24]), such  X  X isual items X  tend to be much more ambiguous than the case of transaction and text data. Such imperfect clustering of visual items brings large challenges when directly applying traditional data mining methods into image data.
In addition to the continuous high-dimensional features, visual patterns have more complex structure than transac-tion and text pattern. The difficulty of representing and dis-covering spatial patterns in images prevents straightforward generalization of traditional frequent pattern mining meth-ods that are applicable for transaction data. For example, unlike traditional transaction database where records are in-dependent of each other, the induced transactions generated by image patches can be correlated due to spatial depen-dency. Although there exist methods [12] [32] [11] for spa-tial collocation pattern discovery from geo-spatial data, they cannot be directly applied to image data which are charac-terized by high-dimensional features. Moreover, the spatial co-occurrences of the items do not necessarily indicate the real associations among them, because a frequent spatial collocation pattern can be generated by the self-repetitive texture in the image and thus is not semantically meaning-ful. Thus, finding frequent patterns based on FIM may not always output meaningful and informative patterns in the image data.

Given a collection of unlabeled images, the objective of image data mining is to discover (if there is any) seman-tically meaningful spatial patterns that appear repetitively among the images. For example, given a set of images each of which contains an identical object (e.g. a book or a logo) but with possibly different locations, scales and views, the task is to efficiently discover and locate them in the im-ages. This is a challenging problem because we have no prior knowledge of the object X  X  size, location and pose, or whether such object exists at all. Some existing methods based on graph matching are computational demanding and the solution is prone to local minimum [25] [10]. Thus more efficient and robust algorithm is desirable. In this paper, we aim at an even more challenging problem: given a cat-egory of images, for example each image contains a frontal face but from different persons, we expect to discover some meaningful patterns like eyes and noses that have semantic meanings and can well interpret the face category. To this end, the following three issues need to be further addressed.
This paper presents a novel approach to discovering se-mantically meaningful visual patterns from images. By ad-dressing the above three difficulties, our contributions are three-fold:
Each image in the database is described as a set of vi-sual primitives: I = { v i = ~ f i , x i , y i } , where ~ the high-dimensional feature and { x i , y i } denotes the spa-tial location of v i in the image. For each visual primi-tive v i  X  I , its local spatial neighbors form a group G { v i , v i 1 , v i 2 ,  X  X  X  , v i K } . For example, G i can be the spatial K-nearest neighbors (K-NN) or -nearest neighbors of v i ( -NN) under Euclidean distance. The image database D I = {I t } T t =1 can generate a collection of such groups, where each group G i is associated to a visual primitive v i . By further quantizing all the high-dimensional features ~ f i  X  D I into M classes through k -means clustering, a codebook  X  can be obtained. We call every prototype W k in the codebook  X  = { W 1 , ..., W M } a visual item . Because each visual prim-itive is uniquely assigned to one of the visual items W i group G i can be transfered into a transaction T i . More for-mally, given the group dataset G = {G i } N i =1 generated from D
I and the visual item codebook  X  ( |  X  | = M ), the induced transaction database T is defined as follows.
 Definition 1. Induced Transaction Database The induced transaction database T = {T i } N i =1 contains a collection of N transactions with M visual items. A sparse binary matrix X N  X  M can represent T , where x ij = 1 de-notes the i th transaction contains the j th visual item in the codebook and x ij = 0 otherwise.
Such an induced transaction database is essentially based on the centric reference feature model for mining association rules [32], although collocation pattern models like [12] are also feasible in our approach. Given the visual item code-book  X  , a set P  X   X  is called a visual itemset (itemset for short). For a given itemset P , the transaction T i which includes P is called an occurrence of P , i.e. T i is an occur-rence of P , if P  X  T i . Let T ( P ) denote the set of all the occurrences of P in T , and the frequency of P is denoted as:
For a given threshold  X  , called a minimum support , item-set P is frequent if f rq ( P ) &gt;  X  . If an itemset P appears frequently, then all of its sub-sets P 0  X  P will also appear frequently, i.e. f rq ( P ) &gt;  X   X  f rq ( P 0 ) &gt;  X  . To eliminate this redundancy, we tend to discover closed frequent item-sets [8]. The number of closed frequent itemsets can be much less than the frequent itemsets, and they compress informa-tion of frequent itemsets in a lossless form, i.e. the full list of frequent itemsets F = {P i } and their corresponding fre-quency counts can be exactly recovered from the compressed representation of closed frequent itemsets. Thus this guar-antees that no meaningful itemsets will be left out through FIM. The closed frequent itemset is defined as follows. Definition 2. closed frequent itemset If for an itemset P , there is no other itemset Q X  X  that can satisfy T ( P ) = T ( Q ) , we say P is closed. For any itemset P and Q , T ( P  X  X  ) = T ( P )  X  T ( Q ) , and if P  X  Q then T ( Q )  X  T ( P ) .

In this paper we apply the modified FP-growth algorithm [6] to implement the closed FIM. As FP-tree has a prefix-tree structure and can store compressed information of frequent itemset, it can quickly discover all the closed frequent sets from transaction dataset T .
We present the overview of our visual pattern discovery method in Fig. 1. In Sec. 3, we present our new criteria for discovering meaningful itemsets  X  = {P i } , where each P i  X   X  is a meaningful itemset. Further in Sec. 4, a top-down self-supervised clustering method is proposed by feed-ing back the discovered meaningful itemsets  X  to supervise the clustering process. A better visual item codebook  X  is then obtained by applying the trained similarity metric for better representing visual primitives. Finally, in Sec. 5, in order to handle the incomplete sub-pattern problem, we propose a pattern summarization method to further cluster those meaningful itemsets (incomplete sub-patterns) and re-cover the integral semantically meaningful pattern H j .
We apply the PCA-SIFT points [13] as the visual prim-itives . Such visual primitives are mostly located in the in-formative image regions such as corners and edges, and the features are invariant under rotations, scale changes, and slight viewpoint changes. Normally each image may contain hundreds to thousands of such visual primitives based on the size of the image. According to [13], each visual prim-itive is a 41  X  41 gradient image patch at the given scale, and rotated to align its dominant orientation to a canonical direction. Principal component analysis (PCA) is applied to reduce the dimensionality of the feature. Finally each vi-sual primitive is described as a 35-dimensional feature vector ~ f . These visual primitives are clustered into visual items through k -means clustering, using Euclidean metric in the feature space. We will discuss how to obtain a better visual item codebook  X  based on the proposed self-supervised met-ric learning scheme in Sec. 4.
Given an image dataset D I and its induced transaction database T , the task is to discover the meaningful itemset (MI) P  X   X  ( |P|  X  2). To evaluate the significance of an itemset P  X   X  , simply checking its frequency f rq ( P ) in T is far from sufficient. For example, even if an itemset appears frequently, it is not clear whether such co-occurrences among the items are statistically significant or just by chance. In order to evaluate the statistical significance of a frequent itemset P , we propose a new likelihood ratio test criterion. We compare the likelihood that P is generated by the mean-ingful pattern versus the likelihood that P is randomly gen-erated, i.e. by chance.

More formally, we compute the likelihood ratio for an itemset P  X   X  based on the two hypotheses, where H 0 : occurrences of P are randomly generated; H 1 : occurrences of P are generated by the hidden pattern.
Given a transaction database T , the likelihood ratio L ( P ) of an itemset P = { W i } |P| i =1 can be calculated as: where P ( T i | H 1 ) = 1 N is the prior, and P ( P|T i , H likelihood that P is generated by a hidden pattern and is ob-served at a particular transaction T i , such that P ( P|T 1, if P  X  X  i ; and P ( P|T i , H 1 ) = 0, otherwise. Consequently, based on Eq. 1, we can calculate P ( P| H 1 ) = frq ( P ) assume that the items W i  X  X  are conditionally independent under the null hypothesis H 0 , and P ( W i | H 0 ) is the prior of item W i  X   X  , i.e. the total number of visual primitives that are labeled with W i in the image database D I . We thus refer L ( P ) as the  X  X ignificance X  score to evaluate the devi-ation of a visual itemset P . In fact if P = { W A , W B } is a second-order itemset, then L ( P ) is the mutual information criterion, e.g. , the lift criterion, to test the dependency.
It is worth noting that L ( P ) may favor high-order itemsets even though they appear less frequently. Table 1 gives an example, where 90 transactions have only items A and B ; 30 transactions have A , B and C ; 61 transactions have D and E ; and 19 transactions have C and E .

From Table 1, It is easy to evaluate the significant scores for P 1 = { A, B } and P 2 = { A, B, C } with L ( P 1 ) = 1 . 67 and L ( P 2 ) = 1 . 70 &gt; L ( P 1 ). This result indicates that P more significant pattern than P 1 but counter-intuitive. This observation challenges our intuition because P 2 is not a co-hesive pattern. For example, the other two sub-patterns of P , P 3 = { A, C } and P 4 = { B, C } , contain almost indepen-dent items: L ( P 3 ) = L ( P 4 ) = 1 . 02. Actually, P 2 treated as a variation of P 1 as C is more likely to be a noise. The following equation explains what causes the incorrect result. We calculate the significant score of P 2 as: Therefore when there is a small disturbance with the dis-tribution of C over T 1 such that P ( C | A, B ) &gt; P ( C ), P will compete P 1 even though P 2 is not a cohesive pattern ( e.g. C is not related with either A or B ). To avoid those free-riders such as C for P 1 , we perform a more strict test on the itemset. For a high-order itemset P ( |P| &gt; 2), we perform the Student t-test for each pair of its items to check if items W i and W j ( W i , W j  X  X  ) are really dependent (see Appendix 8 for details.) A high-order itemset P i is mean-ingful only if all of its pairwise subsets can pass the test individually:  X  i, j  X  P , t ( { W i , W j } ) &gt;  X  , where  X  is the confidence threshold for the t-test. This further reduces the redundancy among the discovered itemsets.

Finally, to assure that a visual itemset P is meaningful, we also require it to appear relatively frequent in the data-base, i.e. f rq ( P ) &gt;  X  , such that we can eliminate those itemsets that appear rarely but happen to exhibit strong spatial dependency among items. With these three criteria, a meaningful visual itemset is defined as follows. Definition 3. Meaningful Itemset (MI) An itemset P  X   X  is (  X ,  X ,  X  ) -meaningful if it is: 1. frequent : f rq ( P ) &gt;  X  ; 2. pair-wisely cohesive : t ( { W i , W j } ) &gt;  X ,  X  i, j  X  X  ; 3. significant : L ( P ) &gt;  X  .
Suppose primitives v i and v j are spatial neighbors, their induced transaction T i and T j will have large spatial over-lap. Due to such spatial dependency among the transac-tions, it can cause over-counting problem if simply calculat-ing f rq ( P ) from Eq. 1. Fig. 2 illustrates this phenomena where f rq ( P ) contains duplicate counts.
 Figure 2: Illustration of the frequency over-counting
In order to address the transaction dependency problem, we apply a two-phase mining scheme. First, without consid-ering the spatial overlaps, we perform closed FIM to obtain a candidate set of meaningful itemsets. For these candi-dates F = {P i : f rq ( P i ) &gt;  X  } , we re-count the number of their real instances exhaustively through the original image database D I , not allowing duplicate counts. This needs one more scan of the whole database. Without causing confu-sion, we denote  X  f rq ( P ) as the real instance number of P and use it to update f rq ( P ). Accordingly, we adjust the calculation of P ( P| H 1 ) =  X  frq ( P )  X  the approximated independent transaction number with K the average size of transactions. In practice, as  X  N is hard to estimate, we rank P i according to their significant value L ( P ) and perform the top-K pattern mining.

Integrating all the contents in this section, our meaningful itemsets mining (MIM) algorithm is outlined in Algorithm 1.
Algorithm 1 : Meaningful Itemset Mining (MIM) input : Transaction dataset T , MI parameters: (  X ,  X ,  X  ) output : a collection of meaningful itemsets:  X  = {P i }
Init: closed FIM with f rq ( P i ) &gt;  X  : F = {P i } ,  X   X  X  X  X  ; foreach P i  X  F do GetRealInstanceNumber( P i ) for P i  X  F do
Return  X 
Toward discovering meaningful visual patterns in images, it is critical to obtain optimal visual item codebook  X  . A bad clustering of visual primitives brings large quantization errors when translating the continuous high-dimensional vi-sual features ~ f  X  R d into discrete labels W i  X   X  . Such quantization error reflected in the induced transaction data-base can affect the data mining results significantly, and thus needs to be minimized.

To improve the clustering results, one possible method is to provide some supervisions, e.g. partially label some in-stances or give some constrains for pairs of instances belong-ing to the same or different clusters. Such a semi-supervised clustering method has demonstrated its ability in greatly improving the clustering results [2]. However, in our un-supervised clustering setting, there does not exist apparent supervisions. Thus an interesting question is: is it possible to obtain some supervisions from the completely unlabeled vi-sual primitives ? Although it is amazing to see the answer is yes, we can explain the reason based on the hidden structure of the image data. It is worth noting that those visual prim-itives are not independently distributed in the images and appearing in the transactions. There are hidden patterns that bring structures in the visual primitive distributions. And such structures can be observed and recovered from the transaction database. For example, if we observe that item W i always appears together with item W j in a local region, we can infer that they should be generated from a hidden pattern rather than randomly generated. Each pair of W i and W j is thus an instance of the hidden pattern. When such hidden patterns (structures) of the data are discovered through our meaningful itemsets mining, we can apply them as supervision to further improve the clustering results.
By discovering a set of MIs  X  = {P i } , we firstly define the meaningful item codebook as follows: Definition 4. Meaningful Item Codebook  X  + Given a set of meaningful itemsets  X  = {P i } , an item W i  X   X  is meaningful if it belongs to any P  X   X  :  X  X   X   X  , such that W i  X  P . All of the meaningful items form the meaningful item codebook  X  + = |  X  | i =1 P i .

Based on the concept of meaningful item codebook, the original  X  can be partitioned into two disjoined subsets:  X  =  X  +  X   X   X  , where  X   X  =  X  \  X  + . For any P i  X   X  , we have P i  X   X  + and P i *  X   X  . Since only  X  + can compose MI,  X  + is the meaningful item codebook. Correspondingly we denote  X   X  as the meaningless item codebook , because an item W i  X   X   X  never appears in any P i  X   X  . In such a case, W i  X   X   X  should be a noisy or redundant item that is not of interests, for example, located in the clutter background of the image.

For each class W i  X   X  + , its positive training set D + W tains the visual primitives v i  X  D I that satisfy the following two conditions simultaneously: 1. Q ( v i ) = W i , where Q (  X  ) is the quantization function 2. v i  X  T ( P 1 )  X  T ( P 2 )  X  ...  X  T ( P c ), where P In summary, not all v i labeled with W i are qualified as pos-itive training samples for item class W i  X   X  + . We only choose those visual primitives that can constitute meaning-ful itemsets. Such visual primitives are very likely generated from the hidden pattern H that explains the MI.

With these self-labeled training data for each meaningful item W i  X   X  + , we transfer the originally unsupervised clus-tering problem into semi-supervised clustering. Still, our task is to cluster all the visual primitives v i  X  D I . But now some of the visual primitives are already labeled after MIM. Thus many semi-supervised clustering methods are feasible to our task. Here we apply the nearest component analysis (NCA) [5] to improve the clustering results by learning a better Mahalanobis distance metric in the feature space. Neighborhood Component Analysis (NCA) Similar to linear discriminative analysis (LDA), NCA tar-gets at learning a global linear projection matrix A for the original features. However, unlike LDA, NCA does not need to assume that each visual item class has a Gaussian distrib-ution and thus can be applied to more general cases. Given two visual primitives v i and v j , NCA learns a new metric A and the distance in the transformed space is: d A ( v i , v ( ~ f i  X  ~ f j ) T A T A ( ~ f i  X  ~ f j ) = ( A ~ f i  X  A ~ f
The objective of NCA is to maximize a stochastic variant of the leave-one-out K-NN score on the training set. In the transformed space, a point v i selects another point v j as its neighbor with probability:
Under the above stochastic selection rule of nearest neigh-bors, NCA tries to maximize the expected number of points correctly classified under the nearest neighbor classifier (the average leave-one-out performance): where C i = { j | c i = c j } denotes the set of points in the same class as i . By differentiating f , the objective function can be maximized through gradient search for optimal A . After obtaining the projection matrix A , we update all the visual features of v i  X  D I from ~ f i to A ~ f i , and re-cluster the visual primitives based on their new features A ~ f i .
As discussed before, there are imperfections when trans-lating the image data into transactions. Suppose there exists a hidden visual pattern H j ( e.g. a semantic pattern  X  X ye X  in the face category) that repetitively generates a number of in-stances (eyes of different persons) in the image database. We can certainly observe such meaningful repetitive patterns in the image database, for example, discovering meaningful itemsets P i based on Def. 3. However, instead of observing a unique integral pattern H j , we tend to observe many in-complete sub-patterns with compositional variations due to noise, i.e. many synonyms itemsets P i that correspond to the same H j (see Fig. 3). Again, this can be caused by many reasons, including the missing detection of visual primitives, quantization error of visual primitives, and partial occlusion of the hidden pattern itself. Therefore, we need to cluster those correlated MIs (incomplete sub-patterns) in order to recover the complete pattern H .

According to [28], if two itemsets P i and P j are correlated, then their transaction set T ( P i ) and T ( P j ) (Eq. 1) should Figure 3: Motivation for pattern summarization. An in-also have a large overlap, implying that they may be gener-ated from the same pattern H . As a result,  X  i, j  X   X  , their similarity s ( i, j ) should depend not only on their frequencies  X  f rq ( P i ) and  X  f rq ( P j ), but also the correlation between their transaction set T ( P i ) and T ( P j ). Given two itemsets, there are many methods to measure their similarity including KL-divergence between pattern profiles [28], mutual information criterion and Jaccard distance [16]. We apply the Jaccard distance here although others are certainly applicable. The corresponding similarity between two MI P i and P j is de-fined as:
Based on this, our pattern summarization problem can be stated as follows: given a collection of meaningful itemsets  X  = {P i } , we want to cluster them into unjoined K-clusters. Each cluster H j = {P i } |H j | i =1 is defined as a meaningful vi-sual pattern , where  X  j H j =  X  and H i  X  X  j =  X  ,  X  i, j . The observed MI P i  X  H are instances of the visual pattern H , with possible variations due to imperfections from the im-ages. We propose to apply the normalized cut algorithm [22] for clustering MI. Normalized cut is a well-known algorithm in machine learning and computer vision community. Orig-inally it is applied for clustering-based image segmentation. Normalized Cut ( NCut ) Let G = { V , E } denote a fully connected graph, where each vertex P i  X  V is an MI, and the weight s ( i, j ) on each edge represents similarity between two MIs P i and P j . Normal-ized cut can partition the graph G into clusters. In the case of bipartition, V is partitioned into two disjoined sets A  X  B = V . The following cut value needs to be minimized to get the optimal partition: where cut ( A , B ) = i  X  A ,j  X  B s ( i, j ) is the cut value and assoc ( A , V ) = i  X  A ,j  X  V s ( i, j ) is the total connection from the vertex set A to all vertices in G . To minimize the N cut in Eq. 7, we need to solve the following standard eigenvector problem: where D is a diagonal matrix with j s ( i, j ) on its diagonal and otherwise are 0; S is a symmetric matrix with s ( i, j ) its element. The eigenvector corresponding to the second smallest eigenvalue can be used to partition V into A and B . In the case of multiple K -class partitioning, the bipartition can be utilized recursively or just apply the eigenvectors corresponding to the K + 1 smallest eigenvalues.

We summarize our visual pattern discovery algorithm as follows.

Algorithm 2 : Main Algorithm input : Image dataset D I , output : A set of meaningful patterns: H = {H i }
Init: Get visual item codebook  X  0 and induced transaction DB T 0  X  ; i  X  X  X  0; while i &lt; l do
S = GetSimMatrix (  X  i );
H = NCut ( S , | H | ); /*pattern summarization */
Return H ;
Given a large image dataset D I = {I i } , we first extract the PCA-SIFT points [13] in each image I i and treat these interest points as the visual primitives. We resize all images by the factor of 2 / 3. The feature extraction is on average 0 . 5 seconds per image. Multiple visual primitives can be located at the same position, with various scales and ori-entations. Each visual primitives is represented as a 35-d feature vector after principal component analysis. Then k -means algorithm is used to cluster these visual features into a visual item codebook  X  . We select two categories from the Caltech 101 database [4] for the experiments: faces (435 im-ages from 23 persons) and cars (123 images of different cars). We set the parameters for MIM as:  X  = 1 4 | D I | , where | D is the total number of images, and  X  is associated with the confidence level of 0 . 90. Instead of setting threshold  X  , we select the top phrases by ranking their L ( P ) values. We set visual item codebook size |  X  | = 160 and 500 for car and face database respectively when doing k -means clustering. For generating the transaction databases T , we set K = 5 for searching spatial K-NN to constitute each transaction. All the experiments were conducted on a Pentium-4 3 . 19GHz PC with 1GB RAM running window XP.
To test whether our MIM algorithm can output meaning-ful patterns, we want to check if the discovered MI are asso-ciated with the frequently appeared foreground objects ( e.g. faces and cars) while not located in the clutter backgrounds. The following two criteria are proposed for the evaluation: (1) the precision of  X  :  X  + denotes the percentage of dis-covered meaningful itemsets P i  X   X  that are located in the foreground objects, and (2) the precision of  X   X  :  X   X  denotes the percentage of meaningless items W i  X   X   X  that are lo-cated in the background. Fig. 4 illustrates the concepts of our evaluation. In the ideal situation, if  X  + =  X   X  = 1, then every P i  X   X  is associated with the interesting object, i.e. located inside the object bounding box; while all meaning-less items W i  X   X   X  are located in the backgrounds. In such a case, we can precisely discriminate the frequently appeared foreground objects from the clutter backgrounds, through an unsupervised learning. Finally, we use retrieval rate  X  to denote the percentage of retrieved images that contain at least one MI.
 Figure 4: Evaluation of meaningful itemsets mining.
In Table 2, we present the results of discovering meaning-ful itemsets from the car database. The first row indicates the number of meaningful itemsets ( |  X  | ), selected by their L ( P ). It is shown that when adding more meaningful item-sets into  X  , its precision score  X  + decreases (from 1 . 00 to 0 . 86), while the percentage of retrieved images  X  increases (from 0 . 11 to 0 . 88). The high precision  X  + indicates that most discovered MI are associated with the foreground ob-jects. It is also noted that meaningful item codebook  X  + is only a small subset with respect to  X  ( |  X  | = 160). This implies that most visual items actually are not meaningful as they do not constitute the foreground objects. There-fore it is reasonable to get rid of those noisy items from the background. Examples of meaningful itemsets are shown in Fig. 9 and Fig. 10.

We further compare three types of criteria for selecting meaningful itemsets P into  X  , against the baseline of se-lecting the individual visual items W i  X   X  to build  X  . The three MI selection criteria are: (1) occurrence frequency:  X  f rq ( P ) (2) T-score (Eq. 9) (only select the second order itemsets, |P| = 2) and (3) likelihood ratio: L ( P ) (Eq. 2). The results are presented in Fig. 5. It shows the changes of  X  + and  X   X  with increasing size of  X  ( |  X  | = 1 , ..., 30). We can see that all three MI selection criteria perform sig-nificantly better than the baseline of choosing the most fre-quent individual items as meaningful patterns. This demon-strates that FI and MI are more informative features than the singleton items in discriminating the foreground objects from the clutter backgrounds. This is because the most fre-quent items W i  X   X  usually correspond to common features ( e.g. corners) which appear frequently in both foreground objects and clutter backgrounds, thus lacking the discrim-inative power. On the other hand, the discovered MI is the composition of items that function together as a sin-gle visual pattern (incomplete pattern though) which cor-responds to the foreground object that repetitively appears in the database. Among the three criteria, occurrence fre-quency  X  f rq ( P ) performs worse than the other two criteria, which further demonstrates that not all frequent itemsets are meaningful patterns. It is also shown from Fig. 5 that when only selecting a few number of MI, i.e.  X  has a small size, all the three criteria yield similar performances. How-ever, when more MI are added, the proposed likelihood ra-tio test method performs better than the other two, which shows our MIM algorithm can discover meaningful visual patterns. Figure 5: Performance comparison by applying three By taking advantage of the FP-growth algorithm for closed FIM, our pattern discovery is very efficient. It costs around 17 . 4 seconds for discovering meaningful itemsets from the face database containing over 60 , 000 transactions (see Ta-ble 3). It thus provides us a powerful tool to explore large object category database where each image contains hun-dreds of primitive visual features.
 Table 3: CPU computational cost for meaningful item-435 62611 1.6 sec 17.4 sec
To implement NCA for metric learning, we select 5 mean-ingful itemsets from  X  ( |  X  | = 10). There are in total less than 10 items shared by these 5 meaningful itemsets for both face and car categories, i.e. |  X  + | &lt; 10. For each class, we se-lect the qualified visual primitives as training samples. Our objective of metric learning is to obtain a better represen-tation of the visual primitives, such that the the inter-class distance is enlarged while the intra-class distance is reduced among the self-labeled training samples.

After learning a new metric using NCA, we reconstruct the visual item codebook  X  through k -means clustering again, with the number of clusters slightly less than before. The comparison results of the original visual item codebooks and those after refinement are shown in Fig. 6. It can be seen that the precision  X  + of  X  is improved after refining the item codebook  X  . Figure 6: Comparison of visual item codebook before
For both car and face categories, we select the top-10 meaningful itemsets by their L ( P ) (Eq. 2). All discovered MI are the second-order or third-order itemsets composed of spatially co-located items. We further cluster these 10 MI ( |  X  | = 10) into meaningful visual patterns using the normal-ized cut. The best summarization results are shown in Fig. 7 and Fig. 8, with cluster number | H | = 6 and | H | = 2 for the face and car category respectively. For the face category, the semantic parts like eyes, noses and mouths are identified by various patterns. For the car category, the wheels and car bodies are identified.

To evaluate our pattern summarization results, we apply the precision and recall scores defined as follows: Recall = # detects / (# detects + # miss detects) and Precision = # detects /( # detects + # false alarms). From Fig. 7 and Fig. 8, it can be seen that the summarized meaningful visual patterns H i are associated with semantic patterns with very high precision and reasonably good recall score.
Traditional data mining techniques are not directly ap-plicable to image data which contain spatial information and are characterized by high-dimensional visual features. To discover meaningful visual patterns from image data, we present a new criterion for discovering meaningful item-sets based on traditional FIM. Such meaningful itemsets are statistically more interesting than the frequent itemsets. By further clustering these meaningful itemsets (incomplete sub-patterns) into complete patterns through normalized cut, we successfully discover semantically meaningful visual patterns from real images of car and face categories.
In order to bridge the gap between continuous high dimen-sional visual features and discrete visual items, we propose a self-supervised clustering method by applying the discov-ered meaningful itemsets as supervision to learn a better feature representation. The visual item codebook can thus be increasingly refined by taking advantage of the feedback from the meaningful itemset discovery. Pair-wise Dependency Test .

If W i , W j  X   X  are independent, then the process of randomly generating the pair { W i , W j } in a transaction T is a (0/1) Bernoulli trial with probability P ( W i , W j P ( W i ) P ( W j ). According to the central limit theory, as the number of trials (transaction number N ) is large, the Bernoulli distribution can be approximated by the Gaussian random variable x , with mean  X  x = P ( W i ) P ( W j ). At the same time, we can measure the average frequency of { W i , W j } by counting its real instance number in T , such observation P ( W i , W j ) is drawn from the Gaussian distrib-ution x with mean  X  x , the following T-score is calculated; S 2 is the estimation of variance from the observation data. t ( { W i , W j } ) = P ( W i , W j )  X   X  x This work was supported in part by National Science Foun-dation Grants IIS-0347877 and IIS-0308222. The authors thank the anonymous reviewers for their helpful comments. H 1: left eyes 
