 Privacy is becoming an increasingly importa nt issue in many data mining applications. A considerable amount of work on privacy preserving data mining [2,1,11,10] has been investigated recently. Among them, randomi zation has been a primary tool to hide sen-sitive private data for privacy preserving data mining. The issue of maintaining privacy Most of techniques are based on a data perturbation or Randomized Response (RR) ap-proach [5], wherein the 0 or 1 (0 denotes absence of an item while 1 denotes presence of an item) in the original user transaction vector is distorted in a probabilistic manner that is disclosed to data miners.

In [13,4,3], the authors proposed the MASK technique to preserve privacy for fre-quent itemset mining and addressed the issue of providing efficiency in calculating the estimated support values. Their results empirically showed a high degree of privacy to users and a high level of accuracy in the min ing results can be simultaneously achieved. To evaluate the privacy, they defined a priv acy metric and presented an analytical for-mula for evaluating the privacy obtained under the metric. However, accuracy metric on data mining results was only defined in an aggregate manner as support error and identity error computed over all discovered frequent itemsets.

Our paper moves one step further to address the issue of providing accuracy in pri-vacy preserving mining of a ssociation rules. We investigate the issue of how the accu-racy (i.e., support and confidence) of each a ssociation rule mined from randomized data is affected when the randomized response technique is applied.

Specifically, we present an analytical fo rmula for evaluating the accuracy (in terms of bias and variance of estimates) of both support and confidence measures of associ-ation rules derived from the randomized data. From the derived bias and variance of estimates, we further derive approximate interquantile ranges. Data miners are ensured that their estimates lie within these ranges with a high confidence, say 95%. We would emphasize that providing confidence on estimated data mining results is significant to data miners since they can learn how accurate their reconstructed results are. We illus-trate the importance of those estimated interquantile ranges using an example.
Figure 1 shows the original support values, the estimated support values from the randomized data, and their c orresponding 95% interquantile ranges of 7 association and support threshold sup min = 23% were used in the experiment. The interquantile range of each rule can give data miners confi dence about their estimate derived from randomized data. For example, the estimated support of rule 2 is 31.5% and its 95% interquantile range is [23.8%,39.1%], whic h suggests the original support value lies in this range with 95% probability. Furthermore, we can observe the 95% interquantile ranges for rules 1-3 are above the support threshold, which guarantees those are true frequent itemsets (with at least 95% confidence).

We emphasize providing accuracy of data mi ning results is important for data miners during data exploration. When the support threshold is set as 23%, we may not only take rule 2 and 6 as frequent sets from the estimated support values, but also conclude rule 6 (35.9%) is more frequent than rule 2 (31.5%). However, rule 2 has the original support as 36.3% while rule 6 has the original support as 22.1%, we mistakenly assign the infrequent itemset 6 as frequent. By using the derived interquantile ranges, we can determine that rule 2 is frequent with high confidence (since its lower bound 23.8% is above the support threshold) and rule 6 may be infrequent (since its lower bound 12.3% is below the support threshold).

The remainder of this paper is organized as follows. In Section 2, we present the dis-tortion framework and discuss how the Randomized Response techniques are applied to privacy preserving market association rule mining. We conduct the theoretical analysis on how distortion process affects the accu racy of both support and confidence values derived from the randomized data in Section 3. In Section 4, empirical evaluations on various datasets are given. We conclude our work in Section 5. 2.1 Association Rule Revisited Denoting the set of transactions in the database D by T = { T 1 ,  X  X  X  ,T n } and the set of items in the database by I = { A 1 ,  X  X  X  ,A m } . An association rule X X  X  ,where X , Y X  X  and X X  X  =  X  , has two measures: the support s defined as the s (100%) of the transactions in T contain X X  X  , and the confidence c is defined as c (100%) of the transactions in T that contain X also contain Y . 2.2 Randomization Procedure Let there be m sensitive items A 1 ,A 2 ,  X  X  X  ,A m , each being considered as one dichoto-mous variable with 2 mutually exclusive and exhaustive categories (0 = absence, 1 = presence). One transaction can be logically translated as a fixed-length sequence of 0 X  X  and 1 X  X . For each transaction, we apply the Warner RR model [15] independently on each item using different settings of distortion. If the original value is in the ab-the distortion probability matrix P j generally takes the form P j = In this paper, we follow the original Warner RR model by setting  X  0 =  X  1 = p j .This setting indicates users have the same level of privacy for both 1 X  X  and 0 X  X . In general customers may expect more privacy for their 1 X  X  than for their 0 X  X , since the 1 X  X  denote specific actions whereas the 0 X  X  are the default options.
 portions corresponding to item A j in the original (randomized) data set, where j = 1 ,  X  X  X  ,m .Wehave is  X  2.3 Estimating k -Itemset Supports We can easily extend Equation 1, which is applicable to one individual item, to com-pute the support of an arbitrary k -itemset. For simplicity, let us assume that we would compute the support of an itemset which contains the first k items { A 1 ,  X  X  X  ,A k } (The general case with any k items is quite straightforward but algebraically messy). (
A arranged in a fixed order. The combination vector corresponds to a fixed order of cell entries in the contingency table formed by the k -itemset. When we have k items, the number of cells in the k -dimensional contingency table is 2 k . Table 1(a) shows one A ( B ) is absent from a transaction. The vector  X  =(  X  00 , X  01 , X  10 , X  11 ) corresponds proportion of transactions which contain both A and B while  X  10 denotes the proportion of transactions which contain A but not B . The row sum  X  1+ represents the support frequency of item A while the column sum  X  +1 represents the support frequency of item B .

The original database D is changed to D ran after randomization. Assume  X   X  1 ,  X  X  X  , X  k is the probability of getting a response (  X  1 ,  X  X  X  , X  k ) and  X  the vector with elements  X  1 ,  X  X  X  , X  k arranged in a fixed order (e.g., the vector  X  sponds to cell entries  X  ij in the randomized contingency table as shown in Table 1(b) ), we can obtain where  X  stands for the Kronecker product.

Let P = P 1  X  X  X  X  X  P k , an unbiased estimate of  X  follows as where  X   X  is the vector of sample proportions corresponding to  X  and P  X  1 j denotes the known, they can only be utilized to estimate t he proportions of itemsets of the original data, rather than precisely reconstruct the original 0-1 data.

In this paper we follow the Moment Estimation method as shown in Equation 2 to get the unbiased estimate of the distribution for original data. This method has been broadly adopted in the scenarios where RR is used to perturb data for preserving pri-vacy. Although it has good properties as computational simplicity and unbiasedness, some awkward property exists due to random errors [5,6]. That is, the estimate may fall out of the parameter space, which makes the e stimate meaningless. This is one reason that Maximum Likelihood Estimation (MLE) is adopted to estimate the distribution in literature [6].

It has been proved in [6] that a good relation holds between these two methods in the scenarios of RR: The moment estimate is equal to the MLE estimate within parameter space. Based on that, we can know that moment estimate from Equation 2 achieves the Cram  X  e r-Rao bound as MLE does. Therefore, moment estimate is the minimum variance unbiased (MVU) estimator in RR contexts. Our later analysis on accuracy of association rule is based on such unbiased estimate under the assumption that the estimate is within parameter space. any individual association rule X X  X  . To derive their interquantile ranges, we also analyze the distributions of those estimates derived from the randomized data. 3.1 Accuracy on Support s From Equation 2, we know how to derive the estimate of support values of any itemset from the observed randomized data. Now we address the question how accurate the estimated support value is.

The whole contingency table is usually modeled as a multinomial distribution in statistics. When we have k items, the number of cells in the contingency table is 2 k . For each cell d ,where d =1 , 2 ,  X  X  X  , 2 k , it has a separate binomial distribution with the number of successes in a sequence of n independent 0/1 experi ments, each of which yields success with probability  X  i .When n is large enough (one rule of thumb is that by the normal distribution N ( n X  i ,n X  i (1  X   X  i )) .
  X  )100% interquantile range can be approximated as z  X / 2 is the upper  X / 2 critical value for the standard normal distribution.  X  var ( X   X  i 1  X  X  X  i k ) can be derived from the covariance matrix [5]: to the data size for estimation. While the data size is usually large in most market bas-ket analysis scenarios, it can be neglected.  X  2 represents the component of dispersion associated with RR distortion.
 the normal distribution of each cell. An (1  X   X  )100% interquantile range, say  X  =0 . 05 , shows the interval contains the original  X  i 1 ,  X  X  X  ,i m with 95% probability.
To illustrate this result, we use a simple example G  X  H (rule 2 in Figure 1). The proportion of itemsets of the original data is given as
Using the RR scheme presented in the previous section, with the distortion parame-ters p 1 = p 2 =0 . 9 , we get the randomized responses By applying Equation 2, we derive the unbiased estimate of  X  as The covariance matrix of  X   X  is unbiasedly estimated as The diagonal elements of the above matrix represent the variances of the estimated  X   X  ,
From Result 1, we can derive 95% interquantile range of s GH as We can also see this derived interquantile range [0.346, 0.378] for rule 2 with p 1 = p 2 =0 . 9 is shorter than [0.238, 0.391] with p 1 = p 2 =0 . 65 as shown in Figure 1. 3.2 Accuracy on Confidence c We first analyze the accuracy on confide nce of a simple association rule A  X  B where A and B are two single items which have 2 mutually exclusive and exhaustive cate-gories. We denote s A , s B ,and s AB as the support values of A , B ,and AB respectively. ized data of A , B ,and AB respectively.
 Result 2. The confidence ( c ) of a simple association rule A  X  B has estimated value as with the expectation of  X  c approximated as and the variance of  X  c approximated as according to the delta method [12].
 Confidence can be regarded as a ratio ( W ) of two correlated normal random variables ( X, Y ), W = X/Y . However, it is hard to derive the critical value for the distribu-tion of W from its cumulative density function F ( w ) [14], we provide an approximate interquantile range of confidence based on Chebyshev X  X  Inequality. Theorem 1. (Chebyshev X  X  Inequality ) For any random variable X with mean  X  and variance  X  2 Chebyshev X  X  Inequality gives a conservative estimate. It provides a lower bound to the proportion of measurements that are within a certain number of standard deviations from the mean.
 Result 3. The loose (1  X   X  )100% interquantile range of confidence ( c )of A  X  B can be approximated as From Chebyshev X  X  Inequality, we know for any sample, at least (1  X  1 /k 2 ) of the ob-servations in the data set fall within k standard deviations of the mean. When we set  X   X  var ( X  c ) (from Equation 4) as an estimate of  X  ,where  X  and  X  are unknown parame-ters of the distribution of confidence. An approximate (1  X   X  )100% interquantile range of confidence c is then derived.

All the above results can be straightforwardly extended to the general association rule X X  X  and further details can be found in [9]. In our experiments, we use the COIL Challenge 2000 which provides data from a real insurance business. Information about customers consists of 86 attributes and in-cludes product usage data and socio-demographic data derived from zip area codes. The training set consists of 5822 descriptions of customers, including the information of whether or not they have a Caravan insurance policy. Our binary data is formed by collapsing non-binary categorical attributes into binary form (the data can be found at www.cs.uncc.edu/  X  xwu/classify/b86.dat), with n = 5822 baskets and m =86 binary items. 4.1 Accuracy of Individual Rule vs. Varying p Ta b l e 2 2 shows the 7 randomly chosen association rules derived from the randomized estimated confidence value. We have shown how the accuracy of the estimated support values varies in Figure 1 (Section 1). One observation is that interquantile ranges of confidence estimates are usually wider than that of support estimates. For example, three reasons. First, we set the distortion parameter p =0 . 65 which implies a relatively large noise (the perturbed data will be completely random when p =0 . 5 ). Second, the variance of the ratio of two variables is usually larger than the variance of either single variable. Third, the estimated support can be modeled as one approximate normal the loose interquantile range of confidence using the general Chebyshev X  X  Theorem. We expect that the explicit form of the F ( w ) distribution can significantly reduce this width. We will investigate the explicit form of the distribution of confidence and all other measures, e.g. correlation, lift, etc. to derive tight bounds in our future work.
Our next experiment shows how the derived estimates (support, confidence, and their corresponding interquantile ranges) of one i ndividual rule vary with the distortion pa-rameter p . We vary the distortion parameter p from 0.65 to 0.95. Figure 2(a) (2(b)) shows the accuracy of the estimated support (c onfidence) values with varied distortion p values for a particular rule G  X  H . As expected, the larger the p , the more accu-rate the estimate and the tighter the interquantile range is. It was empirically shown in provide both privacy and good data mining results for the sparse market basket data. We can observe from Figure 2(b) that the 95% interquantile range of the confidence estimate with p  X  0 . 9 is tight. 4.2 Accuracy of All Rules vs. Varying p The above study of the accuracy of the estim ate in terms of each individual rule is based accuracy of data mining results using the av erage support error, the average confidence 100 indicates the percentage of false positives and  X   X  = | F  X  R | percentage of false negatives where R ( F ) denotes the reconstructed (actual) set of we define the following three measures.  X  c-p: the number of pairs of conflict confidence estimates (similarly defined as the
Errors in support estimation due to the distortion procedure can result in falsely iden-tified frequent itemsets. This becomes especially an issue when the support threshold setting is such that the suppor t of a number of frequent itemsets lie very close to this threshold value ( s min ). Such border-line itemsets can cause many false positives and passes has a ripple effect in terms of causing errors in later passes.

Table 3(a) shows how the above measures are varied by changing distortion parame-ter p from 0.65 to 0.95. We can observe all measures (the support error  X  , the confidence number of conflict support pairs (s-p) and conflict confidence pairs (c-p) also have the same trend. Our experiment shows that when p  X  0 . 85 , there are no or very few conflict support (confidence) pairs, which implies the reconstructed set of association rules is close to the original set. However, when p  X  0 . 80 , there are significant number of con-flict pairs, which implies the reconstructed set may be quite different from the original one. By incorporating the derived interquantile range for each estimate, we can de-crease the error caused by conflict pairs. In Section 1, we have shown one conflict sup- X  s 6 l &lt;s min , data miners can safely determine rule 2 is frequent but rule 6 may be infrequent. We would emphasize again that providing estimates together with their in-useful for data exploration tasks conducted on the randomized data.

Table 3(b) shows the comparison between the identity errors derived using lower constructed set of association rules us ing lower (upper) bound of interquantile range is significantly lower than  X   X  while  X  + l is significantly lower than  X  + .Inotherwords, using the upper bound of the derived interquantile range can decrease the false nega-tives while using the lower bound can decrease the false positives. In some scenario, we may emphasize more on decreasing the false positive error. Hence, we can use the lower bound of the derived interquantile range, rather than the estimated value, to de-otherwise). 4.3 Other Datasets Since the COIL Challenge data is very sparse (5822 tuples with 86 attributes), we also conducted evaluations on the following representative databases used for association rule mining. 1. BMS-WebView-1 3 . Each transaction in the data set is a web session consisting of all 2. A synthetic database generated from the IBM Almaden market basket data gener-
Tables 4 and 5 show our results on these two data sets respectively. We can observe similar patterns as shown in COIL data set. In this paper, we have considered the issu e of providing confidence ranges of support and confidence in privacy preserving associ ation rule mining. Providing the accuracy of discovered patterns from randomized data is important for data miners. To the best of our knowledge, this has not been previously explored in the context of privacy pre-serving data mining.

Randomization still runs certain risk of disclosures. It was observed as a general phenomenon that maintenance of item privacy and precise estimation were in conflict. We will investigate how to determine distortion parameters optimally to satisfy both privacy and accuracy constraints. We will e xplore some scenario where some sensitive items are randomized while the remaining are released directly or where some transac-tions are randomized while the remaining are unperturbed. We also plan to investigate the extension of our results to generalized and quantitative association rules. This work was supported in part by U.S. National Science Foundation IIS-0546027.
