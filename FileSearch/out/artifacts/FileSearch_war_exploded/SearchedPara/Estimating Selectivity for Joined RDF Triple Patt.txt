 A fundamental problem related to RDF query processing is selectivity estimation, which is crucial to query optimiza-tion for determining a join order of RDF triple patterns. In this paper we focus research on selectivity estimation for SPARQL graph patterns. The previous work takes the join uniformity assumption when estimating the joined triple patterns. This assumption would lead to highly inaccurate estimations in the cases where properties in SPARQL graph patterns are correlated. We take into account the depen-dencies among properties in SPARQL graph patterns and propose a more accurate estimation model. Since star and chain query patterns are common in SPARQL graph pat-terns, we first focus on these two basic patterns and propose to use Bayesian network and chain histogram respectively for estimating the selectivity of them. Then, for estimat-ing the selectivity of an arbitrary SPARQL graph pattern, we design algorithms for maximally using the precomputed statistics of the star paths and chain paths. The experiments show that our method outperforms existing approaches in accuracy.
 H.2.4 [ Systems ]: Query processing Performance SPARQL Query Processing, Selectivity Estimation
The Resource Description Framework (RDF) is a stan-dard format for encoding machine-readable information on the Semantic Web. Recently, more and more data is be-ing stored in RDF format. RDF data is a set of triples and each triple called statement is of the form ( subject , property , object ). RDF data can be represented as a graph with nodes representing resources or their property values and labeled arcs representing properties. This data repre-sentation is general and flexible. However, this fine-grained model leads to queries on RDF data with a large number of joins, which is an inherent characteristic of querying RDF data [1].

Since the use of RDF to represent data has grown dramat-ically over the last few years, query processing on RDF data becomes an important issue in realizing the semantic web vision. Some query languages such as SPARQL have been developed. As we know, accurate estimation of the result size of queries is crucial to query processing. Cost-based query optimizers use estimated intermediate result size to choose the optimal query execution plan.

As a SPARQL query has a large number of joins, esti-mating precisely the joined triple pattern is very important. Some work has been done in this area. In [4, 9] the join uniformity assumption is made when estimating the joined triple patterns with bound subjects or objects (i.e., the sub-jects or objects are concrete values). They assume that each triple satisfying a triple pattern is equally likely to join with the triples satisfying the other triple pattern. However, this assumption does not hold in many cases. And when the data are inconsistent with this assumption, it could cause a highly inaccurate estimation.

For example, in Figure 1, a SPARQL query is posed on an RDF database, which retrieves academic staff members and the courses they teach with some conditions. Suppose we want to estimate the selectivity sel ( t 1 t 3 )ofthejoined triple pattern t 1 t 3 .Itwould overestimate the result size sel ( t 1 t 3 ) using the formula (1) proposed in [4] as follows: where S P is the result upper bound of the joined triple pat-tern ( ?Z, Income, ?W )( ?Z, Position, ?Y )and | T | is the number of triples in the database. sel ( Income , X   X  70 K  X ) and sel ( Position ,  X  X rof. X ) are the object selectivities of t and t 3 . It assumes that each triple satisfying triple pat-tern t 1 is equally likely to join with triples satisfying triple pattern t 3 . But in fact, the triple matching t 3 which indi-cates that the person is a professor, who is supposed to have higher income ( X  &gt; 70 k  X ). Thus, the triples matching t less likely to join with triples matching t 3 .

There are two observations from this example. First, we can observe that the join uniformity assumption is not ap-plicable to those cases where properties have correlations (dependencies) with each other. In fact, this assumption is rarely satisfied in real data, so we need a more accurate model to relax this assumption for estimating the result size of a SPARQL query. Second, the query in Figure 1 is a composition of a chain query pattern and a star query pat-tern, and these two types of patterns are very common in SPARQL queries. It is desirable to find appropriate ways to estimate the selectivity of these common patterns.
Based on the observations, we propose new methods for selectivity estimation of SPARQL queries. The contribu-tions of this paper can be summarized as follows:
The remainder of this paper is organized as follows. Sec-tion 2 introduces some preliminary knowledge. In Section 3 and Section 4, we propose two methods to estimate the selec-tivity of star and chain query patterns, respectively. Section 5 presents the algorithms for estimating the selectivity of arbitrary composite SPARQL graph patterns. Section 6 de-scribes an experimental evaluation of our approach. Some related work is discussed in Section 7. At last, in Section 8 we conclude our work in this paper.
A triple ( s, p, o )  X  ( I  X  B )  X  ( I  X  B )  X  ( I  X  B  X  L ) is called an RDF triple, where I is a set of IRIs (Internationalized URIs), B a set of blank nodes and L a set of literals. In the triple, s is called subject, p the property (or predicate), and o the object or property value. An RDF triple pattern variables disjoint from the sets I , B and L .An RDF graph patterns. A SPARQL query can be represented as an RDF graph pattern and called a SPARQL graph pattern.

Problem Definition. The problem that we tackle in this paper can be summarized as follows: GivenanRDF database D and a SPARQL graph pattern Q ,weestimate the selectivity sel ( Q ) of Q . sel ( Q ) stands for the count of results in database D satisfying Q . In this paper, our interest is to estimate the selectivity of joined triple patterns , which will be discussed in the following sections.
In this section, we present the method of using Bayesian networks to estimate the selectivity of star query patterns.
Star query pattern that commonly occurs in SPARQL queries is used to retrieve entities with some constraints. It has the form of a number of triple patterns with differ-ent properties sharing the same subject variable. Normally, the properties involved in a star query patterns are corre-lated. For example, Figure 2 shows a star query pattern Q and the properties Income , Education , Position , TeacherOf are correlated. Thus, as shown in the introduction, the join uniformity assumption would cause a highly inaccurate se-lectivity estimation for star query patterns.

For estimating the selectivities of star query patterns pre-cisely, we need some helpful statistics. In this paper, we precompute statistics for star paths. A star path is a set of properties which share the same domain. Given a star path with properties prop 1 , prop 2 ,  X  X  X  , prop n ,weuse( prop prop 2 ,  X  X  X  , prop n ) to denote. In Figure 2, Q s ( Income , Edu-cation , Position , TeacherOf ) is a star path. Since the num-ber of possible star paths could be huge (2 n ,where n is the number of properties in the database), it is impossible to precompute statistics for all star paths. In this paper, we target the star paths in which properties are strongly corre-lated.

We assume that if a star path appears in the data graph frequently then the properties contained in this star path are strongly correlated. This assumption is also made in data mining field to find association rules. Using the algorithm presented in [9], we can find frequent star paths. A fre-quent star path is logically associated with a table R called cluster-property table [2] that contains a set of entities to be queried. For example, Figure 2 shows a cluster-property table of star path Q s . Each row of the table is an academic staff with values of properties Education , Income , Position and TeacherOf . Note that different with [2], we do not store cluster-property tables for frequent star paths. More com-pressed structures would be constructed for selectivity esti-mation.

Given a frequent star path Q s ( prop 1 , prop 2 ,  X  X  X  , prop and its cluster-property table R , the selectivity of star query Q (? x , prop 1 , o 1 )(? x , prop 2 , o 2 )  X  X  X  ,(? x , prop by sel ( Q ), can be computed as follows: sel ( Q )=Pr( prop 1 = o 1 ,prop 2 = o 2 , ..., prop n = o where Pr( prop 1 = o 1 ,prop 2 = o 2 , ..., prop n = o n ) denotes the joint probability distribution over object values of prop-erties prop 1 , prop 2 ,  X  X  X  , prop n in table R and | R | number of rows in the cluster-property table R .

Since the number of rows | R | is easy to know, we will focus on computing the joint probability distribution Pr( prop 1 o , prop 2 = o 2 ,  X  X  X  , prop n = o n ). Unfortunately, it is im-possible to explicitly store Pr( prop 1 = o 1 , prop 2 = o prop n = o n ) because the possible combinations of property values would be exponential. Thus, we need an appropri-ate structure to approximately store the joint probability distribution information.

The Bayesian network [3] can approximately represent the probability distribution over a set of variables using a lit-tle space. Bayesian networks make use of Bayes X  Rule and conditional independence assumption to compactly repre-sent the full joint probability distribution. Let X , Y , Z be three discrete valued random variables. We say that X is conditionally independent of Y given Z if the probability distribution of X is independent of the value of Y given a value for Z ;thatis:
Pr( X = x i | Y = y j ,Z = z k )=Pr( X = x i | Z = z k ) where x i , y j , z k are values of variables X , Y , Z . The condi-tional independence assumptions associated with a Bayesian network and conditional probability tables (CPTs), deter-mine a joint probability distribution. For example, in Fig-ure 3, a Bayesian network is constructed on cluster-property tableinFigure2.Wecanseethatproperties Education and Income are conditionally independent given condition Posi-tion , which means if we already know the position of some person, knowing his education information does not make any difference to our beliefs about his income.

For a star query pattern Q with properties and object values prop 1 = o 1 , prop 2 = o 2 ,  X  X  X  , prop n = o n ,givena Bayesian network  X  ,wehave: where Parents ( prop i ) denotes the set of immediate prede-cessors of prop i in the network and o k denotes the set of val-ues of Parents ( prop i ). Note that for computing Pr( prop Parents ( prop i )= o k ), we only need to know the values of prop i  X  X  parent properties, which would save a lot of space in practice. So given the Bayesian network  X  ,wecanuse Pr  X  ( prop 1 = o 1 ,prop 2 = o 2 , ..., prop n = o n )to approxi-mately represent Pr( prop 1 = o 1 ,prop 2 = o 2 , ..., prop We have: sel ( Q )=Pr( prop 1 = o 1 ,prop 2 = o 2 , ..., prop n = o Example 1. Given the star pattern Q ( ?Z, income,  X   X 70K X  ) ( ?Z, Education,  X  X hD X  )( ?Z, Position,  X  X rof. X  )( ?Z, TeacherOf,  X  X dc X  ) shown in Figure 2 and Bayesian network described in Figure 3, we compute the selectivity of Q as follows: sel ( Q )=Pr( Edu = X  X hD X  ,Pos = X  X rof. X  ,Inc  X   X 70K X  ,
To approximately represent the joint distribution of prop-erty values for selectivity estimation, we first generate the cluster-property tables for frequent star paths, and then Bayesian networks can be learned from these tables. From the Aprior property, we know that if a star path is frequent then its sub paths are also frequent. Given a frequent star path ( p 1 ,p 2 ,p 3 ) and one of its sub path ( p 1 ,p 3 equal frequencies, namely | ( p 1 ,p 2 ,p 3 ) | = | ( p 1 cates that they have the same instance set. In this case, the information contained in the cluster-property table of path ( p 1 ,p 3 ) is fully covered by the cluster-property table of path ( p 1 ,p 2 ,p 3 ) and we do not need to generate the cluster-property table for path ( p 1 ,p 3 ). Note that we will drop all cluster-property tables once Bayesian networks have been constructed.

Before building Bayesian networks, the property values are first clustered into equi-width subsets (abstracted val-ues). Bayesian network construction algorithms can be grouped into two categories: one category of algorithms uses heuristic searching methods to construct a model and then evaluates it using a scoring method [5, 6, 8]. The other category of algorithms constructs Bayesian networks by analyzing de-pendency relationships among nodes. The dependency re-lationships are measured by using some kind of conditional independence (CI) test. In this paper we adopt the algo-rithm in [7], which employs mutual information to measure how close the relationship between two variables. The mu-tual information I ( X,Y )oftwovariables X , Y is defined as follows: and the conditional mutual information is defined as
I ( X,Y | M )= where M is a set of observed variables. When I ( X,Y | M ) is smaller than a threshold , X and X are conditionally independent and we also call X and X are d-separated by the condition set M .

The construction algorithm contains three steps. The first step is to compute mutual information of each pair of nodes as a measure of closeness, and creates a draft graph based on this information. The draft graph is a singly connected graph (a graph without loops). In the second phase, the algorithm adds edges when the pairs of nodes cannot be d-separated. In the third step, each edge of the Independent-map is examined using CI tests and will be removed if the two nodes of the edge can be d-separated. At the end of the third phase, the algorithm also carries out a procedure to orient the edges of the graph. For details on specific three steps, please see [7]. The time complexity of this algorithm is O ( n 4 ), where n is the number of properties involved in the cluster property table. In our problem, it requires that the conditional probability tables learned fit in a small amount of memory, therefore we restrict the threshold for space limit when building Bayesian networks. The other kind of correlated properties commonly occur in SPARQL graph patterns is chain path . A chain path consists of a sequence of triple patterns where the object of a triple pattern is also the subject of the next triple pattern. For ex-ample, in Figure 4, (?course, TakenBy, ?student)(?student, Age, ?age) is a chain path. Note that triple patterns of a chain path have only unbound subjects and objects. Given a chain path with a sequence of properties p 1 ,  X  X  X  , p n use ( p 1 -p 2 - X  X  X  -p n ) to denote it. For instance, in Figure 4, ( TakenBy-Age ) is a chain path and tuples in the data graph matching this chain path fall in the subject-property matrix table [10] of this chain path.

A chain query pattern is a chain path with possible lit-eral constraints on the start node (the subject of the first triple pattern) or end node (the object of the last triple pat-tern) in the chain path. For example, c 1 (?course, TakenBy, ?student)(?student, Age,  X 22 X ) shown in Figure 4 is a chain query pattern. The straightforward way to estimate the se-lectivity of a chain query pattern is combining the selectiv-ity of each triple pattern in the product form. Obviously, it would be highly inaccurate if the properties are correlated.
We first select frequent chain paths and construct the his-togram for them. For these frequent chain paths, we can construct a chain frequency table CFT (shown in Figure 5), which has two attributes: Abstracted chain query pattern and Frequency . The values of attribute Abstracted chain query pattern are abstracted chain query pattern descrip-tors consisting of frequent chain paths with possible value combinations of the start node and end node. Note that the possible values occurring on the start node and end node of paths are clustered into equi-width subsets called abstracted values. Each value of attribute Frequency is the count of tu-ples that match the abstracted chain pattern query in the data graph. We can see that each row of CFT indicates an abstracted chain query pattern with its frequency, so it is easy to get the selectivity of abstracted chain patterns from CFT . For example, the first tuple ( Prc/p1/20-22 , 26 )of chain frequency table shown in Figure 5 indicates a chain query pattern (  X  X rc X , TakenBy, ?student )( ?student, Age,  X 20-22 X  ) with its frequency 26 . However, chain frequency table CFT could be too large to fit in a small amount of main memory. Thus, we construct the new data struc-ture called chain histogram to approximate the selectivity of chain query patterns.
 Figure 5: An example of the chain frequency table and the chain histogram.  X ? X  indicates a variable. We first group the abstracted chain query patterns in CFT into B buckets according to their frequencies. That means the abstracted chain query patterns with similar fre-quencies are grouped into one bucket, which is helpful to guarantee the accuracy of estimation. For example, in Fig-ure 5, abstracted chain patterns ( Prc/p1/20-22 )and( Adc/p1/ 23-25 ) are grouped into one bucket since they have similar frequencies. For each bucket we only need to save the aver-age frequency and its abstracted chain query pattern mem-bers. Given an abstracted chain query pattern and which bucket it belongs to, we can easily get the approximate fre-quency of this chain query pattern.

To space-efficiently store the elements of each bucket and accelerate the membership query processing, we employ the bloom filter technique. Bloom filter [11, 12] is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. Here we use bloom filters to store buckets and construct the chain histogram. An ex-ample of chain histogram is shown in Figure 5.
Givenachainquerypattern C and a histogram H ,for estimating the selectivity of C , we first map the basic level values of start node and end node of C to their abstracted values and generate the abstracted chain query pattern C a For example, if a chain query pattern C ( ?course, Takenby, ?student )( ?student, Age,  X 22 X  ) is posed on the database, we map the value  X  22  X  X oitsabstractedvalue X  20-22  X  X ndget the abstracted chain query pattern C a ( ?y, Takenby, ?stu-dent )( ?student, Age,  X 20-22 X  ). The uniformity assumption is made here when computing the coefficient  X  between the selectivities of C and C a . We can acquire the approximate selectivity of chain query pattern C a from the chain his-togram H . Note that though it is rare, it is possible that the pattern C a is reported to fall in multiple buckets due to the false positive error of the bloom filter. In this case, we simply return the count of the first bucket which reports that the pattern C a is its member. This process is given in Algorithm 1 GetChainSel . The time complexity of obtain-ing the selectivity of a chain pattern is O ( KB ), where K is the maximal number of hash functions used in a bloom filter and B is the number of buckets (or bloom filters) in the chain histogram.
 Algorithm 1 GetChainSel 7: return 0;
For example, given a chain query pattern ( ?y, Takenby, ?student )( ?student, Age,  X 22 X  ), we compute the selectivity of this chain query pattern as follows:
In this section, we discuss how to estimate the selectivity of an arbitrary composite graph pattern with the maximum use of the precomputed statistics of star and chain paths.
From previous sections, we know that the precomputed statistics of the star and chain paths can help to estimate the selectivity of star and chain query patterns respectively. As a SPARQL graph pattern SC (an example is shown in Figure 6) is a composite graph pattern, we decompose it into a set of star and chain query patterns and then use the precomputed statistics of star and chain paths to obtain the overall selectivity. For maximally using precomputed statis-tics, we hope to obtain a set of edge-disjoint precomputed star and chain paths as building blocks that constitute a subgraph SC of graph pattern SC , which has the largest number of edges.

Thereisaspecialcaseweneedtoconsider. ASPARQL query may include non-property-bound triple patterns. For this case, we remove such triple patterns from graph pat-terns (reserving join nodes) and estimate the selectivities of these triple patterns as single triple patterns. At last, we combine the selectivity of the whole graph pattern with the independent assumption.

We use PS to refer to the set of star and chain paths whose statistics have been precomputed. Removing a path p from graph Q , denoted by Q \ p , consists of removing all edges of p from Q , followed by removing all stand-alone nodes. Given a query pattern Q and precomputed path set PS ,the maximum path cover of Q with respect to PS denoted by Cov PS ( Q ) is a subset of edge-disjoint paths in PS ,which can constitute a subgraph Q of Q such that Q has the largest number of edges. Note that Q could be Q itself. Algorithm 2 GreedyFindpathCover 5: while not empty ( PQ ) do 10: if Q is empty then
Our problem is to obtain Cov PS ( Q ). It is similar to the classic 0-1 knapsack problem: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than a given limit and the total value is as large as possible.
In our problem, the items are precomputed chain paths and star paths. For a chain or star path item, the value of this item is the length of the path and the weight is just the path since we use this path as a building block. Now, we need to determine the status for each path item ( X  X icked X  or  X  X ot picked X ) so that the picked edge-disjoint paths (or sub paths) can constitute a subgraph Q of Q , which has the largest number of edges. The weight limit here is the query graph pattern Q .
 Algorithm 3 OptimalFC 1: Initial flag [1..n]= false ; 3: for k := 1 to n do 4: if flag [ k ]= true then 1: Function int maxCov ( n, Q ); 2: if n =0or Q = X  then 3: return 0; 4: if path PS [ n ] Q then 5: return maxCov ( n  X  1 ,Q ); 7: Q = Q \ PS [ n ]; 9: return maxCov ( n  X  1 ,Q \ PS [ n ]); 10: else 11: return maxCov ( n  X  1 ,Q );
The straightforward way of finding Cov PS ( Q ) is to adopt greedy strategy (presented in Algorithm 2 GreedyFindpath-Cover ), which is eager to use the longest paths in PS to constitute a subgraph of Q . The time complexity of the al-gorithm is O ( | PS | X | E | ), where | PS | is the number of paths in PS and | E | is the number of edges of Q . Due to the lo-cal optimum nature of the algorithm, GreedyFindpathCover would fail to achieve the global optimum (i.e., the maximum path cover of Q ) in some cases. However this algorithm is still of interest because it provides an efficient way to find a path cover of Q when graph pattern Q is huge and dense.
Now we propose an optimal algorithm based on dynamic programming . Dynamic programming is a method for solv-ing those problems which exhibit the property of optimal substructures by breaking them into overlapping subprob-lems. It is also applicable to our problem. Given a graph pattern Q ,werepresent PS as a path array PS [1 ..n ]that contains all precomputed chain paths, star paths. Let max-Cov(i,Q) be the number of edges of subgraph Q of Q such that Q consists of edge-disjoint paths from PS [1 ..i ]and has the largest number of edges. Here we use the last item i and the remaining graph pattern (current weight limit) to index subproblems. edgeNum ( PS [ j ]) indicates the edge number of path item j in PS . maxCov ( i, Q ) can be obtained through combining the optimal solutions of its subproblems. We have the following properties:  X   X   X   X   X   X   X   X   X   X   X   X   X 
Ob viously, when i = 0 or graph pattern Q is a null graph, maxCov ( i, Q )=0. Ifpath PS [ i ] is not a subgraph of Q , we drop PS [ i ]and maxCov ( i, Q ) is equal to its subprob-lem maxCov ( i  X  1 ,Q ); otherwise, maxCov ( i, Q ) is computed through comparing the values in two cases (where PS [ i ]is picked or not picked). Based on these properties, we present the algorithm in Algorithm 3 OptimalFC . The time com-plexity of our optimal algorithm is O ( | PS | X | Q | ), where is the number of subgraphs of Q , which is exponential to the number of edges of Q . In practice, if query graph pattern Q is sparse (i.e., the constituent vertices are of low degree) and not large, we can employ this optimal algorithm; otherwise, the greedy algorithm proposed can be used instead.
After a composite graph pattern is decomposed into a set of star and chain query patterns, how do we compute the selectivity of this graph pattern? To address this problem, let us first begin with a simple case shown in Figure 6.
In Figure 6, the composite graph pattern SC can be de-composed into a star query pattern S and a chain query pattern C whose corresponding star path and chain path have been computed. The variable ? Y is the join node. We use val ( C.Y ) to refer to the value set of variable C.Y and val ( S.Y ) for variable S.Y . Our aim here is to estimate the selectivity of SC denoted by sel ( SC ).

Using the Bayesian networks and the chain histogram con-structed, we can get sel ( S )and sel ( C ). However, estimat-ing precisely the selectivity of SC is not trivial. A tuple s matching star query pattern S may not join with a tuple c matching chain query pattern C , since they may have dif-ferent values on variable ? Y . For estimating precisely the selectivity of SC , it is crucial to know the probability distri-butions over val ( S.Y )and val ( C.Y ). However, obtaining the exact probability distributions of values over val ( S.Y )and val ( C.Y ) is hard without executing the star query pattern S and chain query pattern C . Fortunately, the probabil-ity distribution of abstracted values on the join variable ? Y (target variable) can be inferred from the Bayesian network.
For example, suppose we have the Bayesian network shown in Figure 3 and we can infer the probability distribution of abstracted values over val ( S.Y ) (i.e., the probability distri-bution over the object values of property TeacherOf ( TOf ) for short) as follows: We can see that 70 percent of tuples matching pattern S have value  X  Adc  X  and 30 percent of tuples have value  X  Prc  X  X nvariable? Y . Using the probability distribution over val ( S.Y ), we can easily compute the selectivity of SC . Recall that we use the chain histogram to estimate the se-lectivities of chain query patterns. Now we take the values  X  Adc  X  X nd X  Prc  X  as the values of start node of chain C sep-arately, then test chain query patterns  X  Adc/C  X ,  X  Prc/C  X  in the chain histogram and get the selectivities sel ( Adc/C ) and sel ( Prc/C ). Since bloom filter is efficient for answer-ing set membership queries, this process is time efficient. At last, we can obtain the selectivity of SC as 0 . 7  X  sel ( S ) sel ( Adc/C )+0 . 3  X  sel ( S )  X  sel ( Prc/C ). Thisprocessisde-scribed in Algorithm 4 ComSelStar-Chain .
 Algorithm 4 ComSelStar-Chain 1: count =0; 7: return count ; Algorithm 5 ComSelStar-Star 1: count =0; 7: return count ;
We have discussed the case where a composite graph pat-tern can be decomposed into one star pattern and one chain pattern. What if a composite graph pattern is decomposed into two star patterns (Figure 7(a)) or two chain patterns (Figure 7(b))? If a graph pattern S ( S 1 S 2 )isdecomposed into two star patterns S 1 and S 2 , we go through each possi-ble abstracted value a i on the join node ? Y of two star paths. Through inference on the Bayesian networks constructed, it is easy to acquire the probabilities Pr( Y = a i | M ( S 1 Pr( Y = a i | M ( S 2 )) such that M ( S 1 )and M ( S 2 ) are observed object values in two star patterns. We then get the selec-tivity of the composite graph pattern through cumulating the count of tuples from two star patterns which have the same values on the join node. The algorithm is described in Algorithm 5 ComSelStar-Star .

In the case where the graph pattern C ( C 1 C 2 )isde-composed into two chain query patterns C 1 , C 2 that join on variable ? Y , the method is similar (presented in Algorithm 6 ComSelChain-Chain ). We go through each possible ab-stracted value a i of the join node, and the selectivities of Figure 7: Join between chain patterns and star pat-terns, where  X ?Y X  is the join node.
 Algorithm 6 ComSelChain-Chain 1: count =0; 6: return count ; two chain query patterns with value a i can be obtained us-ing the chain histogram. sel ( C 1 C 2 | Y = a i ) is computed through combining sel ( a i /C 1 )and sel ( a i /C 2 ) in the prod-uct form. At last, we cumulate sel ( C 1 C 2 | Y = a i ) with all possible abstracted values. Since bloom filter is quick to get the selectivities of two chain patterns, this method is also time efficient.
 Algorithm 7 ComSelectivity 11: if Q is  X  then
Now we extend our method to the more general case where the path cover set Cov PS ( Q )ofquery Q consists of multiple star patterns and chain patterns. We select two patterns p and p j from Cov PS ( Q ) that can join together. Compute the selectivity for the composite pattern p ij = p i p j maintain a selectivity distribution table T p ij indicating the selectivity distributions over the possible abstracted values of the join node in p ij . Iterate this process until there is only one com-posite graph pattern left in Cov PS ( Q ). For the part of Q that is not covered by Cov PS ( Q ), we deal with it as a set of single triple patterns and combine the results of them in the product form with the independence assumption. The for-mal description of this algorithm is presented in Algorithm 7 ComSelectivity .
We implemented the proposed algorithms. All algorithms are run on a windows XP professional operating system. The hardware is a PC with Intel Pentium 4 3.0GHz CPU, 4 GB memory.
Data sets. Both synthetic and real-world datasets are used in our experiments. In this paper, we present results on three datasets: (1) LUBM [13] is developed by Lehigh University; (2) SwetoDBLP [14] is a dataset describing real computer science bibliography information. (3) YAGO [15] consists of facts extracted from Wikipedia. Compared with the former two datasets, it is relatively heterogeneous. The details of three datasets are shown in Table 1.

Query Loads. There are three kinds of query patterns used in our experiments: star, chain and composite query patterns. Figure 8 shows some information about query sets used in the experiments. We use the seed queries (part of them are shown in Appendix) with some random literal val-ues to generate different query sets.

Previous Techniques. We compare our method with two previous works: 1) In [4], the authors propose the se-lectivity estimation method PF based on the probabilistic framework. They build summary statistics of joined triple patterns for RDF data; 2) In [9], the authors propose the estimation method in RDF-3X system, which computes two kinds of statistics for selectivity estimation. The first one is called specialized histograms which assumes independence among predicates. The second statistics computes frequent join paths in the data.

Evaluation Method. We use relative error RE to present the performance of different techniques of selectivity estima-tion. Given a true selectivity sel and its estimate sel ,weuse the relative error formula RE ( sel, sel )= | sel  X  sel | itive queries ( sel &gt; 0) and negative queries ( sel =0).
Learning Bayesian Networks. Before evaluating our methods for star query patterns, we first constructed Bayesian networks for frequent star paths in three datasets. For LUBM dataset, we constructed cluster property tables for 6 com-mon star paths that correspond entities such as Faculty , Student , Course and so on. An entity may correspond sev-eral classes. Similarly, we constructed cluster property ta-bles for 7 common star paths) in DBLP dataset. Then Bayesian networks are learned from these cluster property tables. Clearly, with the increase of space for storing learned Bayesian networks, the constructed Bayesian networks and CPTs are more accurate over the data. Due to the efficiency consideration, we set the space limit of each dataset for stor-ing each Bayesian network to 16 KB. Some details about learning Bayesian networks are shown in Table 2. From Ta-ble 2, we can see that the learning time of Bayesian networks for three datasets are 127.3s, 232.2s, 2112.9s, respectively. In DBLP dataset, we have learned 7 Bayesian networks from cluster property tables and the largest cluster property ta-ble contains 362,907 tuples. However, the time of learning Bayesian network for this cluster property table is 72.2s, which is acceptable. For YAGO dataset, since the data is heterogeneous there are more frequent paths and we con-struct 100 Bayesian Networks for them.
 Table 2: Some information about leaning Bayesian networks #L UBM DBLP YAGO # Learned Bayesian Networks 6 7 100 # Maximum tuples in cluster property tables 100,532 362,907 161,496 # Properties in cluster property tables 2-9 2-8 2-8 Learning time (s) 227.3 372.7 2112.9
Space limit (KB) for storing each Bayesian Network 16 16 16
Constructing chain histograms. To employ our chain histogram based method to estimate the selectivity of chain patterns, we constructed chain histograms for chain paths of three datasets. The space limit for storing the chain his-togram of each dataset is 16 KB. Some details of construct-ing chain histograms are shown in Table 3.
 Table 3: Some information about chain histograms #L UBM DBLPYAGO # Chain paths Contained 124 323 500 Length of chain paths 2-5 2-7 2-5 Constructing time (s) 179 357s 840s
Space limit (KB) 16 16 16
In this set of experiments, we study and compare the ac-curacy of different selectivity estimation methods. Figure 9 shows the accuracy of three methods for different query sets on three datasets. In all figures, X-axis are different query sets and Y-axis is the average relative error RE. Performance of methods for star query patterns.
 In this experiment, we evaluate our method of selectivity estimation for star query patterns. For each dataset, 15 star query patterns are developed. They are grouped into 3 query sets: Qs 1, Qs 2, Qs 3(seeFigure8). Qs 1, Qs 2and Qs 3con-tain star query patterns that have 2, 3 to 4, and more than 5 properties, respectively. Each query set contains 5 star query patterns. The relative errors of different methods on three datasets are shown in Figure 9(a), (b) and (c).  X  X NM X  indicates our Bayesian network based selectivity estimation method for star query patterns.

From Figure 9(a), (b) and (c), we can see that our method for star patterns dominates the other methods because we capture the dependencies among properties in the queries. The RDF-3x method also have better results on query set Qs 1 in which query patterns have the small number of prop-erties and literal constraints. In this case, the join uniformity assumption does not affect much. Conversely, in query sets Qs 2and Qs 3, queries have more properties and literal con-straints. It is more likely that the join uniformity assump-tion affects the accuracy of estimation. Thus our Bayesian network based method obtains much better results on query sets Qs 2and Qs 3 than the other two methods that adopt the join uniformity assumption.
 Performance of methods for chain query patterns.
 From Figure 9(d), (e) and (f), we can see performance of methods on chain query patterns. For each dataset, 15 chain query patterns are developed. They are grouped into 3 query sets: Qc 1, Qc 2, Qc 3 (See Figure 8 for more details).  X  X HM X  indicates our chain histogram based method for estimating the selectivity of chain patterns. Our method dominates the other methods. The RDF-3x method also have better results on query set Qc 1 in which the lengths of chain query patterns are relatively small. Conversely, in query sets Qc 2and Qc 3, lengths of chain query patterns are longer. It is more likely that the join uniformity assumption affect the accuracy of estimation. Thus our method obtains much better results on query sets Qc 2and Qc 3 than the other two methods.
Performance of methods for composite query pat-terns. In this experiment, we evaluate our method for com-posite query patterns. We develop 20 composite query pat-terns for each dataset. All these queries are also grouped into 4 query sets: Qsc 1, Qsc 2, Qsc 3, Qsc 4. Some details of query sets are shown in Figure 8. Figure 9(g), (h) and (i) show the results of four methods on three datasets, where  X  X reedy X  and  X  X ptimal X  stand for two decomposition algo-rithms (shown in Algorithm 2 and Algorithm 3 respectively). Our methods obtain more accurate estimation since we con-struct the refined model when dealing with joined triple pat-terns and do not adopt join uniformity assumption. The optimal algorithm also outperforms the greedy algorithm because the latter may not find the maximum path cover of query patterns. And for the uncovered part of queries, greedy algorithm would adopt the independence assump-tion for selectivity estimation, which affects the estimation accuracy. However, figures show that Greedy algorithm still performs better than PF and RDF-3x methods.
In this experiment, we study the on-line running time of our estimation methods adopting two decomposition algo-rithms: Greedy algorithm and Optimal algorithm on YAGO dataset which is relatively heterogeneous. Figure 10 shows the average running time of two methods as the average number of edge of query sets increases. The running time of the method with the Greedy algorithm reveals a slow growth with the increase of the average number of edge of queries sets. We can observe that the time cost is reasonable even when the average number of edge of query set Qsc 4risesto 11. Compared with the Greedy algorithm, the running time of the method with the Optimal algorithm is higher because the Optimal algorithm tries to find the maximum path cover of each composite query pattern. The time costs of two cases are close when graph patterns are small (the number of edge 8), but the running time of Optimal algorithm rises sig-nificantly when the number of edge exceeds 8. It suggests the Greedy decomposition algorithm is preferable for larger and dense graph patterns. Selectivity estimation is well studied in relational databases. Histogram is one of the most important statistical estima-tion data structure in relational DBMSs. [17] offers a latest, comprehensive survey on this subject. Join Synopses [16] en-able accurate estimation by summarizing the combined join and value distribution across foreign-key joins of several ta-bles. In our work, we estimate the composite graph patterns by summing over the possible abstracted values of the joined variables and do not have the restriction of foreign-key joins.
In [4], the authors propose the framework of static Ba-sic Graph Pattern (BGP) optimization based on selectivity estimation. They devise a number of heuristics for the se-lectivity estimation of joined triple patterns. The heuristics range from simple variable counting techniques to more so-phisticated selectivity estimation based on the probabilistic framework. In [9] the authors propose two kinds of statis-tics for selectivity estimation. The first one is a histogram, which is generic and can handle any kind of triple patterns and joins with independence assumption. The second statis-tics computes frequent join paths in the data, which are similar to our methods. Nevertheless, [4, 9] both take the join uniformity assumption when estimating the selectivity of joined triple patterns. We avoid this assumption and propose a more accurate estimation model that takes corre-lations among properties into account.
With the increasing amount of RDF data, efficient and scalable management of RDF data has become a fundamen-tal challenge to achieve the semantic web vision. Selectivity estimation is critical to processing RDF queries efficiently. To address this problem, we employ the Bayesian network and chain histogram techniques for estimating the selectivity of star and chain query patterns. For an arbitrary compos-ite graph pattern, the algorithm which combines the pre-computed statistics of chain and star paths to estimate the overall selectivity is proposed. The experiments show that our method outperforms existing approaches in accuracy sig-nificantly. In the future, we will investigate the appropriate way to generalize our methods to support other applications in SPARQL query processing. This research was supported by the Australian Research Council Discovery Projects DP110102407 and DP0878405.  X  Qs1: ?s hasProductionLanguage  X %Random Language% X .  X  Qs2: ?s yagoResource:interestedIn  X %Random Interest% X .  X  Qc3: ?x0 hasChild ?x1. ?x1 hasPredecessor ?x2. ?x2  X  Qsc2: ?s type wordnet actor 109765278. ?s actedIn
