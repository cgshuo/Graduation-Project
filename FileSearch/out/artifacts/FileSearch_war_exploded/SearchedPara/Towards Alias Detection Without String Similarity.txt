 Entity aliases commonly exist and accurately detecting these aliases plays a vital role in various applications. In this pa-per, we use an active-learning-based method to detect en-tity aliases without string similarity. To minimize the cost on pairwise comparison, a subset-based method restricts the alias selection within a small-scale entity set. Within each generated entity set, an active learning based logistic regres-sion classifier is employed to predict whether a candidate is the alias of a given entity. The experimental results on three datasets clearly demonstrate that our proposed approach can effectively detect this kind of entity aliases.
Solving the problem of alias detection is important for a large number of applications including entity identifica-tion, terrorist detection, and social network analysis. Some aliases can be detected through string similarity measures, such as  X  X orld Trade Organization X  and its alias  X  X TO X . While another aliases have a quite low string similarity with their original entities, such as  X  X ill Clinton X  and its nick-name  X  X lick Willie X . Detecting the second type of entity aliases turns out to be the aim of this paper.

Some previous work [1][2][4][5] has investigated this prob-lem. These studies hold the same goal with ours but focus on a special domain respectively. This paper tries to detect en-tity aliases regardless of domains. To solve this problem, we employ an active-learning-based method taking three issues into consideration: picking the potential alias candidates for each given entity, choosing features and labeled samples to train a high-quality classifier. The contributions of this pa-per include: 1) a subset-based method to reduce the cost of pairwise comparison when picking alias candidates; 2) an active learning method, which selects informative samples in training classifier; 3) experiments on three types of datasets and comparison with other four baseline methods.
Given a document corpus D and an entity e , the task of alias detection is to extract all the entities from D which denote the same real-world object with e . Firstly, since the given entity may have no/(quite low) string similarity with its aliases, and especially certain type of aliases (e.g., terror-ist or ghostwriter) are intentionally hidden from their real identities, the commonly used rules[1] (e.g.,  X  X ka X ,  X  X s well known as X , and  X  X lso called X ) will not always work. Sec-ondly, it is difficult to locate entity aliases, since the number of aliases of an entity is rather fewer compared to the mil-lions of strings/entities in the document corpus, let alone the increasing volume of internet. Thirdly, data labeling for training a classifier is time-consuming. To address these challenges, we propose a solution depicted in Figure 1 and describe it in the following subsections.
Given a document corpus D and some entities to be re-solved, the candidate extractor firstly employs a NLP tool (http://alias-i.com/lingpipe/) to extract all the named enti-ties from D as candidates. Considering the first and second challenges mentioned above, the cost for pairwise compari-son between entities and their alias candidates is non-trivial, which requires solutions to reduce the cost. The subset-based pairwise comparator in Figure 1 is designed to narrow the pairwise comparison scope for each given entity.

It is motivated by the observations that among the en-tities which are aliases with each other: 1) some ones fre-quently occur in documents, while others are mentioned in-frequently; 2) the document union, where some of the most frequ e nt entities appear, is often the superset of the docu-ments union of the infrequent entities.

Based on these observations, the subset-based pairwise com-parator is trying to keep each entity and its aliases in the same subset and it runs as follows: 1) builds an entity-document index and extracts the top N % most frequently used entities in the documents, and 2) for any two entities in the top N % ones and their corresponding occurring docu-ment sets, D 1 and D 2 . If D 1 is the subset of D 2 , D 1 removed. And when the intersection ratio between D 1 and D 2 is larger than a predefined threshold  X  , they are merged. After such iterations, we get a list of document sets. Finally, all the entities appearing in the documents of the same set are formed as an entity subset. Here, two parameters,  X  and N are trained using the 10 cross-validation, among which  X  denotes the entity overlapping ratio deciding whether two document sets are merged.
For each given entity, the entities in the same entity subset are its alias candidates. We use a logistic regression classifier to output two values, among which p 1 denotes the probabil-ity that they are aliases and p 2 denotes the opposite. When p 1 is larger than p 2 , we will conclude they may denote the same object. The classifier training is carried out in combi-nation of active user learning, which tries to achieve higher performance with fewer training labels. The training sam-ples selector randomly selects a subset of U and get labels for each sample, and then iteratively adds the labeled sam-ples with high uncertainty[6] to the training set and trains a classifier until the trainers satisfy or U is null. The used features in the classifier training are presented as follows.
Co-occurrence relevance Pointwise mutual informa-tion(PMI) is used to measure the co-occurrence relevance in the corpus D between entities e i and e j , P MI ( e i , e ( p ( e i , e j ) / ( p ( e i ) p ( e j ))). Herein, p ( e is the occurring document set of e i . p ( e i , e j ) denotes the co-occurrence probability of e i and e j , calculated through dividing the co-occurring document count by the size of doc-ument union they appear respectively.

Social relevance Entities can be connected as a net-work based on their co-occurrence. For example, organiza-tion o and its aliases may co-occur with the persons affiliat-ing with o . Thus, the social relevance between e i and e CF ( e i , e j ) = ( F ( e i )  X  F ( e j )) / ( F ( e i )  X  F ( e the number of common nodes with e i in the entity network.
Topic relevance We build an entity-document matrix M and each element in M is computed as T F IDF ( e i , d k ) = ( N d k /N  X  d k )  X  log( | D | / | D e i | ). Herein, N e appearing in document d k , N  X  d in document d k , | D | denotes the overall document count, and | D e i | denotes the number of documents in which entity e i appears. The similarity between any two entities can be calculated as LSA ( e i , e j ) = cosine ( V ( e i ) , V ( e V ( e i ) is the entity-document frequency vector of e i .
We use three datasets to evaluate the proposed approach and compare it with other four methods in terms of F 1 . The first dataset[1] contains 50 person names, 50 location names, and their aliases. To extend this dataset, we col-lected Web pages through issuing these names/aliases as queries to Google. The second dataset[3] contains terror-ism d a ta and spam emails. The third dataset was collected from an IT company including about 100k documents, 300k employee names and emails. As each email address can uniquely match one person, thus we regard an email as an alias of the person. We divided these datasets into training dataset and test dataset. The training data is used for pa-rameter setting and classifier training, while the test dataset is used for evaluation. All ground-truth for evaluation is given in these datasets.

Baselines Four s tate-of-the-art methods were implemented for comparison, namely, PMI based method( BPMI )[5], the graph based method( BGraph )[2], the logistic regression based method( BLR )[3], and a two-step LSA method( TLSA )[4].
Experimental Evaluation In Table 1, we present the optimization results in terms of pairwise comparison counts, which show that the subset-based method can reduce the comparison counts between all the given entities and their alias candidates. According to the experimental study, we found that the documents union in which only the top 50% entities appear can totally cover all the documents of the given corpus. Through parameter training, we choose merge threshold  X  as 0.2 and N as 50% for all test cases. Table 2 presents the performance of different methods in terms of F . We see that our method outperforms the four baseline methods significantly.
