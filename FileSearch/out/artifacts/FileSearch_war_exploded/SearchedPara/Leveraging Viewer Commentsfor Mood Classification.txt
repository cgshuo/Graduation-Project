 This short paper proposes a method to classify music video clips uploaded to a video sharing service into music mood categories such as  X  X heerful, X   X  X istful, X  and  X  X ggressive. X  The method lever-ages viewer comments posted to the music video clips for the mu-sic mood classification. It extracts specific features from the com-ments: (1) adjectives in comments, (2) lengthened words in com-ments, and (3) comments in chorus sections. Our experimental re-sults classifying 695 video clips into six mood categories showed that our method outperformed the baseline in terms of macro and micro averaged F -measures. In addition, our method outperformed the existing approaches that utilize lyrics and a udio signals of songs. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Music Mood Recognition, Music Information Retrieval, User Gen-erated Media
Music is a fundamental form of entertainment in our lives, and the expansion of the Web has drastically increased the amount of the digital music contents available for us. Particularly, in Japan, a singing synthesizer software called Vocaloid [8] has made it sur-prisingly much easier for those who have never done so before to create original songs. As a result, such user-generated songs are now uploaded to the Web every day. For example, as of the end of August 2012, over 100,000 music video clips of the user-generated Vocaloid songs (typically with one or more picture images) had been uploaded to NicoNico Douga 1 , one of the most popular video sharing services in Japan. Furthermore, over 1,000 music video clips of Vocaloid songs are being uploaded to it every month. http://nicovideo.jp, h ttp://en.wikipedia.or g/wiki/Nico_Nico_Douga
While the amout of digital music contents available for us has been rapidly increasing, the ways to search for desired songs are still limited. Usually, users have to retrieve songs on the basis of their meta-data such as artist name, song name, play count, or social tags annotated by users. To offer more ways to search for desired songs, some researchers in the field of Music Information Retrieval (MIR) have recently been working on the music mood recogni-tion [7, 9]. If we can offer a mood-based music retrieval system to users, they can search for their desired songs with their sub-jective information needs:  X  X  feel depressed after quarreling with my friend, so I want to listen a cheerful song to clear my mind, X  or  X  X  X  X  unfamiliar with Vocaloid songs, but I love wistful songs, so I X  X  like to listen to popular wistful Vocaloid songs. X  In this way, such mood-based retrieval can offer users another way to find songs. In particular, it would be benefici al to novices wh o know little a bout the target music domain.

In MIR, most work on the music mood recognition basically relies on the features extracted from the audio signals. Some re-cently combined the lyrics of the songs with the audio signals [6, 10]. However, mood recognition is much harder than genre or artist recognition [9]. One alternative w ay for the mood-based retrieval is to use social tags related to the music moods annotated by users. However, according to our preliminary study with 186,987 music video clips on NicoNico Douga, only 5% of them contained the music mood related tags. Similarly, on Last.fm 2 , which is a popu-lar music social networking service, only 14% of songs had mood related tags [6]. These results suggest that the simple exploitation of social tags is also insufficient. The goal of our work is to achieve a more effective mood-based music retrieval system to help users search for many songs.

As the first step to this goal, in this paper, we propose a method to classify music video clips uploaded to NicoNico Douga into mood categories by leveraging viewer comments. On NicoNico Douga, viewers can post comments at arbitrary temporal playback posi-tions in the video clip while they are watching it (See Section 2). These comments can be seen as viewers X  direct responses to music video clips, thus they should contain useful information to estimate the music mood of the music video clips.

This short paper tackles the following two issues. First, to ex-tract useful features for the mood recognition from viewer com-ments, we propose a method that combines three feature extraction approaches: (1) adjectives in comments, (2) lengthened words in comments, and (3) comments in chorus sections. In this paper, we examined the effectiveness of the proposed method with a dataset consisting of 695 music video clips in six mood categories (See Section 4.3). Second, to investigate the effectiveness of viewer comments for the music mood recognition, we also compared our http://www.last.fm
Figure 1: Overview of viewer interface in NicoNico Douga. method with the existing techniques that use lyric and audio signal features (See Section 4.4).
NicoNico Douga is one of the most popular video sharing ser-vices in Japan. It had about 29.6 m illion signe d-up users as of the end of June 2012, and over 5,000 video clips are uploaded to it ev-ery day. Figure 1 shows a viewer interface in NicoNico Douga. One unique feature of this service is that viewers can post comments at arbitrary playback positions to the video clip, and such com-ments of many other viewers are overlaid directly onto the video and synchronized to a specific playback position. This gives view-ers a sense of sharing the viewing experiences of the video with others, who originally watched the video at different times.
In addition to its unique comment feature, like other content sharing services such as YouTube and Flickr, viewers can anno-tate tags to the video. In this work we employ such tags to build the data for the music mood recognition (See Section 4.1).
Recently some researchers have been working on leveraging viewer comments on the video sharing services like YouTube to improve video retrieval [2, 3]. While comment analyses of NicoNico Douga have also attracted the attension of some researchers [13], to our knowledge, our method is the first to use viewer comments for mu-sic mood classification.
In this work, we treat music mood recognition of a music video clip as a multiclass classification problem. That is, given a mu-sic video clip, we classify it into one of the predefined mood cat-egories. Figure 2 shows the overview of our feature extraction methods from viewer comments. To extract effective features from comments, our proposed method combines three feature extraction approaches: (1) adjectives in comments, (2) lengthened words in comments, and (3) comments in chorus sections. The rest of this section describes how each method extracts features from com-ments. One simple approach to obtaining features is to extract Bag-of-Words from viewer comments. However, comments posted to a music video clip may contain various pieces of information, but most have nothing to do with the mood of the song. If we extract unnecessary features, they may deteriorate the classification perfor-mance.
 In this work, we focused on adjectives, rather than all Bag-of-Words, as features. It is natural to think that the viewers who are listening a wistful song may write comment about the clip such as  X  X  feel sad , X  or  X  X his song is tear-jerking . X  In this way, adjectives have strong relationship with music moods.

The method works as follows. The method first splits all the comments into words by using a morphological analyzer. The method then extracts adjectives from the words and uses them as the fea-tures for the classification.
The method described in Section 3.1 applies a morphological analysis to comments and extracts adjectives as features. For viewer comments in NicoNico Douga, however, a morphological analyzer often fails to split the comments into the correct words, since the form of comments are far from standard Japanese. One good En-glish equivalent is tweets on Twitter 3 , as reported by Brody and Diakopoulos [1]. Brody and Diakopoulos pointed out that some wordsintweetsareoften lengthened by repeating some charac-ters. For example, word  X  X ool X  is often lengthened as  X  X ooolll X  or  X  X oooooollllllll X  on Twitter. Br ody and Diakopoulos further pointed out that these lengthened words are strongly related to the sentiments (positive or negative) of the tweets and proposed a method to leverage such lengthened words to analyze these sentiment. On NicoNico Douga, similar to the above case on Twitter, viewers of-ten write comments with such lengthened words. We hypothesized that such lengthened words in comments are also good indicators of the music moods and can compensate for a morphological analysis approach described in Section 3.1.

The method works as follows. The method first detects length-ened words from the comments by using the method proposed by Brody and Diakopoulos [1]. It then obtains the normalized forms of these lengthened words and uses them as features. For example, both  X  X oooolllll X  and  X  X  ooool X  are normalized to  X  X ool X  and treated as the same feature.
Neither approach described in Sections 3.1 and 3.2 considers as-sociated playback positions of viewer comments. However, the likelihood that a comment is related to the mood of the music video clips might depend on its associated playback position. For exam-http://twitter.com/ ple, comments posted before the music actually starts might have nothing to do with the mood of the music. In this paper, we hypoth-esized that comments posted in the chorus 4 section of the music are more likely to relate to the mood of the music, as the chorus sec-tions are a song X  X  most representative and memorable portions.
The method works as follows. The method first detects the cho-rus sections of the song of the target music video clip. To detect chorus sections of songs, in this work we use the method pro-posed by Goto [4]. After detecting the chorus sections by using Goto X  X  method, our method then collects comments whose associ-ated playback positions are within the detected chorus sections and extracts the features from them by using the methods described in Sections 3.1 and 3.2.

Note that the three methods described above might extract the same feature in terms of its characters (e.g., all methods might ex-tract  X  X ool X  as features). We thus assign different feature IDs to them in order to distinguish them.
To evaluate our method, we built a dataset from music video clips in NicoNico Douga. As for the music moods, many researchers have proposed ways to model moods of music [5, 11]. In this work, we took an approach similar to Hu et al.  X  X  work [6]. Hu et al. have proposed a method of mood r ecognition t hat uses both audio signal and lyric features. To evaluate their method, they created a dataset from the social tags annotated to the songs on Last.fm. They picked up 135 music mood related tags and manually merged them into 18 mood categories. The created mood categories are public to researchers as MIREX [7] Mood Tag Dataset 5 . Their approach, which creates a dataset from social tags, has two merits: (1) flexible mood categories that suit the target domain can be obtained, and (2) ground truth (i.e., which song belongs to which mood category) can be almost automatically created from the relationships between tags and songs.

In our work, we first manually collected 50 tags related to the music moods from NicoNico Douga. Then we carefully merged similar tags into the same group so that the res ulting m ood groups would be similar to the Mood Tag Dataset. After this operation, we obtained 11 mood categories. Then, for each mood category, we retrieved music video clips that had (1) more than 200 viewer com-ments and (2) one of the associated tags in the mood category. Fi-nally, we dropped mood categories that had fewer than 40 retrieved video clips, resulting in six mood categories with 695 associated music video clips. Table 1 shows the statistics of the dataset used in the experiments. In the table,  X # of tag X  represents the associ-ated tags with the mood, and  X # of clips X  represents the number of music video clips in the mood category. a.k.a. refrain http://music-ir.org/mirex/wik i/2010:Audio_T ag_Classification
We used Support Vector Machine (SVM) to construct a multi-class classifier. We used LIBSVM 6 to implement the SVM and chose the linear kernel, since the preliminarily experiment showed that the linear kernel outperformed the other kernels. We used MeCab 7 as a Japanese morphological analyzer. Following Hu et al.  X  X  work [6], each feature was weighted by the tf-idf weighting.
For each mood category, F -measure was calculated over a five-fold cross validation. We also computed the macro and micro av-erages over the six mood categories to evaluate the classification performance.
The purpose of this experiment is to examine the effectiveness of our methods for feature extraction from viewer comments. First, we explain the baseline and proposed methods we used and then discuss their results.

Methods. For the experiment, we prepared one baseline and five proposed methods. For the baseline, BoW uses all the nouns, verbs, and adjectives obtained from viewer comments as features. As for the proposed methods, Adj , Len ,and Cho correspond to the methods described in Section 3.1, 3.2, and 3.3, respectively. We also prepared the combination of these methods: Adj+Len uses features extracted by both Adj and Len ,and Adj+Len+Cho uses all the features extracted by Adj , Len ,and Cho .

Results. Table 2 shows the result of classification performances obtained from the methods described above. First, when we com-pare the results of Adj , Len ,and Cho ,wefindthat Adj achieved the highest macro (.632) and micro (.676) F -measures. In addi-tion, when we compare Adj with BoW , we can see that Adj also achieved higher performance than the baseline in terms of macro and micro F -measures. These results indicate that adjectives in viewer comments play an important role in recognizing the mu-sic moods. When we see the results of Len , we found that Len could recognize the  X  X ggressive X  category better than the other cat-egories. Note that Len uses lengthened words like  X  X oooolll. X  Such words might be likely to be posted to express viewers X  highly in-tense emotions, rather than calmness or relaxation. Thus, Len might be useful to recognize the mood categories that are related to highly intense moods (high arousal space in Russel X  X  model [11]).
From the table, we can see that Adj+Len+Cho outperformed the other methods in terms of macro and micro averaged F -measures. This result suggests that combining the different approaches can improve the music mood recognition. In particular, note that Cho considers associated playback positions of viewer comments. This indicates that considering the playback positions of comments can improve the music mood recognition. Although in this work we just combined the features extracted from different methods with the linear kernel, we plan to explore a more sophisticated method to integrate features extracted from these three methods.
The purpose of this experiment is to examine the effectiveness of our method, which uses viewer comments for the mood recog-nition, compared with the existing approaches that uses lyric and audio signal features [6].

Methods. Following Hu et al.  X  X  work, we prepared two methods named Lyric and Audio , which extract features from the lyrics http://www.csie.ntu.edu.tw/  X cjlin/libsvm/ https://code.google.com/p/mecab/ and audio signals, respectively. For the lyric features, we collected the lyrics for the music video clips in the dataset from the lyric database 8 . We then split the lyrics into words and extracted nouns, verbs, and adjectives from them for the lyric features, weighting with the tf-idf weighting. As for the audio signal features, also following Hu et al.  X  X  work, we used MARSYAS [12], the best per-forming audio system evaluated in the MIREX 2007 audio mood classification task [7]. MARSYAS extracts spectral features such as Spectral Centroid, Rolloff, and MFCCs. We extracted the audio signals from the music video clips in the dataset and then applied MARSYAS to them to extract 63 dimensional audio features.
In this experiment, we compare five methods. Comment repre-sents the method that extracts features from viewer comments and exactly equals to Adj+Len+Cho described in Section 4.3. Lyric +Audio use both features extracted by Lyric and Audio ,and Comment+Lyric+Audio uses all features extracted from Co -mment , Lyric ,and Audio .
 Results. Table 3 shows the classification results of each method. From the table, when we compare the results of Comment , Lyric , and Audio , we can see that Comment outperformed the others. This suggests that the viewer comments are useful resources for the mood recognition. Hu et al. reported that the classification ac-curacy is slightly improved when combining lyric and audio signal features are combined. The similar results can be seen from the results of Lyric , Audio ,and Lyric+Audio : the classification performance of Lyric+Audio improved on these of Lyric and Audio .

Finally, we can see that Comment+Lyric+Audio outperformed the other methods in terms of the macro (.670) and micro (.693) averaged F -measures. Although our dataset is not large, this result indicates that combining features extracted from different sources can enhance the performance of the music mood recognition.
One primary limitation of our method is that it cannot be ap-plied to music video clips that have few comments, as they have just been uploaded a few days previously. In contrast, a classifi-cation method based on audio signals can be applied to such new videos. To achieve a more flexible classification method, we plan to integrate comments, lyrics, and audio signals differently, rather than just combing features from them. For example, for new video clips, we first apply an audio-based classification method, and a few days later when the video clips receive enough comments, a comment-based classification will be applied to it. We believe such a flexible integration of different methods might be an interesting research topic for music mood classification.
In this study, we examined the effectiveness of viewer comments for the music mood classification. In the future, we plan to continue http://www5.atwiki.jp/hmiku/ this research and develop a more sophisticated method for extract-ing features from viewer comments. Also, we would like to explore a method to integrate comments, lyrics, and audio signals for more flexible and reliable music mood recognition.
This work was supported in part by CREST, JST and KAKENHI (#23680006, #24240013). [1] S. Brody and N. Diakopoulos.
 [2] C. Eickhoff, W. Li, and A. de Vries. Exploiting user [3] K. Filippova and K. Hall. Improved video categorization [4] M. Goto. A chorus section detection method for musical [5] K. Hevner. Experimental studies of the elements of [6] X. Hu, J. Downie, and A. Ehmann. Lyric text mining in [7] X.Hu,J.Downie,C.Laurier,M.Bay,andA.Ehmann.The [8] H. Kenmochi and H. Ohshita. Vocaloid X  X ommercial singing [9] Y. Kim, E. Schmidt, R. Migneco, B. Morton, P. Richardson, [10] C. Laurier, J. Grivolla, and P. Herrera. Multimodal music [11] J. Russell. A circumplex model of affect. Journal of [12] G. Tzanetakis and P. Cook. MARSYAS: A framework for [13] K. Yoshii and M. Goto. MusicCommentator: Generating
