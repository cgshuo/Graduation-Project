
Nenad Toma X  sev 1 , Milo X  s Radovanovi  X  c 2 , Dunja Mladeni  X  c 1 , and Mirjana Ivanovi  X  c 2 Clustering in general is an unsupervised process of grouping elements together, so that elements assigned to the same cluster are mo re similar to each other than to the remain-ing data points [1]. This goal is often diffi cult to achieve in practice. Over the years, various clustering algorithms have been proposed, which can be roughly divided into four groups: partitional , hierarchical , density-based ,and subspace algorithms. Algo-rithms from the fourth group search for clusters in some lower-dimensional projection of the original data, and have been generally preferred when dealing with data that is high-dimensional [2,3,4,5]. The motivation for this preference lies in the observation that having more dimensions usually leads to the so-called curse of dimensionality , where the performance of many standard m achine-learning algorithms becomes im-paired. This is mostly due to two pervasive effects: the empty space phenomenon and concentration of distances. The former refe rs to the fact that all high-dimensional data sets tend to be sparse, because the number of points required to represent any distri-bution grows exponentially with the number of dimensions. This leads to bad density estimates for high-dimensional data, causing difficulties for density-based approaches. The latter is a somewhat counterintuitive property of high-dimensional data represen-tations, where all distances between data points tend to become harder to distinguish as dimensionality increases, which can give rise to problems with distance-based algo-rithms [6,7,8].
 The difficulties in dealing with high-dimens ional data are omnipr esent and abundant. However, not all phenomena which arise are n ecessarily detrimental to clustering tech-niques. We will show in this paper that hubness , which is the tendency of some data points in high-dimensional data sets to occur much more frequently in k -nearest neigh-bor lists of other points than the rest of the points from the set, can in fact be used for clustering. To our knowledge, this has not been previously attempted. In a limited sense, hubs in graphs have been used to represent typical word meanings in [9]. This, however, was not used for data clustering. Therefore, we focused first of all on exploring the po-tential value of using hub points in clustering by constructing hubness-based clustering algorithms and testing them in high-dime nsional settings. The hubness phenomenon and its relation to clustering will be further addressed in Section 3.

The rest of the paper is structured as follows. In the next section we present re-lated work, Section 3 discusses in general the phenomenon of hubness, while Section 4 describes the proposed algorithms that are e xploiting hubness for data clustering. Sec-tion 5 presents the experiments we performed on both synthetic and real world data, and in Section 6 we give our final remarks. Even though hubness has not been given much attention in data clustering, hubness information is drawn from k -nearest-neighbor lists, which have been used in the past to perform clustering in various ways. These lists may be used for computing den-sity estimates, by observing the volume of space determined by the k nearest neigh-bors. Density-based clustering methods often rely on this kind of density estimation [10,11,12]. The implicit assumption made by density-based algorithms is that clusters exist as high-density regions separated fro m each other by low-density regions. In high-dimensional spaces this is often difficult to estimate, due to data being very sparse. There is also the issue of choosing the proper neighborhood size, since both small and large values of k can cause problems for density-based approaches [13]. Enforcing k -nearest-neighbor consistency in algorithms such as K -means was also experimented with [14]. This approach proposed moving closed neighbor-sets between clusters in iterations instead of using single data points. However, the most typical usage of k -nearest-neighbor lists relates to constructing a k -NN graph, where nodes are connected by an edge if one of them is in the k -nearest-neighbor list of the other [15]. The problem is then reduced to graph clustering, with a number of approaches available.
 Hubness is an aspect of the curse of dimens ionality pertaining to nearest neighbors which has only recently come to attention, un like the much discussed distance concen-tration phenomenon. Let D  X  d be a set of data points and let N k ( x ) denote the number of k -occurrences of point x , i.e., the number of times x occurs in k -nearest-neighbor lists of other points from D . As the dimensionality of data increases, the distribution of k -occurrences becomes considerably skewed [16]. As a consequence, some data points, which we will refer to as hubs , are included in many more k -nearest-neighbor lists than other points. Moreover, in the rest of the text we will refer to the number of k -occurrences of point x  X  D as its hubness score . It has been shown that hubness appears in high-dimensional data a s an inherent property of high dimension-ality, and is not an artefact of finite samples nor a peculiarity of some specific data sets [16]. 3.1 The Emergence of Hubs Hubness is closely related to the aforementioned concentration of distances in high-dimensional spaces. If distances do concen trate for a given data set, then its points are lying approximately on a hypersphere centered at the data mean. Naturally, if data is drawn from several distributions, as is usually the case in clustering problems, this could be rephrased by saying that data are l ying approximately on several hyperspheres centered at the corresponding distribution means. However, it has been shown that the variance of distances to the mean is still non-neg ligible, regardless o f the concentration phenomenon  X  for any finite number of dimensions [7]. This implies that some of the points will still end up being closer to the data (or cluster) mean than other points. It is well known that points closer to the mean tend to, on average, be closer to all other points, for any observed dimensionality. However, in high-dimensional data, this tendency is amplified [16]. On average, points which are closer to all other points will naturally have a higher probability of being included in k -nearest-neighbor lists of other points in the data set, which gives rise to an increase in their hubness scores. 3.2 Relation of Hubs to Data Clusters There has been some previous work on how well high-hubness elements cluster, as well as the general impact of hubness on clustering algorithms [16]. A correlation between low-hubness elements and outliers was also observed. A low hubness score indicates that a point is on average far from the rest of the points and hence probably an outlier. In high-dimensional spaces, however, low -hubness elements are expected to occur by the very nature of these spaces and data distributions. These data points will lead to an average increase in intra-cluster dissimilar ity. It was also shown for several clustering algorithms that hubs do not cluster well compared to the rest of the points. This is due to the fact that some hubs are actually close to points in different clusters. Hence, they also lead to a decrease in inter-cluster dissimilarity. However, this does not necessarily hold for an arbitrary cluster configuration.
It was already mentioned that points closer to cluster means tend to have higher hub-ness scores than the rest of the points. A natural question which arises is: Are hubs medoids? When observing the problem from the perspective of partitioning clustering approaches, of which K -means is the most commonly used representative, a similar question might also be posed: Are hubs the closest points to data centroids in clustering iterations? To answer this question, we ran K -means++ [17] multiple times on sev-eral randomly generated Gaussian mixtures for various fixed numbers of dimensions, observing the high-dimensi onal case. We measured in each iteration the distance from current cluster centroid to the medoid and to the hub, and scaled by the average intra-cluster distance. This was measured for every cluster in all the iterations, and for each iteration the minimal and maximal distance from any of the centroids to the correspond-ing hub and medoid were computed. Figure 1 gives example plots of how these ratios evolve through iterations for the case of 10 -cluster data, used neighborhood size 10 , with 30 dimensions for the high-dimensional case, and 2 dimensions to illustrate low-dimensional behavior.

It can be noticed from the charts that, in the low-dimensional case, hubs in the clusters are far away from the centroids, eve n farther than average points. There is no correlation between data means and high-hubness instances in the low-dimensional scenario. On the other hand, for the high-dimensional case, we observe that the minimal distance from centroid to hub converges to m inimal distance from centroid to medoid. This implies that some medoids are in fact cluster hubs. Maximal distances to hubs and medoids, however, do not match. There exist hubs which are not medoids, and vice versa. Also, we observe that maximal distance to hubs also drops with iterations, hinting that as the iterations progress, centroids are becoming closer and closer to data hubs. This brings us to the idea that will be explained in detail in the following section: Why not use hubs to approximate data centers? After all, we expect points with high hubness scores to be closer to centers of relatively dense regions in high-dimensional spaces than the rest of the data points, making them viable candidates for representa-tive cluster elements. We are not limited to observing only the points with the highest hubness scores, we can also take advantage of hubness information for any given data point. More generally, in case of irregularly shaped clusters, hubs are expected to be found near the centers of compact sub-clusters, which is also beneficial. If hubness is viewed as a kind of local centr ality measure, it may be possible to use hubness for clustering in various ways. In order to test this hypothesis, we opted for an approach that allows observations about th e quality of resulting clustering configura-tions to be related directly to the propert y of hubness, instead of being a consequence of some other attribute of the clustering algorithm. Since it is expected of hubs to be located near the centers of comp act sub-clusters in high-dimensional data, a natural way to test the feasibility of using them to approxi mate these centers is to compare the hub-based approach with some centroid-based technique. For that reason, the considered algorithms are made to resemble K -means, by being iterative approaches for defining clusters around separated high-hubness data elements.

As Fig. 1 showed, centroids and medoids in K -means iterations tend to converge to locations close to high-hubness points. This implies that using hubs instead of either of these could actually speed up the convergence of the algorithms leading it straight to the promising regions in the data space. To illustrate this point, consider the simple example shown in Fig. 2, which mimics in two dimensions what normally happens in multidi-mensional data, and suggests that not only might taking hubs as centers in following iterations provide quicker convergence, but that it also might prove helpful in finding the best end configuration. Centroids depend on all current cluster elements, while hubs depend mostly on their neighboring elements and therefore carry local centrality infor-mation. We will consider two types of hubness below, namely global hubness and local hubness. We define local hubness as a restriction of global hubness on any given cluster, considered in the context of the current algorithm iteration. Hence, the local hubness score represents the number of k -occurrences of a point in k -nearest-neighbor lists of elements from within the same cluster. 1
The fact that hubs emerge close to centers of dense subregions might suggest some sort of a relationship between hubness and the density estimate at the observed data point. There are, however, some important differences. First of all, hubness does not depend on scale. Let D 1 and D 2 be two separate sets of points. If the local distance matrices defined on each of them separately are proportional, we might think of D 1 and D 2 as two copies of the same abstract data model appearing at different scales. Even though the density estimate might be significantly different, depending on the defining volumes which are affected by scale, there will be a perfect match in hubness scores of the corresponding points. However, there is a more subtle difference. Let D k ( x ) be the set of points where x is among the k nearest neighbors. Hence, the hubness score of x is then given by N k ( x )= | D k ( x ) | . For each x i  X  D k ( x ) , whether point x is among the k nearest neighbors of x i depends on two things: distance ( x, x i ) , and the density estimate at point x i , not the density estimate at point x . Consequently, a hub might be a k -neighbor for points where density is high, as well as for points where density is low. Therefore, there is no direct correspondence between the magnitude of hubness and point density. Naturally, since hubs tend to be close to many points, it would be expected that density estimates at hub points are not low, but they do not necessarily correspond to the points of highest density among the data. Also, in order to calculate the exact volume of the neighborhood around a given point, one needs to have a suitable data representation. For hubness, one only needs the distance matrix.

Computational complexity of hubness-based algorithms is mostly determined by the cost of computing hubness scores. Computing the entire distance matrix may not be feasible for some very large datasets. However, it was demonstrated in [18] that it is possible to construct a k -NN graph (from which hubness scores can be read) in  X  ( ndt ) , where the user-defined value t&gt; 1 expresses the desired quality of graph construction. It was shown that good quality may be achieved with small values of t . 4.1 Deterministic Approach A simple way to employ hubs for clustering is to use them as one would normally use centroids. Also, it allows us to make a direct comparison with the K -means algorithm. The algorithm, referred to as K -hubs , is given in Algorithm 1.
 Algorithm 1 K -hubs
After initial evaluation on synthetic data , it became clear that even though the algo-rithm manages to find good and even best configurations often, it is quite sensitive to initialization. To increase the probability of finding the global optimum, we resorted to the stochastic approach described in the following section. However, even though K -hubs exhibited low stability, it converges to th e stable configurations very quickly, in no more than four iterations on all the data sets used for testing, most of which contained around 10000 data instances. 4.2 Probabilistic Approach Even though points with highest hubness are without doubt the prime candidates for cluster centers, there is no need to disregard the information about hubness scores of other points in the data. In the algorithm described below, we implemented a squared hubness-proportional stochastic scheme based on the widely used simulated annealing approach to optimization [19]. The temperature factor was introduced to the algorithm, so that it may start as being entirely probabilistic and eventually end by executing de-terministic K -hubs iterations. We will refer to this algorithm, specified by Algorithm 2, as hubness-proportional clustering (HPC).
 Algorithm 2 HPC
The reason why hubness-proportional clustering is reasonable in the context of high dimensionality lies in the skewness of the distribution of k -occurrences. Namely, there exist many more data points having a low hubness score, making them bad candidates for cluster centers. Such points will have a low probability of being selected. To further emphasize this, we use the square of the actual hubness score instead of making the probabilities direc tly proportional to N k ( x ) .

The HPC algorithm defines a search through the data space based on hubness as a kind of a local centrality estimate. It is possible to take as the output the best solution according to some predefined criterion like m inimum squared error, rather than sim-ply taking the last produced cluster configuration. This may in some situations produce even better clustering results. We were mostly focused on finding the best stable hub configuration, thus we only used the last produced configuration to estimate the results for tests presented in the rest of the paper. To justify the use of the proposed stochastic scheme, we executed a series of initial tests for a synthetic mixture of Gaussians, for dimensionality d =50 , n = 10000 instances, and K =25 clusters in the data. Neigh-borhood size was set to k =10 and for each preset number of probabilistic iterations in the annealing schedule, the clustering was run 50 times, each time re-initializing the seeds. The results are displayed in Fig. 3. The silhouette index [20] was used to estimate the clustering quality. Due to the significan t skewness of the squared hubness scores, adding more probabilistic iterations helps in achieving better clustering, up to a certain plateau that is eventually reached. The same shape of the curve also appears in the case of not taking the last, but the error-minimizing configuration. We tested our approach on various high-dimensional synthetic and real-world data sets. We will use the following abbreviations in the forthcoming discussion: KM ( K -Means), GKH (Global K -Hubs), LKH (Local K -Hubs), GHPC (Global Hubness-Proportional Clustering) and LHPC (Local Hubness-Proportional Clustering), local and global re-ferring to the type of hubness score that was used (see Section 4). For all algorithms, including KM, we used the D 2 initialization procedure descr ibed in [17]. Hubness could also be used for cluster initialization, an option which we have not fully explored yet. For determining N k ( x ) we used k =10 by default in our experiments involving syn-thetic data, since the generated sets were large enough to insure that such a value of k would not overly smooth out hubness. There is no known way of selecting the best k for finding neighbor-sets, with the problem also depending on the particular application. To check how the choice of k reflects on hubness-based clustering, we ran a series of tests on a fixed synthetic data set for a range of k values. The results suggest that the algo-rithms proposed in this paper are not very sensitive to changes of k , with no observable monotonicity in scores, meaning that the cl ustering quality does not rise with rising k , or vice versa. (We omit these charts due to space considerations.)
In the following sections, as a baseline we will use K -means++, since it is suitable for determining the feasibility of using hubne ss to estimate local centrality of points. 5.1 Synthetic Data: Gaussian Mixtures For comparing the resulting clustering qua lity, we used mainly the silhouette index as an unsupervised measure of configuration validity, and average cluster entropy as a su-pervised measure of clustering homogeneity . Since most of the generated data sets are  X  X olvable, X  i.e., consist of non-overlapping Gaussian distributions, we also report the normalized frequency with which the algorithms were able to find these perfect config-urations. We ran two lines of experiments, one using 5 Gaussian generators, the other using 10. For each of these, we generated data of ten different high dimensionalities, more specifically for 10 , 20 , 30 ,..., 100 . In each case, 10 different Gaussian mixtures were randomly generated, resulting in 200 different generic sets, 100 of them contain-ing 5 data clusters, the other containing 10 . On each of the data sets, KM and all of the hub-based algorithms have been executed 30 times and the averages were calculated.
Table 1 shows the final summary of all these runs. (Henceforth, we use boldface to denote measurements that are significantly better than others, in the sense of having no overlap of surrounding one-standard deviation intervals.) Global hubness is definitely to be preferred, especially in the presence of m ore clusters, which further restricts neigh-bor sets in the case of local hubness scores. P robabilistic approaches significantly out-perform the deterministic ones, even though GKH and LKH also sometimes converge to the best configurations, but much less frequently. More importantly, the best overall algorithm in these tests was GHPC, which outperformed KM on all basis, having lower average entropy, a higher silhouette index, and a much higher frequency of finding the perfect configuration. This suggests that GHPC is a good option for clustering high-dimensional Gaussian mixtures. Regarding the number of dimensions when the actual improvements begin to show, in our lower-dimensional test runs, GHPC was better al-ready on 6 -dimensional mixtures. Since we conc luded that using global hubness leads to better results, we only consider GKH and GHPC in the rest of the experiments. 5.2 Clustering in the Presence of High Noise Levels Real-world data often contains noisy or erroneous values due to the nature of the data-collecting process. It is natural to assume that hub-based algorithms will be more ro-bust with respect to noise, since the hubness-proportional search is driven mostly by the highest-hubness elements, not the outliers. I n the case of KM, all of the instances within the current cluster directly determine the location of the centroid in the next iteration. When the noise level is low, some sort of outlier removal technique may be applied. In setups involving high levels of noise this is not the case. We generated a data set of 10000 instances as a mixture of 5 clearly separated Gaussians, farther away from each other than in the previously described e xperiments. To this data we incrementally added noise, 250 instances at a time, drawn from a uniform distribution on a hypercube containing all the data points. In other words, clusters were immersed in uniform noise. The highest level of noise for which we tested was the case when there was an equal number of actual data instances in original clusters and noisy instances. At each noise level, KM, GKH and GHPC were run 50 times each. To reduce the influence of noise on hubness estimates, k =20 was used. The silhouette index and average entropy were computed only on the non-noisy restriction of the data, i.e., the original Gaussian clus-ters. A brief summary of total averages is given in Table 2. The hub-based algorithms show substantial improvements in higher noise levels, which is a useful property. The difference in entropy was quite convincing, 0 . 62 for KM and only 0 . 28 for GHPC on average over all the runs. Even though KM had a smaller square error calculated on the combined noisy data set, hub-based approaches were better at finding the underlying structure of the original data. 5.3 Experiments on Real-World Data The two-part Miss-America data set ( cs.joensuu.fi/sipu/datasets/ ) was used for evaluation. Each part consists of 6480 instances having 16 dimensions. Results were compared for various predefined numbers of clusters in algorithm calls. Each algorithm was tested 50 times for each number of clusters. Neighborhood size was set to 5 .The silhouette index was again used to measure quality. For all the experiments on real-world data we used only the silhouette inde x because categories in real world data sets often violate the cluster assumption, so any conclusions based on label entropy would be less reliable. The results for both parts of the data set are given in Table 3. GHPC clearly outperformed both other algorithms, showing highest improvements for smaller numbers of clusters. Observe that for K =2 it achieved a double of KM X  X  silhouette index, 0 . 42 compared to 0 . 21 on Part I and 0 . 36 compared to 0 . 18 on Part II.
Tests were also run on several UCI datasets ( archive.ics.uci.edu/ml/ datasets.html ). Values of all the individual features in the data sets were normal-ized prior to testing. The results, shown in Table 4, are mostly comparable between the algorithms. Value of k was set to 20 . The datasets were simple, composed only of few clusters, so the results being similar is not surprising. Note, however, that GHPC did as well as KM on Iris dataset, which is only 4 -dimensional. This suggests that hubness-based algorithms might also be successfully applied in some lower-dimensional cases. Using hubness for data clustering has not previously been attempted. We have shown that using hubs to approximate local data centers is not only a feasible option, but also frequently leads to improvement over the centroid-based approach. In our experiments GHPC (Global Hubness-Proportional Clustering) had an overall best performance in various test settings, on both synthetic and real-world data, as well as in the presence of high levels of artificially introduced noise. Global hubness estimates are generally to be preferred to the local ones if used in the proposed framework. Hub-based algorithms are designed specifically for high-dimensional data. This is an unusual property, since the performance of most standard clustering algorithms deteriorates with an increase of dimensionality. Hubness, on the other hand, is an inherent property of h igh-dimensional data, and this is precisely where GHPC may offer greatest improvement.

The proposed algorithms represent only one possible approach to using hubness for improving high-dimensional data clustering. Next, we will explore related agglomera-tive approaches. However, even the described algorithms offer space for improvements, since some questions are left unanswered: What is the best choice of k ? Is it possible to automatically determine the appropriate number of clusters by carefully inspecting the hubs? In cases such as one depicted in Fig. 2 that would probably be possible. What is the best annealing schedule in GHPC? Is it possible to use several different values of k in LHPC to avoid over-smoothing the hubness estimates for small clusters over iterations and make local hubness more useful? Is there a better way to initialize hubness-based algorithms? We plan to address all these details in our future work. Acknowledgments. This work was supported by the Slovenian Research Agency pro-gram Knowledge Technologies P2-0103, and the Serbian Ministry of Science and Tech-nological Development project no. OI174023.

