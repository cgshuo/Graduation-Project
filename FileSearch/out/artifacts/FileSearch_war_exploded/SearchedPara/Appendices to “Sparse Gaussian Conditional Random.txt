 function. Recall that the log-likelihood is given by R differentials; for instance the second order approximation is given by f ( A +  X )  X  f ( A ) + df ( A ;  X ) + denotes the Hessian.
 log-likelihood f ( X  ,  X ), and from this expression we can easily determine the relevant gradients Similarly, we can differentiate again to find the second differential Algorithm 1 Newton Coordinate Descent for SGCRF Output: Optimized parameters  X ,  X 
Initialize :  X   X  I ,  X   X  0,  X   X   X   X  1 while (not converged) do end while in order to achieve fast performance.
 B.1. Coordinate descent updates for the Newton approximation second-order Taylor expansion which for our problem has the form  X  = arg min  X  h ( X   X  +  X e i e T i ,  X   X  ) which has the explicit form where  X  =  X   X  1 and  X  =  X  X  T S xx  X  X .
 Algorithm 2 Coordinate descent inner loop Output: Approximate regularized Newton direction D  X  , D  X 
Initialize : D  X   X  0, D  X   X  0 ,U  X  0 ,V  X  0 while (not converged) do end while where  X  =  X  X  T S xx V  X . Finally, we consider optimizing over an element of D  X  and the form min  X  1 2 a X  2 + b X  +  X  | c +  X  | which has the solution B.2. Optimizations reduce the running time of the algorithm.
 of  X   X  and  X   X  .
 We include a coordinate of  X , respectively  X , if computing  X   X  1 .
 presentation Assumption 1. Underlying model The data is generated according to the graphical model of the CRF have maximum degree d ).
 For simplicity, we will also denote  X  ? =  X  ?  X  1 .
 to hold for each output variable.
 set X  of edges directly connecting an input to y i ), we have that not the support set.
 Assumption 3. Mutual incoherence Let S denote the active set of all variables in vector form complement. Then for H =  X  2  X  ,  X  f ( X  ,  X ) defined above (Ravikumar et al., 2011).
 Lemma 1. Given data generated by the model in Assumption 1 we have that Furthermore, for 0 &lt;  X  &lt; 40 c  X  ? .
 penalty.
 1. The solution  X   X  is unique. 3. max {k X   X  f (  X  ? ) k  X  , k R ( X ) k  X  } X   X  X / 8 then the ` 1 solution recovers the restricted solution,  X   X  =  X   X  .  X 
 X  =  X   X   X  ? and Define Taylor expansion of the function itself), and Lemma 3. Under the definitions above, if then small.
 Lemma 4. Under the model above, suppose that Then These elements allow us to prove the desired theorem.
 Theorem 1. Using assumptions 1-3 above, suppose we have sample size then with probability greater than 1  X  c 1 exp(  X  c 2 m X  2 ) we have 2. The solution satisfies the elementwise bounds Proof. Let Then by Lemma 1 and the minimum bound on m we have that and thus Lemma 4 applies, which gives Therefore, the assumption of k  X  k  X   X  1 d min { 1 3 c the thus claim 1 and 2 are satisfied.
 C.1. Proofs of Lemmas P ( k X   X  f ( X  ? ,  X  ? ) k  X  &gt; ); as shown in Appendix A, we have thus By our assumptions that k X j k 2 / have for any columns X i and Z j . Therefore by the union bound and Gaussian tail probability we have Next, we consider P ( k X   X  f ( X  ? ,  X  ? ) k  X  &gt; ) and again from Appendix A, we have which we can rewrite as Now we can apply Lemma 1 in (Ravikumar et al., 2011) and arrive at the desired bound for 0 &lt; &lt; 40 c  X  ? .
 Proof. (of Lemma 2) can be written as the solution  X   X  cannot have support outside the support of  X  ? . simplicity Using the fact that we can solve for z  X  S gives Thus the mean value theorem we have that the exists t  X  [0 , 1] such that did for the differential of the likelihood function) of these terms individually.
 we use the fact that second differential is bounded by Now, first note that since  X  as a most d entries per column Now, note that and so that Furthermore, Combining these expressions results in the bound as required.
  X  Define and note that by assumption we have To bound k  X  k  X  observe that we have  X  C = 0 and that k  X  S k X  r and by uniqueness (from Assumption 2) this solution must be (  X   X  ,  X   X ). Taking infinity norm, we have For the first term, through application of the bound on R ( X ) and by assumption on k  X  k  X  And for the second term and thus the claim is proven.
 In particular, the class of models specified by with positive  X  and  X  .
 We are interested in characterizing the range over which the mutual incoherence condition is no longer valid.
 quadratic approximation. In Neural Information Processing Systems , 2011. Magnus, X and Neudecker, Heinz. Matrix differential calculus. New York , 1988. 2011.

