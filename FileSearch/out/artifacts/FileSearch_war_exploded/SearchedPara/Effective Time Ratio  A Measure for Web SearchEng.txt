 Evaluation of information retrieval (IR) systems is critical for improving search techniques. So far, the dominant method for IR evaluation has been the Cran-field evaluation method. It involves creating a test collection with a document collection, a set of topics, and relevance judgments, and then measuring the performance of a retrieval system or comparing the performances of different systems on the test collection. This ev aluation methodology has been very use-ful especially because of the availability of many test collections created in TREC (http://trec.nist.gov).

Although being quite successful and popular, the Cranfield experimental model, has received extensive questioning in the literature, e.g. [1,2]. One major criti-cism is that the relevance judgment may not represent real users X  information need thus the evaluation results may not reflect the real utility of the system. There are some research to investigate the correlation between the Cranfield paradigm ex-periment and user study. Some early research [3,4] found that the correlation is weak or even negative. However, some recen t evaluation work [5,6,7], which em-ployed larger scale of topics and users, re ported the positive correlation between the Cranfield evaluation and real user study.

In the use of a real search engine, the e ffectiveness and the satisfaction of a user is also affected by some factors oth er than retrieval system performance. The document snippet is a very important feature for modern search engine. Good snippets can guide the users to find out the relative documents from the retrieved results, or even contain relative information itself. On the contrary, users may miss relevant documents or waste time clicking and examining irrel-evant documents due to bad snippets. However, the traditional metrics used in Cranfield paradigm have not consider the quality of the document snippets.
Turpin et al. [8] investigated the problem of including document snippet qual-ity in search engine evaluation. It extended the existing IR metrics by including snippet quality factor to measure the ret rieval performance. Compared to the original metric, the new one changes the absolute retrieval score, topic difficulty and relative system performance. It indi cates that it makes change by including snippet quality in search engine evaluation. This work also opens a direction of adding features other than document relevance in search engine evaluation. How-ever, it has not validated that the proposed metric that includes snippet quality can really reflect the user satisfaction. Fu rthermore, the snippet relevance in the work was simulated from the document r elevance, so it may not reflect the real situation.

In this paper, we interpret traditional IR metric precision as effective time ratio of the real user, i.e, the ratio between the time used in reading relevant information and the total search time. By this interpretation, we can easily extend the effective time ratio in the scenario when the search engines provide document snippets. The theoretical analysis proves that this metric can reflect both retrieval performance and snippet quality. We further validate this metric by user study, finding that it has larger correlation with user satisfaction than the existing metrics that consider document r elevance only, and the metric proposed by Turpin et al. [8]. Further scenario analysis shows that it can work better on both open-information and close-information questions. There are a large body of researches on information retrieval evaluation. Cran-field paradigm is the most commonly used methodology in IR evaluation. It employs a test collection to evaluate ret rieval system by some standard perfor-mance measures such as MAP and nDCG.

Since there is many known differences between Cranfield paradigm evaluation and evaluation through user studies, it is thus important to understand how they correlate with each other. Some reported negative correlation. For example, [3] asked 24 users for 6 instance-recall tasks, and [4] involved 24 users for 6 question-answer tasks, but there is no significant d ifference in user task effectiveness found between systems with significantly differ ent metric scores. The limitation of the two studies is that the small number of topics may explain why no correlation was detected.

Some recent and larger scale evaluatio n work reported positive correlation between batch system evaluation and user evaluation. [5] involved 33 users to search for 45 topics and showed that differences in bpref could result in statis-tically significant differences in user effect iveness of retrieving faceted document passages. Using 7 participants working on 200 Google queries, [6] demonstrated that satisfaction of assessors correlates fairly strongly with relevance among top three documents measured using a version of DCG .In[7],56usersperformed a recall-based task on 56 queries on top of  X  X ood X  and  X  X ad X  systems. The authors showed that user effectiveness (time consumed, relevant documents col-lected, queries input, satisfactio n, etc) and system effectiveness ( P @ n , MAP ) are correlated.

Turpin et al. [8] suggested that one possible reason for the limited correlation between real users X  satisfaction and Cranfield paradigm metric score is that it has not considered snippet quality in cal culating the metric score. They pro-posed one new metric combining document and snippet relevance, finding that it affects the system performance ranking. In this paper, we propose another metric considering snippet relevance, and find that the correlation between user satisfaction and our proposed metric is higher. Intuitively, for a search engine with snip pets, the users X  satisfaction is affected by two factors: 1) whether the relevant documents are highly ranked; and 2) whether the snippet can indicate the corresponding document relevance. In this paper, We combine these two factors in one metric effective time ratio . 3.1 Precision as Effective Time Ratio Precision and recall are two most important basic metrics in information re-trieval. In the context of Web search engine, it is very difficult to estimate real recall value because it nearly impossible to find out all relevant documents. Some researchers even states that recall is not well founded as a measure of satisfac-tion [9]. Therefore, precision and its exte nsions are very important in Web search engine evaluation.

In order to derive the metric that can be used to evaluate search engines with document snippet, we first interpret precision as effective time ratio (denoted as ETR for short) when using retrieval system without snippet presentation. Definition 1. (Effective Time Ratio): Effective Time Ratio is the ratio between effective time used in get relevant information and the total search time. A traditional IR system presents to th e user a ranked list of documents for a query. We assume the user examines every (top) retrieved document to seek relevant information . We further assume the time spent on examining each document is same (denoted as T ). Thus a user needs T  X  N time to examine top N documents, but the effective time , which is used to examine relevant documents, is T  X  N i =1 R ( d i ), where d i is the i -thrankeddocumentintheresult list, and R is a binary function, indicating whether a document is relevant or not (1 for relevant and 0 for irrelevant). Thus effective time ratio at a cutoff N can be formulated as Equation 1. With very simple derivation, we can find that ETR @ N is identical to P @ N , so we can interpret precision as effective time ratio when using the retrieval system without providing document snippets. 3.2 Effective Time Ratio for Search Engines with Snippets The modern search engines usually present the users a list of snippets. The un-derlying idea is that: 1) the user can judge whether a document is relevant by just examining its snippet; and 2) it costs users less time to examine a snippet than to examine the corresponding document. For a search engine with snip-pet, the assumption used in section 3.1 that users examine each top retrieved document is not valid. Instead, the users would examine the document snippets before clicking and examining the real documents, and some documents whose snippets seem to be irrelevant would not be examined. The search process can be presented as Figure 1. In the process, a user examines i -th snippet, which is generated from i -thrankeddocument( i is initially assigned 1). If he finds the snippet is relevant, he would click and examine the corresponding document; otherwise, he would examine the next snippet. After examining a document, the users may quit search or continue to examine i + 1-th snippet. This process uses examination hypothesis and cascade hypothesis, which are commonly used in modeling user X  X  search behaviors [10,11].

According to the definition of effective time ratio , it is easy to include snippet quality. The effective time used in getting really relevant information include the time spent on reading the snippet and the original text of the relevant documents. The total time is composed by two parts: 1) the time spent on reading all snippets of the (top) retrieved documents, and 2) the time spent on reading the text of the clicked documents, whose snippets seem to be relevant. We further assume that the time spent on reading a snippet and a document are two static values( T 1 and T 2 ). In the top N documents, only the relevant documents with relevant snippets can provide relevant information, so the effective time is and R is a function indicating whether a document or a snippet is relevant or not). Total time spent on reading snippets is N  X  T 1 . The user would read all documents with relevant snippet, so the total time spent on reading documents is N i =1 T 2  X  R ( s i ). Thus the effective time ratio canbeformulatedasEquation 2. Though it seems that there are two parameters ( T 1 and T 2 , we find that their rate affects the effective time ratio value. In Equation 3, we rewrite ETR by the reading time rate between reading document time and reading snippet time ( c = T 2 /T 1 ).
 3.3 Theoretical Analysis To further understand the effect of including snippet quality, we would compare versions of effective time ratio implementation with and without snippet quality factor.

Intuitively, high-quality document snippets used in the search engine can guide the users to find relevant information more efficiently, so the effective time ra-tio is supposed to be higher for the search engine with high-quality snippets. The quality of the snippet (or so-called query dependent summarization) is dis-cussed in some early research [12,13]. The objective of these work is to find out query related informatio n from a document. However, in the context of search engine snippet, the objective is to indicate the relevance of the original document accurately. Thus the metrics used in t his field can not be directly used here.
A snippet is good if it can indicates the document relevance accurately. Other-wise it may mislead users to miss a relevant document or waste time reading an irrelevant document. Here we define a relevant snippet iff it indicates the corre-sponding document is relevant. Therefore the quality of snippet can be qualified by possibility two types of errors.
 Definition 2. (First Type Error) Error of generating a relevant snippet for an irrelevant document Definition 3. (Second Type Error) Error of generating an irrelevant snippet for a relevant document The first type of errors would lead the users to waste time clicking and exam-ining irrelevant documents, and the users would miss relevant documents be-cause of second type of errors. Given a snippet generation algorithm, we define p 1 = Pr ( R ( s )=1 p 2 = Pr ( R ( s )=0 expected value of effective time and tota l time of a search engine for a query can be estimated by the precision of its underlying retrieval system and possibility of these two types of errors of the snippet generation algorithm. The ratio between expected expected effective time and expected total time (denoted as EETR ) can be presented as Equation 4 (The der ivation is presented in appendix). Be-cause P @ N is also an implementation of effective time ratio without considering snippet factor, this equation describes the relations of effective time ratio with and without including snippet quality.
 We can derive three properties of expected effective time ratio from this equation. Proposition 1 shows that a search engine with an error-free snippet generation algorithm has higher expected effective time ratio than a retrieval system with-out snippet. It means that in modern search engine, the high-quality snippet can improve the ratio of effective time that is used in achieving really relevant information.
 Proposition 1. p 1 =0 ,p 2 =0  X  EETR @ N&gt;P @ N Proposition 2 validates that the with two different retrieval system and one snip-pet generation algorithm, the EETR values can reflect the underlying retrieval performance. Similarly, proposition 3 validates that with one retrieval system and two different snippet generation algorithm, the EETR values can reflect the performance of the snippet generation algorithms. These two propositions prove that effective time ratio can reflect both retrieval performance and snippet quality.
 Proposition 2. p 1 ( A )= p 1 ( B ) ,p 2 ( A )= p 2 ( B ) ,P @ N ( A ) &gt;P @ N ( B )  X  EETR @ N ( A ) &gt; EETR @ N ( B ) EETR @ N ( A ) &lt; EETR @ N ( B ) To validate the effective time ratio metric, a user study is designed to resemble the common search engine usage. Some users are employed to collect relevant information for some questions, and they are asked to answer the questions and report the satisfaction in using the search engine. The satisfaction values are compared with scores of various IR metrics including the proposed effective time ratio .
 4.1 Data Collecting The user study is designed to resemble the common search engine usage. The study process is quite similar to the user study research deployed by Huffman and Hochster [6]. We recruited 10 college students, and most of them are familiar with using search engines.
 The users were asked to answer 50 questions with the help of a search engine. Each question is categorized as either open-information (with many answers) or close-information (with only one answer). Its category is determined by three assessors X  voting. Each question is also assigned a difficulty score from 1 to 5. The final difficulty is assigned by three assesso r and these three values are averaged. Each question also has a topic category ( sports, computers, etc.). The selected 50 questions contain 25 open-information questions and 25 close-information questions. The questions are also uniformly distributed in difficulty and topic. For each question, we designed a query, submitted it to a commercial search engine and collected the top 100 results (including both snippets and documents) from the engine. Some question examples are given in Table 1. In the table, the five columns are the question (information need), its submitted query, topic category, open/close-information type and difficulty.

In the user study, for each question, a user was presented with the question following top 10 ranked snippets in the result page, and he can jump to other results by clicking  X  X ext page X ,  X  X revious page X  or page number link at the bottom of the page. For each question, a user can view totally 100 results from an individual search engine. The user was asked to answer this question with the help of the listed results and he can click the link for browsing the original document. The snippets and documents were crawled and stored on the server, so that all the users are presented with s ame results. A user can end the search once he thought he was able to answer the current question or he found that the presented results did not contain an appropriate answer. Once ending a question, the user was asked to answer the question and to report his satisfaction, whose values ranging from 1 to 4 (the higher the better).

The manual relevance judgments were a lso collected. Thr ee assessors were employed to judge relevance of both documents and snippets for the queries. The relevance judgment is binary and the final value is determined by voting. With the collected dataset, we can compare the user reported satisfaction with various metric scores. 5.1 Metrics In this paper, we focus on three groups of metrics. Their scores are used to compare with the user satisfaction score.

The first group of metrics use the docum ent relevance information only, in-cluding precision, DCG, RR and cumulated precision. The reason for using cu-mulated precision instead of average precision is that the total number of relevant documents is unknown.

The second group of metrics have the e xactly same forms as those in the first group, but they use both document relevance and snippet relevance. In this group, one document is considered to be relevant if and only if the original document and its snippet are both relevant (see Equation 5). This group of metrics were proposed by Turpin et al. [8].
 The third groups of metrics are effective time ratio and its extensions. As stated in Section 3.1, precisi on can be interpreted as effective time ratio . As extending precision to cumulated precision, we can also define the cumulated effective time ratio as the sum of effective time ratio at the cutoffs where both the document and the snippet are relevant. The cumulated effective time ratio (denoted as CETR )isdefinedinEquation6.
 There are two parameters in the metric effective time ratio :cutoff N and docu-ment/snippet reading time rate c = T 2 /T 1 .For N , we can tune it in the exper-iment. For c , we can estimate it from the real user query log. We still assume the user search follows the process descr ibed in Figure 1. For a query submission and the corresponding clicks, we can get c by six values: the query submission timestamp t q , the timestamp and the rank of the first click t c 1 and r c 1 ,the timestamp of the last click t c 2 , the maximal rank of the clicks r c 2 , and the total number of clicked documents C .

Before a user X  X  first click c 1 , he has examined all snippets above the rank r c 1 , so the time cost before first click t c 1  X  t q can be presented as Equation 7. Similarly, the total time cost before last click is composed by two parts: time spent on reading snippets before maximal rank of the clicks and time spent on reading all clicked documents (not include the last clicked document). So we have: From Equation 7 and 8, we can get value of T 1 and T 2 and can further derive c value. To estimate the value, we use two query logs: the log of our user study and a one-month log from a commercial search engine. The estimated c value is 8 . 25 for the former log and is 10 . 36 for the later one. We also find that most c values are near 10, so we use 10 as c value in effective time ratio metric. 5.2 Basic Results A good evaluation metric is supposed to re flect the users X  satisfaction in using a search engine to find out relevant information for a need. In this paper, we follow Huffman and Hochster X  X  work [6] to use correlation between metric score and user reported satisfaction to validate the metrics. The users X  satisfaction of a search engine on a topic is averaged by all users X  reported rates. If the correlation is larger, the metric can reflect the users X  satisfaction better.

Table 2, 3 and 4 present the correlation results for three groups of metrics respectively. The highest score in each group is in bold. It shows that the effective time ratio has overall highest correlation with the users X  satisfaction, and RR also has relative high correlation. Surprisingly, though average precision is the most commonly used metric in IR, its unnormalized version of cumulated precision and cumulated effective time ratio work not so well when compared with the real users X  satisfaction.

We can also find that the metrics in the second group don X  X  work as well as the correponding metrics in the first group, though they use both document and snippet relevance information. Another finding is that metrics at cutoff 5 can reflect users X  satification better than those metrics at other cutoffs. It may be because the user can see about 5 snippets without scrolling the mouse at the search engine result page.

To investigate how good the correlation is, we also calculated the correlation between each invididual user X  X  satisfact ion scores and the average users X  satis-faction scores of the results. The average correlation is 0 . 706, so it is higher compared to any metric used. It indica tes that the users agree more on the system X  X  performance, so there is st ill room to improve the metric to reflect users X  satisfaction better.
 5.3 Question Categories The above experimental results show effective time ratio can perform well on average, but they provide limited information on how it performs in multiple scenarios. It would be much more informative to compare these metrics in various scenarios, such as open-information vs. close information query.

In the data collecting, we co llect 25 close-informatio n queries (corresponding to the question that can be answered by one document, such as the birthday of Abraham Lincoln) and 25 open-information queries (corresponding to the question that can be answered by multiple document, such as the information about domain IR). We can calculate the correlation between user satisfactions and the metric scores on these two topi c categories respectively. Due to the space limit, we just present the 3 metrics with highest correlation from each of three metric groups in Table 5 and 6. Effcetive time ratio still has higher correlation with user satisfaction than other metrics. It is obviously that the correlation is higher for the close-information questions. Another finding is that the correlations are larger for metrics with larger cutoffs for open-information questions, because the users are likely to view more documents for such questions. Traditional IR metrics can o nly measure the performan ce of a pure retrieval sys-tem, but the modern search engine includes some other features, such as snippet quality. The users X  satisfaction of using a search engine is not only affected by its underlying retrieval system, but is also affected by its snippet generation al-gorithm. We propose a search engine performance metric effective time ratio ,in which the snippet quality can be easily embedded. Theoretical analysis proves that it can reflect both retrieval and snip pet generation quality. The user study shows that this metric can reflect the rea lusers X  X atisfactionbetterthanthe existing metrics based on document relevance and/or snippet relevance. This work is supported by NSFC grant No.70903008 and 60933004, CNGI grant No.2008-122, 863 Program No.2009AA01Z143, and the Open Fund of the State Key Laboratory of Software Development Environment under Grant No.SKLSDE-2010KF-03, and by 973 Program Grant No.2005CB321901.
 We derive the Equation 4 from 3 here. From Equation 3, we get: The precision at a cutoff is defined as P @ N = relevant ( R ( s i )=1)iff R ( d i ) = 1 and the second type error does not happen or R ( d i ) = 0 and the first type error happens. Therefore we have N i =1 R ( d i )= N  X  P @ N . So the numerator part(denoted as U )of EETR @ N can be rewritten as Similarly, the denominator part(denoted as L ) can be written as Therefore, the expected effective time ratio can be presented by p 1 ,p 2 and P @ N .
