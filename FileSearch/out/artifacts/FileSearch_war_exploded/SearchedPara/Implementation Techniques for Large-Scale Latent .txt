 The technique of latent sema ntic indexing (LSI) has wide applicability in information retrieval and data mining tasks. To date, however, most applications of LSI have addressed relatively small collections of data. This has been due partly to hardware and software limitations and partly to overly pessimistic estimates of the processing requirements of the singular value decomposition (SVD) process. In recent years, advances in hardware capabilities and software implementations have enabled much larger LSI applications. Moreover, experience with large LSI indexes has shown that th e SVD is not the limitation on scalability that it was long thought to be. This paper describes techniques applicable to creating large-scale (multi-million document) LSI indexes. Detaile d data regarding the LSI index creation process is presented for collections of up to 100 million documents. Four key factors are shown to contribute to the scalability of LSI. First, in mo st situations, the time required for calculation of the singular value decomposition (SVD) of the term-document matrix is not the dominant factor determining the overall time required to build an LSI index. Second, the time required to calculate the SVD in LSI is linear in the number of objects indexed. Third, incremental index creation greatly facilitates use of LSI in dyna mic environments. Fourth, distributed query processing can be employed to support large implementation in modern distri buted computing environments. This paper provides the first m easurements of the execution time for large-scale LSI build pro cesses in a cloud environment. H.3.4 [ Information Storage and Retrieval ]: Systems and Software  X  performance evaluation. Algorithms, Performance, Experimentation. Latent Semantic Indexing, LSI, SVD, scaling, large-scale applications, cloud computing. Latent semantic indexing (LSI) is a powerful analytic technique that has proven to be useful in a wi de variety of applications [8]. Based on a technique from linear al gebra, LSI provides an elegant solution to the key problem of dimensionality reduction. Considering the impressive performance of LSI in multiple applications [2], it has not achie ved as wide usage as might be expected. One of the principal reasons for this has been a significant underestimate of the scalability of the technique. This paper provides an overview of the technical considerations involved in applying LSI to large document collections. Section 2 provides a brief synopsis of the LS I technique. Section 3 presents an historical perspective on the LSI scalability issue. Section 4 describes the software, data, and parameter choices employed in the experiments and applications reported here. Section 5 presents details on the infrastructure and techniques applicable to deployment of large-scale LSI a pplications. Section 6 provides concluding remarks. Latent semantic indexing (LSI) is a machine learning technique that (classically) takes as input a collection of documents and produces as output a vector space representation of the documents and terms of the collection. 1 The central feature of LSI is the use of singular value decomposition (SVD) to achieve a large-scale dimensionality reduction for the problem addressed. The technique of latent semantic indexing (as applied to text) consists of the following primary steps [5]: 1) A matrix is formed, wherein each row corresponds to a term LSI actually can be used for a wide variety of data types. The discussion here focuses on text, the traditional domain of LSI. 
However, all of the results presented here are immediately applicable to large-scale LSI analysis of other data types. 2) Local and global term weighting is applied to the entries in 3) Singular value decomposition (SVD) [9] is used to reduce 4) Dimensionality is reduced by deleting all but the k largest 5) Both terms and documents are represented by k-dimensional In many applications, it is desirable to represent in the LSI space documents that were not present in the original collection indexed. The technique of fold ing-in allows rapid creation of vectors that represent new documents in the space. To fold in a document, it is parsed, and a vector created that is a weighted average of the vectors for all of the constituent terms of the document [5]. The idea of latent semantic indexing was first introduced in 1988 computationally intensive. In 1988, this was a significant drawback for LSI. Throughout the 1990s and early 2000s, most published papers on LSI dealt with data collections of no more than a few tens of thousands of documents. Many au thors made negative statements about the scalability of the technique. In general, these comments were based on two factors: a be lief that the SVD calculation was the most time-consuming step in the LSI algorithm and an overestimate of the time required for that calculation. Even quite recent publications ha ve shown significant hesitancy regarding the scalability of LSI. For example, in a major textbook on information retrieval, published in 2008, the authors noted that they were not aware of any successful experiment using LSI on a collection of over one million documents [12]. The authors of a 2009 paper expressed the commonly-held feeling that large-scale LSI applications require supercomputers or even special-purpose hardware [10]. A 2010 paper described a collec tion of 131,321 documents as a very large dataset for LSI indexing [15]. This paper presents details of the creation of LSI representation spaces built from collections of from 1 to 100 million documents. Roughly half of the data presented comes from testing activities. The rest comes from instrumentati on of deployed LSI-based data mining and knowledge discovery app lications. All data presented here is for LSI indexing of textua l data. All results shown here encompass calculation of the (rank reduced) SVD for all documents and all selected terms (see section 4.2 for term selection criteria) for each respective document collection. Data used in the reported test activities consisted primarily of news articles comparable to those us ed in TREC testing. Some of the largest tests employed data from the Gov2 collection. Data from applications included news articles, resumes, technical articles, biomedical reports, a nd government documents. Such material includes large numbers of abbreviations, acronyms, technical terms, and proper names. Accordingly, the number of terms in these collections gr ows far beyond the number of dictionary words present in the material. In most of the examples presente d here, the documents have been processed using entity extraction software prior to creating the LSI indexes. For example, when a name such as John Smith has been identified by the entity extraction software, it has been treated as a single term in the creation of the term-document matrix. Depending on the specific documents, the entity types chosen, and the entity extractor em ployed, this process increases the number of terms in these coll ections by a factor of two to three. Thus, most of the LSI index creation (build) times shown here are significantly larger than would be the case for classical LSI indexes of the same documents, where entity extraction is not employed. Except for the dimensionality testi ng described in section 5.6, all LSI spaces discussed here were created with the number of dimensions set equal to 300. Stopword lists of  X  600 terms were used. There was no stemming of terms. Terms generally were indexed only if they occurred at least twice and in at least two different documents. This is a frequently employed occurrence threshold, which eliminates many noise terms but few desirable terms (particularly in very large collections). In all cases presented here, the LSI software employed was the Content Analyst LSI engine from Content Analyst Company in Reston, Virginia. 2 Multiple versions were employed, with the most recent data obtained using version 3.7. This software implements the classical LSI algorithm as described in [5], with extensions allowing multi-process / multi-host parallel execution of a Lanczos technique for computing the largest singular values of the SVD [16]. The following sections provide de tails of the creation of LSI indexes for large document collections. The key computational tasks to be carried out in creating an LSI space are: This software was chosen as it is the only known LSI engine that provides a full parallel implementation of both build and query functions. In addition to computational demands, the initial step requires documents of interest. Figure 1 presents the times required for each of the fundamental elements of LSI index creation for collections ranging from 1 to 100 million documents. 3 The x-axis corresponds to the total number of objects indexed, bot h terms and documents. Two aspects of this figure are of particular interest: Figure 1. Timing of steps in the creation of an LSI space. Figure 2 shows the number of term s versus number of documents for a range of representative LSI applications. LSI indexing using 8 processors on HP DL 580 servers. The high term counts reflect the large number of acronyms, abbreviations, proper names, technical terms, equipment designators, etc. that occur in real-world text collections. In addition, of the text collections described here, nearly all have been pre-processed using entity extraction software, which typically expands the number of terms by a factor of two to three. The time required for LSI index creation results from a complex interaction among multiple factors: The following sections address these factors, with emphasis on the infrastructure and techniques required to enable large-scale applications using LSI. Over the past few years, the time required to build large LSI spaces using modest hardware (mid-range servers) has decreased dramatically, as shown in Figure 3. Figure 3. Reduction of build ti mes with improving technology. The advances shown in Figure 3 are due primarily to the confluence of four technology developments: The top three trend lines presented in Figure 3 represent (from top to bottom) older, intermediate , and current mid-range server technology 4 , as summarized in Table 1. Most of the data presented in this paper is from te sting and applications employing DL 580 servers. The line labeled cloud (projected) in Figure 3 is based on initial tests using a cloud computing arch itecture for implementation of the key steps in the LSI algorithm. Key constituent processes required for creation of an LSI space: ingestion, parsing, matrix population, and vector creation; are all well-suited to the map-reduce paradigm. For all of the collections repres ented in Figure 1 above, the time required to ingest the documents, parse them, and create the term-versus document matrix was significantly greater than the time required for (truncated) SVD processing of the resulting matrix. High-speed magnetic disks, efficient RAID arrays, and solid state disks are all applicable approaches for minimizing ingest time. One key requirement for creating large LSI indexes is that all of the intermediary processes must be carried out in RAM. If insufficient RAM is available, a nd the hardware begins to page, build times will be dramatically increased. (All build times presented here are from servers where sufficient RAM is available to prevent paging). Having a sufficient amount of RAM is a key scalability enabler for large-scale LSI applications. The matrix generation stage of LSI index creation is the most memory-intensive, as shown in Figure 4. One key reason for this is that pruning based on term occurrence 5 occurs in this phase. When purchased new, these servers originally cost in the range of $50K-$100K apiece. These HP servers are similar to those from several vendors that are used for large database applications and in virtualization environments. As noted in section 4.2, a term occurrence threshold requiring at least two occurrences of a term in at least two different documents was applied in the spaces reported here. Requirements for high-speed memo ry also apply to LSI query operations. In order to obtain acceptable query times, the vectors for objects of interest (terms and documents) must all be kept in RAM. In Figure 4, note that the amount of RAM required for query operations is less than a fi fth of what must be available during the creation of the LSI space. This characteristic is of great practical importance for applications. It allows approaches wherein a single server with large memory is used to build an LSI space, with that space then being distributed to a smaller server (or multiple servers) for query operations. Replication of the term and document vectors across multiple servers is a standard approach for supporting large numbers of users. Historically, discussions of the scalability of LSI have focused on the time required for SVD computati on. As shown in Table 2, a variety of estimates for the time complexity of the truncated SVD have been produced. In general, there has been a tendency for the estimated complexity to decrease over the years as researchers have become more familiar with LSI. Figure 5 shows actual times for the truncated SVD calculation for several large LSI indexes. The measured SVD calculation time is plotted as a function of the total number of objects (terms plus documents) being indexed. The gr owth in required time is linear over the entire range of applications, through 100 million documents. This behavior is due, in great part, to the fact that the term-document matrices being ope rated on are extremely sparse  X  typically  X  .003% populated. The calculations are carried out using sparse matrix techniques. In general, there is reason to believe that, the larger the document collection, the more dimensions will be required in order to appropriately represent the range of concepts in that collection. One study has suggested that the number of dimensions should scale according to the log of the size of the collection [1]. Increasing the number of dimensions increases the total truncated SVD calculation time, as additi onal singular values must be computed. In addition, the vector creation time is increased, as more components have to be calculated. The combined increase is essentially linear in the numbe r of dimensions, as shown in Figure 6. 6 As the number of dimensi ons increases, the amount of RAM required for the build process increases. Also, as noted in section 5.4, all vectors for objects of interest must be held in RAM for query operations to be effi cient. Thus, as the number of dimensions is increased, the RAM requirements for the query server also increase. Collection of 1.6 million documents, 2 million terms, built on a 
DL 585 using hard disk storage. Once an LSI build process is complete, there are several files which constitute the repository for that space. The most important files are those containi ng the documents themselves (if they are stored locally), metada ta about the documents, the term and document vectors, ancillary data used to speed query processing, and intermediate re sults. The storage requirements for the repositories grow roughl y linearly with the number of objects indexed, as shown in Figure 7. The repository size also grows linearly with the number of dimensions chosen, as shown in Figure 8. In many LSI applications there is a requirement to accommodate ongoing acquisition of new documents. On a short-term basis, this typically is done through use of the folding-in process described in section 2.2. On a longer-term basis, periodic re-indexing of the data typically is carried out. As noted above, the time required to ingest documents and create the term-document matrix currently is the dominant factor in determining the total time required to produce an LSI inde x. Accordingly, in large LSI applications with updating, there is much to be gained from saving intermediate results. Storing the term-document matrix (sans global weighting) typically can yield a savings of  X  50% in the time required for creating an updated index. As noted in section 5.5, the term-document ma trix is very sparse and is represented using sparse matrix techniques. Thus, the size of the intermediate result file that must be stored is manageable. To date, the technique of latent semantic indexing typically has been applied to relatively small collections of data. Systematic analysis of the technique, however , shows that this need not be the case. All of the constituent processes making up the overall LSI algorithm can be executed in tim e that is linear in the number of objects being indexed. Moreove r, all of these processes are amenable to a high degree of parallel execution. Historically, it has been felt that the time required for calculation of the singular value decomposition of the LSI matrix was the primary impediment to large-scal e LSI applications. However, architectures, calculation of the SVD typically is not the most resource-intensive component of the basic LSI indexing processes. Moreover, the tim e required for (truncated) SVD calculation is linear in the number of objects being indexed. Thus, this process does not fundamentally constrain the ability to apply LSI to large problems. With today X  X  multi-core, multi-processor architectures, the time required to ingest the data bei ng indexed typically is the most resource-intensive element of LS I index creation. Use of high-performance disk arrays and solid state disks can significantly reduce the time required for this da ta ingestion. In dynamic LSI applications, saving intermediate results (e.g., the term-document matrix in a text application) obviates the need to re-ingest and re-parse earlier data. This can resu lt in dramatic time savings when re-building LSI indexes to accommodate new data. The requirements for storage and for processing differ significantly for the individual steps in LSI indexing. Most of the Accordingly, modern cloud compu ting architectures are ideal for large-scale LSI applications. Such architectures will enable much larger LSI applications in the near future. The ability to address large data co llections also will make LSI an increasingly attractive approach for applications involving non-textual data, including audio, video, and sensor data. I would like to thank the Semantic Engineering Team at Agilex Technologies for their pioneering work in creating the large-scale LSI implementations represented here. [1] Bradford, R. 2008. An empirical study of required [2] Bradford, R. 2009. Comparability of LSI and human [3] Carbonell, J., et al. 1997. Tran slingual information retrieval: [4] Chen, C., et al. 2001. Telcor dia LSI engine: implementation [5] Deerwester, S., et al. 1988. Improving information retrieval [6] Deerwester, S., et al. 1990. Indexing by Latent Semantic [7] Dumais, S. 1996. Information retrieval: finding needles in [8] Dumais, S. 2004. Latent Semantic Analysis. Annual [9] Kalman, D. 1996. A singularly valuable decomposition: the [10] Kettani, H., and Newby, G. 2009. On the ranking of text [11] Landauer, T., and Dumais, S. 2008. Latent semantic analysis. [12] Manning, C., Raghavan, P., and Schutze, H. 2008. [13] Skillicorn, D. 2004. Applying Matrix Decompositions to [14] Xu, Y., and and Umemura, K. 2003. Very low-dimensional [15] Zaman, A., and Brown, C. 2010. Latent semantic indexing [16] Berry, M., et al. 1993. SVDPACKC User X  X  Guide . Technical 
