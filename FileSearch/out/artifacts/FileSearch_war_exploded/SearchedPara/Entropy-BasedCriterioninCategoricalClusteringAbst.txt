 IBM T. J. Watson Research Center , Ha wthorne, NY 10532 USA Clustering is the problem of partitioning a finite set of points in a multi-dimensional space into classes (called clusters) so that (i) the points belonging to the same class are similar and (ii) the points belonging to dif ferent classes are dissimilar . Clustering has been extensi vely studied in machine learning, databases, and statistics from various perspecti ves. Man y applications of clustering have been discussed and man y clustering techniques have been devel-oped.
 An important step in designing a clustering technique is defining a way to measure the quality of partitioning in terms of the abo ve two objecti ves. For clustering numer -ical data, it is natural to think of designing such a measure based on a geometrical distance. Given such a measure, an appropriate partition can be computed by optimizing some quantity (e.g., the sum of the distances of the points to their cluster centroids). Ho we ver, if the data vectors contain cat-egorical variables, geometric approaches are inappropriate and other strate gies must be developed (Bock, 1989). Appearing in Proceedings of the 21 st International Con-fer ence on Mac hine Learning , Banf f, Canada, 2004. Cop y-right by the authors.
 The problem of clustering becomes more challenging when the data is cate gorical, that is, when there is no inherent distance measures between data values. This is often the case in man y applications where data is described by a set of descripti ve or binary attrib utes, man y of which are not numerical. Examples of such include the country of origin and the color of eyes in demographic data.
 Man y algorithms have been developed for clustering cate-gorical data, e.g., (Barbara et al., 2002; Gibson et al., 1998; Huang, 1998; Ganti et al., 1999; Guha et al., 2000; Gyl-lenber g et al., 1997). Entrop y-type measures for similarity among objects have been used from early on. In this paper , we sho w that the entrop y-based clustering criterion can be formally deri ved in the frame work of probabilistic cluster -ing models. We also establish the connections between the entrop y-based criterion with the approach based on dissim-ilarity coef ficients. We then develop an efficient Monte-Carlo procedure to find the optimal partition for minimiz-ing the entrop y-based criterion. Experiments demonstrate the efficac y and effecti veness of our approach. The rest of the paper is organized as follo ws: Section 2 for -mulates the problem of cate gorical clustering and sets down notations, Section 3 introduces the traditional entrop y-based clustering criterion, Section 4 sho ws the equi valence between the entrop y-based criterion with the classifica-tion lik elihood, Section 5 establishes the relations between entrop y-based criterion with dissimilarity coef ficients, Sec-tion 6 presents the iterati ve Monte-Carlo based procedure for minimizing the entrop y criterion, Section 7 gives our experimental results, and finally Section 8 concludes. Let D be a dataset of n points d each i , 1 i n , d For each i , 1 i n , and for each j , 1 j p , let d be the j -th component of d D into classes C each class are similar to each other .
 Each cate gorical variable can be decomposed into a col-lection of indicator variables. Suppose that every cate-gorical variable in D has at most m possible values. For each variable v , let the m values naturally correspond to the numbers from 1 to m and let v (1) ; ; v ( m ) be the bi-nary variables such that for each k , 1 k m , v ( k ) = 1 if and only if the v tak es the k -th value. Then the data set can be expressed as a collection of m n r matrices ij ) ; 1 i n; 1 j r; 1 k m if the j -th attrib ute of the i -th data point is in the egory . Hence the follo wing discussion is based on binary variables.
 We set down some notations. Suppose that a set of n r -dimensional binary data vectors, X , represented as an n r matrix, ( x ij ) , is partitioned into K classes C = ( C 1 ; : : : ; C K ) are similar to each other . We vie w C as a partition of the indices f 1 ; : : : ; n g . So, for all i , 1 i n , and k , 1 k K , we write i 2 C th vector belongs to the k -th class. Let N = nr . For each k , 1 k K , let n for each j , 1 j r , let N N N Consider a discrete random vector Y = ( y with r independent components, where for each i , 1 i r , y i tak es a value from a finite set V i .
 We will use ^ H for the estimated entrop y of the partition. 3.1. Entr opy Criterion The classical clustering criterion (Bock, 1989; Celeux &amp; Go vaert, 1991) searches for a partition C that maximizes the follo wing quantity O ( C ) : O ( C ) = the partition, i.e., the weighted sum of each cluster X  s en-mizing the expected entrop y of the partition: 3.2. Kullback X  X eibler Measur e The abo ve criterion can be interpreted as the Kullback X  Leibler measure (K X  X  measure) as follo ws: Suppose the observ ed dataset is generated by a number of classes. We first model the unconditional probability density function and then seek a number of partitions whose combination yields the density function (Roberts et al., 2000). The K X  X  measure then tries to measure the dif ference between the unconditional density and the density under partition. Let p ( y ) and q ( y ) be two distrib utions. Then For each j , 1 j r , and for each t 2 f 0 ; 1 g , let p ( y j = t ) = N j;t n and q ( y j = t ) = p tity is equal to P r is equal to ^ H ( X ) P r ^
H ( X ) + n k n ^ H ( C k ) . Thus, O ( C ) is equal to 1 r So, minimizing the K X  X  measure is equi valent to mini-mizing the expected entrop y of partition over the observ ed data. In this section, we sho w that the entrop y-based clustering criterion can be formally deri ved using a lik elihood princi-ple based on Bernoulli mixture models. In mixture models, the observ ed data are thought of as coming from a number of dif ferent latent classes. In our case, the observ ed data, assume that the data are samples from f 0 ; 1 g r and are sub-ject to a mixture of multi variate Bernoulli distrib utions: p ( x i ) = Here for each i , 1 i n , x and for each k , 1 k K , the k -th latent class is selected (so P K for each j , 1 j r , and for each k , 1 k K , a the k -th latent class. For each k , 1 k K , let a k = ( a k ; : : : ; a the abo ve p ( x a . 4.1. Maximum Lik elihood and Classification Recall that the Maximum Lik elihood Principle states that the best model is the one that has the highest lik elihood of generating the observ ed data. In the mixture model ap-proach, since the data points are independent and identi-cally distrib uted, the maximum lik elihood of obtaining the entire sample X can be expressed as: L ( a ) = log p ( X j a ) = log = We introduce auxiliary vectors, u 1 k K , where u i;k = 1 if and only if x i comes from the cluster C parameters. The classification lik elihood (Symons, 1981), denoted by CL ( a ; u ) , is equal to:
X It is easy to see that where
LP ( a ; u ) = Note that LP ( a ; u ) 0 and the quantity can be thought of as corresponding to the log arithm of the probability of the partition induced by u . Hence, the classification lik elihood is the standard maximum lik elihood penalized by a term measuring the quality of the partition. 4.2. Maximizing the Lik elihood From Equation 4, CL ( a ; u ) is equal to:
X If u is fix ed maximization of CL ( a ; u ) over a reduces to simultaneous maximization of CL 1 k K , and j , 1 j r , where CL k;j ( a ( j ) k ) = N and j , 1 j r , 0 &lt; a ( j ) ( N By replacing a ( j ) 1 j r , we have Once a dataset is given, the quantities n , p , and ^ H ( X ) Equation 1, since both aim at minimizing the expected en-trop y over the partition. Note that a  X  X enter X  for the cluster C information theoretical criterion and the maximum lik eli-hood criterion suggests a way to assess the number of clus-ters when using the entrop y criterion: to look at the lik eli-hood ratio based on latent classes. In addition, each cluster C In this section, we sho w the relations between the en-trop y criterion and the dissimilarity coef ficients. A pop-ular partition-based criterion (within-cluster) for clustering is to minimize the summation of dissimilarities inside the cluster . The within-cluster criterion can be described as minimizing where ( x x . In general, the distance function can be defined us-ing L tering, howe ver, the dissimilarity coef ficients are popular measures of the distances. 5.1. Dissimilarity Coefficients Given two data points, w and w 0 , there are four fundamen-tal quantities that can be used to define similarity between the two (Baulieu, 1997): a = kf j j w kf j j w j = 1 ^ w 0 j = 0 gk , c = kf j j w j = 0 ^ w 0 j and d = kf j j w been sho wn in (Baulieu, 1997) that the presence/absence based dissimilarity measure can be generally 1 written as Dissimilarity measures can be transformed into a similarity function by simple transformations such as adding 1 and inverting, dividing by 2 and subtracting from 1, etc. (Jar -ignored, i.e., is set to 0 , then the binary dissimilarity mea-sure can be generally written as D ( a; b; c; d ) = b + c where &gt; 0 . Table 1 sho ws several common dissimilarity coef ficients and the corresponding similarity coef ficients. Table 1. Binary dissimilarity and similarity coef ficients. The 5.2. Global Equi valence on Coefficients In cluster applications, the rankings based on a dissimilar -of the dissimilarity coef ficient. The follo wing results from (Baulieu, 1997) establish the equi valence among dissimi-larity coef ficients.
 Definition 1 Two dissimilarity coef ficients, D and D 0 , are said to be globally order equi valent if for all ( a that D ( a D ( a 2 ; b 2 ; c 2 ; d 2 ) &lt; D 0 ( a 1 ; b 1 ; c 1 ; d 1 Pr oposition 1 Let D = b + c globally order equivalent.
 Cor ollary 1 For all ; 0 &gt; 0 , D = b + c In other words, if the paired absences are to be ignored in the calculation of dissimilarity values, then there is only one single dissimilarity coef ficient modulo the global order lowing discussion is then based on the single dissimilarity coef ficient. 5.3. Entr opy and Dissimilarity Coefficients between binary vectors. If we assume that there is no joint absence in the dataset, i.e., for every single pairwise com-parison d = 0 , then a + b + c is constant because the vec-tors are dra wn from the same set and the y all have the same number of attrib utes.
 It is easy to see that the follo wing property holds: Remark 1 Let u , v , and w be binary vector s. If ( u; v ) &lt; ( u; w ) , then H ( f u; v g ) &lt; H ( f u; w g ) . No w examine the within-cluster criterion in Equation 6. We have: Here for each k , 1 k K , and for each j , 1 j r , Ha vrda and Charv at (Ha vrda &amp; Charv at, 1967) proposed a generalized entrop y of degree s , s &gt; 0 and s 6 = 1 , for a discrete probability distrib ution Q = ( q It holds that If we use the entrop y with s = 2 , then Thus, we have established the connections between the entrop y-criterion and the dissimilarity coef ficients. Fig-ure 1 sho ws the relations between the entrop y-based cri-terion and other criteria. The relations of the entrop y-based criterion to Minimum Description Length (MDL) / Mini-mum Message Length (MML) and to rate distortion theory 1994).
 Figure 1. A summary of relations among various clustering cri-We note here that Wallace (W allace, 1989) proposed a two-step procedure for numerical hierarchical cluster analysis by minimizing Gaussian entr opy , defined based on the log-arithm of the covariance matrix determinant. The rela-tionships between minimization of Gaussian entrop y and other objecti ve functions such as minimum variance, maxi-mum lik elihood and information radius were also discussed in (W allace, 1989). In the pre vious sections, we sho wed the connections be-tween the entrop y-based criterion and other criteria. The relations seem to indicate that man y clustering problems can be reduced to the problem of minimizing the entrop y criterion. In this section, we develop an efficient procedure to find the optimal partition for minimizing the entrop y-based criterion. 6.1. The Entr opy-Based Criterion The entrop y-based criterion (eq. 2) can be written as Pr oposition 2 ^ H ( X ) H ( C ) = 1 Pr oof We have The inequality follo ws from the log sum inequality in (Co ver &amp; Thomas, 1991). Note that Hence, we have ^ H ( X ) H ( C ) .
 Proposition 2 sho ws that any clustering process decreases the entrop y. The goal of clustering is to find a partition such that the increase in the entrop y, i.e., ^ H ( X ) H ( C ) is maximized (as in eq. 1). In other words, we should min-imize H ( C ) .
 Pr oposition 3 H ( C ) is maximized when all data points are in the same cluster .
 Pr oof If all the data points are put in one cluster , then H ( C ) = ^ H ( X ) . Then, by Proposition 2, H ( C ) is max-imized.
 For each j , 1 j r , and each k , 1 k K , let ( j j k ) be the probability that the j -th component of a vector is given that it belongs to C where ( k ) = n k P sho ws that H ( C ) is con vex when varying the clustering since p ( j; x evolv es lik e a negative log arithm which is a con vex func-tion. The con vexity of H ( C ) allo ws the optimization pro-cedures to reach global minimum. 6.2. Optimization Pr ocedur e We use a Monte-Carlo method to perform the optimization. Initially , all the points are placed in the same cluster . By Proposition 3, this initialization attains the maximal crite-rion. We then perform an iterati ve Monte-Carlo process to find the optimal partition. The clustering procedure is the follo wing Algorithm 1.
 Algorithm 1 uses a Monte-Carlo method to perform op-timization (Rubinstein, 1981). Randomly picking a data point x and putting it into another cluster is a trial step of modifying the parameters ( j j k ) . We then check whether the entrop y criterion is decreased, and if so, we accept the update and continue; otherwise, no modification will be made. This is repeated until there are no changes in the cluster assignment. The con vergence property of the Monte-Carlo optimization is sho wn in (Rubinstein, 1981). 7.1. Perf ormance Measur es There are man y ways to measure how clustering algorithms perform. One is the confusion matrix . Entry ( o; i ) of the confusion matrix is the number of data points assigned to output class o and generated from input class i . The input map I is the map of the data points to the input classes. So, the information of the input map can be measured by Algorithm 1 clustering procedur e Input: (data points: X , # of classes: k ) Output: cluster assignment; begin 1. Initialization: 1.1 Put all data points into one cluster 1.2 Compute Initial Criterion H 2. Iteration: 2.1 Randomly pick a point x from a cluster A 2.2 Randomly pick another cluster B 2.3 Put x into B 2.4 Compute the new entrop y H 2.5 if H H 2.5.1 Put x back into A 2.5.2 H = H 2.6 end 2.7 H 2.8 Goto Step 2.1 3. Retur n the cluster assignment end put map O that reco vers the information. Thus, the condi-the input map given the output map O , i.e., the proportion of information not reco vered by the clustering algorithm. Therefore, the reco very rate of a clustering algorithm, de-be used as a performance measure for clustering. The pu-rity (Zhao &amp; Karypis, 2001), which measures the extent to which each cluster contains data points primarily from a single class, is also a good measure. The purity of a clus-tering solution is obtained as a weighted sum of the purity of indi vidual clusters, given by where P ( C of the i -th input class that were assigned to the j -th clus-of purity , the better the clustering solution is. 7.2. Zoo Dataset We evaluate the performance of the algorithm on the zoo database available at the UC Irvine Machine Learning Repository . The database contains 100 animals, each of which has fifteen boolean attrib utes and one cate gorical at-six features, which correspond to 0 , 2 , 4 , 5 , 6 , and respecti vely . Table 2 sho ws the confusion matrix of this ex-periment and Table 3 sho ws comparison against K-means. At each step of the iteration process, one data point exper -iments a new cluster . A Monte-Carlo method accepts any better solutions instead of performing systematic searches. Figure 2 sho ws the entrop y descent of our Monte-Carlo method.
Figure 2. Entrop y Descent of the Monte-Carlo Method. 7.3. Clustering Document Datasets We also evaluate our entrop y-based clustering algorithm on document datasets. In our experiments, documents are rep-resented using binary vector -space model where each docu-ment is a binary vector in the term space and each element of the vector indicates the presence of the corresponding term. Our main goal is to find out how well this method would perform on document corpus. Therefore, we evalu-ate the method on standard labeled corpus widely used in information retrie val literature to evaluate supervised text cate gorization algorithms. In this way, we vie w the labels of the dataset as the objecti ve kno wledge on the structure of the datasets and then use the purity as the performance measure. For our experiments we use a variety of datasets, most of which are frequently used in the information retrie val research. The range of the number of classes is from four to ten, and the range of the number of documents is from 476 to 8280, which seem varied enough to ob-tain good insights on the algorithm. Table 4 summarizes the characteristics of the datasets. CSTR: This is the dataset of the abstracts of technical reports published in the Department of Computer Science at the Uni versity of Rochester between 1991 and 2002. The TRs are available et al., 2003) for text cate gorization. The dataset contained 476 abstracts, which were divided into four research ar-eas: Natural Language Processing(NLP), Robotics/V ision, Systems, and Theory . WebKB: The WebKB dataset con-tains webpages gathered from uni versity computer science departments. There are about 8280 documents and the y are divided into seven cate gories: student, faculty , staf f, course, project, department and other . The raw text is about 27MB. Among these seven cate gories, student, fac-ulty , course and project are four most populous entity-representing cate gories. The associated subset is typically called WebKB4 . In this paper , we perform experiments on both seven-cate gory and four -cate gory datasets. Reuters: The Reuters-21578 Text Cate gorization collection contains documents collected from the Reuters newswire in 1987. It is a standard text cate gorization benchmark and contains 135 cate gories. In our experiments, we use a subset of the data collection which include the ten most frequent cate-gories. To pre-process the datasets, we remo ve the stop words use a standard stop list and perform stemming using a porter stemmer , all HTML tags are skipped and all header fields except subject and organization of the posted article are ig-nored. In all our experiments, we first select the top 200 words by mutual information with class labels. The fea-ture selection is done with the rainbo w package (McCal-lum, 1996). In our experiments, we compare the perfor -mance of our entrop y-based method with the popular vec-tor space variant of the partitioning algorithms pro vided in the CLUT O package (Zhao &amp; Karypis, 2001). CLUT O package is built on a sophisticated multi-le vel graph par -titioning engine and offers man y dif ferent criteria that be used to dri ve both partitional and agglomerati ve cluster -ing algorithms. Figure 3 sho ws the comparison and the entrop y-based method is effecti ve on all four datasets. Figure 3. Performance Comparison on Document Datasets. In this paper , we study the entrop y-based criterion for cate-gorical data clustering and illustrate its relations with other criteria. An efficient, iterati ve Monte-Carlo procedure for optimization that tak es adv antage of the con vexity of the criterion is presented. The experimental results indicate the effecti veness of the proposed method.
 We would lik e to thank the revie wers for their valuable comments and suggestions. The first and the third authors are supported in part by NSF grants EIA-0080124 and EIA-0205061 and in part by NIH grants P30-A G18254.
 Barbara, D., Couto, J., &amp; Li, Y. (2002). COOLCA T: an entrop y-based algorithm for cate gorical clustering. Pro-ceedings of the Ele venth ACM CIKM Confer ence (pp. 582 X 589).
 Baulieu, F. B. (1997). Two variant axiom systems for pres-ence/absence based dissimilarity coef ficients. Journal of Classification , 14 , 159 X 170.
 Baxter , R. A., &amp; Oli ver, J. J. (1994). MDL and MML: simi-larities and dif fer ences (Technical Report 207). Monash Uni versity .
 Bock, H.-H. (1989). Probabilistic aspects in cluster analy-sis. In O. Opitz (Ed.), Conceptual and numerical analy-sis of data , 12 X 44. Berlin: Springer -verlag.
 Celeux, G., &amp; Go vaert, G. (1991). Clustering criteria for discrete data and latent class models. Journal of Classi-fication , 8 , 157 X 176.
 Co ver, T. M., &amp; Thomas, J. A. (1991). Elements of infor -mation theory . John Wiley &amp; Sons.
 Ganti, V., Gehrk e, J., &amp; Ramakrishnan, R. (1999). CA C-
TUS -clustering cate gorical data using summaries. Pro-ceedings of the Fifth ACM SIGKDD Confer ence (pp. 73 X  83).
 ing cate gorical data: An approach based on dynamical systems. Proceedings of the 24r d VLDB Confer ence (pp. 311 X 322).
 Guha, S., Rastogi, R., &amp; Shim, K. (2000). ROCK: A rob ust clustering algorithm for cate gorical attrib utes. Informa-tion Systems , 25 , 345 X 366.
 Gyllenber g, M., Koski, T., &amp; Verlaan, M. (1997). Classifi-cation of binary vectors by stochastic comple xity . Jour -nal of Multivariate Analysis , 47 X 72.
 classification processes: Concept of structrual a-entrop y. Kybernetika , 3 , 30 X 35.
 Huang, Z. (1998). Extensions to the k-means algorithm for clustering lar ge data sets with cate gorical values. Data Mining and Knowledg e Disco very , 2 , 283 X 304.
 Jardine, N., &amp; Sibson, R. (1971). Mathematical taxonomy . John Wiley &amp; Sons.
 Li, T., Zhu, S., &amp; Ogihara, M. (2003). Efficient multi-w ay text cate gorization via generalized discriminant analy-sis. Proceedings of Twelfth ACM CIKM Confer ence (pp. 317 X 324).
 McCallum, A. K. (1996). Bo w: A toolkit for statistical lan-guage modeling, text retrie val, classification and cluster -ing. http://www .cs.cmu.edu/ mccallum/bo w.
 Roberts, S., Ev erson, R., &amp; Rezek, I. (2000). Maximum certainty data partitioning. Pattern Reco gnition , 33 , 833 X 839.
 Rubinstein, R. Y. (1981). Simulation and the monte carlo method . John Wiley &amp; Sons.
 Symons, M. J. (1981). Clustering criteria and multi variate normal mixtures. Biometrics , 37 , 35 X 43.
 Wallace, R. S. (1989). Finding natur al cluster s thr ough entr opy minimization (Technical Report CMU-CS-89-183). Carne gie Mellon Uni versity .
 Zhao, Y., &amp; Karypis, G. (2001). Criterion functions for document clustering: Experiments and analysis (Tech-nical Report). Department of Computer Science, Uni-
