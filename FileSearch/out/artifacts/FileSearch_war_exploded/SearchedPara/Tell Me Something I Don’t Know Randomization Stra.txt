 There is a wide variety of data mining methods available, and it is generally useful in exploratory data analysis to use many different methods for the same dataset. This, how-ever, leads to the problem of whether the results found by one method are a reflection of the phenomenon shown by the results of another method, or whether the results de-pict in some sense unrelated properties of the data. For example, using clustering can give indication of a clear clus-ter structure, and computing correlations between variables can show that there are many significant correlations in the data. However, it can be the case that the correlations are actually determined by the cluster structure.

In this paper, we consider the problem of randomizing data so that previously discovered patterns or models are taken into account. The randomization methods can be used in iterative data mining. At each step in the data mining process, the randomization produces random samples from the set of data matrices satisfying the already discovered patterns or models. That is, given a data set and some statistics (e.g., cluster centers or co-occurrence counts) of the data, the randomization methods sample data sets having similar values of the given statistics as the original data set. We use Metropolis sampling based on local swaps to achieve this. We describe experiments on real data that demonstrate the usefulness of our approach. Our results indicate that in many cases, the results of, e.g., clustering actually imply the results of, say, frequent pattern discovery.
 H.2.8 [ Database management ]: Database Applications X  Data mining ; G.3 [ Probability and Statistics ]: Markov processes Algorithms, experimentation, theory Statistical significance, matrix randomization, inverse fre-quent set mining
The data mining research of the past 15 years has pro-duced a wide collection of algorithms for exploratory data analysis.

In this paper we consider a simple question. Suppose we have a dataset D , and we first run an analysis using algo-rithm A 1 on D , obtaining interesting results A 1 ( D ). Then we use another method A 2 on D , and again obtain fine re-sults A 2 ( D ). How do we know whether the second result is actually just a consequence of the first, or does it somehow increase our information about the data? So does A tell us something we don X  X  know, or is it just rephrasing the result that we already saw when A 1 ( D ) was given to us?
For example, using clustering can give indication of a clear cluster structure, and computing correlations between vari-ables can show that there are many significant correlations in the data. However, it can be the case that the correlations are actually determined by the cluster structure.

In this paper, we consider the problem of randomizing data so that previously discovered patterns or models are taken into account. The randomization methods can be used in iterative data mining. At each step in the data mining process, the randomization produces random samples from the set of data matrices satisfying the already discovered patterns or models.
 Consider the toy task of analyzing the dataset shown in Figure 1. We can first look at the row and column margins (sums of rows and columns) in the data, observing for in-stance that the column margins vary between 3 and 7 and the row margins between 1 and 6.

The next step in the analysis could be to find the frequent itemsets using minimum frequency of, say, 3. The resulting itemsets are all with frequency 3 expect itemsets A,E,F,H,AC,CH with frequency 4, itemset D with frequency 5 and itemset C with frequency 7.

The result contains multiple itemsets. Thus it is natural to ask which of them are interesting?
A possible method is to use randomization. Given the row and column margins, we can [5, 9] generate datasets that have these margins but are otherwise random. Then we can see how often the frequencies of the sets above are higher in the real data than in the generated datasets, i.e., we can compute empirical p -values for the frequencies of the dataset. 1
By preserving row and column margins in randomization, the frequencies of the itemsets are found to be statistically significant with significance level  X  = 0 . 05. Thus even given the information about the row and column margins the frequencies of these itemsets are interesting. The corresponding empirical p -values are The other empirical p -values are notably larger.

Now the question is, are the significances of the itemsets independent of each other? In other words, are the larger itemsets ABC,ABH,BCH,ABCH statistically significant only because the smaller itemsets AB,BH are?
To answer the question, we would like to compute empir-ical p -values in a way that takes the already known infor-mation into account. That is, we would like to constrain the sampling of datasets so that each dataset will have the same frequencies for the itemsets AB,BH . This effectively forms a new null hypothesis, which states that the dataset is a random dataset with specific row and column margins and frequencies of the itemsets AB,BH . Using this ran-domization and assessing the statistical significances of the itemsets ABC,ABH,BCH,ABCH , we discover that none of these itemsets are statistically significant with this new null hypothesis. The corresponding empirical p -values are We can therefore conclude that in this example, the fre-quencies of the smaller itemsets AB,BH and the marginal distributions explain the frequencies of the larger itemsets.
We continue analyzing the dataset by clustering the rows into two clusters using k -means. The first four rows are found to form the first cluster and the last five rows the sec-ond cluster. To assess the significance of the clustering given the row and column information, we compare the original k -means clustering error to clustering errors on randomized datasets where the row and column margins are preserved.
As there are several patterns, we should also correct for multiple testing [1, 11], but for simplicity we omit that con-sideration in the example.
 We obtain a p -value of 0 . 011 implying that the dataset con-tains a clustering structure which is independent of the row and column margins.

Now it would be tempting to conclude that the dataset contains some significant itemsets and an interesting clus-tering structure. But are the frequent itemsets and the clus-ters independent of each other? If we again preserve the frequencies of the itemsets AB,BH and the row and col-umn margins in randomization, we get an empirical p -value of 0 . 096 for the clustering structure of the dataset! That is, the cluster structure does not seem that interesting, given also the data about the two itemsets.

We can also do the reverse: We can preserve the cluster-ing structure in addition to the row and column margins in randomization and test the significances of the frequencies of the itemsets AB,BH,ABC,ABH,BCH,ABCH . In this case we obtain the empirical p -values respectively, thus implying that the clusters and the frequent itemsets depend on each other.

In this paper we consider the approach described in the above example in a general form. We give a general prob-lem statement, show hardness of the randomization problem and give simple approximate algorithms for the task. We describe experiments on real data that demonstrate the use-fulness of our approach. Our results indicate that in many cases, the results of, e.g., clustering actually imply the re-sults of, say, frequent pattern discovery.

The rest of this paper is organized as follows. In Section 2 we give the basic definitions. The general problem of ran-domizing datasets given the information of some other anal-yses is stated in Section 3. Section 4 gives some complexity results for this problem, showing that it is in most cases NP-hard. The randomization algorithms based on Metropolis techniques are given in Section 5, while the empirical results are described in Section 6. Section 7 is a short conclusion.
Randomization is a widely used method in statistics [10, 20]. The main benefit is that the user is releaved from the often difficult, and sometimes impossible, task of defining an analytical distribution for the test statistic. It is sometimes easier to devise a way of sampling from the null hypothesis than to actually define it analytically. And integrating over the analytical distribution, which is needed for the p -value calculation, may not be straightforward. MCMC methods are constructed to this purpose, which are also methods for randomization.

Based on a wide body of work in ecology (see [5]), [9] dis-cusses the randomization of binary matrices when the size of the matrix as well as the column and row margins are to be maintained. The authors use a Markov chain using local swaps that respect the marginal distributions. The pa-per [16] discusses the same idea for real matrices, where the marginal distributions are no longer single integers for rows or columns, but distributions. They use the Metropolis-Hastings method to define transition probabilities for their local changes to maintain the desired marginal distributions. We take influence from these papers to produce our methods.
Our concepts are closely related to the concept on non-derivability [4]. An itemset X is derivable if we can reduce the exact support of X from the supports of its sub-itemsets. For example, if the support of A is 0, then the support of AB must also be 0. This means that if we preserve the supports of the sub-itemsets, then the support of X will be constant, thus its p -value will be 1. Hence, our method can be seen as a generalization of non-derivability in which we can still remove insignificant X even though we do not know the exact support of X .

A closely related idea for iterative knowledge discovery is presented in [13], where randomization is used in model selection to test if a candidate model is significantly better than the current model. The current model is replaced by a significantly better candidate model and the process is repeated, effectively carrying out model selection iteratively.
The pattern ordering problem considered in [15] tries to order a collection of patterns so that each pattern gives as much information about the data as possible, given the ear-lier patterns. However, in that approach there is no consid-eration of randomization or significance, and the approach is only applicable to simple patterns. A related idea for closed itemsets, where statistical significance is used to order the patterns, is presented in [7]. For another type of approach to significance of patterns, see [12, 18, 19].
In this section, we describe how randomization approach is used in significance testing, give the definition of empirical p -values and discuss the method by Besag and Clifford for calculating MCMC p -values. First, however, we introduce the notation used in the paper.
Let D be a 0 X 1 dataset with m rows and n columns. The approach we describe is not limited to 0 X 1 data, but for sim-plicity we consider just such data in the paper. The rows of D correspond to transactions and the columns to attributes . The notation D tx refers to the element at row t and col-umn x in the matrix D . An itemset X is a subset of the attributes, i.e., X  X  { 1 ,...,n } . A transaction t covers an itemset X if D tx = 1 for all x  X  X . The frequency of an itemset X in the dataset D is the number of transactions t that cover the itemset X , denoted by fr( D ; X ). A family of itemsets F is a set of itemsets, F = { X 1 ,...,X k } . A clus-tering C of the rows of D is a partition of the set { 1 ,...,m } . The row and column margins of a dataset D are the row and column sums of D . Consider a 0 X 1 dataset D with m rows and n columns. Assume that some data mining task, such as frequent item-set mining, is performed on D . Let S ( D ) be the result of the data mining task. We assume that the result S ( D ) can be described with a single number, and we call such a result the structural measure of D . It can be, e.g., the frequency of a given itemset, the number of frequent itemsets or the clustering error of the dataset. Any measure can be used as long as larger (or smaller) values mean stronger presence of the structure.

To assess whether the result S ( D ) is explained by certain characteristics of the original dataset, we generate random m  X  n sized 0 X 1 datasets  X  D , which share the given charac-teristics with D , and compare the original result S ( D ) with the results S (  X  D ) on the randomized datasets. We can, e.g., preserve the row and column margins in randomization and assess the number of frequent itemsets.
Let  X  D = {  X  D 1 ,...,  X  D k } be a collection of independent ran-domized versions of the original dataset D . The one-tailed empirical p -value of S ( D ) for S ( D ) being large is This gives the fraction of randomized datasets whose struc-tural measure is larger than the original S ( D ). The one-tailed empirical p -value when small values of S ( D ) are in-teresting, and the two-tailed empirical p -value are defined similarly. If the obtained p -value, adjusted for multiplicity if needed, is less than a given threshold  X  , say,  X  = 0 . 05, we can regard the result to be independent of the characteristics preserved in randomization.
We will use Markov chain Monte Carlo methods to pro-duce the randomized datasets. The samples produced by Markov chains are generally not independent thus break-ing the validity of the empirical p -value. However, we will use the approach by Besag and Clifford [3] to guarantee the exchangeability of the samples. In the approach the chain is started from D and run backwards for K steps to pro-duce a new starting state  X  D 0 . Each randomized dataset is produced by starting a new chain from  X  D 0 and running the chain K steps forwards. This produces an exchangeable set of samples { D,  X  D 1 ,...,  X  D k } which ensures the validity of the empirical p -value regardless of the independence. If the samples are dependent, we obtain just more conservative p -values, see [2, 3] for more details. Our methods turn out to be time-reversible, i.e., running the chain backwards is the same as running the chain forwards.
In this section we formulate the general specific random-ization problems discussed in the introduction.
In randomization we want to preserve certain character-istics of the original 0 X 1 dataset D , e.g., the row and col-umn margins. Let f ( D ) be a statistics function which cal-culates these statistics of the dataset D . To measure the similarity between two datasets D and  X  D in the correspond-ing statistics f ( D ) and f (  X  D ), we define a difference measure h ( f ( D ) ,f (  X  D )) which is a positive function between two sets of statistics f ( D ) and f (  X  D ), where D and  X  D are 0 X 1 datasets with the same size. Any difference measure can be used as long as a smaller difference means that the statistics f ( D ) and f (  X  D ) are closer to each other and a zero difference that the statistics are equal.

We introduce two general randomization problems. In the first problem the statistics are preserved exactly where as in the second problem datasets with small difference in the statistics are sampled with high probability. The problem statements are as follows.

Problem 1 (ExactRand). Given a 0 X 1 dataset D and a statistics function f , generate a dataset  X  D chosen indepen-dently and uniformly from the set of 0 X 1 datasets having the same size and the same statistics as D , i.e., f ( D ) = f (
Problem 2 (SoftRand). Given a 0 X 1 dataset D , a statistics function f , a difference measure h ( f ( D ) ,f ( and a scaling constant w &gt; 0 , generate a dataset sen with a probability from all 0 X 1 datasets having the same size as D .
 Note that when w =  X  , SoftRand reduces to ExactRand .
Next we give the problem statements of the three spe-cific randomization tasks discussed in the introduction. The problems Margins , ClusterMargins and ItemsetMar-gins are examples of the problem ExactRand .

Problem 3 (Margins). Given a 0 X 1 dataset D , gen-erate a dataset  X  D chosen independently and uniformly from the set of 0 X 1 datasets having the same row and column mar-gins as the dataset D .
 Problem 4 (ClusterMargins). Given a 0 X 1 dataset D and a clustering C of the rows of D , generate a dataset  X  D chosen independently and uniformly from the set of 0 X 1 datasets having the same row and column margins as well as the same cluster centers and variances for each cluster in C as the dataset D .
 Problem 5 (ItemsetMargins). Given a 0 X 1 dataset D and a family of itemsets F , generate a dataset  X  D chosen independently and uniformly from the set of 0 X 1 datasets having the same row and column margins as well as the same frequencies for the itemsets in F as the dataset D .
It turns out that the problem ItemsetMargins is much harder than Margins and ClusterMargins , see Section 4. Thus we introduce SoftRand version of ItemsetMargins . Let D be the original dataset,  X  D a randomized dataset and F a family of itemsets which we are trying to preserve in randomization. We define the difference in the itemset fre-quencies of F between the datasets D and  X  D as where we have combined the statistics function directly into the difference measure.

Problem 6 (ItemsetMarginsSoft). Given a binary dataset D , a family of itemsets F and a scaling constant w &gt; 0 , generate a dataset  X  D chosen with a probability from the set of 0 X 1 datasets having the same row and column margins as the dataset D .
In this section we prove that the ItemsetMargins variant defined in Section 3.2 is intractable in general case. We will do this by reducing the HamiltonCycle to ItemsetMar-gins . This negative result shows the need for ItemsetMar-ginsSoft where we allow some variation in the frequencies.
Theorem 1. Assume that there is a random polynomial algorithm for ItemsetMargins . Then RP = NP even if the algorithm is provided with an example of such dataset.
A random polynomial algorithm is a Turing machine that is bounded by a polynomial time with an oracle produc-ing random independent numbers. A language is said to be in RP if there is a non-deterministic Turing machine such that, given a  X  X es X -instance, at least half of the com-putational paths end up with  X  X es X . It is easy to see that RP  X  NP , and the usual conjecture is that RP 6 = NP .
Our reduction is based on Hamiltonian cycles . A Hamil-tonian cycle is a cycle in a graph such that every node of the graph is visited only once. Alternatively we can see the cy-cle as a permutation of the nodes such that adjacent nodes (including the first and the last nodes) are connected. The problem HamiltonCycle is FNP -complete [17].
 To prove the main result we will need a couple of lemmae. The first lemma states that we can connect Hamiltonian cycles and the datasets satisfying some specific constraints.
Lemma 2. Assume that we are given a graph G . There are column and row margins, itemset frequency constraints, and a function m that maps a dataset satisfying the con-straints into a Hamiltonian cycle of G .

Proof. Assume that we are given a graph G with M nodes and N edges. Our goal is to construct appropriate constraints. At first, we will focus only on itemsets and column margins. The row margins will be discussed later. In our construction we will have 6 different attribute groups.
The first attribute group O = { o 1 ,...,o M } contains M attributes. We impose the frequencies fr( o i ) = 1, fr( o 0 for i,j = 1 ,...,M , i 6 = j . These frequencies will force that for each o i there is a row on which the attribute value is 1 and that there are no other active o j on that row, i.e., if we stack the rows into a matrix we will have a permutation matrix.
The second group V = { v 1 ,...,v M } is similar to the first, that is, we have fr( v i ) = 1 and fr( v i v j ) = 0. This construc-tion gives us also a permutation matrix. Also note that for each o i there is a unique v j such that there is a row in which both o i and v j are present. This allows us to define a per-mutation of  X  ( i ) = j . The main idea is that  X  represents the Hamiltonian cycle. This can be achieved if we can force that the nodes in G corresponding to  X  ( i ) and  X  ( i + 1) are con-nected. We will achieve this with the rest of the attributes.
The third group B = { b 1 ,...,b M } contains the attributes satisfying b i = o i  X  o i +1 . This is done by imposing the item-sets fr( b i ) = 2, fr( b i o i ) = 1, and fr( b i 1 ,...,M  X  1 and fr( b M ) = 2, fr( b M o M ) = 1, and fr( b 1. We see that the attribute b i is present on two rows and the rows are precisely those in o i and o i +1 are present.
Our fourth group of items resembles greatly the third group. We denote the group by C =  X  c 1 ,...,c M ( M  X  1) / 2 The group contains M ( M  X  1) / 2 attributes, where each c correspond to a pair of nodes ( j,k ) in the graph G . We de-fine the frequencies such that c i is exactly v j  X  v k . This is done by setting fr( c i ) = 2 and fr( c i v j ) = fr( c i
Now let us consider what happens if  X  ( i ) and  X  ( i + 1) are not connected in G . There is a c j corresponding to the node pair (  X  ( i ) , X  ( i + 1)) such that the columns b are equivalent. In other words, we have fr( b i c j ) = 2. Thus, to guarantee that  X  is indeed a valid Hamiltonian cycle we need to make sure that for all b i and c j corresponding to unconnected pairs in G we have fr( b i c j ) &lt; 2.
Since we do not have inequality constraints in our setup we will have to simulate it. Let L = M ( M  X  1) / 2  X  N be the number of pairs of unconnected nodes. We define the fifth set of attributes, denoted by S = { s 1 ,...,s LM } , to contain LM attributes, one s ij for each pair ( b i ,c j ), where c resent a pair of unconnected nodes. The needed inequality constraint is simulated by setting fr( s ij ) = 1, fr( s ij and fr( s ij c j ) = 0. This construction forces b i and c unequal so that the frequency fr( b i c j ) 6 = 2.

We see now that using the defined itemsets and column margins from all 5 groups forces the permutation  X  to be a valid Hamiltonian cycle.

Let us now turn the attention to the row margins. The number of 1s in a single row is as follows: one 1 from group O , one 1 from group V , two 1s from group B and M  X  1 1s from group C . Group S is problematic since the number of 1 varies per row. We remedy this by defining the sixth group E = { e 1 ,...,e LM } . This group contains LM attributes. We set e i to be the negation of s i . We achieve this by setting fr( e i ) = M  X  1 and fr( s i e i ) = 0. Now the common number of 1s on single row in group S and E is LM . Hence by setting all the row margins to 1 + 1 + 2 + M  X  1 + LM we have created a set of constraints such that a dataset satisfying the constraints corresponds to a Hamiltonian cycle.
Our second lemma states that for each Hamiltonian cycle in a given graph there is exactly same number of datasets satisfying the constraints.
 Lemma 3. Given a graph G and a Hamiltonian cycle H . Let m be the map given in Lemma 2. Define X = { D ; m ( D ) = H } to be the datasets which m maps into H . Then |X| is a constant not depending on the Hamiltonian cycle H .
Proof. To prove the lemma first note that a dataset sat-isfying the constraints has M unique rows. Thus there are M ! datasets obtained by permuting the rows. Assume now that the group O has the shape of the identity matrix. There are exactly 2 M different permutations for a given Hamilto-nian cycle, i.e., exactly 2 M different configurations for V . Note that the groups B and C are determined completely by the groups O and V and that the group E is determined by the group S . The theorem is proved if we can show that there are a constant number of configurations for S .
Note that there are 2 U configurations for S where U is the number of s ij for which fr( b i c j ) = 0. The frequency fr( b is 0 when both the  X  ( i ) and  X  ( i +1) do not contain the nodes corresponding to the c j . For a fixed j there are M  X  4 of such s ij . Hence we have U = ( M  X  4) L .
 Combining the numbers together we have that for a fixed Hamiltonian cycle we have exactly M !2 M 2 ( M  X  4) L datasets which is a constant. This proves the theorem.

Proof of Theorem 1. We start the proof by consider-ing a related NP -complete problem called SecondHamil-ton [17]. In this problem we are given a Hamiltonian graph, a Hamiltonian cycle, and we are asked if there is a second Hamiltonian cycle different from the given one.

Now let us consider a problem RandomHamilton hav-ing the same input as SecondHamilton in which we are asked to give a random Hamiltonian cycle of the graph. As-sume that we have a random polynomial algorithm H for this problem. Now consider the problem of SecondHamil-ton . We can replace H with a non-deterministic machine by considering all the possible random outputs of the oracle. Then we compare the random Hamiltonian cycle returned by H with a given Hamiltonian cycle. If the cycles differ, then we return  X  X es X , otherwise  X  X o. This means that if there is another cycle in the graph, then at least 1 / 2 of the com-putation paths ends up with  X  X es X . Thus we have shown that SecondHamilton is in RP . But SecondHamilton is NP -complete and this proves that NP = RP .

Now assume that there is a polynomial algorithm for Item-setMargins . Such an algorithm is given a set of constraints and an example dataset satisfying the constraints. By using the construction given in Lemma 2 we can use that algo-rithm for sampling Hamiltonian cycles in polynomial time. The given Hamiltonian cycle can be also easily transformed into a dataset needed for the input of the algorithm.
The only thing we need to show is that this reduction produces Hamiltonian cycles from the uniform distribution. Since we have assumed that the datasets are coming from the uniform distribution, it suffices to prove that there are same number of datasets for each Hamiltonian cycle in a fixed graph. But this is exactly the statement of Lemma 3. Next we introduce algorithms for solving the problems Margins , ClusterMargins and ItemsetMarginsSoft .
 First we introduce the method by Gionis et al. [9] for solving Margins . Our methods for ClusterMargins and Item-setMarginsSoft extend the method for solving Margins .
The method for producing random datasets  X  D having the same row and column margins as the original dataset D is based on swaps . In each swap two rows s,t and two columns x,y are selected such that D sx = D ty = 1 and D sy = D tx = 0. In a swap the four elements are swapped as shown in Figure 2. A swap preserves the row and columns sums. A randomized dataset  X  D is produced by performing K attempts of swaps. This is given in Algorithm 1. The existence of self-loops guarantees that the stationary distri-bution is uniform, see [9] for more details.
 Algorithm 1 Swap Input: Dataset D , num. of swap attempts K Output: Randomized dataset  X  D 1:  X  D  X  D 2: for i  X  1 to K do 3: Pick s,t and x,y such that  X  D sx = 1,  X  D ty = 1 4: if  X  D sy = 0 and  X  D tx = 0 then 5:  X  D  X  swapped version of  X  D 6: end if 7: end for 8: return  X  D
To obtain an algorithm for the problem ClusterMar-gins , we modify Algorithm 1 to preserve the given clustering C . At each step we pick a cluster C  X  X  and attempt a swap inside that cluster. When the rows s,t belong to the same cluster, the cluster centers and variances do not change. The pseudocode of this approach is given in Algorithm 2. Algorithm 2 Cluster-Swap Input: Dataset D , partition C of D , num. of swap at-Output: Randomized dataset  X  D 1:  X  D  X  D 2: for i  X  1 to K do 3: Pick a cluster C  X  X  4: Pick s,t  X  C and x,y such that  X  D sx = 1,  X  D ty = 1 5: if  X  D sy = 0 and  X  D tx = 0 then 6:  X  D  X  swapped version of  X  D 7: end if 8: end for 9: return  X  D
As was mentioned in Section 3, preserving itemset fre-quencies exactly in general is hard. Thus we give an al-gorithm for solving ItemsetMarginsSoft where the item-set frequencies are preserved approximately. We use the Metropolis algorithm [14] to produce random samples from the probability distribution where h F is defined in Equation (2). At each step in the Metropolis algorithm, a proposal modification D 0 of the cur-rent state  X  D is formed. The proposal is accepted as the new state with a probability min(1 , X  ( D 0 ) / X  (  X  D )). A direct implementation of the Metropolis algorithm with swaps is given in Algorithm 3.
 Algorithm 3 Itemset-Swap Input: Dataset D , itemsets F , num. of swap attempts K Output: Randomized dataset  X  D 1:  X  D  X  D 2: for i  X  1 to K do 3: Pick s,t and x,y such that  X  D sx = 1,  X  D ty = 1 4: if  X  D sy = 0 and  X  D tx = 0 then 5: D 0  X  swapped version of  X  D 6: a  X  Uniform(0,1) 7: if a &lt; exp { X  w ( h F ( D,D 0 )  X  h F ( D,  X  D )) } then 8:  X  D  X  D 0 9: end if 10: end if 11: end for 12: return  X  D The same approach can be used to solve the problem SoftRand in general. However, if too many characteris-tics are preserved at the same time, the chain may not mix well enough. In such cases parallel tempering can be used to overcome the problem [8]. In our experiments, we consider only a reasonable amount of small itemsets as constraints.
In this section, we carry out several data mining exper-iments to demonstrate the use of our framework. We will first present a method to automatically analyze the conver-gence of the Markov chain. The method is then used in the subsequent experiments with two real data sets.
We use two real data sets: Paleo and Courses , whose basic characteristics are presented in Table 1. The Paleo data contains paleontological absence/presence information about species in excavation sites [6]. The Courses data set records the courses individual students have taken in the Department of Computer Science in Helsinki University of Technology.

We used the method by Webb [18] to ensure correct treat-ment of multiple hypothesis. We split both data sets ran-domly half on rows, and used the other part for mining fre-quent itemsets of size 2 and 3. The other part was used for randomization, i.e., p -value calculation. 3
The Paleo data set was mined with minimum support 4, and Courses with 200. It turned out that only 41 columns of Courses are covered by the found frequent sets. We choose to ignore the columns of Courses that are not present in any of the frequent sets. We chose to consider only the fre-quent sets of size 2 and 3, since it may be fairly easy to understand the co-occurrence of 2 or 3 variables, but under-standing itemsets of larger size is increasingly difficult.
When calculating the p -values of itemsets, we use the support as the test statistic, where a higher value is more extreme, i.e., more interesting. We always sample 1000 matrices and use them for p -value calculation. The unad-justed, raw p -values are always adjusted for multiplicity with Benjamini-Hochberg method [1], which controls the false dis-covery rate (FDR). The level of FDR is set to 0.05, and thus, if the p -value of a pattern does not exceed this threshold value, the pattern is considered statistically significant. We also cluster the data sets in some of the experiments. We use the traditional k -means with 3 clusters for Paleo and 20 clusters for Courses . Since k -means may get stuck in local optima, we run k -means 10 times and use the clus-tering with the smallest error. When calculating the p -value of a clustering, our test statistic is the sum of the square of the L 2 distances between rows and their respective cen-troids. This is almost the same as optimized by k -means, but we use this slightly modified version since we know the ClusterMargins maintains the value of this test statistic.
We use the Algorithm 3 to solve ItemsetMarginsSoft , with w = 4. That is, a swap which would increase the total error in itemset frequencies by one is accepted with probability exp(  X  4)  X  0 . 018.
NOW public release 030717 available from [6].
We also tried not using Webb X  X  method and use the com-plete data sets for mining itemsets as well as randomization. However, the results did not differ significantly. Figure 3: Boxplot of distances between the orig-inal matrix and 5 ItemsetMarginsSoft randomized matrices of Courses data with different number of swap attempts. The x-axis depicts the multiplier to use with the number of ones in Courses to get the number of swap attempts.
In our experiments we used an automatic convergence analysis method that is based on the distance between the original and the randomized matrix. As a distance we used the square of the Frobenius norm between these two matri-ces [16]. With 0 X 1 matrices, this is essentially the number of cells where the two matrices differ. When swapping, this distance usually first increases rapidly, until after some num-ber of swaps it tends to settle around a certain value. The intuition is that when the distance does not change much, the Markov chain is considered to have converged.

We calculate the number of swap attempts K needed as follows. We first assign to K the amount of ones in the ma-trix, as it gives an indication of the number of swaps possible in that matrix, given that the matrix is sparse. We then ran-domize the original matrix separately 5 times with K swaps and calculate the mean of the matrix distances between the randomized data and the original data. If the mean distance with the current K has changed less than 1% with respect to the previous step, we consider the chain to have converged, stop iterating, and use the final value of K in the sampling. Otherwise, we multiply K by 2 and repeat the step.

As an example, we run the automated convergence analy-sis on Courses with 40 frequent sets of size 2 as constraints. The development of the matrix distances is given in Figure 3.
Note that since we use backward forward sampling, the chain is first traversed backwards with K swap attempts, and the resulting matrix is used as a basis for further ran-domizations. Eventually, the number of swap attempts from the original matrix to a matrix sample is 2 K .
In the first experiment, we are interested in the covari-ances between the columns when constraining the random-ization in different ways. Therefore, we focus on all possible itemsets of size 2, and randomize with Margins , Itemset-MarginsSoft and ClusterMargins . For the constraints of ItemsetMarginsSoft we used itemsets of size 2 con-taining adjacent items. Notice that we did not use Webb X  X  method here since we consider all possible patterns. Table 2: Average execution times in milliseconds to produce a single random matrix with Margins (M), ItemsetMarginsSoft (IM) and ClusterMargins (CM). The implementations are written in Java and the randomizations run on a 2.5GHz machine.
 Table 3: Some of the contingency tables of the significance of itemsets in randomizations Margins (M), ItemsetMarginsSoft (IM) and ClusterMargins (CM). S represents statistical significance and N the opposite. All pairs of columns in a data set were used as itemsets.

The execution times to produce a single random matrix with different constraints and data sets are listed in Table 2.
Table 3 depicts several interesting contingency tables of the significance of itemsets in two randomizations at a time. We also measured the significance of the clustering result. Using the defined test statistic, the clustering was statis-tically significant in Margins and ItemsetMarginsSoft , but naturally not in ClusterMargins .

When comparing the contingency table of ItemsetMar-ginsSoft and Margins in Paleo , it is evident that many of the itemsets were not significant in either randomization. Still, several itemsets were statistically significant in both cases. One example of such an itemset with two species is Brachyodus and Bunolistriodon . Both were contained in sep-arate itemsets constrained in ItemsetMarginsSoft . Ap-parently, having both species in separate itemset constraints did not require to maintain the support of their combined itemset, and hence, it was statistically significant.
Other interesting itemsets are the ones that were statisti-cally significant in ItemsetMarginsSoft but not in Mar-gins . One example of such itemset is Hyainailouros and Amphimoschus . Again both species were contained in sepa-rate itemset in the constraints, but their combined itemset was not. These constraints had a clear decreasing effect in the support of the itemset, and therefore, it was found significant when randomizing with ItemsetMarginsSoft , but not with Margins .
 The contingency table between ClusterMargins and ItemsetMarginsSoft shows that no itemset was statisti-cally significant when constraining the randomization with clustering. Even though the Paleo data set is known to have three clear clusters, the extent of this constraint was surpris-ing. We expected to see at least few itemsets significant in ClusterMargins . Evidently the clustering result fully ex-plains the pairwise correlations of the species in the data set.
Similar results were found from Courses . An example of a statistically significant itemset in Margins and Item-setMarginsSoft is Computer Organization and Database Systems I . This was expected, since 90% of students who took Database Systems I also took Computer Organization .
One example of a course pair that was significant in Item-setMarginsSoft but not significant in Margins is Models for Programming and Computing and Introduction to the Use of Computers . One possible reason for this is that this pair of courses is taken by very different kind of people. The itemsets of these groups introduce anti-correlation between the courses in the example, and therefore, that itemset be-comes statistically significant in ItemsetMarginsSoft .
The clustering results are again similar to the ones in Pa-leo , with the exception that now some itemsets are signifi-cant in ClusterMargins . An example of a pair of courses that is significant in ItemsetMarginsSoft and Cluster-Margins is Reading Comprehension in English and English Oral Test . These courses are not specific to any study pro-gramme in computer science, and therefore, are not affected by clustering. The clustering mostly separates students with respect to computer science courses, which makes these gen-eral courses poorly clustered. However, this allows them to be statistically significant even when constraining the ran-domization with the clustering.
The motivation of the second experiment is to simulate an actual data mining scenario, where the patterns are discov-ered by a data mining algorithm and these are then used to assess which are significant in different randomization scenarios. Essentially, the idea is to find out the possible interrelations between itemsets and clustering.

We focus on frequent sets of size 2 and 3 mined from the used data sets. Here we use again the same randomizations Margins , ItemsetMarginsSoft and ClusterMargins , but with different constraints for ItemsetMarginsSoft .
The first set of constraints in ItemsetMarginsSoft is constructed by adding the 40 most significant itemsets from Margins , i.e., the ones with the smallest p -values. The interest is how the significance of the other itemsets change when constraining with the most significant itemsets. The second set of itemset constraints is used for Itemset-MarginsSoft , but now the set contains the itemsets that had the largest increase in their p -value from Margins to ClusterMargins . The intuition is that these itemsets may explain the clustering since they were explained by it. We use ICM for short for this randomization.

Table 4 depicts a few interesting contingency tables of the significance of itemsets in two randomizations at a time. The clustering was statistically significant in Margins , setMarginsSoft and ICM, but not in ClusterMargins . The Paleo results between ItemsetMarginsSoft and ClusterMargins are very similar to the previous section. None of the itemsets were significant in ClusterMargins , and a portion was significant in ItemsetMarginsSoft . The strength of the clustering result clearly also affect the item-sets of size 3.

The contingency table between ICM and ItemsetMar-ginsSoft displays the fact that neither of the constraint sets completely explain the data since roughly 500 itemsets were significant in both, and few hundred in either. Still, 60% of the itemsets were explained by both constraint sets along Table 4: Some of the contingency tables of the significance of itemsets in randomizations Mar-gins (M), ItemsetMarginsSoft (IM), ClusterMargins (CM) and ItemsetMarginsSoft constrained by item-sets found using ClusterMargins (ICM). S repre-sents statistical significance and N the opposite. Fre-quent itemsets of size 2 and 3 were used as itemsets. with the margins. However, 1419 itemsets were never signif-icant even with Margins , which means that the expressive power of both constraint sets is very limited.

ClusterMargins is very different from ICM. Although we tried to construct the set of itemsets to constrain in ICM in such a way that the results would be the same as with ClusterMargins , the numbers of significant itemsets do not show this.
 The results for Courses display similar behavior between ItemsetMarginsSoft and ClusterMargins as in previ-ous experiment, and different from Paleo . Clearly, the clus-tering of Courses does not describe the data as well as for Paleo . One example of an itemset of courses significant in both is Programming Project and Programming in Java . Ev-idently, these have much in common and are most likely taken both instead of just either of them.

The comparison between ClusterMargins and ICM ex-presses the same as with ClusterMargins and Itemset-MarginsSoft . We can conclude also here that ICM with these constraints is not as strict, and conversely, does not explain as much, as ClusterMargins . An example itemset significant in ICM but not in ClusterMargins is Informa-tion Systems Project , Programming Project and Data Com-munications . The last two was an itemset constrained in ICM, and the first was also a part of a separate constraint.
This itemset had high frequency in some clusters but very low in others. Because of this, the randomization had little room to break the itemset in ClusterMargins . We conjec-ture that these courses are most likely all required in some study programme. Additionally, Programming Project and Data Communications together did not explain the occur-rence of the triplet, and therefore it was significant in ICM.
In our final experiment we conduct an iterative data min-ing process in which the itemsets are added iteratively to constraints. We used again the frequent sets of size 2 and 3.
First, the data is randomized with Margins and the item-set with the smallest p -value is inserted to the set of itemset constraints. The data is then randomized at each iteration with ItemsetMarginsSoft and always the itemset with the smallest p -value is added to the set of constraints. This resembles the situation where the user iteratively tries to un-derstand one pattern at a time and wants to find which pat-terns are not explained by the already understood patterns. Figure 4: The number of significant itemsets at each iteration. Notice the different vertical scale and that the lower parts of the bars have been cropped to better see the difference between iterations.

We carried out a total of 10 iterations. Figure 4 displays the number of itemsets found significant at each iteration, including the initial Margins randomization.

The results follow almost with no fail the intuition that when constraints are added, the number of significant item-sets decreases. However, this may not always be the case, as seen in the previous experiments. Sometimes adding con-straints increases the statistical significance of some patterns by introducing anti-correlation. Still, the intuitive results may be due to how the itemset constraints were selected. At any time, the constrained itemsets do not explain the itemsets found statistically significant. However, the signifi-cant itemsets are still likely to have some correlation between them, and adding one of them to the set of constraints will restrict the rest. Selecting itemsets in another fashion may produce very different results.
Our focus in the paper was to study the concept of it-erative data mining. The idea behind this approach is the question whether the results of one analysis explains or im-plies the results of another analysis. This approach can be then refined into iterative data mining process in which the user iteratively selects interesting patterns or models that are then used for updating the significance of the rest of the patterns or models.

Our approach is to produce random data sets having the same selected statistics as the original dataset. As constrain-ing statistics we used row and column margins, itemsets, and clustering structure. Using these random data sets we computed empirical p -values for our test statistics. Our ex-periments demonstrated that our method works in practice. [1] Yoav Benjamini and Yosef Hochberg. Controlling the [2] Julian Besag. Markov chain Monte Carlo methods for [3] Julian Besag and Peter Clifford. Generalized Monte [4] T. Calders and B. Goethals. Non-derivable itemset [5] G. W. Cobb and Y.-P. Chen. An application of [6] Mikael Fortelius and Jussi Eronen. Now  X  neogene of [7] Arianna Gallo, Tijl Bie, and Nello Cristianini. Mini: [8] Charles J. Geyer. Markov chain monte carlo maximum [9] Aristides Gionis, Heikki Mannila, Taneli Mielik  X  ainen, [10] Phillip Good. Permutation Tests: A Practical Guide [11] S. Holm. A simple sequentially rejective multiple test [12] Szymon Jaroszewicz. Interactive hmm construction [13] David Jensen. Knowledge discovery through induction [14] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, [15] Taneli Mielik  X  ainen and Heikki Mannila. The pattern [16] Markus Ojala, Niko Vuokko, Aleksi Kallio, Niina [17] Christos H. Papadimitriou. Computational [18] Geoffrey I. Webb. Discovering significant patterns. [19] Geoffrey I. Webb. Layered critical values: A powerful [20] Peter H. Westfall and S. Stanley Young.

