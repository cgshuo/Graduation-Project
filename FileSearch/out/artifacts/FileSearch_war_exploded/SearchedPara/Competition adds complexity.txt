 From online auctions to Texas Hold X  X m, AI is captivated by mu lti-agent interactions based on com-petition. The problem of finding a winning strategy harks back to the first days of chess programs. Now, we are starting to have the capacity to handle issues lik e stochastic games, partial informa-tion, and real-time video inputs for human player modeling. This paper looks at the complexity of computations involving the first two factors: partially obs ervable stochastic games (POSGs). There are many factors that could affect the complexity of di fferent POSG models: Do the players, collectively, have sufficient information to reconstruct a state? Do they communicate or cooperate? Is the game zero sum, or do the players X  individual utilities depend on other players X  utilities? Do the players even have models for other players X  utilities? The ultimate question is, what is the complexity of finding a w inning strategy for a particular player, with no assumptions about joint observations or knowledge o f other players X  utilities. Since a special case of this is the DEC-POMDP, where finding an optimal (joint , cooperative) policy is known to be NEXP-hard [1], this problem cannot be any easier than in NEXP .
 We show that one variant of this problem is hard for the class N EXP NP . 2.1 Partially observable stochastic games A partially observable stochastic game (POSG) describes multi-player stochastic game with imper-fect information by its states and the consequences of the pl ayers actions on the system. We follow the definition from [2] and denote it as a tuple M = ( I , S , s 0 , A , O , t , o , r ) , where A POSG where all agents have the same reward function is calle d a decentralized partially-observable Markov decision process (see [1]).
 according to the transition probability function t . A run of M is a sequence of steps that starts in the initial state s 0 . The outcome of each step is probabilistic and depends on the actions chosen. For each agent, a policy describes how to choose actions depending on observations m ade during the run of the process. A (history-dependent) policy  X  chooses an action dependent on all observations made by the agent during the run of the process. This is descri bed as a function  X  : O  X   X  A , mapping each finite sequence of observations to an action.
 accordingly. Then prob (  X  ,  X  1 , . . . ,  X  k ) is defined by the reward obtained in s by the actions according to  X  k 1 weighted by the probability that s is reached after l steps, A POSG may behave differently under different policies. The quality of a policy is determined by its performance , i.e. by the sum of expected rewards received on it. We use | M | to denote the size of the representation of M . 1 The short-term performance for policies  X  k 1 for agent i with POSG M is the expected sum of rewards received by agent i during the next | M | steps by following the policy , i.e. The performance is also called the expected reward.
 Agents may cooperate or compete in a stochastic game. We want to know whether a stochastic game can be won by some agents. This is formally expressed in the fo llowing decision problems. The cooperative agents problem for k agents: The competing agents problem for 2k agents: It was shown by Bernstein et al. [1] that the cooperative agen ts problem for two or more agents is complete for NEXP. 2.2 NEXP NP A Turing machine M has exponential running time, if there is a polynomial p such that for every can be decided by a nondeterministic Turing machine within e xponential time. NEXP NP is the class of sets that can be decided by a nondeterministic oracle Turi ng machine within exponential time, when a set in NP is used as an oracle. Similar as for the class NP NP , it turns out that a NEXP NP computation can be performed by an NEXP oracle machine that a sks exactly one query to a co NP oracle and accepts if and only if the oracle accepts. 2.3 Domino tilings Domino tiling problems are useful for reductions between di fferent kinds of computations. They have been proposed by Wang [3], and we will use it according to the following definition. satisfies both the following conditions. a string consisting of k 1 s (k  X  N ), such that there exists a T -tiling of the 2 k -square. It was shown by Savelsbergh and van Emde Boas [4] that the expo nential square tiling problem is complete for NEXP. We will consider the following variant , which we call the exponential  X  2 2 k -square with final row w , such that there exists no T -tiling of the 2 k -square with initial row w ? The proof technique of Theorem 2.29 in [4], which translates Turing machine computations into tilings, is very robust in the sense that simple variants of t he square tiling problem can analogously be shown to be complete for different complexity classes. To gether with the above characterization of NEXP NP it can be used to prove the following.
 Theorem 2.2 The exponential  X  2 square tiling problem is complete for NEXP NP . POSGs can be seen as a generalization of partially-observab le Markov decision processes (PO-MDPs) in that POMDPs have only one agent and POSGs allow for ma ny agents. Papadimitriou and Tsitsiklis [5] proved that it is PSPACE-complete to deci de the cooperative agents problem for POMDPs. The result of Bernstein et al. [1] shows that in case o f history-dependent policies, the complexity of POSGs is greater than the complexity of POMDPs . We show that this difference does not appear when stationary policies are considered ins tead of history-dependent policies. For POMDPs, the problem appears to be NP-complete [6]. A station ary policy is a mapping O  X  A from observations to actions. Whenever the same observatio n is made, the same action is chosen by a stationary policy.
 Theorem 3.1 For any k  X  2 , the cooperative agents problem for k agents for stationary policies is NP -complete.
 Proof We start with proving NP-hardness. A POSG with only one agent is a POMDP. The problem of deciding, for a given POMDP M , whether there exists a stationary policy such that the shor t-term performance of M is greater than 0, is NP-complete [6]. Hence, the cooperativ e agents problem for stationary policies is NP-hard. for the k agents. This sequence can be straightforwardly represente d using not more space than the representation of t takes. Under a fixed sequence of policies, the performance of the POSG for all of the agents can be calculated in polynomial time. Using a guess and check approach (guess the stationary policies and evaluate the POSG), this shows t hat the cooperative agents problem for stationary policies is in NP. 2 In the same way we can characterize the complexity of a proble m that we will need in the proof of Lemma 3.3.
 Corollary 3.2 The following problem is coNP -complete. The cooperative agents problem was shown to be NEXP-complet e by Bernstein et al. [1]. Not surprisingly, if the agents compete, the problem becomes ha rder.
 Lemma 3.3 For every k  X  1 , the competing agents problem for 2 k agents is in NEXP NP .
 construct a POSG that  X  X mplements X  these policies and leave s open the actions chosen by agents k + 1 , . . ., 2 k .
 This new POSG has states for all short-term trajectories thr ough the origin POSG. Therefore, its size is exponential in the size of the origin POSG. Because th e history is stored in every state, and the POSG is loop-free, it turns out that the new POSG can be tak en as a POMDP for which a (joint) policy with positive reward is searched. This problem is kno wn to be NP-complete.
 The meaning of state ( s , u )  X  S  X  is, that state s can be reached on a trajectory u (that ends with s ) All rewards gained in the sink state are 0. Now for the transit ion probabilities. If s is reached on in
M  X  from ( s , u ) . In the formal description, the sink state has to be consider ed, too. (( s , u ) , a k , . . . , a 2 k , (  X  s ,  X  u )) = The observation in M  X  is the sequence of observations made in the trajectory that i s contained in agents have no impact on this, only the actions the other agen ts choose. Therefore, agent i obtains the i = k + 1 , . . . , 2 k . Notice that the size of M  X  is exponential in the size of M . The sink state in M  X  is the only state that lies on a loop. This means, that on all trajectories through M  X  , the sink state is the only state that may appear more than once. All states other than the sink stat e contain the full history of how they are reached. Therefore, there is a one-to-one corresponden ce between history-dependent policies for M and stationary policies for M  X  (with regard to horizon | M | ). Moreover, the corresponding policies have the same performances.
 Claim 1 Let  X  1 , . . . ,  X  2 k be short-term policies for M , and let  X   X  k stationary policies for M  X  .
 For | M | steps and i = 1 , 2 , . . . , k, perf i ( M ,  X  2 k 1 ) = perf i Thus, this yields an NEXP NP algorithm to decide the competitive agents problem. The inp ut is a nondeterministic exponential time. In the second step, the POSG M  X  is constructed from the input M and the guessed policies. This takes exponential time (in th e length of the input M ). Finally, the oracle is queried whether M  X  has positive performance for all agents under all stationar y policies. This problem belongs to coNP (Corollary 3.2). Henceforth, t he algorithm shows the competing agents problem to be in NEXP NP . 2 Lemma 3.4 For every k  X  2 , the competing agents problem for 2 k agents is hard for NEXP NP . Proof We give a reduction from the exponential  X  2 square tiling problem to the competing agents problem.
 Let T = ( T , 1 k ) be an instance of the exponential  X  2 square tiling problem, where T = ( V , H ) is a tile type. We will show how to construct a POSG M with 4 agents from it, such that T is a positive instance of the exponential  X  2 square tiling problem if and only if (1) agents 1 and 2 have a ti ling for the 2 k square with final row w such that (2) agents 3 and 4 have no tiling for the 2 k square with initial row w .
 The basic idea for checking of tilings with POSGs for two agen ts stems from Bernstein et al. [1], but we give a slight simplification of their proof technique, and in fact have to extend it for four agents later on. The POSG is constructed so that on every traj ectory each agent sees a position in the square. This position is chosen by the process. The only a ction of the agent that has impact on the process is putting a tile on the given position. In fact , the same position is observed by the agents in different states of the POSG. From a global point of view, the process splits into two parts. The first part checks whether both agents know the same tiling , without checking that it is a correct tiling. In the state where the agents are asked to put their ti les on the given position, a high negative reward is obtained if the agents put different tiles on that p osition.  X  X igh negative X  means that, if there is at least one trajectory on which such a reward is ob tained, then the performance of the whole process will be negative. The second part checks wheth er the tiling is correct. The idea is to give both the agents neighboured positions in the square and to ask each which tile she puts on that position. Notice that the agents do not know in which part of t he process they are. This means, that they do not know whether the other agent is asked for the same p osition, or for its upper or right neighbour. This is why the agents cannot cheat the process. A high negative reward will be obtained if the agents X  tiles do not fit together.
 For the first part, we need to construct is a POSG P k for two agents, that allows both agents to make the same sequence of observations consisting of 2 k bits. This sequence is randomly chosen, and encodes a position in a 2 k  X  2 k grid. At the end, state same is reached, at which no observation is made. At this state, it will be checked whether both agents pu t the same tile at this position (see later on). The task of P k is to provide both agents with the same position. Figure 1 sho ws an example independent of the actions. The observation of agent 1 is wri tten on the left hand side of the states, and the observations of agent 2 at the right hand side. In s 4 , the agents make no observation. In P k both agents always make the same observations.
 The second part is more involved. The goal is to provide both a gents with neighboured positions in the square. Eventually, it is checked whether the tiles th ey put on the neighboured positions are according to the tile type T . Because the positions are encoded in binary, we can make use representation of strings. if n w = n u + 1, then for some index l it holds that (1) u i = w i for i = 1 , 2 , . . . , l  X  1, (2) w l = 1 and u l = 0, and (3) w j = 0 and u j = 1 for j = l + 1 , . . ., k . The POSG C l where the index of the leftmost bit of the column encoding whe re both positions distinguish is l . (The C stands for column .) Figure 2 shows an example for the 2 4 -square. The  X  X inal state X  of C l In the same way, a POSG R l whether two tiles in neighboured rows correspond to a correc t tiling. This POSG has the final state vert , from which on it is checked whether two tiles fit vertically.
 Finally, we have to construct the last part of the POSG. It con sists of the states same , hori , vert (as mentioned above), good , bad , and sink . All transitions between these states are deterministic (i .e. with probability 1). From state same the state good is reached, if both agents take the same action  X  otherwise bad is reached. From state hori the state good is reached, if action a 1 by agent 1 and a 2 bad is reached. Similarly, from state vert the state good is reached, if action a 1 by agent 1 and a 2 by sink is reached on every action with reward 1, and from state bad the state sink is reached on every and all agents obtain reward 0. All rewards are the same for bo th agents. (This part can be seen in the overall picture in Figure 4).
 From these POSGs we construct a POSG T 2 tiling for a 2 k  X  2 k square, as described above. There are 2 k + 1 parts of T 2 part can be reached with one step from the initial state s 0 of T 2 independent on the action chosen by the agents. We will give t ransition probabilities to the transition probability. In the initial state s 0 and in the initial states of all parts, the observation  X  is made. When a state same , hori , vert is reached, each agent has made 2 k + 3 observations, where the first and last are  X  and the remaining 2 k are each in { 0 , 1 } . Such a state is the only one where the actions of the agents have impact on the process. Because of the partial observabi lity, they cannot know in which part of
T 2 , k they are. The agents can win, if they both know the same correc t tiling and interpret the sequence of observations as the position in the grid they are asked to put a tile on. On the other hand, if both agents know different tilings or the tiling the y share is not correct, then at least one trajectory will end in a bad state and has reward  X  ( 2 2 k + 2 ) . The structure of the POSG is given in Figure 4.
 Claim 2 Let ( T , 1 k ) be an instance of the exponential square tiling problem. both agents use the same policy according to this tiling. Und er these policies, state bad will not be reached. This guarantess performance &gt; 0 for both agents. For the other direction: if there exist policies for the agents under which T 2 both agents use the same policy. It can be shown inductively t hat this policy  X  X s X  a T -tiling of the 2 k square. The POSG for the competing agents problem with 4 agents consi sts of three parts. The first part is a copy of T 2 2). In this part, the negative rewards are increased in a way t hat guarantees the performance of the POSG to be negative whenever agents 1 and 2 do not correctly ti le their square. The second part is a modified copy of T 2 agents 3 and 4). Whenever state bad is left in this copy, reward 0 is obtained, and whenever state good is left, reward  X  1 is obtained. The third part checks whether agent 1 puts the s ame tiles into the last row of its square as agent 3 puts into the first row of it s square. (See L 3 example.) If this succeeds, the performance of the third par t equals 0, otherwise it has performance 1. These three parts run in parallel.
 If agents 1 and 2 have a tiling for the first square, the perform ance of the first part equals 1. Lemmas 3.3 and 3.4 together yield completeness of the compet ing agents problem.
 Theorem 3.5 For every k  X  2 , the competing agents problem for 2 k agents is complete for NEXP NP . We have shown that competition makes life X  X nd computation X  more complex. However, in order to do so, we needed teamwork. It is not yet clear what the compl exity is of determining the existence of a good strategy for Player I in a 2-person POSG, or a 1-again st-many POSG.
 There are other variations that can be shown to be complete fo r NEXP NP , a complexity class that, shockingly, has not been well explored. We look forward to fu rther results about the complexity of POSGs, and to additional NEXP NP -completeness results for familiar AI and ML problems.
 [1] Daniel S. Bernstein, Robert Givan, Neil Immerman, and Sh lomo Zilberstein. The complexity [2] E. Hansen, D. Bernstein, and S. Zilberstein. Dynamic pro gramming for partially observable [3] Hao Wang. Proving theorems by pattern recognition II. Bell Systems Technical Journal , 40:1 X  [4] M. Savelsbergh and P. van Emde Boas. Bounded tiling, an al ternative to satisfiability. In Gerd [5] C.H. Papadimitriou and J.N. Tsitsiklis. The complexity of Markov decision processes. Mathe-[6] Martin Mundhenk, Judy Goldsmith, Christopher Lusena, a nd Eric Allender. Complexity results
