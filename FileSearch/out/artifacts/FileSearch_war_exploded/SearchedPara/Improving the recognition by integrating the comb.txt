 ORIGINAL ARTICLE J.-P. Salmon  X  L. Wendling  X  S. Tabbone Abstract A new method for combining shape descrip-tors based on a behavior study from a learning set is proposed in this paper. Each descriptor is applied on several clusters of objects or symbols. For each cluster and for any descriptor a pertinent map is directly carried out from the learning database. Then existing conflicts are assessed and integrated in such a map. At last, we show that the use of combination of descriptors enables to improve the recognition using real data.
 Keywords Shape descriptors  X  Additive combination  X  Recognition of dropped initials 1 Introduction There have been relatively few studies of combining the similarity responses of shape descriptors. The relatively low amount of works in this area is probably due to the huge variety of objects encountered, depending on the type of image or documentation which has to be pro-cessed. Generally the  X  X etter X  1 descriptor is searched and applied. This, in turn requires efficient ways for rec-ognizing objects [7], or at least for identifying object signatures.

Generally photometric shape descriptors can be split intotwocategories [26]: contour-basedandregion-based approaches with well-known advantages and drawbacks. Due to the large variability of the objects the use of invariant descriptors is required for their identification and recognition, and hence to find efficient and useful invariants [24]. Many approaches have been proposed in the early years either based on Fourier descriptors [14, 18,20], on moments [3,5,16,23], on Hough transform [2, 13] or on angular description [6,22]. In this paper we focus on basic global methods having low processing time but allowing to keep nice geometrical properties as translation, rotation and scale factor. For a thorough survey of the various techniques and descriptors which could be used, the reader may refer to [12,17,26].
Another problem encountered in pattern recognition is the kind of distance calculated during the matching. In this scope, generally the metric is related to the appli-cation problem under consideration. Metrics such as Bhattacharya [4], Euclidean distance or EMD [19] are widely used from statistical studies and related to the studied application. Here we used Euclidean distance to compare scalar values and the classical Tanimoto index (min over max) to match vector descriptions of sym-bols. This yields interesting results in our application, with only low processing time requirements.

Once again the goal of our paper is not to point out one descriptor among the set of methods but to combine the power of recognition of each of them embeded in a pattern recognition application, that is, one descrip-tor may be more discriminant than another one follow-ing the application under consideration. The aim of our method is to assign to each descriptor a recognition map, that is, to define automatically a descriptor measure for each cluster of samples to be found. Such map could be different for another symbol using the same descriptor.
Basic methods used in our study and associated prop-erties are provided in Sect. 2. Simple classification measures are proposed in Sect. 3 to study the behavior of our method. The scheme used to defined measures from the behavior of shape descriptors is presented in Sect. 4. Finally experimental studies and a discussion about the advantages and limitations of our approach are given in Sects. 5 and 6. 2 Set of basic descriptors We focus here on basic operators having low processing time and easy to implement. We draw for each of them their behavior from standard geometric transforms like homotety, rotation or translation. 2.1 Compactness The compactness is the more common descriptor which can be found in the literature. Its approximation is maxi-mal for a large disk (near one) and minimal for a discrete straight line. An usual formulation is C = 4 with P the perimeter and M 11 the surface. Such feature is by definition invariant to basic geometric transforms as rotation, translation and scale factor. Its calculation requires a low processing time. Nevertheless, it has a high sensitivity to noise and gives rise to low recognition rates when the number of objects to recognize grows. Such descriptor is rather suitable to split databases in more or less elongated clusters or dedicated for specific Al applications having well-differentiated shapes. In this paper, objects are classified according to the distance of their compactness value. 2.2 Ellipticity degree Such descriptor is defined from the moment of order 2. Let us recall the general expression allowing to compute a centered moment of order n is given by ( j + k = n ) M where I is the intensity function, null for a pixel of the background and unitary for a pixel belonging to an ob-ject. The ellipticity corresponds to the ratio of the big axis and the little axis [23]. Its expression is given by: E =
Such descriptor is invariant to rotation, translation and scale factor. Moreover it is low sensitive to noise as it considers the global shape of the object. Normalized values of distance are used to compare the ellipticity degree of two objects. 2.3 Angular signature We used here a simplified version of the angular signa-ture proposed by Bernier and Landry [6]. A vector is computed from the centroid  X  X c ( x c , y c ) of the shape as follows: S with d the Euclidean distance. S A corresponds to the suc-cessive distances, with a regular angular step, between the centroid X c and the farer point X i of the contour normalized by the area of shape and n is the number of processed directions.

The method proposed in [6] was adapted to keep fast processing. The derivative of the signature is not studied to take into account the rotation by extracting the maxima. We consider here the maximal diameter of the object. The angle i , which maximizes the value S maximal one, following a preset threshold, can also be taken into account to avoid the problem of local maxima.
Obviously the more the angles are tested the higher the processing time is. Then a similarity ratio is directly computed from superimposed signatures recentered us-ing their maximal diameter. 2.4 Generic Fourier descriptors The polar discrete transform [25] is required to compute such signature. It is similar to Fourier transform but con-sidering the polar image in a polar space as a rectangular image in a Cartesian space. The mathematical expres-sion is: PF ( X  ,  X ) =
The radius r ( x , y ) and the angle v ( x , y ) are the polar coordinates of the point ( x , y ) from the centroid frame of the object. I is the intensity function. The parameters  X  and  X  are bounded: 0  X   X &lt; R and 0  X   X &lt; T with R and T respectively the radial and angular reso-lutions. The Fourier descriptor is parametrized by two frequencies: m the radial frequency and n for the angular frequency. It is defined by [25] GFD = The first element of the vector is normalized by the area and the others by the value | PF ( 0, 0 ) | . Then GFD becomes invariant to scaling factor. Moreover such descriptor is by definition invariant by translation and rotation and low sensitive to noise effect.

The distance between two shapes is directly given by the distance of the associated vectors. We can denote that such method is easy to implement and a full algo-rithm is provided in [25].

At last this approach gives rise to interesting results in many applications as recognition of logos. 2.5 R-signature The Radon transform of a function f , denoted T R f ,is defined as its line integral along a line inclined at an angle  X  and at a distance  X  from the frame [8]. Mathe-matically, it is written as: T where  X  X  X  &lt; X &lt;  X  ,0  X   X &lt; X  and  X  is the Dirac function.

Intuitively, that remains to integrate the function f along a line for any parameters ( X  ,  X ) .

A shape measure, called R -signature [22], can be de-fined from the Radon transform as follows: R
Such signature allows to have an idea of the angular distribution of a shape considering the global aspect of the object and none is centroid. It is by definition invari-ant to translation. The normalization by the area allows to take into account the scale factor. Circular shifts are performed to keep the rotation property: a rotation of an object remains to a translation in the signature modulo  X  . At last, the similarity measure used to compare two R-signatures is based on the classical Tanimoto index ( min over max ). 2.6 Fourier X  X ellin The analytical Fourier X  X ellin transform of a function f , has been introduced in [11] to avoid mathematical application problems of the standard version: M
As there exists no discrete transform, the used of approximations [9] is required to process numerical data. 2
Such approach is obviously more interesting using gray-level images, but it has the advantage to be invari-ant to rotation and scale factors, and to the translation from the centroid of the shape. Moreover the signature is unique and so reversible. 2.7 Moments of Zernike The use of complex polygons of Zernike { A f ( n , m ) } , orthogonal with the unity circle, are given by the fol-lowing expression [23,16]: A ( n , m ) = n with V and n  X  N , m  X  Z such that n  X  X  m | is even and | m | X  n : R
Such moments are rather robust to noise [15]. The module of Zernike moments is computed to warrant the invariance to the rotation. 3 Classification measures 3.1 Confusion matrix A confusion matrix allows to evaluate the power of rec-ognition of a descriptor following the current applica-tion. A head of cluster is usually assumed to be a good representation of the cluster. The samples are stored in the matrix by considering the cluster values of the distance reached from all the cluster heads. Such a pro-cess requires static objects. Another way is to add in the matrix the cluster numerous of the nearest sample (that is these having the minimal distance ratio after apply-ing a descriptor). Here, we consider each sample of the database as a query to have a more discriminate matrix. For each query, we sort the other samples by decreasing distance (or decreasing similarity ratio). Then we ana-lyze the clusters corresponding to each element of the series achieved. The ranking takes into account all the positioning of the samples of the cluster associated with the query. The confusion matrix is made by correspond-ing the numerous clusters found in the initial positions. 3.2 Ranking The ranking is a well-known measure of retrieval [10] adapted here to show the robustness of the method. Let us consider a cluster c , composed of S samples, its rank-ing value is defined by r = 1 where N = C  X  S is the number of objects contained in the database, o j c is the j th object o of the cluster c increasing series of scores (see previously) computed from the object query o j c .  X  is set to one when element i corresponds to the cluster c and 0 otherwise. Values between 0 and 1 can also be integrated if we have a pri-ori knowledge of valuated similarity between clusters. When the size of the database grows, the evolution of the ranking provides an interesting assessment about the stability of the shape descriptor. The more the value of the ranking is near 1, the more the samples are well ordered. Considering larger databases, if the identifica-tion of a cluster is similar the ranking increases. A similar ranking implies a weak degradation of the recognition. 4 Automatical definition of measures A descriptor is generally considered discriminant, under an application, when the associated recognition rates are high. A confusion matrix is used here to assess the power of recognition of descriptors. By definition if objects (or samples) are correctly recognized they should be assigned to the corresponding bin cluster. Bad recogni-tion remains to a disparate distribution of the objects on the lines of the confusion matrix. Let Table 1 be a confusion matrix achieved using the database provided in the experimental section. There exists 9 clusters hav-ing 11 samples. The descriptor used is the compactness. The aim is not to study the compactness but to under-line the processing step performed for any descriptor considering any confusion matrix.
 Let us consider, for example, the line 6th of the matrix. The values correspond to the distribution of the objects of the cluster number 6 achieved from the calculation of compactness criterion. We can denote that the samples are rather assigned to three clusters (3, 6 and 8). The rec-ognition rate is around 40% (number of correctly clas-sified objects divided by the number of tested objects that is 49/121). Let us now consider a low recognized cluster. For instance, in the 5th line, the distribution is relatively homogeneous between the different clusters and so less discriminate than the previous example. That is underlined by a graphical representation of the rates provided Fig. 1. The dissimilarities are obtained by com-puting the mean of the distances. The distance, for one cluster, is equal to the difference between the values of the applied of these objects and the one of the searched cluster.

We can conclude that it is important to take into ac-count not only the recognition rate but also the distri-bution of the errors in order to improve the recognition process. The aim is to integrate the behavior of each symbol of the database. Obviously functions may be different for two objects. Then it is important to define different approximations to improve the global recogni-tion. The higher the value of the weight the higher the confidence on the descriptor is. Nevertheless consider-ing one cluster a descriptor can be more discriminating than another one, that is, underlined by the value of the weight reached. The score is given by Score = where d is the number of descriptors (here 5). F dc is a weighted map of descriptor d for the cluster c defined from the distance distribution of cluster samples. oc and ox are, respectively, the vector of descriptor for an object of the cluster c and of the cluster searched.

The weight map is defined from a study of distance distribution in order to take into account the samples may be far away from the model. The aim is to increase the weight corresponding to such samples to bring them closer to the model. Moreover, the influences of the other clusters is set by integrating negative weights in the map.

All the points of F dc are calculated as follows. For each given distance the ratio is set by the number of sym-bols belonging to the cluster divided by all the symbols found at this distance. For instance, the map associated to cluster 5 using the compactness is provided in the Fig. 2. Obviously the maps are rather different because the influence of the descriptors differs and the clusters to be recognized are different.

We also propose to combine such maps in an addi-tive combination process to improve the recognition as follows: Let us consider a learning database.
Let D ={ d 1 , ... , d n } be the set of descriptors and ={  X  tion rates reached using such database. Let x beasam-ple. The normalized weighed sum W S calculated from x is given by: W assuming that i = 1, n  X  i = 1. 5 Experimental study 5.1 Database of shapes Experimental results within a shape recognition pro-cess are provided to show the efficiency of the proposed algorithm. First, a database of Sharvit [21] who made it kindly available to us on his Web site: http://www.lems. brown.edu/vision/researchAreas/SIID has been used. Such database consists of 9 categories with 11 shapes in each cluster. Samples are occluded or distorted. More-over shapes are scaled or rotated. The Fig. 3 presents the recognition rates achieved for all the descriptors. We can see that the proposed approach improves the recogni-tion rates for most of the descriptors considering the whole clusters.

Table 2 presents the mean recognition rates obtained with C the compactness, E the ellipticity degree, S A the angular signature, G FD the generic Fourier descriptor, T R f the signature defined from the Radon transform, FM the Fourier-Mellin descriptor and MZ the moments of Zernike. The results are provided before and after the running of the approach proposed in Sect. 4. The overlapping of the curves, considering the basic meth-ods, shows differences to discriminate the clusters, that is, approaches should be more suitable for the recogni-tion of objects but performances can fall down for other samples even using approaches having high main rec-ognition rates (Table 2). We can also remark that the use of weight maps during the process gives rise to bet-ter results in most of the cases. Moreover, the additive combination of measures W S induces a real improve-ment of the recognition. Results achieved are higher for the whole clusters which is generally the case in all the applications presented in this paper (except for one or two clusters but with close rates to the best descriptors found).

It is also underlined by the ranking values given in the Fig. 3. Our method gives rise to more discriminate results, that is, the samples of each cluster are close to the target queries. 5.2 Database of graphical dropped initials Another test database consists of graphical dropped ini-tials extracted from archival documents. 3 This study is done within a consortium of french research teams (see http://l3iexp.univ-lr.fr/madonne/).

Such letters require a first processing in order to be usable within the framework of our application. First a classical algorithm of binarization, based on an entropy criterion calculated on the gray-level histogram, is carried out to extract both object and background clus-ters. Obviously the useful information can be contained in any cluster of the binary partition. Then after apply-ing dilation and erosion steps to clean the region, a set of rules and measures (size, compactness...) is applied to choose the good related component, that is, to extract the target letter.
 An example of decomposition is provided in the Table 4. The first line shows a set of dropped initial coming from noisy ancient documents. Binary images with removed artifacts are given in the second line. Then five main connected components are presented. We can remark that the letters are generally included in compo-nents 3 to 5. The last line presents the extracted letters.
Thus, we obtained a database made up of graphical letters, which are declined into 11 noisy samples. The scale factor should be taken into account because the size of the dropped initial differs following the archival documents. Moreover few letters are rotated, so we have kept the similarity approach. Obviously, to decrease the processing time (and also to improve the recognition), we can consider only a little pencil of angles.
The Tables 5 and 6 provide the results obtained using this database. We can remark a consistent improvement using our method. As previously, using Sharvit X  X  data-base, the combination of descriptors gives rise to the bet-ter results. The ranking criterion confirms such behavior. 5.3 Tests on larger databases Then we have considered increasing sizes of databases by merging database of 9 cluster composed of 11 sam-ples more or less degraded until a dabatase of 45 clusters (that is the size of the first database multiplies by a factor 5). The results achieved show that our approach remains stable, in this example, when the database grows (see Tables 7 and 8). We can also note the good behavior of the ranking criterion even considering a large number of samples. 5.4 Learning using half a database To have a better idea of the discriminate power of our approach we have split each cluster of the database into two parts. On the one hand the samples used for the learning (five shapes) and on the other hand the sam-ples used for the classification (6 shapes). We draw Fig. 4 some results achieved using dropped initials (embedded in a database of size r  X  5) to show the accuracy of our approach (criterion W S used before and after ). Obvi-ously most of the artifacts are removed if we consider a weak pencil of directions or all the database. Nev-ertheless, such results show that our method is more discriminant than the previous one. The accuracy of Fdc is set to 10  X  4 (step of digitalization applied to distance X -axis) to process with learning data.

The Tables 9 and 10 present the new results with increasing sizes of database. We have compared our approach to the well-known method of Ann [1] imple-mented by David D. Mount and S. Arya (package avail-able at the url://www.cs.umd.edu/mount/ANN).

ANN supports data structures and algorithms for both exact and approximate nearest neighbor search-ing in arbitrarily high dimensions. The learning is done for each cluster and considering the distance matrix between symbols. The recognition rates are obviously lower that considering the whole database but they re-main high using our approach (around 75 % of recogni-tion and 97 % for the ranking).

Other graphs are provided to show the robustness of our approach. As in [6], the recongition reached follow-ing the rank of each queries is studied. All the results (see Fig. 5) are given after applying our method. Five descriptors are computed here. We can remark that the additive combinaison supersedes all the approaches.
Figure 6 presents additive recognition rates follow-ing the rank (the last column corresponds to the global average). We can show the stability of the proposed approach. The last Fig. 7 corresponds to the average size of first series carried out, that is, the mean of number of nearest samples corresponding to the target cluster achieved for any query. Each series is stopped when a sample belonging to another cluster is found. Such graph also underlines the robustness of our approach. 6 Conclusion In this paper, we have shown that the definition of mea-sures for a set of descriptors can be of great interest for recognizing objects. The computation of features is fast and the combination improves the recognition rate. These results are very promising. However, they still need further validation by processing much larger data-bases in order to assess the discriminating power and the robustness of the proposed method. In this case, it may be necessary to add some indexing scheme, both for efficiency and for discrimination. We are planning to test the construction of a binary search tree associating each node with the more discriminating feature found. More-over, it would be interesting to compare the weighting of features with the output of a Principal Components Analysis and also to consider the score pair of dissimi-lar classes. Investigations using the behavior of Choquet integral are currently perform to integrate both positive and negative interactions in the combination process in order to improve the recognition.
 References
