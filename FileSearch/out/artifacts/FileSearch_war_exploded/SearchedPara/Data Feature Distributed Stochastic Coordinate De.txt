 How can we scale-up logistic regression, or L 1 regularized loss minimization in general, for Terabyte-scale data which do not fit in the memory? How to design the distributed algorithm e ffi Although there exist two major algorithms for logistic regression, namely Stochastic Gradient Descent (SGD) and Stochastic Coor-dinate Descent (SCD), they face limitations in distributed environ-ments. Distributed SGD enables data parallelism (i.e., di chines access di ff erent part of the input data), but it does not allow feature parallelism (i.e., di ff erent machines compute di sets of the output), and thus the communication cost is high. On the other hand, Distributed SCD allows feature parallelism, but it does not allow data parallelism and thus is not suitable to work in distributed environments.
 In this paper we propose DF-DSCD (Data / Feature Distributed Stochastic Coordinate Descent), an e ffi cient distributed algorithm for logistic regression, or L 1 regularized loss minimization in gen-eral. DF-DSCD allows both data and feature parallelism. The ben-efits of DF-DSCD are (a) full utilization of the capabilities provided by modern distributing computing platforms like M ap R educe alyze web-scale data, and (b) independence of each machine in up-dating parameters with little communication cost. We prove the convergence of DF-DSCD both theoretically, and also show empir-ical evidence that it is scalable, handles very high-dimensional data with up to 29 millions of features, and converges 2 . 2  X  competitors.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithms, Design, Experimentation Logistic Regression; L 1 regularized loss minimization; Distributed Computing; Coordinate Descent; MapReduce; Hadoop
How can we scale-up logistic regression, or L 1 regularized loss minimization in general, for Terabyte-scale data which do not fit in the memory? How to design the distributed algorithm e ffi Logistic regression, or L 1 regularized loss minimization in gen-eral, is a crucial task with many applications including biologi-cal data mining [11], threat classification [7], text processing [9], matrix factorization [6, 25], anomaly detection [19], etc. The ma-jor algorithms for learning the parameter for the logistic regression problem are descent based algorithms, including Stochastic Gradi-ent Descent (SGD) and Stochastic Coordinate Descent (SCD). Due to the growing size of the data, there have been expanding inter-ests in developing parallel or distributed version of the SGD and SCD [5, 28]. Ideally, the distributed algorithm for the logistic re-gression should have two desired parallelism properties in the dis-tributed computing environment. The first property is data or input parallelism , which we define as follows.

D efinition 1(D ata P arallelism ). Data parallelism is the prop-erty of distributed algorithm that di ff erent machines access di ent parts of the input data, and the processing of the input data in a machine is not a ff ected by those of other machines.
Algorithms satisfying data parallelism are run in distributed sys-tems e ffi ciently, since the data can be divided into machines, and ceives. The second property is feature or output parallelism which we define as follows.

D efinition 2(F eature P arallelism ). Feature parallelism is the property of distributed algorithm that di ff erent machines compute di ff erent subsets of the output features, and the computation of the output in a machine is not a ff ected by those of other machines.
Algorithms satisfying feature parallelism can handle high dimen-sional features e ffi ciently since the task of updating the features is distributed over machines. Also, the communication cost for ex-changing the features becomes much smaller since there is no need to aggregate the features from machines to finalize the features. Previous works on parallel or distributed version of SGD and SCD satisfy only one of the two desired parallelism properties. Zinkevich et al. [28] developed Parallel SGD (PSGD). The PSGD algorithm provides data parallelism, but it does not provide fea-ture parallelism. Therefore, it is impossible for each machine to select a subset of the output features and update it independently. In the SCD side, Bradley et al. [5] developed ShotGun, a Parallel while D-SGD fails to run on it. (c) DF-DSCD converges 2.2 Table 1: Comparison of methods. Our proposed DF-DSCD is the only one that 1) handles very large data, 2) process high-dimensional data, and 3) provides fast convergence.
 Scalable to Large Data Stochastic Coordinate Descent (PSCD) algorithm in a shared mem-ory setting. ShotGun provides feature parallelism since each core works independently on subsets of the features. However, it does not provide data parallelism; i.e., each core should read all the data.
In this paper, we tackle the following problem: how can we de-sign a distributed logistic regression, or L 1 regularized loss min-imization, algorithm that provides both data and feature paral-lelism? Our solution is DF-DSCD, a distributed algorithm that achieves both data and feature parallelism, overcoming the limi-tations PSGD and PSCD. DF-DSCD provides data parallelism by letting each machine work on di ff erent parts of the data indepen-dently; furthermore, DF-DSCD provides feature parallelism by let-ting each machine work on di ff erent features. We give theoret-ical analysis about the convergence property of DF-DSCD. We also show extensive experimental results showing that DF-DSCD is scalable, handles very high dimensional data, and converges faster (up to 2 . 2  X  ) than its competitors. Table 1 shows the advantage of DF-DSCD compared to other competitors. The main contributions are the followings.

The rest of this paper is organized as follows. Section 2 presents the preliminaries of logistic regression. Section 3 describes our proposed DF-DSCD algorithm. Sections 4 analyzes the conver-gence properties and the time complexity of DF-DSCD. Section 5 presents the experimental results. After reviewing related works in Section 6, we conclude in Section 7. Table 2 lists the symbols used in this paper.
In this section, we describe the preliminaries of logistic regres-sion and its algorithms.
Logistic regression is a special case of the general problem called the L 1 regularized loss minimization. Let S = { ( x i  X  R be a set of n training examples where { x i  X  R d } are d -dimensional inputs, and { y i  X  R } are target labels. Let X be an n by d design matrix whose i th row is x i . In the logistic regression, the target variable y takes discrete values; for the sake of simplicity we focus on binary classification where y takes either  X  1 or 1; multi-class generalization is straightforward, too. The probability that y takes the value 1 is given by p ( y = 1 | x ;  X  ) = 1 the weight parameters. p ( y =  X  1 | x ;  X  ) is defined naturally: p ( y  X  1 | x ;  X  ) = 1  X  p ( y = 1 | x ;  X  ). The logistic regression problem is to find  X  that minimizes the following loss function based on the negative log likelihood: where  X  is a regularization term to penalize large weight parame-ters.
We review algorithms for learning the parameter  X  in the logis-tic regression. The goal is to minimize the loss function F ( There are two lines of learning algorithms for the logistic regres-sion: Newton-Rhapson method and descent based methods. The Newton-Rhapson X  X  method is not used practically since the matrix inversion it involves is very expensive, and many of the design ma-trices are not invertible. For the reason, we focus on the descent based methods which can handle large data.
The Gradient Descent (GD) algorithm starts with an initial guess  X  (0) of the parameter  X  , and iteratively updates  X  by moving it to-ward the direction of negative gradient. Since the loss function F (  X  ) of the logistic regression is convex [12], the GD algorithm al-ways converges to the global minimum. It can be shown that the duplicate features as explained in Section 3.3, is given by by iteratively updating  X  using the equation  X   X   X   X   X   X  F (  X  is the positive learning rate. Notice that the parameter  X  incrementally over instances.

Stochastic Gradient Descent (SGD) randomizes iterating through the instances. SGD gets a random approximation of the partial derivatives in much less than O nd time where n is the number of instances and d is the number of features, and thus the param-eters are updated much more rapidly than in GD. Moreover, if j th feature of a training example x is 0, then updating  X  j based on x can be skipped. This means that the time for each iteration is O nsd where s is the density (average proportion of nonzero feature val-ues per example). Therefore, for sparse data, SGD is very e in practice.
Coordinate Descent (CD) also optimizes for the set of parame-ters. However, instead of optimizing all parameters at a time, CD updates a single parameter (or a coordinate)  X  j at a time using all the data. That is,  X  j  X   X  j  X   X  (  X  F (  X  )) j .

Often, CD converges too slowly to be useful. However, CD can be useful in problems where computing solutions over all of the features is di ffi cult, but computing solutions over a subset of fea-tures is relatively easy. Therefore, between the non-parallel version of these two variants (SGD and CD), SGD is the faster algorithm. Stochastic Coordinate Descent (SCD) is a variant of CD where the coordinate to be updated is randomly chosen at each iteration.
Mann et al. [15] proposed Parallel Stochastic Gradient Descent (PSGD) by partitioning the data of size n into n R pieces where R is the number of parallel processors. Further, Zinkevich et al. [28] showed detailed analysis and experimental evidence. In the work, each processor independently learns the parameters of the opti-mization problem using SGD for multiple iterations. Finally after the last iteration by all processors, the parameters from all proces-sors are collected and averaged to give a final single set of parame-ters. This method achieves the scalability by dividing the large data into smaller manageable size for each processor. The advantage of only combining the parameters at the end of all iterations re-duces the communication overhead of exchanging parameters. The M ap R educe version of Parallel Stochastic Gradient Descent is pro-posed in Section 3.
Bradley et al. [5] proposed ShotGun, a Parallel Stochastic Co-ordinate Descent (PSCD) algorithm which updates parameters in parallel. Assume that C processors exist in a multi-processor ma-chine. At each iteration, Shotgun randomly selects C coordinates to update, and assigns each coordinate to a distinct processor. Each processor then solves for the assigned parameter using coordinate descent. Each time a parameter changes, it is written to a data struc-ture that is shared by all other processors. Given that reads will not result in any data inconsistency and each processor only writes to parameters that they are responsible for, the data remain consistent. Algorithm 1 shows the Shotgun algorithm [5] which uses the dupli-cate feature notation described in Section 3.3. In line 6, the is a constant, and the (  X  F (  X  )) j is as defined in Eq. (2). Algorithm 1: Shotgun: Parallel SCD
Input : Set S = { ( x i  X  R d , y i  X  X  0 , 1 } ) } n i = 1
Output :  X  .  X   X  0; while not converged do 3 Choose random subset of C weights in { 1 ,..., 2 d } ; 4 In parallel on C processors 5 // In each processor, update the assigned coordinate 6 Get assigned weight j ; 7  X  j  X   X  j + max { X   X  j ,  X  (  X  F (  X  )) j / X  } ; 8 end end
Shotgun achieves scalability by dividing the high dimensional feature space into blocks of features. The M ap R educe version of Parallel Stochastic Coordinate Descent is proposed in Section 3.
Comparing PSGD and PSCD, PSGD scales with the number of instances while PSCD scales with the number of features. To take the advantages of both approaches, we propose DF-DSCD algo-rithm in Section 3.
In this section, we describe DF-DSCD, our proposed algorithm for fast and scalable logistic regression in distributed environment. We first propose two preliminary algorithms: Distributed Stochas-tic Gradient Descent (D-SGD) and Distributed Shotgun (D-Shotgun). D-SGD is a M ap R educe version of Parallel Stochastic Gradient De-scent [28] in distributed environment. D-Shotgun is a M ap version of Parallel Coordinate Descent [5] in distributed environ-ment. Unfortunately, each of them has a drawback: D-SGD does not provide feature parallelism, and D-Shotgun does not provide data parallelism. To overcome the problem, in Section 3.3 we propose DF-DSCD which couples the best of both D-SGD and D-Shotgun to provide both data and feature parallelism.
Distributed Stochastic Gradient Descent (D-SGD) is a data par-allel version of the Stochastic Gradient Descent algorithm. The M ap R educe algorithm of D-SGD is shown in Algorithm 2.
 Algorithm 2: M ap R educe algorithm for D-SGD
Input : Set S = { ( x i  X  R d , y i  X  X  0 , 1 } ) } n i = 1
Output : Weight  X  ( t + 1)  X  R d at time t + 1.
D-SGD-Map1(Key k, Value v) begin 3 ( x i , y i )  X  ( k , v ); 4 p  X  rand() % number_reducer ; 5 Output( p ,( x i , y i )) ; end
D-SGD-Reduce1(Key k, Value v[1..r]) begin 10 foreach ( x i , y i )  X  v [1 .. r ] do end
Given the set S = { ( x i  X  R d , y i  X  X  0 , 1 } ) } n i = amples, each machine receives an equal number | S | M of instances in D-SGD where M is the number of machines. This is done by as-signing data in mappers. Then, the i th reducer gets S i which is the i th piece of S after dividing it into M equal pieces. The parame-ter update is performed in lines 9-12, which is exactly the standard Stochastic Gradient Descent (SGD). The mapper D-SGD-Map1() and the reducer D-SGD-Reduce1() are run for total T iterations.
After one iteration of D-SGD-Map1() and D-SGD-Reduce1() procedure, the k th reducer outputs weight k  X  ( t + 1) where k The final weight  X  ( t + 1) is calculated by averaging all ducers, because each weight is updated from an independent set of training instances (data parallelism). That is,  X  ( t + 1)  X 
The weight parameters in D-SGD are passed to the reducers by the distributed cache functionality of M ap R educe . D-SGD is illus-trated in Figure 2(a), where each black box denotes a piece of data assigned to each machine, and the red rectangles are the features updated from each machine.

D-SGD achieves data parallelism by dividing the data into pieces and assigning each piece into each machine. Each machine can independently work on its own data. However, D-SGD does not achieve feature parallelism: i.e., each machine is not allowed to select a subset of features to update. Distributed Shotgun (D-Shotgun) is a distributed version of the Shotgun [5] (Parallel SCD) algorithm. In D-Shotgun, each machine receives all the data instances, and then updates the parameters as-signed to it independently from each other. Of course, this incurs Figure 2: Illustration of D-SGD, D-Shotgun and DF-DSCD. Each black box denotes a piece of data assigned to each machine, and the red rectangles are the features updated from each machine. While D-SGD enables only data parallelism by partitioning data instances into machines, and D-Shotgun enables only feature parallelism by computing a subset of the output ( d ) from each machine, DF-DSCD enables both data and feature parallelism by partitioning data in-stances to machines and computing a subset of the output from each machine. heavy network tra ffi c since the data instances should be broadcast to all the machines. We will see how to overcome this problem in Section 3.3.

The M ap R educe algorithm of D-Shotgun is as follows. The main program selects C coordinates to update, and write each of the co-ordinates in a separate command file; thus, total C command files are created. Each of the file is read by a mapper, and the mapper reads all the data instance in HDFS to perform coordinate descent for the assigned coordinates. Since mappers perform all the com-putation needed for the coordinate descent, no reducer is needed. D-Shotgun is illustrated in Figure 2(b), where the large black box is all the data, and the red rectangles are outputs from each ma-chine.

D-Shotgun achieves the feature parallelism: i.e., each machine randomly selects a subset of features, and updates them. As demon-strated in Bradley et al. [5] theoretically and empirically, D-Shotgun has strong convergence bound and linear speedup. The upper bound of number of parallel updates is d 2  X  where d is size of feature dimen-sion and  X  is the spectral radius of X T X [5]. Thus, by choosing the optimal number of coordinate to update w.r.t. the given data, D-Shotgun can be e ff ectively optimized. However, D-Shotgun has a significant problem to be used in a distributed environment: it does not provide data parallelism, and thus each machine is forced to access all the data which can be a significant bottleneck, as we will see experimentally in Section 5.
Even though D-SGD and D-Shotgun achieve respectively data or feature parallelism, none of them achieves both. Then, a natural question arises: can we design an algorithm that has both data and feature parallelism? In this section, we propose Data / Feature Dis-tributed Stochastic Coordinate Descent (DF-DSCD), a distributed algorithm for logistic regression which achieves the goal. DF-DSCD takes the best properties of D-SGD and D-Shotgun. As in D-SGD, DF-DSCD let each machine receive the equal number | of instances where S is the set of training data and M is the number of machines. In other words, the i th machine gets S i which is the i th piece of S after dividing it into M equal pieces. Also, as in D-Shotgun, DF-DSCD let total PM coordinates be equally assigned to M machines, so that each machine gets P coordinates.
Before describing DF-DSCD algorithm in detail, we transform the original problem into an equivalent problem to ease the descrip-
Algorithm 3: DF-DSCD: Data / feature distributed stochastic coordinate descent.

Input : Set S = { (  X  x i  X  R 2 d , y i  X  X  0 , 1 } ) } n i
Output :  X   X  .  X   X   X  0; while not converged do 3 Choose PM unique coordinates randomly, and divide it 4 Split S into equal sized sets S 1 , ..., S M ; 5 For each machine M i , assign P i and S i ; 6 In M distributed machines 7 // In machine M k , update coordinates in P k using S k 8 foreach j  X  P k do 9 Set  X  to satisfy Eq. (6) in Section 4; 10  X   X  j  X   X   X  j +  X   X  max { X   X   X  j ,  X  (  X  F k (  X   X  )) j end  X  x = [ x  X  X whose i th row is  X  x i . Then the objective function becomes
If  X   X   X  R 2 d + minimizes Eq. (3), then  X   X  R d , where  X  minimizes Eq. (1) [23]. Thus, from this point we aim to find which minimizes the loss function in Eq. (3).

Let  X  X k and y k be the set of  X  x and y in S k , respectively. With-out loss of generality, we can express  X  X after some permutation as follows:
We next define the partial loss function F k (  X   X  ) computed from the data in S k : where we use the notation i  X   X  X k to specify the row indices of belonging to  X  X k . Then, F (  X   X  )isgivenby
The DF-DSCD algorithm is shown in Algorithm 3. The coordi-nates and data are assigned to machines in lines 3-5. Then, each machine performs parallel SCD on the assigned data where the up-date equation (line 10) is slightly changed from the standard update equation (line 7 of Algorithm 1). As we will see in Section 4, the objective function F () decreases over iterations in DF-DSCD. The M ap R educe algorithm of DF-DSCD is shown in Algorithm 4. The data assignment is performed in the mapper. The reducer up-dates the assigned coordinates using the same equation expressed Algorithm 4: M ap R educe algorithm for DF-DSCD
Input : Set S = { (  X  x i  X  R 2 d , y i  X  X  0 , 1 } ) } n
Output : Weight  X   X  ( t + 1)  X  R 2 d at time t + 1.
DF-DSCD -Map1(Key k, Value v) ; begin 3 (  X  x i , y i )  X  ( k , v ); 4 p  X  rand() % number_reducer ; 5 Output( p ,(  X  x i , y i )) ; end
DF-DSCD -Reduce1(Key k, Value v[1..r]) ; begin 9 foreach j  X  P k do 12 foreach j  X  P k do 13 Set  X  to satisfy Eq. (6) in Section 4; 16 foreach j  X  P k do end in line 10 of Algorithm 3. Since each reducer updates di ff ordinates, there is no need to sum up the updated parameters from di ff erent reducers. DF-DSCD is illustrated in Figure 2(c), where each black shaded box is a piece of data assigned to each machine, and the red rectangles are outputs from each machine.

DF-DSCD achieves the data parallelism by dividing the data into pieces and assigning each piece into each machine. Each machine can independently work on its own data. Moreover, DF-DSCD achieves the feature parallelism since each machine is allowed to select a subset of features to update.
In this section, we give a theoretical convergence analysis and time complexity of DF-DSCD.
We prove that DF-DSCD decreases the loss function F () of the logistic regression problem at each iteration. Since DF-DSCD ran-domly chooses coordinates, it is necessary to bound the expecta-tion of the loss function where the expectation is over the random choices of the coordinates. Our main result is Theorem 1 which states that the expectation of the loss function decreases at each it-eration when DF-DSCD is run with a proper small step size. We first present several lemmas, and use them to prove Theorem 1. Without loss of generality, we assume that diag (  X  X T  X  lowing [5]. The Hessian of F (  X   X  )isgivenby where  X  p i = 1 / (1 + ext (  X  y i  X  x T i  X   X  )). Let  X   X   X  iteration, and  X  k  X   X  k . We first show the upper bound of F (  X   X  + X   X   X  )  X  F (
L emma 1. For any  X  X ,F (  X   X  + X   X   X  )  X  F (  X   X  )  X  (  X   X   X  , where  X  = 1 4 is a constant.
 P roof . See the supplementary material [1].

Next, we give the relation between  X  F (  X   X  ) and  X  F k ( L emma 2.  X  F (  X   X  ) = M k = 1  X  F k (  X   X  ) .
 P roof . See the supplementary material [1].

Next, we give a loose bound of the di ff erence between E [ F (  X   X   X  )] and E [ F (  X   X  )].
 L emma 3. For M  X  2 ,E [ F (  X   X  + X   X   X  )  X  F (  X   X  )] is bounded by E [ F (  X   X  + X   X   X  )  X  F (  X   X  )]  X  PE j [ P roof . See the supplementary material [1].

Let P (  X   X  ) be a diagonal matrix with P (  X   X  ) i , i = corresponding to  X  X k . The gradients of F (  X   X  ) and F by P (  X   X  ) and P k (  X   X  ) as follows: bound: P roof . See the supplementary material [1].

Now we provide the main theorem about the convergence prop-erties of DF-DSCD.

T heorem 1. In DF-DSCD , for any iteration, feature j, and there exists a step size  X  such that the expectation of the loss func-tion of DF-DSCD decreases: i.e., P roof . See the supplementary material [1].

Note that the step size  X  can be any value satisfying the following condition (see [1] for details).
Table 3 shows the time complexity of DF-DSCD. Per iteration, each mapper requires O ( n / M ) number of operations to split the original n data instances into M machines. The number of oper-ations each reducer requires to update the assigned coordinates is O ( nsd / M ) where s is the density and d is the dimension of the data. Thus, DF-DSCD for T iterations has the time complexity O ( T ( n / M + nsd / M ) ) . Note that DF-DSCD runs linearly on the number of data instances and the dimension of the data. D-SGD has the same time complexity as DF-DSCD. However, the actual number of operations is smaller in the case of DF-DSCD because each machine only updates P parameters instead of d parameters in DF-DSCD, where P &lt;&lt; d . Also, as we will show empirically in Section 5.3, DF-DSCD requires a smaller number of iterations to converge compared to D-SGD. Compared to D-Shotgun whose time complexity is O ( Tnsd ) , DF-DSCD runs M times faster. Table 3: Time complexity of DF-DSCD and other methods. DF-DSCD X  X  time complexity is lower than that of D-Shotgun by M times. Although DF-DSCD X  X  time complexity is the same as that of D-SGD, the actual number of operations is smaller in the case of DF-DSCD because each machine only updates P parameters in-stead of d parameters in DF-DSCD, where P &lt;&lt; d . Also, DF-DSCD converges more quickly than D-SGD (see Section 5.3).
O ( T ( n / M + nsd / M ) ) O ( T ( n / M + nsd / M ) ) O
We perform experiments to answer the following questions:
We present extensive experimental results for the questions. Af-ter describing the experimental setting in Section 5.1, we answer the first question Q1 in Section 5.2: with synthetic data of vari-ous sizes, we compare scalability of DF-DSCD with those of D-SGD and D-Shotgun. The second question Q2 is answered in Sec-tion 5.3: with real-world datasets, we compare likelihood conver-gence and accuracy improvement of DF-DSCD with those of D-SGD and D-Shotgun. Finally, the third question Q3 is answered in Section 5.4: we evaluate the performance when the number P of coordinates to update in DF-DSCD changes. We describe the settings: cluster, data, and algorithms.
We run experiments in a 60-node Hadoop cluster. Each node in the cluster has Intel Xeon E5620 2.4GHz CPU and 24GB memory. We use both real-world and synthetic datasets listed in Table 4.
Scalability Experiments. To validate scalability in Section 5.2, we test our algorithms on synthetically generated binary datasets. We generate the synthetic data as similar as real world data such as Netflix [3] with di ff erent sizes of scales. The Netflix data provides Table 4: The properties of real-world and synthetic dataset: num-ber of instances ( n ), number of features ( d ), density ( s ), number of nonzero items ( nz ), and size in disk ( size ).
 KDDa 8 , 407 , 752 20 , 216 , 830 0 . 0002 2 , 607 KDDb 19 , 264 , 097 29 , 890 , 095 0 . 0001 5 , 011 News20 17 , 996 1 , 355 , 191 0 . 03 137 RCV1 20 , 242 47 , 236 0 . 536 S1-8 480 k  X  2  X  4 18 k  X  2  X  4 100 m  X  2  X  8 44 S1-4 480 k  X  2  X  2 18 k  X  2  X  2 100 m  X  2  X  4 752 S1-0 480 k 18 k 100 m 12 , 559 S1 + 4 480 k  X  2 + 2 18 k  X  2 + 2 100 m  X  2 + 4 207 , 174 S2-8 480 k  X  2  X  8 18 k 100 m  X  2  X  8 48 S2-4 480 k  X  2  X  4 18 k 100 m  X  2  X  4 784 S2-0 480 k 18 k 100 m 12 , 559 S2 + 4 480 k  X  2 + 4 18 k 100 m  X  2 + 4 200 , 951 S3-8 480 k 18 k  X  2  X  8 100 m  X  2  X  8 52 S3-4 480 k 18 k  X  2  X  4 100 m  X  2  X  4 764 S3-0 480 k 18 k 100 m 12 , 559
S3 + 4 480 k 18 k  X  2 + 4 100 m  X  2 + 4 222 , 766 that 480 , 189 users gave to 17 , 770 movies. Similarly, we generate a synthetic data, which consists of 480 , 000 instances and 18 features with 1 percent of non-zero items where a value of each item is restricted between 0 and 1. Then, we change the number of instances and features both equally ( S1 ), only instances ( S2 ), and only features ( S3 ). Each data with di ff erent orders of magnitude has its own name for convenience. For example, when changing both instances and features in S1 , we scale down / up the size of instances and features by 2  X  4 ( S1-8 ), 2  X  2 ( S1-4 ), 2 0 ( S1-0 ), and 2 respectively.

Convergence and Accuracy Experiments. To validate the like-lihood convergence and accuracy in Sections 5.3 and 5.4, we evalu-ate our algorithms on real-world datasets: KDDa, KDDb, News20, and RCV1. 1. KDDa / KDDb : KDD Cup 2010 dataset obtained from Carnegie 2. News20 : size-balanced two-class variant of the UCI 20 News-3. Reuters Corpus Volume I (RCV1) : an archive of over 800
For each real-world dataset, we used the training and testing sets given by the data provider. Each algorithm iterates T times until the di ff erence between successive parameter values is less than a given threshold (e.g., 0.01). For step size, we use a fixed step size D-SGD and DF-DSCD. The C parameter for D-Shotgun, which denotes the number of coordinates updated per iteration, is set to the optimal value d / (2  X  ) as suggested in the paper [5].
We compare DF-DSCD with the following competitors. 1. Distributed Stochastic Gradient Descent (D-SGD) that paral-2. Distributed Shotgun (D-Shotgun) that parallelizes over fea-
When choosing the parameter P of DF-DSCD in Section 5.3, we use P = d / M since it provides the best performance as we will describe in Section 5.4. We vary the parameter P in Section 5.4.
We empirically show that DF-DSCD scales well with the size of data, dimension of the data, and the number of machines.
To evaluate data scalability, we increase both the size of the num-ber of instances and the features (Figure 3 (a)), only the number of instances (Figure 3 (b)), and only the number of the features (Fig-ure 3 (c)) of synthetic datasets, and measure the running time of the large datasets due to the overhead of broadcasting the dataset to all the machines. DF-DSCD and D-SGD show similar scalabil-ity for relatively low-dimensional datasets (S1 and S2) as shown in Figure 3(a,b). However, DF-DSCD outperforms D-SGD by 1.45 for relatively high-dimensional dataset S3 as shown in Figure 3(c). Below, We will discuss this in detail.

DF-DSCD processes very high dimensional data e ff ectively thanks to its feature parallelism which D-SGD lacks. In the previous para-graph we verified the claim on a synthetic dataset with relatively high dimension (Figure 3(c)). The performance di ff erence becomes more evident for even higher dimensional data: see Figure 5(a,b,c) whose dimensions are 20 million, 29 million, and 1.3 million, resp. Note that only DF-DSCD successfully analyzes the data while oth-ers fail.

Finally, we show that DF-DSCD scales almost linearly with the number of machines. We measure the speed-up by increasing the number of machines used on our Hadoop platform. To test on very large data, we run DF-DSCD a very large synthetic dataset S1 (207 GB) that consists of 1 . 6 billions of non-zero items with 1 millions of instances and 72 thousands of features (see Table 4). Figure 4 shows the speed-up (throughput 1 / T M , where T running time with M machines) of DF-DSCD by increasing the number of machines from 15, 30, 45 to 60. Note that DF-DSCD speeds up near linearly in the beginning, while the performance flattens as the number of machines increases, due to the overhead in distributed systems (e.g. JVM loading time, synchronization time, etc.).
We show that DF-DSCD converges faster than competitors. We data. Figure 4: Runtime of DF-DSCD with di ff erent number of ma-chines on a very large synthetic dataset S1 + 4 . Note that DF-DSCD speeds-up near linearly in the beginning, while the performance flattens as the number of machines increases, due to the overhead in distributed systems (e.g. JVM loading time, synchronization time, etc.). also find the tendency that its performance becomes better as the number of coordinates updated by each machine becomes larger.
To evaluate how quickly DF-DSCD converges, we plot accuracy vs. time of all methods, where the accuracy is evaluated on test-ing set of each dataset. Figure 5 shows the experimental results of accuracy for the real world datasets: (a) KDDa, (b) KDDb, (c) News20, and (d) RCV1. For D-Shotgun, we choose C = d / (2 following the paper [5]; for DF-DSCD we choose the maximum possible number P = d / M based on the results from Section 5.4. Note that for all the results D-Shotgun fails to continue progress: the M ap R educe job for D-Shotgun failed while working due to net-work overhead. While the original Shotgun [5] in multicore set-ting showed successful performance in accuracy, our result shows that it is intractable for Shotgun to work e ffi ciently in M environment because all instances should be broadcast to all the machines.
 Compared to D-SGD, DF-DSCD shows faster convergence than D-SGD to achieve the same accuracy. In KDDa, KDDb, and News20 (Figure 5 (a-c)) data, D-SGD fails: the reason is that KDDa, KDDb, and News20 have very large number of features (millions), and D-SGD needs to update all the coordinates. On the contrary, our DF-DSCD performs well in the high dimensional data since DF-DSCD can select a subset of the coordinates to update. Although D-SGD does not fail in RCV1 (Figure 5 (d)) data, DF-DSCD is 2 . than D-SGD to get the same accuracy. In conclusion, the data par-allelism allows DF-DSCD to work on large data instances which could not be handled by D-Shotgun, and the feature parallelism en-ables DF-DSCD to work on high dimensional data which could not be handled by D-SGD.
DF-DSCD requires a parameter P which is the number of coordi-nates to update per machine. The question is, which P provides the best performance? To answer the question, we vary P from d d / (4 M ), d / (2 M ) to the maximum possible value d / M , and compare the running time, likelihood, and accuracy of DF-DSCD on RCV1 data. Figures 6 and 7 show the result with di ff erent plotting scheme. Figure 6 shows the rate of (a) running time, (b) likelihood, and (c) (a), note that the running time is almost the same for di Figure 7: (a) Likelihood and (b) Accuracy vs. Running time of DF-DSCD with di ff erent number P of coordinate on RCV1 data. In concordance with our intuition, larger number of coordinates leads to better performance in DF-DSCD: P = d / M provides the best negative log likelihood and the best accuracy for a given running time. accuracy over di ff erent number P of coordinates after 1 st ,5 th , and gence and (b) accuracy for di ff erent number of coordinates in DF-DSCD by plotting the likelihood or accuracy in y-axis against time (seconds) in x-axis.

In Figure 6 (a), notice that the running time is almost the same for di ff erent number P of coordinate to update per machine. This happens because the M ap R educe algorithm for DF-DSCD heavily depends on disk accesses, and larger P increases mainly the CPU time which is small compared to the disk access time.

Figure 6 (b) shows that the negative likelihood decreases more quickly over iterations as the number P of coordinates to update increases. This means that if each machine uses the maximum number P = d / M of coordinates to update, the data likelihood converges the most quickly. Figure 6 (c) shows the accuracy for di ff erent number P of coordinates to update per machine. As in the negative likelihood, the P = d / M provides the best accuracy. Fig-ure 7 (a) and (b) also show the similar result: data likelihood and accuracy is the best when choosing P as the maximum possible value d / M .
In this section, we review related works on parallel / distributed machine learning focusing on logistic regression.

Logistic Regression. Logistic regression is a widely-used method to solve classification problems in data mining. It has been applied to many applications such as life science [11], threat classification and temporal link analysis [7], anomaly detection [19], collabora-tive filtering [24] and text processing [17].

In the logistic regression problem, there have been several ap-proaches of maximizing the likelihood of the entire dataset [9, 16]. However, the stochastic gradient descent and coordinate descent are the most famous approaches.

Parallelized / Distributed algorithms. Recently, parallel stochas-tic gradient descent algorithms for multicore [18] and distributed setting [15, 28] are studied. Especially, Zinkevich et al. [28] re-duced I / O overhead of the algorithm by restricting training data to be accessed only locally and communicating at the very end. Even though data parallel stochastic gradient descent scales to large num-ber of instances, it does not consider large number of features.
Bradley et al. [5] proposed Shotgun, a parallel coordinate de-scent algorithm for minimizing L 1 -regularized losses. In the ideal case where all features are uncorrelated, Shotgun can do parallel updates up to the number of features. The algorithm is empirically proved to be one of the most scalable algorithms for L 1 minimiza-tion problem. Moreover, two preprocessing schemes [21,22] to im-prove Shotgun, which also can be easily applied to our DF-DSCD, are proposed. Although Shotgun provides feature parallelism, it does not provide data parallelism and its distributed algorithm.
Recently, Richt X rik et al. [20] developed a distributed coordinate descent method that is similar with D-Shotgun algorithm in Sec-tion 3.2. Even though the authors partition big data into multiple machines, the paper does not include theoretical proof for conver-gence and empirical evidence. Alekh Agarwal et al. [2] proposed a tera-scale linear system which achieves fast convergence speed by combining an online learning algorithm with a batch one. Al-though their method is compatible with Hadoop, it does not follow M ap R educe model and requires an additional communication in-frastructure for e ff ective communication between mappers.
Application beyond logistic regression. Besides logistic re-gression problem, other applications also can be solved using Stochas-tic Gradient Descent or Coordinate Descent. Gemulla et al. [6] pro-posed a matrix factorization algorithm using distributed stochas-tic gradient descent. Cho-Jui et al. [8] also used fast coordinate descent methods for variable selection in nonnegative matrix fac-torization. Recently, Hsiang-Fu et al. [25], proposed a coordinate descent based matrix factorization technique for recommendation system. Our DF-DSCD can be directly applied to solve other L regularized loss minimization problem like Lasso. More broadly, our data and feature parallelism technique can be applied to other machine learning algorithms such as matrix / tensor factorization, linear SVM [26], large-margin learning [14], and conditional ran-dom field [4]. In this paper, we propose Data / Feature Distributed Stochastic Coordinate Descent (DF-DSCD), an e ffi cient algorithm for solv-ing logistic regression, or L 1 regularization in general, in a fully distributed way. The main contributions are the followings:
Future works include extending DF-DSCD for other related tasks such as matrix factorization and tensor decomposition.
 This work was supported by the Basic Science Research Program through the National Research Foundation of Korea funded by the Ministry of Science, ICT and Future Planning (grants No. 2013005259 and No. 2013R1A1A1064409). [1] http://kdm.kaist.ac.kr/papers/dfdscd_supp_ [2] A. Agarwal, O. Chapelle, M. Dud X k, and J. Langford. A [3] J. Bennett, S. Lanning, and N. Netflix. The netflix prize. In [4] J. K. Bradley. Learning Large-Scale Conditional Random [5] J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. [6] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis. [7] A. Goldenberg, J. Kubica, and P. Komarek. A comparison of [8] C.-J. Hsieh and I. S. Dhillon. Fast coordinate descent [9] C. jen Lin, R. C. Weng, and S. S. Keerthi. Trust region [10] S. S. Keerthi and D. DeCoste. A modified finite newton [11] P. R. Komarek and A. W. Moore. Fast robust logistic [12] S.-I. Lee, H. Lee, P. Abbeel, and A. Y. Ng. E ffi cient L [13] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new [14] P. Long and R. Servedio. Algorithms and hardness results for [15] G. Mann, R. T. McDonald, M. Mohri, N. Silberman, and [16] T. P. Minka. A comparison of numerical optimizers for [17] D. Nguyen, N. A. Smith, and C. P. Ros X . Author age [18] F. Niu, B. Recht, C. R X , and S. J. Wright. Hogwild!: A [19] H. Qiu, Y. Liu, N. A. Subrahmanya, and W. Li. Granger [20] P. Richt X rik and M. Tak X   X  c. Distributed coordinate descent [21] C. Scherrer, M. Halappanavar, A. Tewari, and D. Haglin. [22] C. Scherrer, A. Tewari, M. Halappanavar, and D. Haglin. [23] S. Shalev-Shwartz and A. Tewari. Stochastic methods for l [24] S. Vucetic and Z. Obradovic. Collaborative filtering using a [25] C.-J. H. S. S. Yu, Hsiang-Fu and I. S. Dhillon. Parallel matrix [26] H.-F. Yu, C.-J. Hsieh, K.-W. Chang, and C.-J. Lin. Large [27] H.-F. Yu, H.-Y. Lo, H.-P. Hsieh, J.-K. Lou, T. G. McKenzie, [28] M. Zinkevich, M. Weimer, A. J. Smola, and L. Li.

