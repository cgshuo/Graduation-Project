 Currently, most research on nonnegative matrix factorization (NMF) focus on 2-factor X = FG T factorization. We provide a systematic analysis of 3-factor X = FSG T NMF. While unconstrained 3-factor NMF is equivalent to unconstrained 2-factor NMF, constrained 3-factor NMF brings new features to constrained 2-factor NMF. We study the orthogonality constraint b ecause it leads to rigorous clus-tering interpretation. We provide new rules for updating F and prove the convergence of these algorithms. Experiments on 5 datasets and a real world case study are performed to show the capability of bi-o rthogonal 3-factor NMF on simultaneously clus-tering rows and columns of the input data matrix. We provide a new approach of evaluating the quality of clustering on words us-ing class aggregate distribution and multi-peak distribution. We also provide an overview of various NMF extensions and examine their relationships.
 I.2 [ Artificial Intelligence ]: Learning; I.5.3 [ Pattern Recogni-tion ]: Clustering Algorithms, Experimentation, Theory nonnegative matrix factorization (NMF), orthogonal factorization, clustering, tri-factorization, multi-peak distribution
The nonnegative matrix factorization (NMF) has been shown re-cently to be useful for many applications in environment, pattern recognition, multimedia, text mining, and DNA gene expressions [3, 5, 15, 20, 27, 32]. This is also extended to classification [30]. NMF can be traced back to 1970s (Notes from G. Golub) and is studied Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. extensively by Paatero [27]. The work of Lee and Seung [18, 19] brought much attention to NMF in machine learning and data min-ing fields. They suggest that NMF factors contain coherent parts of the original data (images). They emphasize the difference between NMF and vector quantization (which is essentially the K -means clustering). However, later experiments [16, 20] do not support the coherent part interpretation of NMF. In fact, most applications make use of the clustering aspect of NMF, which is de-emphasized by Lee and Seung [18]. A recent theoretical analysis [9] shows the equivalence between NMF and K -means / spectral clustering.
Below we briefly outline NMF which provides notations and fur-ther motivations. In general, NMF factorizes input nonnegative data matrix X into 2 nonnegative matrices, where X  X  p  X  n + , F  X  p  X  k + and G  X  n  X  k + ( n  X  k + all n -by-k matrices whose elements are nonnegative). Generally, the rank of matrices F , G is much lower than the rank of X (i.e., k min ( p , n ) ).

In this paper, we emphasize th e orthogona lity of matrix factors in NMF. Specifically, we solve the one-sided G -orthogonal NMF, The main advantages are (1) uniqueness of the solution. (2) Clus-tering interpretations. We will show it is equivalent to K-means clustering.

Furthermore, it is natural t o consider imposing orthogonality on both F and G simultaneously in NMF. This corresponds to the simultaneous K -means clustering [9] of the rows and columns of X ,where F is the cluster indicator matrix for clustering rows and G is the cluster indicator matrix for clustering columns. However, this double ort hogona lity is very restrictive and it gives a rather poor matrix low-rank approximation. One needs an extra factor S to absorb the different scales of X , F , G . S provides additional degrees of freedom such that the low-rank matrix repre-sentation remains accurate while F gives row clusters and G gives column clusters. Thus we consider the following nonnegative 3-factor decomposition For the objective of the function approximation, we optimize We note X  X  p  X  n + , F  X  p  X  k + and S  X  k  X  + and G  X  n allows the number of row cluster ( k ) differ from the number of col-umn cluster ( ). In most cases, we set k = . This form gives a good framework for simultaneously clustering the rows and columns of X . Simultaneously rows and columns clustering using Laplacian matrix has been studied in [7, 35].

NMF is one type of matrix factorizations. There are other types of factorizations [6, 17, 33, 24, 23, 21]. Others include Latent Se-mantic Indexing [1], scaled PCA [10], generalized SVD [28], etc.
Here are some more notations. We often write F =( f 1 ,  X  X  X  , and G =( g 1 ,  X  X  X  , g k ) . The matrix norm A =  X  ij a 2 lowing, we emphasize the benefit of orthogonality for the unique-ness of the solution in  X  2. In  X  3 the benefits of orthogonality are discussed in detail. In  X  4 the computational algorithm for uni-orthogonal NMF is given. In  X  5 strong motivations for 3-factor NMF are discussed and the computing algorithm is given. In the detailed proof of algorithm correctness and convergence for uni-orthogonal NMF is presented. In  X  8 algorithm correctness and convergence for bi-orthogonal NMF are presented.  X  9isdevoted to experiments on 6 datasets. In particular,  X  9.3 shows the results on document clustering (columns of X );  X  9.4 provides a new and systematic analysis on word clustering (rows of X ). This differs substantially from the usual (document) clustering. In  X  10, an in-teresting case study of clustering system log data is presented. In  X  11, we provide an overview of various NMF extensions and ex-amine their relationships. The summary is given in  X  12.
Generally speaking, for any given solution ( F , G ) of NMF: X FG T , there exist large number of matrices ( A , B ) such that Thus ( FA , GB ) is also the solution with the same residue X FG T . With orthogona lity condition, we show that this degree of freedom is eliminated.
 Proposition 1 . With the o rthogona lity condition F T F = NMF, there exist no matrix A , B that satisfies both Eq.(6) and the orthogona lity condition ( FA ) T ( FA )= I , except when A mutation matrices, i.e., A = P , B = P T , P T P = I , P ij Proof . ( FA ) T ( FA )= I implies A T A = I . Except A = tion matrix, at least one off-diagonal element of A must be negative. Say, A k 1 , 1 &lt; 0. Because of orthogonality, each row of F has ex-actly one nonzero element. Suppose for i 1 -th row of F ,the k element is nonzero. Thus ( FA ) i 1 1 =  X  k F i 1 , k A k Thus there can be no negative elements in A .  X 
We note that for any matrix factorization, the freedom of col-umn/row permutation always exists.
Lee and Seung [18] emphasizes the difference between NMF and vector quantization (which is K -means clustering). Later experi-ments [16, 20] empirically show that NMF has clear clustering ef-fects. Theoretically, NMF is inherently related to kernel K-means clustering.
 Theorem 1 . Orthogonal NMF, is equivalent to K-means clustering.
 This theorem has been previously proved[9] with additional nor-malization conditions. Here we give a simpler and more general proof, which can easily gene ralize to bi-orthogona lity. Proof . We write J = || X  X  FG T || 2 = Tr ( X T X  X  2 F T The zero gradient condition  X  L /  X  F =  X  2 XG + 2 F = 0gives F XG . Thus J = Tr ( X T X  X  G T X T XG ) . Since Tr ( X T X the optimization problem becomes According to Theorem 2 below, this is identical to K-means clus-tering.

We note that Theorem 1 holds even if X and F are not nonnega-tive, i.e., X and F have mixed-sign entries. This motives generaliz-ing NMF to semi-NMF in  X  11.1.
 Theorem 2 [8, 34]. The K -means clustering minimizes where c k is the cluster centroid of the k -th cluster. More generally, the Kernel K-means with mapping x i  X   X  ( x i ) minimizes where  X   X  k is the centroid in the feature space, and G is cluster in-dicator matrix: G ik = 1if x i  X  C k and G ik = 0 otherwise. Let the the optimization problem where W ij =  X  ( x i ) T  X  ( x j ) is the kernel. For K -means, W
Now we generalize Theorem 2 to simultaneous row/column K -means clustering.
 Theorem 3 .Let G be the cluster indicator matrix for K -means clustering of columns of X and F be the cluster indicator matrix for K -means clustering of rows of X . The simultaneous row/column clustering can be solved by optimizing Proof . The clustering of columns of X is the same as in Theorem 2. Let the rows of X be ( y 1 ,  X  X  X  , y k )= X T . Applying Theorem 2 to rows of X , the simultaneous row and column clustering becomes simultaneous optimizations: We now show that  X  F and  X  G , or their un-normalized counterparts, F and G , can be obtained via From this, let the diagonal matrix D F =( || f 1 || ,  X  X  X  , || diagonal matrix D G =( || g 1 || ,  X  X  X  , || g k || ) . We can write FG (
To show the optimization in Eq.(14) is equivalent to that in Eq.(13), we write J = X  X  FG T 2 = Tr ( X  X  FG T ) T ( X  X  FG T ) . = 0, we obtain G = X T ( F T F )  X  1 . Substituting back, we have J Tr ( X T X  X   X  F T XX T  X  F ) , where  X  F = F ( F T F )  X  Thus min J becomes maxTr  X  F T XX T  X  F . This is part of Eq.(13) for  X  F .From  X  T /  X  F = 0, we can show min J becomes maxTr  X  alent to that of Eq.(13).  X 
Without the diagonal factor D in Eq.(12), Theorem 3 has been noted in [9]. The more careful treatment here reveals that the si-multaneous row/column K -means clustering allows an extra scale diagonal factor D in the NMF formulation. Generalizing D to full matrix S , we arrive at the bi-orthogonal 3-factor NMF of Eq.(5). Proposition 2 . The bi-orthogonal 3-factor NMF is equivalent to and alternatively, Proof . Expanding J 4 = Tr ( X T X  X  2 X T FSG T + S T S ) derivative  X  J 4 /  X  S = 0 , we obtain or where | R | is the size of the -th row cluster, and | C k the k -th column cluster. S k represents properly normalized within-cluster sum of weights ( = k ) and between-cluster sum of weights ( = k ). The meaning of NMF is that if the clusters are well-separated, we would see the off-diagonal elements of S are much smaller than the diagonal elements of S .

Substituting S = F T XG into J 4 ,wehave J 4 = Tr ( X T X G X T FF T XG ) . This leads to optimization in Eqs.(15,16). Now, applying Theorem 2 to Eqs.(15,16), we have Theorem 4 . In the bi-orthogonal 3-factor NMF, G gives the solu-tion for kernel K-means clustering of columns of X using kernel W = X T FF T X (inner product of the projection of X into the sub-space spanned by F ). Similarly, F gives the solution for clustering rows of X using kernel W = XGG T X T (inner product of the pro-jection of X into the subspace spanned by G ).
We are interested in solving the following one-side orthogonal ( F -orthogonal) NMF We solve it using an iterative update algorithm. For this F -orthogonal optimization problem Eq.(34) the update rules are The correctness and convergence proofs involve optimization the-ory, auxiliary function and several matrix inequalities. The lengthy proofs are given in  X  7.

Alternatively, we optimize the G -orthogonal NMF The update rules are
Update rules Eqs.(20,24) are standard NMF rules[19]. Update rules Eqs.(23,21) are results of this paper (see  X  7).
First, we emphasize the role of orthogonal in 3-factor NMF. Con-sidering the unconstrained 3-factor NMF we note that this 3-factor NMF can be reduced to the unconstrained 2-factor NMF by mapping F  X  FS . Another way to say this is that the degree of freedom of FSG T is the same as FG T .

Therefore, 3-factor NMF is interesting only when it can not be transformed into 2-factor NMF. This happens when certain con-straints are applied to the 3-factor NMF. However, not all con-strained 3-factor NMF differ from their 2-factor NMF counterpart. For example, the following 1-sided orthogonal 3-factor NMF is no different from its 2-factor counterpart Eq.(19), because the mapping F  X  FS reduces one to another.

It is clear that has no corresponding 2-factor counterpart. We call it the bi-orthogonal tri-factorization and is the focus of this paper. It can be computed using the following update rules These rules are obtained as the following: Update G . Clearly, fixing ( FS ) , updating G is identical to Eq.(2) (replacing D n by I ). The updating rule is given by Eq.(23). Replac-ing F by FS , update rule Eq.(23) becomes Eq.(28) Update F . Similarly, fixing SG T , the rule updating F is obtained from Eq.(21). Replacing G by GS T , we obtain the updating rule of Eq.(29).
 Update S . Fixing F , G , we update S . The correctness and conver-gence of this update are proved in  X  8.
An important special case is that the input X contains a matrix of pairwise similarities: X = X T = W . In this case, F = G = optimize the symmetric NMF: This can be computed using
We wish to solve the following optimization problem where X is input, G is fixed and D is nonnegative and diagonal. Following the standard theory of constrained optimization, we in-troduce the Lagrangian multipliers  X  (a symmetric matrix of size  X  K ) and minimize the lagrangian function Noting || X  X  FG T || 2 = Tr ( X T X  X  2 F T XG + G T GF T dient of L 3 is The KKT complementarity condition gives This is the fixed point relation that local minima for S must hold.
The standard approach is to solve the coupled equations Eq.(37) and constraint F T F = I for F ,  X  .Thereare nk variables for F and k ( k + 1 ) / 2for  X  and the same number of equations. This system of nonlinear equations is generally difficult to solve. Below we provide an algorithm to compute the solution. We show that given an initial guess of F , successive update of will converge to a local minima of the problem. There are two issues here: (1) the convergence of the algorithm, and (2) the cor-rectness of the converged solution.
 The correctness is assured by the fact that at convergence, from Eq.(38). the solution will satisfy This is the same as the fixed point condition of Eq.(37), i.e., either F ik = 0 or the left factor is zero.

The convergence of the algorithm is guaranteed by the following monotonicity theorem Theorem 5 . The Lagrangian function L 3 is monotonically de-creasing (non-increasing) under the update rule Eq.(38), assuming G G +  X   X  0.

Since L 3 is obviously bounded from below, the successive itera-tion will converge.

Let us first prove the following proposition which plays a key role in the proof of Theorems 5 and 7.
 Proposition 6 . For any matrices A  X  n  X  n + , B  X  k  X  k + + , and A , B are symmetric, the following inequality holds Proof .Let S ip = S ip u ip . Using the explicit index, the difference  X  = LHS-RHS can be written as Because A , B are symmetric, this is equal to When B = I ,and S is a column vector, this result reduces to the one shown in [19]. Now we are ready to prove Theorem 2.
 Proof of Theorem 5 .

We use the auxiliary function approach [19]. A function Z is called an auxiliary function of L ( H ) if it satisfies for any H ,  X  H .Define By construction, L ( H ( t ) )= Z ( H ( t ) , H ( t ) )  X  ) .
 Thus L ( H ( t ) ) is monotonic decreasing (non-increasing). The key is to find appropriate Z ( H ,  X  H ) .
 We write L 3 of Eq.(35) as where we ignore the constraints X T X and Tr  X  . Now we show that the following function Z (
F , F )=  X   X  is an auxiliary function of L 3 ( F ) . First, it is obvious that when F = F the equality holds Z ( F , F )= L 3 ( F ) . Second, the inequality holds Z ( F , F )  X  L 3 ( F ) . This is because: (A) The second term in Z (
F , F ) is always bigger than the second term in L 3 ( F ) Proposition 6. (B) The first term in Z ( F , F ) is always smaller than the first term in L 3 ( F ) , because of the inequality Now according to Eq.(41), we find the minimum of min F Z ( fixing F . This minimum is given by Solving for F ik , the minimum is According to Eq.(41), F ( t + 1 ) = F and F = F ( t ) , we recover Eq.( 38).  X 
It remains to determine the Lagrangian multiplier  X  and make sure G T G +  X   X  0. In Eq.( 38), summing over index i ,wehave (  X 
F T XG + DG T G + D  X  ) kk = 0 . Therefore we obtain the diagonal elements of the Lagrangian multipliers The off-diagonal elements of the Lagrangian multipliers are ap-proximately obtained by ignoring the non-negativity of F , i.e., it is given by Eq.(36), Combining Eqs.( 44, 45), we have a compact solution for the La-grangian multipliers Clearly, the condition in Theorem 2, G T G +  X   X  0 is satisfied. Sub-stituting this in Eq.(38) we obt ain the update rule of Eq.(21).
So far we assume G is fixed. Given F , we can update G using the standard rule of Eq.(20). We can alternatively update F residue J ( F , G ) will monotonically decrease In summary, we have proved that the minimization of Eq.(34) can be solved by the updating rules of Eqs.(20,21), and shown the con-vergence of the algorithm. In 3-factor NMF, the key is the factor S in the middle. Factors F , G can be dealt with in the same way as in 2-factor NMF. Theorem 7 .Let F , G be any fixed matrices, is monotonically decreasing under the update rule of Eq.(30). Proof . First we prove the correctness. Following the same ap-proach in  X  7, The KKT complementarity condition gives At convergence, the solution from the update rule Eq.(30) satisfies (  X  F T XG + FF T SG T G ) ik F 2 ik = 0, which is equivalent to Eq.(37). This proves the correctness of rule Eq.(30).

Next, we prove the convergence. We use the auxiliary function approach in the proof of Theorem 5 near Eqs.(40, 41). Now we show that is an auxiliary function of J 5 ( S ) . First, the third term in Z is always bigger than the third term in J 5 ( S ) , due to Proposition 6in  X  7 Eq.(39). Second, the second in Z ( S , S ) (aside from the negative sign) is always smaller than the second term in J to the inequality Eq.(43). Thus the condition Z ( S , S )  X  Z ( S , S ) is an auxiliary function of J 5 ( S ) .

According to Eq.(41), S ( t + 1 ) is given by the minimum of J while fixing S = S ( t ) . The minimum is obtained by setting 0 = which is equal to Under this update rule, J 5 ( S ) decreases monotonically. In this section, we apply the bi-orthogonal 3-factor NMF (BiOR-NM3F) clustering algorithm to cluster documents and compare its performance with other standard clustering algorithms. In our ex-periments, documents are represented using the binary vector-space model where each document is a binary vector in the term space.
We use a variety of datasets, most of which are frequently used in the information retrieval research. Table 1 summarizes the char-acteristics of the datasets.

CSTR . This is the dataset of the abstracts of technical re-ports (TRs) published in the Department of Computer Science at a research university. The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Process-ing(NLP), Robotics/Vision, Systems, and Theory.

WebKB4 . The WebKB dataset contains webpages gathered from university computer science departments. There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other. The raw text is about 27MB. Among these 7 categories, student, faculty, course and project are four most populous en tity-representing categories. The associated subset is typically called WebKB4 . In this paper, we perform experiments on the 4-category dataset.

Reuters . The Reuters-21578 Text Categorization Test collec-tion contains documents collected from the Reuters newswire in 1987. It is a standard text categorization benchmark and contains 135 categories. In our experiments, we use a subset of the data col-lection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10 .

WebAce . This is from WebACE project and has been used for document clustering [2, 13]. The dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997. These documents are divided into 20 classes.
Newsgroups . The 20 newsgroups dataset contains approxi-mately 20,000 articles evenly divided among 20 Usenet newsgroups. The raw text size is 26MB.

To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored. In all our experiments, we first select the top 1000 words by mutual information with class labels. The feature selection is done with the rainbow package [25].
The above document datasets are standard labeled corpora widely used in the information retrieval literature. We view the labels of the datasets as the objective knowledge on the structure of the datasets. To measure the clustering performance, we use purity [36] and Adjusted Rand Index (ARI) [26] as our performance measures. We expect these measures would provide us with good insights on how our algorithm works.

Purity measures the extent to which each cluster contained data points from primarily one class [36]. The purity of a clustering solution is obtained as a weighted sum of individual cluster purity values and is given by K is the number of clusters and n is the total number of points In general, the larger the values of purity, the better the clustering solution is.
 Entropy measures how classes distributed on various clusters [36]. The entropy of the entire clustering solution is computed as: where m is the number of original labels, K is the number of clus-ters. Generally, the smaller the entropy value, the better the clus-tering quality is.

The Rand Index is defined as the number of pairs of objects which are both located in the same cluster and the same class, or both in different clusters and different classes, divided by the total number of objects [29]. Adjusted Rand Index which adjusts Rand Index is set between [ 0 , 1 ] [26]. The higher the Adjusted Rand In-dex, the more resemblance between the clustering results and the labels.
We compare our bi-orthogonal 3-factor NMF (BiOR-NM3F) clus-tering with the K-means algorithm. The comparisons are shown in Table 2. Each entry is the corresponding performance measure value of the algorithm on the row dataset. From Table 2, we observe that our BiOR-NM3F clustering achieves better purity results than K-means on CSTR, WebKB4, Reuters and Newsgroup datasets. In particular, on Newsgroup, the improvement of the purity value is significant (from 0 . 330 to 0 . 507). The purity results of K-means are slightly better than BiOR-NM3F on WebAce: the difference is only 0 . 005.
 Table 2: Performance Comparisons of clustering algorithms. Each entry is the corresponding performance value of the algo-rithm on the row dataset.

The performance of purity and ARI is consistent in our com-parison, i.e., higher purity values usually correspond to higher ARI
P ( S i ) is also called the individual cluster purity.
 values. However, there exist slight differences in the relative perfor-mance of purity and entropy in our comparison, i.e., higher purity values do not necessarily correspond to lower entropy values (e.g., on Reuters dataset). This is because the entropy measure takes into account the entire distribution of the documents in a particular clus-ter and not just the largest class as in the computation of the purity.
In summary, the comparison shows that BiOR-NM3F is a viable and competitive algorithm in document clustering domain, espe-cially considering that BiOR-NM3F is performing document clus-tering and words clustering simultaneously, while K-means is per-forming document clustering only.
Our BiOR-NM3F algorithm performs clustering of words simul-taneously, where the factor F is the cluster indicator for words.
In this section, we describe the experimental results on word clustering. We consider two clustering strategies: i) hard clustering where a word is assigned to a single cluster and ii) soft clustering where a word can be assigned to several clusters. We analyse hard clustering using class conditional word distribution. We analyse soft clustering using multi-peak distribution. To our knowledge, both of these two analysis methods are new. Quantitatively, we can view the i -th row of the cluster indicator F as the posterior probability that word i belongs to each of the K word clusters. For hard clustering, we assign a word to the cluster that has the largest probability value.

Word clustering has no clear aprior labels to compare with. We resolve this difficulty by considering the class conditional word dis-tribution. For each document class (with known labels), we com-pute the aggregate word distribution, the frequency of word occur-ring in different documents in the class. For hard clustering, we assign each word to the class with highest probability in the ag-gregate distribution. We expect this assignment would provide a reasonable criterion for evaluating word clustering, i.e., we expect the word clustering results match this assignment. We also use pu-rity, entropy and ARI for evaluating the match. Table 3 shows the hard clustering results on words.
 Table 3: Performance of hard clustering on words. Each entry is the corresponding performance value of the word clustering on the row dataset.
Note that in general, the cluster indicator F for words is not ex-actly orthogonal. This is because the off-diagonal Lagrangian mul-tipliers of Eq.( 45) are obtained ignoring the non-negativity con-Table 4: Words Multi-Peak Distribution for CSTR dataset. straints of G . This slight deviation from rigorous orthogonality produces a benefit of soft clustering.

Here we also provide a systematic analysis of the soft cluster-ing of words. Quantitatively, we view i -th row of F as the poste-rior probability that word i belongs to each of the K word clusters. Let this row of F be ( p 1 ,  X  X  X  , p k ) , which has been normalized to  X  p k = 1. Suppose a word has a posterior distribution of it is obvious that this word is cleanly clustered into one cluster. We say this word has a 1-peak distribution. Suppose another word has a posterior distribution of ( 0 . 48 , 0 . 48 , 0 . 04 this word is clustered into two clusters. We say this word has a 2-peak distribution. In general, we wish to characterize each word as belonging to 1-peak, 2-peak, 3-peak etc. For K word clusters, we set K prototype distributions: For each word, we assign it to the closest prototype distribution based on the Euclidean distance, allowing all possible permuta-tions of the clusters. For example, ( 1 , 0 , 0 ,  X  X  X  , 0 ( 0 , ponents decrease from the left to the right, and then assign it to the closest prototype. Generally speaking, the less peaks of the posterior distribution of the word, the more unique content of the word has. To further illustrate the soft clustering evaluation, we take a closer look at the CSTR dataset. Table 4 lists several words in 1-peak, 2-peak, 3-peak and 4-peak categories respectively. We see that these words are meaningful and are often representatives of the associated document clusters. For example, multiproces-sor and cache are 1-peak words that are associated with the Sys-tems cluster; polynomial and complexity are 1-peak words related to the Theory cluster; recognition and learning are 2-peak words associated with Robotics/Vision and NLP clusters; Interface is a 2-peak words associated with Robotics/Vision and System clus-ters; system , process ,and user are 3-peak words associated with Robotics/Vision , System and NLP clusters; present , algorithm and paper are 4-peak words. To summarize, the word clustering is capable of distinguishing the contents of words. The results of peak words are consistent with what we would expect from a sys-tematic content analysis. This aspect of tri-factorization shows a unique capability that most other clustering algorithms are lacking.
In this section, we present a case study of applying our clustering technique to system log data. In system management applications, to perform automated analysis of the historical data across multiple components when problems occur, we need to cluster the log mes-sages with disparate formats to automatically infer the common set of semantic situations and obtain a brief description for each situa-tion [22].

The log files used in our experiments are collected from sev-eral different machines with different operating systems using log-dump2td (NT data collection tool) developed at IBM T.J. Watson Research Center. The data in the log files describe the status of each component and record system operational changes, such as the starting and stopping of services, detection of network applica-tions, software configuration modifications, and software execution errors. The raw log files contain a free-format ASCII description of the event. In our experiment, we apply clustering algorithms to group the messages into different semantic situations. To pre-process text messages, we remove stop words and skip HTML la-bels. The raw log messages have been manually labeled with its semantic situation by domain experts [22]. The set of semantic sit-uations include start , stop , dependency , create , connection , re-port , request , configuration ,and other . The detailed explanations of these situations can be found in [4].
 We obtain good message clustering results as shown in Table 5. The performance of BiOR-NM3F is better than K-means on all three measures. Table 6 shows the words in 1-peak, 2-peak, 3-peak and 4-peak categories for the log data respectively. We can derive meaningful common situations from the word cluster results. For example, situation start can be described by 1-peak words such as started , starting ,and service , and 2-peak words such as version . The situation configure can be described by 1-peak words such as configuration , two-peak words such as product , and 3-peak words such as professional .

The case study on clustering log message files for computing system management provides a successful story of applying the cluster model in real applications. The log messages are relatively short with a large vocabulary size [31]. Hence they are usually represented as sparse high-dimensional vectors. In addition, the log generation mechanisms implicitly create some associations be-tween the terminologies and the situations. Our clustering method explicitly models the data and word assignments and is also able to exploit the association between data and features. The synergy of these factors leads to the good application on system management. Besides 3-factor extension in this paper, there are many other NMF extensions. Here we provide an overview.
 First, we consider different types of nonnegative factorizations. The standard NMF can be written as using an intuitive notation for X , F , G  X  0.

The classic matrix factorization is Principal Component Analysis (PCA) which uses the singular value decomposition X  X  U  X  where we allow U , V to have mixed-signs; the input data could have mixed-signs. absorbing  X  into U , we can write
However, even if X have mixed-signs, we could enforce G to be nonnegative (since G can be interpreted as cluster indicators, as in  X  3). This is called semi-NMF [11]: Theorem 1 provides the basis for this semi-NMF formulation.
Both NMF and semi-NMF have clustering capabilities which are generally better than the K-means. In fact, PCA is effectively doing K -means clustering[8, 34]. Let G be the cluster indicators for the k clusters then (1) GG T VV T ; (ii) the principal directions, UU project data points into the subspace spanned by the k cluster cen-troids.
So far, the cost function we used for computing NMF is the sum of squared errors, || X  X  FG T || 2 . Another cost function KL diver-gence: PLSI [14] maximizes the likelihood where the joint occurrence probability is factorized (i.e., parame-terized or approximated ) as
In [12], it is shown that Objective function of PLSI is identical to the objective function of NMF, i.e., J PLSI =  X  J NMF-KL + and the EM algorithm in training PLSI are alternative methods to optimize the same objective function.
We study computational algorithms for orthogonal 2-factor NMF and 3-factor NMF. The bi-orthogonal 3-factor NMF provides a strong capability of simultaneously clustering rows and columns. We de-rive new updating rules and prove the convergence of these algo-rithms. Experiments show the usefulness of this approach. We also provide a new approach of evaluating the quality of word clus-tering. In addition, we also present an overview of various NMF extensions and examine their relationships.
 Chris Ding is partially supported by the US Dept of Energy, Office of Science. Tao Li is partially supported by a 2005 IBM Faculty Research Award, and the National Science Foundation CAREER Award under grant no. NSF IIS-0546280. Wei Peng is supported by a Florida International University Presidential Graduate Fellow-ship. [1] M.W. Berry, S.T. Dumais, and Gavin W. O X  X rien. Using [2] D. Boley. Principal direction divisive partitioning. Data [3] J.-P. Brunet, P. Tamayo, T.R. Golub, and J.P. Mesirov. [4] M. Chessell. Specification: Common base event, 2003. [5] M. Cooper and J. Foote. Summarizing video using [6] I. Dhillon and D. Modha. Concept decom position for large [7] I. S. Dhillon. Co-clustering documents and words using [8] C. Ding and X. He. K-means clustering and principal [9] C. Ding, X. He, and H.D. Simon. On the equivalence of [10] C. Ding, X. He, H. Zha, and H. Simon. Unsupervised [11] C. Ding, T. Li, and M. Jordan. Convex and semi-nonnegative [12] C. Ding, T. Li, and W. Peng. Nonnegative matrix [13] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, [14] T. Hofmann. Probabilistic latent semantic analysis. In UAI , [15] P. O. Hoyer. Non-negative matrix factorization with [16] P. O. Hoyer. Non-negative matrix factorization with [17] G. Karypis and E.-H. Han. Concept indexing: A fast [18] D.D. Lee and H. S. Seung. Learning the parts of objects by [19] D.D. Lee and H. S. Seung. Algorithms for non-negatvie [20] S.Z. Li, X. Hou, H. Zhang, and Q. Cheng. Learning spatially [21] T. Li. A general model for clustering binary data. In KDD , [22] T. Li, F. Liang, S. Ma, and W. Peng. An integrated [23] T. Li, S. Ma, and M. Ogihara. Document clustering via [24] B. Long, Z. Zhang, and P.S. Yu. Co-clustering by block value [25] A.K. McCallum. Bow: A toolkit for statistical language [26] G.W. M illigan and M.C . Cooper. A study of the [27] P. Paatero and U. Tapper. Positive matrix factorization: A [28] H. Park and P. Howland. Generalizing discriminant analysis [29] W.M. Rand. Objective criteria for the evaluation of clustering [30] F. Sha, L.K. Saul, and D.D. Lee. Multiplicative updates for [31] J. Stearley. Toward informatic analysis of syslogs. In [32] W. Xu, X. Liu, and Y. Gong. Document clustering based on [33] D. Zeimpekis and E. Gallopoulos. Clsi: A flexible [34] H. Zha, C. Ding, M. Gu, X. He, and H.D. Simon. Spectral [35] H. Zha, X. He, C. Ding, M. Gu, and H.D. Simon. Bipartite [36] Y. Zhao and G. Karypis. Empirical and theoretical
