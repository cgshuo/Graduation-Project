 The rapid development of information and network technologies facilitates efficient in-formation exchanges on an everyday basis especially via online communication. How-ever, such advantages may not have been fully enjoyed by Chinese users, as there are two versions of written characters currently in use in different Chinese commu-nities, that is, the BIG5 code Traditional version of Chinese characters (henceforth TC) prevalent in Hong Kong, Macao, and Taiwan, and the GB code Simplified version of Chinese characters (henceforth SC) in mainland China and Singapore. Admittedly, the majority of the SC characters are either identical with their TC counterparts, for example, and ( ren  X  X uman X ) or formally  X  X implified X  from their TC counterparts without any change in meaning or usage, such as the SC ( fu )andtheTC ( fu ) in E ( pi-fu  X  X kin X ). While such cases require no more than a straightforward one-to-one converting operation, it is of interest to note that, of the 41,321 SC char-acters we have surveyed, 1,404, or 3.398%, have several TC counterparts different in semantic meaning. For example, the SC ( fa ) should be in ( fa-zhan  X  X evel-opment X ) but in ( mao-fa  X  X air X ) (see also Wang and Wei [2008], p. 150). On top of that, complexity may also arise on the word level because of regional variations even though no SC-TC conversion is involved. For example, strawberry is ( shi-duo-bei-li , a transliteration of  X  X trawberry X ) in Hong Kong TC, but ( cao-mei ) in both Taiwan TC and mainland SC.

In view of such complexities, which to a large degree are responsible for flawed char-acter conversions [Feng 2000], individual researchers and institutions have for decades been working on the improvement of the accuracy and reliability of conversion sys-tems. A variety of automatic conversion tools have been developed for general use in nearly every Office suite, such as Microsoft Office, Sun OpenOffice, and KingSoft WPS. Free software systems and applications are also available online.

Conversion results produced by these tools, however, often fall short of a professional standard, and human editing is required. Yet, since the characters have been converted automatically and  X  X uietly, X  so to speak, without leaving behind any traceable mark-ing, human editing can turn out to be a tedious and time-consuming operation to check out all such changes for verification and rectification purposes. In serious document processing such as diplomatic documentation, public discourse and TV subtitling, a flawed conversion can cause unexpected problems as a wrongly converted character may create embarrassing if not hilarious effects if left uncorrected [Wang and Wei 2008]. Moreover, users may get different results by using different tools. Such inaccu-rate conversions are mostly due to what Halpern and Kerman have called the  X  X itfalls and complexities X  in Chinese character conversion [Halpern and Kerman 1999]. In 2010, Li and Wu proposed a method called  X  X hinese Characters Conversion System based on Lookup Table and Language Model X , which is based on an n -gram statistical model. With the training on a large-scale data corpus, the system is reported to have produced much better results than those of the state-of-the-art tools such as Google Translate and Word 2007 [Li and Wu 2010]. The conversion accuracy reaches 94.8% by their evaluation method and exceeds Google Translate by 6%, so far the best as our survey indicates. However, it has an obvious weakness in converting long phrases since the n -gram only uses bigrams and unigrams for the reason of reducing the oth-erwise high calculation costs. For example, in the phrase ( si-shi-lai-li wai  X  X bout forty li away X ), the two adjacent characters (here referring to a Chinese unit of length of 500m) and ( X  X way, X   X  X eyond X ) form a combination which may easily be read by the machine as a regular word ( li wai ) meaning  X  X nside and outside, X  since both contain a one-to-many character .As is a character of high frequency in n -gram training, it is impossible to convert correctly by Li X  X  method. (See further discussion of the case in Section 4.3 below.) In addition, since the system did not avail itself of such ready lexicons as dictionaries, it seems unnecessarily slow in actual oper-ation, as it takes much computation to determine from scratch each of the one-to-many characters.

This article therefore proposes a new algorithm called  X  X used Conversion algorithm from Multi-Data resources (FCMD). X  The method is based on a revised n -gram sta-tistical model with large-scale corpus training. Apart from the trained terms, it also draws on philological resources such as conversion-dedicated as well as general Chi-nese dictionaries to ensure authenticity and authority of the output. With the help of an online segmentation dictionary [Ministry of Education of the People X  X  Republic of China Characters 1986], the algorithm uses reverse maximum matching to handle the straightforward one-to-one and the troublesome one-to-many cases, and identification and conversion of the latter is conducted with further support from a revised n -gram model and a pattern matching model, with a view to improving precision. Moreover, to benefit from advantages of different data resources, a priority-based data resource management model is proposed to enhance the data quality, which means users can decide in the process whether to activate a specific data resource, for instance the Mapping Table of Common Words Used in Mainland and Hong Kong provided by the system, in order to render a character more in keeping with the target region usage. As such, it can greatly increase conversion flexibility as well as authenticity compared with the existing methods.

The proposed model has gone through three training processes. The first one is to identify in a large corpus the one-to-many characters and their frequencies of occur-rence. In this connection, Tagged Chinese Gigaword Version 2.0 [Huang 2009], which is of 10,105MB in total, is used to train our proposed FCMD model to acquire n -grams for one-to-many conversion purposes. The second is to calculate and determine the best parameter of balance weight between dictionary matching and n -gram computation. The third is to train the best thresholds of support and confidence so as to learn more applicable patterns. With these trained parameters, the experimental results on Wang and Wang X  X  published dataset [Wang and Wang 2005] show that the one-to-many con-version precision rate reaches 91.5% and the overall precision rate reaches 99.8%. Our model is thus found to be outperforming the best method proposed by Li and Wu [2010] with the performance improved by 5.9% on one-to-many conversion.

Based on the FCMD model, we designed and implemented a Chinese character conversion platform (henceforth the Platform) including automatic conversion, data resource management, n -gram training, pattern learning, and result comparison modules. The n -gram training module is to train n -grams on larger corpus continu-ously and the pattern learning module is to learn suitable patterns for one-to-many conversion on the training corpus. In addition to improving the precision of automatic Chinese character conversion, we take into consideration professional needs in, for ex-ample, the publication industry for human verification of documents after conversion. There may also be the need of terminology localization when converting a document produced in one region for consumers in another. To cater to such needs, we designed an advanced manual revision-verification function, which enables users to choose specific data resources for conversional tasks that involve terminology localization. It also allows users to automatically add and store in their on-platform personal term banks of specially verified or rectified terms (e.g., proper nouns involving one-to many cases) to support future conversions.

The rest of this article is organized as follows: Section 2 gives an overview of the dif-ficulties in Chinese character conversion, the performance of the existing conversion tools, and related research efforts. Section 3 describes our priority-based data resource management model and its implementation, followed by Section 4, which presents our FCMD algorithm in detail. In Section 5, the operation of the platform is demonstrated and explained with related user interfaces. Section 6 shows the training, pattern learn-ing, experiments, evaluation, and result analysis, followed in Section 7 by a conclusion and a brief description of the envisaged work to be undertaken in future. As mentioned above, there are two scripts of Standard Chinese: the SC used in mainland China and Singapore, and the TC in Taiwan, Hong Kong, Macao, and most overseas Chinese communities. A common fallacy is that there is a straightforward correspondence between the two systems, and that conversion between them merely requires mapping from one character set to another, such as from the GB 2312-80 in SC to BIG5 in TC. Yet what makes the conversion complicated is the fact that there are a certain number of characters in one system that have more than one counterpart in the other according to specific collocations and semantic contexts. A further example is as follows: in different compounds, the SC character ( tai ) should take a different TC form depending on the character it collocates with in forming a compound, for exam-ple, ( tai  X  X able/desk X ) as in ( tai-deng  X  X esk lamp X ), ( tai  X  X latform/tower X ) as in ( ping-tai  X  X errace X ), and ( tai )in ( tai-feng  X  X yphoon X ). However, as noted below, few of the current conversion tools have an efficient mechanism to deal with such one-to-many conversions.

The situation of flawed conversions is discussed by Wang and Wei [2008]. According to them, factors responsible for the problems can be classified as follows: 1) mechan-ical adherence to the standard SC-TC conversion list, which does not address one-to-many cases; and 2) lack of attention to regional variations, for example the term ( gen-zhe  X  X ollowing X ) in SC should be ( gen-zhe ) in Taiwan TC. All this has been attributed in general to a lack of philological and linguistic training and contextual sensitivity among designers and programmers, which is confirmed by Halpern in Halpern and Kerman [1999] and Halpern [1996]. For high-end document process-ing, errors in machine conversion have to be rectified manually, which, as Halpern [Halpern 1996; Halpern and Kerman 1999] noted, is a costly operation.

Hence, on the one hand, there are a variety of conversion tools developed by, for example, the Software Institute of Chinese Academy of Science, Richwin Information Technology Co. Ltd., New World Company, IBM Taiwan Co., and Eten AG Co., besides those provided by commonly used office software systems such as Microsoft Office, Sun OpenOffice, YoZo Office, King WPS, and online language tools such as Google Translate; on the other hand, however, very few of these tools are capable of supporting one-to-many conversions satisfactorily. Taking Office Word 2003 as an example, Wang and Wang found a large number of mistakes in its one-to-many conversions, when using the function of Chinese character conversion to render a 150 million-character corpus of novels from SC into TC [Wang and Wang 2005].
 Solutions to the problem are also being suggested, of course. For example, Liu and Wu proposed a hierarchical conversion method [Liu and Wu 2008], which uses larger linguistic units in an attempt to eliminate ambiguity. Since its operation mainly relies on dictionaries, the method does not cover all usages that may occur in real-life doc-umentation. Moreover, the one-to-many problem remains. Xin et al. proposed and de-signed a combined and more sophisticated conversion system aided by dictionaries and specific word lists, and operated via segmentation, disambiguation, and auto-adaption [Xin and Sun 2000]. The overall conversion precision reaches 99.8%. However, since the evaluation does not place due emphasis on one-to-many cases and is not described very clearly in the report for instance in comparison with other systems, it is hard to perceive the assessment of the performance clearly.

One of the most recent developments is Li X  X  conversion system, which uses a method-ology termed by the author as  X  X ookup table X  and  X  X anguage model X  [Li and Wu 2010] to disambiguate one-to-many cases and to tackle regional variations of lexical terms, with all the data coming from Wikipedia. The experimental results show that the system outperforms other popular conversion systems significantly. However, relying solely on one data resource without recourse to other more authoritative data resources can-not ensure conversion quality especially in view of the fact that some mappings may differ in different data resources. For example, the TC version of the SC ( shuo  X  X peak/say X ) is ! in the official Complete Table of Simplified Characters [1986] is-sued by the Ministry of Education of China, but " in Wikipedia. The operation also separates word conversion from character conversion. As such, ambiguity at the word level conversion cannot be dealt with systematically, as in the case of the SC word #$ ( hui-dang  X  X everberation X ), which can be #% , &amp;' ,or above, the performance is also slowed down by the large-scale n -gram calculations the system has to go through in every conversion operation.

Apart from the shortcomings noted above, current popular online conversion tools, such as Google Translate [Google 2011], KDD conversion [KDD 2011], and Tongwentang [2011] may not appear to be user-friendly in the sense that they do not provide facilities for those high-end, serious users such as editors to track and supervise in a more cost-efficient manner the automatic conversions done by the ma-chine for verification and rectification purposes, or for specialist users to compile and maintain an automatically updatable personal term bank for regular reference. Com-mercial tools in software form appear to be weak on this front as well. Of STGuru [Anasoft Studio 2011], which claims to be a leading conversion engine providing a full series of code conversion services of professional quality, users need to track down and correct duplicated conversional mistakes one by one through manual searching. With-out such functions as user-initiated error detection, en bloc correction and follow-up prevention on the one hand, and conversion tracking and personal term bank building on the other, a conversion tool cannot be regarded as cost efficient and user friendly, especially to professionals and specialists.

To construct a conversion platform that addresses such problems more profession-ally, we propose a new priority-based multi-data resource management model and a new fused conversion algorithm called FCMD, which are described in the following two sections. Any single data resource may not provide a working pool of words that covers suffi-ciently one-to-many cases for automatic conversion purposes because of the high com-plexity of the Chinese language and its regional variations. A corpus incorporating more than one data resource can presumably increase conversion capacity, and our model proves that such a corpus, thanks to its extended lexical coverage, has a sig-nificant advantage in processing delicate cases that may not have been attended to in other models.

Currently, there are a series of data resources, in the form of a dictionary or map-ping list, that can be used as basic references for character conversion, such as the official Complete Table of Simplified Characters and the multi-volume Comprehensive Chinese Dictionary [Commercial Press Ltd. 2001]. Despite their authority and authen-ticity, however, it is technically impossible for the coverage of such  X  X uality X  data re-sources to keep abreast of real-time development of regional variations and constant occurrence of new terms in the reality of language use. Wikipedia, on the other hand, given its easy and unlimited access, always provides a  X  X opular X  source of the most up-dated language data good for conversion reference. Data conflict, therefore, is expected between the quality and the popular resources in constructing and implementing a corpus of multi-data resources. For example, ( ( tai-li  X  X esk calendar X ) is converted as ) in Comprehensive Chinese Dictionary but as *) in Wikipedia [2011]. In our system, a new priority-based multi-data resource management model is introduced, which takes into account the status of a particular data resource and which may enlist the help of other algorithms such as the language model, in tackling the problem of data conflict to improve accuracy and efficiency.

At present, the proposed model in this article has a total of five resource categories: namely, personal term bank, regional-oriented term bank, words, one-to-many charac-ters, and one-to-one characters to indicate the status of a data item. In each category, data items can be from more than one data resource. This structure enables us to work out two kinds of priority: preference priority and authority priority, to evaluate the desirability of conversion candidates from the data resources. The preference priority is mainly related to users X  conversion requirements, for example, whether they prefer their target text to be localized with, say, Taiwan or Hong Kong terms. Under this priority, users can even choose to use their personal term banks compiled with the aid of the system, in which case such personal term banks have the highest priority. The authority priority is mainly related to the status of the data resource a conversion candidate is taken from. Under this priority, for example, the Complete Table of Simplified Characters and Comprehensive Chinese Dictionary have a higher status than the Dictionary of Chinese Word Segmentation 1 their publication venues. That is, while the former two are formally issued either by a government department or an established publisher, the latter is a freelance publication on the web. When there is more than one conversion candidate under the preference priority, the authority priority will be used as the determining factor.
We can see that the preference priority is dynamic and more responsive to users X  requirements while the authority priority is static and acts as a stable reference for quality control. The priority-based management model, designed as such, provides fa-cilities for users to easily and efficiently modify and extend their personal collection of data resources into term banks. The user can also choose whether to activate the preference priority in an actual operation of conversion.
 In our system, the lexical corpus at present draws on a total of eight data resources. In addition to those mentioned above, in the same category there are two region-oriented data resources, viz. Mapping Table of Common Words Used in Mainland and Taiwan [Tourism Bureau 2011] and Mapping Table of Common Words Used in Main-land and Hong Kong [Wikipedia 2011]. These two data resources are optional, as it is up to the user to decide which of them should be used or whether to use them at all on a case-by-case basis. And Common Mistakes in Character Conversion Between Sim-plified and Traditional Chinese [Zhuang 1998], as an additional data resource to the one-to-many category, has the same preference priority as the Wikepedia: Resources for Chinese Character Conversion . Categories of the data resources with their default preference priority and authority priority are listed as numerical values in Table I below.

In an operation of conversion, our system preprocesses an input document by split-ting it into sentences. The FCMD then extracts a string of candidates each time using a reverse maximum matching algorithm. After that, the candidates are checked against the multi-data resources in a desired order of priority. It first finds the data with the highest preference priority by comparing their default priority values so that the target character will firstly stay in keeping with the user X  X  preference. Figure 1 below gives the details of the operation.

The purpose is to ensure the use of the most pertinent data resource so that the target character can maintain a plausible degree of satisfaction. However, if there are conflicting candidates from data resources of the same preference priority, the match-ing will follow the authority priority, which means to use the most recognized candi-date across the data resources. If this cannot solve the conflict, the candidates from all data resources are combined for further disambiguation undertaken by the FCMD model. The FCMD module presented in the flowchart is further described in detail in the next section. To improve the quality of Chinese character conversion, a new comprehensive algo-rithm is proposed. The method develops a revised n -gram statistical model trained with a large-scale corpus. Instead of relying solely on trained terms, it also draws on multi-data resources described in the previous section. Reverse maximum matching is used to convert the straightforward one-to-one cases found among common terms, compounds and phrases. A pattern-based matching is also developed to assist with one-to-many conversions with automatic pattern learning techniques. Since the algo-rithm proposed is based on varied techniques, we call it  X  X used Conversation algorithm from Multi-Data resources X  (FCMD). By this algorithm, characters involved in one-to-many cases are matched at the word level through priority-sensitive matching. The revised n -gram model and the pattern-based matching model are to tackle cases of one-to-many characters and compounds, with a view to improving precision. The reverse maximum matching used in the FCMD is a string matching technique. The basic idea of string matching is to match a string of characters with the corpus of multiple data resources by using a specific strategy. If a candidate has found a match in the corpus, it is chosen and returned as the matched result. The method has been widely applied to Chinese segmentation for its high efficiency. In terms of scan direc-tion, the mentioned specific strategies include forward and reverse matching. In terms of string length, the strategies include maximum and minimum matching. Generally speaking, the reverse matching has a higher rate of precision and a lower degree of ambiguity than forward matching. According to statistical results from the Nanjing University of Aeronautics and Astronautics Library Group [2005], the error rate of forward maximum matching is 1/169 and that of reverse maximum matching is 1/245. Considering the low efficiency of combined maximum matching, we implement an ex-isting reverse maximum matching [Chen et al. 2009] as a part of our algorithm to process candidate strings against the data resources for matching purposes. The main algorithm of reverse maximum matching is not described here due to space limit. A statistical language model, in Blitzer X  X  [2005] terms, is a model that  X  X stimates the probability that a word will occur in a given context, wherein general a context spec-ifies a relationship to one or more words. X  In practice, the n -gram language model is one of such models most commonly used at present in information retrieval, as they are sufficient to determine the topic in a piece of writing [Zitouni and Zhou 2007]. By using Maximum Likelihood Estimation (MLE), the n -gram method can calculate the frequency of each word, and therefore the conversion probability of Chinese characters. For example, Zhang et al. tried to train Chinese character-based n -grams and applied them for copy detection [Zhang et al. 2011]. Details of the n -gram model have been described in Li and Wu [2010] including the smoothing strategy. Our focus is on the revised part of this model.

The probability of a string to serve as the conversion target is calculated by the probability of the first character multiplied by all the conditional possibilities of the sub strings from the first character to the end of the string as follows:
In Equation (1), the P ( c n 1 ) is the probability of a string to serve as the target in-cluding characters c from 1 to n . The c 1 is the first character. Since the words of one or two-character length are most common in the Chinese language, we use unigram and bigram in our research, similar to Li and Wu [2010]. According to In-terpolated Kneser-Ney smoothing [Goodman 2001], the formula is then transformed to Equation (2): where the  X  is a weight to balance bigram and unigram . At the same time, we re-place Li X  X  probability calculation formula with Equation (3) below. The purpose of this change is to avoid the problem of division by 0.

For one-to-many cases, the FCMD operates by matching entries in data resources as well as by using our improved n -gram model. Therefore, our algorithm works to both calculate the match possibilities and determine the  X  X ost matched X  candidate string as the conversion target. If a string c i has a match of ds in a data resource, the matching probability of ds is calculated by Equation (4). Otherwise, the probability is 0. In Equation (4) above, the preference priority and authority priority are used to calcu-late the probability of the c i to be chosen as the most qualified conversion candidate. The higher the priority is, the higher the probability of the c fore, the maximum priority probability of c i over all data resources can be extracted as P priority ( c i ) . Based on this revision, the final probability of the c revised and calculated by Equation (5): where the  X  is a weight of priority-based probability. The purpose is to balance the data resources and n -gram-based calculation to identify the best candidate for conversion. It is obtained through training and is described in the experiment section. Though the revised n -gram algorithm can greatly benefit one-to-many character conversions, the operation still has blind spots. For example, a phrase in SC /012 ( si-shi-lai-li wai de yan-zi-ya  X  X bout forty li away de Swallow Cliff X ) contains a numeric ( si-shi-lai  X  X bout 40 X ) followed by a one-to-many character ( li ), which, in isolation, can be converted to TC as ( li , a Chinese unit of length of 500m), 3 or 4 ( li  X  X n, X   X  X nside X ). / ( X  X bout forty li away de  X ), as a phrase, is hard to be converted directly not because it is not a regular word but because the ( wai  X  X ut, X   X  X utside, X  or  X  X eyond X ) that follows the . In other words, without word space as in languages such as English, a conversion tool may easily take side and outside. X  It will then convert it into TC accordingly as outside X ), rendering the phrase the Swallow Cliff some forty li away meaningless X  X  situation we have found in the operation of tools such as MS office 2003. Even with the assistance of the n -gram statistical model, the problem still persists due to the high frequency of the word 3 . To solve this problem, Wikipedia uses an open re-source for developers to accumulate such ad hoc kind of phrases that may pass for legitimate words or phrases if parsed by the machine in a  X  X orrect X  but unintended way. For example, the TC 5 56 to match the SC 5 57 ( wu-tian hou  X 5 days later X ) is treated as a word for one-to-many character conversion, so as to obviate confusion of the combination with 57 ( tian-hou  X  X mpress of heaven X ) which is identical with its SC counterpart as a legitimate regular word [Wikepedia 2011]. However, the dis-tinction between an ad hoc phrase and a regular one in such cases depends on the presence of a numeric in a non-word combination, which in theory can render a list of numeric-led combinations endless and the sheer size of such ad hoc phrases can slow the conversion efficiency dramatically.

We thus propose a pattern-based conversion model to identify and convert such phrases, as a complementary conversion method to the revised N-Gram model. Each pattern is derived from a generalization of ad hoc phrases of similar structures which contain one-to-many characters. Table II shows two groups of such kind of phrases and their corresponding patterns.

With a phrase 89:;&lt;=&gt;? @ ( X  X he Japanese are some dozens of li away from Guiyang X ) in the table as an example, the pattern generation method including candidate generation, pattern mining and merging is described as the following steps.
 This step first identifies phrases in TC which contain both one-to-many characters and numerics. The numerics are then replaced with specific tags. For example, in the example phrase is replaced and tagged as [ num ] thus the sentence is changed to 89:;&lt;=&gt; [ num ] @ .

By treating the one-to-many character in the above sentence as the basis, the structure can be augmented each time with an extension that covers the character immediately preceding/following the structure proper. After that, the augmented ver-sions of the structure are filtered by the constraint of the numeric tag [ num ]. That is, only the versions that contain both the one-to-many character and the number tag are kept as pattern candidates in TC. A pattern candidate in TC and a pattern candidate in SC using the same generation method in turn form a complete pattern candidate. For the example given above, there are a total of 20 structures generated and 18 complete pattern candidates produced, as listed in Table III below.
 Apriori is a classic algorithm for learning association rules and is designed to operate on databases containing transactions (for example, collections of items bought by cus-tomers, or details of a website frequentation) [Agrawal and Srikant 1994]. However, conversions between TC and SC are not always a transfer between two equivalents and one-to-many conversions are mainly those from SC to TC. As such, the Apriori cannot be applied directly because transactions in the traditional Apriori algorithm do not contain any sequence factor. Therefore, we modify the Apriori so that it can be applicable to character conversion following the sequence from SC to TC in complete pattern candidates as well as a sequence factor in calculations to identify frequent can-didate items. The metric of  X  X upport X  for a pattern candidate p is revised and defined as the proportion of pattern candidates which contain p over all pattern candidates. Since a candidate in SC may have several candidates in TC, the  X  X onfidence X  calcula-tion of conversion from a pattern candidate p SC in SC to a pattern candidate p is revised and defined as the proportion of the support of conversion from p over the maximum of those support values. The formulas of support and confidence are presented below as Equations (6) and (7), respectively.

In learning frequent pattern candidates, that is, those whose occurrences are over a threshold, only one or two frequent candidate items are considered since Chinese character conversions are merely between SC and TC. Let X , Y be any two pattern candidates, and observe that if X  X  Y ,then Support ( X )  X  to the following two corollaries: 1) If X is frequent, then any subset Z frequent; 2) If X is not frequent, then any superset Y  X  X cannot be frequent.
With those two corollaries, the learned pattern candidates with one candidate item can be filtered by the support threshold thus the candidates whose support is lower than a support threshold T s can be removed to reduce calculation complexity. The obtained pattern candidates with two candidate items can be further evaluated by a confidence threshold T c in order to ensure the reliability of the structure conversion. The pattern candidates that fit both support and confidence thresholds are the learned patterns in this process.
 With the same thresholds of support and confidence, the learned patterns may overlap sometimes. For example, both  X  &lt; SC &gt; [ num ]  X  &lt;
SC &gt; &gt; [ num ]  X  &lt; TC &gt; &gt; [ num ]  X  are frequent patterns that satisfy the thresh-olds with the former as the structural base for the latter. Therefore, overlapping patterns can be merged to improve the matching and conversion efficiency without compromising on precision since all the learned patterns have cleared the thresholds of support and confidence. The merging is evaluated by the calculation of coverage value, which is defined as the proportion of matched sentences by a pattern in all sen-tences. The selection of p SC or p TC depends on the language of the training corpus for pattern generation. The related formula of coverage is presented as Equation (8) below.

Patterns with larger coverage can match more sentences thus can be more versatile in converting ad hoc phrases. Therefore, a merger always takes place in the form of the secondary one merging into its primary parent which has a larger coverage value. In the case mentioned above, the coverage of  X  &lt; SC &gt; [ num ] while that of  X  &lt; SC &gt; &gt; [ num ]  X  &lt; TC &gt; &gt; higher coverage and is the final learned pattern.

In this section, to make the FCMD more efficient, we divide the FCMD matching into three main procedures. The first procedure is to convert one-to-one characters and tag one-to-many characters by reverse maximum matching over all the data resources according to priorities, so that the following steps can concentrate on matching and converting one-to-many characters. In the second procedure, the matching calculation takes into account both priority probability and n -gram-based probability. The entry identified with the highest probability is chosen as the source string X  X  most viable match in the target. If there is no n -gram matched for a one-to-many conversion, then in the third procedure, the FCMD will try to identify the ad hoc phrases and use the pattern-based model to convert characters by previously learned conversion patterns. Based on the proposed FCMD model, we designed and implemented a Professional Chinese Character Conversion Platform (PCCCP), or the Platform in this article which consists of six major modules: automatic conversion, advanced conversion, data re-source management, corpus training, pattern learning, and conversion result compar-ison modules. The platform currently supports three languages: English, Simplified Chinese, and Traditional Chinese, which are assumed to be the most common working languages of target users.

When using the platform, a user may either copy text or load a document file into the textbox on the left-hand side. Two conversion options are provided, viz. Automatic and Advanced. Figure 2 presents an example of Automatic Conversion, in which the converted characters are flagged in green for ease of identification. Whether to mark out conversions or not can be decided by the user by clicking on the button  X  X se color to distinguish X  at the bottom of the interface.

In addition to the Automatic, the Advanced conversion is designed for professional or more region-specific uses. Unlike the Automatic, the Advanced marks out conver-sions in a system of four colors: namely green to stand for one-to-one automatic con-versions, yellow for automatic one-to-many conversions which may need verification or rectification by the user, blue for revisions or rectifications done by the user in the current operation, and gray for automatic punctuation conversion such as from the SC quotation marks  X  X nd X  to its TC counterpart of and . As each color flags a specific category of conversions for the user X  X  attention to make manual revisions easy, the user can remove each color by clicking on the corresponding color button at the bottom of the interface to endorse conversions of the category after necessary verification or rectification. Revisions made by the user are then automatically added to the user X  X  personal term bank as legitimate conversion candidates for future use. Aided by the highly flexible data resource selection, the Advanced is particularly helpful for profes-sional and expert users.
 The operation of the Advanced conversion is outlined step by step as follows (see Figure 3 for the interface). (1) Tick the  X  X dvanced conversion X  box to activate the option. (2) A list of data resources appears as a popup menu beneath the box for the user to (3) The selection menu becomes hidden once the selection is done and the conversion (4) In the manual revision mode the user can correct conversion errors directly in the (5) Clicking on the Search button will take the user to the character/word on the Baidu (6) The user can choose to tick the  X  X pdate all similar X  and all cases similar to the (7) The user can also choose to tick the  X  X pdate my dict X  box to add the current cor-
The Platform also allows users to view and maintain the data resources by using functions such as  X  X nsert, X   X  X elete, X   X  X ave, X   X  X ave as, X  and  X  X earch. X  In addition, since certain characters or words within the same system may have two or more formal vari-ations that look closely alike, for example, w and a font enlargement function is provided for convenience of distinction, which can be ac-tivated by clicking on the character directly. The related user interface of this module is shown in Figure 4.

Besides the above functions for general use, the Platform has the following modules to cater to more specific operations.

It allows users to extract n -grams themselves by training corpora. The corpora re-quired for the training have to be in TC since n -gram is mainly used to convert one-to-many cases from SC to TC. The training can either be based on the previous trained result or conducted as a totally new process. The user can choose to manage the trained grams independently or merge them with the existing trained n -grams. It allows users to test and compare the performance of the four conversion methods: viz. maximum reverse matching, maximum reverse matching + n -gram, maximum reverse matching + n -gram + pattern, and Microsoft Office 2010. The running time of each operation is reported for users X  reference. A working interface is shown in Figure 5.

The platform also provides a pattern learning module to users who want to learn patterns specific to the documentation of their professional subject areas through cor-pus training. This is done by importing relevant training corpora or documents in TC and clicking the button  X  X enerate candidates X  to generate candidates on the sentences of the training corpus. These candidates are then filtered by their calculated support and confidence values compared with the predefined thresholds. The users can set the support and confidence thresholds to adjust the obtained patterns accordingly. With the coverage calculation, as discussed above, the generated patterns can be merged by clicking on the button  X  X erge pattern by coverage calculation X  to finalize the patterns. A working user interface is shown in Figure 6.

The platform also contains a conversion result comparison module to generate statis-tical reports to help evaluate conversion performance of a specific conversion method, for example, the FCMD, by comparing the conversion result with the ground truth text. It can also be used to compare the conversion differences between different conversion methods. All differences are highlighted in respective colors in the corre-sponding versions of the document for ease of identification with a detailed statisti-cal report presenting the differences regarding: number of total characters, number of different characters, proportion of differences, and overall conversion precision. Moreover, the user can select to generate a focused report on one-to-many related statistics, for example the number of total one-to-many characters, the number of in-correctly converted one-to-many characters, and precision of conversion of the one-to-many characters involved in the operation. A working user interface is shown in Figure 7 below.

With the six main modules and the corresponding user interfaces, it is hoped that the platform can provide both general and professional users with a convenient and reliable tool for converting Chinese documents from SC to TC, or vice versa. As mentioned above, the proposed FCMD model requires three training operations. The first one is to calculate the n -gram-based probability, which needs to acquire words containing one-to-many characters with their frequencies of occurrence. A very large corpus, Tagged Chinese Gigaword Version 2.0 [Huang 2009], is used to train the model. It contains a total of more than 10 GB data or more than 2 million documents in TC. The data quality is also ensured [Huang et al. 2008]. The content corpus is described in Table IV below.
 This training only extracted words containing one-to-many characters for efficiency. Since these characters are of 378 groups involving 814 of such characters in total, the training time is acceptable. We extracted 129,431 words (in which a one-to-many character may occur at the beginning, in the middle, or at the end of a word/phrase) with their frequencies of occurrence. Table V below shows some of the trained results.
The purpose of the second training operation is to determine the optimal parame-ter  X  , which is a balance weight between data resource matching and n -gram com-putation. Experimental data is from the complete Wang and Wang X  X  271 sentences in their one-to-many dataset [Wang et al. 2005]. The dataset is mainly focused on one-to-many cases which are prone to incorrect conversions. Each one-to-many case is clearly tagged and verified thus it is suitable to be used as an experimental dataset, which is also adopted in Li X  X  experiments [Li and Wu 2010]. In our training of the FCMD, 30 sentences from the experimental dataset are used as the training dataset. The best weight has been established by using the comparison module described in Figure 7, and comparing the conversion performances observed each time with a different value of the  X  . When the  X  is 0.89, the best performance on these 30 sentences are obtained. Therefore, we used this value in the later experiments.

The third training operation is to determine the best parameter of the support threshold T s and confidence threshold T c , described above in the section on pattern mining based on the revised Apriori algorithm, so as to learn more applicable pat-terns. We randomly select a document from Tagged Chinese Gigaword and generate pattern candidates with different thresholds by the pattern generation method de-scribed earlier on. With manually checking and evaluating, the support threshold T is set at 0.0002, which means the same candidate occurs at least 50 times in the doc-ument. Since patterns are used to convert one-to-many characters with determinate conversion structures to further solve ambiguous cases, the confidence threshold T set at 0.95, which means there is at least 95% probability that a structure is converted to its defined corresponding structure containing a one-to-many character. To learn conversion patterns, we use the same Tagged Chinese Gigaword as training corpus. Take the newspaper Zaobao (December 2004) for example. With the pattern generation method, our system processes and extracts 3,181,287 pattern candidates which contain both numerics and one-to-many characters. The support threshold is trained and the best value is 0.0002, which means a same candidate occurs at least 65 times in the document. The 65 times is larger than the value obtained in the training process (50 times) because of the larger quantity of phrases in the corpus. There are a total of 3,725 pattern candidates that have cleared the support threshold. After that, with the trained confidence value 0.95, 1,240 patterns are obtained. After merging with coverage calculation, 128 patterns are acquired finally. The learned patterns contain both patterns in SC and TC with one-to-many characters and numerics. They can thus be used for one-to-many conversion on the structural level. Some of the patterns extracted from this learning process are listed in Table VI below.
 A total of seven data resources are used for our Chinese character conversion experi-ments. Details of the data resources and the numbers of entries are listed in Table VII below. It is worth noting that only entries containing one-to-many characters are col-lected from the Comprehensive Chinese Dictionary .

The evaluation method is based on the precision calculation defined in Equation (9), which is a commonly used mechanism for information retrieval. In this formula, the  X  X ne-to-many cases X  indicates all the one-to-many characters in the testing data and the  X  X orrectly converted cases X  is all the correctly converted one-to-many characters in the conversion result. As mentioned earlier on, the remaining 241 sentences in Wang and Wang X  X  published one-to-many dataset [Wang and Wang 2005], after 30 sentences already being chosen as our training data, are used as testing dataset in the first experiment.
To test its performance against the state-of-the-art methods, Li X  X  method, which is reported to have produced much better results than other tools [Li and Wu 2010], and three very popular tools used in the industry, that is, Google Translate [Google 2011], Microsoft Word 2003, and Word 2010, are selected as the baseline, together with STGuru [Anasoft Studio 2011]. STGuru is specifically incorporated as it claims to  X  X ave a leading intelligent code conversion engine, and provide a full series of code conversion services of professional quality. X  In our experiment, all these systems have been used to convert the SC testing data into TC and results are compared against the ground truth, which is also provided by Wang and Wang X  X  verified data [Wang and Wang 2005]. To further compare the functions of the FCMD, we use FCMD( n -gram) to indicate operation of the FCMD with reverse maximum matching and n -gram-based conversion functions, and FCMD( n -gram + pattern) to indicate operation of the FCMD with reverse maximum matching, n -gram-based, and pattern-based conversion func-tions. The experimental results and comparison are shown in Figure 8 below. From the figure we can see our proposed FCMD( n -gram) method attains 88.6% precision and the FCMD( n -gram + pattern) method attains 91.5% on one-to-many conversion, which is the most difficult part of the task. The precision rate of Google translate, Li X  X  method, STGuru, and Word 2010 are 84.6%, 85.6%, 82.4%, and 67.3%, respectively. Word 2003 has the lowest rate of 66.6%. Therefore, our method outperforms all the other systems on one-to-many character conversion.

We also test the systems on characters other than the one-to-many ones. A docu-ment from the National Palace Museum website [2011] was selected as the testing data since it provides standard SC and TC version. The evaluation method is to calcu-late the probability of correct conversion on all characters therein, which is similar to Equation (9). Experimental results on this document show that the overall conversion precision of the FCMD( n -gram) and the FCMD( n -gram + pattern) reaches 99.68% and 99.8%, which is slightly higher than the other systems. Detailed results are shown in Table VIII below.

The efficiency of our FCMD has also been evaluated and compared with Li X  X  method, which is regarded as the best method prior to the FCMD. I Ching , a classic text in Chinese philosophy, was used to test both systems. This text contains 29,264 Chinese characters and the size is 73.4KB. Each algorithm converts it from SC to TC six times on the same computer and removes the records with the highest and the lowest running times to get more stable evaluation results. The remaining four running times are shown in Table IX. From the table, we can see that the average running time of the FCMD( n -gram) is 0.262s, less than Li X  X  0.2656s with an improvement rate of 1.3%. Moreover, the efficiency of the FCMD( n -gram) can be further improved with the assistance of the data resources that contain one-to-many words, since data-supported conversion can be more straightforward and further reduce the volume of computation caused by n -gram calculation. However, the average running time of the FCMD ( n -gram + pattern) is 0.3477s, which is much more than that of the FCMD( n -gram). It shows that pattern matching and the subsequent processing of matched conversion patterns require much calculation time, especially when dealing with a sizable text (in this case one of more than 29,000 characters). But even converting such lengthy documents, users would not normally notice a difference of, say, 0.0857s in running time. The platform, to be sure, allows the user to disable the pattern matching function in cases where the document to be converted is huge in size (e.g., of 1 million characters) and accuracy is not a vital imperative. Chinese character conversion is becoming increasingly important in facilitating com-munication between Chinese communities. According to our observation, conversion tools currently in use tend to fall short of professional standards in terms of preci-sion especially when the conversion involves one-to-many cases. This article proposes a new priority-based data resource management model operating under two priori-ties: preference priority and authority priority, together with a new algorithm called  X  X used Conversion Algorithm from Multi-Data resources X  (FCMD) based on reverse maximum matching, a revised n -gram-based statistical model, and a pattern-based conversion model. The experimental results show that, our conversion method can ensure better character-conversion quality in terms of precision and is presumably more responsive to regional character variations with the support of multiple data re-sources and by dealing with one-to-many conversion complexity not only on the char-acter but also on the word level. After being trained on a large-scale corpus, the FCMD registered a precision rate of 91.5% on one-to-many conversion and 99.8% on overall conversion. It is thus outperforming the state-of-the-art baseline methods. The effi-ciency in terms of running time has also been observed. Based on the FCMD, a Chi-nese character conversion platform (PCCCP) is implemented with six major modules to cater for serious professional as well as general uses with a series of functions in an advanced conversion mode alongside the automatic one.

Tests reported in this article have shown that the Platform has produced more sat-isfactory results than those conversion tools available in the market at present. To continue to enhance its performance as an on-going project, semantic analysis, among others, is currently being considered as a further mechanism to increase conversion accuracy.

