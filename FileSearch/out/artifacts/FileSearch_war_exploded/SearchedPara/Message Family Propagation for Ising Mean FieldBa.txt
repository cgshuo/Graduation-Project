 Ising mean field is a basic variational inference method for Ising model, which can provide an effective approximate so-lution for large-scale inference problem. The main idea is to transform a probabilistic inference problem into a func-tional extremum problem by variational calculus, and solve the functional extremum problem to obtain approximate extremum is an important step and a computational core for eration methods make the variable information intercross with each other deeply. From the view of incomplete varia-tional iterations, we propose a message family propagation method for Ising mean field to compute a marginal distri-bution family of object variable.

First we define the concepts of iteration tree and prun-cess of Ising mean field inference. Then we design the mes-sage family propagation method based on the iteration trees . The method propagates mean field message families and be-lief message families from bottom to top of the iteration in root node. Finally we prove the marginal distribution bound theorem, which shows that the marginal distribution family computed by the method in the pruning iteration tree contains the exact marginal distribution. Theoretica l and experimental results illustrate that the message famil y propagation method is valid and the marginal distribution bounds are tight.
 duction and Theorem Proving X  Uncertainty, probabilistic reasoning, and  X  X uzzy X  Theory Ising Mean Field, Iteration Tree, Message Family Propaga-tion
Ising model is a particular kind of Markov random field model over Bernoulli random vector [13]. The model pro-vides a natural way to describe real systems, and has wide application in imagine analysis and natural language pro-model are intractable, various approximate inference meth -ods are developed [3, 1, 2, 18]. Ising mean field is a basic variational approximate inference method in Ising model, which is to transform a probabilistic inference problem int o a functional extremum problem by variational calculus, and solve the functional extremum problem to obtain approxi-mate marginal distributions [20, 19, 7]. This method plays ational formulation and low computational complexity.
The process of solving the functional extremum is an im-portant step and a computation core for variational infer-ence. Traditional variational methods solved the function al extremum problem by corresponding Euler equation, and calculated approximate marginal distributions through th e intercross with each other deeply, and new training data would be difficult to add in. And then approximate vari-ational inference methods were developed based on incom-plete variational iterations [17, 4].

Sutton first provided the local training method based on incomplete belief propagation [16, 17]. The method exe-cuted the early belief message iterations in graph model to compute the approximate marginal distributions, and then trained the model parameters over each clique inde-pendently. It had lower computational complexity and was convenient to adding new training data, but it was lack of presented a method to compute the accuracy bound on ap-proximate marginal distribution of belief propagation bas ed on self-avoiding walk tree (BP-SAW) [5, 4]. First it imple-mented the belief propagation method in SAW tree to com-pute approximate marginal distributions; then propagated the message error bound in the SAW tree to calculate the accuracy bound of the approximate distributions. The BP-SAW method provided a way to analyze the error of finite belief message iterations; but the bound was determined by a fixed SAW tree, and was lack of flexibility for error bound control [5, 4].

According to the above analysis, current approximate vari-ational methods make use of incomplete message propaga-tion to compute the approximate marginal distributions, and they mainly focus in the belief propagation method, pay little attention to other variational methods, such as mean field method. Besides, the error analysis is the key problem for the incomplete computation process. But the local train -ing method doesn X  X  analyze the computation error; and BP-SAW X  X  message error bound propagation process has weak flexibility for accuracy control.

Instead of conventional incomplete message propagation, we propose a message family propagation (MFP) method for Ising mean field, which propagates message families to calculate a marginal probability family of object variable ; and also derive a marginal distribution bound theorem of the MFP method. First, we define the concepts of iteration tree and pruning iteration tree to describe the incomplete computation process of Ising mean field. Then we design the MFP method based on the iteration trees, which prop-agates message families in the trees to compute a marginal probability family. Finally we prove the marginal distribu -tion bound theorem, which shows that the marginal distri-bution family of MFP in pruning computation tree contains the exact distribution.
In this section, we introduce the Ising model and Ising mean field inference method.
The Ising model is a probabilistic model established on graph G = ( V,E ), where the vertex set V corresponds to a and the edge set E reflects the conditional independences among variables [13]. The exponential probability distrib u-tion of Ising model is function.
For Ising model p ( x ;  X  ), the key problem of probabilistic important approximate inference method, which transforms a probabilistic inference problem into a functional extrem um tremum problem to obtain the approximate marginal dis-tributions [19]. The functional extremum problem of A (  X  ) based on variational inference is is the constraint set of free distribution.

Since it is difficult to compute the functional extremum problem (2) exactly, Ising mean field inference method is proposed based on tractable constraint set S , where the Q tremum problem of A (  X  ) based on Ising mean field is 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; :
A (  X  ) = sup
F ( q ( x )) = X
S = n q ( x ) | q ( x ) = According to Euler-Lagrange equation [9], we can obtain m distribution iterations for the functional extremum probl em (3), and the iteration of the probability distribution q  X  is q  X  ( c  X  )  X  exp Where, 2) E ( c  X  ) is the edge set based on the variable cluster c
According to the iteration equation (4), we can compute approximate marginal distribution conveniently.
In this section, we propose the concepts of iteration tree and pruning iteration tree to describe the iteration compu-tation process of Ising mean field.

Definition 1. For Ising mean field inference method with tion tree for variable cluster c  X  is a 4-tuple Where 1) D  X  : variable cluster set with root node c  X  . 2) R : relation set.
 3) M  X  : mean field message set.
 4) Q : probability distribution set.

The iteration tree T ( D  X  ,R,M  X  ,Q ) has unrolled the it-eration computation process of Ising mean field inference method from some variable cluster c  X  . Let root node c  X  initialize the leaf nodes with mean field messages. Then equivalent to the probability distribution at node c  X  on the
Pruning iteration tree is a special type of iteration tree, which is constructed by pruning the backtrack cluster node in the iteration tree.

Definition 2. For Ising mean field inference method with free probability distribution q ( x ) = Q m  X  =1 q  X  ( c  X  ciated pruning iteration tree for variable cluster c  X  is a special of the iteration tree. Where
The pruning iteration tree avoids the self-updates of iter-ation process by pruning the backtrack cluster nodes, and it has a maximum depth m , where m is the number of vari-able clusters of free distribution. For the pruned node, we should initialize the mean field messages from the pruned node to its parent nodes. Then, for the depth d pruning it-messages: the mean field messages to the leaf nodes and the mean field messages from the pruned nodes.

Example: For the 3  X  3 two-dimension lattice Ising model, as shown in Figure 1, we establish the iteration trees and pruning iteration trees based on two different Ising mean field methods, as shown in Figure 2 and Figure 3 respec-tively.

Figure 2(a) is the free distribution structure of the Ising mean field method based on full factorial variable cluster construct the associated depth d = 3 iteration tree with x as root, as shown in Figure 2(b). The nodes x 1 in the third layer of the iteration tree repeat the root node, and they are the backtrack nodes of root node; then we prune the backtrack nodes and the corresponding descendant nodes, and obtain the depth d = 3 pruning iteration tree, as shown in Figure 2(c).

Figure 3(a) is the free distribution structure of the Ising mean field method based on 3  X  1 factorial variable clus-ter set {{ x 1 ,x 2 ,x 3 } , { x 4 ,x 5 ,x 6 } , { x 7 ,x { x 1 ,x 2 ,x 3 } ,c 2 = { x 4 ,x 5 ,x 6 } ,c 3 = { x 7 ,x 8 ,x Ising mean field method, we establish the associated depth the iteration tree is the backtrack node of the root node, we should prune the node and corresponding descendant nodes. tree is also backtrack node, and we should prune the node and the corresponding descendant nodes. Finally we gener-ate the associated pruning iteration tree with depth d = 3, as shown in Figure 3(c). We can see that the pruning iter-ation tree has the depth d = 3, which is the maximum tree depth equal to the variable cluster number m = 3.
In this section, we describe the MFP method based on the iteration tree, which is to propagate message families from bottom to top in the iteration tree, and compute a marginal distribution family of variable in root node.
Probability distribution family is a set of probability dis -tributions on random variables. For Bernoulli random vari-bution with single variable cluster set {{ x 1 } , , { x n 3 pruning iteration tree for cluster { x 1 } . c 1 c 2 distribution with disjoint variable cluster set {{ x 1 ,x } , { x bility distribution family on x i , that is S ( p ( x i )).

Now we describe the basic operations on probability dis-tribution family. For non-negative function set T = { f ( x on f ( x i ) with N ( f ( x i )) = f ( x i ) / P x {N ( f ( x i )) | f ( x i )  X  X } .

Operations on probability distributions. Let  X  i ( x i ) = tribution families on x i ,x j respectively, and p 1 ( x i M ( p ( x i )), p 1 ( x j )  X  X  ( p ( x j )), then Operations on probability distribution families. The product of the elements of the two families: S ( p ( x i )) S ( p ( x j )) = The basic properties of convex probability distribution fa m-ilies are described by the following lemma.

Lemma 1. Let  X  i ( x i ) = exp(  X  i x i ) ,  X  ij ( x i ,x denote convex families of probability distributions on x i 3) The partial summation of the product of potential func-
X
According to the mean field iteration (4) and the sum-product method [14], we can compute the approximate margina l gation c , and m ji ( x i ) denotes the belief message from x j to x cluster c  X  , that is 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; : variables x k  X  c  X  ,c  X   X  Ch( c  X  ) to variable x i  X  c  X 
As an example, for the variable cluster c 1 in the third cess of belief message propagation and mean field message propagation in c 1 is showed in Figure 4(a).

Instead of message propagation process, the MFP method basic idea is to propagate belief message families and mean compute a marginal distribution family of variable in root messages.

According to the Lemma 1, the process of message family propagation can be implemented through the extreme dis-tributions. And the mean field message family S ( m  X  ki ( x x  X  c  X  can be calculated by S ( m  X  ki ( x i ))=Cov  X  N  X  exp  X   X  ki x i X The belief message family S ( m ji ( x i )) from variable x in cluster c  X  can be computed by 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :
S ( m ji ( x i ))=Cov  X  N  X  X c  X  can be computed by 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; :
Continuing the above example, the process of mean field message family propagation and belief message family prop-agation in cluster c 1 is showed in Figure 4(b); and the pro-cess of corresponding extreme distribution propagation is showed in Figure 4(c).

For the depth d iteration tree T ( D  X  ,R,M  X  ,Q ), let c note the leaf cluster in T , c  X   X  Ch( c  X  ) denote the child iteration tree T , and the probability distributions of x M ( p ( x k )). And we can initialize the mean field message family from the child variable x k to leaf variable x i  X  c with ters with (8), we propagate the belief message families and mean field message families in the iteration tree T by the equations (5)(6)(7), and compute a marginal distribution the MFP algorithm is shown in Algorithm 1.
 Algorithm 1 : MFP algorithm Data : Iteration tree: T ( D c  X  ,R,M  X  ,Q ); tree depth: d .
Result : Marginal distributions:  X  S ( q ( x s )) | x s  X  c begin end
Theorem 1 (Algorithm Convergence). For the Ising mean field inference method with free distribution q ( x ) = Q eration tree, d the depth of the iteration tree, S ( q ( x marginal distribution family based on MFP algorithm in the convergence value of marginal distribution of the Ising mea n field method. Then the marginal distribution family S ( q ( x converges to q  X  ( x s ) as the depth d increasing
In this section, we prove the marginal distribution bound theorem. It shows that the marginal distribution families computed by the MFP method based on pruning iteration tree contain the exact marginal distributions.

Now we execute the MFP Algorithm 1 in the depth d tialize all the leaf nodes with (8), then propagate the mes-sage families in the pruning iteration tree, and compute the marginal distribution families of variables in cluster nod e. Then we have the following theorem: Theorem 2 (Marginal Distribution Bound ). For bution of variable x s . And for Ising mean field inference eration tree associating with the Ising mean field method, algorithm in T P . Then the marginal distribution family is a marginal distribution bound of p ( x s ) , that is
Proof. First of all, we prove the theorem in the tree structure Ising model. Suppose Ising model G is a tree structure model with positive parameters  X   X  st &gt; 0, and its probability distribution p ( x ) is The exact marginal distributions can be computed by the sum-product algorithm [19] 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; : where N ( s ) denotes the set of neighbors of node s , and m ts ( x s ) denotes the belief message function. The ratio of belief message can be computed by According to the Yound inequation and Bernoulli inequa-tion [8], we obtain the following relationship based on the parameter conditions  X   X  st &gt; 0 where
Let T P ( D c  X  ,R,M  X  ,Q ) be a pruning iteration tree based on full factorial mean field inference, which has the worst variational accuracy [19], the mean field message propaga-tion process is And the ratio of mean field message can be computed by m The child variable set Ch( t ) contains a backtrack variable x , which should be pruned, and the message m  X  st ( x t ) from the pruned variable x s to x t is an initialization message.
Apply the MFP method in the pruning iteration tree T P , and initialize the mean field message family associated with the pruned variable with For m  X  st ( x t )  X  X  ( m  X  st ( x t )), we have Propagate the mean field message family, we have where The mean field message family S ( m  X  ts ( x s )) is According to the equations (9) X (12), we obtain Then And therefore
When the model parameters  X  st take random values, the theorem can be proved according to the Yound and Bernoulli inequations in the same way.

For the loopy structure Ising model, the theorem can be proved based on the junction tree method in the same way.
In this section, we demonstrate our theoretical analysis with some numerical experiments, and compare the marginal distribution bounds of MFP algorithm and Ihler X  X  BP-SAW algorithm.
 For 3  X  3 two-dimension lattice Ising model, as showed in Figure 1, we generate the general Ising model G 1 by a ran- X   X  ij  X  (  X  2 , 0).

Experiment 1: For the Ising model G 1 , we first run the distribution, as showed in the Figure 2(b) and Figure 2(c); Figure 5: Convergence of MFP algorithm on iter-ation tree with d = 2 , 3 , 4 , where solid lines denote the intervals of marginal probability p ( x i ) = 1 with k = 2 , 3 , 4 from left to right, dots the exact probabil-ities.
 Figure 6: Marginal probability bounds on pruning iteration tree with d = 2 , 3 , 4 , where solid lines denote the bounds of marginal probability p ( x i ) = 1 with k = 2 , 3 , 4 from left to right, dots the exact probabilities. and run the MFP algorithm in those iteration trees to com-pute marginal distribution families respectively. The exp er-iment results of the probability p ( x i = 1) are showed in Figure 5 and Figure 6.

Experiment 2: For the attractive Ising model G 2 and re-pulsive Ising model G 3 , we first run the JT algorithm to compute exact marginal distributions, and execute the BP-SAW algorithm to calculate marginal distribution bounds. Then create the depth 3 pruning iteration trees based on full respectively, as showed in the Figure 2(c) and Figure 3(c); and run the MFP algorithm on the trees to compute cor-responding marginal distribution bounds. The comparison
From the Figure 5, we can draw that the marginal distri-bution families of MFP algorithm on iteration tree converge to the convergence values of associated mean field inference method as depth d increasing, but they don X  X  converge to the exact distributions. From the Figure 6, we can draw that the marginal distribution families of MFP algorithm on pruning iteration tree contain the exact marginal distributions, b ut they don X  X  converge, which verifies the Theorem 2.
Figure 7 and Table 1 are the comparison results of the marginal distribution bounds of MFP algorithm and BP-SAW algorithm. For the attractive Ising model G 2 depicted in Figure 7(a) and Figure 7(c), the MFP algorithm provides tighter bounds than BP-SAW algorithm; and the MFP al-gorithm based on 3  X  1 factorial free distribution presents (b) Comparison in repulsive Ising model G 3 , where MFP algorithm is based on full factorial free distribution (c) Comparison in attractive Ising model G 2 , where MFP algorithm is based on 3  X  1 factorial free distribution (d) Comparison in repulsive Ising model G 3 , where MFP algorithm is based on 3  X  1 factorial free distribution repulsive Ising model G 3 depicted in Figure 7(b) and Figure 7(d), the BP-SAW algorithm presents two wrong bounds on x 1 and x 7 . In addition, the MFP algorithm based on full algorithm, and looser bounds on three variables x 3 ,x 8 ,x And the MFP algorithm based on 3  X  1 factorial free dis-tribution shown in Figure 7(d), presents tighter bounds on all model variables. Meanwhile, the Table 1 shows that the MFP algorithm has higher efficiency than BP-SAW algo-rithm.

In summary, those experiments show that (1) The marginal distribution families of MFP algorithm (2) The marginal distribution families of MFP algorithm in (3) The MFP algorithm proposes more tighter marginal dis-(4) The marginal distribution bounds of MFP algorithm can
In this paper, first we have defined the iteration tree and the pruning iteration tree concepts to formalize the incom-we have proposed the MFP algorithm based on the iter-ation trees to calculate a marginal distribution family of object variable. The method provides an ingenious approx-imate strategy for marginal distribution computation; and has natural characteristics for the accuracy control, such as by controlling the depth of iteration tree and the structure of associated free distribution. Finally we have proved the marginal distribution bound theorem, which shows that the MFP algorithm can present an efficient distribution bound in pruning iteration tree.

Theoretical analysis and experimental results have eluci-dated that: the message family propagation presents a valid and convenient approach to computing marginal distribu-tion, and the marginal distribution bound is a tight and efficient bound for marginal distribution.
This work was supported in part by Natural Science Foun-dation of China under Grant No. 60678049 and Natural Science Foundation of Tianjin under Grant No. 07JCY-BJC14600. The authors would like to thank the four anony-mous reviewers for their detailed comments that helped to improve the quality of the paper. [1] C. M. Bishop. Pattern recognition and machine [2] A. P. Boedihardjo, C.-T. Lu, and F. Chen. A [3] Z. Ghahramani and M. J. Beal. Graphical models and [4] A. T. Ihler. Accuracy bounds for belief propagation. [5] A. T. Ihler, J. W. F. III, and A. S. Willsky. Message [6] T. S. Jaakkola. Tutorial on variational approximation [7] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and [8] J. Kuang. Applied inequalities . Shan Dong Science &amp; [9] D. Lao. Fundamental of variational calculus . National [10] D. J. MacKay. Information theory inference and [11] T. Minka. Divergence measures and message passing. [12] A. Moschitti. Kernel methods, syntax and semantics [13] G. Parisi. Statistical field theory . Addison-Wesley: [15] C. Sutton. Efficient training methods for conditional [16] C. Sutton and A. McCallum. Piecewise training for [17] C. Sutton and T. Minka. Local training and belief [18] H. Tong, Y. Sakurai, T. Eliassi-Rad, and C. Faloutsos. [19] M. J. Wainwright and M. I. Jordan. Graphical models, [20] J. Winn and C. M. Bishop. Variational message [21] E. P. Xing, M. I. Jordan, and S. Russell. A generalized
