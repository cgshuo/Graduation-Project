 1. Introduction
It is often said that we live in an information age. Efficient storage and retrieval of information is paramount. Given ad-vances in computer hardware, information storage space has receded as a major concern. On the other hand, the amount of information being produced continues to expand exponentially. As a result, the long established subfield of information re-trieval (IR) has taken on greater importance in the field of library and information science (LIS), as well as in computer sci-ence and other allied disciplines. Periodic assessment of the nature of this important area of research and practice is necessary to clarify its major focuses, its directions of inquiry, and how best to shape its future.

An indicator of the maturity of an area of inquiry is the growth in the number and quality of research publications ( Van den Beselaar &amp; Leydesdorff, 1996 ). Insight into the nature of a field can be gained by examining  X  X  X he publications produced by its practitioners. To the extent that practitioners in the field publish the results of their investigations, this mode for assessing the state of a field can reflect with great specificity the content and problem orientations of the group. Of the many ways that publications can be analyzed and counted, perhaps the most revealing kind of data are the references cited by the practitioner group in their publications X  X  ( Small, 1981 , p. 39).

A field, discipline, or other area of study can be broadly divided into an intellectual base and current research fronts ( Chen, its intellectual base) can inform us about current research fronts. It is through references to sources that authors make con-nections between concepts ( Small, 1981 ). Collectively, such connections create a  X  X  X epresentation of the cognitive structure of the research field X  X  ( Small, 1981 , p. 39).  X 
Analysis of citation data helps define the field at issue by highlighting various areas of research and its future directions ( Small, 1981 ). Although a number of studies have examined the literature of library and information science (e.g.,  X str X m, 2007, 2010 ; Chen, Ibekwe-SanJuan, &amp; Hou, 2010; Cronin &amp; Meho, 2007; Donohue, 1972; Harter &amp; Hooten, 1992; Harter,
Nisonger, &amp; Weng, 1993; Persson, 1994; Rice, 1990; White &amp; McCain, 1998; Zhao &amp; Strotmann, 2008a, 2008b ), the nature of the literature concerning information retrieval has not been so thoroughly investigated ( Ding, Chowdhury, &amp; Foo, map the intellectual and cognitive structures of the IR subfield. By presenting various visualizations of citation data from
Web of Science for a 10-year period (2000 X 2009), we attempt to identify and map out the IR subfield X  X  co-authorship net-work, highly productive authors, highly cited journals and papers, author-assigned keywords, active institutions, and the im-port of ideas from other disciplines. We hope that our findings provide some insight into the nature of the body of IR knowledge and some guidance as researchers shape the future of the subfield. 2. Background 2.1. Information retrieval (IR)
Originally coined by Mooers (1951) ,  X  X  X nformation retrieval X  X  is now a standard and widely known term in the English lan-guage ( Saracevic, 1999 ). Information retrieval (IR) forms a major component of both the interdisciplinary field of information science and the discipline of computer science. Generally, IR is concerned with the representation, storage, organization, and information science as dealing, in the main, with practical and theoretical implementations.

Saracevic (1999) considers the problems tackled by information retrieval practitioners at the core of information science, although information science is much more than IR. That information retrieval is a core subfield of LIS has been confirmed by a recent study ( Zhao &amp; Strotmann, 2008 a). Although the early years of IR research focused on retrieval systems and methods, researchers have since incorporated the cognitive, interactive, and contextual aspects of information seeking and searching into IR research and system design ( Saracevic, 1999 ). In doing so, the IR subfield expanded to address issues related to users, cluster into two subclusters: systems-centered and user-centered ( Harter, 1992; Saracevic, 1992; White &amp; McCain, 1998 ).
With a critical mass of researchers and practitioners from a variety of fields working on IR topics by the early-to mid-1950s, the subfield further coalesced as a coherent entity with the rise of well-regarded retrieval system evaluation exer-cises/campaigns such as the Cranfield evaluation (see Cleverdon, 1987 ) and the Text REtrieval Conference or TREC ( http:// trec.nist.gov/overview.html )( Saracevic, 1999 ). 2.2. Visualization of the intellectual structure of a field
Visualization is often to present, to the user/searcher, the results of information retrieval queries from large collections of information sources. Cluster analysis, multidimensional analysis, and other techniques have been used in the visualization of used cluster analysis to study prominent researchers and publications, trends and evolution of specific science and engineer-ing fields, and trends in the growth of scientific and publishing activities (e.g., Sharabchiev, 1989 ). More recently, White (2003) used the Pathfinder Networks technique to map the structure of the LIS field from White &amp; McCain X  X  data (1998) .
Newer tools for visualizing a discipline X  X  research fronts and intellectual bases X  X ncluding CiteSpace ( http:// cluster.cis.drexel.edu/ cchen/citespace/), an open source application for visualizing patterns and trends in scientific litera-ture ( Chen, 2006 ) X  X ontinue to be developed. According to Chen (2006) , the trends and patterns of scientific literatures provide researchers or communities of similar interests an overview of the related field(s) and relationships among the specific fields. More specifically, such information as the most influential articles or books, the evolvement of terms, noun phrases, keywords, the most reputed researchers, the connection between different institutions and countries over time can show trends and patterns that provide more overview. This has made citation analysis and other scientometric techniques more powerful and useful.

Several reasons lead to increased interest in mapping information, including citation data, to study the structure and and accessibility of citation data in citation indexing services ( Bar-Ilan, 2008; Meho &amp; Yang, 2007 ) and other repositories computer applications (e.g., CiteSpace); and the need for inclusive and easy-to-understand methods for managing and com-prehending an ever increasing amount of electronic data.

Citation analysis and other bibliometric and scientometric methods started as rudimentary, pre-1960 techniques applied to only a few scientific fields. Today, they are full fledged systems applied to the study of both science and social science fields, encompassing the productivity, in terms of publications, of these fields, their researchers, and institutions. Citation analysis can be used in formulating science policies by research institutions, governments, and other funding agencies. It can also assist librarians and other information professionals by providing objective measures as they make decisions regarding collection development. Championing citation analysis does not mean that science policies and decisions about collection development should be based on citation analysis alone. We simply assert that citation analysis works such as this article have value beyond the academic purpose of advancing knowledge and generating novel information and data. 3. Related work
Our work continues the tradition of using citation analysis to study the structure and trends of the literature of a major field and/or its subfields. In that sense, it supplements works by others who studied the literature of information retrieval 2007, 2008; Donohue, 1972; Harter &amp; Hooten, 1992; Harter et al., 1993; Persson, 1994; Rice, 1990; Tang, 2004; White, 2003; ature of other LIS subfields such as image indexing and retrieval ( Chu, 2001 ) and human information behavior ( McKechnie et al., 2005 ); and the literature of science in general (e.g., Small, 1999 ). 3.1. Prior analysis sources
Previous studies of the literature of library and information science drew from a variety of citation sources. Some consid-paper in the LIS field was obtained from a set of journals that published information science-related articles produced by gov-ernment-funded research programs ( Donohue, 1972 ). More recently, Brooks (2000) focused his brief research communication on an analysis of the best articles in the Journal of the American Society for Information Science published from 1969 to 1996.
Others did not discriminate in journal selection but rather searched citation databases (e.g., Web of Science) using queries appropriate for their topic (e.g., Chu, 2001 ). Ellis et al. (1999) searched the Social SciSearch database and analyzed articles included in comprehensive reviews of research in three fields: user studies, information retrieval, and information systems. White and Griffith (1981) used a similar approach, basing their selections on a volume edited by the second author.
Ding et al. (2009) first searched citation databases (e.g., Web of Science) using queries appropriate for their topic, then selected only the most highly cited authors from the retrieved set. Others ( Chen et al., 2010; Zhao &amp; Strotmann, 2008 a) re-trieved citation data from the Web of Science for the 12 journals considered by White and McCain (1998) .

Borgman and Rice X  X  (1992) study used data from a core set of journals included in the ISI Journal Citation Reports (JCR) list for  X  X  X ibrary science and information science X  X  and another set of journals in the JCR list for  X  X  X ommunication. X  X  Tang (2004) randomly sampled 150 publications in library and information science. McKechnie et al. (2005) considered a sample of 155 papers from six prominent LIS journals that are international in scope and frequently publish work by human informa-tion behavior researchers. In their unique work, Cronin and Meho (2007) considered publications by authors who have won one or both of two prestigious LIS awards: the American Society for Information Science and Technology (ASIST) Award of
Merit and the Research in Information Science award. 3.2. Prior analysis time periods
The time periods of citation data considered by previous researchers also differ. Although ranging from 1 year of publi-cations ( Harter et al., 1993 ) to far more, the most common period for citation data was around 10 years. Most recently, anal-yses of citation data in library and information science have considered 10-year spans ( Zhao &amp; Strotmann, 2008a, 2008b ), a 23-year span ( Chen et al., 2010 ), and a roughly 30-year span ( Ding et al., 2009 ). 3.3. Stable, multidisciplinary field
Citation analysis of the literature of library and information science and information retrieval by previous authors re-vealed a number of important findings. The most prominent conclusions address the stable, multidisciplinary nature of the field. For instance, Persson (1994) found that, for a 5-year period (1986 X 1990), the intellectual base of the Journal of the American Society for Information Science (consisting of the most frequently co-cited authors) and its research front (con-sisting of articles sharing at least five cited authors) had similar maps (depicted in two-dimensional spaces). This finding highlights the stable nature of the topics explored by the information studies field.

White and McCain (1998) conducted an author co-citation analysis and used multidimensional scaling to produce two-dimensional maps of information studies literature for a 24-year period (1972 X 1995). They included papers by the top 120 authors in 12 library and information science journals. The same data set was analyzed by White (2003) who used the Path-finder Networks technique to produce a map of the field of LIS. These researchers found that LIS authors have trained in numerous fields in the arts and sciences and are affiliated with universities, public organizations, and private firms ( White, 2003; White &amp; McCain, 1998 ). In their findings, White and McCain (1998) observed early signs of a new focus by information retrieval researchers, as they put it, on  X  X  X ser-system relation. X  X  Their data revealed two main subfields, namely, library sci-ence and information science ( White &amp; McCain, 1998 ). A follow up work by Zhao and Strotmann (2008a) , using a different set of citation data, confirmed the two-subfield structure of LIS identified by White and McCain (1998) .

Ding et al. (2000) conducted one of the earliest studies on the literature of information retrieval specifically. They ana-lyzed co-citation data of 50 highly cited journals using multidimensional scaling and cluster analysis. They produced two-dimensional maps of the structure of the literature of information retrieval for an 11-year period (1987 X 1997). The visu-alizations revealed strong relationships between information retrieval and five other disciplines: computer science, physics, chemistry, psychology, and neuroscience. Their maps also show information retrieval as being part of both the computer sci-journals and publications.

Zhao and Strotmann X  X  (2008b) work extended White and McCain X  X  (1998) analysis into subsequent periods of citation data (1996 X 2005). Zhao and Strotmann (2008b) argue that library and information science has evolved into an interdisci-plinary field with five subfields: user studies, citation analysis, experimental retrieval, Webometrics, and visualization of knowledge domains. They confirmed White &amp; McCain X  X  finding about the then-emerging  X  X  X ser-system relation X  X  focus, claiming that user studies is now among the most researched subfields. This result indicates that LIS researchers appear to be paying more attention to what they preach X  X hat information system design should be user-centered.

Unlike earlier studies that used co-cited author data,  X str X m (2007) employed document co-citation analysis (DCA) which addresses co-cited references. He too identified the two subfields of LIS X  X nformation science and library science X  X hat White and McCain (1998) did.

Library and information science X  X  interdisciplinary reach also appears to be highly outward-looking. Tang (2004) identi-fied the most common disciplines to which LIS exports ideas (based on the number of citations it received from the disci-plines): computer science, communication, education, management, business, and engineering. Another study looked at sharply with the state of the field 20 years ago when few researchers from other disciplines cited LIS literature ( Cronin &amp;
Pearson, 1990 ). Conclusions about the broad reach of LIS are not completely unanimous, however. One relatively dated study drew a distinct boundary. Contrary to the common perception that linguistics plays a role in information studies research, linguistic theory was infrequently cited ( Warner, 1991 ). 3.4. Other findings
Other library and information science researchers have noted the lack of relationship between the nature of the funding that resulted in the publication of the article (whether it was funded or not) and its quality or utility (as measured by the fields of user studies, information retrieval, and information systems ( Ellis et al., 1999 ); and the predominantly single authorship, unusual length, and frequent self-citations of the best papers ( Brooks, 2000 ). Cronin and Meho (2007) identified two very broad categories of innovators in information science and concluded that chronological age alone is not a predictor of intellectual innovation. All in all, according to Cronin and Meho (2007) , creativity is not dependent on time and intensity. 3.5. Place of this work
As we briefly described above, a few researchers have used visualization and mapping techniques to study the literature the previous works. Earlier citation analysis studies consider either shorter time periods or focused on fewer publications ( Cronin &amp; Meho, 2008 ). They took into account only the most highly cited authors (e.g., Ding et al., 2009; Ellis et al., 1999 ) or journals (e.g., Ding et al., 2000 ).

This study responds to Chen et al. X  X  (2010) recent call for studies to confirm prior results (those based on a dataset of 12 or fewer journals), and inquiries that account for a broader context. Others believe that not enough research has been done to agree that the findings of previous authors should be examined on a regular basis. The changes in the structure of a discipline and its literature over time should be assessed continuously to identify trends and directions in terms of research topics, issues, methods, and theories.

Through the use of a novel visualization method based on CiteSpace, our work extends the efforts of previous authors by providing a fuller and more up-to-date picture of information retrieval. One aim was to investigate, using the visualization methods, whether the literature of information retrieval during the past 10 years exhibits characteristics typical of a subfield of an interdisciplinary field like library and information science. Our hope is that the findings of this study add to existing works and enrich our understanding of both the general field of library and information science and its information retrieval subfield.

Information retrieval has progressed by leaps and bounds due to the ubiquity and increased use of new technologies such as mobile devices, computers, the Internet, the Web, digital libraries, and other means of information access. The extent to which these new developments have changed the IR literature landscape is well worth investigation.
 4. Materials and methods 4.1. Materials
Our approach to data collection combines that of Borgman and Rice (1992), Chu (2001) , and Ding et al. (2009) . We first retrieved relevant records from the Web of Science using a query, then choose source articles from journals in specific 2008 Journal Citation Reports lists.

Our dataset X  X itation data on publications in information retrieval X  X as obtained by searching the ISI Web of Science during the last 2 weeks of March 2010. Web of Science encompasses the following seven databases: Science Citation Index Ex-ings Citation Index X  X cience (CPCI X  X ), Conference Proceedings Citation Index X  X ocial Sciences &amp; Humanities (CPCI X  X SH), Index
Chemicus (IC), and Current Chemical Reactions (CCR X  X xpanded). The following query was used on the TI = Title and TS = Topic fields: ((Information) AND (Index OR Retriev OR Organi?ation OR brows OR search ))
This query was chosen based on our understanding of the field of information retrieval and the research focus and re-search interest of this paper. The query was narrowed according to the following specifications: language  X  X  X nglish X  X  and doc-ument type  X  X  X rticles. X  X  Following the most common prior approach to choosing time periods, and focusing on most recent trends in the subfield, we choose a 10-year period from which to draw citations. We added the specifications: published be-tween the years of  X  X 2000 X 2009 X  X  and time span  X  X 2000 X 2009. X  X 
The query produced 56,160 records. This dataset was further narrowed by limiting it to those journals included in the 2008 Journal Citation Report under the subject categories of  X  X  X ibrary science and information science X  X  and  X  X  X omputer science, information systems. X  X  That curtailed the dataset to 48,390 records, which were retrieved and saved in a data-base. We reference this corpus of records as  X  X  X he information retrieval dataset X  X  or  X  X  X ur dataset X  X  in the remainder of this article.

While analyzing and reviewing data concerning the most productive authors in the information retrieval dataset, we found that some authors had multiple entries under variations of their names. In addition to the standard format of last name and initials, some entries included full names. Therefore, we re-processed the dataset after removing the  X  X  X F X  X  field and keeping only the  X  X  X U X  X  field. This re-processing was only for the author-related data analysis. For the rest of the data analysis, we used the original retrieved data. 4.2. Data analysis
This study X  X  main tool for the production of visualizations was CiteSpace ( http://cluster.cis.drexel.edu/ cchen/cite-space/), an open source application developed by Chen (2004a, 2004b, 2006) for the purpose of visualizing patterns and trends in scientific literature. CiteSpace was originally created to identify intellectual turning points ( Chen, 2004a, 2004b ). It does so by constructing co-citation networks among highly cited articles. Users can manipulate the Cite-
Space-produced graphical networks in a variety of ways, such as displaying multiple time periods and setting different thresholds.

CiteSpace visualization graphs are composed of nodes and lines connecting the nodes. The types of nodes include authors, noun phrases, keywords, institutions, cited authors, cited references, cited journals, grants, papers, category, and countries.
CiteSpace can be used to produce eight different visualization graphs representing the patterns of scientific literature. Users can specify the time period of the literature they want to search, choose the nodes, and set up thresholds all in the same screen. 5. Results and discussion 5.1. Authorship and co-authorship
Analysis of authorship and co-authorship is critical to the understanding of scholarly communication and knowledge dif-fusion ( Chen, 2006 ). We address both in this study. Our authorship analysis focuses on identifying the most productive authors in the dataset and their impact on the subfield (based on citations they received). We advance a dynamic structure for the contributing research community that forms a useful guide for new researchers as well as for those seeking potential collaborators and reviewers in multidisciplinary research areas.

Table 1 displays a ranked list of the most productive authors in the information retrieval dataset, together with their cur-rent or recent schools, institutions, and departments. The list includes the 32 researchers who published 10 or more papers in our list of journals during the 10-year period at issue (2000 X 2009). A paper is counted as published by an author regard-less of the order of authorship (first, second, etc.) and the total number of authors (sole authored, multi-authored). Roughly half of these authors are from computer science departments or schools; the other half are from information science programs.

To show the extent of collaboration by the most productive researchers in our dataset, we used CiteSpace to create a co-authorship network map. Two researchers have co-authorship if they have co-authored at least one paper together. Cite-
Space generates networks by measuring betweenness centrality scores.  X  X  X n a network, the betweenness centrality of a node measures the extent to which the node plays a role in pulling the rest of the nodes in the network together. The higher the centrality of a node, the more strategically important the node is. X  X  (Chen et al., 2009, p. 236).

Among co-authorship contributors, we measure productivity based on the number of papers they published as a co-author and as first author. An author X  X  impact is determined by the number of citations attributed to his or her name. The author co-citation analysis network helps highlight higher-order connectivity patterns among authors.

Fig. 1 shows the co-authorship network of information retrieval researchers in our dataset. This network is composed of several clusters grouping the most collaborative authors. The core researchers are at the center of the clusters; isolated authors and those at the periphery of the co-authorship clusters and/or the entire map could be considered less collaborative.

Two of the author co-citation clusters are large enough to contain at least eight members/authors. Two highly productive authors (see Table 1 ) anchor the two clusters: Jarvelin K and Chen HC. For instance, the cluster anchored by Jarvelin K shows co-authorship with Keskustalo H, Niemi T, Sormunen E, Ingwersen P, Kekalainen J, Airio E, Juhola M, Hansen B, Laurikkala J,
Hedlund T, and Pirkola A in the past 10 years. These results point to a relation between collaboration frequency and the most productive authors. We are tempted to conclude that the more actively an author collaborates, the more productive she or he is. Further research is necessary to confirm this assertion.

Overall, the co-authorship clusters are dispersedly distributed, which could be a result of dealing with two sets of authors, those from library and information science and those from computer science. Fig. 1 reveals that collaborations between the two sets of authors are not noticeable as evidenced by the fact that the two anchors of the two clearly identifiable clusters are from information science (Jarvelin K) and computer science (Chen HC) departments.
 5.2. Most cited publications
Chen, Cribbin, Macredie, and Morar (2002) showed that visualization can be used to track the development of a scientific discipline and present the long-term process of its competing paradigms. They also assert that, among a discipline X  X  co-cited publications, the cluster consisting of the most highly cited publications may represent the discipline X  X  core or predominant paradigm. At the very least, the number of citations a publication (a journal or article) receives is a primary indicator of its importance.
 Table 2 presents the journals most cited by information retrieval researchers in the 10-year information retrieval dataset. The top five are: J AM SOC INFORM SCI, 1900, SO, V, P; INFORM PROCESS MANAG, 1900, SO, V, P; COMMUN ACM, 1900, SO, V, P; J AM SOC INF SCI TEC, 1900, SO, V, P; and J DOC, 1900, SO, V, P. Two of these (J AM SOC INFORM SCI and J AM SOC INF SCI
TEC) are the same journal, revealing that the Journal of the American Society for Information Science and Technology (JASIST) is the primary outlet for information retrieval research and the dominant source of references for IR researchers. Among the top highly cited journals in Table 2 are those identified as important to information retrieval by Ding et al. (2000) : Journal of the American Society for Information Science (currently JASIST), Information Processing and Management (INFORM PROCESS MANAG), Journal of Documentation (J DOC), Annual Review of Information Science and Technology (ANNU REV INFORM SCI), Journal of Information Science (J INFORM SCI), and Proceedings of the Annual International ACM SIGIR Conference (P 21 ANN
INT ACM SIG). The stability and continued influence of many of the journals listed in Table 2 lends credence to the claim that information retrieval is a mature and stable subfield ( Van den Beselaar &amp; Leydesdorff, 1996 ).

Table 3 reveals the publications cited 60 or more times in our dataset. The list suggests several interesting features of the information retrieval subfield. The highly cited publications list is relatively top-heavy. The most cited publications are two popular books, published 16 years apart, by Salton and Baeza-Yates, respectively. These two sources account for 18.64% of the total citations received by the top 20 highly cited publications. Adding in the third most cited source, these sources ac-count for a quarter (25.47%) of the total citations received by this group.

Table 3 also shows a concentration of respected sources. The third most cited paper in the information retrieval dataset was published in Information Processing and Management (INFORM PROCESS MANAG), the second most cited journal (see Ta-ble 2 ). The seventh most cited publication is also from this journal. Five highly cited papers were published in what is now called Journal of the American Society for Information Science and Technology (J AM SOC INFORM SCI or J AM SOC INF SCI TEC).
Three appeared in Journal of Documentation (J DOC). The two most influential books were both written by Salton (SALTON G, 1983, INTRO MODERN INFORMA and SALTON G, 1989, AUTOMATIC TEXT PROCE).

The highly cited publications list indicates that computer scientists take the leading role in the information retrieval sub-field. The top three highly cited researchers in the dataset are Salton (26.1% of total citations received), Jansen (9.88%), and Baeza-Yates (8.38%) X  X ll from computer science.

Combined with Table 1 , the highly cited publications list reveals no meaningful relationship between recent productivity and impact. Several highly cited researchers (except for Jansen, who was the 8th highly productive author) were not among the most highly productive authors in the dataset. For example, Spink was the second most highly productive author during the 10-year period; her most highly cited paper ranked 10th in Table 3 . Contrarily, Porter placed 4th on the most highly cited list and does not appear on the table of most productive authors.

Table 3 mirrors the evidence in Table 2 (top journals) about the maturity of the information retrieval subfield. About 43% of the most cited papers were published between the years of 1970 and 1989.

The most cited publications list contains both single-author papers and co-authored papers. Although the presence of highly cited co-authored papers suggests that greater collaboration may lead to greater influence in the information retrieval subfield, collaboration is not a necessity for impact. Future research is necessary on the relationships between productivity, collaboration, and impact of authors.

A product of CiteSpace, Fig. 2 displays a document co-citation network, generated from the collective citing behavior in our information retrieval dataset. The network is composed of 121 reference nodes and 1163 co-citation links. A reference node is displayed in colored, concentric circles indicating the relative age of publications that cite that reference. The blue and green rings represent early citations, the yellow ring mid-period citations, and light brown and orange rings cover the most recent citations. A co-citation link is shown by a line between two reference nodes, such as the one that appears in the upper left of the diagram. A co-citation instance occurs when a single paper cites two given nodes; a co-citation link is drawn when at least two co-citation instances point to the linked reference nodes. The texts highlighted in Fig. 2 are the most cited papers listed in Table 3 .

Fig. 3a, b, c and d presents citation history graphs for the four most cited papers in Table 3 (the vertical axis is the number of citations and the horizontal axis is the year). For instance, Fig. 3c reveals that the overall citing trend for Jansen X  X  paper experienced a sharp upturn from 2004 to 2005, reaching a high of 24 in 2005. It is evident from Fig. 3a that Salton X  X  book has been cited at a fairly uniform level (between 15 and 26 times per year) throughout the dataset X  X  10-year period. This highlights its importance to IR researchers. The rising trend of citations to the relatively old source by Porter also shows its importance to IR. 5.3. Author-assigned keywords
Author-assigned keywords can reveal specific focus areas of research in a field. Table 4 presents the keywords most fre-quently assigned by the authors of our dataset and the measure of their betweenness centrality (Chen et al., 2009, p. 236).
As can be seen from Table 4 , the most frequently used keyword terms are  X  X  X nformation retrieval, X  X   X  X  X etrieval, X  X   X  X  X nforma-tion, X  X   X  X  X nternet, X  X  and  X  X  X ystems. X  X  Given that our search query to retrieve the dataset involved the terms  X  X  X nformation X  X  and  X  X  X etrieval, X  X  it seems unsurprising that these are the most frequent author-assigned keywords. Over the 10-year period, de-spite the dynamic changes brought on by new developments and new technologies, the dominance of the terms  X  X  X nforma-tion X  X  and  X  X  X etrieval, X  X  either together or separately, provides some indication of the stability and homogeneity of the field.
Some of the most frequently used keyword terms also had higher betweenness centrality values (see Table 4 ). For in-stance,  X  X  X nformation retrieval X  X  and  X  X  X nternet X  X  are both frequently assigned, and have the highest centrality values (Chen et al., 2009, p. 236).

Fig. 4 uses CiteScape to visualize the betweenness centrality values of the most frequent author-assigned keywords in the cates that other related terms have taken on a central role in the subfield:  X  X  X nformation seeking, X  X   X  X  X nformation system, X  X   X  X  X valuation, X  X  and  X  X  X ser studies. X  X  This highlights the rising emphasis on user-centered system design and retrieval, as well as the importance of user studies in the evaluation of IR systems. Information retrieval research is stronger today because it has increasingly focused on user-centered design. Current user studies research is more about  X  X  X sers X  interaction with information retrieval systems than about user information behavior in general X  X  ( Zhao &amp; Strotmann, 2008 a, p. 2077).
Fig. 4 also suggests that the information retrieval subfield has its own special areas of inquiry. Four main clusters can be discerned on the visualization map, centered around user studies, Web information retrieval, citation analysis/scientomet-rics, and information retrieval system evaluation. That user studies is one of the major clusters is fully consistent with high profile campaigns such as interactive TREC and the interactive track in INitiative for the Evaluation of XML Retrieval (INEX) ( http://inex.is.informatik.uni-duisburg.de/ ). On the other hand, we are surprised that cross-language information retrieval X  X n emerging subfield with a literature that is steadily growing partly due to information retrieval evaluation cam-paigns such as the Cross-Language Evaluation Forum or CLEF ( http://www.clef-campaign.org/ ) X  X oes not feature on the map. 5.4. Institutions of information retrieval authors
Table 5 displays the top 20 institutions of the information retrieval authors in our dataset. A plurality of these institutions are located in the United States (45.5%), and all but one (IBM Corporation) are affiliated with academic institutions and uni-versities. The top five institutions are University of Wisconsin X  X adison (Univ Wisconsin), Nanyang Technological University (Nanyang Technol Univ), University of North Carolina at Chapel Hill (Univ N Carolina), the Pennsylvania State University (Penn State Univ), and the University of Sheffield (Univ Sheffield).

Fig. 5 maps the collaboration between the top 20 institutions of information retrieval authors in our dataset. Four major clusters of collaborating institutions appear: (1) the University of Washington (Univ Washington) closely collaborated with Syracuse University (Syracuse Univ), University of Michigan (Univ Michigan), and the University of North Carolina at Chapel Hill (Univ N Carolina); (2) Georgia State University (Georgia State Univ) worked with the Pennsylvania State University (Penn State Univ) and the University at Buffalo (SUNY Buffalo); (3) Indiana University (Indiana Univ) collaborated with the Chinese University of Hong Kong (Chinese Univ Hong Kong) and Seoul National University (Seoul Natl Univ); (4) the University of
Arizona (Univ Arizona) engaged the University of Hong Kong (Univ Hong Kong). These diverse groupings indicate that the information retrieval subfield encourages collaboration across institutions and countries. As ever-increasing information generation has no borders and presents global challenges, information retrieval solutions require multinational efforts. 5.5. Other related disciplines
Information retrieval researchers in our dataset cite primarily computer science and library and information science pub-lications (see Table 6 ). Those two fields account for 82.79% of the citations. This high percentage confirms that information retrieval draws heavily from its broader field (LIS) and one with which it shares a number of journals and researchers (com-puter science). Apart from LIS and computer science, the third, fourth, and fifth other disciplines from which information retrieval imports ideas are engineering, telecommunications, and management, respectively. In fact, 91.6% of the citations by information retrieval authors whose articles were published between 2000 and 2009 were to these five disciplines.
We should note that the manner in which we retrieved our dataset may have contributed to this concentration. We nar-rowed our Web of Science search to those journals in the  X  X  X ibrary science and information science X  X  and  X  X  X omputer science, information systems X  X  categories. 6. Conclusions and recommendations
We analyzed citation data in the information retrieval subfield for the past decade (2000 X 2009) and presented the results in terms of co-authorship network, highly productive authors, highly cited journals and papers, author-assigned keywords, active institutions, and the import of ideas from other disciplines.

The conjecture that articles written by authors who collaborate often are also likely to be highly cited has been tested and Thelwall (2009) to mean co-authorship. That is, if an article has two or more authors, it is considered an output of col-laboration among its authors. Based on two parallel studies, Levitt and Thelwall (2009) concluded that, in general, although collaborative research among LIS researchers may lead to being highly cited, collaboration is not essential to increased cita-tion, especially for influential information scientists. A study by Persson et al. (2004) based on the entire Science Citation
Index (SCI) database for a 21-year period (1980 X 2000) found similar results. They showed that the average (mean) number of citations and the number of authors of articles in SCI have a linear relationship. According to our findings, although some of the highly productive authors had a larger co-authorship network and are at the center of co-authorship clusters, a num-ber of single-author publications were heavily cited. It seems that further analysis is required. Possible differences between various disciplines should also be considered in future inquiries.

The identification and study of the highly cited publications and researchers in a field is important because citation counts are objective measures of influence and impact ( Ding et al., 2009 ). Highly cited works and authors are often considered to tion, the structure of the co-authorship patterns of currently active authors in a field may provide clues as to the structure of
For this reason, we produced lists of highly cited publications and authors in IR during the last 10 years (2000 X 2009) as well as visualizations of collaborations among highly cited IR authors (see Figs. 1 X 3a,b,c and d and Tables 1 X 3 ). Our analysis of the co-authorship and collaboration patterns of IR researchers provides some indication of the connection between highly pro-ductive authors and those who exert more intellectual influence.

Generally, our findings about the most cited papers and journals parallel the findings of others (e.g., Ding et al., 2000, 2009 ). Still, slight differences appear, perhaps due to our dataset being broader (in terms of number of journals) than the datasets used by other researchers.

Others have previously found information retrieval to import ideas from and export them to computer science, commu-nication, education, management, business, engineering, physics, chemistry, psychology, and neuroscience ( Ding et al., 2000; puter science, library and information science, engineering, telecommunications, and management.

We also conclude that that the information retrieval subfield X  X  body of literature is expanding into areas not extensively covered during the years prior to 2000. One of our visualizations ( Fig. 4 ) suggests an expansion into the areas of Web infor-mation retrieval and user studies (although we were surprised that the visualization map did not clearly show more prom-inent research into cross-language information retrieval). This expansion may be due to the fact that, like human information behavior ( McKechnie et al., 2005 ), IR could be considered a second stage discipline (with LIS as the third or fourth stage dis-cipline). Second stage disciplines tend to show an exponential growth in their numbers of articles, researchers, publications, and topics ( Crane, 1972 ). 6.1. Limitations of the work and recommendations for future research
Our work and dataset are accurate and valid to the extent that the ISI Web of Science X  X  representation of the cited and citing authors and publications are accurate and valid. It has been noted before that the ISI Journal Citation Reports may not list some LIS journals under the category  X  X  X nformation science and library science X  X  ( McKechnie et al., 2005 ). As a result of this and other reasons, the visualizations and conclusions about the information retrieval research literature may be dif-ferent from those that consider a different set of journals. Our work simply depicts a snapshot of the intellectual structure of the information retrieval subfield for a period of 10 years. Other citation analysis methods and datasets should be explored to elaborate a more complete picture. In addition, as one of the reviewers pointed out, future researchers should also consider conducting analyses of the IR literature by selecting the same time span for the  X  X  X ited X  X  sources.
 Acknowledgement
We would like to thank M. Alexander Jurkat for able editorial assistance and extensive comments on an earlier draft of this article.
 References
