 Metaphor lends vividness, sophistication and clar-ity to our thought and communication. At the same time, it plays a fundamental structural role in our cognition, helping us to organise and project knowledge (Lakoff and Johnson, 1980; Feldman, 2006). Metaphors arise due to systematic associ-ations between distinct, and seemingly unrelated, concepts. For instance, when we talk about  X  X he turning wheels of a political regime X ,  X  rebuilding the campaign machinery  X  or  X  mending foreign pol-icy X , we view politics and political systems in terms of mechanisms , they can function, break, be mended etc. The existence of this association allows us to transfer knowledge and imagery from the domain of mechanisms (the source domain) to that of po-litical systems (the target domain). According to Lakoff and Johnson (1980), such metaphorical map-pings, or conceptual metaphors , form the basis of metaphorical language.

Metaphor is pervasive in our communication, which makes it important for NLP applications deal-ing with real-world text. A number of approaches to metaphor processing have thus been proposed, us-ing supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexico-syntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on tex-tual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Tur-ney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2013; Tsvetkov et al., 2014). However, all of these methods used manually-annotated linguistic resources to deter-mine these properties (such as the MRC concrete-ness database (Wilson, 1988)). To the best of our knowledge, there has not yet been a metaphor pro-cessing method that employed information learned from both linguistic and visual data. Ample re-search in cognitive science suggests that human meaning representations are not merely a product of our linguistic exposure, but are also grounded in our perceptual system and sensori-motor experience (Barsalou, 2008; Louwerse, 2011). Semantic mod-els integrating information from multiple modalities have been shown successful in tasks such as model-ing semantic similarity and relatedness (Silberer and Lapata, 2012; Bruni et al., 2014), lexical entailment (Kiela et al., 2015a), compositionality (Roller and Schulte im Walde, 2013) and bilingual lexicon in-duction (Kiela et al., 2015b). Using visual informa-tion is particularly relevant to modelling metaphor, where imagery is ported across domains.

In this paper, we present the first metaphor identi-fication method integrating meaning representations learned from linguistic and visual data. We construct our representations using a skip-gram model of Mikolov et al. (2013a) trained on textual data to ob-tain linguistic embeddings and a deep convolutional neural network (Kiela and Bottou, 2014) trained on image data to obtain visual embeddings. Linguis-tic word embeddings have been previously success-fully used to answer analogy questions (Mikolov et al., 2013b; Levy and Goldberg, 2014). These works have shown that such representations capture the nu-ances of word meaning needed to recognise rela-tional similarity (e.g. between pairs  X  king : queen  X  and  X  man : woman  X ), quantified by the respective vector offsets ( king  X  queen  X  man  X  woman ). In our experiments, we investigate how well these repre-sentations can capture information about source and target domains and their interaction in a metaphor. We then enrich these representations with visual in-formation. We first acquire linguistic and visual embeddings for individual words and then extend the methods to learn embeddings for longer phrases. The focus of our experiments is on metaphorical ex-pressions in verb X  X ubject, verb X  X irect object and ad-jectival modifier X  X oun constructions. We thus learn embeddings for verbs, adjectives, nouns, as well as verb X  X oun and adjective X  X oun phrases. We then use a set of arithmetic operations on word and phrase embedding vectors to classify phrases as literal or metaphorical. To the best of our knowledge, our ap-proach is also the first one to apply word or phrase embeddings to the task of metaphor identification.
Our results demonstrate that the joint model in-corporating linguistic and visual representations out-performs the linguistic model in isolation, as well as being competitive with the best-performing meta-phor identification methods that rely on hand-crafted information about domains, concreteness and im-ageability. A strand of metaphor processing research cast the problem as a classification of linguistic expressions as metaphorical or literal. They experimented with a number of features, including lexical and syn-tactic information and higher-level features such as semantic roles and domain types. Gedigian et al. (2006) classified verbs related to MOTION and CURE within the domain of financial discourse. They used the maximum entropy classifier and the verbs X  nom-inal arguments and their semantic roles as features, reporting encouraging results. Dunn (2013a) used a logistic regression classifier and high-level proper-ties of concepts extracted from SUMO ontology, in-cluding domain types ( ABSTRACT , PHYSICAL , SO -OBJECT ). Tsvetkov et al. (2013) also used logis-tic regression and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses. They have shown that the model learned with such coarse semantic features is portable across languages. The work of Hovy et al. (2013) is notable as they focused on compositional rather than categorical features. They trained an SVM with dependency-tree kernels to capture com-positional information, using lexical, part-of-speech tag and WordNet supersense representations of sen-tence trees. Mohler et al. (2013) aimed at modelling conceptual information. They derived semantic sig-natures of texts as sets of highly-related and inter-linked WordNet synsets. The semantic signatures served as features to train a set of classifiers (max-imum entropy, decision trees, SVM, random forest) that map new metaphors to the semantic signatures of the known ones.

Turney et al. (2011) hypothesized that metaphor is commonly used to describe abstract concepts in terms of more concrete or physical experiences. Thus, Turney and colleagues expected that there would be some discrepancy in the level of concrete-ness of source and target terms in the metaphor. They developed a method to automatically measure concreteness of words and applied it to identify ver-bal and adjectival metaphors. Neuman et al. (2013) and Gandy et al. (2013) followed in Turney X  X  steps, extending the models by incorporating information about selectional preferences.

Heintz et al. (2013) and Strzalkowski et al. (2013) focused on modeling topical structure of text to identify metaphor. Their main hypothesis was that metaphorical language (coming from a different do-main) would represent atypical vocabulary within the topical structure of the text. Strzalkowski et al. (2013) acquired a set of topic chains by linking se-mantically related words in a given text. They then looked for vocabulary outside the topic chain and yet connected to topic chain words via syntactic de-pendencies and exhibiting high imageability. Heintz et al. (2013) used LDA topic modelling to identify sets of source and target domain vocabulary. In their system, the acquired topics represented source and target domains, and sentences containing vocabulary from both were tagged as metaphorical.

Other approaches addressed automatic identifica-tion of conceptual metaphor. Mason (2004) auto-matically acquired domain-specific selectional pref-erences of verbs, and then, by mapping their com-mon nominal arguments in different domains, ar-rived at the corresponding metaphorical mappings. For example, the verb pour has a strong preference for liquids in the LAB domain and for money in the FINANCE domain, suggesting the mapping MONEY is
LIQUID . Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora. For example, the feature vector for politics would contain GAME or MECH -ANISM terms among the frequent features. As a re-sult, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source do-main (or sets of source domains). Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples. Shutova and Sun (2013) used hi-erarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way. 3.1 Learning linguistic representations We obtained our linguistic representations using the log-linear skip-gram model of Mikolov et al. (2013a). Given a corpus of words w and their con-texts c , the model learns a set of parameters  X  that maximize the overall corpus probability where C ( w ) is a set of contexts of word w and p ( c | w ;  X  ) is a softmax function: where v c and v w are vector representations of c and w . The parameters we need to set are thus v c v context vocabulary C , and the set of dimensions i  X  1 ,...,d . Given a set D of word-context pairs, embeddings are learned by optimizing the following objective: pus. The text was lemmatized, tagged, and parsed with Stanford CoreNLP (Manning et al., 2014). Words that appeared less than 100 times in their lem-matized form were ignored. The 100-dimensional word and phrase embeddings were learned in two stages: in a first pass, we obtained word-level em-beddings (e.g. for white and rabbit ) using the stan-dard skip-gram with negative sampling of Eq. (3); we then obtained phrase embeddings (e.g. for white rabbit ) through a second pass over the same corpus. In the second pass, the vectors v c and v c 0 of Eq. (3) were set to their values from the first pass, and kept fixed. Verb-noun phrases were extracted by finding nsubj and dobj arcs with V B head and NN de-pendent; analogously, adjective-noun phrases were extracted by finding amod arcs with NN head and JJ dependent. No frequency cutoff was applied for phrases. All embeddings were trained on the corpus for 3 epochs, using a symmetric window of 5, and 10 negative samples per word-context pair. 3.2 Learning visual representations Visual embeddings were obtained in a manner simi-lar to Kiela and Bottou (2014). Using the deep learn-ing framework Caffe (Jia et al., 2014), we extracted image embeddings from a deep convolutional neural network that was trained on the ImageNet classifi-cation task (Russakovsky et al., 2015). The network (Krizhevsky et al., 2012) consists of 5 convolutional layers, followed by two fully connected rectified lin-ear unit (ReLU) layers that feed into a softmax for classification. The network learns through a multi-nomial logistic regression objective: where 1 { X } is the indicator function and we train on D examples with K classes. We obtain image embeddings by doing a forward pass with a given image and taking the 4096-dimensional fully con-nected layer that precedes the softmax (typically called FC7) as the representation of that image.
To construct our embeddings, we used up to 10 images for a given word or phrase, which were ob-tained through Google Images. It has been shown that images from Google yield higher quality repre-sentations than comparable resources such as Flickr and are competitive with hand-crafted datasets (Fer-gus et al., 2005; Bergsma and Goebel, 2011). We created our final visual representations for words and phrases by taking the average of the extracted image embeddings for a given word or phrase. 3.3 Multimodal fusion strategies While it is desirable to jointly learn representations from different modalities at the same time, this is often not feasible (or may lead to poor performance) due to data sparsity. Instead, we learn uni-modal representations independently, as described above, and then combine them into multi-modal ones. Pre-vious work in multi-modal semantics (Bruni et al., 2014) investigated different ways of combining, or fusing , linguistic and perceptual cues. When calcu-lating similarity, for instance, one can either com-bine the representations first and subsequently com-pute similarity scores; or compute similarity scores independently per modality and afterwards combine the scores. In contrast with joint learning (which has also been called early fusion ), these two possibilities represent middle and late fusion, respectively (Kiela and Clark, 2015).

We experiment with middle and late fusion strate-gies. In middle fusion, we L-2 normalise and con-catenate the vectors for linguistic and visual repre-sentations and then compute a metaphoricity score for a phrase based on this joint representation. In late fusion, we first compute the metaphoricity scores based on linguistic and visual representations in-dependently, and then combine the metaphoricity scores by taking their average. 3.4 Measuring metaphoricity We investigate a set of arithmetic operations on the linguistic, visual and multimodal embedding vectors to determine whether the two words in the phrase belong to the same domain or rather a word from one domain is metaphorically used to describe another. 3.4.1 Word-level embeddings
In our first set of experiments, we compare em-beddings learned for individual words in order to de-termine whether they come from the same domain. This is done by determining similarity between the representations of the two words in a phrase: where word 1 is either a verb or an adjective, word 2 is a noun, and similarity is defined as cosine similar-ity: We expect the similarity of word representations to be lower for metaphorical expressions (where one word comes from the source domain and one from the target), than for the literal ones (where both words come from the target domain). We will fur-ther refer to this method as W ORD C OS . 3.4.2 Phrase-level embeddings
In our second set of experiments, we investigate compositional properties of metaphorical phrases by comparing the embeddings learned for the whole phrase with those of the individual words in the phrase. This allows us to determine which proper-ties the phrase shares with each of the words, provid-ing another criterion for metaphor identification. We expect that the embeddings of literal phrases will be more similar to the embeddings of individual words in the phrase (or a combination thereof) than those of metaphorical phrases. We use the following mea-sures to test this hypothesis: P HRAS C OS 1: cos ( phrase  X  word 1 ,word 2 ) (7) P HRAS C OS 2: cos ( phrase  X  word 2 ,word 1 ) (8)
P HRAS C OS 3: cos ( phrase,word 1 + word 2 ) , (9) where phrase is the phrase embedding vector, and word 1 and word 2 are defined as above. 3.4.3 Classification
We use a small development set (a collection of phrases annotated as metaphorical or literal) to de-termine an optimal classification threshold for each of the above scoring methods. We have optimized the threshold by maximizing classification accuracy ues above the threshold were considered literal and those with values below the threshold metaphorical. The thresholds were then applied to classify the test instances as literal or metaphorical. 4.1 Annotated datasets We evaluate our method using two datasets manu-ally annotated for metaphoricity: Mohammad et al. dataset ( MOH ) Mohammad et al. (2016) annotated different senses of WordNet (Fellbaum, 1998) verbs for metaphoricity. They ex-tracted verbs that had between three and ten senses in WordNet and the sentences exemplifying them in the corresponding glosses. The verb uses in the sentences (1639 in total) were then annotated for metaphoricity by 10 annotators each via the crowd-selected the verbs that were tagged by at least 70% of the annotators as metaphorical or literal to create their dataset. We extracted verb X  X irect object and verb X  X ubject relations of the annotated verbs from this dataset, discarding the instances with pronom-inal or clausal subject or object. This resulted in a dataset of 647 verb X  X oun pairs, 316 of which were metaphorical and 331 literal. Figure 1 shows some examples of annotated verbs from Mohammad et al. X  X  dataset.
 Tsvetkov et al. dataset ( TSV ) Tsvetkov et al. (2014) created a large dataset of adjective X  X oun pairs that they annotated for metaphoricity. Start-ing with a 1000 frequent adjectives, they extracted using SketchEngine and in collections of metaphor on the Web. Tsvetkov et al. divided the data into a training set (containing 884 literal and 884 metaphorical pairs) and test set (111 literal and 111 metaphorical pairs). We will refer to their train-ing set as TSV -TRAIN and to the test set as TSV -TEST . The test set was annotated for metaphoricity by 5 annotators with an inter-annotator agreement of  X  = 0 . 76 . Figure 2 shows a portion of the anno-tated test set. Metaphorical phrases that depend on wider context for their interpretation (e.g. drowning students ) were removed. The training set was anno-tated by one annotator only, and it is thus likely that the annotations are less reliable than those in the test set. We thus evaluate our methods on Tsvetkov et al. X  X  test set ( TSV -TEST ). However, we will also re-port results on TSV -TRAIN to confirm whether the observed trends hold in a larger, though likely nois-ier, dataset.

We selected the above two datasets since they in-clude examples for different senses (both metaphor-ical and literal) of the same verbs or adjectives. This allows us to test the extent to which our model is able to discriminate between different word senses, as opposed to merely selecting the most frequent class for a given word. 4.2 Experimental setup We divided the verb-and adjective-noun datasets into development and test sets. The verb X  X oun de-velopment set contained 80 instances from MOH (40 literal and 40 metaphorical), leaving us with the test set of 567 verb-noun pairs from MOH . We cre-ated the adjective X  X oun development set using 80 adjective-noun pairs (40 literal and 40 metaphorical) from TSV -TRAIN , leaving all of the 222 adjective X  noun pairs in TSV -TEST for evaluation. In a separate experiment, we also applied our methods to the re-mainder of TSV -TRAIN (1688 adjective X  X oun pairs) to evaluate our system on a larger adjective dataset. We used the development sets to determine an op-Features Method P R F 1 Linguistic W ORD C OS 0.67 0.76 0.71 Visual W ORD C OS 0.49 0.97 0.65 Multimodal W ORD M ID 0.56 0.86 0.68 timal threshold value for each of our scoring meth-ods. The thresholds for verb-noun and adjective-noun phrases were optimized independently using the corresponding development sets. We experi-mented with the three phrase-level scoring methods on the development sets, and found that P HRAS -C OS 1 consistently outperformed P HRAS C OS 2 and P
HRAS C OS 3 for both verb X  X oun and adjective X  noun phrases. We thus report results for P HRAS -C OS 1 on our test sets.

We first evaluated the performance of W ORD C OS and P HRAS C OS 1 using linguistic and visual repre-sentations in isolation, and then evaluated the mul-timodal models using middle and late fusion strate-gies. In middle fusion, we concatenated the linguis-tic and visual vectors, and then applied W ORD C OS and P HRAS C OS 1 methods to the resulting multi-modal vectors. We will refer to these methods as W
ORD M ID and P HRAS M ID respectively. In late fusion, we used an average of linguistic and vi-sual scores to determine metaphoricity. We exper-imented with three different scoring methods: (1) W ORD L ATE , where linguistic and visual W ORD -C
OS scores were combined; (2) P HRAS L ATE , where linguistic and visual P HRAS C OS 1 scores were com-bined; and (3) M IX L ATE , where linguistic and W
ORD C OS and visual P HRAS C OS 1 scores were combined. 4.3 Results and discussion We evaluated the performance of our methods on the MOH and TSV -TEST test sets in terms of preci-sion, recall and F-score and the results are presented in Tables 1 and 2 respectively. When using lin-guistic embeddings alone, W ORD C OS outperforms Features Method P R F 1 Linguistic W ORD C OS 0.73 0.80 0.76 Visual W ORD C OS 0.50 0.95 0.66 Multimodal W ORD M ID 0.59 0.85 0.70 P
HRASE C OS 1 for both verbs and adjectives by 17-19%. This suggests that linguistic word embeddings already successfully capture domain and composi-tional information necessary for metaphor identifi-cation. In contrast, the visual P HRASE C OS 1 model, when applied in isolation, tends to outperform the visual W ORD C OS model. P HRAS C OS 1 measures to what extent the meaning of the phrase can be composed by simple combination of the represen-tations of individual words. In metaphorical lan-guage, however, a meaning transfer takes place and this is no longer the case. Particularly in visual data, where no linguistic conventionality and stylistic ef-fects take place, P HRAS C OS 1 captures this prop-erty. For adjectives this trend was more evident than for verbs. The visual P HRASE C OS 1 model, even when applied on its own, attains a high F-score of 0.73 on TSV -TEST , suggesting that concreteness and other visual features are highly informative in iden-tification of adjectival metaphors. This effect was present, though not as pronounced, for verbal meta-phors, where the vision-only P HRASE C OS 1 attains an F-score of 0.66.

The multimodal model, integrating linguistic and visual embeddings, outperforms the linguistic mod-els for both verbs and adjectives, clearly demon-strating the utility of visual features across word classes. The late fusion method M IX L ATE , which combines the linguistic W ORD C OS score and the vi-sual P HRASE C OS 1, attains an F-score of 0.75 for verbs and 0.79 for adjectives, which makes it best-performing among our fusion strategies. When the same type of scoring (i.e. either W ORD C OS or P
HRAS C OS 1) is used with both linguistic and visual embeddings, middle and late fusion techniques at-tain comparable levels of performance, with W ORD -C
OS being the leading measure. The reason behind the higher performance of M IX L ATE is likely to be the combination of different scoring methods, one of which is more suitable for the linguistic model and the other for the visual one.

The differences between verbs and adjectives with respect to the utility of visual information can be ex-plained by the following two factors. Firstly, pre-vious psycholinguistic research on abstractness and concreteness (Hill et al., 2014) suggests that humans find it easier to judge the level of concreteness of ad-jectives and nouns than that of verbs. It is thus possi-ble that visual representations capture the concrete-ness of adjectives and nouns more accurately than that of verbs. Besides concreteness, it is also likely that perceptual properties in general are more im-portant for the semantics of nouns (e.g. objects) and adjectives (their attributes), than for the semantics of verbs (actions), since the latter are grounded in our motor activity and not merely perception. Sec-ondly, following the majority of multimodal seman-tic models, we used images as our visual data rather than videos. However, some verbs, e.g. stative verbs and verbs for continuous actions, may be better cap-tured in video than images. We thus expect that using video data along with the images as input to the acquisition of visual embeddings is likely to im-prove metaphor identification performance for ver-bal metaphors. However, we leave the investigation of this issue for future work.

In an additional experiment, we evaluated our methods on the larger TSV -TRAIN dataset (specifi-cally using its portion that was not employed for de-velopment purposes) and the trends observed were the same. M IX L ATE attained an F-score of 0.71, outperforming language-only and vision-only mod-els. The performance of all scoring methods on TSV -TRAIN was lower than that on the TSV -TEST . This may be the result of the fact that the labelling of TSV -TRAIN was less consistent than that of TSV -TEST . As TSV -TEST is a set of metaphors annotated by 5 annotators with a high agreement, the evaluation on TSV -TEST is likely to be more reliable (Tsvetkov et al., 2014).

It is important to note that, unlike other super-vised approaches to metaphor, our methods do not require large training sets to learn the respective thresholds. The results reported here were ob-tained using only 80 annotated examples for train-ing. This is sufficient since the necessary lexical knowledge and the knowledge about domain, con-creteness and visual properties of concepts is al-ready captured in the linguistic and visual embed-dings. However, we additionally investigated how stable the thresholds learned by the model are us-ing the TSV -TRAIN dataset. For this purpose, we di-vided the dataset into 10 portions of approximately 170 examples (balanced for metaphoricity). We then trained the thresholds first on a small set of 170 ex-amples and then increasing the dataset by 170 ex-amples at each round. The thesholds appear to be relatively stable, with a standard deviation of 0.03 for M IX L ATE ; 0.02 for W ORD C OS (linguistic); and 0.05 for P HRASE C OS 1 (visual). This suggests that our methods do not require a large annotated dataset and training on a small number of examples is suffi-cient.

Despite the limited need in training data and no reliance on hand-coded lexical resources, the perfor-mance of our method favourably compares to that of existing metaphor identification systems (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Dunn, 2013b; Tsvetkov et al., 2013; Hovy et al., 2013; Hovy et al., 2013; Shutova and Sun, 2013; Strzalkowski et al., 2013; Beigman Klebanov et al., 2015), that typically use such resources. For instance, Turney et al. (2011) used hand-annotated abstractness scores for words to develop their sys-tem, and reported an F-score of 0.68 for verb X  X oun metaphors and an accuracy of 0.79 for adjective X  noun metaphors (though the latter was only evalu-ated on a small dataset of 10 adjectives and Tur-ney and colleagues did not report results in terms of F-score, which is likely to be lower). Our use of visual features is in line with Turney X  X  hypoth-esis concerning the relevance of concreteness fea-tures to metaphor processing. However, our re-sults indicate that extracting this information from image data directly is a more suitable way to cap-ture the concreteness itself, as well as capturing other relevant perceptual properties of concepts. The method of Tsvetkov et al. (2014) used both con-creteness features (which they extracted from the MRC concreteness database) and hand-coded do-main information for words (which they extracted from WordNet). They report a high F-score of 0.85 for adjective X  X oun classification on TSV -TEST . The performance of our method on the same dataset is a little lower than that of Tsvetkov et al. How-ever, we do not use any hand-annotated resources and acquire linguistic, domain and perceptual infor-mation in the data-driven way. It is thus encour-aging that, even though resource-lean, our methods approach the performance level of the methods us-ing hand-annotated features (as in case of Tsvetkov et al. (2014)) or outperform them (as in case of Turney et al. (2011), Neuman et al. (2013), Dunn (2013b), Mohler et al. (2013), Gandy et al. (2013), Strzalkowski et al. (2013), Beigman Klebanov et al. (2015) and many others). For further comparison with these approaches and their results see a recent review by Shutova (2015). We presented the first method that uses visual features for metaphor identification. Our results demonstrate that the multi-modal model combining both linguistic and visual knowledge outperforms language-only models, suggesting the importance of visual information for metaphor processing. Un-like previous metaphor processing approaches, that employed hand-crafted resources to model percep-tual properties of concepts, our method learns visual knowledge from images directly, thus reducing the risk of human annotation noise and having a wider coverage and applicability. Since the method relies on automatically acquired lexical knowledge, in the form of linguistic and visual embeddings, and is oth-erwise resource-independent, it can be applied to un-restricted text in any domain and easily tailored to other metaphor processing tasks.

In the future, it would be interesting to apply mul-timodal word and phrase embeddings to automati-cally interpret metaphorical language, e.g. by deriv-ing literal or conventional paraphrases for metaphor-ical expressions (similarly to the task of Shutova (2010)). Multimodal embeddings are also likely to provide useful information for the models of meta-phor translation, as they have already proved suc-cessful in bilingual lexicon induction more generally (Kiela et al., 2015b). Finally, it would be interest-ing to further investigate compositional properties of metaphorical language using multimodal phrase em-beddings and to apply the embeddings to automati-cally generalise metaphorical associations between distinct concepts or domains.
 We are grateful to the NAACL reviewers for their helpful feedback. Ekaterina Shutova X  X  research is supported by the Leverhulme Trust Early Career Fellowship. Douwe Kiela is supported by EPSRC grant EP/I037512/1.

