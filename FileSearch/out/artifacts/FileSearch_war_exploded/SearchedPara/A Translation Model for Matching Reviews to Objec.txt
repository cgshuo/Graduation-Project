 We develop a generic method for the review matching prob-lem, which is to match unstructured text reviews to a list of objects, where each object has a set of attributes. To this end, we propose a translation model for generating re-views from a structured description of objects. We develop an EM-based method to estimate the model parameters and use this model to find, given a review, the object most likely to be the topic of the review. We conduct extensive experi-ments on two large-scale datasets: a collection of restaurant reviews from Yelp and a collection of movie reviews from IMDb. The experiments show that our translation model-based method is superior to traditional tf-idf based methods as well as a recent mixture model-based method for the re-view matching problem.
 Categories and Subject Descriptors. I.2.7 [ Comput-ing Methodologies ]:Natural Language Processing X  Language models General Terms. Algorithms, Experimentation, Measure-ments Keywords. Language model; review matching; translation model
Reviews are ubiquitous on the web  X  they exist in a variety of places on the web including online e-commerce websites (e.g., amazon.com ), verticals (e.g., imdb.com ), ded-icated review websites (e.g., yelp.com ), aggregation sites (e.g., nextag.com ), blogs, forums, newspapers, and so on. With an increased pressure on search engines to present a holistic view of search results than mere ten blue links in response to a user query, it becomes critical for them to col-lect and understand user-generated content such as reviews. This will be especially useful for queries for which many web reviews exist  X  such queries include those related to shop-ping, dining, products, movies, etc. An important part of this understanding necessitates deciphering the object that is being reviewed, in other words, the search engine is faced with the review matching problem. Solving this problem is the stepping stone to enabling more sophisticated appli-cations such as review aggregation, sentiment and polarity analysis, and more (Section 2.3).

At first glance, review matching seems a non-problem: af-ter all, any reasonable review should have information about the object that is reviewed. Unfortunately, this turns out to be deceptive for two main reasons: review webpages might mention other objects that are peripheral to the review and the information revealed about the object in a review might be partial or surrogate; we discuss these issues at length in Section 2.1. In fact, these reasons make the review match-ing problem not amenable to vanilla techniques based on information retrieval, entity matching, or clustering; more on this in Section 2.2.
 Our contributions. In this paper we explore the problem of matching reviews to objects using a translation model. Given a set of objects where each object has a set of at-tributes, we posit a simple model for generating a word in the review for the object from its attributes according to the following process. The process first chooses an attribute (independent of the object), and then selects a word in the chosen attribute of the object, and finally outputs a trans-lation of this selected word according to a global, attribute-dependent translation model.

We then obtain a method to estimate the parameters of the model using a training dataset consisting of a set of aligned reviews, i.e., a set of pairs of reviews and their cor-responding objects. The parameter estimation is based on the Expectation Maximization algorithm, where the maxi-mization step is a non-linear maximization problem solved using gradient descent. We use these learned parameters in order to find, given a review, the object that is most likely to have generated the review.

Our generative model and the parameter estimation method represent a substantial generalization of the mixture model for review matching proposed in [2]. (See Section 3 for a discussion of the differences.)
We then apply our review matching method on a dataset of over 83,000 restaurant reviews from Yelp and on a dataset of over 36,000 movie reviews from IMDb. For the restaurant reviews, we obtain more than 50% improvement over the tf-idf method and more than 28% improvement over the mix-ture model [2]. For the movie reviews, we obtain around 10% improvement over tf-idf and 4.5% over the mixture model. The improvements are directly due to the power of the trans-lation model and fully exploiting object attributes.
Our methodology is applicable beyond review matching; for example, it can be used to extract the primary politi-cian discussed in a political news article. Going further, our method can be used in matching unstructured text to struc-tured objects whenever it is plausible to assume that the text is produced from the objects using a model similar to ours.
 Organization. In Section 2, we illustrate what makes the problem hard. Section 3 reviews related work on language modeling, information extraction, and opinion mining. Sec-tion 4 presents our generative model and an EM-based method to estimate the parameters of the model, with Section 5 dis-cussing certain practical considerations. Section 6 contains a description of the data used in our experiments. Sec-tion 7 presents our experimental results on restaurant re-view ( Yelp ) and movie review ( IMDb ) datasets. Section 8 contains concluding remarks.
In this section we discuss three points that highlight the motivation for studying the review matching problem in depth. First, we ask what makes the problem difficult. Sec-ond, we address the issues with using existing technologies, from information retrieval to information extraction, to solve this problem. Third, we outline a set of applications that are enabled by solving the review matching problem.
It is most natural for a review to mention the object that it reviews. In fact, almost all review webpages typically con-tain a mention of the object that is reviewed in one form or another  X  either the object is mentioned in the review itself or is explicitly mentioned on the webpage that contains the review. Given this, how can the problem of review matching still be hard? We examine two main reasons: (1) Review webpages may contain mentions of more than one object. This can occur in the review itself (e.g., the re-view compares the object to one or more other objects; see Figure 1). Alternatively, a webpage can contain reviews of more than one object (this is common in blogs and review ag-gregating websites, e.g., http://tasteofthesuburbs.blogspot. com/search/label/chinese%2Fdim%20sum contains a review of several chinese restaurants); or the reviews are about one single object, but the webpage can mention nearby or related objects as part of a website-wide template (this is common in review websites managed by content-generation software, e.g., http://www.yelp.com/biz/gochi-japanese-fusion-tapas-cupertino contains a list of nearby restaurants). Figure 1: A review snippet mentioning two restau-rants. (2) A review might mention an object in a partial manner or in a way that results in ambiguities. Reviews often tend to contain partial mentions (e.g.,  X  X ochi X  instead of  X  X ochi Japanese Fusion Tapas X , the official name), abbreviate cer-tain words (e.g.,  X  X an Fran X  instead of  X  X an Francisco X  or  X  X d X  instead of  X  X oad X ), or use related terms (e.g.,  X  X outh bay X  or  X  X ay area X  instead of  X  X upertino X ); see Figure 2.
 Figure 2: A review snippet with partial and substi-tute information.

Hence, any review matching method has to cope with both ambiguity and partial information that may be present on review webpages. One the other hand, a redeeming aspect of many review webpages is that they tend to contain clues about other attributes of the object. For example, a restau-rant review may contain words that hint at the cuisine of the restaurant or its location. To achieve good performance, a review matching scheme should utilize such clues.
Having examined the difficulties posed by the review match-ing problem, we consider the tempting proposition to re-purpose existing methods to solve the review matching prob-lem. Here we discuss the caveats.
 Classical IR methods. It may appear that the review matching problem is an instance of the standard IR setting: the review is the query and the set of objects, along with their attributes, are the documents. Unlike in traditional IR, however, the query is long and the document is short; this stipulates adapting established IR concepts such as idf (inverse document frequency) and document-length normal-ization to this setting. A dual view of considering reviews as documents and objects as queries is still problematic since the goal is to find the best  X  X uery X  for a given  X  X ocument X ; such a question is not explicitly addressed by classical IR. For a detailed discussion of these issues, see [2]. Wrapper induction and information extraction. If we manage to extract all the objects in the review through some means, then review matching might become simpler. There are two ways of extracting objects from webpages, namely, wrapper induction and information extraction. Wrapper in-duction constructs rules to navigate to certain portions of the HTML structure in order to extract objects (e.g., [16]). The main disadvantages of wrapper-based methods are two-fold: they primarily apply to highly-structured websites and they involve considerable human labeling effort that is both expensive and prone to error. The latter is particularly un-desirable since it is not a one-time cost: the wrappers have to be constantly kept up-to-date. The former places a limi-tation on the scope of the applicability of such methods; in particular, it is not feasible to study the less structured tail websites using wrappers. Information extraction methods, including named-entity recognition, often have limited accu-racy [8, 23]. Even when they rely on an extensive dictionary lookup mechanism to identify object mentions in text, an additional co-reference resolution step might be necessary to resolve different mentions of the same physical object. In addition, when they extract multiple objects from a review, we are still left with the task of selecting one from among these candidates; this, for instance, will happen for the re-view snippet in Figure 1. In fact, the relatively large number of candidates yielded in the candidate generation phase in our experiments, which can be viewed as light-weight infor-mation extraction, suggest that this is a practical concern. Classifier-based methods. Another possibility would be to try classifier-based methods to classify a review according to the values of various attributes. This calls for building a (multi-class) classifier for each of the attributes. Even if such classifiers are available, one needs to build additional layers on top of them to combine classifier output with other evidence present in the text, which is a non-trivial task in itself. In contrast, our model naturally combines all such evidence in a principled way.
Matching reviews to objects is the first step in enabling many review-related applications. Below, we state three such applications. (1) Fine-grained review localization. The problem here is to localize the review on a webpage such a blog, where the review is not the only piece of text present on the page. This can be done, especially in our language model-based method, by using the attributes of the object. Localized review text can help in many search engine tasks by indirectly removing noisy sections of a webpage. Likewise, our language model-based method can also be used to identify and tease out reviews of multiple objects in a single webpage. (2) Review aggregation. The problem here is to aggre-gate the information contained in all the reviews for a given object. Aggregation might be as simple as computing the average rating of the object to as sophisticated as extract-ing the most hated aspect of the object by a detailed text analysis of the individual reviews. (3) Automated information extraction. The task here is to extract structured information from webpages on a review website. By identifying the review objects on many web-pages on a given website, it is possible to learn the HTML structure of the webpages (such as the HTML DOM node that contains various attributes of the object); this can be used in automatic information extraction.
The other related work falls into three main categories, namely, the topic of language modeling in general and its connections to information retrieval in particular, the litera-ture on entity matching and information extraction, and the work on opinion topic identification.
 Relationship to review language model [2]. In a re-cent work [2], we proposed a framework of using a review language model (RLM) to match reviews to objects. The specific instantiation explored in that work was a simple mixture model for generating reviews from a description of an object, which we refer to as Mixture in this paper. The current paper is a substantial generalization and extension of this earlier work. The salient differences between the two papers are the following. (1) The model in this paper incorporates two generaliza-tions over [2]: using the structured nature of the objects in an explicit fashion and using a translation model; (2) Consequently, the parameter estimation method is sig-nificantly more complicated; for instance, there was no need for EM in [2]; (3) The experiments in this paper were conducted not only on a much larger restaurant review collection compared to [2] but also on movie review collection, obtained from IMDb which is not present in [2].

The experiments in this paper firmly establish that our en-hanced model indeed outperforms the basic Mixture model. Language modeling. Language modeling has been a pow-erful paradigm in the context of several information retrieval applications [21, 24, 17, 27]. The principle behind this is to first estimate a language model for each document and for a given query, rank the documents in order of the likelihood of the query according to the estimated model of each docu-ment [21]. One of the main issues with language modeling is data sparsity; smoothing is an important means to manage data sparsity [10, 28]. Hofmann introduced probabilistic latent semantic indexing in which he proposed a mixture model with latent topics to generate words in documents [12]; while he also aims to go beyond exact word match, un-like in his case, our topics are patent: they are the attributes of the objects. Berger and Lafferty [4] introduced the idea of treating information retrieval as a statistical machine trans-lation problem [6, 7]; our work is inspired by this translation viewpoint. Besides being in a non-IR setting and utilizing the structure in the objects, our setting also has access to more naturally aligned data, unlike in [4].
 Information extraction and entity matching. Entity matching is a topic that is well studied in databases. There are three main approaches to entity matching, namely, non-relational, relational, and collective. Non-relational approaches consider pairwise attribute similarities between entities [19, 11]. Relational approaches exploit the relationships that exist between entities [1, 14]. And, collective approaches exploit the relationship between various matching decisions [5, 18]. The EROCS system [9], whose goal is to link struc-tured data with unstructured text, is closest in spirit to our work. This system, based on information extraction and en-tity matching uses tf-idf for solving the matching problem. As our experiments establish, this is sub-optimal.
 Opinion mining. Opinion topic identification is also some-what related to our work. There has been a lot of research on opinion extraction from reviews [15, 26, 22, 13, 25]. These papers focus on finding the attributes of the object under re-view, rather than identifying the object itself. For the case when the objects are products, dictionary-lookup methods have limited success on general non-product review texts [25]. As we mentioned earlier, dictionary-lookup methods have limitations when applied to our problem: they can be more effective at identifying presence of object mentions than at disambiguating similar objects. There has been some work on finding reviews (regardless of their subjects) in large-scale collections [20, 3]; this is a logical step that precedes the review matching step.
In this section we present the main technical content of the paper. First we start with the problem formulation and introduce the basic set up. We then present, in Section 4.2, a probabilistic model for generating reviews from objects. In Section 4.3 we briefly describe how to use this generative model for review matching and in Section 4.4 we describe a method based on Expectation Maximization (EM) to esti-mate the model parameters.
Let  X  be a given set of objects. Each object  X   X   X  has a set of attributes. For an object  X  , let  X   X  be the contents of its  X  th attribute; we treat the content of each attribute to be a bag of words. We use the notation  X   X   X   X  to denote that the word  X  is present in the  X  th attribute of  X  .
The review matching problem is the following: given a review which is mainly about one of the objects in  X  , find the object that is being reviewed.

Example 1. We use the following example from the restau-rant domain as our running example. Let  X  be the set of restaurants. Each restaurant  X   X   X  has three attributes as-sociated with it: the name of the restaurant, the city where it is located, and the type of cuisine it serves. A particular instantiation of the restaurant object is given below:
We posit a simple model for generating a word in the re-view for an object from its attributes according to the follow-ing process. To begin, we describe it informally: the process first chooses an attribute (independent of the object), and then selects a word in the chosen attribute of the object, and finally outputs a translation of this selected word according to a global, attribute-dependent translation model.
Before formally defining it, we illustrate the model us-ing the restaurant object from Example 1. In this exam-ple, a review about Gochi restaurant might be composed of words generated from its name (words such as gochi , restau-rant ) its city ( cupertino ), or its cuisine (words such as japanese , tapas ). These words sometimes appear verbatim, and sometimes get translated into other words. For instance, cupertino ( city ) might generate words such as south bay or bay area and japanese ( cuisine ) might generate words such as croquette or unagi . In fact, any word in the review can be accounted for by this translation: for example, a generic word such as is or of can be translated from any word in any attribute of any object.

Now we describe the model formally. Let  X  denote the set of attributes of the objects and let  X  denote the set of all possible words in object attributes. Let  X  denote the vocabulary of reviews. Let  X  be a probability distribution over  X  . We use  X   X  to denote the probability of choosing attribute  X  . For each  X   X   X  , let  X   X  be a function that assigns a positive real weight  X   X  (  X  ) to each word  X  in  X  . Finally, for each  X   X   X  and  X   X   X  , let  X   X  (  X  X  X   X  ) be a distribution over  X  such that the probability of a word  X  being translated from  X  is given by  X   X  (  X   X   X  ). The parameters  X  ,  X  , and  X  are collectively referred to as  X  .

Given parameters  X  , a word  X  in review is generated from a word in the attribute content of an object  X  as follows. First, an attribute  X  is picked with probability  X   X  . Then, a word  X  is picked from the set  X   X  with probability pro-portional to  X   X  (  X  ); this is given by probability  X   X  (  X   X   X  ) =  X  (  X  ) / X   X  (  X  ), where  X   X  (  X  ) = P  X   X   X  ing factor for  X  . Finally, a word  X  is picked with probability  X  (  X   X   X  ). Note that the word  X  is generated from the attribute X  X ord pair (  X , X  ).

Thus, the probability of a word  X  being generated from an object  X  is given by
And the probability of a review  X  being generated from an object  X  is given by where  X  (  X  ) is a normalizing constant depending only on the length of  X  .
 Example 2. We continue with the setting in Example 1. Suppose we have the following parameters Then, to generate a review word for the Gochi restaurant, the attribute cuisine will be chosen with probability 0.7. Suppose the  X  cuisine values are as follows: Then, given that the attribute cuisine was chosen for Gochi, japanese will be picked from the set { japanese , tapas } with  X  cuisine (  X  X  X  japanese ) values to pick a review word. Suppose we have the following: Then, given japanese , unagi will be picked with probability 0.3. The final probability of generating unagi from ( cuisine , japanese ) is 0 . 7  X  0 . 8  X  0 . 3 = 0 . 168 .
Given the review language model  X  , matching objects to reviews is straightforward. For a review  X  , we want to output the most likely object  X   X  given by In the absence of any information, we assume a uniform dis-tribution for P (  X  ). (Additional information about objects, such as their rating/popularity, can be used to model P (  X  ) more accurately.) From this, we get
In this section we describe the methodology to estimate the parameters of our generative model. Our training data consists of a set of aligned reviews, i.e., a set of pairs of reviews and their corresponding objects. For the sake of presentation, we treat the training data as a sequence of pairs (  X , X  ) where  X  is a word from a review and  X  is the object of corresponding review. We use  X  (  X  ) to denote the word in the  X  th pair and  X  (  X  ) to denote the object in the  X  th pair.

Clearly, if we knew which attribute X  X ord pair (  X , X  ) in  X  generated the word  X  (  X  ) , for all  X  , then the parameter esti-mation would be easy. However, such alignment information at a word level is not available to us as part of the training data. We therefore introduce a hidden variable,  X  (  X  )  X , X  denotes the event that  X  (  X  ) is generated from the attribute X  word pair (  X , X  ). Consider the following function: Then, we have Clearly, Our goal is to estimate  X  so as to maximize or equivalently, to maximize We use the Expectation Maximization (EM) method to solve this maximization problem.
 E-step. In the E-step, given the current model  X  (  X  ) compute using (4).
 M-step. Since the original objective function is difficult to directly optimize for, we instead compute where
Using the Lagrange multipliers method, we compute the optimal parameters  X   X   X  , X   X   X  , X   X   X  of the new model  X 
First, where
Next, where
Since we cannot obtain a closed form for  X   X   X  , we use the gradient descent method to estimate  X   X   X  . We obtain  X   X  = X We solve the above non-linear equation using standard gra-dient descent. In fact, we can show that the solution is optimal; we omit the details in this version.
Our set of equations is under-constrained, i.e., two differ-ent set of parameters can give rise to equivalent models. For instance, in the case of restaurants in Example 1, if there is a generic word  X  that is not correlated with any specific city or cuisine , we can distribute its probability among city and cuisine in any way we want and get the same model.
 We say that  X   X   X   X  if for all reviews  X  and objects  X  , P (  X   X   X  ) = P  X   X  (  X   X   X  ).

Theorem 5.1. Let  X  be a model such that for some  X  and  X  , min  X   X   X  (  X  0  X   X  ) &gt; 0 . Then, there is a model  X   X   X   X   X  and min  X   X   X  (  X  0  X   X  ) = 0 .
 Proof. Let  X  = min  X   X   X  (  X  0  X   X  ); note that  X   X  (0 , 1). Consider any attribute  X   X  =  X  . We will move the probability mass of  X  0 from  X  to  X  without affecting the model.
Define  X   X  as follows. Let  X  =  X   X   X   X  / X   X  , and consider the following set of new parameters: All other parameters of  X   X  are same as those of  X  . Clearly, min  X   X   X   X  (  X  0  X   X  ) = 0. One can verify that for any  X  and  X  , P (  X   X   X  ) = P  X   X  (  X   X   X  ), i.e.,  X   X   X   X  . We omit the details in this version. Theorem 5.1 shows that our system is under-constrained. It also suggests an approach to regularize the model as fol-lows. We add an extra attribute generic to each object, which has a value Review . The intuition is that this attribute accounts for the generic review words that are not correlated with any specific attribute. For instance, in restaurant re-views, words like tasty and dinner do not have strong correla-tions with any specific city or cuisine. We want the attribute generic to account for all such words. Specifically, we want  X  generic (  X   X  Review ) to denote the probability that a word  X  is chosen from a generic review language.

We can use Theorem 5.1 to achieve this. For each at-tribute  X  and each word  X  such that  X   X  (  X   X   X  ) &gt; 0 for all  X  , we can take the minimum  X   X  (  X , X  ), subtract it from everyone and move it to the generic attribute.

Since regularization does not change the model, it does not affect the accuracy of the model in the matching task. However, it serves two purposes. First, it can make individ-ual translation models, i.e.,  X   X  cleaner since now they do not have to include generic words not related to the attribute. Second, the regularization can be used as a tool to avoid overfitting of the model, as we explain in the next section.
Recall that we introduce the generic attribute to account for generic review words. When we learn a model with this additional attribute, we find that  X  generic gets driven down to zero after EM, i.e., the generic attribute gets no probability mass and the model prefers to explain even the seemingly generic terms using other object-dependent attributes. This is the result of overfitting. Consider a generic word like din-ner . While all cuisines and cities should see the word with roughly the same frequency, it will not occur with the exact same frequency due to sampling errors. Thus, the model will let dinner get completely accounted for by words in city and cuisine , so as to account for the small differences in observed frequencies with different objects. Overfitting not only drives  X  generic to zero, it also makes individual transla-tion models noisy as sampling errors get amplified.
We use a very simple yet effective heuristic to overcome the problem of overfitting. While learning the model, we constrain  X  generic to have a high value. Given this con-straint, the generic attribute accumulates all the generic review words, and the individual translation models for spe-cific attributes only account for words specific to them.
We can improve model accuracy and efficiency by using additional knowledge about attributes, namely the scope of their translations. Certain attributes like cuisine and city can get translated to several words specific to the attribute, while certain other attributes like name or phone number can only translate to themselves. In general, if we know the set of words each attribute can translate into, we can incorporate the knowledge to learn a more effective model.

We use a very simple version of this idea. For each at-tribute, we label it as either flexible or inflexible . A word in a flexible attribute can be translated into any word in the vocabulary while a word in an inflexible attribute can only get translated into itself. By declaring attributes like name and phone number as inflexible, we can avoid learning complete translation models for all possible terms in such attributes, making models more compact and less noisy.
We consider two datasets, Yelp and IMDb , from two different domains where objects have different types of at-tributes.

We obtained the Yelp dataset based on the dataset used in [2]. The dataset consists of a set of reviews extracted from the Yelp website, yelp.com and the database of restaurant listings in Yahoo! Local, local.yahoo.com . The matching task is to determine, for each Yelp review, the corresponding restaurant listing in Yahoo! Local, where each restaurant has a name , city , and cuisine attribute. The dataset also contains the ground truth, i.e., the true mapping between Yelp and Yahoo! Local, which we use for training as well as for evaluation.

Some of the Yelp reviews do not contain any identifying information and can be as short as  X  X reat place. Awesome food!! X . In [2], only a subset of reviews, which explicitly contained the names and cities of restaurants that a human can use to uniquely identify the restaurant, were selected. However, a human can match a review even if it does not contain explicitly information, e.g., a mention of Manhattan can be used to infer that the restaurant is very likely in New York. In our evaluation, we consider all reviews, with the only requirement that they at least mention the restaurant name (even if only partially). As a result, our dataset is a superset of the one used in [2], and raw numbers are not directly comparable.

The resulting dataset consists of 83,478 Yelp reviews cov-ering 8,006 unique restaurants, which we seek to match with 680,000 Yahoo! Local restaurant listings. We split the restaurants into training and test sets; 12,500 reviews on the training restaurants were set aside as training data, and 48,623 reviews are left in the test data.
 The IMDb dataset consists of movie reviews from the IMDb website, www.imdb.com . For each movie, IMDb has a webpage that contains all the information about it. Also, each movie page has  X  X ser Comments X  section where users submit reviews for the movie. We used hand-crafted extrac-tion rules to extract all the movie information as well as reviews from IMDb. We extracted 36,321 reviews covering 3,786 randomly selected movies.
 Our task is to match these reviews against the complete IMDb database of 156,355 movies. For each movie, we use name , year , genre , director , actor list , and cast list as attributes. Since the reviews were extracted from the IMDb for the corresponding movies, we know the true match for each review. We set aside half of the reviews for training and use the other half for evaluation. The split was done in such a way that movies do not overlap across the two sets.
Note that unlike the restaurant domain, a movie can of-ten be uniquely identified by a combination of different at-tributes even when its name is not explicitly mentioned in the review.
We first describe the performance of our method on the two datasets in Section 7.1. Then, in Sections 7.2 and 7.3, we discuss in depth the models we learn on the Yelp and IMDb datasets respectively, and analyze how different aspects of our modeling help improve the accuracy of review matching.
We refer to our review matching method using translation models as Tmodel . We compare it with two other methods: (i) the TfIdf method, which uses the classic tf-idf score between the review and the objects and (ii) the Mixture method [2], which uses a simpler instantiation of a mixture model to match reviews to objects and has been shown to outperform TfIdf in certain cases. Figure 3: Accuracy of TfIdf , Mixture , and Tmodel on Yelp and IMDb data.

Figure 3 shows the accuracy of top-one predictions from all three methods on Yelp and IMDb . As we see, Tmodel obtains more than 28% improvement in accuracy over Mix-ture and more than 50% improvement over TfIdf . Note that the absolute accuracy numbers are low in general for the Yelp dataset. This is because a large fraction of reviews do not have identifying information, and the maximum ac-curacy that can be achieved even by a human will be sub-stantially lower than 100%. Likewise, we see that Tmodel improves over both Mixture (4.5%) and TfIdf (10%).

We then look at all the (review, top-one match) pairs, sorted by the score given by (3). Note that the reviews can be of different lengths and hence we normalize the scores by scaling them by Q  X   X   X  P  X  (  X   X  generic ). Figures 4(a) and 4(b) present the precision X  X ecall curves. We observe the same overall trends as in the case of accuracy: Tmodel outperforms Mixture , which in turn outperforms TfIdf .
In this section we discuss the model learned on the Yelp data in more detail. We start by discussing the models learned for the two flexible-match attributes: cuisine and city .

As we will see from the example translation tables, while initially both attributes receive all of the words in a review as candidate translations, in the final model, top transla-tions for words in the cuisine attribute are predominantly food-related, while top translations for words in the city at-tribute are predominantly location-related. In other words, as a natural outcome of optimizing for the maximum likeli-hood of the data, food-related (and object-dependent) words are mostly accounted for by the cuisine attribute, and location-related words are mostly accounted for by the city attribute.
 On cuisine . Recall that according to our model, words with higher  X   X  values are more likely to be chosen from attribute  X  . Intuitively, it is advantageous for more salient words  X  tokens that better distinguish an object from other objects  X  to receive higher  X  values, so that more generic words in reviews are left to be explained by the generic re-view language, and the salient features of the object can  X  X oncentrate X  on explaining more object-dependent words. Figure 4: Precision X  X ecall of Tmodel , Mixture , and TfIdf on the test data in Yelp and IMDb .
 We now examine the cuisine type attribute of restaurant objects to see whether the model learned conforms to our intuition.

In the Yelp aligned data (Yelp reviews aligned with Ya-hoo! Local listings), there are 96 cuisine types with at least five restaurants represented in the data. A generic cuisine type restaurants is associated with all of the objects. Ta-ble 1 presents the most and least frequent cuisines types, as well as cuisine types with the highest and lowest  X  values. Figure 5 plots the  X  value assigned to different cuisine types against the frequency of that cuisine type in the data (i.e., number of restaurants associated with that type).

Intuitively, cuisine types that are associated with many re-views tend to be more generic types that are not very salient. Indeed,  X  cuisine ( restaurants ) is an order of magnitude lower than all the other  X  values. Thus, when an object is not associated with any specific cuisine type, when words are generated from this attribute, they will be drawn entirely from the restaurants translation table. On the other hand, when an object is associated also with a specific cuisine type, words for this attribute will be predominantly drawn from the more specific one, given the striking difference in the  X  values.
 This trend also holds true for the non-generic cuisine types. For instance, the more general cuisine type southeast asian is seen more frequently than the more specific cuisine type viet-namese . As we can see from Table 1,  X  cuisine ( southeast asian ) is among the lowest, while  X  cuisine ( vietnamese ) is among the highest. Thus, if an object is labeled as both southeast asian and vietnamese , given that  X  cuisine ( vietnamese ) is high, it is the aligned data shown in parenthesis). beta value Figure 5:  X  cuisine (saliency of a cuisine type in the model) vs. cuisine frequency (number of restaurants associated with a cuisine type in the aligned data) for all cuisine types with frequency  X  5 (the most generic cuisine type restaurants is omitted.) more likely to be chosen to generate words for reviews writ-ten about this object. Indeed, many of the most frequent cuisine types (see line 1 of Table 1) were assigned the lowest  X  values in the model (see line 4 of Table 1).

Very infrequent cuisine types are not necessarily all salient (see line 2 of Table 1 for examples). However, they may appear to be salient as a result of overfitting to the few reviews associated with them. On the other hand, the salient ones among them do not receive enough training data to reach really high  X  values. In effect, infrequent cuisines were mostly assigned mid-range  X  values (Figure 5).

Most of the most salient cuisine types turned out to be  X  X id-size X  cuisines: cuisines not too general that they are used to describe too many restaurants, but still popular enough to receive enough training data. Thus, the area surrounding the data-points in Figure 5 roughly assumes a triangle (or sickle, to be more accurate) shape.

Furthermore, we can examine the translation table for each cuisine type. Table 2 presents tokens with the high-est  X  values for selected cuisine types, in decreasing order of their  X  values.

First, perhaps not surprisingly, the most likely token for most cuisine types is the country of origin. One exception is Japanese , where, at least in the Yelp data, the word sushi is apparently more representative of Japanese restaurants than the word Japanese (which comes in as second). On the other hand, the American restaurants as a label used in Yahoo! Local data seems to lack a clear definition: the word American is not even among the top translations. Its lack of identity is clearly reflected in both the low  X  cuisine value and the lack of focus in its translation table. Healthy is a quasi-cuisine-type that did not have an existence as distinctive as other cuisine types listed here. While its translation table is not quite as focused, we still observe reasonable transla-tions such as organic, vegan, grains that are indeed quite representative of this type of food. In general, we observe both distinctive food items ( naan, masala for Indian ) and cuisine-related geographic terms ( havana, miami for Cuban ) in these top translations.
Reasonable translations were learned for very infrequent cuisine types as well. For instance, only eight restaurants were associated with Moroccan in the aligned data, yet it received words with distinctively Moroccan taste (e.g., cous-cous, tagine ) as its top translations. Note that even those cuisines with relatively lower  X  values (apart from Ameri-can ) received reasonable translations (e.g., foie gras, souffle for French ).
 On city . We now proceed to examine the translation Table 3: Examples from translation tables for the city attribute: top words  X  that are translated from  X   X  { chicago, (new) york, (san) francisco, boston } . Table 4: Examples from translation tables for the city attribute: top words  X  that translate into  X   X  { chicago, boston, bay, seattle } . tables learned for the city attribute. 1
Table 3 shows examples of top words  X  that are translated from  X   X  { chicago, (new) york, (san) francisco, boston } . First note that for (new) york and (san) francisco , popu-lar abbreviations ( nyc and sf ) were indeed found among top translations. In addition, when the physical location of a restaurant is in a metropolitan city, top translations often include neighborhood names (e.g., manhattan, meatpacking, chelsea, queens for (new) york ), notable streets (e.g., geary for (san) francisco and arguably michigan (ave) for chicago), and tourist attractions (e.g., fisherman, wharf for (san) fran-cisco and copley, faneuil for boston ).

On the other hand, when the physical address of a restau-rant is technically in a satellite city in a metropolitan area,
Since the city attribute will have only one city in its value, and we normalize  X  value for each attribute separately, there is essentially no competition, and  X  city is uniformly dis-tributed, thus not as interesting to examine. names of the metropolitan cities often appear in the corre-sponding reviews and are discovered to be likely translations. Table 4 presents notable examples. For instance, words that are most likely to translate into boston (i.e.,  X   X  { boston, newton, brighton, brookline, allston } ) are all neighborhoods in the  X  X reater Boston X  metropolitan area.
 On name . As we mentioned earlier, name is an inflexible-match attribute. Thus, there is no translation table learned: words picked (according to  X  name (  X   X   X  )) will translate only to itself. However, the  X  name values are still worth a brief dis-cussion. The main intuition that we hope to be captured by  X  name values is: certain common words such as  X  X estau-rant X  or  X  X afe X  are likely to be dropped when people refer to restaurant names in informal reviews. Indeed, our model captures this intuition. Words with the lowest  X  name values (in increasing order) are: restaurant, incorporated, bar, cui-sine, drive-in, ristorante, lounge, grill , etc. Note at match-ing time, this leads to higher normalized  X  name (  X   X   X  ) values to the other non-generic terms in name , thereby giving them higher weights in the name part of the matching score. This is a good time to step back for a moment and reflect. One might think the above effect is trivial to achieve by sim-ply reducing the weights (in the matching score) for common words in the corpus, which can be achieved by TfIdf . Un-fortunately, this does not work. First, the  X  values do not monotonically decrease with the word frequencies: some rel-atively frequent words (such as  X  X hai X ) still receive high  X  scores, which suggests that the model finds it more  X  X rof-itable X  to use the name attribute to account for such words. On the other hand, for words like  X  X estaurant X , even when they are officially part of the name of the object, our model finds it more  X  X rofitable X  to use the generic review language to account for them, leaving the mass of the probability from the name attribute to concentrate on other words that are best explained by this attribute. Through correct  X  X ttribu-tions X , such words are less likely to be picked from the name attribute in the end. Second, while lower  X  values are de-sired for some of the words with high collection frequency, this is not universally true and depends on different seman-tics behind different attributes. As we will see in Section 7.3, for the actor attribute of movie objects, the intuition is the exact opposite: words with higher frequency in collec-tion (names of more popular actors) are more likely to be picked than words with lower frequency in collection (names of unknown actors).
Our model provides a fairly general framework. Even though movie objects are very different from restaurant ob-jects, the underlying inference process remains the same.
In this section we briefly examine two attributes of movie objects. The first is a flexible-match attribute genre , where the results largely corroborate our findings on the dataset in that reasonable translation tables are learned. The second is an inflexible-match attribute actor , where the  X  values learned provide interesting contrast to our findings on the name attribute of restaurant objects.
 On genre . Table 5 presents examples from translation ta-bles for the genre attribute. Top translations for selected genres are summarized.
 On actor . As briefly mentioned in discussions on name in Section 7.2, higher  X  actor values are observed for well-known different genres. actors (e.g., bogart, hepburn, nicholson, pacino, hanks ), who appear more frequently in reviews, and lower  X  actor values are observed for unknown actors, who appear less frequently in reviews. This conforms to our intuition of the likelihood of actors being picked from cast members when a movie is being discussed in a review, but is in stark contrast to the case of restaurant name attribute, where the more frequent words were assigned lower  X  name values. As we noted earlier, even though the underlying inference process is the same, the different semantics behind different attributes lead to differences in the parameters quite naturally.
We developed a generic method for the review matching problem. We proposed a statistical translation model that incorporates the structured description of objects, for gener-ating reviews. The parameters of the model were estimated using an EM algorithm. This model was used to find, given a review, the object most likely to be the topic of the re-view. We conducted experiments on two real-world datasets, namely, a restaurant review collection from Yelp and a movie review collection from IMDb. Our experiments showed that the translation model is superior not only to traditional tf-idf based methods but also to a recent mixture model-based method for the review matching problem.
