 Score normalization is indispensable in distributed retrieval and fu-sion or meta-search where merging of result-lists is required. Dis-tributional approaches to score normalization with reference to rel-evance, such as binary mixture models like the normal-exponential, suffer from lack of universality and troublesome parameter estima-tion especially under sparse relevance. We develop a new approach which tackles both problems by using aggregate score distributions without reference to relevance, and is suitable for uncooperative engines. The method is based on the assumption that scores pro-duced by engines consist of a signal and a noise component which can both be approximated by submitting well-defined sets of arti-ficial queries to each engine. We evaluate in a standard distributed retrieval testbed and show that the signal-to-noise approach yields better results than other distributional methods. As a significant by-product, we investigate query-length distributions.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.2.4 [ Database Management ]: Systems X  query processing Experimentation, Performance, Theory Distributed retrieval, resource selection, meta-search, fusion, score normalization, score distribution, filtering, query length distribu-tion, query model, Zipf X  X  law, power-law.
Modern best-match retrieval models calculate some kind of score per collection item which serves as a measure of the degree of rel-evance to an input request. Scores are used in ranking retrieved items. Their range and distribution varies wildly across different models making them incomparable across different engines [25], Figure 1: Min-max normalized score distributions (top-1,000 results) even across different requests on the same engine if they are in-fluenced by non-semantic query characteristics, e.g. length. Even most probabilistic models do not calculate the probability of rel-evance of items directly, but some order-preserving (monotone or isotone) function of it [21].

For single-collection ad-hoc retrieval, the variety of score types is not an issue; scores do not have to be comparable across mod-els and requests, since they are only used to rank items per request per system. However, in advanced applications, such as distributed retrieval, fusion, or applications requiring thresholding such as fil-tering or recall-oriented search, some form of score normalization is imperative. In the first two applications, several rankings (with non-overlapping and overlapping sets of items respectively) have to be merged or fused to a single ranking. Here, score normalization is an important step [9]. In practice, while many users never use meta-search engines directly, most conventional search engines are faced with the problem of combining results from many sub-engines. For example, blending images, text, inline answers, stock quotes, and so on, has become common.

In filtering, bare scores give no indication on whether to retrieve an incoming document or not. Usually, a user model is captured into some evaluation measure. Some of these measures can be opti-mized by thresholding the probability of relevance at some specific level [19], thus a method of normalizing scores into probabilities is needed. Moreover, thresholding has turned out to be important in recall-oriented retrieval setups, such as legal or patent search, where ranked retrieval has a particular disadvantage in comparison with traditional Boolean retrieval: there is no clear cut-off point where to stop consulting results [22]. Again, normalizing scores to expected values of a given effectiveness measure allows for optimal rank thresholding [5].

Popular methods, e.g. range normalization based on minimum and maximum scores, are rather naive, considering the wild va-riety of score outputs across search engines, because they do not take into account the shape of score distributions (SDs). Figure 1 demonstrates the variety of outputs across different systems for constant query and collection. Although these methods have worked reasonably well for merging or fusing results [18], more advanced approaches have been seen which try to improve normalization by investigating SDs. Such methods have been found to work at least as well (or in some cases better) than the simple ones in the context of fusion [20, 13]. They have also been found effective for thresh-olding in filtering [7, 29, 11] or thresholding ranked lists [5]. We are not aware of any empirical evidence in the context of distributed retrieval.

The main aim of this paper is to analyse and further develop score distributional approaches to score normalization. Our under-lying assumption is that normalization methods that take the shape of the SD into account will be more effective than methods that ignore it. We want to make no assumptions on the search engines generating the scores to be normalized other than that they produce ranked lists sorted by decreasing score. Thus, we treat each engine as a  X  X lack-box X  and are interested in approaches based only on ob-serving their input-output characteristics: the queries and resulting score distributions.

In Section 2 we examine the state of affairs in using mixture-model normalization methods to map scores to probabilities of rel-evance. We note theoretical as well as practical problems and limi-tations in the applicability of such methods. In Section 3 we inves-tigate the single SD model of [12], arguing that some of its com-plexity is unnecessary. In Section 4 we take a novel approach and introduce three new methods. Two single SDs are used, represent-ing score signal and noise, and input scores are normalized accord-ing to their signal to noise ratios. The two SDs are generated as score aggregates of two different types of artificial queries, human-like and noise respectively. For query generation, we develop two query models in Section 5. As a significant by-product, we investi-gate the length distributions of such input queries; theoretically for noise, and using real data for humans. In Section 6 we evaluate the methods in a standard distributed retrieval testbed. This is also the first evaluation of the currently most-popular mixture model in a distributed retrieval setup. Conclusions are drawn in Section 7. Under the assumption of binary relevance, classic attempts model SDs, on a per-request basis, as a mixture of two distributions: one for relevant and the other for non-relevant documents. Given the two component distributions and their mix weight, the probabil-ity of relevance of a document given its score can be calculated straightforwardly [7, 20], essentially allowing to normalize scores into probabilities of relevance. Furthermore, the expected numbers of relevant and non-relevant documents above and below any rank or score can be estimated, allowing to calculate precision, recall, or any other traditional measure at any given threshold enabling its optimization [5]. Assuming proper component choices, such meth-ods are theoretically  X  X lean X  and non-parametric.

Various combinations of distributions have been proposed since the early years of IR; for a recent extended review and theoretical analysis of the choices, we refer the reader to [25]. The currently most popular model being that of using a normal for relevant and an exponential for non-relevant, introduced in [3] and [7, 20] and followed up by [29, 11] and others. The latest improvements of the normal-exponential model use truncated versions of the component densities, trying to deal with some of its shortcomings [5]. In this study, we do not set out to investigate alternative mixtures, but just use the standard normal-exponential.
In this section, we investigate the theoretical as well as the em-pirical evidence for using a mixture of normal-exponential for mod-eling SDs in IR. We note theoretical as well as practical problems.
A theorem claims that the distribution of relevant document sco-res converges to a Gaussian central limit (GCL) quickly, with  X  X or-rections X  diminishing as O(1/k) where k is the query length [7]. Roughly, three assumptions were made: a) terms occurring inde-pendently, b) scores are calculated via some linear combination of document term weights, and c) relevant documents cluster around some point in the document space with some hyper-ellipsoidal den-sity with fast-falling tails. Based on those assumptions, the proof is more likely to hold in setups with vector space or geometric models with dot-product or cosine similarity scoring and long queries. This does not mean that the Gaussian does not get along well with other retrieval models or setups, but we have not found any supportive theory in the literature.

Although the GCL is approached theoretically quickly as query length increases, practically, queries of length above a dozen terms are only possible through relevance feedback and other learning methods. For short queries, the Gaussian may simply not be there to be estimated. Empirically, using a vector space model with scores which were unbounded above on TREC data, [7] found us-able Gaussian shapes to form at around k = 250 . k also seemed to depend on the quality of a query; the better the query, the fewer the terms necessary for a normal approximation of the observed dis-tribution. Along similar lines, [20] noticed that better systems (in terms of average precision) produce better Gaussian shapes.
Under a similar set of assumptions and approximations to the ones mentioned above, [7] investigate also the distribution of non-relevant document scores and conclude that a GCL is unlikely and if it appears it does only at a very slow rate with k  X  X ractically never seen even for massive query expansion. Nevertheless, such a theorem does not help much in determining a usable distribu-tion. In absence of a related theory or a simpler method, the use of the exponential distribution has been so far justified empirically: it generally fits well to the high-end of non-relevant item scores, but not to all.

Different cutoffs have been used for fitting purposes: [7, 11] fit on the top 50 X 100, [20] fit on almost the top-1,000 (1,000 minus the number of relevant documents). [2] fits even on a non-uniform sam-ple of the whole score range, but the approach is rather system/task-specific. In general, it is difficult to get a good exponential fit on the whole score range. Figure 2 shows the total score densities pro-duced by a combination of two queries and two sub-collections of TREC-4 using KL -DIVERGENCE as a retrieval model. Obviously, none of these SDs can be fitted in totality with the mixture. Candi-date ranges are usually [ s peak , +  X  ) where s peak is set at the mode (or higher) of the total SD.
Figure 2: KL-divergence score densities; 2 queries on 2 collections.
The normal-exponential model has a serious theoretical problem: the estimated probability of relevance as a function of the score is non-monotonic. This behavior is a direct result of the Gaussian falling more rapidly than the exponential and hence the two density functions intersect twice. In most cases, this non-monotonicity ap-pears inside the effective score range making the probability of rel-evance decline as the score increases above some point. Robertson [25] formulates the problem as non-convexity of the recall-fallout curve as seen from recall=1, fallout=0.

In adaptive filtering, [3, 7] deal with the problem by selecting as threshold the lower solution of the 2nd degree equation resulting from optimizing linear utility measures, while [29, 11] do not seem to notice or deal with it. In meta-search, [20] noted the problem and forced the probability to be monotonic by drawing a straight line from the point where the probability is maximum to the point [1 , 1] . Both procedures, although they may have been suitable for the above tasks, are theoretically unjustified.

In [5], the two component SDs were set to uniform within the offending score range; this is equivalent to randomization of the corresponding sub-ranking and it is justified as enforcing recall-fallout convexity. Nevertheless, this implies that the normal-expo-nential mixture is theoretically valid and that scoring formulas pro-duce suboptimal rankings which can be improved by randomiz-ing; evidence of such scoring schemes within modern text retrieval systems has not been found [6]. Consequently, we choose not to use any fixes in the experiments in this paper; we just assign the maximum-reached output probability to input scores affected by the non-monotonicity. 1
Despite the above-mentioned theoretical problems, the model was applied successfully in fusion, with short queries and even with a scoring system which produces scores between 0 and 1 with-out worrying about the implied truncation at both ends for the nor-mal and at the high end for the exponential [20]. In the context of thresholding for document filtering, with the generally unbounded scoring function BM25 and a maximum of 60 query terms per pro-file, the method performed well (2nd best, after Maximum Likeli-
In practice, however, since trec_eval would sort on docid all documents with the same score, for the only purpose of preserving the order in evaluation we break the ties by multiplying each of the affected scores with 1 + , where is a very small number whose magnitude depends on the original score. hood Estimation) on 3 out of 4 TREC data sets [11]. We are not aware of any work related to the use of normal-exponential in dis-tributed setups.

A recent study on the applicability of the mixture on a wide vari-ety of modern text retrieval systems shows support for vector space or geometric models, as well as BM25, as being amenable to the normal-exponential. While longer queries tend to lead to smoother SDs and improved fits, the end-result in thresholding is better for the short title queries with high quality keywords [6].
All mixture models, irrespective of component choices, present some practical problems. Estimating the component densities is best done when many relevance judgements are available. In prac-tice, relevance judgements are not available at all, or they are sparse, incomplete, or biased, making difficult the parameter estimation of a mixture.

In the contexts of meta-search [20] and adaptive filtering [2], the mixing parameter and the parameters of the component normal and exponential densities have been estimated without using any rele-vance judgements. The standard iterative expectation maximization (EM) method [24] was used with some success. The method can be modified to take into account relevance judgements, if any, never-theless, it was found to be  X  X essy X  and difficult to tune. It was very sensitive to the choice of the initial parameter values in converging to a global optimum rather than a local one.

When normalizing scores, especially of non-cooperative engines, one should keep in mind that systems produce scores in order to rank documents and do not care about the scale or shape of the scoring function. Therefore, system components which do not af-fect the ranking may be added or removed arbitrarily, in order to, e.g., simplify calculations. Components which affect only the scale are not a problem for mixture models. However, many transforma-tions affect the shape as well, e.g. using a logistic function to map (  X  X  X  , +  X  ) to [0 , 1] ; in such cases, the initial choice of the density components may not apply any longer.
The analysis of Section 2 suggests that the normal-exponential mixture is not universal in modeling SDs in IR; some retrieval mod-els perhaps could better be fitted with different mixtures, as in the case of KL -DIVERGENCE (Figure 2). Furthermore, the model has a serious theoretical problem: it does not satisfy the convexity con-dition, i.e. the output score does not monotonically increase with the input score. The problem shows always at the top of rankings, and it does not seem to be severe for thresholding tasks where an optimal threshold may often be lower than the non-convex  X  X lind X  range, depending on the measure under optimization [5]. The prob-lem is more acute in environments favoring initial precision such as in meta-search and distributed retrieval.

To make things worse, there are practical problems in estimat-ing the parameters of mixture models, usually due to insufficient numbers of relevance judgements or quality of them (biases, in-completeness). Approaches which do not use relevance judgements seem difficult to tune, especially when relevance is sparse. Test col-lections are usually made in such ways that there is some minimum number of relevant items per request. In reality, given a collec-tion, there can be no relevance for some queries. The same can happen when test collections are split further in order to facili-tate distributed retrieval setups. As a result, score distributional approaches to score normalization without reference to relevance may have some merit.
A standard method for score normalization that takes the SD into account is the Z -SCORE . Scores are normalized, per topic and en-gine, to the number of standard deviations that they are higher (or lower) than the mean score: where  X  is the mean score and  X  the standard deviation. The mean and standard deviation depend on the length of the ranking. SCORE seems to assume a normal distribution of scores, where the mean would be a meaningful  X  X eutral X  score. As it is well-known, actual SDs are highly skewed and clearly violating the assump-tion underlying the Z -SCORE . Although not very popular in IR, -
SCORE was used with reasonable success in [26, 16].
A recent attempt models aggregate SDs of many requests, on per-engine basis, with single distributions [12, 13]; this enables normalization of scores to probabilities X  X lbeit not of relevance X  comparable across different engines. Nevertheless, it is not clear X  if it is even possible X  X ow using a single distribution can be applied to thresholding, where for optimizing most common measures a reference to (or probabilities of) relevance are needed. Per engine, the proposed normalization is where P ( S  X  s ) is the cumulative density function (CDF) of the probability distribution of all scores aggregated by submitting a number of queries to the engine, and F is the CDF of  X  X he score distribution of an ideal scoring function that matches the ranking by actual relevance. X  The F  X  1 transformation is called  X  X tandard-ization step, X  it is common across all engines participating in a fu-sion or distributed setup, and considered critical to the method for compensating for potential individual system biases.

In a large fusion experiment using TREC Web Track data, [13] found that the method performs better than CombSUM (with stan-dard or rank-sim normalization) and CombMNZ [18]. For score aggregation, historical queries were used, and only 25-50 seemed enough for good end-results. The method seems very promising, however, unnecessary complicated as we explain next.
By definition, F is monotonically increasing since it is a CDF. Its quantile function F  X  1 is also monotonically increasing, and since it is applied as a constant transformation to all engines it has no effect on rankings or the comparability of normalized scores across engines. Thus, at least in distributed retrieval setups where normal-ized ranked lists are simply merged, F  X  1 has no impact and it can safely be removed from the calculation. Nevertheless, it has an un-clear impact and interpretation when scores are combined, e.g. in meta-search/fusion setups. The distribution in question is roughly approximated by the  X  X verage distribution of several good scoring systems X , not a very well-defined concept.

Consequently, we find it hard to see why the combination of functions in Equation 2 returns the probability of relevance or any other meaningful number, and since F  X  1 is constant across engines we settle for the simpler method where HIS refers to the fact that historical queries are used for ag-gregating the SD that the random variable S HIS follows. HIS malizes input scores s to the probability of a historical query scor-ing at or below s . The aggregate historical SD is an average which can be seen as produced by an  X  X verage X  historical query. In this respect, HIS normalizes the SD of the  X  X verage X  query to uniform in [0 , 1] . This is equivalent to the Cormack model [15], assuming such an  X  X verage X  query is sensible and exists.
In this section we introduce a novel approach based on dual ag-gregate SDs but without reference to relevance. Assuming that scores produced by an engine consist of two components, signal and noise, the score random variable S can be decomposed as: The probability densities of the components are given respectively by p SIGNAL and p NOISE defined across the engine X  X  output score range. As we will discuss in detail in the next section, the probability den-sities will be estimated, per engine, by submitting appropriate sets of queries to an engine and observing the output scores.
Furthermore, we assume  X  X table X  system characteristics for the engine in the sense that the signal and noise levels at a score depend only on the score. We can define a function which normalizes input scores s into the fraction of the signal at s : Since engines are expected to produce increasing signal-to-noise ratios as score increases, this may be an interesting normalization. Nevertheless, the magnitude of the original score is not taken into account.

An obvious improvement would be to multiply S / N with s , a pro-cedure which would reduce s to its fraction of the signal it contains. But bare scores are usually not directly comparable across engines. As a form of calibration, we could instead use the HIS normaliza-tion of scores: The resulting scores would be comparable across engines, however, the distribution of the variable S HIS depends on some ill-defined and maybe problematic set of historical queries.

Using historical queries, although very feasible and no coopera-tion is required, may lead to instabilities and biases: To deal with this, we can instead use the variable S SIGNAL The last factor calibrates s to the probability of having signal at or below s .

None of the three proposed normalizations (Equations 4-6) guar-antees to preserve ranking order, while the original historical CDF and HIS (Equations 2 and 3) do. In theory, the S / N component does not have to be monotonically increasing with the score. This may be a desirable effect X  X mproving rankings X  X r not. In practice, is more or less found to be monotonically increasing, at least in the current experimental setup (Section 6). Trying to enforce mono-tonicity in the S / N transfer functions with interpolation led to prac-tically the same end-results, so we report without interpolation.
The question is how to approximate p SIGNAL and p NOISE per en-gine. Seeing engines as black-boxes similarly to the original his-torical CDF approach and HIS , we can feed each one with queries of appropriate types and generate the needed functions based on the statistical properties of the observed output scores. Next, we de-velop two models for generating artificial queries given a document collection. The resulting query sets may be suitable for producing
The SDs that modern retrieval systems produce may be affected by several query characteristics. We consider three statistical fea-tures of queries: 1. Frequencies of term occurrences (e.g., Zipf, uniform). 2. Term dependencies (e.g., serial, long-range, independent). 3. Length (e.g., power-law, Poisson, etc.).
 In this section, we examine these features for two query types sit-ting at the two extreme sides of the query spectrum: a) garbage (monkey/random) queries, and b) natural language queries.
While current query-logs show real queries consisting of bags of keywords, in any order, maybe mixed with natural language frag-ments (a result of the realization by users of how current systems treat their queries), this behavior is most likely to change once users realize that some system is  X  X nderstanding X  natural language. The rationale for considering the two extreme types of queries is that those bounds are not going to change as systems or user behavior changes, so they provide a well-defined framework for our meth-ods.
In parallel to the popular thought experiment of a monkey hitting keys at random on a typewriter, let us imagine a keyboard with the terms of a query language on its keys plus  X  X nter X . The keys are considered equally accessible and of equal size, except  X  X nter X  which has a different size and thus different probability to be hit if keys are hit at random.

The monkey, not understanding the grammar and semantics of the query language, will select terms uniformly. Moreover, terms will be independent. Next we will examine the query length. Prob-abilistic foundations for the following analysis can be found in [23].
If p is the probability of hitting  X  X nter X , then the probability that the monkey will type k terms before hitting  X  X nter X  is given by (the discrete analogue of the exponential distribution called) the geometric distribution : Note that a p fraction of the total queries will be of zero-length. The mean query length will be 1 /p .

Assuming r monkeys using identical keyboards (characterized by the same p ) are typing independently, the random variable K = P m =1 K m , where K m is the geometrically distributed variable associated with the m th monkey, follows a negative binomial dis-tribution : Under an alternative parameterization, lim r  X  X  X  g ( k ; r,p ) converges to the Poisson distribution with a rate  X  = r (1 /p  X  1) .

Alternatively, one may model a single monkey X  X  query length as a Poisson distribution, i.e. as the number of terms the mon-key will issue in a fixed time period t if it provides terms with a known average rate  X  1 and independently of the time passed since the last issued term. This setup may also be plausible and it in-volves the speed of typing and a fixed-time restriction. Neverthe-less, for r monkeys producing independently Poisson distributed query lengths K 1 ,K 2 ,...K r with  X  1 , X  2 ... X  r , the sum K is also Poisson distributed with  X  = P r m =1  X  m .

Consequently, the Poisson distribution is a good approximation of query lengths generated by a large number of monkeys. This query model is not dependent on the query language and it may apply to non-text retrieval as well.
Restricting the problem to textual data and natural language que-ries, it is well-known and many times confirmed that the distri-bution of word frequencies follows the (generalized) Zipf X  X  law (Equation 7 in Section 5.2.1) with s slightly more than 1 for most types of texts. Exceptions include legal texts which have s  X  0 . 9 , showing that lawyers use more unique words than other people [14]. In any case, it seems that across natural languages and text types, s is varying a little around 1.

Query terms occur, in general, in a dependent way (i.e. the occur-rence of one makes the chances of occurrence of some others better than random) due to all of them pointing at the same topic. For nat-ural language queries, there exists also serial dependence, imposed by grammar and semantics. When incorporating dependencies, re-trieval models are becoming practically intractable, which led in the past to the infamous term independence assumption .

Instead of trying to model term probabilities of occurrence and dependencies, we can rather tackle both features at once by picking real text fragments out of a corpus. The remaining question is how long those fragments should be.
From analyzing query-logs, previous research has found that the distribution of query lengths can be approximated with the (gen-eralized) Zipf X  X  law 2 [27, 30]. However, the law appears to fit well to the largest query-length observations, k  X  k 0 , but not for the whole sample, where k 0 depends on the domain. For exam-ple, empirical observations show that the length frequency for web queries peaks at 2 rather than at single keyword queries, suggesting a k 0 &gt; 2 . Others, without empirical justification, modeled query lengths with a Poisson distribution by setting its mean to the aver-age query length [8].

Using a Zipf distribution, in a population of N queries the num-ber of queries with length k is given by rounded to the nearest integer, where s is a positive real num-ber and H N,s is the N th generalized harmonic number , H
Or, in general, a power-law . Zipf X  X  law can also be shown equiv-alent to Pareto X  X  distribution by variable exchange. Essentially, the Zipf and Pareto distributions are both power-laws [1]. P quencies, it may be naive to extend this to query lengths.
We analyzed the query lengths of the 10 k queries of TREC Mil-lion Query Track 2007. This set contains queries with lengths be-tween 1 and 30 with a mean length of 4.11 and the peak of the dis-tribution at length 3. We set k 0 = 5 (just above the mean) and esti-mated s for k  X  k 0 . Rather than (logarithmically) binning the data, we instead looked at the Pareto CCDF P ( X  X  x ) = ( x/k 0 obtain a good fit (Figure 3). The tail naturally smoothes out in the cumulative distribution and no data is  X  X bscured X  as in a binning procedure. This procedure is described in [1]. Fitting the Pareto CCDF, we found a = 4 . 51 , consequently s = a + 1 = 5 . 51 . Since this is a very good fit, we have no doubts that for k  X  k the distribution can be modeled with a generalized Zipf-law with s = 5 . 51 .

We tried the same procedure on the union of query sets ( &lt;Re-questText&gt; fields) of the TREC Legal Track 2006 and 2007 and estimated an s 0 = 4 . 9 . Although this number is not to be trusted due to the small size of the query set (only 96 queries) which gave a very rough fit, it strengthens the evidence that the Zipf shape parameter is closer to 5 for query lengths rather than 1 (as for word frequencies). This gives a much steeper slope on log-log plots. In a further study, using 3 extra data-sets and a different X  X ore widely-accepted X  X ethod for fitting, we have reached similar results [4].
Figure 4 shows the resulting Zipf fit on the data for s = 5 . 51 and H 10 4 , 5 . 51 = 1 . 025 . We also show (with impulses) the Poisson distribution used by [8], by setting the mean to the average query length. 3 We can see that while it matches well the data from 1 to 10, it lacks the tail for k&gt; 10 . The Zipf model matches better in a wider range of lengths, from 5 to longer than 20.

In summary, the bulk of queries concentrates at short lengths where a power-law does not fit at all given the current query lan-guages, therefore it makes practical sense to use a truncated mix of Poisson-Zipf to generate query lengths. In such a practical model, the lengths are Poisson-distributed for k&lt;k 0 while they are Zipf-distributed for k  X  k 0 . The choice of k 0 depends on the specific domain (i.e., a combination of features of the document collection, query/indexing language, and pattern of use of the system). As a rule of thumb, k 0 seems to be just above the mean observed query length.
We expect the garbage input to generate aggregate SDs which approximate S NOISE . Furthermore, generating queries by picking natural language fragments out of collection documents, some rel-evance is guaranteed; so we expect to approximate S SIGNAL gregating the resulting SDs. Of course, those two aggregate SDs are bound to contain spillovers, i.e. noise contains some signal and the other way around, but they may be good enough for the signal-to-noise score normalization methods.

A disadvantage of the proposed query models in the current con-text is that they use the collection indexed in each engine. There-fore, they can only be applied by vendors themselves (co-operation with other engines is not needed), if there is interest to provide nor-malized output for further use. If there is no such interest, signal-to-noise characteristics of non-cooperative engines can still be de-duced by others using alternative ways. Garbage queries could be generated in a more generic way without looking into individual collections. For approximating signal, using the internal collec-tion seems more suitable; nevertheless, query-based sampling tech-niques [10] of non-cooperative engines X  collections may come to rescue.
In this section, we will experiment with the new signal-to-noise approach to score normalization. Earlier work has so far concen-trated on fusion, but we will focus on distributed retrieval where the engines have no documents in common. In fusion experiments with TREC-3 and TREC-6 data, [20] found that the normal-exponential mixture performs as good as CombSUM and CombMNZ. In a large fusion experiment using TREC Web Track data [13], the histori-cal CDF method performed better than CombSUM (with standard or rank-sim normalization) and CombMNZ [18]. Consequently, both methods perform at least as well or better than CombSUM and CombMNZ in fusion, but they have never been compared against each other. Furthermore, none of these methods has been evalu-ated before in distributed retrieval which is more challenging than fusing systems retrieving the same sets of documents.
The L EMUR Toolkit 4 v4.6 was employed in three different  X  X la-vors X : i) TF . IDF with scores in (0 , +  X  ) , ii) the KL language model with scores in (  X  X  X  , 0] , and iii) the OKAPI
According to the analysis in Section 5.1, [8] used query lengths generated by r monkeys rather than humans. www.lemurproject.org Table 1: Relative performance of 3 retrieval models on TREC-123 KL -DIV 0.4980 0.4590 0.4347 0.4345 0.4157 0.1646 KL -DIV 0.4680 0.4260 0.4000 0.3700 0.3407 0.2036 bilistic model with scores in (  X  X  X  , +  X  ) . 5 The default parameters that come with this version of the toolkit were used; we just enabled Porter stemming and the SMART stop-word list in indexing. To test the setup, we did runs on a standard index with all the documents, using each of the three  X  X lavors. X  Table 1 presents the results of the three retrieval models, and may serve as a performance ceiling for the distributed retrieval experiment. All three models perform reasonably at MAP and early precision, making our experimental results representative for a large group of systems.

We used two collections, the TREC-123 and TREC-4 data, each one partitioned in 100 non-overlapping sub-collections according to the CMU split [10]. The TREC-123 split is by document source and relevant documents appear scattered throughout all the sub-collections. The TREC-4 split is topically clustered and relevant documents appear in relatively few sub-collections. To build the test engines, we applied the three retrieval models to the sub-col-lections in a round-robin fashion, i.e., TF . IDF / KL -DIV .
 IDF / . . .

As queries, we used the TREC topics corresponding to the col-lections, i.e. 51-150 for TREC-123 and 201-250 for TREC-4. We only processed the &lt;desc&gt; field; this is the only field that the ex-tended set of topics 51-250 have in common. We evaluate on 100 and 50 queries respectively but we use topics 151-200 as historical data as we will see below.

Most of the normalization methods we compared are based on observed SDs for which data are sparse at the top scores/ranks. Thus, following common practice in distributed retrieval, we found most appropriate to evaluate with average precision at ranks 5, 10, 15, 20, and 30.

We were also interested in how the methods interact with re-source selection. We employed the standard resource selection method of L EMUR , CORI [9], to select only the top-10 sub-col-lections per query.
Our approach is based on the assumption that effective score nor-malization needs to take the SD into account. Before trying the new normalization methods, we will investigate whether our assumption indeed holds.

Standard score normalization methods like the MinMax ignore the score distribution: s 0 = s  X  min max  X  min , with min ( max ) the mini-mal (maximal) score per query and engine [17]. That is, MinMax forces all scores in [0,1], resulting in a maximal score per topic and engine of 1. In our distributed retrieval set-up, as a result, the
We also intended to use I N Q UERY in order to have as diverse models as possible, but we discovered a bug in its implementation in the current L EMUR version. The bug most likely does not af-fect ad-hoc retrieval performance, but it affects tasks where score normalization is needed.
 Table 2: Distributed retrieval results for TREC-123 and TREC-4 over ROUNDROBIN 0.1835 0.1835 0.1835 0.1835 0.1835 first 100 results per topic will have the maximal score of 1, and we will be doing effectively a round-robin picking the top result of each engine. Hence, we will look at ROUNDROBIN to represent the familiar score normalization methods that ignore the shape of the SD.

The effectiveness of ROUNDROBIN is strongly dependent on the order of the engines, and given their disjoint collections, the pre-cision up to rank 100 depends only on their order. To avoid this arbitrariness, we calculated the average precision over all possible orderings of the engines; this turns out to be equal to P100 for all ranks down to 100. 6 The results of ROUNDROBIN are shown in Ta-ble 2. It performs poorly on the topically clustered TREC-4, and somewhat better on TREC-123. Although MinMax is one of the most obvious baseline methods for score normalization, it is clear that it is a weak baseline with flat precision up to rank 100 in our set-up.

We calculate also the Z -SCORE over the top 1,000 results, and over the top 250 results per query. The results of Z -SCORE also shown in Table 2, and it performs significantly better than ROUNDROBIN . This is a strong argument in favor of taking the SD into account. Even more so given that Z -SCORE seems to as-sume a normal distribution of scores, where the mean would be a meaningful  X  X eutral X  score. As we have seen before, actual SDs are highly skewed and clearly violating the assumptions underlying the -
SCORE . Hence, we would expect even better performance from a method that is tailored to the sort of SDs we deal with in IR.
As we will see below, the normal-exponential model gave poor fits on sub-collections assigned the KL -DIVERGENCE model and on some with few relevant documents; thus, it would have made a weak baseline. The historical CDF approach [13] we discussed in Section 3 is one of the latest well-performing distributional meth-ods in the literature. It derives a transfer function from historical data (hence we call its simplified version without the extraneous
The average precision over n engines with disjoint content, when averaged over all possible orderings, is equal to P n for all ranks down to n . We omit the full proof but only calculate the P n and P1: Suppose that m (0  X  m  X  n ) engines have a relevant top re-sult. No matter how the engines are ordered, at rank n we will have selected precisely the top result of all n engines, and hence P n will be m/n . What will the P1 be? In exactly m of the choices of the first engine, P1 will be 1, and in the remaining n  X  m choices of the first engine, P1 will be 0. Hence, averaging over all possible orderings, average P1 will also be m/n . The proof for the interme-diate ranks is more elaborate, but follows similar arguments, and is omitted. standardization step X  X s we argued in Section 3.3 X  HIS ), and its effectiveness is shown in Table 2. As it turns out, HIS is also signif-icantly better than ROUNDROBIN . On TREC-123, HIS is similarly effective to Z -SCORE , and on the topically clustered TREC-4 it is clearly better than Z -SCORE . HIS achieves roughly 50% of the pre-cision obtained on the full index in Table 1. Hence, we will use as a strong baseline for the experiments with the signal-to-noise approach.
We compared HIS against the older normal-exponential normal-ization NORMEXP , and the new signal-to-noise methods S /  X  HIS , and S / N  X  SIG .

As historical queries we used the &lt;desc&gt; fields of the remain-ing TREC topics 151-200 in order to a) construct the transfer func-tions for the HIS runs, and b) set the average query length parame-ter of both artificial query models to  X  = 23 . 3 and k 0 reported good end-results even with only 25-50 historical queries, and since we only have 50 historical, we use 50 artificial ones for the signal-to-noise runs per query type (i.e. 50 for signal and 50 for noise).
 Figure 5 shows the high-ends of some typical transfer functions. For illustration we randomly selected 3 engines, one for each re-trieval model, and depict the log-odds of the output scores since they are very close to 1. The calculations are limited by the ma-chine and software arithmetic precision at around 36.74 log-odds, where the transfer functions flatten out. The extend of the non-flat areas were deemed sufficient for the distributed retrieval setup for three out of the four methods; only S / N moves to too large num-bers sometimes a bit too early, i.e. before the top score of a run is reached. We employed kernel density estimation techniques [28] for estimating the required probability densities from data.
Concerning the NORMEXP run, using EM for parameter estima-tion without relevance judgements or any indication of where the component densities may lie, is a very difficult task. We tried fit-ting all scores in [ s peak , +  X  ] but the reasonable fits did not cap-italize in end-results; fitting, e.g., on a fixed top-1,000 gave better retrieval precision. We tried numerous initial settings in EM, but no settings seemed universal. While some settings helped a lot some queries, they had a negative impact on others. After a few cycles of optimizing-evaluating, we obtained the best end-results for the following settings: fit on the top-100 scores with initial parameter values for the densities (i.e. the means of the normal and exponen-tial, and standard deviation of the normal) bootstrapped randomly in the interval ( s 100 ,s 1 ) and a mixing parameter in (0 , 1) . The ran-dom initial parameters approach worked better than looking into specific ranges, because we run EM ten times and selected the fit with the least square error with the score data.
 We did two batches of runs: with and without resource selection. Without resource selection, retrieval was performed on all sub-collections. All document scores per engine were passed through the engine X  X  transfer function, and the resulting ranked lists from all engines were simply merged. For the runs with resource selection, we merged only the results of the top-10 sub-collections as selected by CORI.
Table 3 presents the distributed retrieval results without resource selection. Compared to the most similar X  X ut not directly compara-ble X  X etup in the literature, namely that of [21] on TREC-123 only, our precision seems to be lower. Our setup is more challenging than the last-cited study due to the use of 3 different retrieval models as-signed in a round-robin fashion over the 100 sub-collections rather than just a single model. Furthermore, we did not tune anything on the collection but used the default LEMUR settings.

Overall, the S / N  X  HIS and S / N  X  SIG runs show significant improve-ments over the strong baseline of HIS , while the consistent improve-ments in S / N are mostly non-significant. S / N  X  SIG obtains roughly 70% of the scores on the full index in Table 1. NORMEXP is dis-appointing across the board, especially for the topically clustered TREC-4, and it does even worse than ROUNDROBIN shown in Ta-ble 2. Table 3: Distributed retrieval results for TREC-123 and TREC-4 over Table 4: Distributed retrieval results for TREC-123 and TREC-4 over
Table 4 presents the distributed retrieval results with resource selection. Here, HIS performs substantially better than in Table 3 above. Also NORMEXP now performs very close to the baseline of
HIS in TREC-123, but it still loses in TREC-4. Overall, S tends to be less effective than HIS , but the differences are mostly insignificant. S / N  X  HIS and S / N  X  SIG are most of the time better than HIS but also here the differences are insignificant.

While NORMEXP is disappointing in the distributed experiment, it works well in fusion, as other studies have found in the past. Further examination revealed two main reasons for not performing well in this particular distributed retrieval experiment. First, some sub-collections have very few or no relevant documents. In these cases, Gaussian fittings became spurious, hurting end-performance. Resource selection effectively eliminates most of this problem. Sec-ond, the KL -DIVERGENCE , as shown in Figure 2, was a problem-atic model fitting-wise. In the current setup, we obtained reason-able normal-exponential fits on the SDs produced by OKAPI .
 IDF , with the latter giving the best fits.
 Remarkably, while S / N and S / N  X  HIS are also profiting from CORI, /
N  X  SIG tends to perform worse on the 10 selected engines than on all engines in Table 3. Apparently, this normalization does not need resource selection, or in other words it takes care of resource selec-tion by assigning the higher scores to the engines containing the relevant documents. Of course, resource selection may be needed in practice for efficiency reasons.
We investigated new methods for score normalization, a funda-mental IR problem affecting all settings where results from differ-ent search engines have to be compared or merged. The approach taken deduces normalization functions from the observed aggregate SDs for statistically well-defined query inputs. For this purpose, we have developed two concrete input query models, one for natu-ral language fragments and the other for garbage/noise, dealing not only with the choice of keywords but also with query length. These query models produce SDs which are supposed to approximate the score signal and noise of each engine, but X  X espite the positive end-results X  X hey may not be the best for the purpose. Further re-search into more suitable input query models may lead to improved performance.

The proposed normalization methods characterize each engine with a single score transfer function. The implicit assumption made is that engines provide  X  X table X  outputs, in the sense that scores pro-duced by a given engine for different queries are at least compara-ble with each other. In this respect, we generated SDs for  X  X verage X  queries. Nevertheless, if the SDs of a retrieval model are affected greatly by some specific query feature, e.g. the sum of IDFs, query length, etc., then profiling could be done in ranges of values of such a feature. The methods may be computationally expensive but practically feasible and efficient, since transfer functions can be pre-calculated offline and may only have to change with signifi-cant collection updates.

We conducted a series of experiments trying to establish the practical utility of the resulting normalization methods for merging rankings in a distributed retrieval setup. In a first set of experiments we compared existing methods of score normalization: i) round-robin corresponding to score normalizations that maps the highest score to 1, ii) z-score as an non-IR normalization method that takes the distribution into account, and iii) a cumulative density function (CDF) derived from aggregating scores resulted from a set of his-torical queries. The results clearly showed that taking the shape of the SD into account leads to significantly better performance. We decided to take the historical CDF approach, performing at roughly 50% of the non-distributed runs, as a strong baseline for further experiments.

The second set of experiments investigated the effectiveness of the signal-to-noise approaches, in relation to the earlier normal-exponential model and to the historical CDF method. The signal-to-noise methods led to significantly better performance than the historical CDF method, performing as high as 70% of the non-distributed runs. We also compared against the normal-exponential mixture model for score normalization X  X ts first evaluation in a dis-tributed setup as far as we are concerned X  X hose performance did not live up to its underlying theory. The review of the normal-exponential should serve as a starting point for improving mixture SD models.

Any mixture model would be bound by sparse, incomplete, or biased relevance, making the estimation of the component densi-ties difficult. The sparsity weakness showed up in our distributed retrieval experiments where some sub-collections had few or no rel-evant documents. Where enough relevant documents exist, or when some relevance judgements are given, the model has performed well as shown in previous studies. In addition, its lack of universal-ity affected greatly our results: the mixture did not fit well in one of the three retrieval models we used, namely, the KL-divergence. How we dealt with the non-convexity may have also played a role; effectiveness may vary across different ways of fixing the problem, especially in tasks favoring initial precision.

The third set of experiments looked at the effectiveness of the above methods in combination with resource selection (RS). With RS, the signal-to-noise methods still improve over the historical CDF method, but the improvement is no longer significant. Where the RS leads to substantial better scores for the historical CDF method, it fails to improve for the signal-to-noise approach. This can be interpreted as a positive sign. In an ideal normalization, e.g. where scores are normalized to probabilities of relevance, any kind of RS will hurt effectiveness. In such an ideal situation, the more systems one combines, the better the effectiveness. The fact that RS no longer improves the effectiveness of distributed retrieval suggests that we are breaking a  X  X heoretical ceiling X  in normaliza-tion methods. The ultimate goal is to perform as well as the non-distributed index.
This research was supported by the Netherlands Organization for Scientific Research (NWO), CATCH programme, under project number 640.001.501. We thank Nir Nussbaum (University of Am-sterdam) for the programming support, George Paltoglou (Univer-sity of Macedonia) for the technical advice, and Marijn Koolen (University of Amsterdam) for participating in the initial valuable discussions. [1] L. Adamic and B. Huberman. Zipf X  X  law and the internet. [2] A. Arampatzis. Unbiased s-d threshold optimization, initial [3] A. Arampatzis, J. Beney, C. H. A. Koster, and T. P. van der [4] A. Arampatzis and J. Kamps. A study of query length. In [5] A. Arampatzis, J. Kamps, and S. Robertson. Where to stop [6] A. Arampatzis, S. Robertson, and J. Kamps. Score [7] A. Arampatzis and A. van Hameren. The score-distributional [8] L. Azzopardi, M. de Rijke, and K. Balog. Building simulated [9] J. P. Callan. Distributed Information Retrieval , chapter 5, [10] J. P. Callan and M. E. Connell. Query-based sampling of text [11] K. Collins-Thompson, P. Ogilvie, Y. Zhang, and J. Callan. [12] M. Fern X ndez, D. Vallet, and P. Castells. Probabilistic score [13] M. Fern X ndez, D. Vallet, and P. Castells. Using historical [14] L. Q. Ha, E. I. Sicilia-Garcia, J. Ming, and F. J. Smith. [15] D. Hawking and S. Robertson. On collection size and [16] J. Kamps, M. de Rijke, and B. Sigurbj X rnsson. Combination [17] J. H. Lee. Combining multiple evidence from different [18] J. H. Lee. Analyses of multiple evidence combination. In [19] D. D. Lewis. Evaluating and optimizing autonomous text [20] R. Manmatha, T. M. Rath, and F. Feng. Modeling score [21] H. Nottelmann and N. Fuhr. From uncertain inference to [22] D. W. Oard, B. Hedin, S. Tomlinson, and J. R. Baron. [23] A. Papoulis. Probability, Random Variables, and Stochastic [24] B. D. Ripley and N. L. Hjort. Pattern Recognition and [25] S. Robertson. On score distributions and relevance. In [26] J. Savoy. Report on CLEF-2003 multilingual tracks. In [27] S. Sharma, L. T. Nguyen, and D. Jia. Ir-wire: A research tool [28] L. Wasserman. All of Statistics: A Concise Course in [29] Y. Zhang and J. Callan. Maximum likelihood estimation for [30] I. Zukerman and E. Horvitz. Using machine learning
