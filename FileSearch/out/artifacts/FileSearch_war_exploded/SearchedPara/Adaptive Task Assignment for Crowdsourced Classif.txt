 Chien-Ju Ho, Shahin Jabbari cjho@cs.ucla.edu, shahin@cs.ucla.edu University of California, Los Angeles Jennifer Wortman Vaughan jenn@microsoft.com Crowdsourcing markets provide a platform for inex-pensively harnessing human computation power to solve tasks that are notoriously di ffi cult for computers. In a typical crowdsourcing market, such as Amazon Mechanical Turk, registered users may post their own  X  X icrotasks X  which are completed by workers in ex-change for a small payment, usually around ten cents. A microtask may involve, for example, verifying the phone number of a business, determining whether or not an image contains a tree, or determining (subjec-tively) whether or not a particular website is o ff ensive. The availability of diverse workers willing to complete tasks inexpensively makes crowdsourcing markets ap-pealing as tools for collecting data ( Wah et al. , 2011 ). Classification tasks, in which workers are asked to pro-vide a binary label for an instance, are among the most common tasks posted ( Ipeirotis , 2010 ). Unfortunately, due to a mix of human error, carelessness, and fraud  X  the existence of spammy workers on Mechanical Turk is widely acknowledged  X  the data collected is often noisy ( Kittur et al. , 2008 ; Wais et al. , 2010 ). For classification tasks, this problem can be overcome by collecting labels for each instance from multiple work-ers and combining these to infer the true label. In-deed, much recent research has focused on developing algorithms for combining labels from heterogeneous la-belers ( Dekel &amp; Shamir , 2009 ; Ipeirotis et al. , 2010 ). However, this research has typically focused on the in-ference problem, sidestepping the question of how to assign workers to tasks by assuming that the learner has no control over the assignment. One exception is the work of Karger et al. ( 2011a ; b ), who focus on the situation in which all tasks are homogeneous (i.e., equally di ffi cult and not requiring specialized skills), in which case they show that it is not possible to do better than using a random assignment.
 One might expect the assignment to matter more when the tasks are heterogeneous. Classifying images of dogs versus images of cats is likely easier for the av-erage worker than classifying images of Welsh Terriers versus images of Airedale Terriers. It might be neces-sary to assign more workers to tasks of the latter type to produce high confidence labels. The assignment can also be important when tasks require specialized skills. A worker who knows little about dogs may not be able to produce high quality labels for the Terrier task, but may have skills that are applicable elsewhere. We investigate the problem of task assignment and la-bel inference for heterogeneous classification tasks. In our model, a task requester has a set of tasks, each of which consists of an instance for which he would like to infer a binary label. Workers arrive online. The learner must decide which tasks to assign to each worker, and then use the noisy labels produced by the workers to infer the true label for each task. The goal of the learner is to output a set of labels with su ffi ciently low error while requesting as few labels from workers as possible. Building on online primal-dual methods ( Buchbinder &amp; Naor , 2005 ), we propose an exploration-exploitation algorithm that is provably competitive with an optimal o ffl ine algorithm that has knowledge of the sequence of workers and their skills in advance. We then evaluate this algorithm in a va-riety of experiments on synthetic data and show that adaptively allocating tasks helps when the worker dis-tribution is diverse or the tasks are heterogeneous. Our research is mostly closely related to that of Karger et al. ( 2011a ; b ) and Ho &amp; Vaughan ( 2012 ). Karger et al. introduced a model in which a requester has a set of homogeneous labeling tasks he must assign to workers who arrive online. They proposed an as-signment algorithm based on random graph generation and a message-passing inference algorithm inspired by belief propagation, and showed that their technique is order-optimal in terms of labeling budget. In par-ticular, let p j be the probability that worker j com-pletes any given task correctly and q = E [(2 p j  X  1) 2 ], where the expectation is over the choice of a random worker j . They proved that their algorithm requires O ((1 /q ) log(1 / ! )) labels per task to achieve error less than ! in the limit as the numbers of tasks and work-ers go to infinity. They also showed that adaptively assigning tasks does not help in their setting, in that  X  ((1 /q ) log(1 / ! )) labels are still needed in general. We generalize this model to allow heterogeneous tasks, so that the probability that worker j completes a task correctly may depend on the particular task. In this generalized setting, assigning tasks adaptively can pro-vide an advantage both in theory and in practice. Our techniques build on the online primal-dual frame-work, which has been used to analyze online opti-mization problems ranging from the adwords prob-lem ( Buchbinder et al. , 2007 ; Devanur et al. , 2011 ) to network optimization ( Alon et al. , 2004 ) and pag-ing ( Bansal et al. , 2007 ). Ho &amp; Vaughan ( 2012 )were the first to apply this framework to crowdsourcing. In their model, a requester has a fixed set of tasks of dif-ferent types, each of which must be completed exactly once. Each worker has an unknown skill level for each type of task, with workers of higher skill levels produc-ing higher quality work on average. Workers arrive on-line, and the learner must assign each worker to a sin-gle task upon arrival. When the worker completes the task, the learner immediately receives a reward, and thus also a noisy signal of the worker X  X  skill level for tasks of that type. Workers arrive repeatedly and are identifiable, so the learner can form estimates of the workers X  skill levels over time. The goal is to maximize the sum of requester rewards. Ho &amp; Vaughan provide an algorithm based on the online primal-dual frame-work and prove that this algorithm is competitive with respect to the optimal o ffl ine algorithm that has access to the unknown skill levels of each worker.
 Our model di ff ers from that of Ho &amp; Vaughan in sev-eral key ways. Their analysis depends heavily on the assumption that the requester can evaluate the quality of completed work immediately (i.e., learn his reward on each time step), which is unrealistic in many set-tings, including the labeling task we consider here; if the requester could quickly verify the accuracy of la-bels, he wouldn X  X  need the workers X  labels in the first place. In their model, each task may be assigned to a worker only once. In ours, repeated labeling is neces-sary since there would be no way to estimate worker quality without it. These di ff erences require a di ff erent problem formulation and novel analysis techniques. Repeated labeling has received considerable empiri-cal attention, dating back to the EM-based algorithm of Dawid &amp; Skene ( 1979 ). Sheng et al. ( 2008 ) consid-ered a setting in which every worker is correct on every task with the same probability, and empirically evalu-ated how much repeated labeling helps. Ipeirotis et al. ( 2010 ) extended this idea to heterogeneous workers and provided an algorithm to simultaneously esti-mate workers X  quality and true task labels. More re-cently, there has been work showing that label infer-ence can be improved by first estimating parameters of the structure underlying the labeling process using techniques such as Bayesian learning ( Welinder et al. , 2010 ), minimax entropy ( Zhou et al. , 2012 ), and vari-ational inference ( Liu et al. , 2012 ).
 On the theoretical side, there have been several re-sults on learning a binary classifier using labeled data contributed by multiple teachers, each of which labels instances according to his own fixed labeling func-tion ( Crammer et al. , 2005 ; 2008 ; Dekel &amp; Shamir , 2009 ). These require PAC-style assumptions and focus on filtering out low quality workers. Tran-Thanh et al. ( 2012 ) used ideas from the multi-armed bandit litera-ture to assign tasks. Bandit ideas cannot be applied in our setting without further assumptions since the reward corresponding to an assignment depends on whether the worker X  X  label is correct, which cannot be inferred until the task has been assigned to others. Ghosh et al. ( 2011 ) studied a model similar to that of Karger et al. , also with homogeneous tasks, and used eigenvalue decomposition to estimate each worker X  X  quality. Their bounds depend on a quantity essentially identical to the quantity q defined above, which they refer to as the population X  X  average compe-tence . A similar quantity plays a role in our analysis. In our model, a task requester has a set of n tasks, indexed 1 ,  X  X  X  ,n . Each task is a binary classification problem. The true label of task i , denoted " i ,iseither 1 or  X  1, and is unknown to the requester.
 Workers arrive online. When worker j arrives, she announces the maximum number of tasks that she is willing to complete, her capacity , M j . No other infor-mation is known about each worker when she arrives. Each worker j has a skill level , p i,j  X  [0 , 1], for each task i . If the algorithm assigns worker j to task i , the worker will produce a label " i,j such that " i,j = " with probability p i,j and " i,j =  X  " i with probability 1  X  p i,j , independent of all other labels. The algorithm may assign worker j up to M j tasks, and may observe her output on each task before deciding whether to as-sign her to another or move on, but once the algorithm moves on, it cannot access the worker again. This is meant to reflect that crowdsourced workers are nei-ther identifiable nor persistent, so we cannot hope to identify and later reuse highly skilled workers. Several of our results depend on the quantity q i,j = (2 p i,j  X  1) 2 . Intuitively, when this quantity is close to 1, the label of worker j on task i will be informative; when it is close to 0, the label will be random noise. To model the fact that the requester cannot wait arbi-trarily long, we assume that he can only assign tasks to the first m workers who arrive, for some known m .We therefore index workers 1 ,  X  X  X  ,m . Later we consider an additional  X  m workers who are used for exploration. In addition to assigning tasks to workers, the learn-ing algorithm must produce a final estimate  X  " i for the label " i of each task i based on the labels provided by the workers. The goal of the learner is to produce estimates that are correct with high probability while querying workers for as few labels as possible. Task structure: A clever learning algorithm should infer the worker skill levels p i,j and assign workers to tasks at which they excel. If the skills are arbitrary, then the learner cannot infer them without assigning every worker to every task. Therefore, it is necessary to assume that the p i,j values exhibit some structure. Karger et al. ( 2011a ; b ) assume that all tasks are iden-tical, i.e., p i,j = p i ! ,j for all j and all i and i ! sider a more general setting in which the tasks can be divided into T types , and assume only that p i,j = p i ! ,j if i and i ! are of the same type.
 Gold standard tasks: As is common in the litera-ture ( Oleson et al. , 2011 ), we assume that the learner has access to  X  X old standard X  tasks of each task type. 1 These are instances for which the learner knows the true label a priori. They can be assigned in order to estimate the p i,j values. Of course the algorithm must pay for these  X  X ure exploration X  assignments. Random permutation model: We analyze our algorithm in the random permutation model as in Devanur &amp; Hayes ( 2009 ). The capacities M j and skills p i,j of each worker j may be chosen adversarially, as long as the assumptions on task structure are satis-fied. However, the arrival order is randomly permuted. Since only the order of workers is randomized, the of-fline optimal allocation is well-defined.
 Competitive ratio: To evaluate our algorithm, we use the notion of competitive ratio , which is an upper bound on the ratio between the number of labels re-quested by the algorithm and the number requested by an optimal o ffl ine algorithm which has access to all worker capacities and skill levels, but must still assign enough workers to each task to obtain a high-confidence guess for the task X  X  label. The optimal o ffl ine algorithm is discussed in Sections 4 and 5 . To gain intuition, we first consider a simplified o ffl ine version of our problem in which the learner is provided with a full description of the sequence of m workers who will arrive, including the skill levels p i,j and ca-pacities M j for all i and j . The learner must decide which tasks to assign to each worker and then infer the task labels. We discuss the inference problem first. 4.1. Aggregating Workers X  Labels Suppose that the learner has already assigned tasks to workers and observed the workers X  labels for these tasks. How should the learner aggregate this informa-tion to infer the true label for each task? We consider aggregation methods that take a weighted vote of the workers X  labels. Fix a task i . Let J i denote the set of workers assigned to this task. We consider methods that set  X  " i = sign( of weights { w i,j } . The following lemma shows that this technique with weights w i,j =2 p i,j  X  1 is guaranteed to achieve a low error if enough high quality workers are queried. Recall that q i,j =(2 p i,j  X  1) 2 . Lemma 1. Let  X  " i = sign( of weights { w i,j } . Then  X  " i # = " i with probability at minimized when w i,j  X  (2 p i,j  X  1) , in which case the probability that  X  " i # = " i is at most e  X  1 2 The proof, which uses a simple application of Hoe ff d-ing X  X  inequality, is in the appendix. 2 This tells us that to guarantee that we make an error with probability less than ! on a task i , it is su ffi cient to select a set of labelers J i such that  X  j  X  J gate labels by setting  X  " i = sign( One might ask if it is possible to guarantee an error of ! with fewer labels. In some cases, it is; if there ex-ists an i and j such that p i,j = q i,j = 1, then one can achieve zero error with only a single label. However, in some cases this method is optimal. For this rea-son, we restrict our attention to algorithms that query subsets J i such that  X  j  X  J shorthand C ! =2ln(1 / ! ). 4.2. Integer Programming Formulation There is a significant benefit that comes from re-stricting attention to algorithms of the form described above. Let y i,j be a variable that is 1 if task i is as-signed to worker j and 0 otherwise. The requirement that straint of these variables. This would not be possible using unweighted majority voting to aggregate labels; weighting by 2 p i,j  X  1 is key. This allows us to express the optimal o ffl ine assignment strategy as an integer linear program (IP), with variables y i,j for each ( i, j ): Constraint ( 1 ) guarantees that worker j does not ex-ceed her capacity. Constraint ( 2 ) guarantees that ag-gregation will produce the correct label of each task with high probability. Constraint ( 3 ) implies that a task is either assigned to a worker or not.
 Note that there may not exist a feasible solution to this IP, in which case it would not be possible to guarantee a probability of error less than ! for all tasks using weighted majority voting. For most of this paper, we assume a feasible solution exists; the case in which one does not is discussed in Section 5.1 .
 For computational reasons, instead of working directly with this IP, we will work with a linear programming relaxation obtained by replacing the last constraint with 0  X  y i,j  X  1  X  ( i, j ); we will see below that this does not impact the solution too much. 4.3. Working with the Dual Solving the linear program described above requires knowing the values q i,j for the full sequence of workers j up front. When we move to the online setting, it will be more convenient to work with the dual of the re-laxed linear program, which can be written as follows, with dual variables x i , z j , and t i,j for all ( i, j ): We refer to x i as the task weight for i , and define the task value of worker j on task i as v i,j = q i,j x i  X  1. Suppose that we were given access to the task weights x i for each task i and the values q i,j .(Wewilldiscuss how to approximate these values later.) Then we could use the following algorithm to approximate the opti-mal primal solution. Note that to run this algorithm, it is not necessary to have access to all q i,j values at once; we only need information about worker j when it comes time to assign tasks to this worker. This is the advantage of working with the dual.
 Algorithm 1 Primal Approximation Algorithm Input: Values x i and q i,j for all ( i, j ) For every worker j  X  { 1 ,...,m } , compute the task values, v i,j = q i,j x i  X  1, for all tasks i . Let n j be the number of tasks i such that v i,j  X  0. If n j  X  M j ,then set y ij  X  1 for all n j tasks with non-negative task value. Otherwise, set y i,j  X  1 for the M j tasks with highest task value. Set y i,j  X  0 for all other tasks. The following theorem shows that this algorithm pro-duces a near-optimal primal solution to our original IP when given as input the optimal dual solution for the relaxed LP. The condition that q i,j x  X  i # = q i ! ,j all i # = i ! is needed for technical reasons, but can be relaxed by adding small random perturbations to q i,j values as in Devanur et al. ( 2011 ). 3 For the rest of the paper, we assume that perturbations have been added and that the condition above holds. Call this the  X  X er-turbation assumption. X  In our final algorithm, we will perturb our estimates of the q i,j values for this reason. Theorem 1. Let y  X  be the primal optimal of the IP and x  X  be the dual optimal of the relaxed formulation. Let y be the output of the Primal Approximation Algo-rithm given input x  X  and the true values q . Then y is feasible in the IP, and under the perturbation assump-The proof shows that the y i,j values assigned by the Primal Approximation Algorithm di ff er from the opti-mal solution of the relaxed LP for at most min( m, n ) pairs ( i, j ), and that this implies the result. We have shown that, given access to q and the optimal task weights x  X  , the Primal Approximation Algorithm generates an assignment which is close to the optimal solution of the IP in Section 4.2 . However, in the online problem that we initially set out to solve, these values are unknown. In this section, we provide methods for estimating these quantities and incorporate these into an algorithm for the online problem.
 Our online algorithm combines two varieties of explo-ration. First, we use exploration to estimate the op-timal task weights x  X  . To do this, we hire an addi-tional  X  m workers on top of the m workers we origi-nally planned to hire, for some  X  &gt; 0, and  X  X bserve X  their q i,j values. (We will actually only estimate these values; see below.) Then, by treating these  X  m work-ers as a random sample of the population (which they e ff ectively are under the random permutation model), we can apply online primal-dual methods and obtain estimates of the optimal task weights. These estimates can then be fed to the Primal Approximation Algo-rithm in order to determine assignments for the re-maining m workers, as described in Section 5.1 . The second variety is used to estimate workers X  skill levels. Each time a new worker arrives (including the  X  m extras), we require her to complete a set of gold standard tasks of each task type. Based on the labels she provides, we estimate her skill levels p i,j and use these to estimate the q i,j values. The impact of these estimates on performance is discussed in Section 5.2 . If we require each worker to complete s gold standard tasks, and we hire an extra  X  m workers, we need to pay for an extra (1+  X  ) ms assignments beyond those made by the Primal Approximation Algorithm. We pre-cisely quantify how the number of assignments com-pares with the o ffl ine optimal in Section 5.3 . 5.1. Estimating the Task Weights In this section, we focus on the estimation of task weights in a simplified setting in which we can observe the quality of each worker as she arrives. To estimate the task weights, we borrow an idea from the litera-ture on the online primal-dual framework. We use an initial sampling phase in which we hire  X  m work-ers in addition to the primary m workers, for some  X  . We observe their skill levels and treat the distribu-tion over skills of the sampled workers as an estimate of the distribution of skills of the m primary workers. Given the q i,j values from the sampled  X  m workers, we can solve an alternative linear programming problem, which is the same as our relaxed o ffl ine linear program-ming problem, except that m is replaced by  X  m and C ! is replaced by  X  C ! . Let  X  x  X  be the optimal task weights in this  X  X ampled LP X  problem. We show that if ! is small enough, running the Primal Approximation Al-gorithm using  X  x  X  and q yields a near-optimal solution, with a number of assignments close to optimal, and a prediction error close to ! after aggregation. Theorem 2. For any ! ,  X   X  (0 , 1 / 2) ,forany  X  = " /m with "  X  { 1 , 2 ,  X  X  X  ,m } and  X   X  [1 /C ! , 1] , let  X  y  X  x  X  be the primal and dual optimal solutions of the sam-pled LP with parameters ! and  X  . Let  X  y  X  be the out-put of the Primal Approximation Algorithm with in-puts  X  x  X  and q , and let  X  y  X  be the optimal assignment of the relaxed o ffl ine formulation with parameter ! . Let q tion assumption, with probability at least 1  X   X  , If the labels collected from the resulting assignment are used to estimate the task labels via weighted majority voting, the probability that any given task label is pre-The requirement that  X   X  1 /C ! stems from the fact that if C ! is small, the total number of assignments will also be small, and the quality of the assignment is more sensitive to estimation errors. If C ! is large, small estimation errors e ff ect the assignment less and we can set the sampling ratio to a smaller value. In the proof, we show that the gap between the ob-jectives of the primal solution generated by the Pri-mal Approximation Algorithm using  X  x  X  and q and the corresponding dual solution is exactly the summation if enough workers are sampled. By weak duality, the optimal number of assignments is between the primal and the dual objectives, so the primal solution output by the algorithm must be near-optimal.
 A note on feasibility: We have implicitly assumed that the sampled LP is feasible. In practice, it may not be, or even if it is, there may exist tasks i such value of q min . If either of these things happen, the task requester may want to discard some of the tasks or lower his desired error, solve the sampled LP with these modified constraints, and continue from there, as there is no way to guarantee low error on all tasks. 5.2. Using Estimates of Skill Levels We now discuss the e ff ect of estimating worker skills. Given observations of the gold standard tasks of type  X  that worker j completed, we can estimate p i,j for any task i of type  X  as the fraction of these tasks she labeled correctly. The following lemma, follows from a straightforward application of the Hoe ff ding bound; we state it here as it will be useful, but omit the proof. Lemma 2. For any worker j ,foranytasktype  X  , and for any t,  X   X  (0 , 1) , suppose that worker j labels ln(2 /  X  ) / (2 t 2 ) gold standard tasks of type  X  . Then with probability at least 1  X   X  , for all tasks i of type  X  ,ifwe set  X  p i,j to the fraction of gold standard tasks of type  X  answered correctly then | p i,j  X   X  p i,j |  X  t . This estimate of p i,j can then be used to derive an estimate for q i,j , with error bounded as follows. Lemma 3. For any worker j and task i ,if  X  p i,j is an estimate of p i,j such that | p i,j  X   X  p i,j |  X  t ,and  X  q set to (2 X  p i,j  X  1) 2 , then | q i,j  X   X  q i,j |  X  4 t . Of course the use of estimated values impacts per-formance. Consider the o ffl ine problem discussed in the previous section. One might hope that if we ap-plied the Primal Approximation Algorithm using  X  q , the number of assignments would be close to the num-ber made using q . Unfortunately, this is not true. Consider this toy example. Let q i, 1 = q i, 2 = q i, 3 =1 for all i , q i,j = 10  X  4 for all i and j&gt; 3, and M j for all j .Set ! =0 . 224 so that C !  X  3. In the op-timal solution, each task i should be assigned only to workers 1 , 2 , and 3. If we underestimate the q i,j val-ues, we could end up assigning each task to many more workers. This can be made arbitrarily bad.
 To address this, instead of solving the relaxed o ffl ine formulation directly, we consider an alternative LP which is identical to the relaxed o ffl ine formulation, except that q is replaced with  X  q and C ! is replaced with a smaller value C ! ! (corresponding to a higher al-lowable error ! ! ). We call this the approximated LP . We show that, if ! ! is chosen properly, we can guar-antee the optimal solution in the relaxed o ffl ine for-mulation is feasible in the approximated LP, so the optimal solution of the approximated LP will yield an assignment with fewer tasks assigned to workers than the optimal solution of the relaxed o ffl ine formulation, even though it is based on estimations.
 To set ! ! , we assume the requester has a rough idea of how hard the tasks are and how inaccurate his estimates of worker skills are. The latter can be achieved by applying Lemma 2 and the union bound to find a value of t such that | p i,j  X   X  p i,j |  X  t for all ( i, j ) pairs with high probability, and setting each  X  q i,j =(2 X  p i,j  X  1) 2 as in Lemma 3 . For the former, let  X  y  X  be the optimal solution of the relaxed o ffl ine formu-that the requester can produce a value  X  q  X  min such that  X  q ! formation, he can conservatively set  X  q  X  min much smaller than min i {  X  q  X  i } , but will both need more accurate esti-mates of p i,j and sacrifice some prediction accuracy. Theorem 3. Assume that we have access to a value  X  q min such that  X  q that | p i,j  X   X  p i,j |  X  t for all ( i, j ) pairs for any known value t&lt;  X  q  X  min / 4 . Then for any ! &gt; 0 ,theopti-mal solution of the approximated LP with parameter ! bigger than the optimal solution of the relaxed o ffl ine formulation with parameter ! and skill levels q i,j . Of course this guarantee is not free. We pay the price of decreased prediction accuracy since we are using ! ! in place of ! . We also pay when it comes time to ag-gregate the workers X  labels, since we must now use  X  q in place of q when applying the weighted majority voting method described in Section 4.1 . This is quantified in the following theorem. Note that this theorem applies to any feasible integer solution of the approximated LP and therefore also the best integer solution. Theorem 4. Assume again that we have access to for any known value t&lt;  X  q  X  min / 4 .Forany ! &gt; 0 , let y be any feasible integer assignment of the approxi-set of workers that are assigned to task i according to y , and define  X  q i =  X  j  X  J signed according to y and the results aggregated using weighted majority voting with weights w i,j =2 X  p i,j  X  1 , the error probability of the predicted task label for task Theorem 4 tells us that if our estimates of the worker skills are accurate (i.e., if t is small), then our pre-diction error will be close to the error we would have achieved if we had known the true worker skills. How good the estimates need to be depends on the quality of the workers, as measured by  X  q  X  min and  X  q i .Intu-itively, if  X  q  X  min is small, there may exist some task i at which workers perform poorly in the optimal solu-tion. In this case, the assignment will be very sensitive to the value of ! ! chosen, and it will be necessary to set ! ! larger to guarantee that the true optimal solu-tion is feasible in the approximated LP. If  X  q i is small, then a small amount of error in estimated worker qual-ity would dramatically change the weights used in the weighted majority voting aggregation scheme. 5.3. Putting it All Together We have separately considered relaxations of the task assignment and label inference problem in which the optimal task weights or worker skill levels are already known. We now put all these pieces together, give a combined algorithm, and state our main theorem. Algorithm 2 Main Algorithm The complete algorithm is stated in Algorithm 2 , and its performance guarantee is given below. Recall that T is the number of task types. Again, we assume that the optimization problems are feasible.
 Theorem 5. For any ! ,  X   X  (0 , 1 / 2) ,forany  X  = " /m for an "  X  { 1 , 2 ,...,m } such that  X   X  [1 /C ! , 1] ,as-sume we have access to a value  X  q  X  min satisfying the condition in Theorem 3 , let s be any integer satis-fying s  X  8ln(4 T (1 +  X  ) m/  X  ) /  X  q  X  2 min , and let ! ! turbation assumption, with probability at least 1  X   X  , when the Main Algorithm is executed with input ( ! ,  X  , s,  X  q  X  min ) , the following two things hold: 1) The number of assignments of to non-gold standard tasks is no more than times the optimal objective of the IP, where  X  q min = 2) The probability that the aggregated label for each where l 1 =4 t/  X  q  X  min ,l 2 =6ln(4 /  X  ) / and t = When ! is small, C ! ! is large, and l 2 approaches 0. The competitive ratio may shrink, but if ! is too small,  X  q min will shrink as well, and at some point the problem may become infeasible. When s is large, t is small, and so l and l 3 ,i approach 0, leading to error almost as low as if we knew the true q values, as we would expect. In this section, we evaluate the performance of our algorithm through simulations on synthetically gener-ated data. As a comparison, we also run the message-passing inference algorithm of Karger et al. ( 2011a ; b ) on the same data sets. As described in Section 2 , Karger et al. use a non-adaptive, random assignment strategy in conjunction with this inference algorithm. We show that adaptively allocating tasks to workers using our algorithm can outperform random task as-signment in settings in which (i) the worker distribu-tion is diverse, or (ii) the set of tasks is heterogeneous. We create n =1 , 000 tasks and m = 300 workers with capacity M j = 200 for all j , and vary the distribution over skill levels p i,j . We would like to compare the error rates of the algorithms when given access to the same total number of labels. In the message-passing algorithm, we can directly set the number of labels by altering the number of assignments. In our algorithm, we change the parameter ! and observe the number of labels (including exploration) and the prediction error. 6.1. Worker Diversity In their analysis, Karger et al. assume there is only one task type (that is, p i,j = p i ! ,j for all i , i ! , and j ), and claim that in this setting adaptively assigning tasks does not yield much of an advantage. Our first experiment simulates this setting. We would like to see if our algorithm can perform better if the worker distribution is diverse, even though it requires some  X  X ure exploration X   X  we need to pay each worker to complete the gold standard tasks, and we need to hire an extra  X  m workers to estimate the task weights. For our algorithm, we set  X  =0 . 3 and sample 90 ex-tra workers from the same distribution to learn task weights. Each worker is required to complete s = 20 gold standard tasks of each type when she arrives. These values were not optimized, and performance could likely be improved by tuning these parameters. We examine two settings. In the first, every worker gives us a correct label with probability 0 . 6414 for all tasks. In the second, the population is 50% spammers and 50% hammers. The spammers give random an-swers, while the hammers answer correctly with prob-ability 0 . 7. Note that these values are chosen such that E [ q i,j ]= E [(2 p i,j  X  1) 2 ] is the same in both settings. The results are shown in Figure 1 . The performance of the message-passing algorithm is almost identical in the two settings. Our algorithm performs relatively poorly in the setting with uniform workers since we can X  X  benefit from adaptive assignments but still pay the exploration costs. However, our algorithm outper-forms message passing in the setting with two types of workers, quickly learning not to assign any tasks to the spammers beyond those used for exploration. 6.2. Heterogeneous Tasks We next examine a setting in which there are multiple types of tasks, and every worker is skilled at exactly one type. We generate k task types and k correspond-ing worker types, for k = 1, 2, and 3. Type  X  workers complete type  X  tasks correctly with probability 0 . 7, but other tasks correctly with probability 0 . 5. For our algorithm, we set  X  =0 . 3. Each worker com-pletes s = 10 gold standard tasks of each type. The results are shown in Figure 2 . Not surprisingly, since the message-passing algorithm does not attempt to match tasks to suitable workers, its performance degrades quickly when k grows. Since our algorithm attempts to find the best match between workers and tasks, the performance degrades much more slowly when k grows, even with the extra exploration costs. We conclude by mentioning several extensions of our model. We have assumed that the requester pays the same price for any label. Our results can be extended to handle the case in which di ff erent workers charge di ff erent prices. Let c i,j denote the cost of obtaining a label for task i from worker j . The objective in the is linear and the same techniques would apply. The framework can also be extended to handle more intricate assumptions about the structure of tasks. We have assumed that there are T task types, with p i,j = p i ! ,j whenever i and i ! are of the same type. However, this assumption is used only in the explo-ration phase in which workers X  skills are estimated. While the amount of exploration required by the al-gorithm depends on the particular task structure as-sumed, the derivation of our algorithm and the general analysis are independent of the task structure. This research was partially supported by the National Science Foundation under grant IIS-1054911.

