
Peculiar data are objects that are relatively few in num-ber and significantly different from the other objects in a data set. In this paper, we propose the PDD framework for detecting multiple categories of peculiar data. This frame-work provides an extensible set of perspectives for viewing data, currently including viewing data as a set of records, attributes, frequencies, intervals, sequences, or sequences of changes. By using these six views of the data, multiple categories of peculiar data can be detected to reveal dif-ferent aspects of the data. For each view, the framework provides an extensible set of peculiarity measures to detect outliers and other kinds of peculiar data. The PDD frame-work has been implemented for Oracle and Access. Experi-ments are reported for data sets concerning Regina weather and NHL hockey.
Peculiar data are objects that are relatively few in num-ber and significantly different from the other objects in a data set [28, 29, 30]. If the objects in the data set are records (also called points or examples), the peculiar data are called outliers . Zhu et al. argued that the notion of outlier varies among users, problem domains, and datasets [30], and they recommended that examples of outliers be obtained from users. We approach the problem in an orthogonal way by providing a framework for detecting a wide variety of kinds of peculiarities.

Detection of outliers and other peculiar data has been found useful in a range of applications where rare, yet in-teresting, events occur. The application areas include fraud detection[5], network intrusions [8, 16], medical diagnosis [23], and data cleaning [31]. Fraud detection based on pe-culiarity finds objects that deviate from normal ones and brings attention to possibly fraudulent activities, especially previously unknown types of frauds [21]. This approach assumes that activities that deviate from normal are more likely to be fraudulent activities and that the number of nor-mal activities will be overwhelmingly greater than the num-ber of fraudulent activities. In medical diagnosis, especially in cancer diagnosis, images showing cancer are a small frac-tion of all images, and they often appear peculiar in some way [18].

The core task in peculiar data detection is to determine the peculiarity factor (PF) of each object, which is the dif-ference between the object and all other objects in the data set. The PF may be defined based on a variety of properties of objects. Peculiar data detection methods using different definitions for the PF are not simply different methods of detecting the same set of peculiar data, because they may detect totally different set s of peculiar data. The appropri-ate definition of the PF depends on the application. A peculiarity measure is a method of calculating the PF of an object by measuring some quantifiable properties of the data. A user can obtain different types of peculiar data using different peculiarity measures and can explore the outcome. Some users have a definite idea about the type of peculiarity that they find interesting, while others would welcome from results from a variety of perspectives.
In this paper, we propose the Peculiar Data Detection (PDD) framework for detecting six categories of peculiar data using four peculiarity measures. With the frame-work, data are examined from six views: record , attribute , frequency , interval , sequence ,and sequence of changes . Four peculiarity measures, cumulative distance , fraction of data outside neighborhood , neighborhood density ratio , and composite z-score are used to detect peculiar data in each of these six categories.

The PDD framework offers three contributions to the state of the art. First, it is able to detect more categories of peculiar data than any previous approach. Existing pe-culiar data detection methods, some of which are presented in the Section 2, take data as a set of records and detect the peculiar records (outliers) in this set. In addition to the record view, our framework views a data set from a vari-ety of other perspectives an d detects peculiar data accord-ing to each. For example, one might want to find peculiar attributes or peculiar gaps in the occurring values. Although record based approaches can be adapted to the other views, the PDD framework systematizes and automates this pro-cess. Secondly, the framework simplifies the task of mea-suring peculiarity by providing a collection of peculiarity measures. This collection integrates past research in out-lier and peculiar data detection. Thirdly, using the frame-work simplifies the addition of new views or measures as required. For example, our composite z-score measure was synthesized from three existin g peculiarity measures in a straightforward fashion.
 The remainder of this paper is organized in four sections. Section 2 describes previous work with emphasis on three existing peculiar data detection methods. Section 3 presents the PDD framework, including its views and measures. In Section 4, experiments are reported that show the categories of peculiar data detected using the PDD framework, show the application of the compos ite z-score measure, and com-pare our approach with previous approaches. Finally, Sec-tion 5 presents conclusions and lists several possible exten-sions.
To detect outliers and other types of peculiar data dur-ing data mining, several methods have been developed and studied. Given a data set consisting of objects, the goal of outlier detection is to identify objects that are outliers in the set, i.e., objects that are in some way unlike the other ob-jects in the data set. The literature on outliers is vast, and researchers have defined the term  X  X utlier X  in various ways. Let us distinguish two types of outliers: a model-based out-lier is an object that deviates significantly from the accepted model for the data set; a distance-based outlier is an object that far from the other objects, according to some distance measure.

In a model-based outlier detection method , a model that represents the data set is first specified or learned, and then the model-based outliers are defined as all objects that de-viate significantly from the model. Some common types of models used in outlier detection are statistical models[3], neural networks[9], and support vector machines[17]. The Outlier-by-Example(OBE) method provides a recent exam-ple of a model based method. In this method, the user spec-ifies a set of outliers, which are augmented by outstanding outliers (the most extreme outliers in the dataset) and artifi-cial objects that are derived from the user specified outliers and outstanding outliers. These objects are labeled as posi-tive objects. The OBE method also selects a number of the objects with the lowest outlier scores as negative objects. Then it uses these positive and negative objects to train a support vector machine (SVM) to classify objects as out-liers or not. This training process iterates until the ratio of the objects that it classifies as positive converges to the frac-tion specified by the user.

In a distance-based method , a distance function is first defined for the data set, then the neighborhood of each ob-ject in the data set is defined based on this distance func-tion, and finally the distance-based outliers are defined with respect to the neighborhoods. The neighborhood of an ob-ject may be defined based on a fixed radius or by a fixed number of neighbors. A radius neighborhood (or d -neigh-borhood ) of an object consists of all objects at a distance of at most d from the object. A cardinality neighborhood (or k -neighborhood ) of an object consists of itself and its k nearest neighbors. Thus, a d -outlier is an object for which fewer than p other objects occur within a fixed distance d [15], and a k -outlier is an object for which the distance to the k th nearest neighbor is greatest [22]. In general, since clustering aims to find groups of objects that are similar to other objects in the group and dissimilar to objects outside of the group, finding clusters could be considered as op-posite to outlier detection. Therefore, outliers can also be found as a byproduct of clustering [27]. In recent work on distance based outliers, Bay and Schwabacher devised a ef-ficient nested algorithm that maintains a list of the top n outliers according to an outlier score [4]. As new potential outliers with high scores are found, candidate outliers below a threshold are progressively eliminated.

Let us now consider three distance-based methods in more detail. To simplify the description of these methods, we assume the data are input in the form of a table. Let R = { R 1 ,...R n } be the set of records in the data, and A = { A 1 ,...,A x ij be the value of record R i for attribute A j . The objec-tive of the existing outlier detection methods is to detect peculiar records in the set R .Let Distance ( R i ,R j ) be a distance function that gives the distance between R i = ( x be the number of the elements in set S . Zhong X  X  method [28, 29] calculates the pecu liarity factor of each record in R based on the sum of the distances between it and every other record. In this framework we refer to this as the cumulative distance (CD) of a record, defined as where p is a parameter denoting the importance of the dis-tance, which can be adjusted by the user. By default, p =0 . 5 . The threshold value ( t ) is a linear combination of the standard deviation and the mean of the peculiarity factors: where  X  and  X  are the mean and standard deviation of the peculiarity factors PF ( R 1 ) ,...,PF ( R m ) ,and a is the threshold adjustment factor that can be adjusted to control the number of peculiar data. All records having peculiar-ity factors greater than the threshold value t are classified as peculiar records. We do not assume that the data fits a normal distribution, we are merely interested in records that have a suitably high peculiarity factor.

Knorr X  X  method [14, 13, 15] detects peculiar data based on the proportion of records that lie at more than some spec-ified distance from a specific r ecord. The peculiar records are called distance based (DB) outliers. A record R i is a DB ( p, d ) outlier if at least fraction p of the records in R lie greater than distance d from R i . For a record R i ,the radius-neighborhood with radius d of R i , denoted ( RN d ( R i ) contains the set of records R j  X  R that are within distance d of R i For record R i to qualify as a DB ( p, d ) outlier, at least frac-tion p of the objects in R must be outside RN d ( R i )
Breunig X  X  method [6, 7, 11] calculates the local outlying factor (LOF) for each record by taking the ratio of the av-erage density of the neighborhoods of the neighbors of the record over the density of the neighborhood of the record. In Breunig X  X  terminology, the peculiar data are called dis-tance based local (DBL) outliers. If the density of the neighborhood of a record is lower than that of its neighbors, its LOF will be higher and it will be called a DBL outlier.
LOF is calculated as follows [7]. The k-cardinality neighborhood of record R i (denoted CN k ( R i ) ) contains the k nearest neighbors of R i .The k -distance of R i (de-noted D k ( R i ) ) is the distance between R i and the k nearest neighbor of R i . CN k ( R i ) contains every record whose distance from R i is not greater than D k ( R i ) . The local reachability density of R i is
LRD k ( R i )= 1 The local outlying factor of R i is
After LOF is calculated for all records in R , records are ranked in terms of their LOF values. The records with the highest LOF values are the most peculiar, and the records with the lowest LOF values are the least peculiar.
Several previous research efforts have attempted to in-tegrate various techniques for outlier detection. For exam-ple, Tang et al., in a series of papers beginning with [25], have presented a framework for outlier detection. They in-tegrated a connectivity measure and a density measure to create a connectivity-based outlier factor (COF) scheme. Recently, He et al. have provided a unified framework based on subspaces for detecting outliers in high dimen-sional spaces [10], by building on work by Aggarwal and Yu [2].
The Peculiar Data Detection (PDD) framework for de-tecting peculiar data distinguishes six categories of pecu-liar data based on the six views of the data mentioned in Section 1. Each category of peculiar data is further cat-egorized into three subcategories based on three peculiar-ity measures. The framework also provides three options for the presentation of the det ected peculiar data. With the first option, only objects with peculiarity factors higher than a threshold value are presented. With the second option, used in experiments reported in this paper, the top k objects [11, 4] according to the peculia rity factors are presented. With the third option, only objects fulfilling the require-ments of both other options are reported.

The PDD framework consists of three parts. View repre-sentation uses a view from the View Base and extracts that view from the input data table and stores it in an interme-diate form called the View Repr esentation table (hereafter VR-table ). New views can be added to the framework by making additions to the View Base. Peculiar factor mea-surement calculates the peculiar factor of each record in the VR-table based on the measures stored in the PF-Measure Base. Peculiar data presentation displays the peculiar data in the required format. Presentation is straightforward, but the other two parts are discussed in detail in two subsec-tions.
A data table can be viewed in many ways. The peculiar data detection methods discussed in Section 2 view an input data table as a set of records. We call this the record view of data. The peculiar data detected from the record view are referred to as the record peculiar data . With this view, peculiarity relating to only one aspect is produced. This re-sults in peculiarity relating to only one aspect, giving linear results. We propose categorizing the data into several views, so that unique peculiarities can be detected from a variety of viewpoints.

We use six views of data: record , attribute , frequency , interval , sequence ,and sequence of deltas . The record, at-tribute, frequency, and interval views can be applied to any data for which a distance function can be defined, includ-ing nominal, discrete, ordinal, integer, and continuous data. The sequence and sequence of deltas views can be applied only when the records can be ordered by reference to an im-plicit or explicit sequencing attribute or attribute combina-tion. For example, the records may be presented in temporal order.

We chose these six views because they were relatively di-verse, able to detect the types of peculiarity we saw in a vari-ety of data sets, and sufficien tly compatible for common im-plementation because all were c ompatible with a distance-based approach to peculiarity detection. Given a data table, the most unusual rows (records) and columns (attribute) are natural types of peculiarity. For example, the record view is suited to finding the  X  X astest, X   X  X ottest, X   X  X ost profitable, X  etc. of the objects. However, the  X  X ottest X  day would not be considered peculiar unless it was unlike other days. The attribute view is suited to identifying the most unusual fea-ture. For example, given a data set that contains grades for many students (rows) in many courses (columns), this view allows peculiar classes to be identified based on stu-dent grades. The frequency view allows values that occur with unusual frequency, e.g., most often, least often, or at some intermediate frequency that is uncommon. For ex-ample, if rainfall is examined, the frequency of days with no rain may be very high compared to that of days with any other amount of rain. The interval view allows gaps in the set of occurring values to be identified. For example, in the student grades example, there may be no values in the interval [47-49] if 50 is a passing grade, due to instruc-tors passing students with borderline performance. The se-quence view is appropriate for sequences or time series of objects, where the differences between consecutive objects or some other property of a fixed window of objects is of interest. For example, from this perspective, a large change in sales between two consecutive days or the hottest seven day period might be peculiar. The sequence of deltas view was to some extent, a test of the extensibility of the frame-work. Using this view, the week with most peculiar changes in temperature might be identified.

The view representation part of the PDD framework constructs a VR-table for each view by constructing a set of records to represent each view. After construction of the VR-tables, we can extract peculiarity data corresponding to each view. Record peculiar data are detected from the VR-table representing the record view, and so on.

For construction of the VR-tables we use the data con-tained in a base-table . This base-table is a subset of the data and consists of the relevant attributes and correspond-ing records. In this way we can test for peculiarities for certain attributes or within subsets of the records. In the PDD framework we have automated the base-table extrac-tion process.

The VR-tables will contain attributes pertaining to that particular view, as well as a remark attribute to store the link between the record in the VR-table and the record(s) it represents in the base table. The remark attribute relates the entry in the VR-table back to the original data in the base-table.

Once we have constructed the VR-tables, the remaining steps of the PDD process are applied. We discuss the pro-cess for each view separately below.
 Record view . The record view is the traditional view of a data table. The VR-table (excluding the remark attribute) is identical to the base-table, and the remark attribute is left blank. The VR-table representing the record view of the base-table shown in Table 1 is shown in the top of Table 2. Attribute view . In the attribute view, each record stores the values of an attribute in the base table. This view might be of interest when the base-table has a large number of similar attributes. For example, suppose we have a marks data ta-ble and rows correspond to students and columns to courses. If students have to take 40 courses to complete a program, then it may be useful to detect the peculiar course(s) based on the marks obtained by stude nts. Since each attribute in the base table is represented as a record in the VR-table containing its values, the VR-table for the attribute view will be the transpose of the base-table. The remark attribute stores the attribute in the base table which corresponds to that record. For the base-table shown in Table 1, the VR-table representing the attribute view is shown in the attribute part of Table 2.
 Frequency view . In this view we store the frequencies of all unique records in the base-table. To determine whether two records are equivalent a user defined equivalence func-tion is used. To prevent inconsistent results, this equiv-alence function should be reflexive, symmetric, and tran-sitive. By default, two records R i =( x i 1 ,...,x in )and R ues in one record match with the other in each attribute (i.e. x the value of the record that corresponds to the frequency. For example, for the base-table as shown in Table 1, the VR-table representing the frequency view is as shown in the frequency part of Table 2.

Table 2. The VR-tables for six views of the base-table given in Table 1.
 Interval view . The VR-table for the interval view stores the intervals between value-adjacent records from the base table are considered value-ad jacent if no other record has a value between the values of R i and R j . The remark at-tribute indicates the records which comprise the endpoints of the interval. Intervals in the base table are found in the following manner. Let U = { U 1 ,...,U q } be the set of unique records in the base-table, where q is the total num-ber of unique records. Let V be a value function that yields a set of totally ordered values. For example, V could be the value of one attribute or the sum of two attributes, etc. First, records in the set U are sorted in ascending order according to V. Let S = { S 1 ,...,S q } be the sorted list of records in U. The interval view then consists of the set { x | x = S this view is to look for intervals which contain no records. For example, for the base-table Table 1 we can define the value of each record as being the sum of the two attributes, b 1 and b 2 . The interval view then gives the VR-table as shown in the interval part of Table 2.
 Sequence view . This view is applicable when records in the base-table are a sequence, i.e., sorted according to some attribute. In the sequence view, the base-table is viewed as a set of subsequences. For example, if we are interested in subsequences of length two, in a base-table with n records, there will be n -1 subsequences ( R 1  X  R 2 ,...,R n  X  1  X  R n ). Although, many methods have been developed to de-tect frequently occurring subsequences [1, 20], we want to detect peculiar subsequences, such as differences between consecutive objects in time series [26].

If we are looking at subsequences of length l then we put l attributes in the VR-table. Each attribute will stores a record in the sequence. The r emark attribute stores the individual sequence of each attr ibute in the base table. For example, when searching for subsequences of length three in the base-table shown in Table 1, the VR-table is as shown in the sequence part of Table 2.
 Sequences of Deltas . This view is similar to the sequence view and also requires the records in the base-table to be or-dered. However, this view highlights the differences (called deltas between consecutive records in the sequence. To cal-culate deltas, we use a user defined function (with reason-able defaults, such as simple subtraction in the case of nu-meric attributes), as in the interval view. A delta may be either negative or positive. For example, a subsequence of length one gives standard deltas as analyzed in time series. As another example, considering subsequences of length three for the base-table as shown in Table 1 and using the same definition of record value as the one given in the inter-val view, the VR-table is as shown in the sequence of deltas part of Table 2.

In Table 3 we provide a summary of the properties needed for the data to be represented by each view. With these properties all data types, including nominal, ordi-nal, integer, and continuous, can be processed by the PDD framework. The following functions are needed: a Dis-tance Function ( DF ) to measure the distance between two records, an Equivalence Function ( EF ) to test the equality of two records, a Record Value ( RV ) to define the over-all value of a record, and a Sequence Distance Function ( SDF ) to define the distance between two sequences. The additional parameter l is used to define the length of a se-quence.
As reviewed in Section 2, we can use several peculiar-ity detection methods to measure different properties of a record in an effort to determine its peculiarity factor. A record that qualifies as peculia r according to one measure may not qualify under another measure, so we may obtain totally different sets of peculia r data by using different pe-culiarity measures.

The PDD framework provides three peculiarity mea-sures: cumulative distance (CD), fraction of data outside neighborhood (FDON), and neighborhood density ratio (NDR). These methods are derived from the peculiar data detection methods discussed in Section 2. Each method fo-cuses on particular properties of a record. The PDD frame-work also includes our new composite z score (CZ) mea-sure, which calculates overall peculiarity by combining val-ues for the CD, FDON, and NDR measures in an appropri-ate way.

The objective of the peculiar ity measurement step is to calculate the peculiarity f actor of each record in the VR-table. We add one attribute, called PF, to the VR-table to store the peculiarity factor of record. Let R be the set of records in the VR-table, R i =( x i 1 ,...,x in ) be the target record whose peculiarity factor is to be calculated and n be the number of attributes in the VR-table, excluding the re-mark and PF attributes. For each method we assume there is some defined function to calculate the distance between two records. We will describe how the peculiarity factor of R is obtained for each of the CD, FDON and NDR measures.
Cumulative Distance (CD). The CD of a record is the summation of distance between the record and every other record. Cumulative distance is a global measure, which measures any given record against all other records in the data set. The records with lowest and highest values are more peculiar than a record that is close to the mean. CD was introduced by Zhong et al. [28, 29] and has already been covered in Section 2.
 Fraction of Data Outside Neighborhood (FDON). The FDON of a record is the percentage of records that lie out-side its d -neighborhood. This measure takes into account a record X  X  local neighborhood in comparison to all data avail-able. The d -neighborhood of R i is discussed in Knorr X  X  method in Section 2. In our framework, the FDON of R i with distance d is the fraction of records in R that lie at a greater distance from R i than d . Formally, the FDON of R with distance d is: FDON d ( R i )=
Neighborhood Density Ratio (NDR). The NDR of a record is the ratio of the average density of the k-neighbor-hoods of the k-neighbors of the record to the density of the k-neighborhood of the record. This measure focuses only on the local properties of a record. If the density of the k-neighborhood of a record is the same or higher than that of its neighbors, then that record is not peculiar. If the density of the k-neighborhood of a record is lower than that of its neighbors, then it is peculiar. NDR is based on the local out-lying factor in the Breunig X  X  method [6, 7, 11] as discussed in Section 2.

The density of a k-neighborhood N k ( R i ) is the inverse of the total distance between R i and all records in N k (
Density ( N k ( R i )) = The density of N k ( R i ) is  X  when at least k other records have the same value as R i . Breunig X  X  method [6, 7, 11], for simplicity, assumes that there are no duplicates. Bre-unig et al. suggest using the k -distinct-distance instead of the k -distance with the additional requirement that there be at least k distinct objects different from R i .However,when we consider only distinct objects, a record that has a dense k-neighborhood but has distant distinct neighbors may have a lower density than a record having closer distinct neigh-bors but a relatively scarce k-neighborhood. In the PDD framework we solve this problem by adding a small con-stant c to the difference between two records to avoid dis-tances of 0. This maintains the relative order according to the density function.
 Density ( N k ( R i )) =
The Neighborhood Density Ratio, using the k-neighbor-hood, of record R i is NDR k ( R i )=
Composite Z Score Measure . We define the composite z score (CZ) measure to assess the overall peculiarity, based on the other three peculiarity measures. For a particular view, every record in the VR-table is scored by each of the first three peculiarity measure. During the calculations of each measure, the mean and standard deviation of the PF values are calculated. An additional pass is made over the data to calculate the z-scores of each PF value for each of the first three measures.

In more detail, the zscore of any given value xX is defined as z ( x )= x  X   X  X  X  X , where  X  X is the mean of all X values and  X  X is the standard deviation of all X values. Let m ( x ) be the value of peculiarity measure m applied to  X 
MX and  X  MX are the mean and standard deviation of all m ( x ) values. Given a set M of peculiarity measures, a set of weights w m such that mM w m =1 , the overall pe-culiarity CZ of a record x in the VR-table for a particular view is the weighted sum of its z-scores from each mea-sure mM , i.e., CZ ( x )= mM w m z m ( x ) . In this paper, M = { CD, FDON, NDR } , so using equal weights for these weights gives CZ ( x )= z CD ( x )+ z FDON ( x )+ z NDR ( The CZ measure takes into cons ideration a record X  X  global and local positioning when determining peculiarity. A software system called the Peculiar Data Detection System (PDDS) that detects all categories of peculiar data relevant to the PDD framework was developed. PDDS was initially implemented using Oracle; the current version is implemented in Visual Basic as an add in to Microsoft Ac-cess or Excel. Given a Excel worksheet containing data, PDDS presents its results in a series of added worksheets corresponding to the peculiarity views.

Experiments are reported here concerning NHL hockey data and Regina weather data in Tables 4-9. The top three to five peculiar data according t o each view are presented. Four peculiarity measures were used: CD, FDON, NDR, and CZ. Other experiments, not shown, were conducted us-ing New Zealand Stock Market data and Crescent coast wa-ter level data, with similar results.
 The distance function for each view was implemented as Euclidean distance. We used an option in our software to normalize the values for the attributes using z scores. With the z score, when every item in a distribution is converted to its z score, the transformed scores necessarily have a mean of zero and a standard deviation of one. This transformation is useful when comparing relative standings of items from distributions with different means and standard deviations, as one commonly encounters with dissimilar attributes.
Experiments were conducted using player data from the 1993-1994 National Hockey League (NHL) season and Regina weather data. The hockey data consists of 871 records, one per hockey player (provided by E. M. Knorr). The hockey data was previously used in well known related research[15]. The weather data consists of 18,098 daily records between 1-Feb-1900 and 31-Dec-1949 and five at-tributes: highest temperature, lowest temperature, snow precipitation, rain precipitation and precipitation (in mm). This data set was large enough to hold some surprises and interpreting weather peculiarities was relatively simple. To simplify interpretation, we used a single attribute, such as precipitation, rather than combinations of attributes, for all views except the attribute view.
 Record View Experiment: The record view experiment presented in Table 4 was conducted on the NHL input table consisting of the attributes goals (G) and shots (S). The aim of this experiment was to detect players (records) that had peculiar combinations of goals and shots. The # and Name fields were used as record iden tifiers. The data were nor-malized before computations, with the normalized values of the attributes given in the G z-norm and S z-norm fields. The next four columns represent the results of each mea-sure, with the results sorted according to the CZ measure. The top five results are presented in Table 4. The records for Brett Hull, Pavel Bure, and Brendan Shanahan represent players who were among the league leaders in both cate-gories, and thus, are very peculiar (in a good way!). The records for Ray Bourque and Cam Neely are peculiar be-cause they either have extremely high shots and low goals (Bourque) or extremely high goals and low shots (Neely). Without normalization, the records for Neely and Bourque would not have been detected as peculiar. They were pecu-liar for having high values in one attribute, and low values in the other. If the data had not been normalizing, the mea-sures would have been dominated by the records that had extremely high values for the attributes.
 Attribute View Experiment: The attribute view experi-ment presented in Table 5 was conducted on the NHL input table consisting of all attributes representing goal subcat-egories (i.e. power play goals, short handed goals, game winning goals, and game tying goals). Each attribute rep-resents a similar statistic. The goal of this experiment was to detect which of these attributes is the most peculiar. In Table 5, the results are sorted according to the CZ measure. The most peculiar attribute as detected by all measures is the power play goals (PPG) attribute. This attribute has an average value much higher than the other attributes. The next most peculiar is the game tying goals (GTG) attribute. This attribute has an average v alue that is the lowest of all the attributes.
 Frequency View Experiment: The frequency view experi-ment presented in Table 6 was conducted on the NHL input table attribute player number (#). The purpose of this exper-iment is to identify the numbers worn by NHL players that are the most peculiar. The top five results are presented in Table 6. The Record column represents the player number, while the Frequency column stores the number of players in the league that wear the corresponding number. The num-bers 25, 23, 17, 20, and 27 are the most peculiar, because they occur so often. For comparison, there are 99 possible numbers that a player can wear, and there are 872 players, so one could expect each number to be worn by about 9 players. However, there are 19 numbers that are worn by only one player, and 22 numbers that are not worn by any players at all.
 Interval View Experiment: The interval view experiment presented in Table 7 was conducted on the total precip-itation (Precipitation) attribute of the Regina Weather in-put table. The data were sorte d according to p recipitation amount, and then empty intervals were discovered. The pur-pose of this experiment is to discover precipitation amounts that are uncommon in Regina. The top five results of the experiment are presented in Table 7, where precipitation is recorded in mm x 10. The first two columns in Table 7 rep-resent the record at the beginning of the interval. The next column, Next Record, stores the value of the endpoint of the interval. The Precipitation Interval column stores the size of the interval. The largest and most peculiar interval is between 95.3 mm and 78.7 mm. This means that there are no records that had an amount of precipitation in that interval. The next largest interval was 10.5 mm. These in-tervals all represent large gaps in precipitation amounts. By comparison, the average interval is under 0.5 mm. Sequence View Experiment: In this experiment for the se-quence view we used the Regina weather data. This data can be ordered according to date, and then searched for peculiar sequences of records. We used the attribute High Temper-ature to find consecutive days that had peculiar changes in temperature. The temperatu re is recorded in degrees Cel-sius x 10. We used a sequence of length two. The first column in Table 8, Date, represents the date of the start-ing record in the sequence. Th e column High Temp repre-sents the high temperature of the starting record in the se-quence. The next columns, Sequence and Sequence Value, represent the sequence in its entirety. Sequence stores each record in the sequence. Sequence Value stores the value of the sequence. In this case we used the absolute value of the distance between records in the sequence as the overall measure of a sequence value. We can see the most peculiar sequence was 0  X  -300, a difference of 30 Celcius degrees between consecutive days. All records in the top five results represent large differences between two consecutive days. By comparison, the average difference was 3.95 degrees. Sequence of Deltas Experiment: The experiment for the sequence of deltas view was performed on the Regina Weather input table attribute High Temperature. We used a sequence of length 6 to try and detect weeklong periods with a lot of fluctuation in temperature. The temperature is recorded in degrees Celsius x 10. The Date column of Table 9 represents the start of the week represented by the sequence of changes. The High Temp column represents the high temperature on the first day of the week. The sequence of deltas column represents the consecutive differences in a period of seven days. The sequence of deltas value stores the sum of the differences in the sequence of changes. The top peculiar Sequence of deltas, which have differences in temperature of -5  X  11  X  0  X  0  X  0  X  0, represents a week in which the temperature fluctuated a total of only 16 / 10 = 1.6 Celsius degrees. This is peculiar because it is the smallest sequence of change in the entire record set. By comparison, the average change in temperature totaled over any weeklong period was 23.7 Celsius degrees. The remaining sequences of change all represent weeklong pe-riods where the total change in temperature was relatively large (between 86.2 and 88.4 Celsius degrees). We also noted that all weeks in the mo st peculiar records occurred during winter months, which may be of interest to an expert or specialist in the field.

Let us also mention a few of the peculiarities that are found for the precipitation attribute of the weather data (not shown). In the record view, the precipitation values of 95.3 mm, 78.7 mm, and 76.5 mm are the top 3 record peculiar data according to all four measures. These values are the highest daily precipitations observed in 50 years. They are far from the average precipitation of 10.49 mm and likely to be of interest to a person interested in peculiar weather. The top frequency peculiar data was precipitation of 0.0 mm, since it occurred with a frequency much different than that of any other amount of precipitation. For the interval pecu-liar data, both the largest and the smallest intervals are de-tected as peculiar by the PDD framework. The interval from 78.7 mm to 95.3 mm is the largest interval and is classi-fied as the most peculiar interval by all four measures. This interval tells us that there were no days with amounts of precipitation between 78.7 mm and 95.3 mm. The small-est empty interval, which is between 0.0 mm and 0.3 mm is selected as third most peculiar by the the FDON mea-sure. In sequence peculiar data, the sequence 95.3 mm  X  0.0 mm, which is the biggest drop in precipitation between con-secutive days, is the most p eculiar according to the FDON and NDR measures. The sequence 10.7 mm  X  95.3 mm is most peculiar according t o the CD measure. This in-formation about peculiar frequencies, intervals, sequences, and sequences of deltas provided by the PDD framework contributes perspectives on th e data that were not available from previous approaches. In comparison, Zhong X  X  method, Knoor X  X  method, and Breunig X  X  method would find only the record peculiar data, and each of these methods would use only one peculiarity measure. Thus, they would not find the peculiar records selected from the other views.
We developed the PDD framework to detect peculiar data. The framework has three major advantages. First, it detects a variety of categories of peculiar data with each category revealing a possibly unique aspect of peculiarity present in the data. Second, our framework unifies exist-ing peculiarity data detection methods. For example, the three peculiar data detection methods discussed in Section 2 have been converted and generalized into three peculiar-ity measures in the framework. These measures vary from a purely global approach to a purely local approach. Third, our framework can be extended to include new views and peculiarity measures. For ex ample, the composite measure in the PDD framework is a new measure that combines the CD, FDON, and NDR peculiarity measures.

To implement the PDD framework, we developed the Pe-culiar Data Detection System (PDDS) and used it to detect peculiar data from the NHL hockey data and the Regina weather data. Our experiments demonstrated that a vari-ety of categories of peculiar data can be detected from data. Each category revealed a unique aspect of the data. Differ-ent peculiarity measures often gave different results, pro-viding diverse information for the user.

Future research could focus on identifying additional views that may be useful in peculiar data detection. For example, diversity measures or clustering algorithms might provide inspiration for other views. Future research could also determine the usefulness of detected peculiar data in a particular domain with the help of domain experts. [1] Agarwal, R. and Srikant, R. Mining Sequential Pat-[2] Aggarwal, C.C., and Yu, P., Outlier Detection for High [3] Barnett, V. and Lewis T. Outliers in Statistical Data . [4] Bay, S.D. and Schwabacher, M. Mining Distance-[5] Bolton, R.J. and Hand D.J. Statistical Fraud Detection: [6] Breunig, M.M., Kriegel, H.P., Ng, R.T. and Sander, [7] Breunig, M.M., Kriegel, H.P., Ng, R.T., and Sander, J. [8] Eskin, E., Arnold, A., Prerau, M., Portnoy, L, and [9] Hawkins, S., He, H., Williams, G., and Baxter, R. [10] He, Z., Xu, X., and Deng, S. A Unified Subspace Out-[11] Jin, W., Tung, A.K.H., and Han, J. Mining Top-n [12] Keogh, E., and Folias, T. The UCR Time Se-[13] Knorr, E.M., and Ng, R.T. Algorithm for Mining [14] Knorr, E.M., and Ng, R.T. A Unified Notion of Out-[15] Knorr, E.M., Ng, R.T., and Tucakov, V. Distance-[16] Lane, T. and Brodley C.E. Temporal Sequence Learn-[17] Lazarevic A., Ertoz, L., Ozgur, A., Srivastava, J., and [18] Lazarevic, A. and Kumar V. Feature Bagging for [19] Liu, B., Hsu W., and Chen, S. Using General Im-[20] Mannila, H., Toivonen, H., and Verkamo, A.I. Discov-[21] Portnoy, L. Intrusion Detection with Unlabeled Data [22] Ramaswamy, S., Rastogi, R., and Shim, K. Efficient [23] Sigurdsson, S., Larsen, J., Hansen, L.K., Philipsen, [24] Srikant, R., and Agarwal, R. Mining Sequential Pat-[25] Tang, J., Chen, Z., Fu, A., and Cheung, D., A Ro-[26] Yamanishi, K., and Takeuchi, J. A Unifying Frame-[27] Yu, D., Sheikholeslami G., and Zhang, A. FindOut: [28] Zhong, N., Yao, Y.Y., Ohshima, M., and Ohsuga, [29] Zhong, N., Yao, Y.Y., Ohshima, M., and Ohsuga, S. [30] Zhu, C., Kitagawa, H., Papadimitriou, S., and Falout-[31] http://www.rulequest.com/.

