 Automatic classification of data items, based on training samples, can be boosted by considering the neighborhood of data items in a graph structure (e.g., neighboring documents in a hyperlink environment or co-authors and their publica-tions for bibliographic data entries). This paper presents a new method for graph-based classification, with particular emphasis on hyperlinked text documents but broader ap-plicability. Our approach is based on iterative relaxation labeling and can be combined with either Bayesian or SVM classifiers on the feature spaces of the given data items. The graph neighborhood is taken into consideration to exploit lo-cality patterns while at the same time avoiding overfitting. In contrast to prior work along these lines, our approach em-ploys a number of novel techniques: dynamically inferring the link/class pattern in the graph in the run of the itera-tive relaxation labeling, judicious pruning of edges from the neighborhood graph based on node dissimilarities and node degrees, weighting the influence of edges based on a dis-tance metric between the classification labels of interest and weighting edges by content similarity measures. Our tech-niques considerably improve the robustness and accuracy of the classification outcome, as shown in systematic experi-mental comparisons with previously published methods on three different real-world datasets.
 Categories and Subject Descriptors: I.5.2 [Pattern Recognition]: Classifier design and evaluation H.3.3 [Information Systems]: Information Search and Re-trieval General Terms: Theory, Algorithms, Reliability Keywords: Exploiting Link Structure, Automatic Classifi-cation
Automatic classification is a supervised learning technique for assigning thematic categories to data items such as cus-tomer records, gene-expression data records, Web pages, or text documents. It has numerous applications in fields like data mining, information retrieval, Web analysis, business analytics, bioinformatics, etc. The standard approach is to represent each data item by a feature vector, e.g., the fre-quencies of words or word stems in a Web page, and learn parameters of mathematical decision models such as Sup-port Vector Machines, Bayesian classifiers, or decision trees, based on intellectually labeled training data. When the trained classifier is later presented with test data with un-known category labels, the standard paradigm is to apply the decision model to each data item in a  X  X ontext-free X  manner: the decision is based only on the feature vector of a given data item, disregarding the other data items in the test set.

In many settings, this  X  X ontext-free X  approach does not exploit the available information about relationships between data items. For example, if we are asked to classify a book into a genre, we can take advantage of some additional in-formation like the genre of other books by the same author or what other books were bought by the readers of this one. Similarly, the hyperlink neighbors of a Web page give us clues about the topic of a page.

Using the relationship information, we can construct a graph G in which each data item (e.g., Web page) is a node and each relationship instance (e.g., a hyperlink) forms an edge between the corresponding nodes. Then the classifi-cation problem can be formulated as a graph labeling or coloring problem on such a graph. In the following we will mostly focus on text documents with links to and from other documents; thus, we will often use the document/link ter-minology, but the approach is applicable to a wide variety of graph types derived from structured, semistructured, or unstructured data items and their relationships.

A straightforward approach to capturing a document X  X  neighbors (i.e., the directly related nodes in the graph) would be to incorporate the features and feature weights of the neighbors into the feature vector of the given document it-self. This has indeed been attempted in the literature for the case of hyperlinked text documents. However, such fea-ture engineering seems to face tremendous problems of pa-rameter tuning, e.g., for adjusting the weights of neighbor features when added to the document X  X  own features, and does generally not lead to a robust solution [3]. Rather, a more advanced approach is to model the mutual influ-ence between neighboring documents, aiming to estimate the class labels of all test documents simultaneously. In prin-ciple, such a model could even consider long-range influences among transitively related documents, with decreasing influ-ence as the distance in the graph increases. For tractability, however, it makes sense to focus on the strongest dependen-cies among immediate neighbors. Such a model is called a first-order Markov Random Field or MRF [14, 17]. Com-puting the parameters of an MRF such that the likelihood of the observed training labels is maximized is a difficult problem that cannot be solved in closed analytic form and
Figure 1: Relaxation labeling (RL) illustration. is typically addressed by an iteration technique known as relaxation labeling (RL) . Our approach builds on this math-ematical technique.

A simple example for RL is shown in Figure 1. Assume a partially supervised classification scenario, i.e., some of the documents in the test graph have already assigned class la-bels as in Figure 1 a). Let our set of classes be C = {4 , We wish to assign to every document marked  X ? X  its most probable label. The label assignments have to be consistent with the observed data from the training procedure. For our example purposes, let the contingency matrix in Figure 1b) be estimated from the training data. It contains the prob-abilities of class pairs in neighboring nodes. As discussed, the RL technique aims to combine the available node infor-mation with the information that can be induced from the graph structure. First it uses a bootstrap mechanism to as-sign initial labels to all documents without any label. In our case, a content-only classifier assigns labels 4 or 2 to all test documents marked with  X ? X , shown in Figure 1 c). Later, RL iteratively corrects this initial assignment if the neigh-boring documents have labels that are unlikely according to the matrix in b). Suppose after step c), test document d is assigned a label 2 . Since all neighbors of d are labeled 4 the RL procedure may decide to  X  X orrect X  this label to 4 and output label assignments as in Figure 1d).
The theory paper by Kleinberg and Tardos [12] views the classification problem for nodes in an undirected graph as a metric labeling problem where we aim to optimize a combi-natorial function consisting of assignment costs and separa-tion costs. The assignment costs are based on the individual choice of label we make for each object while the separation costs are based on the pair of label choices we make for two neighboring objects. The combination of the assignment and the separation costs gives the total cost. A labeling that minimizes the total cost is called a maximally likely la-beling of the test graph. [12] shows that the metric labeling problem is NP-hard and presents an approximation scheme for this problem, based on integer linear programming.
Instead of seeking a global optimization, which obtains a unique labeling for all nodes in the test graph, S. Chakrabarti et al. [3, 1] propose to start with a greedy labeling of the graph, paying attention only to the node-labeling (assign-ment) cost, and then iteratively  X  X orrect X  the neighborhood labeling where the presence of edges leads to a very high penalty in terms of the separation costs. In the context of link information, the relaxation labeling algorithm first uses a Naive Bayes text classifier to assign class probabilities to each node. Then it considers each page in turn and reevalu-ates its class probabilities in light of the latest estimates of the class probabilities of its neighbors.

Oh et al. [16] present a  X  X ingle step X  approach in which the label of each node d in the graph is influenced by the popularity of this label among all immediate neighbors of d and the level of confidence in the labels of the documents in the neighborhood. To this end, the term weight w t for any document is adjusted using the term frequencies in the neighboring documents and a parameter  X  that controlls the degree of influence.

A regression model was proposed by Lu and Getoor [15] which combines the text features for every given test docu-ment with the label assignments of its neighbors and itera-tively tries to improve the classification result. [15] distin-guishes three ways of constructing a feature vector for each document. In Binary mode , a document feature vector in-cludes its  X  X ext X  features and their weights plus additional new features, derived from the underlying link structure, namely every class label c i  X  X  that appears at least once in the neighborhood of d , N ( d ). In Count mode , the docu-ment feature vector is enhanced with the neighbors X  labels c weighted by their frequencies in N ( d ). In Single mode , only the most popular class label is included into the feature vec-tor of document d , with weight equal to its frequency in N ( d ).

In general, graph-aware mining has lately received much attention in machine learning, but the major focus has been on issues like authority ranking, information extraction, en-tity disambiguation, and relation learning (see, e.g., [10, 18, 6, 9, 7, 5, 13, 2]).
We propose a new algorithm that builds on, but extends and generalizes the earlier work by Chakrabarti [3] that uses the theory of Markov Random Fields to derive a relaxation labeling technique for the graph labeling problem. The ma-jor novelties lie in making the approach more robust (i.e., less susceptible to particularities of the data or hand-tuned parameters) and achieve higher classification accuracy than the prior work. Salient points of our graph-based classifier (GC) are: We present a systematic performance evaluation of our GC algorithm, based on three different real-world datasets and with detailed comparison to all known competitors [3, 15, 16]. Our method achieves significant gains in accuracy in some cases and performs as well as the best alternative in all other cases. All previously proposed methods, while show-ing good results in some cases, fail in robustness as their performance dramatically deteriorates in other cases. Some of our novel techniques could also be integrated into prior methods, but combining all of them into the full-fledged GC algorithm provides synergies and achieves the best perfor-mance.
Our approach is based on the probabilistic formulation of the classification problem and uses a relaxation labeling technique to derive two major approaches for finding the maximally likely labeling  X  of the given test graph: hard and soft labeling. Let D be a set of documents. Also, let G be a graph whose vertices correspond to documents d  X  X  and edges represent the link structure of D . As input, the classifier has the text of each document d and information about which documents of G constitute its neighborhood, N ( d ). Since we consider graphs whose nodes are being la-beled we will refer to nodes in the graph and documents interchangeably, as well as to edges and links.  X  ( u ) denotes the label of node u , either an initial, a final, or an intermedi-ate label whose validity can be associated with a probability. The feature vector that locally captures the content of doc-ument d , derived from the set of terms that occur in d , is denoted by  X  ( d ). The output of the classification should be an assignment of labels to the graph nodes such that each document d has its maximally likely label c that belongs to a finite set of labels C .

Taking into account the underlying link structure and doc-ument d  X  X  content-based feature vector, the probability of a label c  X  X  to be assigned to d is: Pr [  X  ( d ) = c |  X , G ] = Pr [  X  ( d ) = c |  X  ( d ) ,  X  ( d where d 1 through d l are the documents in D .

In the spirit of the introduction X  X  discussion on emphasiz-ing the influence of the immediate neighbors for each doc-|  X  ( d ) , N ( d )] and denote it by  X  c,d . This reflects the MRF assumption that the label of a node (as a random variable) is conditionally independent of the labels of other nodes in the graph given the labels of its immediate neighbors. We abbreviate Pr [  X  ( d ) = c |  X  ( d )] , the graph-unaware probabil-ity based only on d  X  X  local content, by  X  c,d . The probability of a specific labeling,  X  ( N ( d )) , to be assigned to the test doc-ument neighborhood N ( d ) and is denoted by Pr [  X  ( N ( d ))] . Applying, for tractability, the additional independence as-sumption that there is no direct coupling between the con-tent of a document and the labels of its neighbors, the follow-ing central equation holds for the total (prior, i.e., uncondi-tional) probability  X  c,d , summing up the posterior (i.e., con-ditional) probabilities for all possible labelings of the neigh-borhood:  X 
In the same vein, if we further assume independence among all neighbor labels of the same node (but still capturing the dependence between a node and each of its neighbors), we reach the following formulation for our neighborhood-conscious classification problem:  X  This can be computed in an iterative RL manner as follows:  X  where r &gt; 1. With the short-hand notation  X  c,c 0 = Pr [  X  ( d ) = c  X   X  ( d 0 ) = c 0 ] we can rewrite this into:
So far, we presented our basic framework. The remaining part, which poses the major difficulties and where our in-novation lies, is the estimation of parameters, for the initial solution and during the iterations. For iteration ( r = 0) all test nodes receive their initial labels from the purely content-based classifier, e.g., a Multinomial Naive Bayesian Classi-fier or an SVM classifier. All iterations that follow are based on Equation (1). We iterate until the probabilities  X  c,d each document and class, stabilize, i.e., the magnitude of change drops below some stop parameter . The relaxation is guaranteed to converge to a locally consistent assignment if initiated sufficiently close to a consistent labeling [14, 17]. In [17] it is shown that the relaxation algorithms are lo-cal optimizers by nature (similarly to EM methods). They do not necessarily arrive at the global optimum. Given a relaxation labeling algorithm and the data, two factors af-fect the solution: the initial label assignment and the cost function used to iteratively find the global maximum of the labeling function. In our case, the iterative scheme contains both factors: the initial label assignment from the content-based classification in iteration r = 0, and the cost function involving a dynamically computed separation cost (i.e., la-beling neighbors with different topics), re-adjusted in each iteration. The latter is absent in the prior work [3], which computed its notion of separation cost only once upfront.
Calculating the sum over all possible labelings in Equa-tion (1) is hard as we have m | N ( d ) | summands, where m is the number of distinct class labels. To solve this problem we employ two major methods described in Subsections 2.2 and 2.3. Depending on the chosen method of classification, we approximate the sum over all possible labelings of the neigh-borhood to either its most significant summand, treating it as if it were the true set of labels ( hard labeling), or the p most significant summands ( soft labeling) and their associ-ated probabilities where p is a tunable constant. The latter method was employed in [3], but computes these neighbor-labeling probabilities only once before the first RL iteration whereas our GC method efficiently re-computes the updated probabilities after each RL iteration. To this end, we pro-pose a fast algorithm to compute the p most significant sum-mands, and present it in Section 2.2.1.
The soft labeling approach aims to achieve better accuracy of the classification by avoiding the overly eager  X  X ounding X  that the hard labeling approach does. Instead, we take into account the p most significant combinations of labelings of the test document neighborhood (Equation (1)). This is motivated by the observation that apart from the few most probable labelings of the neighborhood ( p ), the remaining combinations of class assignments have very low probabili-ties. Thus, they do not contribute much to the calculation of  X  ( r ) c,d (the probability of document d to be labeled c ) and can be ignored. This reduces the exponential number of summands in Equation (1) from m | N ( d ) | to p and makes its computation feasible.
Reducing the number of possible neighborhood labelings corresponds to finding the p shortest 0  X  n paths in the graph shown in Figure 2. To do that we propose an efficient algorithm that works as follows.

In the graph each neighbor of d is a node i ( i = 1 ..n ) and the edges between the nodes have weights (  X  log  X  c,d sorted in decreasing order. We denote by  X  i the i th shortest path and by  X  ij , the path obtained by changing the edge between nodes v j and v j +1 in  X  i with the next higher edge. To avoid the problem that a path can have more than one parent, we enforce the condition that if path  X  k is formed as  X  , i.e., derived from path  X  i by changing the j th edge, then it is not allowed to create paths  X  kl with l &lt; j . We store the lengths of all paths  X  ij (as well as its corresponding i and j ) that can be derived from each path  X  i computed so far. We create the next path by picking the smallest path length.
The running time of this algorithm is O ( nm logm + pn logp ) where n is the number of nodes and m is the number of dis-tinct labels. Its advantages are simplicity of implementation and a slightly worse running time than the optimum O ( pn ). The approach used in [3] and proposed by Eppstein [8] is very complicated to implement and does not yield the ex-plicit paths but some encoding of the paths from which one can compute the actual paths incurring extra cost.
In contrast to the presented soft labeling approach equiv-alent to [3], we also consider a method that takes into ac-count only the most probable label assignments in the test document neighborhood to be significant for the  X  ( r ) c,d Pr [  X  ( d ) = c |  X , N ( d )] ( r ) computation. We call this newly employed approach hard labeling . It might be seen as a crude approximation of the sum in Equation (1) but de-pending on the sets C and D this approximation gives very good empirical results (see Section 4.3). Our hopes for this method would be that it is more efficient and possibly more robust than the soft labeling method. Hard labeling can be thought of as a  X  X igh-level X  noise reduction, since it trusts only the most probable neighborhood labeling and ignores labelings with lower probability, thus reducing the chance of an incorrectly labeled node in the neighborhood to influence the label assignment of the current test node.

Let c 0 max be the maximum probable label for each docu-ment d 0  X  N ( d ) as of iteration ( r  X  1):
Then considering only the maximum probable  X  ( N ( d )) ac-cording to the node label probabilities of iteration ( r  X  1) , Equation (1) can be written as: and its simplified form, presenting the product over the set of classes rather than documents in the neighborhood N ( d ): where n ( c 0 ) is the frequency of label assignment c 0 in N ( d ): n ( c 0 ) = |{ d 0  X  N ( d ) |  X  ( d 0 ) = c 0 }| .
The probability  X  c,c 0 of a pair of labels c, c 0 to be as-signed to the endpoints of the edge between documents d and d 0 depends on the class-conditional probabilities of the corresponding nodes from the previous iteration. We have developed two major ways of calculating  X  c,c 0 : We use the first method in combination with the hard node labeling approach and abbreviate the classifier as GC [ H ]. The latter one is used with the soft node labeling method. The corresponding abbreviation is GC [ S ].
Our method uses link weights based on the following ra-tionale. The label of a document should be more influenced by neighbors with a homogeneous content, that is themat-ically closely related to the document X  X  own content. As an intuitive example suppose we are interested if a given Amazon product page sells cosmetics or routers. Each page contains highly valuable outgoing links, e.g., links to the product description on the manufacturer page or compari-son between similar products. But all too often we would also find on the same product page many non-relevant links like a link to the Amazon X  X  generic  X  X atest products X  page, user X  X  shopping cart, profile, etc.

We aim to ignore the unnecessary and most probably noisy information behind these irrelevant links by assign-ing to each edge e a weight w e equal to the cosine similarity between the feature vectors of the documents connected by the edge.

This edge weighting schema is applied for noise reduction in two ways: 1) we prune edges whose weight w e is below a specified similarity threshold (S-threshold) ), and 2) we dif-ferentiate links by using higher w e to promote links that are considered more important. These considerations lead to the following revised label probability for the RL algorithm:
A Naive-Bayesian Multinomial classifier (NB) as a con-tent based classifier is a natural choice in the MRF frame-work [3]. However, there are more powerful text classifiers; most notably, Support Vector Machines (SVM) are among the very best performing ones. SVM is a discriminative classifier that predicts only the class label, no label prob-ability distribution. Methods for translating the output of SVM into probability estimates are discussed and compared in [19]. In our implementation we use the libsvm package [4] which supports probability estimates for SVM classifier based on the methods of [19].
Intuitively, neighboring documents should receive similar class labels. For example, suppose we have a set of classes C = { Culture (C), Entertainment (E), Science (S) } and we wish to find the most probable label for a test document d . Typically, documents that are related to culture or enter-tainment have overlapping areas of discussion like concerts, exhibitions, etc. This means, the labels C and E are close to each other. On the other hand, a document discussing scien-tific problems ( S ) would be much farther away from both C and E . So, a similarity metric  X  (  X  ,  X  ) imposed on the set of labels C would have high values for the pair ( C, E ) and small values for class pairs ( C, S ) and ( E, S ). Back to our exam-ple, suppose that document d has four neighbors and that these are labeled  X  ( N ( d )) = { C, C, E, E } . Then, the prob-ability that document d would be assigned class C should be higher than the same probability if the neighboring doc-uments of d were labeled  X  ( N ( d )) = { C, C, S, S } . Note that both of these neighbor labelings have the same number of C labels; the key lies in the different similarities between C and the other topic in each of the two labelings.
This is why introducing a metric  X  (  X  ,  X  ) should help im-prove the classification result. In this metric, similar classes are separated by a shorter distance and impose smaller sep-aration cost on an edge labeling.

The theory paper by Kleinberg and Tardos [12] suggests constructing an r-HST tree approximation for the label dis-tances, which is then plugged into their formulation of the global optimization problem. The argument for this specific approximation is tractability of the otherwise NP-hard op-timization problem, but the approach loses generality. In a classification scenario the r-HST tree can not capture the similarities between classes in a realistic manner (while also preserving the triangle inequality).

Our approach, on the other hand, is general, and we con-struct the metric  X  automatically from the training data. The metric value for a pair of labels ( c i , c j ) is computed by the content similarity between the corresponding sets of doc-uments that were  X  X and-tagged X  as c i or c j at training time. In our experiments, we use the term-vector based cosine sim-ilarity between the  X  X uper-documents X  that concatenate all training documents of the same class. This metric is com-pletely precomputed, and does not change as the relaxation labeling proceeds. It would also be easy to include a human supervision step, for an expert to adjust the metric values based on additional domain knowledge.

We incorporate the label metric into the iterations for computing the probability of an edge labeling  X  c i ,c j by treat-ing  X ( c i , c j ) as a scaling factor. This way, we magnify the impact of edges between nodes with similar labels and scale down the impact of edges between dissimilar ones:
 X 
The product of the  X  and  X  terms on the right-hand side can be viewed as the probability that d and d 0 are themat-ically related, having specific labels with probability  X  and the labels being thematically close with probability  X . Our experimental results (Section 4.6) show that incorporating  X  into the relaxation labeling significantly contributes to more accurate classification results especially when the training dataset is very small. In such cases the algorithm exploits the additional  X  X uidance X  that the label metric provides for estimating the edge label probabilities in each iteration.
We have tested our graph-based classifier on three differ-ent data sets. The first one includes approximately 16000 scientific publications chosen from the DBLP database. The set of classes includes  X  X atabase X  (DB),  X  X achine Learn-ing X  (ML), and  X  X heory X  as labels. These labels are exposed to the classifier only if the corresponding documents belong to the training data. We labeled the documents based on the conference in which they were published. For example, if a paper appeared in SIGMOD or VLDB proceedings, then it was assigned to the DB set. We chose co-authorship as the relation determining the connectivity between documents. For the initial step of purely content-based classification we use the document titles as the only source.

The second dataset has been selected from the Internet movie database IMDB. For our tests we took into account all movies in which a given set of 80 famous actors occur (e.g., Johnny Depp, Bruce Willis, Mel Gibson, etc.). This resulted in about 5000 movies grouped in 4 genres:  X  X ction X ,  X  X om-edy X ,  X  X ocumentary X , and  X  X rama X . This dataset is chal-lenging for automatic classification because initially many movies were  X  X and-tagged X  with more than one genre. For our test we merged multiple genres of the same movie by using only the most  X  X rominent X  one as a  X  X rue X  genre of a movie. For example, if a movie had a label Action and Sci-ence fiction , we considered it to be Action . For each movie we took its title and plot as a source of features for the ini-tial content-based classification. We removed all stopwords and applied stemming to reduce the dimensionality of the feature space. This resulted in a feature set of around 19 000 terms. We built an edge between two documents (movies) in the graph if they have a starring actor in common. We applied similarity-based edge pruning for noise reduction. Our default S -threshold 0.25 left us with 800 nodes each of which has at least one neighbor. All  X  X ingleton X  nodes without edges were disregarded in the experiments as all graph-aware methods would behave identically to content-based classifier on these nodes.

The third dataset used in the experiments was the online encyclopedia Wikipedia. We crawled about 5000 documents from the released Wikipedia dump file. As links between the pages we used their natural hyperlink connectivity. We re-stricted the crawler to follow only links within Wikipedia. The feature space had about 70 000 unique terms; no fea-ture selection was performed. The distinguished classes are  X  X olitics X ,  X  X omputer Science X ,  X  X hysics X ,  X  X hemistry X ,  X  X iology X ,  X  X athematics X , and  X  X eography X . We started the crawl from the main pages for each of these subjects and used topic-specific words in the anchor text as indica-tors for whether an outgoing link should be followed. For example, we started gathering documents from the page http://en.wikipedia.org/wiki/Chemistry by following links that contained manually selected word stems like  X  X hemist X ,  X  X somer X ,  X  X ranch X ,  X  X rganic X , etc. All pages gathered from the starting chemistry page were considered to be  X  X and-tagged X  as Chemistry . When the same page was discovered following paths with starting points of two different seeds, the page was discarded since no decision about its  X  X rue X  label without human assessment could be taken.

All datasets for our experiments are available at the URL http://www.mpi-inf.mpg.de/  X  angelova.
Each of the tested classifiers was given a fraction of doc-uments as training data to estimate its parameters. Our GC classifier used as the input parameters k , M , and the choice of method for calculating label probabilities. Param-eters k and M are used in the training graph construction as follows. The classifier first picks up k random documents, seeds, from the whole set D . Once all the k documents d i are selected, a subgraph induced by each document d i and its neighbors up to radius M is formed, and the training set D trn is enhanced with all newly discovered nodes. All gath-ered nodes, the k seeds and their neighbors up to depth M , form the set of vertices V trn . The links connecting these ver-tices form the set E trn . Finally, we define a training graph as G trn = { V trn , E trn } . The training graph is used by the algo-rithm to learn parameters like prior probabilities of classes and term distributions in the data. After the training phase, the algorithm classifies the given test graph which is basi-cally constructed by excluding the training graph from the available data. This construction procedure led to fairly bal-anced class-probability distributions on some of the datasets and to fairly skewed distributions on others. For example, on the IMDB set the class-probability distribution for the corresponding classes in the graph is: Action -0.19, Comedy -0.26, Documentary -0.18, and Drama -0.37. On the other hand, in the Wikipedia data set classes Chemistry (0.05) and Physics (0.09) are much less frequent than all other classes (e.g., class Politics -0.22, Mathematics -0.18).
For the experiments we repeat the construction of the test and training graph with different seeds of the random number generator while keeping all initial parameters un-changed. Then, the classifier is trained on the correspond-ing training graph and is applied to the test graph. Average values from 30 runs of each algorithm are reported as the result of an experiment.

We present the results of the graph-based classification (GC) and its variants and compare them to a content-only classification and against other neighborhood aware meth-ods previously proposed in the literature. We present the performance of all classifiers in terms of the standard mea-sures accuracy (A, i.e. 1-error) and F1 (i.e., the harmonic mean of recall and precision). Since the micro averaged F1 measure is usually considered most insightful in the litera-ture [1, 11], when aggregating results over multiple classes, we always present micro-averaged results. The macro-aver-aged results merely confirmed our findings and are omitted for space reasons.

The different algorithms under test are abbreviated as fol-lows:
To indicate the content-only classifier that we used as a seed for the initial iteration, Naive-Bayesian Multinomial classifier (NB) or Support Vector Machine (SVM), we write GC N and GC S , respectively. For SVM we used the libsvm package [4]; all other methods were our own implementa-tions in Java using JDK 1.5. Unless stated otherwise, we fix parameters as shown in Table 1.
 Table 1: Default parameters for all three data sets.
We first present the performance of GC versus content-only classifiers on the DBLP data. In this baseline exper-iment GC used no label similarity metric, edge weighting or pruning. For building the training graph we used radius M = 1 and different sizes of the initial set of documents k as seeds. The results are shown in Table 2.

Due to the features space exploited for each data set as well as the nature of both content-based classifiers, NB clas-sifier outperforms SVM on the IMDB and DBLP data sets, while SVM shows better performance than NB on the Wiki-pedia set. Because of space limitations, we present in detail the results derived by the better performing content-based classifier for the different data sets and briefly explain how the other one behaves on the corresponding set.

As we see from Table 2, the improvement of the graph-based methods versus the content-only methods is signif-icant, and this holds for both NB and SVM as underlying classifiers. In most cases, the GC method wins 2 to 4 percent in the F1 measure over the content-only classifiers. These are really significant gains given the already very good per-formance of the underlying content-only classifiers. Only for very small amounts of training data, NB and SVM slightly outperform the GC methods. We will see in Subsection 4.5 that this susceptibility to particularities of very small train-ing sets can be overcome by using the label similarity metric. Both the hard and the soft methods perform very well; the hard method seems slightly superior.

The performance on the IMDB data set is shown in Ta-ble 3 and the results for the Wikipedia data set in Table 4. The IMDB experiment faced the particular challenge that the data had very noisy links: on average a movie node was connected to almost 100 other nodes. However, our S-thresholding technique (set to 0.25 in the IMDB case) was very successful in pruning out the noise. With the edge pruning, GC outperformed the content-only classifiers by a big margin. We provide a sensitivity analysis on the S-threshold in Subsection 4.5. The improvement shown on the IMDB corpus confirms once more the hypothesis that even in scarcely connected graph, neighborhood aware classifica-tion achieves good improvement over the pure text-based classification.

The Wikipedia experiment reconfirms the good perfor-mance of GC. Here the variant with SVM as a basic content-only classifier performs best. This is easily explained by the big and rich feature set of this data. SVM is well known for excelling on large feature spaces.

In summary, the graph-based approach gains up to 5 per-cent in micro-averaged F1. The gains are most prominent for small training sets, which we consider the most important scenario. Training involves significant human labor and is time-consuming and expensive. This is why the graph-based classification is an attractive method, with very good per-formance in situations when extensive training efforts are prohibitive.

Table 4: Performance for the Wikipedia data set.
We compared the performance of our GC algorithm against the existing algorithms discussed in Section 1.1. The algo-rithm suggested by Chakrabarti et al.[3] is abbreviated as ChAlg , the one by Oh et al.[16] as OhAlg , the one by Lu and Getoor [15] as LGAlg . The reader should also note, that OhAlg takes advantage of the noise pruning strategy, based on similarity thresholding. Our implementation of LGAlg uses SVM as a basic classifier while the original paper used logistic regression, but SVM is certainly competitive to lo-gistic regression.

In Table 5 we show the results of all algorithms over the default settings of all three data sets. Note, that the micro-averaged F1 is a much more meaningful measure for the classifier X  X  performance than its accuracy.
 Table 5: Comparison of known algorithms vs. GC
GC clearly outperforms all algorithms discussed in Section 1.1. We performed Student X  X  t-test on this data and found that our results are significant for a test level of 1  X   X  = 0 . 95 or higher. Following the arguments mentioned in Section 4.3, on the Wikipedia data set, GC performs significantly better than all other competitors, when using SVM as a pure content based classifier. OhAlg performs equally well on DBLP only in the case of  X  = 1 . 0 in combination with Naive Bayesian as a text-only classifier. All other improve-ments shown on the DBLP data are significant according to the t-test. IMDB was a challenge for all graph-based clas-sifiers. It is very noisy and often the neighborhood infor-mation could be misleading. On this set we perform signifi-cantly better than ChAlg, OhAlg with all three variations of  X  , and LGAlg[Single], but LG[Binary] and LG[Count] show an even better performance here.

Most of the algorithms give good performance when the training data set is big because they require reliable knowl-edge about the term distribution (OhAlg) or the class/link pattern (ChAlg). Table 5 focuses on the case with relatively small training sets (see Table 1 for the settings). All of our competitors exhibit significant shortcomings on at least one of the data sets. ChAlg does not work well with IMDB and Wikipedia because of the richness but also noise of their link structures. ChAlg lacks the additional capabilities of sim-ilarity thresholding/weighting and label similarity metrics that GC can exploit. Also, ChAlg does not re-adjust its edge label probability estimator during the RL iterations and is thus very critically dependent on sufficient training infor-mation on link/class patterns. The performance of LGAlg deteriorates dramatically on DBLP. OhAlg shows poor per-formance on IMDB because its rationale of incorporating weighted features from neighbors overreacts to the noise in the training data.

In contrast, GC does not critically depend on the link structure in the training data and thus performs well also with small training sets. One of its key strengths is that it computes the edge label probabilities dynamically, in each iteration of the RL framework. Thus, it benefits from the  X  X p-to-date X  information that has so far been learned on the test data, not just the initialization from training.
Another novel idea, that was incorporated in the GC al-gorithm, takes advantage of a metric label similarity as a corrective measure over the predictions of the text-based classifiers when the training data size is small ( wmGC in Table 5). The role of the label metric is discussed in detail in Section 4.6 and the experiments show that it is significant.
This section discusses the influence of similarity thresh-olding and its effectiveness for noise reduction. Table 6: Similarity threshold influence for IMDB .
This issue arose only for IMDB and Wikipedia, DBLP, on the other hand, is a well structured database with a clear and meaningful almost noise-free, link structure.
For IMDB, we used common starring actors to form the link structure. Unfortunately, actors, especially in the be-ginning of their career, play in movies that often belong to extremely diverse movie genres. Table 6 shows the GC per-formance for different S -threshold values. It becomes clear that at least a similarity threshold of 0.25 is necessary to form a meaningful neighborhood around each test document from the IMDB database.
 Table 7: Similarity threshold influence for Wikipedia .
The Wikipedia data set contains many cross-links. For example, a page discussing the contributions of the famous chemist Friedrich Woehler contains links to years or places where he made his discoveries. Following such links, we end up on pages that are completely irrelevant to the topic of Chemistry, e.g.: on 1st of March  X  X ntonio Garca Gutirrez X  X  play El Trovador played for the first time X . Since we are interested in neighbors that would enhance and not degrade the quality of the classification, similarity thresholding is crucial for removing such links (or reducing their weight). In Table 7 we show the results of varying the S -threshold.
In general, finding the optimal similarity threshold is a data-dependent tuning issue. But even with a suboptimal, yet reasonable choice, GC performs much better than the other graph-conscious methods that do not consider neigh-bor similarities and are thus very susceptible to neighbor-hood noise. Automatic tuning of the S -threshold is an in-teresting topic for future work.
We show how the micro-averaged F1 score is influenced by the similarity metric induced over the set of possible labels,  X (  X  ,  X  ). The metric for the Wikipedia data is shown in Table 8. For DBLP, the similarity value was 0.5131 between labels  X  X B X  and  X  X heory X , 0.5114 between  X  X B X  and  X  X L X , and 0.5748 between  X  X heory X  and  X  X L X .
 Table 8: Label similarity metric for Wikipedia.

Table 9 and 10 show results for DBLP and Wikipedia with and without label similarity, and this is combined with edge weighting or without edge weighting. IMDB results are not shown here for lack of space.

For all datasets, incorporating  X  in the computation causes significant improvement in the GC performance. The im-pact is more prominent when the training data size is small and decreases gradually when the size of training data is increased. The predictive power of the GC is influenced by two factors: the content based initialization and the itera-tive scheme for finding the best possible labeling of the given test graph. Typically, when the training data size is small the content based classifiers are not very confident in their probabilistic estimates, which leaves space for the metric  X (  X  ,  X  ) to lead the GC by gently imposing constraints on the separations cost estimates.

We observe the same degree of influence for both SVM and NB as initial content-based classifiers. We show results only for SVM.

In Table 9 we show results for the DBLP data set. There is a significant positive influence of the label distance metric. The role of the metric is especially prominent in the results of the GC [ S ] variant which would be more susceptible to poor initialization from the content-based classifier. The influence of edge weighting is less significant but still helps improving the GC performance.

Table 10: Label similarity influence for Wikipedia. The results for Wikipedia dataset are shown in Table 10. We note the same tendency: the label distance metric  X (  X  ,  X  ) is a very powerful  X  X uide X  especially when the training data size is small.
The presented GC method for graph-based classification is a way of exploiting context relationships of data items. These kinds of models are not new, but our method intro-duces a variety of novel techniques that lead to a much im-proved level of robustness and classifier accuracy: similarity-based neighbor pruning and edge weighting to reduce neigh-borhood noise, class label similarity for further weight ad-justment, and the ability of GC to dynamically adjust the edge label probability estimates during the RL iterations. These are salient and unique properties of the GC algorithm. Our extensive experiments clearly provide strong evidence for the superiority of the GC method.

Some of our techniques could be integrated into prior methods, too, and would improve them. However, our ex-periments showed that the full combination of techniques that constitute our GC algorithm performed best. Our aim has been to develop a particularly robust method, and we believe that GC has achieved this goal to a large extent. In situations when neighborhood information is rich and strongly indicative (e.g., in the Wikipedia test case), GC exploits it to boost classification accuracy. In other situa-tions where the neighborhood information is too noisy to be useful (e.g., in the IMDB test case), GC has built-in throt-tling capabilities that automatically regulate the degree to which GC considers neighborhood information.

Incorporating metric distances among different labels con-tributed to the very good performance of the GC method. This is one new form of exploiting knowledge about the re-lationships among category labels and thus the structure of the classifier X  X  target space. We plan to further pursue this line of research, by investigating additional forms of such background knowledge.
