 Collaborative filtering systems based on ratings make it eas-ier for users to find content of interest on the Web and as such they constitute an area of much research. In this paper we first present a Bayesian latent variable model for rat-ing prediction that models ratings over each user X  X  latent interests and also each item X  X  latent topics. We describe a Gibbs sampling procedure that can be used to estimate its parameters and show by experiment that it is competitive with the gradient descent SVD methods commonly used in state-of-the-art systems. We then proceed to make an im-portant and novel extension to this model, enhancing it with user-dependent and item-dependant biases to significantly improve rating estimation.

We show by experiment on a large set of real ratings data that these models are able to outperform 3 common base-lines, including a very competitive and modern SVD-based model. Furthermore we illustrate other advantages of our approach beyond simply its ability to provide more accu-rate ratings and show that it is able to perform better on the common and important case where the user profile is short.
 H.3.3 [ Information Storage &amp; Retrieval ]: Information Search &amp; Retrieval Algorithms, Experimentation, Performance Recommender systems, Topic Models, User modelling
The sheer number of items available in online systems can be overwhelming for users and makes finding items of interest extremely challenging. Furthermore the rapid and continual expansion of the Web makes it impossible to man-ually evaluate each new item to determine if it might be of interest. Content filtering systems, based on techniques from information retrieval, are designed to assist in this pro-cess by narrowing down the number of items a user has to look through in order to fulfil a particular information need. These systems rely on textual descriptions of items and seek to match these descriptions with a user X  X  profile in order to suggest useful items. One significant issue with this content-based filtering is that for some types of items it can be extremely difficult to choose suitable descriptive terms to search for.

Another, more accurate, approach to discovering items of interest is provided by ratings-based collaborative filtering systems, which use past ratings to predict items the user may like. Such systems predict which items a given user will be interested in based on the information provided in their user profile. These profiles consist of votes or ratings for items in the system that the user has already viewed and evaluated. The profiles of other users are frequently also exploited to improve predictions for the target user.
Profiles are generally constructed explicitly from user rat-ings, however they may also be compiled implicitly by con-sidering a user X  X  purchase or bookmark history. Explicit rat-ings systems are commonly found on movie and music rec-ommendation sites such as MovieLens [14] or imdb where users can give each item a rating from 0 to 5 stars. Zero indicates that the user strongly dislikes the item and five indicates that they really like the item, however any dis-crete set of values could be used. Implicit systems can also be used, for example in online retail stores such as Amazon [12] where users purchase items or add them to a  X  X ish list X , indicating that they are interested in that kind of item.
Given the collection of ratings provided by users, the goal is to use the data to suggest additional items or resources that a particular user may like or be interested in. This pro-cess of machine recommendation is frequently called  X  X atings-based collaborative filtering X  or simply  X  X ollaborative filter-ing X  and the systems themselves are often referred to as  X  X ec-ommender systems X . Note that if a system is able to predict unknown ratings then the suggestion task can be achieved by simply ordering unrated items in descending order of their predicted rating. In doing so, such an algorithm is effectively achieving two outcomes: the  X  X orced prediction X  of whether or not a user will have a preference for an item and the  X  X ree prediction X  of the expected rating itself.

In this work we focus on the problem of recommending items to users based purely on the explicit ratings they have assigned to other items in the system. We introduce 2 novel (and related) Bayesian probabilistic models designed to ac-curately predict ratings given the very sparse training data commonly found on e-commerce and recommendation web sites. We go on to show by experiment with a large set of real ratings data that our models are able to outperform 3 common baselines, including a very competitive and modern SVD-based model. Furthermore we illustrate a number of other advantages of our approach, beyond simply its ability to provide more accurate ratings.

The rest of the paper is structured as follows: we first dis-cuss some relevant related work in the field of collaborative filtering and describe how the majority of modern methods approach the problem. In section 3 we describe the rating estimation problem in more detail and outline common mea-sures for algorithm performance. In section 4 we introduce a pair of novel Bayesian models designed to decompose the problem by use of latent factors and describe how these mod-els can be used to estimate unknown ratings. In sections 5 and 6 we describe experiments conducted with a large data set of 10 million movie ratings comparing the performance of these new models to 3 appropriate baselines and discuss the result of these experiments. Finally we offer our conclud-ing remarks and make suggestions of possible future work building upon the results of this paper.
Collaborative filtering systems can be placed in the con-text of information retrieval by considering that in a retrieval system items are  X  X ulled X  by users through the issuing of ex-plicit search queries. Filtering systems on the other hand are described as  X  X ush X  systems since they quite literally push items at a user that is predicted he/she will like. Much early work was done in the 90s and the field has seen a resurgence of interest lately, primarily due to the Netflix prize [11]. Col-laborative filtering algorithms can be generally classified into 2 distinct types: memory-based and model-based.
Early systems were memory-based and made use of the original ratings matrix to formulate predictions. Such sys-tems follow a relatively simple 2 step process: First they identify a neighbourhood of users similar to the target user and then use an aggregate weighted summation of the neigh-bours X  ratings for an item in order to predict the rating for the target user. These algorithms form the basis of most fil-tering currently performed on the Web including sites such as Amazon and CDNow and were the cornerstone of much early research in the field [7, 4]. We refer to [1] for a com-prehensive description of how these methods operate. It has been speculated that their popularity is due to their relative simplicity and their inherently intuitive nature [10].
Unfortunately these simple, memory-based algorithms suf-fer from 2 major shortcomings. Firstly the number of items rated by most users is oftentimes small and therefore it can be difficult to choose a good neighbourhood of similar users. Once a neighbourhood is chosen only a very small number of similar users may have rated the item for which we wish to predict a rating. Secondly, these algorithms require all of the ratings data to be utilised when making a prediction and, although it is possible to maintain a cache of user sim-ilarities, this will have to be updated whenever new ratings are made. This shortcoming is perhaps the most problem-atic as without clever implementation, it severely restricts the potential growth of the recommendation system. This is because, over time the amount of ratings data becomes too large to allow for the efficient computation of both the neighbourhoods and the predictions themselves.
These issues led to the development of the second general type of collaborative filtering algorithm: the model-based approach. In this approach we use the observed ratings to construct a model of the data and we decompose the ob-served ratings into a sum of biases; one for the user, one for the item and a third joint bias. Modern examples of these models frequently use some form of dimensionality reduc-tion to uncover latent factors and to calculate the joint bias. These latent factors are constructed in a manner that best explains the training ratings and if we make the assumption that any further ratings will be drawn IID 1 from the same distribution then the model should be able to predict new ratings well.

These model-based algorithms are able to overcome many of the scalability problems associated with the earlier memory-based systems. This is particularly the case when real-time recommendations are required, which is obviously the most likely situation given the on-line nature of the systems where collaborative filtering is most often used. The most time-consuming task is the generation of the model itself, after which the task of new predictions is extremely quick due to the significant reduction in dimensionality afforded by the latent factors. With model-based systems the entire mod-eling operation can be completed off-line thus allowing for near-instantaneous real-time predictions as and when users need them.

Many examples of this approach, including most attempts at the Netflix prize, use gradient descent algorithms to esti-mate an approximation of the Singular Value Decomposition (SVD) of the original sparse ratings matrix [15]. The val-ues computed for the SVD matrices are regularised so as to prevent over-fitting and individual biases for each user and movie are commonly added to improve prediction perfor-mance. Other techniques use probability theory to construct the models where observed ratings are assumed to arise from some latent variables which have to be estimated. In [13], Marlin represents each user as a mixture of  X  X ttitudes X  with each rating being generated by selecting one of these atti-tudes and then selecting the rating based on the ratings dis-tribution for that attitude. Hofmann [10] extends his earlier pLSI model to model ratings by again assuming that users have a distribution over  X  X nterests X  or  X  X ttitudes X  and that each rating is associated with a single interest drawn from the user X  X  interest distribution. His work differs from that of Marlin [13] however by then assuming that there is a rating distribution for each latent interest and item pair. So the observed rating is assumed conditional on both the latent
IID stands for independently and identically distributed . interest of the user who rated the item and also on the item itself.

Other probabilistic approaches include [19] in which the authors introduce a novel adaptation of the EM algorithm to learn the parameters of a prediction model for person-alised content-based prediction. Stern et. al. [17] instead use Expectation Propagation and Variational Message Pass-ing to learn a model using both ratings data and content. In other recent work Chen et al. [5] compare the performance of Latent Dirichlet Allocation (LDA) [2] with association rule mining (ARM) for the purpose of community recommen-dation. This is a similar problem to rating prediction but instead involves the suggestion of online communities of in-terest rather than items. They show that LDA consistently outperforms ARM for this task, particularly when consider-ing later recommendation. They also demonstrate that it is less likely to make extreme errors due to its Bayesian nature, certainly a useful property when recommending items.
In this paper we discuss 2 probabilistic model-based col-laborative filtering algorithms that can be seen as compara-ble to these models and draw on similar background theory. Blei [2] in fact uses collaborative filtering as an example of a problem for which LDA could be used and shows that it is able to outperform both probabilistic LSA [9] and a sim-ple unigram model. Our models do not use LDA itself in order to predict ratings but use its ability to extrude latent factors from sparse data as a strong basis from which to build models more suited to the task at hand. Also in this work we extend the basic ratings model by including indi-vidual biases for each user and item requiring an iterative fixed-point optimisation procedure to be interleaved with the Gibbs sampler and providing significant improvements in performance.
Before proceeding any further we will formally define the problem of rating prediction and describe the format of the data we have to work with. We have a set of users of the system U = { u 1 ,..., u U } , a set of items M = { m 1 ,..., m and a discrete set of possible ratings { 1 ,..., R } . Our training data is a collection of ratings given by the users to items. We can view each individual rating as a tuple: ( u i ,m i for i := 1 ...N train . For example the tuple ( u i = 1 ,m 1 ,r i = 4) would indicate that user 1 had given item 1 a rating of 4. It can sometimes be convenient to visualise the complete collection of user ratings as a large and very sparse matrix R of size U  X  M where r um would indicate the rating given by user u to item m . The prediction problem is best described by saying that we would like to  X  X ill in X  the sparse ratings matrix, extrapolating (or predicting) a rating  X  r every possible user-item pair from the limited data available. More practically we wish to define some function or model which will minimise the root mean squared prediction error over the test data: The RMSE is commonly used in statistics for measuring the difference between the set of values predicted by a model and the values actually observed from the system being mod-elled. It is a good measure of precision and is an unbiased estimator of the standard deviation of the predictions and Figure 1: Latent Interest and Topic Ratings Model (LITR) where each observed change in the rating from the mean is due to both a user interest and an item topic or genre and Biased-LITR (BLITR) where it is also dependent on the bias for the user and the item. is therefore a good choice of error function for collaborative filtering. We also report the Mean Absolute Error (MAE) which is simply the mean absolute difference between the predicted rating and the actual rating, over the whole test set.
In order to implement a new latent variable model for col-laborative filtering it is necessary to choose how best to rep-resent the latent factors and how to incorporate the ratings data, both in terms of statistical distributions. A popular modern latent variable model called Latent Dirichlet Allo-cation is commonly used to estimate latent variable distri-butions and serves as a good starting point for a new model. LDA is predominantly used to model textual data where it represents documents as random mixtures over latent topics where each topic is a distribution over the observed words in the vocabulary. Each word position i in the document col-lection is assigned a latent random variable z i which ranges over the latent topics.

The model possesses a number of advantageous attributes; it is fully generative meaning that it is easy to make infer-ences on new documents and its Bayesian nature helps to overcome the over-fitting problem present in models such as Probabilistic Latent Semantic Indexing (pLSI) [9]. Also since in LDA each document is a mixture of topics it is far more flexible than models that assume each document is only drawn from a single topic. These advantages also extend to models based on LDA with both the added flexibility and ability to easily incorporate new data being very important for our own models, as we shall discuss later. Many useful models have used LDA as a base to work from, for example the Author-Topic model [18] which models academic pub-lications and citations. It has even been used to develop models to annotate images based on their visual features [3].

As we can see from these publications, LDA can serve as a very flexible starting point for more complex models even when those models are not just using discrete data. In the case of collaborative filtering we do not have documents and terms, but rather a collection of observed ratings from a set of users over a set of items. We must therefore significantly adapt the model to fit the problem. Since we are primarily interested in predicting ratings with the smallest possible er-ror in aggregate it is sensible to consider models (and there-fore distributions) that are continuous in nature. In doing so the predictions will not be constrained to be bound to the finite discrete values of the original ratings but will have the freedom to model the complex interactions of biases in the data at higher granularity.
Perhaps the simplest possible prediction algorithm one could imagine would be to assign the mean rating over the training data, denoted  X  (where  X   X  = 1 N P i r i ), as a pre-diction to each item for every user, (i.e.  X  r um overly simplistic model corresponds to a generative process in which each rating r i is considered a normally distributed random variable with mean  X  and standard deviation  X  : This model makes a large number of assumptions and ig-nores a lot of the complexity in the data. It assumes that ratings are completely independent of both the item and user and that there are no interactions between the combi-nations of user and item that would affect the rating. We relax some of these assumptions and extend this model by following similar conclusions to both Hofmann [10] and Mar-lin [13]. That is that the change in the rating is dependent on the user and that each user can be characterised by a distribution over a small number of latent interests.
In addition, (and in contrast to previous work) we then as-sume that the change in rating is also equally dependent on the items, which themselves can be characterised by a distri-bution over a small number of latent topics. For example in the case of movies this may be more intuitively thought of as their latent genres or for general items in a web store it could be the category/ies to which they could be assigned. This conjecture leads to a more useful generative model for per-sonalised item filtering and ranking involving three random variables: a user interest z i , an item topic (or movie genre) y and a rating r i , where only the last variable, the rating itself, is observed. Following previous work on topic models, we then assume the user-interest and item-topic variables are drawn from Multinomial distributions. 2 The same as-sumption could also be made regarding the ratings, as the original ratings assigned by users are indeed drawn from a discrete set. However as noted earlier, the flexibility of the model can be increased as can the granularity of its predic-tions, by instead modelling them as being drawn from Nor-mal distributions. These assumptions can be summarised as follows:
Thus the model consists of a discrete probability distribu-tion over interests for each user  X  u , a discrete distribution over topics for each item  X  m , a mean rating  X  , a bias value b yz for every pair of interests and topics, and a standard deviation parameter  X  . A graphical model corresponding to this generative process is shown at the top of Figure 1.
The intuition for introducing the bias b yz in this model is that we believe each interest and topic combination will have an effect on how the item is rated. For example in the case of movies we might expect that a user who likes romance would give a horror movie a lower than average rating, meaning that the bias b yz for this interest-topic pair would be negative. Similarly if the same user was to rate a romance movie then we would expect them to give a higher
The interest and topic variables are actually drawn from discrete distributions, but we use the term Multinomial in keeping with the literature. than average rating and the bias would therefore be positive. Since these biases are calculated over the low-dimensionality latent spaces they will not be sparse and should allow the model to generalise well to unseen user-item combinations, a key objective of any collaborative filtering model.
Given suitable estimates for these parameters, we can pre-dict the rating for a user u and item m by calculating the expected value as follows: Here  X  z | u and  X  y | m denote probability of an interest given a user and a topic given an item respectively. This model is quite intuitive. It states that the rating given by a user to an item will be the product of a user X  X  affinity for an interest, the item X  X  probability of belonging to a topic and the average bias for that interest-topic combination, summed over all possible combinations of interests and topics.

In this model we have associated a possibly non-zero bias with every pair of interest topic dimensions. Thus not only is a positive bias associated with  X  X orresponding X  X  interests and topics (e.g. the user-interest  X  X orror X  and the movie-genre  X  X orror X ) but also a possibly negative bias with  X  X on-corresponding X  X  interests and topics. For instance if a user X  X  primary interest is  X  X orror X , they may still have a positive bias towards a  X  X hriller X  while having a negative bias against a  X  X omedy X . In SVD terms this is to some extent equivalent to replacing the diagonal singular-value matrix with a ma-trix containing non-zero off-diagonal values. These values then allow us to model both positive and negative correla-tions across different factors. By defining the predictions in terms of a generative model, we can interpret and explain the parameters of the model in a way that is not possible with SVD-based prediction algorithms. Given vectors of latent variable assignments z = ( z 1 ,...,z and y = ( y 1 ,...,y N ), we can compute estimates of both the probability of an interest given a user  X  z | u and a topic given an item  X  y | m . Following principles from LDA, and in keeping with Bayesian statistics, we place symmetric Dirichlet pri-ors on both of these distributions, resulting in the following expectations for the parameter values under their respective posterior distributions: Here N zu , N ym , N u and N u are counts denoting the number of times the interest z appears (in z ) together with user u , the number of times topic y appears (in y ) with item m , and the total ratings by user u and for item m respectively. Z is the number of interests and Y is the number of topics. The hyperparameters  X  and  X  act as pseudo-counts, allowing the model to fall back on the (uniform) prior probability in the event of sparse data, which is particularly useful in this setting where sparse data is common.

In addition to estimating the distributions over interests and topics we need to estimate the bias for each interest and topic pair denoted b yz . Given that we have a complete set of assignments for these latent variables for each observed r we can calculate an estimate of this bias as follows: Here N yz denotes the number of ratings for which y and z appear together and  X  is a smoothing parameter. It is related to the variance of the zero mean Gaussian prior on b yz , which keeps the model Bayesian and helps to deal with sparsity in the data. 3 We note that it would also be possible to estimate a variance parameter separately for each ( y,z ) pair, but we instead make the simplifying assumption that all biases have the same fixed variance.
 While exact inference of latent variable models such as LITR is intractable, a number of methods of approximating the posterior distribution exist including mean field varia-tional inference [2] and Gibbs sampling [8]. Gibbs sampling is a Markov chain Monte Carlo method where a Markov chain is constructed that converges to the target distribution of interest over a number of iterations. In Gibbs sampling the next state in the chain is reached by sampling all vari-ables from their conditional distribution given the current values of all other variables [6].

Gibbs sampling for this model involves sampling first z i and then y i for each rating r i . To sample for z tioned on the current assignment to all interest variables except z i . Similarly for y i we sample from the distribution P ( y | r i ,z i ,m i , X , X , y  X  i ). Note that our estimates for the pa-rameters  X  z | u ,  X  y | m and b yz depend on the interest and topic assignments z and y , so when calculating estimates using Equations 8, 9 and 10, we simply remove the i th rating from the sample. The conditional probability distributions are then estimated as follows: Here p ( r | y,z ) denotes the conditional probability density at rating r for the interest y and topic z . Since the algorithm only require estimates proportional to the true probabilities the normalising factor of the Normal distribution is not re-quired. Therefore the first parts of Equations 11 and 12 are the unnormalised probabilities of a Normal distribution. This model provides a method of predicting ratings by con-sidering perturbations from the mean rating over a number of latent interests and topics.

After sufficient iterations of the sampler, the Markov chain converges and the parameters of the model can then be esti-mated from the assignments z and y via equations 8 and 9. For increased accuracy, we average parameter estimates over consecutive samples from the Markov chain. Gibbs sampling is a preferable alternative to methods such as Expectation
The value  X  is equal to the ratio of the variances of the likelihood and the prior (both Normal distributions). Maximisation as it is able to sample from the entire pos-terior distribution and is therefore unlikely to get  X  X tuck X  in local maxima and does not require the use of additional machinery such as simulated annealing to get around this problem [16]. Another benefit of this technique is the ability to quickly  X  X old-in X  new data into the model, this is partic-ularly useful for this task as new ratings, items and users are likely to appear quite frequently. To include this new data into the model we can simply run the Gibbs sampler over the new data, holding all of the pre-existing interest and topic allocations fixed. After the sampler has converged on this new data (which usually occurs within less than 50 iterations) we can simply recalculate parameter estimates in equations 8 and 9.
As noted in section 2, the most successful models compet-ing in the Netflix prize also estimate a bias for each user and a bias for each item as well as the bias due to the user and the item together. This is a sensible assumption as some users may naturally rate items higher than others and some may naturally choose from a lower baseline score. Simi-larly some items are intrinsically better than others and are therefore likely to be rated higher by all users, while items of a lesser quality will be given a lower than average score by most users. While we would expect that these biases would be at least partially accounted for by the joint biases over the reduced genre and interest spaces it is likely that users and movies that give/have unusually high or low rat-ings (outliers) would affect the accuracy of the biases for other users. By calculating a separate bias for each user and item separately we effectively remove these eccentrici-ties from the ratings, giving the joint biases the freedom to deal purely with the variations caused by observing the var-ious interest/genre pairs. The Biased-LITR model (BLITR) is therefore an extension of the model described previously to also include these biases in order to improve prediction accuracy. The graphical representation for this model is on the bottom in Figure 1.

The generative model is the same as the previous case, except that the mean of the Gaussian distribution that gen-erates the rating r i takes into account the user and item biases b u i and b m i as follows:
Given estimates for the parameters of this more compli-cated model, the predicted rating for a user u and an item m is now:
Note that the prediction under this new model and the previous model can be viewed as perturbing the mean  X  by a combination of biases. Both models add a bias for the likely interests and topics given the user and item pair, while the second model adds also explicit biases for the user and for the item.

Estimates for the parameters  X  z | u and  X  y | m are the same as in the previous model, while the estimate for b now include the effects of the extra biases as follows:
Furthermore we must also compute estimates for the new user and item-dependent biases. The most obvious way to compute these biases is to take the mean difference of all ratings for a given user/item from the mean rating for all users/items. However since we are also computing an im-plied bias (denoted b um ) for each user-item pair we need to include the effects of this bias in the estimations. We therefore estimate the user and item biases as follows:
We note that the Equations 15 and 16 are mutually depen-dent and thus an iterative fixed-point calculation is required to estimate the biases. Holding the joint b um biases fixed this procedure converges very quickly and stabilises within less than ten iterations. Finally the distributions used for the Gibbs sampling routine must also be updated to include the new biases:
Since the user and item biases are not strongly dependent on the allocations of ratings to y and z we can simply esti-mate them after every k th iteration of the Gibbs sampler and the algorithm will still converge. Not only does this speed up computation of the model but it also gives the sampler time to re-converge after changes to the user and item bi-ases. In all the experiments performed we re-calculate these biases after every 10 Gibbs iterations.
We now discuss the experiments we performed on a large sample of ranking data from the MovieLens 4 movie rating web site, this data is freely available from the GroupLens website. 5 The data consists of 10 million ratings for 10,681 movies made by 71,567 users. The users are selected at random and have all rated at least 10 movies. Consequently the average number of ratings per user is 140 and per movie is 936. The ratings are all given on a scale of 0 to 5 stars with increments of 0.5 stars. The mean rating over all users and movies is 3.53 and the variance is 0.96.

We separated this data set into training and test sets by randomly choosing k percent of the ratings to be kept for testing and used the remaining ratings to train the models. For our experiments we set k to be 20%. Therefore the results reported in the remainder of the paper are based on http://www.movielens.org/ http://grouplens.org/node/73 predictions over all of the test data, amounting to almost 2 million individual predictions.

To evaluate the relative performance of the various mod-els we report both the RMSE, as described previously, and also the Mean Absolute Error (MAE). This is simply the mean absolute difference between the predicted rating and the actual rating over all testing examples. We report both metrics as they provide different information regarding the performance of predictions: the RMSE penalises large errors much more than small errors while the MAE penalises all errors equally relative to their size.
In order to evaluate the utility of our new models we must choose suitable baseline methods with which to compare their performance. For this work we compare our methods to 3 baselines from CF literature: mean-r a na  X   X ve, simple baseline which returns the mean neighbourhood a nearest-neighbour method using Pear-SVD a fully-regularised gradient descent SVD model with
For the LITR and BLITR models we set the concentra-tion parameters of both  X  and  X  to 5, providing some light smoothing to the user-interest and item-topic distributions. The settings for  X  and  X  were 0.5 and 0.1 respectively. The models were not particularly sensitive to parameter values, provided excessively low or high values were not chosen, i.e. applying almost no smoothing, or in the other extreme smoothing out the information from the data completely.
For the SVD method we optimised the parameter val-ues based on performance over a small sub-sample of the test set. The values obtained in doing this are very similar to the standard best performing parameters values as de-scribed in the literature[15]. Specifically the learning rate was set to 0.002 and the 2 regularisation constants  X  and  X  2 were set to 0.02 and 0.05. For the gradient descent algo-rithm, prediction errors on a sub-sample of the test set were observed to stabilise after approximately 30 iterations, how-ever to ensure convergence we continued until 50 iterations had elapsed. For the neighbourhood method the number of neighbours used for the estimates was set to 100.
 For sampling in the LITR models we use the Rao-Blackwellised Gibbs sampler [8]. For both models we sampled the chain for 300 iterations in total, as this appeared to consistently give good convergence in terms of model likelihood. We dis-carded the first 200 samples from the chain as  X  X urn-in X . The remaining 100 samples from the end of the chain were averaged to obtain the final parameter values.
The results from our experiments are summarised in ta-ble 2. We can see, somewhat unsurprisingly, that all of the methods significantly outperform the most simple choice of estimate: the mean over all ratings. The nearest-neighbour Table 2: Comparison of best results from each model. For latent factor/variable models the num-ber of latent variables is set to 50. Percentages in-dicate improvement over baseline.

Model MAE RMSE MAE RMSE  X  rating 0.8516 1.0521 --n X  X ood 0.6582 0.8481 22.7% 19.4% SVD 0.6516 0.8401 23.5% 20.1% LITR 0.6496 0.8384 23.7% 20.3%
BLITR 0.6334 0.8236 25.6% 21.7% Figure 2: RMSE over different numbers of latent topics/factors. method performs well, however the more modern model-based approaches are all able to outperform it by a large margin. In testing we encountered one of the main disad-vantages of memory-based approaches as prediction using the neighbourhood model took orders of magnitude longer than the model-based approaches.
 In comparing the model-based approaches, we see that LITR, which does not include individual biases for each user and item, is still able to outperform the SVD method, how-ever not by a significant margin. The more complex BLITR model on the other hand, which is able to leverage predic-tive power from the user and item biases as well as from the latent variable mixture of Gaussians, is able to outperform all of the other methods over both reported metrics by a sta-tistically significant margin. In terms of MAE the BLITR model outperforms SVD by 2.7% and by 2% in terms of RMSE (paired t-test, 99% confidence, p-value = 4 . 5  X  10 and 1 . 2  X  10  X  05 ). Furthermore it improves upon the nearest neighbour approach by 3.8% for the MAE metric and 2.9% for the RMSE metric.

These results demonstrate the appropriateness of the LITR models to the problem of rating prediction and indicate that the generative models are able to make predictions with less error. Considering that the SVD model includes individual item and user biases it is notable that the LITR model with-out these additional biases is able to remain competitive. By incorporating these biases in the BLITR model we are able to significantly outperform SVD, however. Thus it is clear that the inclusion of these biases into the generative process is important if we are to achieve optimal prediction results.
We now look at how performance of the models vary as we increase the number of topics. By referring to the chart in Figure 2 we can see that all of the model-based approaches increase in performance as we increase the number of topics or latent factors. Notice that when the number of latent topics is set to 5 only BLITR is able to clearly outperform the memory-based nearest neighbour model, however as the number of factors is increased all of the model-based ap-proaches begin to outperform it.

Initially the performance of the LITR model appears to be quite poor in comparison to the other latent variable mod-els. This is because when the number of latent variables is small both SVD and BLITR can rely on user and item bi-ases to explain the differences in ratings and thus improve their predictions while LITR cannot. As the number of fac-tors increases we see that LITR is able to approach and then eventually exceed the performance of SVD, however it is still unable to come close to the performance of BLITR. The performance of all of the models appears to have reached a plateau by around 40 factors with any further improvements after this point being quite small. Table 3: Comparison of results over different user profile sizes.
 #ratings/user  X  20  X  50  X  100 all SVD 0.9115 0.8840 0.8692 0.8401
BLITR 0.8536 0.8435 0.8379 0.8236 #users 9,404 33,965 50,297 71,567 Figure 3: Prediction error and user count for vary-ing profile sizes
As discussed earlier, an important consideration for any collaborative filtering algorithm is how well it does in the most difficult cases. This is generally users (or items) with a very small number of training ratings, for example the case of a new user or item. Analysing the data set we find that 13.4% of all users have 20 or fewer ratings and nearly half (47.5%) have 50 or fewer. We expect that these will be the users for whom the algorithms struggle the most to make accurate predictions for.

Table 3 shows how the performance of the best performing baseline and the best of the new models (SVD and BLITR) vary over different user profiles sizes. The results show that the BLITR model performs much better for smaller pro-files (relative to its performance over all users) than SVD. The SVD model X  X  performance decreases by 8.4% when deal-ing with small user profiles (20 or fewer ratings) whereas BLITR X  X  performance only sees a very small decrease of 3.6%.

This result is perhaps illustrated more clearly in Figure 3, which shows the mean error over varying user profile sizes, for all users with a profile size smaller than or equal to the value on the x-axis. This is plotted for both SVD (dotted red line) and BLITR (dashed blue lines). The figure also shows on the right-hand y-axis the density of user counts over profile sizes (solid black line). We can see clearly that a large proportion of the users have a small number of ratings with very, very few having a large number. The maximum number of ratings for any user is 2876, 97% of users have fewer than 500 ratings and the minimum is 10 (this lower limit is imposed by the MovieLens web site). We can see from this plot that SVD X  X  error for users with small profiles is quite high and that it fairly rapidly decreases as the profile size increases. On the other hand LITR has much smaller error for users with small profiles and is able to produce much smaller errors than SVD over the whole range of profile sizes.

This is an important outcome as it demonstrates that our models are able to perform much better when data is par-ticularly sparse which is the most common case and are the situations for which we are most interested in improving performance. We speculate that this is a consequence of the Bayesian nature of the models; allowing them to cope better when there is little data available to base predictions on. It may also be because the LITR models are better at leverag-ing the limited information obtained from the small number of ratings that are available in these cases.
The main focus of rating prediction is of course to make predictions with minimal error, however of course there will always be some error and it is not possible to always make perfect predictions. This being the case, a secondary focus is to try to ensure that when errors are made they are not too large as this can frustrate and confuse users and even a single instance of poor prediction can cause a user to lose faith in the system X  X  abilities. Figure 4 shows a density plot of mean errors for users over the testing data made by both SVD and the BLITR model. The plot shows that the errors made by SVD have larger variance (0.065 versus 0.045 for BLITR) and also have a much thicker tail at the higher end of the errors. This means that not only are the predictions made by BLITR better in the expectation but they are also less likely to be extreme and as a result are less likely to frustrate users. Figure 4: Density plot of mean error over all users
In this work we have presented two Bayesian models for collaborative rating prediction based on clustering users and items into latent interests and latent topics simultaneously. We first introduced the concept of collaborative filtering and briefly discussed related work, including both memory and model-based methods. We motivated our work by outlining the benefits of model-based collaborative filtering systems and of the Bayesian approach to data modelling. The first model predicts a user-item rating by perturbing the mean rating across items and users by the weighted summation of interest and topic biases, where the weights are the proba-bility of the interest-topic pair given the particular user and item. The second model is an important and novel exten-sion of the first, including user and item specific biases in the prediction.

We describe an estimation procedure for this model which alternates between Gibbs sampling of the latent variables and an iterative fixed-point estimation of the additional bi-ases. We explain reasons why Gibbs sampling is both suit-able and necessary for this task, in particular that it allows new data to be easily, quickly and accurately folded into the existing model. We motivate the use of latent topics in the model due to their flexibility and the ease of interpreta-tion of results that they permit. We concede that the ideas of performing item recommendation using Bayesian proba-bilistic models and the inclusion of user and item biases are not in themselves novel. However the combination of these techniques via the interleaving of Gibbs sampling and fixed-point optimisation and the use of multinomial distributions for latent vectors for users and items and the b yz presents an important new contribution.

By experiment over a large, freely-available and commonly used data set of real item ratings we have shown that the models are extremely competitive, with the extended model significantly outperforming the most competitive baselines. Furthermore we investigated how well the strongest base-line and our best model performed in cases where the user profiles were very short (where the user had rated very few items). We found that our model is able to cope far better than the SVD baseline in these cases which is an important result as such cases are both common and an area where improvement in performance is most useful. Lastly, analy-sis of the residual errors showed that BLITR X  X  errors had much smaller variance than those of SVD and as such is much less likely to generate extremely erroneous predictions which could frustrate and confuse users.

This work opens up many potential opportunities for fu-ture research by taking advantage of the modular structure of the models to provide a framework for various extensions. Here we make a number of suggestions for future work: [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [3] D. M. Blei and M. I. Jordan. Modeling annotated [4] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [5] Wen Y. Chen, Jon C. Chu, Junyi Luan, Hongjie Bai, [6] Andrew Gelman, John B. Carlin, Hal S. Stern, and [7] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry. [8] T. Griffiths and M. Steyvers. Finding scientific topics. [9] T. Hofmann. Unsupervised learning by probabilistic [10] T. Hofmann. Latent semantic models for collaborative [11] Yehuda Koren. Factorization meets the neighborhood: [12] G. Linden, B. Smith, and J. York. Amazon.com [13] B. Marlin. Modeling user rating profiles for [14] B. N. Miller, I. Albert, S. K. Lam, J. A. Konstan, and [15] A. Paterek. Improving regularized singular value [16] A. F. M. Smith and G. O. Roberts. Bayesian [17] David H. Stern, Ralf Herbrich, and Thore Graepel. [18] Mark Steyvers, Padhraic Smyth, Michal R. Zvi, and [19] Yi Zhang and Jonathan Koren. Efficient bayesian
