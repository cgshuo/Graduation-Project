 Traditional feature selection methods assume that the data are independent and identically distributed (i.i.d.). How-ever, in real world, there are tremendous amount of data which are distributing in a network. Existing features selec-tion methods are not suited for networked data because the i.i.d. assumption no longer holds. This motivates us to study feature selection in a network. In this paper, we present a su-pervised feature selection method based on Laplacian Reg-ularized Least Squares (LapRLS) for networked data. In detail, we use linear regression to utilize the content infor-mation, and adopt graph regularization to consider the link information. The proposed feature selection method aims at selecting a subset of features such that the empirical er-ror of LapRLS is minimized. The resultant optimization problem is a mixed integer programming, which is difficult to solve. It is relaxed into a L 2 , 1 -norm constrained LapRLS problem and solved by accelerated proximal gradient descent algorithm. Experiments on benchmark networked data sets show that the proposed feature selection method outper-forms traditional feature selection method and the state of the art learning in network approaches.
 I.2.6 [ Arti cial Intelligence ]: Learning; I.5.1 [ Pattern Recognition ]: Models Algorithms, Experimentation Feature Selection, Network, Graph Regularization, Lapla-cian Regularized Least Squares
In many application of data mining, one is often con-fronted with very high dimensional data. It significantly increases the time and space complexity for processing the data. Moreover, in the presence of many irrelevant and/or redundant features, learning methods tend to over-fit and become less interpretable. One way to resolve this problem is feature selection [23] [8], which reduces the dimensionality by selecting a subset of features from the input feature set. Figure 1: Sample data of WebKB (Cornell) dataset: (a) Content information (page-term co-occurrence matrix); (b) Link information (adjacency matrix). Note that the adjacency matrix is not symmetric because the link has direction.

However, traditional feature selection methods [8] assume that the data are sampled i.i.d. from some unknown distri-bution. While in real world, there are tremendous amount of data which are distributing in a network. For example, in the classification of web pages, there are not only the con-tent information within web pages, but also the hyper-links between the web pages. See Figure 1 for example. Figure 1 (a) shows the page-term matrix where the blue dot repre-sents the occurrence of a term in a page. Figure 1 (b) shows the adjacency matrix of the pages, where the blue dot rep-resents the link between pages. It is often the case that if there is a link from one page to another, then it is likely that these two pages are related. Another example is research pa-per cla ssification. Besides the content information of each paper, a citation relation between two papers provides an evidence that they belong to the same topic. That is, if one paper cite another paper, or is cited by another one, then there is a high chance that the papers are belonging to the same topic. It can be seen that content information and link information complement each other. Hence, if one can utilize both the content information and link information in the network, one should achieve better classification result of the networked data than either using the content infor-mation or link information alone. In the past decade, there are quite a lot of works on learning in network along several directions. For example, combining content and link infor-mation for classification [6] [26] [12], learning based on graph regularization [2], collective classification of networked data [18], active learning for networked data [3] [19] and subspace learning in network [13]. However, feature selection for net-worked data is rarely touched. Traditional feature selection methods are not suited for networked data because the i.i.d. assumption no longer holds.

In this paper, based on the above motivation, we present a supervised feature selection method for networked data. It is built upon Laplacian Regularized Least Squares (LapRLS) [2]. The key idea is to use the least squares regression to fit the labels with respect to the content information, and adopt graph regularization [20] [25] to utilize the link infor-mation. We study graph regularization on both undirected graph and directed graph. The basic assumption of graph regularization is that if two nodes are linked in a network (or one node links another node), then their labels are likely to be the same. The proposed feature selection method aims at selecting a subset of features such that the empirical er-ror of LapRLS is minimized. The resultant optimization problem is a mixed integer programming [4], which is dif-ficult to solve. It is relaxed into a L 2 , 1 -norm constrained LapRLS problem and solved by accelerated proximal gradi-ent descent algorithm [15]. It is worth noting that we do not need to specify the number of features to select for the pro-posed feature selection method. It is implicitly controlled by a regularization parameter. Experiments on benchmark data sets indicate that the proposed method outperforms traditional feature selection method and the state of the art learning in network approaches.
 The remainder of this paper is organized as follows. In Section 2, we review traditional feature selection methods and learning in network approaches. We present the feature selection method in Section 3. The experiments on bench-mark data sets are demonstrated in Section 4. Finally, we draw a conclusion and point out the future work in Section 5.
The generic problem of supervised feature selection in net-work is as follows. Given a networked data set { ( x i ; y where x i  X  R d is the feature vector of the i -th node, and y  X  { 1 ; 2 ; : : : ; c } is the label of the i -th node, A is the ad-jacency matrix of the networked data, such that A ij = 1 if there is a link from i -th node to the j -th node, and A ij otherwise. For undirected graph, we have A ij = A ji . In other words, the adjacency matrix A is symmetric. For di-rected graph, the adjacency matrix A is asymmetric. The goal of feature selection is to find a feature subset of size m which contains the most informative features. We use X = [ x 1 ; x 2 ; : : : ; x n ]  X  R d n to represent the data matrix, and Y  X  R n c to represent the target label matrix, where Y ik = 1 if y i = k , and Y ik = 0 otherwise. Given a matrix W  X  R d m , we denote the i -th row of W by w i , and the j -th column of W by w j . The Frobenius norm of W is defined as || W || F = ||
W || 2 , 0 = card( || w 1 || 2 ; : : : ; || w d || 2 ), and the L W is defined as || W || 2 , 1 = ones with an appropriate length. 0 is a vector of all zeros. I is an identity matrix with an appropriate size.
In this section, we give a brief review of traditional feature selection methods and existing learning methods in network respectively.
In feature selection [23], the features may be scored either dependent or independent on a classifier. In general, there are three families of approaches to score them: filter-based, wrapper-based, and embedded methods [8]. Filter-based methods score the features as a pre-processing step, indepen-dently of the classifier. The most representative filter-based methods include Information Gain (IG), 2 [23], Fisher score [17] [7] and so on. Wrapper-based methods score the features according to their prediction performance when used with the classifier. Finally, embedded methods combine feature selection with the classifier. While the design of embed-ded methods is tightly coupled with the specific classifier, they are often considered as more effective than filters and wrappers [8]. It is worth noting that the proposed feature selection method for networked data belongs to the family of embedded method, because it is specifically designed for Laplacian Regularized Least Squares (LapRLS) [2]. Fea-ture selection methods rely on search strategies to guide the search for the  X  X est X  feature subset. While a large number of search strategies can be used, one is often limited to the greedy (forward or backward) strategies. The search for the  X  X est X  X eatures in the proposed method is in a principled way rather than greedy.
Learning in network has received increasing interest in the past decade. [6] proposed a missing-link model, which gen-eralize probabilistic latent semantic analysis (PLSA) [10] to consider both content and link information. [26] proposed link-content matrix factorization (LCMF) method, which in-tegrates content and link information into a matrix factor-ization framework. [12] proposed relation regularized ma-trix factorization (RRMF), which overcomes some limita-tions of LCMF. However, all the methods mentioned above are transductive learning methods [21]. Transductive learn-ing methods work on the training and testing set together. They do not output a parametric classifier, hence they can-not generalize to new unseen data. When new data come, they need to re-learn based on the training and the test-ing data. This motivates inductive learning, which induces a parametric decision function in the whole sample space. [2] proposed Laplacian Regularized Least Squares (LapRLS) based on graph regularization. Although it is originally pro-posed for semi-supervised learning, it can be adapted to learning in network and works very well. [18] studied collec-tive cl assification of networked data. [13] proposed prob-abilistic relation principle component analysis (PRPCA), which is the sate of the art subspace learning method in network. More recently, [3] [19] suggested active learning for networked data. On the other hand, there are some works such as modularity of network [16], which study how to measure the partition of a network. However, as far as we know, feature selection for networked data is still a rarely touched topic. This motivates the method presented in this paper.
In this section, we will present a feature selection method for networked data. In detail, we first introduce Graph regu-larization and Laplacian Regularized Least Squares (LapRLS). Then we present the feature selection method in network, followed with its optimization algorithm and theoretical anal-ysis.
Link information in network characterizes the structure of the network and relation between the nodes. In order to consider the link information, we turn to use Graph Regular-ization [20] [25], which is based on spectral graph theory [5]. Graph regularization has achieved great success in dimen-sionality reduction [1] [9] [22] and semi-supervised learning [27] [24] [2]. In the following, we will introduce graph reg-ularization for both undirected graph and directed graph respectively.
For undirected network, the basic assumption of undi-rected graph regularization is: if two nodes are linked to-gether, then their labels are likely to be the same. It can be mathematically formulated as [20] wh ere L = D  X  A is the graph Laplacian [5] and D is a diag-onal matrix called degree matrix with D ii = that in undirected graph regularization, A is symmetric. For directed graph, the undirected graph regularization in Eq.(1) does not work anymore since the adjacency matrix A is asymmetric. To revolve this problem, one way is to simply use A = max( A ; A T ). Another way is to use directed graph regularization [25] instead. An edge of a directed graph is an ordered pair ( i; j ) where i and j are the node indices. The in-degree of the i -th node is defined as D i = j  X  i denotes the j -th node has a directed link pointing to the i -th node, while out-degree of the i -th node is defined as D + i = a directed link pointing to the j -th node. Given the adja-cency matrix A of a directed graph, we define a transition probability of random walk as P ij = A ij =D + i . It is obvious that it satisfies bution for i -th node is i . Then it satisfies regularization is: if the i -th node link the j -th node, then their labels are likely to be the same. It is mathematically formulated as [25], = 1 + = + = tr( W T X (  X  1 = tr( W T XLX T W ) ; (2) wh ere L =  X  1 2 ( P + P T ) is graph Laplacian of directed graph, is a diagonal matrix with  X  ii = i and P is the transition matrix of random walk.
Till now, we have introduced the graph regularization on both undirected and directed graph. Since the proposed feature selection method is built up on Laplacian Regular-ized Least Squares (LapRLS), we will give a preliminary in-troduction here. In the setting of multi-class classification, LapRLS aims to learn c linear classifiers f k ( x ) = w T k  X  c by the following optimization problem, wh ere A ; I &gt; 0 are positive regularization parameters, W = [ w 1 ; : : : ; w c ]. More specifically, the first term in the above objective function is traditional least squares, which is able to fit the input content information and labels. The second term is Frobenious norm regularization on W which controls the complexity of the linear classifier. The third term is graph regularization as we introduced before. It can be either undirected graph regularization in Eq. (1) or directed graph regularization in Eq.(2). It is used to encode the link information of the network (undirected or directed). The above problem has a closed form solution Note that the inversion of the big matrix XX T + A I +
XLX T is time consuming. Fortunately, it can be solved efficiently as a linear system equation.

Then the label of each data point x can be predicted by l = arg max k f k ( x ).
In order to do feature selection, we introduce an indicator variable p , where p = ( p 1 ; : : : ; p d ) T and p i  X  { 1 ; : : : ; d , to represent whether a feature is selected or not. We further introduce a diagonal matrix diag( p ). Then the input data matrix is now represented as diag( p ) X . In order to indicate that m features are selected, we constrain p T 1 = m .
The proposed feature selection method aims at selecting a subset of features such that the empirical error of LapRLS is minimized. It can be mathematically formulated as where A ; I &gt; 0 are positive regularization parameters. It is worth noting that, when p = 1 , the proposed feature selection reduces to LapRLS in the input space. As a re-sult, LapRLS can be seen as a special case of the proposed method without feature selection. We call Eq. (5) as Fea-ture Selection in Network (FSNet). It is worth noting that the proposed method is heavily built upon LapRLS, so it is an embedded method.

As can be seen, the problem in Eq. (5) is a mixed integer programming [4], which is difficult to solve. In the following, we will relax it into a continuous optimization problem and present an efficient algorithm.

Suppose we find the optimal solution of Eq. (5), i.e., W and p , then p is a binary vector, and diag( p ) W is a ma-trix where the elements of many rows are all zeros. This motivate us to absorb the indicator variables p into W , and use L 2 , 0 -norm on W to achieve feature selection, leading to the following problem Or equivalently the regularized problem, where &gt; 0 is a regularization parameter. Note that it is difficult to give an analytical relationship between m and . Fortunately, such a relationship is not crucial for our problem. The objective function in Eq. (7) is a non-smooth and nonconvex function. We relax || W || 2 , 0 to its convex hull [4], and obtain the following convex problem, In th e following, we will present an algorithm for solving Eq. (8).
The most natural approach for solving the problem in Eq. (7) is the sub-gradient descent method [4]. However, its convergence rate is very slow, i.e., O ( 1  X  2 ) [15].
Rec ently, proximal gradient descent has received increas-ing attention in the machine learning community [11] [14]. It achieves the optimal convergence rate, i.e., O ( 1  X  ) for the firs t-order method and is able to deal with large-scale non-smooth convex problems. It can be seen as an extension of gradient descent, where the objective function to minimize is the composite of a smooth part and a non-smooth part. As to our problem, let It is easy to show that f ( W ) is convex and differentiable, while || W || 2 , 1 is non-smooth but convex.

In each iteration of the proximal algorithm, F ( W ) is lin-earized around the current estimate W t , and the value of W is updated as the solution of the following proximal gradient descent problem, where G  X  t ( W ; W t ) is defined as
In our problem, we have  X  f ( W t ) = XX T W t  X  XY + A W t + I XLX T W t : (12)
The philosophy under this formulation is that if the op-timization problem in Eq. (10) can be solved by exploiting the structure of the L 2 , 1 norm, then the convergence rate of the resulting algorithm is the same as that of gradient descent method, i.e., O ( 1  X  ), since no approximation on the non-smooth term is employed.

By ignoring the terms in G  X  t ( W ; W t ) that is independent of W , the optimization problem in Eq. (10) boils down to where U t = W t  X  1  X  into c separate subproblems of dimension d where w i t +1 , w i and u i t are the i -th rows of W t +1 U t respectively. It has a closed form solution [14] as follows Thus, the proximal gradient decent in Eq. (10) has the same convergence rate of O ( 1  X  ) as grad ient descent for smooth problem.
To achieve more efficient optimization, we employ Nes-terov X  X  method [15] to accelerate the proximal gradient de-cent in Eq. (10), which owns the convergence rate as O ( 1 p More sp ecifically, we construct a linear combination of W and W t +1 to update V t +1 as follows: where the sequence { t } t 1 is conventionally set to be t +1  X  rectly present the final algorithm for optimizing Eq. (8) in Algorithm 1.
 Algorit hm 1 Feature Selection in Network
Initiali ze: 0 ; W 1  X  R d m ; 1 = 1; repeat until con vergence
Once w e obtain W , we can obtain the selected features as follows. We calculate the score for each feature as Then we select all those features whose score is nonzero.
Given the selected features, we can train a classifier such as regularized least squares (RLS) [17] or LapRLS on the reduced data to do classification. Since the proposed method is specifically designed for LapRLS, we use LapRLS as the classifier in our experiments. More importantly, we do not need to re-train a LapRLS using the selected features. We can use the learnt weight matrix W directly, because it is already a linear classifier based on the selected features.
It can be seen that one additional advantage of the pro-posed method is that we do not need to predefine the number of selected features, i.e., m . In fact, the number of features to select is implicitly controlled by the regularization param-eter .
The convergence property of Algorithm 1 is stated in the following theorem.
 Theorem 3.1 [15] Let { W t } be the sequence generated by Algorithm 1, then for any t  X  1 we have wh ere L is the Lipschitz constant of the gradient of f ( W ) in the objective function, W = arg min W F ( W ) .

Theorem 3.1 shows that the convergence rate of the accel-erated proximal gradient method is O ( 1 p  X  ). The deta il proof of the above theorem can be found in [15] [11].
In this section, we will evaluate the proposed method for web page classification and research paper classification. The task of the experiments is to classify the networked data based on their content information and link structure. We compare it with some state of the art methods.
We use the same data sets 1 used in [26] to evaluate our method.

WebKB data set contains about 6,000 web pages col-lected from the web sites of computer science departments of four universities (Cornell, Texas, Washington, and Wis-consin). Each web page is labeled with one out of seven categories: student, professor, course, project, staff, depart-ment, and  X  X ther X . The characteristics about the WebKB data set are briefly summarized in Table 1.
 Data Sets #samples #features #links #classes Wash ington 1166 4165 2218 7 Wiscon sin 1210 4189 3200 6
Cora data se t contains the abstracts and references of about 34,000 research papers from the computer science community. The task is to classify each paper into one of the subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programming language (PL). The characteristics about the Cora data set are sum-marized in Table 2.
 Data Sets #samples #features #links #classes
We can see that the dimensionality of the data is very high (thousands of words). In addition, the link in the above two data sets is in nature directed. In the following ex-periments, we study both undirected graph regularization and directed graph regularization. To use undirected graph regularization, we use a symmetric adjacency matrix, i.e., A = max( A ; A T ).
In order to evaluate the proposed feature selection meth-ods, we compare it with the following baselines.

The first baseline is regularized least squares (RLS) [17], which is the state of the art classifier for i.i.d. data and is very related to LapRLS. In detail, RLS is applied in the following 3 settings. http: //www.nec-labs.com/  X  zsh/files/link-fact-data.zip
The second baseline is LapRLS [2], which is able to utilize both content information and link information. It can be seen as special case of the proposed method. That is, it does not do feature selection. So it uses all the original features. In detail, we adopt LapRLS in the following 2 scenarios. ). The third baseline is a traditional feature selection method. We choose Fisher score [17] because previous study [7] showed that it is comparable to or even better than the other fea-ture selection methods [23]. We first apply Fisher score to select features, then apply RLS in 3 ways as before. As a result, we get three methods: FS+RLS on con-tent , FS+RLS on link , and FS+RLS on content+link . In addition, we are also interested in first applying Fisher score on the content information to select features, and then apply undirected LapRLS and directed LapRLS . We name these two methods as FS+undirected LapRLS and FS+directed LapRLS
The fourth baseline is probabilistic relation principle com-ponent analysis (PRPCA) [13] which is proposed recently. It is the state of the art subspace learning method in net-work. As we know, both subspace learning and feature se-lection can achieve dimensionality reduction. As a result, we are very interested in comparing the proposed method with PRPCA.

For the proposed feature selection method, i.e., FSNet, similar with LapRLS, we also evaluate it in two setting: undirected FSNet and directed FSNet . Note that we do not need to train a classifier after selecting the features, because FSNet not only selects the features, but also outputs a linear classifier, i.e., W .

We did not compare the proposed method with the meth-ods proposed in [25] [26] and [12] because these two methods are transductive learning method [21] rather than inductive learning methods. In other words, those methods utilize the testing data when training. Hence it is unfair to do compar-ison.
For RLS, we set A = 1 on WebKB data set and A = 0 : 1 on Cora Data set.

For undirected LapRLS and directed LapRLS, we simply 0 : 05 ; 0 : 1 } .

For Fisher score, we select { 50% ; 60% ; : : : ; 90% } of the original features (content, link, or content+link), and the best result is reported.

For PRPCA, according to [13], we set the dimensionality of the subspace to 50 and number of iterations to 5.
For undirected FSNet and directed FSNet, we fix A = 1, In addition, on WebKB data set, we set = 1, while on Cora data set, we use = 0 : 001. In the accelerated proxi-mal gradient descent, there is a parameter . We simply set = 1 : 1.

We randomly split data into five folds and repeat the ex-periment for five times. For each time, we use one fold for testing, and the other four folds for training.
Before reporting the classification results, in this subsec-tion, we first examine the convergence of the accelerated proximal gradient descent in Algorithm 1 and original prox-imal gradient descent. In Figure 2, we plot the objective function value in Eq. (8 with respect to the number of iter-ations on the WebKB (Cornell) data subset. The parameter setting is A = 1 ; = 1 ; I = 0 : 01. In the figure, the y-axis is the value of objective function and the x-axis denotes the iteration number. Figure 2: The objective function value of FSNet with respect to the number of iterations for (a) ac-celerated proximal gradient descent and (b) proxi-mal gradient descent on the WebKB (Cornell) data subset.

We can see that the accelerated proximal gradient descent (Accelerated PGD) converges faster than proximal gradient descent (PGD). In detail, the Accelerated PGD usually con-verges within 50 iterations, while the original PGD needs more than 100 iterations to converge on WebKB data sets. This is consistent with the theoretical result in Section 3.7. That is, the convergence rate of Accelerated PGD is O ( 1 p which is faster than that of PGD, i.e., O ( 1  X  ). Similar results can be observed on other data sets.

The experimental results of FSNet in the rest part are all achieved by accelerated proximal gradient descent. Note that given sufficient number of iterations, both accelerated proximal gradient descent and proximal gradient descent will converge to the same solution, because the optimiza-tion problem of FSNet in Eq. (8) is convex.
To get a better understanding of our approach, we plot the learnt weight matrix, i.e., W of our method and LapRLS on WebKB (Cornell) data subset in Figure 3. In the figure, (a) shows the weight matrix of LapRLS, while (b) shows the weight matrix of FSNet. The parameter settings are
A = 1 ; I = 0 : 01 for LapRLS, and A = 1 ; I = 0 : 01 ; = 1 for FSNet. It can be seen that many rows of the weight matrix of FSNet are all zero, which leads to feature selection. We call it  X  X ow-sparsity X . That means, the corresponding features are eliminated. In contrast, the weight matrix of LapRLS is not row-sparse, which is not able to do feature selection. Figure 3: The weight matrix W of the learnt clas-si er: (a) LapRLS; (b) FSNet on WebKB (Cornell) data subset. For better viewing, please see it in col-ored pdf.
The classification results on WebKB data set and Cora data set are reported in Table 3 and Table 4 respectively. For clear comparison, we also show the results in Figure 4 and Figure 5.

It can be observed that: 1. The proposed method outperforms the baseline meth-ods consistently on WebKB data set, while the proposed method outperforms the baseline methods on 3 out of 4 sub-sets in Cora data set. 2. The proposed method outperforms LapRLS, which can be seen as a special case of the proposed method without feature selection. This indicates that feature selection is crucial and beneficial for classification of networked data. 3. FS+LapRLS is inferior to FSNet. This is because that Fisher score is based on i.i.d. assumption, while the net-worked data violate this assumption. In contrast, FSNet is able to utilize both content and link information in a prin-cipled way. On the other hand, FS+LapRLS is no better than LapRLS. This is because Fisher score in FS+LapRLS cannot consider the link information, which is crucial for learning in network. 4. PRPCA also achieves very good result on WebKB data set. However, it is not as good as FSNet. The reason is that PRPCA is an unsupervised subspace learning method, while FSNet is a supervised method. Supervised dimensionality reduction methods are generally better than unsupervised methods for classification. 5. The reason why the proposed method is not as good as some baseline methods (RLS on content+link) on the Cora data set may be that the citation relation among pa-pers is more complicated than hyper-link among web pages. Graph regularization may not be that useful to utilize the underlying information of citation. We will investigate other techniques to fully use the citation information in the future. 6. The proposed methods with undirected graph regu-larization and directed graph regularization achieve compa-rable results. We deem that FSNet with directed graph regularization will achieve better result than FSNet with undirected graph regularization because the link information in the two data sets is directed. However, they eventually achieve very similar results. As far as we know, there is no theoretical work on the comparison between undirected and directed graph regularization. This needs further study.
FSNet has three parameters, which are the regulariza-tion parameters, i.e., A ; I and . In our experiments, we simply set A = 1, and = 1 on WebKB data set while = 0 : 001 on Cora Data set. The only parameter needed to tune is I . It controls the contribution of link informa-tion. Hence we investigate the classification accuracy with respect to the regularization parameter I . We vary the value of I , and plot the corresponding classification accu-racy on the WebKB and Cora data sets in Figure 6 and Figure 7 respectively.

As can be seen, FSNet is not sensitive to the regulariza-tion parameter I in a wide range of I . In detail, FSNet achieves consistently better performance than LapRLS with the I varying from 0 : 001 to 0 : 1 on both data sets. It is a very appealing property because we do not need to tune the regularization parameter painfully in the application.
In this paper, we proposed a feature selection method based on Laplacian Regularized Least Squares (LapRLS) for the data distributing in a network. We use linear regression to fit the content information, and use graph regularization to consider the link information. Then the feature selection method is casted as selecting a subset of features such that the training error of LapRLS is minimized. The resultant op-timization problem is a mixed integer programming, which is difficult to solve. It is relaxed into a L 2 , 1 -norm constrained least squares problem and solved by accelerated proximal gradient algorithm. Experiments on benchmark networked data show that the proposed method outperforms the state of the art methods.

In the future work, we will study other kinds of techniques to utilize the link information. For example, [16] proposed modularity to measure the strength of a partition for real-world networks by taking into account the degree distribu-tion of nodes. It is shown to be effective in various kinds of complex networks. We will investigate how to incorporate modularity into our method. The w ork was supported in part by NSF IIS-09-05215, U.S. Air Force Office of Scientific Research MURI award FA9550-08-1-0265, and the U.S. Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053 (NS-CTA). The views and conclusions contained in this docu-ment are those of the authors and should not be interpreted as representing the official policies, either expressed or im-plied, of the Army Research Laboratory or the U.S. Govern-ment. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstand-ing any copyright notation here on. We thank the anony-mous reviewers for their helpful comments. the four subsets of WebKB dataset. [1] M. Belkin and P. Niyogi. Laplacian eigenmaps for [2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [3] M. Bilgic, L. Mihalkova, and L. Getoor. Active [4] S. Boyd and L. Vandenberghe. Convex optimization . [5] F. R. K. Chung. Spectral Graph Theory . American [6] D. A. Cohn and T. Hofmann. The missing link -a [7] Q. Gu, Z. Li, and J. Han. Generalized fisher score for [8] I. Guyon and A. Elisseeff. An introduction to variable the four subsets of Cora dataset. [9] X. He and P. Niyogi. Locality preserving projections. [10] T. Hofmann. Unsupervised learning by probabilistic [11] S. Ji and J. Ye. An accelerated gradient method for [12] W.-J. Li and D.-Y. Yeung. Relation regularized matrix [13] W.-J. Li, D.-Y. Yeung, and Z. Zhang. Probabilistic [14] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via [15] Y. Nesterov. Introductory Lectures on Convex [16] M. E. Newman. Modularity and community structure [17] P. E. H. R. O. Duda and D. G. Stork. Pattern [18] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, [19] L. Shi, Y. Zhao, and J. Tang. Combining link and [20] A. J. Smola and R. I. Kondor. Kernels and [21] V. N. Vapnik. The nature of statistical learning theory . [22] S. Yan, D. Xu, B. Zhang, H. Zhang, Q. Yang, and [23] Y. Yang and J. O. Pedersen. A comparative study on [24] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [25] D. Zhou, J. Huang, and B. Sch  X  olkopf. Learning from [26] S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining [27] X. Zhu, Z. Ghahramani, and J. D. Lafferty.

