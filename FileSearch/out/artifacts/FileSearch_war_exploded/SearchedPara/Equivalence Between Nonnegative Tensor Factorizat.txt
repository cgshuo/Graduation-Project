 This paper establishes a connection between NMF and PLSA on multi-way data, called NTF and T-PLSA respectively. Two types of T-PLSA models are proven to be equivalent to non-negative PA-RAFAC and non-negative Tucker3. This paper also shows that by running NTF and T-PLSA alternatively, they can jump out of each other X  X  local minima and achieve a better clustering solution. Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering-Algorithms General Terms: Algorithms, Experimentation, Performance Keywords: Multi-way data, Tensor, NTF, PLSA
For decomposing non-negative data (document data, social net-work data, etc.), the two most widely used methods are NMF [5] (from the linear algebra perspective) and PLSA (from the statistics perspective) [4]. Recently, decomposing multi-way (tensor) non-tion. NMF and PLSA can be extende d on multi-way non-negative data, called NTF (Non-negative Tensor Factorization) and T-PLSA (Tensorial Probabilistic Latent Semantic Analysis), respectively. The two most well-known tensor factorization models are: (1) Parafac [7]: Parafac model can be thought of as a multi-linear form of decomposition for the objective tensor; (2) Tucker3: Tucker3 model is regarded as a form of multi-way principle component anal-ysis [3]. Parafac and Tucker3 solved with non-negative constraints are called NParafac and NTucker . T-PLSA is studied in some re-cent work. Shashanka et.al. proposed a probabilistic latent variable model for generalization of PLSA [8]. Chi et.al. proposed an Prob-abilistic Polyadic Factorization model for personalized recommen-dation [1]. Although PLSA and NMF are proven to be equivalent in [2], there have been very few attempts to establish the connec-tion between T-PLSA and NTF models. The contribution of this paper is then: (1) Establishing a connection between two types of T-PLSA models ( ParaAspect [8] and TuckAspect [1]) and two most well-known NTF models ( NParafac and NTucker ). ParaAspect and NParafac, TuckAspect and NTucker are proven to optimize the same objective functions, and have equivalent factorizations with column L1-normalization, respectively; (2) T-PLSA and NTF will be compared in terms of objective functions, convergence rate, and computational complexity by experiments on real-world datasets. (3) This paper will reveal that a hybrid algorithm of NTF and T-PLSA achieves a better solution.
  X  V = V ( D v )  X  1 are denoted as the column L1-normalized matri-ces of U , S ,and V . NParafac factorization can be written as where H = D u D S D v is a diagonal matrix. We can see that this factorization is equivalent to the ParaAspect model where  X  u ip = P ( Similar to the ParaAspect model, i  X  u ip =1 , j  X  v jp =1 , l  X  s lp =1 , by column L1-normalization, and f torization model of NTucker is equivalent to that of TuckAspect, first NTucker can be reformulated as where H ( 1 ) = D u G ( 1 ) ( D s  X  D v ) . Note that it has the same factorization formulae as TuckAspect. Similar to the proof of the equivalence between NParafac and ParaAspect, i  X  u ip =1 ,  X  v Based on the above proofs, T-PLSA and NTF are equivalent .
Althought T-PLSA and NTF are equivalent, they use different algorithms. NParafac and NTucke r use multiplicative update rules proposed by [5]. ParaAspect and TuckAspect use the standard EM algorithm. NParafac and ParaAspect have the same computational complexity, O ( mntk ) . However, TuckAspect has larger computa-+ nmtk 1 ) ,or O ( nmtk 1 ) when n k 2 k 3 . ParaAspect requires O ( mntk ) space units for calculating the posterior probabilities of the latent variables. NParafac only requires O ( nmt ) with k min ( n, m, t ) . Moreover, TuckAspect needs O ( nmtk 1 k 2 k 3 ) space units while NTucker only needs space O ( nmt + mtk 2 k 3 + ntk 1 k 3 + nmk 1 k 2 ) ,or O ( nmt ) when n k 2 k 3 , m k 1 k 3 ,and t k k 2 . Overall, NTF requires less computation and space than T-PLSA. However, T-PLSA has a more solid statistical foundation as a probabilistic mixture model.

Since T-PLSA and NTF employ different algorithms, they con-verge to different local minima. However, a hybrid algorithm that runs them alternatively can help them jump out of each other X  X  local minima and achieve a better solution. This will be shown in experimental results in the next section.
Experiments are conducted on three real-world datasets. Two are from DBLP, one from Enron emails. DBLP datasets are extracted from the DBLP computer science bibliography. One DBLP dataset named DBLP9 has 1000 authors from 9 research areas, and 1000 key terms in 20 years. The other dataset is named DBLP4 with 100 authors  X  200 terms  X  20 years from 4 areas. The Enron email dataset is a 184 users  X  184 users  X  44 months three-way data. The number of classes is set to 4 according to [6], which defined 4 roles for Enron email senders/receivers. All experimental results are averages over 10 runs of four algorithms.

Equivalent Objective Functions: Objective functions J NP arafac (Eq.3) and J ParaAspect (Eq.4) are calculated for both NParafac and ParaAspect. We can see that they have very similar objective func-tion values for all datasets as shown in first two rows of Table 2.
