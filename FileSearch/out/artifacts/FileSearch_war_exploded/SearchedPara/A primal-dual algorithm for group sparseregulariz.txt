 Sparsity has become a popular way to deal with small samples of high dimensional data and, in a sparsity based methods are the key towards finding interpretable models in real-world problems. In lem of variable selection, since it provides sparse solutions by minimizing a convex functional. The success of ` 1 regularization motivated exploring different kinds of sparsity properties for (general-ized) linear models, exploiting available a priori information, which restricts the admissible sparsity patterns of the solution. An example of a sparsity pattern is when the input variables are partitioned into groups (known a priori), and the goal is to estimate a sparse model where variables belonging to the same group are either jointly selected or discarded. This problem can be solved by regularizing with the group-` 1 penalty, also known as group lasso penalty, which is the sum, over the groups, of the euclidean norms of the coefficients restricted to each group.
 A possible generalization of group lasso is to consider groups of variables which can be potentially overlapping, and the goal is to estimate a model which support is the union of groups. This is a common situation in bioinformatics (especially in the context of high-throughput data such as gene expression and mass spectrometry data), where problems are characterized by a very low number of samples with several thousands of variables. In fact, when the number of samples is not sufficient to guarantee accurate model estimation, an alternative is to take advantage of the huge amount of prior knowledge encoded in online databases such as the Gene Ontology. Largely motivated by ap-plications in bioinformatics, a new type of penalty is proposed in [12], which is shown to give better performances than simple ` 1 regularization.
 A straightforward solution to the minimization problem underlying the method proposed in [12] is to apply state-of-the-art techniques for group lasso (we recall interior-points methods [3, 20], block coordinate descent [16], and proximal methods [9, 21], also known as forward-backward splitting algorithms, among others) in an expanded space, built by duplicating variables that belong to more than one group.
 As already mentioned in [12], though very simple, such an implementation does not scale to large datasets, when the groups have significant overlap, and a more scalable algorithm with no data du-plication is needed. For this reason we propose an alternative optimization approach to solve the group lasso problem with overlap. Our method does not require explicit replication of the features and is thus more appropriate to deal with high dimensional problems with large groups overlap. Our approach is based on a proximal method (see for example [18, 6, 5]), and two ad hoc results that allow to efficiently compute the proximity operator in a much lower dimensional space: with Lemma 1 we identify the subset of active groups, whereas in Theorem 2 we formulate the reduced dual problem for computing the proximity operator, where the dual space dimensionality coincides with the number of active groups. The dual problem can then be solved via Bertsekas X  projected Newton method [7]. We recall that a particular overlapping structure is the hierarchical structure, where the overlap between groups is limited to inclusion of a descendant in its ancestors. In this case the CAP penalty [24] can be used for model selection, as it has been done in [2, 13], but ancestors are forced to be selected when any of their descendant are selected. Thanks to the nested structure, the proximity operator of the penalty term can be computed exactly in a finite number of steps [14]. This is no longer possible in the case of general overlap. Finally it is worth noting that the penalty analyzed here can be applied also to hierarchical group lasso. Differently from [2, 13] selection of ancestors is no longer enforced.
 The paper is organized as follows. In Section 2 we recall the group lasso functional for overlap-ping groups and set some notations. In Section 3 we state the main results, present a new iterative optimization procedure, and discuss computational issues. Finally in Section 4 we present some numerical experiments comparing running time of our algorithm with state-of-the-art techniques. The proofs are reported in the Supplementary material. We first fix some notations. Given a vector  X   X  R d , while k X k denotes the ` 2 -norm, we will use the Then, for any differentiable function f : R B  X  R , we denote by  X  r f its partial derivative with respect to variables r , and by  X  f = (  X  r f ) B r =1 its gradient.
 We are now ready to cast group ` 1 regularization with overlapping groups as the following varia-of variables G = { G r } B r =1 with G r  X  { 1 ,...,d } , we assume the estimator to be described by a where  X  is the n  X  d matrix given by the features  X  j in the dictionary evaluated in the training set the cost function 1 ` : R  X  Y  X  R + is the square loss, ` ( f ( x ) ,y ) = ( y  X  f ( x )) 2 . The penalty term  X  G overlap : R d  X  R + is lower semicontinuous, convex, and one-homogeneous, (  X  G overlap (  X  X  ) =  X   X  G overlap (  X  ) ,  X   X   X  R d and  X   X  R + ), and is defined as allow overlapping groups, while maintaining the group lasso property of enforcing sparse solutions which support is a union of groups . When groups do not overlap,  X  G overlap reduces to the group lasso penalty. Note that, as pointed out in [12], using P B r =1 k  X  k G penalty leads to a solution which support is the complement of the union of groups . For an extensive we therefore refer the interested reader to [12]. If one needs to solve problem (1) for high dimensional data, the use of standard second-order meth-ods such as interior-point methods is precluded (see for instance [6]), since they need to solve large systems of linear equations to compute the Newton steps. On the other hand, first order methods inspired to Nesterov X  X  seminal paper [19] (see also [18]) and based on proximal methods already proved to be a computationally efficient alternative in many machine learning applications [9, 21]. 3.1 A Proximal algorithm following acceleration of the iterative forward-backward splitting scheme for a suitable choice of  X  . Due to one-homogeneity of  X  G overlap , the proximity operator associated the origin, which is a closed and convex set. We will denote such a projection as  X   X / X K , where K =  X   X  G overlap (0) . The above scheme is inspired to [10], and is equivalent to the algorithm named FISTA [5], which convergence is guaranteed, as recalled in the following theorem a constant C 0 such that the iterative update (10) satisfies As it happens for other accelerations of the basic forward-backward splitting algorithm such as [19, 6, 4], convergence of the sequence  X  p is no longer guaranteed unless strong convexity is assumed. However, sacrificing theoretical convergence for speed may be mandatory in large scale applications. Furthermore, there is a strong empirical evidence that  X  p is indeed convergent (see Section 4). 3.2 The projection Note that the proximity operator of the penalty  X  G overlap does not admit a closed form and must be computed approximatively. In fact the projection on the convex set cannot be decomposed group-wise, as in standard group ` 1 regularization, which proximity operator resolves to a group-wise soft-thresholding operator (see Eq. (9) later). Nonetheless, the following  X  B = |  X  G|  X  B active groups. This equivalence is crucial for speeding up the algorithm, in fact  X  B is the number of selected groups which is small if one is interested in sparse solutions. convex set  X K with K = { v  X  R d , k v k Gr  X  1 for r = 1 ,...,B } is given by where  X  G := { G  X  X  , k  X  k G &gt;  X  } . The proof (given in the supplementary material) is based on the fact that the convex set  X K is the intersection of cylinders that are all centered on a coordinate subspace. Since  X  B is typically much smaller than d , it is convenient to solve the dual problem associated to (4).
 convex set  X K with K = { v  X  R d , k v k G where  X   X  is the solution of  X  Equation (6) is the dual problem associated to (4), and, since strong duality holds, the minimum of (4) is equal to the maximum of the dual problem, which can be efficiently solved via Bertsekas X  projected Newton method described in [7], and here reported as Algorithm 1.
 Algorithm 1 Projection Given:  X   X  R d , X  init  X  R  X  B , X   X  (0 , 1) , X   X  (0 , 1 / 2) , &gt; 0
Initialize: q = 0 , X  0 =  X  init end while Bertsekas X  iterative scheme combines the basic simplicity of the steepest descent iteration [22] with the quadratic convergence of the projected Newton X  X  method [8]. It does not involve the solution of a quadratic program thereby avoiding the associated computational overhead. 3.3 Computing the regularization path In Algorithm 2 we report the complete G roup L asso with O verlap pri mal-du al (GLO-pridu) scheme for computing the regularization path, i.e. the set of solutions corresponding to different values of proved to guarantee convergence, despite the local nature of Bertsekas X  scheme. 3.4 The replicates formulation An alternative way to solve the optimization problem (1) is proposed by [12], where the authors show that problem (1) is equivalent to the standard group ` 1 regularization (without overlap) in an expanded space built by replicating variables belonging to more than one group: Algorithm 2 GLO-pridu regularization path Given:  X  1 &gt;  X  2 &gt;  X  X  X  &gt;  X  T , G , X   X  (0 , 1) , X   X  (0 , 1 / 2) , 0 &gt; 0 , X  &gt; 0 Let:  X  = ||  X  T  X  || /n
Initialize:  X  (  X  0 ) = 0 for t = 1 ,...,T do end for return  X  (  X  1 ) ,..., X  (  X  T ) where  X   X  is the matrix built by concatenating copies of  X  restricted each to a certain group, i.e. (  X 
 X  | G
B | ,..., maps  X   X  in v  X  R d , such that supp ( v )  X  G r and ( v j ) j  X  G r = (  X   X  j ) j  X   X  G advantage of the above formulation relies on the possibility of using any state-of-the-art optimization procedure for group lasso. In terms of proximal methods, a possible solution is given by Algorithm 3, where S  X / X  is the proximity operator of the new penalty, and can be computed exactly as Algorithm 3 GL-prox Given:  X   X  0  X  R d , X  &gt; 0 , X  = ||  X   X  T  X   X  || /n
Initialize: p = 0 ,  X  h 1 =  X   X  0 ,t 1 = 1 while convergence not reached do end while return  X   X  p Note that in principle, by applying Lemma 1, the group-soft-thresholding operator in (9) can be com-of the active groups has the same computational cost of the thresholding itself. 3.5 Computational issues For both GL-prox and GLO-pridu, the complexity of one iteration is the sum of the complexity of computing the gradient of the data term and the complexity of computing the proximity operator of the penalty term. The former has complexity O ( dn ) and O (  X  dn ) for GLO-pridu and GL-prox, projection onto K . This can be neglected for the case of replicated variables.On the other hand, the time complexity of one iteration for Algorithm 1 is driven by the number of active groups  X  B . This number is typically small when looking for sparse solutions. The complexity is thus given and  X  B + is the number of active constraints, then the complexity of inverting matrix H is at most O (  X  H where q is the number of iterations necessary to reach convergence. Note that even if, in order to guarantee convergence, the tolerance for evaluating convergence of the inner iteration must decrease with the number of external iterations, in practice, thanks to warm starting, we observed that q is rarely greater than 10 in the experiments presented here.
 Concerning the number of iterations required to reach convergence for GL-prox in the replicates formulation, we empirically observed that it requires a much higher number of iterations than GLO-pridu (see Table 3). We argue that such behavior is due to the combination of two occurences: 1) the local condition number of matrix  X   X  is 0 even if  X  is locally well conditioned, 2) the decomposition of  X   X  as  X   X   X  is possibly not unique, which is required in order to have a unique solution for (8). The former is due to the presence of replicated columns in  X   X  . In fact, since E  X  is convex but not necessarily strictly convex  X  as when n &lt; d  X , uniqueness and convergence is not always guaranteed unless some further assumption is imposed. Most convergence results relative to ` 1 regularization link uniqueness of the solution as well as the rate of convergence of the Soft Thresholding Iteration Proposition 4.1 in [11], where the Hessian restricted to the set of relevant variables is required to groups have non null intersection, then  X  H restricted to the set of relevant variables is by no means full rank. Concerning the latter argument, we must say that in many real world problems, such as bioinformatics, one cannot easily verify that the solution indeed has a unique decomposition. In fact, we can think of trivial examples where the replicates formulation has not a unique solution. In this section we present numerical experiments aimed at comparing the running time performance of GLO-pridu with state-of-the-art algorithms. To ensure a fair comparison, we first run some pre-liminary experiments to identify the fastest codes for group ` 1 regularization with no overlap. We refer to [6] for an extensive empirical and theoretical comparison of different optimization proce-dures for solving ` 1 regularization. Further empirical comparisons can be found in [15]. 4.1 Comparison of different implementations for standard group lasso solve group lasso: interior-point methods, (group) coordinate descent and its variations, and prox-Matlab code at http://www.di.ens.fr/  X  fbach/grouplasso/index.htm described in [1]. For coordinate descent methods, we employed the R-package grlplasso , which imple-ments block coordinate gradient descent minimization for a set of possible loss functions. In the following we will refer to these two algorithms as  X  X  X L-IP X  and  X  X L-BCGD X . Finally we use our Matlab implementation of Algorithm GL-prox as an instance of proximal methods.
 We first observe that the solutions of the three algorithms coincide up to an error which depends on each algorithm tolerance. We thus need to tune each tolerance in order to guarantee that all iterative algorithms are stopped when the level of approximation to the true solution is the same. Table 1: Running time (mean and standard deviation) in seconds for computing the entire regular-ization path of GL-IP, GL-BCGD, and GL-prox for different values of B , and n . For B = 1000 , GL-IP could not be computed due to memory reasons. Toward this end, we run Algorithm GL-prox with machine precision,  X  = 10  X  16 , in order to have a good approximation of the asymptotic solution. We observe that for many values of n and d , and over a large range of values of  X  , the approximation of GL-prox when  X  = 10  X  6 is of the same order of the approximation of GL-IP with optparam.tol = 10  X  9 , and of GL-BCGD with tol = 10  X  12 . Note also that with these tolerances the three solutions coincide also in terms of selection, i.e. their supports are identical for each value of  X  . Therefore the following results correspond to optparam.tol = 10  X  9 for GL-IP, tol = 10  X  12 for GL-BCGD, and  X  = 10  X  6 for GL-prox. For the other parameters of GL-IP we used the values used in the demos supplied with the code. Concerning the data generation protocol, the input variables x = ( x 1 ,...,x d ) are uniformly drawn and so on), for different values of n and B . In order to make sure that we are working on the correct range of values for the parameter  X  , we first evaluate the set of solutions of GL-prox corresponding to a large range of 500 values for  X  , with  X  = 10  X  4 . We then determine the smallest value of  X  which corresponds to selecting less than n variables,  X  min , and the smallest one returning the null solution,  X  max . Finally we build the geometric series of 50 values between  X  min and  X  max , and use it to evaluate the regularization path on the three algorithms. In order to obtain robust estimates of the running times, we repeat 20 times for each pair n,B .

In Table 1 we report the computational times required to evaluate the entire regularization path for the three algorithms. Algorithms GL-BCGD and GL-prox are always faster than GL-IP which, due to memory reasons, cannot by applied to problems with more than 5000 variables, since it requires to store the d  X  d matrix  X  T  X   X  . It must be said that the code for GP-IL was made available mainly in order to allow reproducibility of the results presented in [1], and is not optimized in terms of time and memory occupation. However it is well known that standard second-order methods are typically precluded on large data sets, since they need to solve large systems of linear equations to compute the Newton steps. GL-BCGD is the fastest for B = 1000 , whereas GL-prox is the fastest for B = 10 , 100 . The candidates as benchmark algorithms for comparison with GLO-pridu are GL-prox and GL-BCGD. Nevertheless we observed that, when the input data matrix contains a significant fraction of replicated columns, this algorithm does not provide sparse solutions. We therefore compare GLO-pridu with GL-prox only. 4.1.1 Projection vs duplication The data generation protocol is equal to the one described in the previous experiments, but  X  depends on the first 12 / 5 b variables (which correspond to the first three groups) G B  X  3 groups are built by randomly drawing sets of b indexes from [1 ,d ] . In the following we We also vary the number of groups B , so that the dimension of the expanded space is  X  times the parameter  X  can be thought of as the average number of groups a single variable belongs to. We identify the correct range of values for  X  as in the previous experiments, using GLO-pridu with loose tolerance, and then evaluate the running time and the number of iterations necessary to compute the entire regularization path for GL-prox on the expanded space and GLO-pridu, both with  X  = 10  X  6 . Finally we repeat 20 times for each combination of the three parameters d,b , and  X  . Table 2: Running time (mean  X  standard deviation) in seconds for b = 10 (top), and b = 100 (below). For each d and  X  , the left and right side correspond to GLO-pridu, and GL-prox, respectively. d =1000 0 . 15  X  0 . 04 0 . 20  X  0 . 09 1 . 6  X  0 . 9 5 . 1  X  2 . 0 12 . 4  X  1 . 3 68  X  8 d =5000 1 . 1  X  0 . 4 1 . 0  X  0 . 6 1 . 55  X  0 . 29 2 . 4  X  0 . 7 103  X  12 790  X  57 d =10000 2 . 1  X  0 . 7 2 . 1  X  1 . 4 3 . 0  X  0 . 6 4 . 5  X  1 . 4 460  X  110 2900  X  400 Table 3: Number of iterations (mean  X  standard deviation) for b = 10 (top) and b = 100 (below). For each d and  X  , the left and right side correspond to GLO-pridu, and GL-prox, respectively. d =1000 100  X  30 80  X  30 1200  X  500 1900  X  800 2150  X  160 11000  X  1300 d =5000 100  X  40 70  X  30 148  X  25 139  X  24 6600  X  500 27000  X  2000 d =10000 100  X  30 70  X  40 160  X  30 137  X  26 13300  X  1900 49000  X  6000 d =1000 913  X  12 2160  X  210 894  X  11 2700  X  300 895  X  10 4200  X  400 d =5000 600  X  400 600  X  300 1860  X  110 4590  X  290 1320  X  30 6800  X  500 d =10000 81  X  11 63  X  11 1000  X  500 1800  X  900 2100  X  60  X  Running times and number of iterations are reported in Table 2 and 3, respectively. When the degree of overlap  X  is low the computational times of GL-prox and GLO-pridu are comparable. As  X  increases, there is a clear advantage in using GLO-pridu instead of GL-prox. The same behavior occurs for the number of iterations. We have presented an efficient optimization procedure for computing the solution of group lasso with overlapping groups of variables, which allows dealing with high dimensional problems with advantage with respect to state-of-the-art algorithms for group lasso applied on the expanded space built by replicating variables belonging to more than one group. We also mention that computational methods, such as [23]. Finally, as shown in [17], the improved computational performance enables to use group ` 1 regularization with overlap for pathway analysis of high-throughput biomedical data, without pre-processing for dimensionality reduction. [1] F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine [2] F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning. [3] F. R. Bach, G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo [4] A. Beck and Teboulle. M. Fast gradient-based algorithms for constrained total variation image [5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse [6] S. Becker, J. Bobin, and E. Candes. Nesta: A fast and accurate first-order method for sparse [7] D. Bertsekas. Projected newton methods for optimization problems with simple constraints. [8] R. Brayton and J. Cullum. An algorithm for minimizing a differentiable function subject to. J. [9] J. Duchi and Y. Singer. Efficient online and batch learning using forward backward splitting. [10] O. Guler. New proximal point algorithm for convex minimization. SIAM J. on Optimization , [11] E. T. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for l1-minimization: Methodology [12] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In ICML , [13] R. Jenatton, J.-Y . Audibert, and F. Bach. Structured variable selection with sparsity-inducing [14] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical [15] I. Loris. On the performance of algorithms for the minimization of l 1 -penalized functionals. [16] L. Meier, S. van de Geer, and P. Buhlmann. The group lasso for logistic regression. J. R. [17] S. Mosci, S. Villa, Verri A., and L. Rosasco. A fast algorithm for structured gene selection. [18] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of con-[19] Y. Nesterov. Smooth minimization of non-smooth functions. Math. Prog. Series A , [20] M. Y. Park and T. Hastie. L1-regularization path algorithm for generalized linear models. J. R. [21] L. Rosasco, M. Mosci, S. Santoro, A. Verri, and S. Villa. Iterative projection methods for [22] J. Rosen. The gradient projection method for nonlinear programming, part i: linear constraints. [23] V. Roth and B. Fischer. The group-lasso for generalized linear models: uniqueness of solutions
