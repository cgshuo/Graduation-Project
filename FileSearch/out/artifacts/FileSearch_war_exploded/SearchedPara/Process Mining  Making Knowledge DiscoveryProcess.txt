 Recently, the Task Force on Process Mining released the Process Mining Manifesto . The manifesto is supported by 53 organizations and 77 process mining experts contributed to it. The active contributions from end-users, tool vendors, consultants, analysts, and researchers illustrate the growing relevance of process mining as a bridge between data mining and business process modeling . This paper summarizes the manifesto and explains why process mining is a highly rele-vant, but also very challenging, research area. This way we hope to stimulate the broader ACM SIGKDD community to look at process-centric knowledge discovery . Process mining is a relatively young research discipline that sits between computational intelligence and data mining on the one hand, and process modeling and analysis on the other hand. The idea of process mining is to discover, mon-itor and improve real processes (i.e., not assumed processes) by extracting knowledge from event logs readily available in today X  X  (information) systems [1]. Process mining in-cludes (automated) process discovery (i.e., extracting pro-cess models from an event log), conformance checking (i.e., monitoring deviations by comparing model and log), social network/organizational mining, automated construction of simulation models, model extension, model repair, case pre-diction, and history-based recommendations.
 Figure 1 illustrates the scope of process mining. Starting point for process mining is an event log . All process mining techniques assume that it is possible to sequentially record events such that each event refers to an activity (i.e., a well-defined step in some process) and is related to a particular case (i.e., a process instance). Event logs may store ad-ditional information about events. In fact, whenever possi-ble, process mining techniques use extra information such as the resource (i.e., person or device) executing or initiating the activity, the timestamp of the event, or data elements recorded with the event (e.g., the size of an order). Event logs can be used to conduct three types of process mining [1; 2]. The first type of process mining is discov-ery . A discovery technique takes an event log and produces a model without using any a-priori information. Process discovery is the most prominent process mining technique. For many organizations it is surprising to see that existing techniques are indeed able to discover real processes merely based on example executions in event logs. The second type of process mining is conformance . Here, an existing process model is compared with an event log of the same process. Conformance checking can be used to check if reality, as recorded in the log, conforms to the model and vice versa. The third type of process mining is enhancement . Here, the idea is to extend or improve an existing process model using information about the actual process recorded in some event log. Whereas conformance checking measures the alignment between model and reality, this third type of process mining aims at changing or extending the a-priori model. For in-stance, by using timestamps in the event log one can extend the model to show bottlenecks, service levels, throughput times, and frequencies.
 Figure 1 shows how first an end-to-end process model is dis-covered. The model is visualized as a BPMN (Business Pro-cess Modeling Notation) model, but internally algorithms are often using more formal notations such as Petri nets, C-nets, and transition systems [1]. By replaying the event log on the model it is possible to add information on bottle-necks, decisions, roles, and resources. The growing interest in log-based process analysis motivated the establishment of the IEEE Task Force on Process Min-ing . The goal of this task force is to promote the research, development, education, and understanding of process min-ing. The task force was established in 2009 in the context of the Data Mining Technical Committee of the Computational Intelligence Society of the IEEE. Members of the task force include representatives of more than a dozen commercial software vendors (e.g., Pallas Athena, Software AG, Futura Process Intelligence, HP, IBM, Fujitsu, Infosys, and Fluxi-con), ten consultancy firms (e.g., Gartner and Deloitte) and over twenty universities.
 Concrete objectives of the task force are: to make end-users, developers, consultants, managers, and researchers aware of the state-of-the-art in process mining, to promote the use of process mining techniques and tools, to stimulate new process mining applications, to play a role in standardization e  X  orts for logging event data, to organize tutorials, special sessions, workshops, panels, and to publish articles, books, videos, and special issues of journals. For example, in 2010 the task force standardized XES ( www.xes-standard.org ), a standard logging format that is extensible and supported by the OpenXES library ( www.openxes.org )andbytools such as ProM, XESame, Nitro, etc. See http://www.win. tue.nl/ieeetfpm/ for recent activities of the task force. The IEEE Task Force on Process Mining recently released a manifesto describing guiding principles and challenges [2]. The manifesto aims to increase the visibility of process min-ing as a new tool to improve the (re)design, control, and support of operational business processes. It is intended to guide software developers, scientists, consultants, and end-users. As an introduction to the state-of-the-art in process mining, we briefly summarize the main findings reported in the manifesto [2]. As with any new technology, there are obvious mistakes that can be made when applying process mining in real-life set-tings. Therefore, the six guiding principles listed in Table 1 aim to prevent users/analysts from making such mistakes. As an example, consider guiding principle GP4 : X  X vents Should Be Related to Model Elements X . It is a miscon-ception that process mining is limited to control-flow dis-covery, other perspectives such as the organizational per-spective, the time perspective, and the data perspective are equally important. However, the control-flow perspective (i.e., the ordering of activities) serves as the layer connect-ing the di  X  erent perspectives. Therefore, it is important to relate events in the log to activities in the model. Confor-mance checking and model enhancement heavily rely on this relationship. After relating events to model elements, it is possible to  X  X eplay X  the event log on the model [1]. Replay may be used to reveal discrepancies between an event log and a model, e.g., some events in the log are not possible according to the model. Techniques for conformance check-ing quantify and diagnose such discrepancies. Timestamps in the event log can be used to analyze the temporal behav-ior during replay. Time di  X  erences between causally related activities can be used to add average/expected waiting times to the model. These examples illustrate the importance of guiding principle GP4; the relation between events in the log and elements in the model serves as a starting point for di  X  erent types of analysis. Process mining is an important tool for modern organiza-tions that need to manage non-trivial operational processes. On the one hand, there is an incredible growth of event data. On the other hand, processes and information need to be aligned perfectly in order to meet requirements related to compliance, e ciency, and customer service. Despite the applicability of process mining there are still important chal-lenges that need to be addressed; these illustrate that pro-cess mining is an emerging discipline. Table 2 lists the eleven challenges described in the manifesto [2]. As an example consider Challenge C4 :  X  X ealing with Concept Drift X . The term concept drift refers to the situation in which the pro-cess is changing while being analyzed. For instance, in the beginning of the event log two activities may be concur-rent whereas later in the log these activities become sequen-tial. Processes may change due to periodic/seasonal changes (e.g.,  X  X n December there is more demand X  or  X  X n Friday afternoon there are fewer employees available X ) or due to changing conditions (e.g.,  X  X he market is getting more com-petitive X ). Such changes impact processes and it is vital to detect and analyze them. However, most process mining techniques analyze processes as if they are in steady-state. Although the process mining spectrum is much broader than just learning process models (see for example conformance checking and model enhancement), process discovery is by far the toughest problem. Discovering end-to-end processes is much more challenging than classical data mining prob-lems such as classification, clustering, regression, association rule learning, and sequence/episode mining.
 Why is process mining such a di cult problem? There are obvious reasons that also apply to many other data mining and machine learning problems, e.g., dealing with noise, con-cept drift, and a complex and large search space. However, there are also some specific problems: To illustrate the challenging nature of process mining we consider the process model shown in Fig. 2. This Petri net models the process that starts with a and ends with d .In-between k activities can occur in parallel. For paral-lel branch i there is choice between b i and c i . The process model is able to generate 2 k k ! di  X  erent traces, i.e., for k =10 there are 3715891200 possible execution sequences. Two c b 7 c 8 b 9 c 10 d . Concurrency and choice typically result in heaps of possible traces. In fact, if there are loops, there are Figure 2: A Petri net with 2 k k !possibleexecutionsequences. potentially infinitely many traces. Hence, it is completely unrealistic to assume that all possible traces will be observed in some event log. Even for smaller values of k and event logs with millions of cases, it is often still unlikely that all possible traces will be seen.
 Fortunately, existing process discovery algorithms do not need to see all possible interleavings to learn a model with concurrency. For example, the classical  X  algorithm can learn the Petri net based on less than 4 k ( k 1) example traces. For the  X  algorithm it is su cient to see all  X  X irect successions X  rather than all  X  X nterleavings X , i.e., if x can be directly followed by y it should be observed at least once. Traditional knowledge discovery techniques are unable to discover the process model shown in Fig. 2. However, for organizations interested in process improvement and compli-ance it is essential to discover the actual processes and these exhibit the control-flow patterns used in Fig. 2. Various management trends related to process improvement (e.g., Six Sigma, TQM, CPI, and CPM) and compliance (SOX, BAM, etc.) can benefit from process mining.
 Therefore, we hope that the manifesto will stimulate the ACM SIGKDD community to think about new techniques for process-centric knowledge discovery. The process mining manifesto can be obtained from http:// www.win.tue.nl/ieeetfpm/ . The manifesto has been trans-lated into Chinese, German, French, Spanish, Greek, Ital-ian, Korean, Dutch, Portuguese, Turkish, and Japanese. The reader interested in process mining is also referred to the recent book on process mining [1]. Also visit www. processmining.org for sample logs, videos, slides, articles, and software. [1] W. van der Aalst. Process Mining: Discovery, Con-[2] IEEE Task Force on Process Mining. Process Min-
