 David Jurgens 1  X  Mohammad Taher Pilehvar 2  X  Roberto Navigli 2 Abstract Semantic similarity has typically been measured across items of approximately similar sizes. As a result, similarity measures have largely ignored the fact that different types of linguistic item can potentially have similar or even identical meanings, and therefore are designed to compare only one type of lin-guistic item. Furthermore, nearly all current similarity benchmarks within NLP contain pairs of approximately the same size, such as word or sentence pairs, preventing the evaluation of methods that are capable of comparing different sized items. To address this, we introduce a new semantic evaluation called cross-level semantic similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence. Our pilot CLSS task was presented as part of SemEval-2014, which attracted 19 teams who submitted 38 systems. CLSS data contains a rich mixture of pairs, spanning from paragraphs to word senses to fully evaluate similarity measures that are capable of comparing items of any type. Furthermore, data sources were drawn from diverse corpora beyond just newswire, including domain-specific texts and social media. We describe the annotation process and its challenges, including a comparison with crowdsourcing, and identify the factors that make the dataset a rigorous assessment of a method X  X  quality. Furthermore, we examine in detail the systems participating in the SemEval task to identify the common factors associated with high performance and which aspects proved difficult to all systems. Our findings demonstrate that CLSS poses a significant challenge for similarity methods and provides clear directions for future work on universal similarity methods that can compare any pair of items.
 Keywords Similarity Evaluation Semantic textual similarity Semantic similarity measures the degree to which two linguistic items have the same meaning. Accurately measuring semantic similarity is an essential component of many applications in natural language processing (NLP), such as ontology learning, thesauri generation, and even machine translation evaluation. Therefore, multiple evaluations have been proposed for testing these computational approaches on their ability to accurately measure similarity, e.g., the RG-65 dataset (Rubenstein and Goodenough 1965 ) or the TOEFL synonym test (Landauer and Dumais 1997 ).
Semantic similarity evaluations have largely focused on comparing similar types of linguistic items. Most recently, a large amount of work has focused on semantic textual similarity (STS) (Agirre et al. 2012 , 2013 , 2014 ), which measures the similarity between similar-sized sentences and phrases. However, other widely-used semantic similarity evaluation datasets have been built around word similarity (Rubenstein and Goodenough 1965 ; Finkelstein et al. 2001 ) and associativity (Finkelstein et al. 2001 ); and, furthermore, a few works have proposed datasets based on identifying similar-dissimilar distinctions between a word X  X  senses (Snow et al. 2007 ; Navigli 2006 ; Kilgarriff 2001 ). Notably, all of these evaluations have focused on comparisons between similar types of entity, e.g., comparing words, in contrast to the uses of semantic similarity in applications such as summarization and compositionality which compare entities of different sizes, e.g., measuring the similarity between a multi-word expression X  X  meaning and a single word X  X  meaning. 1
To address this broader class of semantic similarity comparisons between textual items of different sizes, we introduce a new evaluation where similarity is measured between items of five different types: paragraphs, sentences, phrases, words and senses. Given an item of the lexically-larger type, a system is tasked with measuring the degree to which the meaning of the larger item is captured in the smaller type, e.g., comparing a paragraph to a sentence. We refer to this task as cross-level semantic similarity (CLSS). Our pilot CLSS task was presented as part of SemEval-2014 (Jurgens et al. 2014 ).

A major motivation of this task is to produce semantic similarity systems that report meaningful similarity scores for all types of input, thereby freeing downstream NLP applications from needing to consider the type of text being compared. For example, CLSS measures the extent to which the meaning of the sentence  X  X  X o u know where i can watch free older movies online without download? X  X  is captured in the phrase  X  X  X treaming vintage movies for free, X  X  or how similar  X  X  X ircumscribe X  X  is to the phrase  X  X  X eating around the bush. X  X  Furthermore, by incorporating comparisons of a variety of item sizes, the evaluation unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality.

Because CLSS generalizes STS to items of different type, successful CLSS systems can directly be applied to all STS-based applications. Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (Specia et al. 2012 ), keyphrase identification (Kim et al. 2010 ), lexical substitution (McCarthy and Navigli 2009 ), summarization (Spa  X  rck Jones 2007 ), gloss-to-sense mapping (Pilehvar and Navigli 2014b ), and modeling the semantics of multi-word expressions (Marelli et al. 2014 ) or polysemous words (Pilehvar and Navigli 2014a ).

The proposed CLSS task was designed with three main objectives. First, the task should include multiple types of comparison in order to assess each type X  X  difficulty and whether specialized resources are needed for each. Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types. Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text-and sense-based similarity methods within a single framework. Effectively measuring semantic similarity is a long-standing objective of NLP and related fields, with most work focusing on datasets that measure the similarity of the same type of linguistic item (e.g., words or sentences). Most related to the CLSS objective are the works on Semantic Textual Similarity for phrases and sentences. Dolan et al. ( 2004 ) and Li et al. ( 2006 ) initially proposed two sentence similarity datasets focused on evaluating the quality of paraphrases, containing on the order of a hundred pairs for comparison. Most recently, STS tasks proposed as a part of SemEval have provided relatively large training and tests sets primarily for sentence similarity, with some phrases also included (Agirre et al. 2012 , 2013 , 2014 ). In these three tasks, sentence pairs were rated on a scale from zero (completely unrelated) to five (semantically identical). Sentence pairs were drawn from multiple corpora primarily consisting of paraphrases, newswire, and video descriptions. Notably, because of differences in source corpora, similarity rating distributions were not evenly distributed across the rating scales for each corpus.

The data used by these STS evaluations differs in two key ways from that used in the SemEval-2014 CLSS task (hereafter, CLSS-2014). First, with the exception of the 2014 STS task, which included social media text from Twitter, the source domains have focused on a small number of text genres that are widely supported by NLP tools. In contrast, our CLSS data includes data from a diverse selection of genres including (1) social media genres that are likely to contain numerous spelling and grammatical mistakes, (2) genres that include idiomatic or metaphoric language, and (3) domain-specific genres that include phrases or words not present in many semantic resources. Although humans have little difficulty in rating similarity in these various genres (cf. Sect. 4.6 ), these genre differences create a more challenging evaluation setting for computational approaches and ensure that system performance is likely to generalize across a large number of domains.
The second key difference in the data of our CLSS task from the STS tasks is in the sizes of items being compared. Table 1 illustrates this by showing the average lengths of source and target items being compared, in terms of their number of content words. Nearly all pairs used in STS tasks have identical sizes (a ratio close to 1.0), with the exception of the FNWN subset from STS-2013 which is obtained from the definitions of manually mapped sense pairs of FrameNet 1.5 and WordNet 3.1 where the average gloss length is about three times larger in the former sense inventory. 2 In contrast, the average pair size ratio is about three in our datasets, ranging from 2.7 (sentence-to-phrase test set) to 4.1 (phrase-to-word training set). In addition to the size difference, the pairs in the STS datasets are different to those in the CLSS datasets as the two sides belong to the same lexical level in the former (usually a sentence) whereas in the latter they belong to two different lexical levels, e.g., sentence and phrase, which yield different syntactic structures.

Work on semantic similarity resources has also focused on comparing word meanings, which closely relates to the phrase-to-word similarity judgments of our CLSS task. Rubenstein and Goodenough ( 1965 ) propose a dataset of 65 word pairs rated by 51 human annotators on a scale from 0 X 4 for their similarity. Finkelstein et al. ( 2001 ) created the widely-used WordSim-353 dataset; however, the annota-tion process for this dataset conflated similarity and relatedness, leading to high similarity scores for pairs such as computer-keyboard or smart-stupid despite the dissimilarity in their meanings. Separating similarity from relatedness is essential for many semantic tasks, such as recognizing synonyms or appropriate paraphrases. To correct the conflation, Agirre et al. ( 2009 ) partitioned the WordSim-353 dataset into two subsets: one containing related pairs and a second containing similar pairs; however, as Hill et al. ( 2014 ) note, evaluation with these datasets is still problematic as (1) the annotation values were not gathered specific to similarity alone, so the rating values are difficult to interpret, and (2) the evaluation of a similarity-measuring system also required testing that the system does not highly rate the pairs in the relatedness-based dataset. Most recently, Hill et al. ( 2014 ) propose a new dataset, SimLex-999, which uses a revised annotation procedure to elicit only similarity judgments.

Our CLSS comparison differs in two key ways from these word comparison tasks. First, our rating scale (discussed later in Sect. 3 ) explicitly captures the differences between relatedness and synonymy and required annotators and systems to distinguish between the two. Second, our evaluation recognizes that multiword expressions and phrases can be similar or even synonymous with single words, e.g.,  X  X  X  very large, expensive house X  X  and  X  X  X ansion. X  X  Given that a semantic similarity measure is ideally expected to model a phrase as a whole, and not as a combination of the individual models of its constituent words, our framework provides an evaluation benchmark to bridge from semantic representation of individual words to that of phrases.

Word similarity datasets come with an implicit disambiguation where annotators must identify the concepts to which the words refer and compare those. In contrast, a small number of works have used sense-based data to explicitly mark the concepts being compared. Most related to our CLSS work are that of Kilgarriff ( 2001 ) and Navigli ( 2006 ), whose datasets reflect sense similarity judgments on WordNet senses. Unlike word similarity judgments, these two datasets provide only a binary distinction between senses, indicating whether two senses are sufficiently similar that they can be considered identical or whether they are semantically distinct. In contrast to these works, our CLSS datatset compares a word with a sense along a graded similarity scale, capturing a wider range of semantic relationships between the word and sense such as synonymy or topical association. Also related are the works of Erk and McCarthy ( 2009 ), Erk et al. ( 2013 ) and Jurgens and Klapaftis ( 2013 ), who measure applicability of a word sense to a usage, which is analogous to measuring the similarity of the sense to a word in context. In contrast to these judgments, our CLSS datatset compares a word with a sense that is not necessarily a meaning of the word, capturing a broad range of semantic relationships between the two. The SemEval-2014 task on CLSS is intended to serve as an initial task for evaluating the capabilities of systems at measuring all types of semantic similarity, indepen-dently of the size of the text. To accomplish this objective, systems were presented with items from four comparison types: (1) paragraph to sentence, (2) sentence to phrase, (3) phrase to word, and (4) word to sense. Given a pair of items, a system must assess the degree to which the meaning of the larger item is captured in the smaller item. WordNet 3.0 was chosen as the sense inventory (Fellbaum 1998 ). Following previous SemEval tasks (Agirre et al. 2012 ; Jurgens et al. 2012 ), CLSS-2014 recognizes that two items X  similarity may fall within a range of similarity values, rather than having a binary notion of similar or dissimilar. Initially a six-point (0 X 5) scale similar to that used in the STS tasks was considered (Agirre et al. 2012 ); however, annotators found difficulty in deciding between the lower-similarity options. After multiple revisions and feedback from a group of initial annotators, we developed a five-point Likert scale for rating a pair X  X  similarity, shown in Table 2 . 3
The scale was designed to systematically order a broad range of semantic relations: synonymy, similarity, relatedness, topical association, and unrelatedness. Because items are of different sizes, the highest rating is defined as very similar rather than identical to allow for some small loss in the overall meaning. Furthermore, although the scale is designed as a Likert scale, annotators were given flexibility when rating items to use values between the defined points in the scale, indicating a blend of two relations. Table 3 provides examples of pairs for each scale rating for all four comparison types. We use the sense notation of Navigli ( 2009 ) and show the n th sense of the word with part of speech p as word n p .
For each of the levels, the ability to distinguish between the rating scale X  X  points supports multiple types of application, even when distinguishing lower-similarity pairs. For example, separating items rated as 0 from those as 1 can aid in improving topical coherence by removing linguistic items that are too dissimilar from a topic X  X  current content. Further, at the phrase, word, and sense level, distinguishing between items rated 1 from 2 can potentially identify those items with highly-salient semantic relationships (e.g., meronymy) from those items that are just likely to appear in the same topic, thereby aiding taxonomy enrichment. At the paragraph-to-sentence level, distinguishing between a 2 and 3 can aid in multi-document summarization by identifying sentences that are novel and related to the current summary (rating 2) from those that are similar to the existing content (rating 3) and might be redundant. The task X  X  pilot dataset was designed to test the capabilities of systems in a variety of settings. Except for the word-to-sense setting, the task data for all comparison types was created using a three-phase procedure. First, items of all sizes were selected from publicly-available corpora. Second, each of the selected items was used to produce a second item of the next-smaller level (e.g., a sentence inspires a phrase). Third, the pairs of items were annotated for their similarity. Because of the expertise required for working with word senses, the word-to-sense dataset was constructed by the organizers using a separate but similar process. We generated 1000 pairs for each of the four comparison types which are equally distributed among training and test sets. In this Section we first describe the corpora used for the generation of CLSS datasets (Sect. 4.1 ) followed by the annotation process (Sect. 4.2 ). The construction procedure for the word-to-sense comparison type is detailed in Sect. 4.3 . 4.1 Corpora CLSS datasets were constructed by drawing from multiple publicly-available corpora and then manually generating a paired item for comparison. To achieve our second objective for the task, item pairs were created from different corpora that included texts from specific domains, social media, and text with idiomatic or slang language. Table 4 summarizes the corpora and their distribution across the test and training sets for each comparison type, with a high-level description of the genre of the data. We briefly describe the corpora next.

The WikiNews, Reuters 21578, and Microsoft Research (MSR) Paraphrase corpora are all drawn from newswire text, with WikiNews being authored by volunteer writers and the latter two corpora written by professionals. Travel Guides text was drawn from the Berlitz travel guides data in the Open American National Corpus (Ide and Suderman 2004 ) and includes very verbose sentences with many named entities. Wikipedia Science text was drawn from articles tagged with the category Science on Wikipedia. Food reviews were drawn from the SNAP Amazon Fine Food Reviews dataset (McAuley and Leskovec 2013 ) and are customer-authored reviews for a variety of food items. Fables were taken from a collection of Aesop X  X  Fables. The Yahoo! Answers corpus was derived from the Yahoo! An-swers dataset, which is a collection of questions and answers from the Community Question Answering (CQA) site; the dataset is notable for having the highest degree of ungrammaticality in our test set. SMT Europarl is a collection of texts from the English-language proceedings of the European parliament (Koehn 2005 ); Europarl data was also used in the PPDB corpus (Ganitkevitch et al. 2013 ), from which phrases were extracted. Wikipedia was used to generate two phrase datasets from (1) extracting the definitional portion of an article X  X  initial sentence, e.g.,  X  X  X n [article name] is a [definition], X  X  and (2) captions for an article X  X  images. Web queries were gathered from online sources of real-world queries. Last, the first and second authors generated slang and idiomatic phrases based on expressions contained in Wiktionary.

In order to evaluate the ability of the participating systems at generalizing to data from a novel domain, the test dataset in each comparison type included one surprise genre that was not seen in the training data. In addition, we included a new type of challenge genre with Fables; unlike other domains, the sentences paired with the fable paragraphs were potentially semantic interpretations of the intent of the fable, i.e., the moral of the story. These interpretations often have little textual overlap with the fable itself and require a deeper interpretation of the paragraph X  X  meaning in order to make the correct similarity judgment.

Prior to the annotation process, all content was filtered to ensure its size and format matched the desired text type. On average, a paragraph in our dataset consists of 3.8 sentences. Typos and grammatical mistakes in the community-produced content were left unchanged.
 4.2 Annotation process In order to ensure high-quality datasets, a two-phase procedure was used for the generation of all datasets but word-to-sense. Phase 1 deals with the generation of item pairs while ensuring a uniform distribution of items along the similarity scale. In Phase 2 the annotators rate the produced item pairs for their similarity. Phase 1 The goal of this phase is to produce item pairs with an expected uniform distribution of similarity values along the rating scale. To this end, the larger texts that were drawn from different corpora were shown to annotators who were asked to produce the smaller text of the pair at a specified similarity. For instance, an annotator was given the phrase  X  X  X rop a dime X  X  and asked to write the paired word that is a  X  X 3 X  X  rating. The annotator provided  X  X  X nform X  X  for this phrase and the specified similarity value. Annotators were instructed to leave the smaller text blank if they had difficulty understanding the larger text.

The requested similarity ratings were balanced to create a uniform distribution of similarity values. The procedure was only used for the generation of items with 1 X 4 ratings. Unrelated pairs (i.e., with 0 similarity score) were automatically generated by pairing the larger item with an item of appropriate size extracted randomly from the same genre. Annotators were not instructed to intentionally change the surface text to use synonyms or to paraphrase, though they were free to do. While requiring these changes would necessitate that systems use more semantic analysis for comparison instead of string similarity, we intended for performance on the dataset to be representative of what would be expected in the real world; thus, string similarity-based approaches were not implicitly penalized through construction of the dataset.

Four annotators participated in Phase 1 and were paid a bulk rate of  X  110 for completing the work. In addition to the four annotators, the first two authors also assisted in Phase 1: Both completed items from the S CIENTIFIC genre and the first author produced 994 pairs, including all those for the M ETAPHORIC genre, and those that the other annotators left blank.
 Phase 2 Once item pairs were produced for different similarity ratings, they were stripped of their associated scores and were given to annotators for their similarity to be rated. An initial pilot annotation study showed that crowdsourcing did not produce high-quality annotations that agreed with the expert-based gold standard. Furthermore, the texts used in our datasets came from a variety of genres, such as scientific domains, which some workers had difficulty in understanding. While we note that crowdsourcing has been used in prior STS tasks for generating similarity scores (Agirre et al. 2012 , 2013 ), both tasks X  efforts encountered lower worker score correlations on some portions of the dataset (Diab 2013 ), suggesting that crowdsourcing may not be reliable for judging the similarity of certain types of text.
Therefore, to ensure high quality, the first two organizers rated all items independently. Because the sentence-to-phrase and phrase-to-word comparisons contain slang and idiomatic language, a third American annotator was added for those datasets. The third annotator was compensated  X  250 for their assistance.
Annotators were allowed to make finer-grained distinctions in similarity using multiples of 0.25. For all items, when any two annotators disagreed by one or more scale points, we performed an adjudication to determine the item X  X  rating in the gold standard. The adjudication process revealed that nearly all disagreements were due to annotator mistakes, e.g., where one annotator had overlooked a part of the text or had misunderstood the text X  X  meaning. The final similarity rating for an unadjudicated item was the average of its ratings. 4.3 Word-to-sense The word-to-sense dataset was produced in three phases. In Phase 1, we picked words in order to construct the word side of the dataset. Based on the type of their word side, the items in the word-to-sense dataset were put into five categories:  X  Regular The word and its intended meaning are in WordNet. Words in this  X  OOV The word does not exist in the WordNet X  X  vocabulary, e.g., the verb  X  Slang Slang words not already in WordNet were selected from slang  X  OOS The word is in WordNet, but has a novel meaning that is not defined in the  X  Challenge A set of challenge words where one of the word X  X  senses and a second In Phase 2, we first associated each word with a particular WordNet sense for its intended meaning, or the closest available sense in WordNet for OOV or OOS items. To select a comparison sense, a neighborhood search procedure was adopted: All synsets connected by at most three edges in the WordNet semantic network were drawn. Given a word and its neighborhood, the corresponding sense for the item pair was selected by matching the sense with an intended similarity value for the pair, much like how text items were generated in Phase 1. The reason behind using this neighborhood-based selection process was to minimize the potential bias of consistently selecting lower-similarity items from those further away in the WordNet semantic network.

In Phase 3, the annotators were provided with the definitions for the word X  X  intended meaning and for the senses for all word-sense pairs and asked to rate each pair according to the rating scale. Definitions were drawn from WordNet or from Wiktionary, if the word was OOV or OOS. Annotators had access to WordNet for the compared sense in order to take into account its hypernyms and siblings. 4.4 Trial data The generation procedure for the trial dataset was similar but on a smaller scale. For this dataset we generated pairs by sampling text from WikiNews and words from WordNet X  X  vocabulary. The smaller side was then manually produced resulting in a total of 156 pairs for the four comparison types. Four fluent annotators were asked to independently rate all items. Inter-annotator agreement rates varied in 0.734 X 0.882, in terms of Krippendorff X  X  a (Krippendorff 2004 ) on the interval scale. 4.5 Dataset OOV analysis A major goal of the CLSS evaluation is to create robust semantic representations of arbitrary-sized texts that are meaningfully comparable. Given the use of WordNet in one comparison level (i.e., word to sense), we anticipated that WordNet might serve as either a common representation across levels or as a semantic resource for comparing items. However, WordNet is limited in the number of word forms it contains and often omits many technical or jargon terms. Therefore, we analyzed the percentages of words in each level that are not present in the vocabulary of WordNet 3.0. These OOV words present a significant challenge for WordNet-based similarity systems which must find alternate ways of representing and comparing such words.

Table 5 shows the percentage of content words 6 in the CLSS datasets that do not exist in WordNet 3.0 X  X  vocabulary, for different genres and for different comparison types in the training and test sets. Travel, CQA, and Newswire are the genres with most WordNet OOV percentage in the paragraph-to-sentence and sentence-to-phrase comparison types. These genres are characterized by their high number of named entities, words that are less likely to exist in the WordNet X  X  vocabulary (cf. Sect. 4.1 ). The OOV percentage is relatively balanced across the two sides in different genres in these two comparison types (i.e., paragraph to sentence and sentence to phrase). Exceptions are Idiomatic and Slang in which the larger side tends to have a higher percentage of its content words not defined in WordNet.
Specifically, in the paragraph to sentence datasets, on average, 8.9 and 7.2 % of words are WordNet OOV in training and test sets, respectively. The mean OOV percentage value in this comparison type ranges from 1.2 for the Metaphoric genre to 11.48 for the Travel genre, both in the test set. Sentence to phrase datasets have 7.4 and 6.8 % of their words not defined in WordNet which is slightly lower in comparison to the paragraph to sentence type.

In the phrase-to-word datasets, about 6.5 % of words are not covered in WordNet in both training and test sets. It is notable that, in this comparison type, about a quarter of content words in the phrase side of the Descriptive genre are not defined in WordNet, denoting the high number of named entities in the image captions of Wikipedia, a resource from which these phrases have been obtained.

Finally, in the word-to-sense training set, 12.2 % of WordNet senses are paired with words that are not defined in the same sense inventory. This figure rises to more than 19 % in the test set. All these WordNet OOV words in the word-to-sense datasets belong to either OOV or slang type. 4.6 Dataset discussion Our multi-phase annotation procedure proved to result in high-quality datasets. Table 6 reports the inter-annotator agreement (IAA) statistics for each comparison type on both the full and unadjudicated portions of the dataset. IAA was measured using Krippendorff X  X  a for interval data. Because the disagreements that led to lower a in the full data were resolved via adjudication, the quality of the full dataset is expected to be on par with that of the unadjudicated data. 7 The annotation quality for our datasets was further improved by manually adjudicating all significant disagreements.

In contrast, the datasets of current STS tasks aggregated data with moderate inter-annotators correlation (Diab 2013 ); the inter-rater Pearson correlation varied between 0.530 X 0.874, 0.377 X 0.832, and 0.586 X 0.836 for different datasets in STS-2012 (Agirre et al. 2012 ), STS-2013 (Agirre et al. 2013 ), and STS-2014 (Agirre et al. 2014 ), respectively. However, we note that Pearson correlation and Krippendorff X  X  a are not directly comparable (Artstein and Poesio 2008 ), as annotators X  scores may be correlated, but completely disagree.

In addition, thanks to the two-phase procedure used for the construction of CLSS datasets, the similarity scores in these datasets are evenly distributed across the rating scale, shown in Fig. 1 as the distribution of the values for all datasets. However, we note that this creation procedure was very resource-intensive and, therefore, semi-automated or crowdsourcing-based approaches for producing high-quality data will be needed in future CLSS-based evaluations. Nevertheless, as a pilot task, the manual effort was essential for ensuring a rigorously-constructed dataset. 4.7 Crowdsourcing replication After our initial tests using crowdsourcing produced unsatisfactory similarity ratings, the annotation process for CLSS-2014 used trained experts, which resulted in a large bottleneck for scaling the annotation process to larger datasets. However, given crowdsourcing X  X  use in creating other STS datasets (Agirre et al. 2012 , 2013 , 2014 ), after the task had concluded, we performed a partial replication study to compare the expert-based annotations from the test set with crowdsourced similarity ratings. The replication study X  X  goal was to assess three main points: (1) what is the overall degree of worker rating bias on items, (2) how does rating bias vary by genre, and (3) how does rating bias vary by comparison level. Our aim is to identify specific portions of the annotation task that would be suitable to crowdsource.
To quantify the bias, CrowdFlower workers were recruited to annotate 15 items from each genre for all levels except word-to-sense. 8 To control for differences in the items X  similarities, items were balanced across the similarity ratings seen within that genre. Workers were shown an identical set of instructions as the expert annotators, which included examples of pairs at each similarity level.

Annotation tasks included pairs of only a single comparison type. In order to have tasks with similar completion times across levels, workers were shown four pairs per task for paragraph-to-sentence and nine pairs for the two other levels. In all cases, workers were paid $0.10 per task. Three worker judgments were collected per pair. Workers were required to pass a testing phase consisting of correctly rating five pairs within 1 of the gold standard rating; testing phase pairs consisted of a separate set of pairs, gathered in an identical way from the data. For all levels, this testing process removed approximately 40 X 50 % of the initial worker pool from participating further, underscoring the need to control for worker skill level and fluency.

Figure 2 shows the distribution of deviations from the expert ratings as a box-and-whisker plot, revealing that, on average, workers tend to assign higher similarity scores than experts. On the whole, workers more closely matched experts on paragraph-to-sentence comparisons (i.e., had the least scoring variance). Rating texts required reading and understanding the text, which often resulted in workers missing the text X  X  interpretation and rating the fable and its moral as unrelated. I
DIOMATIC and S LANG required familiarity with the expressions; given the wide background of crowdsourced workers, we suspect that these deviations were due to lower fluency for colloquial language (Pavlick et al. 2014 ). Examining genres where workers had a significantly higher similarity rating (e.g., S CIENTIFIC and T
RAVEL ), we find workers were likely to rate two items with higher similarity if they shared named entities in common, regardless of how those entities functioned in the text, suggesting that workers were operating more on the basis of surface similarity than by reading comprehension. As a follow-up analysis, worker agreement per genre was measured using Krippendorff X  X  a for interval ratings. Table 7 reveals that the resulting agreement rates are largely below those considered acceptable for high-quality data (e.g., 0.8) and decrease as workers compare shorter pairs of text. However, one notable exception is the agreement forNewswire pairs atthe paragraph-to-sentence level, which had an a of 0.874. While this agreement approaches that of the expert annotators, the worker X  X  mean similarity rating per item for this data was 0.43 points higher on average than that of experts, as shown in Fig. 2 . Thus, while workers have high agreement, they are agreeing on an answer that differs from the expert judgment. Furthermore, for the remaining comparison levels, the genres with highest agreement in the other two levels also have worker-based ratings that are higher than those of experts, indicating that increased worker agreement does not correspond to increased accuracy.

The results of the replication study suggest that directly crowdsourcing ratings would encounter four main challenges. First, crowdsourced workers did not have consistent ratings, with many genresseeing inflated similarity values. Second, workers encountered difficulties when rating texts such as idioms, which require native fluency to comprehend. Third, workers did not appear to spend sufficient effort to understand the text and instead relied on surface similarity, leading to higher variance in the ratings when text comprehension was required or when a pair X  X  items had text in common but the two items had very different meanings. Fourth, even when workers do have high agreement on the similarity, the ratings did not match experts X , indicating that examining IAA alone is insufficient for assessing dataset quality. Together, these findings suggest that crowdsourcing is not readily feasible as an alternative strategy for gathering CLSS similarity rating annotations for any level unless the process can be further adapted to control for worker ability and annotation quality. 5.1 Participation The ultimate goal of the CLSS-2014 pilot task is to benchmark systems that can measure similarity for multiple types of items. Therefore, we strongly encouraged participating teams to submit systems that were capable of generating similarity judgments for multiple comparison types. However, to further the analysis, participants were also permitted to submit systems specialized to a single domain. Teams were allowed at most three system submissions, regardless of the number of comparison types supported. 5.2 Scoring Systems were required to provide similarity values for all items within a comparison type. However, systems were allowed to produce optional confidence values for each score, reflecting their certainty in the item X  X  rating. In practice, few systems reported confidence scores, so we omit further discussion. Following prior STS evaluations, systems were scored for each comparison type using Pearson correlation. Additionally, we include a second score using Spearman X  X  rank correlation, which is only affected by differences in the ranking of items by similarity, rather than differences in the similarity values. Pearson correlation was chosen as the official evaluation metric since the goal of the task is to produce similar scores to those made by humans. However, Spearman X  X  rank correlation provides an important metric for assessing systems whose scores do not match human scores but whose rankings might, e.g., string-similarity measures. Ulti-mately, a global ranking was produced by ordering systems by the sum of their Pearson correlation values for each of the four comparison levels. 5.3 Baselines String similarity measures have provided competitive baselines for estimating the official baseline system was based on the longest common substring (LCS) measure, normalized by the length of items using the method of Clough and Stevenson ( 2011 ). Given a pair, the similarity is reported as the normalized length of the LCS. In the case of word-to-sense, the LCS for a word-sense pair is measured between the sense X  X  definition in WordNet and the definitions of each sense of the pair X  X  word, reporting the maximal LCS. Because OOV and slang words are not in WordNet, the baseline reports the average similarity value of non-OOV items. Baseline scores were made public after the evaluation period ended.

Because LCS is a simple procedure, a second baseline based on greedy string tiling (GST) (Wise 1996 ) was also added. Unlike LCS, GST accounts for transpositions of tokens across the two texts and can still report high similarity when encountering reordered text. The minimum match length for GST was set to 6. Nineteen teams submitted 38 systems. Of those systems, 34 produced values for paragraph-to-sentence and sentence-to-phrase comparisons, 22 for phrase-to-word, and 20 for word-to-sense. Two teams submitted revised scores for their systems after the deadline but before the test set had been released. These systems were scored and noted in the results but were not included in the official ranking. Table 8 shows the performance of the participating systems across all the four comparison types in terms of Pearson correlation. The two right-most columns show system rankings by Pearson (official rank) and Spearman X  X  ranks correlation.

The SimCompass system attained first place, partially due to its superior performance on phrase-to-word comparisons, providing an improvement of 0.10 over the second-best system. The late-submitted version of the Meerkat Mafia pairingWords system corrected a bug in the phrase-to-word comparison, which ultimately would have attained first place due to large performance improvements over SimCompass on phrase-to-word and word-to-sense. ENCU and UNAL-NLP systems rank respectively second and third while the former being always in top-4 and the latter being among the top-7 systems across the four comparison types. Most systems were able to surpass the naive LCS baseline; however, the more sophisticated GST baseline (which accounts for text transposition) outperforms two thirds of the systems. Importantly, both baselines perform poorly on smaller text, highlighting the importance of performing a semantic comparison, as opposed to a string-based one.

Within the individual comparison types, specialized systems performed well for the larger text sizes. In the paragraph-to-sentence type, the run1 system of UNAL-NLP provides the best official result, with the late RTM-DCU run1 system surpassing its performance slightly. Meerkat Mafia provides the best performance in sentence-to-phrase with its SuperSaiyan system and the best performances in phrase-to-word and word-to-sense with its late pairingWords system. 6.1 Systems analysis Systems adopted a wide variety of approaches for measuring similarity, partly due to the different levels in which they participated. Table 9 summarizes the major resources and tools used by each system. Three main trends emerge. First, many systems benefited by combining the outputs of multiple similarity methods, with the corpus-based distributional similarity being the most common approach. Among the top-5 systems, three, i.e., SemantiKLUE, ECNU, SimCompass, used different classes of similarity measures such as distributional, knowledge-based, and string-based. Knowledge-based measures such as Lin ( 1998 ), that view WordNet as a semantic graph and measure similarity based on the structural properties of this graph, have been used in all the three systems. As for the distributional similarity measures, ECNU and SemantiKLUE used the conventional count-based models whereas SimCompass benefited from the more recent predictive model of word embeddings (Mikolov et al. 2013 ). The two systems of UNAL-NLP are the only ones that benefit from only one class of similarity measures, i.e., string similarity. The systems performed surprisingly well considering the fact that they only utilize a set of simple string-similarity features based on soft cardinality (Jimenez et al. 2010 ). Interestingly, the UNAL-NLP run1 system that ranked fifth does not also use any machine learning procedure for training, mirroring the potential for unsuper-vised semantic similarity measured seen in the recent work of Sultan et al. ( 2014 , 2015 ).

Second, many systems modeled word senses using their textual definitions, rather than representing them using their structural properties in WordNet. In fact, in the word-to-sense comparison type, all the top-5 systems use WordNet sense inventory to transform a word sense to a textual item given by the corresponding synset X  X  definition. This transformation permits the systems to model a word sense in the same manner they do for longer textual items such as phrases and sentences. Further, given the limited text in these definitions, many systems enriched the definition by including additional text from the neighboring senses or from other lexical resources. For example, the MeerkatMafia-pairingWords system used WordNik 9 in order to tackle the problem of uncovered WordNet OOV words. WordNik is a compilation of several dictionaries such as The American Heritage Dictionary and Wiktionary. As a result of this addition, the system attains the best performance in the word-to-sense comparison type whereas the second best performance of the system is in the paragraph-to-sentence type where it ranks no better than 10th. Given that many of the other top-performing systems have superior performance to MeerkatMafia-pairingWords for large texts but used only WordNet for glosses and synonyms, the addition of expanded semantic taxonomies for covering OOV words may provide a significant performance improvement.
 Alternatively, Jurgens and Pilehvar ( 2015 ) show that using C ROWN , an extension of WordNet with Wiktionary content, results in a large CLSS performance benefit to off-the-shelf WordNet-based techniques on the word-to-sense subtask simply due to the presence of OOV terms in the resource.

Third, high performance on the phrase-to-word and word-to-sense comparisons requires moving beyond textual comparison, which is most clearly seen in the purely string-based UNAL-NLP systems, which both perform in the top six systems for the larger levels but whose ranks drop significantly when comparing smaller items. Indeed, systems that included additional lexical resources and those using distributional models at the word level tended to do better on average, though no clear trend emerges in which resources to use. 6.2 Comparison-type analysis Performance across the comparison types varied considerably, with systems performing best on comparisons between longer textual items. As a general trend, both the baselines X  and systems X  performances tend to decrease with the size of linguistic items in the comparison types. A main contributing factor to this is the reliance on textual similarity measures (such as the baselines), which perform well when two items may share content. However, as the items X  content becomes smaller, e.g., a word or phrase, the textual similarity does not necessarily provide a meaningful indication of the semantic similarity between the two. This performance discrepancy suggests that, in order to perform well, CLSS systems must rely on comparisons between semantic representations rather than textual representations (see also Pilehvar and Navigli 2015 for further analysis). The two top-performing systems on these smaller levels, Meerkat Mafia and SimCompass, used additional resources beyond WordNet to expand a word or sense to its definition or to represent words with distributional representations. 6.3 Per-genre results and discussion The CLSS-2014 task includes multiple genres within the dataset for each comparison type. Figure 3 shows the correlation of each system for each of these genres, with systems ordered left to right according to their official ranking in Table 8 . An interesting observation is that a system X  X  official rank does not always match the rank from aggregating its correlations for each genre individually. This difference suggests that some systems provided good similarity judgments on individual genres, but their range of similarity values was not consistent between genres, leading to a lower overall Pearson correlation. For instance, in the phrase-to-word comparison type, the aggregated per-genre performance of Duluth-1 and Duluth-3 are among the best whereas their overall Pearson performance puts these systems among the worst-performing ones in the comparison type.

Among the genres, CQA, S LANG , and I DIOMATIC prove to be the more difficult for systems to interpret and judge. These genres included misspelled, colloquial, or slang language which required converting the text into semantic form in order to meaningfully compare it. Furthermore, as expected, the M ETAPHORIC genre was the most difficult, with no system performing well; we view the M ETAPHORIC genre as an open challenge for future systems to address when interpreting larger text. On the other hand, S CIENTIFIC ,T RAVEL , and N EWSWIRE tend to be the easiest genres for paragraph-to-sentence and sentence-to-phrase. All three genres tend to include many named entities or highly-specific language, which are likely to be more preserved in the more-similar paired items. Similarly, D ESCRIPTIVE and S EARCH genres were easiest in phrase-to-word, which also often featured specific words that were preserved in highly-similar pairs. In the case of word-to-sense, R EGULAR proves to be the least difficult genre. Interestingly, in word-to-sense, most systems attained moderate performance for comparisons with words not in WordNet (i.e., OOV) but had poor performance for slang words, which were also OOV. This difference suggests that systems could be improved with additional semantic resources for slang. 6.4 Spearman rank analysis Although the goal of CLSS-2014 is to have systems produce similarity judgments, some applications may benefit from simply having a ranking of pairs, e.g., ranking summarizations by goodness. The Spearman rank correlation measures the ability of systems to perform such a ranking. Surprisingly, with the Spearman-based ranking, the Duluth1 and Duluth3 systems attain the third and fifth ranks X  X espite being among the lowest ranked with Pearson. Both systems were unsupervised and produced similarity values that did not correlate well with those of humans. However, their Spearman ranks demonstrate the systems ability to correctly identify relative similarity and suggest that such unsupervised systems could improve their Pearson correlation by using the training data to tune the range of similarity values to match those of humans. Nevertheless, Pearson correlation is still an important evaluation metric since it requires systems to judge the similarity of a pair independently from all other pairs, which is essential when a single, arbitrary pair X  X  similarity value is needed as a feature in further applications. 6.5 Analysis of held-out genres The test datasets for the three text-based levels featured at least one text genre not seen in the training data (cf. Table 4 ). These held-out genres provide a way to assess the generalizability of a system to novel text styles. The paragraph-to-sentence and sentence-to-phrase test sets contained data from the S CIENTIFIC genre, which was gathered from Wikipedia articles marked with scientific categories and frequently featured jargon or domain-specific terminology. The sentence-to-phrase and phrase-to-word test sets contained texts from the S LANG genre, whose texts were designed to include many colloquial expressions or slang usages of common words.

Performance on the surprise genres differed, with systems having higher Pearson correlation with the gold standard on S CIENTIFIC text than with the observed genres, while lower correlation on S LANG text. Figure 4 shows the relative differences in each system X  X  performance on the unseen genres versus performance on genres observed in the training data. An analysis of the texts for the S CIENTIFIC genre revealed that its performance was improved due to the presence of jargon and domain-specific terms (Fig. 4 a); because these terms are difficult to summarize, similar pairs tend to contain identical jargon terms in both texts. As a result, string similarity measures provide a more accurate estimation of semantic similarity than with other genres. In contrast to S CIENTIFIC texts, S LANG pairs often have little string resemblance to one another. As a result, a meaningful semantic comparison cannot be performed using string similarity and requires comparing alternate representa-tions. A major challenge therefore is to have lexical resources that correctly recognize and represent the slang usage of the word or phrase. We found that many systems did not include special resources for slang text and therefore were unable to recognize and compare these texts meaningfully, resulting in lower scores (Fig. 4 c). The success of this pilot task in CLSS provides several avenues for future work. First, the current evaluation is only based on comparing similarity scores, which omits information on why two items are similar. In a future extension, we plan to develop a complementary subtask based on semantic alignment where systems must identify the portions of a pair X  X  items that are semantically similar and the cause of their degree of similarity (e.g., synonymy, slang paraphrase).

Second, the methods developed for the CLSS task are intended to have practical utility for other NLP tasks, such as summarization. Therefore, in future versions of the task, we plan to include an application-based evaluation where a CLSS system X  X  similarity scores on the pairs in the test set are used in a downstream application (e.g., used to rank the quality of summaries) and the CLSS system is evaluated based on how well the application performed. 10
Third, CLSS-2014 included a variety of corpora, which revealed notable differ-ences in the capabilities of systems. In particular, systems performed worst on (1) informal texts, such as those from CQA and those containing slang and idioms and (2) on M ETAPHORIC comparison that require deeper semantic interpretation. In future work, we plan to develop CLSS datasets targeting these two particular aspects. The first dataset will use informal texts such as microtext and email where the medium lends itself to more lexically-compressed writing style. The second will focus on comparison between news stories and their analytical summaries, which may be thematic interpretations of the story content.

Fourth, a key objective will be to develop annotation methodologies that do not require expert intervention. The current annotation process proved time-intensive which prevented the creation of larger datasets. Furthermore, our pre-task investigations and later replication study (Sect. 4.7 ) showed that crowdsourcing using rating-scale questions did not produce annotations of sufficiently high quality. Therefore, we plan to investigate further adapting the annotation task to the crowdsourced setting, such as requiring workers to explicitly comment on why two items are similar; furthermore, we plan to pursue annotation using the video-game annotation methods (Vannella et al. 2014 ; Jurgens and Navigli 2014 ), which have proven highly successful for other linguistic annotations. This paper introduces a new semantic similarity task, Cross-Level Semantic Similarity, for measuring the semantic similarity of linguistic items of different sizes. Using a multi-phase annotation procedure, we have produced a high-quality dataset of 4000 items drawn from various genres, evenly-split between training and test with four types of comparison: paragraph-to-sentence, sentence-to-phrase, phrase-to-word, and word-to-sense. The task was organized as a part of SemEval-2014 and 19 teams submitted 38 systems, with most teams surpassing the baseline system and several systems achieving high performance in multiple types of comparison. However, a clear performance trend emerged where many systems perform well only when the text itself is similar, rather than its underlying meaning. Nevertheless, the results of CLSS-2014 are highly encouraging and point to clear future objectives for developing CLSS systems that operate more on semantic representations rather than text. All task data and resources are available at http://alt.qcri.org/semeval2014/task3/ .
 References
