 Today, more and more product reviews become available on the Internet, e.g., product review forums, discussion groups, and Blogs. However, it is almost impossible for a customer to read all of the different and possibly even contradictory opinions and make an informed decision. Therefore, mining online reviews (opinion mining) has emerged as an inter-esting new research direction. Extracting aspects and the corresponding ratings is an important challenge in opinion mining. An aspect is an attribute or component of a product, e.g.  X  X creen X  for a digital camera. It is common that review-ers use different words to describe an aspect (e.g.  X  X CD X ,  X  X isplay X ,  X  X creen X ). A rating is an intended interpretation of the user satisfaction in terms of numerical values. Reviewers usually express the rating of an aspect by a set of sentiments, e.g.  X  X lurry screen X . In this paper we present three proba-bilistic graphical models which aim to extract aspects and corresponding ratings of products from online reviews. The first two models extend standard PLSI and LDA to generate a rated aspect summary of product reviews. As our main contribution, we introduce Interdependent Latent Dirichlet Allocation (ILDA) model. This model is more natural for our task since the underlying probabilistic assumptions (in-terdependency between aspects and ratings) are appropriate for our problem domain. We conduct experiments on a real life dataset, Epinions.com, demonstrating the improved ef-fectiveness of the ILDA model in terms of the likelihood of a held-out test set, and the accuracy of aspects and aspect ratings.
 H.3.3 [ Information Search and Retrieval ]: Text Min-ing; G.3 [ Mathematics of Computing ]: Probability and Statistics X  statistical computing, multivariate statistics Algorithms, Design, Experimentation opinion mining, probabilistic graphical models, variational methods, aspect identification, rating prediction
Other people X  X  opinions have always been an important piece of information during the decision-making process when buying a new product [13]. Today people like to make their opinions available to strangers via the Internet. As a re-sult, the Web has become an excellent source for gathering consumer opinions [8]. Amazon, Cnet, ZDnet, Rateitall and Epinions are examples of the most important Web resources containing such opinions. However, reading all product re-views to make a good decision will be a time-consuming job. Therefore, mining online product reviews (opinion mining) has emerged as a new research direction.

Some of the review websites ask reviewers to express an overall rating (as stars) for the reviewed item. While these ratings can be helpful in decision making, focusing on just overall ratings may not be sufficient for a user to make de-cisions. Rated aspect summarization addresses this limita-tion [10]. Providing aspects (also called product-features) and the corresponding ratings of a product does not only help users gain more insight into the quality of the product, but also enables them to compare different products. The problem definition is illustrated in Figure 1.
 Rated aspect summarization consists of two tasks [18]. The first is aspect identification with the goal of finding a set of relevant aspects for the target product. The second task is rating prediction with the goal of providing the user a numeric rating for each aspect. Most of the current works consider aspect identification as main task and treat rating prediction at most as a side task. However, from the user X  X  point of view, having the ratings and aspects together is important in decision making.

During the last decade, many methods have been pro-posed to detect product aspects in reviews, including statis-tical approaches and model based techniques. The existing statistical approaches [7, 9, 12, 14] usually apply some con-straints on high-frequency noun phrases to identify product aspects. One of the limitations of these methods is that they may produce too many non-aspects and miss low-frequency aspects and their variations [5]. In addition, statistical ap-proaches require the manual tuning of various parameters which makes them hard to port to another dataset. Exist-ing model based techniques [19, 22, 10, 20] overcome the limitations of statistical approaches by automatically learn-ing the model parameters from the data. However, all of the existing models perform aspect identification and rat-ing prediction in separate steps leading to the accumulation of errors. For example, a separate rating prediction algo-rithm will rate the sentiment  X  X ong X  equally for the aspects  X  X attery life X  and  X  X hutter lag X , although  X  X ong X  expresses a positive opinion for  X  X attery life X  but a negative opinion for  X  X hutter lag X .

Furthermore, most of the current works [19, 18, 11, 5, 20] use the bag-of-words representation of reviews. As shown in [19], representing a document as a mixture of latent top-ics which generate all words of the document, can mainly be used in document clustering, by finding an overall topic of each document. However, in rated aspect summarization problem, the goal is not to cluster reviews, but to identify as-pects and their corresponding ratings. For example, a topic modeling method applied to a collection of digital camera reviews is likely to infer some overall topics for that collec-tion, such as  X  X ony digital camera X , and  X  X eviews of Sony X . Though these are valid topics, they do not represent prod-uct aspects. A rated aspect summarization model, on the other hand, tries to infer product aspects, such as  X  X oom X  and  X  X attery life X , from the same collection of reviews. A solution that has been proposed in [10] is to first preprocess the reviews (chunk them into opinion phrases containing of a head term and a sentiment) and then learn models that generate only opinion phrases, not all the words of a review.
In this paper, we propose a series of increasingly sophis-ticated probabilistic graphical models to jointly identify as-pects and predict their ratings from online reviews. The first model is an extension of the PLSI model proposed in [10], and the second model is extending the standard LDA to generate a rated aspect summary of product reviews. We consider these two models as baselines. As our main con-tribution, we introduce the Interdependent Latent Dirich-let Allocation (ILDA) model. We argue that ILDA is most natural for our problem since the underlying probabilistic assumptions (interdependency between aspects and ratings) are appropriate for the problem domain.

Since all of our proposed models are based on the opin-ion phrases representation of reviews, and no benchmark dataset of opinion phrases is publicly available, we crawled the well-known reviewing website Epinions.com and built a new dataset containing 29,609 opinion phrases in 2,483 reviews from 5 categories. This data set has been made publicly available for research purposes 1 . We conduct ex-periments on this real life dataset and evaluate the perfor-mance of the proposed models according to various criteria: likelihood of the held-out test set, and accuracy of aspect identification and rating prediction.

The remainder of the paper is organized as follows. The next section is devoted to related work. Section 3 introduces the problem statement and discusses our contributions. Sec-tion 4 presents three probabilistic graphical models for the considered problem. Section 5 describes the inference algo-rithms for our proposed models. In Section 6 we report the results of our experimental evaluation. Section 7 concludes the paper with a summary and the discussion of future work.
Most of the current works on opinion mining have focused on the aspect identification task, and ignored or treated rat-ing prediction as a side problem. However, there are several lines of related work which we will review in this section. We first review some models based on the bag-of-words rep-resentation of reviews and then describe a recent work to learn a model from preprocessed reviews (i.e. from opinion phrases).

The authors of [22] assume that words in the same context have similar semantic association. They propose two alter-native latent semantic association (LaSA) models for iden-tifying aspects from reviews. The first LaSA model groups words into a set of aspects according to their context in the reviews. Given a word t i , its context is defined to be com-posed of all the context units (adjacent sentiments) around t in the corpus. The second LaSA model groups words ac-cording to their latent semantic structures and context in the review. Given a latent aspect a i , its semantic structure is defined to be composed of the context units of all the words generated by a i .

Guo et al. [5] present a probabilistic graphical model for modeling the page-independent content information and the page-dependent layout information of text fragments. One characteristic of their model is that an aspect can be dis-covered based on the layout format. The authors show that content and layout information can collaborate and improve the performance of aspect identification. However, they ig-nore the rating prediction task.

In [20] the authors assume that for each review an overall rating and k aspects of the reviewed item are given as input. They first use a boot-strapping algorithm to obtain more related words for each aspect, and then assign each sentence of the review to the aspect whose set of related words has the maximum overlap with that sentence. As their main contribution, they propose a latent rating regression (LRR) model to infer ratings and weights of the given aspects for each review. LRR assumes the overall rating is generated based on the weighted combination of the latent ratings over all the aspects. While they consider the rating prediction task, they do not perform aspect identification and assume that the aspects are given.

Titov et al. in [19] propose a model, based on LDA, for modeling two types of topics in reviews: global topics and local topics. They assumed global topics correspond to a global property of the product in the review, such as its brand, and local topics correlate with the product aspects. h ttp://www.sfu.ca/ sam39/ILDA/ Ho wever, they conclude that representing a document as a mixture of latent topics which generate all words of the doc-ument, is not a good choice for the rated aspect summariza-tion problem since the extracted topics are very general. In [18] the same authors extend their proposed model to find the correspondence between the extracted topics and the product aspects. The second model introduces a set of clas-sifiers (sentiment predictors) for each aspect, which is used to find the correspondence between topics and aspects. Note that none of their models generates the rating of aspects.
Recently, Lu et al. [10] show that using preprocessed re-views can improve the ability of models in identifying as-pects. The authors assume that each review can be parsed into opinion phrases of the format &lt; headterm,sentiment &gt; , and propose a probabilistic model based on PLSI to identi-fies major aspects of a product by clustering the head terms. Since our proposed models use preprocessed reviews as in-put, this method will be the only comparison partner for our work.

There is yet another line of the research in text mining which tries to model the mixture of topics in documents [2, 17, 21]. However, none of these works has tried to model the sentiment related to the topics, thus cannot be applied to our problem. There are also some recent works on sentiment classification which all use some external knowledge (in the form of word lists [23] or training examples [11]) to distin-guish positive and negative polarities. Our work is focused more on solving the rated aspect summarization problem in a general way and we propose general models to both de-composing a review into a set of rated aspects and predicting ratings for the identified aspects.

As we mentioned before, there are also some statistical approaches [7, 9, 12, 14] to detect product aspects in reviews. These approaches usually apply some constraints on high-frequency noun phrases to identify product aspects. The main limitation of these methods is that they may produce too many non-aspects and miss low-frequency aspects and their variations [5].
In this section we introduce the problem statement, pro-vide a motivating example for our problem, and discuss our contributions for the considered problem.
Let P = f P 1 ,P 2 ,...,P l g be a set of products which can be from different categories, like  X  X pple Smartphone X ,  X  X anon Powershot digital camera X ,  X  X enon DVD player X , etc. For each product P i there is a set of reviews R i = f d 1 ,d d g . Each review d j consists of a set of opinion phrases, such as  X  X reat zoom X ,  X  X xcellent quality X , etc. In the following, we define the problem addressed more formally.

Aspect : An aspect is an attribute or component of the product that has been commented on in a review. For ex-ample,  X  X attery life X  in the opinion sentence  X  X he battery life of this camera is too short X .

Rating : A rating is an intended interpretation of the user satisfaction in terms of numerical values. Most of the reviewing websites use ratings (number of stars) in the range from 1 to 5.

Opinion Phrase : An opinion phrase f = &lt; t,s &gt; is a pair of head term t and sentiment s [10]. Usually the head term is an aspect, and the sentiment expresses some opinion towards this aspect, e.g. &lt; battery life , short &gt; .
Review : A review is a bag of opinion phrases d j = f f = &lt; t,s &gt; j f 2 d j g .
 Problem Definition : Given a set of reviews for product P , the task is to identify the k major aspects of P and to predict the rating of each aspect.

Since different products in a category share many similar aspects, it would also make sense to identify the k major aspects of a product category. However, many products have some unique aspects which usually play an important role in purchase decisions. In addition, different products will have different ratings for one and the same aspect. To consider the product specific aspects and to be able to predict the ratings of aspects for each product, we treat reviews of each product as a separate dataset and identify aspects for each product separately.
A sample rated aspect summary of two camcorders is shown in Table 1. Rated aspect summarization decomposes the review X  X  overall rating into individual ratings for the major aspects so that a user can gain different perspectives towards the target product [10].
 Table 1: Rated Aspect Summary of Two Cam-corders
W e can see that although two camcorders have the same overall ratings, Camcorder1 has better zooming aspect while Camcorder2 has better screen and sound. This decomposed view of rated aspects clearly provides more detailed informa-tion than the overall rating and helps users to make better decisions.

Rated aspect summary can also be used as input for var-ious computer systems: in summarization systems to find sentences which summarize the review more accurately, in recommendation systems to provide explanations for rec-ommendation, and in opinion-based question answering sys-tems to answer opinion-based questions by comparing as-pects and ratings of different products.
As discussed in the preceding section, our goal is to pro-vide a method to identify aspects and predict their ratings from online product reviews without any human supervision. Therefore, we use probabilistic graphical models, which rep-resent each review as a mixture of latent aspects and ratings. We first extend the proposed model in [10] which is based on Probabilistic Latent Semantic Indexing (PLSI) model [6]. Then we extend the most well-known method for unsuper-vised modeling of documents, Latent Dirichlet Allocation (LDA) [3] to solve the considered problem. We make sim-ple adaptions on both models for the aspect identification a nd rating prediction problem and consider them as base-line models. As our main contribution, we propose a novel model, called ILDA, for jointly extracting major product as-pects and predicting their ratings from online reviews. Un-like existing graphical models that treat aspect identification and rating prediction as separate tasks [5, 22, 10], ILDA can perform both tasks simultaneously in an unsupervised manner. We argue that considering the interdependency be-tween aspects and ratings improves the performance of the model. To illustrate the importance of this interdependency, consider the following examples:  X  X ow LCD resolution X  and  X  X ow price X . The sentiment  X  X ow X  expresses a negative opin-ion for  X  X CD resolution X , while it is a positive opinion for  X  X rice X . Treating rating prediction as a separate task, both aspects receive equal ratings. We also present algorithms for approximate inference and parameter estimation for the proposed LDA and ILDA models.
 We have conducted experiments on a real life dataset from Epinions.com from five product categories. The experimen-tal results show that ILDA consistently outperforms the baseline PLSI and LDA models.
In this section, we first describe our two baseline prob-abilistic models of reviews, noting their strengths and lim-itations: A PLSI model, and a multinomial LDA model. Then we introduce a novel model, ILDA, which models the interdependency between aspects and ratings.

All of the following models assume that aspects and their ratings can be represented by multinomial distributions and try to cluster head terms into aspects and sentiments into ratings. As a shortcoming of unsupervised models, the corre-spondence between identified clusters and the actual aspects or ratings is not explicit.
Probabilistic Latent Semantic Indexing (PLSI) [6] has re-cently been applied to many text mining problems. Lu et al. applied the PLSI model on opinion phrases to identify aspects from reviews [10]. However, as we described in sec-tion 2, they used PLSI only for aspect identification, and their model does not generate ratings for the identified as-pects. We extend their PLSI model to identify aspects and predict their ratings simultaneously, as shown in Figure 2. Following the standard graphical model formalism, nodes represent random variables and edges indicate possible de-pendence. Shaded nodes are observed random variables and unshaded nodes are latent random variables. Finally, a box around groups of random variables is a  X  X late X  which denotes replication. The outer plate represents reviews and the inner plate represents opinion phrases. N and M are the number of product reviews and the number of opinion phrases in each review, respectively. Since M is independent of all the other data generating variables ( a and r ), its randomness is generally ignored [3].

To extend the PLSI model in [10] for our problem, we add the second row (dependency of the observed sentiment s to the latent rating r , and the latent rating r to the observed review d ). For each product P , a PLSI model is generated to associate unobserved aspect a m and rating r m with each observation, i.e., with each opinion phrase f m = &lt; t m in a review d 2 R . The adapted generative PLSI model can be defined in the following way: 1. Select a review d from R with probability P ( d ). 2. For each opinion phrase &lt; t m ,s m &gt; , m 2f 1 , 2 ,...,M
Translating this process into a joint probability distribu-tion results in the expression: P ( d, a , r , t , s ) =
An equivalent symmetric version of the model can be ob-tained by inverting the conditional probabilities P ( a m P ( d j a m ) P ( a m ) and P ( r m j d ) = P ( d j r m ) P ( r of Bayes X  rule. Adopting the likelihood principle, P ( d ) and P ( a , r j d ) can be determined by maximization of the log-likelihood. The standard procedure for maximum likelihood estimation in latent variable models is the Expectation Max-imization (EM) algorithm [4]. EM alternates two steps: ex-pectation (E) step which compute the posterior probabili-ties for latent variables, and maximization (M) step which update the parameters. The E-step equation for the PLSI model is: P ( a m ,r m j d,t m ,s m ) = and the M-step formulas are: where n ( d,t,s ) denotes the frequency of the phrase &lt; t,s &gt; occurred in review d . The EM algorithm obtains a local maximum of the log-likelihood by alternating E-step (2) with M-steps (3)-(8). It is important to note that d is a multinomial random variable with as many possible values as there are training reviews (i.e a dummy index into the list of reviews in the training set) and the PLSI model learns the P ( a , r j d, t , s ) only for those reviews on which it is trained. For this reason, PLSI is not a well-defined generative model of reviews [3]. Furthermore, the number of PLSI parameters which must be estimated grows linearly with the number of training reviews which causes overfitting. One reasonable approach to avoid overfitting is assigning probability to pre-viously unseen data by marginalizing over seen data [3]. We use this approach to smooth the parameters of the PLSI model for acceptable predictive performance as follows:
P ( t , s ) =
The Latent Dirichlet Allocation (LDA) model is a genera-tive probabilistic model for collections of discrete data such as text corpora [3]. The basic idea is that each item of a collection is modeled as a finite mixture over an underlying set of latent variables.

The aspect identification and rating prediction problem can be modeled as an extension of LDA (Figure 3). We make a simple adaption to the basic LDA by adding the second row (dependency among s , r , and  X  ) to the basic LDA model. This model is similar to the GM-LDA model presented in [1] for the annotation of images. In our adapted LDA model a review is assumed to be generated by first choosing a value of  X  , and then repeatedly sampling M opinion phrases &lt; t m ,s m &gt; conditional on the chosen value of  X  . Similar to [1], where  X  represents the image/caption pairs, in our LDA model we can view  X  as a high-level representation of the col-lection of aspect/rating pairs. For every pair,  X  contains the probability of generating that combination of aspect and rat-ing. The variable  X  is sampled once per review, and is held fixed during the process of generating opinion phrases for that review. After sampling  X  , the latent variables a m and r m are sampled independently (conditional independency) and then an opinion phrase &lt; t m ,s m &gt; is sampled condi-tional on the sampled a m and r m . Our adapted LDA model assumes the following generative process: 1. Sample  X  Dir (  X  ).
 2. For each opinion phrase &lt; t m ,s m &gt; , m 2f 1 , 2 ,...,M tions conditioned on aspect a m and rating r m , respectively. The resulting joint distribution is as follows: P ( a , r , t , s , X  j  X , X  1 , X  2 ) = P (  X  j  X  )
The key inferential problem is to compute the posterior distribution of the latent variables given a review &lt; t , s &gt; (collection of M opinion phrases &lt; t m ,s m &gt; ):
Similar to the basic LDA model, due to the coupling be-tween  X  and  X   X  X , the conditional distribution of latent vari-ables given observed data is intractable to compute. Al-though the posterior distribution is intractable for exact in-ference, a wide variety of approximate inference algorithm can be considered for LDA [3]. In this paper, we use vari-ational inference to compute an approximation for the pos-terior distribution. The variational inference and parameter estimation of the LDA model will be discussed in Section 5.
The LDA model overcomes both of the PLSI problems (mentioned in Section 4.1) by using a latent random variable  X  rather than a large set of individual parameters which are explicitly linked to the training reviews [3]. However, con-ditional on the latent variable  X  , the LDA model generates aspects and ratings independently, and so the dependency between specific aspects and specific ratings is ignored. We will show experimentally that due to the lack of this depen-dency, the LDA model cannot capture the correspondence between aspects and ratings.
We introduce Interdependent Latent Dirichlet Allocation (ILDA) which models the conditional interdependency be-tween the latent aspects and ratings. As described in sec-tion 3.2 and shown in Table 1, different aspects of a product can have different quality and consequently different ratings. As pointed out already, one and the same sentiment word Fi gure 5: Graphical Model Representation of the Variational Distribution Used to Approximate the Posterior in LDA and ILDA models may show different opinions for different aspects, and such methods that model aspects and ratings separately miss this interdependency between aspects and expressed sentiments.
We present the ILDA model, shown in Figure 4, to over-come this weakness by jointly modeling latent aspects and ratings. ILDA can be viewed as generative process that first generates an aspect and subsequently generates its rating. In particular, for generating each opinion phrase, ILDA first generates an aspect a m from an LDA model. Then it gen-erates a rating r m conditioned on the sampled aspect a m Finally, a head term t m and a sentiment s m are drawn con-ditioned on a m and r m , respectively. Formally, the k -factor ILDA model assumes the following generative process for a review (collection of opinion phrases): 1. Sample  X  Dir (  X  ). 2. For each opinion phrase &lt; t m ,s m &gt; , m 2f 1 , 2 ,...,M tions conditioned on the aspect a m and rating r m , respec-tively. ILDA thus specifies the following joint distribution: P ( a , r , t , s , X  j  X , X  1 , X  2 , X  3 ) = P (  X  j  X  )
The dependency assumption of the ILDA model over-comes the lack of correspondence in the LDA model, where the head terms and sentiments are generated independently, conditional on the latent variable  X  . In the ILDA model, the head term is conditional on a generated aspect and the sentiment must be conditional on a rating which is corre-spondent to that aspect. In fact, the ILDA model captures the phenomenon that the aspect is generated first and then the sentiment rates the aspect.
In this section, we describe approximate inference and pa-rameter estimation for the LDA and ILDA models, adopting a variational method.
 Al gorithm 1 Variational Inference Algorithm for ILDA 1 : initialize  X  0 mi := 1 /k for all i and m 2: initialize  X  0 mj := 1 / 5 for all j and m 3: initialize  X  0 i :=  X  i + M/k for all i 4: repeat 5: for m = 1 to M do 6: for i = 1 to k do 8: end for 9: normalize  X  t +1 mi to sum to 1 10: for j = 1 to 5 do 12: end for 13: normalize  X  t +1 mj to sum to 1 14: end for 15:  X  t +1 :=  X  + 16: until convergence
Computing the posterior distribution of the latent vari-ables for both LDA and ILDA models is intractable. A common way to obtain a tractable lower bound is to con-sider simple modifications of the original graphical model [3]. In particular, we simplify these models into the graphical model shown in Figure 5. This model specifies the following variational distribution on the latent variables: Q ( a , r , X  j , , X  ) = Q (  X  j  X  ) where the Dirichlet parameter  X  and the multinomial pa-rameters (  X  1 ,..., X  m ) and (  X  1 ,.., X  m ) are free variational pa-rameters.

To have a good approximation, the KL-divergence be-tween the variational distribution and the true posterior P ( a , r , X  ) needs to be minimized. This minimization can be achieved via an iterative method. Taking derivatives of the KL-divergence with respect to variational parameters and setting them equal to zero, we obtain the update equations. The pseudo-code of the variational inference procedure for the ILDA model is presented in Algorithm 1. These up-date equations are invoked repeatedly until the change in KL-divergence is small.

In Algorithm 1,  X  1 ix is P ( t x m = 1 j a i = 1) for the appro-priate x . Recall that each t m is a vector with exactly one component equal to one; we can select the unique x such that t x m = 1. In the same way  X  3 iy ,  X  2 zj , and  X  3 wj P (  X  t my = 1 j a i = 1), P ( s z m = 1 j r j = 1), and P (  X  1) for the appropriate y , z , and w respectively. With the ap-proximate posterior in hand, we can find a lower bound on the joint probability, P ( a , r , X  ). In the next Section we use this lower bound to estimate the ILDA parameters.
The variational inference update formulas for the LDA model are as follows 2 :
Th e detailed derivation of the variational EM algo-rithm for both LDA and ILDA models is available at http://www.sfu.ca/ sam39/ILDA
Given a corpus of reviews R = f d 1 ,d 2 ,...,d N g about prod-uct P , we want to find parameters  X  and  X   X  X  that maximize the (marginal) log likelihood of the data:  X  (  X , X  1 , X  2 , X  3 ) =
As we have described, the computation of the posterior distribution of the latent variable given a review is intractable, and therefore we use variational inference to obtain a tractable lower bound on the log likelihood. We can thus find approx-imate estimates for the ILDA model via an alternative Vari-ational EM procedure [3]. The variational EM maximizes the lower bound of the log likelihood with respect to the variational parameters, and then for fixed values of the vari-ational parameters, maximizes the lower bound with respect to the model parameters.

To maximize with respect to each variational parameter, we take derivatives with respect to it and set it to zero. The derivation yields the following iterative algorithm: 1. (E-step) for each review, find the optimizing values 2. (M-step) Maximize the resulting lower bound on the
The variational EM algorithm alternates between these two steps until the bound on the expected log likelihood converges.

The M-step updates for the conditional multinomial pa-rameters  X  1 and  X  2 in both LDA and ILDA models are as follows:
The M-step update for  X  3 in the ILDA model is equal to:
The M-step update for the Dirichlet parameter  X  is imple-mented using an efficient Newton-Raphson method in which the Hessian is inverted in linear time [3]. The Newton-Raphson optimization technique finds a stationary point of a function by iterating: where H (  X  ) and g (  X  ) are the Hessian matrix and gradient respectively at the point  X  .
Overfitting has always been a serious problem when work-ing with conditional distributions [3]. A new review is very likely to contain head terms or sentiments that did not ap-pear in any of the reviews in a training corpus. Maximum likelihood estimate of the multinomial parameters  X  1 and  X  assign zero probability to such head terms or sentiments, and so zero probability to new reviews. The standard approach to dealing with this problem is smoothing the parameters which are dependent to the observed data, by assigning pos-itive probability to all vocabulary items whether or not they are observed in the training set [3]. We deal with this issue, in both the LDA and ILDA models, by treating  X  1 and  X  2 , which are dependent to the observed data, as random ma-trices whose rows are multinomial parameters for the latent variables a and r . Each row is independently drawn from an exchangeable Dirichlet distribution (a Dirichlet distribution with a single scalar parameter). We now extend our infer-ence procedure to treat  X  1 i Dirichlet (  X  1 , X  1 ,..., X   X  2 i Dirichlet (  X  2 , X  2 ,..., X  2 ) where  X  1 and  X  2 are scalar parameters.

A variational approach can again be used to find an ap-proximation to this posterior distribution. We adopt a vari-ational approach that places a separable distribution on the random variables  X  1 ,  X  2 ,  X  , a , and r : Q ( a , r , X , X  1 , X  2 j , , X , X  1 , X  2 ) = where  X  1 and  X  2 are variational Dirichlet parameters for  X  1 and  X  2 , respectively, and Q d ( a d , r d , X  d j  X  d , variational distribution defined in Equation (13). The only change to our M-step algorithm (in both LDA and ILDA models) is to replace the maximization with respect to  X  1 and  X  2 with the following variational updates:
Bayesian methods often assume a noninformative prior which means setting  X  1 = 1 and  X  2 = 1 [1]. Iterating these equations to convergence yields an approximate posterior distribution on  X  1 ,  X  2 ,  X  , a , and r . To find the optimal number of aspects we compute the Bayesian Information Criterion (BIC) [16]. BIC is a crite-rion for model selection among a class of parametric mod-els with different numbers of parameters. When estimating model parameters using maximum likelihood estimation, it is possible to increase the likelihood by adding parameters, which may however result in overfitting. The BIC resolves this problem by introducing a penalty term for the number of parameters in the model.

Let m be the number of opinion phrases in the given test set, k be the number of free parameters to be estimated Fi gure 6: BIC of ILDA for Different Numbers of Aspects (k) (number of aspects), and L be the maximized value of the likelihood function for the estimated model. The formula for the BIC is:
Given any two learnt models, the model with the lower value of BIC is the one to be preferred. Hence, lower BIC implies either fewer free parameters, better fit, or both. Fig-ure 6 shows BIC values of the ILDA model for one repre-sentative product in the digital camera category. For this product, the BIC value of the model reaches its minimum for k = 14, i.e. ILDA identifies 14 aspect clusters in the given reviews. For each product, we train models for a range of values of k and pick the model with the optimal k , i.e. lowest BIC.
Since none of the existing benchmark datasets of opinion phrases is publicly available, we had to build a new dataset. We crawled the well-known reviewing website Epinion.com and made the dataset publicly available for research pur-poses.

We experimentally compare the three models proposed in this paper, i.e. PLSI, LDA, and ILDA. As pointed out already, most of the current works use the bag-of-words rep-resentation of reviews, and so cannot be compared to our proposed model which uses the opinion phrases represen-tation. There is only one recent model using the opinion phrases representation [10]. This model identifies the as-pects by clustering the head terms, and a separate statis-tical method is applied on the sentiments to predict their polarity. Our PLSI model is identical to the model of [10] for aspect identification. We also test a simple multinomial model (ML), that treats the head terms and sentiments as independent multinomials, as a simple baseline method.
In the next subsections, we first briefly describe our dataset and then present the evaluation of the models in terms of test set likelihood and accuracy of aspect identification and rating prediction.
We built a crawler to extract reviews from the well-known reviewing website Epinions.com. The dataset contains 2,483 reviews (29,609 opinion phrases) about 40 products from 5 categories: camcorder, cellular phone, digital camera, DVD player, and Mp3 player. For preprocessing, we adopt the technique proposed in [12] to identify opinion phrases in the form of a pair of head term and sentiment. In Table 2, for eac h category the number of reviews, opinion phrases, and phrases per product (on average) are shown.

A standard approach for evaluation of graphical models is comparing the achieved likelihoods of a held-out test set. In this paper we evaluate our main proposed model, ILDA, by comparing its obtained likelihood on a held-out test set with the likelihoods achieved by the comparison partners. How-ever, the accuracy of aspect identification and rating predic-tion cannot be inferred easily by this evaluation. Therefore, to make the evaluation stronger we compute the accuracy of the model X  X  results using a standard measure of cluster-ing. While accuracy evaluation provides more information about the performance of the learned model, it needs a truly labeled test set which makes it subjective in contrast with the likelihood evaluation approach which is completely ob-jective. To this extent, we manually create a set of true aspects and ratings for each product as gold standard . We asked some judges to label each opinion phrase with a pair of &lt; g a ,g r &gt; based on the given head term and corresponding sentiment. g a is a number showing the aspect cluster of the given head term, and g r is the rating of the given sentiment with respect to the given head term (considering the cor-respondence between head term and sentiment). This gold standard is used in the evaluation of aspect identification and rating prediction.
We held out 20% of the reviews for testing purposes and used the remaining 80% to learn the model. Our goal is to achieve high likelihood on a held-out test set. Note that, un-like in text modeling problems which learn one model from the whole collection of documents, in rated aspect summa-rization one independent model is learnt per product. In the following, we present evaluation results for each cate-gory which is the average of the results of the products of that category.

To evaluate how well a model fits the data, we computed the perplexity of the held-out test set on all models for var-ious values of aspects, k . The perplexity is monotonically decreasing in the likelihood of the test data, and a lower perplexity score indicates better performance [3]. More for-mally, for a test set of N reviews, the perplexity is:
Figure 7 shows the perplexity of different models for dif-ferent product categories. As described in sections 4.1 and 5.3, we use smoothing for all models to avoid overfitting. As expected, the latent variable models perform better than the T able 3: Rand Index of Aspect Identification for Different Models si mple Multinomial model (ML). LDA, which suffers from neither of the PLSI problems, consistently performs better. Most notably, ILDA performs much better than either LDA or PLSI and provides better fit which indicates that ILDA models the reviews more accurately.

The major reason for significant performance enhance-ment of ILDA and LDA compared to PLSI is that they effectively capture the latent semantic association among as-pects. Moreover, ILDA consistently outperforms LDA. We believe that this is due to the fact that ILDA captures the interdependency between latent aspects and the sentiments used to rate them. Also, it is notable that all the models perform better when the size of the training dataset is larger (e.g. digital camera category).
In this section , we evaluate the accuracy of identifying the k major aspect clusters in the given test set. Since all of the proposed models are soft clustering techniques, for each head term the cluster with the highest probability is selected as its aspect cluster. For each model, the accuracy of identified aspects is evaluated using the Rand Index [15], a standard measure of clustering similarity often used to compare clusterings against a gold standard.
 where P i and P m represent the clustering produced by an algorithm i and manual labeling, respectively. The agree-ment of P i and P m is checked on their k ( k 1) pairs of aspects, where k is the number of identified aspects. For each two aspects, P i and P m either assign them to the same cluster or to different clusters. In the above equation, x is the number of pairs belonging to the same cluster in both partitions, and y is the number of pairs belonging to different clusters in both partitions.

Table 3 presents the Rand Index (the higher the better) of different models for the optimal numbers of aspects, i.e. for the k with the minimum BIC value for that model. The Rand Index of aspect identification for each category is the average of the Rand Index of its products. Not surprisingly, Table 3 shows that ILDA and LDA achieve better accuracy than PLSI in all of the categories and ILDA clearly outper-forms LDA.
For each model, the accuracy of predicted ratings is eval-uated using the Rand Index. The Rand Index scores of dif-ferent models are shown in Table 4. Note that the number of clusters in rating prediction for all models is known and Table 4: Rand Index of Rating Prediction for Dif-ferent Models fix ed ( k = 5). Again, for each sentiment the cluster with the highest probability is selected as its rating cluster.
Again, ILDA and LDA achieve better accuracy than PLSI because of capturing the latent semantic association among ratings. Moreover, by capturing the interdependency be-tween latent ratings and the head terms, ILDA outperforms LDA. Note that the gain of ILDA vs. LDA is much larger than the gain of LDA vs. PLSI. In Table 3 and 4 again, all the models perform better for digital camera category which has the largest training set.
Rated aspect summarization provides very useful infor-mation for users to make their purchase decision. However, such summaries are usually unavailable in practice. In this paper we have proposed an unsupervised graphical model, ILDA, that learns a set of product aspects and correspond-ing ratings from a collection of product reviews that have been preprocessed into a collection of opinion phrases. We performed an experimental evaluation on real life data from the Epinions.com website and compared ILDA against base-line PLSI and LDA models. ILDA clearly outperformed all of the comparison partners in terms of likelihood of the test set, and accuracy of aspect identification and rating pre-diction. We argue that the major reason for the consistent enhancement is that ILDA better captures the interdepen-dency between latent aspects and ratings.

One of the shortcomings of unsupervised models is that the correspondence between generated clusters and latent variables are not explicit [18]. As future work we plan to investigate the correspondence between identified clusters and real aspects or ratings. Most of the reviewing websites such as Epinions.com provide some additional information on top of the review text and overall rating, including a set of predefined aspects and their ratings, and a rating guideline which shows the intended interpretation of the numerical ratings. Using this prior knowledge may help in establishing these correspondences. [1] D. M. Blei and M. I. Jordan. Modeling annotated [2] D. M. Blei and J. D. Lafferty. Correlated topic [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] A. P. Dempster, N. M. Laird, and D. B. Rubin. Categories [5] H. Guo, H. Zhu, Z. Guo, X. Zhang, and Z. Su.
 [6] T. Hofmann. Probabilistic L atent S emantic I ndexing. [7] M. Hu and B. Liu. Mining and summarizing customer [8] M. Hu and B. Liu. Opinion extraction and [9] B. Liu, M. Hu, and J. Cheng. Opinion observer: [10] Y. Lu, C. Zhai, and N. Sundaresan. Rated aspect [11] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. [12] S. Moghaddam and M. Ester. Opinion digger: An [13] B. Pang and L. Lee. Opinion mining and sentiment [14] A.-M. Popescu and O. Etzioni. Extracting product [15] W. M. Rand. Objective criteria for the evaluation of [16] G. Schwarz. Estimating the dimension of a model. The [17] E. E. Stephen, S. Fienberg, and J. Lafferty. Mixed [18] I. Titov and R. McDonald. A joint model of text and [19] I. Titov and R. McDonald. Modeling online reviews [20] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating [21] X. Wang and A. McCallum. Topics over time: a [22] T.-L. Wong, W. Lam, and T.-S. Wong. An [23] J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack.
