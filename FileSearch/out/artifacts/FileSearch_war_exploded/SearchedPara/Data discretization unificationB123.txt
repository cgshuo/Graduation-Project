 Ruoming Jin  X  Yuri Breitbart  X  Chibuike Muoh Abstract Data discretization is defined as a process of converting continuous data attribute valuesintoafinitesetofintervalswithminimallossofinformation.Inthispaper,weprovethat discretization methods based on informational theoretical complexity and the methods based on statistical measures of data dependency are asymptotically equivalent. Furthermore, we defineanotionofgeneralizedentropyandprovethatdiscretizationmethodsbasedonMinimal description length principle, Gini index, AIC, BIC, and Pearson X  X  X 2 and G 2 statistics are all derivable from the generalized entropy function. We design a dynamic programming algorithm that guarantees the best discretization based on the generalized entropy notion. Furthermore, we conducted an extensive performance evaluation of our method for several publicly available data sets. Our results show that our method delivers on the average 31% less classification errors than many previously known discretization methods.
 Keywords Discretization  X  Entropy  X  Gini index  X  MDLP  X  Chi-square test  X  G 2 test 1 Introduction Many real-world data mining tasks involve continuous attributes. However, many of the exis-ting data mining systems cannot handle such attributes. Furthermore, even if a data mining task can handle a continuous attribute, its performance can be significantly improved by replacing a continuous attribute with its discretized values. Data discretization is defined as a process of converting continuous data attribute values into a finite set of intervals and associating with each interval some specific data value. There are no restrictions on discrete values associated with a given data interval except that these values must induce some orde-ring on the discretized attribute domain. Discretization significantly improves the quality of discovered knowledge [ 8 , 30 ] and also reduces the running time of various data mining tenfold performance improvement for domains with a large number of continuous attributes with little or no loss of accuracy.

However, any discretization process generally leads to a loss of information. Thus, the goal of the good discretization algorithm is to minimize such information loss. 25 ]. There is a wide variety of discretization methods starting with naive (often referred to as unsupervised ) methods such as equal-width and equal-frequency [ 26 ], to much more or Wilks X  G 2 statistics based discretization algorithms [ 5 , 18 ]. Unsupervised discretization methods are not provided with class label information whereas supervised discretization methods are supplied with a class label for each data item value. Liu et. al. [ 26 ] introduce a nice categorization of a large number of existing discretization methods.

In spite of the wealth of literature on discretization methods, there are very few attempts to analytically compare them. Typically, researchers compare the performance of different algo-rithms by providing experimental results of running these algorithms on publicly available data sets. In [ 12 ], Dougherty et al. compare discretization results obtained by unsupervised discretization versus a supervised method proposed by Holte [ 20 ] and the entropy based method proposed by Fayyad and Irani [ 15 ]. They conclude that supervised methods are bet-ter than unsupervised discretization method in that they generate fewer classification errors. In [ 25 ], Kohavi and Sahami report that the number of classification errors generated by the discretization method of Fayyad and Irani [ 15 ] is comparatively smaller than the number of errors generated by the discretization algorithm of Auer et al. [ 2 ]. They conclude that entropy based discretization methods are usually better than other supervised discretization algorithms.

Recently, many researchers have concentrated on the generation of new discretization algorithms [ 5 , 6 , 24 , 28 , 35 ]. The goal of the CAIM algorithm ptoposed by Kurgan and Klas [ 24 ] is to find the minimum number of intervals that minimize the loss between class-attribute interdependency. Boulle [ 5 ] has proposed a new discretization method called Khiops ,which uses Pearson X  X  X 2 statistic to merge consecutive intervals in order to improve the global dependence measure. MODL is another latest discretization method proposed by Boulle [ 7 ]. This method builds an optimal criteria based on a Bayesian model. A dynamic programming approach and a greedy heuristic approach are developed to find the optimal criteria. Yang and Webb [ 36 ] have studied discretization for naive-Bayes classifiers. They have proposed a coupleofmethods,suchas proportional k-interval discretization and equal size discretization , to manage the discretization bias and variance .Bay[ 3 ] has studied multivariate discretization instead of the single variate discretization using a multivariate test of differences. All these algorithms have shown certain advantages, such as improving classification accuracy and/or complexity.

Several fundamental questions of discretization, however, remain to be answered. How these different methods are related to each other and how different or how similar are they? Is there an objective function which can measure the goodness of different approaches? If so, how would this function look like? In this paper we provide a set of positive results toward answering these questions. 1.1 Problem statement For the purpose of discretization, the entire dataset is projected onto the targeted continuous attribute. The result of such a projection is a two dimensional contingency table , C with I rows and J columns. Each row corresponds to either a point in the continuous domain, or an initial data interval. We treat each row as an atomic unit which cannot be further subdivided. Each column corresponds to a different class and we assume that the dataset has a total of J classes. A cell c ij represents the number of points with j th class label falling in the i th point (or interval) in the targeted continuous domain. Table 1 lists the basic notations for the contingency table C .

Inthemoststraightforwardway,eachcontinuouspoint(orinitialdatainterval)corresponds contains points from different classes and thus, c ij may be more than zero for several columns in the same row.
 where each row in the new table C is the combination of several consecutive rows in the original C table, and each row in the original table is covered by exactly one row in the new table.

The quality of the discretization function is measured by a goodness function, which depends on two parameters. The first parameter (termed cost ( data ) ) reflects the number of classification errors generated by the discretization function, whereas the second one (termed penalt y ( model ) ) is the complexity of the discretization which reflects the number of dis-cretization intervals generated by the discretization function. Clearly, the more discretization intervals created, the fewer the number of classification errors, and thus the cost of the data is lower. That is, if one is interested only in minimizing the number of classification errors, the best discretization function would generate I intervals X  X he number of data points in the initial contingency table. Conversely, if one is only interested in minimizing the number of intervals (and therefore reducing the penalty of the model), then the best discretization function would generate a single interval by merging all data points into one interval. Thus, finding the best discretization is to find the best trade-off between the cost ( data ) and the penalt y ( model ) . 1.2 Our contribution Our results can be summarized as follows: 1. We demonstrate a somewhat unexpected connection between discretization methods 2. We define a notion of generalized entropy and introduce a notion of generalized goodness 3. We design a dynamic programming algorithm that guarantees the best discretization 4. We conduct an extensive performance evaluation of our discretization method and 1.3 Paper outline Section 2 discusses goodness functions based on the MDLP [ 31 ], AIC, BIC [ 17 ], and good-any good discretization function should satisfy, and we prove that all the functions introduced in Sect. 2 satisfy these goodness function properties. Section 4 contains the main result of the paper. We prove there that each goodness function defined in Sect. 2 is a sum of G 2 defined by Wilks X  statistic [ 1 ] and degree of freedom of the contingency table multiplied by a function that is bounded by O ( log N ) ,where N is the number of data samples in the contingency table. Section 5 describes a generalized entropy function and formulates ageneralized notion of the goodness function. We prove then that any goodness function discussed in Section 2 is derivable from the generalized goodness function. Sect. 6 experimentally compare the new appoarach which utilizes the generalized goodness function with existing well-known discre-tization approaches, including Entropy and Chi-Merge , among others. Section 7 concludes the paper. 2 Goodness functions In this section, we introduce a list of goodness functions which are used to evaluate different discretization for numerical attributes. These goodness functions intend to measure three different qualities of a contingency table: the information quality (Sect. 2.1 ), the fitness of statistical models (Sect. 2.2 ), and the confidence level for statistical independence tests (Sect. 2.3 ). 2.1 Information theoretical approach and MDLP In the information theoretical approach, we treat discretization of a single continuous attri-bute as a 1 -dimension classification problem. The MDLP is a commonly used approach for tization fit the data, and the penalty for the discretization which is based on the complexity of discretization. Formally, MDLP associates a cost with each discretization, which has the following form: where both terms correspond to these two factors, respectively. Intuitively, when a classifi-cation error increases, the penalty decreases and vice versa.

In MDLP, the cost of discretization ( cost ( model ) ) is calculated under the assumption that there are a sender and a receiver. Each of them knows all continuous points, but the receiver is without the knowledge of their labels. The cost of using a discretization model to describe the available data is then equal to the length of the shortest message to transfer the label of message to transfer the label of all data points of each interval of a given discretization. The second term penalt y ( model ) corresponds to the coding book and delimiters to identify and based on MDLP ( Cost MDLP ) is derived as follows: and the rest corresponds to penalt y ( model ) .

Before we provide the detailed derivation for Cost MDLP , we first formally introduce a notion of entropy and show how a merge of some adjacent data intervals results in information loss as well as in the increase of the cost ( data | model ) .
 Definition 1 [ 11 ]. The entropy of an ensemble X is defined to be the average Shannon information content of an outcome: where A x is the possible outcome of x .
 simplicity, consider that we have only two intervals S 1 and S 2 in the contingency table, then the entropies for each individual interval is defined as follows: If we merge these intervals into a single interval (denoted by S 1  X  S 2 ) following the same rule, we have the entropy as follows: Further, if we treat each interval independently (without merging), the total entropy of these two intervals is expressed as H ( S 1 , S 2 ) , which is the weighted average of both individual entropies.Formally,wehave every merge operation leads to information loss. The entropy gives the lower bound of the cost to transfer the label per data point. This means that it takes a longer message to send all data points in these two intervals if they are merged ( N  X  H ( S 1  X  S 2 ) ) than sending both intervals independently ( N  X  H ( S 1 , S 2 ). However, after we merge, the number of intervals is reduced. Therefore, the discretization becomes simpler and the penalty of the model in Cost MDLP becomes smaller.
 Derivation of Cost MDLP :
For an interval S 1 , the best way to transfer the labeling information of each point in the interval is bounded by a fundamental theorem in information theory, stating that the average length of the shortest message is higher than N 1  X  H ( S 1 ) . Though we can apply the Hoffman coding to get the optimal coding for each interval, we are not interested in the absolute minimal coding. Therefore, we will apply the above formula as the cost to transfer follows. In the meantime, we have to transfer the model itself, which includes all the intervals and the coding book for transferring the point labels for each interval. The length of the message to transferring the model is served as the penalty function for the model. The cost to transfer all be approximated as Next, we have to consider the transfer of the coding book for each interval. For a given interval S i , each code will correspond to a class, which can be coded in log 2 J bits. We need to transfer such codes at most J  X  1 times for each interval, since after knowing J  X  1 classes, the remaining class can be inferred. Therefore, the total cost for the coding book, denoted as L , can be written as Given this, the penalty of the discretization based on the theoretical viewpoint is penalt y 1 ( model ) = L 1 ( I , N ) + L 2 = ( I  X  1 ) log 2 Put together, the cost of the discretization based on MDLP is Goodness function based on MDLP: To facilitate the comparison with other cost functions, we formally define a goodness function of a MDLP based discretization method applied to contingency table C to be the difference between the cost of C 0 , which is the resulting table after merging all the rows of C into a single row, and the cost of C . We will also use natural log instead of the log 2 function. Formally, we denote the goodness function based on MDLP Note that for a discretization problem, any discretization method shares the same C 0 . Thus, the least cost of transferring a contingency table corresponds to the maximum of the goodness function. 2.2 Statistical model based goodness functions A different way to look at a contingency table is to assume that all data points are gene-rated from certain distributions (models) with unknown parameters. Given a distribution, the maximal likelihood principle (MLP) can help us to find the best parameters to fit the data [ 17 ]. However, to provide a better data fitting, more expensive models (including more parameters) are needed. Statistical model selection tries to find the right balance between the complexity of a model corresponding to the number of parameters, and the fitness of the data to the selected model, which corresponds to the likelihood of the data being generated by the given model.
 In statistics, the multinomial distribution is commonly used to model a contingency table. Here, we assume the data in each interval (or row) of the contingency table are independent and all intervals are independent. Thus, the kernel of the likelihood function for the entire contingency table is: where  X  = ( X  1 | 1 , X  2 | 1 ,..., X  J | 1 ,..., X  J | I ) are the unknown parameters. Applying the i  X  I ,1  X  j  X  J . We commonly transform the likelihood to the log-likelihood as follow: According to Hand et al. [ 17 ], S L (  X ) is treated as a type of entropy term that measures how well the parameters  X  can compress (or predict) the training data.

Clearly, different discretizations correspond to different multinomial distributions (models). For choosing the best discretization model, the Akaike information criterion or AIC [ 17 ] can be used and it is defined as follows: where, the first term corresponds to the fitness of the data given the discretization model, and the second term corresponds to the complexity of the model. Note that in this model for each row we have the constraint  X  1 | i + X  X  X +  X  J | i = 1. Therefore, the number of parameters for each row is J  X  1.

Alternatively, for choosing the best discretization model based on Bayesian arguments that take into account the size of the training set N is also frequently used. The Bayesian information criterion or BIC [ 17 ] is defined as follows: In the BIC definition, the penalty of the model is higher than the one in the AIC by a factor of 1 / 2log N .
 Goodness function based on AIC and BIC: For the same reason as MDLP, we denote the goodness function of a given contingency table based on AIC and BIC as the difference between the cost of C 0 (the resulting table after merging all the rows of C into a single row), and the cost of of C .
 2.3 Confidence level based goodness functions Another way to treat discretization is to merge intervals so that the rows (intervals) and columns (classes) of the entire contingency table become more statistically dependent .In other words, the goodness function of a contingency table measures its statistical quality in terms of independence tests.
 Pearson X  X  X 2 : In the existing discretization approaches, the Pearson statistic X 2 [ 1 ]iscom-monly used to test the statistical independence. The X 2 statistic is as follows: X 2 statistic has an asymptotic  X  2 distribution with degrees of freedom df = ( I  X  1 )( J  X  1 ) , where I is the total number of rows. Consider a null hypothesis H 0 (the rows and columns are statistically independent) against an alternative hypothesis H a . Consequently, we obtain the confidence level of the statistical test to reject the independence hypothesis ( H 0 ). The confidence level is calculated as where, F  X  2 as our goodness function to compare different discretization methods that use Pearson X  X  X 2 statistic. Our goodness function is formally defined as We note that 1  X  F  X  2 pendence test [ 7 ]. The lower the P value (or equivalently, the higher the goodness), with more confidence we can reject the independence hypothesis ( H 0 ). This approach has been used in Khiops [ 5 ], which describes a heuristic algorithm to perform discretization. Wilks X  G 2 : In addition to Pearson X  X  chi-square statistic, another statistic called likelihood-is derived from the likelihood-ratio test, which is a general-purpose way of testing a null hypothesis H 0 against an alternative hypothesis H a . In this case we treat both intervals (rows) and the classes (columns) equally as two categorical variables , denoted as X and Y . i and column j ,where {  X  ij } is the joint distribution of X and Y ,and  X  i + and  X  + j are the marginal distributions for the row i and column j , respectively.

Based on the multinomial sampling assumption (a common assumption in a contingency table) and the maximal likelihood principle, these parameters can be estimated as  X   X  i + = N likelihood is maximized when  X   X  ij = c ij / N . Thus the statistical independence between the rows and the columns of a contingency table can be expressed as the ratio of the likelihoods: where the denominator corresponds to the likelihood under H a , and the nominator corres-ponds to the likelihood under H 0 .

Wilks has shown that  X  2log , denoted by G 2 , has a limiting null chi-squared distribution, as N  X  X  X  .
 For large samples, G 2 has a chi-squared null distribution with degrees of freedom equal to ( the entire contingency table, which serves as our goodness function Indeed, this statistic has been applied in discretization (though not for the global goodness function), and is referred to as class-attributes interdependency information [ 35 ]. 3 Properties of goodness functions An important theoretical question we address is how these methods introduced in Sect. 2 are related to each other and how different they are. Answering these questions helps to understand the scope of these approaches and shed light on the ultimate goal: for a given dataset, automatically find the best discretization method.

We first investigate some simple properties shared by the aforementioned goodness func-tions (Theorem 1 ). We describe four basic principles we believe any goodness function for discretization must satisfy. 2. Symmetric principle (P2): Let C j be the j -th column of contingency table C . GF ( C ) = 3. MIN principle (P3): Consider all contingency tables C which have I rows and J 4. MAX principle (P4): Consider all the contingency tables C which have I rows and J The following theorem states that all aforementioned goodness functions satisfy these four principles.
 Theorem 1 GF MDLP ,GF AIC ,GF BIC ,GF X 2 ,GF G 2 satisfy all four principles, P1 , P2 , P3 , and P4 .
 Proof We will first focus on proving for GF MDLP . The proof for GF AIC and GF BIC can be derived similarly.
 Merging principle (P1) for GF MDLP : Assuming we have two consecutive rows i and i + 1 c two rows. Then we have In addition, let  X  = ( I  X  1 ) log = ( I  X  1 ) log For I = 2,  X  = log 2 N + J  X  log 2 J &gt; 0.
 For I  X  3, we need more detailed analysis. Let f = N /( I  X  1 )  X  1. Given this, we have Based on the first order derivative of and second order derivative analysis, we can see that (
I  X  2 ) log we have Symmetric principle (P2) for GF MDLP : This can be directly derived from the symmetric property of entropy.
 MIN principle (P3) for GF MDLP : Since the number of rows ( I ), the number of samples ( N ), and the number of classes ( J ) are fixed, we only need to maximize N  X  H ( S 1 ,..., S I ) . MAX principle (P4) for GF MDLP : Since the number of rows ( I ), the number of samples Now, we prove the four properties for GF X 2 .
 Merging principle (P1) for GF X 2 : Assuming we have two consecutive rows i and i + 1 c two rows. Then we have We note that the degree of freedom in the original contingency table is ( I  X  1 )( J  X  1 ) and the second one is ( I  X  2 )( J  X  1 ) . In addition, we have for any t &gt; 0, F  X  2 F Symmetric principle (P2) for GF X 2 : This can be directly derived from the symmetric property of X 2 .
 MIN principle (P3) for GF X 2 : Since the number of rows ( I ), the number of samples ( N ), we can see that M j = N / J .

X 2 = Since X 2  X  0, we achieve the minimal of X 2 .
 MAX principle (P4) for GF X 2 : Since the number of rows ( J ), the number of samples ( N ), and the number of classes ( J ) are fixed, we only need to maximize X 2 .
 Note that this bound can be achieved in our condition. Basically, in any row k , we will have one cell c kj = N k . Therefore, F  X  2 best possible discretization given existing conditions.

The proof for GF G 2 can be derived similarly from GF MDLP and GF X 2 . 4 Equivalence of goodness functions In this section, we analytically compare different discretization goodness functions intro-duced in Sect. 2 . In particular, we find some rather surprising connection between these seemingly quite different approaches: the information theoretical complexity, the statistical fitness, and the statistical independence tests. We basically prove that all these functions can be expressed in a uniform format as follows: where, df is a degree of freedom of the contingency table, N is the number of data points, I is the number of data rows in the contingency table, J is the number of class labels and f is bounded by O ( log N ) .Thefirstterm G 2 corresponds to the cost of the data given a discretization model ( cost ( data | model ) ), and the second corresponds to the penalty or the complexity of the model ( penalt y ( model ) ).

To derive this expression, we first derive an expression for the cost of the data for different goodness functions discussed in Sect. 2 (Sect. 4.1 ). This is achieved by expressing G 2 sta-tistics through information entropy (Theorem 3 ). Then, using a Wallace X  X  result [ 33 , 34 ]on approximating  X  2 distribution with a normal distribution, we transform the goodness function based on statistical independence tests into the format of Formula 10 . Further, a detailed ana-lysis of function f reveals a deeper relationship shared by these different goodness functions (Sect. 4.3 ). 4.1 Unifying the cost of data to G 2 In the following, we establish the relationship among entropy , log-likelihood and G 2 .This is the first step for an analytical comparison of goodness functions based on the information theoretical, the statistical model selection, and the statistical independence test approaches.
First, it is easy to see that for a given contingency table, the cost of the data transfer log likelihood S L (used in the statistical model selection approach) as the following theorem asserts.
 Theorem 2 For a given contingency table C I  X  J , the cost of data transfer ( cost 1 ( data | model )) is equal to the log-likelihood S L ,i.e.
 Proof N  X  H ( S 1 ,..., S I ) = X  The next theorem establishes a relationship between entropy criteria and the likelihood independence testing statistics G 2 . This is the key to discover the connection between the information theoretical and the statistical independence test approaches.
 Theorem 3 Let C be a contingency table. Then Proof G 2 / 2 = X  log = Consequently, we rewrite the goodness functions GF MDLP , GF AIC and GF BIC as follows. For the rest of the paper we use the above formulas for GF MDLP , GF AIC and GF BIC .
It has long been known that they are asymptotically equivalent. The next theorem provides tool to connect the information theoretical approach and the statistical independence test approach based on Pearson X  X  chi-square ( X 2 ) statistic.
 Theorem 4 [ 1 ] . Let N be the total number of data values in the contingency table T of I  X  J dimensions. If the rows ( columns ) of contingency table are independent, then probability of X 2  X  G 2 = 0 converges to one as N  X  X  X  .
 In the following, we mainly focus on the asymptotic properties shared by X 2 and G 2 based cost functions. Thus, our further discussions on G 2 can also be applied to X 2 .
Note that Theorems 2 and 3 basically establish the basis for Formula 10 of goodness functions based on the information theoretical approach and statistical model selection approaches. Even though Theorems 3 and 4 relate the information theoretical approach (based on entropy) to the statistical independence test approach (based on G 2 and X 2 ), it is still unclear how to compare them directly since the goodness function of the former one is basedonthetotal cost of transferring the data and the goodness function of the latter one is the confidence level for a hypothesis test. Section 4.2 presents our approach on tackling this issue. 4.2 Unifying statistical independence tests In order to compare the quality of different goodness functions, we introduce a notion of equi-valent goodness functions. Intuitively, the equivalence between goodness functions means that these functions rank different discretization of the same contingency table identically. Definition 2 Let C be a contingency table and GF 1 ( C ) , GF 2 ( C ) be two different goodness functions. GF 1 and GF 2 are equivalent if and only if for any two contingency tables C 1 and C , GF 1 ( C 1 )  X  GF 1 ( C 2 )  X  X  X  GF 2 ( C 1 )  X  GF 2 ( C 2 ) .

Using the equivalence notion, we transform goodness functions to different scales and/or to different formats. In the sequel, we apply this notion to compare seemingly different goodness functions.

The relationship between the G 2 and the confidence level is rather complicated. It is clearly not a simple one-to-one mapping as the same G 2 may correspond to very different confidence level depending on degrees of freedom of the  X  2 distribution and, vice versa the same confidence level may correspond to very different G 2 values. Interestingly enough, such many-to-many mapping actually holds the key for the aforementioned transformation. Intui-tively, we have to transform the confidence interval to a scale of entropy or G 2 parameterized by the degree of freedom for the  X  2 distribution.
 Our proposed transformation is as follows.
 is, the following equality holds: where, F  X  2 we define as a new goodness function for C .
 The next theorem establishes the equivalence between a goodness functions GF G 2 and Theorem 5 The goodness function G F G 2 = u ( G 2 ) is equivalent to the goodness function Proof Assuming we have two contingency tables C 1 and C 2 with degree of freedom df 1 and df 2 , respectively. Their respective G 2 statistics are denoted as G 2 1 and G 2 2 .Clearly,wehave This basically establishes the equivalence of these two goodness functions.
 The newly introduced goodness function GF G 2 is rather complicated and it is hard to find for it a closed form expression. In the following, we use a theorem from Wallace [ 33 , 34 ]to derive an asymptotically accurate closed form expression for a simple variant of GF G 2 . u ( If df  X  X  X  and w( t )&gt;&gt; 0 , then Therefore, Thus, we can have the following goodness function: Similarly, function GF  X  2 is obtained from GF G 2 by replacing in the GF G 2 expression G 2 can be (asymptotically) expressed in the same closed form (Formula 10 ). Specifically, all of them can be decomposed into two parts. The first part contains G 2 , which corresponds to the cost of transferring the data using information theoretical view. The second part is a linear function of degrees of freedom, and can be treated as the penalty of the model using thesameview. 4.3 Penalty analysis In this section, we perform a detailed analysis of the relationship between penalty functions of these different goodness functions . Our analysis reveals a deeper similarity shared by these functions and at the same time reveals differences between them.

Simply put, the penalties of these goodness functions are essentially bounded by two extremes. On the lower end, which is represented by AIC, the penalty is on the order of degree of freedom, O ( df ) . On the higher end, which is represented by BIC, the penalty is O ( df log N ) .
 Penalty of GF G 2 (Formula 15 ): The penalty of our new goodness function GF G 2 = u 2 ( G 2 ) is between O ( df ) and O ( df log N ) . The lower bound is achieved, provided that G 2 being strictly higher than df ( G 2 &gt; df ). Lemma 1 gives the upper bound.
 Lemma 1 G 2 is bounded by 2 N log J(G 2  X  2 N log J). Proof cases corresponding to the lower bound and upper bound of G 2 , respectively. The second case is further subdivided into two subcases. 1. If N / df  X  N /( IJ ) = c ,where c is some constant, the penalty is O ( df ) . 2. If N  X  X  X  and N / df  X  N /( IJ )  X  X  X  , the penalty is Penalty of GF MDLP (Formula 11 ): The penalty function f derived in the goodness function based on the information theoretical approach can be written as Here, we again consider two cases: 1. If N /( I  X  1 ) = c ,where c is some constant, we have the penalty of MDLP is O ( df ) . 2. If N &gt;&gt; I and N  X  X  X  , we have the penalty of MDLP is O ( df log N ) .
 Note that in the first case, the contingency table is very sparse ( N /( IJ ) is small). In the second case, the contingency table is very dense ( N /( IJ ) is very large).

To summarize, the penalty can be represented in a generic form as df  X  f ( G 2 , N , I , J ) (Formula 10 ). This function f is bounded by O ( log N ) . Finally, we observe that different penalty clearly results in different discretization. The higher penalty in the goodness function the following theorem.
 Theorem 7 Given an initial contingency table C with log N  X  2( the condition for the penalty of B I C is higher than the penalty of A I C ), let I AIC be the number of intervals of the discretization generated by using G F AIC and I BIC be the number of intervals of the discretization generated by using G F BIC .ThenI AIC  X  I BIC .
 Note that this is essentially a direct application of the well-known facts from statistical machine learning research: higher penalty will result in more concise models [ 17 ].
Finally, we note that several well-known discretization algorithms based on local indepen-dence test include ChiMerge [ 23 ]andChi2[ 27 ], etc. Specifically, for consecutive intervals, these algorithms perform a statistical independence test based on Pearson X  X  X 2 or G 2 .If they could not reject the independence hypothesis for those intervals, they merge them into one row. A simple analysis in [ 22 ] suggests that the local merge condition essentially shares the penalty in the same order of magnitude as GF AIC . Interested readers can refer [ 22 ]for detailed discussion. 5 Parametrized goodness function The goodness functions discussed so far are either entropy or  X  2 or G 2 statistics based. In this section we introduce a new goodness function which is based on gini index [ 4 ]. Gini index based goodness function is strikingly different from goodness functions introduced so far. In this section we show that a newly introduced goodness function GF Gini along with the goodness functions discussed in Sect. 2 are all can be derived from a generalized notion of entropy [ 29 ]. 5.1 Gini based goodness function Let S i be a row in contingency table C . Gini index of row S i is defined as follows [ 4 ]: and Cost Gini ( C ) = I i = 1 N i  X  Gini ( S i ) .

The penalty of the model based on gini index can be approximated as 2 I  X  1 (see detailed derivation in the technical report [ 22 ]). The basic idea is to apply a generalized MDLP cost of transferring the coding book as well as necessary delimiters ( penalt y ( model ) )are treated as the complexity measure. Therefore, the gini index can be utilized to provide such a measure. Thus, the goodness function based on gini index is as follows: 5.2 Generalized entropy In this subsection, we introduce a notion of generalized entropy, which is used to uniformly represent a variety of complexity measures, including both information entropy and gini index by assigning different values to the parameters of the generalized entropy expression. Thus, it serves as the basis to derive the parameterized goodness function which represents all the aforementioned goodness functions, such as GF MDLP , GF AIC , GF BIC , GF G 2 ,and GF Gini , in a closed form.
 Definition 4 [ 29 , 32 ]. For a given interval S i , the generalized entropy is defined as When  X  = 1, we can see that When  X   X  0, Let C I  X  J be a contingency table. We define the generalized entropy for C as follows. Lemma 2 H  X  [ p 1 ,..., p J ]= J j = 1 p i ( 1  X  p  X  i )/ X  is concave when  X &gt; 0 . Proof Thus, Let C I  X  J be a contingency table., We define the generalized entropy for C as follows. Similarly, we have Theorem 8 There always exists information loss for the merged intervals : H  X  ( S 1 , S 2 )  X  H  X  ( S 1  X  S 2 ) Proof This is the direct application of the concaveness of the generalized entropy. 5.3 Parameterized goodness function Based on the discussion in Sect. 4 , we derive that different goodness functions basically can be decomposed into two parts. The first part is for G 2 , which corresponds to the information theoretical difference between the contingency table under consideration and the marginal distribution along classes. The second part is the penalty which counts the difference of com-plexity for the model between the contingency table under consideration and the one-row contingency table. The different goodness functions essentially have different penalties ran-ging from O ( df ) to O ( df log N ) .

In the following, we propose a parameterized goodness function which treats all the aforementioned goodness functions in a uniform way.
 Definition 5 Given two parameters,  X  and  X  ,where0 &lt; X   X  1and0 &lt; X  , the parameterized goodness function for contingency table C is represented as The following theorem states the basic properties of the parameterized goodness function. Theorem 9 The parameter goodness function G F  X , X  , with  X &gt; 0 and 0 &lt; X   X  1 , satisfies all four principles, P1 , P2 , P3 , and P4 .
 Proof Merging principle (P1) for GF  X , X  : Assuming we have two consecutive rows i and i c two rows. Then we have In addition, we have Thus, we have GF  X , X  ( C )&lt; GF  X , X  ( C ) .
 Symmetric principle (P2) for GF  X , X  : This can be directly derived from the symmetric property of entropy.
 MIN principle (P3) for GF  X , X  : Since the number of rows ( I ), the number of samples ( N ), and the number of classes ( J ) are fixed, we only need to maximize N  X  H ( S 1 ,..., S I ) .By the concaveness of the H  X  (Theorem 8 ), MAX principle (P4) for GF  X , X  : Since the number of rows ( I ), the number of samples ( N ), and the number of classes ( J ) are fixed, we only need to minimize N  X  H  X  ( S 1 ,..., S J ) . Note that the proof of GF  X , X  immediately implies that the four principles hold for GF AIC and GF BIC .
 By adjusting different parameter values, we show how goodness functions defined in Sect. 2 can be obtained from the parametrized goodness function. We consider several cases: 2. Let  X  = 1 / log N and  X   X  0. Then GF 1 / log N  X   X  0 = GF AIC . 3. Let  X  = 1 / 2and  X   X  0. Then GF 1 / 2 , X   X  0 = GF BIC . 4. Let  X  = const , X   X  0and N &gt;&gt; I .Then GF const , X   X  0 = G 2  X  O ( df log N ) = 5. Let  X  = const , X   X  0, and G 2 = O ( N log J ), N /( IJ )  X  X  X  .Then GF const , X   X  0 = The parameterized goodness function not only allows us to represent the existing goodness functions in a closed uniform form, but, more importantly, it provides a new way to understand and handle discretization. First, the parameterized approach provides a flexible framework to access a large collection (potentially infinite) of goodness functions. Any valid pair of  X  and  X  corresponds to a potential goodness function. Note that this treatment is in the same spirit of regularization theory developed in the statistical machine learning field [ 16 , 19 ].
Secondly, finding the best discretization for different data mining tasks for a given dataset is transformed into a parameter selection problem. However, it is an open problem how we may automatically select the parameters without running the targeted data mining task. In other words, can we analytically determine the best discretization for different data mining tasks for a given dataset? This problem is beyond the scope of this paper and we plan to investigate it in future work.

Finally, the unification of goodness functions allows to develop efficient algorithms to discretize the continuous attributes with respect to different parameters in a uniform way. This is the topic of the next subsection. 5.4 Dynamic programming for discretization Thissectionpresentsadynamicprogrammingapproachtofindthebestdiscretizationfunction to maximize the parameterized goodness function. Note that the dynamic programming has been used in discretization before [ 14 ]. However, the existing approaches do not have a global goodness function to optimize, and almost all of them have to require the knowledge of targeted number of intervals. In other words, the user has to define the number of intervals for discretization. Thus, the existing approaches can not be directly applied to discretization for maximizing the parameterized goodness function.

In the following, we introduce our dynamic programming approach for discretization. To facilitate our discussion, we use GF for GF  X , X  , and we simplify the GF formula as follows. Since a given table C , N  X  H  X  ( S 1  X  X  X  X  X  S I ) (the first term in GF , Formula 17 )isfixed, we define Clearly, the minimization of the new function F is equivalent to maximizing GF .Inthe following, we will focus on finding the best discretization to minimize F .First,wedefine a sub-contingency table of C as C [ i : i + k ]={ S i ,..., S i + k } ,andlet C 0 [ i : i + k ]= S  X  X  X  X  X  S i + k be the merged column sum for the sub-contingency table C [ i : i + k ] . Thus, the new function F of the row C 0 [ i : i + k ] is: Let C be the input contingency table for discretization. Let Opt ( i , i + k ) be the minimum of the F function from the partial contingency table from row i to i + k , k &gt; 1. The optimum which corresponds to the best discretization can be calculated recursively as follows:
Opt ( i , i + k ) = min F ( C 0 [ i : i + k ] ), gramming to find the discretization with the minimum of the goodness function, which are described in Algorithm 1 . The complexity of the algorithm is O ( I 3 ) ,where I is the number of intervals of the input contingency table C .
 Algorithm 1 Discretization (Contingency table C I  X  J ) 6 Experimental results The major goal of our experimental evaluation is to demonstrate that the dynamic program-ming approach with appropriate parameters can significantly reduce the classification errors compared with the existing discretization approaches.

We chose 12 datasets from the UCI machine learning repository [ 37 ]. Most of the datasets have been used in the previous experimental evaluation for discretization study [ 12 , 26 ]. Table 2 describes the size and the number of continuous and nominal features of each dataset. We apply discretization as a preprocessing step for two well-known classification methods: the C4.5 decision tree and Naive Bayes classifier [ 17 ]. For comparison purpose, we apply four discretization methods: equal-width (EQW), equal-frequency (EQF), Entropy [ 15 ], and ChiMerge [ 23 ]. The first two are unsupervised approaches and the last two are supervised approaches. We set the number of discretization intervals to be 10 for the first two. All their implementations are from Weka 3 [ 38 ].

Our dynamic programming approach for discretization (referred to as Unification in the experimental results) depends on two parameters,  X  and  X  . How to analytically determine the best parameters which can result in the minimal classification error is still an open question and beyond the scope of this paper. Here, we apply an experimental-validation approach to choose the optimal parameters  X  and  X  . For a given dataset and the data mining task, we create a 10  X  10 uniform grid for 0  X   X   X  1and0  X   X   X  1 In addition, we use a value 10  X  5 to replace 0 for  X  since it cannot be equal to 0. Then we apply the dynamic programming at each grid point to discretize the dataset. We score each point using the mean classification the unification approach with parameters from the 10  X  10 grid points. Figure 1 b illustrates glass dataset [ 37 ]. Clearly, we can see that different  X  and  X  parameters can result in very different classification error rates. Given this, we choose the  X  and  X  pair which achieves the minimal classification error rate as the selected unification parameters for discretization. For instance, in these two figures, we choose  X  = 0 . 3and  X  = 0 . 3 as the parameters to discretize Note that the objective of using five trials instead of only one is to choose parameters in a more robust fashion to avoid outliers.

Finally, for each of the discretization method (our Unification method with the best pre-dicated parameter), we run a five-trial five-fold cross-validation, and report their mean and and is different from the trials in the parameter selection process.
Tables 3 and 4 show the experimental results for C4.5 and Naive Bayes Classifier, res-pectively. In the left part of each table, we show the mean classification error and standard deviation using different discretization methods (the first one, Continuous corresponding to no-discretization). The right part of each table shows the percentage differences bet-ween two leading discretization approaches, Entropy and ChiMerge , with our new approach Unification . The last column chooses the minimal classification errors from all five existing approaches to compare with the unification approach.

We can see that the unification approach performs significantly better than the existing approaches. First, based on the average classification error for all the 12 datasets, the unifi-cation is the best among all these approaches (14 . 45% error rate for C4.5 and 10 . 60% for Naive Bayes classifier). For C4.5, it reduces the error rate on an average of 19 . 40% compared with Entropy , and reduces the error rate on average of 26 . 65% compared with ChiMerge . For Naive Bayes classifier, it reduces the error rate on an average of 58 . 74% compared with Entropy , and reduces the error rate on an average of 20 . 82% compared with ChiMerge .The overall improvement is on an average of 31% in terms of classification error rate. Finally, in 9 out of 12 datasets for C4.5, the unification approach shows better or equal performance with the best existing approach. In other 3 datasets, the performance are fairly close to the minimal error rate as well. For Naive Bayes classifier, the unification method perform the best in 10 out of 12 datasets and the second for the other 2 datasets. 7 Conclusions In this paper, we introduced a generalized goodness function to evaluate the quality of a discretization method. We have shown that seemingly disparate goodness functions based on entropy, AIC, BIC, Pearson X  X  X 2 , Wilks X  G 2 , and Gini index are all derivable from our generalized goodness function. Furthermore, the choice of different parameters for the generalized goodness function explains why there is a wide variety of discretization methods. Indeed, difficulties in comparing different discretization methods were widely known. Our results provide a theoretical foundation in understanding these difficulties and offer rationale as to why evaluation of different discretization methods for an arbitrary contingency table is difficult. We have designed a dynamic programming algorithm that for given set of parameters of a generalized goodness function provides an optimal discretization which achieves the minimum of the generalized goodness function. We have conducted an extensive performance tests for a set of publicly available data sets. Our experimental results demonstrate that our discretization method consistently outperforms the existing discretization metods on the average by 31%. These results clearly validate our approach and open a new way of tackling discretization problems.
 References Author X  X  biography
 Ruoming Jin  X  Yuri Breitbart  X  Chibuike Muoh Abstract Data discretization is defined as a process of converting continuous data attribute valuesintoafinitesetofintervalswithminimallossofinformation.Inthispaper,weprovethat discretization methods based on informational theoretical complexity and the methods based on statistical measures of data dependency are asymptotically equivalent. Furthermore, we defineanotionofgeneralizedentropyandprovethatdiscretizationmethodsbasedonMinimal description length principle, Gini index, AIC, BIC, and Pearson X  X  X 2 and G 2 statistics are all derivable from the generalized entropy function. We design a dynamic programming algorithm that guarantees the best discretization based on the generalized entropy notion. Furthermore, we conducted an extensive performance evaluation of our method for several publicly available data sets. Our results show that our method delivers on the average 31% less classification errors than many previously known discretization methods.
 Keywords Discretization  X  Entropy  X  Gini index  X  MDLP  X  Chi-square test  X  G 2 test 1 Introduction Many real-world data mining tasks involve continuous attributes. However, many of the exis-ting data mining systems cannot handle such attributes. Furthermore, even if a data mining task can handle a continuous attribute, its performance can be significantly improved by replacing a continuous attribute with its discretized values. Data discretization is defined as a process of converting continuous data attribute values into a finite set of intervals and associating with each interval some specific data value. There are no restrictions on discrete values associated with a given data interval except that these values must induce some orde-ring on the discretized attribute domain. Discretization significantly improves the quality of discovered knowledge [ 8 , 30 ] and also reduces the running time of various data mining tenfold performance improvement for domains with a large number of continuous attributes with little or no loss of accuracy.

However, any discretization process generally leads to a loss of information. Thus, the goal of the good discretization algorithm is to minimize such information loss. 25 ]. There is a wide variety of discretization methods starting with naive (often referred to as unsupervised ) methods such as equal-width and equal-frequency [ 26 ], to much more or Wilks X  G 2 statistics based discretization algorithms [ 5 , 18 ]. Unsupervised discretization methods are not provided with class label information whereas supervised discretization methods are supplied with a class label for each data item value. Liu et. al. [ 26 ] introduce a nice categorization of a large number of existing discretization methods.

In spite of the wealth of literature on discretization methods, there are very few attempts to analytically compare them. Typically, researchers compare the performance of different algo-rithms by providing experimental results of running these algorithms on publicly available data sets. In [ 12 ], Dougherty et al. compare discretization results obtained by unsupervised discretization versus a supervised method proposed by Holte [ 20 ] and the entropy based method proposed by Fayyad and Irani [ 15 ]. They conclude that supervised methods are bet-ter than unsupervised discretization method in that they generate fewer classification errors. In [ 25 ], Kohavi and Sahami report that the number of classification errors generated by the discretization method of Fayyad and Irani [ 15 ] is comparatively smaller than the number of errors generated by the discretization algorithm of Auer et al. [ 2 ]. They conclude that entropy based discretization methods are usually better than other supervised discretization algorithms.

Recently, many researchers have concentrated on the generation of new discretization algorithms [ 5 , 6 , 24 , 28 , 35 ]. The goal of the CAIM algorithm ptoposed by Kurgan and Klas [ 24 ] is to find the minimum number of intervals that minimize the loss between class-attribute interdependency. Boulle [ 5 ] has proposed a new discretization method called Khiops ,which uses Pearson X  X  X 2 statistic to merge consecutive intervals in order to improve the global dependence measure. MODL is another latest discretization method proposed by Boulle [ 7 ]. This method builds an optimal criteria based on a Bayesian model. A dynamic programming approach and a greedy heuristic approach are developed to find the optimal criteria. Yang and Webb [ 36 ] have studied discretization for naive-Bayes classifiers. They have proposed a coupleofmethods,suchas proportional k-interval discretization and equal size discretization , to manage the discretization bias and variance .Bay[ 3 ] has studied multivariate discretization instead of the single variate discretization using a multivariate test of differences. All these algorithms have shown certain advantages, such as improving classification accuracy and/or complexity.

Several fundamental questions of discretization, however, remain to be answered. How these different methods are related to each other and how different or how similar are they? Is there an objective function which can measure the goodness of different approaches? If so, how would this function look like? In this paper we provide a set of positive results toward answering these questions. 1.1 Problem statement For the purpose of discretization, the entire dataset is projected onto the targeted continuous attribute. The result of such a projection is a two dimensional contingency table , C with I rows and J columns. Each row corresponds to either a point in the continuous domain, or an initial data interval. We treat each row as an atomic unit which cannot be further subdivided. Each column corresponds to a different class and we assume that the dataset has a total of J classes. A cell c ij represents the number of points with j th class label falling in the i th point (or interval) in the targeted continuous domain. Table 1 lists the basic notations for the contingency table C .

Inthemoststraightforwardway,eachcontinuouspoint(orinitialdatainterval)corresponds contains points from different classes and thus, c ij may be more than zero for several columns in the same row.
 where each row in the new table C is the combination of several consecutive rows in the original C table, and each row in the original table is covered by exactly one row in the new table.

The quality of the discretization function is measured by a goodness function, which depends on two parameters. The first parameter (termed cost ( data ) ) reflects the number of classification errors generated by the discretization function, whereas the second one (termed penalt y ( model ) ) is the complexity of the discretization which reflects the number of dis-cretization intervals generated by the discretization function. Clearly, the more discretization intervals created, the fewer the number of classification errors, and thus the cost of the data is lower. That is, if one is interested only in minimizing the number of classification errors, the best discretization function would generate I intervals X  X he number of data points in the initial contingency table. Conversely, if one is only interested in minimizing the number of intervals (and therefore reducing the penalty of the model), then the best discretization function would generate a single interval by merging all data points into one interval. Thus, finding the best discretization is to find the best trade-off between the cost ( data ) and the penalt y ( model ) . 1.2 Our contribution Our results can be summarized as follows: 1. We demonstrate a somewhat unexpected connection between discretization methods 2. We define a notion of generalized entropy and introduce a notion of generalized goodness 3. We design a dynamic programming algorithm that guarantees the best discretization 4. We conduct an extensive performance evaluation of our discretization method and 1.3 Paper outline Section 2 discusses goodness functions based on the MDLP [ 31 ], AIC, BIC [ 17 ], and good-any good discretization function should satisfy, and we prove that all the functions introduced in Sect. 2 satisfy these goodness function properties. Section 4 contains the main result of the paper. We prove there that each goodness function defined in Sect. 2 is a sum of G 2 defined by Wilks X  statistic [ 1 ] and degree of freedom of the contingency table multiplied by a function that is bounded by O ( log N ) ,where N is the number of data samples in the contingency table. Section 5 describes a generalized entropy function and formulates ageneralized notion of the goodness function. We prove then that any goodness function discussed in Section 2 is derivable from the generalized goodness function. Sect. 6 experimentally compare the new appoarach which utilizes the generalized goodness function with existing well-known discre-tization approaches, including Entropy and Chi-Merge , among others. Section 7 concludes the paper. 2 Goodness functions In this section, we introduce a list of goodness functions which are used to evaluate different discretization for numerical attributes. These goodness functions intend to measure three different qualities of a contingency table: the information quality (Sect. 2.1 ), the fitness of statistical models (Sect. 2.2 ), and the confidence level for statistical independence tests (Sect. 2.3 ). 2.1 Information theoretical approach and MDLP In the information theoretical approach, we treat discretization of a single continuous attri-bute as a 1 -dimension classification problem. The MDLP is a commonly used approach for tization fit the data, and the penalty for the discretization which is based on the complexity of discretization. Formally, MDLP associates a cost with each discretization, which has the following form: where both terms correspond to these two factors, respectively. Intuitively, when a classifi-cation error increases, the penalty decreases and vice versa.

In MDLP, the cost of discretization ( cost ( model ) ) is calculated under the assumption that there are a sender and a receiver. Each of them knows all continuous points, but the receiver is without the knowledge of their labels. The cost of using a discretization model to describe the available data is then equal to the length of the shortest message to transfer the label of message to transfer the label of all data points of each interval of a given discretization. The second term penalt y ( model ) corresponds to the coding book and delimiters to identify and based on MDLP ( Cost MDLP ) is derived as follows: and the rest corresponds to penalt y ( model ) .

Before we provide the detailed derivation for Cost MDLP , we first formally introduce a notion of entropy and show how a merge of some adjacent data intervals results in information loss as well as in the increase of the cost ( data | model ) .
 Definition 1 [ 11 ]. The entropy of an ensemble X is defined to be the average Shannon information content of an outcome: where A x is the possible outcome of x .
 simplicity, consider that we have only two intervals S 1 and S 2 in the contingency table, then the entropies for each individual interval is defined as follows: If we merge these intervals into a single interval (denoted by S 1  X  S 2 ) following the same rule, we have the entropy as follows: Further, if we treat each interval independently (without merging), the total entropy of these two intervals is expressed as H ( S 1 , S 2 ) , which is the weighted average of both individual entropies.Formally,wehave every merge operation leads to information loss. The entropy gives the lower bound of the cost to transfer the label per data point. This means that it takes a longer message to send all data points in these two intervals if they are merged ( N  X  H ( S 1  X  S 2 ) ) than sending both intervals independently ( N  X  H ( S 1 , S 2 ). However, after we merge, the number of intervals is reduced. Therefore, the discretization becomes simpler and the penalty of the model in Cost MDLP becomes smaller.
 Derivation of Cost MDLP :
For an interval S 1 , the best way to transfer the labeling information of each point in the interval is bounded by a fundamental theorem in information theory, stating that the average length of the shortest message is higher than N 1  X  H ( S 1 ) . Though we can apply the Hoffman coding to get the optimal coding for each interval, we are not interested in the absolute minimal coding. Therefore, we will apply the above formula as the cost to transfer follows. In the meantime, we have to transfer the model itself, which includes all the intervals and the coding book for transferring the point labels for each interval. The length of the message to transferring the model is served as the penalty function for the model. The cost to transfer all be approximated as Next, we have to consider the transfer of the coding book for each interval. For a given interval S i , each code will correspond to a class, which can be coded in log 2 J bits. We need to transfer such codes at most J  X  1 times for each interval, since after knowing J  X  1 classes, the remaining class can be inferred. Therefore, the total cost for the coding book, denoted as L , can be written as Given this, the penalty of the discretization based on the theoretical viewpoint is penalt y 1 ( model ) = L 1 ( I , N ) + L 2 = ( I  X  1 ) log 2 Put together, the cost of the discretization based on MDLP is Goodness function based on MDLP: To facilitate the comparison with other cost functions, we formally define a goodness function of a MDLP based discretization method applied to contingency table C to be the difference between the cost of C 0 , which is the resulting table after merging all the rows of C into a single row, and the cost of C . We will also use natural log instead of the log 2 function. Formally, we denote the goodness function based on MDLP Note that for a discretization problem, any discretization method shares the same C 0 . Thus, the least cost of transferring a contingency table corresponds to the maximum of the goodness function. 2.2 Statistical model based goodness functions A different way to look at a contingency table is to assume that all data points are gene-rated from certain distributions (models) with unknown parameters. Given a distribution, the maximal likelihood principle (MLP) can help us to find the best parameters to fit the data [ 17 ]. However, to provide a better data fitting, more expensive models (including more parameters) are needed. Statistical model selection tries to find the right balance between the complexity of a model corresponding to the number of parameters, and the fitness of the data to the selected model, which corresponds to the likelihood of the data being generated by the given model.
 In statistics, the multinomial distribution is commonly used to model a contingency table. Here, we assume the data in each interval (or row) of the contingency table are independent and all intervals are independent. Thus, the kernel of the likelihood function for the entire contingency table is: where  X  = ( X  1 | 1 , X  2 | 1 ,..., X  J | 1 ,..., X  J | I ) are the unknown parameters. Applying the i  X  I ,1  X  j  X  J . We commonly transform the likelihood to the log-likelihood as follow: According to Hand et al. [ 17 ], S L (  X ) is treated as a type of entropy term that measures how well the parameters  X  can compress (or predict) the training data.

Clearly, different discretizations correspond to different multinomial distributions (models). For choosing the best discretization model, the Akaike information criterion or AIC [ 17 ] can be used and it is defined as follows: where, the first term corresponds to the fitness of the data given the discretization model, and the second term corresponds to the complexity of the model. Note that in this model for each row we have the constraint  X  1 | i + X  X  X +  X  J | i = 1. Therefore, the number of parameters for each row is J  X  1.

Alternatively, for choosing the best discretization model based on Bayesian arguments that take into account the size of the training set N is also frequently used. The Bayesian information criterion or BIC [ 17 ] is defined as follows: In the BIC definition, the penalty of the model is higher than the one in the AIC by a factor of 1 / 2log N .
 Goodness function based on AIC and BIC: For the same reason as MDLP, we denote the goodness function of a given contingency table based on AIC and BIC as the difference between the cost of C 0 (the resulting table after merging all the rows of C into a single row), and the cost of of C .
 2.3 Confidence level based goodness functions Another way to treat discretization is to merge intervals so that the rows (intervals) and columns (classes) of the entire contingency table become more statistically dependent .In other words, the goodness function of a contingency table measures its statistical quality in terms of independence tests.
 Pearson X  X  X 2 : In the existing discretization approaches, the Pearson statistic X 2 [ 1 ]iscom-monly used to test the statistical independence. The X 2 statistic is as follows: X 2 statistic has an asymptotic  X  2 distribution with degrees of freedom df = ( I  X  1 )( J  X  1 ) , where I is the total number of rows. Consider a null hypothesis H 0 (the rows and columns are statistically independent) against an alternative hypothesis H a . Consequently, we obtain the confidence level of the statistical test to reject the independence hypothesis ( H 0 ). The confidence level is calculated as where, F  X  2 as our goodness function to compare different discretization methods that use Pearson X  X  X 2 statistic. Our goodness function is formally defined as We note that 1  X  F  X  2 pendence test [ 7 ]. The lower the P value (or equivalently, the higher the goodness), with more confidence we can reject the independence hypothesis ( H 0 ). This approach has been used in Khiops [ 5 ], which describes a heuristic algorithm to perform discretization. Wilks X  G 2 : In addition to Pearson X  X  chi-square statistic, another statistic called likelihood-is derived from the likelihood-ratio test, which is a general-purpose way of testing a null hypothesis H 0 against an alternative hypothesis H a . In this case we treat both intervals (rows) and the classes (columns) equally as two categorical variables , denoted as X and Y . i and column j ,where {  X  ij } is the joint distribution of X and Y ,and  X  i + and  X  + j are the marginal distributions for the row i and column j , respectively.

Based on the multinomial sampling assumption (a common assumption in a contingency table) and the maximal likelihood principle, these parameters can be estimated as  X   X  i + = N likelihood is maximized when  X   X  ij = c ij / N . Thus the statistical independence between the rows and the columns of a contingency table can be expressed as the ratio of the likelihoods: where the denominator corresponds to the likelihood under H a , and the nominator corres-ponds to the likelihood under H 0 .

Wilks has shown that  X  2log , denoted by G 2 , has a limiting null chi-squared distribution, as N  X  X  X  .
 For large samples, G 2 has a chi-squared null distribution with degrees of freedom equal to ( the entire contingency table, which serves as our goodness function Indeed, this statistic has been applied in discretization (though not for the global goodness function), and is referred to as class-attributes interdependency information [ 35 ]. 3 Properties of goodness functions An important theoretical question we address is how these methods introduced in Sect. 2 are related to each other and how different they are. Answering these questions helps to understand the scope of these approaches and shed light on the ultimate goal: for a given dataset, automatically find the best discretization method.

We first investigate some simple properties shared by the aforementioned goodness func-tions (Theorem 1 ). We describe four basic principles we believe any goodness function for discretization must satisfy. 2. Symmetric principle (P2): Let C j be the j -th column of contingency table C . GF ( C ) = 3. MIN principle (P3): Consider all contingency tables C which have I rows and J 4. MAX principle (P4): Consider all the contingency tables C which have I rows and J The following theorem states that all aforementioned goodness functions satisfy these four principles.
 Theorem 1 GF MDLP ,GF AIC ,GF BIC ,GF X 2 ,GF G 2 satisfy all four principles, P1 , P2 , P3 , and P4 .
 Proof We will first focus on proving for GF MDLP . The proof for GF AIC and GF BIC can be derived similarly.
 Merging principle (P1) for GF MDLP : Assuming we have two consecutive rows i and i + 1 c two rows. Then we have In addition, let  X  = ( I  X  1 ) log = ( I  X  1 ) log For I = 2,  X  = log 2 N + J  X  log 2 J &gt; 0.
 For I  X  3, we need more detailed analysis. Let f = N /( I  X  1 )  X  1. Given this, we have Based on the first order derivative of and second order derivative analysis, we can see that (
I  X  2 ) log we have Symmetric principle (P2) for GF MDLP : This can be directly derived from the symmetric property of entropy.
 MIN principle (P3) for GF MDLP : Since the number of rows ( I ), the number of samples ( N ), and the number of classes ( J ) are fixed, we only need to maximize N  X  H ( S 1 ,..., S I ) . MAX principle (P4) for GF MDLP : Since the number of rows ( I ), the number of samples Now, we prove the four properties for GF X 2 .
 Merging principle (P1) for GF X 2 : Assuming we have two consecutive rows i and i + 1 c two rows. Then we have We note that the degree of freedom in the original contingency table is ( I  X  1 )( J  X  1 ) and the second one is ( I  X  2 )( J  X  1 ) . In addition, we have for any t &gt; 0, F  X  2 F Symmetric principle (P2) for GF X 2 : This can be directly derived from the symmetric property of X 2 .
 MIN principle (P3) for GF X 2 : Since the number of rows ( I ), the number of samples ( N ), we can see that M j = N / J .

X 2 = Since X 2  X  0, we achieve the minimal of X 2 .
 MAX principle (P4) for GF X 2 : Since the number of rows ( J ), the number of samples ( N ), and the number of classes ( J ) are fixed, we only need to maximize X 2 .
 Note that this bound can be achieved in our condition. Basically, in any row k , we will have one cell c kj = N k . Therefore, F  X  2 best possible discretization given existing conditions.

The proof for GF G 2 can be derived similarly from GF MDLP and GF X 2 . 4 Equivalence of goodness functions In this section, we analytically compare different discretization goodness functions intro-duced in Sect. 2 . In particular, we find some rather surprising connection between these seemingly quite different approaches: the information theoretical complexity, the statistical fitness, and the statistical independence tests. We basically prove that all these functions can be expressed in a uniform format as follows: where, df is a degree of freedom of the contingency table, N is the number of data points, I is the number of data rows in the contingency table, J is the number of class labels and f is bounded by O ( log N ) .Thefirstterm G 2 corresponds to the cost of the data given a discretization model ( cost ( data | model ) ), and the second corresponds to the penalty or the complexity of the model ( penalt y ( model ) ).

To derive this expression, we first derive an expression for the cost of the data for different goodness functions discussed in Sect. 2 (Sect. 4.1 ). This is achieved by expressing G 2 sta-tistics through information entropy (Theorem 3 ). Then, using a Wallace X  X  result [ 33 , 34 ]on approximating  X  2 distribution with a normal distribution, we transform the goodness function based on statistical independence tests into the format of Formula 10 . Further, a detailed ana-lysis of function f reveals a deeper relationship shared by these different goodness functions (Sect. 4.3 ). 4.1 Unifying the cost of data to G 2 In the following, we establish the relationship among entropy , log-likelihood and G 2 .This is the first step for an analytical comparison of goodness functions based on the information theoretical, the statistical model selection, and the statistical independence test approaches.
First, it is easy to see that for a given contingency table, the cost of the data transfer log likelihood S L (used in the statistical model selection approach) as the following theorem asserts.
 Theorem 2 For a given contingency table C I  X  J , the cost of data transfer ( cost 1 ( data | model )) is equal to the log-likelihood S L ,i.e.
 Proof N  X  H ( S 1 ,..., S I ) = X  The next theorem establishes a relationship between entropy criteria and the likelihood independence testing statistics G 2 . This is the key to discover the connection between the information theoretical and the statistical independence test approaches.
 Theorem 3 Let C be a contingency table. Then Proof G 2 / 2 = X  log = Consequently, we rewrite the goodness functions GF MDLP , GF AIC and GF BIC as follows. For the rest of the paper we use the above formulas for GF MDLP , GF AIC and GF BIC .
It has long been known that they are asymptotically equivalent. The next theorem provides tool to connect the information theoretical approach and the statistical independence test approach based on Pearson X  X  chi-square ( X 2 ) statistic.
 Theorem 4 [ 1 ] . Let N be the total number of data values in the contingency table T of I  X  J dimensions. If the rows ( columns ) of contingency table are independent, then probability of X 2  X  G 2 = 0 converges to one as N  X  X  X  .
 In the following, we mainly focus on the asymptotic properties shared by X 2 and G 2 based cost functions. Thus, our further discussions on G 2 can also be applied to X 2 .
Note that Theorems 2 and 3 basically establish the basis for Formula 10 of goodness functions based on the information theoretical approach and statistical model selection approaches. Even though Theorems 3 and 4 relate the information theoretical approach (based on entropy) to the statistical independence test approach (based on G 2 and X 2 ), it is still unclear how to compare them directly since the goodness function of the former one is basedonthetotal cost of transferring the data and the goodness function of the latter one is the confidence level for a hypothesis test. Section 4.2 presents our approach on tackling this issue. 4.2 Unifying statistical independence tests In order to compare the quality of different goodness functions, we introduce a notion of equi-valent goodness functions. Intuitively, the equivalence between goodness functions means that these functions rank different discretization of the same contingency table identically. Definition 2 Let C be a contingency table and GF 1 ( C ) , GF 2 ( C ) be two different goodness functions. GF 1 and GF 2 are equivalent if and only if for any two contingency tables C 1 and C , GF 1 ( C 1 )  X  GF 1 ( C 2 )  X  X  X  GF 2 ( C 1 )  X  GF 2 ( C 2 ) .

Using the equivalence notion, we transform goodness functions to different scales and/or to different formats. In the sequel, we apply this notion to compare seemingly different goodness functions.

The relationship between the G 2 and the confidence level is rather complicated. It is clearly not a simple one-to-one mapping as the same G 2 may correspond to very different confidence level depending on degrees of freedom of the  X  2 distribution and, vice versa the same confidence level may correspond to very different G 2 values. Interestingly enough, such many-to-many mapping actually holds the key for the aforementioned transformation. Intui-tively, we have to transform the confidence interval to a scale of entropy or G 2 parameterized by the degree of freedom for the  X  2 distribution.
 Our proposed transformation is as follows.
 is, the following equality holds: where, F  X  2 we define as a new goodness function for C .
 The next theorem establishes the equivalence between a goodness functions GF G 2 and Theorem 5 The goodness function G F G 2 = u ( G 2 ) is equivalent to the goodness function Proof Assuming we have two contingency tables C 1 and C 2 with degree of freedom df 1 and df 2 , respectively. Their respective G 2 statistics are denoted as G 2 1 and G 2 2 .Clearly,wehave This basically establishes the equivalence of these two goodness functions.
 The newly introduced goodness function GF G 2 is rather complicated and it is hard to find for it a closed form expression. In the following, we use a theorem from Wallace [ 33 , 34 ]to derive an asymptotically accurate closed form expression for a simple variant of GF G 2 . u ( If df  X  X  X  and w( t )&gt;&gt; 0 , then Therefore, Thus, we can have the following goodness function: Similarly, function GF  X  2 is obtained from GF G 2 by replacing in the GF G 2 expression G 2 can be (asymptotically) expressed in the same closed form (Formula 10 ). Specifically, all of them can be decomposed into two parts. The first part contains G 2 , which corresponds to the cost of transferring the data using information theoretical view. The second part is a linear function of degrees of freedom, and can be treated as the penalty of the model using thesameview. 4.3 Penalty analysis In this section, we perform a detailed analysis of the relationship between penalty functions of these different goodness functions . Our analysis reveals a deeper similarity shared by these functions and at the same time reveals differences between them.

Simply put, the penalties of these goodness functions are essentially bounded by two extremes. On the lower end, which is represented by AIC, the penalty is on the order of degree of freedom, O ( df ) . On the higher end, which is represented by BIC, the penalty is O ( df log N ) .
 Penalty of GF G 2 (Formula 15 ): The penalty of our new goodness function GF G 2 = u 2 ( G 2 ) is between O ( df ) and O ( df log N ) . The lower bound is achieved, provided that G 2 being strictly higher than df ( G 2 &gt; df ). Lemma 1 gives the upper bound.
 Lemma 1 G 2 is bounded by 2 N log J(G 2  X  2 N log J). Proof cases corresponding to the lower bound and upper bound of G 2 , respectively. The second case is further subdivided into two subcases. 1. If N / df  X  N /( IJ ) = c ,where c is some constant, the penalty is O ( df ) . 2. If N  X  X  X  and N / df  X  N /( IJ )  X  X  X  , the penalty is Penalty of GF MDLP (Formula 11 ): The penalty function f derived in the goodness function based on the information theoretical approach can be written as Here, we again consider two cases: 1. If N /( I  X  1 ) = c ,where c is some constant, we have the penalty of MDLP is O ( df ) . 2. If N &gt;&gt; I and N  X  X  X  , we have the penalty of MDLP is O ( df log N ) .
 Note that in the first case, the contingency table is very sparse ( N /( IJ ) is small). In the second case, the contingency table is very dense ( N /( IJ ) is very large).

To summarize, the penalty can be represented in a generic form as df  X  f ( G 2 , N , I , J ) (Formula 10 ). This function f is bounded by O ( log N ) . Finally, we observe that different penalty clearly results in different discretization. The higher penalty in the goodness function the following theorem.
 Theorem 7 Given an initial contingency table C with log N  X  2( the condition for the penalty of B I C is higher than the penalty of A I C ), let I AIC be the number of intervals of the discretization generated by using G F AIC and I BIC be the number of intervals of the discretization generated by using G F BIC .ThenI AIC  X  I BIC .
 Note that this is essentially a direct application of the well-known facts from statistical machine learning research: higher penalty will result in more concise models [ 17 ].
Finally, we note that several well-known discretization algorithms based on local indepen-dence test include ChiMerge [ 23 ]andChi2[ 27 ], etc. Specifically, for consecutive intervals, these algorithms perform a statistical independence test based on Pearson X  X  X 2 or G 2 .If they could not reject the independence hypothesis for those intervals, they merge them into one row. A simple analysis in [ 22 ] suggests that the local merge condition essentially shares the penalty in the same order of magnitude as GF AIC . Interested readers can refer [ 22 ]for detailed discussion. 5 Parametrized goodness function The goodness functions discussed so far are either entropy or  X  2 or G 2 statistics based. In this section we introduce a new goodness function which is based on gini index [ 4 ]. Gini index based goodness function is strikingly different from goodness functions introduced so far. In this section we show that a newly introduced goodness function GF Gini along with the goodness functions discussed in Sect. 2 are all can be derived from a generalized notion of entropy [ 29 ]. 5.1 Gini based goodness function Let S i be a row in contingency table C . Gini index of row S i is defined as follows [ 4 ]: and Cost Gini ( C ) = I i = 1 N i  X  Gini ( S i ) .

The penalty of the model based on gini index can be approximated as 2 I  X  1 (see detailed derivation in the technical report [ 22 ]). The basic idea is to apply a generalized MDLP cost of transferring the coding book as well as necessary delimiters ( penalt y ( model ) )are treated as the complexity measure. Therefore, the gini index can be utilized to provide such a measure. Thus, the goodness function based on gini index is as follows: 5.2 Generalized entropy In this subsection, we introduce a notion of generalized entropy, which is used to uniformly represent a variety of complexity measures, including both information entropy and gini index by assigning different values to the parameters of the generalized entropy expression. Thus, it serves as the basis to derive the parameterized goodness function which represents all the aforementioned goodness functions, such as GF MDLP , GF AIC , GF BIC , GF G 2 ,and GF Gini , in a closed form.
 Definition 4 [ 29 , 32 ]. For a given interval S i , the generalized entropy is defined as When  X  = 1, we can see that When  X   X  0, Let C I  X  J be a contingency table. We define the generalized entropy for C as follows. Lemma 2 H  X  [ p 1 ,..., p J ]= J j = 1 p i ( 1  X  p  X  i )/ X  is concave when  X &gt; 0 . Proof Thus, Let C I  X  J be a contingency table., We define the generalized entropy for C as follows. Similarly, we have Theorem 8 There always exists information loss for the merged intervals : H  X  ( S 1 , S 2 )  X  H  X  ( S 1  X  S 2 ) Proof This is the direct application of the concaveness of the generalized entropy. 5.3 Parameterized goodness function Based on the discussion in Sect. 4 , we derive that different goodness functions basically can be decomposed into two parts. The first part is for G 2 , which corresponds to the information theoretical difference between the contingency table under consideration and the marginal distribution along classes. The second part is the penalty which counts the difference of com-plexity for the model between the contingency table under consideration and the one-row contingency table. The different goodness functions essentially have different penalties ran-ging from O ( df ) to O ( df log N ) .

In the following, we propose a parameterized goodness function which treats all the aforementioned goodness functions in a uniform way.
 Definition 5 Given two parameters,  X  and  X  ,where0 &lt; X   X  1and0 &lt; X  , the parameterized goodness function for contingency table C is represented as The following theorem states the basic properties of the parameterized goodness function. Theorem 9 The parameter goodness function G F  X , X  , with  X &gt; 0 and 0 &lt; X   X  1 , satisfies all four principles, P1 , P2 , P3 , and P4 .
 Proof Merging principle (P1) for GF  X , X  : Assuming we have two consecutive rows i and i c two rows. Then we have In addition, we have Thus, we have GF  X , X  ( C )&lt; GF  X , X  ( C ) .
 Symmetric principle (P2) for GF  X , X  : This can be directly derived from the symmetric property of entropy.
 MIN principle (P3) for GF  X , X  : Since the number of rows ( I ), the number of samples ( N ), and the number of classes ( J ) are fixed, we only need to maximize N  X  H ( S 1 ,..., S I ) .By the concaveness of the H  X  (Theorem 8 ), MAX principle (P4) for GF  X , X  : Since the number of rows ( I ), the number of samples ( N ), and the number of classes ( J ) are fixed, we only need to minimize N  X  H  X  ( S 1 ,..., S J ) . Note that the proof of GF  X , X  immediately implies that the four principles hold for GF AIC and GF BIC .
 By adjusting different parameter values, we show how goodness functions defined in Sect. 2 can be obtained from the parametrized goodness function. We consider several cases: 2. Let  X  = 1 / log N and  X   X  0. Then GF 1 / log N  X   X  0 = GF AIC . 3. Let  X  = 1 / 2and  X   X  0. Then GF 1 / 2 , X   X  0 = GF BIC . 4. Let  X  = const , X   X  0and N &gt;&gt; I .Then GF const , X   X  0 = G 2  X  O ( df log N ) = 5. Let  X  = const , X   X  0, and G 2 = O ( N log J ), N /( IJ )  X  X  X  .Then GF const , X   X  0 = The parameterized goodness function not only allows us to represent the existing goodness functions in a closed uniform form, but, more importantly, it provides a new way to understand and handle discretization. First, the parameterized approach provides a flexible framework to access a large collection (potentially infinite) of goodness functions. Any valid pair of  X  and  X  corresponds to a potential goodness function. Note that this treatment is in the same spirit of regularization theory developed in the statistical machine learning field [ 16 , 19 ].
Secondly, finding the best discretization for different data mining tasks for a given dataset is transformed into a parameter selection problem. However, it is an open problem how we may automatically select the parameters without running the targeted data mining task. In other words, can we analytically determine the best discretization for different data mining tasks for a given dataset? This problem is beyond the scope of this paper and we plan to investigate it in future work.

Finally, the unification of goodness functions allows to develop efficient algorithms to discretize the continuous attributes with respect to different parameters in a uniform way. This is the topic of the next subsection. 5.4 Dynamic programming for discretization Thissectionpresentsadynamicprogrammingapproachtofindthebestdiscretizationfunction to maximize the parameterized goodness function. Note that the dynamic programming has been used in discretization before [ 14 ]. However, the existing approaches do not have a global goodness function to optimize, and almost all of them have to require the knowledge of targeted number of intervals. In other words, the user has to define the number of intervals for discretization. Thus, the existing approaches can not be directly applied to discretization for maximizing the parameterized goodness function.

In the following, we introduce our dynamic programming approach for discretization. To facilitate our discussion, we use GF for GF  X , X  , and we simplify the GF formula as follows. Since a given table C , N  X  H  X  ( S 1  X  X  X  X  X  S I ) (the first term in GF , Formula 17 )isfixed, we define Clearly, the minimization of the new function F is equivalent to maximizing GF .Inthe following, we will focus on finding the best discretization to minimize F .First,wedefine a sub-contingency table of C as C [ i : i + k ]={ S i ,..., S i + k } ,andlet C 0 [ i : i + k ]= S  X  X  X  X  X  S i + k be the merged column sum for the sub-contingency table C [ i : i + k ] . Thus, the new function F of the row C 0 [ i : i + k ] is: Let C be the input contingency table for discretization. Let Opt ( i , i + k ) be the minimum of the F function from the partial contingency table from row i to i + k , k &gt; 1. The optimum which corresponds to the best discretization can be calculated recursively as follows:
Opt ( i , i + k ) = min F ( C 0 [ i : i + k ] ), gramming to find the discretization with the minimum of the goodness function, which are described in Algorithm 1 . The complexity of the algorithm is O ( I 3 ) ,where I is the number of intervals of the input contingency table C .
 Algorithm 1 Discretization (Contingency table C I  X  J ) 6 Experimental results The major goal of our experimental evaluation is to demonstrate that the dynamic program-ming approach with appropriate parameters can significantly reduce the classification errors compared with the existing discretization approaches.

We chose 12 datasets from the UCI machine learning repository [ 37 ]. Most of the datasets have been used in the previous experimental evaluation for discretization study [ 12 , 26 ]. Table 2 describes the size and the number of continuous and nominal features of each dataset. We apply discretization as a preprocessing step for two well-known classification methods: the C4.5 decision tree and Naive Bayes classifier [ 17 ]. For comparison purpose, we apply four discretization methods: equal-width (EQW), equal-frequency (EQF), Entropy [ 15 ], and ChiMerge [ 23 ]. The first two are unsupervised approaches and the last two are supervised approaches. We set the number of discretization intervals to be 10 for the first two. All their implementations are from Weka 3 [ 38 ].

Our dynamic programming approach for discretization (referred to as Unification in the experimental results) depends on two parameters,  X  and  X  . How to analytically determine the best parameters which can result in the minimal classification error is still an open question and beyond the scope of this paper. Here, we apply an experimental-validation approach to choose the optimal parameters  X  and  X  . For a given dataset and the data mining task, we create a 10  X  10 uniform grid for 0  X   X   X  1and0  X   X   X  1 In addition, we use a value 10  X  5 to replace 0 for  X  since it cannot be equal to 0. Then we apply the dynamic programming at each grid point to discretize the dataset. We score each point using the mean classification the unification approach with parameters from the 10  X  10 grid points. Figure 1 b illustrates glass dataset [ 37 ]. Clearly, we can see that different  X  and  X  parameters can result in very different classification error rates. Given this, we choose the  X  and  X  pair which achieves the minimal classification error rate as the selected unification parameters for discretization. For instance, in these two figures, we choose  X  = 0 . 3and  X  = 0 . 3 as the parameters to discretize Note that the objective of using five trials instead of only one is to choose parameters in a more robust fashion to avoid outliers.

Finally, for each of the discretization method (our Unification method with the best pre-dicated parameter), we run a five-trial five-fold cross-validation, and report their mean and and is different from the trials in the parameter selection process.
Tables 3 and 4 show the experimental results for C4.5 and Naive Bayes Classifier, res-pectively. In the left part of each table, we show the mean classification error and standard deviation using different discretization methods (the first one, Continuous corresponding to no-discretization). The right part of each table shows the percentage differences bet-ween two leading discretization approaches, Entropy and ChiMerge , with our new approach Unification . The last column chooses the minimal classification errors from all five existing approaches to compare with the unification approach.

We can see that the unification approach performs significantly better than the existing approaches. First, based on the average classification error for all the 12 datasets, the unifi-cation is the best among all these approaches (14 . 45% error rate for C4.5 and 10 . 60% for Naive Bayes classifier). For C4.5, it reduces the error rate on an average of 19 . 40% compared with Entropy , and reduces the error rate on average of 26 . 65% compared with ChiMerge . For Naive Bayes classifier, it reduces the error rate on an average of 58 . 74% compared with Entropy , and reduces the error rate on an average of 20 . 82% compared with ChiMerge .The overall improvement is on an average of 31% in terms of classification error rate. Finally, in 9 out of 12 datasets for C4.5, the unification approach shows better or equal performance with the best existing approach. In other 3 datasets, the performance are fairly close to the minimal error rate as well. For Naive Bayes classifier, the unification method perform the best in 10 out of 12 datasets and the second for the other 2 datasets. 7 Conclusions In this paper, we introduced a generalized goodness function to evaluate the quality of a discretization method. We have shown that seemingly disparate goodness functions based on entropy, AIC, BIC, Pearson X  X  X 2 , Wilks X  G 2 , and Gini index are all derivable from our generalized goodness function. Furthermore, the choice of different parameters for the generalized goodness function explains why there is a wide variety of discretization methods. Indeed, difficulties in comparing different discretization methods were widely known. Our results provide a theoretical foundation in understanding these difficulties and offer rationale as to why evaluation of different discretization methods for an arbitrary contingency table is difficult. We have designed a dynamic programming algorithm that for given set of parameters of a generalized goodness function provides an optimal discretization which achieves the minimum of the generalized goodness function. We have conducted an extensive performance tests for a set of publicly available data sets. Our experimental results demonstrate that our discretization method consistently outperforms the existing discretization metods on the average by 31%. These results clearly validate our approach and open a new way of tackling discretization problems.
 References Author X  X  biography
