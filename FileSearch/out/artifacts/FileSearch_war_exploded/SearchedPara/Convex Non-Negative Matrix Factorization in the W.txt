
Factorizing matrices is a fundamental step in many data mining and machine leaning approaches. It can be found in virtually all application areas of data mining and machine learning, including computational biology, computer vision, activity recognition, social network analysis, and informa-tion extractions, among others. Recent work in machine learning has focused on matrix factorizations that address particular constraints inherent to the nature of certain data which of course should be accounted for in statistical data analysis. In particular, non-negative matrix factorization (NMF) focuses on the analysis of data matrices whose elements are non-negative, a common occurrence in data sets derived for example from text and images. More precisely, it aims at factorizing a non-negative input matrix V into two non-negative matrix factors Convex NMF approaches further restrict the columns of W to be convex combinations of data points in V , in order to hull NMF can be easily applied to large scale data sets. Specifically, we apply it to two large-scale, real world data sets, one consisting of 1.6 million images and one consisting We proceed as follows. We start of by reviewing NMF. Then, in Section III, we will develop convex-hull NMF. Sec-tion IV will present our experimental evaluation on several synthetic and real-world data sets. Before concluding, we will touch upon related work.

Assume an m  X  n input data matrix V = ( v 1 , . . . , v n ) consisting of n column vectors of dimensionality m . We consider factorizations of the form The resulting matrix W contains a set of k n basis vectors which are linearly combined using the coefficients in H to represent the data. Common approaches to achieve such a factorization include Principal Component Analysis (PCA) [1], Singular Value Decomposition (SVD) [2], Vector Quantization (VQ), or non-negative Matrix Factorization (NMF) [3]. Note that the factorizations resulting from these methods differ since each method imposes other constraints: PCA constrains W to be composed of orthonormal vectors and results in a holistic H , VQ constrains H to unary vectors, and NMF assumes V , W and H to be non-negative matrices and often leads to part-based, sparse representations of the data. That is, in the case of NMF, the matrix W often represents meaningful parts and H tends to be sparse. From this point of view, NMF marks a middleground between distributed and unary representations [3]. In addition to the the data-compression aspects of NMF, the intuitive interpretability of the resulting factors makes it especially interesting for data-mining.

Various variants and improvements to NMF have been introduced in recent year. For example, Cai et al. [4] present a matrix factorization that obeys the geometric data structue. In [5] a speed improvement to NMF is achieved using a novel algorithm based on an alternating nonnegative least squares framework. Another interesting variation is presented in [6] where optimization is based on a block-iterative acceleration technique. In this work, however, we build on Semi-NMF and Convex-NMF (C-NMF) recently introduced by Ding et. al [7]. Semi-NMF relaxes the non-negativity constraint of NMF by allowing V and W to have mixed signs thereby extending the applicability of the method. Convex-NMF, on the other hand, represents the data matrix V as a convex combination of data points, i.e. where each column i of G is a stochastic vector that obeys Moreover, although the iterative algorithm comes down to simple matrix multiplications, the size of the involved matri-ces quickly becomes another limiting factor (similar to the intermediate blowup problem in tensor decomposition [9]), since V T V results in an n  X  n matrix. Switching to an online update rule would avoid memory issues but it would at the same time introduce additional computational overhead. Overall, we can say that C-NMF does not scale to large data sets. In the following, we will present Convex-Hull NMF (CH-NMF) , which is a novel C-NMF method for large-scale data analysis.

Convex-Hull NMF aims at a data factorization based on the data points residing on the data convex hull. Such a data reconstruction has two interesting properties: first, the basis vectors are real data points and mark, unlike in most other clustering/factorization techniques, the most extreme and not the most common data points. Second, any data point can be expressed as a convex and meaningful combination of these basis vectors. This, however, offers interesting new opportunities for data interpretation, as indicated in Fig. 2 and further demonstrated in Section IV.

Following Ding et al. [7], we consider a factorization of the form where V  X  R m  X  n , G  X  R n  X  k , H  X  R n  X  k . We further restrict the columns of G and H to convexity, i.e., We note again that Ding et al. [7] also consider convex combinations but not for the matrix H . In other words, in 2: Project V onto the 2D-subspaces 5: Optimize 6: Optimize see e.g. [10], [11]. Consequently, computing the convex hull of large data-sets quickly becomes practically infeasible. In this paper, we therefore propose an approximate solution that subsamples the convex hull but still offers convenient data reconstruction.

Our approach exploits the fact that any data point on the convex hull of a linear lower dimensional projection of the data also resides on the convex hull in the original data dimension. Formally, since V contains finitely many points and therefore forms a polytope in R m , we can resort to the main theorem of polytope theory , see e.g. [12].
Theorem 1 (Main Theorem of Polytope Theory): Every image of a polytope P under an affine map  X  : x  X  Mx + t is a polytope.
 In particular, every vertex of an affine image of P , i.e., every point of the convex hull of the image of P , corresponds to a vertex of P .

In other words, computing the convex hull of several 2D affine projections of the data offers a way of subsampling H cvx ( V ) . Moreover, this is an efficient way as computing the convex hull of a set of 2D points can be done in O ( n log n ) time, [13].
 This subsampling strategy is the main idea underlying Convex-Hull NMF, and its soundness directly follows from Theorem 1. Moreover, various methods can be used for linearly projecting the data to a 2D space: (a) complete projections : for lower dimensional data (b) random selection : randomly select the dimensions Since p n , solving (5) can be done efficiently using a quadratic programming solver. Note that the data di-mensionality is m . The convex hull projection only served to determine a candidate set; all further computations are carried out in the original data space.

By obtaining a sufficient reconstruction accuracy for S , we can set and thereby select k convex hull data points for solving Eq. (4). We found that I usually results in unary represen-tations. If this is not the case, we simply map SI to their nearest neighboring data point in S .

Given X , the computation of the coefficients H is straight forward. For smaller data-sets it is possible to use the iterative update rule from Eq. (2). However, since we do not further modify the basis vectors X , we can also find an optimal solution for each data point v i individually J can be easily parallelized.
 In the following, we present experimental evaluation of CH-NMF on three data-sets. We decided for three experi-mental setups designed for evaluating different aspects of the proposed algorithm. Our first experimental setup evaluates and compares the run-time performance against C-NMF using synthetically generated data. As evaluation of C-NMF and other factorization methods for very large data-sets is often not feasible, we limit the maximum number of generated data to 4000 points. As we are mainly interested in the analysis of large real-world data sets, the last two experiments evaluate CH-NMF for two very large data-sets. Here, for evaluating the performance on high dimensional data, we decided for one rather low-dimensional and one high-dimensional data set. Thus, the first large data-set has only 80 dimensional feature vectors consisting of player activity scores for the Massively Multiplayer Online Game million images of the Tiny image data-set [16]. The images are at size 32  X  32 resulting in 3072 dimensional feature vectors.
 Experimental evaluation is based on a basic Python implementation running on a standard Intel-Quadcore 2.GHz computer. For optimization we used the cvxopt library by Dahl and Vandenberghe ( http://abel.ee.ucla.edu/cvxopt/ ). Although CH-NMF can be easily parallelized, we used a strictly serial implementation.
 A. Synthetic Data
Similar to the experimental procedure by Ding et al. [7], we evaluate the run-time performance using varying number of data points sampled from three randomly positioned to 6. The resulting average reconstruction errors can be seen in Fig. 4. The resulting runtime performance is shown in Fig. 5, both Figures have log space scaling. Following Ding et al. [7], we carried out 100 iterations for each approach.
Overall CH-NMF leads to a very accurate data recon-struction. This follows the definition of the convex hull that is at the basis of our work. Using the convex-hull for reconstructing the inner data-points must lead to a perfect reconstruction. Since we exploit this property and subsample the convex hull we usually obtain very accurate data reconstruction. In comparison, C-NMF often converges against basis vectors that lie within data clusters. While this is by itself a wanted property, it leads to slightly worse reconstruction. A reason for this could be the non-negativity constraint of coefficients H T . This conic reconstruction limits perfect reconstruction to data-points within the conic hull of basis vectors [17].
 Regarding runtime performance, it can be seen that while CH-NMF initially takes longer for fewer data-samples (up to 1000), its computation time increases moderately with an increase of data samples. This is explained by the moderate increase of convex hull data points  X ( Gaussian in the plane [15]. For k mixtures of Gaussians we usually have far less than  X ( k points. It also holds for larger number of samples, e.g. it takes about 54 seconds for 50000 data points. It should be noted that about 90% of the time is spent optimizing J = v requirements for CH-NMF are rather low as it requires to store l convex hull data points at maximum.

This data-set consists of recordings of the online ap-pearance of characters in the computer game World of Warcraft R  X  . Since it is  X  X o our knowledge X  the first time that vast data recordings of Massively Multiplayer Online games are considered as a source for data mining, we will very experienced level 80 characters has a higher chance for achievements than a guild of level 10 players. Also, a level histogram gives an indicator for player activity over time. If players are continuously staying with a particular guild, we expect an equally distributed level histogram, as the characters are continuously increasing their level over time. Following [18], we use logarithmic histogram values in our analysis. In total, we collected 150 million votes of 18 million characters belonging to 1.4 million guilds. revealed some very interesting structures. Fig. 7 shows a projection of all level-guild histograms into the space spanned by the CH-NMF basis vectors. We decided for 8 basis vectors since we assumed that one basis vector would function as a unity vector and encode a range of 10 levels. However, the actual outcome is quite different from what we expected, as can be seen in Fig. 6. Here it shows that the interpretability of basis vectors can be very useful for data-mining. By describing the basis vectors of CH-NMF that correspond to a particular guild, we gain an intuitively understandable distribution of all World of Warcraft R  X  guilds. In particular, it is interesting that the majority of guilds is very close to the  X  X eldom active X  guild. Thus, far more guilds or players seem to be casual gamers. In contrast, none of the most successful guilds belong to data. Using larger numbers of basis vectors added more and more structured images to the set of basis images. Due to the high dimension of the data used application of The reason is mainly the expensive computation of the covariance matrix required for the convex-hull projection. Here, the use of a faster projection scheme as proposed in Section III might be useful. Computing the covariance and the eigenvectors took about 1-2 days as we did not try to further optimize the process and simply iterated over all images. Application of CH-NMF took about 6 hours (approx. 3 hours for finding basis vectors and 3 hours for computing coefficients).

Machine learning and data mining techniques typically consists of two parts: the model and the data. Most effort in recent years has gone into the modeling part. Large-scale datasets, however, allow one to move into the opposite direction [16], [20]: how much can the data itself help us to solve the problem. This direction is particularly appealing given that the Internet nowadays offers a plentiful supply of large-scale datasets for many challenging tasks.
Motivated by this, we have presented a data-driven NMF approach, called convex-hull NMF , that is fast and scales well. The key idea is to restrict the  X  X lusters X  to be com-binations of vertices of the convex hull of the dataset; thus directly exploring the data itself to solve the convex NMF problem. Our experimental results reveal that convex-hull NMF can indeed effectively extract meaningful  X  X lusters X  from datasets containing millions of images and rating.
For future work it is interesting to apply convex-hull NMF to other challenging data-sets, such as Wikipedia, Netflix, Facebook, or the blogsphere, and to use it for applications,
