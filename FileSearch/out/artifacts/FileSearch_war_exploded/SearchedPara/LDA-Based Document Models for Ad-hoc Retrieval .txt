 Search algorithms incorporating so me form of topic model have a long history in information retrie val. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in effectiveness in information retr ieval is mostly unknown. In this paper, we study how to efficien tly use LDA to improve ad-hoc retrieval. We propose an LDA-ba sed document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is em ployed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained w ith reasonable efficiency. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Retrieval models .
 Theory, Experimentation Information Retrieval, Language Model, Latent Dirichlet Allocation (LDA), Topic Model, Document Model. Representing the content of text documents is a critical part of any approach to information retrieval (IR). Typically, documents are represented as a  X  X ag of words X , meaning that the words are assumed to occur independently. To capture important relationships between words, researchers have proposed approaches that group words into  X  X opics X . Techniques such as word clustering and document cluste ring have been used for many years to enhance document repr esentations. Word or term clustering, for example, was studi ed in the 60s (Sparck Jones, 1971). The well-known Latent Seman tic Indexing (LSI) technique was introduced in 1990 (Deerwester et al, 1990). More recently, Hoffman (1999) described the probabilistic Latent Semantic Indexing (pLSI) technique. This appr oach uses a latent variable model that represents documents as mixtures of topics. Although Hoffman showed that pLSI outperformed LSI in a vector space model framework, the data sets used were small and not representative of modern IR environments. Specifically, the collections in these experiment s only contained a few thousand document abstracts. recently been an area of considerab le interest in machine learning. Latent Dirichlet Allocation or LDA (Blei et al, 2003), has quickly become one of the most popular probabilistic text modeling techniques in machine learning and has inspired a series of research papers (e.g., Girolami and Kaban, 2005; Teh et al, 2004). LDA has been shown to be effec tive in some text-related tasks effectiveness of using LDA in IR tasks remains mostly unknown. overcomes the drawbacks of previ ous topic models such as pLSI (Hoffman, 1999). Language modeling (Ponte and Croft, 1998; Berger and Lafferty, 1999), which is one of the most popular statistically principled approaches to IR, is also a generative model, motivating us to examine LDA-based document representations in the language modeling framework. builds topic models using doc ument clusters, known in the machine learning literature as the mixture of unigrams model (McCallum, 1999). Liu and Croft (2004) showed that document clustering can improve retrieval effectiveness in the language modeling framework. Retrieval ba sed on cluster models (referred to here as cluster-based retrie val) performed consistently well across several TREC collections , and significant improvements over document-based retrieval m odels were reported. In the language modeling framework, the cluster-based topic models were used to smooth the probabilities in the document model (Liu and Croft, 2004). As a much simpler topic model, the mixture of unigrams model generates a w hole document from one topic under the assumption that each document is related to exactly one topic. This assumption may, however , be too simple to effectively model a large collection of documen ts. In contrast, LDA models a document as a mixture of multiple topics. model of documents, and the en couraging results with topic models in previous work, we carried out a detailed evaluation of the effectiveness of LDA-based retrieval in large collections. Azzopardi et al. (2004) also disc ussed the applications of LDA models and reported inconclusi ve results on several small collections. In this paper, we propose an LDA-based document model for IR, evaluate it on TREC collections, and discuss efficiency issues. In Section 2, we discuss related work in topic model based retrieval, including pL SI and the cluster model. We present the new retrieval model based on LDA in Section 3, and compare the complexity of LDA with the clustering algorithms used in Liu and Croft (2004) in Section 4. We then describe the data sets and experi mental methods in Section 5. Retrieval performance is discussed in S ection 6. Finally, Section 7 concludes and discusse s possible directions for future work. The probabilistic Latent Seman tic Indexing model, which was introduced by Hoffman (1999) quickly gained acceptance in a number of text modeling applicati ons. pLSI, also called the aspect model, is a latent variable m odel for general co-occurrence data which associates an unobserved cl ass (topic) variable with each observation (i.e., with each occurrence of a word). The roots of pLSI go back to Latent Semantic Indexing/Analysis (Deerwester et al, 1990). pLSI was designed as a discrete counterpart of LSI to provide a better fit to text data. It can also be regarded as an attempt to relax the assumption ma de in the mixture of unigrams model that each document is generated from a single topic. pLSI models each document as a mixture of topics. The following process generates documents in the pLSI model: 1) Pick a topic mixture distribution P ( .|d ) for each document d , 3) Generate the word token w with probability P ( w|z ). The probability of generating a document d , as a bag of words 1 ( d N is the number of words of document d ), is: Space Model framework, albeit on small collections. He exploited pLSI both as a context-dependent unigram model to smoothen the empirical word distributions and as a latent space model to provide a low-dimensional doc ument/query representation. Significantly better retrieval performance over the standard term matching method based on the raw term frequencies and Latent Semantic Indexing (LSI) was re ported on all four collections, which contained 1033, 1400, 3204, and 1460 document abstracts. The smoothing parameter was optimized by hand for each collection. sizes and the document lengths in the collections are far from representative of realistic IR environments, making the effectiveness of the mixture-of-topics model on IR tasks still unclear. In addition, the baseline retrieval model was far from state-of-the-art. The pLSI model itself has a problem in that its generative semantics are not well-defined (Blei et al, 2003); thus there is no natural way to predic t a previously unseen document, and the number of parameters of pLSI grows linearly with the number of training documents, which makes the model susceptible to overfitting. The cluster model, also known as the mixture of unigrams model, has been well examined in IR research. In the cluster model, it is assumed that all documents fall into a finite set of K clusters (topics). Documents in each cluster discuss a particular topic z , and each topic z is associated with a multinomial distribution P ( w|z ) over the vocabulary. The process of generating a document d ( 1) Pick a topic z from a multinomial distribution with parameter 2) For cluster model is: unigrams model is to cluster documents in the collection into K groups and then use a maximum likelihood estimate a topic model P(w|z) for each cluster. Liu and Croft (2004) adopted this method with a K-means clustering algorithm. They incorporated the cluster information into langua ge models as smoothing: With the new document model they conducted experiments on several TREC collections, finding that cluster-based retrieval improvements over document-base d retrieval are obtained. the assumption that each string (document) is generated from a single topic is limiting and may become problematic for long documents and large collections. As we described in Section 2.1, the pLSI model has a problem with inappropriate generative se mantics. Blei et al. (2003) introduced a new, semantically c onsistent topic model, Latent Dirichlet Allocation (LDA), which immediately attracted a considerable interest from the statistical machine learning and natural language processing comm unities. The basic generative process of LDA closely resemb les pLSI. In pLSI, the topic mixture is conditioned on each document. In LDA, the topic mixture is drawn from a conjugate Dirichlet prior that remains the same for all documents. The proce ss of generating a corpus is as follows (we consider the smoothed LDA here): 1) Pick a multinomial distribution 2) For each document d , pick a multinomial distribution 3) For each word token w in document d , pick a topic 4) Pick word w from the multinomial distribution Thus, the likelihood of generating a corpus is: model in Figure 1. consistent generative semantics by treating the topic mixture distribution as a k -parameter hidden random variable rather than a large set of individual parameters which are explicitly linked to the training set; thus LDA overcomes the overfitting problem and the problem of generating new documents in pLSI. Compared to the cluster model, LDA allows a document to contain a mixture of topics, rela xing the assumption made in the cluster model that each document is generated from only one topic. This assumption may be too limited to effectively model a large collection of documents; in contrast, the LDA model allows a document to exhibit multiple topics to different degrees, thus being more flexible. Figure 1. Graphical model representation of LDA. K is the number of topics; N is the number of documents; N number of word tokens in document d . The basic approach for using langua ge models for IR is the query likelihood model where each document is scored by the likelihood of its model generating a query Q , where D is a document model, Q is the query and q is a query term in Q. P(Q|D) is the likelihood of the document model generating the query terms under the  X  X ag-of-words X  assumption that terms are independent given the documents. ) | ( D q P specified by the document model with Dirichlet smoothing (Zhai and Lafferty, 2001), where P X  ( w|D ) is the maximum likelihood estimate of word w in the document D , and P X  ( w|coll ) is the maximum likelihood estimate of word w in the entire collection.  X  prior, and in our reported experime nts we used a fixed value with =1000 since the best results are c onsistently obtained with this setting. Document modeling (estimating P ( w|D )) is crucial to retrieval. Compared to the standard query likelihood model, LDA offers a new and interesting framework to model documents. However, as in other topic models, a topic in the LDA model represents a combination of words; and it may not be as precise a representation as words in non-topic models like the unigram model. Therefore LDA itself (commonly used with a relatively limited number of topics) may be too coarse to be used as the only representation for IR. Indeed, our preliminary experiments show that directly employing the LDA model hurts retrieval performance. So, we instead combine the original document model (Eqn. 6) with the LDA model and construct a new LDA-based document model. Motivated by the significant improvements obtained by Liu a nd Croft (2004), we formulate our model through a linear combination obtained in one of the following ways: (a) linearly combining the original document model and LDA, which is illustrated in (7), (b) additively combining the LDA model with the maximum likelihood estimate of word w in the document D , and (c) combining the LDA model with the Dirichlet smoothing pa rt, i.e. the maximum likelihood estimate of word w in the entire collection. Option (c) is similar to the combination used in Liu and Croft (2004). All methods have empirically shown similar pe rformance with appropriate parameters, and we will only repor t results of Option (a) which performs slightly better in our e xperiments (parameter setting in our paper is for (a); it may be necessary to adjust  X  and  X  in (b) and (c)). The LDA model has a new representation for a document based on topics. After we ge t the posterior estimates of , we can calculate the probability of a word in a document as following, (8) where  X  respectively. The LDA model is very complex and cannot be solved by exact inference. There are a few approximate inference techniques available in the literature: variational methods (Blei et al, 2003), expectation propagation (Griffith s and Steyvers, 2004) and Gibbs sampling (Geman and Geman, 1984; Griffiths and Steyvers, 2004). We use Gibbs sampling and the approximation of ) can be obtained directly. From a Gibbs sample, we use number of instances of word including the current token,  X  and  X  are hyper-parameters that determine how heavily this empi rical distribution is smoothed, and can be chosen to give the desired resolution in the resulting document that token i belongs to) assigned to topic z=j , not including the current token. Thus  X  words assigned to topic z=j ; and  X  words in document d, not includi ng the current one (Griffiths and Steyvers, 2004). Thus (7) will be Although Eqn.(9) involves the appr oximated posterior distribution using one Gibbs sample, we can use the samples from different Markov chains with different initializations. Our experiment shows that using multi-Markov chai ns is useful. So the actual value of ) | ( D w P Markov Chains. Complexity is often a big con cern for topic models. Even the simple cluster model suffers fro m potentially high computational costs. Liu and Croft (2004) used a three-pass K-means algorithm primarily motivated by its efficiency. They showed that the running time for each pass/iteration grows linearly with the number of documents ( N ) and the number of classes ( K ), i.e., O(KN) . Roughly speaking, the complexity of each iteration of the Gibbs sampling for LDA is also linear with the number of topics/clusters and the number of documents, which is also O(KN) . Due to the large sizes of document collections, we give a more detailed analysis. model is linear with I , K and documents and document. In K-means clustering algorithm, the computation is linear with I , N , and passes/iterations, and in one cluster. (We use the average numbers, instead of the corresponding sums to make the following comparison easier.) compare realistic values of these items. (1) K : The selected number of topics ( K ) in the LDA model is generally less than the selected number of topics/clusters in the cluster model because in the LDA model topics can be mixed to represent one document, but in the cluster model one document can based on only one topic. (2) I : The number of iterations (I) will probably have a larger value in the LDA algorithm. In Liu and Croft (2004), the number of iterations for K-means is 3. Such a small I does not work well for Gibbs sampling in the L DA model. The selection of I is very important to make sure that the Markov chains reach equilibrium. In Section 4.3.1, we show that experiments. (3) relationship of these two items, especially since related to the selection of K . While in our experiments and settings, the number of unique terms in a cluster is often larger than algorithms is similar. In experi ments, we also find that the difference in running times between LDA and K-means is trivial. Based on our experience based on using several IR collections, these two algorithms are comparable in computational costs and there is no clear evidence showi ng that one algorithm is obviously more efficient. We conducted experime nts on five data sets taken from TREC: the Associated Press Newswire (AP) 1988-90 with queries 51-150, Wall Street Journal (WSJ) 1987-92 with queries 51-100 and 151-200, Financial Times (FT) 1991-94 with queries 301-400, San Jose Mercury News (SJMN) 1991 with queries 51-150, and LA Times (LA) with queries 301-400. Queries are taken from the  X  X itle X  field of TREC topics. Re levance judgments are taken from the judged pool of top retrieved documents by various participating retrieval systems from previous TREC conferences. Queries that have no relevant doc uments in the judged pool for a specific collection have been removed from the query set for that collection. Statistics of the collec tions and query sets are given in Table 1. These five collections, including the query sets and relevance judgments, were the same as used by Liu and Croft (2004) in order to compare LDA-based retrieva l with cluster-based retrieval. The only difference between the two experimental settings is that we left out the Federal Register (FR) collection for two reasons: (1) The query set of this collection contains only 21 valid queries (the query sets of other coll ections contain around 100 (&gt;=94) valid queries); (2) In these 21 valid queries there are six that have only one relevant document in th e collection and thus may cause biased results.  X  X alid queries X  means queries that have relevant docs. There are several parameters that need to be determined in our experiments. For the retrieval e xperiments, the proportion of the LDA part in the linear combination must be specified (  X  in (6)). For the LDA estimation, the number of topics must be specified; the number of iterations and the number of Markov chains also need to be carefully tuned due to its influence on performance and running time. We use the AP coll ection as our training collection to estimate the parameters. The WSJ, FT, SJMN, and LA collections are used for testing whether the parameters optimized on AP can be used consistently on other collections. At the current stage of our work, the parameters are selected through exhaustive search or manually h ill-climbing search. All parameter values are tuned based on average precision since retrieval is our final task. The parameter selecti on process, including the training set selection, also follows Liu and Croft (2004) to make the results comparable. Mean average precision is used as the basis of evaluation throughout this study. We use symmetric Dirichlet priors in the LDA estimation with  X  and  X  =0.01, which are common settings in the literature. Our experience shows that retrieval results are not very sensitive to the values of these parameters. Document models consisting of mi xtures of topics, like pLSI and LDA, have previously been tested mostly on small collections due to their relatively long running time. From Section 3.3 it is shown that the iteration number in LDA estimation plays an important role in its complexity. Generally, more iterations means that the Markov chain reaches equilibrium with higher probability, and after a certain number of iterations (burn-in period) the invariant distribution of the Markov chain is equivalent to the true distribution. So it would be ideal if we could take samples right after the Markov chain reach equilibrium. However, in practice, convergence detection of Markov ch ains is still an open research question. That is, no realistic method can be applied on the large IR collections to determine the convergence of the chain. Researchers in the area of topic modeling tend to use a large number of iterations to guarantee convergence. However, in IR tasks it is almost impossible to run a very large number of topic model does not naturally mean good retrieval performance. Instead, a less accurate distribution of topics may be good enough for IR purposes. Furthermore, we have  X  and  X  in our model to adjust the influence of the LDA model. For example, if the LDA estimation is coarse, we may re duce the smoothing weight and let the LDA estimation share a part of smoothing. Figure 2. Retrieval results (in average precision) on AP with different number of iterations. K =400;  X  =0.7; 1 Markov chain. Figure 3. Retrieval results (in average precision) on AP with different number of Markov chains. K =400;  X  =0.7; 30 iterations. In order to get a good itera tion number that is effective for IR applications, we use the AP collection for training and maximizing the average precision score as the optimization criterion since it is our final evaluation metric. We try different iteration numbers, and also do experiments with different numbers of Markov chains, each of which is initialized with a different random number, to see how many chains are needed for 
TREC topics 51-150 (title only) 99 
TREC topics 301-400 (title only) 95 
TREC topics 51-150 (title only) 94 
TREC topics 301-400 (title only) 98 
TREC topics 51-100 &amp; 151-200 (title only) 100 our purposes. The results are presented in Figure 2 and Figure 3, respectively. After 50 iterations and with more than 3 Markov chains, performance is quite stable, so we use these values in the final retrieval experiments. The running time of each iteration with large topic numbers can be expensive; 30 iterations and 2 chains are a good trade off between accuracy and running time, and these values are used in the parameter-selecting experiments, especially when selecting a suitable number of topics. Selecting the right number of topics is also an important problem in topic modeling. N onparametric models like the Chinese Restaurant Process (Blei et al, 2004; Teh et al, 2004) are not practical to use for large data sets to automatically decide the number of topics. A range of 50 to 300 topics is typically used in the topic modeling literature. 50 topics are often used for small collections and 300 for relatively large collections, which are still much smaller than the IR collecti ons we use. It is well known that larger data sets may need more topics in general, and it is confirmed here by our experime nts with different values of K (100, 200, ...) on the AP collection. K =800 gives the best average precision, as shown in Table 2. Th is number is much less than the corresponding optimal K value (2000) in the cluster model (Liu and Croft, 2004). As we explained in Section 3.3, in the cluster model, one document can be base d on one topic, and in the LDA model, the mixture of topics for each document is more powerful and expressive; thus a smalle r number of topics is used. Empirically, even with more pa rsimonious parameter settings like K =400, 30 iterations, 2 Markov chai ns, statistically significant improvements can also be achieved on most of the collections. 
Table 2. Retrieval results (in average precision) on AP with 
Average precision 0.2431 0.2520 0.2579 0.2590 0.2557 In order to select a suitable value of  X  , we use a similar procedure as above on the AP collection and fi nd 0.7 to be the best value in our search. From the experiment s on the testing collections, we also find that  X  =0.7 is the best value or almost the best value for other collections. consistently obtained with this setting. The value of be adjusted when the other co mbination methods discussed in Section 3.2 are applied. In all experiments, both the queri es and documents are stemmed, and stopwords are removed. The retrieval results on the AP collection are presented in Table 3, with comparisons to the result of query likelihood retrieval (QL) and cluster-based retrieval (CBD M). Statistically significant improvements of LDA-based retr ieval (LBDM) over both QL and CBDM are observed at many recall levels, with 21.64% and 13.97% improvement in average precision respectively. Table 3. Comparison of query likelihood retrieval (QL), cluster-based retrieval (CBDM) and retrieval with the LDA-based document models (LBDM). The evaluation measure is average precision. AP data set. Stars indicate statistically significant differences in performance with a 95% confidence according to the Wilcoxon test. Rel. 21819 21819 21819 Rel. 
Retr. 10130 10751 12064 +19.09* +12.21* 0.00 0.6422 0.6485 0.6795 +5.8* +4.8* 0.10 0.4339 0.4517 0.4844 +11.6* +7.2* 0.20 0.3477 0.3713 0. 4131 +18.8* +11.2* 0.30 0.2977 0.317 0. 3661 +23.0* +15.5* 0.50 0.2081 0.2274 0. 2666 +28.1* +17.2* 0.60 0.1696 0.1794 0. 2245 +32.4* +25.1* 0.70 0.1298 0.1444 0. 1665 +28.3* +15.3* 0.90 0.0480 0.0571 0. 0694 +44.7* +21.6 1.00 0.0220 0.0201 0.0187 -15.1 -6.8 Avg 0.2179 0.2326 0.2651 +21.64* +13.97* Markov chains, we run experime nts on other collections and present results in Table 4. We compare the results with CBDM, reference. On all five collecti ons, LDA-based retrieval achieves improvements over both of query likelihood retrieval and cluster-model based retrieval, and f our of the improvements are significant (over CBDM). Considering that CBDM has already obtained significant improveme nts over the query likelihood model (and Okapi-style weighting, see Liu and Croft) on all of these collections, and is therefore a high baseline, the significant performance improvements from LBDM are very encouraging. document model is not limited to only the literal words in a document, but instead describe s a document with many other related highly probable words from th e topics of this document. For example, for the query  X  X everaged buyouts X , the document  X  X P900403-0219 X , which talks about  X  X arley Unit Defaults On Pepperell Buyout Loan X , is a rele vant document. However, this document does not contain the exact query term  X  X everage X , which makes this document rank ve ry low. Using the LDA-based representation, this document is cl osely related to two topics that have strong connections with th e term  X  X everage X : one is the bankruptcy topic that is strongly asso ciated with this document because the document contains many representative terms of this topic, such as  X  X illion X ,  X  X ompany X , and  X  X ankruptcy X ; the other is the money market topic which is closely connected to  X  X ond X , also a very frequent word in this document. In this way, the document is ranked higher with the LDA-based document model. Having multiple topics represent a document tends to give a clearer association between words than the single topic model used in cluster-based retrieval. Table 4. Comparison of cluster-based retrieval (CBDM) and retrieval with the LDA-based document models (LBDM). The evaluation measure is average precision. %chg denotes the percentage change in performance (measured in average precision) of LBDM over QL and CBDM. Stars indicate statistically significant differences in performance between LBDM and QL/CBDM with a 95% confidence according to the Wilcoxon test. Collection QL CBDM LBDM SJMN 0.2032 0.2171 0.2307 +13.57* +6.26* 
WSJ 0.2958 0.2984 0.3253 +9.97* +9.01* In Table 5 we compare the retrieva l results of the LBDM with the relevance model (RM), which in corporates pseudo-feedback information and is known for excellent performance (Lavrenko and Croft, 2001). On some collections, the results of the two models are quite close. RM us es pseudo-feedback information and thus needs online processing, i.e., it effectively does an extra search for each query, which makes it less efficient in reacting to users X  inputs. As an offline -processing model that does not do any extra processing on queries, the LDA-based retrieval model performance is quite impressive. In another words, we estimate the LDA model offline only once, and then LBDM can process real-time queries much more efficiency than RM with similar performance. Table 5. Comparison of the relevance models (RM) and the LDA-based document models (LBDM). The evaluation measure is average precision. %diff indicates the percentage change of LBDM over RM. The improvement on the AP collection in Table 4 is relatively larger than on the other collecti ons. Although we tune parameters on the AP collection, paramete r adjustment for the other collections does not improve the performance much. Compared This improvement is significant according to t-test, and almost significant (with a 93% confidence) according to the Wilcoxon test. to the relevance model results in Table 5, we conjecture that it is due to the characteristics of the documents and queries that the improvement on the AP collection is larger than on the other collections. We can also combine the relevance model and LBDM to do retrieval. In this case, the retr ieval results using LBDM are used as the pseudo-feedback for the relevance model. Results are shown in Table 6, and results of the query likelihood model are also listed as a reference. Mode rate improvements are obtained, which are better than the very sm all improvements reported in Liu and Croft (2004) for the combination of RM and CBDM. Table 6. Comparison of the relevance model (RM) and the combination of RM and the LDA-based document model (RM+LBDM). The evaluation measure is average precision. %chg denotes the percentage change in performance (measured in average precision) of RM+LBDM over RM. performance between RM+LBDM and RM with a 95% confidence according to the Wilcoxon test. 
Collection QL 3 RM RM+LBDM We have proposed LDA-based document models for ad-hoc retrieval, and evaluated th e method using several TREC collections. Based on the experime ntal results, we can make the following conclusions. Firstly, experiments performed in the language modeling framework, in cluding combination with the relevance model, have demons trated that the LDA-based document model consistently outperforms the cluster-based approach, and the performance of LBDM is close to the Relevance Model, which incorporates pseudo-feedback information. Secondly, we have shown that the estimation of the LDA model on IR tasks is feasible with suitable parameters based on the analysis of the algorithm complexity and empirical parameter selections. More importantly, unlike the Relevance Model, LDA estimation is done o ffline and only needs to be done once. Therefore LDA-based retrieval can potentially be used in applications where pseudo-relevance feedback would not be possible. In summary, LDA-based retrieval is a promising method for IR, although more work needs to be done with even larger collections, such as the Web data from the TREC Terabyte track. The QL&amp;RM baseline in Table 6 is slightly different with Table 5 because in the experiments of Table 5, in order to compare with the results in Liu and Croft (2004), we directly load their index into our system and then run the experiments on their index to get nearly identical results. For future work, we have begun to investigate whether other topic models (e.g. Griffiths et al, 2005; Li and McCallum, 2006) that have recently been developed can further improve retrieval performance. An approximation th at can improve LDA estimation will also be helpful. In addition, we plan to re-examine some traditional topic modeling methods (i.e. term clustering) as to their efficiency and effectiveness in retrieval tasks. This work was supported in part by the Center for Intelligent Information Retrieval, in part by Advanced Research and Development Activity and NSF gr ant #CCF-0205575, and in part by NSF grant #IIS-0527159. Any opinions, findings and conclusions or recommendations expr essed in this material are the author(s) and do not necessarily reflect those of the sponsor. Azzopardi, L., Girolami, M and va n Rijsbergen, C.J. Topic Based Language Models for ad hoc Information Retrieval. In Proceedings of the International Joint Conference on Neural Networks , Budapest,Hungary, 2004. Berger, A. and Lafferty, J. Info rmation Retrieval as Statistical Translation. In Proceedings of the 22nd International ACM SIGIR Conference on Research and Developm ent in Information Retrieval , 1999, 222-229. Blei, D. M., Ng, A. Y., and Jordan, M. J. Latent Dirichlet allocation. In Journal of Machine Learning Research , 3, 2003, 993-1022. Blei, D., Griffiths, T., Jordan, M., Tenenbaum, J. Hierarchical topic models and the nested Chines e restaurant process. In Advances in Neural Information Processing Systems 16 , Cambridge, MA, MIT Press, 2004. Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. Indexing by latent se mantic analysis. Journal of the American Society for Information Science, 41(6), 1990, 391-407. Geman, S., and Geman, D. St ochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images. In IEEE Transactions on Pattern Anal ysis and Machine Intelligence , 6, 1984, 721-741. Girolami, M. and Kaban, A. Sequential activity profiling: latent Dirichlet allocation of Markov chains. Data Mining and Knowledge Discovery , 10, 2005, 175 X 196. Girolami, M. and Kaban, A. On an equivalence between PLSI and LDA. In Proceedings of the 26th International ACM SIGIR Conference on Research and Developm ent in Information Retrieval , 2003, 433-434. Griffiths, T. L., and Steyvers, M. Finding scientific topics. In Proceeding of the National Academy of Sciences, 2004, 5228-5235. Griffiths, T. L., Steyvers, M., Blei, D. and Tenenbaum, J. Integrating topics and syntax. In Advances in Neural Information Processing Systems 17, 2005 Hofmann, T. Probabilistic latent semantic indexing. In Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval , 1999, 50-57. Lavrenko, V. and Croft, W. B. Relevance-based language models. In Proceedings of the 24th Inter national ACM SIGIR Conference on Research and Development in Information Retrieval , 2001, 120-127. Li, W. and McCallum, A. DAG-Structured Mixture Models of Topic Correlations. To appear in Proceedings of the 23rd International Conference on Machine Learning (ICML-06) , Pittsburgh, Pennsylvania, USA, 2006. Liu, X. and Croft, W. B. Cluste r-based retrieval using language models. In Proceedings of the 27th International ACM SIGIR Conference on Research and Devel opment Information Retrieval , 2004, 186-193. McCallum, A. Multi-label text classification with a mixture model trained by EM. In AAAI X 99 workshop on Text Learning , 1999. Ponte, J. and Croft, W.B. A language modeling approach to information retrieval. In Proceedings of the 21s t International ACM SIGIR Conference on Research and Development Information Retrieval , 1998, 275-281. Rosen-Zvi, M., Griffiths, T., Steyvers, M., and Smyth, P. The author-topic model for authors and documents. In Proceedings of the 20 th Conference on Uncertainty in Artificial Intelligence. Banff, Alberta, Canada, 2004. Sparck Jones, K. Automatic keyword classification for information retrieval . Butterworths, London, 1971. Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. Hierarchical Dirichlet processes. Technical Report, Depart ment of Statistics, UC Berkeley, 2004. Zhai, C. and Lafferty, J. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2001, 334-342. 
