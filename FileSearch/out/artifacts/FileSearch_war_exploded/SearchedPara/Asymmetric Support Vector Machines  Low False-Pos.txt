 Many practical applications of classification require the classifier to produce a very low false-positive rate. Although the Support Vector Machine (SVM) has been widely applied to these applica-tions due to its superiority in handling high dimensional data, there are relatively little effort other than setting a threshold or changing the costs of slacks to ensure the low false-positive rate. In this pa-per, we propose the notion of Asymmetric Support Vector Machine (ASVM) that takes into account the false-positives and the user tol-erance in its objective. Such a new objective formulation allows us to raise the confidence in predicting the positives, and therefore obtain a lower chance of false-positives. We study the effects of the parameters in ASVM objective and address some implementa-tion issues related to the Sequential Minimal Optimization (SMO) to cope with large-scale data. An extensive simulation is conducted and shows that ASVM is able to yield either noticeable improve-ment in performance or reduction in training time as compared to the previous arts.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing ; I.2 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Support Vector Machine (SVM), Classification, Low False-Positive Learning
In many real-world applications of classification, users are par-ticularly sensitive to the wrong predictions of a certain class. For example, in spam filtering [1, 6, 16], users may overlook/delete im-portant information if a good mail is misclassified to spam image recognition [31] and network intrusion detection [3], costly but wrong decisions may follow if a false match or alarm is fired in computer-aided disease diagnosis [12, 33], patients may lose the golden period of treatment if their symptoms are wrongly classified as negative. A classifier must produce a very low false-positive (or negative) rate when applied to these applications.

Many efforts [1, 7, 10, 15, 19, 20, 21, 24, 25, 32] have been made upon different classifiers to reduce the false-positive rate. Al-though the Support Vector Machines (SVMs) have demonstrated high prediction accuracy in the literature [9, 14, 15, 30], there are relatively few studies [15, 19, 28] on further reducing the SVMs X  false-positive rate. Two common techniques, the parameter tuning [11, 19] and thresholding [15, 28], are applied prior and posterior to the SVM algorithms respectively. The former adjusts the para-meter (i.e., cost) of each slack variable in an SVM objective. This approach requires either time-consuming searches for the optimal combination of the costs [11] or domain-specific knowledge of the pattern contents (e.g., relations between different email categories in spam filtering [19]) based on which the learned classifier may not generalize well due to the heuristic nature in setting the costs. The latter establishes a threshold (larger than 0 ) based on the Re-ceiver Operating Characteristic (ROC) curve of a testing data. Only those patterns with predicted scores higher than the threshold will be classified as positive. The false-positive rate can be lowered as the threshold increases, yet fewer patterns may be predicted as pos-itive meanwhile. Such a technique suffers from an unwanted trade-off between minimizing the false-positive rate and maximizing the true-positive rate.

Note the objective of traditional SVMs is to maximize the mar-gin between the positive and negative classes in order to obtain high classification performance such as accuracy or Area Under Curve (AUC). For applications sensitive to the false-positives, keeping the resultant false-positive rate under a maximal user tolerance is usu-ally a concern prior to achieving high classification performance [32]. For example, users are unlikely to accept a spam filter capa-ble of identifying 10 0% of spam but half of the spam predictions are actually good mails. There is a basic need for a new SVM that seeks high classification performance only when the false-positive rate meets the user tolerance.

In this paper we propose the Asymmetric Support Vector Ma-chine (ASVM), a support vector learning algorithm that takes into account the false-positive rate and user tolerance in its objective formulation. ASVM is asymmetric in the sense that it maximizes themarginbetweenthenegativeclassandthe core [5] (i.e., high confidence subset) of the positive class. Basically, the smaller the core (i.e., the higher the confidence), the less chance a false-positive may occur. Given a user tolerance, we are able to determine a proper size of the core that ensures satisfactory false-positive rate, and at the same time the class-margin is maximized to yield high classification performance. ASVM avoids the trade-off between the false-and true-positive rates in thresholding, and is applicable to any applications since no prior or domain-specific knowledge is required.

To the best of our knowledge, this is the first work that exploits the asymmetry in SVM X  X  objective to control the false-positive rate. Following summarizes our contributions:
The rest of the paper is organized as follows. In Section 2, we re-view some related studies and explain the basics of SVMs. Section 3 introduces ASVM. We also look into the effect of each parameter in the ASVM objective. In section 4 we evaluate the performance of ASVM based on the simulation results. We also discuss some implementation and training issues to handle large-scale data. Sec-tion 5 concludes the paper.
In this section, we brie fl y review related studies, and give prelim-inaries of SVMs. We specify some terminologies and assumptions that will be used throughout the text.
The naive bayes classifier [1, 24, 25] is probably the earliest method used in the low false-positive learning. Parameters of the probability model can be easily adjusted to associate the positive predictions with high confidence. Recent efforts on low false-positive learning include utility [8, 19], boosting [10], compression [7], cas-caded classifiers [32], and ensemble [21]. Studies [8, 19] employ the utilities, sometimes called stratifications, to change the prior of a decision tree or costs of SVM slacks. The study [10] induces a decision tree that is able to give confidence-rated predictions by following the AdaBoost algorithm. Authors of [7] derive two com-pression models for the positive and negative classes respectively, and assign the label of a pattern to the class having higher com-pression rate. These compression models are adaptive so the false-positive rate may be controlled. Authors of the study [32] pro-poses a two-stage cascaded classifier. Patterns reported as posi-tive in the first stage are further validated in the second to reduce the false-positive rate. The study [21] merges different classifiers (those submitted to TREC 2005 Spam Evaluation Track [13]) and combines their outputs using the log-odd average to achieve low false-positive rate.

In this paper we focus on the support vector learning. Following details the objective formulations of SVMs as they are relevant to our study. m training instances drawn i.i.d. from X X { X  1 } ,where x i  X  X  denotes a pattern and y i  X  X  X  1 } is a class label. Our goal for classification is to find a real value function f such that X X { X  1 } , f ( x )  X  0 ,if y =1 ; f ( x ) &lt; 0 otherwise. The value f ( x ) is called the decision value .

The SVM Classifier. The Support Vector Machine (SVM) [9, 14, 30] searches a hyperplane { f ( x ): x  X  X } that maximizes the margin between the two classes of training patterns. To separate the overlapped classes, x i are usually mapped to a high dimensional Reproducing Kernel Hilbert Space (RKHS), H , by a function Let { w ,  X ( x ) + b : X ( x )  X  X } , w  X  X  and b  X  R ,beahyper-plane corresponding to a linear function f ( x )= w ,  X ( x H , the primal objective of SVM can be formulated as a quadratic optimization problem: for all i =1 ,  X  X  X  ,m ,where  X  i are slack variables and C stant denoting the cost of each slack. The above objective puts the positive training instances ( x i , 1) at one side of the margin { w ,  X ( x ) + b )  X  1: X ( x )  X  X } , and the negative ones ( at another side { w ,  X ( x ) + b )  X  X  X  1:  X ( x )  X  X } .Instances ( x i ,y i ) falling outside of their corresponding regions are called outliers and have positive penalties  X  i &gt; 0 .Theparameter trols the trade-off between maximizing the margin (i.e., 2 / and minimizing the training error (i.e., m i =1  X  i ). Eq. (1) can be solved efficiently [18, 22, 28]. Obtaining w and b , one may pre-dict the label of a testing pattern x by using sg n ( f ( [4, 29, 30] show that the large margin can actually lead to better generalization performance in prediction.

One-Class SVM. There is another type of SVM [5, 26] that aims at distinguishing the regular patterns from outliers. Given a sample m =( x 1 ,  X  X  X  , x m ) of m unlabeled patterns drawn i.i.d. from X with distribution D , the one-class SVM searches for the small-est ball that encloses the support of D .Whendataaremappedto an RKHS, finding the smallest ball is equivalent to searching a hy-perplane that approaches the dataset as close as possible from the origin [26]. Let { w ,  X ( x )  X   X  : X ( x )  X  X } ,  X   X  R , be the hy-perplane, the objective of one-class SVM is formulated as follows: the upper side of the hyperplane { w ,  X ( x )  X   X   X  0:  X ( and let the boundary w ,  X ( x )  X   X  =0 approach the elements of m by maximizing its margin from the origin (i.e.,  X / w ). Pat-terns x i falling outside the region { w ,  X ( x )  X   X   X  0: X ( H} are called outliers and have  X  trols the trade-off between maximizing the margin (i.e.,  X / and minimizing the training error (i.e., m i =1  X  i ). Solving Eq. (2), the function sg n ( w ,  X ( x )  X   X  ) can be used to indicate whether
Note that solving Eqs. (1) and (2) may involve calculating the dot product  X ( x i ) ,  X ( x j ) in an infinite-dimensional RKHS. Choos-ing a positive definite kernel k , by Mercer X  X  theorem, one may ef-ficiently obtain the above term using  X ( x i ) ,  X ( x j ) = k ( In this paper, we restrict our discussion on the Gaussian Radial Ba-sis Function (RBF) kernel, i.e., k ( x i , x j ) = exp(  X  q where q is a constant.

To reduce the false-positive rate of the SVM classifier, current solutions either set a threshold [15, 28] or differentiate the cost of the slack variables [11, 19]. In thresholding [15, 28], a testing in-where t&gt; 0 is a threshold whose value is determined from the ROC curve. Clearly, the larger the value of t , the less chance a false-positive occurs in a prediction. However, fewer true-positives can be identified. The latter approach [11, 19] associates different costs C i to different slacks  X  i in Eq. (1). This approach is time con-suming as it requires either human interaction [19] or extra searches [11] to obtain proper values of C i .
In this section, we introduce the Asymmetric Support Vector Ma-chine (ASVM) and its rationale. We also show how ASVM can in-corporate the user tolerance to achieve low false-positive learning
Recall that in traditional SVM classifier, the margin are maxi-mized between the positive and negative classes described by the training (noisy) instances. To lower the false-positive rate, we aim at searching for a better described positive class that is able to catch a higher confidence area amongst the positive training patterns. Note changing the value of C in Eq. (1) to identify more outliers from the positive patterns may not lead to a better description since the underlying data distribution. One naive solution is to adopt two one-class SVMs, with different values of C in Eq. (2), to estimate proper borders of the two classes and let the decision boundary sit at the middle of the two balls. However, the balls are independent of each other. This approach does not take into account the interaction (e.g., overlap, margin) between the two classes, and the accuracy of predictions is expected to be low from the statistical learning theory [30] point of view.

We formulate the objective of ASVM as follows: for i =1 ,  X  X  X  ,m ,where  X  and  X  are constants. The concept of Eq. (3) is illustrated in Figure 1. Note we use the shorthand Consider two parallel hyperplanes { w ,  X ( x )  X   X  : X ( x )  X  X } and { w ,  X ( x )  X   X  +  X  : X ( x )  X  X } . The above objective puts the positive patterns at the upper side of the first plane { w second { w ,  X ( x )  X   X   X   X  : X ( x )  X  X } . Instances falling out-side their corresponding regions are called slacks and have positive
Due to the space limitation, we focus ourselves on the two-class classification problem. The ASVM objective proposed in this arti-cle can be easily extended to the muti-class problem. Figure 1: A logic view of ASVM in RKHS. Two margins, the core-margin (  X / w ) and class-margin (  X / w ), are maxi-mized simultaneously to allow classifying the negative class and the core of the positive class. penalties  X  i &gt; 0 .Weset f ( x )= w ,  X ( x )  X   X  +  X  2 the label of a testing instance x by s gn( f ( x )) .

ASVM maximizes two margins, the core-margin (i.e.,  X / w ) and the traditional class-margin (i.e.,  X / w )asinSVM.Thera-tional behind is that, by enlarging the core-margin, we are able to enclose the core [5] (i.e., high confidence description) of the posi-tive class in a set {  X ( x ): w ,  X ( x )  X   X  } . At the same time, the class-margin is maximized between the negative class and this core to achieve high accuracy in prediction as well as its generalization. The false-positive rate is expected to be lowered when  X  increases. Note ASVM is orthogonal to most previous studies described in Section 2, and can be readily integrated with the techniques like thresholding [15, 28], utility/cost-tuning [8, 19], cascading [32], and ensemble [21].

We may transform Eq. (3) by using the Lagrangian into the fol-lowing dual objective: The details can be found in Appendix. We will discuss how to solve this problem efficiently later.

Learning Under the User Tolerance. Consider two toy datasets shown in Figures 2(a) and (b). Figure 2(c) depicts the margin (with decision values  X  1 ) and the decision line {  X ( x ): w ,  X ( 0 } returned by the SVM classifier given parameters C =1 , 0 . 5 . The parameters are found using the cross-validation [17]. We mark the slacks with squares. Figure 2(d) depicts an enclosing ball of the positive class returned by the one-class SVM with parameters C =0 . 25 , q =1 . The outputs of ASVM for these two datasets are shown in Figures 2(e) and (f) with parameters  X  =0 . 15 , 0 , q =0 . 5 and  X  =0 . 15 ,  X  =0 . 02 25 , q =1 . 5 respectively. Comparing Figures 2(c) and (e), we can see that ASVM behaves similarly to the SVM classifier when  X  is close to 0 .

By increasing  X  , we are able to obtain a larger margin, as de-picted in Figure 2(g) (  X  =0 . 3 ,  X   X  0 , q =0 . 5 ). The effect of ROCs that meet a user tolerance 0 . 1 to the false-positive rate. analogous to that of C in SVM. On the other hand, as illustrated in Figure 2(h), we are able to capture the dense region of the positive classes by increasing  X  (  X  =0 . 15 ,  X  =0 . 05 , q =0 . 5 core-margin grows as  X  increases. The dense region, unlike those captured by one-class SVM, are antagonistic to the negative class since by Eq. (3) it aims at excluding as many negative instances as possible. We may see this clearly by comparing Figures 2(d) and (f). Note we omit the decision line in Figure 2(f) for simplicity. The captured dense region may reasonably represent the high con-fidence area of the positive class due to its high density, purity (in class label), and long distance to the negative class.

ASVM is useful in the situations where a given a user tolerance tothefalse-positiveratemustbemet.Figure2(i)showstwotypical ROC curves resulted by the SVM and ASVM classifiers in Figures 2(c) and (h) respectively. Both SVM and ASVM achieve 95 % ac-curacy in prediction. The AUC given by ASVM is 0 . 95 ,whichis slightly lower than that ( 0 . 96 ) achieved by SVM. However, ben-efiting from a better description of the positive class, ASVM can significantly reduce the chance that a false-positive occurs from an instance with high decision value. Denote t -AUC the area under the ROC curve in y -axis and t in x -axis. Suppose t =0 . 1 ure 2(j) depicts the performance of SVM and ASVM when the false-positive rate must be less then 0 . 1 . In such a case, the 0 AUC given by ASVM is 0 . 86 t , which is about 56% higher than that ( 0 . 55 t )givenbySVM.
Although we have seen by Figure 2 the relations between the pa-rameters,  X  and  X  , and the margins, the values of these parameters are still unintuitive to users. In this section, we show that the ef-fects of  X  and  X  can actually be quantified in terms of the portion of outliers.

Let m + (resp. m  X  )bethenumberofthepositive(resp. neg-negative) in-bound support vectors, i.e., instances ( x i ( tive (resp. negative) outliers, i.e., instances ( x i , 1) having  X  i = 1  X m , as depicted in Figure 1. Let Pr emp ( Pr emp ( o  X  i ) ) be the portions of the positive (resp. negative) in-T P
ROOF . At KKT complementarity conditions,  X &gt; 0 implies  X  (see Appendix). Therefore the term m i =1  X  i  X  2  X   X  +1 in Eq. (4) becomes an equation. We have Summing the above two equations we have m + i =1  X  i =  X   X  i  X  1  X m . Thereexistatmost (  X   X  +1) ( 1  X m ) positive instances
Now subtract the above two equations. We have m  X  i =1  X  0 0 therefore Combining Eqs. (5) and (6), we obtain T P
ROOF . Consider m can contribute at most 1  X m ,thereexistatleast (  X   X  +1) ( 1 ( have Combining Eqs. (7) and (8), we obtain T
HEOREM 3.3. Assume  X &gt; 0 and  X &gt; 0 . Suppose the instances in Z m are generated i.i.d. from a distribution D that is continuous non-constant. The difference Pr emp ( o + i )  X  Pr emp ( o almost surely to  X  ,i.e., Pr(lim m  X  X  X  (P r emp ( o + i )  X   X  )=1 .
 P
ROOF . With Theorems 3.1 and 3.2, this can be proofed intuitively by claiming that, when m  X  X  X  , both Pr emp ( s + i )  X  We can see that the parameter  X  controls the difference between the outliers from the positive and negative classes. As a byproduct, we can see from Eqs. (5) and (7) that and from Eqs. (6) and (8) that The parameter  X  controls the basic portion of the outliers from each class. Note the effect of  X  in ASVM is similar to that of the para-meter  X  in  X  -SVM classifier [27]. Using the above conclusions ASVM may incorporate with the prior knowledge (in portion of the outliers) to obtain a more sophisticated high confidence area.
In this section, we evaluate the performance of ASVM. We also study the scalability of ASVM and discuss some implementation issues to cope with large-scale data.
We implement ASVM based on LIBSVM [11]. To evaluate the performance of ASVM, we consider several public real-world datasets obtained from the UCI machine learning repository [2] and IJCNN 2001 competition [23]. We control a 1:9 ratio between the positive and negative instances by either resampling (for two-class datasets) or merging the class labels (for multi-class datasets) [31]. Users under such a ratio are sensitive to the false-positives since any increment in the false-positive rate may seriously affect the positive predictions. In each dataset the training and testing instances are Figure 3: The scalability of ASVM based on the SMO imple-mentation. split according to a 5:1 ratio. We use 10-fold cross validation in eachtrainingprocess.

This paper focuses on low false-positive learning. In particular, we are interested in the performance of a classifier provided that a user tolerance t , 0  X  t  X  1 , to the false-positive rate must be met . We focus on the t -ROC space, i.e., an ROC space with the axis of false-positive rate ranging from 0 to t . We use the following metrics in our performance evaluation:
We compare ASVM with the ThresHolding (TH) [15, 28] and the Parameter Tuning (PT) [11, 19] techniques, which are both available in LIBSVM by default. Note that since we focus on a general purpose classifier, no prior knowledge, such as that used in [19], is assumed. In thresholding, the standard SVM classifier is used and has two parameters, C and q , as we have seen in Sec-tion 2.2, which need to be determined during the training time. We adopt a 2 -dimensional grid search [17] for the optimal combina-tion of these two parameters that maximizes t -AUC. In parameter tuning, we differentiate the parameter C of a standard SVM be-tween the positive ( C + ) and negative ( C  X  ) classes, and employ a 3 -dimensional grid search for the optimal combination of C + C  X  ,and q maximizing t -AUC. In ASVM, there are three parame-ters,  X  ,  X  ,and q , as we have seen in Sections 2.2 and 3. Rather than adopting a 3 -dimensional grid search directly, we first fix a very small  X  (to simulate the conventional SVM classifier) and ap-ply a 2 -dimensional grid search for the optimal combination of and q that maximizes t -AUC. After proper  X  and q are obtained, we perform a linear search (i.e., 1 -dimensional grid search) for maximizing the t -AUC further.
For better scalability, we reduce the ASVM dual to the Sequen-tial Minimal Optimization (SMO) [22] problem. In order to match the SMO input, we need to rewrite the constraint m i =1  X  1 in Eq. (4) as m i =1  X  i =2  X   X  +1 . Doing so effectively relaxes the constraint  X   X  0 intheASVMprimal(Eq.(3))andthereforeaspe-cial care is needed when selecting  X  in the training time to prevent a 1 -AUC 0 . 1 -AUC 0 . 05 -AUC % % % givenintrainingtime. negative class-margin  X  . One easy way is to check whether during each iteration of a grid search and skip the corresponding candidates. Another way is to train an auxiliary hyperplane with  X  always equals to 0 in Eq. (3) first during each iteration of the grid search. We are able to estimate the basic portion of zero by calculating the portions of the negative instances falling across the auxiliary hyperplane. Following Eqs. (9) and (10), we can see that  X   X  0 as long as This approach, called bi-training , is particularly useful to those cases, such as on-line training, where the grid-search technique is infeasible. We adopt the former approach and omit the detailed dis-cussions about the latter due to the space limitation. Figure 3 shows the scalability of ASVM. Currently, we are able to handle about thousand instances within a minute.
In this section, we compare the testing results of ASVM with those of ThresHolding (TH). TH is based on traditional SVM clas-sifier. As mentioned in Section 3, ASVM is also compatible to this technique and therefore we consider setting up different thresh-olds for ASVM X  X  positive predictions as well. The resultant perfor-mance of both the classifiers can be easily arranged and shown in an ROC space, where each point on an ROC curve presents a trade-off between the true-and false-positive rates given a certain threshold (not necessarily larger than 0 in this case).

We use datasets including Pima Indian Diabetes, Statlog Ger-man, Wisconsin Breast Cancer, Ionosphere, Statlog Australian, Cover-type, and IJCNN in our experiments. We consider t =1 and 0 . 1 for each dataset in the training phase. For larger datasets such as Covertype and IJCNN, we consider t =0 . 05 additionally since un-der such a configuration the training instances are still sufficient to apply the learned model to the testing data. Note that since the ratio between the positive and negative instances is 1:9, we differentiate the parameter C in TH between the positive ( C + ) and negative (
C  X  ) classes and set C + : C  X  =9 : 1 to compensate for the skew data distribution 2 .

Table 1 shows the maximal t -AUCs achieved by TH and ASVM respectively. As we can see, for Diabetes the 1 -AUCs given by TH and ASVM are very close to each other. By comparing the AUCs of the rest datasets, we can see that, generally, ASVM give similar performance as SVM in classification. When focusing on 0 . 1 -AUCs, however, we observe that ASVM is able to give 33 % improvement over TH. The other datasets based on which ASVM can make noticeable improvement include Ionosphere ( 10 . 9% 0 . 1 -AUC) and IJCNN ( 5 . 1% for 0 . 1 -AUC, 3 . 8% for 0 . 05
This is suggested in LIBSVM [11]. Figure 4: The ROC curves of TH and ASVM given t =0 . 1 and 0 . 05 in training time.
 We believe this is mainly because that ASVM successfully obtain a high confidence area of the positive class in these datasets. Overall, ASVM gives about 6 . 4% improvement in t -AUC when t  X  0 . 1 Notice that in the Statlog Australian dataset, the advantage of ASVM does not help a better performance. We believe this is be-cause that the classes are separable in RKHS. Under such a case, SVM is good enough to make low false-positive predictions.
Next, we study the detailed performance of ASVM and TH within the 0 . 1 -and 0 . 05 -ROC space. Our observation shows that ASVM is usually the best classifier at the very first segment of the false-positive rate (starting from 0 ). This is true even for the Covertype dataset, despite the fact that ASVM does not achieve the highest 0 . 1 -AUC in Table 1. Figure 4(a) illustrates the ROC curves re-turned by ASVM and TH using t =0 . 1 in training time. As we can see, ASVM is the best classifier when the false-positive rate ranges from 0 to 0 . 019 and gives the sharpest range of slope, [1 5 . 129 , along the ROC Convex Hull. The true-positive rate is 0 . 774 point of false-positive rate 0 . 019 . Figure 4(b) illustrates the ROC curves when t =0 . 05 is used. Again, ASVM is the best classi-fier when the false-positive rate is above 0 and under 0 . 00 2 gives the sharpest slopes ranging from 32 . 780 to  X  along the ROC Convex Hull. The true-positive rate is 0 . 38 7 at the point of false-positive rate 0 . 002 . ASVM is useful in the situations that the cost of the false-positives is high (or, the slope of the iso-performance line is sharp).
In this section, we compare the testing results of ASVM with those of Parameter Tuning (PT). Although both PT and ASVM have three parameters ( C + , C  X  , q and  X  ,  X  , q respectively), they are trained in different way. In PT, the effects of C + and 1 -AUC 0 . 1 -AUC 0 . 05 -AUC % % % givenintrainingtime. Figure 5: Decision planes in RKHS. (a) In PT, the movement of a decision plane is unpredictable when the values of C + and are changed. (b) In ASVM, changing the value of  X  effectively shifts the decision boundary toward the positive class. Figure 6: The ROC curves of PT and ASVM given t =0 . 1 and 0 . 05 in training time. correlated. Changing any value of C + , C  X  ,and q may result in movement of a decision boundary as well as its margin, as shown in Figure 5(a). Under such a case, we need to search the entire 3 -dimensional space for the best combination of C + , C  X  ,and In ASVM, on the other hand, we can see from Figure 5(b) that given  X  and q , increasing the value of  X  effectively shifts the de-cision boundary toward the positive class. The class margin is en-larged, but its placement, which is determined by  X  and q fected by  X  . Based on this observation we adopt a heuristic training method aiming at reducing the training times of a 3 -dimensional grid search. As mentioned before, we first apply a 2 -dimensional grid search for  X  and q to determine a proper placement of the de-cision boundary when  X   X  0 , and then increase  X  to obtain a high confidence area of the positive class.

The maximal t -AUCs achieved by PT and ASVM are summa-rizedinTable2. Noteweomitsmalldatasetsduetothespace limitation. As we can see, the difference between the results of ASVM and PT is not significant, ranging between  X  3% . Figure 7: Number of iterations required to complete a grid ser-ach.

To see the detailed performance of ASVM and PT within the and 0 . 05 -ROC space, let X  X  consider again the Covertype dataset. Figure 6(a) illustrates the ROC curves returned by ASVM and PT using t =0 . 1 in training time. As we can see, ASVM is the best classifier when the false-positive rate ranges from 0 and gives the sharpest range of slope, [26 . 47 6 ,  X  ] , along the ROC Convex Hull. The true-positive rate is 0 . 355 at the point of false-positive rate 0 . 00 2 . Figure 6(b) illustrates the ROC curves when t =0 . 05 is used. In this case ASVM remains the best in the range of slope [26 . 47 6 ,  X  ] along the ROC Convex Hull. The true-positive rate is 0 . 387 at the point of false-positive rate 0 . 002 ASVM is able to give comparable performance against PT in terms of either t -AUC, t  X  0 . 1 ,orslopes.

Next, we compare the number of training times required in the grid searches adopted by ASVM and PT respectively. The results are depicted in Figure 7 whose x -axis denotes the granularity, i.e., the number that a search range in each dimension is divided into. As we can see, ASVM requires an order less training times than PT. This is because we perform only a 2 -dimensional search (for q ) with one extra linear search (for  X  )ratherthana 3 -dimensional search as PT does. From the above discussions, ASVM is able to give comparable performance as compared with PT while signifi-cantly reducing the total training times.
Another advantage of ASVM is that it is able to give more insight into the dataset. In Section 3, we showed that there is an asymptotic relationship on the difference of the portion of the outliers between two classes. In order to give a more comprehensive view, we test the asymptotic property of  X  in a synthetic dataset with 90 tive labeled and 10 negative labeled instances. Figure 8 shows the experimental results and compares the the difference derived theo-retically with that obtained in the simulation under different values of  X  . Note the dotted line along the diagonal depicts the values of  X  .

As we can see, the actual portion of outliers lies within the the-oretical upper and lower bounds. Actually, these three lines will converge to a single when the number of training data increases. From above, the relation between the difference of the portion of outliers and  X  is justified.
We proposed ASVM, an Asymmetric Support Vector Machine that takes into account the false-positives and the user tolerance. ASVM maximizes the margin between the negative class and the core of the positive class. This allows us to raise the confidence in predicting the positives and obtain a lower false-positive rate. We quantitated the effects of  X  and  X  in terms of the portion of outliers. Experimental results showed that ASVM is able to either give 6 . 4% improvement in AUC and stay as the best classifier in the low-false positive region of the ROC Convex Hull as compared to the thresholding, or achieve a significant reduction in training time as compared to the parameter tuning. [1] I. Androutsopoulos, J. Koutsias, K. Chandrinos, and [2] A. Asuncion and D.J. Newman. UCI Machine Learning [3] D. Barbara, N. Wu, and S. Jajodia. Detecting novel network [4] P. Bartlett and J. Shawe-Taylor. Generalization performance [5] A. Ben-Hur, D. Horn, H.T. Siegelmann, and V. Vapnik. [6] P. Boykin and V. Roychowdhury. Leveraging social networks [7] A. Bratko, G. Cormack, B. Filipic, T. Lynam, and B. Zupan. [8] L. Breiman. Classification and Regression Trees .Chapman [9] C. Burges. A tutorial on support vector machines for pattern [10] X. Carreras and L. Marquez. Boosting trees for anti-spam [11] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support [12] H.D. Cheng, X. Cai, X. Chen, L. Hu, and X. Lou.
 [13] G. Cormack and T. Lynam. Overview of the trec 2005 spam [14] C. Cortes and V. Vapnik. Support vector networks. Machine [15] H. Drucker, D. Wu, and V. Vapnik. Support vector machines [16] J. Goodman, G. Cormack, and D. Heckerman. Spam and the [17] C.-W. Hsu, C.-C. Chang, and C.-J. Lin. A practical guide to [18] J. Kivinen, A. Smola, and R. Williamson. Online learning [19] A. Kolcz and J. Alspector. SVM-based filtering of e-mail [20] H.-Y. Lam and D.-Y. Yeung. A learning approach to spam [21] T. Lynam, G. Cormack, and D. Cheriton. On-line spam filter [22] J. Platt. Sequenital minimal optimization: A fast algorithm [23] D. Prokhorov. IJCNN 2001 neural network competition , [24] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A [25] K. Schneider. A comparison of event models for naive bayes [26] B. Scholkopf, J. Platt, J. Shawe-Taylor, A. Smola, and [27] B. Scholkopf and A. Smola. Learning with Kernels:: [28] D. Sculley and G. Wachman. Relaxed online support vector [29] J. Shawe-Taylor, P.L. Bartlett, R.C. Williamson, and [30] V. Vapnik. Statistical Learning Theory . Wiley, NY, 1998. [31] P. Viola and M. Jones. Fast and robust classification using [32] W. Yih, J. Goodman, and G. Hulten. Learning at low false [33] B. Zheng, W. Qian, and L.P. Clarke. Digital mammography: To solve Eq. (3), we introduce a Lagrangian:
L = where  X  i ,  X  i ,and  X  are Lagrange multipliers larger than or equal to 0 . The Lagrangian L must be maximized with respect to  X  i  X  ,and  X  , and minimized with respect to w ,  X  ,  X  ,and  X  i Karush-Khun-Tucker (KKT) condition, we have
Replacing the corresponding terms in Eq. (11) by those in Eqs. (12)-(15) and substituting the kernel function k ( x i , x j product  X ( x i ) ,  X ( x j ) , we obtain the dual objective of ASVM. The values of  X  and  X  can be recovered using the KKT comple-mentarity conditions. At optimum, we have  X  1  X  i  X  m . For each positive in-bound support vector s second term at the left hand side of Eq. (16) must be zero. We have
