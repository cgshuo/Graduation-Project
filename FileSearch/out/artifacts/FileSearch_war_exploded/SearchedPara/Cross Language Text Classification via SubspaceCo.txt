 Yuhong Guo yuhong@temple.edu Min Xiao minxiao@temple.edu With the rapid growth of multilingual data in all as-pects of human society, it is very common that doc-uments in different languages share the same set of categories. In such multilingual learning scenarios, applying standard monolingual classification methods directly requires costly and time-consuming document annotation in each language. Thus developing effec-tive cross language text classification methods, which transfer the categorization knowledge in a label-rich language, source language , to assist classifications in a label-scarce language, target language , is becoming increasingly important.
 Previous work on cross language text classification mainly focuses on the use of automatic machine trans-lation technology. Most of these methods translate documents from the source language to the target lan-guage or vice versa, and then apply standard mono-lingual classification methods. However, due to the difference in language and culture, there exists a word drift problem. That is, while a word frequently ap-pears in one language, its translated version may rarely appear in the other language. This creates a data dis-tribution discrepancy between the translated training documents from the source language and the origi-nal testing documents in the target language, which poses a standard domain adaptation problem. Al-though many domain adaptation methods can be used in cross language text classification on the top of ma-chine translation, e.g., the work in (Shi et al., 2010; Wei &amp; Pal, 2010; Prettenhofer &amp; Stein, 2010; Wan et al., 2011), they nevertheless suffer from the informa-tion loss and translation error introduced in machine translation process without direct access to the orig-inal documents. Multi-view learning methods on the other hand treat each language as one independent view of the data and use both the translated docu-ments and the original documents in each language for text classification (Wan, 2009; Amini et al., 2009; Amini &amp; Goutte, 2010).
 In this paper, we propose a novel subspace co-regularized multi-view learning method to address cross language text classification based on machine translation. Our assumption is that a document and its translated version describe the same data object in two different views. The underlying discriminative subspace representations of the same data object in the two views thus should be very similar regarding the same classification task. We then simultaneously train two different classifiers, one for each language, by formulating a semi-supervised optimization prob-lem that minimizes the training losses on the labeled data in both views and penalizes the distance between the two projected subspace representations of all data objects. We develop a gradient descent optimization algorithm with curvilinear search to solve the pro-posed optimization problem for a local optimal solu-tion. Our extensive empirical study on a large number of cross language text classification tasks suggests the proposed approach consistently outperforms a number of comparison inductive methods, domain adaptation methods, and multi-view learning methods. Previous work on cross language text classification mostly relied on machine translation methods, by translating the test data into the language of the train-ing data or vice versa, so that classification algorithms for monolingual texts can be applied (Bel et al., 2003; Shanahan et al., 2004). Although simple and intu-itive, these methods suffer from the error and noise in-troduced in machine translation and the discrepancy of data distribution across languages (Shi et al., 2010). Various methods have been proposed to tackle these is-sues on translated data to increase cross language text classification accuracy, including an information bot-tleneck method (Ling et al., 2008), EM-based model translation techniques (Shi et al., 2010; Rigutini &amp; Maggini, 2005), and cross language domain adapta-tion methods (Wei &amp; Pal, 2010; Prettenhofer &amp; Stein, 2010; Wan et al., 2011).
 Domain adaptation refers to the problem of adapting a prediction model trained on data from a source domain to a different target domain, where the data distribu-tions in the two domains are different. Effective do-main adaptation techniques are essential when labeled data are scarce or barely available in the target domain while there are plenty labeled instances in the source domain. A major challenge of domain adaptation is the data distribution divergence between the source and target domains. Some domain adaptation meth-ods attempt to bridge the distribution gap between the two domains by conducting instance weighting (Bickel et al., 2007) or co-training (Chen et al., 2011). Many others propose to reduce the domain divergence by learning generalizable features from the two domains, including structural correspondence learning (Blitzer et al., 2006), coupled subspace learning (Blitzer et al., 2011), and feature augmentation methods, easyadapt (EA) (Daum  X e III, 2007) and its co-regularization based semi-supervised extension (EA++) (Daum  X e III et al., 2010). These methods however are unsuitable for do-main adaptation tasks where the feature spaces of the two domains are different. Nevertheless, with ma-chine translation, the cross language text classification problem naturally forms a domain adaptation prob-lem, where the source domain includes the documents translated from the source language and the target domain includes the original documents in the target language. The domain divergence in this case mainly comes from the word drift due to the differences of culture and linguistic expression in different language regions. Many existing domain adaptation methods can be used for cross-lingual text classification. (Wei &amp; Pal, 2010; Prettenhofer &amp; Stein, 2010) use struc-tural correspondence learning for cross language text classification. (Wan et al., 2011) presents a feature and instance bi-weighting adaptation method for cross language text classification. These domain adaptation methods nevertheless can not directly exploit the origi-nal documents existing in the source language and thus suffer from the information loss introduced in machine translation process.
 Recently, multi-view learning methods in combination with machine translation have been applied on mul-tilingual learning scenarios, including cross language text classification tasks. Using machine translation, documents in each language can be translated into parallel documents in the other language to create two independent views of the text objects in differ-ent feature spaces. A few multi-view learning meth-ods then have been applied on such multi-view data, including the co-training method (Wan, 2009) which is an instance of the standard co-training algorithm of (Blum &amp; Mitchell, 1998), the multi-view majority vot-ing method (Amini et al., 2009), and the multi-view co-classification method (Amini &amp; Goutte, 2010) which is an instance of the co-regularized multi-view classi-fication (Sindhwani et al., 2002; Sindhwani &amp; Rosen-berg, 2008). These multi-view learning methods can exploit the original documents in both languages with-out translation information loss. In this work, we combine the domain adaptation in-tuition of learning generalizable feature representa-tions with the co-regularization principle of multi-view learning, and develop a subspace co-regularized multi-view method for cross language text classification. For simplicity, we consider binary classification tasks. We assume there are documents in two languages, the source language and the target language, for the same classification task. We exploit the data in the label-rich source language to assist training classifiers for the data in the label-scarce target language on top of ma-chine translation. In this section, we first introduce the basic notations, and then present the proposed multi-view learning method. 3.1. Notations Assume there are n s documents in the source language where l s of them are labeled and the remaining u s doc-uments are unlabeled. Similarly, assume there are n t documents in the target language where l t ( l t &lt; l s of them are labeled and the remaining u t documents are unlabeled. Using machine translation, we can translate each document in the source language into a parallel document in the target language, and vice versa. Combing the original and translated data to-gether in each language, we obtain two parallel matri-ces, X 1  X  IR n  X  d 1 in the source view and X 2  X  IR n  X  d in the target view, where n = n s + n t . The first l = l s + l t rows of X 1 and X 2 form the labeled subma-trices, X  X  1 and X  X  2 , respectively. Their corresponding labels are given as a column vector y  X  { X  1 , +1 } l . 3.2. Multi-View Training via Subspace We assume there is a low-dimensional subspace rep-resentation of the data in each view. The linear pre-dictive function in the i th view is derived from the subspace as follows where w i  X  IR m is the linear weight vector, b i  X  IR is the bias parameter,  X  i  X  IR d i  X  m is the linear trans-formation matrix that projects the input data into the low-dimensional subspace, and m is the dimensional-ity of the subspace. The transformation matrix  X  i has orthogonal columns such that  X   X  i  X  i = I where I is an identity matrix. Since the same classification task is shared between the two views, the underlying predictive subspace representations of the parallel doc-uments in the two views should be very similar. We thus formulate the cross language text classification as a semi-supervised multi-view optimization problem that minimizes the training losses on the labeled data in each view while penalizing the distance between the two view subspace representations of both labeled and unlabeled data. Specifically, we conduct training by minimizing the following regularized loss over the model parameters {  X  i , w i , b i } 2 i =1 , subject to the constraints  X   X  1  X  1 = I and  X   X  2  X  2 = I . Here V ( , ) is a general loss function, d ( , ) is a dis-tance function that measures the distance between the two projected low-dimensional matrices, {  X  i } 2 i =1 and  X  are tradeoff parameters. By conducting two-view semi-supervised training, we expect the subspace rep-resentations can capture both the task specific discrim-inative information of the labeled data and the under-lying intrinsic information of the unlabeled data. In this work, we consider a least square loss function and a squared Euclidean distance function, i.e., where k k 2 F denotes the Frobenius norm of matrix. Hence we get the following optimization problem Below we show that the optimal { w i , b i } can be solved in terms of  X  1 and  X  2 from the optimization problem. Lemma 1 The optimal { w  X  i , b  X  i } 2 i =1 that solve the op-timization problem in Eq. (4) is given by for i = 1 , 2 , where H = I  X  1 column vector of length l with all 1 entries. Proof: Taking the derivatives of the objective func-tion in Eq. (4) with respect to b 1 and b 2 respectively, and setting them to zeros, we obtain for i = 1 , 2. Substituting them back into Eq. (4), we have a new objective function as below Then taking derivatives of this new objective function with respect to w 1 and w 2 , and setting them to zeros, we obtain for i = 1 , 2.
 Following Lemma 1, the objective function in Eq. (4) can be rewritten as below by replacing { w i , b i }
L ( X  1 ,  X  2 ) =  X  X 1  X  1  X  X 2  X  2 k 2 F + 2 y  X  H y (7) where M i and z i are defined as Hence the optimization problem in Eq. (4) can be equivalently re-expressed as min The problem above is a non-convex optimization prob-lem. Nevertheless, the gradient of the objective func-tion with respect to {  X  1 ,  X  2 } can be easily computed, and its part corresponding to each  X  i is given as  X  for { i = 1 ,  X  i = 2 } or { i = 2 ,  X  i = 1 } . 3.3. Optimization Algorithm The non-convex optimization problem (8) is generally difficult to optimize due to the orthogonal constraints. In this work, we use a gradient descent optimization procedure with curvilinear search (Wen &amp; Yin, 2010) to solve it for a local optimal solution.
 In each iteration of the gradient descent procedure, given the current feasible point ( X  1 ,  X  2 ), the gradients can be computed using (9), such that We then compute two skew-symmetric matrices It is easy to see F  X  1 =  X  F 1 and F  X  2 =  X  F 2 . The next new point can be searched as a curvilinear function of a step size variable  X  , such that It is easy to verify that Q 1 (  X  )  X  Q 1 (  X  ) = I and Q 2 (  X  )  X  Q 2 (  X  ) = I for all  X   X  IR . Thus we can stay in the feasible region along the curve defined by  X  . Moreover, d d X  Q 1 (0) and d d X  Q 2 (0) are equal to the pro-jections of (  X  G 1 ) and (  X  G 2 ) onto the tangent space point ( X  1 ,  X  2 ). Hence { Q 1 (  X  ) , Q 2 (  X  ) }  X   X  0 path in the close neighborhood of the current point. We thus apply a similar strategy as the standard back-tracking line search to find a proper step size  X  using curvilinear search, while guaranteeing the iterations to converge to a stationary point. We determine a proper step size  X  as one satisfying the following Armijo-Wolfe conditions spect to  X  , =  X  where R i (  X  ) =  X   X  L ( Q 1 (0) , Q 2 (0)) =  X  The overall algorithm is given in Algorithm 1. 3.4. Multi-View Testing After the semi-supervised multi-view training, we ob-tain two prediction models defined in Eq. (1) with model parameters {  X  i , w i , b i } 2 i =1 . We then conduct multi-view testing on new documents. Specifically, given a test document, x  X  IR d 2 , in the target lan-guage, we first translate it into the source language to obtain b x  X  IR d 1 . Then we compute the prediction values using the two prediction models The prediction confidence of each predictor can be cal-set the prediction label for x as the one predicted from the most confident predictor, i.e., Algorithm 1 Optimization procedure
Input:  X   X  0 , 0 &lt; &lt; 1, 0 &lt;  X  1 &lt;  X  2 &lt; 1,
Procedure In this section, we report our empirical results on a set of cross language text classification tasks. 4.1. Experimental Setting The experiments were conducted on cross language text classification (CLTC) tasks constructed from a comparable multilingual corpus used in (Amini et al., 2009), which contains newswire articles written in 5 languages (English( E ), French( F ), German( G ), Ital-ian( I ), Spanish( S )), distributed over 6 classes ( C15, CCAT, E21, ECAT, GCAT, M11 ). In this multilin-gual corpus, each original document was translated into the other 4 languages using a statistical machine translation system. Our first set of experiments aim to evaluate CLTC tasks with different languages. We constructed a set of 20 binary cross language classifi-cation tasks over all possible source-target pairs of 5 languages, using two large classes, CCAT and ECAT, as shown in Table 1. For example, E2F denotes the task that uses English as the source language and uses French as the target language. For each language, we randomly selected 2000 original documents for each class to form the datasets. Thus in each task, we used 4000 original documents and 4000 translated doc-uments in each language. In each language, we used the top 400 features according to the sums of their TFIDF weights over all documents.
 Next , we constructed datasets to evaluate CLTC tasks on different classes. We selected 3 languages, French(F), German(G) and Italian(I), that have suffi-cient number of documents in all 6 classes to use. We then constructed 36 1-vs-all binary classification prob-lems over all 6 classes using 6 source-target pairs of languages, as shown in Table 2. For example, F2G denotes the tasks that use French as the source lan-guage and use German as the target language; C15 denotes the 1-vs-all binary classification task on class C15 . For each task, we randomly selected 2000 origi-nal documents from both the target class and the re-maining classes in each language. Thus same as above, we used 4000 original documents and 4000 translated documents for each task in each language.
 In the experiments, we compared the proposed Sub-space Co-regularized Multi-View learning method ( SCMV ) method with five other methods: (1) TB , a baseline method that trains a classifier using only the labeled original documents in the target language; (2) TSB , a baseline method that trains a classifier on both the labeled original documents in the target language and the labeled documents translated from the source language; (3) EA++ , the co-regularization based semi-supervised domain adaptation method de-veloped in (Daum  X e III et al., 2010), which uses a syn-thetic source domain formed by translating all doc-uments in the source language into the target lan-guage; (4) MVMV , the multi-view majority voting method developed in (Amini et al., 2009); and (5) MVCC , the semi-supervised version of the multi-view co-classification method (Amini &amp; Goutte, 2010), which penalizes the disagreement of the two view pre-dictions on unlabeled data. Among these methods, only the MVCC uses a logistic regression predictor as base classifier, and all other methods use least squares predictors as base classifiers. 4.2. Experiment I The first set of experiments are conducted on the 20 CLTC tasks constructed above. For each task, we randomly chose 900 labeled and 2100 unlabeled origi-nal documents from the source language domain, and chose 100 labeled and 2900 unlabeled original docu-ments from the target language domain for classifi-cation model training. Thus in total we had 1000 labeled documents and 5000 unlabeled documents in each language view for training. We used the remain-ing 1000 original documents in the target language as testing data. Based on this random data parti-tion procedure, we repeated the E2F experiment 3 times to conduct model parameter selection for MVCC and the proposed SCMV . The trade-off parameter for MVCC is selected from { 1 / 10 , 1 / 2 , 1 } . For the pro-posed SCMV , we used  X  1 = 0 . 1 ,  X  2 = 0 . 1 and selected lected 1 / 2 as the trade-off parameter for MVCC and selected 1 / 6 as the  X  parameter for SCMV . Using the selected parameters and the stated random data partition procedure, we then set the subspace di-mension for SCMV as 10, and repeated each experi-ment 10 times for the six methods in consideration. The average classification results on the test data in term of accuracy are reported in Table 1. We can see that between the two baseline methods, by exploiting the translated labeled documents from the source lan-guage, TSB has slight advantages over TB on many tasks. The domain adaptation method, EA++ , how-ever, produced similar performance as the baseline TSB . By exploiting both original data and translated data in the two languages, even the simple multi-view method, MVMV , works well on most tasks ex-cept the G2I and G2S. The semi-supervised multi-view co-classification method, MVCC , consistently outper-forms the four methods mentioned above, although the improvements are not significant on a few tasks in-cluding E2G, I2G and I2S. The proposed SCMV on the other hand consistently outperforms the other five methods on all tasks. The improvements over the first four methods are significant across all tasks. Even comparing to MVCC , the improvements are significant over most tasks.
 Subspace dimensions. Next, we empirically studied the influence of subspace dimension size on the pro-posed SCMV method. We repeated the experiments above for SCMV with different subspace dimension sizes, m= { 10,20,30,40,50 } . The average test accuracy results on tasks E 2 F, E 2 G, E 2 I and E 2 S are reported in Figure 1. We can see the performance of SCMV is not very sensitive to the subspace dimension size within the considered dimension range. 4.3. Experiment II The second set of experiments are conducted on the 36 CLTC tasks constructed with 1-vs-all classification problems. We used the same random data partition procedure and model parameters stated in experiment I. For the proposed method SCMV , we used 10 as the subspace dimension size. We repeated each experi-ment 10 times and the average test accuracy results are reported in Table 2. Similar to the first set of experiments, the proposed approach consistently out-performs all the other five methods on all 36 tasks, and the improvements are significant on 22 tasks compar-ing to the best comparison results.
 All these results suggest that identifying two-view con-sistent subspace representations based on both the original and translated data in two languages can ef-fectively overcome the cross language divergence and achieve good prediction models. In this paper, we proposed a novel subspace co-regularized multi-view learning method to address cross language text classification. By training two sub-space based prediction models in two language views together while penalizing the distance between the two projected subspace representations of both labeled and unlabeled instances, the underlying discriminative subspace representations can be identified to produce prediction models with better generalization perfor-mance. We developed a gradient descent algorithm with curvilinear search to solve the proposed joint op-timization problem for a local optimal solution. Our extensive empirical results on a large number of cross language text classification tasks demonstrated the su-perior performance of the proposed method comparing to a few inductive methods, domain adaptation meth-ods, and multi-view learning methods.
 Amini, M. and Goutte, C. A co-classification ap-proach to learning from multilingual corpora. Ma-chine Learning , 79:105 X 121, 2010.
 Amini, M., Usunier, N., and Goutte, C. Learning from multiple partially observed views -an application to multilingual text categorization. In Advances in Neural Information Process. Systems (NIPS) , 2009. Bel, N., Koster, C., and Villegas, M. Cross-lingual text categorization. In Proc. of the European Conference on Digital Libraries (ECDL) , 2003.
 Bickel, S., Br  X uckner, M., and Scheffer, T. Discrimina-tive learning for differing training and test distribu-tions. In Proc. of the International Conference on Machine Learning (ICML) , 2007.
 Blitzer, J., McDonald, R., and Pereira, F. Domain adaptation with structural correspondence learning. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2006.
 Blitzer, J., Foster, D., and Kakade, S. Domain adap-tation with coupled subspaces. In Proc. of the In-ternational Conference on Artificial Intelligence and Statistics (AISTATS) , 2011.
 Blum, A. and Mitchell, T. Combing labeled and unla-beled dta with co-training. In Proc. of the Annual Conference on Learning Theory (COLT) , 1998. Chen, M., Weinberger, K., and Blitzer, J. Co-training for domain adaptation. In Advances in Neural In-formation Processing Systems (NIPS) , 2011.
 Daum  X e III, H. Frustratingly easy domain adaptation.
In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL) , 2007.
 Daum  X e III, H., Kumar, A., and Saha, A. Co-regularization based semi-supervised domain adap-tation. In Advances in Neural Information Process-ing Systems (NIPS) , 2010.
 Ling, X., Xue, G., Dai, W., Jiang, Y., Yang, Q., and
Yu, Y. Can chinese web pages be classified with english data source. In Proc. of the international conference on World Wide Web , 2008.
 Prettenhofer, P. and Stein, B. Cross-language text classification using structural correspondence learn-ing. In Proc. of the Annual Meeting of the Associa-tion for Computational Linguistics (ACL) , 2010. Rigutini, L. and Maggini, M. An EM based training algorithm for cross-language text categorization. In Proc. of the Web Intelligence Conference , 2005. Shanahan, J. G., Grefenstette, G., Qu, Y., and Evans,
D. A. Mining multilingual opinions through classifi-cation and translation. In Proc. of AAAI X 04 Spring Symp. on Explor. Attitude and Affect in Text , 2004. Shi, L., Mihalcea, R., and Tian, M. Cross language text classification by model translation and semi-supervised learning. In Proc. of the Conference on
Empirical Methods in Natural Language Processing (EMNLP) , 2010.
 Sindhwani, V. and Rosenberg, D. An RKHS for multi-view learning and manifold co-regularization. In Proc. of the International Conference on Machine Learning (ICML) , 2008.
 Sindhwani, V., Niyogi, P., and Belkin, M. A co-regularization approach to semi-supervised learning with multiple views. In Proc. of the Workshop on Learning with Multiple Views, ICML , 2002.
 Wan, C., Pan, R., and Li, J. Bi-weighting domain adaptation for cross-language text classification. In
Proc. of the Interational Joint Conference on Arti-ficial Intelligence (IJCAI) , 2011.
 Wan, X. Co-training for cross-lingual sentiment clas-sification. In Proc. of the Annual Meeting of the Association for Comput. Linguistics (ACL) , 2009. Wei, B. and Pal, C. Cross lingual adaptation: an ex-periment on sentiment classifications. In Proc. of the Annual Meeting of the Association for Compu-tational Linguistics (ACL) , 2010.
 Wen, Z. and Yin, W. A feasible method for optimiza-tion with orthogonality constraints. Technical re-
