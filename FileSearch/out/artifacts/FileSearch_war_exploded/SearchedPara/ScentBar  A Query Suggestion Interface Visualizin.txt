 For intrinsically diverse tasks, in which collecting extensive infor-mation from different aspects of a topic is required, searchers often have difficulty formulating queries to explore diverse aspects and deciding when to stop searching. With the goal of helping searchers discover unexplored aspects and find the appropriate timing for search stopping in intrinsically diverse tasks, we propose ScentBar, a query suggestion interface visualizing the amount of important in-formation that a user potentially misses collecting from the search results of individual queries. We define the amount of missed infor-mation for a query as the additional gain that can be obtained from unclicked search results of the query, where gain is formalized as a set-wise metric based on aspect importance, aspect novelty, and per-aspect document relevance and is estimated by using a state-of-the-art algorithm for subtopic mining and search result diversi-fication. Results of a user study involving 24 participants showed that the proposed interface had the following advantages when the gain estimation algorithm worked reasonably: (1) ScentBar users stopped examining search results after collecting a greater amount of relevant information; (2) they issued queries whose search re-sults contained more missed information; (3) they obtained higher gain, particularly at the late stage of their sessions; and (4) they obtained higher gain per unit time. These results suggest that the simple query visualization helps make the search process of intrin-sically diverse tasks more efficient, unless inaccurate estimates of missed information are visualized.
 query suggestion interface; intrinsic diversity; search stopping
Searchers often issue more than one query and browse multiple documents in exploratory search tasks [31]. These tasks can be characterized as open-ended and multi-faceted, requiring searchers  X 
Currently at the University of Tokyo and at the National Institute of Information and Communications Technology
Figure 1: Our concept of visualizing missed information. to collect multiple relevant documents [32]. Many people conduct exploratory searches in the medical and health domains [4]. Take, as an example, a searcher who investigates the effect of smoking through Web searches in order to decide whether to continue smok-ing or not. This search topic contains various aspects such as dis-eases caused by smoking, increases in the price of cigarettes, and how smoking affects mental health. To fully understand this topic and make appropriate decisions, the searcher needs to collect exten-sive information covering different aspects. Such tasks are called intrinsically diverse search tasks in the literature [25]. There are several problems common to intrinsically diverse tasks. First, searchers often cannot easily come up with effective queries for collecting documents that cover diverse aspects. They have to issue more queries to complete the tasks if search engines re-turn few documents relevant to unexplored aspects. Second, decid-ing when to stop searching is also difficult for searchers. Quitting the tasks too early without in-depth exploration prevents searchers from finding essential information; on the other hand, if they have already obtained extensive information, continuing these tasks for too long wastes time and effort, as they will not be able to acquire much more gain. The above problems are primarily due to the fact that (1) searchers may not know what aspects exist in the search topic and how important they are and (2) they cannot guess how much important information on each aspect is available on the Web and how much of the information remains unexplored.

The present study proposes a query suggestion interface, which we call ScentBar, with the goal of helping searchers discover unex-plored aspects and find the appropriate timing for search stopping in intrinsically diverse tasks. As shown in Figure 1, ScentBar visu-alizes, for both the search query and suggestion queries, the amount of missed information 1 important to the current search topic in the form of a stacked bar chart so that users can grasp their search progress visually. We define the amount of missed information for a query as the additional gain that can be obtained from unclicked search results of the query. Gain is formalized as a set-wise metric based on aspect importance, aspect novelty, and per-aspect docu-
We use the phrase  X  X issed information X  to refer to information that has been retrieved by the system but that the searcher misses collecting, similar to the definition by Mansourian and Ford [20]. ment relevance and is estimated by using a state-of-the-art algo-rithm [30] for subtopic mining and search result diversification.
We conducted a user study involving 24 participants to investi-gate how ScentBar affected their search strategies and search out-comes. Post-hoc analyses revealed that ScentBar had the following advantages when the gain estimation algorithm worked reasonably: (1) ScentBar users stopped examining search results after collecting a greater amount of relevant information; (2) they issued queries whose search results contained more missed information; (3) they obtained higher gain, particularly at the late stage of their sessions; and (4) they obtained higher gain per unit time. These results sug-gest that the simple query visualization helps make the search pro-cess of intrinsically diverse tasks more efficient, unless inaccurate estimates of missed information are visualized.

The main contributions of this paper are as follows.
Existing studies related to this work cover the following research areas: (1) search interfaces showing information scent, (2) under-standing and modeling searchers X  stopping behavior, and (3) subtopic mining for search result diversification. The concept of information scent was introduced in Information Foraging Theory [22], which explains the information seeking be-havior of human beings by making an analogy to the food forag-ing behavior of animals. In this theory, information scent indicates proximal cues from which searchers perceive the value of distal information sources [6]. As summarized in Hearst X  X  book [12], considerable research has been conducted on developing search in-terfaces that show users information scent [11, 13, 17] with the ob-jective of gaining a better understanding of users X  search behavior and/or making their searches more effective.

To help searchers make quick relevance assessments, Hearst [11] proposed the TileBars interface visualizing the query term occur-rence as a rectangle for each search result, where the horizontal bar represents the document length and the vertical one represents the query terms. Iwata et al. [15] also proposed a tile-based visualiza-tion interface, called AspecTiles, which was designed to help users issue queries with multiple aspects. Zha et al . [37] proposed an in-terface that suggests queries with their representative images so that users can efficiently convey their specific search intents. Aiming to help searchers understand inter-query relationships, Kato et al . [16] proposed SParQS, which presents query suggestions classified into automatically generated categories.

One of the studies closest to ours is the query preview interface proposed by Qvarfordt et al . [24]. Their interface visualizes three kinds of information on the search results of the current search query in the form of a stacked bar chart: (1) the number of newly re-trieved search results, (2) the number of re-retrieved but not clicked ones, and (3) the number of re-retrieved ones that have already been clicked by the searcher. While both interfaces provide query-level proximal cues, ScentBar is designed to help searchers conduct in-trinsically diverse tasks, where the click information alone is, we think, less informative as they need to collect extensive informa-tion covering a variety of different aspects. Therefore, given the complexity of intrinsically diverse tasks, we consider aspect-level factors when formalizing the amount of missed information.
Much effort has been devoted to understanding how users decide when to stop searching [20, 23, 33, 35] and to modeling searchers X  stopping behavior [18]. Most studies have been based on inter-views that clarify the qualitative characteristics of search stopping. Prabha et al . [23], for example, interviewed people in academia to analyze how much information is enough to meet their information needs. They reported that study participants had qualitative crite-ria for search stopping, including whether they feel that sufficient information has been collected (the sense of  X  X ood enough X  [35]).
Toms and Freund X  X  work [29] is one of the few quantitative stud-ies on search stopping. They analyzed the characteristic actions that preceded people stopping their searches. Wu et al . [34] con-ducted a user study in which they varied the number and distri-bution of relevant search results and found that these factors had different effects on when searchers left the search engine results pages (SERPs). There have also been attempts to mathematically model search stopping behavior [2]. Maxwell et al . [21] very re-cently investigated different search stopping rules to find which one approximated actual search behavior most closely.

Dostert and Kelly [8] demonstrated through a user study the gap between cognitive and actual recall: while the study participants believed that they had found about 51 X 60% of relevant documents when they stopped their search tasks, the actual recall they obtained was less than 10% on average. These results suggest that searchers have difficulty in accurately estimating the amount of relevant in-formation they have (or have not) found.
Researchers have proposed various search result diversification algorithms [1, 9, 27] to satisfy different user intents behind ambigu-ous or underspecified queries. Most of these algorithms depend on subtopic mining, which is a technique for mining the intents under-lying a given query, in order to find a set of documents that cover as many important intents as possible. Approaches to subtopic min-ing can be classified into two categories: subtopics that are mod-eled explicitly or implicitly [10]. Maximal Marginal Relevance (MMR) proposed by Carbonell and Goldstein [3] is based on the implicit modeling of subtopics; it iteratively selects a document that is relevant to a given query and dissimilar from already se-lected documents as the next element of a diversified document list. Approaches belonging to explicit subtopic modeling include IA-Select proposed by Agrawal et al . [1], who defined the problem of search result diversification as discovering a set of documents covering important subtopics.

Tsukuda et al . [30] proposed one of the state-of-the-art algo-rithms for explicitly mining subtopics. Their algorithm utilizes dif-ferent resources to collect subtopics for a given query and clusters the resulting subtopics to identify intents behind the query. We use their algorithm to identify aspects underlying intrinsically diverse tasks. The resulting aspects are used to estimate the amount of missed information for individual queries. We also use the MMR algorithm to diversify search results returned by a search engine.
In this section, we illustrate how ScentBar works through an ex-ample and describe the research questions addressed in this work. (a) At beginning of search task Figure 2: How ScentBar works. The state of missed informa-tion after the initial search is shown in Figure 1.
When a user is typing a search query, ScentBar visualizes the amount of missed information for both the search query and sug-gestion queries in the form of a stacked bar chart. To be precise, missed information for a query represents information that (1) can be obtained from the search results of the query, (2) is important to the search topic, and (3) the user has not yet obtained.
 Take the search topic described in Section 1 as an example again. Figure 2a illustrates the state of ScentBar visualization when a user starts his/her search task with the query  X  X moking cancer risk X  to learn the effect of smoking. At the beginning of this task, the amount of missed information for a query can be interpreted as the total amount of important information that the user can obtain from the search results of this query. For example, the user can infer from Figure 2a that the queries  X  X moking cancer risk X  and  X  X iseases caused by smoking X  return search results containing more impor-tant information than those such as  X  X igarettes price increase X .
When the user has obtained sufficient important information from the returned documents, the amount of missed information for the search query decreases, as shown in Figure 1. The length of the bar for the search query (indicated in deep pink) is much shorter than its initial length (indicated in light pink), which suggests this query contains only a small amount of missed information. Note that the bar for the suggestion query  X  X iseases caused by smoking X  also has a short length compared to the initial state. This is because the sug-gestion query shares with the search query a certain amount of im-portant information. In contrast, the visualized bars for other sug-gestion queries ( e.g. ,  X  X moking ruins your looks X ) remain almost unchanged from the initial ones, which suggests great amounts of important information are still unexplored for these queries.
When the user has exhaustively collected important information on the search topic from a variety of angles through several query reformulations, the amount of missed information considerably de-creases for any query related to the topic. Figure 2b illustrates the state of ScentBar visualization at the end of the search task. As suggested in the figure, there is little important information left for either the search query or the suggestion queries.
In the present study, we investigate the effect of displaying missed information on searchers X  strategies and outcomes. More specifi-cally, we address the five research questions listed below.
In intrinsically diverse tasks, searchers are more likely to issue multiple queries to exhaustively collect relevant information from different angles. Thus, we frame the following two research ques-tions on individual searches ( i.e. , after a query is issued before an-other one is issued or the session ends): RQ1 How does ScentBar affect users X  decisions on when to stop RQ2 How does ScentBar affect users X  decisions on which query to We expect our visualization enables searchers to utilize more ra-tional strategies in individual searches. For example, ScentBar users may be able to stop the current search after collecting a suffi-cient amount of important information by monitoring the bar visu-alization for the search query. Missed information may also affect searchers X  query formulation strategies; ScentBar users may use a more effective query for the subsequent search by comparing the visualized bars for suggestion queries.

The remaining three research questions relate to the overall search sessions of intrinsically diverse tasks: RQ3 How does ScentBar affect the temporal change in gain that RQ4 How does ScentBar affect users X  decisions on when to stop RQ5 How does ScentBar affect the relationship between the effort Missed information may affect searchers X  strategies and outcomes at the session level as well as at the query level. For example, Scent-Bar users may acquire high gain at any point in their sessions. Anal-ogous to RQ1 , ScentBar may also enable users to rationally decide when to stop the task sessions 2 . It can also be hypothesized that ScentBar makes users X  search processes more cost-effective: they may be able to collect a sufficient amount of important information from a variety of angles without expending much effort.
In this section, we first introduce gain , a metric for evaluating search outcomes in intrinsically diverse tasks, and then define the amount of missed information by using the gain metric. We then describe our algorithm of estimating the gain components.
Let us consider how searchers can obtain gain in intrinsically di-verse tasks. As described in Section 1, these tasks require searchers to collect extensive information covering a variety of different as-pects. Considering this characteristic, it would be natural to assume that the gain they obtain through their searches is independent of their browsing order of documents. Thus, we formalize gain as a set-wise metric, whose relation to other metrics is discussed in Section 4.2. We also derive from this characteristic the following requirements that the gain metric should satisfy: Importance Documents relevant to a central aspect of the search topic produce higher gain than those relevant to a peripheral one. Relevance Highly relevant documents produce higher gain than partially relevant ones.
 Novelty Documents relevant to an unexplored aspect produce higher gain than those relevant to a fully explored aspect.

First, we decompose the gain metric from the topic level to the aspect level in an intent-aware manner [1]. More specifically, given a topic t , we formulate the topic-level gain Gain-IA t ( D ) that can be obtained from a set of documents D = { d 1 ,d 2 ,... } as where A t is a set of aspects for t , Pr( a | t ) is a probability mass function representing the importance of an aspect a to t , and Gain is the per-aspect gain that can be obtained from the documents D with respect to a . To satisfy the first requirement, the above for-mula puts greater value on the per-aspect gain for highly important aspects when calculating the topic-level gain. As in the literature [21], we refer to stopping behavior regarding
RQ1 as query stopping and that regarding RQ4 as session stop-ping . Search stopping is used as the generic phrase for these two. Next, the per-aspect gain is calculated by where Rel a ( d )  X  [0 , 1] denotes the degree of relevance of a docu-ment d to the aspect a . To satisfy the second requirement, this for-mula sums up the per-aspect relevance degrees of individual docu-ments and returns a high value if D contains many documents that are highly relevant to a . Note that Gain a (  X  ) = 0 .

The other term Disc a (  X  ) in the above formula is a function de-signed to satisfy the third requirement; it discounts the degree of per-aspect document relevance Rel a ( d i ) when the aspect a is well covered by a set of documents { d 1 ,...,d i  X  1 } that have already been browsed. We define the discount function as When D 0 contains many documents that are highly relevant to the aspect a , the above formula returns a value of nearly zero. In this case, newly browsed documents relevant to a receive high dis-counts from this formula and therefore contribute little to the per-aspect gain for a . Note that Disc a (  X  ) = 1 .
Gain-IA, the intent-aware gain metric defined above, apparently looks similar to the intent-aware version of Expected Reciprocal Rank (ERR-IA) [5], which is an evaluation metric for diversified search results. The only difference between ERR-IA and Gain-IA is that the latter does not have any reciprocal rank factor. As ERR-IA evaluates the effectiveness of ordered search results, it discounts the value of lower-ranked documents. Unlike ERR-IA, our objec-tive is to evaluate the gain that searchers can obtain from unordered documents, separately from their expended effort. Thus, we de-signed Gain-IA to be a set-wise metric that is affected by neither the rank of nor the browsing order of documents. This is obvious from the fact that Gain-IA has the following equivalent form:
Gain-IA t ( D ) = X which is derived from the following lemma 3 : L EMMA 1. Gain a ( D ) = 1  X  Disc a ( D ) .

Note that the rewritten Gain-IA is exactly the same as the ob-jective function formulated by Agrawal et al . [1] for search result diversification. Assuming that Rel a ( d ) represents the probability that a document d satisfies a searcher who believes an aspect a is as important to a topic t as Pr( a | t ) , this function can be viewed as the probability that the searcher gains satisfaction with the topic t by browsing a set of documents D .
Using the above-mentioned gain metric, we define the amount of missed information for a query as the additional gain that can be obtained from unclicked search results of the query. Let D set of documents that a user u has already browsed in the current task on a topic t and D K q be a set of top-K documents returned for a query q . Then, MI u,t ( q ) , the amount of missed information for q , is defined as Under situations where the above formula behaves ideally, a high value of MI u,t ( q ) indicates that unclicked documents D contains a large amount of information important to the topic t that the user u misses collecting from the search results of the query q .
Due to space limitation, we omit the proof of Lemma 1, which can be accomplished by mathematical induction.
The following three components are required to calculate the gain and the amount of missed information on a topic t : (1) A a set of aspects for t ; (2) Pr( a | t ) , a probability representing the importance of an aspect a to t ; and (3) Rel a ( d ) , the degree of rel-evance of a document d to a . As for the parameter K ( i.e. , the number of search results to be fetched), which is also required to calculate the amount of missed information, we report the value used in our experiment in Section 5. We describe below our algo-rithm of estimating these three components. Note that, if the topic is known beforehand, these components can be estimated before ScentBar users conduct their searches.

As overviewed in Section 2.3, existing approaches to explicitly mining subtopics have the same task of estimating the above com-ponents. Thus, we decided to use one of the state-of-the-art explicit subtopic mining algorithms developed by Tsukuda et al . [30] to perform this estimation task. Their algorithm, which is easy to im-plement, achieved the second-best performance among 14 submis-sions in the Subtopic Mining subtask of the NTCIR-10 INTENT-2 task [26]. In what follows, we describe the outline of the algorithm and clarify some differences from theirs 4 . As described hereinafter, this algorithm relies on a search engine to fetch search results for given queries. The search engine for the algorithm is identical to the one for our search interface (see Section 5 for more details). Topic Aspects. To estimate a set of aspects A t for a given topic t , we first mine a set of subtopics S t for t using three resources, following Tsukuda et al . [30]. The first resource consists of query suggestions ( i.e. , related queries and auto-completion queries) re-turned by Web search engines in response to the topic query t . Each suggested query is regarded as a subtopic of t . The second resource is query logs, from which queries starting from t are extracted as subtopics. The last resource is the search results for the topic query. The clustering algorithm proposed by Zeng et al . [36] is applied to the search results for extracting key phrases as subtopics from in-dividual clusters. Once S t is mined in this way, Ward X  X  method is applied to S t for obtaining a set of subtopic clusters C which is regarded as belonging to an aspect of the topic t .
As described in Section 5, the search topics used in our exper-iment were selected from tasks in past NTCIR workshops. Thus, we utilized the resources distributed to the task participants to mine subtopics for these topics.
 Aspect Importance. To estimate the importance probability of an aspect of a topic t , we first estimate Imp t ( s ) , the importance of a subtopic s to t , by using Imp t ( s ) = P d  X  D N as with Tsukuda et al . [30], where Rank t ( d ) denotes the rank of a document d returned for the topic query t . Next, we select a representative subtopic a from each cluster C  X  C t by using a = arg max s  X  C Imp t ( s ) . The selected subtopic a is regarded as a member of the aspect set A t for the topic t . Finally, we estimate the importance probability Pr( a | t ) of each aspect a  X  A by Tsukuda et al . [30] performs the relevance estimation only for top-N documents returned for each aspect query a  X  A t . This is probably because their objective is to obtain diversified search results comprising about ten documents. However, searchers are expected to browse many more documents in intrinsically diverse tasks. To obtain more accurate estimates of missed information,
We use parameter values reported in the original paper [30].
Figure 3: Screenshot of ScentBar (topic:  X  X lobal warming X ). the relevance should be estimated for as many related documents as possible. Thus, we expand the target of the relevance estimation with respect to an aspect a to D a = S s  X  C is a cluster of subtopics belonging to a . In light of the subtopic im-portance, we estimate Rel a ( d ) , the relevance degree of a document d  X  D a to an aspect a  X  A t , by where Rel s ( d ) is d  X  X  relevance degree to a subtopic s and is esti-mated by Rel s ( d ) = 1 / p Rank s ( d ) , as with Tsukuda et al . [30].
We conducted a user study in a laboratory setting to investigate the research questions listed in Section 3.2. The details of the ex-perimental design are described below.
We designed the type of interfaces was to be a within-subjects factor in our user study, where we compared two search interfaces: (1) w/ scent , i.e. , the proposed interface ScentBar, which visualizes missed information for individual queries; and (2) w/o scent , i.e. , a baseline interface without missed information visualization.
Figure 3 shows the screenshot of ScentBar for the sample search topic  X  X lobal warming X . Note that the baseline interface has the same appearance as ScentBar except for the query visualization. The brief description of the current search topic is shown at the top of these interfaces. While a user is typing a query into the search box (at the upper left), the interfaces obtain at most ten auto-completion queries through the Google Suggest API 5 them at the bottom of the search box. ScentBar also visualizes missed information for these queries in the form of a stacked bar chart. The suggested auto-completion queries remain at the same place even after the user issues the current query to our search sys-tem. This behavior allows the user to easily reformulate his/her search query by clicking the bar of a suggestion query. The top K search results returned for the query are shown on the right side of the interfaces. When the user clicks the title of a search result, a document viewer is displayed within the interfaces and the user can browse the landing page on the viewer. The interfaces also have a feature for bookmarking Web pages displayed on the document viewer and present the information on which search results have already been clicked and bookmarked for the current query (on the right side of each search result) and on how many documents have already been clicked and bookmarked through the current search task (at the bottom of the interfaces). http://www.google.com/complete/search
Both interfaces described above used the same internal search system to fetch search results in response to queries. We built the search system on Apache Solr and indexed documents in ClueWeb09-JA, the Japanese portion of the ClueWeb09 collection 6 , into this system. We utilized the Okapi BM25 algorithm with default Solr parameters ( i.e. , k 1 = 1 . 2 and b = 0 . 75 ) to rank documents.
When testing our implemented interfaces, however, we found that near-duplicate documents sometimes occupied high positions in the search results, which would prevent users from exhaustively collecting information from multiple aspects. To deal with this problem, we decided to diversify the search results by applying the MMR algorithm [3] to R K q , the top-K documents retrieved by BM25 for a query q . More precisely, MMR selects d k , a document ranked at the k -th position, by using d = arg max where S k  X  1 denotes a set of k  X  1 documents that MMR has already selected and Rel BM25 q ( d ) returns a document d  X  X  relevance score that BM25 calculates for a query q . As for Sim(  X  ,  X  ) , we used the cosine similarity between tf-idf vectors, each of which was con-structed from a set of terms appearing in the title and snippet areas. In this way, the resulting list of top-K diversified documents D was returned to the experimental interfaces in response to a query q . The control parameter was set to  X  = 0 . 3 , following Tsukuda et al . [30] and Dou et al . [9]. The number of search results to be fetched was set to K = 100 , following Qvarfordt et al . [24].
As the topic source, we decided to use the NTCIR INTENT-1 [28], INTENT-2 [26], and IMine-1 [19] tasks, where the as-pects of each topic and their imporance are provided for evaluating search result diversification. We selected four topics that satisfied the characteristics of intrinsically diverse tasks ( i.e. , having diverse documents relevant to different aspects). These topics are listed in Table 1. Two of them were selected from the medical domain, in which people often conduct exploratory searches to collect di-verse information from multiple sources [4], while the remaining two were from non-medical domains. Table 1 also shows a sub-set of oracle aspects for these topics, which were provided by the NTCIR task organizers. As shown in this table, there are a number of different aspects in these topics (the mean number of oracle as-pects per topic was 10 . 50 ), and multiple relevant documents may be needed to cover each aspect.

We used these topics to design four types of intrinsically diverse search tasks. More specifically, the following description was given to the participants of our user study for the search task on a topic t :
You were given the assignment of submitting a thorough report on the topic t . To fully understand t , collect relevant informa-tion on this topic from a number of different aspects that you think is important. You may end this search task when you feel there is little important information left.
We recruited study participants via the recruiting Web site of our university. As a result, we had 24 participants (4 females and 20 males) in our user study: 16 undergraduate students, 6 graduate stu-dents, and 2 researchers. Their mean age was 23 . 29 ( SD = 3 . 67 ) http://www.lemurproject.org/clueweb09.php
We normalized Rel BM25 q (  X  ) to [0 , 1] by using the BM25 score of a document ranked at the first position for a query q . and their fields of study included informatics (six participants), en-gineering (four), law (four), science (three), etc . At the end of the experiment, they received bookstore gift cards (equivalent to $ 16 per person) as a reward for participating. Prior to the actual tasks, participants were assigned a training task (on the topic of global warming) to get familiar with the tasks and interfaces. During the training task, we provided participants with detailed instructions on how to use the interfaces and on what they were expected to do in our experiment. The training task took approximately 15 minutes.
A demographic questionnaire was administered at the beginning of the experiment. After filling out the questionnaire, participants worked on the four tasks one by one (two tasks with ScentBar and two with the baseline interface). The order of interfaces and tasks was determined with a Graeco-Latin square to remove the order-ing effect of these variables. All tasks followed the same proce-dure. First, participants read the task description and completed a pre-task questionnaire asking about their knowledge on the topic and their estimated difficulty of the task. Next, they started the task with the assigned interface. To make the search process as realistic as possible, they were allowed to freely formulate search queries and were asked to use the bookmark feature provided by both interfaces when they found documents useful for completing the task. We did not allow them to follow links in the landing page because we could not guarantee that the linked pages were stored in ClueWeb09-JA, the document corpus used by our search system. After completing the task, participants filled out a post-task ques-tionnaire asking about task-level feedback ( e.g. , their experienced difficulty). Finally, we administered an exit questionnaire at the end of the experiment to gather feedback on both interfaces.
As mentioned above, we asked the participants to finish each task when they felt they had exhaustively collected relevant information from various aspects of the search topic ( i.e. , each task had no time limit). We did not tell them the total number of tasks so as to avoid its effect on their stopping decisions, since they knew in advance from the recruitment information that the maximum amount of time required for our experiment was two hours. Instead, we instructed them that there were a certain number of tasks and that the experi-ment would finish when they had completed all tasks or two hours had passed from the beginning of the experiment. Note that, in a pilot study, every task was completed within 20 minutes. In fact, all participants completed the whole experiment within two hours.
This section reports the results of our user study and attempts to answer the research questions listed in Section 3.2. As normal-ity was not guaranteed for the experimental data, we used non-parametric significance tests in our post-hoc analyses. Significant effects are reported on the significance level  X  = 0 . 05 .
To evaluate gain that participants obtained, we prepared the ground-truth data for each search topic used in our user study. As described in Section 4.4, the following three components are required to cal-culate the gain and the amount of missed information: (1) a set of aspects for the search topic, (2) the importance of each aspect, and (3) per-aspect document relevance. As these topics were selected Table 2: Correlations between oracle and estimated gains. The top three values for each measure are shown in bold face. from the NTCIR tasks, we used oracle data provided by the task organizers as for the first two components.
 Relevance Assessment. To prepare the remaining ground-truth data ( i.e. , per-aspect document relevance), three assessors judged the relevance of each document that had been browsed by partici-pants. A four-grade scale was used to assess the relevance of a doc-ument for each oracle aspect: irrelevant ( = 0 ), partially relevant ( = 1 ), highly relevant ( = 2 ), and perfect ( = 3 ). The following cri-teria were shared between assessors to make the assessed relevance as consistent and reliable as possible: documents covering (1) less than 30%, (2) at least 30% but less than 60%, (3) at least 60% but less than 90%, and (4) more than 90% of the information on a cer-tain aspect should be respectively labeled as (1) irrelevant, (2) par-tially relevant, (3) highly relevant, and (4) perfect for that aspect. The resulting relevance grade g a,d  X  { 0 , 1 , 2 , 3 } of a document d for an aspect a was converted into the oracle relevance degree by Rel  X  a ( d ) = (2 g a,d  X  1) / 2 3 . This conversion is often used when cal-culating evaluation metrics, such as ERR [5], and guarantees that the resulting relevance degree is any of { 0 , 0 . 125 , 0 . 375 , 0 . 875 } . mation is defined as the additional gain, which is estimated by the algorithm mentioned in Section 4.4. If the estimated gain is far dif-ferent from the oracle one, our interface ends up showing searchers missed information that is unrealistic and unreliable, which may have a negative effect on their searches. Thus, we decided to inves-tigate the gap between the gain estimated by our algorithm and that calculated using the ground-truth data, before conducting post-hoc analyses. The former is referred to as estimated gain and the latter as oracle gain hereinafter.

To this end, we calculated the estimated and oracle gain scores at each point where participants browsed documents and measured correlations for pairs of gain scores using Pearson X  X  r , Spearman X  X   X  , and Kendall X  X   X  . Note that strong correlation for a topic indicates high consistency between the change in estimated gain and that in oracle gain, which suggests the gain algorithm works reasonably for that topic. Table 2 shows the values of these three correlation measures calculated for each topic.

As shown in this table, quite high correlation was observed for all topics except T4 ( X  X inosaurs X ), which suggests that ScentBar users who conducted searches on the topic T4 might have experienced unexpected changes in visualized missed information. When ana-lyzing the possible causes of the inaccurate gain estimation for T4, we found that aspects mined by our algorithm as important to this topic included  X  X erchandising sales X  and  X  X keleton models sell-ing X . As can be seen in Table 1, none of these were contained in the oracle aspects for this topic. We also found that many documents retrieved by the query  X  X inosaurs X  at high positions tended to be Table 3: Mean change (with SD) in oracle missed information at the query level. Significant differences from the baseline in-terface at p &lt; 0 . 05 are shown in bold face.
 less relevant to the oracle aspects. As described in Section 4.4, the gain estimation algorithm relies on aspect mining and document ranking. Thus, the aspect mismatch and poor document ranking would cause the estimated gain to be inaccurate for the topic T4.
As noted above, inaccurate gain estimation could have an unde-sirable effect on participants X  fulfilling search tasks; they might get confused by seeing unreliable missed information and quit search-ing at an inappropriate point. To investigate the effect of the accu-racy of the estimated gain, the analyses reported in the remainder of this section were conducted at the following three topic levels: (1) all topics; (2) HC topics , which comprise T1, T2, and T3, where the estimated gain had high correlation with the oracle one; and (3) LC topic , which comprises T4, where low correlation was observed between the estimated gain and the oracle one.
To answer RQ1 , we investigated the effect of missed informa-tion on query stopping ( i.e. , the point at which a user stops exam-ining search results for the current query). We hypothesized that ScentBar users would stop the current search once they had ex-haustively collected relevant documents from the search results. If this is the case, query stopping should occur when the amount of missed information for the query greatly decreases from its initial value. Thus, for each query issued by participants, we calculated the change in the amount of oracle missed information between the beginning of the task and the query stopping point. A large change indicates that the searcher obtained a great amount of relevant in-formation from the current search.

Table 3 shows the mean change (with SD) in the amount of oracle missed information at the query level. As for all topics, the amount of oracle missed information decreased greatly when participants used ScentBar. The difference from the baseline in-terface was shown to be significant by the Mann-Whitney U test ( p = 0 . 001 ). The greater difference was observed for the HC topics ( p &lt; 0 . 001 ). In contrast, the change in oracle missed information was quite small for the LC topic ( p = 0 . 683 ). These results indi-cate that when gain was estimated reasonably accurately, ScentBar users collected a greater amount of relevant information through in-dividual searches than the baseline interface users, which supports our hypothesis on query stopping. When inaccurate estimates were presented to participants, however, ScentBar had no effect on the amount of collected information at the query level.
To answer RQ2 , we investigated the effect of missed information on users X  query selection decisions in the subsequent search. We hypothesized that ScentBar would help users issue queries whose search results contained much missed relevant information. To test this hypothesis, we calculated, for each query issued by partici-pants, the amount of oracle missed information on query issuing.
Table 4 shows the mean (with SD) of the amount of oracle missed information on query issuing. On average, queries with large amounts of oracle missed information were issued through ScentBar rather than through the baseline interface for all topics. The Mann-Whitney U test revealed a significant difference between these interfaces Table 4: Mean (with SD) of oracle missed information on query issuing. Significant differences from the baseline interface at p &lt; 0 . 05 are shown in bold face.
 ( p = 0 . 002 ). While a clearer trend was observed for the HC top-ics ( p &lt; 0 . 001 ), the difference was insignificant for the LC topic ( p = 0 . 521 ). These results indicate that when the gain was es-timated reasonably, ScentBar users issued queries more effective for exhaustively collecting relevant information than the baseline interface users. This supports our hypothesis on query selection. When the gain estimation was inaccurate, however, queries issued through the two interfaces differed little from one another in terms of the amount of oracle missed information on query issuing.
To answer RQ3 , we investigated the effect of missed informa-tion on the change in gain over time. More specifically, we split participants X  task sessions into one-minute segments and calculated the (cumulative) oracle gain they had obtained at each cutoff time point. Note that when calculating the gain at a time point, we ex-cluded participants who had completed the task before that time.
Figure 4 shows the gain curves averaged over the individual in-terfaces. The two gain curves for all topics (Figure 4a) behaved similarly until seven minutes had passed from the task initiation. After that, the oracle gain for ScentBar outperformed that for the baseline interface at nearly every time point. The difference be-tween the interfaces widened for the HC topics (Figure 4b) while the gain curve for ScentBar underperformed that for the baseline interface at most time points (Figure 4c). These results indicate that when the gain estimation was accurate enough, ScentBar users obtained as much gain at an early time of their task sessions as, and higher gain after that time than, the baseline interface users. In con-trast, when ScentBar presented inaccurate estimates to participants, they obtained less gain than the baseline interface users who spent the same amount of time or more searching.
To answer RQ4 , we investigated the effect of missed informa-tion on session stopping ( i.e , the point at which a user completed the current search task). As in the case of our analysis on query stopping (reported in Section 6.2), we hypothesized that ScentBar users would complete the current task after exhaustively collecting information relevant to the search topic. If this is the case, ses-sion stopping should occur when the amount of oracle missed in-formation for most queries related the task greatly decreases from the initial values. Thus, for each query issued by participants, we calculated the change in the amount of oracle missed information between the beginning and end of the task. In addition, we also calculated the (total) oracle gain obtained from the documents that individual participants had collected through their task sessions.
In Table 5, the mean change (with SD) in the amount of oracle missed information at the session level is shown at the top, and the mean (with SD) of the oracle gain obtained through task sessions is at the bottom. As for the former measure, the amount of oracle missed information decreased greatly for all topics when partici-pants used ScentBar. The Mann-Whitney U test revealed a signifi-cant difference from the baseline interface ( p = 0 . 002 ). The larger difference was observed for the HC topics ( p &lt; 0 . 001 ), while there was no significant difference for the LC topic ( p = 0 . 845 ). As for (a) All topics Table 5: Top: mean change (with SD) in oracle missed informa-tion at the session level. Bottom: mean (with SD) of oracle gain obtained through task sessions. Significant differences from the baseline interface at p &lt; 0 . 05 are shown in bold face. the latter measure, the mean oracle gain for ScentBar was higher for all topics than that for the baseline interface. While the same trend was observed for both the HC and LC topics, we could not find any significant differences between the two interfaces ( p = 0 . 097 for all topics, p = 0 . 191 for the HC topics, and p = 0 . 410 for the LC topic). These results partially support our hypothesis on ses-sion stopping: when the gain estimation algorithm worked reason-ably, ScentBar users completed their tasks after collecting greater amounts of relevant information from the SERPs of their issued queries. However, the resulting total gain obtained through Scent-Bar did not show statistically significant improvement compared to the baseline interface, which implies some users of ScentBar might complete their tasks earlier than others who used the baseline inter-face. The analysis related to this issue is reported in Section 6.6.
To answer RQ5 , we investigated the effect of missed informa-tion on the relationship between expended effort and acquired gain. In the present study, the time that participants spent completing the tasks was regarded as the effort that they expended. We plot-ted scatter charts in which the x -axis represented the task comple-tion time (in minutes) and the y -axis represented the oracle gain that participants obtained through their task sessions. We also per-formed a linear regression analysis to better understand the rela-tionship between the effort (modeled as the explanatory variable) and the gain (as the response variable).

Figure 5 shows the scatter charts and regression lines of the effort-gain data. As can be seen in Figure 5a, little difference ex-isted between the two interfaces for all topics. In fact, the slope  X  of both regression lines was almost the same:  X  = 0 . 008 ( R 0 . 187 ) for ScentBar and  X  = 0 . 007 ( R 2 = 0 . 158 ) for the baseline interface. As for the HC topics (Figure 5b), the great improve-ment was observed for ScentBar (  X  = 0 . 014 ,R 2 = 0 . 281 ), while the slope was nearly unchanged for the baseline interface (  X  = 0 . 006 ,R 2 = 0 . 120 ). In contrast, for the LC topic (Figure 5c), the slope of the regression line for ScentBar (  X  = 0 . 007 ,R was lower than that for the baseline interface (  X  = 0 . 012 ,R 0 . 416 ). These results indicate that when the gain estimation was accurate enough, ScentBar users obtained higher gain per unit time than the baseline users. However, the former users could not obtain gain as effective as the latter ones when ScentBar displayed inac-curate estimates. Figure 4 also demonstrates that some ScentBar Table 6: Results of search interaction analysis. The mean (with SD) values are reported. Significant differences from the base-line interface at p &lt; 0 . 05 are shown in bold face.
 users completed tasks earlier without expending much effort while others spent longer to obtain much more gain. We analyzed participants X  interaction data to investigate how Scent-Bar affects their search behavior in intrinsically diverse tasks. To investigate its effect on query selection, we calculated the number of queries issued by participants (#Queries) and the fraction of sug-gestion queries selected by them (%SuggQueries). As document-related measures, we calculated the number of, precision of, and re-call of documents browsed by participants (#Docs, Precision, and Recall, respectively) and the fraction of aspects covered by their browsed documents (AspectCoverage). We also calculated the num-ber of clicks after the last relevant document both at query and ses-sion levels (#ClicksLastRel-Q and -S) for analyzing search stop-ping behavior. While Precision, Recall, and AspectCoverage are interaction metrics related to search outcomes, the other metrics are related to search behavior.

Table 6 shows the results of our analysis based on these mea-sures. On average, more than ten queries were issued through both interfaces for all topics. While the difference in #Queries between the interfaces was not significant according to the Mann-Whitney U test, ScentBar users issued significantly more suggestion queries in the HC topics ( p = 0 . 017 ). We also calculated some query complexity measures ( e.g. , query length), but did not find any clear trends. As for the document-related measures, ScentBar users, on average, browsed more documents covering more aspects, al-though the difference was not statistically significant. Remarkably, ScentBar users obtained significantly higher recall than baseline interface users ( p = 0 . 048 ), which demonstrates the advantage of ScentBar because recall is one of the most desired properties in in-trinsically diverse tasks. Last but not least, no significant difference was observed for the number of irrelevant document clicks just be-fore search stopping. This result was contrary to our expectation that ScentBar could prevent searchers from continuing tasks when no additional gain would be obtained. We discuss this issue in more detail in Section 7.
Finally, we report the results of the post-task and exit question-naires, which are summarized in Table 7. Note that the per-interface results of the exit questionnaire for the HC and LC topics in the table were calculated from the ratings given by participants who addressed those topics with that interface.

The results for the questions Q1 and Q2 indicate that most partic-ipants faced difficulty exhaustively collecting relevant information in the LC topic. The question Q3 suggests that the low quality of search results presented by the search system would be a possible cause of this difficulty. As unreliable missed information was dis-played in this topic (Q10 and Q11), ScentBar did not guide partici-pants in effective decision-making on search stopping (Q7 and Q9). In fact, The Mann-Whitney U test revealed that the baseline inter-face was rated significantly better than ScentBar for the question Q2 ( p = 0 . 039 ), which indicates that the inaccurate estimation had harmful effects on the subjective efficiency for ScentBar, as well as the objective one (Section 6.6). The results for the questions Q7 X  Q11 indicate that the ratings for missed information improved in the HC topics. As for usability, ScentBar was much more preferred by participants than the baseline interface (Q12 X  X 14). The differ-ences between the two interfaces for these questions were shown to be significant by the Wilcoxon signed-rank test ( p &lt; 0 . 001 for Q12, p &lt; 0 . 001 for Q13, and p = 0 . 004 for Q14).
Our user study uncovered how ScentBar affected users X  search strategies and search outcomes. This section discusses the implica-tions of our experimental results and the limitations of this work.
Our results demonstrated that ScentBar had the following advan-tages when the gain estimation algorithm worked reasonably: (1) ScentBar users stopped individual searches after collecting much relevant information from the SERPs (Section 6.2); (2) they issued search queries whose search results contained much missed infor-mation (Section 6.3); (3) they obtained high gain, particularly at the late stage of their sessions (Section 6.4); and (4) they obtained high gain per unit time (Section 6.6). Our post-hoc analysis of search in-teraction data revealed that query suggestion was more frequently used by ScentBar users (Section 6.7), which implies that suggest-ing queries with missed information affected searchers X  query for-mulation strategies and thus enabled them to conduct intrinsically diverse tasks more efficiently.

The search interaction analysis also revealed that displaying missed information did not affect the number of irrelevant document clicks just before search stopping. A possible explanation for this phe-nomenon is that seeing visualized missed information made partic-ipants more cautious about making decisions on search stopping. In fact, ScentBar users spent, on average, longer completing the tasks (13.6 minutes) than the baseline interface users (11.8 minutes), al-though the difference was not statistically significant. These results suggest that additional features would be needed for supporting searchers X  decision-making on search stopping. While ScentBar suggests which queries have much missed information, it does not provide any clues about which search results users should assess to obtain additional gain. Possible solutions to this issue include visu-alizing the per-aspect relevance for each search result, like Aspec-Tiles [15], and/or re-ranking search results containing much missed information at high positions, both of which can be incorporated with ScentBar. Another approach could be notifying searchers of the effort they have expended so far as well as their missed infor-mation. This could help searchers more rationally decide whether they should conduct additional searches to obtain more gain or stop searching to avoid wasting time. Techniques for quantifying search cost ( e.g. , [2]) would be useful for implementing this idea.
Another finding from our user study is that ScentBar worsened rather than improved search performance when missed informa-tion was estimated inaccurately. More specifically, the gain per unit time obtained by ScentBar users was lower than that by the baseline interface users. This is probably because unconvincing visualization of missed information and its unexpected change pre-vented searchers from making rational decisions on query selection and search stopping. Our results suggest that the accuracy of gain estimation is critical for ScentBar to produce intended effects. This work has several limitations that we should acknowledge. First, while we formalized gain as an intent-aware metric, as with other evaluation metrics like ERR-IA [5], this approach has a draw-back in that the content overlap among documents is not taken into account when the gain is evaluated. Thus, this metric has the poten-tial to give a high score to documents with high overlap. While we tried to prevent participants from collecting overlapped documents by presenting diversified search results to them, this is an ad-hoc so-lution, not an optimal one. Ideally speaking, gain should be mod-eled on the basis of more fine-grained units, such as nuggets [7]. Thus, for more accurate evaluation, our gain formalization and es-timation algorithm would require further improvement by, for ex-ample, modeling and mining aspects hierarchically [14].

Second, we used only four search topics to evaluate the effective-ness of ScentBar in intrinsically diverse tasks. Limiting the number of topics was necessary given that the interfaces were designed to be the within-subject factor in our user study ( i.e. , every participant used both interfaces) and that we could not use a predefined length of task execution time as our research questions included the ef-fect of ScentBar on session stopping. To validate the generality of our claim about the effectiveness of ScentBar, we would need to conduct additional experiments with more search topics and more diverse participants in the future.

Third, we selected a simple interface as the baseline in our user study while various interfaces showing information scent have been proposed. As outlined in Section 2.1, the query preview interface proposed by Qvarfordt et al . [24] provides a similar functionality as ours: their interface visualizes the number of (un)clicked search results for the current query. To estimate its effectiveness in intrin-sically diverse tasks, we measured correlations between the number of unclicked search results and the oracle missed information scent in a manner similar to the gain accuracy estimation (Section 6.1). The correlation coefficient was 0 . 334 even in the best topic. Given the fact that ScentBar worsened the search performance for the LC topic, this suggests that such simple visualization could be less ef-fective for intrinsically diverse tasks, while direct comparison in a user study setting would be needed to gain more insights.
In this paper, we proposed ScentBar, a query suggestion inter-face visualizing the amount of missed information for individual queries. We defined the amount of missed information for a query as the additional gain that can be obtained from unclicked search results of the query. Results of our user study involving 24 partic-ipants showed the following advantages of our interface for search topics where gain was estimated reasonably accurately: (1) Scent-Bar users stopped SERP examination after collecting more relevant information; (2) they issued queries whose search results contained more missed information; (3) they obtained higher gain, particu-larly at the late stage of their sessions; and (4) they obtained higher gain per unit time. These results suggest that unless inaccurate es-timates of missed information were visualized, ScentBar enabled users to utilize effective query formulation strategies, while it had little noticeable effect on their search stopping behavior.
Future directions of this work include exploring ways for utiliz-ing missed information. While ScentBar visualizes what is left for browsing ( i.e. , the visualization decreases as searchers obtain gain), showing how much searchers have browsed might have a different effect on their search behavior. Providing the visualization in dif-ferent places and/or at different timing would also be worth explor-ing. Missed information could also be incorporated with query sug-gestion algorithms to rank suggestion queries from which searchers would be able to obtain much additional gain at high positions.
This work was supported in part by JSPS Grants-in-Aid for Sci-entific Research (Nos. 13J06404, 15H01718, and 16K16156).
