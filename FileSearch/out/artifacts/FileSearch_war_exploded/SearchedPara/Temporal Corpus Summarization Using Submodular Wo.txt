 In many areas of life, we now have almost complete electronic archives reaching back for well over two decades. This includes, for example, the body of research papers in computer science, all news articles written in the US, and most people X  X  personal email. However, we have only rather limited methods for analyzing and understanding these collections. While keyword-based retrieval systems allow efficient access to individual documents in archives, we still lack methods for understanding a corpus as a whole. In this paper, we explore methods that provide a temporal summary of such corpora in terms of landmark documents, authors, and topics. In particular, we explicitly model the temporal nature of influence between documents and re-interpret summarization as a coverage problem over words anchored in time. The resulting models pro-vide monotone sub-modular objectives for computing informative and non-redundant summaries over time, which can be efficiently optimized with greedy algorithms. Our empirical study shows the effectiveness of our approach over several baselines.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Design, Theory summarization, temporal, submodular
From news and blogs to twitter feeds, and from research papers to patents, we are accumulating unprecedented amounts of text in digital form. Advances in storage technology have allowed us to maintain complete records of these text streams, and information retrieval research has developed excellent tools for accessing indi-vidual documents in the resulting collections. However, our ability to analyze and interpret archives on a macroscopic level is still lim-ited. Macroscopic questions one may ask about a collection range from the creation of a timeline of influential documents or authors, to the automatic summarization of the main chains of discussion.
To answer such macroscopic questions about a corpus of text documents, we draw upon methods from document summariza-tion (see [14]). Instead of summarizing a single (or small num-ber of) individual documents using extracted sentences, we aim to summarize a collection using extracted documents, authors, or key-words. This shift implies substantial differences in what constitutes a meaningful summary. In particular, time is more important for the creation of corpus summaries than it is for conventional sum-maries, and we argue that corpus summaries should reflect the in-fluence that a document or author had on the future development of the collection. Therefore, our summaries take the form of time-lines, where components of a summary are defined with respect to intervals or points of time.

More specifically, we formulate several variants of the corpus summarization problem. First, we seek to identify k documents that had the largest influence on the content of the corpus. Second, for each point in time, we seek to identify those documents that were most influential for that time. Third, we similarly identify the most influential authors for each time-point. And fourth, we identify key phrases at each time-point that were influential and represent a coherent segment of the corpus.

All four corpus-summarization problems will be formulated as coverage problems, where we approximate coverage of abstract concepts through coverage of words in time. For conventional sum-marization and diversified retrieval, coverage approaches [21, 28, 16, 23, 19] and, more generally, submodular summarization meth-ods [10] represent the state of the art. In particular, they provide an elegant model of the relevance/redundancy trade-off inherent in all summarization problems. The key technical challenge for the prob-lem of corpus summarization is the ability to model time-points and time-intervals effectively, without sacrificing the computational ap-proximation guarantees that the greedy algorithm provides for sub-modular function maximization [13, 8]. Our approach models the fact that different ideas have varied novelty, and that the influence of an idea changes over time. An important feature of our approach is that it does not rely on observed or inferred link structure between documents, but requires only the time-stamped document text. This makes our approach applicable to a wide range of corpora for which citation information is not available or not reliable. Furthermore, it allows us to use citation information as  X  X round truth X  for quan-titative evaluation. On three scientific corpora, we perform such evaluations, compare to several baselines and find that our methods provide qualitatively interesting results.
Our approach in this paper is based on the idea of extractive sum-marization; in this approach, a summary is created by selecting smaller units (i.e. sentences) [3, 23, 10]. Corpus summarization has similarities to extractive document summarization: instead of a document to summarize, we have a corpus and instead of selecting sentences, we select entire documents. Extractive summaries can be obtained by maximizing the coverage of words over the corpus. This idea has been used both in document summarization [23] and information retrieval [28]. While existing approaches focused on covering words as a proxy for concepts, we extend the notion to identify documents from a corpus that had substantial influence on the future development of the corpus. This gives us a more versatile framework than, say, selecting sentences based on time-dependent event weights [25].

There are several approaches that deal with the corpus as a whole in an attempt to improve its accessibility. Recommender systems strive to suggest what to read next based on past behavior and per-sonal preferences [7] or on a query set consisting of example papers [6]. A related approach uses collaborative filtering and/or content filtering [24]. The goal of recommender systems, broadly, is to suggest documents based on a query; the goal of our work is to un-derstand how a corpus evolved over time and which authors, papers and key-words demonstrated their influence during the evolution.
There has been previous work on modelling and visualizing text corpora. On the macro level, we can describe a corpus in terms of topics [2]. There has also been work [4] which show trends in topics, indicating the main turning points. A different approach considers intra-corpus relations and describes influence between documents [18]. The disadvantage of these approaches is that the units being summarized (e.g. documents) and being visualized (e.g. turning points) are distinct objects. Bridging this disconnect is an important aim for summarization systems since providing an ex-plicit guide in the visualization (e.g. by providing the influential documents in the corpus) helps a user become familiar with the novel content in a corpus.
 Temporal text mining [20] is another popular area of research. There has been work [1] which views a news topic as a sequence of events and selects those events that are relevant and novel to form the summary. Further, timelines of events can be created [27, 22] which show the major developments in a news topic. There is also work on event threading [12] where events are not viewed as a flat hierarchy of topics; these approaches model the dependencies be-tween events. In addition, temporal features can also be used when doing document summarization [9] to improve the performance. Our work implicitly models sequences of events over time, but al-ways presents explicit examples that highlight influential contribu-tions. The main insight of this work is that explicitly modelling the temporal context in which words appear in documents provides a simple and very effective approximation to the flow of ideas in a corpus.

In this paper, we explore several variants of the corpus summa-rization problem, providing a temporal summary of the corpus in terms of landmark papers, authors and key-phrases. All approaches are formulated as maximum coverage problems, which have been found to provide elegant and effective methods for conventional summarization and diversified retrieval problems [10, 28, 21]. We start by reviewing the coverage-based summarization idea in the re-mainder of this section, and then extend it to corpus summarization in Section 4.
Coverage-based summarization methods make a direct analogy between a summary covering the information content in the ob-ject to be summarized, and maximum coverage problems as de-fined in theoretical computer science [8]. The key assumption of coverage-based summarization methods is that coverage of words can be used as a proxy for the coverage of information content. By achieving a good coverage over words, the word coverage ap-proach aims to select a summary which covers different topics. In this way, coverage-based summarization methods elegantly avoid redundancy and promote diversity.

While document summarization involves selecting a diverse set of sentences from it, the idea can be naturally extended to corpus summarization by selecting a diverse set of documents that maxi-mizes coverage. In this approach, every word in a document has a weight associated with it, indicating how important it is to cover this word in the summary. These document weights are either de-termined through a heuristic [10] or learned [19]. Documents are selected so that the total weight of the covered words is maximized; this is illustrated in Figure 1. In this example we want to select at most two documents out of three and each covered word has unit weight. We see that by selecting d 1 and d 2 we achieve the best score of the summary, since we cover the maximum number of words.

Formally, let U = { d 1 ,d 2 ,... } be the set of all documents in the corpus, where each document is represented as a bag of words. Algorithm 1 for greedy submodular function maximization. S  X   X  X  X 
A  X  U = { d 1 ,... } while A 6 =  X  and | S  X  | &lt; k do end while The word coverage objective function, for any S  X  U , is defined as follows: where,  X  ( d,w ) represents the weight of a word w in the document d . One common choice of  X  is the TFIDF score [17]. Moreover  X  ( w ) is the weight for the word w depending on our belief of the word X  X  importance.
With the objective function defining the score of a summariza-tion S , corpus summaries are constructed by finding the set S with the highest score F ( S ) . To obtain a summary we have to solve the following optimization problem: An important property that enables the fast and accurate solution of this optimization problem lies in the structure of F ( S ) . It is well known that the coverage objective F ( S ) is monotone (i.e. | S | S | =  X  F ( S 0 )  X  F ( S ) ) and submodular [8].

D EFINITION 1. Given a set U , a function f : 2 U  X  R submodular iff for all u  X  U and all sets S and T such that S  X  T  X  U ,we have, Submodular functions have a property that says that adding u to a subset s of t increases f at least as much as adding it to t . While maximizing monotone sub-modular functions is NP-hard [11], it is known that the greedy Algorithm 1 achieves a 1  X  1 /e approxima-tion to the optimum solution for any linear budget constraint [10, 8]. Further, this algorithm provides a 1  X  1 /e approximation for any monotone submodular scoring function. The algorithm starts with an empty summary. In each step, a document is added to the sum-mary that results in the maximum relative increase of the objective. The algorithm terminates when the budget k is reached.
While submodular summarization approaches have been very successful for conventional summarization problems, we argue that corpus summarization should not only optimize coverage of infor-mation content, but also reflect which documents and authors were important in the development of the corpus. In particular, we aim to include influence between documents into the summarization ob-jective.

To illustrate the difference between a conventional summariza-tion problem and the type of corpus summarization we envision, consider a corpus consisting of research papers covering two decades of a particular field. In such a scenario, conventional word-based coverage approaches would pick several (non-redundant) survey papers, or papers that otherwise touch on a lot of different areas, since their union will tend to cover the largest subset of words in the corpus. However, while these survey papers are indeed a good summary of the content, this selection will not provide any information about how the corpus developed over time, what pa-pers opened new areas of activity, and which authors influenced the direction of the field. In the subsequent sub-sections, we show how the conventional coverage-based approach can be extended to provide summaries that not only optimize information content, but also reflect influence and importance of individual documents and authors.

To achieve this goal, the remainder of this section shows how to (a) incorporate time into the summarization problems, (b) for-mulate the summarization objective in terms of influence, and (c) show how corpora can be summarized not only through landmark documents, but also through influential authors and key-phrases. Figure 2: Illustrating the coverage function for revealing influ-ential documents.
We now explore how the word-based coverage objective can be extended to summarize a corpus through a non-redundant set of in-fluential documents. A pictorial illustration is shown in Figure 2, indicating how influential documents introduce ideas that increas-ingly cover the content of documents observed in later years. To get to an operational formalization of influence in the coverage model, we start with the following properties: Spread: An influential document contains ideas that spread to other Novelty: A document should only be credited for generating in-To capture novelty in the word-based coverage approach, we rede-fine  X  ( d,w ) in the coverage objective (1). Intuitively, we do not want to give document d credit for an idea  X  as represented by its word distribution  X  if there already exist older documents d t ( d 0 ) &lt; t ( d ) that already cover this idea. t ( d ) denotes the year of publication of the document d . More formally, let N ( d ) denote the k -nearest neighbors (for example, based on cosine similarity) of the document d among all the documents published before it, then we capture the novel contribution of a document as
In order to capture spread in the coverage objective, we enlarge the set of objects that need to be covered to word-time pairs w for all words w and years y . This allows the coverage objective to make a distinction between covering the word w in a year y and covering the same word in year y 0 . Formally, we generalize  X  ( w ) in (1) and make it dependent on time, where each  X  ( w,y ) separately defines how important word w is for the given time y . We say that the importance of a word w in year y is determined by the sum of the TFIDF scores of the documents in year y : Other weighting schemes can work here too (e.g. per-year inverse document frequencies) because our model is oblivious to the pre-cise choice of  X  (as long as it well represents the spread). Note that this allows us to model that some documents cover a word in cer-tain years, but not in others. In particular, we say that a document d only covers a word in those years that are later than its publica-tion date t ( d ) . This allows us to formulate the objective for finding influential papers as follows: The above objective multiplies the novel aspect of a word in a pa-per with how important a word is in the future years. Intuitively, the score is large when the set of selected documents S contains documents with high novelty scores as well as a high influence in the future. We therefore maximize the above objective using the greedy Algorithm 1.

Note that this approach will not tend to select survey papers un-like the word coverage approach for two reasons. First (and most importantly), a survey paper will have a low novelty score since it is mostly based on previous work. Second, usually survey papers are written after a field is well developed, hence it does not cover all those documents that appeared before it in (5).

The key feature that differentiates our model from related coverage-based approaches is the insight that modelling the temporal context in which words appear (and not just the within-document context) can provide a strong signal for summarization tasks. We achieve this by not just using words as proxy for ideas in a coverage ob-jective, but anchor words at specific time-points to gauge the influ-ence. Additionally, modelling novelty helps us correctly attribute impact and avoids the pitfalls of citation-based impact measures.
The previous section showed how the coverage objective can be extended to focus on influential papers, producing summaries that are organized by the publication date of the influential documents. However, dual to such summaries, we may also ask the following question: for each year y , what are the documents that most influ-enced the content of this year? This is illustrated in Figure 3.
In the following subsection, we formulate a coverage objective that identifies the k documents that had the most influence in a year Figure 3: Illustrating the influence of documents in a particular year. (for every year). Intuitively, a document d influences a year y , if it was published before year y , i.e. t ( d ) &lt; y , and the novel ideas from d have substantial coverage in year y . Instead of selecting documents independent of year as in the previous section, we now allow our method to select an influential document to cover a partic-ular year y . This means that our optimization problem now selects from a universe of document-year pairs U y = { ( d i ,y i d is any document from the corpus, and y i is any year such that y &gt; t ( d i ) . This leads to the following objective which we seek to maximize.
 Similar to (5), the above objective multiplies the novelty score of word w in a document d with the importance of the word for a year y . However, (6) allows picking a different set of documents for each year y i .

It is easy to see that F ( S ) in (6) decomposes into a set of in-dependent optimization problems  X  one for each year. We may therefore solve the following subproblem separately for each year and concatenate the solution for each year to obtain the solution of the original problem. Formally, Each of the above objectives is monotone submodular and can be solved using the greedy Algorithm 1.
Analogous to selecting documents that had a large influence on a given year, we can also ask which authors were most influen-tial. It is easy to extend the optimization problem from the previ-ous section so that it selects influential authors. Denote with d ( a ) the documents in the corpus that were authored by author a . The universe of items to select from now consists of author-year pairs U y = { ( a i ,y i ) ,... } . Selecting an author a i for year y all documents the author wrote before year y i get selected. This leads to the following objective, Figure 4: Illustrating the difference in word distributions over time between a bogus term and a genuine keyword. which again can be broken into independent optimization problems for each year.
Summaries in terms of documents and authors still require the user to read through some documents from the collection. We now explore whether timelines of influence can be summarized through key-phrases. In particular, we aim to identify the points in time when new and influential ideas  X  as represented by a key-phrase  X  entered the collection.

While we already have operational definitions of novelty and in-fluence, we still need to define what makes a key-phrase a good rep-resentative of an idea. We conjecture that a key-phrase that repre-sents an idea well will be accompanied by stable word distribution over the years. For instance, documents that mention the phrase  X  X ITS algorithm X  will probably also mention several words related to that idea, whereas documents mentioning  X  X elated work X  need not have such a coherent set of overlapping words. The keyphrase T1 in Figure 4 is an example of a good key-phrase, since documents that contain T1 also share many other words. On the other hand, a key-phrase that is not a good representative of an idea may occur in documents talking about a variety of different ideas. T2 in Figure 4 is an example of a bad key-phrase.

We formalize this definition of key-phrases as follows. Define the universe of elements to choose from, U = { ( p,y ) ,... } , where p is a candidate key-phrase and y denotes the year when the key-phrase became influential. Let the subset of the corpus that men-tions a candidate key-phrase p be D p . Intuitively, we wish to as-sociate with ( p,y ) a representative document d  X   X  D p which was published in year y and which was the most influential document in the subsequent development of D p . According to our conjec-ture, for a bogus keyphrase, the associated d  X  will achieve very poor coverage of the word content observed in documents of D that were published after y , while influential keyphrases will have a document that covers the associated stable word distribution very well. Following (4), we model the importance of covering a word Algorithm 2 for greedy submodular function maximization with budget constraint.
 S  X   X  X  X 
A  X  U = { p 1 ,... } z  X   X  arg max while A 6 =  X  and C ( S  X  ) &lt; k do end while if F ( S  X  ) &lt; F ( z  X  ) then else end if in D p as  X  ( w,y ) p . More precisely,
With this d  X  for each element in U , we can formulate the objec-tive
F ( S ) = X Again, the objective decomposes into independent sub-problems for each year, and we can rewrite it for each year y as, Unlike in the previous optimization problems, we now associate a cost C ( p,y ) = |{ d  X  D p : t ( d ) = y }| with each element of S . This is done to encourage associating a key-phrase with the point in time when it begins to gain popularity. The number of documents published in a year mentioning a key-phrase is used as a proxy for the maturity of an idea. The optimization problem is,
This formulation is an instance of the budgeted coverage prob-lem with a linear cost constraint, and the greedy Algorithm 2 is (1  X  1 /
For simplicity, so far we have decomposed the timeline-generating optimization problems into independent sub-problems, one for each year. This was possible since we imposed a cardinality constraint for each year. However, we can also define a global optimization problem across all years that constrains the maximum amount of content covered in each year. This results in a summary that will choose more documents from years that actually contain more in-teresting information. Formally, We change the global objective from (6) into where the parameter  X  determines how much relative word content per year we want to cover, F y ( S y ) is as defined in (7) and U whole universe for year y . This global F ( S ) is also monotone sub-modular and can be solved using the greedy Algorithm 1. Whereas earlier we had a cardinality constraint parameter k to set for each year, we have to set one global parameter  X  (setting it higher results in more detailed summary) and one global k (higher values result in longer summary) now.
In this section, we empirically evaluate our proposed models on publicly available datasets. We first describe the datasets and then present the results of our experiments along with evaluation metrics. The experimental results show the advantage of our ap-proaches compared to other baselines in addition to good qualita-tive results.
We used three corpora containing research publications for eval-uating our proposed approaches. The Neural Information Process-ing Systems (NIPS) corpus contains 1955 published papers over a span of 14 years. Similarly, the Association for Computational Linguistics (ACL) corpus [15] contains 18041 papers published in a number of conferences over a span of 39 years. We also collected the set of papers published in the proceedings of SIGIR and CIKM conferences over the years available from CiteSeer. This corpus contains 2097 papers published over a span of 18 years (the last year being 2007). In all cases, we associate each document (paper) with its publication year and limit ourselves to 12 consecutive years ending at the year before last (the citation graph is also constrained only to those years). Since we compute novelty of a document based on the nearest neighbors from the past, we also used the year immediately before our subset for this purpose in the subsequent experiments. The last year is skipped because it does not have any citations from the future. We did not use the early years in ACL and SIGIR-CIKM corpus because they contain significantly less papers and citations compared to other years.

All datasets include citation graphs which we use for evaluation purposes, however our method does not require citation informa-tion and could thus be easily applied to other document collections. Note that the citation graphs are sparse as they do not include refer-ences to and from the papers outside the corpus. The NIPS collec-tion has 1512, the ACL collection has 82892 and the SIGIR-CIKM collection has 1750 citations between papers inside the corpus. In addition to regular research papers, NIPS corpus also contains meta documents representing volume indices. We removed such docu-ments manually since they are very easy to spot. We also pruned the words and retained only those words which occurred at least twice in a document and in at least three documents in the cor-pus. This simple heuristic removed a lot of noise introduced by the OCR system, and allowed us to meaningfully interpret influ-ence. We represent every document by the TFIDF score (computed on the whole corpus) of the words contained in it after pruning, and normalize the resulting document vector to unit length. To compute the nearest neighbor in the past (to determine novelty), we use the cosine similarity between the document vectors. We do not require the exact nearest neighbors, and in case of a very large corpus, ap-proximate methods to find similar documents can be employed to sidestep the quadratic time complexity of this step.
The word coverage approach (from Section 3) obtains a sum-mary by maximizing the word coverage. In Section 4.1 we argued that influential documents have novel ideas which subsequently spread through the corpus. In this experiment, we select the most influential papers based on our objective (5) which captures nov-elty of a document and its sphere of influence, and compare it with those selected by the simple word coverage objective (1). Since there is no standard way of measuring the influence of a paper, we resort to the citation structure available in the corpora. To quantita-tively evaluate whether the selected papers were indeed influential, we compute the total citation count for the set of papers (i.e., the number of times these papers were cited by documents in the cor-pus) selected by any algorithm. There have been several criticisms of citation-based impact measures and some effort [5, 26] addresses them. However, for a comparative study, we believe citation counts are the least biased choice in this setup.

To provide a point of comparison for the coverage based ap-proaches, we considered several baselines. The simplest baseline was to randomly select documents until the budget was reached ( random ). Another baseline that we considered was to select the most prolific authors in the conference (in terms of number of ac-cepted papers) and then select the required number of papers from the union of their papers ( authors ). More concretely, we first rank the authors according to the number of papers in the collection they authored. Next, we pick the 10 most prolific authors. Finally, we sample uniformly at random with replacement from the set of au-thored papers the same number of papers for each author. We also computed the upper bound on the total citation count possible for a selection by selecting papers with the highest observed citation inlinks in the corpus ( bound ).
 We selected 100 documents from the NIPS, SIGIR-CIKM and ACL corpus that maximize the respective objective function. Our experiments presented here are using unigrams as the elements in the universe. However, our methods can use other types of ele-ments (e.g. bigrams formed from consecutive words, for which we observed a similar trend in the results). The results of this experi-ments are provided in Table 1. We also provide standard error of these results; they were estimated from the citation count on 10 re-runs of 70% subsampling of the corpus.

From the table, it is clear that our approach ( infl. papers ) gets significantly higher citations compared to the word-coverage ap-proach ( word cover ) in all corpora. Moreover, finding influential papers is computationally cheap (with running time linear in the size of the corpus multiplied by the number of selected papers if we do not count the preprocessing step of computing nearest neigh-bors for novelty score) and, for example, takes a few seconds for the NIPS corpus on a standard desktop computer. Table 1: Total citations obtained by the papers selected for in-fluential documents and baselines using unigrams. All results use 1-NN for novelty score. The values in parentheses indicate standard error. Figure 5: Comparison of results of word coverage approach when using different values of k (number of nearest neighbors for computing novelty score) on NIPS and ACL corpus for un-igrams and bigrams. The horizontal axis represents the value of k and the vertical axis relative performance (number of cita-tions) when compared to not using the novelty score (i.e. k = 0 ). Our approach uses the novelty score (introduced in Section 4.1, Eq. (3)) to credit a document for an idea only if it was the first one proposing it. Novelty is captured by considering k nearest neigh-bors in the past and subtracting their word weights (clipping at 0 to prevent negative values) from the current document. In this subsec-tion we explore the impact of choosing different values of parame-ter k .

Results for the word coverage approach on NIPS and ACL (as examples of two slightly different behaviors) using unigrams or bigrams as elements in the universe are presented in Figure 5 . We would expect word coverage to improve when using novelty scores because the coverage most likely does not choose the initial (highly cited) paper but some later one with better coverage (e.g. a derivative paper that also incorporates some other ideas). This intu-ition is confirmed by our results showing that using more neighbors improves the score as we incorporate more and more information about novelty. After a point we can see that performance starts dropping again because we are subtracting too much content.
Almost all coverage approaches benefit from using 1-NN, but increasing k only improves performance for word coverage ap-proach. We believe that using 1-NN helps because it mimics a language background model and penalizes frequent non-content words, while increasing k above that does not bring significant ben-efits because we already model temporal behavior with the choice of our model.
In this sub-section, we evaluate our approach to create timelines of document influence and compare it against several other base-lines. For each NIPS, ACL and SIGIR-CIKM corpus, we select 10 documents per year.

Again, we considered the random baseline ( random ) and the 10 most prolific authors ( authors ). The authors baseline is constructed as follows: first create a union of all papers by 10 authors with the highest number of accepted papers, and for each year select 10 doc-uments randomly from this union (with replacement) published on or before this year. We computed the upper bound on the citations ( bound ) by selecting papers with highest citation count in a given year (i.e. we count only citations occurring in that particular year)  X  we call this the current citations .

We evaluate the selections based on the citation network as be-fore. In the previous section, our evaluation was based on the total number of citations a paper obtained. However, in this section, it is based on the current citations. To select a timeline of influential pa-pers, we select papers that have maximum influence in a particular year (for each year). So, to quantitatively evaluate the selections, if a paper is selected as influential in the year y , we count the num-ber of citations it gets in the year y (i.e. only citations from papers citing it in this year count) and then sum them across all years.
Results for this experiment are summarized in Table 2. We can see that random baseline and authors have inferior performance compared to our approach ( timeline ). Note that the gap between our approach ( timeline ) and the bound is larger than in the influen-tial papers experiment. We believe this is due to timeline being an inherently harder problem  X  not only do we have to find influential papers but we also have to specify exactly when were they influ-ential (as the evaluation metric counts only citations from papers citing in that selected year). Our approach to constructing time-lines is fast to compute (time complexity is linear in the number of years, papers selected and corpus size) and, e.g., takes less than 3 seconds on the NIPS corpus on a standard desktop computer. Table 2: Current citations (i.e. number of citations from papers citing in that particular year) obtained by the papers selected for timeline and baselines using unigrams as elements of the universe. All results are for 10 re-runs of 70% subsampling and using 1-NN for novelty score. The values in parentheses indicate standard error.
In this section, we conducted experiments based on the objective proposed in Section 4.3 to select timelines of Author Influence. In-stead of selecting papers we now consider meta-documents describ-ing authors and construct a timeline showing which authors were important and when. In addition to this, for each selected author we constrain the corpus to that author X  X  papers and find the most influential ones (including their timeframe of prominence).
We present a visualization of the results for NIPS corpus in Fig-ure 6. By just looking at the plot it is easy to gain some insight into the development and content of the corpus. Features such as some authors having an influence throughout the whole corpus (e.g. Jor-dan, Sejnowski) are easy to spot. We can also see that some authors have had more influence only in specific timeframes (e.g. Hinton in the early years and Smola in the later years). In addition, looking at the selection of an author X  X  most influential papers gives us insight into what topics they usually write about.

Although there are quantitative metrics which might be used to judge the output of the system to pick influential authors (e.g. H-index ), note that we require these metrics to be computed for the timeframe of the collection only. Given the very sparse citation graph, using only observed intra-collection citations is expected to be a noisy signal. Qualitative results clearly indicate good perfor-mance of our approach and we feel that any simple adaptation of existing measures would not give a significantly better insight.
With the formulation described in 4.4, we ran experiments to find prominent key-phrases in each of the scientific corpora. For the set of candidate key-phrases, we considered trigrams and bigrams that occurred in at least 0 . 2% of the documents in the respective corpus. Moreover, if a trigram is admitted to the set of candidate key-phrases, the constituent bigrams are not considered as candi-dates. This is a simple heuristic that recognizes that the lexical unit for phrases is usually a trigram or bigram and greedily prefers tri-grams. More sophisticated ways to determine the set of candidates are possible, say independently running a Part-Of-Speech tagger and considering only noun phrases. The number of candidate key-phrases using this heuristic rule is 3035 for the NIPS corpus, 8139 for the SIGIR-CIKM corpus and 4687 for the ACL corpus. The fewer number of candidates in the ACL corpus is explained by the fact that requiring the document frequency of bigrams or trigrams to be 0 . 2% of a much larger corpus is a more restrictive filter.
We lack ground truth key-phrases to evaluate the output of our system; also, it is hard to quantitatively judge the quality of an in-fluential key-phrase X  X  associated timestamp. We therefore estimate the average citations in the collection of documents that mentions a key-phrase as a measure of its quality. Concretely, for a key-phrase p and the subset of the corpus D p that mentions it, where Cite ( d  X  d 0 ) indicates that document d is cited by d optimized the objective in Section 4.4 for each year with a bud-get of 3, and collected the set of all unique key-phrases. The re-ported scores for this approach (presented as TimeCov in Table 4) are the sum of Score ( p ) for each unique collected key-phrase p . As a point of comparison, we also report the number of unique key-phrases collected as Count . A simple baseline for this experi-ment would be to pick the most frequent key-phrases occurring in the corpus in each year: this approach is hindered by the frequent occurrence of redundant phrases. For instance,  X  X eural network X  in the NIPS corpus, X  X atural language X  in the ACL corpus and  X  X n-formation retrieval X  in the SIGIR-CIKM corpus appear in such an overwhelming majority of documents over all the years as to drown out other informative candidates. This baseline is reported as Most-Freq in our results. Another approach we compare with is to pick candidates that optimize the Score directly in each year; this can be interpreted as an upper bound for this evaluation metric. We also provide the collected candidates from the coverage approach and one that optimizes the Score ( t ) directly for the SIGIR-CIKM corpus in Table 3. Several informative phrases that come from di-verse areas of research covered in SIGIR and CIKM get selected in the coverage approach. Furthermore, a visualization of the key-phrases over years for the NIPS corpus is shown in Fig. 7. Area of the shaded region corresponding to a term represents the fraction of documents observed in the corpus in that year that mention that particular term.
 Method Count Score Count Score Count Score MostFreq 13 2.13 20 34.33 20 7.41 TimeCov 17 4.68 77 116.92 29 12.28 Bound 13 6.09 96 124.10 29 18.82
This paper presented a submodular framework for temporal cor-pus summarization. We extended the notion of word coverage and asserted that summaries cover important concepts by covering as-sociated words over a time interval. A timeline of influential doc-uments, or authors, or coherent key phrases was constructed using training set 5.0 pearson correlation 5.0 new system 2.0 index construction 2.0 budget of 3 and by optimizing the citation score. our approach, providing concrete suggestions for further and more detailed exploration of the corpus contents. Our approach lever-aged both the novelty of a document as well as its influence in the development of the corpus and relied only on word features; fluence across time. Therefore it is applicable to any textual col-lection which provides timestamped documents. Our optimization objectives used monotone submodular functions to trade-off rele-vance and redundancy elegantly, and were solved using an efficient greedy algorithm with a constant factor approximation guarantee. We empirically demonstrated that our approach performs better than several baselines using citation based performance measures and provided qualitative timelines for a few scientific corpora. This research was funded in part through NSF Awards IIS-0812091, IIS-0905467, and IIS-1217686. [1] J. Allan, R. Gupta, and V. Khandelwal. Temporal summaries [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [3] J. Carbonell and J. Goldstein. The use of mmr, [4] C. C. Chen and M. C. Chen. Tscan: a novel method for topic [5] P. Chen, H. Xie, S. Maslov, and S. Redner. Finding scientific [6] K. El-Arini and C. Guestrin. Beyond keyword search: [7] K. El-Arini, G. Veda, D. Shahaf, and C. Guestrin. Turning [8] S. Khuller, A. Moss, and J. S. Naor. The budgeted maximum [9] J.-M. Lim, I.-S. Kang, J.-H. Bae, and J.-H. Lee. Sentence [10] H. Lin and J. Bilmes. Multi-document summarization via [11] R. McDonald. A study of global inference algorithms. In [12] R. Nallapati, A. Feng, F. Peng, and J. Allan. Event threading [13] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An [14] A. Nenkova and K. McKeown. Automatic summarization. [15] D. R. Radev, P. Muthukrishnan, and V. Qazvinian. The ACL [16] K. Raman, T. Joachims, and P. Shivaswamy. Structured [17] G. Salton and C. Buckley. Term weighting approaches in [18] B. Shaparenko and T. Joachims. Information genealogy: [19] R. Sipos, P. Shivaswamy, and T. Joachims. Large-margin [20] I. Suba X i  X  c and B. Berendt. From bursty patterns to bursty [21] A. Swaminthan, C. Metthew, and D. Kirovski. Essential [22] R. Swan and J. Allan. Automatic generation of overview [23] H. Takamura and M. Okumura. Text summarization model [24] R. Torres, S. M. McNee, M. Abel, J. A. Konstan, and [25] M. Wu, W. Li, Q. Lu, and K.-F. Wong. Event-based [26] E. Yan and Y. Ding. Weighted citation: An indicator of an [27] R. Yan, X. Wan, J. Otterbacher, L. Kong, X. Li, and [28] Y. Yue and T. Joachims. Predicting diverse subsets using
