 Over the course of the past decade, researchers have developed various types of semi-supervised learning methods. Co-training [1] is a representative paradigm of semi-supervised learning methods that are based on the multiple representa-tions from difference views. Co-training was inspired by the observation discov-ered in the Web pages classification [1], in which a Web page has two different representations (views): the words occurring on the page itself; and the words contained in the anchor text of hyperlinks pointing to the page. The initial form of co-training is to train two classifiers separately on two sufficient and redun-dant views of data, and let these two cla ssifiers label some unlabeled instances for each other. Like other semi-supervise d learning methods, co-training requires its own assumptions to guarantee its su ccess. Blum and Mitchell [1] proved that co-training can be successful if the two sufficient and redundant views are condi-tionally independent given the class label. Many researchers have supported the observation that co-training is sensitive to this theoretical assumption [18] [2]. However, the sufficient and redundant views are rarely found in most real-world application scenarios.
 In order to tease out the effect of view-splitting from the effect of labeling, Nigam and Ghani [2] proposed a hybrid algorithm of expectation-maximization (EM) and co-training, called co-EM . Like co-training, co-EM tries to divide the instance space into two conditional independent views, and to train two EM classifiers based on these two views, respectively. But unlike co-training, co-EM uses all the unlabeled data every time, instead of incrementally selecting some confident predictions to update the training set. Nigam and Ghani [2] also provided a method for ideally splitting the view of instance space based on the conditional mutual information criteria between two subsets of attributes. However, this method is NP-hard and difficult to apply in practice.

Since both co-training and co-EM suffer from the conditional independence assumption, variants of co-training have been developed based on only a single view (without splitting the attribute set). For example, Goldman and Zhou [3] used two different learning algorithms in the paradigm of co-training without splitting the attribute set. Steedman et a l. [4] developed a similar co-training algorithm that applies two diverse statistical parsers. Wang and Zhou [5] proved that if the two classifiers are largely diverse, co-training style algorithms are able to succeed. Because these variants substitu te multiple views by multiple classi-fiers, these algorithms are referred to as multiple-learner algorithms. Since the multiple-learner algorithms are trained on the same attribute set, it is impor-tant to keep the two classifiers different during the process in order to prevent early convergence. Maintaining separated training sets is one approach for this purpose. However, assigning labeled inst ances to two different initial training sets will cause the ineffective utilization of labeled data sets in a semi-supervised learning scenario.

Considering the framework structures of co-training, co-EM, and multiple-learner algorithms, we can see that each structure is symmetrical. The co-training algorithm splits the instance space into two symmetrical views, trains two clas-sifiers symmetrically, and lets two classifiers teach each other in a symmetrical way. Similarly, the co-EM algorithm sets up two symmetrical EM classifiers based on their related views. And likewise, th e multiple-learner algorithm also has a symmetrical structure, where two classi fiers are trained in parallel and combined together to score the unlabeled instances . Therefore, we define these algorithms as the symmetrical-style co-training algorithms .
In this paper, we propose an unsymmetrical co-training algorithm -anovel, semi-supervised, unsymmetrical-style al gorithm. The unsymmetrical co-training algorithm combines the advantages of other co-training style algorithms and overcomes their disadvantages. The unsymmetrical co-training algorithm com-bines two unsymmetrical classifiers, namely, an EM classifier and a self-training classifier. In the algorithm, the self-training classifier takes the responsibility for a section of unlabeled instances in a data pool; and the EM classifier maintains a global view over the entire instance set. Without the two-view splitting, the unsymmetrical co-training algorithm uses the full set of attributes so that it can avoid the intractable constraint of the conditional independence assump-tion. Although both classifiers are initially trained by labeled instances, the EM classifier and the self-training classifier have different training sets once they en-ter the iteration procedure. The unsymmetrical co-training algorithm does not need to hold different initial training sets and is therefore able to utilize the labeled instances more effectively. Furt hermore, according to the study of Wang and Zhou [5], the multiple-learner algorithm could not further improve the per-formance after a number of learning rounds because the difference between the two learners become smaller and smaller. Since the unsymmetrical co-training algorithm uses two unsymmetrical classi fiers in an unsymmetrical structure, it does not need to worry about the difference between these two classifiers fading too quickly. We conduct the experiments to compare the performances of co-training, co-EM, multiple-learner, and unsymmetrical co-training algorithms on 30 data sets from Weka [6]. From the experimental results, we can see that the unsymmetrical co-training algorithm outperforms other algorithms.

The remainder of this paper is organized as follows. After introducing some preliminaries about co-training, co-EM, and multiple-learner algorithm in Sec-tion 2, we present our unsymmetrical co-training algorithm in Section 3. Then, the experiments to compare the performa nces of algorithms a re reported in Sec-tion 4. Finally, we give our conclusion and look toward future work in Section 5. Suppose we have the instance space X = X 1  X  X 2 ,where X 1 and X 2 corre-spond to the two different views of the instance space, respectively, and the class label space Y . Given the data set L  X  U ,wehavealabeleddataset L = { h 1 and h 2 , which are used to compose the following algorithms. 2.1 Co-training Co-training [1] first tries to divide the instance space X into two different views X 1 and X 2 , which are conditionally independent given the class label. Then, two classifiers h 1 and h 2 are trained based on these two d ifferent views, respectively. Classifier h 1 classifies the unlabeled instances an d  X  X eaches X  the other classifier h 2 the predicted class labels of unlabel ed instances about which it feels most confident. These confident unlabeled instances are added into the training set of h 2 together with their predicted class l abels. At the same time, classifier h 2 teaches h 1 the predicted class labels about which it feels more confident. After that, each classifier is retrained with the updated training set. Such a process can be repeated until a cer tain stopping condition is satisfied. The framework structure of co-training is shown in Figure 1. From Figure 1, we observe that the framework structure of co-training is symmetrical: the instance space is divided into two views symmetrically; the two classifiers are trained symmetrically; and they update each other X  X  training set in a symmetrical way.

Some theoretical studies have analyzed why and how the co-training algorithm can succeed. With proposing the co-traini ng algorithm, Blum and Mitchell [1] de-fined the co-training model in a PAC-style theoretical framework and proved that the two different views are supposed to satisfy the following conditions: (1) each view is sufficient and consistent to train a good classifier; (2) each view is condi-tionally independent to the other one given the class label. Dasgupta et al. [20] also provided a PAC-style theoretical analysis for co-training. Yu et al. [7] proposed a graphical model for the co-training algorithm based on the conditional indepen-dence assumption. Abney [8] showed that weak dependence can also guarantee co-training X  X  working. Balcan et al. [9] proposed a much weaker  X  X xpansion X  as-sumption on the underlying data distribution, and proved that it is sufficient for the iterative co-training to succeed. Wan g and Zhou [10] analyzed the co-training algorithm as a combinative label propagation over two views, and provided the sufficient and necessary condition for co-training to succeed.
 2.2 Co-EM Nigam and Ghani [2] compared the performances of the co-training algorithm with the EM algorithm, and studied how sensitive the co-training algorithm is to the conditional independence assumption. They the proposed a hybrid algorithm of EM and co-training, called co-EM. The co-EM algorithm is similar to the EM algo-rithm, which is an iterative procedure tha t uses all the unlabeled instances every time, instead of incrementally selectin g some unlabeled instances to update the training set. On the other hand, the co-EM algorithm is also like the co-training algorithm, which tries to divide the instance space into two conditionally inde-pendent views. Nigam and Ghani [2] argued that the co-EM algorithm is a closer match to the theoretical argument established by Blum and Mitchell [1].
In the co-EM algorithm, two EM classifiers, h 1 and h 2 , are chosen to cor-respondtothetwodifferentviews X 1 and X 2 . Initially, classifier h 1 is trained only based on the labeled data set L with view X 1 . Then, classifier h 1 proba-bilistically labels all the unlabeled instances in data set U . Next, classifier h 2 is trained using the labeled instances from the view X 2 of data set L ,plusthe unlabeled instances from the view X 2 of data set U with the class labels given by h 1 . Classifier h 2 then relabels the instances for the retraining of h 1 .This process iterates until the classifiers conv erge. The framework structure of co-EM is shown in Figure 2. From Figure 2, we can see the symmetrical structure of the co-EM algorithm. 2.3 Multiple-Learner Since the paradigmatic assumptions of co-training are difficult to satisfy in real-world application scenarios, many researchers begin to study the variants of co-training that do not require the two-view splitting [3] [4] [11]. Because those algorithms usually use multiple learners, they are referred to as the multiple-learner algorithms. Ng and Cardie [12] summarized multiple-learner algorithms and proposed their own algorithm 1 . In their multiple-learner algorithm, two different classifiers h 1 and h 2 are used and trained based on the single view of instance space. At each iter ation, each classifier labels and scores all the instances in a data pool. Some instances with scores found to be high by classifier h 1 are added to the training set of classifier h 2 together with their predicted class labels from h 1 , and vice verse. Then, the entire data pool is flushed and replenished using instances drawn from the unlabeled data set U after each iteration. The process is repeated until no further ins tances can be labeled. The framework structure of multiple-learner algorithm is shown in Figure 3. Here we can see that the structure of multiple-learner algorithm is also symmetrical, where two classifiers are trained in parallel and co mbined together to score the unlabeled instances in the data pool. As we have already emphasized, the co-training algorithm, the co-EM algorithm, and the multiple-learner algorithm all have symmetrical framework structures. Some of the constraints that restrict these algorithm are related to their sym-metrical structures. For example, both the co-training algorithm and the co-EM algorithm are required to symmetrically divide the instance space into two suf-ficient and conditionally independent views, and their performances are quite sensitive to these assumptions. However, fulfilling these assumptions is a NP-hard problem, and these assumptions are intractable in practice. Although the multiple-learner algorithm does not suffer from the same intractable assump-tions, it nevertheless still requires two different classifiers that are trained in a symmetrical way. Wang an Zhou [10] reported that if the two initial classifiers have large difference, they can be improve d together using the multiple-learner algorithm. They also discovered that, as th e multiple-learner algorithm proceeds, more and more unlabeled data are labeled, which makes the difference between the two learners become smaller and smaller. In other words, even though the two classifiers have big difference initially, they become more and more similar after several learning rounds since they are trained in a symmetrical way, and the performance of the multiple-learner algorithm cannot be further improved. Moreover, if the two selected classifiers are only slightly different from one an-other, two different training sets are required in order to avoid convergence of the algorithm at an early stage in the symmetrical structure. However, in the scenario of semi-supervised learning, a ssigning labeled instances to two different initial training sets will cause the ineffective utilization of labeled data set. To escape the constraints of symmetrical structures, we attempt to design a new algorithm that still performs with the style of co-training but has an unsymmet-rical framework structure.

In this paper, we propose the unsymmetrical co-training algorithm. This al-gorithm uses two unsymmetrical classifiers, namely, the EM classifier and the self-training classifier. The EM classifier is a kind of generative model that has been successfully applied in semi-supervised learning [13]. The EM algorithm includes two steps: the E-step and the M-step. The E-step estimates the expec-tations of the class information of unlabeled instances, and the M-step maximizes the likelihood of the model parameters using the expectations from the previous E-step. The EM classifier performs an iterative hill-climbing process to find a local maxima of model probability and then assigns the class labels in terms of the established model. Self-training might be the earliest technique for semi-supervised learning [14] [15] and is commonly applied in many domains [17] [16]. In the self-training classifier, an underlying classifier is iteratively trained and used to label the unlabeled instances, a nd then some unlabeled instances having the confident predictions are used to update the training set for the next round training of the underlying classifier.

Not only are two unsymmetrical classi fiers applied in the unsymmetrical co-training algorithm, but these two classifiers are also applied within an unsym-metrical framework structure. In the st ructure, a data pool has been created for the self-training classifier by randomly s electing instances from the unlabeled in-stance set U . This data pool is the labeling objective on which the self-training classifier focuses in the process of the algorithm. But the EM classifier does not focus on a specific section of unlabeled in stances. Instead, it faces the entire un-labeled instance set U during the algorithm. Moreover, the training sets used to train these two classifiers are different. For the self-training classifier, the train-ing set consists of the labeled instan ces and the unlabeled instances from the data pool with their predicted class lab els. For the EM classifier, the training set is the labeled instances plus the whole unlabeled instances together with the class labels assigned from the previous learning round. The framework structure of unsymmetrical co-training algorithm is shown in Figure 4.

The unsymmetrical co-training algorithm learns the two classifiers in an un-symmetrical way. Initially, both of the classifiers are trained based on the labeled instance set L with the single view (the full set of attributes). Then, the EM clas-sifier labels all the unlabeled instances, and the self-training classifier predicts the labels of unlabeled instances in the data p ool. The predicted class labels of unla-beled instances in the data pool will be used to substitute the class labels of cor-responding unlabeled instances that have been assigned by the EM classifier. The EM classifier is then retrained by the updated training set and relabels the unla-beled instances. The unlabeled instances in the data pool, for which class labels from the self-training classifier are iden tical to the class labels from EM classifier, will be selected to update the training set of self-training classifier together with their predicted class labels. If there are not enough such unlabeled instances, the confidence degree metric will be used to select other unlabeled instances with high confidence in the data pool to update the training set together with the predicted class labels from the self-training classifier. Next, the data pool will be replenished by other unlabeled instances. The procedure is repeated until there are no further instances in the data pool. The predictions for new-coming instances are obtained using the combination of predictions from the EM classifier and the self-training classifier, just as the co-training algorithm does [1]. The formal description of un-symmetrical co-training algorithm is shown in Figure 5.

According to the theoretical study of Wang and Zhou [5], the co-training style algorithm is able to succeed if the two cla ssifiers are vastly different. From the description above, we can see that the unsymmetrical co-training is consistent with their theory. The self-training classifier and the EM classifier not only dis-play different characteristics on learning, but also are deployed differently in the unsymmetrical framework structure. Alt hough they are both initially trained by the same labeled instances, these two classifiers have different training sets once they enter the iteration procedure. Theref ore, the unsymmetrical co-training al-gorithm avoids the early convergence of both classifiers to the same hypothesis, and utilizes the labeled and unlabeled instances more effectively than does the multiple-learner algorithm. Moreover, unlike the multiple-learner algorithm, in which two classifiers become more and m ore similar as the algorithm proceeds, our algorithm always maintains the differences between the two classifiers due to its unsymmetrical way of learning. On the other hand, the unsymmetrical co-training algorithm uses the single view of instance space so that it avoids the intractable conditional independent view-splitting.
 In the unsymmetrical co-training algorithm, the self-training classifier and the EM classifier can complement each other. The EM algorithm essentially uses the naive Bayes method to assign class membership probabilities to unlabeled instances. The EM classifier is expected to do well when it satisfies the con-ditional independence assumption of naive Bayes. However, these probabilities are always poorly estimated because the conditional independence assumption is violated. Self-training, on the other hand, uses the class membership probabil-ities for ranking the confidences of unlabel ed instances instead of directly using them for the classification. Thus, the conditional independence assumption influ-ences the self-training classifier more w eakly than does the EM classifier. From another point of view, self-training is an incremental procedure and always suf-fers from the reinforcement of any misclassifications from previous updates. In the unsymmetrical co-training algorithm, the EM classifier is like a supervisor beside the self-training classifier, which checks the predicted class labels made by self-training and provides opinions for these predictions in terms of its own knowledge. It is useful to reduce the chance of adding misclassifications to the next iteration in the procedure. Moreover, the self-training classifier restricts its view only on the labeled instances and t he unlabeled instances in the data pool. Since the EM classifier is able to see the entire set of labeled and unlabeled instances, the view of the EM classifier is much broader than that of the self-training classifier. Therefore, the EM cla ssifier might be seen to make predictions from a global view to help self-training, and the self-training classifier likewise boosts EM from its local view.

We summarize the differences of co-train ing, co-EM, multiple-learner, and un-symmetrical co-training algorithms from several points of view in Table 1. From Table 1, we can see that the unsymmetrical co-training algorithm not only com-bines the advantages of other algorithms, but also overcomes their disadvantages. In the next section, we design the exper iments to compare the performances of these algorithms in the next section.
 In this section, we design the experimen ts to compare the pe rformances of co-training, co-EM, multiple-learner, and unsymmetrical co-training algorithms. The experiments are conducted on 30 data sets from Weka [6], which are selected from the UCI repository. There are some preprocessing stages adopted on each data set. First, we use the filter ReplaceM issingV alues in Weka to replace the missing val-ues of attributes in each data set. Second, we use the filter Discretize in Weka, which is the unsupervised ten-bin discretization, to discretize numeric attributes. Thus, all the attributes are nominal. Moreover, we notice that some attributes do not contribute any information for the purpose of prediction if the numbers of these attributes are almost equal to the numbers of instances in the corresponding data sets. The third preprocessing stage is to use the filter Remove in Weka to delete such attributes. We implement co-training, co-EM, multiple-learner and unsym-metrical co-training algorithms in the Weka framework. The underlying classifiers used by algorithms are naive Bayes classifiers, except the co-EM and a part of the unsymmetrical co-training algorithms that use the EM classifiers.

Our experiments are configured as follows. In each data set, 10% of the in-stances are used as testing instances; 10% of the remaining data set is used as the set of labeled instances; and all other in stances are used as unl abeled instances. For the co-training and co-EM algorithms based on two views, the attribute set is randomly divided into two disjointed subsets. For the co-training, co-EM, and unsymmetrical co-training algorithms that need the data pool, the size of the data pool is set to 10% of the unlabeled instance set. For the co-training and multiple-learner algorithms that fix the number of instances added per iteration, the number of added instances for each class label is decided by the class label distribution in the original labeled instance set: for the class label with the min-imum percentage, the number is set to 1; and for all the other class labels, the numbers are set to the times of that the class label has the minimum percentage. The accuracy score is used to evaluate the performances of algorithms. In our experiments, the accuracy scores of each algorithm are obtained via 10 runs of ten-fold cross-validation and evaluated on the same testing sets. Finally, we con-duct two-tailed t-test with a 95% confidence level to compare the unsymmetrical co-training algorithm to the other algorithms. The results are shown in Table 2.
In Table 2, the two-tailed t -test results are shown in the bottom row, where each entry has the format of w/t/l . This means that, comparing with the unsym-metrical co-training algorithm, the algorithm in the corresponding column wins w times, ties t times, and loses l times. From the experimental results, we ob-serve that the unsymmetrical co-training algorithm outperforms other algorithms, where it wins 8 times and loses 2 times against the co-training and multiple-learner algorithms, and wins 10 times and never loses against the co-EM algorithm. The evidences provided by the above experiments can be explained as follows. The unsymmetrical structure is more effect ive in the scenario of semi-supervised learning than are the symmetrical struct ures. The unsymmetrical co-training algorithm combines the EM classifier, which learns from a global view, and the self-training classifier, which boosts the learning from the local view. These two clas-sifiers, which complement each other in the unsymmetrical structure, enhance the learning ability of the co-training style framework. The co-training and co-EM algo-rithms are sensitive to the conditional independence assumption. Randomly split-ting the attribute sets decrea ses the overall performances of co-training and co-EM algorithms. Although the multiple-learner algorithm does not need two-view split-ting, just as the unsymmetrical co-training algorithm does, the small number of labeled instances cannot be u tilized effectively in the sy mmetrical structure. Be-sides, as the multiple-learner algorithm proceeds, its underlying classifiers become more and more similar so that the performance cannot be further improved. In this paper, we propose the unsymmetrical co-training algorithm, which is a novel semi-supervised learning metho d in the co-training style. In the algo-rithm, two unsymmetrical classifiers, namely, the self-training classifier and the EM classifier, are learned in an unsymmetrical way within an unsymmetrical framework structure. Compared with other co-training style algorithms, such as co-training, co-EM, and multiple-learner, the unsymmetrical co-training algo-rithm has several advantages. First, the unsymmetrical co-training algorithm is based on the single view of instance space, so it does not suffer from the violation of conditional independence assumption as co-training and co-EM algorithms do. Second, the unsymmetrical structure makes the utilization of labeled instances more effective since there is no need to sp lit the labeled instance set into two different initial training sets for two underlying classifiers. Moreover, the two un-symmetrical classifiers do not easily become more similar after several learning rounds because the unsymmetrical training of the algorithm prevents growing similarity. We conduct the experiments to compare the performances of these algorithms. The experimental results show that the unsymmetrical co-training algorithm overall outperforms other algorithms.

In the future, we will continue the study of semi-supervised learning meth-ods in the co-training style, especially within the unsymmetrical framework structure. More experiments will be conducted to compare our unsymmetrical co-training algorithm with other co-training style methods under various circum-stances. The different underlying classifiers will be tested within this unsymmet-rical structure to see whether the performance can be improved further. It will also be interesting to apply the unsymmetrical co-training algorithm to real-world applications, especially for the applications suitable for semi-supervised learning, such as natural Language processing (NLP) and bioinformatics.
