 In this paper, a novel approach based on recommendation model is proposed for automatic image annotation. For any to-be-annotated image, we first select some related ima ges with tags from training dataset according to their visual similarity. And then we estimate the initial ratings for tags of the training images based on tag ranking method and construc t a rating matrix. We also construct a trust matrix based on visual similarity with a k-NN strategy. Then a recommendation model is built on both matrices to rank candidate tags for th e target image. The proposed approach is evaluated using two benchmark image datasets, and experimental results have indicated its effectiveness. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Experimentation, Verification image annotation, recommendation m odel, TrustWalker, retrieval The exponential growth of Web im ages has created a compelling need for innovative methods to re trieve and manage them. As one of the promising solutions, au tomatic image annotation has recently attracted numerous research attentions, whose ambition is to find several tags that can well represent the visual content when given a to-be-annotated image with no or few tags. Though many effective algorithms and models have been proposed for this challenge, e.g. JEC [4], automatic image annotation is still far from practical and needs further research. In this paper, image annotation problem is transformed into personal item recommendation problem, where images are modeled as users and tags as items. Then the process of image annotation is to recommend tags for to-be-annotat ed images. To apply any recommendation model, the so-called  X  X old start X  problem must be firstly taken into consideration, because to-be-annotated images have no or fe w tags. Here we adopt a trust-based and item-based recommendation model named TrustWalker [2] as our basic model, which re commends tags mostly according to the trust between images, w ithout requiring that the to-be-annotated image should have any existing tags. Furthermore, the initial ratings for tags of each training image, and the trust between images, need to be est imated. Then a model developed from TrustWalker can be applied to recommend tags for to-be-annotated images. The whole process of our proposed approach, rating. The process of searching and selecting ratings can be simulated with several random walks, the step of which is equivalent to the depth of sear ching. Candidate items then are ranked according to their predicted ratings, and the top  X  will be recommended for  X   X  . To better fit the challenge of automatic image annotation, we have made some im provements from basic TrustWalker , including restraint on the search range, introduction of real-valued trust degree, etc. When a nnotating a given image  X   X  , firstly we select  X  related images with tags from training dataset based on visual similarity, acting as a restricted search range and their tags being candidates. Such a restriction helps to reduce tag noise and computing complexity. Then the trust degree  X  X   X   X  to image  X   X  is calculated with the Gaussian kernel function [1]. That is,  X  X   X  distance, and  X  is a radius parameter. Then each of the  X  X  X  X  X  X  X  images can respectively constr uct its own trust set with  X  of its most trusted images within the  X  related images. Finally a trust matrix  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  consisting of real-valued trust degree can be constructed. To predict the rating  X   X   X  candidate tag  X   X  , several random walks over the trust matrix are performed, as in basic TrustWalker . By calculating and merging the probability for rating selection, we can finally get the probability for the random walk to stop at image  X  and return its rating on tag  X  , namely,  X  X , X  X  X  X  . Then  X   X   X  where  X   X , X  is a boolean variable denoting whether image  X  owns tag  X  ,  X   X , X  is the tag rating, and  X  X  X  X  X  X  means that for any related image, we just merge the returned ratings of the top 3 most similar tags, hoping to reduce th e negative effects of dissimilar tags. Then we rank all candidate tags based on their corresponding predicted ratings, and take the top  X  as annotating result. Our proposed approach, IARM , is evaluated with two benchmark image datasets, namely, Ground Trut h Database of University of Washington ( UW ) [5] and the real-world MIRFlickr [6]. In UW , there are quite few noisy tags, and many similar images are of the same scene in different views. In MIRFlickr , however, the existing tags are fairly noisy, including much metadata and many meaningless tags, and the images are quite distinct from each other. In both datasets, we respectively take 1/10 th of the images to form our test sets and the remaining are used as training datasets. And all tags in training datasets are pre-processed with WordNet, for stemming and ki cking out strange words. As for baselines, we take the LNP proposed by Wang [3] and the baseline JEC proposed by Makadia [4]. Moreover, we extract six kinds of features for each image, th at is, ColorLayout, Gabor, Sift, ScalableColor, EdgeHistogram and Tamura. When calculating visual distance between imag es, we normalize and merge distances of different features with equal weights. As for experimental parameters, the thre shold for random walk is set as 0.001, and  X  in Gaussian kernel function is set as the average visual distance. In UW , we set  X  as 20,  X  as 5, while in MIRFlickr , both are respectively set as 40 and 10. And in UW , the top 5 tags of each algorithm are selected as its annotating result, while top 10 in MIRFlickr . The performances of all three approaches are measured with top N precision rate [7], which 
