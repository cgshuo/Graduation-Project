 Previous studies of web search result examination have pro-vided valuable insights in understanding and modelling searcher behavior. Yet, recent work (e.g., [3]) has been developed based on the assumption that the time a searcher spends examining a particular result abstract or snippet, correlates with result relevance. While this idea is intuitively attrac-tive, to the best of our knowledge it has not been empirically tested. This poster investigates this hypothesis empirically, in a controlled setting, using eye tracking equipment to com-pare search result examination time with result relevance. Interestingly, while we replicate previous findings showing examination time to be indicative of whole-page relevance, we find that viewing time of individual results alone is a poor indicator of either absolute result relevance or even of pairwise preferences. Our results should not be taken as negating the usefulness of modeling searcher examination behavior, but rather to emphasize that snippet examination time is not in itself a good indicator of relevance. H.4 [ Informational storage and retrieval ]: evaluation, search process.
 Web search snippet examination; web search behavior; search evaluation.
As terabytes of search log data are being generated daily, correct interpretation of patterns in these data becomes ever more challenging. Mining of click data has become a fertile area for many applications, including result relevance esti-mation, automatic query suggestion, and many others. One interesting application of using search logs to infer result examination behavior on Search Result Pages (SERP) was introduced by He et al. [3]. Another successful application of examination data is identification of relevant sub-parts of documents, which can be used for query expansion or term re-weighting (see Buscher et al. [1] for more details). Yet, while result examination data is likely to provide rich con-textual information for machine learning algorithms for ac-curate user modeling and interpretation of past clicks, to our knowledge, the examination behavior on the SERP itself as a predictor of relevance has not been empirically evaluated in prior research [3].
 This poster reports preliminary results on analyzing whether viewing time of a search result snippet can be directly used to estimate its relevance label. Using eye tracking equip-ment, we confirm that previously suggested measures of viewing behavior (e.g., the distribution of viewing time across all result ranks) indeed correlate with overall result quality. However, we also find that examination time alone is un-likely to be a good indicator of relevance of individual results , primarily due to severe position bias in the examination be-havior. Our results are consistent with and complement the findings of Guan and Cutrell [2], where users were observed to spend more time finding relevant results placed in the bottom of the result list, as opposed to the original result ordering. Next, we describe the details of eye tracking user study that we conducted, after what we present our findings on correlating result viewing time and relevance, both for the whole set of results and for individual search results.
Our study used 25 benchmark search tasks selected from the Web Track of the TREC 2009 competition. The goal for each task (the description) was provided to the partici-pants. For example, the goal of the query  X  X oilet X  was stated as:  X  X ind information on buying, installing, and repairing toilets X . For each task, the query keywords were submitted to the Google search engine, and the Search Engine Result Pages (SERPs), as well as all the result URLs linked from each SERP, were cached. We did not remove non-organic results such as ads, embedded image results, news or local results, etc. -thereby recreating a realistic search experi-ence for the subjects. Paid annotators rated every search result for relevance, with at least five ratings per document, on a three-point scale ( X  X ad X ,  X  X artially Relevant X  and  X  X er-fect X ). We retained the majority opinion as the final result relevance label. For the rest of the details about user study we refer to [5].
While our main goal is to investigate whether snippet ex-amination time can be used as a proxy for result relevance, we first confirm that our data are in correspondence with prior work on studying user examination behavior on the search result page.

Figure 6(a) of [5] reports the fraction of the examination time of the snippets, broken down by the result rank. Specif-ically, the values were computed for each subject and query (that is, the viewing time for each result abstract by a sub-ject was divided by the total viewing time of the correspond-ing SERP) for an individual query, and then averaged across all queries. The decaying shape of the viewing time distri-bution is similar to that reported in prior work (e.g., by Joachims et al. [4]). The fact that web search users tend Pairwise -Above 0.119 0.063 Pairwise -Adjacent 0.007 0.003 Table 1: Pearson correlation between individual re-sult examination time and relevance labels. to spend more time reading top results irrespective to the result relevance is well known and typically referred as po-sition bias (sometimes called presentation bias). However, when considering viewing time for individual queries, the ef-fect of the position bias is smaller. In fact, for some of the queries in our dataset, the viewing time does not peak at rank one, and actually can be used to estimate the whole page relevance , as we discuss below.
 Viewing time vs. individual result relevance : we now investigate whether variation in snippet examination time, as measured per result or by entire distribution, could be attributed to result relevance. Figure 1 reports the box plot of relative viewing time, broken down by relevance labels, where the boxes correspond to 25th and 75th percentiles, and the median is marked with a red line. As Figure 1 reports, there is a substantial overlap in values of viewing time for different relevance levels. Statistical tests on dif-ferences of result viewing time between Bad and Partially Relevant groups, and Bad and Perfect groups fail to reject the null hypothesis, leading us to conclude that snippet ex-amination time alone can not be used to infer individual re-sult relevance . To verify our findings, we pool viewing time and relevance labels from all queries, and compute Pear-son correlation between them. Overall, we find that there is only a weak correlation (0.157) between snippet viewing time and relevance. In order to mitigate the effects of posi-tion bias, we also compared relative result preferences based on relevance vs. based on the difference in examination time. Specifically, we compared the differences of snippet viewing time per result to the differences in relevance levels, in three ways: Pairwise-All , pairwise preferences derived based on all (usually 10) results on the SERP; Pairwise-Above , pairwise preferences between each result and only the results ranked above it; Pairwise-Adjacent , pairwise preferences between adjacent results only. Surprisingly, as reported in Table 1, we find that even pairwise preferences do not exhibit a stronger correlation with viewing time, except for Pairwise-All , where pairwise preferences have a higher correlation with viewing time compared to using the absolute document relevance. Additionally, we re-calculated these correlation coefficients using absolute viewing time, instead of the rel-ative viewing time. Finally, we repeated our computations and considered only the results viewed at least once, with-out finding significantly higher correlation levels. We omit details on these experiments due to space constraints. Viewing time distribution vs. whole page relevance : while viewing time on individual results is a poor predic-tor of relevance, we also investigated whether we can relate viewing behavior on the SERP as a whole, with result rel-evance of the whole result set. To quantify overall page quality we calculated standard information retrieval met-rics, namlely: Mean Average Precision ( MAP ), Normalized Discounted Cumulative Gain ( NDCG ) and Expected Recip-rocal Rank ( ERR ). Intuitively, when the result ranking is poor, the users are expected to spend more time reading Figure 1: Viewing time on search results broken down by relevance label.
 Table 2: Pearson correlation between viewing time and whole page relevance. result abstracts at lower ranks. To validate our intuition we compute correlation between relative viewing time at rank one ( Views@1 ) and mean rank of viewing time distri-bution (analog of distribution mean). Note that the relative Views@1 is linearly depended on viewing time at lower ranks (due to normalization constraint), hence when more atten-tion is spent on lower ranks, the value of Views@1 decreases. Table 2 reports Person correlation between Views@1 and mean rank and whole page quality represented by one of the three metrics above. We find that Views@1 has substantial correlation of 0.452 with the MAP metric, while exhibiting smaller, but non-negligible correlation with NDCG (0.249) and to a smaller degree with ERR (0.102). Summary : to our knowledge, we performed the first empirical examination of the result examination hypothesis , disproving the intuition that longer snippet examination times indicate higher rele-vance of individual results. Surprisingly, this was the case both for absolute relevance levels, and for pairwise result preferences, under commonly observed strategies of result examination (i.e., the  X  X kip-Above X  and  X  X kip-Next X  strate-gies in reference [4]). However, we confirmed that the dis-tribution of the viewing times on the SERP does correlate with ranking quality of the SERP as a whole, suggesting a more promising direction for further research. In our future work we plan to conduct more thorough analysis on result examination behavior and associated relevance that would account for consistent biases in result perception such as abstract length and judgeability.
