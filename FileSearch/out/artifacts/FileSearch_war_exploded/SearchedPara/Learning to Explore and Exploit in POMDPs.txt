 goal of employing available experience to define perceived o ptimal actions. expert advice is available.
 vehicle, depending upon the application.
 taken from [6], with the proofs omitted here.
 z stochastic policies with  X  ( z,a ) denoting the probability of taking action a in z . and Z = { 1 , 2 ,..., |Z|} . We abbreviate ( a as h t . Let  X  = {  X ,,W } denote the RPR parameters. Given h distribution of z By marginalizing z distribution of action choices is obtained as follows: which gives a stochastic policy for choosing the action a 2.1 Learning Criterion [10], we represent the experiences by a set of episodes.
 a , and r are respectively observations, actions, and immediate rew ards. Definition 2.3 (The RPR Optimality Criterion) Let D ( K ) = { ( a k  X  p where h k 0 &lt;  X  &lt; 1 is the discount, and  X  denotes the RPR parameters. that lim of b with overwhelming probability. 2.2 Bayesian Learning Let G where b V ( D ( K ) ) = R b V ( D ( K ) ;  X ) G prior as a product of Dirichlet distributions, where p ( |  X  ) = Dir (1) , , ( |Z| )  X  , p (  X  |  X  ) = Q |Z| p ( W |  X  ) = a variational posterior by maximizing a lower bound to ln R b V ( D ( K ) ;  X ) G LB( { q k t } ,g ( X )) = ln where { q k the Kullback-Leibler (KL) distance between probability me asure q and p . The factorized form { q  X  { q summarized in Theorem 2.4; the proof is in [6].
 updates produces a sequence of monotonically increasing lo wer bounds LB ( { q k converges to a maxima. The update of { q k e and  X  is the digamma function. The g ( X ) has the same form as the prior G hyper-parameter are updated as where  X  k b  X  episodes before the RPR becomes optimal.
 exploitation is performed with the goal of accumulating a la rge long-run reward. 1) doing so is highly rewarding, while following  X  ( y = 1 ) in belief region z . Let  X  have the prior In order to encourage exploration when the agent has little e xperience, we choose u the primary RPR, is updated upon completion of each new episo de. H based on a subset of samples from H . Let H the agent, i.e., the primary RPR is optimal in H upon entering H 0 | h,  X  , X  )  X  1 } , if the posterior of  X  is updated by where  X  is a small positive number, and  X  k meta-reward received at t in episode k . We have m k episode k , and m k After a certain number of queries, b u ploration is actually discredited when  X  because it has been pre-assigned a credit ( u monotonically decrease with the accumulation of episodes. The parameter u prior for the amount of needed exploration. When c &gt; 0 , u needs a larger u amount of exploration monotonically increases with u enough u However, an unnecessarily large u analysis below. The possible range of u min value function in (2), where the first summation is over all elements in E ( K ) repeated elements. The condition y indicates that the agent always follows the RPR (  X  ) here. Note b V ( E ( K ) function of  X  defined on E ( K ) up to a difference in acquiring the episodes: E ( K ) value function defined using E ( K ) function b V where  X  t ment. Similarly we can define b V ( D ( K ) ;  X  , X ,  X  The following lemma is proven in the Appendix.
 Lemma 4.1 Let b V ( E ( K ) P some episode in E ( K ) (8), with u b b b V We now show b V optimistic value function, in which the agent receives R the premise,  X  is updated according to (7) and (8) and u (see the discussions following (7) and (8)), which implies  X  is optimal in { h Thus, the inequality holds. Q.E.D.
 smaller than  X  (1  X   X  ) R  X  1 is either receiving sufficient rewards or it is performing su fficient exploration. towards exploration by using u in the environment and stops learning when K with various values of u switching between exploration and exploitation with a fixed exploration rate P of policy  X  r rewards result from exploitation alone. ( iii ) the exploration rate P in Figure 1 when  X  Figure 1: Results on Shuttle with a random exploration policy, with K It is seen from Figure 1 that, with random exploration and u to optimality and, accordingly, P u abound 2500 learning episodes. With u P 3 and 4: ( i ) the primary policy improves as P such that the primary policy is optimal if P u to converge.
 The results in Figure 2, with  X  1 when  X  or it remains optimal if it is already optimal. existence of a small u needed to find u min agent uses a large u tions. This work is supported by AFOSR.

