 To learn concepts over massive data streams, it is essen-tial to design inference and learning methods that operate in real time with limited memory. Online learning methods such as perceptron or Winnow are naturally suited to stream processing; however, in practice multiple passes over the same training data are required to achieve accuracy compa-rable to state-of-the-art batch learners. In the current work we address the problem of training an on-line learner with a single pass over the data. We evaluate several existing meth-ods, and also propose a new modification of Margin Bal-anced Winnow, which has performance comparable to lin-ear SVM. We also explore the effect of averaging, a.k.a. vot-ing , on online learning. Finally, we describe how the new Modified Margin Balanced Winnow algorithm can be nat-urally adapted to perform feature selection. This scheme performs comparably to widely-used batch feature selection methods like information gain or Chi-square, with the ad-vantage of being able to select features on-the-fly. Taken together, these techniques allow single-pass online learning to be competitive with batch techniques, and still maintain the advantages of on-line learning.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation.
 Online Learning, Single-pass, Averaging, Voting, Winnow
Compared to batch methods, online learning methods are often simpler to implement, faster, and require less mem-ory. For such reasons, these techniques are natural ones to consider for large-scale learning problems.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.
Online learning algorithms have been traditionally trained using several passes through the training data [3, 11, 14]. In the current work we address the problem of single-pass online learning , i.e., online learning restricted to a single training pass over the available data. This setting is partic-ularly relevant when the system cannot afford several passes throughout the training set: for instance, when dealing with massive amounts of data, or when memory or processing re-sources are restricted, or when data is not stored but pre-sented in a stream.

In this paper, we experimentally compare the performance of different online learners to traditional batch learning in the single-pass setting, and we introduce a new online algo-rithm  X  MBW or Modified Balance Winnow  X  that out-performs all other single-pass online learners and achieves results comparable to Linear SVM in several NLP tasks.
Voting (a.k.a. averaging) an online classifier is a technique that, instead of using the best hypothesis learned so far, uses a weighted average of all hypotheses learned during a train-ing procedure. The averaging procedure is expected to pro-duce more stable models, which leads to less overfitting [13]. Averaging techniques have been successfully used on the Perceptron algorithm [14], but never in other online learners such as Winnow, Passive-Aggressive[10] or ROMMA[16]. In the current work, we provide a detailed performance com-parison on how averaging affects the aforementioned online learners when restricted to a single learning pass only. Re-sults clearly indicate that voting improves performance of most mistake-driven learning algorithm, including learners to which it has not traditionally been applied.

We also propose an effective Online Feature Selection scheme based on the  X  X xtreme X  weights stored by the MBW algorithm. Performance results indicate that this scheme shows surprisingly good accuracies in NLP problems, being competitive with Chi-Square or Information Gain, but having the advantage of being able to select the most meaningful features on-the-fly.

Below, section 2 presents different online learners, and introduces the MBW algorithm. Section 3 presents the av-eraging technique. Section 4 compares results and presents the first two contributions: the impressive results of MBW in NLP tasks, and the boost in performance obtained on non-NLP datasets by averaging classifiers. In section 5, we in-troduce a new MBW-based online feature selection scheme. Finally, section 6 presents our conclusions. To present this method, we will first describe the general format for mistake-driven online learning algorithms, illus-trated in Table 1. For each new example x t presented, the current model will make a prediction y t  X  X  X  1 , 1 } and com-pare it to the true class y t  X  X  X  1 , 1 } . The prediction will be based on the score function f , on the example x t and on the current weight vector w i . In the case of a prediction mis-take, the model will be updated. Different mistake-driven algorithms differ in terms of the score function f and in the way the weight vectors w i are updated, as we shall detail in the next sections.
 1. Initialize i = 0, success counter c i =0,model w 0 2. For t =1 , 2 , ..., T : The Positive Winnow , Balanced Winnow and Modified Balanced Winnow algorithms are based on multiplicative updates. For all three, we assume the incoming example x t is a vector of positive weights, i.e., x j t  X  0,  X  t and assumption is usually satisfied in NLP tasks, where the x values are typically the frequency of a term, presence of a feature, TFIDF value of a term, etc.

In preliminary experiments, we found that the Winnow variants performed better if we applied an augmentation and a normalization preprocessing step, in both learning and testing phases. When learning, the algorithm receives a new example x t with m features, and it initially augments the example with an additional feature(the ( m +1) th feature), whose value is permanently set to 1. This additional feature is typically known as  X  X ias X  feature. After augmentation , the algorithm then normalizes the sum of the weights of the augmented example to 1, therefore restricting all feature weights to 0  X  x j t  X  1.

In testing mode, the augmentation step is the same, but there is a small modification in the normalization . Before the normalization of the incoming instance, the algorithm checks each feature in the instance to see if it is already present in the current model ( w i ). The features not present in the current model are then removed from the incoming instance before the normalization takes place. The Balanced Winnow algorithm is an extension of the Positive Winnow algorithm [17, 11]. Similar to Positive Winnow, it is based on three parameters: a promotion pa-rameter  X &gt; 1, a demotion parameter  X  ,where0 &lt; X &lt; 1, and a threshold parameter  X  th &gt; 0.

Let x t ,w i denote the inner product of vectors x t and w Here, the model w t is a combination of two parts: a positive model u t and a negative model v t . The score function is f = sign ( x t ,u i  X  x t ,v i  X   X  th ), and the update rule is: For all j s.t. x j t &gt; 0, u
The initial model u 0 and v 0 are set to the positive values  X  0 and  X  simplicity, Positive Winnow and Balanced Winnow are able to perform very well in different NLP tasks [3, 4, 11].
The Modified Balanced Winnow , henceforth MBW, is de-tailed in Table 2. Like Balanced Winnow, MBW has a pro-motion parameter  X  , a demotion parameter  X  and a thresh-old parameter  X  th . It also uses the same decision function f as Balanced Winnow, as well as the the same initialization. However, there are two modifications.

The first modification is the  X  X hick X -separator (or wide-margin) approach [11]. The prediction is considered mis-taken, not only when y t is different from y t , but also when the score function multiplied by y t is smaller than the  X  X ar-gin X  M ,where M  X  0. More specifically, the mistake con-dition is ( y t  X  ( x t ,u i  X  x t ,v i  X   X  th ))  X  M .
The second modification is a small change in the update rules, such that each multiplicative correction will depend on the particular feature weight of the incoming example. The change is illustrated in Table 2.

Initially proposed in 1958 [20], the Perceptron learner al-gorithm uses a very simple and effective update rule. In spite of its simplicity, given a linearly separable training set, the Perceptron algorithm is guaranteed to find a solution that perfectly classifies the training set in a finite number of iterations.
 Another learner, the Relaxed Online Maximum Margin Algorithm , or ROMMA [16], incrementally learns linear threshold functions classify previously-presented examples correctly with a maximum margin. ROMMA uses additive as well as multiplicative updates.

The Passive-Aggressive algorithm [10] is also based on ad-ditive updates of the model weights. However, the update policy here is based on an optimization problem closely re-lated to the one solved in Support Vector Machine tech-niques. Passive-Aggressive has two characteristic parame-ters: the relaxation parameter  X   X  0, and the insensitivity parameter . In our implementation, we arbitrarily set =1 and  X  =0 . 1 based on preliminary tests.
The Averaging technique can be briefly described in the following terms: instead of using the best hypothesis learned so far, the final model will be a weighted average of all hy-potheses learned during the training procedure. The aver-aging procedure is expected to produce more stable models, which leads to less overfitting [13]. For instance, an averaged version of the Perceptron learner (a.k.a. Voted Perceptron) is described by Freund &amp; Schapire [14].

In the current work, we consider the final hypothesis of the voted learners to be the average of the intermediary hypotheses weighted by the number of correct predictions that each hypothesis made in the learning process. More specifically, referring to Table 1, the averaged model w is w a = 1 Z i w i  X  c i ,where c i is the number of correct predictions made by the intermediary hypothesis w i ,and Z = i c i is the total number of correct predictions made during training.

Averaging can be trivially applied to any mistake-driven online algorithm. We applied it to all learners presented pre-viously and we refer to it using a  X  X - X  prefix. For instance, v-MBW and v-ROMMA refer to the voted (or averaged) versions of Modified Balanced Winnow and ROMMA, re-spectively.
The algorithms described above were evaluated in several datasets, from different sources. The RequestAct dataset [9] labels email messages as having a  X  X equest speech act X  or not. In addition to the single word features [9], all word sequences with a length of 2, 3, 4 or 5 tokens were extracted and considered to be different features. The RequestAct dataset has 70147 features and 518 examples. The Spam dataset has 3302 examples and 118175 features. The task is to detect spam email messages [2]. The Scam dataset has 3836 examples and 121205 features. Here we attempt to separate  X  X cam X  messages from the others [2]. The Reuters dataset [15] has 11367 examples and 30765 features. We attempt to classify the category  X  X oney X  [1]. The 20news-group dataset [18] has 5000 examples and 43468 features, and the problem is classifying newsgroups posts according to one of the topics. The MovieReviews dataset [19] has 1400 examples and 34944 features. In this problem we try to associate a positive or negative sentiment with a movie review. The Webmaster dataset has 582 examples and 1406 features. The task is to classify web site update requests as  X  X hange X  from  X  X dd or Delete X  [8].

The Signature and the ReplyTo datasets are related to the tasks of detecting signature lines and  X  X eply-to X  lines in email messages, respectively, using a basic set of features [5]. Both datasets have 37 features and 33013 examples.
The next datasets were obtained from the UCI data repos-itory. The Adult dataset originally had 14 attributes and, using only the training partition provided, 30162 examples. Examples with missing attributes were discarded and the 8 nominal attributes were turned into different binary at-tributes. The final dataset had 104 different attributes. The Congressional dataset has 16 binary features and has 435 examples. The Credit (or Japanese Credit Screening) dataset has 690 examples and 15 features originally. Af-ter removing examples with missing attributes and turning nominal attributes into different binary features, the dataset had 46 features and 653 examples. The Ads dataset (or Internet Advertisements) has 3279 examples and 1558 fea-tures, mostly binary. Missing features were disregarded in this data. The WiscBreast database represents the breast cancer database obtained from the University of Wisconsin. The data has 9 integer-valued features, and after removing examples with missing attributes, a total of 683 examples and89featuresremained. The Nursery dataset has 12960 instances and originally 8 nominal features. After turning nominal features into different binary features, 89 features can be found in the dataset. The task here is to distinguish between  X  X riority X  and the other classes.
In all Winnow variants, we set the promotion parameter  X  =1 . 5 and the demotion parameter  X  =0 . 5. These are the same values used in previous Winnow implementations [3, 4]. Additionally, all Winnow variants used threshold  X  th =1 . 0 motivated by the fact that all incoming exam-ples go through the normalization preprocessing step. Also motivated by the normalization procedure, the  X  X argin X  M was set to 1.0 in MBW.

Based on ideas from Dagan et. al. [11], the initial weights in Positive Winnow were initialized as  X  0 =1 . 0,andinBal-anced Winnow as  X  + 0 =2 . 0and  X   X  0 =1 . 0. Similar to Bal-anced Winnow, the MBW initial weights were  X  + 0 =2 . 0and  X  0 =1 . 0.

For comparison, we added performance results of two pop-ular learning algorithms that are typically used in batch mode: linear SVM 1 and Naive Bayes [18]. Results were evaluated in terms of F1 measures. F1 is the harmonic
We evaluated the general classification performance of the algorithms in 5-fold cross-validation experiments. All algo-rithms were trained using only a single pass through the training data. Results are illustrated in Tables 3 and 4.
Table 3 describes the performance of five different online methods, along with their voted versions. The first eight datasets in Table 3 are NLP-like datasets, where the feature space is very large and the examples are typically  X  X parse X , i.e., the number of non-zero features in the examples is much smaller than the size of the feature space. The last seven datasets (non-NLP) in Table 3 have a much smaller feature space and the examples are not sparse.

Median F1 values and the average rank values over the two different types of data are also included in Table 3. The best results for each dataset are indicated in bold. Two-tailed T-TestsrelativetoMBWareindicatedwiththesymbols * ( p  X  0 . 05) or ** ( p  X  0 . 01).

In general, the non-voted Winnow variants performed bet-
We used the LIBSVM implementation [7] with default pa-rameters RequestAct 68.0 65.4 76.7 67.3 56.85 20newsgroup 88.8 67.9 93.7 91.9 94.42 MovieReviews 78.5 71.4 75.1 77.1 71.85 Webmaster 88.9 88.5 88.6 86.6 77.38 Avg. Rank 2.25 4.25 2.12 2.62 3.62 Congressional 96.2 95.7 94.2 95.9 91.7 WiscBreast 96.6 97.1 96.8 97.2 98.2 Avg. Rank 1.71 3.00 4.00 3.00 3.14 Table 4: General Performance -F1 measure (%).
 NB=Naive Bayes, v-P= Voted Perceptron. ter (higher Median F1 and lower Avg. Rank) than non-voted Passive-Aggressive or non-voted ROMMA on both types of datasets. Passive-Aggressive typically presented better re-sults than ROMMA; and Balanced Winnow outperformed Positive Winnow in almost all tests. MBW outperformed all other online learners for NLP datasets, and also all other non-voted learners for non-NLP datasets.
 We compare MBW results to the batch learners SVM and Naive Bayes in Table 4. This Table illustrates F1 results along with their standard errors for SVM, Voted Perceptron (or v-P), MBW, v-MBW and Naive Bayes (or NB) learners. Similar to Table 3, the datasets are presented in two groups (NLP and non-NLP) and best results are indicated in bold. From Tables 3 and 4, it is important to observe that the MBW learner indeed reaches impressive performance num-bers in the NLP-like datasets, outperforming all other learn-ers  X  including SVM. The MBW performance in the NLP dataset is very encouraging, and a more detailed analysis of the behavior of this learner on NLP datasets can be found in [6].

In the non-NLP tasks, however, SVM shows much better results than all other learners and MBW is not competitive at all. In fact, the Voted Perceptron would probably be the best choice for single-pass online learning in this type of data. It is interesting that the Voted Perceptron performs so well in non-NLP tasks and so poorly in NLP-like datasets.
In general, non-voted Winnow variants perform better in NLP-like than in non-NLP datasets. This agrees with the general intuition that multiplicative updates algorithms handle well high dimensional problems with sparse target weight vectors [11].

Table 3 also presents the overall effect of voting over the two types of datasets. It is easy to observe that voting im-proves the performance of all online learners for non-NLP datasets. However, for the NLP-like datasets, the improve-ments due to averaging are not as obvious. For instance, on Balanced Winnow, a small improvement can be observed, particularly when the F1 values are high. On Positive Win-now and Passive-Aggressive, it is not clear if voting is ben-eficial. For ROMMA, voting visibly deteriorates the per-formance. For MBW, voting seems to causes a small per-formance deterioration. It is not clear to the authors the reasons why averaging does not improve performance for NLP tasks in the single-pass setup.

In summary, voting seems to be a consistent and powerful way to boost the overall performance of very distinct single-pass online classifiers for non-NLP tasks. In NLP tasks, voting does not seem to bring the same benefits. More de-tailed experiments, learning curves and graphical analyses can be found in [6]. Feature selection for NLP tasks is usually performed in batch mode. Examples of common metrics for feature se-lection are Information Gain and Chi-Square [12, 21]. Ex-tending such batch feature selection techniques to the on-line learning setting is not obvious. In the online setting the complete feature set is not known in advance. Also, it would be desirable to refine the model every time new examples are presented to the learner: not only by adding new meaningful features to the model, but also by deleting unimportant features that were previously selected. Adapt-ing the batch techniques to the online setting would be very expensive, since each score of each feature would need to be recalculated after every new example.

As previously seen, the MBW learner reaches very good performance in NLP tasks with a single learning pass through the training data. Here we propose a very simple and very fast online Feature Selection scheme called Ex-tremal Feature Selection (or EFS), based on the weights stored by the MBW learner. The idea is to rank the feature importance according to the difference (in absolute value) between its positive and negative MBW weights. More specifically, at each time t the importance score I of the feature j is given by I j t = u j t  X  v j t where u j t and v positive and the negative model weights for feature j .
After the scores I j t are computed, it would be expected that the largest values correspond to the most meaning-ful features to be selected at each time step. We would also expect that the lowest values of I j t correspond to the most unimportant features, prone to be deleted from the fi-nal model. In fact, a detailed analysis in the 20newsgroup dataset indicated that these low score features are typical stop words : e.g.,  X  X s X ,  X  X ou X ,  X  X hat X , and  X  X hey X .
In preliminary experiments, however, we observed an un-expected effect: performance is improved by selecting not only the highest I j t -valued features, but also a small num-ber of features with the lowest values. EFS uses not only the extreme top T features, but also a small number from the extreme bottom B . For instance, in order to select 100 features from a dataset, EFS would select 90% of these 100 features from the extreme top T and 10% from the extreme bottom B .
 The effectiveness of this idea can be seen in Figure 1. This figure illustrates the MBW test error rate for differ-ent numbers of features selected in the training set. More specifically, we first trained a MBW learner on the training set and then selected the features according to I j t -values and P . We then deleted the other features (non-selected) from the MBW model and used this final model as test probe. We used a random split of 20% of the 20newsgroup data as the test set, and the remaining as training examples. The quotient P = T T + B represents the fraction of the selected features with high-score I j t . For example, P =0 . 7 indicates that 70% of the features were selected from the top and 30% from the bottom; and P = 1 indicates that no low-score fea-tures were selected at all.

As expected, the performances in Figure 1 improve as P increases  X  since selecting more high score features trans-lates to better selection overall. This general behavior was also observed in all other NLP datasets. However, the con-dition P =0 . 9 seems to outperform the condition P =1, specially for non-aggressive feature selection (i.e., when the number of selected features is relatively large). This unex-pected behavior was also observed in other NLP datasets (see also Figure 2). It indicates that there is a small set Figure 1: EFS experiment on the 20newsgroup dataset. P = T T + B is the fraction of selected features using high I j t scores. of low score features that are very effective to the proposed online feature selection scheme.

Similar to Figure 1, Figure 2 shows test error rates in other datasets for different numbers of features selected in the training set. Again, 20% of the data was used as the test set and the quotient P = T K represents the fraction of the selected features with high-score I j t . Figure 2 also illustrates MBW performance using two popular batch feature selec-tion schemes: Information Gain (IG) and Chi-Square(CHI). For these two schemes, first a new training set containing only the selected features is created; then a MBW classifier learns a final model using the new training set.

Figure 2 reveals that EFS with P =0 . 9 has a perfor-mance comparable to IG and CHI. In one of the experi-ments (MovieReviews) the EFS performance was generally better than IG or CHI in almost all feature selection ranges. Reuters was the only dataset where the traditional methods outperformed EFS, but only for very aggressive feature se-lection  X  when only a small number of features is selected. Additional experiments can be found in [6].

We speculate that the importance of these low score features is related to a smoothing-like effect in the MBW learner. Recall that MBW uses a normalization preprocess-ing step that is susceptible to the number of non-zero features in the incoming example, and the low-score fea-tures are frequently found in most examples. A more detailed investigation of this issue is a topic of future re-search. Another obvious future research will be applying such technique to other learning algorithms.

EFS results are very encouraging. The ability to perform effective online learning and feature selection in the same framework can largely benefit systems constrained by lim-ited resources.
In this work we investigated the problem of single-pass on-line learning. This setting is particularly relevant when the system cannot afford several passes throughout the train-ing set X  X or instance, when dealing with massive amounts of data, or when memory or processing resources are restricted. To the best of our knowledge, this is the first comprehensive Figure 2: EFS experiments: Comparison with In-formation Gain and Chi-Square. comparison of online learners in the single-pass setting.
We proposed a new modification of the Balanced Winnow algorithm (MBW) that performs surprisingly well in NLP tasks for the single-pass setting, with results comparable and sometimes even better than SVM. We evaluated the use of averaging (a.k.a. voting) on several online learners, and showed that it considerably improves performance for non-NLP tasks. Averaging techniques have been evaluated in the past for the Perceptron algorithm, but not for Winnow, Passive-Aggressive, ROMMA or other mistake-driven online learners.

Finally, we proposed a new online feature selection scheme based on the new MBW algorithm. This scheme is sim-ple, efficient, and naturally suited to the online setting. We showed that the method is comparable to traditional batch feature selection techniques such as information gain. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA). Any opin-ions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not nec-essarily reflect the views of DARPA, or the Department of Interior-National Business Center (DOI-NBC). [1] E.Airoldi,W.W.Cohen,andS.E.Fienberg.
 [2] E. M. Airoldi and B. Malin. Data mining challenges [3] R. Bekkerman, A. McCallum, and G. Huang.
 [4] A. Blum. Empirical support for WINNOW and [5] V. R. Carvalho and W. W. Cohen. Learning to extract [6] V. R. Carvalho and W. W. Cohen. Notes on [7] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [8] W. Cohen, E. Minkov, and A. Tomasic. Learning to [9] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell. [10] K. Crammer, O. Dekel, S. Shalev-Shwartz, and [11] I. Dagan, Y. Karov, and D. Roth. Mistake-driven [12] G. Forman. An extensive empirical study of feature [13] Y. Freund, Y. Mansour, and R. E. Schapire. Why [14] Y. Freund and R. E. Schapire. Large margin [15] D. D. Lewis and M. Ringuette. A comparison of two [16] Y. Li and P. M. Long. The relaxed online maximum [17] N. Littlestone. Learning quickly when irrelevant [18] T. Mitchell. Machine Learning . Mcgraw-Hill, 1997. [19] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [20] F. Rosenblatt. The perceptron: A probabilistic model [21] Y. Yang and J. O. Pedersen. A comparative study on
