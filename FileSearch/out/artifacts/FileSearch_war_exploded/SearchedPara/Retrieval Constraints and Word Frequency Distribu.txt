 We first present in this paper an analytical view of heuris-tic retrieval constraints which yields simple tests to deter-mine whether a retrieval function satisfies the constraints or not. We then review empirical findings on word frequency distributions and the central role played by burstiness in this context. This leads us to propose a formal definition of burstiness which can be used to characterize probabil-ity distributions wrt this phenomenon. We then introduce the family of information-based IR models which naturally captures heuristic retrieval constraints when the underly-ing probability distribution is bursty and propose a new IR model within this family, based on the log-logistic distribu-tion. The experiments we conduct on three different col-lections illustrate the good behavior of the log-logistic IR model: it significantly outperforms the Jelinek-Mercer and Dirichlet prior language models on all three collections, with both short and long queries and for both the MAP and the precision at 10 documents. It also outperforms the InL2 DFR model for the MAP, and yields results on a par with it for the precision at 10.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Theory burstiness, retrieval constraints, log-logistic, IR models
If Information Retrieval (IR) on the web is dominated by systems learning ranking functions from log data, standard ad hoc IR is largely dominated by probabilistic systems with few parameters to set, as Okapi, the language models or the Divergence from Randomness models. These latter models are based on several probabilistic distributions and assump-tions which help their deployment in pratical situations. If these models are well founded from an information retrieval point of view (they satisfy the heuristic retrieval constraints given in [4] for example), the probability distributions they rely on yield in general a poor fit to empirical data. Thus, in the  X  X odel word frequency distributions to retrieve docu-ments X  X pproach, the first part (model word frequency distri-butions) is somehow neglected wrt the second part (retrieve documents) in most models.

We present in this paper a new IR model, based on proba-bility distributions fitting well empirical data, and satisfying heuristic retrieval constraints. To do so, we first explore the links between heuristic retrieval constraints and word fre-quency distributions. After proposing an analytical view of heuristic retrieval constraints which extends the work pre-sented in [4] and yields simple tests to determine whether a retrieval function satisfies the constraints or not, we review empirical findings on word frequency distributions and the central role played by burstiness in this context. This is the subject of Section 2. In Section 3, we introduce the fam-ily of information-based IR models and develop, within this family, a new IR model based on the log-logistic distribu-tion. In Section 4, we illustrate the good performance our model yields in IR when compared to language models and Divergence from Randomness models, and then conclude the paper in Section 5.

The notations we use throughout the paper are summa-rized in table 1.
We first present in this section an analytical version of heuristic retrieval constraints which underlie most IR mod-els. We then review several studies of word frequency dis-tributions, and emphasize the role played by burstiness in these studies. This section enables us to state a few facts concerning IR models deployed over text collections, facts that will help us in building a new IR model. h ( x d w , y d , z w ) Base retrieval function with
Following Fang et al. [4], who proposed formal definitions of heuristic retrieval constraints which can be used to assess the validity of an IR model, we introduce here analytical conditions a retrieval function should satisfy to be valid. We consider here retrieval functions, denoted RSV , of the form: where  X  is a set of parameters and where h , the form of which depends on the IR model considered, is assumed to be of class C 2 and defined over R +  X   X  R +  X   X  R +  X   X   X , where  X  represents the domain of the parameters in  X  and a is often the identity function. Language models [7], Okapi [5] and Divergence from Randomness [1] models as well as vector space models [6] all fit within the above form.

A certain number of hypotheses, experimentally validated, sustain the development of IR models. In particular, it is important that documents with more occurrences of query terms get higher scores than documents with less occur-rences. However, the increase in the retrieval score should be smaller for larger term frequencies, inasmuch as the differ-ence between say 110 and 111 is not as important as the one between 1 and 2 (the number of occurrences has doubled in the second case, whereas the increase is relatively marginal in the first case). In addition, longer documents, when com-pared to shorter ones with exactly the same number of oc-currences of query terms, should be penalized as they are likely to cover additional topics than the ones present in the query. Lastly, it is important, when evaluating the retrieval score of a document, to weigh down terms occurring in many documents, i.e. which have a high document/collection fre-quency, as these terms have a lower discrimination power. We formalize these considerations through the following four conditions: Conditions 1, 3 and 4 directly state that h should be increas-ing with the term frequency, and decreasing with the doc-ument length and the document/collection frequency. Con-ditions 1 and 2 (mentioned by Fang et al. [4]) state that h should be an increasing, concave function of the term fre-quency, the concavity ensuring that the increase in the re-trieval score will be smaller for larger term frequencies.
If IR models have to fulfill the above conditions, the most recent and widely used models also rely on word proba-bility distributions with their own specificities. In Okapi, for example, it is assumed that word frequencies follow a mixture of two Poisson distributions (2P), in both the rele-vant and irrelevant sets. The Divergence from Randomness (DFR) framework proposed by Amati and van Rijsbergen [1] makes use of several distributions, among which the geomet-ric distribution, the binomial distribution and Laplace law of succession play a major role. Language models are, for themselves, built upon the multinomial distribution, which amounts to consider binomial distributions for individual words.

Empirical findings on how words behave in text collections however suggest that none of the above distributions is ap-propriate for accurately describing word frequencies as they do not model burstiness (see Church and Gale, [2]). The term X  X urstiness X  X escribes the behavior of words which tend to appear in bursts, i.e., once they appear in a document, they are much more likely to appear again. Several emirical criteria have been proposed for burstiness. We propose here a general definition:
Definition 1. [General case] A distribution P is bursty iff the function g  X  defined by: is a strictly increasing function of x . A distribution which verifies this condition is said to be bursty. which translates the fact that, with a bursty distribution, it is easier to generate higher values of X once lower val-ues have been observed. The above definition can be used to characterize standard distributions wrt burstiness: The binomial, Poisson and Weibull distributions are not bursty; The geometric and exponential distributions are neutral wrt burstiness, the function g  X  being constant; The Pareto dis-tribution is bursty; The negative binomial can be bursty, neutral or non bursty depending on the parameter values (see for example [3] for this latter case).

From the above developments, one can state that: (a) IR models have to fulfill heuristic retrieval constraints stated in conditions 1 to 4, (b) Word frequency distributions should be bursty according to the above definitions, (c) Word fre-quency distributions used in standard IR models are usu-ally not bursty. The question which naturally follows from these findings is whether one can build an IR model based on bursty distributions and compliant with the heuristic re-trieval constraints. The next section is devoted to the pre-sentation of such a model.
Most IR models do not rely directly on the raw number of occurrences of words in documents, but rather on normal-ized versions of it. Language models for example use the relative frequency of words in the document and the collec-tion. DFR models usually adopt one of the following two term frequency normalizations ( c is a multiplying factor): In the remainder, we assume that one of these last two nor-malizations is used (even though our results apply to other normalization schemes as well).
Let us first consider here the family of retrieval models defined by: This ranking function corresponds to the mean information a document brings to a query (or, equivalently, to the average of the document information brought by each query term) and is similar to the Inf 1 part of DFR models. We will refer to models in this family as information-based IR models.
Prob ( X  X  t d w |  X  w ) is a decreasing function of t d w long as t d w is an increasing function of x d w and a decreasing function of y d (which, in practice, is the case), conditions 1 and 3 are satisfied for this family of models. Furthermore, condition 2 can be re-expressed, in the family of information-based IR models, as:
The following theorem states that, provided one chooses a bursty distribution, condition 2 is satisfied, so that IR models defined by equation 3 and based on bursty distribu-tions satisfy the second heuristic retrieval constraint given previously.
 Theorem 2. Let P be a probability distribution of class C . A necessary condition for P to be bursty is: The above development shows that an information-based IR model built upon a bursty distribution  X  X aturally X  sat-isfies the first three retrieval constraints given in section 2. We present such a model here based on the log-logistic dis-tribution. The log-logistic distribution is defined by: from which one can see that it is a bursty distribution. The information-based IR model built upon this distribu-tion takes the form: Following the general idea sustaining the Divergence from Randomness paradigm, r w is defined from the whole collec-tion and is assumed to be either F w N or n w N , i.e. r w the probability of observing w in a document, assuming that w is uniformly distributed in the collection. With these set-tings and the two normalization formulas we have retained (equations 1 and 2), it is easy to see that the log-logistic model satisfies the last heuristic retrieval constraints of sec-tion 2, for all the admissible values of x , y and z . To assess the validity of our model, we used standard IR collections, from two evaluation campaigns: ROBUST (TREC, trec.nist.gov), CLEF03 AdHoc Task, GIRT (CLEF Domain Specific Task, www.clef-campaign.org). We evalu-ated the log-logistic model against the LM model, with both Jelinek-Mercer and Dirichlet Prior smoothing, as well as against the standard InL2 DFR model. For each dataset, we randomly split queries in train and test (half of the queries are used for training, the other half for testing). We per-formed 10 such splits on each collection. The results we provide for the Mean Average Precision (MAP) and the pre-cision at 10 documents is the average of the values obtained over the 10 splits. The parameters of the different models are optimized (respectively for the MAP and the precision at 10) on the training set. The performance is then mea-sured on the test set. To compare the different methods, a two-sided t-test (at the 0.05 level) is performed to assess the significance of the difference measured between the meth-ods. All our experiments were carried out thanks to the Lemur Toolkit (www.lemurproject.org). In all the following tables, ROB-t represents the robust collection with query ti-tles only, ROB-d the robust collection with query titles and description fields (and similarly for CLEF-t and CLEF-d ). The GIRT collection is particular as all the queries are just made up of a single sentence. The version of the log-logistic model used in all our experiments is based on r w = n w N the length normalization corresponding to equation 2. We refer to this model as the LGD model. As the parameter c in equation 2 is not bounded, we have to define a set of possible values from which to select the best value on the training set. We make use of the typical range proposed in works on DFR models, which also rely on equation 2 for document length normalization.

As the smoothing parameter of the Jelinek-Mercer lan-guage model is comprised between 0 and 1, we use a regular grid on [0 , 1] with a step size of 0 . 05 in order to select, on the training set, the best value for this parameter. Table 2 shows the comparison of our model (LGD) with the Jelinek-Mercer language model (LM). On all collections, on both short and long queries, the LGD model significantly outperforms the Jelinek-Mercer language model.

For the Dirichlet prior language model, we optimized the smoothing parameter from a set of typical values, defined Table 3 shows the results of the comparison between LGD and the Dirichlet prior language model (LM). These results parallel the ones obtained with the Jelinek-Mercer language model, except for the ROB collection with short queries where the Dirichlet prior language model outperforms the log-logistic model (the difference being significant for the precision at 10 only). On the other collections, with both short and long queries and on both the MAP and the preci-Table 2: LM-Jelinek-Mercer versus Log-Logistic af-ter 10 splits: bold indicates best performance,  X  sta-tistical significance MAP ROB-d ROB-t GIRT CLEF-d CLEF-t LM 26.0 20.7 40.7 49.2 36.5 LGD 27.2  X  22.5  X  43.1  X  50.0  X  37.5  X  P10 ROB-d ROB-t GIRT CLEF-d CLEF-t LM 43.8 35.5 67.5 33.0 26.2
LGD 46.0  X  38.9  X  69.4  X  33.6  X  26.6  X  sion at 10, the log-logistic model outperforms the Dirichlet prior language model, the difference being significant in most cases. Again, this shows that the log-logistic model consis-tently outperforms the Dirichlet prior language model. Table 3: LM-Dirichlet versus Log-Logistic after 10 splits: bold indicates best performance,  X  statistical significance MAP ROB-d ROB-t GIRT CLEF-d CLEF-t LM 27.1 25.1 41.1 48.5 36.2 LGD 27.4  X  25.0 42.1  X  49.7  X  36.8  X  P10 ROB-d ROB-t GIRT CLEF-d CLEF-t LM 45.6 44.7  X  68.6 33.8 28.4 LGD 46.2  X  44.4 69.0 34.5  X  28.6
To compare our model with DFR ones, we chose, in this latter family, the InL2 model, based on the geometric distri-bution and Laplace law of succession. This model has been used with success in different works ([1, 3] for example). As this model also relies on equation 2 for document length normalization, we make use here of the same set of possible values for c as the one used for the LGD model, namely: to note that InL2 makes use of discrete distributions (geo-metric and Laplace) over continuous variables ( t d w ) and is thus theoretically flawed. This is not the case of the LGD model which makes use of a continuous distribution, the log-logistic one. Table 4 provides the results of the compar-ison between the LGD and the InL2 models. This time, the results are more contrasted than with the language model. In particular, for the precision at 10, both models perform similarly (LGD being significantly better on GIRT whereas InL2 is significantly better on ROB with long queries, the models being on a par in the other cases) For the MAP, the LGD model outperforms the InL2 model as it is significantly better on ROB (for both sort and long queries) and GIRT, and on a par on CLEF. These results are all the more so interesting that the log-logistic model is simpler than InL2: it directly relies on an information measure (see equation 3) without the re-normalization ( Inf 2 part) used in DFR mod-els.
We have first presented in this paper an analytical view of heuristic retrieval constraints which extends the work pre-sented in [4] and yields simple tests to determine whether a retrieval function satisfies the constraints or not. We have then reviewed empirical findings on word frequency distri-butions and the central role played by burstiness in this context. This has led us to propose a formal definition Table 4: INL versus Log-Logistic after 10 splits: bold indicates best performance,  X  statistical signif-icance MAP ROB-d ROB-t GIRT CLEF-d CLEF-t INL 27.7 24.8 42.5 47.7 37.5 LGD 28.5  X  25.0  X  43.1  X  48.0 37.4 P10 ROB-d ROB-t GIRT CLEF-d CLEF-t INL 47.7  X  43.3 67.0 33.4 27.3
LGD 47.0 43.5 69.4  X  33.3 27.2 of burstiness which can be used to characterize probabil-ity distributions wrt this phenomenon. We have then in-troduced the family of information-based IR models which naturally captures heuristic retrieval constraints when the underlying probability distribution is bursty. In particular, theorem 2 guarantees that the concavity constraint is satis-fied for bursty distributions, whereas the form of the family guarantees the other constraints when the length normaliza-tion function is increasing in x d w and decreasing in y d is the case for all the normalization functions we know of. We have then proposed a new IR model within this family, based on the log-logistic distribution. The experiments we have conducted on three different collections illustrate the good behavior of the log-logistic IR model: this model signif-icantly outperforms the Jelinek-Mercer and Dirichlet prior language models on all three collections, with both short and long queries and for both the MAP and the precision at 10 documents. The log-logistic model also outperforms the InL2 DFR model for the MAP, and yields results on a par with it for the precision at 10. [1] G. Amati and C. J. V. Rijsbergen. Probabilistic models [2] K. W. Church and W. A. Gale. Poisson mixtures. [3] S. Clinchant and  X  E. Gaussier. The BNB distribution for [4] H. Fang, T. Tao, and C. Zhai. A formal study of [5] S. E. Robertson and S. Walker. Some simple effective [6] G. Salton and M. J. McGill. Introduction to Modern [7] C. Zhai and J. Lafferty. A study of smoothing methods
 Ste  X  phane Clinchant  X  Eric Gaussier Abstract We first present in this paper an analytical view of heuristic retrieval constraints which yields simple tests to determine whether a retrieval function satisfies the constraints or not. We then review empirical findings on word frequency distributions and the central role played by burstiness in this context. This leads us to propose a formal definition of burstiness which can be used to characterize probability distributions with respect to this phenomenon. We then introduce the family of information-based IR models which naturally captures heuristic retrieval constraints when the underlying probability distribution is bursty and propose a new IR model within this family, based on the log-logistic distribution. The logistic IR model: It significantly outperforms the Jelinek-Mercer and Dirichlet prior lan-guage models on most collections we have used, with both short and long queries and for both the MAP and the precision at 10 documents. It also compares favorably to BM25 and has similar performance to classical DFR models such as InL2 and PL2.
 Keywords Retrieval constraints Burstiness Information retrieval theory Log-logistic distribution 1 Introduction If Information Retrieval (IR) on the web is dominated by systems learning ranking functions from log data, standard ad hoc IR is largely dominated by probabilistic systems with few parameters to set, as Okapi, the language models or the Divergence from Randomness assumptions which help their deployment in practical situations. If these models are well models.

We present in this paper a new IR model, based on probability distributions fitting well empirical data, and satisfying heuristic retrieval constraints. To do so, we first explore the links between heuristic retrieval constraints and word frequency distributions. After pro-sented in [ 9 ] and yields simple tests to determine whether a retrieval function satisfies the constraints or not, we review empirical findings on word frequency distributions and the language models and Divergence from Randomness models. We then discuss several aspects of our approach in Sect. 6 , prior to conclude.

The notations we use throughout the paper are summarized in Table 1 . 2 IR models and word frequency distributions We first present in this section an analytical version of heuristic retrieval constraints which underlie most IR models. We then review several studies of word frequency distributions, and emphasize the role played by burstiness in these studies. This section thus introduces a few facts concerning IR models deployed over text collections, facts that will help in building a new IR model.
 2.1 Heuristic retrieval constraints Following Fang et al. [ 9 ], who proposed formal definitions of heuristic retrieval constraints functions, denoted RSV , of the form: where x is a set of parameters and where h , the form of which depends on the IR model considered, is assumed to be of class C 2 and defined over R  X  R  X  R  X  X ; where X represents the domain of the parameters in x and a is often the identity function. 1 Language models [ 19 ], Okapi [ 15 ] and Divergence from Randomness [ 2 ] models as well as normalization retrieval formula [ 17 ], x = ( s , m , N ) and: where I is an indicator function which equals 1 when it argument is true and 0 otherwise. A certain number of hypotheses, experimentally validated, sustain the development of IR models. In particular, it is important that documents with more occurrences of query terms retrieval score should be smaller for larger term frequencies, inasmuch as the difference between say 110 and 111 is not as important as the one between 1 and 2 (the number of occurrences has doubled in the second case, whereas the increase is relatively marginal in the first case). In addition, longer documents, when compared to shorter ones with exactly the same number of occurrences of query terms, should be penalized as they are likely to documents, i.e. which have a high document/collection frequency, as these terms have a lower discrimination power. Fang et al. [ 9 ] proposed formal criteria to account for these phenomena. We recall here these criteria and provide an analytical version of them which leads to conditions on h which can be easily tested (the names of the different criteria are directly borrowed from Fang et al. [ 9 ]).
 Criterion 1: TFC1 Let q = w a query with only word w . Suppose that y d 1 = y d 2 .if x w [ x d 2 w ; then RSV ( d 1, q ) [ RSV ( d 2, q ) (Fang et al.).
 dition is: Criterion 2: TFC2 Let q = w a query with only word w . Suppose that y d 1 = y d 2 = y d 3 et x RSV ( d 2, q ) (Fang et al.). ing. A sufficient condition is: Criterion 3: LNC1 Let q = w a query and d 1, d 2 two documents. If, for a word w 0 6 2 (Fang et al.). condition is: x w 2  X  x d 1 w 2  X  x d 2 w 2 .If idf ( w 1) C idf ( w 2) et x (Fang et al.).

A special case of TDC corresponds to the case where w 1 occurs only in document d 1 and w 2 only in d 2. In such a case, the constraints can be written as: x Proposition 4 Criterion 5: LNC2 Let q a query. V k [ 1, if d 1 and d 2 are two documents such that y et al.).
 Proposition 5 LNC 2 ( ) Criterion 6: TF-LNC Let q = w a query with only word w .if x d 1 w [ x d 2 w et y d 1  X  y d 2  X  x d 1 w x d 2 w ; then RSV ( d 1, q ) [ RSV ( d 2, q ) (Fang et al.).
 Proposition 6 TF LNC ( )
Conditions 1, 3 and 4 directly state that h should be increasing with the term frequency, and decreasing with the document length and the document/collection frequency. Condi-tions 1 and 2 (mentioned by Fang et al. [ 9 ]) state that h should be an increasing, concave score will be smaller for larger term frequencies. Lastly, conditions 5 and 6 regulate the x and y . They allow to adjust the functions h satisfying conditions 1, 2, 3 and 4. In the remainder, we will refer to conditions 1, 2, 3 and 4 as the form conditions and conditions 5 and 6 as the adjustment conditions . 2.2 Word frequency distributions If IR models have to fulfill the above conditions, the most recent and widely used models also assumed that word frequencies follow a mixture of two Poisson distributions (2P), in both the relevant and irrelevant sets. The Divergence from Randomness (DFR) framework proposed geometric distribution, the binomial distribution and Laplace law of succession play the major role. Language models are, for themselves, built upon the multinomial distribution, which amounts to consider binomial distributions for individual words.

Empirical findings on how words behave in text collections however suggest that none of the above distributions is appropriate for accurately describing word frequencies. Church and Gale ([ 6 ]) compared the binomial and Poisson distributions with mixtures of experimentally that words tends to reappear in documents [ 5 ], a phenomenon referred to as positive adaptation or burstiness .

The term  X  X  X urstiness X  X  describes the behavior of words which tend to appear in bursts, i.e., once they appear in a document, they are more likely to appear again. The notion of burstiness is similar to the one of aftereffect of future sampling [ 10 ], which describes the fact that the more we find a word in a document, the higher the expectation to find new occurrences. Burstiness has recently received a lot of attention from different communities. Madsen [ 13 ], for example, proposed to use the Dirichlet Compound Multinomial (DCM) distribution in order to model burstiness in the context of text categorization and clustering. Elkan [ 8 ] then approximated the DCM distribution by the EDCM distribution, for which learning time is faster, and showed the good behavior of the model obtained on different text clustering experiments. A related notion is the one of preferential attachment [ 3 , 4 ] often used in large networks, such as the web or social networks. It conveys the same idea: the more we have, the more we will get . In the context of IR, Xu and Akella [ 18 ] studied the use of a DCM model within the Probability Ranking Principle, and argue that multi-nomial distributions alone are not appropriate for IR within this principle (quoting):
Because the multinomial distribution assumes the independence of the word repet-itive occurrences, it results in a score function which incorporates undesired linearity in term frequency. To capture the concave property and penalize document length in the score function, a more appropriate distribution should be able to model the dependency of word repetitive occurrences.

The dependency of word repetitive occurrences is directly linked to burstiness. More quantity: where E P denotes the expectation with respect to P . The drawback of this measure is that it does not give a clear understanding on whether a given distribution accounts for burstiness or not. Clinchant and Gaussier [ 7 ] introduced the following definitions (slightly simplified here for clarity X  X  sake) in order to characterize discrete distributions which can account for burstiness: Definition 1 ( Discrete case ) A discrete distribution P is bursty iff for all integers ( n , n ), n 0 C n : We generalize this definition to the continuous case as follows: Definition 2 ( General case ) A distribution P is bursty iff the function g e defined by: is a strictly increasing function of x for all e [ 0. A distribution which verifies this con-dition is said to be bursty .

This definition directly translates the fact that, with a bursty distribution, it is easier to generate higher values of X once lower values have been observed. Armed with these definitions, one can characterize standard distributions wrt burstiness:  X  The binomial and Poisson distributions are not bursty,  X  The geometric and exponential distributions are neutral wrt burstiness, i.e. the function  X  The Pareto distribution is bursty. 2.3 Summary We can sum up the different points developed in this section as follows: 1. IR models have to fulfill heuristic retrieval constraints stated in conditions 1 X 6, 2. Word frequency distributions should be bursty according to the above definitions, 3. Word frequency distributions used in standard IR models are usually not bursty. The question which naturally follows from these findings is whether one can build an constraints. The next section is devoted to the presentation of such a model. Before that, we analyze the Divergence from Randomness framework with respect to the retrieval constraints. 3 The DFR framework The Divergence from Randomness (DFR) framework proposed by Amati and van informative content provided by the occurrences of terms in documents, denoted Inf 1 ,a following retrieval function:
We now review the two normalization principles behind DFR models. 3.1 The second normalization principle The second normalization principle aims at normalizing the number of occurrences of words in documents by the document length, as a word is more likely to have more occurrences in a long document than in a short one. The different normalizations considered in the literature transform raw number of occurrences into positive real numbers. Language models for example use the relative frequency of words in the document and the collection. Other classical term normalization schemes include the verbosity.

DFR models usually adopt one of the two following term frequency normalizations ( c is a multiplying factor): The important point about the second normalization principle is that, to be fully compliant should be continuous distributions as the variables considered are continuous. 2 This is not the case for DFR models proposed so far which rely on discrete distributions. 3.2 The first normalization principle t  X  x w ; y d  X  (normalized) occurrences of term w in document d according to parameters h w which are estimated or set on the basis of a random distribution of w in the collection. If collection, and w is important to describe the content of d . In this case, Inf 1 will be high. principle? model, the Bose-Einstein model, which can be approximated by a geometric distribution, (denoted I ( n e )). For the last four models, Inf 1 takes the form: where the first line corresponds to the geometric distribution, and the second one to I(n), expected number of documents containing term w ). We assume in the remainder that t  X  x w ; y d  X  is given either by equation 2 or 3. The conclusions we present below are the same in both cases.

Were we to base a retrieval function on the above formulation of Inf 1 only, our model would be defined by: where the first line still corresponds to the geometric distribution, and the second one to conditions 1, 3 and 4 and that the model for the geometric distribution verifies conditions 1 and 3, but only partly condition 4, as the derivative can be positive for some values of to define a valid IR model. 3 One can thus wonder whether Inf 2 serves to make the model compliant with condition 2. We are going to see that this is indeed the case.
Two quantities are usually used for Inf 2 in DFR models: the normalization L or the normalization B . They both lead to the following form: where a is independent of t  X  x d w ; y d  X  :
Thus integrating Inf 2 in the previous models gives: But: with b [ 0, which shows that the models are now compatible with condition 2. The above development thus explains why the Inf 1 models considered previously need be resized with an Inf 2 model which can take into account burstiness (or, equivalently, the aftereffect of sampling). However, the question remains as whether Inf 1 alone can be used to design an interesting IR model. 4 Information-based IR models Several models for IR and textual collections rely on the information brought by a term in a document. In particular, several researchers, Harter [ 11 ] being one of the first ones, have document deviates from its average distribution in the collection, the more likely is this word significant for the document considered. We make use of this notion, underlying DFR models, to define information-based IR models. Indeed, we consider here the family of IR models satisfying the following equation: equivalently, to the average of the document information brought by each query term) and information-based IR models.
 functions used in IR), conditions 1 and 3 are satisfied for this family of models. Furthermore, condition 2 can be re-expressed, in the family of information-based IR models, as: provided one chooses a bursty distribution, condition 2 is satisfied for information-based IR models, so that IR models defined by Eq. 4 and based on bursty distributions satisfy conditions 1, 2 and 3 of the previous section.
 Theorem 1 Let P be a probability distribution of class C 2 . A necessary condition for P to be bursty is: Thus, IR models defined by Eq. 4 and based on bursty distributions satisfy conditions 1, 2 and 3 of the previous section, the concavity property (condition 2) being directly related to condition as well as the adjustment conditions (i.e. conditions 4, 5 and 6). 4.1 Log-logistic distribution Having presented the connection between burstiness and heuristic retrieval constraints for information-based IR models, we now turn to the log-logistic distribution. Following binomial distribution in the context of text modeling. They showed that this distribution was not appropriate for IR as it relies on two parameters. They then assumed a uniform Beta prior distribution over one of the parameters, leading to a distribution they refer to as distribution is that it is a discrete distribution and cannot be used for modeling t d w : How-Figure 2 shows the density function of the log-logistic distribution for b = 1 and different values of h . Setting b to 1, one obtains: 8 x 2 R  X  shows that the log-logistic is bursty:
Finally, using this distribution in the information-based family of IR models leads to the following retrieval function: Following the general idea sustaining the Divergence from Randomness paradigm, h w h w represents the probability of observing w in a document, assuming that w is uniformly distributed in the collection. With these settings, it can be shown that the above retrieval constraints.
We are thus now armed with a simplified DFR model, relying solely on Inf 1 , which is compatible with the theoretical framework we have developed: our model is based on a continuous distribution that verifies the conditions of retrieval heuristic constraints. We now need to experimentally validate the fact that this model behaves as more want to show a connection with the language modeling approach to IR. 4.2 Relation to language models Let L be the number of tokens in the collection. Following [ 19 ], the scoring formula for a language model using Jelinek-Mercer smoothing can be written as: where k is the Jelinek-Mercer smoothing parameter and s  X  k 1 k . Using the retrieval for-mula introduced previously with h w  X  F w N and the length normalization given by Eq. 2 ,we have: Given that F w N  X  m F w L , Eq. 7 is equivalent to Eq. 8 . The LM model with Jelinek-Mercer smoothing can thus be seen as an information-based model making use of a log-logistic distribution, with a particular length normalization, namely the one given by Eq. 2 , and a particular setting of h w .

In the language modeling approach to IR, one starts from term distributions estimated at the document level, and smoothed by the distribution at the collection level. In contrast, the sustaining these two approaches, the above development shows that they can be reconciled through appropriate word distributions, in particular the log-logistic one. Lastly, the above smoothing. A theory for relating these two elements remains however to be established. 4.3 The LGD model A choice has to be made for the log-logistic distribution used within the information-based family of IR models, concerning the document length normalization and the value for the parameter h w . The previous section provides a possible choice for these elements. We will, however, not rely on this choice but will rather consider, in the remainder of the paper, the model defined by the following elements: 1. Document length normalization: t d w  X  x d w log 1  X  c m y 2. t d w are distributed according to a log-logistic distribution with b = 1 and h w  X  N w N 3. The retrieval function corresponds to Eq. 6 , which takes the form:
In other words, we choose the second term frequency normalization of DFR models and the document frequency as the parameter of the word frequency probability distributions. We will refer to this model as LGD . 5 Experimental validation the validity of our model: ROBUST (TREC), CLEF03 AdHoc Task, GIRT (CLEF Domain Specific 2004 X 2006). Table 2 gives the number of documents ( N ), number of lections. For the ROBUST collection, we used standard Porter stemming. For the CLEF03, GIRT, we used lemmatization, and an additional decompounding step for the GIRT collection which is written in German. In all the following tables, ROB-t rep-query titles and description fields, CLEF-t represent titles for the CLEF collection, CLEF-d queries with title and descriptions. The GIRT queries are just made up of a single sentence. 5.1 Empirical fit to observed data We illustrate here the fact that the log-logistic distribution, unlike others like the Poisson this fit.
 TREC collection. For each selected term, we want to compare two candidate distributions modeling the term frequencies in the documents, namely the Poisson and Log-Logistic according to:  X  Poisson: ^ h w  X  F w N  X  Log-Logistic: ^ h w  X  F w N
We then used a standard Chi-Square test. For each selected word w and document intervals corresponds roughly to low, medium and high frequency. The number of observations falling into each interval constitutes statistics that the Chi-Square compares to an expected number predicted by the assumed distribution. For each selected term, we hypothesis. 4 To display the results, we first ranked the selected terms by their frequency plotted the term rank against the log of the Chi-Square statistics for both the Poisson and Log-Logistic distributions. Figure 3 shows the log of the Chi-square statistics against the term rank for the  X  X obust X  TREC collection. One dot with coordinate ( x , y ) on the graph corresponds to a given word in the collection, where x is the term rank and y is the log upper critical value for Chi-square test at the 0.05 confidence level.
Concerning the Poisson plot, there are 2 main clouds of points. The upper left area can be explained by words from the interval [10,100): this is an extremely unlikely event under a Poisson distribution with a very small mean (ex: 0.05). The second area, which looks like a thick band, corresponds to words from the first two intervals only. As one can note, the fit provided by the BNB/log-logistic distribution is good inasmuch as the values obtained by the Chi-square statistics are small. These distributions can thus well explain the behavior of words in all the frequency ranges. The same does not hold for the Poisson, for which large values are observed over all the frequency ranges, many words getting a value above the upper critical value. 5.2 Comparison with language models We evaluated the LGD model against the LM model, with both Jelinek-Mercer and (half of the queries are used for training, the other half for testing). We performed 10 such splits on each collection. The results we provide for the Mean Average Precision (MAP) and the precision at 10 documents are averaged over the 10 splits. The parameters of the different models are optimized (respectively for the MAP and the precision at 10) on the training set. The performance is then measured on the test set. To compare the different methods, a two-sided t-test (at the 0.05 level) is performed to assess the significance of the difference measured between the methods.

For the LGD model, as the parameter c in Eq. 3 is not bounded, we have to define a set of possible values from which to select the best value on the training set. We make use of the typical range proposed in works on DFR models, which also rely on Eq. 3 for docu-ment length normalization.

The set of values we retained is: As the smoothing parameter of the Jelinek-Mercer language model is comprised between 0 training set, the best value for this parameter. Table 3 shows the comparison of the LGD model (LGD) with the Jelinek-Mercer language model (LM). On all collections, on both short and long queries, the LGD model significantly outperforms the Jelinek-Mercer lan-guage model. This is an interesting finding as the complexity of the two models is the same (in a way, they are both conceptually simple). As the results displayed are averaged over 10 Mercer language model and thus yields a more robust approach to IR.

In order to assess the relative behaviors of the log-logistic and Jelinek-Mercer models wrt to their parameter ( k for the Jelinek-Mercer model and c for the log-logistic one), we display in Fig. 4 the MAP scores obtained with different values of these parameters, c being set to c  X  k 1 k , which allows one to compare the two models for any k in [0;1]. As one can note, with the exception of small values of k , the log-logistic model dominates the Jelinek-Mercer model, which again shows that the log-logistic model is consistently better than the Jelinek-Mercer one.

For the Dirichlet prior language model, we optimized the smoothing parameter from a set of typical values, defined by: Table 4 shows the results of the comparison between LGD and the Dirichlet prior prior language model outperforms the LGD model (the difference being significant for the precision at 10 only). On the other collections, with both short and long queries and on language model, the difference being significant in most cases. Again, this shows that the LGD model consistently outperforms the Dirichlet prior language model. 5.3 Comparison with BM25 We adopt the same methodology to compare information models with BM25. We choose only to optimize the k 1 parameter of BM25 among the following values: {0.3, 0.5, 0.8, 1.0, implemented in Lemur (0.75 and 7). Table 5 shows the comparison of the LGD model with lections out of 5 for P10) or on par with Okapi BM25. Overall, the LGD model outper-forms Okapi BM25. 5.4 Comparison with DFR models model, based on the geometric distribution and Laplace law of succession. This model has document length normalization, we make use here of the same set of possible values for c as the one used for the LGD model, namely: It is however interesting to note that InL2 makes use of discrete distributions (geometric case of the LGD model which makes use of a continuous distribution, the log-logistic one.
Table 6 provides the results of the comparison between the LGD and the InL2 cantly better on GIRT whereas InL2 is significantly better on ROB with long queries, the models being on a par in the other cases). For the MAP, the LGD model outperforms the GIRT, and on a par on CLEF. These results are all the more so interesting that the log-Appendix B a comparison, provided by one reviewer, between the LGD model and last model are on par with the models LGD and PL2 (another DFR model). If DFRee is advantage. Parameter-free versions of LGD need be determined, maybe along the line used to derive DFRee. 6 Discussion The log-logistic model we have introduced is compliant with the heuristic retrieval con-account for burstiness. As we have noted before, this model bears strong similarities with DFR ones. The Divergence from Randomness (DFR) framework proposed by Amati and terms in documents, a quantity which is then corrected by the risk of accepting a term as a occurrences by the length of a document ( second normalization principle ). The informative overall IR model is then defined as a combination of Inf 1 and Inf 2 : This latter form shows that DFR models can be seen as information models, as defined by Eq. 4 , with a correction brought by the Inf 2 term, and with the inappropriate use of discrete distributions for modeling continuous variables. With this in mind, we can see the log-logistic model as a simplified DFR model, without the correction through the first nor-malization principle advocated by Amati and van Rijsbergen (this principle aims at justifying similarly to the InL2 DFR model in our experiments. The use of an appropriate distribution, able to model burstiness, is thus fully justified for this class of models.

Moreover, as we showed in Sect. 4.2 , the Jelinek-Mercer model can also be derived from a log-logistic model. However, the Jelinek-Mercer language model and the LGD model differ on the following points: 1. The term frequency normalization; 2. The parameter h w ; 3. The theoretical framework they fit in.
We want to stress an important point: it is because we adopted a new theoretical framework, the information-based family, that we could easily use other term frequency normalizations or settings of h w . In fact, a language model with the same term frequency normalization as LGD is clearly not straightforward to obtain in the language modeling approach to IR when using multinomial distributions to model documents. We know of no way so far to do so.

As we mentioned previously, other works have tried to model burstiness to come up wrt this phenomenon, and hence choose appropriate distributions in a more informed constraint (condition 2 of Sect. 2 ) for the family of information models. Indeed, because of relying on bursty distributions. The LGD model we finally arrive at is thus well founded theoretically. As we have seen, it also outperforms the Jelinek-Mercer and Dirichlet prior language models on most of the collections we have used in our experiments. 7 Conclusion We have in this paper first introduced an analytical characterization of heuristic retrieval constraints and reviewed several DFR models wrt this characterization. This review compliant with retrieval constraints. We have then introduced a new model based on the simplified model contained, as a special case, the standard language model with Jelinek-DFR and language modeling approaches to IR.

We have then reviewed empirical findings on word frequency distributions and the definition of burstiness which can be used to characterize probability distributions wrt this phenomenon. We have then introduced the family of information-based IR models which bution is bursty. In particular, theorem 4 guarantees that the concavity constraint is sat-which is the case for all the normalization functions we know of. We have then proposed a new IR model within this family, based on the log-logistic distribution.

The experiments we have conducted on three different collections illustrate the good behavior of the LGD model: this model significantly outperforms the Jelinek-Mercer and Dirichlet prior language models on most collections, with both short and long queries and for both the MAP and the precision at 10 documents. The LGD also yields results similar to DFR ones, while being simple. Future work will investigate an extension of information framework we have developed.
 Appendix A: Proof of Theorem 1 condition for P to be bursty is: Proof Let P be a continuous probability distribution of class C 2 . V y [ 0, the function g y defined by: is increasing in x (by definition of a bursty distribution).
 using a Taylor expansion of F ( x ? y ), we have: g , we get: Appendix B: Comparison with DFRee and PL2 We display here results provided by one reviewer, whom we gratefully thank, on a comparison between the LGD model and the Terrier X  X  parameter-free DFRee and PL2 models. As one can note, all these models perform similarly.
 References
