 Bilingual dictionaries are widely used in people X  X  daily life and many fields of computer science, such as machine translation[3], cross language information re-trieval[15] and so on. Traditional dictionaries are built manually with human annotation and revision which is expensive, time-consuming and difficult to up-date. Recently many researchers have been mining bilingual dictionaries from web ([4],[8],[14]).
 find the bilingual text, such as the parenthetical bilingual sentences in Figure 1 and the well formed bilingual text in Figure 2, and then use a method to find the correct matching of terms or phrases between two languages in the text. For instance, from Figure 1 and Figure 2 the English-Chinese translation of (socialism,  X  X  X  X  )  X  X  communism,  X  X  X  X  X  ,  X  commissary in charge of studies  X  X  X  X  X  X  X  etc. should be the right entries selected by the method. Clearly how efficiently and precisely the method can find the correct matching is a crucial issue.
 focus on designing better algorithms for sentence segmentation, word alignment, or on complex NLP (Natural Language Processing) models using machine learn-ing techniques etc([6],[19],[16]). However, the efficiency of these methods are affected while data is too big, and many experiments demonstrated that the tra-ditional improving is not as effective as expected ([11],[18]). A 2001 study([1]) showed that the worst algorithms performed better than the best algorithms if they were trained with a larger word database. Namely, for a given problem, the richness of data plus simple algorithms could alternatively solve the problem well or better than complex algorithms. This is particularly the case for Web study. As a matter of fact, in recent years many researchers have turned to works on how to use Web data ([2],[7],[9],[10],[12], [22]) to solve interesting problems. D ependent and D ata B ased Method (SDDB) for Constructing Bilingual Dic-tionaries from the Web based on our following observations to the Chinese Web data -as billions of web pages and huge amount of parenthetical bilingual and well formed bilingual text on the Web are available, we can assume the following. tionaries. Thus, it is realistic to use the Web data alone to build a bilingual dictionary and using no additional recourse. Taking the terms inside parenthesis from the parenthetical bilingual sentences can form a quality corpus, which con-tains single words, phrases and even short sentences, for building dictionaries. On Chinese Web, this is particularly true for some popular languages such as English. In addition, from the Web we can also mind the most up-to-date OOV (Out of Vocabulary) terms not covered by traditional dictionaries.
 tences to form the entries of dictionaries. For every term translation, its frequency i.e. the number of its occurrences in parenthetical bilingual sentences on the Web can be collected. In the case when one term, such English word tea, has many extracted Chinese translations such as (  X  X  X  X  X  X  X  X  X  X  X  ), we can simply select the right one by frequency ranking. That is, the more frequent the translation is used on the Web, the more likely it is right. We believe this is a rule when the data for counting the frequency is big enough to cover the terms of dictionaries. based on that the used web pages contains the right bilingual sentences and the bilingual sentences appear frequent, and we have no requirement for the quality of the web pages, which makes the procedure of SDDB more convenient. gual dictionaries from the Web. However, these works are quite different from SDDB in principle. First, they require the support of other language resources. Moreover, their focus is on the improvement of algorithms rather than on the use of big data. For example, [4] and [14] both used segmentation for Chinese and alignment model when they extract translation pairs. [4] used additional Chinese to English bilingual dictionary to determine translation units and used super-vised learning to build models for transliterations and translations separately. [14] used an unsupervised learning method, whose complexity of computing link score to deal with multi-word alignment is O( N 2 M 2 ), where N is the number of sentence pairs, and M is the number of words each sentence contains. [8] extracted translation from well-formed bilingual texts. They used a linking algo-rithm which need additional dictionaries and SVM classifier to identify potential translation pairs(seeds) and then extracted bilingual data. To our knowledge, the complexity of SVM is at least O( N 2 ), where N is the number of pairs. These methods mainly focus on the accuracy while overlook the efficiency.
 on big data from the Web. It can be characterized by (1) fully automatic. With-out human intervention, it covers the whole process for building a bilingual dictionary, including the construction of corpus, extraction and frequency com-puting of term translations, and noise elimination. (2) Simple. The algorithm of SDDB is O(N) where N is the number of bilingual sentences. This linear property makes SDDB very scalable. (3) Language independent. Without using language concerned sentence segmentation and word alignment, the algorithm of SDDB needs little knowledge of languages. Thus, so long as the data is big, SDDB is easy to be adapted to other languages with different characters in addition to the English-Chinese dictionary example presented in this paper, because we need to distinguish the two languages using characters.
 will give the full procedure of SDDB step by step. Section 3 will describe the algo-rithms of SDDB in detail. The testing results of the quality of an English-Chinese dictionary built through SDDB will be shown and compared with commercial dictionaries in Section 4. Section 4 also includes the result of scalability experi-ment of SDDB. Related work is listed in Section 5 followed by our conclusion. First we will introduce some concepts which will be used in the following sections. 1. parentheses sentence 2. sentence database 3. C2E Sentenc . e the parentheses sentence with Chinese terms in parentheses 4. E2C Sentence the parentheses sentence with Chinese texts outside paren-5. Chinese-corpus the repository formed by the Chinese terms of all C2E 6. prefix when traversing the text before parentheses from right to left starting 7. entry each prefix and English term of an E2C sentence form an entry. 8. candidate entry the entry whose Chinese and English terms are correct 9. entry frequency for an entry e , the entry frequency refer the number of As shown in Procedure SDDB, Line 2 extracts sentences and Line 3 con-structs Chinese-corpus, Lines 4-13 count frequency of each entry. Line 14 groups all the entries by the English term, then in Lines 15-19, for each group of entries with a same English term ENG , the I-tree is constructed and pruned, which will be detailed in next section. Line 19 filter spam using etyma merging method. If the edit distance of two English translations of the same Chinese term is less than a specific threshold, the two translations will be merged. For example,  X   X   X  X  X   X  has translations like  X  X ousten X  and  X  X ouston X , and only  X  X ouston X  is re-tained. Then the top-k translation of each Chinese term are selected as the final translations of this Chinese term. For example, there are such sentences like  X   X   X  X  X  X  X  X  X  (dance) X , because the frequency is not the top-k, it will be filtered.
Our work used all the bilingual web pages indexed by Sogou search gine 1 , and scanned the summary of these pages to extract about 1.2TB e n than 300GB, which is much smaller than our work, because 1.2T parentheses sentences is only a small part of all web pages.
 Procedure SDDB 3.1 Problem Definition There may be multiple entries extracted from an E2C sentence while maybe only one entry is correct, such as  X   X  X  X  X  X  X  X  X  X  (socialism) X  and  X   X  X  X   X  (socialism) X  ( X   X  X  X  X  (socialism) X  is a candidate entry). And for some En-glish terms, there are multiple different entries, for example, the entries with  X  X rink tea X  contain  X   X  X  X  X   X  X nd X   X  X  X  X   X . And we need select the right one. work because it is a fact that the more frequent a translation appears in the paren web, the more probable it is correct. So first we need to use an effective way to represent the entries. In SDDB method, an I-tree is constructed for each English term, and each I-tree node represent an entry.
 Definition 1. I-tree is used to represent all E2C sentences with a same En-glish term. The I-tree has following features: (1)It is built for group of E2C sentences with a same English term ENG; (2)ENG is its root; (3)for the path ( C 1 ,C 2 ,  X  X  X  ,C n ) rooted at the direct child of ENG, every C i is a candidate trans-lation of ENG, and for 1  X  i  X  n  X  1 , C i is a suffix of C i +1 and the frequency of C i +1 is included in the frequency of C i ; (4) there is no suffix relationship described in (3) between any two sibling nodes.
 For example, Table 1 shows the E2C sentences whose term in parentheses is  X  X rink tea X . Based on these E2C sentences, the I-tree as shown in Figure 3 is constructed.
 found in Chinese corpus. (2)C2E sentences are used to modify the frequency of I-tree node. Given a I-tree node N , suppose the term in N is t c , and the term in root is t e . If there is a C2E sentence s whose term outside parentheses is t e and term in parentheses is t c , we add the frequency of N by fp , where fp is the frequency of s . In this example, we can find C2E sentence  X  X rink tea(  X  X  X  ) X  whose frequency is 25, so the frequency of  X   X  X  X   X  is not 78 but 103. glish term can be intuitively seen. Then we extract all candidate entries through pruning of I-tree. 3.2 Problem Solution All entries are represented using different I-tree nodes. Now we need to consider-ing how to prune these I-trees. First we introduce a principle as following about pruning of I-tree based on statistics.
 Principle 1. N is a child of root of I-tree, only one candidate entry can be extracted from the subtree of N .
 The above principle will cause that we miss some correct translations which will affect the recall of SDDB method. So we need evaluate how many transla-tions we may miss. First if the subtree of N contains only one path, the English term is used as the translation of one node in this path on the web. Then we con-sider the case that subtree of N contains multiple paths. We collect about three million English phrases based on which three million I-trees are constructed. However, there are only seventy thousand I-trees contains such non-root node that the subtree of this node contains multiple paths, and the rate is only 2.3%. Then we randomly select one hundred such nodes. There are ten of these one hundred nodes are transliterations or abbreviations. For example,  X  X alayalam X  can be translated as  X   X  X  X  X  X  X  X  X   X  X r X   X  X  X  X  X  X  X  X  X   X . Finally only the most popular usage i.e.  X   X  X  X  X  X  X  X  X   X  is retained, which will not affect the user experience. And there are other ten nodes in which we can find such node that there are multiple translations can be extracted from the subtree of this node. And in these ten nodes, the translations of six nodes are synonym and extract-ing only one translation will not affect user experience either. For example,  X  X us stop X  can be translations as  X   X  X  X  X  X   X  X nd X   X  X  X  X  X  X   X . The other four nodes contain translations with different meaning, for example  X  X oom service X  can be translated as  X   X  X  X  X  X   X  X nd X   X  X  X  X  X   X  and some translations will be missed if we only retain one translation, however the rate of such term is only about 0.092%.
 is a correct translation. Because the existence of spam in web, a entry can be extracted as candidate entry only if the usage of this entry is dominated. And if there are no such dominated nodes, the common part will be selected as candidate entry. So we propose two principles to prune I-tree as Principle 2 and Principle 3.
 Principle 2. For an I-tree non-root node N , if the frequency of one child C is more than half of N  X  X  frequency, retain the node C and prune other children of N .
 Principle 3. For an I-tree non-root node N , if there is no child whose frequency is more than half of N X  X  frequency, retain the common part of these children (just N ) and prune all these children.
 Each I-tree is pruned using the above two principles from the root to leaves of this I-tree. Then we can obtain the lemma as following.
 Lemma 1. Give an I-tree, N is one non-root node. After pruning the subtree of N according to principle 2 and principle 3, and there will be only one path in subtree of N .
 Algorithm 1. pruning of I-tree: pruning( N in ) After pruning I-tree according to principle 2 and principle 3, the leaf node of the retained path is selected as candidate entries. So for an I-tree R Itree , n candidate bilingual entries will be extracted, where n is the number of children of R Itree . The pruning algorithm is shown in Algorithm 1.
 algorithm. In this I-tree, according to Algorithm 1, because there is no child of  X   X  X  X   X  whose frequency is more than half of frequency of  X   X  X  X   X , finally  X   X  X  X   X ,  X   X  X  X  X   X  X nd X   X  X  X   X  are selected, and the candidate entries extracted from this I-tree are shown in Table 2.
 Through scanning twenty billion parentheses sentences, whose size is about 1.2TB, over four million Chinese to English translations and over six million English to Chinese translations are extracted finally.
 a small sample results. And there does not yet exist a common evaluation data set. In this paper, we first compare our work with Youdao 2 through manual annotation. Then we use wikipedia to evaluate our work, and the method is proposed by Lin[14]. In order to evaluate our translation quality on name entity, we collect 695 kinds of brands with both Chinese and English name as the standard set to evaluate our work. Last but not least, we detect the scalability of our work. 4.1 Comparison with Youdao Youdao is a very popular web-based dictionary in China. In order to evaluate the quality of our mined dictionary, we crawled the query log of Youdao, and then get the translations in our mined dictionary and the web translations in Youdao. We crawled three groups of Chinese phrases which contain 73, 74 and 73 terms respectively and three groups of English phrases which contain 61, 70 and 61 terms respectively. Then we annotate these translations through blind test and evaluate the precision. The results are shown in Table 3 and Table 4. creases at approximately 3 percent for English translate and 4.5 percent for Chi-nese translate. Because we do not know the number of all correct translations, we cannot calculate the recall of our system. However more users pay attention to the accuracy of dictionaries. The results indicate that the performance of SDDB outperforms the mature commercial applications of Youdao. 4.2 Evaluation with Wikipedia Lin[14] used the translations in Wikipedia as evaluation on mining dictionary because the translations in Wikipedia contain far more terminologies and proper names than traditional bilingual dictionaries. We extract the titles of Chinese and English Wikipedia articles that are linked to each other. 78,284 such pairs are extracted by us and then we remove the pairs which are not translations or terms by rules consulted Lin X  X  work [14]. After the data cleaning, 70,772 translation pairs are left as gold standard translations. For each Chinese and English word in the Wikipedia data, we look it up in our mined dictionary. The Coverage of the Wikipedia data is measured by the percentage of words for which one or more translations are found. We then see in theses words whether we can find a translation which is an Exact Match of the answer key in the Wikipedia data. We use the dict.cn 3 as comparison.
 sults. We can see that the coverage of SDDB has obvious improvement compared with Dict.cn and the growth are 18% and 10% respectively, which indicated that SDDB has very good coverage. Despite the translations of Dict.cn are manually drafted, SDDB method also has a considerable advantage on Exact Match, es-pecially in English to Chinese results, which is about 11 percentage. 4.3 Evaluation with Brand Name With the development of transnational enterprise, many brands have both Chi-nese and English names, such as  X   X  X  X  X  (Chow Tai Fook) X ,  X   X  X  X  X  X  (Chrysler) X  etc. We can use brands as name entities to evaluate the translation quality of SDDB. We find 695 different brands with both Chinese and English names. For each brand name, we look it up in our mined dictionary, and we measure cov-erage by the percentage of brands for which one or more translations are found. Then we see in these brands whether our most frequent translation is an Ex-act Match of the brand name. Also we look up each brand name in Dict.cn as comparison. We compare the results to evaluate whether SDDB can cover more OOV terms. Table 7 and 8 show the Chinese to English translation and English to Chinese translation results.
 dict.cn because the brands in this experiment contain many up-to-date brands not included in traditional dictionaries. From the results we can conclude that SDDB can deal with the translation of name entities and OOV terms very well. 4.4 Scalability of SDDB The data-driven methods are mainly statistical method and parallelization is a helpful way to increase processing speeds. In our experiment, we first calculate the running time when we use different computers to parallelize the experiment to evaluate the influence of the parallelization. Then we increase the data size gradually and calculate the time to evaluate the scalability while data increases. Fig 4 shows the time spent when we use several computers to run SDDB in parallel and Fig 5 shows the time spent for different size of data.
 SDDB, which is the whole procedure after parentheses sentences are crawled. And data size is the size of all parentheses we used in the experiment. From Figure 4 we can see that through parallelization the time spent for SDDB has significant decrease. And Figure 5 indicates that the time spent increases almost linearly with increase of data size or computer numbers we use. The results demonstrate the better performance of data-driven method. As far as we know, there is no publication extracting bilingual data using lin-ear algorithm and not using any additional resources Some methods only process particular data, such as person name[17], organization name[21] etc. Some meth-ods use search engine to mine bilingual dictionaries([5], [13], [20]). They assumed the English term as input to search engine and extract translations from search results, which is difficult to build a large scale bilingual dictionaries. from parentheses bilingual text. [8] extracted translations from well-formed bilin-gual text. And these methods all use word alignment to evaluate the accuracy of translation pairs. Different from previous work, we process both parentheses and well-formed bilingual texts. The complexity of our method is linear and we use no additional resources.The experimental results show that our method has high precision and coverage and very good scalability. In this paper, we propose a novel method, SDDB, to extract translation dic-tionary from web data. Different from previous works, SDDB is a data-driven method, and never use semantic and additional resources. The method is linear and can be parallelized easily. Because no semantic is used during the construc-tion of bilingual dictionary, the method can be transplanted to other languages conveniently. The experimental results indicate that data-driven method has very good scalability, but also can achieve better accuracy and coverage.
