
Distribution data naturally arise in countless domains, such as meteorology, biology, geology, industry and eco-nomics. However, relatively little attention has been paid to data mining for large distribution sets. Given n dis-tributions of multiple categories and a query distribution Q, we want to find similar clouds (i.e., distributions), to discover patterns, rules and outlier clouds. For example, consider the numerical case of sales of items, where, for each item sold, we record the unit price and quantity; then, each customer is represented as a distribution of 2-d points (one for each item he/she bought). We want to find similar users, e.g., for market segmentation, anomaly/fraud detec-tion. We propose to address this problem and present D-Search , which includes fast and effective algorithms for sim-ilarity search in large distribution datasets. Our main con-tributions are (1) approximate KL divergence, which can speed up cloud-similarity computations, (2) multi-step se-quential scan, which efficiently prunes a significant number of search candidates and leads to a direct reduction in the search cost. We also introduce an extended version of D-Search : (3) time-series distribution mining, which finds similar subsequences in time-series distribution datasets. Extensive experiments on real multi-dimensional datasets show that our solution achieves up to 2,300 faster wall-clock time over the naive implementation while it does not sacrifice accuracy.
Distribution data naturally arise in countless domains, such as meteorology, biology, geology, industry and eco-nomics. Although the datasets generated by the correspond-ing applications continue to grow in size, a common de-mand is to discover patterns, rules and outliers. However, relatively little attention has been paid to data mining for large distribution sets. Here we focus on a less-studied problem, namely on  X  X istribution search X . Given n distribu-tions of multiple categories and a query distribution Q, we want to find similar clouds (i.e., distributions), to meet the above demand. To solve this problem, we present D-Search , which includes fast and effective algorithms for similarity search for large distribution sets.

We will illustrate the main intuition and motivation with a real example. Figure 1 shows three distributions corre-sponding to three motions. Specifically, they are the scatter plots of left and right foot kinetic energy values. Given a query motion, shown on the left, we would like to discover similar objects in large distribution datasets. Figure 1 shows the output of our approach, which successfully identify sim-ilar distributions. For example, D-Search detects Distribu-tion #1 similar to the query distribution (in fact, they both correspond to  X  X unning X  motions). In contrast, Distribution #2 is not be found as a similar object (in fact, it corresponds to a  X  X quatting X  motion).
 In this paper, we propose efficient algorithms called D-Search , which can find similar distributions in large distri-bution datasets. We mainly focus on similarity search for numerical distribution data to describe our approach. How-ever, our solution, D-Search can handle categorical distri-butions as well as numerical ones. Our upcoming algo-rithms are completely independent of such choice.
There are many distribution search applications. In this section, we briefly describe application domains and pro-vide some illustrative, intuitive examples of the usefulness of D-Search .  X  Multimedia : Multimodal data mining in a multime- X  Medical data analysis : The extraction of meaningful while Distribution #2  X  X ooks X  different.  X  Web service : There are numerous, fascinating appli- X  E-commerce : Consider an e-commerce setting, where
We introduce an efficient algorithms called D-Search , which can find similar distributions in large distribution datasets. The contributions are the following: (a) we exam-ine the time and space complexity of our solutions and com-pare them with the complexity of the naive solution. Given n distributions of a m -bucketized histogram and a query distribution, our algorithms require only O ( n ) to compute KL divergence, instead of O ( mn ) the naive method re-quired, and lead to a dramatically reduction in the search cost. (b) Extensive experiments on real multi-dimensional datasets shows that our method is significantly faster than the naive method, while it does not sacrifice accuracy. We also introduce an extended version of D-Search : (c) time-series distribution mining, which finds similar subse-quences in time-series distribution datasets.

The rest of the paper is organized as follows: Section 2 discusses related work. Section 3 introduces preliminary concepts, describes the proposed method, and identifies the main tasks of distribution search. We then explain the techniques and constraints we use to realize efficient KL divergence calculations. We also describe an extended ver-sion of D-Search , namely, time-series distribution mining. Section 4 introduces some of the applications for which our method proves useful, and evaluates our algorithms by con-ducting extensive experiments. Finally, Section 5 concludes the paper.
Related work falls broadly into two categories. The first category includes work on similarity functions be-tween distributions. The other category includes probabilis-tic queries.
There are several statistical methods [15] to de-cide whether two distributions are the same (Chi-square, Kolmogorov-Smirnoff). However, they do not give a score; only a yes/no answer; and some of them cannot be easily generalized for higher dimensions.

Functionals that return a score are motivated from im-age processing and image comparisons: the so-called earth-moving distance [17] between two images is the mini-mum energy (mass times distance) to transform one im-age into the other, where mass is the gray-scale value of each pixel. For two clouds of points P and Q (= black-and-white images), there are several measures of their dis-tance/similarity: one alternative is the distance of the closest pair (min-min distance); another is the Hausdorff distance (max-min -the maximum distance of a set P , to the near-est point in the set Q ); another would be the average of all pairwise distances among P -Q pairs. Finally, tri-plots [21] can find patterns across two large, multi-dimensional sets of points, although they cannot assign a distance score. The most suitable idea for our setting is the Kullback-Leibler (KL) divergence (see Equation (1)), which gives a notion of distance between two distributions. The KL divergence is commonly used in several fields to measure the distance be-tween two PDFs (probability density functions, as in, e.g., information theory [22], pattern recognition [18, 10]).
A remotely related problem is the problem of proba-bilistic queries. Cheng et al. [5] classifies and evaluates probabilistic queries over uncertain data based on models of uncertainty. An indexing method for regular probabilistic queries is then proposed in [6]. In [20] Tao et al. presents an algorithm for indexing uncertain data for any type of PDFs. Distributions for our work can be expressed by PDFs as well as histograms. However, the main difference between our study and [5] is that our work focuses on comparing differ-ences between distributions, while Cheng et al.s X  work fo-cuses on locating areas in distributions that satisfy a given threshold.

Distribution search and mining are problems that, to our knowledge, have not been addressed. The distance func-tions among clouds that we mentioned earlier either expect a continuous distribution (like a probability density func-tion), and/or are too expensive to compute.
We now present our distribution search algorithms. In this section, we introduce some background concepts, de-fine the problem of distribution search, and then propose algorithms for solving it. We also introduce time-series dis-tribution search, as an extended version of D-Search .
Given two distributions P and Q , there are several mea-sures of their distance/similarity, as we described in the lit-erature survey section. However, the above distances suffer from one or more of the following drawbacks: they are ei-ther too fragile (like the min-min distance); and/or they do not take all the available data points into account; and/or they are too expensive to compute. Thus we propose to use information theory as the basis, and specifically the Kullback-Leibler (KL) divergence.
 Let us assume for a moment that the two clouds of points P and Q consist of samples from two (continuous) proba-bility density functions P and Q , respectively. If we knew P and Q we could apply the continuous version of the KL divergence; however, we do not. Thus we propose to bucke-tize all our distributions using a grid with m grid-cells, and employing the discrete version of the KL divergence, de-fined as follows: where p i , q i are the i -th buckets of distributions P and Q , respectively. That is, m i =1 p i = m i =1 q i =1 .
The above definition is asymmetric, and thus we propose to use the symmetric KL divergence D SKL : In the rest of the paper, we shall simply denote D SKL ( P, Q ) as D ( P, Q ) .

There is a subtle point we need to address. The KL diver-gence expects non-zero values, however, histogram buck-ets corresponding to sparse area in multi-dimensional space may take a zero value. To avoid this, we introduce the Laplace estimator [13, 11]: where p i is the original histogram value ( i =1 ,...,m ) and p is the estimate of p i . | P | is the total number of points (i.e., | P | = m i =1 p i ).

Another solution is that we could simply treat empty cells as if they had  X  X inimum occupancy X  of . The value for  X  X inimum occupancy X  should be &lt;&lt; 1 / | P | .We chose the former since it provides better search accuracy, al-though our algorithms are completely independent of such choices. The first problem we want to solve is as follows: Problem 1 (Distribution search) Given n distributions of an m -bucketized histogram and a query distribution Q, find the top k distributions that minimize the KL divergence. This involves the following sub-problems, which we ad-dress in each of the upcoming subsections. (a) How can we represent a distribution of histogram compactly, and ac-celerate distance calculations? (b) How can we prune a sig-nificant number of search candidates and achieve a direct reduction in the search cost? (c) What is the space and time complexity of our method? 3.2.1 Lower bounding KL divergence We described how to measure the distance between distri-butions. The first question is how to store the distribution information, in order to minimize space consumption and response time. Recall that for each m -bucketized distribu-tion P , we could keep the fraction of p i that fall into the i -th bucket.

The naive solution is exactly to maintain an m -bucketized histogram for each distribution, and to use such histograms to compute the necessary KL divergences, and eventually to run the required mining algorithm (e.g., the nearest neighbor search).

However, this may require too much space, especially for higher dimensions. One solution would be to use the top c most populated buckets, in the spirit of  X  X igh end his-tograms X . The reason against it is that we may ignore some sparse-populated bins, whose logarithm would be important for the KL divergence.

For the definition, we compute the KL divergence with c histogram values, that is, we compute ( p i  X  q i )  X  log( p if we select either p i or q i , otherwise, we can simply ignore these values since they are very close to zero. Consider that the sequence describing the positions of the top c values of P and Q is denoted as I pq . We then obtain the lower bounding KL divergence of P and Q : Lemma 1 For any distributions, the following inequality holds.
 Proof: From the definition, Since  X  i, ( p i  X  q i )(log p i  X  log q i )  X  0 , for any c value (1  X  c  X  m ) ,wehave which completes the proof.
 Algorithm 1 D-Search( Q, k ) /* N is the sorted nearest neighbor list */ initialize N for i := 1 to h do end for for all P  X  database do end for return N Algorithm 2 MultiStepScan( N ,Q,k,c ) /* N app is the sorted nearest neighbor list */ initialize N app /* compute approximate KL divergence */ for all P  X  database do end for /* compute exact KL divergence */ for all P  X  X  app do end for /* prune the search candidates */ for all P  X  database do end for return N 3.2.2 Multi-Step Sequential Scan
Instead of operating on lower bounding KL divergence with c buckets of a single computation, we propose to use multiple computations, trying to balance to a trade-off be-tween accuracy and comparison speed. As the number of buckets c increases, the lower bounding KL divergence be-comes tighter, but the computation cost also grows. Accord-ingly, we gradually increase the number of buckets, and thus improve the accuracy of the approximate distance, during the course of query processing.

Algorithm 1 shows our proposed method, which uses the lower bounding KL divergence. In this algorithm N shows the k-nearest neighbor list, and D cb shows the exact KL di-vergence of the current k -th nearest neighbor (i.e., D cb the current best). As the multi-step scan, the algorithm uses breadth-first traversal, and it prunes unlikely distributions at each step, as follows: 1. We first obtain the set of k -nearest neighbor candidates 2. We then compute the exact KL divergence between 3. For all distributions, if the lower bounding KL diver-We compute h steps that form an arithmetic progression: c := i  X  c 1 for i =1 , 2 , ..., h .

The search algorithm gradually enhances the accuracy of the lower bounding KL divergence and prunes dissimilar distributions to reduce the computation cost of KL diver-gence. Finally, we compute the exact KL divergences be-tween distributions, which are not pruned in any steps, and the query distribution.
 Lemma 2 For any distributions, D-Search guarantees ex-actness when finding distributions that minimize the KL di-vergence for the given query distribution.
 Proof: From Lemma 1, we obtain D ( P, Q )  X  d c ( P, Q ) for any granularity, for any distribution. For N , D cb  X  d c ( P, Q ) holds. In the search processing, since D cb  X  D ( P, Q ) , the lower bounding KL divergence of N is less than D cb . The algorithm discards P if (and only if) d c ( P, Q ) &gt;D Therefore, the final k-nearest neighbors in N cannot be pruned during the search processing.

Although we described only a search algorithm for k -nearest neighbor queries, D-Search can be applied to range queries. It utilizes the current k -th nearest neighbor distance D cb for k -nearest neighbor queries, and the search range is used to handle range queries.
We described the basic version of D-Search in the pre-vious subsection, which guarantees the exactness for dis-tribution search while the algorithm efficiently finds distri-butions that minimize the KL divergence. The question is what can we do in the highly likely case that the users need more efficient solution while they practically require high accuracy, not a theoretical guarantee. As the enhanced ver-sion of D-Search , we propose to compress the histograms using SVD (Singular Value Decomposition), and then keep-ing some appropriate coefficients. As we show later, this decision significantly improves both space as well as re-sponse time, with negligible effects on the mining results. The only tricky aspect is that if we just keep the top c SVD coefficients, we might not get good accuracy for the KL di-vergence. This led us to the design of our method that we describe next. The main idea behind SVD is to keep the top c SVD coefficients for the histogram P ( m )=( p 1 ,...p m as well as the top c coefficients for the histogram of the log-arithms (log p 1 ,... log p m ) . We elaborate next.
Let  X  P =( X  p 1 ,...,  X  p m ) be the histogram of the logarithms of P =( p 1 ,...,p m ) , i.e.,  X  p i = log p i . Let S p the SVD coefficients of P and  X  P , respectively. We present our solution using S p and  X  S p .
 Proposed Solution: We represent each distribution as a single vector; we compute S p and  X  S p from P and  X  P for each distribution, and then we compute the necessary KL divergences from the SVD coefficients. Finally, we apply a search algorithm (e.g., the nearest neighbor search) to the SVD coefficients.

The cornerstone of our method is Theorem 1, which ef-fectively states that we can compute the symmetric KL di-vergence using the appropriate SVD coefficients.
 Theorem 1 Let S p =( sp 1 ,...,sp m ) and  X  S p = ( X  sp 1 ,...,  X  sp m ) be the SVD coefficients of P and  X  tively. Then we have Proof: From the definition, Then we have In light of Parseval X  X  theorem, this completes the proof.
The KL divergence can be obtained from Equation (7) using the SVD calculated from histogram data. The number of buckets of a histogram (i.e., m ) could be large, especially for high-dimensional spaces, while the most of buckets may be empty. The justification of using SVD is that very few of the SVD coefficients of real datasets are often signifi-cant and the majority are small, thus, the error is limited to a very small value. When calculating the SVD from the original histogram, we select a small number of SVD coef-ficients (say c coefficients) that have the largest energy from the original SVD array. This indicates that these coefficients will yield the lowest error among all SVD.

For Equation (7), we compute the KL divergence with the top c SVD coefficients, that is, we compute f pq we select either sp i or sq i (  X  sp i or  X  sq i ), otherwise, we can simply ignore these coefficients since they are very close to zero. Thus, we obtain the approximate KL divergence of P and Q :
Figure 2 shows the SVD-based approximation of prob-ability distribution from MoCap . It is represented by a 10  X  10 bucketized histogram (i.e. full coefficients c = m = 100 ). The numerical rank, however, is much lower. As well as the basic version of D-Search described in Section 3.2, the enhanced version also uses the algorithm of multi-step scan (see Algorithms 1 and 2), which efficiently finds simi-lar distributions using their SVD-based approximate KL di-vergences. Figure 2 shows the gradual  X  X efinement X  of the approximation. In Figure 2 (a), we compute the approxi-mate distance from the coarsest version of a distribution P as the first step of the refinement. If the distance is greater than the current k -th nearest neighbor distance (i.e., D we can prune P . Otherwise, we compute the distance from the more accurate version as the second refinement step (see Figure 2 (b)). We compute the exact distance from the orig-inal representation of P only if the approximate distance does not exceed D cb (see Figure 2 (c)).
In this section we examine the time and space complex-ity of our approach and compare it with the complexity of the naive solution. Recall that n is the number of input dis-tributions, m is the number of buckets that we impose on the address space, and c is the number of buckets or SVD coefficients that our methods keeps. 3.4.1 Space Complexity Naive method Lemma 3 The naive method requires O ( mn ) space.
 Proof: The naive method requires the storage of m -bucketized histograms of n distributions, hence the com-plexity is O ( mn ) .
 Proposed Solution ( D-Search ) Lemma 4 The proposed algorithms require O ( m + n ) space.
 Proof: D-Search initially allocates memory to store his-togram of m buckets. The basic version of D-Search selects the top c most populated buckets, and keeps them. The en-hanced version calculates the SVD coefficients and keeps only the top c coefficients. Then they reduce the number of buckets (or SVD coefficients) to O ( c ) and allocate O ( cn ) memory for computing the criterion. However, c is nor-mally a very small constant, which is negligible. We sum up all the allocated memory and we obtain a space com-plexity of O ( m + n ) . 3.4.2 Time Complexity Naive method Lemma 5 The naive method requires O ( mn ) time to com-pute KL divergence for the k -nearest neighbor search. Proof: Computing the KL divergence requires O ( m ) time for every distribution pair. For n distributions, it would take O ( mn ) time.
 Proposed Solution ( D-Search ) Lemma 6 The proposed algorithms require O ( n ) time to compute approximate KL divergence for the k -nearest neighbor search.
 Proof: The calculation of the nearest neighbor search re-quires O ( cn ) time. We handle c histogram values (or SVD coefficients) for each distribution. This is repeated for n number of input distributions. Again, since c is a small con-stant value, the time complexity distribution search can be simplified to O ( n ) .

Figure 3. Time-series distribution search (multiple windows, w 0 =4 , W =2 ).
Many data sources are observations that evolve over time leading to time-series distribution data. For example, fi-nancial datasets depict the prices of every stock over time, which is a common example of time-series distribution data. Reporting meteorological parameters such as temperature readings from multiple sensors gives rise to a numerical distribution sequence. Business warehouses represent time-series categorical distribution sequences such as the sale of every commodity over time. Time-series distribution data depict the trends in the observed pattern over time, and hence capture valuable information that users may wish to analyze and understand.
 In this section we introduce an extended version of D-Search , which can find similar subsequences in time-series distribution datasets. The problem we propose and solve is as follows: Problem 2 (Time-series distribution mining) Given time-series distribution datasets and query distribution Q, find subsequences whose distribution minimizes the KL divergence.
 Consider the time ordered series distribution of d -dimensional points. Distributions performed at different times or by different subjects have different durations, and data sampling rates can also be different at various times. We should solve the following question: How do we effi-ciently find the similar subsequences for multiple windows? In our approach, we choose a geometric progression of win-dows sizes [14]: rather than estimating the patterns for win-dows of lengths w 0 ,w 0 +1 ,w 0 +2 ,w 0 +3 , ... , we estimate them for windows of w 0 , 2 w 0 , 4 w 0 , ... , or, more generally, for windows of length w l := w 0  X  W l for l =0 , 1 , 2 , ... . Thus, the size of the window set W we need to examine is dramatically reduced.

The main idea of our approach is shown in Figure 3. We compute the KL divergence of data points falling within a window, and organize all the windows hierarchically. In this case, query distribution in Figure 3 is similar to P P (12 , 16) at the level 0 ( l =0 ), and, P (8 , 16) at the level 1 ( l =1 ), which are shaded in Figure 3.

With our method, we can also optimally use the sliding window, which is used as general model in time-series pro-cessing [23, 8]. By using the sliding window, we can find similar sequences, which are delayed less than the basic window time.
To evaluate the effectiveness of D-Search , we carried out experiments on real datasets. Our experiments were con-ducted on an Intel Core 2 Duo 1.86GHz with 4GB of mem-ory, running Linux.

The experiments were designed to answer the following questions: 1. How successful is D-Search in capturing time-series 2. How does it scale with the sequence lengths n in terms 3. How well does it approximate the KL divergence? In this section we describe some of applications where D-Search proves useful. Figure 4 shows how D-Search finds similar distributions. Note that, for all experimental results, the enhanced version perfectly captures all similar distributions, that is, the output of the enhanced version is exactly the same as that of the naive method and the basic version.
 MoCap This dataset is the subject numbers 7, 13, 14, 16 and 86, taken from the CMU motion capture database [1]. In our framework, a motion is represented as a distribution of hun-dreds of frames, with each frame being a d -dimensional point. It contains 26 sequences, each consisting of approx-imately 8000 frames. Each sequence is a series of simple motions. Typical human activities are represented, such as walking, running, exercising, twisting, jogging and jump-ing.

The results on this dataset were already presented in Sec-tion 1. As shown in Figure 1, D-Search can successfully identify similar distributions.
 EEG This dataset was taken from a large study that examined the EEG correlates of the genetic predisposition to alco-holism downloaded from the UCI website [2]. It contains measurements from 64 electrodes placed on subjects X  scalps up to 2,300 times faster than the naive implementation. that were sampled at 256 Hz (3.9-msec epoch) for 1 second, that is, the length of each sequence is 256. There were two groups of subjects: alcoholic and control.

Our approach is also useful for classification. Fig-ure 4 (a) shows that D-Search can classify the query distribution and Distribution #1 , (a subsequence from co2a0000364 of 36-37sec., and a subsequence from co3a0000451 of 56-57sec., respectively), into the same group (in fact, they both corresponded to  X  X lcoholic X ). In contrast, Distribution #2 , which is a subsequence from co2c0000364 of 75-76sec., go to another group (in fact, it belongs to  X  X ontrol X ).
 Ondemand TV This dataset is from the Ondemand TV service of 13,231 programs that users viewed in a 6-month timeframe (from 14th May. 2007 to 15th Nov. 2007). We randomly select 10,000 anonymous users from the dataset. Each distribution sequence contains a list of attributes (e.g., content ID, the date the user watched the content, the ID of the user who watched the content).

As shown in Figure 4 (b), our method can find simi-lar Ondemand TV content. For example, D-Search found that Distribution #1 was a similar distribution, and Distri-bution #2 was a dissimilar distribution to the query distri-bution. In fact, query distribution, Distribution #1 and Dis-tribution #2 are  X  X ports: Australian Open Tennis Champi-onships 2007 Women X  X  Final (from 1st Feb. 2007 to 1st Apr. 2008) X ,  X  X ports: Australian Open 2007 Tennis Cham-pionships Women X  X  Semifinal (from 1st Feb. 2007 to 1st Apr. 2008) X ,  X  X ooking: Oliver X  X  Twist No.1 (from 23rd Oct. 2006 1st Aug. 2008 X ) MusicStore This dataset consists of the purchasing records from Music-Store obtained over 16 months, (from 4th Apr. 2005 to 1st Jul. 2006). Each record has 3 attributes: user ID (50,000 anonymous, randomly selected users), music ID (43,896 items of music), date of purchase/sale.

Figure 4 (c) shows that D-Search can identify similar user groups. For example, D-Search found that query distri-bution was similar to Distribution #1 . In fact, the query dis-tribution and Distribution #1 are histograms of purchasers of John Lennon X  X   X  X oman X , and John Lennon X  X   X  X ove X , re-spectively. In contrast, Distribution #2 was not found to be as a similar distribution. In fact, Distribution #2 was a pur-chaser histogram of Nat King Cole X  X   X  X -O-V-E X .
To evaluate the search performance, we compared the basic version and the enhanced version with the naive ap-proach. We present experimental results on search perfor-mance for when the data set size varies.

Figure 5 compares our algorithms with the naive method in terms of computation cost. Database size varied from 100,000 to 400,000. Note that the y -axis uses logarithmic scale. We conducted this experiment with a histogram of m =10 , 000 , starting coefficient c 1 =50 , and step h =4 . Each result reported here is the average of 100 trials.
There, we show the wall-clock time versus the database size n for three datasets. D-Search provides a dramatic reduction in computation time. Specifically, the enhanced (basic) version achieves up to 2,300 times (230 times) faster than the naive implementation in this experiment.

In addition to high-speed processing, our method achieves high accuracy; the output of the enhanced version is exactly the same as those of the naive algorithm and the basic version.
D-Search exploits multiple computations for the approx-imation of KL divergence. In this section we discuss the approximation quality of each granularity.

Figure 6 shows scatter plots of the computation cost ver-sus the approximation quality. The x -axis shows the com-putation cost for KL divergences, and the y -axis shows their relative approximation error rate. We compare the basic ver-sion and the enhanced version in the figure. The figure im-plies a trade-off between quality and cost, but the results of the enhanced version are close to the lower left for both datasets, which means that the enhanced version provides benefit in terms of quality and cost.

Figure 7 shows how often each approximation was used in the basic version and the enhanced version for a dataset size of 100,000. As shown in the figure, most of the data sequences are excluded with the approximations of c = { 50 , 100 , 150 , 200 } . The coarser approximation provides reasonable approximation quality, and its calculation speed is high. On the other hand, although the approximation with higher granularity is not very fast, it offers good approx-imation quality. Accordingly, using approximations with various granularities offers significant advantages in terms of approximation quality and calculation speed. Our algo-rithms, especially the enhanced version efficiently prunes a large number of search candidates, which leads to a signifi-cant reduction in the search cost.
We introduced the problem of distribution search, and proposed D-Search , which includes fast and effective algo-rithms, as its solution. D-Search has all the desired charac-teristics:  X  High-speed search: Instead of O ( mn ) time the naive  X  Exactness: It guarantees no false dismissals.  X  It can be extended to time-series distribution mining,
Our experimental results reveal that D-Search is signif-icantly faster than the naive method, and occasionally up to 2,300 times faster while it perfectly captures all similar distributions. Furthermore, our algorithms can be extended to time-series distribution mining. In fact, we present case studies on real datasets and demonstrate the effectiveness of our approach in discovering patterns among time-series distribution datasets. We believe that the addressed prob-lem and our solution will be of fundamental interest in data mining.

