 Modern businesses use contact centers as a communication channel with users of their produc ts and services. The largest factor in the expense of running a telephone contact center is the labor cost of its agents. IBM Research has built a new system, Contact-Center Agent Buddies (CAB), which is designed to help reduce the average handle time (AHT) for customer calls, thereby also reducing their cost. In this paper, we focus on the call logging subsystem, which help s agents reduce the time they spend documenting those calls. We built a Template CAB and a Call Logging CAB, using a pipeline consisting of audio capture of a telephone conversation, auto matic speech recognition, text analysis, and log generation. We developed techniques for ASR text cleansing, including normalization of expressions and acronyms, domain terms, capita lization, and boundaries for sentences, paragraphs, and call se gments. We found that simple heuristics suffice to generate high-quality logs from the normalized sentences. The pipeline yields a candidate call log which the agents can edit in less time than it takes them to generate call logs manually. Evaluation of the Call Logging CAB in an industrial contact cen ter environment shows that it reduces the amount of time agen ts spend logging calls by at least 50% without compromising the quality of the resulting call documentation. Algorithms, Design, Experi mentation, Management I.2.7 [ Artificial Intelligence ]: Speech Recognition and Synthesis; Text analysis; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing,  X  abstracting methods; J.1 [ Administrative Data Processing ]: Business In recent years, contact centers have grown in importance as an interface of an organization to its customers, both existing and new. A customer X  X  experience with the call center is seen as a factor in customer retention, as well as an opportunity to sell more to existing customers (cross-sell, up-sell) and to sell to new customers. Contact center operations are typically labor-intensive, as they most often involve a voice conversation between a customer and an agent. Recent improvements in automatic speech recognition (ASR), information retrieval, a nd text analytics applied to ASR call audio offer the possibility of improving the quality of call handling and reducing the cost of contact center operations. For instance, off-line analytics can flag  X  X ad X  calls (calls with an undesirable outcome, or calls wher e the agent falls short of call standards) for future study aimed at improvement of agent performance [8] [17] [20] . Or real-time analytics can help an agent find the answer to a caller X  X  question more quickly [8] or reduce the amount of time that ag ents spend documenting calls. We address the last goal in this paper. The Contact Center Agent Buddies (CAB) project was undertaken jointly by IBM Research and a major automobile manufacturer (referred to hencefor th as our  X  X ndustrial partner X ) to develop and test new methods for improving contact center call handling and operations. The CAB project has two primary components: (1) a collection of Agent Buddies that  X  X isten X  to every phone conversation and, in real time, increase agent efficiency; and (2) a Transforma tional Diagnostic Tool (TDT), a background (offline) process that analyzes structured and unstructured data from the contact center. The goals of the CAB system are to reduce the average handle time (AHT) of calls and provide operational dia gnosis. The project culminated in a  X  X roof of Concept X  (PoC) in the industrial partner X  X  contact center, to measure how well we did against the goals in a close-to-real business environment. In this paper, we describe the part of the real-time system that aims to reduce the time spent handling customer calls by performing semi-automatic logging of the calls. This paper is organized as follo ws. In the remainder of the introduction, we describe the agent-created call log and review previous work related to our system and underlying technologies. In Section 2, we present an overview of the CAB system in which the call logger is embedded, highlighting its dependency on other system com ponents. In Section 3, we describe the analysis pipeline and logger; in Section 4, we report and discuss the results of the Po C evaluation. We point toward future work in Section 5. In a contact center, every call to or from an agent has to be documented, to support later con tinuation of the same customer interaction by the same agent, to support call transfer to another agent, or to support call evaluati on by a supervisor. Agents may create documentation during the call or while the customer is on hold or immediately after completion of the call. Time spent in documentation is included in the call handle time. Many contact centers use a customer relationship management (CRM) system to maintain informati on about customer interactions, including call documentation. An agent typically takes notes in a text editor during the call and then transfers the information into a CRM record with cut-and-paste operations. At our industrial partner, a CRM record consists of a collection of fields, called a  X  X emplate, X  contai ning data such as make, model, year, and VIN of the subject vehi cle; owner information such as name, telephone number, and wa rranty status; and space for free-form call documentation (currently 1500 characters) that includes the caller X  X  statement of the problem, relevant facts gathered from the caller (e.g. dealer, repair shop), and the agent X  X  resolution. While this free-form documentation is indeed text, it is often quite irregular, characterized by idiosyncratic abbreviations, misspellings, missing words, grammatical errors, and incorrect case. The agent X  X  goal is speed and completeness of documentation, not readability. This free-form documentation, together with the template fields, is what we term the call log. Automatic, or semi-automatic, call logging is an instance of dialogue summarization, with some specializations. There is already a body of research on summarization of speech of different genres: broadcast news, voice mail [5] , meetings, and dialogue. For broadcast news, approaches range from ones derived largely from text summarization [4] to strategies combining prosodic, lexical, and structural features [6] to exclusive reliance on acoustic/prosodic features. [7] For meetings, Murray, et al. [9] demonstrate that summarization that takes into account features such as speaker activity, turn-taking, and discourse cues out performs text-based methods inherited from text summarization. For dialog summarization, Zechner [19] relies largely on tf/idf scores, tempered with cross-speaker information linking and question/answer detection. Presenting a project for summarizing dialogues in unrestricted domains, a pipeline architecture is described, with functionality distributed among different components. The ar ticle provides a comprehensive overview of the technical issues in summarization of dialogue X  as opposed to summarization of, e.g., broadcast news X  X s well as a good overview of previous literature. While call logging is similar to dialogue summarization, there are notable differences. The domain model for the call log, which describes what should be cap tured in the log, is partially embodied in a script that underlies at least part of the call (e.g. agent asking for phone number, whether the caller is the vehicle X  X  original owner, what the VIN is). Unlike general open-domain dialogue summarization, which needs only to determine domain salience statistically, call log generation must attach semantic labels to domain information to route identified items into the correct fields of the CRM record. In our project, we did not develop automatic methods for domain modeling, as, for exam ple, Roy and Subramaniam [15] . Rather, we defined the domain model from our industrial partner X  X  template structure and lists of vocabulary, such as vehicle makes and models, a nd UCC X  X  (Uniform Commercial Codes). The goals of the Contact Center Agent Buddies project were to reduce the AHT in contact centers by improving the ability of the contact center agents to answer customer questions more rapidly, to reduce time spent in call documentation, and to improve first call resolution, another key contact center performance indicator. The CAB system has three major subsystems: a speech subsystem, a real-time subsys tem (Agent Buddies), and an agent/supervisor survey tool for evaluation of the real-time agent buddies. The Contact-Center Agent Buddies system operates as follows. Every few seconds from the start of a call to the finish, it requests from its environment a speech transcript from the beginning to the current point in time. It then runs a (UIMA) [3] pipeline of annotators, or te xt analysis engines (TAE X  X ) the transcript and updates its display to the agent based on any analysis changes since the last time it ran. The environment may be either a real-time production environment or a prerecorded simulated one. The UIMA annotator pipeline takes the speech transcript and performs a variety of types of processing on it. Currently, the pipeline contains 40 annotators. Many of them perform basic text analysis functions commonl y needed by all of the CABs, including those that we do not discu ss in this paper. Others are more specialized. Basic common analysis involves, primarily, converting the speech stream into reasonably well-formed sentences or utterances, and then extracting the key task(s) that the agent must perform. This task is then evaluated by each of the Agent Buddies (CABs), which are specially designed components that take the task, evaluate it, and try to solve some aspect of it. For example, one CAB may create a log of the call, another may propose a particular document as a solution to a task, or yet another may try to perform operations that are time-consuming for agents, such as identification and data base lookup of vehicle identification numbers (VINs). In order to reduce the integration complexity of the PoC, we did not feed live audio calls into th e agent buddies system. Instead, we prerecorded three weeks worth of calls and generated the In UIMA, a TAE is a module for analyzing information about a document stored in a  X  X ommon analysis structure X  ( X  X AS X ) and for adding annotations to the CAS, representing new document information a sserted by the TAEs. ASR transcripts in one off-line pass. The call simulator provides the output of a pre-transcribed call to the main system polling the transcript in the same timing that the real-time transcription engine would. Th e call simulator decides, based on the timing information, how fa r along the call is and returns to the CAB system the transcript from the start to the current position. A live real-time e nvironment component would generate a transcript on the fly and return its current state. For its ASR component, our system uses a somewhat more recent version of the IBM Research  X  X ttila X  prototype speech transcription system described in [16] . The acoustic models used were trained on approxi mately 2000 hours of general conversational telephony speech data in addition to approximately 340 hours of data that was collected from the PoC call center. The language mode ls used are a mixture of a general language model and a language model built from manual transcriptions of the same 340 hours of data. The general language model contains data from various sources: conversational telephony speech, br oadcast news, etc. The ASR vocabulary is augmented with lists of automobile makes, models, and components obtained from the manufacturer. After applying various optimizations, th e details of which are beyond the scope of this paper, we obt ained raw transcripts which had a measured average word erro r rate (WER) of about 26%. The agent/supervisor survey tool was specifically designed and implemented for use by the agents in the PoC and will be described later. Our approach to call log generation involves the data transformation steps shown at a high level in Figure 1 . Figure 1. High Level View of Log Generation Beginning with the audio of the conversation, we first use ASR to create a raw transcript of the conversation. Next, a pipeline of UIMA TAEs is used to create a normalized transcript that includes sentence boundaries. Finally, sentences are extracted from this normalized transcript to create the candidate call log, which is presented to the agent fo r editing and storage. In this section, we describe how these steps are performed. The raw transcript resulting from speech transcription indicates only which words were spoken when by which speaker; there is no case information or punctuation in it. Tracking speaker turns is important for the Template and Call Logging CABs. The timing detail is more important for other CABs, which must present their results during an on-going phone call, than it is for the Template and Call Logging CA Bs, whose results may not be complete X  X r used X  X ntil the end of the call. A suite of about twelve genera l-purpose UIMA text analysis engines (TAEs)  X  X ormalizes X  the raw transcript on behalf of all the other TAEs and CABs in the system. In effect, the goal of normalization is to produce ordinary written text which a human reader would accept as readable. This is a strong requirement in the call logging application, since it should yield logs that can be easily comprehended when they are later used in other call center operations. We might suggest here that the human-created logs, littered as they are with misspellings and idiosyncratic abbreviations, do not provide too high a bar in comparison. Normalization begins with removal of filler words (e.g.,  X  X h X ) and interjections (e.g.,  X  X kay, X  when interrupting the other speaker). It continues with normalizing spoken numbers (e.g.,  X  X wo thousand and seven X  becomes  X 2007 X ) and spoken spellings and acronyms (e.g.,  X  X  o n e s X  becomes  X  X ones X ;  X  X  b s X  becomes  X  X bs X ). Furthermore, certain expression types, such as CRM record numbers or phone numbers, are transformed into their standard orthographic shapes (e.g.,  X 123-456-7890 X  for U. S. phone numbers). Domain-specific words and acronyms are rendered with the expected capitalization patterns (e.g.,  X  X BS X ), as is the pronoun  X  X . X  Sentence boundaries are detected, using the methods described in [10], and the resulting sentences are given initial capitals and punctuated with periods. It is important to note that these normalizations benefit all of the CABs, and not only the Call Logging and Template CABs. The normalization annotators are actually interleaved, as appropriate, with other UIMA annotators which perform more specialized analysis. Depending on the CAB, the normalized text is used either as a source of text for humans to read (especially true for the Call Logging CAB), as canonical values of domain-specific expressions (e.g., phone numbers or VINs for the Template CAB) to be stor ed in data repositories, or as parameters to other computing systems (e.g., a case or ticket number or a query to the solutions knowledge base). A significant portion of the low-level text normalization work in the CAB system is performed by cascades of finite-state grammars written in the AFst (Annotation-Based Finite State Transducer) language. [1] [2] Briefly, AFst supports a declarative notation for specifying patterns of annotations in a UIMA CAS and for describing creation, modification, and deletion operations on those annotations. Specifically, cleanup and normalization of the raw speech transcripts begins with a set of FST grammars which removes words identified as disfluencies and interjections. A disfluency is a word such as  X  X h X  or  X  X mm X , which is in the ASR vocabulary. An interjection is an expression such as  X  X ep X ,  X  X h wow X ,  X  X reat X , etc., which is spoken by one speaker between two turns of the other speaker. When that occurs, the grammars also combine the speaker turns of the interrupted speaker into a single turn. More grammars are then applied to identify a range of expression types (henceforth typed expressions ), including acronyms, alphanumerics (e.g. VINs) and various kinds of numbers, and to assign types to them (phone numbers, service request/ticket numbers, amounts of money, automobile mileage numbers, etc.). Expressi on typing most often requires grammars that first create  X  X u e X  annotations over cue phrases, typically found in questions by the agent (e.g.  X  X icket number, X   X  X IN number, X   X 17-digit numbe r, X   X  X hone number X ) that are followed by caller turns containing the information of the type corresponding to the cue. Note that alphanumeric expressions are often used in designating automobile models and trim levels (for example  X  X 8 X ,  X 328xi X ). An associated non-grammar-based TAE is used to compute the best normalized value for these expressions. For acronyms it concatenates the spelled initials into a single token; for numbers, it computes the sta ndard decimal representation. As the raw transcript contains no case information about the words it contains, we built a TA E whose job is to determine which words should be upper-cased, leaving everything else in lower-case. It begins simply by upper-casing all tokens based on the pronoun  X  X  X  (e.g.,  X  X  X  X  X ). It then uses annotations of domain-specific words and phrases that have been put into the CAS by a prior application of a dictionary look-up TAE. In our case, the dictionary was created from lists obtained from the industrial partner: automobile makes, models, trims, and components, as well as th eir variants, acronyms, and abbreviations. Whenever the case normalization TAE finds that a variant of a domain term has occurred in the transcript, a case pattern is derived from the term X  X  lexical entry. This allows us to produce standard spellings fo r expressions such as  X  X BS X ,  X  X ercedes-Benz X , or  X  X railBlazer X , for example. Note that capitalization of the first word of a sentence is not done at this time. That must wait for the next step, which is sentence boundary detection. An utterance boundary detector [10] divides the continuous stream of words from the recognition engine into normal sentences. The algorithm, based on Maximum Entropy classification, uses linguistic a nd prosodic features such as the probabilities of unigrams and bi-grams occurring as the first or last unigram (or bi-gram) in utte rances, and the length of pauses between two words. Once sentences have been identified automatically, we capitalize the first words of the resulting sentences and add a sentence-ending punctuation mark to each. Next, analysis of call transcripts reveals that calls can be divided into segments that denote progress through a standard call flow. We work with these twelve domai n-independent segment types: In addition, for the automotive domain, we identify these domain-specific call segment types: Several of our system X  X  processes, including the Call Logging CAB, require knowledge of whic h segment type contains a portion of the transcript that they specialize for. After the utterances are identified, call section segmentation (see [10] ) applies a Support Vector Machine (SVM) classifier trained on manually segmented calls to assign one of the target call segment types to each utterance. Adjacent utterances of the same section type are merged. The Template CAB extracts the information required for the different structured fields in the CRM record (template). This information is identified in one of two ways. In the first case, a manually created domain dictionary or semantic lexicon drives a dictionary annotator to create annotations specifying semantic types (e.g. vehicle brands and m odels). The second method uses extraction patterns encoded as manually written finite state grammars. This method is most often used to extract non-vocabulary artifacts such as telephone number, vehicle year, warranty status, or VIN. The job of the Call Logging CAB is to prepare a candidate call log that is presented to the agent for editing and for storing in the Comments field in the CRM activity record that is created for the call. It creates the candidate log by selecting sentences from the normalized call transcript that appear to contain information that best characterizes what happened during the call. A number of heuristics, based on expression content, lexical content, and syntactic analysis, are used to select these sentences and to assign importan ce scores to them. Further syntactic analysis is used to label the selected sentences which belong to the log sections that de scribe the customer statements, the customer request, and the next action/solution advised by the agent. After labeling, a number of sentences sufficient to create a log record of the required length is extracted from the normalized transcript and copied into the  X  X utomatic log editor X . This processing begins with two cascades of finite state grammars. The first cascade creat es  X  X ue X  annotations over the transcript, indicating locations where material important for a call log might be found. The cue types annotated are: We build the candidate log text by extracting enough of the highest-scoring candidate-annotated sentences to create a log record that fits into the space av ailable for it in the CRM record. The selected sentences are added to the candidate log in the order in which they occur in the transcript. Each sentence (or group of adjacent sentences) is ma rked with a label (e.g.,  X  X ust States: X ), based on the type of its candidate annotation. A final cleanup is performed if the log record begins with a  X  X RM Advises X  label. In that case, we try harder to find a (lower-scoring) sentence that is a CustStates or CustSeeks candidate and that precedes the first CrmAdvises sentence. The second set of preliminary gra mmars uses the cues and other syntactic structures to determine when a sentence should be annotated with one of three  X  X andidate X  annotation types. The types are: After these preliminary annotati ons have been added to the transcript, the call log generator assigns a score to each sentence. Currently, the score for a sentence is a simple count of the number of typed expressi ons, domain terms, and log cues found in the sentence. Although a more sophisticated empirical approach to computing these scores may eventually be preferable, we have not found it necessary in our work so far. In particular, while normalizing the scores for sentence length would ordinarily be advisable to reduce the bias toward long sentences, we have not done so because (a) we prefer longer sentences for our logs and (b) our sentence boundary detection heuristic tends not to produce very long sentences, anyway. An anonymized sample candidate log record is shown in Figure 2. Note that, at the top of the record, we have added the Year, Make, and Model of the customer X  X  vehicle. This information would have appeared in senten ces or sentence fragments spoken by the caller somewhere in the call and would have been annotated as domain terms, but, because they appear in the template as identified by the Template CAB, their sentences were not selected for the log. However, during the PoC testing, the agents requested this display. Ellipses within a paragraph indicate that the surrounding sentences were not adjacent in the normalized transcript. Finally, we save the agents some typing by pre-adding a  X  X ignature line X , containing the agent X  X  name, organization, and work location. Note that we omit some of the typed expressions from the score assigned to a sentence. Specifi cally, those expressions which the Template CAB will present to the user either in the CRM record or in the structured part of the log (see the discussion below under  X  X usiness Results X ) should not be redundantly given in the text of the log. The expression types we ignore include phone numbers, tic ket numbers, and VINs. Figure 2. Sample Candidate Log Record One of the goals for the CAB project was to show our industrial partner that, by using CABs, contact center agents could save substantial amounts of time during call handling. To do this, we designed a Proof of Concept (PoC) test that allowed us to measure actual agents X  use of the integrated CAB system with transcripts generated from record ings of previously-handled calls. The test included a supervis or survey tool that allowed us to get contact center supervisor s X  evaluations of the agents X  performance and quality of results when using the system. We conducted a two-week PoC in one of the several contact centers where our industrial part ner X  X  customer support calls are actually handled. For the test, we selected eight agents who normally provide support for some of the partner X  X  car brands. Four of the agents had fewer th an six months tenure in that support center; the other four had more than six months tenure. We also used two supervisors from the same support center. One of them had been in th is position only two months; the other was much more experienced. Calls used during the PoC were recorded during the three weeks manufacturer X  X  vehicles in Nort h America and could have been handled originally at any of the contact centers, not just the one where the PoC was conducted. Beginning with 3775 audio files of  X  X hone transactions, X  we a pplied filters to select inbound calls only. We also eliminated calls of less than two minutes and more than ten minutes of talking time. This was to ensure a minimum amount of content, to avoid quick transfer calls, and to limit the time spent on calls during the test. Calls that contained no mention of a brand or model handled in the PoC (we focused on a smaller subset of the brands) were also excluded. The set that passed all the filters was used as the basis for the PoC. It contained a total of 646 calls. Each of these calls was assigned to both an experienced and a less experienced agent to evaluate the usefulness of the CAB system for these two groups. As our automatic filters were not perfect, we asked the agents and supervisors to manually exclude (i.e., skip) these same categories and several additional ones that could not be excluded automatically from the analysis: On each day of the PoC, the agen ts each participated in a two-hour session during which they listened to the recorded calls through individual headphones and reacted to the results of the various CABs presented in a surv ey tool on their computers. The agents had a notepad application available to take notes during the call. While listening to the call, each agent was asked to rate the results of the CABs, to create their own log of the call, and to edit and then rate the automatically-created candidate log from the Call Logging CAB. The amount of time agents spent taking notes and creating their own log was captured. The agents also filled out an end-of-call survey and an end-of-day survey to assess their reactions to the various parts of the project. The results of this activity were captured and stored by the survey tool. Supervisors spent three hours each day listening to calls from the same collection, assessing the quality of the call handling and the call logs, and using other tools which will not be discussed in this paper. If an agent or supervisor detected that any call was outside of the scope of the PoC (i.e., if it failed to pass the filters mentioned earlier), they immediat ely clicked the  X  X kip X  button which marked that call so that other agents would not need to listen to it. In general, our survey s ubjects were given no special instructions, except that, on the fi rst day of the PoC, we walked them through all parts of the survey tool to ensure that they understood the tasks. Participants were encouraged to work at their normal speed. The survey was used to evaluate performance improvement for all of the CABs in our system. In the remainder of this paper, however, we restrict our atten tion to measurements concerning the Call Logging CAB. Using data obtained during the PoC, we wanted to establish two things about the logs from the Call Logging CAB. First, we wanted to show that, using the CAB, agents can create logs faster than they do today using NotePad, Microsoft Word, and/or the CRM X  X  text editor. S econd, we wanted to show that this increase in speed does not lead to a reduction in the quality of the logs that are created. The Call Logging CAB was evaluated by having the survey agents create two logs for each call that they listened to. The first one emulated the normal log-writing conditions experienced by the original agen ts when they took the call. That is, the survey agents were given a NotePad-like editor in which they made whatever notes they wanted while listening to the call. At the end of the call, they used these notes within a manual-log-editing window to create their manual log. (All of the survey tool X  X  editing windows were instrumented so that the time agents spent using them could be measured and recorded in the agent evaluation data.) Time spent in both the NotePad editor and the log editor was measured and summed to give the  X  X anual log creation X  time. The second log for each call was created by editing a candidate log generated automatically by the Call Logging CAB, within an automatic-log-editing window. During the test, we collected a dataset comprising 275 survey agent evaluations of 179 calls. (Recall that calls were each evaluated by two agents.) These evaluations contained 275 manual logs and 269 edited automatic logs. It should be stressed that, for our evaluation, our baseline was the manual log creation time of the survey agents and not of the agents who originally handled the calls we recorded. Since we could not instrument the operational system used by the original agents, we did not have access to information about the time they spent writing their own logs for those calls. We believe, however, that the survey agents X  manual log creation times constitute a more  X  X onservative X  baseline for our measurements, since they are, in general, shor ter than those of the original agents. One reason for this is that the original agents often wrote log material while the customer was placed on hold, whereas the survey agents did not. We measured daily averages for manual log creation time and automatic log editing time, over the course of the first nine days of the PoC. As shown in Figure 3 , we observed that, whereas average manual log creation time remained more or less the same throughout the test (rangi ng from 250 to 300 seconds per call), automatic log editing time decreased from an average of 225 seconds per call on the first day to 100 seconds per call on the ninth day). Figure 3. Time for writing (manual) and editing (automatic) We believe that the increased savings result from increased familiarity of the survey agents with the style and properties of the automatically-generated logs. Figure 3 shows that, over the same time period, there was no si gnificant change in the length of time it took the survey agents to write their manually-generated logs. It remained at between 250 and 300 seconds for the whole PoC. However the automatically-generated logs were created steadily faster. Furthermore, several of the agents told us that they thought that the quality of the automatically-generated logs had improved during the course of the experiment. In truth, nothing changed beyond the aforementioned addition of the Template CAB results; certainly nothing in the quality of the ASR or the text normalization improved during the test. The only explanation for the agents X  opinion is that they became more familiar and facile with the automatically-generated logs over time, and perhaps more tolerant of certain types of ASR errors. 
Figure 4. 50% to 65% of log-creation time can be saved by editing the automatic log, compared to creating a manual log Using these timing results, we es timated the proportion of time a trained agent can save using the Call Logging CAB. Figure 4 shows that, by the final day of our measurements, agents editing the automatic logs were using only one-half to one-third of the time required to log the call manually. The range of estimates here is due to our varying estimat es of how much of the time the survey agents spent writing in NotePad contributed to creating the final edited automatic log. The curves in the graph show a final savings of 63%, 58%, or 51%, corresponding to estimates that the adjustment should be 0%, 10%, or 25%, respectively. The actual amount here will depend on the GUI; if the system is able to give early feedback about the extracted facts, agents may not find it necessary to take as many notes. Based on these measurements and estimates, we conservatively claim that the Call Logging CAB will result in at least a 50% reduction in the amount of time ag ents spend logging their calls, once they become familiar with th e tool. This is conservative because it is based on the assumption that 25% of the NotePad contents are used in the edited automatic logs  X  an amount that far exceeds the proportion we observed in the actual logs created by the survey agents. Our second evaluation objective was to show that any speedup we experience with the Call Logging CAB does not come at the expense of logging quality. To show that, we asked the supervisors to evaluate the quality of the logs by assessing the extent to which a number of quality statements applied to them. Our measurements showed that the supervisors judged the edited automatic logs to be better than logs written by the original agents, as well as those written by the survey agents. After listening to each call, the supervisors assessed up to four logs: 
These four logs were presente d to the supervisors in random order, in order to avoid any bias. It was apparent, however, that the supervisors could te ll the difference between the human-written logs and the automatically-generated ones. One difference, for example, was that the automatically-generated ones had more consistent capitaliza tion patterns, due to our text normalization. 
The quality statements assessed were: About right Figure 5. Supervisor eval uations of log accuracy The assessments by the supervisor s showed us that, in general, they preferred the edited automatically-generated logs to the manual logs generated both by the original agents and by the survey agents. Figure 5 shows this for the question  X  X he log accurately captures what was actually said in this call. X  Supervisors strongly agreed with this statement over 30% of the time for the edited logs, while th ey strongly agreed with it less for both types of manual logs. The sum of  X  X gree X  and  X  X trongly agree X  judgments (almost 60%) for edited logs also exceed that for either of the manual log types. Similarly, Figure 6 shows supervisor assessments of log detail. They thought that the edited logs were  X  X bout right X  more frequently than for the manual logs. And the proportion of calls that they thought were  X  X ot de tailed enough X  was lower for the edited automatic logs. Note that the supervisors also thought that some of the automatically-generated logs were  X  X oo detailed. X  This may not be surp rising, since, in general, the CAB X  X  logs were longer than the manual ones. The data for the other two statem ents is not shown here, but is consistent with the claim that the quality of the logs generated by the Call Logging CAB is not lower than the quality of manually-generated ones. This accomplishes our second objective. The quality statements we asked the supervisors to assess were motivated by the function that logs serve in the contact center environment. Logs are required to be accurate and complete records of what was said during customer calls, to support future use both by the original agent and by a new agent who may take up the case. In addition, they are used by supervisors when doing agent performance evaluations. Even though our measurements estab lished that, using the Call Logging CAB, agents can generate log records that meet these requirements at (conservatively) half the cost, we might nevertheless ask ourselves how it can be that imperfect speech recognition and text analysis technology can succeed. We believe that part of the answer lies in the mechanics of the Call Logging CAB. The normalization that it performs yields text that may be more readable because it contains correctly spelled words and consistent capitalization and punctuation. This contrasts with manual logs which are frequently quite irregular. Next, the 26% WER (word error rate) of our ASR tends to be due more to smalle r words (e.g., function words) and less to domain terms. Therefore, word errors often do not affect comprehension. And when an error must be corrected, it may be easier for the agent to correct it than it would have been to capture the whole thought manually. To these observations, we add that automatically-generated logs may be more consistent than manually-generated ones. Automatic paragraph labeling is a reliable indicator of who said what during the call. The management and use of domain terminology allows the Call L ogging CAB to use consistent spelling of domain terms such as names of components, models, and trim levels  X  things which va ry considerably in manual logs. The fact that everything said during a call is available for selection by the log generation heuristics decreases the likelihood that something important will have been missed by the agent in a manual log. When an agent edits an auto matically-generated call log sentence, it is possible to distinguish changes which correct speech recognition errors from changes made for other reasons. Using methods akin to those described in Yu, et al. [18] , the speech corrections could be used as feedback to the ASR subsystem, in order to improve its performance and so reduce editing time. Specifically, lexical corrections can be used to improve the lexicon in the ASR system X  X  language model. Not only will the agent have fewer word or phrase errors to fix, the sentence selection mechanism will make better choices due to fewer errors in domain vocabulary. We plan to experiment with these approaches, as one of our techniques for enhancing the domain adaptability of the CAB system. The accuracy of VIN capture (as well as that of alphanumeric domain artifacts in general) suffers more than other extracted types from ASR errors. In contexts identified by extraction patterns, annotator access to the ASR confusion matrix (not available downstream from the ASR in the current implementation), can improve pr ecision and recall of numeric and alphanumeric data (cf. Rahim et al. [11] [12] ). Having demonstrated that semi-a utomatic call logging can save a considerable amount of time with no loss in quality, we are studying two different directions in usability: adapting to other domains, and exploiting the possibilities of real time. The question of domain adaptation is, how we can easily port the system to a different auto manufacturer with different logging requirements or to a different industry. Because the system described here was implemented quickly, it was expedient to install into it the known domain model by finding out what our industrial partner d eemed important to keep in the CRM record. We chose to build dictionaries of domain terms with their semantic labels from lis ts rather than to discover them automatically. We wrote finite state grammars to search for patterns containing particular cues, specified distance between cue and item as a variable window size, and specified the semantic type of the desired item for the template. Other systems X  X ithout the requirement for attaching semantic labels to extracted data X  X re more easily portable. For porting to another manufacturer or industry, we would, at a minimum, parameterize (from the outside) cue types, grammars, Template CAB, and Logging CAB. For the Template CAB, we are currently considering an a pproach along the lines suggested by [13] , [14] that can learn domain vocabulary and extraction patterns for the new manufacturer or industry. A promising source of training data is existing transcripts in which the data found in the matching CRM records is annotated. Due to a number of difficulties, however, the volume of transcripts that we can accurately match with CRM records is not sufficiently large. Or, more generally, we would study ways to endow an already existing domain modeling strategy with the ability to deliver the required typing information needed to determine the semantic roles filled by the domain terminology and expressions. Prospects of further reducing th e call handling time, e.g. with the Template CAB, open up when the Agent Buddies are running in real time, rather than in our simulated test system. We will mention just one. Analysis of the time spent in different parts of the conversation showed large amounts of time spent in transmission of the VIN from the caller to the agent. While we estimated in the PoC that we could save agents some time in the activities of typing the VIN, context switching for data base look up, looking it up, and copying the VIN to the template field, we determined that a significant time saving possibility lay elsewhere: in the human factors of getting an uninitiated caller to understand what a VIN is, where to find it (vehicle registration sticker on the windshield), and how to read it without tripping, making errors, or restarting. (Recall also that the VIN, as a 17-digit alphanumeric, is most vulnerable to both human errors and ASR errors.) One solution would be for the Automatic Voice Recognition System (AVR) that first picks up the call to tell callers that it wants the VIN now. Recitation to a known machine is probably cl eaner than reading to a human (less likely for the caller to say,  X  X hree two I guess its an eight X ). Or, if agents know the characteris tics and strengths of the Agent Buddy to recognize the information that the caller is transmitting, they can guide the caller to do the optimal thing for recognition and analysis (for example, interrupting a long VIN recitation with nothing more than a reassuring  X  X ep X ) or explain that the caller is talking to a machine. Fewer different caller behavior patterns would have to be supported by the CABs, because the agent is getting instant feedback in the displayed template of elicitation failure and can guide a retry. Variations for other data types can also be imagined. We are grateful to Larry Bates for alerting us to the importance of call logging in the contact center environment and for encouraging us to focus on AHT reduction as a success metric for our work. In any system-building effort in a company like IBM, there are many people behind the scenes. We thank the IBM First-of-A-Kind (FOAK) team for helping us get initial funding for the project and the IBM Managed Business Process Services (MBPS) organization for their support of interactions with our industrial partner. We are grateful to many unnamed people at our industrial partner for answering countless questions about how their operations work. We especially want to acknowledge the expertise a nd generosity of the agents, supervisors, and management at the PoC contact center. Finally, we thank Branimir Boguraev, co-inventor of AFst, for his advice and support as we wrote our analysis grammars. [1] Boguraev, B. and Neff, M. 2008. Navigating through [2] Boguraev, B. and Neff, M. 2003. The Talent 5.1 TFst [3] Ferrucci, D. and Lally, A. 2004. UIMA: An architectural [4] Hori, C., Furui, S., Malkin, R., Yu, H., and Waibel, A. [5] Koumpis, K. and Rena ls, S. 2005. Automatic [6] Maskey, S. and Hirshberg, J. 2005. Comparing lexical, [7] Maskey, S. and Hirshberg, J. 2006. Summarizing Speech [8] Mishne, G., Carmel, D., Hoory, R., Roytman, A., and [9] Murray, G., Renals, S., Carletta, J. and Moore, J. 2006. [10] Park, Y. 2007. Automatic Ca ll Section Segmentation for [11] Rahim, M., Riccardi, G., Saul, L., Wright, J., Buntschuh, [12] Rahim, M., Riccardi, G., Saul, L., Wright, J., Buntschuh, [13] Riloff, E. 1996. Automatically Generating Extraction [14] Riloff, E., and Wiebe, J. 2003. Learning Extraction [15] Roy, S. and Subramaniam, L. 2006. Automatic Generation [16] Soltau, H., Kingsbury, B., and Zweig, G. 2004. The IBM [17] Takeuchi, H., Subramaniam, L., Nasukawa, T., Roy, S., [18] Yu, D., Hwang, M., Acero, A., and Deng. L. 2004. [19] Zechner, K. 2002. Automatic Summarization of Open-[20] Zweig, G., Siohan, O., Saon, G., Ramabhadran, B., Povey, 
