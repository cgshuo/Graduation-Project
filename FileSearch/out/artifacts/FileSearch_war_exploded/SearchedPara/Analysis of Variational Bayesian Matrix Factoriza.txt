 The problem of estimating a matrix that describes a linear relation between two vectors has been extensively studied by the name of multivariate linear regression with multiple responses, canonical correlation analysis, or reduced rank regression [1]. On the other hand, a recent focus of matrix estimation inc ludes imputation of missing entries of a single matrix, e.g., in the context of microarray data analysis [2] and recommender systems 1 [3,4]. In this paper, we consider the problem of interpolating missing entries of a matrix.

The paper [5] proposed the weighted low-rank approximation (WLRA) method, which based on the expectation-maximization (EM) algorithm: a matrix is fitted to the matrices by the singular value decompositio n (SVD) in the M-step. The web article [4] proposed the regularized SVD method, which minimizes the loss function combined with the Frobenius-norm penalty by gradient descent. If the trace-norm penalty is used instead of the Frobenius-norm penalty, a low-rank solution can be obtained without having an explicit low-rank constraint; w hen the trace-norm pena lty is combined with the hinge-loss, a semi-definite programming formulation is obtained [6] (see also [7] for a gradient method with smooth approxi mation). When the trace-norm penalty and the squared-loss are used, a computationally efficient algorithm is obtained [8].
The above methods, minimizing a loss function combined with a regularizer, could be viewed as Bayesian MAP estimation. On the other hand, it is said that full-Bayesian estimation (considering the entire posteri or distribution) is often more accurate than only taking into account the mode of the posterior distribution [9]. However, working tional approximation [10] is known to be a useful approach to coping with this problem. Following this idea, the papers [11,12] proposed variational Bayesian (VB) approaches to matrix factorization and experimentally showed their superior performance. In this paper, we try to give a theoretical insight into the experimental facts that the VB approach often has better performance; more specifically, we investigate how the VB method avoids overfitting and why a low -rank solution tends to be produced. We first show that the VB solution can be regarded as a type of the positive-part James-Stein shrinkage estimator [13], which is known to dominate the least-squares estimator torization model induces non-identifiability , i.e., the decomposition is redundant and therefore the mapping between factorized matrices and the original matrix is not one-to-one [14,15]. We then analyze the generalization performance of the VB solution basedonthe Marcenko-Pastur law [16,17], which elucidates the limiting distribution of eigenvalues of the central Wishart distribution . lem and then review existing approaches. 2.1 Formulation Let us consider the problem of estimating a target matrix X (  X  R L  X  M ) from its obser-vation Y (  X  R L  X  M ) . In the case of recommender systems, the entry X l,m represents Assume that the observed matrix Y is subject to the following additive-noise model: where E (  X  R L  X  M ) is a noise matrix. Assuming that each entry of E is independently subject to the normal distribution with mean zero and variance  X  2 ,wehavethefollow-ing likelihood: where  X  2 Fro denotes the Frobenius norm of a matrix.

If there are missing entries in the observation Y , the likelihood is expressed as where  X  denotes the Hadamard product (or the element-wise product) and W is the L  X  M matrix with W Let 2 H =min( L, M ) , and let us decompose the matrix X into the product of A  X  R M  X  H and B  X  R L  X  H : X = BA , where denotes the transpose of a matrix/vector. Then, the likelihood (3) is written as 2.2 Maximum A Posteriori (MAP) Estimation An estimate of X can be obtained by a procedure similar to the expectation-maximization (EM) algorithm in latent variable models 3 .Let Z (  X  R L  X  M ) be a Then the maximum a posteriori (MAP) solution can be obtained by the following EM algorithm: E-step: Z ( t ) = W  X  Y +(1  X  W )  X  X ( t ) , M-step: X ( t +1) =argmax X p ( Z ( t ) | X )  X  ( X ) , where t =0 , 1 ,... is the iteration number. The M-step corresponds to MAP estimation Weighted Low-Rank Approximation (WLRA): To avoid overfitting, the paper [5] proposed the WLRA method, which approximates the matrix X with a given rank H (  X  H ) . This can be regarded as the WLRA method can be obtained from the following prior distribution on X :  X  ( X )  X  largest singular value of X ,and c 2 is a constant determined by H . Then the M-step yields where  X  h is the h -th largest singular value of Z ,and  X  a right and the left singular vectors, respectively. Thus the WLRA algorithm sharply cuts off irrelevant singular values for avoiding overfitting.
 Matrix Estimation with Trace-Norm Regularization (METR): Another possibil-ity of avoiding overfitting would be regularization X  X he METR method employs the trace-norm regularization , which imposes the 1 -norm constraint on the singular val-ues [8]. METR can be obtained from the following prior distribution on X :  X  ( X )  X  exp  X  1 c 2 H h =1  X  h , where c 2 is a hyperparameter. The M-step yields Note that the METR method can also be obtained as MAP estimation when Gaussian priors are assumed on A and B as Eq.(10) [6].
 Matrix Estimation with Frobenius Regularization (MEFR): Another regularization approach is to use the Frobenius regularization . The MEFR method imposes the 2 -norm constraint on the singular values. MEFR is obtained from the following prior distribution on X :  X  ( X )  X  exp  X  1 2 c 2 H h =1  X  2 h . The M-step yields However, the MEFR method is not useful in missing entry completion as it is since it only proportionally shrinks the original matrix and therefore missing values are always zero. Thus the MEFR method should be combined with a low-rank constraint [4]. Maximum-Margin Matrix Factorization (MMMF): The paper [6] proposed a ma-trix factorization method called MMMF, which involves the trace-norm regularization similar to the METR method, but employs the hinge-loss inspired by the large-margin principle of support vector machines. For the binary observation Y (  X  X  X  1 } L  X  M ) ,the MMMF optimization problem is expressed as where  X  is a regularization parameter and l,m goes over all non-missing entries of Y .
The MMMF method could also be regarded as MAP estimation with the same prior as METR; but the noise model is different from Eq.(2). 2.3 Variational Bayes (VB) Estimation The papers [11,12] proposed matrix factorization algorithms based on the VB approach [10] to approximating the posterior p ( A, B | Y ) .
 of A and B is written as follows: r ( A, B ) : The VB approach approximates the posterior p ( A, B | Y ) within a function class where A and B are independent of each other: Then, using the variational method to minimize Eq.(6), we obtain the following conditions: where  X  p is the expectation over a distribution p .Since p ( Y | A, B ) is bilinear with respect to A and B (see Eq.(4)), the expectations in Eqs.(8) and (9) can be calculated simply by using the Gaussian integration.

Let us assume the Gaussian priors on the factors A and B : Gaussian. Based on this property, the papers [11,12] proposed algorithms that iteratively update the mean and the covariance of A and B by Eqs.(8) and (9), respectively. Then the posterior mean of BA , i.e., BA r ( A,B ) , is outputted 4 as an estimate of X . VB estimation in general is shown to be a useful alternative to MAP estimation [10,9], and its good performance has been theoretically investigated in the light of model non-a parameter value and a probability distribution is not one-to-one [14,15].
The VB-based matrix factorization methods reviewed in Section 2.3 are shown to work well in experiments [11,12]. However, their good performance was not completely understood beyond their e xperimental success. In this section, we theoretically inves-tigate properties of a VB-based matrix factorization method. Note that the factorized matrix model (4) is also non-identifiable since the mapping between ( A, B ) and X is not one-to-one.

In order to make the analysis feasible, let us consider a variant of VB-based matrix factorization which consists of the following VBEM iterations: VBE-step: Z ( t ) = W  X  Y +(1  X  W )  X  X ( t ) , 3.1 Regularization Properties of VBEM Here, we investigate the regularization properties of the above VBEM algorithm. Unlike other MAP estimation methods, the VBM-step is not explicitly given. We first show an analytic form of the VBM-step, and then e lucidate the regularization mechanism of VBEM.
Note that our analysis below can be regarded as an extension of the paper [15], which analyzes properties of reduced rank regression in asymptotic settings. In the current since only one observation matrix is available.
 Analytic Solution of VBM-Step: Let Then, we have a simpler update rule than Eqs.(8) and (9) as follows: Substituting Eqs.(2) (with Y = Z and X = BA ) and (10) into Eqs.(12) and (13), we can express the VB posterior as where N d (  X  ;  X ,  X  ) denotes the density of the d -dimensional normal distribution with mean  X  and covariance matrix  X  . Note that  X  a where I d denotes the d -dimensional identity matrix. Solving the system of equations (15) X (17), we have the following theorem (its proof is omitted due to lack of space): Theorem 1. Let X be the VB posterior mean of X , i.e., X = BA r ( A,B ) .Let Let  X  h be the h -th largest singular value of Z and let  X  a right and the left singular vectors. Then X is analytically given by  X  Furthermore, when L = M , the VB posterior is explicitly given by Eq. (14) with Regularization Mechanism of VBEM: From Theorem 1, we have the following in-terpretation.
 If c a c b  X  X  X  ,  X  h vanishes and the VB estimator is expressed as Thus, the positive-part James-Stein (PJS) shrinkage operator [13] is applied to singular values in a component-wise manner. The PJS estimator has a regularization effect that the estimation variance is reduced by shrinking the estimator (but the bias is increased in turn). It has been proved that the PJS estimator dominates the least-squares estimator under some conditions.
 If L = M and  X  h is large enough, the VB estimator is expressed as as a similar effect to the trace-norm regularization (i.e., the 1 -norm regularization of singular values; see Section 2.2).
 If  X  h is large enough and  X  h  X  c X  h ( 0  X  c  X  1 ), the VB estimator is expressed as Thus, the singular value  X  h is shrunk proportionally. This may be regarded as a similar effect to the Frobenius-norm regularization (i.e., the 2 -norm regularization of singular values; see Section 2.2).

Thus, VBEM regularizes the solution based on the combination of PJS shrinkage, trace-norm regularization, and (possibl y) Frobenius-norm r egularization. Posterior Mode, Posterior Mean, and Model Non-Identifiability: When the uniform prior (i.e., c 2 a ,c 2 b  X  X  X  )isusedandtherankof X is not reduced, one may intuitively think that no reguralization mechanism is involved. This intuition is true when MAP estimation is used X  X AP estimation merely results in maximum likelihood (ML) es-timation which has no regularization effect. However, in VBEM, the solution involves the PJS-type regularization (see Eq.(24)) and therefore overfitting can be avoided even agreement with the exprerimental results reported in the paper [12].
 Based on Theorem 1, we explain the reason for the significant difference between MAP and VB. For illustration purposes, let us start from the simplest case where L = M =1 (i.e., X is a scalar) and the noise variance is  X  2 =1 . The top graphs in Fig.1 shows the contours of the Bayes posterior with the uniform prior on A and B when Z =0 , 1 , 2 is observed (the horizontal and vertical axes correspond to A and B , respectively); the MAP estimators are indicated by the dashed curves (all points on the curves are the MAP estimators, which give the same solution X ). In the bottom graphs of Fig.1, the VB posteriors X  X hich are independent Gaussians X  X re plotted for Z =0 , 1 , 2 . The asterisks indicate their expectations, i.e., the VB estimators. When Z =0 , the MAP and the VB estimators both give the same value X = B A =0 . When Z =1 , the MAP estimator gives X =1 , while the VB estimator is still X =0 . When Z =2 , the VB estimator is off the origin ( X =1 ), but is still closer to the origin (i.e., strongly regularized) than the MAP solution X =2 . More generally, as the observed value Z is increased, the VB estimator a pproaches to the MAP estimator. However, the VB solution is always closer to the origin than the MAP solution. Note that (  X  1 ,  X  1) is another VB solution when Z =2 , although only one VB solution at (1 , 1) is depicted in the figure for clear visibility.

The above analysis shows that even with the same uniform prior on A and B ,MAP and VB give different solutions X  X he VB solution tends to be more strongly regularized than the MAP solution. We focused on L = M =1 and c 2 a ,c 2 b  X  X  X  in the above analysis for illustration purposes. But from Eqs.(18) X (20) we see that the amplitude of each component of the VB estimator is no larg er than that of the PJS estimator (24) for any L , M ,and c 2 a ,c 2 b  X  0 . This means that the VB solution always tends to be more strongly regularized than the MAP solution. 3.2 Generalization Properties of VBEM Here, we investigate the generalization properties of the VBEM algorithm. First the generalization error of an estimated matrix is defined and it is decomposed into the  X  X ecessary X  part and the  X  X edundant X  part. We then elucidate properties of the redundant part, in particular, sparseness of the solu tion and the generali zation performance. Generalization Error of VBEM: Our analysis is based on the assumption that the form (2) with the true matrix X  X  and Z substituted for Y .Let H  X  be the rank of X  X  and assume Let us measure the generalization error of the VB solution X by the average Kullback-Leibler divergence from the true di stribution to the estimated distribution:
Let W d ( m,  X ,  X  ) be the d -dimensional Wishart distribution with m degrees of free-dom, scale matrix  X  , and non-centrality matrix  X  . Then, it is easy to show that ZZ follows the non-central Wishart distribution: If L&gt;M , we may simply re-define X as X so that L  X  M holds.

By assumption, X  X  consists of only H  X  singular components. Let us decompose X into the component projected onto the space spanned by X  X  (the  X  X ecessary X  part) and its complement (the  X  X edundant X  part): Then, Eq.(27) implies that the generalization error can be decomposed as
Since H  X  H by assumption, the contribution of the necessary components would be negligibly small compared with the contribution of the redundant components. Based on this reasoning, we focus on G red in the following analysis.
 Analysis of Eigenvalue Distribution of Redundant Components: Since the Gaussian noise is invariant under rotation, G red can be expressed without loss of generality as where X ( R ) denotes the VB estimator given observation R ,whichisa ( H  X  H  X  )  X  ( K  X  H  X  ) random matrix with entries independently subject to the normal distribution (denoted by N ( R ) ) with mean zero and variance  X  2 . RR follows the central Wishart distribution: distribution of the eigenvalues by matrix X defined by which satisfies 0 &lt; X   X  1 . Then, the following proposition is known regarding the distribution of eigenvalues of the central Wishart distribution.
 Proposition 1 (Marcenko-Pastur law). [16,17] In the large-scale limit where K , H , bution of the eigenvalue u of 1  X  2 ( K  X  H  X  ) RR converges almost surely to where u =(
Fig.2 depicts the eigenvalue distribution of a large-scale Wishart matrix for  X  = by  X  2 ( K  X  H  X  ) in the graph for better comparison.
Remember that the VB estimator X ( R ) eliminates the singular values (of R ) smaller than a certain positive value, which we call the VB threshold .When c a c b  X  X  X  ,  X  2 = K X  2 is the VB threshold (see Eq.(24)). Since H  X   X  0 ,wehave which corresponds to a lower bound of the VB threshold for any H  X  ,c 2 a ,c 2 b  X  0 (see Eq.(18)). In Fig.2, eigenvalues smaller than this threshold (which is normalized to one in the figure) are discarded.
 Analysis of Sparseness of Redundant Components: We can evaluate the proportion of the singular values larger than the VB threshold as follows. Let tribution. J k ( u 0 ) for k =  X  1 , 0 , 1 has analytic forms as follows: Proposition 2. [15] J k ( u 0 ) has the following analytic forms for k =  X  1 , 0 , 1 .
This proposition enables us to calculate th e proportion of nonzero redundant compo-nents as shown in the following theorem: Theorem 2. Let  X  be the proportion of nonzero redundant components in the large scale limit. Then, its upper bound is given by The equality holds when c a c b  X  X  X  .
 This theorem implies that VBEM gives a low-rank solution without explicit rank restric-singular values always become zero in VBEM; in practice the solution would be even more sparser.
 Analysis of Redundant-Component Generalization Error: Next, we obtain the fol-lowing theorem which enable s us to evaluate the value of G red : Theorem 3. The upper bound of the contribution of the redundant components to the generalization error in the large-scale limit is given by The equality holds when c a c b  X  X  X  .
 Based on the above theorem and Proposition 2, we can compute the value of G red analytically. In Fig.4, G red /C for VBEM estimation and MAP estimation (which is equivalent to ML estimation due to the flat prior) are depicted. G red /C for MAP is independent of  X  and is equal to 0 . 5 . On the other hand, G red /C for VBEM is increas-ing with respect to  X  , but is always much smaller than that of MAP. This implies that VBEM is highly robust against large observation noise. In this paper, we have analyzed a variati onal Bayesian expectation-maximization (VBEM) method of matrix factorization. In particular, we elucidated the mechanism of inducing a low-rank solution and avoidi ng overfitting, where the principle of the positive-part James-Stein shrinkage operato r and the Marcenko-Pastur law played im-portant roles in the analysis.

Future work is to explicitly treat the missing values in the VBEM procedure, and to
