 The Web enables persons to link related documents. Similarly it enables persons to link related data. Linking open data project, one of endeavors to link data, aims to use consist of over 13.1 billion RDF triples, which are interlinked by around 142 million RDF links in November 2009 1 . Comparing with its statistics of April 2008 (2 billion RDF triples, and around 3 million RDF links) [1], the linked data have more than six times. The trends show the linked data will be even larger in the future. on entities of Web. Let us take DBLP 2 which data are generally integrated from IEEE, ACM Portal et al, as an example. When we search Jim Smith in DBLP, DBLP returns 34 papers which DBLP thinks the single author named as Jim Smith wrote. But in fact those papers were written by 5 different Jim Smith  X  X . Furthermore, Jim Smith is gen-erally written as Smith, J. in ACM database, but as J. Smith in IEEE database. Those necessary to cleanse linked data. 
Linked data uses URIs and RDF to connect pieces of data, information, and knowl-edge on the Web. The RDF is based upon the idea of making statements about resources (in particular Web resources) in the form of subject-predicate-object ex-multi-graph. Although some solutions were proposed to cleanse relational database, few research are found on RDF data cleansing. Graph data might be transferred to relational data, but not all graph data contain uniform and enough relationship among entities. This makes the performance of traditional approaches not very well when using those approaches to cleanse graph data. 
In this paper, we study the cleansing problem of linked data, and to solve the du-plicate problem in linked data. The primary contributions of this paper are as follows: z Propose a simple and intuitive graph model for RDF data, named RDF-z Propose a simple and efficient method for graph comparison. To avoid the z Propose a solution for RDF data cleansing, based on above two. To the best The rest of the paper is organized as follows. The RDF-Hierarchical Graph model is defined in section 2. Section 3 describes the proposed method K-radius Subgraph comparison method. In section 4, we report the performance of our approach. Related work is discussed in section 5. Finally, we conclude this paper in section 6. RDF ( Resource Description Framework ) proposed by WWW Consortium 1 is used to describe metadata about information resource. RDF statement is a triple which is consisted of a subject, a predicate and an object. A set of RDF triples are RDF graph. statements, and the hypernodes denote subjects, predicates and objects. 
Since hypergraph is hardly analyzed, Haye s et al. proposed bipartite graphs model to represent RDF data [5]. In a bipartite graphs model, both hyperedge and hypernode are represented by a node in the graph, and they are called hyperedge-node and hy-pernode-node respectively. Edges in bipartite graph model are used to connect hyper-edges and their hypernodes. Then the hypergraph is transformed into a bipartite graph. According to RDF, subject and object are different from predicate in types. For duplicates must be of the same type, we distinguish the two parts in a triple by layers. The nodes of same type are in the same layer. Subject and object are set on the same layer, and predicate is on a different layer. We define the layer of predicate is higher. Then we get a new model of RDF graph, called RDF-hierarchical graph. We can not find that there are subject of one type and object of another type in one triple. That is because subject and object of different types could not construct a statement of RDF data. So the model graph is hierarchical. 
We take a triple as a unit in RDF-hierarch ical graph, for RDF-hierarchical graph is consisted of triples. Fig. 1(a) shows a unit in a subgraph of RDF-hierarchical graph. In tively. Fig. 1(b) and 1(c) shows different kinds of subgraphs in RDF-hierarchical graph. 
A unit is also a subgraph of RDF-hierarchical graph. Fig. 1(b) consists of two operations on RDF-hierarchical graph are most unit-oriented. We will give some target unit u, and G is a RDF-hierarchical graph. Definition 1. n -distance. The n -distance is used to measure the distance from a unit to a node, denoted as D . 
Du p Definition 2. K-radius subgraph. If the n -distance between any units in a subgraph SG k (P). If u i  X  SG k (P) , then max(D(u i , P))=k. In Fig. 2, we give a subgraph of RDF-hierarchical graph. From Fig. 2, we can get all distance from units in Fig. 2 to node A is no more than 2. So the subgraph shown in Fig. 2 is a 2-radius subgraph of A . two graphs. Operation 1. Subtraction. This operation, denoted as  X - X , means to subtract the nodes error. So V 1 should be larger than V 2 . Operation 2. Intersection. This operation, denoted as  X   X   X , means to intersect the nodes and edges in another graph. The result is G 1  X  G 2 ={ V 1  X  V 2 , E 1  X  E 2 }. edges in another graph. The result is G 1  X  G 2 ={ V 1  X  V 2 , E 1  X  E 2 }. Generally, in a graph, the nodes, which have smaller distance to a node P , have stronger relationship with P . These nodes restrict P , and help to disambiguate P . We call these constraints one node X  X  context . If two nodes are duplicates, they would have the same or similar context . Thus the principle of KSC is that a node is disambiguated by the context , which is presented by other near nodes and edges in a graph. 
The biggest difference between RDF-hierarchical graph and a general graph is the operations are unit-oriented in RDF-hierarch ical instead of node-oriented. In RDF-hierarchical graph, a node is disambiguated by the units around it. The reason for using units instead of nodes is that every node has inherent neighborhoods in RDF-hierarchical graph. The nearest nodes of them are statement nodes, which cannot help for disambiguating. K-radius subgraph contains most units which the n -distance from the target node is no more than K . Thus we use K-radius subgraph to reflect the con-text around the node. 3.1 K-Radius Subgraph present the process of creating K-radius subgraph intuitively, we simplify the graph in this subsection. In the simple RDF-hierarchical graph, each unit in RDF-hierarchical between the two units in RDF-hierarchical graph, and the number of nodes two units shared is the weight of the edge. As the RD F-hierarchical graph is hierarchy, the sim-ple RDF-hierarchical graph also has hierarchy. The level of the target node is level 0, and all the levels are positive. The level of a unit equals the highest level of the note in it. Fig. 3 is the result directed from Fig. 2. 
According to the Definition 1 and 2, the n -distance from units which contain the target node is 1. So these units consist of 1-radius subgraph. If K is bigger than 1, the process of finding K-radius subgraph is an iteration of extending. 
However, if we try to extend all the nodes, which are within n -distance of K to the target node, the result k-radius subgraph will be huge and complex. The goal of finding K-radius subgraph is to give some constraint to the target-node. Thus the units, which have stronger relationship with the target-node, have more worth. In a simple RDF-hierarchical graph, the edge indicates the two units have overlap. The weight of the edge represents the degree of the overlap. The larger the weight is, the more overlap two units have. More overlap means stronger relationship. The hier-archy of RDF-hierarchical graph also imp lies the degree of the relationship. The nodes in the same layer are in the same type in RDF-hierarchical graph. The units in different layers have weaker relationship. So each time, we choose the lowest level of units. Then we conclude two properties of KSC as follows: Assume the target node is P , K&gt; 1, the RDF-hierarchical graph is G . date set getting from rule 1. 
In each extending, KSC chooses the units having strongest relationship with the units which are already in the subgraph. Thus in Fig. 3, we will take u 6 out from the 2-radius subgraph of A . 3.2 Subgraph Comparison information of a node but also helps disambiguating the node. Thus duplicates should have similar K-radius subgraphs. According to the above analysis, if the K-radius subgraphs of two similar nodes are similar, the two nodes have a higher probability to be duplicates. Here we will give the method of comparing two K-radius subgraphs. 
There are many connections among nodes in different units. These connections contain much information. So the comparison of subgraphs is node-oriented. In gen-eral, most methods of comparing graphs are used in biology, and each edge refers to a regular bond. In that situation, they need only consider the construction of the graph without considering the values of nodes. That is a graph isomorphism problem. Many of them are complex and time-consuming. However, in RDF-hierarchical graph, the values of nodes are different and very important for duplicate detection. Both con-struction and value should be taken into comparing of two K-radius subgraphs. More information is supplied from K-radius subgraph. It includes the nodes, the edges, and help subgraph comparison. Thus, a special method for calculating similarity between two K-radius subgraphs is needed. 
Let us analyze the contributions of different information implied in K-radius sub-graphs. Nodes and edges are more important for comparing. However, not all the nodes in K-radius subgraphs have the same contributions. The nearer the two nodes, target node are more important for identifying the target node. So the n -distance units. The three value nodes in a unit are equal, so the nodes in a unit have the same n -distance as the unit has. According to the above analysis, we give the calculation of similarity between two K-radius subgraphs, which is shown in formula (1). SG 0 (P) = SG 0 (Q)=0. 
SimEnv SimG In formula (1), ii-1 (SG (P)-SG (P)) is a set which contains the nodes with n -distances i valid on them. The intersection is to find the common nodes and edges. Along with i similarity increase slowly. We do some experiments and show results to examine the efficiency and accuracy of the proposed approach in this section. All experiments are performed on an IBM eServer with a 1.25GHz Power4 processor and 4GB of memory, running Suse Linux Enterprise 10.0. All approaches are implemented and tested in Java. 4.1 Dataset We experimentally study the proposed approach on DBLP, which is a real dataset on publication. In summary, the duplicates in DBLP are mainly in two cases, author X  X  name ambiguity and publications X  title sharing. Using the traditional similarity com-parison is invalid in both cases. 
We store all data in DBLP in RDF triples. Since there are huge amount of triples, it is hard to measure the results by hands. So we pick up 3 groups of triples from DBLP. Each contains 5,000 triples. We use a semiautomatic method to calculate the numbers of duplications. Table 1 shows more details of the 3 groups. 
In Table 1, three groups contain different numbers of entities, although they con-tain the same number of RDF triples. This is induced by relationships among RDF triples. The three groups we pick up for testing represent 3 typical situations. In group situation. The number of entities in group 3 is between the above two groups. The ratios of the number of triples to entities are shown in Fig. 4. 4.2 Experimental Analysis The process of the proposed approach can be divided into three steps. The first step is to transfer the RDF triples into RDF-hierar chical graph. Second, calculate the simi-calculate similarity. If the similarity of the pair is higher than the threshold, the pair is context between the data. The context of a node is measured by the K-radius subgraph tween the two nodes. If the total similarity is bigger than the threshold, the two data are duplicates. 
We first pick up two pairs of nodes in RDF-hierarchical graph to test the similarity of K-radius subgraphs. One pair has smaller similarity, and the other pair has bigger similarity. The details of the test data are as follows. The test pairs: pair1 ( X  Thomas Cormen  X ,  X  Charles H. Leiserson  X ), pair2 ( X  John R. Smith  X ,  X  Smith R. John  X ). SimString ( pair1 )=0.098; SimString ( pair2 )=0.975. 
The result shows both similarities increase along with K in Fig. 5. This result can be easily proved by formula (1). The similarity is a summation. We also find some interesting phenomenon in the result. Similarity of K-radius subgraphs in pair1 in-creases slowly at the beginning, but quickly when K is bigger than 4. The similarity of nodes, pair1 has less possibility to be duplicates than pair2 . When K is small, the two nodes in pair2 share little nodes. Most units in subgraphs need to be extended. Along with increasing of K , more and more units are extended in subgraphs. The probability of subgraphs sharing nodes is higher. Meanwhile, most units of K-radius subgraphs in pair2 stop extending. Small nodes are extended in the subgraphs. Thus the above phenomenon happens. 
From the above test, the value of K should be smaller than 4 for distinguishing two posed approach in different K , and K is smaller than 4. The results are shown in Fig. 6 and Fig. 7. The threshold is the same in different K . groups. The trends of the f-measure are the same in three groups. The f -measures increase quickly when K is small, and slow down when K is bigger. The critical val-ues are reached at different K in three groups. In group 1, when K is 1, the change of There is few information of context around each entity. In this group, most duplicates are detected through similarity comparison between entity pairs. In group 3, triples detecting. In group 3, when K is 2, f -measure reaches critical values. We compare the f -measures in the three groups. Group 2 is the highest, and group 1 is the lowest. This shows that context is an important factor in duplicate detection. creases when K increases. The trends are similar in the three groups. We take group 2 comparison is small. When K increases, more and more units are extended to the K-radius subgraph. More time are needed for comparing. When K is even bigger, most units in K-radius subgraphs stop extending. The scales of the K-radius subgraphs increase slowly, and then the time needed to compare is also increasing slowly. Ac-cording to Fig. 6 and 7, although when K is 4, the f -measure is the highest, more time is needed. The f -measure is litter higher when K is 4 than K is 3, but much more time is needed when K is 4. Thus the optional value of K for group 2 is 3. Through similar analysis on group 1 and group 3, we can get the optional values of triples sharing entities with each other. In this situation, K equals to 0. Fig. 8 gives the relationship between the optional value of K and the ratio of triples to entities. 
From Fig. 8, we can induce a formula to calculate K in a dataset. We can calculate the optional value of K using the formula (2). Since RDF graph could be transferred to both entity-relationship model and simple graph model, many methods for entity-relationship model and graph model are also available for RDF graph. On entity-relationship model, many methods are proposed for duplicate detection on publication data [2, 3, 4, 9, 10]. A typical one proposed by Han [3] considers the relationship among data. For graph model, Kalashnikov proposed a domain impendent method in [7], which combines the similarity and connection for identify entities. For few methods have been proposed for RDF data cleansing, we compare the proposed method KSC with methods which are used for and use C to refer to the method proposed by Kalashnikov in [7], KSC is the proposed approach. Here the connect-path is the shorte st and least resistance path. If two units share more nodes, the connection between them is stronger. The longer the path, the bigger resistance two nodes have, the less connection they have. 
As KSC considers context of data, the more context the data have, the higher precision and f -measure. Furthermore KSC has more advantage than other methods in Group2 . The more relationship the entities have, the more effective KSC is. There may be missing some links when transfer RDF graph to entity-relationship model, the improve the precision than connections do, as shown in the result. Fig. 9(d) also shows the running time of the three methods. The proposed method time is cost in Group2 , although many context need more time to deal with. For many extra works have to do when considering the relationship between data in entity-relationship model, R needs most time. 
To present the results intuitively, we in troduce another measure, called efficiency, denoted EFF . The calculation of EFF is shown in formula (3). EFF refers to the number of duplicates whic h are correctly detected in a unit time. In Fig. 9(e), we also show the EFF s of the three methods. By comparing EFFs, the proposed method KSC has the highest efficiency of duplicate detection in any situa-tions. When there are more relationship among entities, the advantage of KSC is more outstanding. Our work solves the duplicate detection of RDF data. This work is related to two main studies: RDF data modeling and duplicate detection. 
RDF is the W3C standard model for describing metadata. The RDF data also has the problem of duplicates. RDF data represent not only the value of the data, but also the relationships among the data. In [8], Klyne et al. proposed a directed labeled graph easy to implement and represents the relationship among data clearly. However, if the relationship is complex, much information would be lost. The RDF graph is different from a common graph. It is a hypergraph, because there may have more than one edge between two nodes. Morales proposed a direct hypergraph model [11]. In this model, each RDF statement is a hyperedge in the hypergraph. This model can represent the complex relationship among data. But it can not deal with the scale of RDF data, and is hard to process more on it. Hayes propos ed a bipartite graph model [5]. This model transfers the hypergraph to a common bipartite graph. It is easy to manage and oper-semantic retrieval, like similar query and related query. They do not focus on the duplicate detection. 
Duplicate is the main inducement of data dirty. Duplicate detection belongs to data [9], which could be achieved by recursive field matching algorithm [10], Smith-Waterman algorithm, and R-S-W algorithm [2] etc. The above methods all consider the records themselves and ignore the relationship among them. 
Recently, researchers shifted their attentio n to the associations among entities. Han et al. proposed an unsupervised learning approach using K-way spectral clustering that disambiguates authors in citations [3]. It utilizes three kinds of citation attributes: co-author names, paper titles, and publication venue titles. A general object distinc-tion methodology is introduced in [12]. The approach combines two complementary measures for relational similarity: set resemblance of neighbor tuples and random walk probability, and then analyzes subtle linkages effectively. Han et al. investigated two supervised learning approaches to di sambiguate authors in the citations [4]. Kalashnikov proposed a domain-independent method [7]. This method analyzes not only object features but also inter-object relationships to improve the disambiguation However, it is difficult to set the association strength between two entities correctly. Nowadays, links among data are increasing explosively. Due to variety sources of data, RDF data have duplicates. Duplicate may cause the inference and inquire error. However, studies are seldom made on this problem. In this paper, we propose an approach to detect the duplicates in RDF data. 
The proposed approach combines both similarity and context among RDF data to detect the duplicates. Considering the co mplexity of the associations among RDF data, we propose a model for RDF, called RDF-hierarchical graph, which is improved from Bipartite Statement-Value Graphs [4]. In the model, we give a K-radius sub-graph comparison method to detect the context of the two similar nodes, to avoid the complex and high cost of graph comparison. This method explores the K-radius subgraphs of the two nodes which reflect the context . By comparing two K-radius subgraphs (KSC), we get the similarity of context between the nodes. Finally, we combine the similarity and the context between the two nodes to decide whether they are duplicates or not. 
We implement the proposed method on publication datasets, and compare the method with the methods on entity-relationship model and graph model, for seldom methods are proposed on RDF data. The results show that the proposed method im-proves accuracy and efficiency in detectin g duplicates obviously. KSC is convinced to be a more simple and quick method. Acknowledgements. The work is supported by the Natio n al 973 Key Bas i c Resear c h Program under grant No.2003CB317003. 
