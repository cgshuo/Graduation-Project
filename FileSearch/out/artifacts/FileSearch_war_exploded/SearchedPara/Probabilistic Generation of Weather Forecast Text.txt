 Over the last decade, there has been a lot of in-terest in statistical techniques among researchers in natural language generation ( NLG ), a field that was largely unaffected by the statistical revolution in NLP that started in the 1980s. Since Langkilde and Knight X  X  influential work on statistical surface real-isation (Knight and Langkilde, 1998), a number of statistical and corpus-based methods have been re-ported. However, this interest does not appear to have translated into practice: of the 30 implemented systems and modules with development starting in or after 2000 that are listed on a key NLG website 1 , only five have any statistical component at all (an-other six involve techniques that are in some way corpus-based). The likely reasons for this lack of take-up are that (i) many existing statistical NLG techniques are inherently expensive, requiring the set of alternatives to be generated in full before the statistical model is applied to select the most likely; and (ii) statistical NLG techniques have not been shown to produce outputs of high enough quality.
There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). Some re-search has moved towards a more comprehensive view, e.g. construing the generation task as a single constraint satisfaction problem. Precursors to cur-rent approaches were Hovy X  X  PAULINE which kept track of the satisfaction status of global  X  X hetori-cal goals X  (Hovy, 1988), and Power et al. X  X  ICON -OCLAST which allowed users to fine-tune different combinations of global constraints (Power, 2000). In recent comprehensive approaches, the focus is on automatic adaptability, e.g. automatically determin-ing degrees of constraint violability on the basis of corpus frequencies. Examples include Langkilde X  X  (2005) general approach to generation and parsing based on constraint optimisation, and Marciniak and Strube X  X  (2005) integrated, globally optimisable net-work of classifiers and constraints.

Both probabilistic and recent comprehensive trends have developed at least in part to address two interrelated issues in NLG : the considerable amount of time and expense involved in building new sys-tems, and the almost complete lack in the field of reusable systems and modules. Both trends have the potential to improve on development time and reusability, but have drawbacks.

Existing statistical NLG (i) uses corpus statistics to inform heuristic decisions in what is otherwise sym-bolic generation (Varges and Mellish, 2001; White, 2004; Paiva and Evans, 2005); (ii) applies n -gram models to select the overall most likely realisation after generation ( HALOGEN family); or (iii) reuses an existing parsing grammar or treebank for surface realisation (Velldal et al., 2004; Cahill and van Gen-abith, 2006). N -gram models are not linguistically informed, (i) and (iii) come with a substantial man-ual overhead, and (ii) overgenerates vastly and has a high computational cost (see also Section 3).
Existing comprehensive approaches tend to in-cur a manual overhead (finetuning in ICONOCLAST , corpus annotation in Langkilde and Marciniak &amp; Strube). Handling violability of soft constraints is problematic, and converting corpus-derived prob-abilities into costs associated with constraints (Langkilde, Marciniak &amp; Strube) turns straightfor-ward statistics into an ad hoc search heuristic. Older approaches are not globally optimisable ( PAULINE ) or involve exhaustive search ( ICONOCLAST ).
The p CRU language generation framework com-bines a probabilistic generation methodology with a comprehensive model of the generation space, where probabilistic choice informs generation as it goes along, instead of after all alternatives have been generated. p CRU uses existing techniques (Belz, 2005), but extends these substantially. This paper describes the p CRU framework and reports experi-ments designed to rigorously test p CRU in practice and to determine whether improvements in develop-ment time and reusability can be achieved without sacrificing quality of outputs. p
CRU (Belz, 2006) is a probabilistic language gen-eration framework that was developed with the aim of providing the formal underpinnings for creating NLG systems that are driven by comprehensive prob-abilistic models of the entire generation space (in-cluding deep generation). NLG systems tend to be composed of generation rules that apply transforma-tions to representations (performing different tasks in different modules). The basic idea in p CRU is that as long as the generation rules are all of the set of all generation rules can be seen as defining a context-free language and a single probabilistic model can be estimated from raw or annotated text to guide generation processes. p CRU uses straightforward context-free technol-ogy in combination with underspecification tech-niques, to encode a base generator as a set of ex-pansion rules G composed of n -ary relations with variable and constant arguments (Section 2.1). In non-probabilistic mode, the output is the set of fully expanded (fully specified) forms that can be de-rived from the input. The p CRU ( p robabilistic CRU ) decision-maker is created by estimating a proba-bility distribution over the base generator from an unannotated corpus of example texts. This distri-bution is used in one of several ways to drive gen-eration processes, maximising the likelihood either of individual expansions or of entire generation pro-cesses (Section 2.2). 2.1 Specifying the range of alternatives Using context-free representational underspecifica-tion, or CRU , (Belz, 2004), the generation space is encoded as (i) a set G of expansion rules composed of n -ary relations relation ( arg 1 , ...arg n ) where the arg i are constants or variables over constants; and (ii) argument and relation type hierarchies. Any sen-tential form licensed by G can be the input to the generation process which expands it under unify-ing variable substitution until no further expansion is possible. The output (in non-probabilistic mode) is the set of fully expanded forms (i.e. consisting only of terminals) that can be derived from the input.
The rules in G define the steps in which inputs can be incrementally specified from, say, content to se-mantic, syntactic and finally surface representations. G therefore defines specificity relations between all sentential forms, i.e. defines which representation is underspecified with respect to which other represen-tations. The generation process is construed explic-itly as the task of incrementally specifying one or more word strings.
Within the limits of context-freeness and atom-icity of feature values, CRU is neutral with respect to actual linguistic knowledge representation for-malisms used to encode generation spaces. The main motivation for a context-free formalism is the advantage of low computational cost, while the inclusion of arguments on (non)terminals permits keeping track of contextual features. 2.2 Selection among alternatives The p CRU decision-making component is created by estimating a probability distribution over the set of expansion rules that encodes the generation space (the base generator), as follows: 1 Convert corpus into multi-treebank: determine for each sentence all (left-most) derivation trees licensed by the base generator X  X  CRU rules, us-ing maximal partial derivations if there is no com-plete derivation tree; annotate the (sub)strings in the sentence with the derivation trees, resulting in a set of generation trees for the sentence. 2 Train base generator: Obtain frequency counts for each individual generation rule from the multi-treebank, adding 1 /n to the count for every rule, where n is the number of alternative derivation trees; convert counts into probability distributions over alternative rules, using add-1 smoothing and standard maximum likelihood estimation.

The resulting probability distribution is used in one of the following three ways to control gener-ation. Of these, only the first requires the genera-tion forest to be created in full, whereas both greedy modes prune the generation space to a single path: 1 Viterbi generation: do a Viterbi search of the gen-eration forest for a given input, which maximises the joint likelihood of all decisions taken in the generation process. This selects the most likely generation process, but is considerably more ex-pensive than the greedy modes. 2 Greedy generation: make the single most likely decision at each choice point (rule expansion) in a generation process. This is not guaranteed to result in the most likely generation process, but the computational cost is very low. 3 Greedy roulette-wheel generation: use a non-uniform random distribution proportional to the likelihoods of alternatives. E.g. if there are two alternative decisions D giving p ( D proportion of times the generator decides D proaches 80% and D 2.3 The p CRU-1.0 generation package The technology described in the two preceding sec-tions has been implemented in the p CRU -1.0 soft-ware package. The user defines a generation space by creating a base generator composed of: 1. the set N of underspecified n -ary relations 2. the set W of fully specified n -ary relations 3. a set R of context-free generation rules n  X   X  , 4. a typed feature hierarchy defining argument
This base generator is then trained (as described above) on raw text corpora to provide a probability distribution over generation rules. Optionally, an n -gram language model can also be created from the same corpus. The generator is then run in one of the three modes above or one of the following: 1. Random : ignoring p CRU probabilities, ran-2. N -gram : ignoring p CRU probabilities, gener-
The random mode serves as a baseline for gen-eration quality: a trained generator must be able to do better, otherwise all the work is done by the base generator (and none by the probabilities). The n -gram mode works exactly like HALOGEN -style gen-eration: the generator generates all realisations that the rules allow and then picks one based on the n -gram model. This is a point of comparison with existing statistical NLG techniques and also serves as a baseline in terms of computational expense: a generator using p CRU probabilities should be able to produce realisations faster. The automatic generation of weather forecasts is one of the success stories of NLP . The restrictive-ness of the sublanguage has made the domain of Figure 1: Meteorological data file and wind forecast for 05-10-2000, a.m. (oil fields anonymised). weather forecasting particularly attractive to NLG re-searchers, and a number of weather forecast genera-tion systems have been created.

A recent example of weather forecast text gener-ation is the S UM T IME project (Reiter et al., 2005) which developed a commercially used NLG system that generates marine weather forecasts for offshore oil rigs from numerical forecast data produced by weather simulation programs. The S UM T IME cor-pus is used in the experiments below. 3.1 Data Each instance in the S UM T IME corpus consists of three numerical data files (the outputs of weather simulators) and the forecast file written by the fore-caster on the basis of the data (Figure 1 shows an example). The experiments below focused on a.m. forecasts of wind characteristics. Content determi-nation (deciding which meteorological data to in-clude in a forecast) was carried out off-line.
The corpus consists of 2,123 instances (22,985 words) of which half are a.m. forecasts. This may not seem much, but considering the small number of vocabulary items and syntactic structures, the cor-pus provides extremely good coverage (an initial im-pression confirmed by the small differences between training and testing data results below). 3.2 The base generator matically in two steps. First, a simple chunker was run over the corpus to split wind statements into wind direction, wind speed, gust speed, gust statements, time expressions, verb phrases, pre-modifiers, and post-modifiers. Preterminal generation rules were automatically created from the resulting chunks. Then, higher-level rules which combine chunks into larger components, taking care of text structuring, aggregation and elision, were manually authored. The top-level generation rules interpret wind statements as sequences of independent units of information, ensuring a linear increase in complexity with increasing input length. Inputs encode meteorological data (as shown in Ta-ble 1), and were pre-processed to determine certain types of information, including whether a change in wind direction was clockwise or anti-clockwise, and whether change in wind speed was an increase or a decrease. The final generator takes as inputs number vectors of length 7 to 60, and generates up to 1 . 6  X  10 31 alternative realisations for an input.
The job of the base generator is to describe the textual variety found in the corpus. It makes no deci-sions about when to prefer one variant over another. 3.3 Training The corpus was divided at random into 90% train-ing data and 10% testing data. The training set was multi-treebanked with the base generator and the multi-treebank then used to create the probabil-ity distribution for the base generator (as described in Section 2.2). A back-off 2-gram model with Good-Turing discounting and no lexical classes was also created from the training set, using the SRILM toolkit, (Stolcke, 2002). p CRU -1.0 was then run in all five modes to generate forecasts for the inputs in both training and test sets.

This procedure was repeated five times for hold-out cross-validation. The small amount of variation across the five repeats, and the small differences be-tween results for training and test sets (Table 2) in-dicated that five repeats were sufficient. 3.4 Evaluation 3.4.1 Evaluation methods
The two automatic metrics used in the evalua-tions, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0 . 82 and 0 . 79 respectively) in this do-main (Belz and Reiter, 2006).
BLEU (Papineni et al., 2002) is a precision met-ric that assesses the quality of a translation in terms of the proportion of its word n -grams ( n  X  4 has become standard) that it shares with several refer-ence translations. BLEU also incorporates a  X  X revity penalty X  to counteract scores increasing as length de-creases. BLEU scores range from 0 to 1.

The NIST metric (Doddington, 2002) is an adapta-tion of BLEU , but where BLEU gives equal weight to all n -grams, NIST gives more weight to less frequent (hence more informative) n -grams. There is evi-dence that NIST correlates better with human judg-ments than BLEU (Doddington, 2002; Belz and Re-iter, 2006).

The results below include human scores from two separate experiments. The first was an experiment with 9 subjects experienced in reading marine fore-casts (Belz and Reiter, 2006), the second is a new The main differences were that in Experiment 1, subjects rated on a scale from 0 to 5 and were asked for overall quality scores, whereas in Experiment 2, subjects rated on a 1 X 7 scale and were asked for lan-guage quality scores.
 In comparing different p CRU modes, NIST and BLEU scores were computed against the test set part of the corpus which contains texts by five different authors. In the two human experiments, NIST and BLEU scores were computed against sets of multi-ple reference texts (2 for each date in Experiment 1, and 3 in Experiment 2) written by forecasters who had not contributed to the corpus. One-way ANOVA s with post-hoc Tukey HSD tests were used to analyse variance and statistical significance of all results.
Table 1 shows forecast texts generated by each of T p CRU -greedy 8.208 (0.033) 0.647 (0.002) R p CRU -roulette 7.035 (0.138) 0.496 (0.010) A p CRU -2gram 6.734 (0.086) 0.523 (0.008) I p CRU -viterbi 6.643 (0.023) 0.524 (0.002)
N p CRU -random 4.799 (0.036) 0.296 (0.002) T p CRU -roulette 6.193 (0.121) 0.496 (0.022) E p CRU -2gram 5.663 (0.185) 0.514 (0.019) S p CRU -viterbi 5.650 (0.161) 0.519 (0.021) T p CRU -random 4.535 (0.078) 0.313 (0.005) Table 2: NIST -5 and BLEU -4 scores for training and test sets (average variation from the mean). the systems included in the evaluations reported be-low, together with the corresponding input and three texts created by humans for the same data. 3.4.2 Comparing different generation modes
Table 2 shows results for the five different p CRU generation modes, for training sets (top) and test sets (bottom), in terms of NIST -5 and BLEU -4 scores av-eraged over the five runs of the hold-out validation, with average mean deviation figures across the runs shown in brackets.

The Tukey Test produced the following results for the differences between means in Table 2. For the training set, results are the same for NIST and BLEU scores: all differences are significant at P &lt; 0 . 01 except for the differences in scores for p CRU -2gram and p CRU -viterbi. For the test set and NIST , again all differences are significant at P &lt; 0 . 01 , except for p CRU -2gram vs. p CRU -viterbi. For the test set and BLEU , three differences are non-significant: p CRU -2gram vs. p CRU -viterbi, p CRU -2gram vs. p CRU -
S UM T IME -Hyb. 3.82 (1) 4.61 (2) p CRU -greedy 3.59 (2) 4.79 (1) p CRU -roulette 3.22 (3) 4.54 (3) Table 3: Scores for handcrafted system and two best p
CRU -systems from two human experiments. roulette, and p CRU -viterbi vs. p CRU -roulette.
NIST -5 depends on test set size, and is necessar-ily lower for the (smaller) test set, but the BLEU -4 scores indicate that performance was slightly worse on test sets. The deviation figures show that varia-tion was also higher on the test sets.

The clearest result is that p CRU -greedy is ranked highest, and p CRU -random lowest, by considerable margins. p CRU -roulette is ranked second by NIST -5 and fourth by BLEU -4. p CRU -2gram and p CRU -viterbi are virtually indistinguishable.

Experts in both human experiments agreed with the NIST -5 rankings of the modes exactly. 3.4.3 Text quality against handcrafted system
The p CRU modes were also evaluated against the S UM T IME -Hybrid system (running in  X  X ybrid X  mode, taking inputs as in Table 1). Table 3 shows averaged evaluation scores by subjects in the two in-dependent experiments described above. There were altogether 6 and 7 systems evaluated in these experi-ments, respectively, and the differences between the scores shown here were not significant when sub-jected to the Tukey Test, meaning that both experi-ments failed to show that experts can tell the differ-ence in the language quality of the texts generated by the handcrafted S UM T IME -Hybrid system and the two best p CRU -greedy systems. 3.4.4 Text quality against human forecasters
In the first experiment, the human evaluators gave an average score of 3.59 to p CRU -greedy, 3.22 to the corpus texts, and 3.03 to another (human) fore-caster. In Experiment 2, the average human scores were 4.79 for p CRU -greedy, and 4.50 for the corpus texts. Although in each experiment separately, sta-tistical significance could not be shown for the dif-ferences between these means, in combination the scores provide evidence that the evaluators thought p
CRU -greedy better than the human-written texts. 3.4.5 Computing time
The following table shows average number of sec-onds taken to generate one forecast, averaged over the five cross-validation runs (mean variation figures across the runs in brackets):
Forecasts for the test sets were generated some-what faster than for the training sets in all modes. Variation was greater for test sets. Differences between p CRU -greedy and p CRU -roulette are very small, but p CRU -viterbi took 1 / 10 of a second longer, and p CRU -2gram took more than 1 second 3.4.6 Brevity bias
N -gram models have a built-in bias in favour of shorter strings, because they calculate the likelihood of a string of words as the joint probability of the words, or, more precisely, as the product of the prob-abilities of each word given the n  X  1 preceding words. The likelihood of any string will therefore generally be lower than that of any of its substrings.
Using a smaller data set for which all systems had outputs, the average number of words in the fore-casts generated by the different systems was: p CRU -random has no preference for shorter strings, its average string length is almost twice that of the other p CRU -generators. The 2-gram generator prefers shorter strings, while the Viterbi generator prefers shorter generation processes, and these pref-erences result in the shortest texts. The poor evalu-ation results above for the n -gram and Viterbi gen-erators indicate that this brevity bias can be harm-ful in NLG . The remaining generators achieve good matches to the average forecast length in the corpus. 3.4.7 Development time
The most time-consuming part of NLG system de-velopment is not encoding the range of alternatives, but the decision-making capabilities that enable se-lection among them. In S UM T IME (Section 3), these were the result of corpus analysis and consultation with writers and readers of marine forecasts. In the p
CRU wind forecast generators, the decision-making capabilities are acquired automatically, no expert knowledge or corpus annotation is used.
 imately 12 person months went directly into devel-oping the S UM T IME microplanner and realiser (the components functionally analogous to the p CRU -generators), and 24 on generic activities such as expert consultation, which also benefited the mi-croplanner/realiser. The p CRU wind forecasters were built in less than a month, including familiari-sation with the corpus, building the chunker and cre-ating the generation rules themselves. However, the S
UM T IME system also generates wave forecasts and appropriate layout and canned text. A generous esti-mate is that it would take another two person months to equip the p CRU forecaster with these capabilities.
This is not to say that the two research efforts re-sulted in exactly the same thing. It is clear that fore-cast readers prefer the S UM T IME system, but the point is that it did come with a substantial price tag attached. The p CRU approach allows control over the trade-off between cost and quality. The main contributions of the research described in this paper are: (i) a generation methodology that improves substantially on development time and reusability compared to traditional hand-crafted sys-tems; (ii) techniques for training linguistically in-formed decision-making components for probabilis-tic NLG from raw corpora; and (iii) results that show that probabilistic NLG can produce high-quality text. Results also show that (i) a preference for shorter realisations can be harmful in NLG ; and that (ii) linguistically literate, probabilistic NLG can outper-form HALOGEN -style shallow statistical methods, in terms of quality and efficiency.

An interesting question concerns the contribution of the manually built component (the base genera-tor) to the quality of the outputs. The random mode serves as an absolute baseline in this respect: it in-dicates how well a particular base generator per-forms on its own. However, different base genera-tors have different effects on the generation modes. The base generator that was used in previous exper-iments (Belz, 2005) encoded a less structured gen-eration space and the set of concepts it used were less fine-grained (e.g. it did not distinguish between an increase and a decrease in wind speed, consid-ering both simply a change), and therefore it lacked some information necessary for deriving conditional probabilities for lexical choice (e.g. freshening vs. easing ). As predicted (Belz, 2005, p. 21), improve-ments to the base generator made little difference to the results for p CRU -2gram (up from BLEU 0 . 45 to 0 . 5 ), but greatly improved the performance of the greedy mode (up from 0 . 43 to 0 . 64 ).

A basic question for statistical NLG is whether surface string likelihoods are enough to resolve re-maining non-determinism in generators, or whether likelihoods at the more abstract level of generation rules are needed. The former always prefers the most frequent variant regardless of context, whereas in the latter probabilities can attach to linguistic ob-jects and be conditioned on contextual features (e.g. one useful feature in the forecast text generators en-coded whether a rule was being applied at the be-ginning of a text). The results reported in this paper provide evidence that probabilistic generation can be more powerful than n -gram based post-selection. The p CRU approach to generation makes it possi-ble to combine the potential accuracy and subtlety of symbolic generation rules with detailed linguis-tic features on the one hand, and the robustness and handle on nondeterminism provided by probabili-ties associated with these rules, on the other. The evaluation results for the p CRU generators show that outputs of high quality can be produced with this approach, that it can speed up development and im-prove reusability of systems, and that in some modes it is more efficient and less brevity-biased than exist-ing HALOGEN -style n -gram techniques.

The current situation in NLG recalls NLU in the late 1980s, when symbolic and statistical NLP were separate research paradigms, a situation memorably caricatured by Gazdar (1996), before rapidly mov-ing towards a paradigm merger in the early 1990s. A similar development is currently underway in MT where  X  after several years of statistical MT dom-inating the field  X  researchers are now beginning to bring linguistic knowledge into statistical tech-niques (Charniak et al., 2003; Huang et al., 2006), and this trend looks set to continue. The lesson from NLU and MT appears to be that higher quality re-sults when the symbolic and statistical paradigms join forces. The research reported in this paper is intended to be a first step in this direction for NLG . This research was in part supported under UK EPSRC Grant GR/S24480/01. Many thanks to the anony-mous reviewers for very helpful comments.

