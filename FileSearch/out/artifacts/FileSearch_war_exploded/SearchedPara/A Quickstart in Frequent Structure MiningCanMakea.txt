 Given a database, structure mining algorithms search for substructures that satisfy constraints such as minimum fre-quency, minimum confidence, minimum interest and maxi-mum frequency. Examples of substructures include graphs, trees and paths. For these substructures many mining al-gorithms have been proposed. In order to make graph min-ing more efficient, we investigate the use of the  X  X uickstart principle X , which is based on the fact that these classes of structures are contained in each other, thus allowing for the development of structure mining algorithms that split the search into steps of increasing complexity. We introduce the GrAph/Sequence/Tree extractiON ( Gaston ) algorithm that implements this idea by searching first for frequent paths, then frequent free trees and finally cyclic graphs. We investigate two alternatives for computing the frequency of structures and present experimental results to relate these alternatives.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications X  Data Mining General Terms: Algorithms, Performance, Theory Keywords: Frequent Item Sets, Graphs, Semi-Structures, Structures
Inrecent yearsdataminingof complicated structuressuch as graphs, trees, molecules, XML documents and relational databases has attracted a lot of research. Especially the idea of discovering frequent substructures of such databases has recently led to a large number of specialized algorithms for mining paths, trees and graphs in databases of trees or graphs. In this paper, we aim to take this research a step further by investigating the interdependencies between the patterns. Experiments on small molecular databases reveal that the largest numbers of frequent substructures in such databases are actually free trees. Free trees are much sim-pler structures than general, cyclic graphs, and efficient al-gorithms exist for free trees. Therefore, we investigate the possibilities of quickstarting the search for frequent struc-tures by integrating a frequent path, tree and graph miner into one algorithm called Gaston .

The main challenge in the development of this algorithm is how to split up the discovery process into several phases efficiently. Ideally, the algorithm should behave like a spe-cialized free tree miner when faced with free tree databases, but should also be able to deal with graph databases ef-ficiently. In this paper, we show how this can be done, and we show that the application of the quickstart principle can indeed make a difference in performance: on a common benchmark dataset, we obtain up to ten-fold speed ups in comparison with other algorithms. An important part of any frequent structure mining algorithm is its strategy for determining the support of a substructure. We consider two different strategies for computing the support of structures and introduceoptimizations for these strategies. Asmolecu-lar databases are currently the foremost application area for graph mining algorithms, we provideextensiveexperimental results for such databases.

As background knowledge for our paper we could mention a large number of publications. We will only refer to recent ones here. Due to space limitations we also omit many de-tails of our own algorithm. More details, references, and the sourcecodeofouralgorithm canbeobtainedfromourhome-page for frequent structure mining, http://hms.liacs.nl/ .
The overview of the rest of the paper is as follows: first we give preliminaries, then we discuss enumeration strate-gies, frequency evaluation and finally we give experimental results.
We will only briefly discuss the mathematical preliminar-ies. The definitions are similar to those used in other papers concerning frequent structure mining, for example [2, 5, 7, 12, 13, 14]. A labeled graph G consists of a finite set of nodes V ,asetofedges E  X  V  X  V and a labeling function : V  X  E  X  X  that assigns labels from L to all edges and nodes. We only consider undirected graphs, i.e., ( v 1 ,v thesameedgeas( v 2 ,v 1 ). An edge is incident to a node if one of its endpoints is in that node. The number of edges that is incident to a certain node is called the degree of that node. Two nodes are adjacent if there is an edge between the two nodes. A sequence of nodes v 1 ,v 2 ,...,v m  X  V is a path if for all i  X  X  1 , 2 ,...,m  X  1 } edge ( v i ,v i +1 The length of a path is defined by the number of edges in the path. If v 1 = v m the path is called a cycle. From now on, we only consider simple paths, which are paths in which v = v j if i = j . If there is a path between each pair of nodes in a graph, the graph is connected. We will only consider connected graphs. Given two graphs G 1 =( V 1 ,E 1 , 1 )and G 2 =( V 2 ,E 2 , 2 ), an embedding of G 1 in G 2 is an injec-tive function f : V 1  X  V 2 such that (1)  X  v  X  V 1 : 1 ( ( f ( v )) and (2)  X  ( v 1 ,v 2 )  X  E 1 :( f ( v 1 ) ,f ( v ( v 1 ,v 2 )= 2 ( f ( v 1 ) ,f ( v 2 )). The graph G 1 is a subgraph of G , denoted by G 1  X  G 2 , if there is an embedding of G 1 in G .If G 1 is a subgraph of G 2 and G 2 is a subgraph of G 1 then G 1 and G 2 are called isomorphic, denoted by G 1  X  G G 1  X  G 2 is a shorthand for G 1  X  G 2  X  G 1  X  G 2 .Aniso-morphism of a graph to itself is called an automorphism.
Three special subclasses of graphs are paths , free trees and rooted trees . Paths are graphs in which two nodes have degree 1, while all other nodes have degree 2. If a graph has no cycles, the graph is called a free tree. A rooted tree is a free tree in which one node, the root, is singled out. The depth of a node is the length of the path from that node to the root. To avoid confusion we will always speak of either free trees or rooted trees. In a rooted tree we draw the root of the tree as the top node. Examples are given in Figure 1.
A special kind of tree is the spanning tree. A tree is a spanning tree of a graph if it has exactly the same number of nodesandthetreeisfurthermoreasubgraphofthegraph.
We assume that a database D consists of a collection of graphs. The frequency of a graph G in D is defined by freq ( G, D )=# { G  X  X | G  X  G } , the support of a graph is given by support ( G, D )= freq ( G, D ) / |D| . primary task that our algorithm deals with is to find all graphs for which support ( G, D )  X  minsup, for some pre-defined threshold minsup that is specified by the user. An important property that holds is that G 1  X  G 2 implies that freq ( G 1 , D )  X  freq ( G 2 , D ). A consequence of this property is that any (large) graph which contains a (smaller) graph which is not frequent, cannot be frequent too. This Apri-ori property is the basis on which many frequent structure mining algorithms have been built. The process of remov-inggraphsfrom thesearch spaceusingthispropertyiscalled (frequency based) pruning.
 If for two connected graphs G 1  X  G 2 there is no G 3 with G 1  X  G 3  X  G 2 ,wecall G 2 arefinementof G 1 . Onecanshow that G 2 is a refinement of G 1 only if there is a G 3  X  G G 3 obtained from G 1 by one of the following two operations: Extending terminology from [2] to cyclic graphs, we call an operation to refine a graph a leg . A node refining leg for a graph G =( V,E, ) consists of a node in V ,anedgelabel and a node label. Examples of node refining legs are l 1 , l and l 4 in Figure 1. A cycle closing refining leg consists of two different nodes in V and an edge label. An example is leg l 5 in Figure 1. The refinement operator  X  ( G, l ) refines a graph G by adding leg l . Edges between graphs in Figure 1 correspond to refinement steps. Note that for legs l 1 and l of graph G , l 3 is also a leg of  X  ( G, l 1 )and l 1 is a leg of  X  (
G, l 3 ). Indeed, the only legs of  X  ( G, l 1 ) which are not legs of
G are legs that connect to the node that is introduced in  X  (
G, l 1 )by l 1 . Furthermore note that two legs, l 1 and l refine graph G to isomorph graphs due to automorphisms in G . One can show that for every graph an isomorphic graph can be constructed using a sequence of node refinements followed by a sequence of closing refinements.

An overview of a depth-first graph mining algorithm is given in Figure 2. To represent graphs codes are used. A graph code is a string that unambiguously defines a series of refinement steps that lead to a certain graph. The k -predecessor of a graph code is the code which contains only the first k refinement steps defined by the code. Differ-ent codes have been proposed, among which DFS codes (in gSpan [14]), adjacency matrices (in FFSM [5]) and tree codes with backtrack symbols (in FreeTreeMiner [2] and TreeMiner [15]). In order to make sure that no two isomor-phic graphs are outputted by the algorithm, in line (2) it is determined whether the current code C , which corresponds to a graph G , is the lowest (or highest) code among all pos-sible codes for graph G ; thus the graph mining algorithm only outputs graphs in a canonical code . If its code is not canonical, a graph is not further refined. To guarantee that the search can still potentially consider all possible graphs, the graph codes used in graph miners should therefore have the property that every k -predecessor of a canonical code is also canonical.

To limit the number of legs that an algorithm has to con-sider, most depth first miners constantly maintain a set of feasible legs. Exploiting the principle of frequency based pruning, once a leg is found to be infrequent, this leg should not be considered as a leg in any refined graph. Therefore, in line (4) only legs are considered which were legs of the previous graph, or which connect to the node that was last added to the graph. In order to obtain all possible legs that can be added to a new node, it is necessary to pass the graph C through the database and to compute all its occurrences. For each occurrence the legs of the new node should be determined.

A condition which is not needed for the correctness of the algorithm, but which is of vital importance for its perfor-mance, is the first condition of line (4), which states that only necessary legs should be evaluated and added to L .A legisnecessary ifamongall descendantsofthecurrentgraph code C , thereis at least one canonical graph codewhich can only be obtained by applying the refinement defined by that leg. Ideally this condition would be computable in constant time and the code  X  ( C ,l ) obtained by applying each nec-essary leg l immediately is also canonical: in that case one could remove the test in line (2) and the support of every graph would be computed exactly once.

However, all existing graph mining algorithms fail to meet these criteria. In gSpan, as part of the  X  X re-pruning X  step in line (4), legs which are not on the rightmost path, as defined by the DFS code, are removed; furthermore legs are discarded by considering the labels of the neighbors to which the leg connects. Other legs are later rejected using  X  X ost-pruning X  in line (2). In FFSM suboptimal canonical adjacency matrices are introduced, which also allow for legs that cannot be immediately added to a canonical graph. In both graph miners high computational costs are introduced as the check of line (2) may require an exhaustive search through all codes. In FreeTreeMiner [2] a linear characteri-zationofnecessary legsisgiven; however, inordertoguaran-tee completeness of the search, the definition of a canonical code for free trees is relaxed to allow two canonical codes for bi-centred free trees , thus introducing additional costs as bi-centred free trees are considered twice.

By dividing the search into three phases, we show that one can avoid the check of line (2) in many cases. More pre-cisely, we define a code for free trees with as property that one can determine necessary legs in constant time, and all these legs are canonical refinements immediately. Further-more, we show that this approach can be combined with approaches for refining paths and cyclic graphs.
In order of increasing complexity we will introduce the canonical strings that we use for each class of structures. Paths. The main problem of path enumeration is that a path can have two orientations, for example: axaxb and bxaxa . For each of these orientations, one can obtain a path code which consists of a sequence of node and edge labels. Each path has two potential predecessors which can be obtained by removing one of the endpoints; each of these predecessors has two orientations. We say that the lexico-Figure 3: An unordered rooted tree, wit hsome of its dept hsequences and legs graphically lowest among these 4 codes is the predecessor of the code. The code of a path is defined by the lowest of its two orientations. Given a path, we will see that each leg is a necessary leg for the free tree mining phase. No pre-pruning of legs in line (4) is therefore possible; some paths will thus be evaluated twice. In line (2) a path is considered to be canonical if it grows from its predecessor code. If this predecessor is symmetric, and a similar leg can therefore be added to both endpoints and there is no structural reason to prefer one endpoint above the other, only the leg which connects to the node v i with the lowest index i is considered to be canonical.
 Free trees. The free tree code that we define here is based on a code for rooted trees that was independently proposed by [1] and [11], and has strong similarities with a method proposed in [9] for unlabeled free trees. Due to space limi-tations, we will summarize the results of these papers using an example. Every rooted tree can be encoded with a depth sequence , of which examples are given in Figure 3. A depth sequence for a tree is obtained by performing a preorder depth first walk; each time that a node is visited for the first time, first its depth is outputted, then the label of the edge going into that edge and finally the label of that node; wecallthiscombinationadepthsequencetuple. Clearly, the depth sequence depends on the order with which the chil-drenof anodein atreeare visited. Foran unordered, rooted tree, the canonical depthsequenceis definedas thedepthse-quence that is the lexicographically highest sequence among all possible depth sequences for that tree.

As refinements for an unordered rooted tree it suffices to only consider legs that connect to the rightmost path of the tree, as illustrated in the figure. Whether a leg may be connected to the rightmost path can be determined in constanttimeusingthe next prefix node andthe left sibling of the new node. For every leg of the rightmost path there is a corresponding depthtuplethat would be appendedafter the depth sequence. This new tuple may not be higher than the tupleof thenextprefixnode, whichis definedby considering the largest suffix of the depth sequence that is a prefix of a corresponding left sibling subtree. Furthermore the new node may not have a higher label than its left sibling. One can show that by only considering legs that immediately yield a canonical depth sequence, no legs are discarded that are needed asrefinement later. Forexample, if aleg is added after several other refinement steps, its node label must still be lower than or equal to that of its left sibling. One can therefore characterize necessary legs efficiently.
To employ these principles in free tree enumeration, we
Figure 4: Free trees, (bi)centres and backbones use the following setup. First, for each free tree we define one path predecessor, as follows. A well-known property of free trees is that one can point out a (bi)-centre; if any longest path in the tree has odd length, the free tree has a bicentre consisting of the two middle nodes on this path; otherwise the tree has a centre (see Figure 4). A centred tree can be conceived as a single rooted tree, a bicentred tree can be seen as two separate rooted trees of which the roots are interconnected. Now consider all oriented paths of maximal length that start in the root of (each) tree. From each of these maximal paths a code can be obtained consist-ing of the labels on the nodes and edges. In centred trees, those two path codes which are lexicographically the high-est and occur in paths that only have the root in common, are called the backbone strings of the free tree. In bicentred trees, the lexicographically highest path codes in each of the two rooted trees are defined to be the backbone strings of that rooted tree. By concatenating the reverse of one back-bone strings with the other backbone string, a single path is obtained which we call the backbone of the free tree. We arrange our procedure such that this path is the predecessor of the free tree.

To refine a given path we use the following idea. First, the path is split into two parts by removing the edge be-tween the middle two nodes (in case of odd length paths) or by removing the single middle node (in even length paths). Each of the resulting paths is a rooted tree. Using the prin-ciple of depth sequences, rooted trees are grown for each of these initial rooted trees; by finally combining the rooted trees again, a free tree is obtained. To guarantee that a free tree grows only from its backbone path, no refinement of each rooted tree is allowed which would result in a different backbone in the final free tree.

In order for this procedure to work, several details have to be specified. Due to space limitations we are obliged to omit most of them here. Cases which require special at-tention are free trees that grow from symmetric paths, and procedures that allow for free trees in which the centre has more than two neighbors. We will only give details for grow-ing bicentred free trees with asymmetric backbones here. Consider one of the two initial rooted trees, then additional constraints apply to depth tuples that may be appended after the depth sequence: (1) no node may be added at a larger depth than the deepest initial node (as otherwise the resulting free tree would have a longer backbone than given by the initial path); (2) if a node is added at the highest allowed depth, the string of labels on the path from the root to the new node must be lower than that of the initial back-bone path. Furthermore, a different lexicographical order on the depth tuples is necessary. To see this consider the rooted tree defined by 0  X b 1 xc 1 xb 2 xb 1 xa 2 xc ; without modi-fied lexicographical order, this would be the canonical depth sequence for the rooted tree T in Figure 4. Depth sequences of this part of the free tree should however grow from the backbone string which corresponds to the depth sequence 0  X b 1 xb 2 xb . To allow for appending depth tuple 1 xc ,we modify the order of the labels in each depth as follows. Let D = { 0  X b, 1 xb, 2 xb } bethesetof depthtuplesobtained from the backbone string, then the order between tuples with dif-ferent depths and the order between tuples which are both not in D remain unaffected. However, if a tuple in D is compared with a tuple not in D , we define that the tuple in
D always sorts higher. In this new order 0  X b 1 xb 2 xb 1 sorts higher than 0  X b 1 xc 1 xb 2 xb and is therefore canonical. One can show that in this new order, canonical strings al-ways begin with the backbonetuples; all trees with a certain backbone can grow from the initial string of backbone tu-ples.

Using this procedure, in constant time all necessary node refining legs can be characterized precisely. The situation is worse for cycle closing legs. In contrast with the DFS code, theopportunitiesforpre-pruningsuchlegsusinglabels are more limited. In most cases all cycle closing legs are necessary for the next phase.
 Cyclic graphs. To strictly divide the frequent graph dis-coveryprocessintophases, weonlyconsider thecycleclosing refinements in the very last phase. All cycle closing refine-mentsconnect twoexisting nodesin a tree; within oursetup, during the first two phases constantly the set of all frequent closings is maintained. Once such a closing is applied, the tree becomes a cyclic graph. We define a code for cyclic graphs by concatenating two separate codes. The first code consists of the depth-sequence corresponding to a free tree. This free tree is a spanning tree of the graph. Each tuple in the sequence introduces a new node in the free tree; the nodes of the tree can be numbered by their occurrence or-der in this sequence. The second part of the code consists of a sequence of tuples of the form ( v i ,v j , ); each such tuple defines two nodes v i &lt;v j that are connected with an edge.
The necessary legs for cyclic graphs are now obtained as follows. First, all node refining legs are discarded, to make surethatcyclicgraphsonlygrowfromspanningtrees. Then, all tuples which sort lower than the closing leg last added, are discarded. The canonical code for a cyclic graph is given by the concatenated code that sorts the lowest among all possible codes for that graph.

Analternative, of which we skip thedetails here, would be to store all cyclic graphs already found and to use a graph isomorphism algorithm like Nauty [8] to check that a graph is not generated twice.
 Theoretical Evaluation. In comparison with the DFS code approach of gSpan, our method has advantages and disavantages. The possibilities for pre-pruning backward edges using labels are more limited in our approach than in gSpan. On the other hand, for one class of graphs  X  X he free trees X  our approach allows for muchmore pre-pruning, and we are able to entirely remove the costly check of line (2). An interesting question is whether it is harder to com-pute if a cyclic graph is canonical in our code than in gSpan. To compute our code several steps are needed: first, we have to enumerate all spanning trees. This numberis boundedby O ( | E c | c ), were E c is the number of edges of the graph that occurs in a cycle and c is the number of edges that should be removed to obtain a tree. Each spanning tree can then be normalized in O ( | E | log | E | ) time; the remaining closing legs can be normalized in O ( mc log c ) time, where m is the number of automorphisms of the spanning tree. In total this is O ( | E c | c ( | E | log | E | + mc log c )). If this computation is polynomial in the size of the graph to be normalized. Just like with a DFS code, this worst case is rarely reached as one can stop as soon as a better code is found than the current one. Overall, we may conclude that our approach is promising in cases where the number of labels is low and the number of cycles is not too large. We have considered several alternatives.
 Embedding lists (EL). This approach is similar to that of MolFA [4], FFSM [5] and FreeTreeMiner [2], and is based on the idea of temporarily storing all occurrences of a graph. For graphs with a single node we store an embedding list of all occurrences of its label in the database. For other graphs a list is stored of embedding tuples that consist of (1) an index of an embedding tuple in the embedding list of the predecessor graph and (2) the identifier of a graph in the database and a node in that graph. If a structure is obtained by a cycle closing refinement, the embedding list consists solely of pointers to embedding tuples of its parent structure. The complete embedding information for a struc-ture can be obtained by scanning its embedding list, and by following thepredecessor pointers. Thefrequency of a struc-ture is determined from the number of different graphs in its embedding list. An example is provided in Figure 4. For each leg of a graph an embedding list is constructed. Graph G in the example has two legs with corresponding graphs and embedding lists. When a graph is canonically refined, embedding lists for the legs of the new graph are computed as much as possible using a list join operation that joins embedding lists of two earlier legs. New embedding lists are constructed for legs that were not present in the predecessor graph. Clearly, thisapproach requiresalot ofmain memory: not only for the current graph embedding lists for all legs have to be stored, but due to the backtracking procedure, embedding lists for legs of predecessor structures must also be stored.
 Recomputed embeddings (RE). Embedding lists are quick, but scale up badly to large databases. Our other approach is similar to that of gSpan, and is based on main-taining a set of  X  X ctive X  graphs in which occurrences are repeatedly recomputed. As subgraph isomorphism is NP complete, essentially a backtracking procedure is required. We found that several techniques can increase the perfor-mance. For each graph we first compute a strategy compa-rable to a query evaluation plan. In general a breadth-first walk of a graph turns out to perform better than a depth-first walk such as used in gSpan. Furthermore, with each node in the database, we reserve space in which the graph miner can store hints . One such hint is whether a node is part of some occurrence of a predecessor structure. As a further optimization, we can apply the quickstart principle to find occurrences of graphs in free trees polynomially.
Anoverviewoftheresultsofourexperimentscanbefound in Figures 6 and 8, a description of the data sets in Figure 7. Unless noted otherwise, all experiments were performed on an Athlon XP1600+ with 512MB main memory, running Mandrake Linux 10; the algorithm was implemented in C ++ using the STL and compiled with the  X  X 3 compilation flag. We compare our algorithm with a broad range of other al-gorithms: the graph miners gSpan, FSG and AcGM [6], and the FTM free tree miners of R  X  uckert [13] and Chi [2]. All algorithms were obtained from their original authors. For our free tree mining experiments we have used a modified version of a data set generator that generates data sets mim-icking webserver access logs as described in [15]. Data set S5 is obtained by sampling 10k trees of maximal depth 5 from a master tree of 10k nodes with 3 node labels and fan-out 20. Set L10 is larger and obtained by sampling 100k trees of maximal depth 10 from a master tree of 10k nodes with 3 node labels and fan-out 20. Our main experiments regard molecular databases. We transform these datasets into graphs using the procedure given in [14]. Figure 6 gives a detailed insight into the performance of several algorithms on a commonly used benchmark [3]. By using linear regres-sion in the scale-up experiment we can estimate how much time each algorithm spends in data size independent proce-dures, such as subgraph normalization. Detailed overhead runtimes are: 1s for Gaston (EL), 3s for Gaston (RE,thus strategies are computed), 14s for gSpan and 230s for FSG.
The other datasets were obtained from the National Can-Figure 9: Differences between AID2DA99-active and compounds in NCI X 99 cer Institute [10]. To test the scale-up properties of our algorithm, we have run our algorithm on the database of all 250,251 compounds in the NCI X 99 release. Here, we subdi-vided some atom types into classes according to their posi-tion in the molecule to obtain labels. To run the algorithm based on embedding lists, we used a Sun Enterprise Server with 4 processors of 400Mhz and 4GB main memory; Gas-ton (LE) required 1.7GB memory, Gaston (RE) 150MB. To exploit the activity information that is available for com-pounds in the CAN2DA99 and AID2DA99 datasets, in [4, 12, 13] it was proposed to use version spaces . The idea is to only output molecular fragments that are frequent in the active part of a dataset, and to discard fragments which are also frequent in the inactive part. Thus better features for classifiers can be obtained. We modified our algorithm to allow for similar experiments. Using the assumption that the entire NCI database is representative for a broad range of molecules, we are interested in discovering frequent frag-ments of active compounds that have a significantly differ-ent support in the total NCI database. We performed this experiment for known active compounds of AID2DA99. Re-sults are summarized in Figure 9.

