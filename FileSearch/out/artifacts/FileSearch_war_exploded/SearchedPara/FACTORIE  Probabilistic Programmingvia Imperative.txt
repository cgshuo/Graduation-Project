 Conditional random fields [1], or discriminatively trained undirected graphical models, have become the tool of choice for addressing many important tasks across bioinformatics, natural language pro-cessing, robotics, and many other fields [2, 3, 4]. While relatively simple structures such as linear chains, grids, or fully-connected affinity graphs have been employed successfully in many contexts, there has been increasing interest in more complex relational structure X  X apturing more arbitrary dependencies among sets of variables, in repeated patterns X  X nd interest in models whose variable-factor structure changes during inference, as in parse trees and identity uncertainty. Implementing such complex models from scratch in traditional programming languages is difficult and error-prone, and hence there has been several efforts to provide a high-level language in which models can be specified and run. For generative, directed graphical models these include BLOG [5], IBAL [6], and Church [7]. For conditional, undirected graphical models, these include Relational Markov Networks (RMNs) using SQL [8], and Markov Logic Networks (MLNs) using first-order logic [9]. Regarding logic, for many years there has been considerable effort in integrating first-order logic and probability [9, 10, 11, 12, 13]. However, we contend that in many of these proposed combinations, the  X  X ogic X  aspect is not crucial to the ultimate goal of accurate and expressive modeling. The power of relational factor graphs is in their repeated relational structure and tied parameters. First-order logic is one way to specify this repeated structure, but it is less than ideal because of its focus on boolean outcomes and inability to easily and efficiently express relations such as graph reachability and set size comparison. Logical inference is used in some of these systems, such as P RISM [12], but in others, such as Markov Logic [9], it is largely replaced by probabilistic inference. This paper proposes an approach to probabilistic programming that preserves the declarative sta-tistical semantics of factor graphs, while at the same time leveraging imperative constructs (pieces of procedural programming) to greatly aid both efficiency and natural intuition in specifying model structure, inference, and learning, as detailed below. Our approach thus supports users in combining both declarative and procedural knowledge. Rather than first-order logic, model authors have access to a Turing complete language when writing their model specification. The point, however, is not merely to have greater formal expressiveness; it is ease-of-use and efficiency.
 We term our approach imperatively defined factor graphs (IDFs). Below we develop this approach in the context of Markov chain Monte-Carlo inference, and define four key imperative constructs X  arguing that they provide a natural interface to central operations in factor graph construction and inference. These imperative constructs (1) define the structure connecting variables and factors, and (4) propose jumps from one possible world to another. A model written as an IDF is a factor graph, with all the traditional semantics of factors, variables, possible worlds, scores, and partition functions; we are simply providing an extremely flexible language for their succinct specification, which also enables efficient inference and learning.
 Our first embodiment of the approach is the system we call F ACTORIE (loosely named for  X  X actor graphs, Imperative, Extensible X , see http://factorie.cs.umass.edu ) strongly-typed, func-tional programming language Scala [14]. The choice of Scala stems from key inherent advantages of the language itself, plus its full interoperability with Java , and recent growing usage in the ma-chine learning community. By providing a library and direct access to a full programming language (as opposed to our own, new  X  X ittle language X ), the model authors have familiar and extensive re-sources for implementing the procedural aspects of the design, as well as the ability to beneficially mix data pre-processing, evaluation, and other book-keeping code in the same files as the proba-bilistic model specification. Furthermore, F ACTORIE is object-oriented in that variables and factor templates are objects , supporting inheritance, polymophism, composition, and encapsulation. The contributions of this paper are introducing the novel IDF methodology for specifying factor graphs, and successfully demonstrating it on a non-trivial task. We present experimental results applying F ACTORIE to the substantial task of joint inference in segmentation and coreference of research paper citations, surpassing previous state-of-the-art results. In comparison to Markov Logic (Alchemy) on the same data, we achieve a 20-25% reduction in error, and do so 3-15 times faster. A factor graph G is a bipartite graph over factors and variables defining a probability distribution over a set of target variables y , optionally conditioned on observed variables x . A factor  X  i com-putes a scalar value over the subset of variables that are its neighbors in the graph. Often this real-and parameters {  X  ik } , where k  X  { 1 ...K i } and K i is the number of parameters for factor  X  i . Factor graphs often use parameter tying , i.e. the same parameters for several factors. A factor template T j consists of parameters {  X  jk } , sufficient statistic functions { f jk } , and a description of an arbitrary relationship between variables, yielding a set of satisfying tuples { ( x i , y i ) } . For each of these variable tuples ( x i , y i )  X  T j that fulfills the relationship, the factor template instantiates a factor that shares {  X  jk } and { f jk } with all other instantiations of T j . Let T be the set of factor templates. In this case the probability distribution is defined: As in all relational factor graphs, our language supports variables and factor template definitions. In our case the variables X  X hich can be binary, categorical, ordinal, real, etc X  X re typed objects in the object-oriented language, and can be sub-classed. Relations between variables can be represented directly as members (instance variables) in these variable objects, rather than as indices into global tables. In addition we allow for new variable types to be programmed by model authors via polymor-phism. For example, the user can easily create new variable types such as a set-valued variable type, representing a group of unique values, as well as traits augmenting variables to represent sequences of elements with left and right neighbors.
 Typically, IDF programming consists of two distinct stages : defining the data representation, then defining the factors for scoring. This separation offers great flexibility. In the first stage the model author implements infrastructure for storing a possible world X  X ariables, their relations and values. Somewhat surprisingly, authors can do this with a mind-set and style they would employ for deter-ministic programming, including usage of standard data structures such as linked lists, hash tables and objects embedded in other objects. In some cases authors must provide API functions for  X  X n-doing X  and  X  X edoing X  changes to variables that will be tracked by MCMC, but in most cases such functionality is already provided by the library X  X  wide variety of variable object implementations. For example, in a linear-chain CRF model, a variable containing a word token can be declared as the Token class shown in Figure 1. 1 A variable for labels can be declared similarly, with the addition that each Label 2 object has an instance variable that points to its corresponding Token . The second stage of our linear-chain CRF implementation is described in Section 2.2.
 Consider also the task of entity resolution in which we have a set of Mentions to be co-referenced to which it is currently assigned. An Entity is a set-valued variable X  X he set of Mentions assigned to it; it holds and maintains a canonical string form representative of all its Mentions (see Figure 1 ). The add / remove methods are explained in section 2.3. 2.1 Inference and Imperative Constraint Preservation For inference, we rely on MCMC to achieve efficiency with models that not only have large tree-width but an exponentially-sized unrolled network, as is common with complex relational data [15, 9, 5]. The key is to avoid unrolling the network over multiple hypotheses, and to represent only one variable-value configuration at a time. As in BLOG [5], MCMC steps can adjust model structure as necessary, and with each step the F ACTORIE library automatically builds a DiffList  X  X  compact object containing the variables changed by the step, as well as undo and redo capabilities. Calculating the factor graph X  X   X  X core X  for a step only requires DiffList variables, their factors, and neighboring variables, as described in Section 2.4. In fact, unlike BLOG and BLAISE [16], we build inference and learning entirely on DiffList scores and never need to score the entire model. This enables efficient reasoning about observed data larger than memory, or models in which the number of factors is a high-degree polynomial of the number of variables.
 A key component of many MCMC inference procedures is the proposal distribution that proposes changes to the current configuration. This is a natural place for injecting prior knowledge about coordination of variable values and various structural changes. In fact, in some cases we can avoid Figure 2: Examples of F ACTORIE factor templates. Some error-checking code is elided for brevity. expensive deterministic factors altogether with property-preserving proposal functions [17]. For ex-ample, coreference transitivity can be efficiently enforced by proper initialization and a transitivity-preserving proposal function; projectivity in dependency parsers can be enforced similarly. We term this imperative constraint preservation . In F ACTORIE proposal distributions may be implemented by the model author. Alternatively, the F ACTORIE library provides several default inference meth-ods, including Gibbs sampling, as well as default proposers for many variable classes. 2.2 Imperative Structure Definition At the heart of model structure definition is the pattern of connectivity between variables and factors, and the DiffList must have extremely efficient access to this. Unlike BLOG, which uses a complex, highly-indexed data structure that must be updated during inference, we instead specify this con-nectivity imperatively: factor template objects have methods ( e.g. , unroll1 , unroll2 , etc. , one for each factor argument) that find the factor X  X  other variable neighbors given a single variable from the DiffList . This is typically accomplished using a simple data structure that is already available as part of the natural representation of the data, ( e.g. , as would be used by a non-probabilistic program-mer). The unroll method then constructs a Factor with these neighbors as arguments, and returns it. The unroll method may optionally return multiple Factors in response to a single changed vari-able. Note that this approach also efficiently supports a model structure that varies conditioned on variable values, because the unroll methods can examine and perform calculations on these values. Thus we now have the second stage of F ACTORIE programming, in which the model author im-plements the factor templates that define the factors which score possible worlds. In our linear-chain CRF example, the factor between two succesive Label s and a Token might be declared as find the corresponding third argument to the factor. This simple example does not, however, show the true expressive power of imperative structure definition . Consider instead a model for dependency defines a template for factors that measure compatibility between a word and its closest verb as measured through parse tree connectivity. Such arbitrary-depth graph search is awkward in first-order logic, yet it is a simple one-line recursive method in F ACTORIE . The statistics method is described below in Section 2.4.
 Consider also the coreference template measuring the compatibility between a Mention and the canonical representation of its assigned Entity . In response to a moved Mention , unroll1 returns a factor between the Mention and its newly assigned Entity . In response to a changed Entity , different unroll methods will construct multiple copies of the same factor; they are automatically deduplicated by the F ACTORIE library. Syntactic sugar for extended first-order logic primitives is also provided, and these can be mixed with imperative constructs; see the bottom of Figure 2 for two small examples. Specifying templates in F ACTORIE can certainly be more verbose when not restricted to first-order logic; in this case we trade off some brevity for flexibility. 2.3 Imperative Variable Coordination Variables X  value-assignment methods can be overriden to automatically change other variable val-ues in coordination with the assignment X  X n often-desirable encapsulation of domain knowledge we term imperative variable coordination . For example, in response to a named entity label change, a coreference mention can have its string value automatically adjusted, rather than relying on MCMC inference to stumble upon this self-evident coordination. In Figure 1, Entity does a basic form of coordination by re-calculating its canonical string representation whenever a Mention is added or removed from its set.
 The ability to use prior knowledge for imperative variable coordination also allows the designer to define the feasible region for the sampling. In the proposal function, users make changes by calling value-assignment functions, and any changes made automatically through coordinating variables are appended to the DiffList . Since a factor template X  X  contribution to the overall score will not change unless its neighboring variables have changed, once we know every variable that has changed we can efficiently score the proposal. 2.4 Imperative Variable-Statistics Mapping In a somewhat unconventional use of functional mapping, we support a separation between factor neighbors and sufficient statistics . Neighbors are variables touching the factor whose changes imply that the factor needs to be re-scored. Sufficient statistics are the minimal set of variable values that determine the score contribution of the factor. These are usually the same; however, by allowing a function to perform the mapping, we provide an extremely powerful yet simple way to allow model designers to represent their data in natural ways, and concern themselves separately with how to parameterize them. For example, the two neighbors of a skip-edge factor [18] may each have cardinality equal to the number of named entities types, but we may only care to have the skip-edge factor enforce whether or not they match. We term this imperative variable-statistics mapping . Consider corefTemplate in Figure 2, the neighbors of the template are  X  Mention , Entity  X  pairs. from the canonical value of the Entity . This allows the template to separate the natural represen-tation of possible worlds from the sufficient statistics needed to score its factors. Note that these sufficient statistics can be calculated as arbitrary functions of the unrolled Mention and the Entity . The models described in Section 3 use a number of factors whose sufficient statistics derive from the domains of its neighbors as well as those with arbitrary feature functions based on their neighbors. An MCMC proposal is scored as follows. First, a sample is generated from the proposal distribution, placing an initial set of variables in the DiffList . Next the value-assignment method is called for each of the variables on the DiffList , and via imperative variable coordination other variables may be added to the DiffList . Given the set of variables that have changed, F ACTORIE iterates over each one and calls the unroll function for factor templates matching the variable X  X  type. This dynamically provides the relevant structure of the graph via imperative structure definition , resulting in a set of factors that should be re-scored. The neighbors of each returned factor are given to the template X  X  statistics function, and the sufficient statistics are used to generate the factor X  X  score using the template X  X  current parameter vector. These scores are summed, producing the final score for the MCMC step. 2.5 Learning Maximum likelihood parameter estimation traditionally involves finding the gradient, however for complex models this can be prohibitively expensive since it requires the inference of marginal distri-butions over factors. Alternatively some have proposed online methods, such as perceptron, which avoids the need for marginals however still requires full decoding which can also be computationally expensive. We avoid both of these issues by using sample-rank [19]. This is a parameter estimation method that learns a ranking over all possible configurations by observing the difference between scores of proposed MCMC jumps. Parameter changes are made when the model X  X  ranking of a proposed jump disagrees with a ranking determined by labeled truth. When there is such a disagree-ment, a perceptron-style update to active parameters is performed by finding all factors whose score has changed (i.e., factors with a neighbor in the DiffList ). The active parameters are indexed by the sufficient statistics of these factors. Sample-rank is described in detail in [20]. As with inference, learning is efficient because it uses the DiffList and the imperative constructs described earlier. Tasks involving multiple information extraction steps are traditionally solved using a pipeline archi-tecture, in which the output predictions of one stage are input to the next stage. This architecture is susceptible to cascading of errors from one stage to the next. To minimize this error, there has been significant interest in joint inference over multiple steps of an information processing pipeline [21, 22, 23]. Full joint inference usually results in exponentially large models for which learning and inference become intractable. One widely studied joint-inference task in information extraction is segmentation and coreference of research paper citation strings [21, 23, 24]. This involves segment-ing citation strings into author , title and venue fields ( segmentation ), and clustering the citations that refer to the same underlying paper entity ( coreference ). Previous results have shown that joint inference reduces error [21], and this task provides a good testbed for probabilistic programming. We now describe an IDF for the task. For more details, see [24]. 3.1 Variables and Proposal Distribution As in the example given in Section 2, a Mention represents a citation and is a random variable that takes a single Entity as its value. An Entity is a set-valued variable containing Mention variables. This representation eliminates the need for an explicit transitivity constraint, since a Mention can hold only one Entity value, and this value is coordinated with the Entity  X  X  set-value.
 Variables for segmentation consist of Tokens , Labels and Fields . Each Token represents an ob-served word in a citation. Each Token has a corresponding Label which is an unobserved variable that can take one of four values: author , title , venue or none . There are three Field variables asso-ciated with each Mention , one for each field type ( author , venue or title ), that store the contiguous block of Tokens representing the Field ; Labels and Fields are coordinated. This alternate repre-sentation of segmentation provides flexibility in specifying factor templates over predicted Fields . The proposal function for coreference randomly selects a Mention , and with probability 0.8 moves segmentation selects a random Field and grows or shrinks it by a random amount. When jointly function for the Field ensures that the Labels corresponding to the affected Tokens are correctly set when a Field is changed. This is an example of imperative variable coordination . 3.2 Factor Templates Segmentation Templates: Segmentation templates examine only Field , Label and Token vari-ables, i.e. not using information from coreference predictions. These factor templates are IDF trans-lations of the Markov logic rules described in [21]. There is a template between every Token and its as the Token of the earlier Label . The sufficient statistics for these factors are the tuples created from the neighbors of the factor: e.g. , the values of two Labels and one Token . We also have a factor template examining every Field with features based on the presence of numbers, dates, and punctuation. This takes advantage of variable-statistics mapping .
 Coreference Templates: The isolated coreference factor templates use only Mention variables. They consist of two factor templates that share the same sufficient statistics, but have separate weight vectors and different ways of unrolling the graph. An Affinity factor is created for all pairs of Mentions that are coreferent, while a Repulsion factor is created for all pairs that are not coreferent. The features of these templates correspond to the SimilarTitle and SimilarVenue first-order features in [21]. We also add SimilarDate and DissimilarDate features that look at the  X  X ate-like X  tokens. Joint Templates: To allow the tasks to influence each other, factor templates are added that are unrolled during both segmentation and coreference sampling. Thus these factor templates neighbor versa. We add templates for the JntInfCandidates rule from [21]. We create this factor template such that ( m,m 0 ) are unrolled only if they are in the same Entity . The neighbors include Label and Mention . Affinity and Repulsion factor templates are also created between pairs of Fields of the same type; for Affinity the Fields belong to coreferent mention pairs, and for Repulsion they belong to a pair of mentions that are not coreferent. The features of these templates denote similarity between field strings, namely: StringMatch, SubString, Prefix/SuffixMatch, TokenIntersectSize , etc. One notable difference between the JntInfCandidate and joint Affinity/Repulsion templates is the possible number of instantiations. JntInfCandidates can be calculated during preprocessing as there are O ( nm 2 ) of these (where n is the maximum mention length, and m is the number of mentions). However, preprocessing joint Affinity/Repulsion templates is intractable as the number of such fac-tors is O ( m 2 n 4 ) . We are able to deal with such a large set of possible factor instantiations due to the interplay of structure definition , variable-statistics mapping , and on-the-fly feature calculation. Our model also contains a number of factor templates that cannot be easily captured by first-order logic. For example consider StringMatch and SubString between two fields. For arbitrary length strings these features require the model designer to specify convoluted logic rules. The rules are even less intuitive when considering a feature based on more complex calculations such as StringEditDis-tance . It is conceivable to preprocess and store all instantiations of these features, but in practice this is intractable. Thus on-the-fly feature calculation within F ACTORIE is employed to remain tractable. The joint segmentation and coreference model described above is applied to the Cora dataset[25]. 4 The dataset contains 1295 total mentions in 134 clusters, with a total of 36487 tokens. Isolated train-ing consists of 5 loops of 100,000 samples each, and 300,000 samples for inference. For the joint task we run training for 5 loops of 250,000 samples each, with 750,000 samples for inference. We average the results of 10 runs of three-fold cross validation, with the same folds as [21]. Segmenta-tion is evaluated on token precision, recall and F1. For coreference, pairwise coreference decisions are evaluated. The fraction of clusters that are correctly predicted (cluster recall) is also calculated. In Table 1, we see both our isolated and joint models outperform the previous state-of-the-art results of [21] on both tasks. We see a 25 . 23% error reduction in pairwise coreference F1, and a 20 . 0% error reduction of tokenwise segmentation F1 when comparing to the joint MLN. The improvements of joint over isolated IDF are statistically significant at 1% using the T-test.
 The experiments run very quickly, which can be attributed to sample-rank and the application of variable coordination and structure definition of the models as described earlier. Each of the isolated tasks finishes initialization, training and evaluation within 3 minutes, while the joint task takes 18 minutes. The running time for the MLNs reported in [21] are between 50-90 minutes for learning and inference. Thus we can see that IDFs provide a significant boost in efficiency by avoiding the need to unroll or score the entire graph. Note also that the timing result from [21] is for a model that did not enforce transitivity constraints on the coreference predictions. Adding transitivity constraints dramatically increases running time [26], whereas the IDF supports transitivity implicitly. Over the years there have been many efforts to build graphical models toolkits. Many of them are useful as teaching aids, such as the Bayes Net Toolbox and Probabilistic Modeling Toolkit (PMTK)[27] (both in Matlab), but do not scale up to substantial real problems.
 There has been growing interest in building systems that can perform as workhorses, doing real work on large data. For example, Infer.NET (CSoft) [28] is intended to be deployed in a num-ber of Microsoft products, and has been applied to problems in computer vision. Like IDFs it is embedded in a pre-existing programming language, rather than embodying its own new  X  X ittle lan-guage, X  and its users have commented positively about this facet. Unlike IDFs it is designed for messaging-passing inference, and must unroll the graphical model before inference, creating factors to represent all possible worlds, which makes it unsuitable for our applications. The very recent language Figaro [29] is also implemented as a library. Like F ACTORIE it is implemented in Scala, and provides an object-oriented framework for models; unlike F ACTORIE it tightly intertwines data representation and scoring, and it is not designed for changing model structure during inference; it also does not yet support learning.
 BLOG [5] and some of its derivatives can also scale to substantial data sets, and, like IDFs, are designed for graphical models that cannot be fully unrolled. Unlike IDFs, BLOG, as well as IBAL [6] and Church [7], are designed for generative models, though Church can also represent condi-tional, undirected models. We are most interested in supporting advanced discriminative models of the type that have been successful for natural language processing, computer vision, bioinformatics, and elsewhere. Note that F ACTORIE also supports generative models; for example latent Dirichlet allocation can be coded in about 15 lines.
 Two systems focussing on discriminatively-trained relational models are relational Markov networks (RMNs) [8], and Markov logic networks (MLNs, with Alchemy as its most popular implementation). To define repeated relational structure and parameter tying, both use declarative languages: RMNs use SQL and MLNs use first-order logic. By contrast, as discussed above, IDFs are in essence an experiment in taking an imperative approach.
 There has, however, been both historical and recently growing interest in using imperative program-ming languages for defining learning systems and probabilistic models. For example, work on the-ory refinement [30] viewed domain theories as  X  X tatements in a procedural programming language, rather than the common view of a domain theory being a collection of declarative Prolog state-ments. X  More recently, IBAL [6] and Church [7] are both fundamentally programs that describe the generative storyline for the data. IDFs, of course, share the combination of imperative programming with probabilistic modeling, but IDFs have their semantics defined by undirected factor graphs, and are typically discriminatively trained. In this paper we have described imperatively defined factor graphs (IDFs), a framework to support efficient learning and inference in large factor graphs of changing structure. We preserve the tradi-tional, declarative, statistical semantics of factor graphs while allowing imperative definitions of the model structure and operation. This allows model authors to combine both declarative and procedu-ral domain knowledge, while also obtaining significantly more efficient inference and learning than declarative approaches. We have shown state-of-the-art results in citation matching that highlight the advantages afforded by IDFs for both accuracy and speed.
 This work was supported in part by NSF medium IIS-0803847; the Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant IIS-0326249; SRI International subcontract #27-001338 and ARFL prime contract #FA8750-09-C-0181; Army prime contract number W911NF-07-1-0216 and University of Pennsylvania subaward number 103-548106. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor.
