
Department of Information Systems, Sultan Qaboos University, Muscat, Oman School of Computing Sciences, University of East Anglia, Norwich, UK Abbreviations 1. Introduction Cluster analysis basically aims to find out how many clusters in a dataset and how they are formed. However, estimating the number of clusters for a given dataset is often problematic simply due to the fact that the solutions depend on the context and purpose of the study and there may be many answers the data points are labeled according to the clusters they should be grouped. By viewing the distribution also possible to say there might be two clusters as shown in Fig. 1(b), or four clusters as shown in Fig. 1(c). Furthermore, the dataset may be partitioned into five smaller clusters as shown in Fig. 1(d). as many researchers [2 X 4] have pointed out, there is no gold-standard for defining the number of clusters for a given non-trivial dataset as there is no single absolutely true answer to the problem.
The basic reason that there may be multiple nondeterministic answers in cluster analysis is that there are not any universally accepted criteria for deciding what make up a cluster and to how many clusters that a given dataset should be partitioned. All these are influenced by many factors, such as, the study accuracy or high complexity and thus their applications are limited. For example, the TwoStep [5]  X  arguably the most commonly used method as it is implemented in IBM/SPSS data mining package PASW, still requires the range value of K , minimum and maximum, to be set in advance; nevertheless, there is no guidance for how to set up the best range for a dataset. Other methods, such as the Gap methods [6], are not suitable for datasets that do not have well-separated clusters, or only suitable for numerical data. Therefore, there is still a need to explore new approach for estimating K automatically with a high accuracy as well as acceptable efficiency. This paper presents a novel method that determines K by considering possibly natural separation of the data based on new similarity measures. 2. Related work
The automatic determination of cluster number has attracted an increasing attention in cluster analysis field [7,8] and many approaches have been proposed and used over the years to tackle such a problem. Milligan and Cooper [9] evaluated a variety of the methods that are selected from different sources and disciplines. Nevertheless, most of the evaluated methods are not widely used in practice, because methods of finding the number of clusters automatically into five types: Monte Carlo cross-validation (MCCV) [11], Penalized likelihood estimation, permutation tests, resampling, and finding the knee of an error curve. In this section, we attempt to give a relatively comprehensive but concise review on some representative methods and algorithms. 2.1. Monte Carlo cross-validation (MCCV)
The basic idea of the MCCV [11] is to employ the Cross-Validation mechanism to overcome the problem of over-fitting too many clusters [10]. The algorithm starts by partitioning the dataset into training and testing subsets. The key point here is that the random selection must not be disjoint. For each subset, the EM algorithm is then employed to fit the K components to the training subset, where K ranged from 1 to K max .Avariantof K -means algorithm is run several times with random initialization and the highest likelihood solution is used to initialize the EM algorithm. The EM algorithm is then is calculated for each of the fitted models with K components by applying each of the fitted models to the unseen test partition. This process is repeated t times, usually hundreds or thousands times, and the t cross-validated estimates are averaged for each K .
 The performance of MCCV algorithm was tested on simulated data and only 3 real-world datasets (Iris, Diabetes, and Vowels) and compared with 3 other methods (AutoClass v2.0, v-fold cross validation and Bayesian Information Criterion ( BIC ) [12]). The experimental results showed that, overall, the MCCV dataset Vowels, which means that it may not be well equipped to handle complex data satisfactorily. 2.2. Penalized likelihood
Penalized likelihood [10] estimation is a nonparametric approach that seeks to find a model to fit the dataset as accurately as possible and also aims to minimize the complexity of the model. Minimum Mes-sage Length Segmentation (MML) [13], Minimum Description Length (MDL) [14], BIC [12], Akaike information criterion (AIC) and Subspace Information Criterion (SIC) [15], all are examples of penal-ized methods and each one of them has different level of complexity and varied performance. 2.3. Permutation test
Permutation Test [16], developed i n 2002, runs a comparison between the quality of the segmentations of both the given dataset and its random permutations. Such a comparison aims avoiding the creation of a piecewise linear approximation that over-fits the dataset. The Permutation Test was evaluated against BIC , oracle BIC and MCCV [11] methods on simulated datasets as well as some real-world datasets (Tsuolbmajavri [17], Dallican and Hockham [18]) obtained from the field of paleoecology. The experi-mental results indicated that it is very competitive and outperformed both BIC and MCCV . Nevertheless, it is computationally expensive and thus not suitable for time critical systems [16] and large datasets. 2.4. Resampling
The Resampling [19] and the Consensus Clustering [20] methods cluster various samples of the dataset and then select the most stable clustering samples among the clustered samples as the appropriate num-ber of clusters. 2.5. Finding the knee of an error curve
There are several methods, such as the Gap Statistic and the Jump Method, derived from the idea of finding the knee in error curve or similar measures. 2.5.1. The gap statistic
The Gaps Statistic [6] is a method of locating the knee of an error curve. The basic idea is to calculate the within-cluster dispersion for the generated clustering result and then compare the change in within-cluster dispersion with the expected under an appropriate reference null distribution of the clustered data.

The Gap Statistic was tested on some simulated datasets and only one small real-world benchmark Gap/unif uses a uniform reference distribution over the range of each observed feature, whereas the Gap/pc uses the uniform reference in the principle components orientation. The obtained results were then compared with 4 other different methods: Calinkski and Harabasz ( CH ) [21], and Krzanowski and Lai ( KL ) [22], Hartigan [23] and Silhouette Statistic [24]. The experimental results showed that Gap/pc method outperformed the compared methods in most cases, but did not perform well for overlapped clusters [6].

For convenience of comparison, we also briefly introduce these four methods below.  X  Calinkski and Harabasz ( CH ) [21]  X  Krzanowski and Lai ( KL ) [22]  X  Hartigan method ( H ) [23]  X  Silhouette Statistic method [24] 2.5.2. The jump method
Like the Gap Statistic method, the Jump Method [25] is based on the distortion. It generates a distor-tion curve for the given dataset by running the standard K -means algorithm several times with different where X represents an N -dimensional random variable vector that has a mixture distribution of G com-ponents with common covariance  X  ,and c 1 ,...c K are the set of K cluster centers, with c x the closest the distortion curve. In the transformed distortion, the Jumps are then calculated as: the average Mahalanobis distance per dimension between X and a set of cluster C . By defining d  X  y 0 =0 the Jump method can produce K = 1 if the dataset has only one cluster.

The Jump method was tested on 3 real-world datasets (Iris [26], Breast Cancer [26], and Auto datasets) and on simulated datasets. The experimental results of the simulation data were compared to 5 different methods: CH ,and KL [22], Hartigan [23] and Silhouette Statistic [24], and Gap Statistic [6] and showed that the Jump method outperformed the compared methods in most cases. An interesting point is that the Gap statistic was used in this study but performed the worst on most cases, even it was claimed to be the best by the developers of the method. 2.5.3. Prediction strength
Unlike the Gap Statistics and the Jump method, the Prediction Strength [27] focuses on the predic-testing subsets. Then it clusters each partition into K clusters using a clustering algorithm, e.g. K -means algorithm. After that, it measures how well the cluster centre in the training partition predicts co-memberships in the training partition. Finally, the prediction strength Eq. (8) is used to calculate the proportion of observations pairs that are assigned to the same clusters by both testing clusters and otherwise. ing operations on the training data set X tr ,and ii represents the observations.
 The Prediction Strength method was tested only on simulated datasets with different complexity level. It then compared with other three methods: the Gap Statistics [6], CH [21], and KL [22]. The exper-imental results showed the Prediction Strength performed well in most cases of the limited simulated not to mention it still relies on running K -means many times and hence reduces its efficiency. 2.6. Density-based spatial clustering algorithm (DBSCAN)
Density-based spatial clustering algorithm (DBSCAN) is another algorithm that is used to estimate value,  X  , and the minimum number of data points that is needed to form a cluster. The procedure of DB-SCAN can be summarized as follows [28]: 1. Select a data point randomly from a given dataset. 2. Calculate similarity between the selected data point and the remaining points in the dataset. 3. If adequate points are found to have their similarity value with the selected data point higher than 4. Repeat Steps 1 to 3 until all data points are checked.

Note that the neglected point will be checked later on with other data points and it will be considered The algorithm produces whatsoever number of clusters necessary for a given dataset but it is obvious that the generated number of clusters certainly depends on the values of the two parameters mentioned above and there is no guideline for choosing their values. In addition, it should be noted that DBSCAN works there is a little spatial separation between clusters or there is large differences in densities [28]. 2.7. SPSS TwoStep method
The TwoStep method [5] is devised by SPSS based on several other methods [5,29] and implemented as a clustering analysis component in an arguably industry leading modeling package IBM/SPSS PASW (Clementine, formerly). This method uses a combination of both BIC and distance change in clustering clusters, respectively. Such calculations help in determining the initial approximate number of clusters, which will then be improved by the largest difference in distance between the closest pair in each stage of the hierarchical clustering stages. TwoStep employs two validation schemes to determine the optimal number of clusters: (1) a range (minimum and maximum of the number of clusters) is specified and the correct number of clusters is calculated within the selected range [5], or (2) a bound on the distance is imposed beyond which no pair of clusters should be merged [30].

The TwoStep method was initially tested on 10 simulated datasets [5]. The experimental results most of the generated clustering results have clustering accuracy of 90% and above. In three datasets, the TwoStep identified the correct number of clusters and achieved clustering accuracy of 100%.
Due to its apparent advantage over the other above reviewed methods in its high scalability and ability in handling mixed type datasets [31] and producing accurate results, it is generally preferred by prac-titioners for estimating the number of clusters automatically. Therefore, for simplicity and clarity, we choose the TwoStep method as the comparison target to our proposed method in this study with an aim of conducting a more focused critical evaluation. 3. A new approach for estimating the number of clusters 3.1. Problems associated with existing methods
Accuracy and complexity are two basic issues that need to be considered when estimating the number of clusters in a dataset. The cost of high accuracy is usually associated with high running time complex-ity. This is actually a common problem in most clustering methods as finding K automatically involves addressing different ranges of clusters and then selecting the optimal one based on the pre-set criteria. For instances, as pointed out in the earlier review section the Permutation tests and the Resampling are extremely inefficient because they involve running the clustering algorithm hundreds or even thousands of times. Other methods that involve locating the knee of an error curve, such as the Gap Statistics, the Jump Method, are also inefficient, as they too require running the embedded clustering algorithm for every value of K . Consequently, the time of the evaluation process is often much longer than the time of existing methods perform well only when the dataset has well-separated clusters. Even for some com-monly used methods such as the TwoStep, there is still a common requirement, that is, the maximum value of K needs to be set in advance and yet, there is no guideline on how to choose the maximum value of K . Moreover, it should be noted that none of the above methods has been tested on dataset with categorical features.

In general, there should be a trade-off between accuracy and complexity, but it appears that most of the existing methods are either too time-consuming or not accurate enough and hence not really used much in reality. We thus present a new technique that attempts to reflect the prospect that there may be nondeterministic, multiple answers in estimating the number of clusters to produce not only just one most likely answer but also a ranking of few other promising candidates for K values whilst keeping its computational time tolerable. 3.2. The proposed method
Whilst developing our new 3-staged clustering algorithm [32], we happened to come across with an idea for identifying K value, so, we investigated it further and resulted in a new method for estimating the appropriate number of clusters, K , automatically without supervision. Since the 3-staged clustering algorithm works with a similarity threshold  X  varying from a starting value to an ending value, we found that within certain range of  X  , the number of clusters formed by the algorithm remains constant. L =(  X  algorithm produces the same number of clusters consecutively. We felt that a longer, stable interval may somehow represent a natural separation of the data and thus the value of K with the longest interval may be the most appropriate number of clusters in a dataset.

As the proposed method is based on the three-staged clustering algorithm [32] and also a new sim-ilarity measure we developed, it is helpful to give a summary of both as well as some other similarity measures in the next subsections before describing our new method for estimating K value. 3.2.1. Three-staged clustering algorithm As shown in Fig. 2, our clustering algorithm consists of three stages.
 First stage
The first task in the first stage is to find a BASE .A BASE is a real sample that acts like a medoid or a centroid in other common clustering methods, e.g. K -means or K -medoids. The major steps in the first stage are: 1. Find a BASE 2. Calculate the similarity between the obtained BASE and the remaining samples. 3. Those samples that have similarity value higher than or equal to the set threshold will be assigned 4. If there are any samples that have not been assigned to any clusters then a new BASE is required. 5. Repeat steps 1 to 4 until no samples left.
 Second stage
The objective of this stage is to refine the initial clusters. The main steps in this stage include: 1. Select the BASE of the second obtained cluster and calculates its similarity with all the samples in 2. If any sample has a greater similarity value than its original cluster, the sample has to be moved to 3. Repeat steps 1 and 2 to the remaining clusters K  X  1 . Third stage
The objective of the third stage is to refine the initial BASES to see whether the solution can be improved further. The main steps in this stage include: 1. Find potential new BASE 2. Calculate the similarity between the BASE and the Potential BASE . 3. Repeat steps 1 to 2 for the remaining refined clusters. 4. If the Potential BASES differ from the original ones, has a similarity less than 1, repeat the second 5. Repeat the third stage until no data sample is moved between clusters, which means that the clus-
This algorithm has been tested on many simulated and real-world dataset (benchmarks as well as new dataset) and outperformed many commonly used clustering methods including K -means and TwoStep in most cases. The details of the work can be found in [32]. 3.2.2. A new similarity definition
The definition of our similarity measure is given as follows: where, sim is an abbreviation of similarity,  X  [  X  1 , 1] ; N is the number of features, x represents the sample; i is sample index; j is feature index; R and Cat represent the numerical and categorical features between sample x and a BASE sample B and is defined as:  X   X  ( x, y ) 0  X  Symmetry:  X  ( x, y )=  X  ( y,x )  X  Triangle inequality:  X  ( x, y )  X  ( x, z )+  X  ( z,y )
It is worth noting that in practice the most commonly used similarity measures, such as Euclidean distance, Manhattan distance, block distance, etc., are defined based on  X  X istance X  between data points. In contrast, our similarity measure is defined particularly to reflect the degree of the relative change between samples. Moreover, our similarity measure has some other special properties such as represent-ing relative changes quantitatively and also qualitatively in numerical variables as well as categorical variables. The mathematical proof of the above three properties, the detailed study and comparison with other existing similarity definitions or distance measures have been done but planned to be presented in a separate follow up paper as they are not the focus of this paper. 3.2.3. Internal similarity measures
Besides the similarity interval length, we introduced two internal measures to assess the quality of the clustering results: InterS and IntraS .The InterS measures the similarity between the generated clusters, and is defined by Eq. (11), where k is a cluster index. On the other hand, the IntraS measures the the clustering results are.
 which is called Average Intra-cluster Similarity (AIntraS) and defined by Eqs (12) and (13). The basic idea is to calculate the average of IntraS of the clustering result at a certain threshold value. The ob-tained value is then used to compare with other values at different intervals. A higher value means a better clustering result.
 where, AIntraS is the Average Intra-cluster Similarity , S k is the number of data samples in cluster k . Note, in this measure, the BASE should be excluded from the calculation; hence, S k is subtracted by 1.
Based on the concept of our similarity measure, clusters produced by a good clustering algorithm should have high IntraS and low InterS . In contrast, if a distance measure is employed, instead of the similarity measure Sim() , then the above definitions will become dissimilarity measures. A smaller av-more compact [33]. Whereas, as InterS measures the dissimilarity between clusters centroids, the higher it is, the less similar between clusters and hence the better clustering result [33] is. 3.2.4. The algorithm for estimating K values
The algorithm of our proposed method for identifying the appropriate number of clusters can be represented as follows: 2. Run the 3-staged algorithm with a step size of, e.g.  X   X  = 1 and terminate the experiments once the 3. Highlight the candidate intervals, V , whose L 2%. 4. For each V , calculate the AIntraS for each  X  (AIntraS  X  ) by using Eq. (12). 5. Calculate the InterS between the BASES of clusters by using Eq. (11). 6. Check the validity of the threshold values,  X  , in each candidate intervals: 7. Output the estimated number of the clusters: Rank the obtained AIntraS  X  of the valid intervals,
The time complexity of our proposed algorithm is O ( K  X  ( n  X  K ) 2  X  I ) and its estimation details are given in the Evaluation and Discussed Section.
This algorithm has been implemented and tested on 3 simulated datasets and 8 real-world benchmark datasets. The experimental results that will be given in Sections 5 and 6 confirmed it works well in most of the cases because the numbers of the clusters it identified are either the same as, or very close to, the true number of clusters. Before presenting the experimental results, however, we will introduce a and the accuracy of the estimating methods. 4. Measuring and evaluating the estimation error
In literature, there is no quantitative measure exis ting for evaluating the methods for estimating the number of clusters automatically. The evaluation of the performance was largely carried out in a crude manner by simply comparing the counts of the right and wrong estimations, i.e. if the estimated K is the same or not as the known number of clusters for few datasets. In this study, we introduced a method that can make the performance evaluation of estimating values of K thesameasthecommonerror evaluation. The error function can be defined as: where, K t and K e represent the true number of clusters and the estimated number of clusters, respec-tively, and  X  K is the difference between the true and estimated value of K .  X  K has the following properties obviously:  X  If  X  K = 0, then the estimated number of clusters is identical to the true clusters. Thus, the method  X  If  X  K&gt; 0, then the estimated number of clusters is less than the true clusters. Therefore, it is an  X  If  X  K&lt; 0, then the estimated number of clusters is higher than the true number of clusters. That of the absolute  X  K for each dataset and is defined by: of the squared difference (  X  K) 2 .

By this measure, the smaller the value of E is, the better the method should be. When E = 0, the method gives correct answers on all the testing datasets. As can be seen, this definition is simple, just used in any of the works found in literature for comparison and evaluation of the methods in this con-text. Hence, we consider ourselves the first to do so. Although it appears nothing technically novel, paring the number of rights or wrongs in estimations, the evaluation between the comparing methods can be done quantitatively and then some conventional statistical significance tests can be applied to test whether the performance difference between the methods is statistically significant, or just down to purely randomness.
 5. Testing with artificial data because our in itial experiments r esults showed that if any t wo or more clusters have InterS more than this value, they are extremely similar; hence, it is recommended to merge them. In addition, if any of considered as outliers. 5.1. Generating artificial datasets
We artificially generated 3 datasets of 5 dimensions, with varying complexity in terms of the degree their mean, median and variance vary for each cluster.

To measure the clusters quality we used two internal criteria: intra-distance and inter-distance. The inter-distance measures the distances between the clusters centroids by using Euclidean distance. High is, the better the cluster quality is. 5.1.1. 4-cluster dataset 4-cluster dataset contains 197 samples in four clusters. There are 59 samples for each of the first three clusters and 20 samples for the last one. As shown in Table 1, the average intra-distance among the with an inter-distance 5.39, although these two clusters have the lowest intra-distances 1.22 and 0.45, respectively. 5.1.2. 5-cluster dataset
The 5-cluster dataset is an extension of the 4-cluster dataset. In this dataset we aimed to increase the degree of the overlapping to some extent. Thus, we created one new cluster of 53 samples; accordingly, we ended up with 250 samples and 5 clusters. The intra-and inter-distance between the first 4 clusters remain the same as shown in Table 1. The average intra-distance among cluster 5 is 9.14 and the inter-distances to the first 4 clusters are 12.6.2, 101.87, 105.79 and 13.60, respectively. 5.1.3. 7-clusters dataset: Unbalanced class distribution
In practice, it is highly possible that samples in a particular dataset are unequally distributed among clusters. Therefore, purposely, unlike the first two datasets, we designed the third dataset with unbal-anced class distribution. To do so, we created three more clusters to the original dataset. The class distribution in this dataset for clusters 1 to 7 is as follow: 59 samples, 4 samples, 59 samples, 59 sam-ples, 10 samples, 20 samples, and 4 samples, respectively. Our ultimate aim is to increase the degree of complexity in the dataset in terms of unbalanced class distribution and the degree of overlapping. This will help in evaluating the proposed method ability in estimating the appropriate number of cluster for cluster average. The low standard deviation indicates that there is a small difference from one sample value to another; hence, the values are not scattered.

Table 2 shows the inter-distance and the intra-distance of the clusters. As the intra-distance value of have high compactness. As presented that the new clusters overlapped highly with each other, especially cluster 6 and 7, and with the existing clusters as well.

To sum up, the clusters in each dataset have the following characteristics:  X  High inter-distance (i.e. clusters are far away from each other).  X  Low intra-distance (i.e. low Euclidean distance between data points in one cluster).  X  High correlation between the data samples and their cluster average.  X  Low standard deviation between the data samples and their cluster average.
 5.2. Experimental results of the artificial datasets 5.2.1. Results for the 4-cluster dataset
The experimental results of 4-cluster dataset are presented in Table 3. Our proposed method initially suggested 6 possible clustering results: from 2 clusters to 7 clusters, respectively, as shown in Table 3 of clusters? To answer this question we need to employ the AIntraS function. However, before doing so, check the validity of the obtained intervals by calculating the InterS and IntraS . According to Table 3, AIntraS of each cluster is higher than its max. InterS , and (2) the max. InterS must not be higher than or equal to 79%. Now we have to compare between the AIntraS of valid intervals. Among the three as shown in Table 3. Thus, the appropriate number of cluster is 4, which is the intended answer set in generating the dataset.

Now let us compare our obtained results with TwoStep method. As TwoStep method required the range of the clustering results (minimum and the maximum number of clusters) to be specified in advance, first, we tried the default range of 2 to 15 clusters. We found that TwoStep produced only 3 clusters. Hence, we decided to minimize the range by 5 clusters and then by 11 clusters, as smaller ranges increase the probability of detecting the appropriate number of clusters. Consequently, we ended up with another two intervals: 2 to 10 and 2 to 4. However, in both cases, TwoStep again produced 3 clusters. So in all the number of clusters is enforced in advance to K = 4, TwoStep then succeeded in finding the appropriate four clusters and produced an accuracy of 100%. 5.2.2. Results for the 5-cluster dataset
In clustering the 5-cluster datase t, initially, the proposed algorithm estimated 6 candidate intervals, as shown in Table 4 and Fig. 4. However, after calculating the InterS and the IntraS and employing the K = 2 as at this value of K the proposed method produced the lowest AIntraS . By comparing between K = 4and K = 5, as shown in Table 4, the AIntraS at K = 5 is higher than the AIntraS at which K = 4. This justifies why K = 5 is the most likely optimal number of clusters, and indeed is the same as the ideal answer.

Comparing with the TwoStep method, we found that, the TwoStep managed to detect the appropriate number of cluster automatically in the default interval. We then tried other three smaller intervals to see whether TwoStep can provide any other options. Nevertheless, the experimental results showed that in this dataset, regardless of the selected intervals, 2 X 15, 2 X 10, 2 X 7, and 2 X 5, TwoStep method always produced one single answer, which is the correct answer.
 5.2.3. Results for the 7 cluster dataset
Regarding the 7-Cluster dataset, following the interval strategy, the proposed method is terminated at  X  values of 78%, as shown in Table 5 and Fig. 5. As presented, only 7 intervals fulfilled the intervals the proposed method succeeded in identifying the appropriate number of cluster automatically.
The TwoStep method failed to detect the appropriate number of clusters for the 7-cluster dataset, within the default interval. Thus, we tried other two smaller intervals, 2 X 10 and 2 X 7; however, the TwoStep method again produced 3 clusters in both of them. When the appropriate number of clus-ters is specified in advance, K = 7, the TwoStep method succeeded in grouping the samples that have the same class label together. 6. Testing with real-world benchmark datasets 6.1. Description of real-world benchmark datasets
Apart from 3 artificial datasets, we conducted experiments on 8 real-world benchmark datasets: Iris dataset, Soybean dataset, Wine dataset, Zoo dataset, Credit Approval dataset, Cleve dataset, Cancer dataset, and Votes dataset. Table 6 shows the demographic details of these datasets which are obtained from UCI Machine learning Repository [26]. For these datasets the samples are all labelled by their class and thus the number of clusters is known in advance. 6.2. Experimental results 6.2.1. Iris dataset For Iris dataset, the proposed method is terminated at  X  value of 98.4%, as shown in Table 7 and Fig. 6. Based on Table 7, there are only two valid intervals to consider. As the interval at which K = 3 has higher AIntraS than that of which K = 2, the appropriate number of cluster for Iris dataset is K = 3, which is the desired answer.

When analyzing the clustering result produced by the TwoStep, we realized that it produced only 2 have high degree of overlapping and the second cluster consisted only of the data samples of the third class (50 samples). 6.2.2. Soybean dataset
Figure 7 shows the interval lengths of Soybean dataset. As shown that 6 intervals are chosen as can-The values of the AIntraS of these intervals are presented in Table 8. As the interval at which K = 4has the highest value, the appropriate number of clusters is K = 4, which again is the correct answer.
Regarding the TwoStep method, in three different intervals, 2 X 15, 2 X 10, and 2 X 7, it produced two clusters only, K = 2. Note that at this value of K our proposed method has the longest interval. Never-AIntraS . 6.2.3. Wine dataset
The experimental result of Wine dataset is presented in Table 9 and Fig. 8. As shown that candidates intervals to consider in this dataset are at K = 2, K = 3, K = 4, and K = 8. But only one of them passed the validation check, so, based on the AIntraS , the appropriate number of clusters is * K = 2. Because of the nature of the dataset K = 2and K = 3 appeared twice while estimating K value. In using the TwoStep to cluster Wine dataset, we tried three smaller intervals, 2 X 15, 2 X 10, and 2 X 7. However, the TwoStep produced three clusters in all cases.
 6.2.4. Zoo dataset
The Zoo dataset is the most complex dataset among the used real-world benchmark datasets as it has unbalanced class distribution. The intervals length of Zoo dataset is presented in Fig. 9. According to AIntraS , the appropriate number of clusters is K = 7. Thus, again the proposed method succeeded in estimating the number for a dataset with unbalanced class distribution.

In using the TwoStep to cluster Zoo dataset first we run it with the default interval and then tried 4 other different intervals, 2 X 10, 2 X 7, 2 X 20, and 5 X 10. Regardless of the interval range, the TwoStep always produce 2 clusters. Note that this result matched the clustering results that is generated by our proposed method when K = 2. However, it is considered as the less possible clustering result as it has the lowest AIntraS . 6.2.5. Credit approval dataset
For the Credit Approval dataset, the proposed algorithm is automatically terminated at  X  = 55%, as shown in Table 11 and Fig. 11. According to Table 11 there are only 2 valid intervals. As the intervals at which K = 3 has the highest AIntraS , the appropriate number of clusters is K = 3, which is not the desire answer, i.e. K = 2.

First, we run the TwoStep with the default interval. After that we minimize the range by 5 and then by 10 and rerun the experiments. However, we found out that the estimation of TwoStep still the same, i.e. K = 3, which is different from the desire answer K = 2. 6.2.6. Cleve dataset
With regards to the clustering results, as presented in Table 12, there are 4 candidate intervals to consider, which have a length of L 2. However, only 2 out of 4 intervals passed the validation check. By comparing the value of the AIntraS between the two valid intervals, we can see that the interval at K = 3 has higher value than that of K = 2. Consequently, the most likely optimal number of clusters is K = 3, which is one more than the desire number K = 2. But our method also identified K = 2asthe second most likely answer.

Concerning TwoStep method, regardless of the interval range, it always estimated 2 clusters, which is the correct answer. 6.2.7. Cancer dataset
As presented in Table 13, only 3 intervals passed the validation check. The candidate interval at K = 3 has the highest AIntraS among the other two intervals. Thus, the appropriate number of cluster for Cancer dataset is K = 3. As mentioned in Section 4, when the estimated number of clusters is higher than the true number, it means that the method managed in finding the true clusters and possibly the sub-clusters.

For Cancer dataset, TwoStep estimated 2 clusters within the default interval. Although we then tried three other interval sizes, 2 X 15, 2 X 10 and 2 X 5, the estimated number of clusters did not change and this estimated number is identical to the true number of classes in the dataset. 6.2.8. Votes dataset
Table 14 shows that only two intervals passed the validation check. The candidate interval at K = 3 has the highest AIntraS ; hence, the appropriate number of cluster for Cancer dataset is K = 3. Table 14 shows the intervals details of the Votes dataset. For TwoStep method, we then tried four different in-tervals sizes, 2 X 20, 2 X 15, 2 X 10 and 2 X 5. However, the estimated number of clusters was always K = 2. 7. Evaluations and discussions
Table 15 summarizes the results of the conducted experiments. Our proposed method succeeded in benchmark datasets, our method identified the correct number of clusters for Iris, Soybean and Zoo. On the other hand, it failed in doing so for Wine dataset only, but the number of clusters is very close to the appropriate number of clusters. The proposed method detected the sub-clusters for Credit Approval, Cleve, Cancer and Votes datasets. It is worth pointing out that the 7-cluster and the Zoo datasets have unbalanced class distribution, even so, our proposed method succeeded in finding the appropriate number of K for both of them. The TwoStep method succeeded in estimating the appropriate number of clusters in 5 datasets: 5-cluster, Wine, Cleve, Cancer, and Votes datasets.
 The  X  K shown in Table 15 calculates the difference between the true and the estimated value of K . As mentioned earlier, the smaller the value is, the better the method is. Our method found the true value of K in 6 cases and also succeeded in finding the sub-clusters in 4 cases. However, it failed to estimate the appropriate number of cluster for Wine dataset. Concerning the TwoStep method, it estimated the K value, it only missed the true answers by a smallest possible margin (just 1). In contrast, the highest error made by the TwoStep is 5. Overall, based on the error measure, E , our proposed method is better than the TwoStep by a big margin 9.

In cases there are two or more valid intervals, deciding which one to select should be based on the values of the AIntraS , i.e. choosing the interval with the higher AIntraS value if they are different. deciding which interval to select is mainly depending on how the user wants to view the dataset. If the fewer clusters based on the principle of Occam X  X  Razor. The proposed method supported the fact that for the same dataset there are possibly different answers by providing other possible clustering results that the dataset could have, our method can produce a ranked suggestion for the most likely answers. significant better than the TwoStep at 95% confidence. The first one is to test if there is a significant difference between the results obtained from each of the two methods, i.e. ours and the TwoStep, and the But this test would not tell which method is better. So, the second test is needed to test the difference
Nevertheless, our proposed algorithm also inevitably has weaknesses, such as a relatively high time complexity as it involves addressing several candidate intervals and then selects the best one based on the defined criteria. However, as mentioned before, this is a common problem in almost all the reviewed algorithms for estimating the value of K automatically, and ours is tolerable because in our experiments it was usually done within few seconds. Two other potential issues associated with our proposed method are the default values of the InterS max (79%) and the interval length limit ( L = 2) that are used to validate the candidate intervals. Our experimental results show that increasing the InterS max value to more than 79% might result in generating over fragmented clusters. On the other hand, reducing this value to 60% or 50% reduces the similarity limit between clusters. In other words, clusters that have an InterS max more than 50% or 60% will be merged to form coarse clusters. Thus, we believe that the empirically obtained default value is reasonable as it avoids both over fragmented and coarse clusters. ignoring some cases where a dataset might have a natural separation with narrower gaps. Therefore, to of K appears, at least, twice (i.e. L = 2) continuously.

The time complexity of our proposed algorithm is O ( I  X  K  X  ( n  X  K ) 2 ) . A main time consuming task is to calculate the similarity between samples and the BASES of the generated K clusters, which has time cluster BASES and tries to replace each of them with one of the remaining ( n  X  K ) samples. If there is no replacement, the algorithm terminates. On the other hand, if a replacement occurs, the next iteration will take place attempting to refine each of K BASEs with potential BASE Q k . This could lead ( n  X  K ) consuming but tolerable in practice based on the experience of our experiments. 8. Conclusion
In this paper, a new method to estimate the appropriate number of clusters has been proposed and tested on 3 simulated datasets and 8 real-world benchmark datasets in order to evaluate its performance. The experimental results demonstrated the strong ability and high accuracy of our proposed method. As presented in all the tested datasets, the proposed method produced the predominant number of clusters as well as the sub-optimal number of clusters that the dataset could have. This reflects the fact that for the same datasets there may be possibly different answers, depending on the purpose and criterion of the study [1]. The average intra-similarity measure AIntraS i is introduced to help in comparing the number of clusters.

The TwoStep method has been used in our experiments as a baseline for comparison because it is gen-erally preferred by practitioners for estimating the number of clusters automatically and had been imple-mented by SPSS as a clustering component for its data modelling software package PASW. According to our experimental results, regardless of what the ranges used, the TwoStep worked satisfactorily for 5 of 11 test datasets but made larger errors on most of the remaining datasets. When our proposed method and the TwoStep method produced the same results, there is always a match in the clustering accuracy. An important point should be emphasized is that, unlike the TwoStep and all the other existing methods, which only give a single answer to the number of clusters for a given data set, our proposed method, can multiple answers for a data set, and then make a final selection depending on the purpose and criteria of the study. In addition, our method also managed to detect the most probable number of clusters even for the unbalanced class distribution in data.

The introduction of an error function for estimating the number of clusters has enabled the evalua-tion and comparison in a quantitative manner and application of most statistical significance tests. The statistical results show that our proposed method is statistically significantly better than the TwoStep method. We believe that our proposed method and the related work presented in this paper have made a considerable contribution to the clustering analysis field. Further work should include improving its efficiency and generating and adapting the similarity thresholds automatically based on the given data. References
