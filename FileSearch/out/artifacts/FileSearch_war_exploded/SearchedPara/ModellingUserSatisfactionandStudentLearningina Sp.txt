 In recent years the development of spok en dialo gue tutoring systems has become more pre valent, in an attempt to close the performance gap between hu-man and computer tutors (Mosto w and Aist, 2001; Pon-Barry et al., 2004; Litman et al., 2006). Student learning is a primary metric for evaluating the per -formance of these systems; it can be measured, e.g., by comparing student pretests tak en prior to system use with posttests tak en after system use.
In other types of spok en dialogue systems, the user' s subjecti ve judgments about using the system are often considered a primary system performance metric; e.g., user satisf action has been measured via surv eys which ask users to rate systems during use along dimensions such as task ease, speech in-put/output quality , user expectations and expertise, and user future use (M  X  oller , 2005b; Walk er et al., 2002; Bonneau-Maynard et al., 2000; Walk er et al., 2000; Shriber g et al., 1992). Ho we ver, it is expen-sive to run experiments over lar ge numbers of users to obtain reliable system performance measures.
The PARADISE model (W alk er et al., 1997) pro-poses instead to predict system performance, using parameters representing interaction costs and bene-ts between system and user , including task success, dialogue efcienc y, and dialogue quality . More for -mally , a set of interaction parameters are measured in a spok en dialogue system corpus, then used in a multi variate linear regression to predict the tar get performance variable. The resulting model is de-scribed by the formula belo w, where there are n in-teraction parameters, p ysis with a coef cient, w or positi ve, depending on whether the model treats p then be used to estimate performance during system design, with the design goals of minimizing costs and maximizing benets.

We investigate using PARADISE to develop pre-dicti ve models of performance in our spok en dia-logue tutoring system. Although to our kno wledge, prior PARADISE applications have only used user satisf action to represent performance, we hypothe-size that other metrics may be more rele vant when PARADISE is applied to tasks that are not optimized for user satisf action, such as our spok en dialogue tu-toring system. We thus use 2 metrics to represent performance: 1) a generic metric of user satisf action computed via user surv ey, 2) a tutoring-specic met-ric of student learning computed via student pretest and posttest scores. We train and test predicti ve models of these metrics on multiple system corpora.
To predict user satisf action, we use 2 types of in-teraction parameters: 1) system-generic parameters such as used in other PARADISE applications, e.g. speech recognition performance, and 2) tutoring-specic parameters, e.g. student correctness. To predict student learning, we also use a third type of parameter: 3) manually annotated user affect. Al-though prior PARADISE applications have tended to use system-generic parameters, we hypothesize that task-specic and user affect parameters may also pro ve useful. We emphasize that user affect parame-ters are still system-generic; user affect has been an-notated and/or automatically predicted in other types of spok en dialogue systems, e.g. as in (Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003).
Our results sho w that, although generic param-eters were useful predictors of user satisf action in other PARADISE applications, overall our parame-ters produce less useful user satisf action models in our tutoring system. Ho we ver, generic and tutoring-specic parameters do produce useful models of stu-dent learning in our system. Generic user affect pa-rameters increase the usefulness of these models. ITSPOKE ( I ntelligent T utoring SPOKE n dialogue system) (Litman et al., 2006) is a speec h-enabled tu-tor built on top of the text-based Why2-Atlas con-ceptual physics tutor (VanLehn et al., 2002). In ITSPOKE, a student rst types an essay into a web-based interf ace answering a qualitati ve physics problem. ITSPOKE then analyzes the essay and en-gages the student in spok en dialogue to correct mis-conceptions and elicit more complete explanations. Student speech is digitized from the microphone in-put and sent to the Sphinx2 recognizer . Sphinx2' s most probable  X transcription X  is then sent to Why2-Atlas for syntactic, semantic and dialogue analy-sis. Finally , the text response produced by Why2-Atlas is con verted to speech as described belo w, then played in the student' s headphones and displayed on the interf ace. After the dialogue, the student revises the essay , thereby ending the tutoring or causing an-other round of tutoring/essay revision.

For this study , we used 3 ITSPOKE corpora, sho wn in Table 1. 1 The SYN03 corpus was col-lected in 20 03 for an evaluation comparing learn-ing in typed and spok en human and computer tu-toring (Litman et al., 2006). ITSPOKE' s voice was syn thesized with the Cepstral text-to-speech system, and its speech recognizer was trained from pilot IT-SPOKE studies and Why2-Atlas evaluations. The PR05 and SYN05 corpora were collected in 20 05 , to evaluate the impact of tutor voice quality (Forbes-Rile y et al., 2006). For these 2 corpora, ITSPOKE used an updated speech recognizer further trained on the SYN03 corpus. For the SYN05 corpus, IT-SPOKE used the syn thesized tutor voice from the SYN03 corpus; for the PR05 corpus, ITSPOKE used a pr e-recorded tutor voice from a paid voice talent. Figure 1 gives an annotated (Section 3) PR05 excerpt ( ASR sho ws what ITSPOKE heard).

The same experimental procedure was used to collect all 3 ITSPOKE corpora: colle ge students who had tak en no colle ge physics: 1) read a small document of background material, 2) took a pretest measuring initial physics kno wledge, 3) work through a set of 5 problems (dialogues) with ITSPOKE, 4) took a posttest similar to the pretest.
Subjects in the PR05 and SYN05 corpora also completed a surv ey probing user satisf action after taking the posttest ( SYN03 corpus subjects did not). Our surv ey, sho wn in Figure 2, is essentially the same as the one used in the DARP A Communicator multi-site evaluation (W alk er et al., 2002). Although tailored lexically for a tutoring system, these state-ments are generally applicable to spok en dialogue systems. Students rated their degree of agreement with each statement on a scale of 1 to 5.
 3.1 Dialogue System-Generic Parameters Prior PARADISE applications predicted user satis-faction using a wide range of system-generic param-eters, which include measures of speech recognition quality (e.g. word error rate), measures of dialogue communication and efcienc y (e.g. total turns and elapsed time), and measures of task completion (e.g. a binary representation of whether the task was com-pleted) (M  X  oller , 2005a; M  X  oller , 2005b; Walk er et al., 2002; Bonneau-Maynard et al., 2000; Walk er et al., 2000; Walk er et al., 1997). In this prior work, each dialogue between user and system represents a sin-gle  X task X  (e.g., booking airline tra vel), thus these measures are calculated on a per -dialogue basis.
In our work, the entire tutoring session represents a single  X task X , and every student in our corpora completed this task. Thus we extract 13 system-generic parameters on a per -student basis, i.e. over the 5 dialogues for each user , yielding a single pa-rameter value for each student in our 3 corpora.
First, we extracted 9 parameters representing dia-logue communication and efcienc y. Of these pa-rameters, 7 were used in prior PARADISE appli-cations: Time on Task, Total ITSPOKE Turns and Words, Total User Turns and Words, Average IT-SPOKE Words/T urn, and Average User Words/T urn. Our 2 additional  X communication-related X  (M  X  oller , 2005a) parameters measure system-user interacti v-ity, but were not used in prior work (to our kno wl-edge): Ratio of User Words to ITSPOKE Words, Ra-tio of User Turns to ITSPOKE Turns.

Second, we extracted 4 parameters representing speech recognition quality , which have also been used in prior work: Word Error Rate, Concept Ac-curac y, Total Timeouts, Total Rejections 2 . 3.2 Tutoring-Specific Parameters Although prior PARADISE applications tend to use system-generic parameters, we hypothesize that task-specic parameters may also pro ve useful for predicting performance. We extract 12 tutoring-specific parameters over the 5 dialogues for each stu-dent, yielding a single parameter value per student, for each student in our 3 corpora. Although these pa-rameters are specic to our tutoring system, similar parameters are available in other tutoring systems.
First, we hypothesize that the corr ectness of the students' turns with respect to the tutoring topic (physics, in our case) may play a role in predicting system performance. Each of our student turns is automatically labeled with 1 of 3  X Correctness X  la-bels by the ITSPOKE semantic understanding com-ponent: Corr ect, Incorr ect, Partially Corr ect . La-beled examples are sho wn in Figure 1. From these 3 Correctness labels, we deri ve 9 parameters: a To-tal and a Percent for each label, and a Ratio of each label to every other label (e.g. Correct/Incorrect).
Second, students write and then may modify their physics essay at least once during each dialogue with ITSPOKE. We thus hypothesize that lik e  X Correct-ness X , the total number of essays per student may play a role in predicting system performance.
Finally , although student test scores before/after using ITSPOKE will be used as our student learning metric, we hypothesize that these scores may also play a role in predicting user satisf action. 3.3 User Affect Parameters We hypothesize that user affect plays a role in pre-dicting user satisf action and student learning. Al-though affect parameters have not been used in other PARADISE studies (to our kno wledge), the y are generic; for example, in various spok en dialogue systems, user affect has been annotated and automat-ically predicted from e.g., acoustic-prosodic and lex-ical features (Litman and Forbes-Rile y, 2004b; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003).
As part of a lar ger investigation into emotion adaptation , we are manually annotating the stu-dent turns in our corpora for affecti ve state. Cur -rently , we are labeling 1 of 4 states of  X Certain-ness X : certain, uncertain, neutr al, mixed (certain and uncertain) , and we are separately labeling 1 of 2 states of  X Frustration/Anger X : frustr ated/angry , non-frustr ated/ang ry . These affecti ve states 3 were found in pilot studies to be most pre valent in our tu-toring dialogues 4 , and are also of interest in other dialogue research, e.g. tutoring (Bhatt et al., 2004; Moore et al., 2004; Pon-Barry et al., 2004) and spo-ken dialogue (Ang et al., 2002). Labeled examples are sho wn in Figure 1. 5 To date, one paid annotator has labeled all student turns in our SYN03 corpus, and all the turns of 17 students in our PR05 corpus. 6
From these labels, we deri ved 25 User Affect pa-rameters per student, over the 5 dialogues for that student. First, for each Certainness label, we com-puted a Total, a Percent, and a Ratio to each other la-bel. We also computed a Total for each sequence of identical Certainness labels (e.g. Certain:Certain), hypothesizing that states maintained over multiple turns may have more impact on performance than single occurrences. Second, we computed the same parameters for each Frustration/Anger label. In this section, we rst investigate the usefulness of our system-generic and tutoring-specic parameters for training models of user satisf action and student learning in our tutoring corpora with the PARADISE frame work. We use the SPSS statistical package with a stepwise multi variate linear regression pro-cedure 7 to automatically determine parameter inclu-sion in the model. We then investigate how well these models generalize across dif ferent user -system congurations, by testing the models in dif ferent corpora and corpus subsets. Finally , we investigate whether generic user affect parameters increase the usefulness of our student learning models. 4.1 Pr ediction Models of User Satisfaction Only subjects in the PR05 and SYN05 corpora com-pleted a user surv ey (Table 1). Each student' s re-sponses were summed to yield a single user satis-faction total per student, ranging from 9 to 24 across corpora (the possible range is 5 to 25), with no dif-ference between corpora (p = .46). This total was used as our user satisf action metric, as in (M  X  oller , 2005b; Walk er et al., 2002; Walk er et al., 2000). 8
We trained a user satisf action model on each cor -pus, then tested it on the other corpus. In addition, we split each corpus in half randomly , then trained a user satisf action model on each half, and tested it on the other half. We hypothesized that despite the decrease in the dataset size, models trained and tested in the same corpus would have higher gen-eralizability than models trained on one corpus and tested on the other , due to the increased data homo-geneity within each corpus, since each corpus used a dif ferent ITSPOKE version. As predictors, we used only the 13 system-generic and 12 tutoring-specic parameters that were available for all subjects.
Results are sho wn in Table 2. The rst and fourth columns sho w the training and test data, respec-tively . The second and fth columns sho w the user satisf action variance accounted for by the trained model in the training and test data, respecti vely . The third column sho ws the parameters that were se-lected as predictors of user satisf action in the trained model, ordered by degree of contrib ution 9 .
For example, as sho wn in the rst row, the model trained on the PR05 corpus uses Total Incorrect stu-dent turns as the strongest predictor of user satis-faction, follo wed by Total Essays; these parameters are not highly correlated 10 . This model accounts for 27.4% of the user satisf action variance in the PR05 corpus. When tested on the SYN05 corpus, it ac-counts for 0.1% of the user satisf action variance.
The low R 2 values for both training and testing in the rst two rows sho w that neither corpus yields a very powerful model of user satisf action even in the training corpus, and this model does not gener -alize very well to the test corpus. As hypothesized, training and testing in a single corpus yields higher R 2 values for testing, as sho wn in the last four rows, although these models still account for less than a quarter of the variance in the test data. The increased R 2 values for training here may indicate over-tting. Across all 6 experiments, there is almost no overlap of parameters used to predict user satisf action.
Ov erall, these results sho w that this method of developing an ITSPOKE user satisf action model is very sensiti ve to changes in training data; this was also found in other PARADISE applica-tions (M  X  oller , 2005b; Walk er et al., 2000). Some applications have also reported similarly low R 2 val-ues for testing both within a corpus (M  X  oller , 2005b) and also when a model trained on one system cor -pus is tested on another system corpus (W alk er et al., 2000). Ho we ver, most PARADISE applications have yielded higher R 2 values than ours for train-ing (M  X  oller , 2005b; Walk er et al., 2002; Bonneau-Maynard et al., 2000; Walk er et al., 2000).
We hypothesize two reasons for why our exper -iments did not yield more useful user satisf action models. First, in prior PARADISE applications, users completed a surv ey after every dialogue with the system. In our case, subjects completed only one surv ey, at the end of the experiment (5 dialogues). It may be that this  X per -student X  unit for user satisf ac-tion is too lar ge to yield a very powerful model; i.e., this measure is not ne-grained enough. In addi-tion, tutoring systems are not designed to maximize user satisf action, but rather , their design goal is to maximize student learning. Moreo ver, prior tutor -ing studies have sho wn that certain features corre-lated with student learning do not have the same re-lationship to user satisf action (e.g. are not predicti ve or have an opposite relationship) (Pon-Barry et al., 2004). In fact, it may be that user satisf action is not a metric of primary rele vance in our application. 4.2 Pr ediction Models of Student Lear ning As in other tutoring research, e.g. (Chi et al., 2001; Litman et al., 2006), we use posttest score (POST) controlled for pretest score (PRE) as our tar get stu-dent learning prediction metric, such that POST is our tar get variable and PRE is always a parameter in the nal model, although it is not necessarily the strongest predictor . 11 In this way, we measure stu-dent learning gains , not just nal test score.
As sho wn in Table 1, all subjects in our 3 corpora took the pretest and posttest. Ho we ver, in order to compare our student learning models with our user satisf action models, our rst experiments predicting student learning used the same training and testing datasets that were used to predict user satisf action in Section 4.1 (i.e. we ran the same experiments except we predicted POST controlled for PRE instead of user satisf action). Results are sho wn in the rst 6 rows of Table 3.

As sho wn, these 6 models all account for more than 50% of the POST variance in the training data. Furthermore, most of them account for close to, or more than, 50% of the POST variance in the test data. Although again we hypothesized that training and testing in one corpus would yield higher R 2 val-ues for testing, this is not consistently the case; two of these models had the highest R 2 values for train-ing and the lowest R 2 values for testing ( PR05:half1 and SYN05:half2 ), suggesting over-tting.

Ov erall, these results sho w that this is an effec-tive method of developing a prediction model of stu-dent learning for ITSPOKE, and is less sensiti ve to changes in training data than it was for user satis-faction. Moreo ver, there is more overlap in these 6 models of parameters that are useful for predict-ing student learning (besides PRE);  X Correctness X  parameters and dialogue communication and ef-cienc y parameters appear to be most useful overall.
Our next 3 experiments investigated how our stu-dent learning models are impacted by including our third SYN03 corpus. Using the same 25 parame-ters, we trained a learning model on each set of two combined corpora, then tested it on the other corpus. Results are sho wn in the last 3 rows of Table 3.
As sho wn, these models still account for close to, or more than, 50% of the student learning vari-ance in the training data. 12 The model trained on PR05+SYN03 accounts for the most student learning variance in the test data, sho wing that the training data that is most similar to the test data will yield the highest generalizability . That is, the combined PR05+SYN03 corpora contains sub-jects dra wn from the same subject pool (2005) as the SYN05 test data, and also contains subjects who interacted with the same tutor voice (synthe-sized) as this test data. In contrast, the combined PR05+SYN05 corpora did not overlap in user pop-ulation with the SYN03 test data, and the combined SYN05+SYN03 corpora did not share a tutor voice with the PR05 test data.  X Correctness X  parameters and dialogue communication and efcienc y param-eters are consistently used as predictors in all 9 of these student learning models. 4.3 Adding User Affect Parameters Our nal experiments investigated whether our 25 user affect parameters impacted the usefulness of the student learning models. As sho wn in Table 1, all 20 subjects in our SYN03 corpus were annotated for user affect, and 17 subjects in our PR05 corpus were annotated for user affect. We trained a model of student learning on each of these datasets, then tested it on the other dataset. 13 As predictors, we included our 25 user affect parameters along with the 13 system-generic and 12 tutoring-specic inter -action parameters. These results are sho wn in the rst two rows of Table 4. We also reran these ex-periments without user affect parameters, to gauge the impact of the user affect parameters. These re-sults are sho wn in the last two rows of Table 4. We hypothesized that user affect parameters would pro-duce more useful models, because prior tutoring re-search has sho wn correlations between user affect and student learning (e.g. (Craig et al., 2004)).
As sho wn in the rst two rows, user affect predic-tors appear in both models where these parameters were included. The models trained on SYN03 use pretest score and Total Time on Task as predictors; when affect parameters are included,  X Neutral Cer -tainness X  is added as a predictor , which increases the R 2 values for both training and testing. Ho we ver, the two models trained on PR05:17 sho w no predic-tor overlap (besides PRE). Moreo ver, the PR05:17 model that includes an affect predictor (Total Se-quence of 2 Non-Frustrated/Angry turns) has the highest training R 2 , but the lowest testing R 2 value. Prior work in the tutoring community has focused on correlations of single features with learning; our re-sults suggest that PARADISE is an effecti ve method of extending these analyses. For the dialogue com-munity , our results suggest that as spok en dialogue systems mo ve into new applications not optimized for user satisf action, such as tutoring systems, other measures of performance may be more rele vant, and generic user affect parameters may be useful.
Our experiments used man y of the same system-generic parameters as prior studies, and some of these parameters predicted user satisf action both in our models and in prior studies' models (e.g., sys-tem words/turn (W alk er et al., 2002)). Nonetheless, overall our user satisf action models were not very powerful even for training, were sensiti ve to training data changes, sho wed little predictor overlap, and did not generalize well to test data. Our user sat-isfaction metric may not be ne-grained enough; in other PARADISE studies, users took a surv ey after every dialogue with the system. In addition, tutoring systems are not designed to maximize user satisf ac-tion; their goal is to maximize student learning.
Our student learning models were much more powerful and less sensiti ve to changes in training data. Our best models explained over 50% of the stu-dent learning variance for training and testing, and both student  X Correctness X  parameters and dialogue communication and efcienc y parameters were of-ten useful predictors. User affect parameters further impro ved the predicti ve power of one student learn-ing model for both training and testing.

Once our user affect annotations are complete, we can further investigate their use to predict stu-dent learning and user satisf action. Unlik e our other parameters, these annotations are not currently available, although the y can be predicted automati-cally (Litman and Forbes-Rile y, 2004b), in our sys-tem. Ho we ver, as in (Batliner et al., 2003), our prior work suggests that linguistic features reecti ve of af-fecti ve states can replace affect annotation (Forbes-Rile y and Litman, 2005). In future work we will use such features in our prediction models. Finally , we are also annotating tutor and student dialogue acts and automating the tutor act annotations; when com-plete we can investigate their usefulness in our pre-diction models; dialogue acts have also been used in prior PARADISE applications (M  X  oller , 2005a). NSF (0325034 &amp; 0328431) supports this research. We thank Pam Jordan and the NLP Group.

