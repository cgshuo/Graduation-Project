 This paper describes Sparse Frequent Directions, a variant of Frequent Directions for sketching sparse matrices. It resem-bles the original algorithm in many ways: both receive the rows of an input matrix A n  X  d one by one in the streaming setting and compute a small sketch B 2 R `  X  d . Both share the same strong (provably optimal) asymptotic guarantees with respect to the space-accuracy tradeo  X  in the streaming setting. However, unlike Frequent Directions which runs in O ( nd ` ) time regardless of the sparsity of the input matrix A , Sparse Frequent Directions runs in  X  O nnz( A ) ` + n ` 2 Our analysis loosens the dependence on computing the Sin-gular Value Decomposition (SVD) as a black box within the Frequent Directions algorithm. Our bounds require recent results on the properties of fast approximate SVD computa-tions. Finally, we empirically demonstrate that these asymp-totic improvements are practical and significant on real and synthetic data.
 Frequent Directions, Sparse Matrix, Matrix Sketching
It is very common to represent data in the form of a ma-trix. For example, in text analysis under the bag-of-words model, a large corpus of documents can be represented as a matrix whose rows refer to the documents and columns correspond to words. A non-zero in the matrix corresponds to a word appearing in the corresponding document. Simi-larly, in recommendation systems [13], preferences of users are represented as a matrix with rows corresponding to users and columns corresponding to items. Non-zero entires cor-respond to user ratings or actions.

A large set of data analytic tasks rely on obtaining a low-rank approximation of the data matrix. These include clus- X  Thanks to support by NSF CCF-1350888, IIS-1251019, ACI-1443046, and CNS-1514520.
 tering, dimension reduction, principal component analysis (PCA), signal denoising, etc. Such approximations can be computed using the Singular Value Decompositions (SVD). For an n  X  d matrix A ( d  X  n ) computing the SVD re-quires O ( nd 2 ) time and O ( nd ) space in memory on a single machine.

In many scenarios, however, data matrices are extremely large and computing their SVD exactly is infeasible. E -cient approximate solutions exist for distributed setting or when data access otherwise is limited. In the row streaming model, the matrix rows are presented to the algorithm one by one in an arbitrary order. The algorithm is tasked with processing the stream in one pass while being severely re-stricted in its memory footprint. At the end of the stream, the algorithm must provide a sketch matrix B which is a good approximation of A even though it is significantly more compact. This is called matrix sketching.

Matrix sketching methods are designed to be paralleliz-able, space and time e cient, and easily updatable. Com-puting the sketch on each machine and then combining the sketches together should be as good as sketching the com-bined data from all the di  X  erent machines. The streaming model is especially attractive since a sketch can be obtained and maintained as the data is being collected. Therefore, eliminating the need for data storage altogether.
Often matrices, as above, are sparse; most of their entries are zero. The work of [9] argues that typical term-document matrices are sparse; documents contain no more than 5% of all words. On wikipedia, most words appear on only a small constant number of pages. Similarly, in recommen-dation systems, in average a user rates or interacts with a small fraction of the available items: less than 6% in some user-movies recommendation tasks [1] and much fewer in physical purchases or online advertising. As such, most of these datasets are stored as sparse matrices.

There exist several techniques for producing low rank ap-proximations of sparse matrices whose running time is O (nnz( A )poly( k, 1 / " )) for some error parameter " 2 (0 , 1). Here nnz( A ) denotes the number of non-zeros in the matrix A . Examples include the power method [19], random pro-jection techniques [35], projection-hashing [6], and instances of column selection techniques [11].
 However, for a recent and popular technique Frequent-Directions (best paper of KDD 2013 [24]), there is no known way to take advantage of the sparsity of the input ma-trix. While it is deterministic and its space-error bounds are known to be optimal for dense matrices in the row-update model [17], it runs in O ( nd ` ) time to produce a sketch of size `  X  d . In particular, it maintains a sketch with ` rows and updates it iteratively over a stream, periodically invok-ing a full SVD which requires O ( d ` 2 ) time. Reliance on exact SVD computations seems to be the main hurdle in reducing the runtime to depend on O (nnz( A )). This paper shows a version of FrequentDirections whose runtime depends on O (nnz( A )). This requires a new understanding and a more careful analysis of FrequentDirections . It also takes advantage of block power methods (also known as Subspace Iteration, Simultaneous Iteration, or Orthogonal Iteration) that run in time proportional to nnz( A ) but incur small approximation error [29].
Throughout the paper we identify an n  X  d matrix A with . The notation a i stands for the i th row of the matrix A .By[ A ; a ] we mean the row vector a appended to the matrix A as its last row. Similarly, [ A ; B ] stands for stack-ing two matrices A and B vertically. The matrices I n and 0 n  X  d denote the n -dimensional identity matrix and the full zero matrix of dimension n  X  d respectively. The notation N (0 , 1) d  X  ` denotes the distribution over d  X  ` matrices whose entries are drawn independently from the normal distribu-tion N (0 , 1). For a vector x the notation k  X  k refers to the Euclidian norm k x k =( a matrix A is defined as k A k F =
The notation nnz( A ) refers to the number of non-zeros in A , and  X  =nnz( A ) / ( nd ) denotes relative density of A . The Singular Value Decomposition of a matrix A 2 R m  X  d for m  X  d is denoted by [ U,  X  ,V ] = SVD( A ). It guarantees that A = U  X  V T , U T U = I m , V T V = I m , U 2 R m  X  m V 2 R d  X  m , and  X  2 R m  X  m is a non-negative diagonal ma-trix such that  X  i,i = i and 1 2 ... m 0.
 It is convenient to denote by U k , and V k the matrices con-taining the first k columns of U and V and by  X  k 2 R k  X  k the top left k  X  k block of  X  . The matrix A k = U k  X  k V is the best rank k approximation of A in the sense that A SVD( A, k ), we mean rank k SVD of A .
 The notation  X  B ( A ) denotes the projection of the rows of A on the span of the rows of B . In other words,  X  B ( A )= AB  X  B where (  X  )  X  indicates taking the Moore-Penrose psue-doinverse. Alternatively, setting [ U,  X  ,V ] = SVD( B ), we have  X  B ( A )= AV V T . We also denote  X  k B ( A )= AV k the right projection of A on the top k right singular vectors of B .
This section reviews only matrix sketching techniques that run in input sparsity time and whose output sketch is inde-pendent of the number of rows in the matrix. We categorize all known results into three main approaches (1) column/row subset selection (2) random projection based techniques and (3) iterative sketching techniques.
 Column selection techniques . These techniques, which are also studied under the Column Subset Selection Problem (CSSP) in literature [16, 10, 3, 8, 15, 2], form the sketch B by selecting a subset of X  X mportant X  X olumns of the input matrix A . They maintain the sparsity of A and make the sketch B to be more interpretable. These methods are not typically streaming, nor running in input sparsity time. The only method of this group which achieves both is [11] by Drineas et al. that uses reservoir sampling to become streaming. They select O ( k/ " 2 ) columns proportional to their squared norm and achieve the Frobenius norm error bound k A  X  k ( A ) k the spectral norm error bound k A  X  B k ( A ) k 2 2  X  X  A A " k A k 2 F holds if one selects O (1 / " 2 ) columns. Rudelson et al. [34] improved the latter error bound to k A  X  B k ( A ) k k A A k k 2 2 + " k A k 2 2 by selecting O ( r/ " 4 log ( r/ " in the result by [11], one would need O ( r 2 / " 2 ) columns to obtain the same bound.

Another similar line of work is the CUR factorization [4, 10, 12, 14, 27] where methods select c columns and r rows of A to form matrices C 2 R n  X  c , R 2 R r  X  d and U 2 R c  X  r and constructs the sketch as B = CUR . The only instance of this group that runs in input sparsity time is [4] by Bout-sidis and Woodru  X  , where they select r = c = O ( k/ " ) rows and columns of A and construct matrices C, U and R with rank( U )= k such that with constant probability k A CUR k 2 F  X  (1 + " ) k A A k k 2 F . Their algorithm runs in O (nnz( A )log n +( n + d )poly(log n, k, 1 / " )) time. Random projection techniques . These techniques [31, 36, 35, 26] operate data-obliviously and maintain a r  X  d ma-trix B = SA using a r  X  n random matrix S which has the Johnson-Lindenstrauss Transform (JLT) property [28]. Random projection methods work in the streaming model, are computationally e cient, and su ciently accurate in practice [7]. The state-of-the-art method of this approach is by Clarkson and Woodru  X  [6] which was later improved slightly in [30]. It uses a hashing matrix S with only one non-zero entry in each column. Constructing this sketch takes only O (nnz( A )+ n  X  poly( k/ " )+poly( dk/ " )) time, and guarantees that for any unit vector x that (1 " ) k Ax k X  k Bx k X  (1 + " ) k Ax k . For these sparsity-e cient sketches using r = O ( d 2 / " 2 ) also guarantees that k A  X  B ( A ) k (1 + " ) k A A k k F .
 Iterative sketching techniques . These operate in streaming model, where random access to the matrix is not available. They maintain the sketch B as a linear combination of rows of A , and update it as new rows are received in the stream. Examples of these methods include di  X  erent version of iter-ative SVD [19, 21, 23, 5, 33]. These, however, do not have theoretical guarantees [7]. The FrequentDirections al-gorithm [24] is a unique in this group in that it o  X  ers strong error guarantees. It is a deterministic sketching technique that processes rows of an n  X  d matrix A in a stream and maintains a `  X  d sketch B (for ` &lt; min( n, d )) such that the following two error bounds hold for any 0  X  k&lt; ` and Setting ` = k +1 / " and ` = k + k/ " , respectively, achieves bounds k A T A B T B k 2  X  " k A A k k 2 F and k A  X  B ( A ) k (1+ " ) k A A k k 2 F . Although FrequentDirections does not run in input sparsity time, we will explain it in detail in the next section, as it is an important building block for the algorithm we introduce.
We present a randomized version of FrequentDirec-tions ,calledas SparseFrequentDirections that receives an n  X  d sparse matrix A as a stream of its rows. It computes a `  X  d sketch B in O ( d ` ) space.

It guarantees that with probability at least 1 (for 2 (0 , 1) being the failure probability), for  X  =6 / 41 and any 0  X  k&lt;  X  X  , and Note that setting ` = d 1 / ( " X  )+ k/  X  e yields and setting ` = d k/ ( " X  )+ k/  X  e yields The expected running time of the algorithm is O (nnz( A ) ` log( d )+nnz( A )log( n/ )+ n ` 2 + n ` log( n/ )) In the likely case where nnz( A )=  X  ( n ` ) and n/ &lt; d the runtime is dominated by O (nnz( A ) ` log( d )). We also ex-perimentally validate this theory, demonstrating these run-time improvements on sparse data without sacrificing accu-racy. In this section we review some important properties about FrequentDirections and SimultaneousIteration which will be necessary for understanding and proving bounds on SparseFrequentDirections . The FrequentDirections algorithm was introduced by Liberty [24] and received an improved analysis by Ghashami et al.[17]. The algorithm operates by collecting several rows of the input matrix and letting the sketch grow. Once the sketch doubles in size, a lossy DenseShrink operation re-duces its size by a half. This process repeats throughout the stream. The running time of FrequentDirections and its error analysis are strongly coupled with the properties of the SVD used to perform the DenseShrink step.
 An analysis of [7] slightly generalized the one in [17]. Let B be the sketch resulting in applying FrequentDirections with a potentially di  X  erent shrink operation to A . Then, the FrequentDirections asymptotic guarantees hold as long as the shrink operation exhibits three properties, for any positive and a constant  X  2 (0 , 1). 1. Property 1: 2. Property 2: 3. Property 3: k A k 2 F k B k 2 F  X  ` .
 For completeness, the exact guarantee is stated in Lemma 3.1.
Lemma 3.1 (Lemma 3.1 in [7]). Given an input n  X  d matrix A and an integer parameter ` ,anysketch `  X  d matrix B which satisfies the three properties above (for some any  X  2 (0 , 1] and &gt; 0 ), guarantees the following error bounds and where  X  k B (  X  ) represents the projection operator onto B top k singular vectors of B .

Another important property of FrequentDirections is that its sketches are mergeable [17]. To clarify, consider par-titioning a matrix A into t blocks A 1 ,A 2 ,  X  X  X  ,A t so that A = [ A of the matrix block A i , and B 0 = FD([ B 1 ; B 2 ;  X  X  X  ; B notes the FD sketch of all B i s combined together. It is shown that B 0 has at most as much covariance and projection er-ror as B = FD( A, ` ), i.e. the sketch of the whole matrix A . It follows that this divide-sketch-and-merging can also be applied recursively on each matrix block without increasing the error.

The runtime of FrequentDirections is determined by the number of shrinking steps. Each of those computes an SVD of B which takes O ( d ` 2 ) time. Since the SVD is called only every O ( ` ) rows this yields a total runtime O ( d ` 2  X  n/ ` )= O ( nd ` ). This e  X  ectively means that on av-erage we are spending O ( d ` ) operations per row, even if the row is sparse.
 In the present paper, we introduce a new method called as SparseFrequentDirections that uses randomized SVD methods instead of the exact SVD to approximate the sin-gular vectors and values of intermediate matrices B .We show how this new method tolerates the extra approxima-tion error and runs in time proportional to nnz( A ). More-over, since it received sparse matrix rows, it can observe more the ` rows until the size of the sketch doubles. As a re-mark, Ghashami and Phillips [18] showed that maintaining any rescaled set of ` rows of A over a stream is not a fea-sible approach to obtain sparsity in FrequentDirections . It was left as an open problem to produce some version of FrequentDirections that took advantage of the sparsity of A .
E ciently computing the singular vectors of matrices is one of the most well studies problems in scientific computing. Recent results give very strong approximation guarantees for block power method techniques [32][38][26][20]. Several vari-ants of this algorithm were studied under di  X  erent names in the literature e.g. Simultaneous Iteration, Subspace Itera-tion, or Orthogonal Iteration [19]. In this paper, we refer to this group of algorithms collectively as SimultaneousIter-ation . A generic version of SimultaneousIteration for rectangular matrices is described in Algorithm 1. Algorithm 1 SimultaneousIteration
Input : A 2 R n  X  d , rank k  X  min( n, d ), and error " 2 (0 , 1) q =  X  (log( n/ " ) / " ) G  X  N (0 , 1) d  X  k
Z = GramSchmidt( A ( A T A ) q G ) return Z # Z 2 R n  X  k
While this algorithm was already analyzed by [19], the proofs of [32, 20, 29, 37] manage to prove stable results that hold for any matrix independent of spectral gap issues. Unfortunately, an in depth discussion of these algorithms and their proof techniques is beyond the scope of this paper.
For the proof of correctness of SparseFrequentDirec-tions , the main lemma proven by [29] su ces. Simultane-ousIteration (Algorithm 1) guarantees the three following error bounds with high probability: 1. Frobenius norm error bound: 2. Spectral norm error bound: 3. Per vector error bound:
In addition, for a constant " , SimultaneousIteration runs in  X  O (nnz( A )) time.

In this paper, we show that SparseFrequentDirections can replace the computation of an exact SVD by using the results of [29] with " being a constant. This alteration does give up the optimal asymptotic accuracy (matching that of FrequentDirections ).
The SparseFrequentDirections ( SFD ) algorithm is de-scribed in Algorithm 2, and is an extension of FrequentDi-rections to sparse matrices. It receives the rows of an input matrix A in a streaming fashion and maintains a sketch B of ` rows. Initially B is empty. On receiving rows of A , SFD stores non-zeros in a bu  X  er matrix A 0 . The bu  X  er is deemed full when it contains ` d non-zeros or d rows. SFD then calls BoostedSparseShrink to produce its sketch matrix B 0 of size `  X  d . Then, it updates its ongoing sketch B of the en-tire stream by merging it with the (dense) sketch B 0 using DenseShrink .
 Algorithm 2 SparseFrequentDirections Input: A 2 R n  X  d ,aninteger `  X  d , failure probability for a 2 A do return B
BoostedSparseShrink amplifies the success probability of another algorithm SparseShrink in Algorithm 3. Sparse-Shrink runs SimultaneousIteration instead of a full SVD to take advantage of the sparsity of its input A 0 . However, as we will discuss, by itself SparseShrink has too high of a probability of failure. Thus we use BoostedSparseShrink which keeps running SparseShrink and probabilistically verifying the correctness of its result using VerifySpec-tral , until it decides that the result is correct with high enough probability. Each of DenseShrink , SparseShrink , and BoostedSparseShrink produce sketch matrices of size `  X  d .
 Algorithm 3 SparseShrink Input : A 0 2 R m  X  d ,aninteger `  X  m Z = SimultaneousIteration ( A 0 , ` , 1 / 4)
P = Z T A 0 ,[ H,  X  ,V ] = SVD( P, ` )  X   X  =
B 0 =  X   X  V T return B 0 Algorithm 4 BoostedSparseShrink
Input : A 0 2 R m  X  d ,integer `  X  m , failure probability while True do Algorithm 5 DenseShrink
Input : A 2 R m  X  d ,aninteger `  X  m [ H,  X  ,V ] = SVD( A, ` )  X   X  = B =  X   X  V T Return B
Our main result is stated in the next theorem. It follows from combining the proofs contained in the subsections be-low.
 Theorem 4.1 (main result). Given a sparse matrix A 2 R n  X  d and an integer `  X  d , SparseFrequentDirec-tions computes a small sketch B 2 R `  X  d such that with probability at least 1 for  X  =6 / 41 and any 0  X  k&lt;  X  X  , and The total memory footprint of the algorithm is O ( d ` ) and its expected running time is
O nnz( A ) ` log( d )+nnz( A )log( n/ )+ n ` 2 + n ` log( n/ ) .
SparseShrink , described in Algorithm 3, calls Simulta-neousIteration to approximate the top rank ` subspace of A 0 .As SimultaneousIteration is randomized, it fails to converge to a good subspace when the initial choice of the random matrix G does not su ciently align with the top ` singular vectors of A 0 (see Algorithm 1). This occurs with probability at most  X  ` = O (1 / prove that with probability of at least 1  X  ` that Sparse-Shrink satisfies the three properties required for Lemma 3.1 using  X  =6 / 41 and = 41 / 8 s 2 ` , but replacing Property 2 with a stronger version where s ` denotes the ` th singular value of A 0 .
However, for the proof of SparseFrequentDirections we require that all SparseShrink runs to be successful. The failure probability of SparseShrink , which is upper bounded by O (1 / bound would not give a meaningful bound on the failure probability of SparseFrequentDirections . We therefore reduce the failure probability of each BoostedSparseShrink , by wrapping each call of SparseShrink in the verifier Ver-ifySpectral . If VerifySpectral does not verify the cor-rectness, then it reruns SparseShrink and tries again until it can verify it. But to perform this verification e ciently, we need to loosen the definition of correctness. In partic-ular, we say SparseShrink is successful if the sketch B 0 computed from its output satisfies k A 0 T A 0 B 0 T B 0 k (the original Property 2 specification in Section 3.1), where =( k A 0 k 2 F k B 0 k 2 F ) /  X  X  . Combining the two inequalities through , a successful run implies that k A 0 T A 0 B 0 T ( k A 0 k 2 F k B 0 k 2 F ) /  X  X  . VerifySpectral verifies the suc-cess of the algorithm by approximating the spectral norm method for c  X  log( d/ i ) steps for some constant c . Algorithm 6 VerifySpectral
Initialization persistent i =0( i retains its state between invocations of this method)
Input : Matrix C 2 R d  X  d , failure probability i = i + 1 and i = / 2 i 2
Pick x uniformly at random from the unit sphere in R d . else return False
Lemma 4.1. The VerifySpectral algorithm returns True if k C k 2  X  1 .If k C k 2 2 it returns False with probability at least 1 i .
 1. If k C k 2, consider execution i of the method. Let v denote the top singular vector of C . Then k C c  X  log( d/ | h v 1 ,x i | 2 c  X  log( d/ i ) 1, for some constant c as long as | h v  X  (poly( i /d )). Let ( t 0 ) denote the density function of the random variable t 0 = h v 1 ,x i . Then Pr[ | h v 1 ,x i |  X  t ]= R t ( t bility to be at most i , we conclude that | h v 1 ,x i | =  X  ( with probability at least 1 i .

Therefore, VerifySpectral fails with probability at most i during execution i . If any of VerifySpectral runs fail, BoostedSparseShrink and hence SparseFrequent-Directions potentially fail. Taking the union bound over all invocations of VerifySpectral we obtain that Sparse-FrequentDirections fails with probability at most P 1 .
Throughout this manuscript we assume the constant-word-size model. Integers and floating point numbers are repre-sented by a constant number of bits. Random access into memory is assumed to require O (1) time. In this model, multiplying a sparse matrix A 0 by a dense vector requires O (nnz( A 0 )) operations and storing A 0 requires O (nnz( A bits of memory.

Fact 4.1. The total memory footprint of SparseFre-quentDirections is O ( d ` ) .

Proof. It is easy to verify that, except for the bu  X  er matrix A 0 , the algorithm only manipulates `  X  d matrices; in particular, observe that the (rows( A 0 )= d ) condition in SparseFrequentDirections ensures that m = d in SparseShrink , and in DenseShrink also m =2 ` .Each of these `  X  d matrices clearly require at most O ( d ` ) bits of memory. The bu  X  er matrix A 0 contains at most O ( d ` ) non-zeros and therefore does not increase the space complexity of the algorithm.
 We turn to bounding the expected runtime of Sparse-FrequentDirections which is dominated by the cumula-tive running times of DenseShrink and BoostedSparse-Shrink . Denote by T the number of times they are exe-cuted. It is easy to verify T  X  nnz( A ) /d ` + n/d . Since DenseShrink runs in O ( d ` 2 ) time deterministically, the total time spent by DenseShrink through T iterations is O ( Td ` 2 )= O (nnz( A ) ` + n ` 2 ).

The running time of BoostedSparseShrink is domi-nated by those of SparseShrink and VerifySpectral , and its expected number of iterations. Note that, in ex-pectation, they are each executed on any bu  X  er matrix A small constant number of times because VerifySpectral succeeds with probability (much) greater than 1 / 2. For asymptotic analysis it is identical to assuming they are each executed once.
 Note that the running time of SparseShrink on A 0 i is O (nnz( A 0 i ) ` log( d )). Since a total running time of O (nnz( A ) ` log( d )). The i th exe-cution of VerifySpectral requires O ( d ` log( d/ i )) opera-tions. This, because it multiplies A 0 T A 0 B 0 T B 0 by a single vector O (log( d/ i )) times, and both nnz( A 0 )  X  O ( d ` ) and nnz( B 0 )  X  d ` . In expectation VerifySpectral is executed O ( T ) times, therefore total running time of it is
Combining the above contributions to the total running time of the algorithm we obtain Fact 4.2.

Fact 4.2. Algorithm SparseFrequentDirections runs in expected time of
O (nnz( A ) ` log( d )+nnz( A )log( n/ )+ n ` 2 + n ` log( n/ )) .
We turn to proving the error bounds of Theorem 4.1. Our proof is divided into three parts. We first show that Sparse-Shrink obtains the three properties needed for Lemma 3.1 with probability at least 1  X  ` , and with the constraint on Property 2 strengthed by a factor 1 / 2. Then we show how loosening Property 2 back to its original bound en-ables BoostedSparseShrink to succeed with probability 1 i for some i  X   X  ` . Finally we show that due to the mergeability of FrequentDirections [25], discussed in Section 3.1, the SparseFrequentDirections algorithm obtains the same error guarantees as BoostedSparseShrink with probability 1 for a small of our choice.
 In what follows, we mainly consider a single run of Sparse-Shrink or BoostedSparseShrink and let s ` and u ` denote the ` th singular value and ` th left singular vector of A spectively. Here we show that with probability at least 1  X  ` that B 0 computed from SparseShrink ( A 0 , ` ) satisfies the three properties discussed in Section 3.1 required for Lemma 3.1. Lemma 4.2. Property 1 holds deterministically for Sparse-Shrink : k A 0 x k 2 k B 0 x k 2 0 for all x .
 Proof. Let P = Z T A 0 be as defined in SparseShrink . Consider an arbitrary unit vector x 2 R d , and let y = A k A 0 x k 2 k Px k 2 = k A 0 x k 2 k Z T A 0 x k 2 = k y k and therefore k A 0 x k 2 k B 0 x k 2 =( k A 0 x k 2 k Px k 2 k B 0 x k 2 ) 0.

Lemma 4.3. With probability at least 1  X  ` ,Property 2holdsfor SparseShrink :foranyunitvector x 2 R d , k A 0 x k 2 k B 0 x k 2  X  41 / 16 s 2 ` .

Proof. Consider an arbitrary unit vector x 2 R d , and note that k A 0 x k 2 k B 0 x k 2 = k A 0 x k 2 k Px k 2 + k Px k 2 k B We bound each term individually. The first term is bounded as k A 0 x k 2 k Px k 2 = x T ( A 0 T A 0 P T P ) x (1) Where transition 5 is true because ( I ZZ T ) is a projection. Transition 7 also holds by the spectral norm error bound of [29] for " =1 / 4. To bound the second term, note that as defined in SparseShrink . where last inequality follows by the Courant-Fischer min-max principle, i.e. as ` is the ` th singular value of the projection of A 0 onto Z , then `  X  s ` . Summing the two terms yields k A 0 x k 2 k B 0 x k 2  X  41 / 16 s 2 ` .
The original bound k A 0 T A 0 B 0 T B 0 k 2  X  =41 / 8 s 2 discussed in Section 4.1 is also immediately satisfied.
Lemma 4.4. With probability at least 1  X  ` ,Property3 holds for SparseShrink : k A 0 k 2 F k B 0 k 2 F ` (3 / 4) s
Proof. k A In addition, The last inequality holds by the per vector error bound of [29] for i = ` and " =1 / 4, i.e. | u T ` A 0 A 0 T u ` | s ` 2 ` |  X  1 / 4 s 2 ` +1  X  1 / 4 s 2 ` , which means 2 ` 3 / 4 s Therefore k A
We now consider the BoostedSparseShrink algorithm, and the looser version of Property 2 (the original version) as By invoking VerifySpectral (( A 0 T A 0 B 0 T B 0 ) / ( / 2) , ), then VerifySpectral always returns True if k A 0 T A 0 B 0 T B 0 k 2  X  / 2 (as is true of the input with probability at least 1  X  ` by Lemma 4.3), and VerifySpectral catches a failure event where k A 0 T A 0 B 0 T B 0 k 2 with probability at least 1 i by Lemma 4.1. As discussed in Section 4.1 all invocations of VerifySpectral succeed with probability at most 1 , hence all runs of BoostedSparseShrink succeed and satisfy Property 2 (as well as Properties 1 and 3) with  X  =6 / 41 and = 41 / 8 s 2 ` , and with probability at least 1 . Finally, we can invoke the mergeability property of FrequentDirections [25] and Lemma 3.1 to obtain the error bounds in our main result, Theorem 4.1.
In this section we empirically validate that SparseFre-quentDirections matches (and often improves upon) the accuracy of FrequentDirections , while running signifi-cantly faster on sparse real and synthetic datasets.
We do not implement SparseFrequentDirections ex-actly as described above. Instead we directly call Sparse-Shrink in Algorithm 2 in place of BoostedSparseShrink . The randomized error analysis of SimultaneousIteration indicates that we may occasionally miss a subspace within a call of SimultaneousIteration and hence SparseShrink ; but in practice this is not a catastrophic event, and as we will observe, does not prevent SparseFrequentDirections from obtaining small empirical error.

The empirical comparison of FrequentDirections to other matrix sketching techniques is now well-trodden [17, 7]. FrequentDirections (and, as we observe, by associa-tion SparseFrequentDirections ) has much smaller error than other sketching techniques which operate in a stream. However, FrequentDirections is somewhat slower by a factor of the sketch size ` up to some leading coe cients. We do not repeat these comparison experiments here. Setup . We ran all the algorithms under a common imple-mentation framework to test their relative performance as accurately as possible. We ran the experiments on an In-tel(R) Core(TM) 2.60 GHz CPU with 64GB of RAM run-ning Ubuntu 14.04.3. All algorithms were coded in C, and compiled using gcc 4.8.4. All linear algebra operation on dense matrices (such as SVD) invoked those implemented in LAPACK.
 Datasets . We compare the performance of the two algo-rithms on both synthetic and real datasets. Each dataset is an n  X  d matrix A containing n datapoints in d dimen-sions.

The real dataset is part of the 20 Newsgroups dataset [22], that is a collection of approximately 20 , 000 documents, par-titioned across 20 di  X  erent newsgroups. However we use the  X  X y date X  version of the data, where features (columns) are tokens and rows correspond to documents. This data matrix is a zero-one matrix with 11 , 314 rows and 117 , 759 columns. In our experiment, we use the transpose of the data and picked the first d = 3000 columns, hence the subset matrix has n =117 , 759 rows and d = 3000 columns; roughly 0 . 15% of the subset matrix is non-zeros.

The synthetic data generates n rows i.i.d. Each row re-ceives exactly z  X  d non-zeros (with default z = 100 and d = 1000), with the remaining entries as 0. The non-zeros are chosen as either 1 or 1 at random. Each non-zero location is chosen without duplicates among the columns. The first 1 . 5 z columns (e.g., 150), the  X  X ead X , have a higher probability of receiving a non-zero than the last d 1 . 5 z columns, the  X  X ail X . The process to place a non-zero first chooses the head with probability 0 . 9 or the tail with prob-ability 0 . 1. For whichever set of columns it chooses (head or tail), it places the non-zero uniformly at random among those columns.
 Measurements . Each algorithm outputs a sketch matrix B of ` rows. For each of our experiments, we measure the e -ciency of algorithms against one parameter and keep others fixed at a default value. Table 1 lists all parameters along with their default value and the range they vary in for syn-thetic dataset. We measure the accuracy of the algorithms with respect to: In all experiments, we have set k = 10. Note that proj-err is always larger than 1, and for FrequentDirections and SparseFrequentDirections the cov-err is always smaller than 1 / ( 6 41 ` k ) due to our error guarantees.
By considering Table 2 on synthetic data and Figure 1 on the real data, we can vary and learn many aspects of the runtime and accuracy of SparseFrequentDirections and FrequentDirections .
 Runtime . Consider the last row of Table 2, the  X  X un Time X  row, and the last column of Figure 1. SparseFrequentDi-rections is clearly faster than FrequentDirections for all datasets, except when the synthetic data becomes dense in the last column of the  X  X un Time X  row, where d =1000 and nnz per row = 500 in the right-most data point. For the default values the improvement is between about a fac-tor of 1 . 5x and 2 x , but when the matrix is very sparse the improvement is 10x or more. Very sparse synthetic exam-ples are seen in the left data points of the last column, and in the right data points of the second column, of the  X  X un Time X  row.

In particular, these two plots (the second and fourth columns of the  X  X un Time X  row) really demonstrate the dependence of SparseFrequentDirections on nnz( A ) and of Fre-quentDirections on n  X  d . In the last column, we fix the matrix size n and d , but increase the number of non-zeros nnz( A ); the runtime of FrequentDirections is basically constant, while for SparseFrequentDirections it grows linearly. In the second column, we fix n and nnz( A ), but in-crease the number of columns d ; the runtime of Frequent-Directions grows linearly while the runtime for Sparse-FrequentDirections is basically constant.

These algorithms are designed for datasets with extremely large values of n ; yet we only run on datasets with n up to 60 , 000 in Table 2, and 117 , 759 in Figure 1. However, both FrequentDirections and SparseFrequentDirections have runtime that grows linearly with respect to the number of rows (assuming the sparsity is at an expected fixed rate per row for SparseFrequentDirections ). This can also be seen empirically in the first column of the  X  X un Time X  row where, after a small start-up cost, both FrequentDi-rections and SparseFrequentDirections grow linearly as a function of the number of data points n . Hence, it is valid to directly extrapolate these results for datasets of increased n .
 Accuracy . We will next discuss the accuracy, as measured in Projection Error in the top row of Table 2 and left plot of Figure 1, and in Covariance Error in the middle row of Table 2 and middle plot of Figure 1.
 We observe that both FrequentDirections and Sparse-FrequentDirections obtain very small error (much smaller than upper bounded by the theory), as has been observed elsewhere [17, 7]. Moreover, the error for SparseFrequent-Directions always nearly matches, or improves over Fre-value of all parameters.
 quentDirections . We can likely attribute this improve-ment to being able to process more rows in each batch, and hence needing to perform the shrinking operation fewer over-all times. The one small exception to SparseFrequentDi-rections having less Covariance Error than FrequentDi-rections is for extreme sparse datasets in the leftmost data points of Table 2, last column  X  we attribute this to some peculiar orthogonality of columns with near equal norms due to extreme sparsity. [1] Nick Asendorf, Madison McGa n, Matt Prelee, and [2] Christos Boutsidis, Petros Drineas, and Malik [3] Christos Boutsidis, Michael W Mahoney, and Petros [4] Christos Boutsidis and David P Woodru  X  . Optimal [5] Matthew Brand. Incremental singular value [6] Kenneth L. Clarkson and David P. Woodru  X  . Low [7] Amey Desai, Mina Ghashami, and Je  X  M Phillips. [8] Amit Deshpande and Santosh Vempala. Adaptive [9] Inderjit S Dhillon and Dharmendra S Modha. Concept [10] Petros Drineas and Ravi Kannan. Pass e cient [11] Petros Drineas, Ravi Kannan, and Michael W. [12] Petros Drineas, Ravi Kannan, and Michael W [13] Petros Drineas, Iordanis Kerenidis, and Prabhakar [14] Petros Drineas, Michael W. Mahoney, and [15] Petros Drineas, Michael W Mahoney, [16] Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast [17] Mina Ghashami, Edo Liberty, Je  X  M Phillips, and [18] Mina Ghashami and Je  X  M Phillips. Relative errors [19] Gene H Golub and Charles F Van Loan. Matrix [20] Nathan Halko, Per-Gunnar Martinsson, and Joel A [21] Peter Hall, David Marshall, and Ralph Martin. [22] Ken Lang. Newsweeder: Learning to filter netnews. In [23] A Levey and Michael Lindenbaum. Sequential [24] Edo Liberty. Simple and deterministic matrix [25] Edo Liberty. Simple and deterministic matrix [26] Edo Liberty, Franco Woolfe, Per-Gunnar Martinsson, [27] Michael W Mahoney and Petros Drineas. Cur matrix [28] Ji X r  X  X  Matou X sek. On variants of the [29] Cameron Musco and Christopher Musco. Stronger [30] Jelani Nelson and Huy L. Nguyen. OSNAP: Faster [31] Christos H. Papadimitriou, Hisao Tamaki, Prabhakar [32] Vladimir Rokhlin, Arthur Szlam, and Mark Tygert. A [33] David A Ross, Jongwoo Lim, Ruei-Sung Lin, and [34] Mark Rudelson and Roman Vershynin. Sampling from [35] Tamas Sarlos. Improved approximation algorithms for [36] Santosh S Vempala. The random projection method , [37] Rafi Witten and Emmanuel Cand`es. Randomized [38] Franco Woolfe, Edo Liberty, Vladimir Rokhlin, and
