 The rapidly increasing availability of electronic health records (EHRs) from multiple heterogeneous sources has spearheaded the adoption of data-driven approaches for improved clini-cal research, decision making, prognosis, and patient man-agement. Unfortunately, EHR data do not always directly and reliably map to phenotypes, or medical concepts, that clinical researchers need or use. Existing phenotyping ap-proaches typically require labor intensive supervision from medical experts.

We propose Marble, a novel sparse non-negative tensor factorization method to derive phenotype candidates with virtually no human supervision. Marble decomposes the observed tensor into two terms, a bias tensor and an in-teraction tensor. The bias tensor represents the baseline characteristics common amongst the overall population and the interaction tensor defines the phenotypes. We demon-strate the capability of our proposed model on both sim-ulated and patient data from a publicly available clinical database. Our results show that Marble derived phenotypes provide at least a 42.8% reduction in the number of non-zero element and also retains predictive power for classifica-tion purposes. Furthermore, the resulting phenotypes and baseline characteristics from real EHR data are consistent with known characteristics of the patient population. Thus it can potentially be used to rapidly characterize, predict, and manage a large number of diseases, thereby promising a novel, data-driven solution that can benefit very large seg-ments of the population.
 H.2.8 [ Database Applications ]: Data mining Tensor; Dimensionality reduction; EHR phenotyping; Ap-plication
Electronic health records (EHRs) are becoming an in-creasingly important source of detailed patient information. Effective integration and efficient analysis of EHRs can aid in solving many of the healthcare problems: making informed clinical decisions, improving patient safety, and facilitating investigations and knowledge discovery. However, several formidable challenges arise from the application of EHR data to clinical research, including diverse populations, het-erogeneous and noisy information, and interpretability con-straints. Medical professionals are accustomed to reasoning based on concise and meaningful medical concepts, or phe-notypes. EHR-based phenotyping is a process to map raw EHR data into meaningful medical concepts, learning med-ically relevant characteristics of the data [15], and is impor-tant for supporting genome-wide association studies [10]. An example is the severe early childhood obesity phenotype 1 which identifies children with increased risk of adult obesity and a potential lifetime of complications.
 State of the art phenotype development, such as the eMerge Network 2 , relies primarily on approaches that are heuristic, rule, and iterative based, and is a collaborative team effort between clinicians and IT experts [15, 22]. Recent work has focused on high-throughput phenotyping, efficient and automated phenotype extractions to reduce manual devel-opment. Although data mining tools have been utilized to automate the phenotype process, current high-throughput methodologies cannot generate large amounts of candidate phenotypes and achieve good performance without human annotated samples [5]. Therefore, a major limitation of ex-isting phenotype efforts is the need for human annotation of case and control samples, which require substantial time, effort, and expert knowledge to develop.

The  X  X deal X  phenotype (i) represents complex interactions between several sources, (ii) is concise and easily understood by a medical professional, and (iii) maps to domain knowl-edge. Thus, phenotyping can be viewed as a form of dimen-sionality reduction, where each phenotype forms a latent space [15]. Matrix factorization, a common dimensional-ity reduction approach, is insufficient as it cannot concisely capture structured EHR source interactions, such as multi-ple medications prescribed to treat a single disease. A more natural transformation is tensor factorization, which utilizes T he phenotype definition can be found in Phenotype KnowledgeBase.
The eMerge network explores the use of EHR to obtain phenotypic information at multiple medical institutions. Figure 1: Example of a Marble-derived phenotype from C MS data. the multiway structure to produce concise and potentially more interpretable results. We conducted a pilot study to evaluate tensor-derived phenotypes using EHR data from the Geisinger Health System [14]. A medical expert evalu-ated 50 tensor-derived phenotypes and found that 82% gen-erally mapped to a medical concept. However, the domain expert X  X  comments primarily revolved around the lack of conciseness and the presence of  X  X nnecessary X  diagnoses and medications. Therefore, a sparse non-negative tensor factor-ization of count data is desired to simultaneously generate multiple concise phenotypes. An efficient and automated high-throughput phenotyping process can help identify ex-isting, as well as novel, medical concepts from large-scale EHR data, and increase our ability to create personalized applications to improve the health and well-being of the gen-eral population.

This paper presents Marble , a novel sparse non-negative tensor factorization model to fit count data. Analogous to the geology domain, where marble rock is used to produce monuments, buildings, and sculptures, our algorithm can serve as the basis of automated high-throughput phenotyp-ing tools. Our model extends the non-negative CANDE-COMP/PARAFAC (CP) Poisson tensor decomposition [6] from two aspects: (i) constraints on the factor matrices to minimize the number of non-zero elements, and (ii) aug-mentation of the tensor approximation. Marble decomposes an observed tensor into two terms, a bias (or offset) ten-sor and an interaction (or signal) tensor. The bias tensor represents the baseline characteristics common amongst the overall population and also provides computational stabil-ity. The interaction term is compromised of concise, intu-itive, and interpretable phenotypes in the data, illustrated in Figure 1. This paper details the tensor factorization model and presents the algorithm to solve the problem formula-tion using both an alternating minimization and sequential unconstrained minimization approach. We corroborate our model on simulation data as well as real EHR data. Our results demonstrate that Marble achieves at least a 42.8% reduction in the number of non-zero elements compared to CP-APR without sacrificing the quality of the tensor de-composition. Furthermore, the phenotypes and the baseline characteristics derived from the real EHR data are consis-tent with existing studies on the population.

The remainder of the paper is structured as follows. Sec-tion 2 presents preliminaries of matrix and tensor factoriza-tion and related work. Next, we detail our model in Section 3. Section 4 demonstrates and evaluates our model on sim-A , B , Z ,  X  ,  X  ,  X  ,  X  matrix ulation data and real EHR data. Finally, we summarize our w ork in Section 5.
This section describes the preliminaries of matrix and ten-sor decomposition and related tensor factorization work. Ta-ble 1 provides a key for the symbols used in the paper. For indexing of matrix A , we denote the ( i, j )th element as a the j th column as a : j , and the i th column as a i : .
Matrix decomposition . Matrix factorization (MF) is a common dimensionality reduction approach, which rep-resents the original data using a lower dimensional latent space. Standard MF approaches find two lower dimensional matrices that when multiplied together approximately pro-duce the original matrix, X  X  WH . Although many matrix decomposition techniques exist, singular value decomposi-tion and nonnegative matrix factorization (NMF) are two common algorithms used to reduce the feature dimension.
Notation Details . Definitions for algebraic operations used in the paper are provided below.
 Definition 1. The outer product of N vectors, a (1)  X  a (2)  X  X  X  X  a ( N ) , produces a N th order tensor X where each element x Definition 2. The element-wise multiplication (and divi-sion) of two same-sized matrices A  X  B ( A  X  B ) produces a matrix Z of the same size such that the element c ~ i = a ( c i = a ~ i /b ~ i ) for all Definition 3. The Khatri-Rao product of two matrices A  X  B of sizes I A  X  R and I B  X  R respectively, produces a matrix Z of size I A I B  X  R such that Z = a 1  X  b 1  X  X  X  a R  X  b where  X  represents the Kronecker product. The Kronecker product of two vectors a  X  b =
Tensor Decomposition . A tensor is a generalization of matrices to higher dimensions. Thus, tensor factorization (decomposition) is a natural extension of matrix factoriza-tion and utilizes information from the multiway structure that is lost when modes are collapsed to use matrix factoriza-tion algorithms [21, 23]. The CANDECOMP / PARAFAC (CP) [3, 13] model is a common tensor decomposition and c an be viewed as a higher-order generalization of singular value decomposition [17]. The CP model approximates the original tensor X as a sum of R rank-one tensors and can be expressed as Note that J  X  ; A (1) ; . . . ; A ( N ) K is shorthand notation to de-scribe the CP decomposition, where  X  is a vector of the weights  X  r and a ( n ) r is the r th column of A ( n ) . The CP ten-sor decomposition has been used for concept discovery [16], network analysis of fMRI data [9], and community discovery [20]. The details of computing the CP decomposition and other tensor decomposition models can be found in [17].
Some domain applications may desire non-negative com-ponents, a higher-order generalization of NMF. Non-negative tensor factorization (NTF) requires the elements of the fac-tor matrices and the weights to be non-negative. A broad survey of practical and useful NMF and NTF algorithms can be found in [8]. Our paper will focus on the nonnega-tive CP alternating Poisson regression (CP-APR) model to fit sparse count data [6]. Details of the algorithm and model are presented in the paper by Chi and Kolda [6].

NTF and NMF algorithms generally produce sparse rep-resentations. However, additional sparsity may be desired for the factor matrices. Traditional sparsity-inducing penal-ties such as  X  1 and  X  2 regularization [23] only deal with the standard least-squares minimization. Non-parametric Bayesian approaches to sparse Tucker decomposition have been recently proposed [24]. Nonetheless, there is a paucity of existing work regarding sparse factor representations us-ing KL divergence as an objective function. A multi-layer NTF has been proposed to achieve sparse representations for various cost functions including KL divergence using a non-linearly transformed gradient descent approach [7]. Yet, the approach is computationally expensive because multiple ten-sor factorization stages are required and sparsity constraints are achieved via an exponential update (i.e. s  X  s 1+  X  , for small  X  ). Sparse factor representation using KL divergence is difficult because (1) an incorrect zero in the factor rep-resentation causes the objective function to be ill-defined as lim i  X  0 log i =  X  X  X  , and (2) data centering, a technique commonly used to remove the bias in continuous data prior to matrix/tensor factorization, is not feasible as the objec-tive function (KL divergence) is defined on the non-negative orthant. Both challenges will be addressed in Section 3.3.
Marble is a sparse non-negative tensor factorization for count data. Marble decomposes an observed tensor into two terms, a bias (or offset) tensor and an interaction (or signal) tensor. The bias tensor represents the baseline characteris-tics common amongst the overall population and also pro-vides computational stability. The interaction term is com-promised of the R most prevalent phenotypes in the data (or medical concepts that are observed). Our model im-poses sparsity constraints by reducing the  X  X robabilistically unlikely X  mode elements. Figure 2 illustrates the factoriza-tion of the patient by diagnosis by procedure tensor into R
Figure 2: Deriving candidate phenotypes using Marble. p henotypes and the bias vectors. We will first formulate the problem and provide a general overview of the algorithm. Next, we introduce the sparse factor representation and the augmented bias tensor. Then, we present the algorithm to solve the problem formulation. Finally, we illustrate how Marble can be used to perform high-throughput phenotyp-ing in EHR data.
Let X denote an observed tensor constructed from count data with size I 1  X  I 2  X  X  X  X  X  I N and M represent a same-sized tensor of Poisson parameters for X . M is split into two terms, a rank one bias tensor C and a rank R interaction tensor V . The bias term, or baseline characteristics of the population, is composed of N positive vectors u (1) ,  X  X  X  , u and a positive scalar  X  . The interaction term is similar to the Poisson decomposition tensor, where each rank one ten-sor is comprised of N stochastic vectors (elements sum to 1 and non-negative) with a non-negative weight  X  r . However, Marble constrains the feasible space of the vectors to either be zero or above some threshold value  X  n . The optimization problem is defined as min f ( M )  X  X
We solve the problem using an alternating minimization approach, cycling through each mode while fixing all other modes. For each mode, the algorithm first calculates the factor matrix associated with the interaction tensor. The matrix is then gradually projected onto the feasible space, described in Section 3.2, using a penalty method approach detailed in Section 3.4.2. Once the interaction factor matrix is computed, the bias vector is computed. After an iteration (where the algorithm has cycled through all the modes), the projection penalty is updated and the whole process is repeated until convergence occurs. A high-level view of the Marble algorithm is illustrated in Algorithm 1, with details described in Section 3.4.
Algorithm 1: O verview of Marble algorithm while n ot converged do end
S parse factor representations are desired to improve inter-pretability and address the problem where the CP-APR ten-sor factorization model produced X  X robabilistically unlikely X  diagnoses and medications. The CP-APR model imposes a stochastic constraint on the columns of the factor matrices. For the r th rank one tensor, each non-zero element along the n th vector a ( n ) r represents a probabilistic estimate of the el-ement X  X  membership (e.g. probability the diagnosis diabetes belongs to this phenotype). Therefore, our model removes small non-zero elements to achieve sparse factor matrices, while also guaranteeing convergence to a local minimum. Marble modifies the stochastic constraints of CP-APR such that each n th factor matrix, A ( n ) , will have non-zero com-ponents that range from  X  n to 1 and the elements of each column sum to 1. Equation (5) captures the sparse factor representation constraint. Each mode can have a different threshold,  X  n , as the desired level of sparsity may depend both on domain constraints and the mode size.
Marble introduces a rank one bias tensor to capture base-line characteristics common amongst the overall population. Traditional tensor (or matrix) factorization approaches, which use a least squares loss, center the data by subtracting the feature mean from all the observations. Thus, the decom-position is only performed on the  X  X ignal X  (or  X  X nteraction X ) aspects of the data. Unfortunately, the data centering tech-nique is not feasible for Marble as the KL divergence is only defined for non-negative x ~ i .

The bias tensor also provides computational stability for the  X  X nadmissible zeros X  resulting from the sparse factor rep-resentation. Under the assumption that  X  n in Equation (5) is set to be non-zero, the sparse factor representation will produce a limited number of non-zero elements. Given that R is not over-specified, the probability an element m ~ i is high. If the corresponding observed tensor element is zero ( x i = 0), then m ~ i  X  x i log m ~ i = 0. However, for a non-zero x , the objective function is ill-defined as lim i  X  0 log i =  X  X  X  , leading to an  X  X nadmissible zero X . Several works have dealt with this problem by either avoiding zeros [11], altering the updates [19], or shifting elements into the interior [6]. How-ever, all these methods adversely effect the sparsity of the resulting factors.

We augment the Poisson tensor decomposition with an ad-ditional rank-one tensor, shown in Equation (2). The scalar constant  X  and the factor vectors u (1) , u (2) ,  X  X  X  , u be strictly positive , which are captured in Equations (3) and (4). Although similar in nature to an additional clinical phenotype, the bias tensor is not equivalent to increasing the rank of the original decomposition V to R + 1. Each factor vector ( u (1) where n &gt; 1) represents the common baseline characteristics amongst the entire population (e.g., the overall likelihood of developing diabetes, getting a chest x-ray, etc) and captures the data bias, or offset, of the ob-served tensor. Furthermore, the positive augmented tensor stabilizes the optimization problem by avoiding inadmissible zeros in M and allows sparse factor matrices in the inter-action tensor, A ( n ) .
The optimization problem presented in Section 3.1 is solved via an alternating minimization approach, where all factor vectors/matrices are fixed except for the one being updated. For each mode, we compute the subproblem solution using an approach similar to CP-APR [6]. For the n th mode, we express the mode-n matricization V ( n ) = B ( n )  X  ( n ) B ( n ) = A ( n )  X  , where  X  = diag(  X  ) (6) The weights (  X  ) are absorbed into the n th factor matrix B ( n ) in Equation (6) and we use the matrix  X  ( n ) to de-note the fixed parts in Equation (7). Note that the size of matrix B ( n ) is I n  X  R while the size of matrix  X  ( n ) R  X  Q N j =1 ,j 6 = n I j . Using the notation above, we can rewrite the objective function, Equation (1) as follows: where e is a vector of all ones, C ( n ) is the mode-n matriciza-tion of the bias tensor and X ( n ) is the mode-n matricization of the observed tensor.

The multiplicative update derivation for the weighted fac-tor matrix B ( n ) can be computed by taking the partial derivative of the objective function respect to a single el-ement b gh . Setting the gradient descent step size set to b gh P the multiplicative update: The non-negative factor matrix constraints are satisfied us-ing the multiplicative update. Generalizing for the entire factor matrix, the update equation is as follows:
We adopt the same alternating update strategy for the bias tensor C . In particular, we update the mode n bias vector u ( n ) holding other modes constant, with the fixed parts denoted as  X  ( n ) .
 The update for the augmented bias vector follows a similar d erivation as B ( n ) .
The alternating minimization updates satisfy the non-negative constraints of the weights (  X  ), factor matrices A and basis vectors u ( n ) . However, the factor matrices will not satisfy the modified stochastic constraints in Equation (5). The projected gradient descent method can be used to threshold the factor matrices after each alternating min-imization update. Experimental results (shown in Section 4) show that zeroing out components too early in the it-erative process negatively impacts the quality of the factor representation. Instead, Marble uses a penalty method ap-proach [2] to gradually adjust the projection threshold at each iteration. During the early iterations, when the factor representations are changing drastically, the  X  X easible set X  is closer to the [0 , 1] range. As the factor representations start to stabilize (difference in objective function starts to approach zero), the projection occurs on the feasible set de-scribed in Equation (5). A new scalar  X  is introduced to calculate the gradual projection, where each iteration uses the threshold  X  X  n . The range of  X  is [0,1], where zero rep-resents no projection and 1 represents the full projection. After each iteration k where all the modes have been cycled through, we update  X  as follows: Marble uses a moving average of the  X  to minimize drastic changes and also ensures that the penalty is non-decreasing at each iteration.
Subproblem Convergence . Marble performs several subproblem iterations (user-defined maximum L ) at each mode n for both the factor matrix A ( n ) and the bias vector u ( n ) . Empirical evidence suggests extra inner iterations can accelerate the convergence [6]. The benefit of subproblem iterations on simulation data is presented in Section 4. The exit criterion (Karush-Kuhn-Tucker (KKT) conditions) for the subproblem iterates is similar to the CP-APR algorithm [6]. For convenience, we introduce the following to matrices: which capture the multiplicative terms in Equations (8) and (10) respectively. These matrices are used to check conver-gence of the subproblem, with the algorithm exiting under the following conditions: where E represents a matrix of all ones. Note that min is the element-wise minimum of the two matrices, and the result should be a matrix of all zeros. We relax the zero equality Algorithm 2: D etailed Marble algorithm Data : X , R,  X ,  X 
Result : V , C for k = 1 , 2 ,  X  X  X  , K do end for practical purposes and check for convergence within a c ertain tolerance, e.g. min( u ( n ) , E  X  Z ( n ) )  X  kktTol . The detailed Marble algorithm is presented in Algorithm 2.
Sparse Implementation . When X is a sparse tensor, we construct V and C using the non-zero elements of the observed tensor and avoid constructing V and C explicitly. Our algorithm adopts the sparse tensor implementation ap-proach presented in [1, 6]. The only calculations necessary are the ones that correspond to the non-zero elements in X . We can store X as a set of values and indices ( v q , c ), where Q are the non-zero elements of the tensor. We then form Q rows of  X  and Z that correspond to each non-zero element of X . The q th vector of  X  and Z are: element ( i, r ) of  X  and Z as: Note that we require storage for the augmented tensor, which entails storing y ( q ) . Thus, we require Q additional storage compared to CP-APR.

Computational Complexity . Note that the subprob-lem iterates in the CP-APR model is the computational bottleneck of the algorithm. In particular, calculating  X  requires on the order of the decomposition rank R times the product of the dimension of each tensor mode I n . If we de-note the size of the largest mode as D , then CP-APR has the computational complexity of O D N . Although our algo-rithm has to compute the additional augmented vector, the computational complexity remains the same as CP-APR.
While Marble is a general non-negative sparse tensor fac-torization model to fit count data, we motivated the prob-lem to simultaneously derive multiple EHR phenotypes with minimal human intervention. We briefly describe the con-struction of an EHR count tensor and the resulting candi-date phenotypes using our algorithm. Figure 3 provides a conceptual illustration of the high-throughput phenotyping process. Each patient is anchored using an index date (i.e. hospital admit date) and the observation window can be de-fined as a fixed time window either before or after the index date depending on the application. Any data that occurs during the observation window is used during the construc-tion process. The EHR tensor is then constructed using the count of the co-occurrences between the various modes. In Figure 2, each tensor element represents the number of times either a normal or abnormal clinical measurement occurred.
Once the tensor is constructed, we can use Marble to fit a non-negative Poisson tensor decomposition to the data. The resultant tensor V is then used to define R candidate phenotypes, similar to Figure 1. Thus, the r th candidate phenotype is defined using the non-zero elements of the r th column from all N factor matrices.

New patients can be projected onto the tensor-derived phenotypes to obtain a phenotype membership vector. We define the phenotype membership vector as the convex com-bination of the tensor-derived phenotypes, where the r th element denotes the probability the patient exhibits charac-teristics consist with the r th phenotype. For notation pur-pose, we will assume the patient mode is the first mode of the tensor. Thus given a new patient X  X  tensor  X  X , we want to find  X   X  and  X a (1) that provides that best approximates the new patient X  X  tensor. Our projection also needs to deter-mine  X   X  , the strength of the bias in  X  X . We observe that this is equivalent to the optimization subproblem for the first mode with several noticeable differences: (1) the phenotype membership vector is obtained by normalizing the entries of  X  b (1) across all R phenotypes instead of the standard column normalization for each phenotype, (2) setting the augmented Figure 3: A high-level depiction of using Marble to generate h igh-throughput phenotypes. vector ( X  u (1) ) to 1 and absorbing the weight into  X   X  , and (3) ignoring the projection onto the feasible set defined in the optimization problem.
In this section, we will first evaluate the algorithmic per-formance using a simulated dataset where the actual tensor factors are known. Then, we evaluate the phenotyping per-formance using a realistic EHR dataset.

Evaluation Metric Details . Definitions for the evaluation metrics are provided below.

S imilarity( a , b ) = a  X  b Similarity is calculated using the cosine similarity, a compo-nent of the factor match score (FMS), to quantify the simi-larity between the computed solution and the actual factor representation. FMS is commonly used to quantify the close-ness of the computed solution and provides a single number between [0 , 1] [6]. However, FMS is an aggregate measure and can mask the mode-specific similarity results. For our analysis, we pair the computed rank-one tensors with the true rank-one tensors using a greedy algorithm, providing a lower bound on the similarity scores.
First, we analyze simulated data where the underlying factor representation is known. Specifically, we consider a third-order tensor of size 100  X  80  X  60 with rank of 10 ( R = 10). We generate the model M = C + V , where C = J  X  ; u (1) ;  X  X  X  ; u ( N ) K and V = J  X  ; A (1) ;  X  X  X  ; A factor matrix A ( n ) is generated as follows: (1) Sample the non-zero element indices for each column according to the sparsity pattern for the mode; (2) From the sampled indices, randomly select 10% of the entries (or minimum of one) to sample uniformly from the interval [0 , 10] to mirror real EHR characteristics (e.g. one or two diagnosis contribute heav-ily to a phenotype); (3) For the remaining indices, sample uniformly from [0 , 1]; and (4) Normalize the column so the elements sum to 1, and absorb the weight into  X  . Each augmented vector is chosen in a similar fashion except the weight is set to  X  = 2. The full tensor M is calculated from the factor matrices and the augmented vectors. Then each tensor element x ijk is sampled from the Poisson distribution with the parameter set to m ijk . Ten observation tensors are generated from the tensor M .
The number of maximum subproblem iterations ( L ) rep-resents the  X  X loseness X  to the subproblem solution. Thus, a single subproblem iteration (equivalent to the Lee-Seung multiplicative update [18]) only takes a step towards the sub-problem solution as observed by [6]. For the simulated data, the average computation time (in seconds) for 1, 5, and 10 subproblem iterations was 37.6, 89.4, and 92.56 respectively. Additional inner iterations do not accelerate convergence, as the computational time increase with the number of itera-tions. However, Figure 4a illustrates the benefit of extra subproblem iterations on the similarity scores from the true solution. The similarity score at 10 inner iterations is the highest across all the modes. Therefore, more subproblem iterations improves the quality of the resulting tensor de-composition.
Empirical evidence for the gradual projection approach is presented in Figure 4b. We compare the gradual projection described in Section 3.4.2 to (i) no projection where  X  = 0 for all iterations, and (ii) full projection where  X  = 1 for all iter-ations. No projection yields the best similarity score across all three modes. However, the gradual projection approach results in tensor factors that are near the ideal non-zero ra-tio of 1. It also has higher similarity scores on two of the modes compared to full projection. Even though the ob-servation matrix was generated from sparse factor represen-tations with a relatively small bias effect (  X  = 2), without any projection yields factor representations that on aver-age contain at least 2  X  more non-zero elements in each col-umn. The results demonstrate that the gradual projection approach can reproduce the same sparsity ratio (number of non-zero elements / size of mode) as the true solution but sacrifices in terms of similarity to the true solution. Next, we perform a comparison with the CP-APR model. The multi-layer sparse NTF model [7] is omitted due to the added computational complexity of their model. Us-ing a sparser underlying factor representation further high-lights the differences between Marble and CP-APR. Marble is slower than CP-APR, primarily due to the computation of the augmented tensor decomposition. On the simulated data, Marble takes about 58 seconds to converge to a local optimal, while CP-APR obtains a solution in 45 seconds. However, in comparison to current phenotyping approaches, the 15 second difference is negligible given that a single dis-ease phenotype can take months to develop.

Figure 4c displays a plot of the two algorithms based on the non-zero ratio used in Figure 4b. Marble X  X  sparse factor representation results in a higher similarity score for the first and second mode. All the Marble modes are close to the ideal non-zero ratio of 1. Furthermore, Marble achieves a 42.8%, 55.1%, and 68.4% reduction on the non-zero ratio. Thus, the non-zero pattern across all three modes is better captured by our model, whereas CP-APR results in a higher number of non-zero elements.
The Centers for Medicare and Medicaid Services (CMS) provides the CMS Linkable 2008-2010 Medicare Data En-trepreneurs X  Synthetic Public Use File (DE-SynPUF) , a pub-licly available dataset that spans 3 years and contains inpa-tient, outpatient, carrier, and prescription drug event claims in addition to the beneficiary summary files 3 . The claims records have been synthesized from 5% of the 2008 Medicare population to protect the privacy of the beneficiaries. Al-though the relationships between some of the variables have been altered to minimize re-identification risk, the sheer vol-ume of patients can still provide interesting and insightful phenotypes.

Our experiments focus on a random subset of 10,000 pa-tients from Sample 1 (CMS released the data in 20 sep-arate samples). We construct the tensor from the carrier claims records using the diagnosis and procedure codes. In-dividual International Classification of Diseases (ICD-9) di-agnosis codes and Healthcare Common Procedure Coding System (HCPCS) procedure codes capture information at a fine-grained level. Thus, similar diagnosis codes (proce-dure codes) were grouped using the Unified Medical Lan-guage System 4 to aggregate the individual ICD-9 codes and HCPCS codes to higher level medical hierarchies. Therefore,
A detailed description can be found on their website.
The Metathesaurus contains the source vocabularies for 150 sources, including ICD-9-CM and HCPCS. F igure 5: The distribution of factor elements along the three CMS tensor modes. Zeros entries are omitted from the plot. the constructed tensor is 10,000 patients by 129 diagnoses by 115 procedures.
The Marble algorithm computes the sparse factor repre-sentation using predefined thresholds  X  n . These thresholds provide a tunable knob to adjust the sparsity of the candi-date phenotypes. Domain constraints can be used to deter-mine the threshold value (e.g. a phenotype should only con-tain a maximum of 3 unique diagnoses). However, given the absence of domain knowledge, the Marble algorithm can be used with  X  n = 0 for all n . A plot of the non-zero elements distribution along each mode can be used to determine the thresholds. Figure 5 shows the mean histogram of the CP-APR non-zero factor values along the three modes using R = 50 using the 10 subsamples. For all three plots, there is a noticeable difference in size (the y-axis uses a log scale) between the first two bins, which suggests the threshold oc-cur at the start of the second bin. Thus, for the remainder of the paper, the thresholds used are  X  = [0 . 0001 , 0 . 01 , 0 . 01].
The phenotypes are evaluated on a classification task of predicting high cost (above 75 th percentile) beneficiaries. Our algorithm is compared against (i) CP-APR derived phe-notypes and (ii) the raw feature matrix with 129  X  115 columns, where each column represents a diagnosis-procedure combination. 10 random subsamples are obtained via strat-ified sampling with a 50-50 train test split, and the phe-notypes are derived from the training dataset only. An  X  1 regularized logistic regression model is trained separately on each of the three feature sets (the phenotype membership matrix is the feature matrix for both Marble and CP-APR) and the predictive performance is evaluated on the test set.
Figure 6 displays a plot of the area under the receiver operating characteristic curve (AUC) as a function of the number of phenotypes ( R ). The predictive performance of both tensor factorization models using 50 phenotypes is sim-ilar to baseline, providing a 300  X  feature reduction from the original raw feature matrix. Beyond 50 phenotypes, the ac-curacy gradually increases with the number of phenotypes. Marble takes approximately 3.5 hours 5 to factor the data using 50 phenotypes. For the remainder of this paper, we use R = 50 to provide detailed analysis of the individual phenotypes.

Marble and CP-APR perform similarly in terms of pre-dictive power. Table 2 illustrates a comparison between the
A computer with an Intel R Xeon R Processor X5660 and 8 GB RAM.
 Figure 6: Area under the receiver operating characteristic c urve for the three feature sets as a function of the number of phenotypes. The error bars display the 95% confidence interval. first, or highest  X  r , Marble-derived phenotype and a  X  X im-ilar X  CP-APR derived phenotype, where FMS is used to quantify overall closeness of the phenotypes. The CP-APR phenotype contains 10  X  more diagnosis and procedure ele-ments because there is no sparsity constraint on the factor-ization. In contrast, a medical professional can easily digest the contents of the phenotype resulting from the Marble al-gorithm. Marble yields concise phenotypes without losing predictive power.
An added benefit of Marble is that it captures the baseline characteristics that exist in the overall population via the rank-one bias tensor. Table 3 shows the 10 highest valued elements from the diagnosis and procedure mode, in decreas-ing magnitude. The diagnosis bias vector shows that Medi-care patients generally visit clinics because of various symp-toms and complications. Furthermore, the diagnosis vec-tor contains several chronic diseases common in the elderly population, such as hypertension, arthritis (arthropathies), heart disease, and diabetes (disease of other endocrine glands). Centers for Disease Control and Prevention (CDC) estimate that 80% of older adults suffer from at least one chronic condition and 50% have two or more chronic conditions [4]. The procedure basis vector also contains procedure codes relevant to the treatment of patients with chronic condi-tions. We verified the results in Table 3 with chronic disease reports provided by CDC 6 .
The United States spends more than 2.1 trillion dollars (75% of medical care) on the treatment of chronic diseases [4]. Thus, obtaining phenotypes related to chronic disease such as heart failure, diabetes, and cancer can help medical professionals tailor treatment options based on the patient X  X  phenotypes. The CMS dataset provides chronic disease in-dicators 7 that we will use to identify phenotypes associated with specific chronic diseases.

Table 4 illustrates two of the heart-failure related pheno-types, which maps to varying degrees of disease severity. Pa-
T he latest disease reports are located at http://www.cdc. gov/chronicdisease/resources/publications/aag.htm
A patient X  X  chronic condition flag cannot be perfectly re-produced due to the synthetic claim process used. and procedure factors, respectively) tients with the second phenotype (titled severe heart failur e) require hospital stays (inpatient services) and have added complications from lung disease. Table 5 depicts two other chronic disease phenotypes. The diabetes phenotype de-scribes patients with complications resulting from diabetes, as the procedures include organ or disease oriented panels. The second phenotype, associated with arthritis, suggests that patients belong to this phenotype are undergoing reha-bilitation to strengthen their joints. Furthermore, all four phenotypes shown are concise, easily interpretable, and map to known characteristics of the chronic disease.
This paper presented Marble, a novel sparse non-negative tensor factorization model to fit EHR count data. Our al-gorithm offers a data-driven solution to simultaneously gen-erate multiple phenotypes from a diverse EHR population without expert supervision. The resulting phenotypes are concise, intuitive, and interpretable; and automatically re-veal patient clusters on specific diagnoses and procedures. Furthermore, Marble captures the baseline characteristics of the overall population via an augmented bias tensor.
The experimental results on simulated data and 10,000 patient records from the CMS De-SYNPUF dataset demon-strate the conciseness, interpretability, and predictive power of Marble-derived phenotypes. They underscore the promise of Marble for high-throughput phenotyping with minimal human intervention. Marble can potentially be used to rapidly characterize, predict, and manage a large number of dis-eases, thereby promising a novel, data-driven solution that can benefit very large segments of the population. Future work will focus on generalizing the sparse non-negative ten-sor factorization to multi-relational tensors [20] to incorpo-rate multiple EHR data sources and examine quasi-Newton methods to improve computational speed of the algorithm [12].
 We would like to thank Yubin Park and Suriya Gunasekar for their comments. This research is supported by the Schlum-berger Centennial Chair in Engineering; Army Research Of-fice under grant W911NF-11-1-0258; and Department of De-fense award under award number 60036907. [1] B. W. Bader and T. G. Kolda. Efficient MATLAB [2] C. L. Byrne. Alternating Minimization as Sequential [3] J. D. Carroll and J.-J. Chang. Analysis of individual [4] Centers for Disease Control and Prevention (CDC). [5] Y. Chen, R. J. Carroll, E. R. M. Hinz, A. Shah, A. E. [6] E. C. Chi and T. G. Kolda. On tensors, sparsity, and [7] A. Cichocki, R. Zdunek, S. Choi, R. Plemmons, and [8] A. Cichocki, R. Zdunek, A. H. Phan, and S.-I. Amari. [9] I. Davidson, S. Gilpin, O. Carmichael, and P. Walker. [10] J. C. Denny. Mining electronic health records in the [11] N. Gillis and F. Glineur. Accelerated multiplicative [12] S. Hansen, T. Plantenga, and T. G. Kolda.
 [13] R. A. Harshman. Foundations of the PARAFAC [14] J. C. Ho, J. Ghosh, S. Steinhubl, W. Stewart, J. C. [15] G. Hripcsak and D. J. Albers. Next-generation [16] U. Kang, E. Papalexakis, A. Harpale, and [17] T. G. Kolda and B. W. Bader. Tensor decompositions [18] D. D. Lee and H. S. Seung. Learning the parts of [19] C.-J. Lin. On the convergence of multiplicative update [20] Y.-R. Lin, J. Sun, H. Sundaram, A. Kelliher, [21] M. M X rup. Applications of tensor (multiway array) [22] K. M. Newton, P. L. Peissig, A. N. Kho, S. J. [23] D. Wang and S. Kong. Feature selection from [24] Z. Xu, F. Yan, Yuan, and Qi. Infinite Tucker
