 This paper is about a variant of k nearest neighbor classification on large many-class high dimensional datasets.

K nearest neighbor remains a popular classification technique, especially in areas such as computer vision, drug activity predic-tion and astrophysics. Furthermore, many more modern classifiers, such as kernel-based Bayes classifiers or the prediction phase of SVMs, require computational regimes similar to k -NN. We believe that tractable k -NN algorithms therefore continue to be important.
This paper relies on the insight that even with many classes, the task of finding the majority class among the k nearest neighbors of a query need not require us to explicitly find those neighbors . This insight was previously used in (Liu et al. , 2003) in two algorithms called KNS2 and KNS3 which dealt with fast classification in the case of two classes. In this paper we show how a different approach, IOC (standing for the International Olympic Committee) can apply to the case of n classes where n&gt; 2
IOC assumes a slightly different processing of the datapoints in the neighborhood of the query. This allows it to search a set of metric trees, one for each class. During the searches it is possible to quickly prune away classes that cannot possibly be the majority.
We give experimental results on datasets of up to magnitude acceleration compared with each of (i) conventional lin-ear scan, (ii) a well-known independent SR-tree implementation of conventional k -NN and (iii) a highly optimized conventional k-NN metric tree search.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Design, Performance k nearest neighbor, classification, high dimension, metric tree Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00.
 This paper is about new approaches to fast k -NN classification. Spatial data structures such as kd-trees [7, 17] and metric trees [21, 16, 5] have often been proposed and used for this computation, but the benefits of finding the k -NN using such data structures have often been observed to degrade substantially compared with linear search when the dimensionality of the data gets high [22]. This paper attempts to take advantage of one extra leverage point that NN classification has that the more general problem of k -NN does not have: all we need to do is to find the majority class of the nearest neighbors X  X ot the neighbors themselves.

During this paper, we show why it is hard to exploit this leverage point for the case of conventional k -NN. We therefore introduce a modified form of k -NN called IOC (standing for International Olympic Committee, explained later in the paper) that selects the predicted class by a kind of elimination tournament instead of a direct majority vote. Interestingly, this alternative voting scheme exhibits no general degradation in empirical performance, and we also prove that the asymptotic behavior of IOC must be very close to that of conventional k -NN.

IOC allows us to take advantage of the above leverage point, and we describe the new metric tree search algorithms that result. Our empirical results show excellent computational acceleration of IOC-with-metric-trees on real-world datasets compared with our own implementation (KNS1), and a well-known standard imple-mentation of classical k -NN search (SR-tree). Our empirical results also show only slight predictive performance loss with typically an order of magnitude acceleration over classical k -NN for predictions on streams of queries.
The metric tree [21, 16, 5] is a data structure that supports effi-cient nearest neighbor search. We briefly describe the data structure and its usage.

First, some notation. Assume that the data are points in a dimensional Euclidean space. Let T = { x 1 ,x 2 ,...,x n } be the set of training data. Each x i has a label y i  X  X  , where we define m = |L| to be the total number of classes. We use q to denote the query point on which the classification algorithm is to make a prediction.

For k -NN classification, one finds the k nearest neighbors of from T  X  we denote them by N = kNN ( q ,k, T )= { w 1 ,w 2 w } .Then k -NN labels q with the class that appears most fre-quently in N . As a notational convention, we call the most frequent class the  X  X inner X  and all other classes the  X  X osers. X  Thus, the NN classification amounts to finding the winner class and assigning it to the query point q .

We use B ( x,r ) to denote a ball centered at point x with radius r .Inotherwords,wehave B ( x,r )= { y : || x  X  y || X  r } .
A metric tree data structure organizes a set of points in a spatial hierarchical manner. It is a binary tree whose nodes represent a set of points. The root node represents all points, and the points represented by an internal node v are partitioned into two subsets, represented by the two children of node . More formally, if we use N ( v ) to denote the set of points represented by node v and v . rc to denote the left child and the right child of node we have for all the non-leaf nodes. At the lowest level, each leaf node con-tains very few points (typically from 1 to 5 ).

Each node v contains a pivot point, denoted by v . pvt ,anda ra-dius , denoted by v . r , such that all points represented by ball centered at v . pvt with radius v . r . Mathematically, we have
We stress that although N ( v . lc ) and N ( v . rc ) are disjoint, the balls representing these two pointsets are not necessarily so. Fur-thermore, v . r is chosen to be the minimal value satisfying (3). As a consequence, we know that the radius of any node is always strictly greater than the radius of its child nodes. The leaf nodes have very small radii.

The pivot points also serve as the criterion for partitioning the nodes: every point x in N ( v ) is assigned to one child of pivot point is closer to x . More specifically, for all non-leaf nodes v ,wehave
We do not discuss the construction of metric trees except that a metric tree can be constructed from the points efficiently, for exam-ple, using methods from [21, 5, 15].

We focus our attention to nearest neighbor (NN) search using metric trees. Intuitively, metric trees can speed up the search by us-ing the triangle inequality. Given any query point q and an arbitrary x  X  N ( v ) , we know that Therefore, by doing only one distance computation (namely, the distance between q and v . pvt ), we can bound the distance between q and any point in N ( v ) , both from above and from below. This information can help us estimate the number of points that are at most distance t from q , as well as the number of points that are at least distance t away from q . In many cases, this insight can help prune away many nodes in the k -NN search.
In this section we discuss the IOC algorithm for approximating the k -NN classification for the many-class setting. We first describe the problem with existing solutions using metric trees.
Ana  X   X ve implementation of the standard k -NN algorithm finds the exact k nearest neighbors using linear search, which needs invocations of distance computation. When the dimension d data is large, these distance computations have time complexity  X ( dN ) , which can be unrealistically expensive. Classical solu-tions such as metric trees can be used to speed up the search, but this technique does not scale well to high dimensions.

Liu et al. [14] proposed several techniques to speed up the binary classification problem. Their techniques rely on the insight that in k -NN classification, one does not need to find the actual k nearest neighbors. Rather, it is often sufficient to answer sim-pler, counting-related problems. Examples of these questions are: 1)  X  X ow many points of class i are in the k nearest neighbors of q ? X  and 2)  X  X oes class i contains at most points in the k est neighbors of q ? X  As demonstrated in [14], these questions can often be answered much more efficiently.

To illustrate this point more clearly, we introduce a new concept, namely the  X  X hreshold nearest neighbor X  function.

D EFINITION 1. The threshold nearest neighbor function, de-noted by tNN , is defined as follows. tNN ( q , T ,T,k, ):= 1 Intuitively, tNN ( q , T ,T,k, ) checks whether of the k nearest neigh-bors of q in T , the subset T contains at most points.

Roughly speaking, the evaluation of tNN ( q , T ,T,k, ) is done by finding a  X  X hreshold bound X  t , such that either 1. B ( q ,t ) contains at most points in T and at least ( k 2. B ( q ,t ) contains more than points in T and less than In the first case, we have tNN ( q , T ,T,k, )=1 ; in the second case, we have tNN ( q , T ,T,k, )=0 . In Section 3.4, we review the evaluation of tNN in more details.

Unfortunately, the insight in [14] does not work in the many-class case. Consider a query point q and its k nearest neighbor set N . For binary classification, q is classified as class i N contains at least k/ 2 points of class i (assuming that Thus the task of finding the winner is reduced to a counting prob-lem, or more specifically, evaluating the function tNN ( q , k/ 2 ) . In the case of many-classes, the situation is very different. We no longer have a fixed threshold that allows us to reduce the search-for-winner problem to a counting problem. We know that for a class to be the winner, it must necessarily contain more than k/m points in N (assuming that k is not a multiple of m ), and it is sufficient that it contains at least k/ 2 points. However, for numbers between k/m and k/ 2 , we cannot prove anything. Therefore, we cannot reduce the k -NN search problem to a simple counting problem. This is the reason why the techniques in [14] do not extend to the many-class case.
The IOC algorithm is a variant of the k -NN algorithm that al-lows speed-up using metric trees. The motivation behind IOC is to modify k -NN in such a way that it can be reduced to a sequence of counting problems. One important observation is that despite the fact that the necessary condition and the sufficient condition com-bined cannot determine if an arbitrary class is the winner in gen-eral, one can always use the necessary condition to find some class that is not a winner. This is simply because that by the pigeonhole principle, there exists at least one class containing at most points, and this class is not the winner.

This algorithm is inspired by the procedure used by the Interna-tional Olympic Committee [11] to select the host city for summer Olympic games (which also explains its name). In the procedure, instead of having a single round of ballots and selecting the favorite city as the winner (which would correspond to the  X  X tandard X  algorithm), multiple rounds of ballots are cast. In each round, if a city gets a majority of the votes, then it is declared the winner and the procedure finishes. Otherwise, the city that gets the fewest votes is eliminated and a new round of ballots is cast. This continues until only one city is left, and this city is declared the winner.
We now describe the IOC algorithm at a high level. IOC starts by building a metric tree for each class respectively, and then proceeds in rounds . In each round, either a winner is selected, or some losers are eliminated. More precisely, in each round, if a class at least k/ 2 points in the k nearest neighbors of q , which can be answered by evaluating the threshold nearest neighbor function tNN ( q , T ,T i ,k, k/ 2 ) , then this class is declared a winner and the algorithm terminates, labeling q with class i .Otherwise,the algorithm finds all the classes that contains at most k/m in the k nearest neighbors of q , and declare these classes the losers, All the  X  X oser X  classes will be removed from consideration. The number of classes, m , is reduced accordingly. This process contin-ues until a winner is selected or there is only one class remaining, in which case the only remaining class is declared a winner.
We notice that the IOC algorithm does not always behave identi-cally to the standard k -NN algorithms, and in particular, the predic-tion made by the IOC algorithm may differ from that by the stan-dard k -NN. As an example shown in Figure 1, there are 3 classes and k =9 .The 9 nearest neighbors of the query point tain 4 points of class 1 , 3 points of class 2 ,and 2 points of class 3 . Therefore, standard k -NN algorithm would select class winner. However, in the IOC algorithm, class 3 would be identi-fied as a loser and removed in the first round. In the second round, the 9 nearest neighbors of q includes two additional points of class 2 .Nowwehave 4 points of class 1 and 5 points of class 2 in this round, and IOC will choose class 2 as the winner.

Incidentally, a similar example occurred in the procedure for picking the host city for the 2000 Olympics game by IOC. The process proceeded in multiple rounds, and Beijing was the favorite city in all but the last round, but never won more than half of the votes. In the last round, Beijing lost to Sydney, and the IOC chose Sydney as the winner. If the standard k -NN algorithm had been used, Beijing would have been chosen.
Recall that the idea of IOC hinges on the ability to evaluate the threshold nearest neighbor function tNN efficiently. There-fore, we first describe an algorithm that evaluates tNN using met-ric trees. The algorithm, denoted by MTtNN thereafter, is adapted from [14].

To begin with, MTtNN builds one metric tree for T i ,thesetof training points of class i . Then, to evaluate function tNN ( q , , MTtNN needs to: 1. Find an appropriate threshold t ,and 2. Prove that either:
First, let us assume that t is known. We see how one can prove statement (2.a) or (2.b) using the metric trees. Consider a node the metric tree for class j . Suppose v represents s points, and the distance between v . pvt and q is x . By the triangle inequality, we know that if t&lt;x  X  v . r , then none of the s points in B ( q ,t ) ;if t&gt;x + v . r , then all the s points are in B cases, node v contributes information about the number of points in B ( q ,t ) and we say v is  X  X seful. X  However, if t  X  [ x  X  v . r ,x + v . r ] node v does not tell us anything, and we say node v is  X  X seless. X  Then MTtNN sums up all the information from the useful nodes and checks if this information can be used to prove (2.a) or (2.b).
In case the information is insufficient, MTtNN selects a useless node v to split , i.e., to replace node v by its two children v . rc . Since child nodes have smaller radii, they provide more  X  X e-fined X  information that might be useful. Ultimately, the leaf nodes However, splitting a node is an expensive operation, as one needs to compute the distance between q and the pivots of the children nodes, and distance computations are the dominant operations in terms of time complexity. Therefore, to achieve optimal efficiency, one needs to minimize the number of splits.

Next, if we drop the assumption that t is known, MTtNN needs to search for t as well. To do so, it maintains a list of  X  X nown X  nodes from the metric trees, i.e., the nodes where the distance be-tween q and their pivots are computed and known, and searches for an appropriate t .Ifnosuch t is found due to insufficient infor-mation, the algorithm selects a node to split according to a certain splitting policy and tries again. As demonstrated in [14], with a carefully designed policy, one can indeed minimize the number of splits and make the algorithm very efficient.
With an efficient implementation of the tNN function, we can implement the IOC algorithm directly, as in Section 3.2. How-ever, this is not very efficient, since MTtNN may need to do a lot of splits in order to find the answer. In fact, observe that in each round, many instances of the tNN functions are evaluated  X  for each class i , we need to evaluate both tNN ( q , T ,T i ,k, k/ 2 ) tNN ( q , T ,T i ,k, k/m ) . We can make progress whenever we find one winner or one loser. This observation allows us to im-prove the efficiency by dove-tailing , i.e., evaluating all the functions simultaneously, and terminates whenever a winner or a loser is found. More precisely, we modify the MTtNN algorithm so that it may also output  X  , standing for  X  X nknown. X  Then we only do a split when all evaluations return  X  .
 The algorithm, IOC, is described in Figure 2. Here is the revised version of MTtNN that partially computes tNN other words, MTtNN may return  X  when it does not have suffi-cient information, but it never splits any node. The splitting of the trees is handled by the procedure do split , which picks a particular class i and performs one split on the metric tree of T i . Effectively, the IOC algorithm minimizes the number of splits by aggressively attempting to evaluate all the tNN functions after each split.
We emphasize that the IOC algorithm is presented in a way to maximize clarity. In particular, we omit all optimizations, some of single point, in which case a leaf node has radius 0.
Procedure IOC( k , q , T , T 1 ,T 2 ,...,T m ) begin end which are obvious. For example, after splitting class j needs to update the information related to class j and there is no need to re-compute all x i and y i for all i  X  X . Additionally, many invocations of the MTtNN can be merged to improve efficiency. Furthermore, some other techniques are used in the algorithm to ensure its robustness. Due to space limitation, we do not discuss them in this extended abstract.
We analyze the behavior of the IOC algorithm from the theoret-ical perspective. Due to space limitation, the proofs are omitted. Theorem 1 IOC behaves identically to the standard k -NN algo-Theorem 2 If class i is not chosen by the IOC algorithm as the Theorem 3 Let q be the query point, m be the number of classes,
Remarks Theorem 1 establishes the fact that IOC and k -NN are identical in many cases. Theorem 2 indicates that even in the many-class case where IOC and k -NN differ, the difference isn X  X  signifi-cant: if a class is not chosen as the winner by IOC, then it will be a loser in k -NN, for a properly chosen k . Theorem 3 implies that if k is large enough, then the asymptotic behavior of IOC and k are identical with very high probability, and in particular, both are approximations of the optimal Bayes prediction.
In this section, we tested the IOC algorithm on both artificial and real-world datasets and compared the results with three other algorithms: 1. Na  X   X ve: a conventional linear-scan k -NN algorithm. 2. SR-tree: an implementation by Katayama and Satoh [12]. 3. KNS1: an optimized k -NN search based on metric trees [21]. We estimate two performance measures: Speed This is the primary concern of this paper. We consider ac-Accuracy We compare the (empirical) classification accuracy be-
We tested our algorithm on a variety of real world datasets (listed in Table 1) with multi-class classification tasks. The datasets are all publicly available.
 1. Letter (Letter Recognition Database [20]) It is from the UCI 2. Isolet (Isolet Spoken Letter Recognition Database [6]) It con-3. CovType (Forest CoverType Database) If is originally from 4. Vi deo (TREC-2001 Video Dataset [10]) It contains 5.8 hours 5. Internet ads (Internet Advertisements [13]) This dataset rep-
For each dataset, we manually partitioned them into a training set and a test set, and we ran our experiments with k =1 ,
For all algorithms, we report the pre-processing time and the er-ror rates (see Table 2), as well as the average prediction time per query (see Table 3) We also plot the speed-up of various algorithms over na  X   X ve k -NN (CPU time) for the case k =5 in Figure 3. Fur-thermore, we report how the CPU time of various algorithms scales with the size of the training data (see Figure 4 for the case and k =9 ).
 Table 2: Pre-processing time and error rates [time(s) : error]. Dataset k Na  X   X ve SR-tree KNS1 IOC
Letter 1 0 : 0.043 54 : 0.043 0.46 : 0.043 6.7 : 0.112
Isolet 1 0 : 0.11 n/a : n/a 4.43 : 0.11 68 : 0.12
CovType 1 0 : 0.14 311 : 0.14 5.1 : 0.14 101 : 0.12
Video 1 0 : 0.15 240 : 0.15 3.5 : 0.15 52 : 0.16
Internet 1 0 : 0.040 n/a : n/a 6.4 : 0.040 80 : 0.049 Error rate TheerrorrateofNa  X   X ve k -NN, SR-tree, and KNS1 are Speed-up The SR-tree algorithm typically does not show a signif-Table 3: Number of distance computations and CPU time. The Na  X   X ve column shows [number : time(s)], all other columns show speedup over Na  X   X ve.
 Dataset k Na  X   X ve SR-tree KNS1 IOC
Letter 1 16000 : 27 n/a : 1.3 14 : 6.5 47 : 16
Isolet 1 6238 : 112 n/a : n/a 1.1 : 1.1 20 : 17
CovType 1 58101 : 40776 n/a : 7.3 36 : 26 79 : 38
Video 1 35049 : 177 n/a : 3.7 28 : 20 555 : 664
Internet 1 2952 : 28 n/a : n/a 2.4 : 2.4 49 : 58 Scalability We performed the simulations for scaling over dataset
Figure 4: CPU time vs. data size (Video, k =1 and k =9 ).
For the problem of finding the k nearest datapoints (as opposed to k -NN classification) in high dimensions, the frequent failure of traditional metric trees to beat na  X   X ve has lead to some very in-genious and innovative alternatives, based on random projections, hashing discretized cubes, and acceptance of approximate answers. For example, Gionis et al. [8] gives a hashing method that was demonstrated to provide speedups over a SR-tree-based approach in 64 dimensions by a factor of 2 X 10 depending on how much er-ror in the approximate answer was permitted. Another approximate k -NN idea is in [3], one of the first k -NN approaches to use a pri-ority queue of nodes, in this case achieving a 3-fold speedup with an approximation to the true k -NN. However, these approaches are based on the notion that any points falling within a factor of times the true nearest neighbor distance are acceptable substitutes for the true nearest neighbor. Noting in particular that distances in high-dimensional spaces tend to occupy a decreasing range of continuous values [9], it remains unclear whether schemes based upon the absolute values of the distances rather than their ranks are relevant to the classification task (indeed, in the extreme of uni-form data in very high dimensions, a randomly chosen data point would be expected to lie in the (1+ ) ball 2 ). In contrast, the IOC algorithm finds an exact answer, though to a modified version of NN classification. Although both approaches have theoretical un-derpinnings, an important piece of future work will be a thorough empirical comparison (which was beyond the scope of the current paper).

Another solution to the cost of k -NN-type queries is editing (or prototypes ): most training points are forgotten and only particu-larly representative ones are used (e.g. [2, 1, 19]). Kibler and Aha extended this further by allowing datapoints to represent local con-sensuses of sets of previously observed datapoints. This can be effective, but requires in advance the choice of the right degree of  X  X moothing X  of the data (i.e. choosing the number of points to be included in the consensus). KNS1 (traditional metric tree kNN search) and IOC can both adaptively be called with varying values of k . A more serious problem with prototypes is that inevitably, very local predictions have detail lost and eventually (as the amount of data increases) the very advantage of non-parametric classifiers (their ability to adapt to local data) is lost if the number of retained datapoints remains fixed. [1] D. W. Aha. A Study of Instance-Based Algorithms for [2] D. W. Aha, D. Kibler, and M. K. Albert. Instance-Based [3] S. Arya, D. Mount, N. Netanyahu, R. Silverman, and A. Wu. [4] Jock A. Blackard. Forest covertype database. [5] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An efficient NIPS 2003 workshops. [6] Ron Cole and Mark Fanty. Isolet spoken letter recognition [7] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm [8] A. Gionis, P. Indyk, and R. Motwani. Similarity Search in [9] J. M. Hammersley. The Distribution of Distances in a [10] CMU informedia digital video library project. The trec-2001 [11] IOC. International olympic committee: Candidature [12] Norio Katayama and Shin X  X chi Satoh. The SR-tree: an index [13] Nicholas Kushmerick. Internet advertisements. [14] Ting Liu, Andrew Moore, and Alexander Gray. Efficient [15] A. W. Moore. The Anchors Hierarchy: Using the Triangle [16] S. M. Omohundro. Bumptrees for Efficient Function, [17] F. P. Preparata and M. Shamos. Computational Geometry . [18] Yanjun Qi, Alexander G. Hauptmann, and Ting Liu.
 [19] D. B. Skalak. Prototype and Feature Selection by Sampling [20] David J. Slate. Letter recognition database. [21] J. K. Uhlmann. Satisfying general proximity/similarity [22] Roger Weber, Hans-J  X  org Schek, and Stephen Blott. A
