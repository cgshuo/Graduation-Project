 Given a controversial political topic, our aim is to classify docu-ments debating the topic into pro or con. Our approach extracts topic related terms, pro/con related terms, and pairs of topic related and pro/con related terms and uses them as the basis for construct-ing a pro query and a con query. Following standard LM tech-niques, a document is classified as pro or con depending on which of the query likelihoods is higher for the document. Our experi-ments show that our approach is promising.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguistic Processing Algorithms, Experimentation Language Models, Sentiment Analysis, Text Classification
The popularity of online forums such as film and book review sites, online political fora, personal blogs, the comments section on newspaper articles, etc., allow people to post their views and opin-ions on a wide range of topics. The proliferation of such opinion oriented content has led to renewed interest in sentiment analysis and opinion mining techniques to facilitate the automatic analysis and classification of opinions. Automated analysis of opinions has a wide range of applications, including, advertising, political policy formulation, business intelligence applications, etc.

In this paper, we focus on the problem of classifying politi-cal opinions (expressed for example, in online debate forums) on controversial questions such as,  X  X hould felons be given voting rights? X  and  X  X hould the death penalty remain a legal option in America? X  into  X  X ro X  opinions and  X  X on X  opinions. Figure 1 shows an example of the kind of documents we would like to classify. Figure 1: A discussion topic with pro and con documents.
Many previous works on sentiment analysis address the problem of query independent opinion mining. For example, classifying a movie review into positive or negative. However, our setting is query dependent and the classification of a document into pro or con can change depending on the topic (see [1]). For example, in Figure 1, if the wording of the topic were changed to  X  X hould the death penalty become illegal in America? X , then the first docu-ment is actually a con document while the second document is pro. Moreover, most previous methods rely on training classifiers with annotated training data (see [2] for an overview and [3] for an ex-ample of classifying political text ), which often has to be annotated manually. In this work, we follow an alternative approach of us-ing language models (LMs) to classify opinions, thus reducing the dependence on annotated training data.
Our approach follows the query likelihood method [4], where a query is regarded as a sample of the document and its likeli-hood gives the measure of relevance of the document to the query. Clearly, in our setting, given a discussion topic and a document which debates it, the document will always be highly relevant to the topic. However, our goal is not to quantify relevance, but to classify the document based on its opinion. If a document contains expressions which are in agreement to similar expressions in the topic statement, then it is likely that the document is a pro docu-ment. Otherwise, it is likely to be a con document. For example, in the pro document in Figure 1, the expression  X  X apital punishment should not be altered X  is in agreement with the discussion topic ex-pression  X  X eath penalty remain a legal option X , while, in contrast, the con document contains the expression,  X  X gainst the continua-tion of death penalty X . Finding such expressions and using them for pro/con classification is one of the key challenges that we ad-dress in this work.

In brief, our technique is based on the following steps. First, we map both the discussion topic and documents to a set of  X  X nterest-ing patterns X . Second, we make use of the interesting patterns from
Term Synonyms Antonyms penalty punishment , retribution award , pardon legal sound , lawful illegal , unlawful , not legal remain stay , continue change , alter , not remain the discussion topic to construct two queries: a  X  X ro X  query and a  X  X on X  query. Third, for a given test document, an LM is estimated based on the interesting patterns in the document and in the back-ground corpus. And finally, both the pro query and the con query likelihoods are computed and the document is classified as pro or con based on the likelihood values. In the rest of this paper, we formalize our technique and present results of our experiments. Interesting patterns. Given a controversial discussion topic and the corpus of documents debating it, we first identify the  X  X is-cussion vocabulary X , consisting of two kinds of terms: topical terms describing the topic, and pro/con terms describing opinions on the topic. In the discussion topic, nouns are assumed to be topic-related while verbs, adjectives, and adverbs are assumed to be pro/con related. For both types of terms, we look up Word-Net for synonyms and antonyms. Table 1 shows examples of syn-onyms and antonyms for both topical as well as pro/con terms of the discussion topic in Figure 1. Note that the term itself can be negated and is indicated with a  X  X ot X  and negations of terms are detected during parsing. In Figure 1, topic X  X  topical terms and their synonyms and antonyms in the pro/con documents are italicized , while topic X  X  pro/con terms and their synonyms and antonyms are in bold . Negations are italicized and bold .

Let T s denote a topic term or its synonym and T a denote an antonym of a topic term. Similarly, PC s and PC a denote a pro/con term or its synonym and an antonym of a pro/con term. We con-struct two types of interesting patterns: (1) binary patterns (lex-ical pairs) : T s , PC s , T a , PC a , T a , PC s , T s , penalty , remain , penalty , against continuation ,etc.),and(2) unary patterns: T s , T a , PC s , PC a .Let B and U denote the set of binary and unary patterns respectively.
 Constructing queries. Similar to general ontology based query expansion, the interesting patterns described above are used to construct two queries: a pro query Q + and a con query document D is estimated as an interpolation of a binary pattern and a unary pattern LMs over all interesting patterns, thus benefiting from both LMs and overcoming the sparseness problem [4]: where P B ( pat i | D ) :LMof D over binary patterns, P U LM of D over unary patterns, pat i : a pattern and  X  : a weight parameter. A Unary pattern LM P U ( pat i | D ) is estimated as: where pat i  X  U , C : a background corpus and  X  : a smoothing parameter. P ( pat i | D ) and P ( pat i | C ) are estimated as: where c ( pat i ; D ) and c ( pat i ; C ) denote the frequency of in document D and corpus C , respectively. The binary LM P B ( pat i | D ) is estimated in an analogous manner.
 test document D , we estimate the query likelihoods of both the pro and con queries, assuming independence between patterns. That is, The document is classified as pro if P ( Q + | M D ) &gt;P ( Q  X  | M and con otherwise.
We used two datasets to evaluate our method: one from http://www.procon.org (dataset D1) and another from http://www.opposingviews.com (dataset D2). Both web-sites contain controversial political questions. Each question has a clearly marked (pro or con) set of documents debating it and thus serving as the ground truth for evaluation. We chose around 350 questions and their corresponding documents from each dataset.
We evaluated our method against two methods: a trained SVM classifier with our patterns as features, and an LM-based method which considers only unary patterns (denoted LM-term). The re-sults in Table 2 show the differences (which are statistically signifi-cant) in both precision and recall between our method and the other two methods on both datasets.
We proposed an LM-based method fo r classifying political texts into pro or con, based on a controversial discussion topic. We eval-uated our proposal and showed that it is promising. In this work, we considered topical and pro/con unigrams. A natural extension is to extend this to n-grams. Our datasets were known to contain formal political opinions expressed in non-emotive language and so, we were able to ignore many other tricky issues, such as, for example, dealing with noisy informal text, and identifying opinion-bearing sentences relevant to the topic. In the future, we plan to extend our techniques to work on other datasets (e.g. newspaper articles, blogs). [1] K. Eguchi and V. Lavrenko. Sentiment retrieval using [2] B. Pang and L. Lee. Opinion mining and sentiment analysis. [3] M. Thomas, B. Pang, and L. Lee. Get out the vote: [4] C. Zhai. Statistical language models for information retrieval.
