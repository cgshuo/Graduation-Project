 } @us.ibm.com The paper presents a simple sentence-level trans-lation pair extraction algorithm from comparable monolingual news data. It differs from similar algorithms that select translation correspondences explicitly at the document level (Fung and Che-ung, 2004; Resnik and Smith, 2003; Snover et al., 2008; Munteanu and Marcu, 2005; Quirk et al., 2007; Utiyama and Isahara, 2003). In these publications, the authors use Information-Retrieval (IR) techniques to match document pairs that are likely translations of each other. More complex sentence-level models are then used to extract par-allel sentence pairs (or fragments). From a com-putational perspective, the document-level filtering steps are needed to reduce the number of candidate sentence pairs. While IR techniques might be use-ful to improve the selection accuracy, the current pa-per demonstrates that they are not necessary to ob-tain parallel sentence pairs. For some data, e.g. the Portuguese-English Reuters data used in the experi-ments in Section 3, document-level information may not even be available.
 In this paper, sentence pairs are extracted by a sim-ple model that is based on the so-called IBM Model-1 (Brown et al., 1993). The Model-1 is trained on some parallel data available for a language pair, i.e. the data used to train the baseline systems in Section 3. The scoring function used in this pa-per is inspired by phrase-based SMT. Typically, a phrase-based SMT system includes a feature that scores phrase pairs using lexical weights (Koehn et al., 2003) which are computed for two directions: source to target and target to source. Here, a sen-tence pair is scored as a phrase pair that covers all the source and target words. The scoring function  X  (
S, T ) is defined as follows: Here, S = s J T = t I 1 is the target sentence of length I . p ( s | T is the Model-1 probability assigned to the source word s given the target sentence T , p ( t | S ) is defined accordingly. p ( s | t ) and p ( t | s ) are word translation probabilities obtained by two parallel Model-1 train-ing steps on the same data, but swapping the role of source and target language. They are smoothed to avoid 0 . 0 entries; there is no special NULL-word model and stop words are kept. The log(  X  ) is applied to turn the sentence-level probabilities into scores. These log-probabilities are normalized with respect to the source and target sentence length: this way the score  X  ( S, T ) can be used across all sentence pairs considered, and a single manually set thresh-old  X  is used to select all those sentence pairs whose score is above it. For computational reasons, the sum  X  ( S, T ) is computed over the following terms:  X  ( t i , S ) where 1  X  i  X  I and  X  ( s j , T ) , where 1  X  j  X  J . The  X   X  X  and  X   X  X  represent partial score contributions for a given source or target position. Note that  X  ( S, T )  X  0 since the terms  X  (  X  , S )  X  0 and  X  (  X  , T )  X  0 .

Section 2 presents an efficient implementation of the scoring function in Eq. 1. Its effectiveness is demonstrated in Section 3. Finally, Section 4 dis-cusses future work and extensions of the current al-gorithm. We process the comparable data at the sentence-level: for each language and all the documents in the comparable data, we distribute sentences over a list of files : one file for each news feed f (for the Spanish Gigaword data, there are 3 news feeds) and publication date d . The Gigaword data comes anno-tated with sentence-level boundaries, and all docu-ment boundaries are discarded. This way, the Span-ish data consists of about 24 thousand files and the English data consists of about 53 thousand files (for details, see Table 2). For a given source sentence S , the search algorithm computes the highest scoring sentence pair  X  ( S, T ) over a set of candidate trans-lations T  X   X  , where |  X  | can be in the hundreds of thousands of sentences .  X  consists of all target sentences that have been published from the same news feed f within a 7 day window from the pub-lication date of the current source sentence S . The extraction algorithm is guaranteed to find the highest scoring sentence pairs ( S, T ) among all T  X   X  . In order to make this processing pipeline feasible, the scoring function in Eq. 1 needs to be computed very efficiently. That efficiency is based on the decompo-sition of the scoring functions into I + J terms (  X   X  X  and  X   X  X ) where source and target terms are treated differently. While the scoring function computation is symmetric, the processing is organized according the source language files: all the source sentences are processed one-by-one with respect to their indi-vidual candidate sets  X  :  X  Caching for target term  X  ( t, S ) : For each tar- X  Array access for source terms  X  ( s, T ) : For a  X  Early-Stopping: Two loops compute the scor- X  Frequency-Sorting: Here, we aim at making  X  Sentence-level filter: The word-overlap filter The parallel sentence extraction algorithm presented scale Spanish-English Gigaword data (Graff, 2006; Graff, 2007). The Spanish data comes from 3 news feeds: Agence France-Presse (AFP), Associ-ated Press Worldstream (APW), and Xinhua News Date-Feed Files 24 , 005 53 , 373 Sentences 19 . 4 million 47 . 9 million Words 601 . 5 million 1 . 36 billion Date-Feed Files 351 355 Sentences 366 . 0 thousand 5 . 3 million Words 11 . 6 million 171 . 1 million Agency (XIN). We do not use the additional news feed present in the English data. Table 1 demon-strates the effectiveness of the implementation tech-niques in Section 2. Here, the average extraction time per source sentence is reported for one of the 24 , 000 source language files. This file contains 913 sentences. Here, the size of the target candidate set  X  is 61 736 sentences. All the techniques presented result in some improvement. The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). Caching the target word proba-bilities results in the biggest reduction. The results are representative: finding the highest scoring target sentence T for a given source sentence S takes about 1 second on average. Since 20 million source sen-tences are processed, and the workload is distributed over roughly 120 processors, overall processing time sums to less than 3 days. Here, the total number of translation pairs considered is close to 1 trillion.
The effect of including additional sentence pairs along with selection statistics is presented in Ta-ble 3. Translation results are presented for a standard phrase-based SMT system. Here, both languages use a test set with a single reference. Including about 1 . 4 million sentence pairs extracted from the Giga-word data, we obtain a statistically significant im-provement from 42 . 3 to 45 . 6 in BLEU (Papineni et al., 2002). The baseline system has been trained on about 1 . 8 million sentence pairs from Europarl and FBIS parallel data. We also present results for a Portuguese-English system: the baseline has been trained on Europarl and JRC data. Parallel sentence pairs are extracted from comparable Reuters news data published in 2006 . The corpus statistics for Data Source # candidates #train pairs Bleu Spanish-English:  X  =  X  4 . 1
Baseline -1 , 825 , 709 42 . 3 + Gigaword 955 . 5  X  10 9 1 , 372 , 124 45 . 6 Portuguese-English:  X  =  X  5 . 0
Baseline -2 , 221 , 891 45 . 3 + Reuters 06 32 . 8  X  10 9 48 , 500 48 . 5 the Portuguese-English data are given in Table 2. The selection threshold  X  is determined with the help of bilingual annotators (it typically takes a few hours). Sentence pairs are selected with a conserva-sorted by descending score. The annotator descends this list to determine a score threshold cut-off. Here, translation pairs are considered to be parallel if 75 % of source and target words have a corresponding translation in the other sentence. Using a threshold  X  =  X  4 . 1 for the Spanish-English data, results in a selection precision of around 80 % (most of the mis-qualified pairs are partial translations with less than 75 % coverage or short sequences of high frequency words). This simple selection criterion proved suf-ficient to obtain the results presented in this paper. As can be seen from Table 3, the optimal threshold is language specific. In this paper, we have presented a novel sentence-level pair extraction algorithm for comparable data. We use a simple symmetrized scoring function based on the Model-1 translation probability. With the help of an efficient implementation, it avoids any translation candidate selection at the docu-ment level (Resnik and Smith, 2003; Smith, 2002; Snover et al., 2008; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Fung and Cheung, 2004). In particular, the extraction algorithm works when no document-level information is available. Its usefulness for extracting parallel sentences is demonstrated on news data for two language pairs. Currently, we are working on a feature-rich ap-proach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. Feature func-tions will be  X  X ight-weight X  such that they can be computed efficiently in an incremental way at the sentence-level. This way, we will be able to main-tain our search-driven extraction approach. We are also re-implementing IR-based techniques to pre-select translation pairs at the document-level, to gauge the effect of this additional filtering step. We hope that a purely sentence-level processing might result in a more productive pair extraction in future.
