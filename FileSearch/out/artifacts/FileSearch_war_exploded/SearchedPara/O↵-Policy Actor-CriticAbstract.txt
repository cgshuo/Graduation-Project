 Thomas Degris thomas.degris@inria.fr Flowers Team, INRIA, Talence, ENSTA-ParisTech, Paris, France Martha White whitem@cs.ualberta.ca Richard S. Sutton sutton@cs.ualberta.ca The reinforcement learning framework is a general temporal learning formalism that has, over the last few decades, seen a marked growth in algorithms and applications. Until recently, however, practical online methods with convergence guarantees have been re-stricted to the on-policy setting, in which the agent learns only about the policy it is executing. In an o  X  -policy setting, on the other hand, an agent learns about a policy or policies di  X  erent from the one it is executing. O  X  -policy methods have a wider range of applications and learning possibilities. Unlike on-policy methods, o  X  -policy methods are able to, for ex-ample, learn about an optimal policy while executing an exploratory policy (Sutton &amp; Barto, 1998), learn from demonstration (Smart &amp; Kaelbling, 2002), and learn multiple tasks in parallel from a single sensori-motor interaction with an environment (Sutton et al., 2011). Because of this generality, o  X  -policy methods are of great interest in many application domains. The most well known o  X  -policy method is Q-learning (Watkins &amp; Dayan, 1992). However, while Q-Learning is guaranteed to converge to the optimal policy for the tabular (non-approximate) case, it may diverge when using linear function approximation (Baird, 1995). Least-squares methods such as LSTD (Bradtke &amp; Barto, 1996) and LSPI (Lagoudakis &amp; Parr, 2003) can be used o  X  -policy and are sound with linear function approximation, but are computationally expensive; their complexity scales quadratically with the num-ber of features and weights. Recently, these problems have been addressed by the new family of gradient-TD (Temporal Di  X  erence) methods (e.g., Sutton et al., 2009), such as Greedy-GQ (Maei et al., 2010), which are of linear complexity and convergent under o  X  -policy training with function approximation. All action-value methods, including gradient-TD methods such as Greedy-GQ, su  X  er from three impor-tant limitations. First, their target policies are deter-ministic, whereas many problems have stochastic op-timal policies, such as in adversarial settings or in par-tially observable Markov decision processes. Second, finding the greedy action with respect to the action-value function becomes problematic for larger action spaces. Finally, a small change in the action-value function can cause large changes in the policy, which creates di culties for convergence proofs and for some real-time applications.
 The standard way of avoiding the limitations of action-value methods is to use policy-gradient algorithms (Sutton et al., 2000) such as actor-critic methods (e.g., Bhatnagar et al., 2009). For example, the nat-ural actor-critic, an on-policy policy-gradient algo-rithm, has been successful for learning in continuous action spaces in several robotics applications (Peters &amp; Schaal, 2008).
 The first and main contribution of this paper is to introduce the first actor-critic method that can be ap-plied o  X  -policy, which we call O  X  -PAC, for O  X  -Policy Actor X  X ritic. O  X  -PAC has two learners: the actor and the critic. The actor updates the policy weights. The critic learns an o  X  -policy estimate of the value func-tion for the current actor policy, di  X  erent from the (fixed) behavior policy. This estimate is then used by the actor to update the policy. For the critic, in this paper we consider a version of O  X  -PAC that uses GTD( ) (Maei, 2011), a gradient-TD method with el-igibitity traces for learning state-value functions. We define a new objective for our policy weights and derive a valid backward-view update using eligibility traces. The time and space complexity of O  X  -PAC is linear in the number of learned weights.
 The second contribution of this paper is an o  X  -policy policy-gradient theorem and a convergence proof for O  X  -PAC when = 0, under assumptions similar to previous o  X  -policy gradient-TD proofs.
 Our third contribution is an empirical comparison of Q( ), Greedy-GQ, O  X  -PAC, and a soft-max version of Greedy-GQ that we call Softmax-GQ, on three bench-mark problems in an o  X  -policy setting. To the best of our knowledge, this paper is the first to provide an empirical evaluation of gradient-TD methods for o  X  -policy control (the closest known prior work is the work of Delp (2011)). We show that O  X  -PAC outper-forms other algorithms on these problems. In this paper, we consider Markov decision processes with a discrete state space S , a discrete action space A , adistribution P : S  X  S  X  A ! [0 , 1], where P ( s 0 | s, a ) is the probability of transitioning into state s 0 from state s after taking action a , and an expected reward function R : S  X  A  X  S ! R that provides an expected reward for taking action a in state s and transitioning into s 0 . We observe a stream of data, which includes states s t 2 S , actions a t 2 A , and rewards r t 2 R for t =1 , 2 ,... with actions selected from a fixed behavior policy, b ( a | s ) 2 (0 , 1].
 Given a termination condition : S ! [0 , 1] (Sutton et al., 2011), we define the value function for  X  : S  X  A ! (0 , 1] to be: where policy  X  is followed from time step t and ter-minates at time t + T according to . We assume termination always occurs in a finite number of steps. The action-value function, Q  X  , ( s, a ), is defined as: Q  X  , ( s, a )= for all a 2 A and for all s 2 S . Note that V  X  , ( s )= P a 2 A  X  ( a | s ) Q The policy  X  u : A  X  S ! [0 , 1] is an arbitrary, di  X  eren-tiable function of a weight vector, u 2 R N u , N u 2 N , with  X  u ( a | s ) &gt; 0 for all s 2 S , a 2 A . Our aim is to choose u so as to maximize the following scalar objec-tive function: where d b ( s )=lim t !1 P ( s t = s | s 0 ,b )isthelimiting distribution of states under b and P ( s t = s | s 0 ,b )is the probability that s t = s when starting in s 0 and executing b . The objective function is weighted by d b because, in the o  X  -policy setting, data is obtained according to this behavior distribution. For simplicity of notation, we will write  X  and implicitly mean  X  u . In this section, we present the O  X  -PAC algorithm in three steps. First, we explain the basic theoretical ideas underlying the gradient-TD methods used in the critic. Second, we present our o  X  -policy version of the policy-gradient theorem. Finally, we derive the for-ward view of the actor and convert it to a backward view to produce a complete mechanistic algorithm us-ing eligibility traces. 2.1. The Critic: Policy Evaluation Evaluating a policy  X  consists of learning its value function, V  X  , ( s ), as defined in Equation 1. Since it is often impractical to explicitly represent every state s , we learn a linear approximation of V  X  , ( s ):  X  V ( s )= v T x s where x s 2 R N v , N v 2 N , is the feature vector.
 Gradient-TD methods (Sutton et al., 2009) incremen-tally learn the weights, v , in an o  X  -policy setting, with a guarantee of stability and a linear per-time-step complexity. These methods minimize the -weighted mean-squared projected Bellman error: where  X  V = X v ; X is the matrix whose rows are all x s ; is the decay of the eligibility trace; D is a matrix with d ( s ) on its diagonal;  X  is a projection operator that projects a value function to the nearest representable value function given the function approximator; and T  X  is the -weighted Bellman operator for the target policy  X  with termination probability (e.g., see Maei &amp; Sutton, 2010). For a linear representation,  X  = X ( X T DX ) 1 X T D .
 In this paper, we consider the version of O  X  -PAC that updates its critic weights by the GTD( ) algorithm introduced by Maei (2011). 2.2. O  X  -policy Policy-gradient Theorem Like other policy gradient algorithms, O  X  -PAC up-dates the weights approximately in proportion to the gradient of the objective: where  X  u,t 2 R is a positive step-size parameter. Start-ing from Equation 3, the gradient can be written: r u J ( u )= r u The final term in this equation, r u Q  X  , ( s, a ), is dif-ficult to estimate in an incremental o  X  -policy setting. The first approximation involved in the theory of O  X  -PAC is to omit this term. That is, we work with an approximation to the gradient, which we denote r u J ( u )  X  g ( u )= The two theorems below provide justification for this approximation.
 Theorem 1 (Policy Improvement) . Given any policy parameter u , let Then there exists an  X  &gt; 0 such that, for all positive  X  &lt;  X  , Further, if  X  has a tabular representation (i.e., sepa-rate weights for each state), then V  X  u 0 , ( s ) V  X  u , for all s 2 S . (Proof in Appendix).
 In the conventional on-policy theory of policy-gradient methods, the policy-gradient theorem (Marbach &amp; Tsitsiklis, 1998; Sutton et al., 2000) establishes the re-lationship between the gradient of the objective func-tion and the expected action values. In our notation, that theorem essentially says that our approximation is exact, that g ( u )= r u J ( u ). Although, we can not show this in the o  X  -policy case, we can establish a re-lationship between the solutions found using the true and approximate gradient: Theorem 2 (O  X  -Policy Policy-Gradient Theorem) . where Z is the true set of local maxima and  X  Z the set of local maxima obtained from using the approximate gradient, g ( u ) . If the value function can be represented by our function class, then Z  X   X  Z . Moreover, if we use a tabular representation for  X  , then Z =  X  Z . (Proof in Appendix).
 The proof of Theorem 2, showing that Z =  X  Z ,requires tabular  X  to avoid update overlap: updates to a single parameter influence the action probabilities for only one state. Consequently, both parts of the gradient (one part with the gradient of the policy function and the other with the gradient of the action-value func-tion) locally greedily change the action probabilities for only that one state. Extrapolating from this re-sult, in practice, more generally a local representation for  X  will likely su ce, where parameter updates influ-ence only a small number of states. Similarly, in the non-tabular case, the claim will likely hold if is small (the return is myopic), again because changes to the policy mostly a  X  ect the action-value function locally. Fortunately, from an optimization perspective, for all u 2  X  Z\Z , J ( u ) &lt; min u 0 2 Z J ( u 0 ), in other words, Z represents all the largest local maxima in  X  Z with respect to the objective, J . Local optimization tech-niques, like random restarts, should help ensure that we converge to larger maxima and so to u 2 Z .Even with the true gradient, these approaches would be in-corporated into learning because our objective, J ,is non-convex. 2.3. The Actor: Incremental Update We now derive an incremental update algorithm using observations sampled from the behavior policy. First, we rewrite Equation 5 as an expectation: g ( u )=E troduce the new notation E b [  X  ] to denote the expecta-tion implicitly conditional on all the random variables (indexed by time step) being drawn from their limiting stationary distribution under the behavior policy. A standard result (e.g., see Sutton et al., 2000) is that an arbitrary function of state can be introduced into these equations as a baseline without changing the expected value. We use the approximate state-value function provided by the critic,  X  V ,inthisway: g ( u )=E b The next step is to replace the action value, Q  X  , ( s t ,a t ), by the o  X  -policy -return. Because these are not exactly equal, this step introduces a further approximation: g ( u )  X  [ g ( u )=E b where the o  X  -policy -return is defined by: Finally, based on this equation, we can write the for-ward view of O  X  -PAC: The forward view is useful for understanding and an-alyzing algorithms, but for a mechanistic implemen-tation it must be converted to a backward view that Algorithm 1 The O  X  -PAC algorithm Initialize the vectors e v , e u , and w to zero Initialize the vectors v and u arbitrarily Initialize the state s For each step: does not involve the -return. The key step, proved in the appendix, is the observation that ventional temporal di  X  erence error, and e t 2 R N u is the eligibility trace of , updated by: Finally, combining the three previous equations, the backward view of the actor update can be written sim-ply as: The complete O  X  -PAC algorithm is given above as Al-gorithm 1. Note that although the algorithm is written in terms of states s and s 0 , it really only ever needs access to the corresponding feature vectors, x s and x s 0 , and to the behavior policy probabilities, b (  X | s ), for the current state. All of these are typically available in large-scale applications with function approxima-tion. Also note that O  X  -PAC is fully incremental and has per-time step computation and memory complex-ity that is linear in the number of weights, N u + N v . With discrete actions, a common policy distribution is the Gibbs distribution, which uses a linear combi-nation of features  X  ( a | s )= e u T s,a P state-action features for state s , action a , and where action features, s,a , are potentially unrelated to the feature vectors x s used in the critic.
 Our algorithm has the same recursive stochastic form as the o  X  -policy value-function algorithms where h : R N ! R N is a di  X  erentiable function and { M t } t 0 is a noise sequence. Following previous o  X  -policy gradient proofs (Maei, 2011), we study the be-havior of the ordinary di  X  erential equation The two updates (for the actor and for the critic) are not independent on each time step; we analyze two separate ODEs using a two timescale analysis (Borkar, 2008). The actor update is analyzed given fixed critic parameters, and vice versa, iteratively (until conver-gence). We make the following assumptions. (A1) The policy viewed as a function of u ,  X  (  X  ) ( a | s ): (A2) The update on u t includes a projection operator, (A3) The behavior policy has a minimum positive value (A4) The sequence ( x t , x t +1 ,r t +1 ) t 0 is i.i.d. and has (A5) For every u 2 U (the compact region to which u Remark 1: It is di cult to prove the boundedness of the iterates without the projection operator. Since we have a bounded function (with range (0 , 1]), we could instead assume that the gradient goes to zero expo-nentially as u !1 , ensuring boundedness. Previous work, however, has illustrated that the stochasticity in practice makes convergence to an unstable equilibrium unlikely (Pemantle, 1990); therefore, we avoid restric-tions on the policy function and do not include the projection in our algorithm Finally, we have the following (standard) assumptions on features and step-sizes. (P1) || x t || 1 &lt; 1 , 8 t ,where x t 2 R N v (P2) Matrices C = E [ x t x t T ], A = E [ x t ( x t x t +1 (S2) Define H ( A ) . =( A + A T ) / 2 and let Remark 2: The assumption  X  u,t /  X  v,t ! 0 in (S1) states that the actor step-sizes go to zero at a faster rate than the value function step-sizes: the actor up-date moves on a slower timescale than the critic up-date (which changes more from its larger step sizes). This timescale is desirable because we e  X  ectively want a converged value function estimate for the current policy weights, u t . Examples of suitable step sizes are  X  (with  X  w,t =  X  X  X  v,t for  X  satisfying (S2)).
 The above assumptions are actually quite unrestric-tive. Most algorithms inherently assume bounded fea-tures with bounded value functions for all policies; unbounded values trivially result in unbounded value function weights. Common policy distributions are smooth, making  X  ( a | s ) continuously di  X  erentiable in u . The least practical assumption is that the tuples ( x t , x t +1 ,r t +1 ) are i.i.d., in other words, Martingale noise instead of Markov noise. For Markov noise, our proof as well as the proofs for GTD( ) and GQ( ), require Borkar X  X  (2008) two-timescale theory to be ex-tended to Markov noise (which is outside the scope of this paper). Finally, the proof for Theorem 3 assumes = 0, but should extend to &gt; 0 similarly to GTD( ) (see Maei, 2011, Section 7.4, for convergence remarks). We give a proof sketch of the following convergence theorem, with the full proof in the appendix. Theorem 3 (Convergence of O  X  -PAC) . Let =0 and consider the O  X  -PAC iterations with GTD(0) 2 for the critic. Assume that (A1)-(A5), (P1)-(P2) and (S1)-(S2) hold. Then the policy weights, u t , converge to  X  Z = { u 2 U| d g ( u )=0 } and the value function weights, v t , converge to the corresponding TD-solution with probability one.
 Proof Sketch: We follow a similar outline to the two timescale analysis for on-policy policy gradient actor-critic (Bhatnagar et al., 2009) and for nonlinear GTD (Maei et al., 2009). We analyze the dynamics for our two weights, u t and z t T =( w t T v t T ), based on our update rules. The proof involves satisfying seven requirements from Borkar (2008, p. 64) to ensure con-vergence to an asymptotically stable equilibrium. This section compares the performance of O  X  -PAC to three other o  X  -policy algorithms with linear memory and computational complexity: 1) Q( ) (called Q-Learning when = 0), 2) Greedy-GQ (GQ( )with a greedy target policy), and 3) Softmax-GQ (GQ( ) with a Softmax target policy). The policy in O  X  -PAC is a Gibbs distribution as defined in section 2.3. We used three benchmarks: mountain car, a pendulum problem and a continuous grid world. These prob-lems all have a discrete action space and a continu-ous state space, for which we use function approxima-tion. The behavior policy is a uniform distribution over all the possible actions in the problem for each time step. Note that Q( ) may not be stable in this setting (Baird, 1995), unlike all the other algorithms. The goal of the mountain car problem (see Sutton &amp; Barto, 1998) is to drive an underpowered car to the top of a hill. The state of the system is composed of the current position of the car (in [ 1 . 2 , 0 . 6]) and its velocity (in [ . 07 ,. 07]). The car was initialized with a position of -0.5 and a velocity of 0. Actions are a throttle of { 1, 0, 1 } . The reward at each time step is 1. An episode ends when the car reaches the top of the hill on the right or after 5,000 time steps. The second problem is a pendulum problem (Doya, 2000). The state of the system consists of the angle (in radians) and the angular velocity (in [ 78 . 54 , 78 . 54]) of the pendulum. Actions, the torque applied to the base, are { 2, 0, 2 } . The reward is the cosine of the angle of the pendulum with respect to its fixed base. The pendulum is initialized with an angle and an angu-lar velocity of 0 (i.e., stopped in a horizontal position). An episode ends after 5,000 time steps.
 For the pendulum problem, it is unlikely that the be-havior policy will explore the optimal region where the pendulum is maintained in a vertical position. Conse-quently, this experiment illustrates which algorithms make best use of limited behavior samples.
 The last problem is a continuous grid-world. The state is a 2-dimensional position in [0 , 1] 2 . The ac-(0 . 0 , . 05), (0 . 0 ,. 05) } , representing moves in both di-mensions. Uniform noise in [ . 025 ,. 025] is added to each action component. The reward at each time step for arriving in a position ( p x ,p y )isde-fined as: 1+ 2( N ( p x ,. 3 ,. 1)  X N ( p y ,. 6 ,. 03) + episode ends when the goal is reached, that is when the distance from the current position to the goal is less than 0.1 (using the L1-norm), or after 5,000 time steps. Figure 1 shows a representation of the problem. The feature vectors x s were binary vectors constructed according to the standard tile-coding technique (Sut-ton &amp; Barto, 1998). For all problems, we used ten tilings, each of roughly 10  X  10 over the joint space of the two state variables, then hashed to a vector of dimension 10 6 . An addition feature was added that was always 1. State-action features, s,a , were also 10 6 + 1 dimensional vectors constructed by also hash-ing the actions. We used a constant =0 . 99. All the weight vectors were initialized to 0. We performed a parameter sweep to select the following parameters: 1) the step size  X  v for Q( ), 2) the step-sizes  X  v and  X  w for the two vectors in Greedy-GQ, 3)  X  v ,  X  w and the temperature  X  of the target policy distribution for Softmax-GQ and 4) the step sizes  X  v ,  X  w and  X  u for O  X  -PAC. For the step sizes, the sweep was done over the following values: { 10 4 , 5  X  10 4 , 10 3 ,...,. 5 , 1 . } divided by 10+1=11, that is the number of tilings plus 1. To compare TD methods to gradient-TD meth-ods, we also used  X  w = 0. The temperature parame-and from { 0 ,. 2 ,. 4 ,. 6 ,. 8 ,. 99 } . We ran thirty runs ,  X   X  v Reward  X  w  X  u ,  X   X  v Reward with each setting of the parameters.
 For each parameter combination, the learning algo-rithm updates a target policy online from the data generated by the behavior policy. For all the prob-lems, the target policy was evaluated at 20 points in time during the run by running it 5 times on another instance of the problem. The target policy was not up-dated during evaluation, ensuring that it was learned only with data from the behavior policy.
 Figure 2 shows results on three problems. Softmax-GQ and O  X  -PAC improved their policy compared to the behavior policy on all problems, while the improve-ments for Q( ) and Greedy-GQ is limited on the con-tinuous grid world. O  X  -PAC performed best on all problems. On the continuous grid world, O  X  -PAC was the only algorithm able to learn a policy that reliably found the goal after 5,000 episodes (see Figure 1). On all problems, O  X  -PAC had the lowest standard error. O  X  -PAC, like other two-timescale update algorithms, can be sensitive to parameter choices, particularly the step-sizes. O  X  -PAC has four parameters: and the three step sizes,  X  v and  X  w for the critic and  X  u for the actor. In practice, the following procedure can be used to set these parameters. The value of , as with other algorithms, will depend on the problem and it is often better to start with low values (less than .4). A common heuristic is to set  X  v to 0 . 1divided by the norm of the feature vector, x s ,whilekeeping the value of  X  w low. Once GTD( ) is stable learning the value function with  X  u = 0,  X  u can be increased so that the policy of the actor can be improved. This corroborates the requirements in the proof, where the step-sizes should be chosen so that the slow update (the actor) is not changing as quickly as the fast inner update to the value function weights (the critic). As mentioned by Borkar (2008, p. 75), another scheme that works well in practice is to use the restrictions on the step-sizes in the proof and to also subsample updates for the slow update. Subsampling updates means only updating every { tN, t 0 } , for some N&gt; 1: the actor is fixed in-between tN and ( t + 1) N while the critic is being updated. This further slows the actor update and enables an improved value function estimate for the current policy,  X  .
 In this work, we did not explore incremental natural actor-critic methods (Bhatnagar et al., 2009), which use the natural gradient as opposed to the conventional gradient. The extension to o  X  -policy natural actor-critic should be straightforward, involving only a small modification to the update and analysis of this new dynamical system (which will have similar properties to the original update).
 Finally, as pointed out by Precup et al. (2006), o  X  -policy updates can be more noisy compared to on-policy learning. The results in this paper suggest that O  X  -PAC is more robust to such noise because it has lower variance than the action-value based methods. Consequently, we think O  X  -PAC is a promising direc-tion for extending o  X  -policy learning to a more general setting such as continuous action spaces. This paper proposed a new algorithm for learning control o  X  -policy, called O  X  -PAC (O  X  -Policy Actor-Critic). We proved that O  X  -PAC converges in a stan-dard o  X  -policy setting. We provided one of the first empirical evaluations of o  X  -policy control with the new gradient-TD methods and showed that O  X  -PAC has the best final performance on three benchmark prob-lems and consistently has the lowest standard error. Overall, O  X  -PAC is a significant step toward robust o  X  -policy control. This work was supported by MPrime, the Alberta In-novates Centre for Machine Learning, the Glenrose Re-habilitation Hospital Foundation, Alberta Innovates X  Technology Futures, NSERC and the ANR MACSi project. Computational time was provided by West-grid and the M  X esocentre de Calcul Intensif Aquitain. Appendix: See http://arXiv.org/abs/1205.4839 References
