 The Markov Random Walk model has been recently exploited for multi-document summarization by making use of the link relationships between sentences in the document set, under the assumption that all the sentences are indistinguishable from each other. However, a given document set usually covers a few topic themes with each theme represented by a cluster of sentences. The topic themes are usually not equa lly important a nd the sentences in an important theme cluster are deemed more salient than the sentences in a trivial theme cl uster. This paper proposes the Cluster-based Conditional Markov Random Walk Model (ClusterCMRW) and the Cluster-ba sed HITS Model (ClusterHITS) to fully leverage the cluster-le vel information. Experimental results on the DUC2001 and DUC2002 datasets demonstrate the good effectiveness of our proposed summarization models. The results also demonstrate that the ClusterCMRW model is more robust than the ClusterHITS model, with respect to different cluster numbers. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  abstracting methods ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  text analysis General Terms : Algorithms, Experimentation, Performance Keywords: Multi-Document Summarization, Cluster-based Link Analysis, Conditional Mark ov Random Walk Model, HITS Multi-document summarization aims to produce a summary delivering the majority of information content from a set of Multi-document summary can be used to concisely describe the information contained in a cluster of documents and facilitate the users to understand the document cl uster. For example, a number of news services (e.g. Google News) have been developed to group news articles into news topics, and then produce a short summary for each news topic. The users can easily understand the topic they have interest in by taking a look at the short summary. Automated multi-document su mmarization has drawn much attention in recent years. In the communities of natural language processing and information retrieva l, a series of workshops and conferences on automatic text summarization (e.g. NTCIR, DUC), special topic sessions in ACL, COLING, and SIGIR have advanced the summarization techni ques and produced a couple of experimental online systems. A particular challenge for multi-document summarization is that a document set might contain diverse information, which is either related or unrelated to the main topic, and hence we need effective summarization methods to analyz e the information stored in different documents and extract th e globally important information to reflect the main topic. Another challenge for multi-document summarization is that the information stored in different documents inevitably overlaps with each other, and hence we need effective summarization methods to merge information stored in different documents, and if possibl e, contrast their differences. In recent years, both unsupervised and supervised methods have been proposed to analyze the informati on contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. Most recently, the Markov Random Walk Model (abbr. MRW) has been successfully used for multi-document summarization by making use of the  X  X oting X  or  X  X ecommendations X  between sentences in the documents [4, 21 , 25]. The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen in to the summary. However, the model makes uniform use of the sent ences in the document set, i.e. all the sentences are ranked without considering the higher-level information beyond the sentence-level information. Actually, given a document set, there usua lly exist a number of themes or subtopics, and each theme or subt opic is represented by a cluster of highly related sentences [6, 7]. The theme clusters are usually understand the document set. F or example, the theme clusters close to the main topic of th e document set are usually more important than the theme clusters far away from the main topic of the document set. The cluster-level information is deemed to have great influence on the sentence ranking process. Moreover, the sentences in the same theme cluster cannot be treated uniformly. Some sentences in the cluster are more important than other sentences because of their different distances to the cluster X  X  centroid. In brief, neither the cluster-level information nor the sentence-to-cluster relationship ca n be taken into account in the Markov Random Walk Model. In order to address the above l imitations of the Markov Random Walk Model, we propose two models to incorporate the cluster-level information into the process of sentence ranking. The first model is the Cluster-b ased Conditional Markov Random Walk Model (abbr. ClusterCMRW), which incorporates the cluster-level information into the link graph. The second model is the Cluster-based HITS Mode l (abbr. ClusterHITS), which considers the clusters and sentences as hubs and authorities in the HITS algorithm. Experiments ha ve been performed on the DUC2001 and DUC2002 datasets, and the results demonstrate the good effectiveness of the two mode ls. The experimental results also demonstrate that the Clus terCMRW model is more robust than the ClusterHITS model, with respect to different cluster numbers. The rest of this paper is organized as follows: Section 2 introduces the related work. The basic Markov Random Walk Model is introduced in Section 3. And the two proposed models are presented in Sections 4. In Section 5, we describe the experiments and results. Lastly we conclude this paper in Section 6. A variety of multi-document summarization methods have been developed recently. Generally sp eaking, those methods can be either extractive summarization or abstractive summarization. Extractive summarization involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and summarization (e.g. NewsBlaster) usually needs information fusion [2], sentence compression [10] and reformulation [20]. In this study, we focus on extractive summarization. The centroid-based method [23] is one of the most popular extractive summarization methods. MEAD 1 is an implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS [15] uses sentence position, term frequency, topic si gnature and term clustering to select important content, and us e MMR [5] to remove redundancy. To further explore user interface issues, iNeATS [14] is developed based on NeATS. XDoX [7] is a cross document summarizer designed specifically to summarize large document sets by identifying the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. The passages are clustered based on n-gram matching. Much other work also explores to find topic themes in the documents for summarization, e.g. Harabagiu and Lacatusu [6] investigate five di fferent topic repr esentations and introduce a novel representation of topics based on topic themes. In addition, Marcu [19] selects important sentences based on the discourse structure of the text . TNO X  X  system [11] scores sentences by combining a unigram language model approach with a Bayesian classifier based on surface features. Most recently, the graph-base d ranking methods have been proposed to rank sentences or passages based on the  X  X otes X  or  X  X ecommendations X  between each other. Websumm [18] uses a graph-connectivity model and oper ates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageR ank [4] is an approach for computing sentence importanc e based on the concept of eigenvector centrality. It constructs a sentence connectivity matrix and computes sentence importan ce based on an algorithm similar to PageRank. Mihalcea and Tara u [21] also propose a similar algorithm based on PageRank [22] to compute sentence importance for single documen t summarization, and for multi-document summarization, they use a meta-summarization process to summarize the meta-document produced by assembling http://www.summarization.com/mead/ all the single summary of each document. Wan and Yang [25] improve the graph-ranking al gorithm by differentiating intra-document links and inter-doc ument links between sentences. All these methods make use of th e relationships between sentences  X  X ecommendations X  from their neighboring sentences. summarization [3], which aims to produce summary biased to a given topic or query. PageRank [22] and HITS [9] are two popular algorithms for link analysis between web pages and they have been successfully used to improve web retrieval. More advanced web link analysis methods have been proposed to leverage the multi-layer relationships between web pages. The Conditional Markov Random Walk Model has been successfully applied in the tasks of web page retrieval based on two-layer web graph [17]. Hierarchical structure of the web graph is also exploited for link analysis in [26]. In recent years, a few researches have focused on using link analysis methods to re -rank search results in order to improve the retrieval performance [12, 13, 27]. The links between documents are induced by com puting the similarity between documents using the Cosine meas ure or language model measure. In addition, link analysis methods ha ve also been applied in social network analysis [28] and other tasks. The Markov Random Walk Model (M RW) is essentially a way of deciding the importance of a vertex within a graph based on global information recursively drawn from the entire graph. The basic idea is that of  X  X oting X  or  X  X  ecommendation X  between the vertices. A link between two vertices is considered as a vote cast from one vertex to the other vertex. The score associated with a vertex is vertices casting these votes. reflect the relationships between se ntences in the document set, as shown in Figure 1. V is the set of vertices and each vertex v weight f ( i  X  j ) between sentences v i and v j computed using the standard cosi ne measure [1] between the two sentences. where Two vertices are connected if their affinity weight is larger than 0 and we let f ( i  X  i )=0 to avoid self transition. The transition probability from v i to v j normalizing the corresponding affinity weight as follows. row-normalized matrix M ~ =( entry corresponding to th e transition probability. In order to make M ~ be a stochastic matrix, the rows with all zero elements are replaced by a smoothing vector with all elements set to 1/| V |. Based on the matrix M ~ , the saliency score SenScore ( v sentence v i can be deduced from those of all other sentences linked with it and it can be formulated in a recursive form as in the PageRank algorithm. And the matrix form is where the sentences. e to 1.  X  is the damping factor usually set to 0.85, as in the PageRank algorithm. The above process can be considered as a Markov chain by taking stationary probability distribution of each state is obtained by the principal eigenvector of the transition matrix. For implementation, the initial scores of all sentences are set to 1 and the iteration algorithm in Eq uation (4) is adopted to compute the new scores of the sentences. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any sentences falls below a given threshold (0.0001 in this study). Note that after the saliency scores of sentences have been obtained, a variant of the MMR algorithm used in [27] is applied to penalize the sentences highly overlap with informative sentences and finally choose both in formative and novel sentences into the summary. In the basic MRW model, all the sentences are indistinguishable from each other, i.e. the sentences are treated uniformly. However, as we mentioned in previous sec tion, there may be many factors that can have influence on the importance analysis of the sentences. As shown in [6, 7], a document set usually contains a few topic themes and each theme can be represented by a cluster of topic-related sentences. The theme clusters are not equally important. Our assumption is that the sentences in an important theme cluster should be ranked higher than the sentences in other theme clusters, and an important sentence in a theme cluster should be ranked higher than other sentences in the cluster. In order to leverage the cluster-level information, we propose two models to make use of the rela tionships between sentences and clusters. The first model is the Cluster-based Conditional Markov Random Walk Model (ClusterCMRW ), which is an improvement of the MRW model or the PageRank algorithm [22] by incorporating the cluster-level info rmation into the link graph. The second model is the Cluster-based HITS Model (ClusterHITS), authority-hub relationships in the HITS algorithm [9]. Both models are based on link analysis techniques. Note that the above models are used to compute the saliency scores of the sentences in a document set, and other steps are needed to produce the final summary. The overall summarization framework consists of th e following three steps: 1. Theme cluster detection: This step aims to detect theme 2. Sentence score computation: This step aims to compute the 3. Summary extraction: The same algorithm [27] as in the in next sections respectively. The last step is quite straightforward and we omit its details in this paper. In the experiments, three popular clustering algorithms are explored to produce theme clusters. In this study, given a document set, it is hard to predict the actual cluster number, and thus we typically set the number k of expected clusters as follows. where | V | is the number of all sentences in the document set. The clustering algorithms are described as follows [8]: Kmeans Clustering : It is a partition based clustering algorithm. The algorithm randomly selects k sentences as the initial centroids closest cluster, and recomputes the centroid of each cluster, until the centroids do not change. The similarity between a sentence and a cluster centroid is computed using the standard cosine measure. Agglomerative Clustering : It is a bottom-up hierarchical clustering algorithm and starts w ith the sentences as individual clusters and, at each step, merge the most similar or closest pair of clusters, until the numbe r of the clusters reduces to the desired number k . The similarity between two clusters is computed using the AverageLink method, which computes the average of the cosine similarity values between any pair of sentences belonging to the two clusters respectively. Divisive Clustering : It is a top-down hier archical clustering algorithm and starts with one, all-inclusive cluster and, at each step, splits the largest cluster (i.e . the cluster with most sentences) into two small clusters using the Kmeans algorithm until the number of clusters increa ses to the desired number k . In order to incorporate the cluster-level information and the sentence-to-cluster relationship, the C onditional Markov Random Walk Model is based on the two-layer link graph including both the sentences and the clusters. The novel representation is shown link graph between sentences in the basic MRW model. And the upper layer represents the them e clusters. The dashed lines between these two layers indi cate the conditional influence between the sentences and the clusters. Formally, the new representati on for the two-layer graph is denoted as G * =&lt; V s , V c , E ss , E sc &gt;, where V the detected theme clusters; E ss = E ={ e ij | v i , v all links between sentences and E sc ={ e ij | v c = clus ( v i )} corresponds to the correlation between a sentence and sentence v i . For further discussions, we let  X  ( clus ( v denote the importance of cluster clus ( v i ) in the whole document set correlation between sentence v i and its cluster clus ( v We incorporate the two factors into the transition probability from v to v j and the new transition probability is defined as follows: sentences v i and v j , conditioned on the two clusters containing the two sentences. We propose to co mputes the conditional affinity weight by linearly combining the affinity weight conditioned on conditioned on the destination cluster (i.e. f ( i  X  j | clus ( v follows: where  X   X  [0,1] is the combination weight controlling the relative contributions from the source clus ter and the destination cluster. Various methods can be used to compute the cluster importance and the sentence-to-cluster correlation strength, including the cosine measure, the language mode l measure, etc. In this study, we adopt the widely used cosine measure to measure the two factors.  X  ( clus ( v i )) aims to evaluate the im portance of the cluster clus ( v the document set D , and it is set to the cosine similarity value between the cluster and the whole document set 2 : similarity value between the sentence and the cluster: The saliency scores for the sentences are then computed based on ~ M by using the iterative form in Equation (4). The final transition matrix in the Markov chain is then denoted by by the principle eigenvector of the new transition matrix A Different from the MRW model a nd the ClusterCMRW model, the HITS model distinguishes the hubs and authorities in the objects. A hub object has links to many good authorities, and an authority object has high-quality content and there are many hubs linking to it. The hub scores and authority scores are computed in a reinforcement way. In this study, we consider the theme clusters as hubs and the sentences as au thorities. Figure 3 gives the bipartite graph representation, where the upper layer is the hubs and the lower layer is the authorities. The HITS model makes only use of the sentence-to-c luster relationships. Formally, the representation for th e bipartite graph is denoted as G authorities) and V c = C ={ c j } is the set of theme clusters (i.e. hubs); E any sentence and any cluster. Each edge e ij is associated with a weight w ij denoting the strength of the relationship between the sentence v i and the cluster c j . Similarly, the weight w A sentence cluster (or document set) is treated as a single text by concatenating all the sentence texts (or document texts). by using the cosine measure. We let adjacency matrix and L is defined as follows. Then the authority score AuthScore ( t+ 1) ( v i ) of sentence v hub score HubScore ( t+ 1) ( c j ) of cluster c j at the ( t +1) computed based on the hub scores and authority scores at the t iteration as follows. And the matrix form is where scores for the sentences at the t t c HubScore h r of the iterative form, a iteration as follows. It can be proved that authority vector a dominant eigenvector of the authority matrix LL T , and hub vector converges to the dominant eigenvector of the hub matrix L For numerical computation of the scores, the initial scores of all sentences and clusters are set to 1 and the above iterative steps are used to compute the new scores until convergence. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any sentences and clusters falls below a given threshold (0.0001 in this study). sentences. The sentences are then ranked and chosen into summary. Generic multi-document summarization has been one of the fundamental tasks in DUC 2001 3 and DUC 2002 4 (i.e. task 2 in DUC2001 and task 2 in DUC2002), and we used the two tasks for evaluation. DUC2001 provided 30 document sets and DUC2002 provided 59 document sets (D088 is excluded from the original 60 document sets by NIST) and generic abstracts of each document set with lengths of approximately 100 words or less were required to be created. The documents were news articles collected from TREC-9. The sentences in each article have been separated and http://www-nlpir.nist.gov/proj ects/duc/guidelines/2001.html http://www-nlpir.nist.gov/pr ojects/duc/guidelines/2002.html the sentence information has been stored into files. The summary of the two datasets are shown in Table 1. We used the ROUGE [16] toolkit 5 for evaluation, which has been widely adopted by DUC for auto matic summarization evaluation. It measures summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the refe rence summary. R OUGE-N is an n-gram recall measure computed as follows: where n stands for the length of the n-gram, and Count match (n-gram) is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries. Count(n-gram) is the number of n-grams in the reference summaries. ROUGE toolkit reports separate scor es for 1, 2, 3 and 4-gram, and also for longest common subs equence co-occurrences. Among these different scores, unigram -based ROUGE score (ROUGE-1) has been shown to agree with human judgment most [16]. We show three of the ROUGE metrics in the experime ntal results: ROUGE-1 (unigram-based), R OUGE-2 (bigram-based), and ROUGE-W (based on weighted longest common subsequence, weight=1.2). In order to truncate summaries l onger than the length limit, we use the  X -l X  option in ROUGE toolkit and we also use the  X -m X  option for word stemming. The proposed ClusterCMRW and ClusterHITS models with different clustering algorithms are compared with the baseline MRW model, the top three performing systems and two baseline systems on DUC2001 and DUC2002 respectively. The top three systems are the systems with hi ghest ROUGE scores, chosen from the performing systems on each task respectively. The lead baseline and coverage baseline are two baselines employed in the generic multi-document summarization tasks of DUC2001 and DUC2002. The lead baseline takes the first sentences one by one in the last document in the collection, where documents are assumed to be ordered chronologi cally. And the coverage baseline takes the first sentence one by one from the first document to the last document. Tables 2 and 3 show the comparison results on DUC2001 and DUC2002 respectively. In Table 2, SystemN, SystemP and SystemT are the top three performing systems for DUC2001. In Table 3, System19, System26, System28 are the top 
We use ROUGEeval-1.4.2 downloaded from http://haydn. isi.edu/ROUGE/ three performing systems for DUC2002. ClusterCMRW and ClusterHITS rely on the underlying clustering algorithm. For example, ClusterCMRW(Kmeans) refers to the ClusterCMRW model using the Kmeans algorithm to detect theme clusters. For the ClusterCMRW models, the combination weight  X  is typically set to 0.5 without tuning, i.e. the two clusters for two sentences contribute equally to the condi tional transition probability. ( indicates that the im provement over the baseline MRW model is statistically significant.) Seen from the tables, both the ClusterCMRW model and the ClusterHITS model with differ ent clustering algorithms can outperform the basic MRW model an d other baselines over almost all three metrics on both DUC2001 and DUC2002 datasets. The results demonstrate the good effectiv eness of the proposed models. Moreover, the three clustering al gorithms are validated to be as proposed model cannot always outperform the other proposed model on both datasets. In order to investigate how the combination weight influences the summarization performance of th e ClusterCMRW model, we vary the combination weight  X  from 0 to 1 and Figures 4-7 show the ROUGE-1 and ROUGE-2 curves on the DUC2001 and DUC2002 datasets respectively . The similar ROUGE-W curves ar e omitted due to the page limit. We can see from the figures that the proposed ClusterCMRW model with different clustering algorithms can almost always outperform the baseline MRW model, under different values of  X  . The results show the robustness of the proposed ClusterCMRW model, with respect to different combination weights. 
Figure 4: ROUGE-1 vs.  X  for ClusterCMRW on DUC2001 
Figure 5: ROUGE-2 vs.  X  for ClusterCMRW on DUC2001 
Figure 6: ROUGE-1 vs.  X  for ClusterCMRW on DUC2002 Figure 7: ROUGE-2 vs.  X  for ClusterCMRW on DUC2002 Note that in the above expe riments, the cluster number k is typically set to the square root of the sentence number. We further vary k to investigate how the cl uster number influences the summarization performance. Gi ven a document set, we let V denote the sentence collection for the document set, and k is set in the following way: where r (0,1) is a ratio controlling the expected cluster number for the document set. The larger r is, the more clusters will be produced and used in the algorithm. r ranges from 0.1 to 0.9 in the experiments and Figur es 8-11 show the ROUGE-1 and ROUGE-2 results of ClusterCMRW and ClusterHITS on the DUC2001 and DUC2002 datasets, respectively. Seen from the figures, the ClusterCMRW models can almost always outperform the baseline MRW model, no matter how many clusters are used. However, the ClusterHITS models are much influenced by the cluster number and very many clusters will deteriorate the performances of the ClusterHITS models. We can see from Figures 8, 10 and 11 that the performances of the ClusterHITS models are even worse than the baseline MRW that the ClusterCMRW model is mo re robust than the ClusterHITS model, with respect to differen t cluster numbers. The results can be explained that the Cluste rCMRW model involves both the sentence-to-sentence relationshi ps and the sentence-to-cluster relationships, while the ClusterH ITS model makes only use of the sentence-to-cluster relationships, so the performance of the ClusterHITS model will be highly affected by the detected theme clusters. In this paper we propose two novel summarization models to make use of the theme clusters in the document set. The first model incorporates the cluster information in the Conditional Markov Random Walk Model and the second model uses the HITS algorithm by considering the cluster as hubs and the sentences as authorities. Experimental results on the DUC2001 and DUC2002 datasets demonstrate the good effectiveness of the models, and the cluster-based Conditional Markov Random Walk Model is validated to be more ro bust than the Cluster-based HITS Model. In this study, the themes in the document set are discovered by simply clustering the sentences, and the quality of the clusters might not be guaranteed. In future work we will use other theme detection methods to find meani ngful theme clusters. Moreover, we will exploit other link analysis methods to incorporating the cluster-level information. This work was supported by the National Science Foundation of China (No.60703064) and the Research Fund for the Doctoral Program of Higher Education of China (No.20070001059). We thank the anonymous reviewer s for their useful comments. [1] R. Baeza-Yates and B. Ribeir o-Neto. Modern Information [2] R. Barzilay, K. R. McKeown, and M. Elhadad. Information [3] H. Daum X  and D. Marcu. Bayesian query-focused [4] G. Erkan and D. Radev. LexPageRank: prestige in [5] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell. [6] S. Harabagiu and F. Lacatusu. Topic themes for [7] H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. Wise, [8] K. Jain, M. N. Murty and P. J. Flynn. Data clustering: a [9] J. M. Kleinberg. Authoritative sources in a hyperlinked [10] K. Knight and D. Marcu. Summarization beyond sentence [11] W. Kraaij, M. Spitters and M. van der Heijden. Combining a [12] O. Kurland and L. Lee. Pa geRank without hyperlinks: [13] O. Kurland and L. Lee. Respec t my authority! HITS without [14] A. Leuski, C.-Y. Lin and E. Hovy. iNeATS: interactive [15] C.-Y. Lin and E.H. Hovy. Fr om Single to Multi-document [16] C.-Y. Lin and E.H. Hovy. Automatic Evaluation of [17] T.-Y. Liu and W.-Y. Ma. Webpag e importance analysis using [18] I. Mani and E. Bloedorn. Summarizing Similarities and [19] D. Marcu. Discourse-base d summarization in DUC X 2001. [20] K. McKeown, J. Klavans, V. Hatzivassiloglou, R. Barzilay, [21] R. Mihalcea and P. Tarau. A language independent algorithm [22] L. Page, S. Brin, R. Motwani and T. Winograd. The pagerank [23] D. R. Radev, H. Y. Jing, M. St ys and D. Tam. Centroid-based [24] H. Saggion, K. Bontcheva, and H. Cunningham. Robust [25] X. Wan and J. Yang. 2006. Im proved affinity graph based [26] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu and Z. Chen. [27] B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, Z. Chen, and [28] D. Zhou, S. A. Orshanskiy, H. Zha and C. L. Giles. 
