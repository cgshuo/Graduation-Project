 Query result clustering has recently attracted a lot of at-tention to provide users with a succinct overview of relevant results. However, little work has been done on organizing the query results for object-level search. Object-level search re-sult clustering is challenging because we need to support di-verse similarity notions over object-specific features (such as the price and weight of a product) of heterogeneous domains. To address this challenge, we propose a hybrid subspace clus-tering algorithm called Hydra . Algorithm Hydra captures the user perception of diverse similarity notions from millions of Web pages and disambiguates different senses using feature-based subspace locality measures. Our proposed solution, by combining wisdom of crowds and wisdom of data , achieves robustness and efficiency over existing approaches. We ex-tensively evaluate our proposed framework and demonstrate how to enrich user experiences in object-level search using a real-world product search scenarios.
 H.3.3 [ Information Search and Retrieval ]: Clustering; H.3.4 [ Systems and Software ]: Performance evaluation Algorithms object-level search, subspace clustering
An important goal of Web search engines is to provide end-users with relevant results. However, as different users often have different intents on the same query keyword, two conflicting pillars can be defined to achieve this goal. The  X  This work was done when the first two authors were at Microsoft Research Asia.
 first pillar, personalization , aims at maximizing the satisfac-tion of a particular user, while the second pillar, diversifica-tion , aims at minimizing the dissatisfaction risk of varying user intents. Our goal is thus to pursue both pillars with complementary strengths, to allow users to not only view a big picture of result space but also quickly drill-down to specific results meeting their personalized needs.
Toward this goal, the most relevant research area is query result organization for document-level search. Specifically, [9, 25, 22, 26, 7, 13, 11, 20] proposed clustering techniques to partition a set of result documents into several subsets cov-ering different topics. Well-known commercial sites include Vivisimo ( www.vivisimo.com ), Grokker ( www.grokker.com ), iBoogie ( www.iboogie.com ), and Kartoo ( www.kartoo.com ), organizing query result clusters into an expandable topic hi-erarchy or visualizing query results as interconnected topic terms on a map. Such interfaces help users see the overview of the entire data visually then drill-down to the specific cat-egory of an interest. More recently, [12] studied how such combination of clustering and ranking can be simultaneously computed for efficiency.

This paper focuses on query result clustering for object-level search engines [16, 15, 14] that automatically extract and integrate the information on Web objects . In particular, we focus on organizing the query results for Microsoft Live Product Search 1 extracting product information from the Web document corpus. Unlike a document represented as a TF-IDF vector where every feature value belongs to a homo-geneous domain, an object is often represented as numerical feature values of heterogeneous domains, e.g. , sensor size , price , and weight . As a result, an appropriate similarity for Web object pairs is highly data-and intent-specific [5], re-quiring domain expertise to be defined appropriately, while the similarity notion for documents is rather well-agreed, which poses an additional challenge to our problem.
To address this challenge, we now allow clusters represent-ing different search intents to have different salient features, e.g. , sensor size is important when searching for DSLR cam-eras and weight for compact cameras. As a result, similar-ity notions also vary significantly in different clusters. This problem, known as subspace clustering [4, 6, 8, 2, 3, 21, 23, 17], has been actively studied. Specifically, these algorithms search through possible subspaces, and then determine a subspace that best presents the  X  X ocal similarity X  of objects, though search and selection schemes vary over algorithms. However, these algorithms typically suffer from the following two major disadvantages: http://search.live.com/products
Figure 1: Clustering results for query  X  X anon 5d X 
In a clear contrast, we propose a novel framework over-coming these two drawbacks, by adopting large-scale im-plicit human feedbacks on pairwise similarity of all object pairs. More specifically, we use co-occurrences of object pairs appearing in the same Web document together. An instance of such co-occurrence can be viewed as an implicit relevance feedback reflecting the document creator X  X  decision to see two objects as relevant and display in the same page. Co-occurrences can be mined from the entire Web corpus to build  X  X round-truth X  pairwise similarity matrix S where S indicates the co-occurrences of objects o i and o j .
However, such matrix reflecting human perception of sim-ilarity represents varying notions of local similarity. To il-lustrate, since the definition of S ij between a camera object o and its accessory o j and that of S ik between two cam-eras o i and o k capture different senses of similarity, clus-tering these three objects into a cluster should be discour-aged. To disambiguate these two different senses, we adopt the intuition of subspace clustering for qualification. That is, we put objects into the same cluster, only if they share enough  X  X alue locality X  in the same feature subspace. Sum-ming up, our proposed framework combines two notions of relevance, co-occurrences and feature-based local similarity, representing wisdom of crowds and wisdom of data respec-tively, to achieve the robustness and efficiency over existing approaches:
Figure 1 shows an example screenshot of our implementa-tion built on Live Product Search. This illustrates how our proposed clustering algorithm, capturing both co-occurrence and feature-based similarity notions, enriches user experi-ences in product shopping scenarios. In the figure, a user starts search using her initial interest X  X anon 5D X  X s a query keyword. Before making a purchase decision, she would be interested in browsing and comparing with other related products. In particular, we graphically visualize the related items with varying degrees of similarity represented as dis-tances between nodes, captured from co-occurrences, e.g. , a closely displayed pair of  X  X anon Powershot G6 X  and  X  X anon Powershot A620 X  is highly related. However, such similarity between DSLR cameras has a different meaning from the similarity between compact cameras  X  X anon EOS 5D X  and  X  X anon EOS 40D X , which we distinguish as different cluster locations (and colors) as illustrated in the figure. Such or-ganization enables customers to browse related items with varying complementary strengths then drill-down to the cat-egory of interest to make an informed purchase decision.
To summarize, we believe that this paper has the following contributions:
The rest of this paper is organized as follows. Section 2 reviews related work to our framework. Section 3 states basic notions to develop our framework. Section 4 designs a hybrid clustering algorithm Hydra . Section 5 validates our proposed framework both using user study on real-life datasets and a larger-scale synthetic data evaluation. Sec-tion 6 finally concludes this paper.
This section overviews the related prior research efforts to our problem context. We then conclude by stating how our work distinguishes itself from these efforts.
Query result organization aims at providing a succinct overview by categorizing the entire results for end-users. As the pioneering work, [9] proposed Scatter/Gather algorithm to cluster documents from search results. [25] proposed STC (Suffix Tree Clustering) algorithm to identify representa-tive phrases from the snippets, based on which documents are associated with each cluster. Furthermore, [26] trans-formed the given problem as a supervised ranking problem for phrases in order to get meaningful labels and the cor-responding clusters. Meanwhile, some research efforts have addressed document clustering based on dimensionality re-duction techniques such as SVD (Singular Value Decompo-sition) [13], NMF (Non-negative Matrix Factorization) [22], and Spectral Analysis [7]. Recently, [12] proposed a spectral clustering algorithm that simultaneously computes cluster-ing and ranking.
Subspace clustering has been studied to address the X  X urse of dimensionality X  in clustering, i.e. , distances between all object pairs become similar in a high-dimensional space. Due to this problem, traditional clustering algorithms, using distance measures over the entire feature space, fail to iden-tify meaningful clusters. In contrast, subspace clustering algorithms, by considering distances in all subspaces and se-lecting only the most meaningful one, generate high-quality clusters. As the subset generation is intractable, proposed algorithms approximate the search, which can be categorized into bottom-up and top-down greedy searches [17].
To enhance unsupervised clustering algorithms to better reflect user-and domain-specific notions of similarity, [19] proposed semi-supervised k-means clustering adopting feed-backs such as must-link and cannot-link between objects, obtained from domain or human knowledge. [1] proposed a human-computer cooperative subspace clustering frame-work where users provide feedbacks on cluster boundaries, based on which the computer iteratively generates satisfac-tory results. Recent work [24] proposed a semi-supervised projected clustering algorithm based on feedbacks, such as membership labels of some objects to specific cluster or the feature space of some clusters. Similarly, [11] proposed a semi-supervised document clustering model with predefined user feedbacks. More recently, [20] used web search logs to generate more informative document cluster labels, and exploited such labels to guide clustering.
Our work extends query result organization to object-level retrieval. To address the challenge of supporting feature values of heterogeneous domains, we adopt the intuition of subspace clustering, but ensure the robustness and efficiency of our proposed solution, by adopting large-scale user feed-backs and restricting the use of feature-based distance for qualification. Our work distinguishes itself from existing work incorporating user feedbacks, typically assuming ex-plicit and consistent feedbacks, by allowing implicit and in-consistent user feedbacks then disambiguating them using feature-based similarity notion.
This section states preliminaries on modeling Web objects (Section 3.1) and measuring the similarity between Web ob-jects using co-occurrences (Section 3.2) and feature values (Section 3.3).
Web object is a concise, recognizable search unit extracted from the Web corpus under object-level search. In general, Web object is represented both as a term phrase as a unique identification, e.g. ,  X  X anon 5D X , and as a set of specific nu-meric features describing objective contents, e.g. , (0.81kg, $2,149). (Though the feature extraction process may intro-duce errors as discussed in [14], such issue is beyond the scope of this paper.) We formally define Web object as fol-lows: Given a set of Web documents D = { D 1 , . . . , D | a Web object o  X  X  is represented both as a representative term phrase o T and a feature value vector ( o 1 , . . . , o domain-specific feature set F = { f 1 , . . . , f n } . We then consider object-level search as identifying relevant Web objects to user-specified query keyword q , which repre-sents either a specific object or a descriptive general term. We define initial set O q as a set of Web objects appeared together in Web documents where term q appears. Our goal is thus to organize such initial result set O q to satisfy both diverse user intents and specific user needs. Towards this goal, the next sections will present two different similarity measures between Web objects.
We first discuss how the similarity of two objects o and o can be quantified based on their co-occurrences in document set D . For such counts, the proximity of o T and o 0 T in co-occurring documents should be considered, as illustrated in one example form: where proximity is the minimum number of words between o
T and o 0 T in D , normalized by the number of words in D . (Depending on application semantics, argmin and proximity can be replaced with other more suitable functions, orthog-onally to our proposed framework.) s ( o, o 0 ) reflects the document creator X  X  perception of the relevance inferred from displaying the two objects closely to-gether in the document. This notion can generalize to quan-tify cluster similarity S ( C, C 0 ), by averaging distance for all possible object pairs. We formally define the co-occurrence score between two clusters S ( C, C 0 ) as follows: where | C | is the size of clusters.
 Clustering challenge: While co-occurrence closely cap-tures the user perception of similarity, it is hard to judge whether to merge two highly co-occurred object pairs into cluster C i , which is only meaningful when both pairs share enough value locality in the same feature subset S i .
We now discuss how to quantify the degree of  X  X alue lo-cality X  of clusters. As the notion  X  X nough locality X  is defined differently over heterogeneous attributes of arbitrary distri-butions, all proposed measures inevitably depend on data-and cluster-specific thresholds to distinguish relevant fea-tures from the rest.

Specifically, relative index notion [23] quantifies the value locality on f j of cluster C i as: where  X  ij is  X  X ocal deviation X  of f j values within the clus-ter and  X  j is  X  X lobal deviation X  of all objects in O q 2 score is close to maximum value 1, when values within the cluster shares high locality such that local variance is small compared to the global one, and 0 when local and global variances are identical. Similar notion, dimension-oriented distance [21], was studied which quantifies R ij as binary val-ues 1 and 0 with respect to the given threshold  X  . Since the two metrics share the same intuition and the former gener-alizes the latter, we focus on relative index notion .
Such locality on f j should be aggregated on all the relevant feature set S i for the cluster. For selecting S i , data-specific parameter R min is used: Clustering algorithms can then decide whether to merge C 1 and C i 2 into C i , using the following similarity measure
As a general assumption, all subspace clustering algo-rithms encourage resulting clusters to share value locality in as many features as possible, i.e. , stronger evidences for the cluster quality. To reflect this assumption, the following qualification condition is checked before each merge, which introduces another data-specific parameter d min . Definition 1 (Qualification condition) For the given pa-rameter d min and R min , any merge of C i 1 and C i 2 into C should satisfy |S i | X  d min .
We adopt refined R ij in [23] to discourage an extreme case of merging two distance clusters with large size difference, i.e. , | C i 1 |  X  | C i 2 | . More details on this refined notion can be found in [23].
 Clustering challenge: While feature-based similarity no-tion captures different local similarity notions in different clusters, the quality of similarity depends heavily on d min and R min , which ideally should be tuned differently for dif-ferent clusters.
This section first discusses baseline algorithms in Section 4.1, then proposes our clustering algorithm in Section 4.2.
As a basis for all algorithms, we adopt a bottom-up ag-glomerative hierarchial clustering method [10], where every object initially corresponds to a singleton cluster, then itera-tively merged into k clusters. In particular, we consider two baseline algorithms, using co-occurrence and feature-based similarity respectively. 1. Base1 : At each iteration, cluster pair ( C, C 0 ) with 2. Base2 : Similarly, we can iteratively merge a pair with
This section discusses how to design a hybrid clustering algorithm using both co-occurrence and feature-based simi-larity notions, to address the clustering challenges discussed in Section 3. We name our algorithm Hydra for HY bri D p R ojected clustering A lgorithm.

To motivate our approach, we visualize the two similarity notions on a real-life dataset for q = X  X anon 5D X . More specif-ically, Figure 2(a) visualizes pairwise co-occurrence similar-ity using a checkerboard plot where each cell represents ob-ject pair and its color represents similarity (the darker, the more similar). For example, the cell in the upper-left corner represents object pair ( o 1 , o 6 ) with high co-occurrence rep-resented by a dark cell. Figure 2(b) similarly visualizes pair-wise feature-based similarity when R min = 0 . 5 and d min
These plots illustrate the complementary strengths of the two notions. Co-occurrence, being parameter independent, robustly distinguishes the similarity differences, while it is unclear, from co-occurrence similarity alone, whether the high co-occurrences of ( { o 1 } , { o 2 } ) and ( { o 1 } , { o explained with the same reason. Meanwhile, feature-based similarity, as the given parameters are suitable only for some pairs, tends to over-or under-estimate the distance of the remaining pairs.

This observation naturally motivates Algorithm Hydra to use robust and parameter-independent co-occurrence met-rics to determine the merge order then use feature-based similarity to qualify each merge decision. Algorithm Hydra thus accesses pair ( { o 1 } , { o 2 } ) with the highest co-occurrence first, such as four cells marked by the solid box in the lower-left corner in Figure 2(a). Before merging this pair, we check their feature-based similarity, i.e. , the matching solid box in Figure 2(b), which is consistently high. This consistency supports that this merge is meaningful.

In contrast, consider the next pair ( { o 1 , o 2 } , { o co-occurrences of ( { o 1 } , { o 6 } ) and ( { o 2 } , { o high, as represented by cells in dotted boxes in Figure 2(a), this merge appears promising from co-occurrence. However, Figure 2(b) shows clear differences in feature-based similar-reasons for two co-occurrences are different. Merging o 1 o into a single cluster will thus be disqualified by qualifica-tion condition in Definition 1, as represented by white cells in the dotted boxes in Figure 2(c).

After iterative qualified merges, Algorithm Hydra renders the two clearly separated clusters of { o 1 , o 2 , o 4 } and { o o , o 6 } as the final results, which corresponds to the clus-ter of related DSLR cameras { Canon 20D, 30D, 40D } and that of lower-end DSLR cameras { Canon 350D, 400D, XT } respectively. Meanwhile, base1 , using only co-occurrence, mixes up two categories by performing the above unqual-ified merge { o 1 , o 2 , o 6 } , while base2 returns a low quality on blurred similarity matrix.
As the notion of feature-based similarity heavily relies on data-specific parameters d min and R min , tuning determines the quality of feature-based clustering. Toward the goal, we first discuss linear tuning developed in [23] for HARP as preliminaries, based on which we later propose a more sophisticated tuning approach.
 Parameter tuning problem can be abstracted as a mul-Algorithm 1 Hydra ( O q ) 1: C  X  X } . 5:  X  R =EstimateR( O q ) 7: for l  X  0 to  X  d  X  1 do 10: while L .movenext () do 14: if |C| = k then 15: Terminate. 16: end if 17: end if 18: end while 19: L .movefirst () 20: end for tivariate interpolation problem of finding ( R min , d min generating k clusters. Figure 3 illustrates an example search space, where the darkest cells represent parameter pairs gen-erating k clusters and white cells represent pairs which can-not generate any merge. HARP interpolates data points in the space, by searching diagonal cells starting from R min and d min = n , i.e. , the lower-left corner in Figure 3.
At each interpolation point, HARP performs clustering us-ing corresponding parameter pairs. For example, for the initial value of R min = 1 and d min = d , no cluster pair can be merged, unless two identical singleton clusters exist in the dataset, which suggests the qualification condition is too  X  X ight X  to generate k clusters. The condition is thus gradually  X  X oosened X , in particular, by choosing the next diagonal cell for the next round of HARP clustering, i.e. , R min = 1  X  l n  X  1 and d min = n  X  l after l rounds. Such rounds continue, as the dotted diagonal arrow in Figure 3 illustrates, until it hits the darkest cell. Algorithm 1 cor-responds to Hydra when the starting values  X  R and  X  d are statically set as 1 and n in line 5 and 6.
However, from the linear tuning, we observe the following two optimization opportunities.
We take these optimization opportunities, by proposing to prune out white cells from search space. Our goal is to efficiently identify a tighter bound for  X  d . As such parameter is data-specific, we first develop an adaptive scheme with correctness guarantee, and then improve it into an efficient approximation scheme.
 Guaranteed bounding: One way to estimate  X  d is to ac-tually perform HARP with  X  R = 1 then set  X  d large enough to qualify all the merges performed, i.e. ,  X  d = argmax C when R min = 0 . As depicted in Figure 3,  X  d monotonically increases as R min decreases.

Since performing HARP is expensive, we show that per-forming an efficient alternative, i.e. , single-linkage cluster-ing, can also identify  X  d with correctness guarantee. In single-linkage clustering,  X  R ij for merging two clusters is computed as the distance between the two closest elements o  X  C i 1 and o 0  X  C i 2 in the two clusters. By considering only the closest value pair,  X  R ij is the upper bound of the actual feature-based cluster distance:
Single-linkage clustering is essentially Kruskal X  X  algorithm for finding minimum spanning trees, by merging element pairs in the order of  X  , until k spanning trees are generated, which correspond to k resulting clusters.  X  S i of the resulting cluster C i can thus be computed as: for every connected pair ( o, o 0 ) such that o, o 0  X  C i
It is immediate from the above upper bounding property that, when Kruskal X  X  and HARP algorithms return the same clustering results C = C 0 ,  X  S i of every Kruskal cluster sub-sumes S i of the corresponding HARP cluster:
We can extend this for the general case when C 6 = C 0 . We can transform the clustering results of HARP into an equiv-alent graph of k spanning trees, by generating a local min-imum spanning tree within each resulting cluster C 0 i where the maximum  X  d 0 for every connected node pair satisfies: Compared to  X  d obtained from the globally minimum span-ning tree,  X  d  X   X  d 0 holds, which ensures Equation 9 to remain correct.
 Conservative starting point can be thus d min =  X  d and R = 1 , which is guaranteed to start from a white cell. How-ever, this scheme still incurs high computational overheads, i.e. , O ( m 2 logm 2 ) for Kruskal X  X  algorithm. We thus develop an approximation of the above procedure in O ( m 2 ) X  With no correctness guarantee, such approximation may intro-duce potential mistakes in early merges by loose qualifica-tion, which we empirically observe not to negatively affect the quality much. To the contrary, we observe that, by aggressively reducing the search regions and searching the focused region more thoroughly, approximation significantly improves the cluster quality.
 Aggressive bounding: We now discuss how we approxi-mate Kruskal X  X  algorithm discussed above. Specifically, we aim at estimating the lower bound  X  R j of  X  j ( o, o 0 ) of all con-nected object pairs in the resulting spanning trees.
Toward the goal, we pick an estimated value of  X  R j from the list L of  X  j for all m ( m  X  1) 2 possible pairs. However, the rank of estimated value in the list can vary significantly, depending on the topologies of the resulting spanning trees. To illustrate, consider an extreme case, when k resulting clusters consist of k  X  1 singleton clusters and one big cluster C . When every object pair in C 0 is extremely close,  X  j ( o, o values of all such pairs are higher than  X  R j , which lowers the estimated rank close to m ( m  X  1) 2 . In another extreme case, where only connected pairs are close, the estimated rank can be as high as m  X  ( k  X  1). As a moderate estimate of the two, we thus pick the median in L , based on which,  X  d can also be estimated as the median of |S i | for every singleton cluster pair. Figure 3 illustrates our aggressive search strategy in the search space. The next section will empirically show how this simple adaptive tuning significantly improves both the effectiveness and efficiency of Hydra .
This section reports our experimental results to validate the accuracy and efficiency of Algorithm Hydra . Our exper-iments were carried out on a Intel(R) Core 2 machine with 2.13 GHz processor and 2GB RAM running Windows XP. All algorithms were implemented in C # language. Our pro-posed algorithm is compared against two baseline approaches X  Base1 using only co-occurrences which we name HAC and Base2 implementing Algorithm HARP [23], a subspace clus-tering algorithm known to have the highest accuracy [18].
First, we performed user study over a real-life product dataset of size 83.9 GBs, including more than 1.1 millions documents crawled in the September of 2008 for Live Prod-uct Search ( http://search.live.com/products ). To obtain co-occurrence scores, we extracted product co-occurrences from the product reviews in the dataset. We also extracted fea-ture values, linearly normalized to [0,1]. We conducted a real-life user study for 32 people (Microsoft Research Asia interns and POSTECH students). Due to the expensive na-ture of user studies, we limit to 6 search tasks for users, in particular, on the currently most popular cameras and lap-tops (3 tasks each) in Korea, according to www.danawa.com , among the products in our dataset.
For each search task, clustering results from the two al-gorithms are displayed to users without labels, where one is ours and the other is a baseline (randomly chosen between HAC and HARP ). Users then blindly vote for the clustering results with higher quality. Figure 5.2 shows the number of user votes, normalized by the number of times displayed to the users. Observe that, both in camera and laptop cate-gories, our clustering results obtain the highest number of votes, which suggests that the hybrid approach of combining two relevance notions improves the user-perceived quality of clustering.
As the scale of user study is inherently limited, we gener-ate a large-scale synthetic dataset with ground truth clusters and validate our results over larger and more diverse exper-imental settings.
 Synthetic dataset : We synthetically generate a dataset with ground truth clusters in mind. For synthetically gen-erating feature values, we first determine the size of each k  X  0 . 8 ]. To make sure every object belongs to at least one cluster, we randomly generate the size of the first k  X  1 clus-ters in the above range and decide that of the last cluster as the number of all remaining objects. Once the size of each cluster is decided, we next randomly choose the dimension-ality of each cluster, in the range of [ S avg  X  3 , S avg on the average dimensionality S avg . Once the dimensional-ity S i of each cluster C i is determined, we randomly pick S relevant features, and then generate values as follows:
We then synthetically generate co-occurrence scores. While the co-occurrence score can be arbitrarily generated, it is non-trivial to decide the ground-truth clusters when feature-based and co-occurrence similarity scores disagree. Due to this difficulty, we consider a special scenario, where co-occurrences are generated based on feature-based similarity. Specifically, the co-occurrence between objects o and o 0 cluster C and C 0 is represented as However, as real-life user feedbacks are prone to noises, we add controlled noises, by choosing only r = 80% of f j from |S  X  S 0 | then rest from the remaining features, i.e. , the smaller r is, the noisier co-occurrences are.

We stress that this scenario is somewhat unfavorable to our proposed algorithm (and favorable to HARP ), since the co-occurrence scores, generated from feature-based relevance, cannot provide any extra information to our hybrid approach. Our intention of using this scenario is to show that, even in this unfavorable setting, Algorithm Hydra outperforms base-lines. Lastly, to reflect the  X  X parsity X  of co-occurrence scores in real-life data where co-occurrence scores are zero for the majority of object pairs, we control sparsity sp to generate sp % of co-occurrences as zero.
 Quality metrics : As quality metrics, we adopt three rep-resentative metrics extensively used from prior works [2, 21]: Clustering Error (CE), F 1 -value, and FF 1 -value.
First, CE [2] is generally used to show the difference be-tween ground-truth clustering C and our clustering results C . This measure is based on |C 0 | X |C| matrix M , where M ij is the number of elements shared by clusters C i and C j from C 0 and C respectively. Note that we use confu-sion matrix M 0 [2] maximizing the sum of diagonal elements D = when C = C 0 , non-diagonal elements of M 0 will be all zero, and in other cases, the error can be quantified as the differ-ence between the sum of diagonal elements and the overall sum, i.e. , CE ( C , C 0 ) = U  X  D U , where U is the sum of all ele-ments in the matrix such that value of this measure is 0, two clustering results are identical.
Next, F 1 -value is a harmonic mean of precision and recall , widely used to measure accuracy in IR literatures. Specifi-cally, precision j and recall j are defined as argmax i { M P value is calculated as the average of k clusters, maximized as 1 when two clustering results are identical.

Similarly, FF 1 -value [21] is an extension of F 1 -value to quantify the overlap in feature space of two clustering re-sults. The overlapped elements are used from F 1 values. FF -precision j and FF -recall j are defined as |S j  X  X  relevant feature sets at j th cluster in C and C 0 , respectively. Experimental results : We present experimental results with synthetic datasets for various parameter settings in Ta-ble 1. Each data point reports the average of 50 runs with 50 different datasets.

First, Figure 5 reports the accuracy using three metrics over varying average number of features of clusters. Note that, FF 1 -metrics does not apply to HAC which does not identify relevant feature sets. Observe that in all three met-rics, all algorithms were highly accurate when S avg is high. An interesting difference from user study results in Figure is HAC performs worse in synthetic evaluations, which can be explained by the noises introduced to co-occurrences that negatively affect the accuracy of HAC depending solely on co-occurrences. In particular, for low S avg the accuracy gaps are apparent. The problem of HARP in such scenario is that, by starting the parameter tuning from the most conservative bound d min = d , all similarity score refinements from early rounds are simply wasted, which limits refinement opportu-nities in the valid range and also affects the quality of merges performed on poorly estimated scores. In a clear contrast, Algorithm Hydra , by using a more robust co-occurrence or-dering, shows higher accuracy in all S avg , which is further enhanced by Algorithm HydraAdaptive , searching a focused space at a finer granularity.

Second, Figure 6 reports the accuracy using three metrics over varying local deviation. Observe that, in all three met-rics, Algorithm Hydra generates significantly better cluster-ing results than Algorithm HARP and HAC , especially when the local deviation is high. As the ideal R min values for clusters are lowered, HARP searching for the ideal parameter from R = 1 wastes refinement efforts in early rounds. In con-trast, Algorithm HydraAdaptive , by exploiting co-occurrence ordering and finer-grained parameter tuning on lower R min search region, reduces the CE error of HARP by 1 2 when  X  ij = [8 , 14].

Third, Figure 7 reports the accuracy using three metrics over varying number of clusters. Observe that, in all three metrics, the accuracy of all algorithms deteriorates as k in-creases, as a single mistake in merging severely affects the datasets with smaller and many clusters, while such sensi-tivity is significantly low for Algorithm HydraAdaptive . To illustrate, when k = 15 , the CE result shows that Algorithm HydraAdaptive shows 3.34, 2.55 and 2.35 times higher accu-racy than Algorithm HAC , HARP , and Hydra respectively.
Fourth, Figure 8 reports the accuracy using three metrics for co-occurrence data with varying sparsity. In particu-lar, we vary sparsity sp for Algorithm HydraAdaptive . As references, we plot the accuracy of HARP (as lower bound accuracy) and that of Hydra using perfect co-occurrence data without sparsity (as theoretical upper bound accuracy), both of which are not affected by sp and thus stay constant. Ob-serve that, Algorithm HydraAdaptive , even at the presence of severe sparsity, e.g. , sp = 90% , is significantly more ac-curate than the lower bound and closely approximates the theoretical upper bound. For lower sparsity, e.g. , sp &lt; 70%, the accuracy of Algorithm HydraAdaptive converges to the theoretical upper bound.
 Lastly, we empirically study the scalability of Algorithm Hydra over varying data size m and feature size n . For varying n , we also vary S avg , to avoid cases only n 0  X  n features are relevant to clusters, in particular, by vary-ing S avg = b n/ 3 c as well. Figure 9 shows that Algorithm HydraAdaptive significantly outperforms Algorithm HARP in all settings and scales more gracefully, by using adaptive loosening which enables fast convergence and thus reduces computational overhead. Figure 9: Efficiency between Algorithm HARP and Algorithm Hydra
This paper studied query result organization to provide highly relevant results that both cover diverse intents and address the user-specific intent. In particular, we focus on object-level search, which poses a new challenge of combin-ing co-occurrence and feature-based similarity notions with complementary strengths. To address this challenge, we pro-posed a hybrid clustering algorithm Hydra using large-scale implicit user feedbacks as similarity metrics representing di-verse intents, disambiguated by feature-based subspace lo-cality. We extensively validated Algorithm Hydra using both real-life user study and large-scale synthetic datasets. The first two authors were supported by Microsoft Research Asia (internet service theme) and Engineering Research Cen-ter of Excellence Program of Korea Ministry of Education, Science and Technology (MEST) / Korea Science and Engi-neering Foundation (KOSEF), grant number R11-2008-007-03003-0.
