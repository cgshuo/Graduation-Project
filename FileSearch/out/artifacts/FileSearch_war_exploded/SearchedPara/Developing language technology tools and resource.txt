 Language technology tools are vital resources that ensure digital existence of a language for a long time. Such tools and resources are necessary for nat-ural language processing and have aplenty applica-tions in the digital era. For instance, cross-lingual technologies such as machine translation help peo-ple across the world communicate with each other using their native languages and access information present in a language they do not know. Similarly, automatic speech recognition helps people interact with machines using natural languages. There are many more such applications where a better under-standing of natural languages by machines could be helpful in various ways. Language technology tools facilitate the understanding of natural languages by computers. A lot of popular languages in the world are equipped with such tools and applications but a larger set of languages in this world lack these basic tools. It is important to protect such languages from being digitally endangered.

Our work is based on one such resource-poor lan-guage, Sindhi. Our aim is to develop some basic re-sources, language processing tools and an applica-tion which will help Sindhi in its digital existence. Sindhi is an Indo-Aryan language spoken by more than 75 million speakers in the world. The major-ity of this population resides in India and Pakistan. 2 Historically, Sindhi was written using several writ-ing systems (Landa, Khojki, Waranki, Khudawadi, Gurumukhi, Perso-Arabic and Devanagari), many of which are extinct now. Currently, Devanagari and Perso-Arabic scripts are primarily used to write in Sindhi. Both these scripts are official scripts of Sindhi in India, whereas only Perso-Arabic is the of-ficial script of Sindhi in Pakistan.
 During the colonial rule, the British chose Perso-Arabic as the standard script, which led to creation of large amount of literature in this script. There are many news websites and blogs in Sindhi (Perso-Arabic) published from Pakistan. 3 This may be be-cause Sindhi speakers are more in Pakistan than In-dia and also have a geographical state called  X  X indh X . In contrast, literature in Sindhi (Devanagari) on the web is very small. In India, Sindhi is an official language but not of a particular geographical state and therefore it does not enjoy the support that other state-official languages do. Sindhi is written using two writing forms, the De-vanagari script and the Perso-Arabic script. Previ-ously, some research on Sindhi has been done with Perso-Arabic as the preferred script. An account of this research is given below.

A rule-based POS Tagger was developed by Ma-har et al. (2010) using a lexicon of 26,355 entries and 67 tags. Its accuracy was reported to be 96.28%. A finite-state machine for capturing noun inflections in Sindhi was developed by Rahman et al. (2010). Zeeshan et al. (2015) have worked on developing a spell checker. Unfortunately, the above described tools are not publicly available. Therefore we could not evaluate and compare them or use them for de-veloping resources for Sindhi (Devanagari).
A computational grammar library for Sindhi (Perso-Arabic) in Grammatical Framework 4 (Ranta, 2009) was developed by Jherna Devi Oad (2012). This library has 44 categories, 190 functions and 360 lexical entries. It was referred to during the devel-opment of our Sindhi (Perso-Arabic) morphological analyser. A dataset is the most important requirement for building language technology tools and resources for any language. The following section describes how we collected and developed the datasets for both the scripts of Sindhi. A summary of the datasets and tools developed by us or other researchers for both scripts of Sindhi is provided in Table 1. 4.1 Sindhi (Devanagari) Datasets The amount of raw texts available on the web for Sindhi (Devanagari) is very small. Initially we contacted various publishers and news agencies to source raw data, but the problem was further com-pounded as many publishers on the web have not yet moved to Unicode standards.
 Raw Textual Data: We collected several short stories, books, articles, etc. and manually created data for Devanagari. Through this manual pro-cess, we were able to handle certain issues such as usage of correct Unicode encoding, normalization, script and grammar. Later, we developed a Unicode Converter 5 for legacy fonts, which helped us col-lect more data. We currently have a raw corpus of 326813 words, with average sentence length of 9.35 words and a vocabulary (unique words) of 22300 words.
 Part-of-Speech Annotated Data: Since Sindhi did not have a POS Tagset, we adapted the BIS Tagset 6 which is comprehensive and designed to be extensible to any Indian Language. We annotated the data using this tagset and help from two annota-tors. We obtained a  X  score (Cohen, 1960) of 0.96 when evaluated for Inter-Annotator Agreement on 793 words. Currently, we have tagged corpus of 44692 words. This data was subsequently used to build an automatic Part-of-Speech Tagger (discussed in Section 5.1). 4.2 Sindhi (Perso-Arabic) Datasets As previously mentioned, large amount of content exists on the web for Sindhi in Perso-Arabic script, which can be used to source raw textual data. Raw Textual Data: We collected textual data from Sindhi Wikipedia dump 7 , news websites and blogs 8 . We currently have a corpus of about 1 mil-lion tokens.
 Parallel Data: A sentence-aligned parallel cor-pora is an indispensable resource for any language pair. Many languages across the world are not for-tunate enough to have such a parallel corpora avail-able, including Sindhi. We have developed a small parallel corpus between Urdu and Sindhi, which are closely related languages. We initiated the devel-opment process by collecting some sentences from the Urdu Treebank (Bhat and Sharma, 2012), general conversations, news articles and essays and translat-ing them to Sindhi manually. We now have a parallel corpus of 1400 sentences and it is being used for var-ious purposes (Section 6), including automatic gen-eration of more parallel data (see 6.3). After developing the datasets, we used them in cre-ation of certain language technology tools which we describe below. Table 1 summarizes some tools de-veloped for Sindhi by us and other researchers. 5.1 Part-of-Speech Tagger Part-of-Speech (POS) tagging is the task of assign-ing an appropriate part-of-speech label to each word in a sentence, based on both its definition as well as its context. POS tagging is a fundamentally impor-tant task, as it gives information about words, their neighbors and the syntactic structure around them. This information is useful in syntactic parsing and other applications such as named-entity recognition, information retrieval, speech processing, etc.
The data that we annotated with POS tags was used to build an automatic POS Tagger 9 for Sindhi (Devanagari) (Motlani et al., 2015) using Condi-tional Random Fields 10 (Lafferty et al., 2001). We employed 10-fold cross validation to train and test the tagger. We experimented with several models by using various set of features, including linguisti-cally motivated features such as affixes (which cap-ture the morphological properties of the language) and context (which capture the syntactic structure of the language).

The current best performing model gives an av-erage accuracy of 92.6% , which is 11% better than baseline 11 tagger. This tagger is being used to gener-ate more POS annotated data through bootstrapping and post-editing methods. 5.2 Morphological Analyser The morphological analysis of a word involves cap-turing its inherent properties such as gender, number, person, case, etc. Morphological features also help in improving the performance of other NLP tools such as, pos tagger, spell-checker, parsers, machine translation, etc. Thus, morphological analysis is a fundamental and crucial task.

We used Apertium X  X  lttoolbox (Forcada et al., 2011) to develop a paradigm based finite-state morphological analyser for Sindhi (Perso-Arabic) (Motlani et al., 2016). This morphological analyser currently has about 3500 entries and a coverage of more than 81% on Sindhi Wikipedia dump consist-ing of 341.5k tokens. This analyser is publicly avail-able on Apertium 12 .

Sindhi is a morphologically rich language (Rah-man and Bhatti, 2010). It uses suffixes for construct-ing derivational and inflectional morphemes. A ma-jor challenge for us is to incorporate the vast mor-phology. We currently have 72 paradigms in the analyser and are expanding them to cover all pos-sible inflections. This, along with adding more en-tries to the lexicon, would help increase the coverage further. Another challenge is processing partially or fully diacritised input. The analyser can handle usual Sindhi texts which lack in diacritics but it tends to make errors for other kinds of input because it is dif-ficult to lookup in the lexicon and disambiguate. 5.3 Transliteration System A transliteration system is a much needed tool to bridge the gap between content in Perso-Arabic and Devanagari scripts. Moreover, such a system could also facilitate sharing of resources developed in ei-ther scripts. Although a transliteration system would be very useful but there are various challenges that we face. Some of them are : 1. Unavailability of Transliteration Pairs : 2. Missing Diacritics : Many Perso-Arabic script 3. Differences in Character-Sets : The alpha-5.3.1 Unsupervised Transliteration Pairs
There is a lot of literature on automatic extraction of transliteration pairs using seed data and parallel corpora (Sajjad et al., 2012; Durrani et al., 2014; Jiampojamarn et al., 2010; Kumaran et al., 2010). Since our scenario is resource-poor, we designed and used an unsupervised approach for translitera-tion pair mining that prescinds from prior knowledge of seed corpus and parallel data.

In this approach, a discriminative transliteration detection engine takes three inputs: a limited char-acter mapping 13 and unique word-list in source and target language.

These lists are iterated over to obtain a list of candidate word pairs. These candidate word pairs are then discriminated based on orthographic simi-larity. The orthographic similarity is measured by converting the characters of source and target word into an intermediate representation using the char-acter mapping and calculating the edit-distance be-tween them normalized by their word-length. The candidate pairs with larger edit-distance are pruned out and the remaining are treated as possible translit-eration pairs. 5.3.2 Preliminary Results
The transliteration problem can be posed as a phrase-based SMT problem, where sentences are analogous to words and words are analogous to char-acters. We used the MOSES (Koehn et al., 2003) toolkit to train transliteration models by treating each transliteration pair (space separated sequence of characters) as the parallel data.

We had mined 112434 possible transliteration pairs from our raw datasets and trained a translitera-tion model. We evaluated it on a short story of 3247 words and obtained the following results shown in Table 3. We have also demonstrated an example in Precision (%) 60.14 83.27 87.12 90.08 Table 4, where words of a source sentence in Sindhi (Perso-Arabic) are transliterated to Sindhi (Devana-gari). 5.3.3 Context Aware Transliteration
The systems developed using previous approach can produce transliteration candidates for a given in-put word (as shown in Table 4), but there are various challenges in case of Sindhi (described in Section 5.3) because of which the precision of best output (top-1) is low. We believe this system can be im-proved using context in selecting the correct translit-eration from candidate transliterations (top-k) of an input word. Currently, we are experimenting with context-based transliteration using Viterbi decoding and Language Model re-ranking. 14 Development of fundamental tools and resources discussed in the previous sections are important for larger NLP applications on Sindhi. An important ap-plication that can be developed without using these tools is an SMT system. Although phrase-based SMT requires only parallel data, rule-based and fac-tored based machine translation systems depend on these fundamental tools.

In this section we shall discuss our ongoing work on developing a Sindhi-Urdu SMT system. 6.1 The Language Pair: Urdu and Sindhi Sindhi and Urdu are spoken by a large number of na-tive speakers (75 million and 159 million 15 around the world). These languages belong to Indo-Aryan language family and have evolved in the same geo-graphical region for many years. Thus, they have many syntactic and semantic similarities. For in-stance, they share vocabulary, typography and sen-tence structures (for example, both follow subject-object-verb word-order). These similarities are ma-jor reasons behind choosing this language pair for the purpose of developing parallel data (Section 4.2) and subsequently a SMT system.
 In our opinion, Sindhi would benefit a lot from Sindhi-Urdu parallel data, as Urdu is comparatively resource rich language and techniques like projec-tion (Yarowsky et al., 2001; Das and Petrov, 2011; T X ckstr X m et al., 2012) can be employed to leverage several resources for Sindhi. 6.2 Development When we started working on SMT for Sindhi-Urdu, we only had about 1200 parallel sentences, a baseline SMT system 16 was created using them.

This baseline system was evaluated on 70 held-out test sentences. The output sentences were given to a translator evaluation by rating each sentence on a scale of 1-5 (where, 1-very bad and 5-very good). The average rating obtained was 2.65 points. We also calculated other popular metrics for evaluation of MT system. BLEU (Papineni et al., 2002) score was 38.62, METEOR (Banerjee and Lavie, 2005) score was 77.97 and TER (Translation Error Rate) (Snover et al., 2006) was 38.28 . Note that, BLEU and METEOR scores are high due to small size of training data and vocabulary. Results of TER and human-evaluation are a better reflection of baseline system X  X  performance. 6.3 Improvement We manually analysed the errors made by the base-line SMT system and found that too many out-of-vocabulary (OOV) words. Other than those, words which were incorrectly translated were either due to presence in very low frequency in training data or due to ambiguity created by multiple possible trans-lations.

Thus, we need to employ various techniques in order to significantly improve over baseline perfor-mance and develop a reasonably good translation system. One such technique is bootstrapping more parallel data using the baseline SMT system. Al-though, creating parallel data is faster in this pro-cess but it is still a time consuming and laborious task. Therefore, we also need to use certain auto-matic techniques. Some of them are described below 6.3.1 Bootstrapping more Parallel Data
The performance of a SMT system depends largely on the amount of parallel data used for train-ing the system, which is very less in our case. There-fore, we are trying to generate more parallel data by using the baseline SMT system to bootstrap more parallel corpus. We source new sentences from the web (news articles, essays, short stories, etc.), trans-late it and then provide it to translators for post-editing. 6.3.2 Bilingual Lexicon Extraction from
Bilingual lexicon extraction is an automatic way to extract parallel data from non-parallel texts. Re-search in this area has been active for past several years and various approaches with promising re-sults have been proposed (Rapp, 1995; Haghighi et al., 2008; Laroche and Langlais, 2010; Vuli X  et al., 2011; IrvineandCallison-Burch, 2013). Theprocess involves finding possible translation candidates for a source word in target data using several features like orthographic, temporal, topical and contextual simi-larity. Presence of seed lexicon further benefits this process. Since Urdu and Sindhi are closely related languages and we have small parallel data, we can compute these features to induce lexicon of Urdu in Sindhi and obtain possible translation pairs.
We are exploring these different techniques on comparable data sourced from Wikipedia pages inter-lingually linked between Sindhi and Urdu and some news articles 17 published in these languages. The extracted parallel data will be supplemented to thephrase-table learnedby Moses(Klementiev et al., 2012). This parallel data shall improve the coverage of the SMT system, although its impact on the SMT system X  X  performance is yet to be evaluated. 6.3.3 Rule-Based Transliteration
The Sindhi (Perso-Arabic) and Urdu alphabets share many characters with very few differences. This typographic similarity can also be used to reduce OOV errors, specially for named entities. Therefore, we are developing a rule-based translit-eration system by mapping the different characters in their scripts. My thesis aims at developing some fundamental tools and resources and an application for a resource-poor and multi-script language, Sindhi. The main contribution of my work includes collection and cre-ation of raw and annotated datasets, constructing NLP tools such as POS tagger, morphological anal-yser, building a transliteration system without paral-lel data in an unsupervised fashion and developing a SMT system for Sindhi-Urdu and improving it us-ing various techniques. While my work shall supple-ment development of NLP applications for Sindhi, it shall also motivate research on languages surviving in similar resource-poor setting.
 The research presented in this paper was done in col-laboration with my advisors, Prof. Dipti M. Sharma and Dr. Manish Shrivastava. The Part-of-Speech an-notation was done in collaboration with Dr. Pinkey Nainwani and Harsh Lalwani. I would like to ac-knowledge the people who have helped me various tasks like collecting the data, translations and un-derstanding Sindhi better such as, Mehtab Ahmed Solangi, Mr. Bhagwan Babani and Mr. Chunnilaal Wadhwani. I would like to thank Nehal J. Wani, Arnav Sharma, Himanshu Sharma and Dr. Francis M. Tyers for their constant motivation and support. Lastly, I am thankful to the anonymous reviewers for their invaluable feedback and suggestions. This pub-lication reflects the authors views only.

