 Model-based algorithms are emerging as a preferred method for document clustering. As computing resources improve, methods such as Gibbs sampling have become more common for parameter estimation in these models. Gibbs sampling is well understood for many applications, but has not been extensively studied for use in document clustering. We explore the convergence rate, the possi-bility of label switching, and chain summarization methodologies for document clustering on a particular model, namely a mixture of multinomials model, and show that fairly simple methods can be employed, while still producing clusterings of superior quality compared to those produced with the EM algorithm.
 I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms ; G.3 [ Prob-ability and Statistics ]: Probabilistic algorithms Algorithms, Experimentation, Performance document clustering, Gibbs sampling, MCMC, EM, collapsed sam-plers, practical guidelines
Document clustering is an unsupervised learning task that parti-tions a set of documents D = d 1 ,...,d N to produce disjoint sub-sets called clusters. Members of the same cluster should be similar, while members of different clusters are dissimilar. Similarity and dissimilarity are subjective, and might refer to similarities based on topic, style, authorship, sentiment, or any number of criteria as dictated by the requirements of the specific clustering problem.
Practical applications of document clustering include the discov-ery of genres in large heterogeneous corpora, the automatic organi-zation of document collections, novelty detection, exploratory data analysis, organizing search results, and text mining. Many of the same techniques that are used to cluster other types of data can be used for document clustering with varying degrees of success. The document feature space is high-dimensional and sparse, dis-tinguishing it from other data clustering problems and requiring techniques suited to these properties to achieve good results. Re-search interest is transitioning from vector-space algorithms to sta-tistical model-based algorithms for clustering [1, 10, 19, 24, 25]. As this transition occurs, many questions involving the methodol-ogy of model-based clustering need to be considered and answered.
Although there are models that are more elaborate, and poten-tially more robust than the one presented here [19], past research has focused mostly on developing new models and has not empiri-cally explored the implications of the choices made during the im-plementation of a parameter estimation algorithm. This paper sets out to empirically investigate many of these questions and hope-fully to serve as a set of guidelines for those who would apply MCMC (Markov Chain Monte Carlo) techniques to model-based document clustering, allowing them to make informed decisions as they implement their own algorithms. Specifically, we explore the use of a collapsed Gibbs sampler on a mixture of multinomials document model.

We will show the empirical effects on cluster quality of using various sample summarization methods and will demonstrate that within-chain label switching (non-identifiability) does not appear to be an issue when using the collapsed sampler with our model. We also provide evidence that the sampler converges quickly, within a relatively small number of samples. Finally, we show that the col-lapsed sampler clustering algorithm presented here produces better clusters than an EM clustering algorithm on the same model, ac-cording to five cluster quality metrics.
Many approaches have been proposed for clustering in general and document clustering in particular. Some of these techniques treat each datum as a vector in n -dimensional space. These tech-niques use measures such as Euclidean distance and cosine sim-ilarity to group the data together using agglomerative clustering, k -means clustering, and other similar methods.

Another set of techniques uses generative statistical models, such as Bayesian networks. These models usually include a latent vari-able denoting cluster assignment. One such model is the mixture of multinomials model [14], which we employ in this paper. In this model, the values of the individual features of each document are assumed to be conditionally independent and exchangeable, given the label of the document. This is called the  X  X ag of words" as-sumption. The model is explained in Section 3.
Many methods can be used to estimate parameters in order to conduct inference and produce clusterings in model-based schemes. One such method is to begin with a random initialization of the pa-rameters and then refine them using the EM algorithm. This ap-proach has been shown superior to some vector-space solutions, such as agglomerative clustering [14]. However, as a hill-climbing algorithm EM is subject to becoming trapped by local maxima in the likelihood surface [3]. An alternative proposed in the litera-ture is to use a Markov Chain Monte Carlo method, such as Gibbs sampling, to sample from the posterior distribution of the model parameters, given the data to be clustered [19, 24]. This approach is more correct from a Bayesian perspective, and has been shown to perform better than maximum likelihood on some problems such as the estimation of parameters for probabilistic grammars [8]. Like EM, Gibbs is naturally attracted to higher points on the likelihood surface, which are more likely to be sampled from. Unlike EM, an MCMC sampler is capable of leaving local maxima to explore less likely configurations. In this way, it is able to transition from local maxima to other areas of high likelihood. Thus, regardless of initialization, a Gibbs sampler may eventually come close to a true global maxima of the distribution in question, while an unfortunate initialization of EM can preclude it from ever reaching that mode, regardless of the number of iterations of the algorithm that are run. Although MCMC-based document clustering techniques have been compared with EM-based techniques in the past by Banerjee and Basu [1], their work did not hold the model constant.

Despite the apparent advantages of MCMC methods, it is not clear how one should proceed when applying these methods specif-ically to document clustering, where high dimensionality, sparse-ness, and large numbers of data points are the rule. Figure 1: Graphical representation of the generative document model used in this paper for clustering. Square nodes are con-stants, shaded nodes are observed values.

The model used here is a mixture of multinomials, with Dirichlet priors. The parameters of the model are distributed as follows: This model is illustrated in Figure 1. Here, w is a matrix represent-ing the words in all M documents in the data. Document d from cluster j consists of a vector of N words w d , distributed according to a multinomial distribution parameterized by the vector  X  suming that z d = j ,  X  j,v is the probability that word i of document d is the word v in the vocabulary V :
Likewise, each z d is distributed according to a categorical distri-bution with parameter vector  X  . This means that the prior distribu-tion on the value of z d is given by:
Since both  X  and  X  are latent variables in the model, we must select prior distributions for each. We choose the uniform Dirich-let distribution. This is also known as an uninformative Dirichlet prior and is achieved by setting the Dirichlet parameter vector (in our case  X  and  X  ) to all ones. The uninformative distribution was chosen because we assume that a priori nothing is known about the word distributions for each class, nor about the marginal distribu-tion of the document labels. As is common, we chose the Dirichlet because it is the conjugate prior for the Dirichlet.

Finally, it should be noted that the model includes an unexpressed parameter K , the number of mixture components. Other work has explored model selection [14] and automatic estimation of this pa-rameter using a Dirichlet process prior [15]. In this work we do not focus on this aspect of the problem. We instead choose a suitable constant value of K for each data set used for experimentation.
A collapsed sampler is one which eschews sampling all of the variables from the joint distribution. Instead, sampling is conducted over a simplified distribution from which all but the specific vari-ables of interest have been marginalized out. Specifically, the vari-ables which will not be sampled are marginalized from the com-plete conditionals of the variables that will be sampled. A complete conditional is the distribution of a single variable, conditioned on all of the other variables in the model.

In the case of clustering, the only specifically relevant variables are the hidden document cluster labels, z . There may be some reason to sample the values for the  X  matrix as well, but this is not strictly part of the clustering task.

In addition to the storage benefit of not sampling from uninter-esting variables, collapsed samplers have also been shown to con-verge relatively quickly because they contain fewer dependencies between sampled parameters and treat the marginalized parameters exactly [4, 9, 22].

Collapsed samplers cannot be used in all cases but have the lim-iting requirement that the variables to be marginalized out must be marginalizable in closed form. Fortunately, conjugacy between the Dirichlet and Multinomial distributions makes this possible for the model shown in Figure 1.

After marginalizing out  X  , and  X  , the complete conditional over the label z d for document d is: and  X  from the conditions in each term for the sake of brevity. The right-hand-side of Equation (1) can also be expanded by explicitly showing that z d = j giving: input : A set of documents D , and the number of desired output : A sample matrix Z for d  X  1 to M do end i  X  1 while More samples are needed do end In order to continue the derivation, it is necessary to have the marginal-ized joint distribution p ( z , w ) . Because of the conjugacy between the Dirichlet distribution and the multinomial distribution, this con-sists of a product of Euler beta functions B ( X ) : where we assume, for the purpose of computing counts, that z and that, also, and  X   X  dk  X  =  X  k  X  for all other k . In addition, n j is the number of times that the label j has been applied to any document, n number of times that word v occurs in a document labeled j , and d is the number of times that word v occurs in document d .
The derivation that produces this result is lengthy, and can be found in the Appendix. It follows a similar derivation for a re-lated model, as presented in [19]. After simplifying we arrive at the following expression for the complete conditional for the label of document d :
Using Equation 4, one may sample from p ( z d = j | w , z given the labelings of every other document. This complete condi-tional can now be used as part of a Gibbs sampling algorithm. The sampling algorithm used to conduct the experiments presented in this paper is shown in Algorithm 1.
Using the Gibbs sampling algorithm described in Section 4 yields a matrix Z of samples for the documents in D , such that z i label sampled for the d th document in D .
 In this section, we will discuss three issues as they relate to using Z to choose a clustering for D . 1. How long does it take to converge? 2. How many samples should be taken? 3. How should the collected samples be summarized?
In this section we show empirically the effects that various al-gorithmic choices have on cluster quality in the text document do-main. We also compare the quality of clusters produced with Gibbs sampling to that of clusters produced with EM. The experiments described here were conducted on three data sets labeled by topic. The first data set is the 20 Newsgroups set which has become a standard in classification and clustering research [12].
The second data set is a portion of the Enron e-mail corpus which has been annotated and made available through LDC [2]. This set consists of about 5000 e-mail messages from the Enron corpus, which have each been identified as being related to one of thirty-two possible topics.

The third data set is a corpus of web pages annotated based on user tags found on the popular collaborative bookmarking website del.icio.us [23]. Del.icio.us allows users to tag each site they book-mark with a set of arbitrary tags. For this data set, 50 topics were selected, and each was converted to a del.icio.us tag intuitively de-rived from the topic label. For example, one of the topics is  X  X at-ural language processing"; documents are assigned to this topic if they have been given the three tags  X  X atural",  X  X anguage" and  X  X ro-cessing". To be assigned to the topic  X  X pple", a page merely has to have been tagged with the single tag  X  X pple".

We designed a feature selection process to facilitate topical clus-tering. Before clustering, each document undergoes feature extrac-tion and vectorization. A list of stop-words were first removed from each document. Next, an unsupervised feature selection process was conducted in which the TF-IDF value for each word in each document was first calculated, then the 10 words in each document with the highest TF-IDF value were added to the feature set. Fi-nally, each document was converted to a feature vector where each element in the vector is the frequency with which a given feature occurs in that document.

To evaluate cluster quality, five metrics were chosen from the literature. These metrics are all external metrics, meaning that they require a reference, or gold-standard, partitioning of the data. These metrics are the F-Measure [20], Variation of Information (VI) [13], the Adjusted Rand Index (ARI) [11], the V-Measure [18], and the Q 2 metric [6]. These metrics will be used below in any ex-periments where the quality of various partitionings of the same data set are compared.
MCMC sampling techniques are guaranteed to converge in the limit to the target distribution, given some reasonable assumptions. However, because consecutive draws in the chain can be highly correlated, the samples from the beginning of the chain can be highly influenced by the random initialization state. To correct this, MCMC algorithms often include a parameter called  X  X urn" or  X  X urn-in", which specifies the number of initial samples that should be discarded from the beginning of the chain to reduce the influence of random initialization on the samples used for parameter estima-tion and inference.

Some authorities recommend general rule-of-thumb guidelines as to how many samples should be burned. Gelman [7] recom-sample. mends burning up to 50% of the collected samples, which is exces-sive when collecting samples is expensive, and when steady-state is achieved rapidly. Principled diagnostics have been proposed to help choose good values for burn [16]. These diagnostics are mostly suited for the case where the variables being sampled are continuous, and they do not handle categorical variables well.
Because the variables being sampled in our algorithm are all cat-egorical, we chose to avoid these formal diagnostics, and instead choose burn using likelihood time-series plots. Although there is no direct proof that this should be the case, we have found that the point at which this plot appears to approach an asymptote corre-sponds with the convergence of the chain to steady-state (c.f. [9]). Several examples of these plots are shown in Figure 2. After ex-amining several such plots for each data set, it was decided that the burn should be 1000 samples for the Enron data, 100 samples for the 20 Newsgroups data, and 50 samples for the del.icio.us data.
The model proposed in Section 3 does not contain any a priori knowledge of the clusters which are to be discovered through pa-rameter estimation and inference. The labels applied by the cluster-ing algorithm do not have any particular meaning before the clus-ters form, and are therefore completely interchangeable. For exam-ple, assume a data set with two distinct clusters, one consisting of documents about space exploration, and another consisting of doc-uments about dogs. The model does not distinguish the case where all of the space exploration documents are labeled 1 and all of the dog related documents are labeled 2 from the case where all of the space exploration documents are labeled 2 and the dog documents are labeled 1 . This is known as non-identifiability and can result in label switching during sampling.

Label switching is often a problem when clustering using MCMC techniques on mixture models. This is because it is possible for la-bel switching to occur mid-chain. In this event, averaging across multiple samples can be worse than taking any one individual sam-ple as the chain summary, because the meaning of each label can change across multiple samples. Solutions to the non-identifiability problem have been proposed. These often take the form of a re-labeling scheme [17, 21], and often feel slightly ad hoc .
We wanted to determine the extent to which label switching oc-curs using the collapsed sampler with our model. If within-chain la-bel switching occurs often, then any summarization technique that attempts to use more than one sample will require steps for label switching diagnosis and compensation to be effective. To test for label switching, we ran a chain on the Enron data set, collecting a total of 6000 samples. The first sample after burn was stored as a reference iteration and we computed estimates for p ( w | z ) given that sample. We used add-one smoothing to calculate this estimate: where n vj is the count of the number of times that word v occurs in a document where the label of that document in the current sample is j .

After that, for the i th sample after burn a new distribution p over words is estimated in the same way. Next, we compute the Kullback-Leibler divergence between p 0 ( w | h ) and p i possible combinations of h and j . These values were then scaled to the range [0 , 1] by dividing by the maximum and plotted in a quilt plot such that the square at square ( h,j ) is the normalized value of the K-L divergence between p 0 ( w | z = h ) and p i ( w | z = j ) .
This plot shows the similarity between clusters produced by dif-ferent samples of the same MCMC chain. For example, the plot of p against itself will obviously always have zeros (black squares) along the diagonal, and various other random values in the off-diagonal cells. If label switching occurs in a chain, however, we would expect that for larger values of i , the plot of p 0 will begin to have higher values (lighter squares) along the diago-nal. If labels h and j were to swap meaning, then we would expect ( j,j ) and ( h,h ) to have high divergence values, while ( j,h ) and ( h,j ) will both have divergence values close to zero.

The plots of p 0 against p 0 and p 0 against p 5000 for this exper-iment are shown in Figure 3. It can be seen that, even after 5000 samples, the diagonal elements are still close to zero. In fact, the differences between the two plots are minor, only noticeable un-der close scrutiny. This indicates that little, if any, label switching is occurring in this chain. This result is not unique to the Enron data set, nor to this particular run of the algorithm, though we omit presenting others because of space constraints. This result is likely the result of the high dimensionality of the data, together with the relatively high correlation between variables.
We summarized sample chains produced by the collapsed sam-pler in three ways. The first method is called the marginal posterior method, because it considers the posterior distribution over labels for each document independent of all other documents. To summa-rize with the marginal posterior method the label selected for doc-ument d ,  X  z d , is chosen to be the label which was most frequently assigned to that document during sampling: where  X  () is the Kronecker delta function, which returns one if its two arguments are the same and zero otherwise.

The second method is the MAP (maximum a posteriori ) sample method. This method takes the sample (a column vector Z  X  ,i Z ) with the highest posterior joint value as the selected partitioning of the data set. Since the priors specified in the model are uniform, this is the same as choosing the sample which maximizes the joint probability of the labeling and the data:
The final method is the random method. In the random method, selected partitioning of the data set.

In order to determine which of these methods produces the best clusterings, 100 chains were run for 80 hours each on each of the three data sets on Dell Poweredge 1955s with two Dual-core 2.6GHz Intel Xeon processors and 8 GB of RAM. Because of the imposed time limit, different numbers of samples were collected for each run, though chains for the same data set tended to have roughly the same number of samples. For each data set, we rounded the number of samples found in the shortest chain down to the near-est 10. Consequently, the Social Bookmarking data chains had 200 samples each, the 20 Newsgroups chains had 750 samples each, and the Enron chains had 7300 samples each. The three summa-rization methods were then used to cluster the data given these 300 MCMC chains. The number K of clusters in each experiment was chosen to be the same as the number of natural classes for each data set: 50 for the Social Bookmarking data set, 20 for the Newsgroups data set, and 32 for the Enron data set. The metrics for the 100 chains, across each data set and summarization method pair were averaged. The results are shown in Tables 1-3 and indicate that there is little difference in the performance of the summarization methods.
 Table 1: Results comparing the performance of the 3 summa-rization techniques on 100 chains of 200 samples each, pro-duced using the Social Bookmarking data set.
 Table 2: Results comparing the performance of the 3 summa-rization techniques on 140 chains of 750 samples each, pro-duced using the 20 Newsgroups data set.
To compare the clusters produced by the collapsed Gibbs sam-pler to those produced by EM, an EM algorithm on the same model was used to cluster each of the three data sets 100 times from ran-dom starting points. The average values of the metrics computed on the partitionings produced by EM are compared with the averaged Table 3: Results comparing the performance of the 3 summa-rization techniques on 100 chains of 7300 samples each, pro-duced using the Enron data set. metrics for the best summarizations produced by the Gibbs sampler in Tables 4-6. It can be seen that the metrics consistently indicate that the Gibbs sampling algorithm produces better clusterings than EM.
 Table 4: Results comparing the performance of the best con-figuration of the Gibbs sampling clustering algorithm to the performance of an EM clustering algorithm on the del.icio.us data.
 Table 5: Results comparing the Gibbs clustering algorithm to the EM clustering algorithm on the 20 Newsgroups data.
 Table 6: Results comparing the Gibbs sampling clustering al-gorithm to the EM clustering algorithm on the Enron data.
The experiments from Section 5.4 and 5.5 show that, in general, the differences between the performance of the various summariza-tion techniques is small compared to the performance of Gibbs ver-sus that of EM.

This fact, coupled with the relatively short times needed for the sampler to reach steady-state, suggests a simplified clustering strat-egy. Instead of letting the sampler converge and collecting many samples thereafter, it may possible to use the first sample after burn (a prespecified number of samples) as the clustering of the data. Table 7: Example contingency table showing the relationship of natural classes to clusters produced by the collapsed Gibbs sampler, using the 100 th sample as the summary for the 20 Newsgroups data.
 The experiments thus far suggest that this approach will likely yield good results, while requiring significantly less time and fewer re-sources.

To evaluate the potential of this simplified clustering algorithm, a new set of summarizations was generated for the samples collected in the previous experiments. These were single-sample summariza-tions that used samples at regular intervals as the clustering for the data set. For example, for the del.icio.us data set, this process was repeated starting at the 10 th sample, in increments of 10 samples up to the 200 th sample. For each data set, the metrics produced for the summarizations based on the i th sample of each chain were aver-aged together. The results of this process are shown in Figure 4.
These results support two points. First, they appear to indicate that the sampler really does converge to steady-state quite quickly, although perhaps not quite as quickly as the likelihood time se-ries plots suggested. Second, they show that, after a certain point, choosing a later sample will yield diminishing returns. For this particular data set, it appears that choosing the 200 th sample will not necessarily yield better results than choosing the 100 acceptable algorithm for this data set would be to run the sampler long enough to collect 100 samples, and then use the last sample as the selected clustering of the data.

When the absolute best clustering is desired, longer chains should be run and either the marginal posterior, or MAP summarization method should be used. However, the short-circuited strategy sug-gested here will require less time and yield competitive results.
The external metrics used here attempt to measure the similarity between partitionings of the data. While they do tend to be useful for comparing clustering algorithm variations on the same data set, there is no way to know whether a clustering is actually good based solely on these numbers. The fact that the Gibbs sampler described here produces clusterings with better external metrics than the EM algorithm does not necessarily mean that these clusterings make sense. It could be the case that the EM algorithm is producing very poor clusterings, and the Gibbs algorithm is producing only slightly less poor clusterings.

In order to verify that the clusterings produced are sensible, we examined contingency tables for several runs of the Gibbs sampling algorithm on the various data sets. For the most part, the algorithm appears to make sensible partitionings of the data, though they are data set. not necessarily the same as those present in the reference partition.
A randomly selected contingency table is shown in Table 7. The clustering contains many empty or small clusters, such as clusters 1, and 5. We have culled the majority of these very small clusters from the table. The bulk of the data have been placed into seven part, make a great deal of sense. For example, two of the clusters are fairly pure; cluster 2 is mostly about cryptography, and cluster 12 is mostly about medical issues. The other clusters consist of intuitive groupings of the remaining classes. Cluster 3 has to do with automotive and electronic hardware. Cluster 4 has to do with politics and sports. Cluster 8 deals mostly with computer-related topics. Cluster 9 is about space exploration and the middle-east and Cluster 16 is about religion.

Another interesting way to view the contingency table is to ex-amine how the classes end up distributed across the clusters. For example, the misc.forsale and sci.electronics documents are almost evenly split between the automotive and computer clusters. This makes sense because computer equipment and automobiles are the most likely products to be sold on newsgroups, and these are the types of forums where discussions of electronics would be most prevalent. Also interesting is the way that the talk.politics.mideast class is divided into a politics cluster, a religious cluster and a clus-ter about space and the middle east.

In general these trends indicate that the partitions are of good quality, perhaps even better than the metric scores would suggest. Although the cluster labels do not match the gold-standard per-fectly, the clusters appear coherent and mostly correspond to a valid way of organizing the documents, one that a human annotator might settle on, if the newsgroup labels were withheld.
The experiments show a great deal of promise for using MCMC methods over EM for clustering documents with a mixture of multi-nomials model.

The absence of label switching shows that summarization meth-ods that use more than one sample in their summary (like the marginal posterior method presented here) can be quite effective in this do-main. This simplifies clustering algorithms that use MCMC on a mixture of multinomials for clustering. However, it does indicate that the sampler is not sampling from the entire distribution, as that would necessarily involve a certain amount of label switching. This is most likely because of the large number of variables present in the model, together with a relatively high amount of correlation be-tween those variables. This failure to explore other modes would not be acceptable for applications for which true estimates of the posterior distributions are needed. In the case of document cluster-ing, however, the true objective is to maximize the quality of the partitions produced by the algorithm, not to maximize the accuracy of the posterior distribution estimates.

It appears that the marginal posterior summarization method was generally superior to the two single-sample summarization tech-niques, except in the case of the Enron data set where the results were mixed, as can be seen in Table 3. It is possible that this is due to the smaller number of documents in the Enron data. The resulting longer sample chains might have allowed the sampler to find MAP samples that were closer to the true MAP values. Al-though not optimal, much simpler summarization techniques yield partitionings of comparable quality.

In summary, we have found the following guidelines that practi-tioners may consult as they create their own sampler-based cluster-ing algorithms: 1. The point at which likelihood plots approach an asymptote 2. Little or no label-switching occurs with our model and data. 3. Once samples have been collected, there is little difference 4. The Gibbs sampling algorithm presented here consistently 5. Longer chains, together with either the marginal posterior, or
We were able to compare MCMC and EM in their more basic forms, but it would be informative to evaluate how other variants of these algorithms that are used in practice (e.g. variational EM, and EM combined with local search techniques [5]) perform in com-parison.

In addition, while it is likely that the results presented here gen-eralize to a broad class of situations where similar models are used on similarly sized data sets, it is important to evaluate how well they extend to more complex models, such as LDA.

Scalability is also an issue that needs to be addressed, as some real-world data sets can be orders of magnitude larger than those used for experimentation here. It is unknown whether the results shown here scale to such large data sets.

Finally, an approach that combines EM and MCMC might per-form better than either technique in isolation. One approach would be to first collect a limited number of samples using an MCMC sampler, and then to use a summary of these samples as a starting point for EM. This approach would leverage the exploratory power of a sampling-based approach to find a more promising starting configuration to begin maximizing with EM. That is, instead of choosing a random hill to climb, MCMC helps find a hill that X  X  likely to be one of the higher hills to climb.
We would like to thank Dr. Shane Reese and Robbie Haertel for their feedback and help related to the techniques and mathematical derivations presented in this paper. [1] A. Banerjee and S. Basu. Topic models over text streams: A [2] M. W. Berry, M. Brown, and B. Signer. 2001 topic annotated [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet [4] M.-H. Chen, Q.-M. Shao, and J. G. Ibrahim. Monte Carlo [5] I. S. Dhillon, Y. Guan, and J. Kogan. Iterative clustering of [6] B. Dom. An information-theoretic external cluster-validity [7] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. [8] S. Goldwater and T. L. Griffiths. A fully bayesian aproach to [9] T. L. Griffiths and M. Steyvers. Finding scientific topics. In [10] A. Haghighi and D. Klein. Unsupervised coreference [11] L. Hubert and P. Arabie. Comparing partitions. Journal of [12] T. Joachims. A probabilistic analysis of the Rocchio [13] M. Meil  X  a. Comparing clusterings X  X n information based [14] M. Meil  X  a and D. Heckerman. An experimental comparison [15] R. M. Neal. Markov chain sampling methods for Dirichlet [16] A. E. Raftery and S. M. Lewis. Implementing MCMC.
 [17] S. Richardson and P. J. Green. On bayesian analysis of [18] A. Rosenberg and J. Hirschberg. V-measure: A conditional [19] M. M. Shafiei and E. E. Milios. Latent Dirichlet [20] M. Steinbach, G. Karypis, and B. Kumar. A comparison of [21] M. Stephens. Dealing with label switching in mixture [22] Y. W. Teh, D. Newman, and M. Welling. A collapsed [23] D. Walker and E. Ringger. New social bookmarking data set. [24] S. Yu. Advanced Probabilistic Models for Clustering and [25] J. Zhang, Z. Ghahramani, and Y. Yang. A probabilistic model The derivation uses the following variables: Y
Next, integrate out  X  and  X  . p ( z d |  X  )
Expand the nested product Y and then group like terms |  X  )
Expand the multinomial and Dirichlet distributions (  X  )
B this is possible because  X  , because they are products over the same quantities, in different orders.

Z ...
 integrals, leaving )  X  B  X  K (  X  )
