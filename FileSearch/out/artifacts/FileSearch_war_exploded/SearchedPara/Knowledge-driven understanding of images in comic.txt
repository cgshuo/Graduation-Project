 ORIGINAL PAPER Christophe Rigaud 1 , 2  X  Cl X ment Gu X rin 1  X  Dimosthenis Karatzas Jean-Christophe Burie 1  X  Jean-Marc Ogier 1 Abstract Document analysis is an active field of research, which can attain a complete understanding of the semantics of a given document. One example of the document under-standing process is enabling a computer to identify the key elements of a comic book story and arrange them according to a predefined domain knowledge. In this study, we propose a knowledge-driven system that can interact with bottom-up and top-down information to progressively understand the content of a document. We model the comic book X  X  and the image processing domains knowledge for information consistency analysis. In addition, different image process-ing methods are improved or developed to extract panels, balloons, tails, texts, comic characters and their semantic relations in an unsupervised way.
 Keywords Document understanding  X  Comics analysis  X  Expert system Comics or  X  X andes dessin X es X  represent an important part of the cultural heritage of many countries, especially in the USA [ 23 , 42 ], Western Europe (particularly France and Bel-gium) [ 41 ] and Japan [ 26 ]. Unfortunately, they have not yet received the same level of attention as music, cinema or liter-ature in terms of their adaptation to the digital format. Using information technology with classic comics would facilitate the exploration of digital libraries [ 3 ], assist translators [ 5 ], provide a tool for augmented reading [ 26 , 50 ], speech play-back for the visually impaired [ 6 , 39 ], story analysis, etc. Nevertheless, the process of conversion and adaptation is not as simple as for films and novels. The comic differs from the latter in that the media itself is intimately linked to the medium. It is defined as juxtaposed sequences of image by McCloud [ 34 ] and Thomas [ 59 ]. The frame of the page, its size and its organization matter to the author who can use them to tell his story the way he wants to. As opposed to books or movies, changing the medium of a comic books might change its artistic dimension, resulting in a potential betrayal of the author X  X  original intention.

Ideally, based on an understanding of the process used by the authors to draw the paper version, the comics would auto-matically be changed into a form adapted to the medium in which the work is to be read (e.g. smartphone, web page and 3D book). This would begin with an analysis of the digitized paper page to extract the different content elements. Then, the story would be reconstructed by placing the extracted ele-ments in the narrative order, through an understanding of the role of each panel in the story, and the association of speech balloons and the comic characters (protagonist) 1 to whom they are addressed, etc. (Fig. 1 ).

In this paper, we address the problem of understanding comics content by automatically improving the consistency of the extracted elements according to the domain knowl-edge and by inferring new information and launching further processing iteratively. Low-level and high-level information is used in a complementary and unsupervised way. We pro-gressively identify the different elements of a digitized comic book image and retrieve their relations. To do so, we concep-tualized and formalized the underlying semantics related to their positions in the image. Another novelty lies in our pro-posal for improving the state-of-the-art methods for panel, balloon, tail, text and comic character detection and descrip-tion to take advantage of this framework.

The proposed framework is based on interactions between low-and high-level processing in order to reduce the chal-lenging semantic gap. In this paper, we call each image processing algorithm an  X  X xtractor X  of panels, balloons (or bubbles), tails and comic characters according to the type of region they are designed for. Extractors provide a set of regions that feed a knowledge base. The knowledge base is associated with rules that form an ontology. The main diffi-culty is to extract but also to model the diversity of styles, format, definition and the differences between design and printing techniques (Fig. 2 ).

What we called our  X  X xpert system X  uses the ontology to assert the relations between regions, according to the formal-ized knowledge. In addition, an inference engine is used to deduce new pieces of information in order to perform further image processing (e.g. hypotheses of the regions of interest for the localization of comic characters or speech balloons). Please note that the term  X  X xpert system X  is used here to name our global reasoning software, not as a reference to the rules-based deduction systems developed a few decades ago in the literature [ 16 ].

The rest of the paper is organized as follows. The related work for holistic document understanding and comic book analysis techniques is summarized in Sect. 2 . The proposed framework is described in Sect. 3 . Subsequently, the models, the low-level processing and the interactions between them are detailed in Sects. 4 , 5 and 6 , respectively. The datasets used to evaluate our method, the evaluation protocols and the final results are shown in Sect. 7 . Finally, results are discussed and conclusions are drawn in Sect. 8 . In this section, we provide an overview of work on the holistic understanding of documents and then provide the state-of-the-art studies related to an analysis of the literature on comic book images. 2.1 Holistic document understanding One of the original goals of image document analysis was to fully understand the content of any images [ 29 ]. The mean-ing of the term  X  X nderstand X , when it comes to machines, can be taken as their ability to recognize graphical elements and to interpret the semantic of their features. This requires solving several sub-tasks simultaneously, for instance region detection, labelling of meaningful regions and semantic interpretation using layout analysis. In the past, researchers have often developed classifiers for tackling each of these sub-tasks independently [ 33 ]. However, these sub-tasks can help each other. For instance, in a comic book page, if we know the panel positions, then we can make a better guess about the location of the comic characters (they are usually in the panels). It is not easy to combine different related sub-tasks. Previous work concerned real scene image analysis [ 4 ], retrieval [ 7 ] and understanding [ 11 , 30 ], med-ical image annotation using description logic and inference engine [ 21 ], object-based image retrieval [ 36 , 49 ] (between keyword-based and query-by-example) and image interpre-tation [ 22 , 37 ]. Most researchers agree on the importance of using the underlying semantic of topological information, spatial distances, directional relative position and complex relations such as  X  X etween X ,  X  X urround X  and  X  X mong X , to obtain an exhaustive description list of the contents for a given context.

Recently, images from comic books have been also con-sidered. In [ 17 ], a semantic annotation tool made use of previous knowledge and consistency information to suggest new knowledge to the user in an interactive way. Spatial infer-ences have been used to infer the reading order of comic books, at the level of panels in each page and balloon in each panel [ 12 ]. The benefit of using contextual informa-tion of a simple object to build more complex ones has been highlighted [ 7 ]. To our knowledge, there is no framework for comic book images understanding in the literature that infers new knowledge iteratively and without user interac-tion (unsupervised). 2.2 Analysis of comics Images in comics are mixed content documents that are processed differently depending on the type of content that we are interested in. The techniques involved can vary a lot depending on whether we focus on panels, balloons, text or processing comic characters. We will review each of these in the next four paragraphs.

Panels Panel extraction and ordering have mainly been studied for panel to panel reading. This kind of require-ment has been continuously increasing in parallel with the evolution of mobile devices. Readers want to have their favourite comics or mangas on the go, while carrying min-imum weight. Printed comics need to be manually scanned and split into screen size parts small enough to avoid zooming and scrolling, which is tedious.

Several techniques have been developed to automatically extract panels [ 24 ], assuming that panels are small enough elements to be comfortably read on mobile devices. Most of them are based on white line cutting with Hough trans-form [ 9 , 31 ], recursive X-Y cut [ 15 ] or density gradient [ 58 ]. These methods do not consider empty area [ 24 ] and border-free panel. These issues have been corrected by connected component approaches, but these approaches are sensitive to regions that sometimes connect several panels, which potentially increases the detection error rate [ 1 , 47 ]. Another approach based on morphological analysis and region grow-ingcanremovesuchconnectingelementsbutatthesametime creates new holes in the panel border [ 18 ]. After region seg-mentation, heuristic filtering is often applied to classify panel region according to their size ratio with the page size [ 2 , 18 ]. More recently, new methods have shown interesting results for manga and European comics with different background colours. They are based on watershed [ 40 ], line segmentation using Canny operator and polygon detection [ 31 ], and region of interest detection [ 53 ] such as corners and line segments.
Balloons Balloons or bubbles are key elements in comics; they link graphic and textual elements and are part of the style of comics. They can have various shapes (e.g. oval and rec-tangular) and contours (e.g. smooth, wavy, spiky and absent). Closed speech balloon have been studied in the main, based on region detection and filtering rules [ 2 , 19 ]. Our group developed a method to extract open (non-closed) balloons based on active contours initialized around text areas [ 45 ] and an initial approach to classify balloons according to their contour styles [ 43 ].

A balloon is a spatial container of information that is related to a protagonist using a specific element: the tail. The tail is often represented by a discontinuity on the con-tour of the balloon towards the concerned protagonist. To our knowledge, there is no work on tail position and direction detection in the literature, but we have stressed its interest for improving balloon classification [ 43 ].

Text Text extraction and recognition have attracted a lot of attention in complex analysis domains such as real scenes, cheques, maps, floor plans and engineering draw-ings.However,fewcontributionsconcerncomics.Bottom-up approaches use connected components, which often rely on a segmentation step [ 40 ]. Su used Sliding Concentric Windows for text/graphic separation and then used mathematical mor-phology and an SVM classifier to classify text from non-text components [ 54 ].

We have previously developed an adaptive binarization process based on minimum connected component threshold-ing followed by a text/graphic separation based on contrast ratio and text line grouping [ 46 ]. Li proposed an unsuper-vised speech text localization for comics based on training a Bayesian classifier on aligned connected component and then detecting the rest of the text using the classifier for text/non-text separation [ 32 ].

Top-down approaches starting from speech balloon (white blob) detection followed by mathematical morphology oper-ations have been proposed by Arai [ 2 ] and layout analysis by Yamada [ 61 ].

Comic characters Human body and face detection in real scene images has progressed a lot during the last decade. Nevertheless, the few studies that are related to comics showed that human detection techniques are not appropriate for comic book characters. Those studies concerned exact and partial copies of mangas [ 55 ], cartoon classification [ 27 ] and the detection of the main characters using redundancy information [ 20 , 56 ]. We have recently developed a query by example approach based on a colour descriptor [ 44 ]. In this section, we introduce an unsupervised framework for understanding comic book images, based on our expert sys-tem combining a knowledge base and reasoning capabilities. We also detail its application in a concrete situation. 3.1 Expert system The purpose of our expert system is to interact with the low level (image processing) iteratively to progressively under-stand the content of an image, moving from simple to more complex elements. This approach is similar to [ 7 ] except that in our case the definition of the complex object is not a com-position of simple objects but context-driven.

In our system, the expert system includes two models, one formalizing the raw data from algorithms ( Image model ) and the other modelling the domain knowledge of the comic books ( Comics model ). These two models are ontologies that work together to express the relations between the pri-mary elements of a comic book image that can be considered as being stable through all instances of the studied domain (Fig. 3 ). Thus, the expression of the constraints applied both to the elements and their relations have to be specific enough because these constraints will be considered as the refer-ence knowledge for the detection of potential errors of the low-level extraction algorithm output. The main advantage of separating image knowledge from the application domain knowledge is that it makes this framework more easily adapt-able to other applications by replacing the second ontology only.

The low-level algorithms are designed to extract spe-cific information from the whole image or a specific region. Low-and high-level systems interact in a loop to feed the knowledge base until there is a complete and consistent understanding of the document, according to the knowledge domain.

In Fig. 4 , the starting point is step 1 (formulate hypothesis) where low-level processing gives basic information to the expert system. Then, step 2 (validate hypothesis) assesses the valid elements and removes obvious mistakes, and in step 3, we infer new information based on the previous information and the knowledge base. In the next iteration of the process loop, the newly validated information can be used by low-level processing (e.g. new parameters and region of interest) to extract more complex elements from the image and so on. This loop can potentially be run as many times as new information is discovered, depending on the application and the amount of information to be retrieved. In our case, the process is made of two iterations as described in Sect. 3.3 . 3.2 Knowledge representation The domain knowledge is conceptualized though the catego-rization of each element composing a page, combined with a set of topological relation with these elements. In our con-text, the elements that compose a given image I are panels P , balloons B , tails Q ,text T and characters C ,aswellastheset of topological relations between them. Because comics, as an art form, do not follow any strict specifications, it is really hard to build a perfect model, which is valid for all kinds of comics. There are some instances of comic books without balloons or without panels. If webcomics are also considered, then a comic is not even necessarily composed of pages. A model that would be true for every type of comic book would be too general to be of any use in this work. Instead we define a general comic book model with more constrained proper-ties that represent a large subset of comics (Franco-Belgian, Japanese, American). We conceptualized the general prop-erties of a comic book image layout as follows:  X  A panel P is related to one and only one comics page  X  A balloon B is related to one and only one panel  X  A character C is related to one and only one panel  X  A same character can appear only once in a panel  X  A text line T is related to one and only one balloon B
Despite the fact that authors are entirely free in their layout choices, some researchers insist a few conventions, widely adopted by comic book X  authors, be respected to avoid the reader being confused [ 8 , 28 ]. The depicted elements and their place in the layout must be clearly identifiable at first sight, meaning, for instance, that balloons and characters should be included inside panels. Whereas one can find some instances of balloons breaking out of their frame, these are usually kept to a minimum.

Therefore, the term  X  X elated X  refers to the situation where an object is overlapped (a fortiori, contained) by another over a significant proportion of its surface. In the case of multi-ple intersections, only the smallest container is considered. Whentheelementisfullycontainedinseveralotheritems,the smallest container is consequently the direct container (e.g. a text line must be considered as being included inside a bal-loon before being included inside the panel containing that balloon). Considering the lack of accurate numbers on the overlapping ratio of comic book X  elements in the literature, we estimated statistically what this significant proportion would be using the eBDtheque dataset [ 13 ]. Figure 5 shows the percentage of panels, balloons, text lines and characters that fit the enumerated constraints, as a function of their cov-ered area.

Considering ideal proportion covered for each type of ele-ment, we obtained the following overall scores: 99.6% of the panels, 87.4% of the balloons, 81.6% of the text lines and 94.9% of the characters are in accordance with the assumed constraints in the eBDtheque dataset. The lowest score was obtained for the text lines and was predictable as our cor-pus integrates ten pages with old-fashioned text captions, the concept of speech balloon being not invented yet at the time of their publication.

Then, we introduced some refinements for balloons, text lines and characters, respectively, named speech balloon SB , speech text ST and speaking characters SC . Their definition is based upon the formalization of the semantic carried by the following spatial configurations:  X  X  SB is a balloon B that has a tail and contains text  X  X  SC is a character C pointed by a tail  X  X  ST isatextlinewhichisincludedinonespeechballoon
The term  X  X ointed X  refers to the fact that the character is included in the part of the panel indicated by the tail. To have a better understanding of how a panel is divided according to tail direction, please refer to Sect. 6.2 .

These figures represent the theoretical limit that can be reached with the model in terms of component extraction performance. 3.3 Processing sequence The expert system asserts the extraction of simple elements such as panels, texts, balloons and tails in order to infer speech balloons before searching for comic book charac-ters (which are much more complex), based on the context defined by the simple elements and their relations. This can be achieved through the two iterations of the process loop as shown in Fig. 4 . These sections are detailed below.
Iteration 1 X  X tep 1 (hypothesis) The initial extraction of panels, text and balloons feeds the knowledge base. In Fig. 6 , dashed elements represent the initial hypotheses. Note that extraction errors can take place at this stage which the system can recover from at a later stage.

Iteration 1 X  X tep 2 (validation) Subsequently, the expert system checks whether their spatial relations (context) match the topological properties of the knowledge base defined in Sect. 3.2 ; otherwise, it solves them using the domain knowl-edge as described in Sect. 6.1 . The result is illustrated in Fig. 7 .

Iteration 1 X  X tep 3 (inference) From the validated infor-mation, the expert system infers the specification of some balloons and text lines into speech balloons and speech text lines, with respect to the formalized semantic in the knowl-edge base (Fig. 8 ).

Iteration 2 X  X tep 1 (hypothesis) This step is the beginning of the second iteration of the process as illustrated in Fig. 4 . At this point, the expert system already has some information about the content of the image from which a further hypoth-esis can be made concerning complex elements such as the region of interest (ROI) of the characters from the speech balloon positions (Fig. 9 ). The ROIs defined by the expert system are given as seeds to the image processing algorithm (extractor of characters) which in turn feeds the expert system with more precise character locations (Fig. 10 ).

Iteration 2 X  X tep 2 (validation) The expert system checks whether the spatial relations of the characters C match the properties of the knowledge base defined in Sect. 3.2 (Fig. 11 ).

Iteration 2 X  X tep 3 (inference) The expert system infers which characters are speaking SC (Sect. 6.2 ) and link them to the corresponding speech balloons, which have already been linked to speech text regions in Iteration 1 X  X nference step. (Fig. 12 ).

At the end of the two iterations, we obtained both a topo-logical and a semantic description of the image content, illustrated here in a single graph. Further iteration could be processed by extracting other low-level elements such as faces or vehicles and by adding extra knowledge in the knowledge base. Ourknowledgebaseiscomposedoftwoontologies,designed using OWL X  X  W3C recommendation [ 35 ], and interacting with each other (Fig. 3 ). The first one was used to model the raw data provided by image analysis algorithms (called image model hereafter), while the second formalize the con-ceptualization of the comic book domain knowledge (called comics model ), described in Sect. 3.2 . These two models are bounded by bridges that are used to perform reasoning over both models, using their own properties.
The comics model formalizes the introduced elements (panels, balloons, text lines and characters) into OWL con-cepts. The relations (affiliation and association) between them are represented by object properties. Their respec-tive range and domain are set accordingly to the constraints introduced in the layout conceptualization to detect potential extraction errors. Additionally, the concepts of  X  X peech Bal-loon X ,  X  X peech Text X  and  X  X peaking Character X  are derived from their respective super concepts via an OWL translation of the rules we defined earlier.

The image model is for the main part composed of the concepts of Image , Region Of Interest (ROI) and Extractor . A region of interest being a part of image identified by an extractor as a relevant piece of content for some applica-tion. In the present case, an image is identified as a comic book X  X  page and a region of interest as one piece of con-tent (e.g. a panel). This equivalence is translated through the OWL axiom  X  X quivalent class X . This provides a way to handle comics elements as regions in the image and allows reasoning processes on their shape and position. Low-level processing is used to extract information from the image without taking into account the context unless it is given from high-level processing. Panel, balloon, text and comic character extractions we used are methods from the literature that we improved or adapted for this work. As far as we know, tail detection has not been studied before. Here, we propose an initial method based on balloon contour analysis. 5.1 Panel extraction We propose to combine the advantages of the already pub-lished panel extraction methods [ 2 , 47 ] and partially solve their weaknesses using the expert system model. They are connected component-based methods that are reliable for comics with disconnected panels (separated by gutters). Line-based decomposition methods are the most efficient methods for line separated comics [ 32 ] (no gutter). A generic method that works best with both styles is not easy to imple-ment. In our system, we combine [ 2 , 47 ] to introduce a new method that is especially suitable for general comics using gutter (white space) between panels.

In [ 2 ], the authors considered panels as being connected components (closed frame). They propose to extract black connected components from a binary image and use heuris-tic filters to prune non-panel candidates. In their approach, the binary image is produced from a fixed thresholding of the grey version of the image. In [ 47 ], we have showed the benefits of using a global and adaptive thresholding method (like Otsu X  X  approach) to extract connected compo-nents corresponding to panels. In this approach, we clustered the components into panel (big), text (medium) and noise (small), but the clusters are interdependents.

So we reused global thresholding [ 47 ] to extract con-nected component similarly to Arai [ 2 ] and propose a novel topology-based filtering process to retrieve connected components corresponding panels. The global thresholding method separates page background, expected to be clear, from its content considered as foreground here (dark ele-ments such as black strokes), see Fig. 13 b.

Note that local thresholding approach is not used here because it has higher chances to split the panel border and over-segment its content. Then, the connected com-ponents from the binary image are extracted, and their topological relations are analysed to filter out non-panel com-ponents [ 57 ]. One of the particularities of the panels is that they are usually not included in other elements. Therefore, we focus only on the external (outermost) connected compo-nents (Fig. 13 c). Note that we filter out small components in order to avoid considering isolated elements (e.g text, logo and page number) as panel (Fig. 13 d). Finally, the con-vex hull or the bounding box of the panel contours can be computed in order to recover from discontinuous contour detection (Fig. 13 e, f).
 The evaluation of the proposed method is shown in Sect. 7.1 . 5.2 Balloon extraction Of the three methods in the literature, two of them are based on text position, which relies on the text extractor perfor-mance [ 19 , 45 ]. We did not use these methods because we preferred to process low-level information independently in order to avoid error propagation. Arai X  X  method does not use text position but connected component filtering based on heuristics (fixed thresholds for binarization, blob size, number of white pixels, number of contained straight lines and width to length ratio) [ 2 ]. We developed a more adaptive method that can handle various styles of comics (e.g. colour, B&amp;W), image formats (e.g. A4, A5, portrait, landscape) and digitization variations (e.g. scanning device and camera cap-ture). Fixed thresholding such as those used in [ 2 ] is a strong assumption and only work for comics with the same style and digitized under the same light condition as already men-tioned in Sect. 5.1 . We relaxed this constraint by using the Otsu threshold selection method to make a binary segmenta-tion of the greyscale image [ 38 ]. Binary segmentation creates a lot of small connected components on textured images. This was pruned using [ 47 ] (minimum average height class). We then analysed their parent X  X hild relationship to make a first selection of candidate components before analysing the spa-tial organization of the set of children. We defined the set of children CH ={ ch 1 , ch 2 , ..., ch n } and compute a confi-dencevalueforeachparentfromtheset P ={ p 1 , p 2 , ..., p thatservesasfinalfilteringcriterion.Theprocessisillustrated in Fig. 14 .

We assume that a balloon has a minimum of chil-dren minNbChildren which are horizontally or vertically aligned and centred inside the balloon region (property of text in comics). The percentage of alignment align is com-puted for each child ch i considering CHA ( ch i ) the subset of children that are aligned to ch i ,Eq. 1 . align ( ch i ) = where | CHA ( ch i ) | is the number of aligned children and n the total number of children.

For instance, if we consider two children E and F ,they are considered as aligned (vertically) if the following con-ditions are verified centroidF x  X  X  minE x , maxE x ] and centroidE x  X  X  minF x , maxF x ] where min x and max x are the left and right limits on the horizontal axis. The child F is also considered to be aligned (horizontally) to E by chang-ing x to y , which become the top and bottom bounding box limits.

The difference of coaxiality is computed for the horizontal ( d x ) and the vertical ( d y ) axis from the Euclidean distance between the parent and children hull centroids (Fig. 15 ). Both differences d x and d y are normalized between zero and one as a percentage of the balloon (parent) width and height, respectively. The average of the alignments, align , d x and d y gives a confidence value for each candidate of P (Formula 2 ).
 C
The confidence value was used for balloon and non-balloon separation in the experiment Sect. 7.3 . 5.3 Tail detection Speech balloons indicate dialogue, sometimes using a tail pointing at their respective speakers [ 48 ]. The tail is usu-ally represented by a discontinuity on the balloon contour (Fig. 16 ). We proposed an initial tail extractor of tail tip (extremity) and tail direction detection based on balloon con-tour analysis. We limited this study to the tail types that can be considered as an extension of the inside region (back-ground) of the speech balloon, namely  X  X omma X ,  X  X igzag X  and  X  X bsent X  (no tail) because they can all be extracted from the segmentation of the balloon background (Fig. 16 ). Other types require a specific study based on the speech balloon extraction and considering contour strokes and surround-ing elements. Note that tail direction was named after the eight cardinal directions (north, north-east, east, south-east, south, south-west, west and north-west). This facilitated fur-ther processing and evaluation.

A tail can be decomposed into four elements: origin, path, tip and pointed direction. The contour of a speech balloon is mainly convex, except in the region where the tail is con-nected to the balloon (origin) which produces the highest convexity defects. If the speech balloon has no tail, there is no particular convexity defect. We based our approach on the analysis of this specific region (where the highest convexity defects are). A convexity defect is defined by a triangle from one segment of the balloon convex hull to the farthest point on the balloon contour (Fig. 17 ). The set F ={ f responding hull segments S ={ s 0 , s 1 , ..., s n } where n is the number of hull segments. We defined the top two far-thest points f a and f b corresponding to the convex hull segments s a and s b , as the coordinates of the tail origin (Fig. 17 ).

Most of the time, the tail tip corresponds to one of the vertices of the convex hull. We defined the set of vertices V ={ v comparing five features that correspond to the Euclidean dis-tances:  X  ds a the distance to the segment s a  X  ds b the distance to the segment s b  X  dc the distance to the centre of mass c of the balloon  X  df a the distance to the tail origin f a  X  df b the distance to the tail origin f b
This can be formulated as: v  X = argmax max ( dc + df where v  X  is the optimal vertex from the set of vertices V . Together with the position, we compute a confidence value C tail related to the mean depth of f a and f b over the mean balloon size meanBalloonSize (Formula 4 ).
 C where depth ( f x ) is the depth of the corresponding defects (e.g. d a and d b in Fig. 17 ) and meanBalloonSize is defined in Eq. 5 . meanBalloonSize = where Wb i and Hb i correspond to the width and height of a speech balloon i and n the number of speech balloons in the image.

Once we found the tail tip, we analysed its neighbourhood to find the orientation and direction of the last part of the tail which is directed towards the speaking character. We com-puted a M x M square region centred on the tail tip position in the balloon mask and used a line fitting algorithm (weighted least-squares) that returns the representative line that mini-mizes the distance between all the points of the local region (Fig. 18 ).

The tail direction was computed from the tail orientation that has initially two possible directions. We defined the tail direction from the closest point of the representative line to the tail origin called fitPt , to the tail tip ( 5.4 Text extraction In our method, the expert system uses the text information to deduce which balloon is actually a speech balloon (Sect. 3.2 ). Several text extractors have been reported in the literature, both for Latin and syllabic scripts. We used our own method because its ability to handle a large variety of comics has already been demonstrated [ 46 ]. In this work, although our aim was not to recognize text, we nevertheless used an optical character recognition (OCR) system to improve the quality of text localization by filtering out regions where no alphanu-meric symbols were recognized. The transcription obtained was stored in the ontology for future processing. Instead of training an OCR for a specific language and font like [ 40 ], we preferred to use Tesseract OCR, which has already been trained with several fonts and languages [ 52 ]. 5.5 Character extraction Unsupervised comic book character extraction becomes a very difficult task when processing heterogeneous comic styles such those in the eBDtheque [ 13 ] dataset (Fig. 2 ). In this context, learning-based approaches [ 27 , 55 ] cannot han-dle such dimensionality induced by the difference of styles. We previously developed a method that learns a specific char-acter based on a single user-defined example [ 44 ]. In this study, we decided to go one step further by detecting comic character positions without requiring user interaction (unsu-pervised). We propose to ask the expert system to  X  X uess X  a ROI where comic characters might be according to the other elements in the panel (ignoring image features) and then apply low-level processing to refine the ROI using image features (Fig. 19 ).

As introduced in Sect. 6.3 , the hypotheses of the ROI of comic characters were given by the expert system using a combination of low-and high-level information. The role of the comic character extractor here is to refine the ROI proposed by the expert system in order to better fit the posi-tion of the character in the image (using image features). This is an optimization problem where we want to maxi-mize the ratio of pixels that belong to the comic character over those that belong to the background in the guessed ROI (from expert system). At this point, deciding whether or not a pixel is part of a character is difficult. Instead, we preferred to work at region level using closed regions (regions fully surrounded by a black stoke) and deciding whether or not a region belongs to the character. We considered a region to be part of the character if most of its pixels were con-tained by the predicted ROI. This method excludes wide region pixels where most of the pixels are out of the ROI such as background regions and completely includes small regions that were partially included. It can be applied to most of the comic book styles and works best when the char-acters are entirely overlapped by the ROI hypothesis with a uniform background (Fig. 20 ). Note that speech balloon and text regions were ignored because we did not expect a character to intersect with those regions as discussed in Sect. 3.2 . This section presents the low-and high-level process-ing interaction to validate, infer information and generate hypotheses about image content. 6.1 Validation of the extractions In order to validate the extraction of the comic page X  X  compo-nents, we made sure that the extracted panels, balloons and text lines were in accordance with the knowledge conceptu-alized in Sect. 3.2 .

Firstly, each item was loaded into the model as it was labelled (page, panel, balloon, text line or comics character) by the low-level processing step. Then, each element was linked to its smallest container (as defined in Sect. 3.2 )in a half-blind way. That is to say that the type of contained element was known, while the type of potential container was not. Not knowing the type of container might have led to incorrect assertions that would have produced inconsis-tencies in the model. Those inconsistencies are the result of possible mistakes made during the extraction process that were filtered out in order to improve the overall detection pre-cision. Consistency checking was performed over the model and inconsistencies were handled one after the other. We observed that the misclassification of elements (e.g. a bal-loon labelled as a panel) was not the greatest weakness of the current algorithms. Therefore, we chose to focus on increas-ing the extraction precision by filtering out the elements that did not fit the constrained model. This caused an inevitable drop of recall that will be addressed in the upcoming works. For the time being, our system can handled, without being limited to, the following inconsistencies:  X  A page (p) contains a balloon (b), a text line (t) or a  X  A panel (p1) contains a panel (p2) or a text line (t) :p2  X  A balloon (b) contains a panel (p) : if p contains some  X  A balloon (b1) contains a balloon (b2) : if b1 does  X  A balloon (b) contains a character (c) : c is deleted.  X  A text line (t) contains a panel (p), a balloon (b) or a  X  A text line (t1) contains a text line (t2) : if t1 contains  X  A character (c) contains a panel (p), a balloon (b) or  X  A character (c1) contains a character (c2) :c2is 6.2 Inferences from the low-level information The expert system is able to infer more specific information than that given by the extractors. For instance, it can deduce which text is spoken or not and which speaking character pronounces the content of which speech balloon.

Speech balloon and speech text The classification of bal-loons and text, respectively, into speech balloons and spoken text can be done by running an inference engine on the model (e.g. Racer [ 14 ] or Pellet [ 51 ]). To allow reasoning, the model has to be consistent, which is the case after validation step 6.1 when all inconsistencies have been resolved. In Sect. 3.2 ,we defined a speech balloon as a balloon that has a tail. In oth-ers words, the concept of a speech balloon can be seen as a specialization of the balloon concept that extends all its prop-erties plus adding a few new ones. This is expressed in the expert system by defining a property hasTail with the con-straint that must have a speech balloon instance as a source. Assessingthispieceofknowledge,thereasonerwillautomat-ically deduce that each instance of balloon extended with a hasTail property can be specialized into a speech balloon .
In a similar way, the concept of text line subsumes the concept of spoken text line . It is rendered equivalent to the set of individuals from the text line concept that is bound with the property isLineOf , which is the inverse property of hasLine ,toa speech balloon . In other words, the text lines marked as being part of a speech balloon are automatically classified as spoken text lines.

Speaking characters Among the validated characters (Sect. 6.1 ), we considered as being potential speakers those who intersect with the hypothetical region computed in Sect. 6.3 . These regions were computed from each speech balloon, i.e. the balloons that had an identified tail. An abstract straight line was drawn from that tail tip in the direction indicated by the tail. The first region of a poten-tial speaker that it touched was considered to be the source of the speech balloon. This relation was asserted into the ontology with the property isSaidBy , between the selected character and the corresponding balloon. Since the range of this property was set to the concept of speaking character ,it automatically classified the character instance involved into this class. 6.3 Hypothesis of ROI of characters To detect comic characters, we started by narrowing down the area where we were going to look for them. This could be done using the structured information built up on the image during the previous steps. We focused on the char-acters emitting a speech balloon (speaking characters) and tried to localize them from the speech balloon informations. That is to say that speech balloons are associated once or several times to each of the characters in different panels.
As we knew which balloons were speech balloons and what panels they appeared in, we could estimate, from the position and direction of the tail (Sect. 5.3 ), which part of the panel contained the speaking character. Tail orientation was quantized in eight cones of  X / 4 radius so we can con-sider that, in a rectangular-shaped panel (or its bounding box), the tail was pointing either towards a corner or towards a panel border. We defined the ROI for the character as a squared region beside the speech balloon. We defined the maximum width w max and height h max of the ROI equal to the mean widths and heights of all the balloons in the image (Eq. 5 ).

The position of the ROI around the speech balloon was defined by two opposite points of a square A x , y and B x according to Formula 6 which is sometimes constrained by a panel border which is why the term min () appears. The different positions of the ROI are illustrated in Fig. 21 .
A
A
B
B where v  X  is the coordinates of the tail tip (Sect. 5.3 ), Pi the coordinates of one of the four corners of the panel bounding box P ={ P 0 , P 1 , P 2 , P 3 } and O the offset for the direc-tion of the tail. The offset O was quantized in heigh values according to the tail direction and the panel corner P i was chosen accordingly (Table 1 ).

The ROI was then passed to the comic character extractor (Sect. 5.5 ), which will locate more precisely the characters based on image features. In this section, we describe the dataset, the ground truth and the metrics we used to evaluate our work. At first, we focus on each low-level image analysis contribution indepen-dently in order to avoid error propagation effects. Secondly, we evaluated the general framework combining the different contributions of this paper (low-and high-level processing). Results and complementary materials are available online for each image of the dataset. 2
Dataset and ground truth We evaluated our different con-tributions using the public dataset eBDtheque [ 13 ]ofone hundred comic book images with the second version of the ground truth  X  X ersion 2014 X  that contains 850 panels, 1550 comics characters, 1092 balloons and 4691 text lines. This dataset contained 46% of images scanned from French comic books at 300 DPI (dots per inch), 37% of images from French webcomics with various formats and definitions, 11% of public domain American comics 3 in A4 format and 300 DPI and 6% of unpublished artwork of Japanese manga in A5 format at 72 DPI. Five percent of the images contains a dou-ble page. In addition to the diversity of styles, format and definition, there are also differences in design and printing techniques since 29% of the images were published before 1953 and 71% after 2000.

In the second version of the ground truth, the main nov-elty compared to the first version concerns the balloons and the comic book characters. The balloons were segmented at pixel level on the external edge of their contour (previously at bounding box level) in order to evaluate their extraction and shape more precisely. An additional annotation encoded the tail tip coordinates and the tail direction (from the tail tip point) as a value from the set of eight cardinal coordinates (N, NE, E, SE, S, SW, W and NW). The characters were stored with the coordinates of the bounding boxes of each character that emitted at least one balloon in the album. This selection aimed to retain only the main characters who had a direct influence on the story and ignore the secondary charac-ters. Certain parts of the characters (e.g. hand and foot) were ignored in certain postures to maximize the area occupied by the character and minimize the background information in its bounding box. Also, the semantic relations between text, balloon and comic character were added.

This information was stored in a Scalable Vector Graphics (SVG) format described and available through the dataset website. 4
Metrics We evaluated the low-level processing (panel, text, balloon and character extraction) in terms of object bounding boxes such as the PASCAL VOC challenge [ 10 ]. In this challenge, the detections were assigned to ground-truth objects and judged to be true or false positives by measuring bounding box overlap. To be considered a correct detection, the overlap ratio a 0 between the predicted bounding box B and the ground-truth bounding box B gt must exceed 0.5 (For-mula 7 ). In the original VOC paper, it is mentioned that  X  X he threshold of 50% was set deliberately low to account for inaccuracies in bounding boxes in the ground-truth data. For example, defining the bounding box for a highly non-convex object, e.g. a person with arms and legs spread, is somewhat subjective X . The predicted objects were considered true posi-tives TP if a 0 &gt; 0 . 5orfalsepositives FP (predictionerrors). a
Detections returned by a method were assigned to ground-truth objects satisfying the overlap criterion ranked by the confidence output (decreasing). Multiple detections of the same object in an image were considered false detections.
The number of TP , FP and false negative (missed ele-ments) FN was used to compute the recall R and the precision P of each of the methods using Formulas 8 and 9 . We also computed the F-measure F for each result.
 R = TP P = TP
The results of each contribution are detailed for each page of the eBDtheque dataset [ 13 ]inFigs. 27 , 28 , 29 , 30 and 31 . Note that the vertical file names on the horizontal axis correspond to the identifier of the image in the dataset. The materials related to the evaluation section are available online. 6 7.1 Panel detection evaluation We evaluated our method on the 850 panels of the eBDtheque dataset [ 13 ]  X  X ersion 2014 X  at bounding box level.
Assuming that a panel is a big region, we ignored the panel detection with a area lower than 4% ( minAreaFactor ) of the page area according to a validation on the eBDtheque dataset.

Table 2 presents the average results we obtained compared to our previous method [ 47 ] and a method from the litera-ture [ 2 ].

Theproposedpanelextractionbasedonconnectedcompo-nent analysis is simple to implement and is a fast and efficient method for comics with disconnected panels (separated by a white gutter). The validation by the expert system was not significant here because the low-level processing had already reached the limits of the model. Figure 27 shows the details for each image tested, which were mainly comics with gut-ters. Our method is not appropriate for gutterless comics (e.b. some mangas) or strip without panel borders such as those with an extra frame around several panels.

Another weakness is when panels are connected by other elements. This experiment was performed in 28s for the whole dataset using one CPU at 2.5GHz (0.05s per panel on average). Note that some of the dataset images were digitized with a dark background surrounding the cover of the book. We automatically remove this by cropping the image where a panel with an area &gt; 90% of the page area was detected. 7.2 Text localization and recognition evaluation We evaluated our method for text extraction on the 4667 text lines of the eBDtheque dataset [ 13 ]  X  X ersion 2014 X  at object bounding box level.

In our previous work [ 46 ], text extraction was evaluated on a subset of 20 pages of the eBDtheque dataset [ 13 ]. Here, we applied it to the whole dataset. We used our previous method as a baseline to show an improvement in the precision of 20% when using an OCR-based filter, without a significant loss in recall. The validation by the expert system improved the precision as expected but also resulted in a drop in recall. The drop in recall can be explained by the fact that the text extractor is also able to detect texts which are not in the speech balloons but the model considers them as noise. As in [ 46 ], this method has some difficulty coping with certain types of text that can be found in the comics, e.g. graphic sounds.

We also evaluated text transcription using string edit dis-tance [ 60 ] between a predicted text transcription given by the OCR and its corresponding transcription in the ground truth. The eBDtheque dataset is composed of English, Japanese and French texts. We evaluated the subset of English and French pages using the OCR with the corresponding training data. This was performed at the text line level taking as correct the text lines that were transcribed exactly as the ground-truth transcription, considering all the letters as lower case and ignoring accents (for predicted and ground-truth regions). We obtained a score of 7.18% accuracy, which constitute a baselineforfutureworkontextrecognitionontheeBDtheque dataset [ 13 ]. We performed a more relaxed evaluation where we also considered as correct the text lines at a text edit dis-tance equal to one, the accuracy improved to 10.46%. The distribution of the text line lengths is given in Fig. 22 .
Note that in Fig. 22 , there are more than a hundred text lines of only one letter corresponding to punctuation or sin-gle letter words such as  X  X  X  or  X  X  X ; this is a particularity of comics. 7.3 Balloon detection evaluation We evaluated our method on the 1092 balloons of the eBDtheque dataset [ 13 ]  X  X ersion 2014 X  at object bounding box level, which includes the tail. Note that our method does not require any previous processing, in contrast to [ 45 ], and it is able to detect closed balloons only. In the eBDtheque ground truth, only 84.5% of the balloons are closed and 15.5% are not. Thus, we did not expect to reach 100% recall and precision (Table 3 ).

The minimum number of children minNbChildren of a balloon was set to 8, just before the first peak in the dis-tribution of the number of letters per speech balloon in the eBDtheque dataset [ 13 ] (Fig. 23 ). Note that in Fig. 23 , there are about 3.5% of the balloons below the selected threshold that contain one or two letters, usually punctuation marks. We voluntary omitted them here to avoid detecting a lot of non-balloon regions. Balloons with a confidence value C balloon lower than 10% were rejected according to the validation experiments on the eBDtheque dataset [ 13 ].

Table 4 shows the average results for the one hundred images of the dataset. We also compare to a state-of-the-art method from the literature [ 2 ], and our previous work [ 45 ] based on the best results we had obtained for text locali-sation (Table 3 ). Our method outperforms [ 2 ] thanks to its genericity, since it can process all the image styles of the eBDtheque dataset. This was expected as [ 2 ] was specifi-cally developed for manga comics that have certain stylistic particularities. We also surpassed our previous method [ 45 ] because it needs text lines as input which were given in our proposed text extraction method (Sect. 5.4 ). Here, we clearly see the limitations of dependency between the processing. The performance of our text extractor was 49.75% (Table 3 ), which was used as input for balloon extraction so the bal-loon extraction [ 45 ] was inevitably affected. The validation of the expert system once again improves the precision but decreased the recall of the extraction while improving the overall F-measure by almost 3%. The drop in recall was due to the balloons that were correctly extracted but which con-tained no detectable element. Figure 30 confirms that our method works best when the balloons are closed, well seg-mented and with non-cursive text inside. This experiment was performed in 22min for the whole dataset using one CPU at 2.5GHz (2.2s per balloon on average). 7.4 Tail tip and tail direction evaluation We evaluated the new tail extraction method on the 1092 balloons of the eBDtheque dataset [ 13 ]  X  X ersion 2014 X .
Tail tip and tail direction are not represented by bound-ing boxes; therefore, they cannot be evaluated using Eq. 7 . For this particular case, we defined two accuracy metrics A tailTip , the accuracy of the detected position of the tip and A tailDir for the accuracy of the tail direction. The Euclid-ean distance d 0 between the predicted position of the tip and its ground truth was measured relative to balloon size (Formula 10 ). Note that we considered incorrect the pre-dicted positions at a distance d 0 superior to the balloon size (
A A where B w idth and B height correspond to the balloon width and height, respectively.

The direction accuracy A tailDir was measured accord-ing to the distance d 1 within the eight cardinal coordinate sequences defined in the introduction of Sect. 7 : A tailDir 1  X  d and the ground truth was SE (south-east), then d 1 = 1. Note that our method can also detect when there is no tail on the balloon contour C tail = 0 . 0 % (confidence equal to zero per-cent); in this case A tailTip = A tailDir = 100 % if there was effectively no tail to detect or A tailTip = A tailDir = 0%. For this experiment, the local window size M of the tail direction process was set to 10% of the mean balloon size (Eq. 5 ) in order to be invariant to the image definition. We obtained an average score of A tailTip = 96 . 77 % and A experiment was performed in 32s for the whole dataset using one CPU at 2.5GHz (0.03s per balloon on average).

Figure 29 shows the detail for each page of the eBDtheque dataset [ 13 ]. This method was very good at locating the tail tip when one and only one tail existed and also at giving a confidence value equal to zero when the balloon was com-pletely flat (no tail). However, the algorithm was confused in case of multi-tail balloons and spiky balloons without tail because one of the peaks was detected as a tail (Fig. 24 bot-tom). Tail direction was sensitive to the quality of tail tip position detection and the eight directions quantization. For instance, in the bottom-left part of Fig. 24 , an incorrect tail position was detected and thus confused the tail direction detection (detected as east instead of no direction because no tail). 7.5 Comic character detection evaluation We evaluated the detection of comic characters on the 1550 characters of the eBDtheque dataset, given the input infor-mation from the ground truth (panel, balloon, tail position and tail direction). In the eBDtheque ground truth, only 880 (56.8%) of the character instances are speaking; thus, we did not expect to reach 100% recall and precision here because our method is able to detect only speaking characters. An example of valid and rejected regions is shown in Fig. 25 . Cumulative results are presented in Table 5 in order to show the performance over each steps of the following process: (a) Hypothesis of ROI from Sect. 6.3 (A) (b) Character extraction from Sect. 5.5 (B) (c) Character extraction validation from Sect. 6.1 (C)
As shown in Fig. 25 , the decision criterion a 0 &gt; 0 . seem restrictive and has to be adjusted according to the appli-cation. The numbers in Table 5 are given for the whole dataset including the characters that are not speaking. If we focus on the subset of 880 speakers, the recall, precision and F-measure were 34.49, 32.02 and 33.21%, respectively. 7.6 Relation retrieval evaluation The links between speech text and speech balloon are called STSB and the ones between speech balloon and speak-ing character SBSC ; they characterize a dialogue. They are considered true or false according to their existence or not in the ground truth. We evaluated the relations STSB and SBSC according to the metadata in the ground truth of the eBDtheque dataset [ 13 ] called isLineOf and isSaidBy , which represent 3427 and 880 relations, respectively. Given the panel, balloon and character positions from the ground truth, our ontologies were consistent with 96.9% of STSB assertions and 70.66% of SBSC . These numbers can be seen as the legitimacy value of the semantics given the rela-tions between balloons, characters and text lines. The 3.1% of missed isLineOf relations came from balloons that were not compliant with our model. In the same way, of the 880 isSaidBy relations,thatlinkspeechballoonstospeakingchar-acters, 9.5% were undetectable because they were generated from balloons outside the panel. 7.7 Framework evaluation We evaluated our framework during the two iterations of the process loop introduced in Sect. 3.3 and particularly at the beginning (Step 1: hypothesis) and at the end (Step 3: inference) of each iteration. Note that using this framework, some processes ( C , STSB , SBSC ) were related to previous processes which propagated errors and reduced their perfor-mance compared to the evaluation in Sects. 7.5 and 7.6 .We evaluated our framework using the F-measure of panel P , balloon B ,text T and character C extractions. Also, the accu-racy of the two STSB and SBSC relations was measured. Table 6 details the number of retrieved elements among all the elements and after the two iterations of the process. The change in the performance of each processing throughout the process is presented in Fig. 26 as F-measure for P , B , T , C and accuracies for STSB and SBSC .

Process details Figure 26 shows the change of the perfor-mance of our framework after the first and second iteration of analysis over the eBDtheque dataset [ 13 ]. The performance of the first iteration was measured after the initial extraction of simple elements, which were considered as hypotheses by the expert system (Fig. 26 a) and after validation by the expert system (Fig. 26 b). Between the first initialization and vali-dation (first row of Fig. 26 ), the F-measure remained stable for P and increased by 3% for the balloon B and text T .At this point, no comic book characters were discovered because theirrulesintheontologyarerelatedtoelementsthatwerenot yet discovered. Nevertheless, the links between speech text and balloon STSB were inferred. The expert system applied the inference rules to automatically label the balloon with a tail that included text such as speech balloon SB and speech text ST , respectively, and created a STSB link between each of them.

The newly inferred links were processed by the expert system along with previously validated regions during a sec-ond iteration in order to get more information by trying to apply more rules from the knowledge base. This time, the expert system could make use of rules related to char-acters because the speech balloons were now part of the knowledge base. Panels and speech balloons were used to create hypotheses for the location of characters, and then, the low-level processing located them more precisely within these regions (Fig. 26 c). Finally, the expert system vali-dated the newly discovered regions and inferred the relations between speech balloons and speaking characters SBSC (Fig. 26 d).

Low-level processing The low-level processing scores (
P , B , T ) always increased between the hypothesis and the validation steps, which confirms the benefits of combining different levels of analysis. The best extraction performance was obtained for the panels that were usually the easi-est elements to extract from a page. The lowest extraction performance was for the comic characters C . There are var-ious reasons for this. First, the limitation of the extractor to process speaking characters; second, the variability of character styles in the eBDtheque dataset; third, the error propagation of previous processes (panel, text, balloon, tail and link extractions) that are required to guess comic char-acter locations.

Relations between elements The expert system was able to retrieve 49.12% of the STSB and 25.76% of the SBSC relations of the modelled relations. Even if these relations were quite well modelled (96.0 and 70.7% in Table 6 ), their extraction remains dependent on the quality of related ele-ment extractions. It should be stressed that these numbers represent theefficiencyof thelast process of thewholeframe-work pipeline (relation retrieval). Individual errors at each recognition and validation step of the pipeline are propagated to the final relation retrieval between elements. Therefore, a single improvement in the detection or the validation of any kind of element would have an impact at the relation retrieval level.

General evaluation It is difficult to combine individual metrics of a different nature into a single global metric able to evaluate this framework. Table 6 gives indications about the amount of discovered information and Fig. 26 provides the performance of the framework over the process loop. According to Table 6 , our model was able to model 88.29% of the information of the dataset despite the diversity of the images. Moreover, 45.01% of the information were retrieved automatically using our framework (Table 6 ). Note that the recall and precision represented by the F-measure in Fig. 26 show good performance for simple element extraction such as P , B and T but suffer from error propagation effect for more complex ones such as C and the relations between ele-ments ( STSB and SBSC ) (Figs. 27 , 28 , 29 , 30 , 31 ). This paper presents a new framework for understand-ing documents that can interact with low-and high-level information suitable for semi-structured and complex back-ground documents such as comics. Several key improve-ments to information extraction and processing methods have been developed. We suggest improved methods for panel and balloon extraction along with a first method in the literature to locate the tail tip and the indicated direction by analysing the balloon contour. We provide a novel generic and unsu-pervised definition of the comic character region of interest that takes into account the spatial organization of the rest of the elements in an image. It relies on an inference engine that interacts with two knowledge models, one for image process-ing and the other one for comics. In the future, we plan to add more iterations of the process in order to retrieve new infor-mation such as spotting the non-speaking comic characters using those already detected (speaking characters) for train-ing. In addition, the expert system will be used to improve text extraction and recognition using system feedback in order to automatically extract open speech balloons from text locations.

