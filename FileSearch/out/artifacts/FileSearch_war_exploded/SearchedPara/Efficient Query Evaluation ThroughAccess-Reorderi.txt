 Web search engines currently process queries over collections composed of sev-eral billion documents. With the number of queries submitted by users of the Web, the need to service requests in a timely and efficient manner is of critical importance.

A key component of modern text search engines that allow efficient query processing is the inverted list [18]. Several techniques have been proposed to optimise the organisation of inverted lists to allow efficient query processing. One such technique is the access-ordered index [7]. In this approach, inverted lists are reorganised based on past user queries to allow faster processing at query time.

One of the major drawbacks of the access-ordered index is the difficulty in compressing the index effectively. In this paper we extend the work on access-ordered indexes and present a variant that we label access-reordered indexes that improves index compression levels over the previous approach.

A second issue that arises when dealing with an access-ordered index is the durability of the index. That is, if the index ordering is based on past user queries, then how long will the reorganisation be able to process queries efficiently as user requests change over time? In this paper we explore the effect of training set size, and show that an access-ordered index based on a minimal number of queries can be used to produce results that are approximately equivalent to those of an index ordered by queries gathered over a significantly larger period of time.
This paper is organised as follows. In Section 2 we present background on efficient query evaluation in search engines. In Section 3 our variant index or-ganisation technique is introduced, and in Section 4 we present experimental results that demonstrate the effectiveness of our proposed technique. Conclu-sions and further research issues are discussed in Section 5. In this section we discuss efficient text search. Specifically, we focus on inverted index organisation, and present recent efficiency optimisations that take advan-tage of index reorganisation.
 A well known data structure used in efficient text search is the inverted index . For each term t in a collection, an inverted list is created that stores information regarding the occurrences of that term. For a ranked search system, an inverted list is composed of postings , consisting of &lt;d,f d,t &gt; pairs, where d is a docu-ment identifier in which the term occurs and f d,t is the within document term frequency of term t in document d . As an example, consider the inverted list for the term Kalimdor : In this list we see that the term appears five times in, document 1, three times in document 12, and twice in document 14.

An inverted list organised by ordinal document identifiers is known as a document-ordered index. Due to the ordinal nature of the document-ordered index, inverted list compression gains can be made by storing the difference between adjacent values. This difference is referred to as the d X  X ap . For our example above, the inverted list for Kalimdor becomes: where the first posting refers to document 1, the second to document 12 (1 + 11), and the third posting to document 14 (12 + 2). By compacting the values in this manner, variable byte and variable bit encoding techniques can more effectively compress the index [6,8]. Scholer et al. found that when compressing inverted lists, variable byte encoding provides effective compression, with efficient en-coding and decoding [13]. In this work we make use of variable byte integer compression for our inverted lists.

Once an index has been constructed, the evaluation of a user query can be performed using one of many well known similarity metrics [12]. We make use of the Okapi BM25 [16] metric that evaluates the predicted similarity between aquery q and a document d as: where N is the number of documents in the collection and f t is the number of distinct documents that contain term t . K is set to k 1 ((1  X  b )+ b  X  L d / avl) where k 1 and b are tuning parameters; L d is the length of document d ,andavl is the average document length in the collection. Our implementation does not consider query-term frequency, which is not used in this context.

Evaluation of a query proceeds as follows. For each term in the query, the corresponding inverted list is loaded from disk. The list is decoded, and for each posting a partial similarity between the document and query is calculated and stored in an accumulator structure. If the posting represents a document that has not been previously seen, a new accumu lator is initiali sed for the document with the calculated partial score. If the posting represents a document that has been previously seen, then the accumulator for that document is updated to include the new partial similarity. When all of the query term lists have been processed, the set of accumulators is partially sorted to produce the top R scoring documents, and the results are returned to the user.

For queries containing terms that are common in the collection, the num-ber of accumulators that need to be initialised can be large. This can increase main memory usage and query evaluation time. Moffat and Zobel have proposed techniques that limit the number of accumulators at query evaluation, and have shown that such techniques can be as effective as having an unlimited number of accumulators [9].

The internal representation of the collection as used by the search engine is critical to service user information needs quickly. Standard query evaluation requires processing the entire inverted list for each query term. Reorganising inverted lists, so that the postings towards the head of each list are the most important postings for that term, has a key benefit: query evaluation time can be reduced by only processing the early portion of each inverted list. The list pruning techniques applied at query evaluation time vary with each reordering scheme and are discussed next.

One well known approach to collection reordering is that of frequency-ordering where the postings within the inverted lists are organised by descending within document frequency f d,t value [11]. This approach is based on the assumption that those documents that contain the term most frequently, are the most impor-tant in the list. At query evaluation time inverted lists are processed in inverse document frequency order. When a new list is about to be processed, two thresh-old values a ins and a add are calculated. Postings are decoded sequentially, and as the list is processed the within document frequency of each posting is com-pared to the a ins threshold. While the within document frequency values remain greater than the a ins threshold, new accumulators are initialised. Then, when the within document frequency values fall below a ins , and while they remain greater than the a add threshold, processing continues with only existing accu-mulators being updated. Finally, when the within document frequency values fall below the a add threshold, the processing of the list terminates.
A side effect of the list reordering is t hat d-gaps can no longer be used to com-pact the inverted lists as adjacent postings are no longer ordered by ascending document identifier. However, Persin and Zobel found that by storing a single f d,t value for blocks of postings that share t he same frequency value, frequency-ordering can achieve comparable compression gains to a standard index that uses d-gap compaction.

Anh and Moffat proposed an alternate inverted list organisation approach where inverted list postings are organised by the contribution of the posting to the similarity metric [1,2]. In an impact-ordered inverted list, the postings are ordered by descending impact of the document on the similarity measure. The definition of an impact varies between similarity metrics.

Several pruning strategies were proposed for impact-ordered indexes that dy-namically prune lists based on the impact of the current posting being processed, the number of equi-impact blocks of postings processed to date, and the docu-ment frequency of the current query term.

Compression of the inverted lists is difficult for impact-ordered lists, as the val-ues in the lists may be real numbers requiring a floating point representation. To overcome this difficulty, Anh and Moffat propose quantising the impacts to a pre-defined integer range. As in frequency-ordered lists, the impact value need only be stored once per group of postings that share the same quantised impact value.
Examination of Web query logs has shown that term appearance in query logs is non-uniform. In an exploration of Web query logs, Spink et. al. found that the 75 most common search terms accounted for 9% all query terms [17]. Garcia et. al. demonstrated that such a skew in query terms leads to a non-uniform distribution of documents returned to the user at query time [7]. That is, given a query log, the search system will in general return a subset of documents from the collection more frequently than all other documents.

Figure 1 shows the skew in distribution of document access on a collection of 7.5 million documents when 20 million queries from Lycos.de are run against the collection. The analysis is based on the top 1,000 results returned for each query. This query log is discussed further in Section 4. The most frequently oc-curring document appears in the result set for over 1 million queries, that is, it appears in the results approximately once every 20 queries. Further, the 10% most frequently accessed documents account for over 70% of the documents in the result sets. Based on such trends, Garcia et. al. proposed an index organisa-tion technique where the most frequently accessed documents are placed towards the head of the inverted lists [7]. They label this technique access-ordering .
In access-ordering, frequency of document access is determined as follows: for each document in a collection a counter is initialised to zero. A query log is run over the collection, and for each document appearing in the top 1,000 results of a query, the counter is incremented by 1. After the query log has been completely processed, each document will have an associated access-count .Eachinverted list in the index is then reordered so that postings are organised by descending access-count.

Consider again the collection from our pr evious example. Assume that we run four queries over our collection, taking the top 3 results per query as follows: Query 1  X  D 5 , D 3 , D 14 ;Query2 X  D 1 , D 14 , D 5 ;Query3: X  D 12 , D 1 , D 9 ;and Query 4  X  D 14 , D 2 , D 7 . After processing the queries, we can determine an access-count for each document. For example, the access-count for document D 1 is 2 as it appears in the result set for two queries. We then store the access-counts in the inverted lists within posting triples &lt;d,f d,t ,a d &gt; ,where d and f d,t are defined as above, and a d is the access count for document d . The access-ordered inverted list for the term Kalimdor becomes: Note that document D 14 now is the first posting in the list, as it appeared most frequently in the result sets.

Garcia et. al. explored several early termination heuristics ranging from sim-ple approaches such as processing a fixed number of postings per list, and only processing postings with a minimum access count, to more elaborate techniques such as tracking the average accumulator contribution of a list as it is processed and terminating when a threshold is passed. They found the most effective prun-ing technique was an adaption of the frequency-ordered pruning approach, where processing terminates when the contribution of a posting falls below a threshold value that is calculated once per inverted list. In this scheme, a threshold value is calculated for each term prior to processing its inverted list as follows: where w t is the weight of the query term t , S max is the largest accumulator seen to date while processing this query, and c allow is a global tuning para-meter. Terms are processed in descending inverse document frequency order, with list processing terminating when the access-count of a term falls below the a allow value. Accumulator limiting is also used to bound main memory usage as required.

The initial work with access-ordering and list pruning reports savings in processing time of up to 25% over the conventional approach. However, as access-ordered lists do not have ordered document identifiers or ordered document con-tributions, the advantages of d-gap compaction are minimal. Indeed, without compaction, the index size grows by 60% when ordering lists by access-count.
An alternative to index reorganisation is collection reordering . In a reordered collection, instead of reorganising the inverted lists independently, the document identifiers of the entire collection are remapped to meet a specified criterion. Typ-ically, similar documents are assigned document identifiers in the same proxim-ity. The key benefit of this approach is that if similar documents are clustered together, then the inverted lists of the terms that appear in these documents will contain postings that have a reduced average d-gap. This in turn leads to compres-sion gains. Blandford and Blelloch [3] and S ilvestri et. al. [14,15] propose collection reordering techniques based on the clustering of similar documents, and show that this can reduce index size by as much as 23% over a conventional approach. Access-ordered indexes have been shown to be an efficient technique to reduce the costs of query evaluation. However, the increase in index size offsets part of the potential benefit of the index organisation. Also, as the index reorganisation is based on an ordering determined from a query log, it is possible that the ordering will become less effective over time, as the pattern of user queries change. In this work, we propose a novel extension to access-ordered indexes that overcomes the issue of index size, and explore the eff ect of temporal changes in user queries.
Unlike other index reorganisation techniques, access-ordered indexes are re-ordered by a collection X  X ide ordering. All lists are ordered by the same set of access-counts.

We propose an improvement to compression by reassigning document iden-tifiers. Specifically, after establishing access-counts, documents are assigned an identifier based on their rank in the access-ordering. That is, the document with the highest access count is assigned the document identifier 1, the document with the second highest access count is assigned the document identifier 2, and so on.
Consider the following example. After processing a query log we are left with the following access scores in a collection of 15 documents: D 1 = 12, D 3 = 11, D 5 =2, D 7 =1, D 9 = 13, D 12 =5, D 14 = 33. All other documents have an access score of 0. Based on these values, D 14 with an access score of 33 is the most accessed document, so it is assigned document identifier 1. Similarly, D 1 with an access-count of 12 is the third highest scoring document, and is assigned document identifier 3. Finally, D 12 is ranked fifth and is assigned identifier 5. Our access-reordered list for Kalimdor becomes: Note that with ascending document identifiers, we can now make use of d-gap compaction, which should lead to reduced index size.
An advantage of access-ordering is that changes to the access-counts can be reflected in the index by resorting the inverted list of each term. Lists can be updated independently, as needed, without reference to the rest of the collection. On the other hand, remapping the document identifiers makes updates difficult as it requires updating all postings in all lists.

For the access-reordering approach to be effective, remapping of the index must not be required at frequent intervals. We therefore need to explore the sta-bility of a generated ordering. In particular, two questions need to be addressed: first, do changes in user queries over time cause the index to become unstable for list pruning? Second, how many training queries are required to generate an access-reordered index that can efficiently service user queries? In this section we present experimental results that demonstrate the effectiveness of access-reordering. We begin with a discussion of the experimental environ-ment. We then explore the effects of pruning at query time, and show that there is minimal difference between the results returned by a conventional system, and the results returned by our approach. We also show that an index trained on as few as 100,000 queries can be used to process as many as 20 million queries with effective query time pruning. Finally, we demonstrate the efficiency of our system by showing that our approach with pruning can process queries up to 62% faster than the conventional approach. 4.1 Experimental Environment All experiments were conducted on an Intel Pentium 4 server with 2Gb of RAM. The server made use of the Linux Fedora Core 3 operating system. For all timed experiments, the system cache was flushed between runs.

We made use of the the Zettair search engine 1 ; an open source search en-gine developed by the RMIT Search Engine Group. Queries are bag-of-words or ranked queries, with similarity scores evaluated using the Okapi BM25 measure described in Section 2. 4.2 Collections and Query Logs Our experiments made use of a subset of the Gov2 collection from the 2004 Text REtrieval Conference (TREC) [4]. The Gov2 collection is composed of Web documents from the .gov domain crawled during 2004. Our subset consisted of the first 7.5 million documents in the collection as distributed by TREC. This subset accounted for the first 100Gb of uncompressed documents.
 To explore the effect of training query set size, a large query log was required. We experimented with two distinct query logs with different properties. The first query log from Lycos.de 2 contains over 200 million time X  X rdered queries from the Lycos Web search engine. Unfortunately, a high proportion of these queries contain German terms that do not occur frequently in the Gov2 collection. The second query log was generated from data provided by Microsoft 3 in 2003. We were provided a list of over 500 million queries ordered from most to least fre-quently occurring. Queries that occurred less than three times during the period in which the queries were gathered were not included in the list. While more compatible with the collection, this list does not provide a time reference for each query, therefore simulating a stream of queries over time requires randomi-sation of the query log, and does not reflect trends in query use such as those explored by Ozmutlu et. al. [10] and Diaz and Jones [5]. For the purposes of this work, we generated a simulated time X  X rdered query log of 10 million queries by sequentially selecting queries from the Microsoft list of queries based on their frequency of occurrence. 4.3 Relevance Framework To evaluate the effectiveness of our system we make use of the well X  X nown in-formation retrieval measures recall , precision and mean average precision [18]. Recall measures the proportion of known relevant documents that are returned by a search system, and precision measures the proportion of relevant documents returned in a result set. Mean average precision is mean of average precision val-ues for each query, where average precisi on is the average of precision values for each relevant document in the retrieved results.

To measure the effect of training set size and query drift, a large number of queries and relevance judgments are required. While the query logs that we work with meet the former requirement, relevance judgments are unavailable. However it is still possible to measure change in system performance by measuring the change in the results returned by two search systems when processing the same query log.

In our approach, we process a query log using system A over our collection and record of all returned results in the set R A . We label this the oracle run. A second run with the same query log and collection is processed using search system B and the results are stored in R B .Bytreating R A as the set of relevant documents, and R B as the returned result set, we can compare the difference between the two runs using precision and recall. Under such an approach precision and recall values of 100% indicate that both systems produce identical results. Lower values of recall indicate dramatically different result sets, and low values of precision at N indicate that system B produces different results ranked within the top N documents to the results returned by system A . 4.4 Effects of Pruning To explore the effects of pruning at query time, we compared the result sets of our access-reordered index with query time pruning with the results returned by a standard index. Using 1,000 queries from each log, we marked the results returned by the standard approach as the oracle run, and compared these to the results returned by our access-reordered scheme. The 1,000 queries selected for this experiment were not part of the qu eries used to train the access-counts. The pruning parameter c allow was trained, for each query log, to process on average 10% of the inverted lists.

Figure 2 shows the difference in result sets between the two runs when we con-sider the top 10, 100 and 1,000 returned results per query. Figure 2(a) compares an index reordered by access-counts based on the first 20 million Lycos queries, while Figure 2(b) compares an index reordered by access-counts generated by 10 million random Microsoft queries. For the collection reordered by access-counts from the Lycos queries, we can see that, for 10 results per query ( N = 10), pre-cision at 100% recall drops to only 85.79%. As we increase the number of results considered per query, the similarity between our approach and the standard dif-fer more, however, across all three levels of considered results, for up to 90% recall, the precision never drops below 80%. Figure 2(b) shows that an access-reordered index based on the Microsoft log produces similar results. The pruning scheme therefore gives robust performance across a broad range of recall levels. 4.5 Stability of Access-Reordering For a search system, the cost of reordering a collection is significant. When the collection is reordered, the entire index must be rebuilt. In this experiment, we show that with as few as 50,000 queries, we can generate a collection ordering that gives search results that do not differ greatly from results obtained using an index reordered on access counts based on 20 million queries.

To examine the effects of query drift over time, we generated indexes based on query logs from 50,000 queries up to 20 million queries from the temporally ordered Lycos log. For each reordered index, we ran 1,000 test queries and col-lected the top 1,000 results per query. We treated the results returned by the index trained on 20 million queries as the oracle run, and compared the differ-ence in results returned by indexes trained on the smaller sized query sets. If the ordering produced is stable, then an index trained on a relatively small amount of queries should be able to return similar results to those of an index trained on a large number of queries.

The mean average precision of results r eturned by indexes trained on query logs of varying sizes are compared to the oracle run in Figure 3. The figure shows that when pruning so that only 20% of the inverted lists are processed on average, the mean average precision obtained when training with only 50,000 queries is already over 90%. Further, with 100,000 training queries the results are 96% similar.

The figure also shows the similarity between the indexes when pruning the inverted lists to process 10% of the postings on average. In this case, little per-formance is lost compared to that of 20% processing.

The Microsoft log is generated by sampling queries based on their frequency of occurrence. As such, results for this experiment using the Microsoft log are not presented as they do not exhibit the temporal property of query drift that is being investigated for this experiment. 4.6 Efficiency We have shown that an access-reordered index with pruning produces results comparable to those obtained using a standard index. We now compare the effi-ciency of query evaluation when using an access-reordered index to the standard approach.
To compare the efficiency of query evaluation, we timed both systems using 100,000 queries from the Lycos log. To avoid bias, the timing queries were dif-ferent to the ones used to train the access-reordered index. An index trained on 20 million queries was used, with the c allow parameter set to process 10% of the inverted lists on average. Main memory was flushed between each run to avoid caching effects.

Table 1 shows the time to process a single query using a standard index and an access-reordered index. The access-reordered approach with pruning is 62% faster.

In our experiments, the access-reordered index was 2% larger than the stan-dard index, as a result of storing a single access-count value for each document in the collection. This is a significant improvement over the results reported by Garcia, et al. with access-ordering where index size grew by as much as 60% [7].
When pruning inverted lists at query time, a large saving comes from not processing postings in terms that occur frequently in the collection. These terms, often referred to as stop-words , are commonly disregarded by search systems. To investigate this effect we performed timings on stopped queries using the same c allow parameter. In the case of stopping, the access-reordered approach remains 44% faster than the standard approach. We have presented a new index representation that builds on access-ordered indexes. This technique overcomes the compression difficulties raised by the access-ordered approach by generating an i ndex that is equivalent in design to a standard document-ordered index. We have shown that even when inverted lists are pruned at query time by up to 90%, the returned results are comparable to the results returned by a standard index with no pruning.

We have also shown that an access-ordering is robust, where an index based on as little as 100,000 queries can produce similar results to orderings based on larger query training sets. This suggests that an access-reordered index has a durable life span, and that the index need not be rebuilt frequently to allow for query drift.
Finally, we have shown that access-reordered indexes with pruning are signif-icantly faster than the standard approach, both with and without stopping.
Several aspects of this work however remain to be explored. Specifically, given the cost of reordering the collection, the ability to add new documents into an already constructed index would be advantageous. However, determining the access-count to assign to new documents is problematic. One possible approach is to query the collection using each new document, and assigning it the access-count of the highest result. Further, we would like to explore optimisations to the collection reordering technique so that t he entire index need not be reordered at once, but instead reordered dynamically as the inverted lists are used. We would like to thank Hugh Williams, Adam Cannane and Falk Scholer for their ideas and input into this work. We would also like to thank Klye Peltonen for access to the Microsoft query log. This work was supported in part by Australian Research Council Grant DP0558916.

