 AND ________________________________________________________________________ ________________________________________________________________________ 1. INTRODUCTION Automatic speech recognition has been the goa l of researchers for many years and it has a long history of being a diffi cult challenge. Early research in speech recognition attained acceptable performances only by imposing constrai nts in the task domain such as speaker dependence, isolated words, small vocabulary, constrained grammar, or the use of a quiet recording environment. During the past fe w years, progress in speech recognition technology strived to tackle some of these constraints enabling the development of are handled by the presently available approaches, with a restriction of performing efficient recognition only in a specific domain. The distinct characteristics of a language can be analyzed based on the order of occurrence of phonemes, syllable patterns and detecting typical language specific characteristics such as phoneme, syllable and word syllable and word recognition phases. highly inflectional languages such as Tamil and other south Indian Dravidian languages languages we encounter the major problem of excessive vocabulary growth caused by change in the word predominan tly affects word endings, wh ereas stem remains constant. However, the number of different endings is relatively very small. The combination of stems and endings leads to large number of derived word forms. In this work the words are decomposed into stems and endings and thes e unit X  X  are treated separately as if they were independent words. By segmenting words into morphemes we can improve the performance of natural language systems including machine translation [Brown et al. 1993], information retrieval [Franz and Mc Carley 2002] and speech recognition [Siivola et al. 2003]. The morphological rules are used to produce sub-word units in highly inflected languages [Kneissler and Klakow 2001; Byrne et al. 2001; Huckvale and Fang 2002]. The rules are based on language dependent prior assumptions about stems and suffixes. Morphological analysis of highly inflected languages can be performed using two methods [Byrne et al. 2001]. In one of the methods the coverage of sub-word units is words and sub-words units. This work is based on the second method of grouping the common sub-words and then generating the rest of the words from the combination of time for detecting the errors at word level. provides the design of a enhanced morp heme-based language model for Tamil speech recognition system and compares the performance of the proposed model with different statistical word-based language models. 1.1 Features of Tamil Language There are two important differences between Tamil and English that are of relevance to statistical language modeling and that are shared in varying degrees by many other languages. They are word formation and word ordering. Tamil words typically have more morphological patterns than English words. For example, a Tamil word will often contain responsible for the nuclear mean ing of the verb, attached to which may be zero or more most relevance to language modeling, however , is the inflection (inflectional suffix), which is appended to the stem and which determines the grammatical case, gender (masculine, feminine, or neuter), number, etc. of the word. The pres ence of the inflection for the equivalent English word, for which th ere are generally few distinct forms [Arden vocabulary than that of the same sized E nglish vocabulary. English compensates for having less grammatical information encoded within the words themselves, by imposing kicks the ball X  X , it is only clear who is doing what to whom from the order in which the words are written. In Tamil, on the other hand, the subject and object of the sentence can only be determined by the associated inflection with the words and by agreement with the translated into Tamil could be expressed by six different permutations for the three words  X  X  X oy X  X ,  X  X  X icks X  X  and  X  X  X all X  X  without loss of meaning. Clearly, this phenomenon has the potential for seriously weakening the predictive power of some statistical language models. However, in reality some word orderings are preferred stylistically to others. In frequencies and this led to the need to desi gn a language model suitable for Tamil speech recognition system. In this work Tamil sp eech recognition system is developed using artificial intelligence ap proach, which is a combination of knowledge based approach and statistical approach to recognize th e words from the speech signals. 1.2 Speech Engine based on Artificial Intelligence approach for Tamil segmentation algorithm designed by us based on Guido Aversano X  X  phoneme level segmentation algorithm [Aversano et al. 2001, Aversano et al. 2003]. Aversano X  X  algorithm was modified to reduce under and over-segmentation rates at phoneme level by re-segmenting under segmented signals and combining the over segmented signals rate, the category based language model was used to alter incorrect segmentation points. The signal segments obtained from segmentation algorithm were mapped to their phoneme category based on acoustic and articulatory features [Ali 1998; Ali et al. 1999, Ali 2001] as shown in table I. determined by using category based phoneme language model, where the category corresponds to the articulat ory features of the phonem es. Detection of improbable co-1 Formant Frequencies Vowels 2 Spectral center of gravity Dental, alveolar, palatal 3 Maximum normalized spectral slope Labial and dental 4 Most dominant peak frequency Palatals and nonpalatal 5 Duration of unvoiced portion, Energy, Most 6 Voice Onset Time, Formant Transitions and assignment of the segmentation points to correct segmentation. For phoneme recognition work we have used information associat ed with voicing and place and manner of articulation to alter segmentation points. Under segmentation errors due to co-joining of identical phonemes, which may occur in the case of germination in Tamil. It is possible that a segment corresponds to two phonemes. Such segments are again re-segmented based on lower level amplitude peaks. Errors due to wrong identification of segmentation points, occurring due to use of spurious maximum amplitude peaks as the threshold value for segmentation. In this case, segments are combined with the neighboring segments and are again re-segmented based on the next lower level amplitude peaks present, ignoring the spurious maximum peaks. altering the segmentation points, based on predicted improbable phoneme co-occurrences. Signals segmented at phoneme leve l are mapped to their IPA equivalent text format based on their acoustic feature  X  PL P, using the Euclidean distance measure. The were detected using language models. 1.3 The Use of Language Models in a Speech Recognition System In general the output of the recognition system produces a sequence of phonemes. During recognition, the sequence of symbols generated by the acoustic component is compared words that will compose the system X  X  final output. Rules are introduced during this stage that will describe the linguistic restrictions present in the language and allow the reduction of possible invalid phoneme seque nces. This is accomplished through the use of language models in the system. A langua ge model comprises two main components: grammar which is a set of rules that regulate the way the words in the vocabulary can be arranged into groups to form sentences. The grammars are made of formal linguistic rules. The linguistic model introduces strong restrictions in allowable sequences of words but becomes computational demanding when incorporated in a speech recognition system. They also have the problem of no t allowing the appearan ce of grammatically incorrect sentences that are often presen t in spontaneous speech. This makes the stochastic models based on probabilities for sequences of words more attractive for use in speech recognition system due to their robustness and simplicity. 2. TAMIL SPEECH AND TEXT CORPORA For the purpose of performing language modeling the text corpus was collected from two different domains. The language modeling was done on the text corpus. The performance of the modeling was tested on the speech corpus collected with respect to these domains. 2.1 Text Corpus For Language modeling purpose text from newspapers and magazines were collected. The newspaper corpus consists of approximately 5,000,000 words (5 MB) and the current political issues corpus, collected from magazines had approximately 2,000,000 words (2 MB). Text data has to go through several preprocessing stages in order to obtain clear and unambiguous data before statistical and morphological analysis. This was done to keep the sentences easy to read and to keep morpho logical analysis correct so that the language model could easily be estimated. The punctuation marks that are not usually pronounced in speech communication are di scarded from the training and the testing set. 2.2 Speech Corpus A talk on current political issues and the read article from newspaper were collected for testing purposes. The news corpus consists of talks of 50 persons 30 minutes each persons, 15 minutes each that yielded 2 hours of speech information. Portions of the speech corpus containing long silence or other non-speech material were marked and they were not considered for recognition. The speech corpora were used to evaluate the WER in the news and politics domain. 3. STATISTICAL N-GRAM LANGUAGE MODEL The n-gram model uses the previous ( n-1 ) words as the only information source to generate the model parameters. The n-grams ar e easy to implement, easy to interface with and good predictors of short-term dependencies. Given any state ( W k , W k+1 ), it is possible al.. 2001]. The n-gram assigns zero probability to words not present in the corpus. There grams. Katz back-off smoothing technique is usually applied for n-gram approach as it provides better results when compared to other smoothing techniques [Wu and Zheng 2000]. The back-off models provide an efficient method for increasing coverage and hence overall performance of the system. The bigram language model that uses Katz back-off smoothing technique is calculated as where  X  takes a value in the range 0-1 . The value of  X  was set to 0.2 in bigram language model. The trigram language model that uses Katz back-off smoothing technique is calculated as where  X  1 and  X  2 takes a value in the range 0-1. Perp lexity values were evaluated for the test data with n words as P is the probability estimation of sequence of n words given by language model. Both Bigram and Trigram models with back-off smoothing effects where applied over the given corpus and the perplexity results were analyzed for values  X  1 and  X  2 ranging from 0.1 to 0.9 as listed in table II. It was found that for values of  X  1 =0.8 and  X  2 = 0.2 improved perplexity and word error rate (WER) results were obtained for both bigram and trigram as shown in table III. The WER values was evaluated as where Subs  X  No.of wrong words substituted in the recognized text. Dels  X  No. of correct words deleted in the recognized text Ins  X  No. of new words inserted in the recognized text The perplexity values evaluated for the test data were high for the politics corpus than the news corpus. This was due to the small vocabulary size and high OOV words in the politics corpus, when compared to the news corpus. The number of unigrams, bigrams and trigrams were high for both the corpora. The performance of the speech recognition system was further enhanced by the use of distance based language model, dependency-based language model, class-based language model and enhanced morpheme-based language model as discussed in the next section. 4. DISTANCE-BASED LANGUAGE MODEL In any natural language a word is not only related to its immediate neighbors but also on predict depends on its history as a single block. Language modeling is thus reduced to a example if we consider the fo llowing sentence,  X  X he book I bought is brown X ,  X  X ook X  is the relationship between the word to predict and its history is a distance relationship. The show respectively the kind of relationship taken into account by classical language models and distance language model. linear interpolation between distant bigrams, a bigram, a unigram and a zero-gram model is defined. A zero-gram model defines a uniform probability over the entire vocabulary. To make into acco unt a history of n-1 words, as for n-gram model, we use n-1 distance model one for each distance word in the past . By this way the model takes into account past. The probability of these contiguous words is given by where C  X  (v,w) is the occurrence of the event w appearing  X  -1 words after v . For  X  = 0.2 better perplexity values were obtained for the both the corpora. The trigram probability value for distance based approach was evaluated as P P For trigram distance based language model best results were obtained for  X  1=0.7 and  X  2=0.3. Distance based bigram and trigram models were tested on the Tamil corpora and the results are listed as shown in table IV. perplexity values when compared to the n-gr am model. Unlike English as Tamil is a free word order language the distance based approach is not suitable for this language. 5. DEPENDENCY BASED LANGUAGE MODEL Dependency based language model was proposed to remedy two important weaknesses of the well known n-gram method. The n-gram models retain only the n-1 most recent words frequently linguistically implausible for n-gram models to blindly discard relevant words simply by virtue for their recency. Second it makes inefficient use of training corpus as the size of the text used for testing the performance of the model is increased. This led to the design of dependency based language model, where the relationship between words is represented by a direct graph [Gao and Hisami 2003]. The edges of the graph can connect words that are arbitrarily far ap art in a sentence. This led to consideration of words that lie outside bigram or trigram range for language modeling. closely related notion of a link grammar [J. Lafferty et al. 1992]. Sub grammars express related words are connected by a graph edge th at bears a label that encodes the nature of their linguistic relationship. Each word in the sentence bears a collection of links referred to as disjunct ( d ). A disjunct is a rule that shows how a word can be connected to other typical parse or linkage of a sentence S , we can define the joint probability where d 0 ....d n are the disjuncts obtained from a legal linkage K . P(S,K) =0 if the disjunct sequence does not constitute a legal linkage. Most languages like Tamil have variable word order. Variable word order language is making the language more fixed this makes it more complex. A dependency grammar is needed to parse a variable word order language like Tamil. Dependency grammar identifies the grammatical relation that connects one word to the other [Xu et al.. 2002]. It takes the head-dependent relation as basic. Dependency rules are framed as follows, corpus is generated. From the dependency tree structure obtained for the training corpus P(S,K), the joint probability of assigning the disjunct d to the words w in the corpus is their linkage with the disjuncts ( d 0 ,...d i-1 ). Let h i = w 0 d 0 ,..... .w i-1 d i-1  X  =0.5. The dependency based trigram value was estimated as Best results were obtained for  X  1=0.2 and  X  2=0.3 and  X  3=0.5. The bigram and trigram probability of the word sequences based on their previous history values of dependencies were analyzed and the results are listed in table V. The performance of the dependency based language model showed improved results when compared to the word-based language model. However the performance of the recognition system can be further improved by using class based language model as discussed in the next section. 6. CLASS-BASED LANGUAGE MODEL The n-gram language model is a stochastic language model, which predicts the next word different classes based on their semantic category. The grouping of words were based on their name, location name, transliterated person name, actions performed, characteristics words were defined as individual classes. The n-gram class models are computed based on the previous n-1 classes [Sun et al. 2002]. The bigram class based language model is estimated as follows language model is estimated as follows obtained by using the bigram and trigram class-based language models are listed in table VI. The performance of the recognition system can be further improved by the use of morpheme-based language model in highly inflected languages like Tamil. The use of morphemes and the design of the enhanced morpheme-based language model for Tamil language are discussed in the next section. 7. MORPHEME-BASED LANGUAGE MODEL One of the main problems for degradation in the performance of speech recognition system is due to out of vocabulary (OOV) words that occur in the test corpus. The solution for this problem is to increase the size of the training vocabulary to reduce the English Sing Sings OOV rate. But it is very difficult task in highly inflected languages like Tamil to reduce the OOV rate by increasing the vocabul ary size because of the great vocabulary expansion caused by large number of different words derived from basic stems. Many words have different forms for singular and plural and may also change with the gender. simple present form is shown in Table VII.  X  X  X   X  X  with different types of endings  X   X  X  X  X  X  X  X ,  X  X  X  X  X  X  X  X ,  X  X  X  X  X  X  X  X  X  X  X ,  X  X  X  X  X  X ,  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  and  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X . The size of the inflectional languages can be thus reduced by just storing only the root word  X  X  X   X  X  as the stem word and all the possible endings of the stem  X  X  X   X  X  as the ending words. existing Tamil morphological anal yser [Anandan et al. 2002]. Morphological Analyzer is a tool used to identify every morpheme of a word. Tamil morphological analyser was a Tamil Dictionary. A morphological analyzer breaks a word into its root word and associated morphemes. assigned a meaning. Tamil is a morphologically rich language in which most of the morphemes coordinate with the root words in the form of suffixes. Verbs can have tense marking, PNG (Person, number, and gender) marking, Auxiliary verbs and Clitics. PNG agrees with the subject of the sentence. No un can take Number marking, Case marking, Postposition, Adverbial/Adjectival suffix and Clitics. Before describing a morphological analyser for Tamil it is necessary to study the morphology of Tamil in detail. The morphology of Tamil can be describe d based on different parts of speech. and/or plural suffix and/or a postposition suffix and/or clitic suffixes. Verb Morphology: Verbs are defined as those words, which can take tense suffixes, verb and Clitic. Phases of Tamil Morphological Analyser: suffix combines with another suffix. These rules help to obtain inflected form from base forms of words. Sandhi Rules:  X  X andhi X  rules are the morpho-graphemic rules describing changes that different combinations of vowels. Some of the salient features of this analyser are 
By using this Tamil morphological analyser morphemes were generated for all the words in the Tamil text corpora. Stems a nd endings are marked with different marks (stems with # sign and endings with $ sign) in order to allow them to be conjoined back morpheme vocabulary generated for the Tamil corpus is shown in table IX. Usage of this model, lead to significant reduction in the size of the corpus and also reduced OOV rate evaluated based on the stem. 
In the general morpheme-based Language mode l morphemes are treated as if they are independent words [Creutz and Lagus 2002] and [Saraswathi and Geetha 2004]. No distinction is made between stems and endings. The major flaws in this morpheme-based following decomposition Word w i  X 1 decomposed as stem s i  X 1 and ending e i-1 Word w i decomposed as stem s i and ending e i s only , i.e., The probability of detecting an ending based on its stem is strong in the case of morphs, because particular stem can be followed by relatively small set of endings only. only i.e., language model does not provide strong dependency among the morpheme sequence generated Therefore some modifications are done on the morpheme-based language model. Corpus No. of distinct 8. ENHANCED MORPHEME-BASED LANGUAGE MODEL should be comparable to word bigram. Prediction of the ending is more complicated. Ending e i should depend on the corresponding stem s i . In addition, the Tamil language makes extensive use agree in gender, number and case. The morp hological categories ofte n affect word-ending e i. So Prediction of the ending e i depends on the previous ending e i-1 and the stem s i Since there is a strong dependency between th e stem and its endings, all possible endings the probability of occurrence of the morphs in the enhanced morpheme-based language model. calculated for stems as follows And the bigram probability estimate for the endings is calculated as ending gives less information on the occurrence of a stem and also the occurrence of the preceding ending gives less information on the occurrence of the next word ending. So The bigram probability of occurrence of a word-based on the stem-ending combination was calculated by occurrence of its stem and endings. If it is not possible to identify the word, based on the stem-ending combination then the number of occurrence of the stem ( s i )of the word ( W i ) is estimated. The bigram probability of the occurrence of the word W i , after smoothing is represented as better perplexity and WER were obtained. The results obtained for the enhanced morpheme-based language model is shown in table X. The enhanced morpheme-based word-based n-gram language models. based language model was analyzed with the results obtained from the distance-based language model, dependency-based language model, and class-based language model as discussed in the next section. 9. PERFORMANCE ANALYSIS For both the news and the politics corpora the enhanced morpheme-based trigram model with Katz back-off smoothing technique gave better perplexity and Word Error Rate when compared to the other language models. The comparison of the perplexity and WER values in the different language models is shown in figures 3 and 4. compared with the politics corpus. The main reason for this was due to the corpora used the politics corpus. The size of the news text corpus used for training purpose was 40% corpus is lower than the political corpus. The perplexity values are also less for the news corpus when compared to the politics corpus. language model gave improved perplexity and WER values over the bigram model. Improvements were done on the n-grams to consider the words that were even n-1 position away from them. This led to the design of distance based language models, whose performance showed only a slight improvement when compared to n-grams because of the free word order characteristics of Tamil language. A sentence in Tamil can be represented by different ways, changing their word order. Hence this led to the design of linguistic based language models for Tamil speech recognition system. The dependency based language model showed improved results than the n-gram approaches. This is due to the following limitations that exist in the dependency model. The performance of dependency model is limited to projective dependency grammar although certain constructions in Tamil are non-projec tive in nature. The dependency relations are limited to relations between heads and their dependents, whereas a more distance relation has to be taken into account for improving the performance of the model. The class based model showed greater improvement than the bigram-and trigram-based language model. For limited domain speech r ecognition class-based models gave good results. But for larger domain the class-based model generated too many n-grams that do not take into points in predicting the word sequences that led to inaccurate word predictions. there is a drastic change in the perplexity values obtained for different language models, morpheme based trigram language model. However, the change that occurs in WER value for the bigram language model and the lowest for the enhanced morpheme-based trigram language model. As Tamil is a free word order language the prediction of a word-help us to reduce the complexities in identifying the consecutive sequence of words. miniature value due to high free word order characteristics of Tamil language that excluded the correct recognition of words. For an increase in perplexity we find a decrease in WER in the politics domain by using class based bigram model. Perplexity system is chosen, and so the lower its value the better. It is a measure that depends on the similarity between words, which means that lower perplexity values may not always result in lower WER during recognition in all co rpora [Souto et al. 2 002]. The perplexity higher perplexity values. This is due to the following reason. In English the occurrence of certain sequence of words is not gender de pendent. For example in the sentence,  X  X e singer. But in Tamil it is represented as  X   X  X  X  X   X  X   X  X  X  X  X  X  X  X  and  X  X  X  X  X   X  X   X  X  X  X  X  X  X  X . The stem word is the same, but the endings ar e different for male and female speakers. So the number of branching factors to detect the occurrence of a word in the given corpus is comparatively low for the Tamil corpora. CONCLUSION In this paper a new language model based on morphs is designed. The performance of the enhanced morpheme-based language model was compared with the general word-based n-gram models, distance based models, dependency based models and class based language models. The performance of the language models were evaluated on two different speech corpora X  X  -news and politic s. The enhanced morpheme-based trigram language model with back-off smoothing technique gave better perplexity and recognition performance for the two Tamil corpora. The results have shown that the proposed enhanced morpheme-based language model is much better than the word-based language models. The future scope of this work is to test this technique on large test sets from various domains to prove their robustness. REFERENCES 
