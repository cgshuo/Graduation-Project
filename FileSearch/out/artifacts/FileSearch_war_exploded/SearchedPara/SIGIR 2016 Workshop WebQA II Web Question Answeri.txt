
Web search engines have made great progress in answering factoid queries, such as They can provide a succinct answer, up to a few words in length, and can sometimes offer additional information such as related facts or entities. However, Web search engines are currently not well-tailored for managing more complex questions, especially when they require explanation or de-scription, e.g., Given a question like this, currently, search engines resort to returning a link to a detailed Web document, which does not make sure the user can find an answer. Alternatively, such a question might be posted on a Community Ques-tion Answering (CQA) site, e.g., Qatar Living, 1 hoping to get a human-authored and detailed response. Other ques-tions submitted on the Web can be short and ambiguous (such as Web queries to a search engine). These issues make the WebQA task more challenging than traditional question answering, and finding the most effective approaches for it remains an open question.

This workshop is a second edition of the successful We-bQA workshop, which was held at SIGIR X 2015 [1]. The new edition continues the exploration of the boundaries of Web question answering for better understanding the spectrum of approaches and possible responses that are more detailed than a short fact, yet are more useful than a full document. In particular, we also focus on methods that can handle complex questions involving the interdependencies between different entities and facts. http://www.qatarliving.com/forum
We believe that a continuation of the first WebQA work-shop is important as many relevant publications have contin-ued to be disseminated across conferences. See for example, [6, 12, 13, 17, 18, 19, 20, 21, 23], also given the advent of neural networks, e.g., [5, 8, 9, 22, 24]. WebQA offers a fo-rum to researchers and practitioners to discuss and possibly collaborate, thus helping advance the state of the art.
This workshop also coincides and complements the LiveQA track at TREC 2015 and 2016 [2]. LiveQA is a revival of the TREC Question Answering track. The track provides a challenge and data for answering real user questions , posted live to the Yahoo! Answers site. The TREC 2016 LiveQA challenge evaluation was held around the time of the work-shop, which would provide both the active and the potential participants a way to discuss ideas and approaches.
Another relevant activity is the challenge on CQA orga-nized in 2015 and 2016 at SemEval, i.e., Task 3 [14, 15], which focused on answering new questions using a CQA fo-rum (Qatar Living). In particular, participants were asked to rerank the results returned by a search engine, and in ad-dition to select the good answers from a community forum (see for example the systems developed in [3, 10, 16]). Ad-ditionally, the challenge proposed a question-question simi-larity and an answer selection subtasks (see e.g., [4, 7, 11]).
The WebQA II workshop was held shortly after SemEval 2016, and thus it allowed the participants to discuss its out-come as well as further ideas in more detail. It is valuable for two more reasons: ( i ) there is still a lot of disagreement re-garding the goals and the nature of Web question answering, mostly related to the question intent (what kind of queries benefit from question answering compared to other meth-ods); and ( ii ) leading search engines are eager to provide question answering services, especially for mobile devices. [1] E. Agichtein, D. Carmel, C. L. Clarke, P. Paritosh, [2] E. Agichtein, D. Carmel, D. Harman, D. Pelleg, and [3] A. Barr  X on-Cede  X no, S. Filice, G. Da San Martino, [4] A. Barr  X on-Cede  X no, G. D. S. Martino, S. Joty, [5] A. Bordes, S. Chopra, and J. Weston. Question [6] L. Braunstain, O. Kurland, D. Carmel, I. Szpektor, [7] S. Filice, D. Croce, A. Moschitti, and R. Basili. KeLP [8] F. Guzm  X an, L. M`arquez, and P. Nakov. Machine
