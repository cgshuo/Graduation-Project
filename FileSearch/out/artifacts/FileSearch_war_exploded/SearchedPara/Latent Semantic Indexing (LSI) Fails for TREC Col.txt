 The aim of latent semantic indexing (LSI) is to uncover the re-lationships between terms, hidden concepts, and documents. LSI uses the matrix factorization technique known as singular value de-composition (SVD). In this paper, we apply LSI to standard bench-mark collections. We find that LSI yields poor retrieval accuracy on the TREC 2, 7, 8, and 2004 collections. We believe that the negative result is robust, because we try more LSI variants than any previous work. First, we show that using Okapi BM25 weights for terms in documents improves the performance of LSI. Second, we derive novel scoring methods that implement the ideas of query ex-pansion and score regularization in the LSI framework. Third, we show how to combine the BM25 method with LSI methods. All proposed methods are evaluated experimentally on the four TREC collections mentioned above. The experiments show that the new variants of LSI improve upon previous LSI methods. Neverthe-less, no way of using LSI achieves a worthwhile improvement in retrieval accuracy over BM25. The primary goal of an information retrieval system is to identify the subset of documents potentially most relevant to a given query. This is typically accomplished via a scoring method, which assigns each document a number measuring its estimated degree of poten-tial relevance to the query. Formally, a scoring method is a function that maps a query to a real-valued score vector of size n is the number of documents in the collection.
 Vector space approach. A query can be represented as a column vector of length m , where m is the vocabulary size. The i ponent of the vector counts the occurrences of word i in the query. This is often called a bag-of-words representation, since the order-ing of words is ignored. Similarly, a document d can also be repre-sented as a column vector of length m whose i th entry i counts the occurrences of term i in the document. Interpreting these vectors as points in Euclidean space motivates the scoring method called cosine similarity: cos ( q, d )= q  X  d/ | q || d | where | x | clidean length of a vector x . The value cos ( q, d ) is the cosine of the angle between the vectors q and d in m -dimensional space. It depends only on the directions of q and d , and is independent of their magnitudes, which is desirable as it avoids assigning higher scores to longer documents.
 Denote by A the m  X  n matrix whose columns are document vec-tors as described above. Given A , which is called the term-document matrix, the score vector for cosine similarity can be computed as where / and  X  are point-wise division and point-wise square root, the diag operator extracts the diagonal of a matrix, and superscript T indicates the transpose of a vector or matrix. As we are only interested in the relative ordering of documents for a fixed query, normalization by the length of q is not needed.
 Latent semantic indexing (LSI). A major drawback of the vector space approach is that it fails to capture important phenomena in language usage, including polysemy, which is the ability of a sin-gle word such as  X  X ire X  to have different meanings, and synonymy, which is the ability of different words such as  X  X irearm X  and  X  X un X  to have similar meanings. From the definition of cosine similarity, it follows that only terms present in both the query and the docu-ment contribute to a document X  X  score. If a query uses the word  X  X eat, X  while a document uses the word  X  X hermal, X  that document is not considered relevant under cosine similarity. In other words, different word choices can render two document vectors dissimilar in direction, even though they are similar in meaning. The key idea of LSI is that documents and words are related via concepts. The phenomenon of synonymy is captured by permitting different word choices within a concept, and the phenomenon of polysemy is captured by permitting a word to have different mean-ings across concepts. The number of independent dimensions is the number of concepts k , which is usually much smaller than the number of words in the vocabulary m .
 Mathematically, LSI is based on the singular value decomposition (SVD) of A [Deerwester et al., 1990]. Let r be the rank of r  X  min ( m, n ) . The SVD is the special matrix product where U is an orthonormal m  X  r matrix, S is an r  X  r diagonal square matrix, and V is an orthonormal n  X  r matrix. The rows of
U and V are interpreted as representations of terms and docu-ments respectively in concept space. The diagonal elements of are numbers that are called the singular values of A , sorted in non-decreasing order. Each value is interpreted as the strength of the concept in the overall document collection.
 LSI truncates the matrices U , S , and V to m  X  k , k  X  k k respectively, where k &lt; r . The columns of U and V that are retained are left unchanged, and the truncated matrices continue to be orthonormal. The hope is that this reduction in dimensionality results in eliminating noise while still capturing the most significant concepts. The truncated matrices are denoted by U k , S k The value of k is called the number of dimensions of LSI. The matrix A k = U k S k V T k is a low rank approximation of the closest approximation to A among all matrices of rank measured by squared error.
 A common way to score documents using LSI uses A k instead of A to compute the score vector for cosine similarity: While A is typically sparse, A k is not. A k can be viewed as a smoothed representation of A : many terms that have a zero-weight in A have a non-zero weight in A k .
 Two major obstacles to using LSI on large collections are apparent. First, for large collections, while A can be represented efficiently owing to its sparse nature, A k cannot be. Second, computing the SVD of large matrices is expensive. Previous work on using LSI for ad hoc retrieval on TREC collections, such as [Dumais, 1995], computes the exact SVD of only a fraction of documents and terms; the rest are included by a heuristic called folding-in. However, present day computational power lets us compute the exact SVD using the entire term-document matrix. The TREC-2 collection for which we report results is about twice the size of the largest col-lection used in previous published research on LSI for information retrieval, according to Table 1 of [Bradford, 2008] (but see also [Yan et al., 2009]).
 Contributions. Before the SVD is computed, the term-document matrix is typically transformed using a weighting scheme. The idea is to weight terms based on their importance and discriminative power. After Section 2 below explains the Okapi BM25 weight-ing scheme, Section 3 shows how to use this weighting scheme to improve LSI performance. While using A k instead of A is one method to score documents using LSI, several other methods can be motivated using the properties of U k , S k , and V k . Section 4 ex-plores novel LSI-based scoring methods and relationships between them. Score vectors from two different scoring methods can be combined to yield a new score vector, and thereby a new scoring method. If the two scoring methods have complementary advan-tages, the combined scoring method may perform better than either scoring method alone. Section 5 defines scoring methods that lin-early interpolate other scoring methods. Next, Section 6 explains the design of our experiments, and Section 7 reports performance results for different term-weighting schemes and scoring methods. Section 8 concludes the paper. The so-called Okapi BM25 method [Robertson and Walker, 1994] is the best currently known vector-based scoring method. It scores a document d according to the following formula: Above, q i is the i th query term, k 1 is a parameter that tunes the scaling of term frequency, b is a parameter that tunes the scaling of document length, count ( q i ,d ) is the frequency of term l ( d ) is the length of document d , and L is the average document length. The factor IDF ( t ) for term t is where n ( t ) is the number of documents with term t and n total number of documents in the collection.
 BM25 weights are a combination of local and global components, similar to many term frequency (TF) and inverse document fre-quency (IDF) weighting schemes. Although the underlying moti-vation is different, BM25 weights can be used computationally in the same way as weights for terms of other origins. In particular, we define the matrix A as follows: each entry ( i, j ) is the BM25 weight assigned to term t i in document d j using the formula where count ( t i ,d j ) is the frequency of term t i in d BM25 score vector can be computed as q T A . We call the matrix just defined the BM25 matrix.
 To be precise, this scoring method assigns the same scores as the standard BM25 formula only if each term in the query occurs ex-actly once. This assumption usually holds for short queries. A difference between the BM25 method and cosine similarity is that the columns of the BM25 matrix are not normalized. A conceptual reason for this difference is that the BM25 motivation is not geo-metric. A practical reason is that the parameter b already provides approximate length normalization. Weighting schemes are critical to the performance of LSI for at least two reasons. To see the first reason, consider the case of func-tion words. These words are likely to occur with many unrelated terms. It is undesirable to have these unrelated terms appear to be related via function words. Therefore a weighting scheme should have the property that terms that co-occur with many others (such as function words) are down-weighted. A weighting scheme that is popular in the LSI literature is the log-entropy scheme, where the weight for term t i in document d j is computed as where p ij is an estimate of the probability of occurrence of term t in document d j (usually a maximum likelihood estimate). From the definition it follows that the log-entropy scheme has the desir-able property that terms that are equally likely to occur in every document have the smallest weights.
 The second reason why weighting schemes are important is that in LSI the rows of the U k matrix representing rare terms, which are often highly discriminative, typically have small norms. This has a negative impact on LSI retrieval performance and both log-entropy and traditional IDF schemes fail to address this problem [Husbands et al., 2005]. Therefore, a second desirable property of a weighting scheme is that terms with low document frequencies should be emphasized.
 We propose the BM25 weighting scheme as an alternative to log-entropy for use with LSI. It is evident from the definition of the IDF factor in BM25 that it has both desirable properties mentioned above. Indeed, if a term appears in more than half the documents in a collection, BM25 assigns it a negative weight. While entropy takes into account the distribution of a term across documents, BM25 does not. This property seems desirable, as terms that occur in many documents should be down-weighted regardless of minor variations in distribution across documents. In this section we explain multiple ways to use LSI for scoring the relevance of documents to queries. Some of these methods are novel, while others are not. All are well-motivated, so choosing be-tween them depends on their empirical performance. Where possi-ble, we identify which methods are seemingly different, but in fact mathematically identical. Two novel methods reveal that query ex-pansion and score regularization are related to LSI.
 As stated earlier, a common way to score documents using LSI is to use A k instead of A , as in equation (1). We call this scoring method s . One way to interpret s 1 is to consider A k as a smoothed version of
A . Another way is as follows: given a query vector q , its repre-sentation in concept space can be obtained as q ! = q T U is a row vector of length k . A representation of documents in con-cept space is S k V T k . Cosine similarity in concept space between a query and all documents can then be computed as The numerator of s ! 1 ( q ) simplifies to q T A k . The denominator of s ( q ) simplifies to Because S k is a square matrix, S T k = S k , and as U k is orthonormal, U k U k is the identity matrix I k . Therefore s ! 1 ( q ) equals Alternatively, a query in concept space can be compared directly to documents in concept space, ignoring the strength of concepts. Recall that each row of V k , i.e. each column of V T k , is the rep-resentation of a document in concept space. This scoring method is The s 2 method differs from s 1 by the absence of S k . Intuitively, s assigns equal importance to all concepts. One motivation for is that the overall strength of concepts in a collection may not be relevant to using the specific concepts in a query to find documents with the same concepts. A second motivation is that S k is prone to overfitting. Suppose that the document collection used to define A is a finite sample from an underlying population. In this case the large entries in S k are guaranteed to be larger than the corre-sponding true entries, while the small entries are guaranteed to be too small [Kjems et al., 2001].
 Query expansion. Query expansion is the idea that a query can be clarified without changing its meaning by adding appropriate terms. Adding appropriate terms enhances the chances of finding documents which are relevant, but different in word choice. Con-sider a word similarity matrix W where the entry at ( i, j ) how similar term t i and term t j are. W is a square, symmetric m  X  m matrix. Given such a W , define q !! = (1 /m ) q T W term t i and q . The scalar factor (1 /m ) can be omitted as it is con-stant for a given query, so q !! can be defined to be q T A trivial example of such a term-similarity matrix W is the identity matrix I m , where each term is only similar to itself. Then, that each row of U k is a representation of a word in concept space. Therefore the inner product of two such rows can be interpreted as a measure of similarity between words. It follows that is an m  X  m matrix of word similarity scores. This word similar-ity matrix can be used for query expansion to expand the original query. The expanded query can then be compared with normalized documents. We call this method s 3 : The s 3 method uses a smoothed representation of queries, com-pared to s 1 which uses a smoothed representation of documents. Several other LSI-based methods turn out to be equivalent to For instance, using A k instead of A in s 3 yields which is the same as s 1 .
 As another example, we could expand documents using U k U stead of expanding queries. This idea yields It turns out to be the same as expanding queries instead of docu-ments. Similarly, expanding both queries and documents also sim-plifies to s 3 .
 A variation on s 3 is to perform query expansion using a row-normalized version of the U k matrix. This is akin to using cosine similarity be-tween term vectors in concept space as a measure of term similarity. Let us denote the row-normalized version of U k by  X  U k , and call this scoring method s 4 : After the rows of U k are normalized, it is no longer orthonormal. Normalization of rows may nevertheless be desirable if similarity between terms should be independent of the magnitude of their rep-resentations.
 Score regularization. The idea of score regularization is that sim-ilar documents should have similar scores [Diaz, 2005]. A way to regularize scores is as follows. For each document, we compute the regularized score as a weighted average of raw scores assigned to all other documents, where similarity scores between documents function as weights. Given a matrix D of similarity scores between documents, and a vector of raw scores s , the regularized score vec-tor is s ! = (1 /n ) sD . The scalar factor (1 /n ) can be omitted with-out affecting the relative ordering of documents, so the definition can be just s ! = sD .
 An example of such a document similarity matrix is D = V k Recall that each row of V k is a representation of a document in concept space. The inner product of two document vectors in con-cept space can be interpreted as a measure of similarity between documents. Then, the matrix V k V T k is an n  X  n matrix of docu-ment similarity scores. Once a score vector is computed, it can be regularized using V k V T k . We call this scoring method Since the division is pointwise, s 5 can also be written as Similar to s 3 , using A k instead of A recovers s 1 since identity matrix I n .
 Analogous to the normalization of the rows of U k in s 4 , the rows of V can be normalized to yield  X  V k , yielding cosine similarity in con-cept space as a measure of document similarity. Note that this nor-malization loses the orthonormal property of V k . Using the  X  matrix to regularize gives the scoring method s 6 : Given two scoring methods s and s ! , a new scoring method be defined by linear interpolation as where 0  X   X   X  1 . We call  X  the interpolation parameter. As it is allowed to take the values 0 and 1, it follows that s !! performs at least as well as the better of s and s ! , after a parameter search for  X  . If the methods s and s ! have complementary advantages, for instance if s performs better on precision and s ! performs better on recall, one can hope s !! performs better overall.
 Of course, linear interpolation can be applied to any IR scoring methods [Vogt and Cottrell, 1999]. We investigate here combina-tions of both cosine similarity and BM25 with multiple LSI meth-ods. The specific combination of standard LSI and cosine similarity has been investigated on small collections previously [Kontostathis, 2007]. Cosine similarity and s 1 can be combined as s ( q )=  X  q T A k / where  X  can be interpreted as calibrating the extent to which smooths A . We call this scoring method s 1 + . The setting recovers s 1 , and the setting  X  =0 recovers cosine similarity. Sim-ilarly, s 2 can be combined with cosine similarity to yield scoring method s + s ( q )= q T [  X  U k V T k / When s 3 is combined with cosine similarity, we get s + where  X  can be interpreted as calibrating the degree of query ex-pansion. Similarly, s 4 can be combined with cosine similarity to yield s + When s 5 is combined with cosine similarity, we get s + And, s 6 can be combined with cosine similarity to yield s The six combination methods above have analogs where BM25 re-places cosine similarity. In these six analogs, the column normal-ization factor is omitted for the BM25 component method. For instance, s + with A being the BM25 matrix. For all combination methods, score vectors are L 1 normalized before interpolation.

TREC-2004 Robust 128360 528140 250 The main goal of our experiments is to investigate whether the new scoring methods described above perform better than tradi-tional BM25 and LSI. The former is a high-performing baseline that most published IR methods fail to improve upon [Armstrong et al., 2009]. The choices include the following: (1) Which test collections to perform experiments on? (2) Preprocessing: which fields to include in documents and queries; whether or not to per-form stemming, stopping, and which algorithms to choose for each; whether to retain all terms or to eliminate terms with small global frequencies. (3) LSI choices: which weighting schemes to use, and whether to normalize the columns (i.e., documents) before SVD. (4) Parameters: what range and step size to choose for various pa-rameters: dimensionality k in LSI, k 1 and b in BM25, and the linear interpolation parameter  X  . (5) Parameter search: what search algo-rithm to use for parameter search. (6) Performance measure: which metric of retrieval success to use. (7) Generalization: whether to perform cross-validation during parameter search; whether to cross-validate on subsets of queries, or on subsets of corpora. For (1) we choose the TREC-7 ad hoc, TREC-8 ad hoc, and TREC-2004 robust test collections as they have the highest frequency of scores reported for ad hoc retrieval in recent years [Armstrong et al., 2009]. Note that these three collections consist of the same docu-ments, but have different topic sets. The TREC-2004 robust topic set is a superset of both the TREC-7 ad hoc and TREC-8 ad hoc topic sets; because it has 250 queries it is best for testing whether differences between scoring methods are significant. Last, we in-clude the TREC-2 ad hoc collection to verify that the scoring meth-ods scale to a relatively large collection. Table 1 presents the basic statistics for each collection.
 For (2) we use the Porter stemming algorithm and a standard stop word list. The term count reported in the table above is after stem-ming and stopping. We eliminate terms that occur in 5 or fewer documents. For example, for TREC-2, three quarters of the terms are eliminated from the vocabulary. We use only the title fields from topics, following common practice, as our aim is to evalu-ate performance on short queries similar to those that casual users would tend to generate. Only the text fields of documents are used. For (3) we compare log-entropy and BM25 matrices. Informal ex-periments show that normalizing the columns of log-entropy matri-ces yields better results, so we do that. Columns of BM25 matrices are not normalized. For (4) we vary the LSI parameter k from 10 to 300, in steps of 10, the BM25 parameters k 1 from 1 to 3 in steps of 0.1 and b from 0.05 to 1 in steps of 0.05, and the interpolation parameter  X  from 0 to 1 in steps of 0.1. All ranges are inclusive of end points. For (5) we use grid search.
 For (6) we use non-interpolated mean average precision as the per-formance measure. This is the most often reported performance measure and is considered stable [Buckley and Voorhees, 2000]. Below, we use the terms  X  X erformance X  and  X  X AP X  interchange-ably. Choice (7) is difficult. In the field of IR, the method of cross validation to find parameters that generalize well is not often used for the following reasons. The number of queries in a topic set is often not large enough to permit effective cross validation, and Table 2: MAP scores for BM25, cosine similarity with log-entropy weights, and LSI methods with BM25 weights, on TREC collec-tions.
 cross validation on a subset of collections may not be desirable as the assumption that different collections are independently identi-cally distributed is not meaningful, or valid. In this work, we find collection-specific parameters that perform best on each query set. Our primary aim is to compare and contrast different scoring meth-ods. As we follow the same methodology for all the methods, we believe that a fair comparison ensues. Table 2 shows the performance of various scoring methods on TREC collections. The first two rows of the table report MAP scores for BM25 and for cosine similarity with log-entropy weights. For the former method, the best parameter values are shown. It is evident that BM25 outperforms cosine similarity dramatically. Given that both methods do local and global (TF and IDF) weighting, the dif-ference in performance suggests that weighting scheme details can have a major impact.
 Table 3 contrasts the performance of various LSI-based scoring methods using the log-entropy and the BM25 weighting schemes on two TREC collections. Using BM25 weights is almost always best. Findings on small collections and on the other two TREC collections (not shown) are similar.
 Performance results for the LSI-based methods s 1 to s 6 in Table 2 are based on the SVD of A using BM25 weights. These weights use the b and k 1 parameters that are optimal for the BM25 method as shown in the first row of each table. When BM25 is used as a weighting scheme prior to LSI, the optimal parameters may be different, but fixed parameters are used for the sake of simplicity. Table 2 shows that the new LSI methods sometimes outperform the standard LSI method s 1 . In particular, method s 2 , which assigns the same importance to all concepts, outperforms s 1 on all four TREC collections. Unfortunately, all methods based on LSI per-form far worse than standard BM25, or even cosine similarity, on TREC collections.
 As the results appeared rather surprising at first, we verified the MAP scores using the TREC evaluation scripts and the Terrier soft-ware [Ounis et al., 2006]. We also verified that LSI methods per-Table 3: MAP scores with BM25 versus log-entropy weighting for LSI-based scoring methods on two TREC collections.
 Figure 1: Mean average precision (MAP) as a function of the di-mensionality k of LSI for method s 1 on the TREC 8 collection. Results for other variants of LSI and other collections are similar. form well on small collections, as reported in previous work. Even though LSI-based methods are inferior by themselves, they may still be complementary to other methods. Table 4 shows the performance of combination methods s + and method, the best  X  and k values are shown. (LSI is not used when  X  =0 is best, so no value for k is shown.) The methods s , s + The improvements are small and not of practical importance. For example, for TREC-2 ad hoc and TREC-8, ad hoc s + about 4% better than optimized BM25. However, these improve-ments are over a strong baseline, and are comparable to the im-provements reported for query expansion or language modeling in previous publications [Armstrong et al., 2009].
 In Tables 2 to 4, the MAP values of s 3 and s 5 , or of s are almost identical. This is an interesting finding, because based on the idea of query expansion, while s 5 is based on the conceptually different idea of score regularization. We hypothesize that the algebraic analysis of Section 4 can be extended to explain this approximate equivalence.
 Table 4: MAP scores for combination methods on TREC collec-tions. LSI-based methods use BM25 weights.
 We have not provided an explicit analysis of the statistical signifi-cance of differences between mean average precision numbers re-ported above. In a nutshell, most differences are not significant. The reason is that most collections have only a small number of queries (50 for TREC 2, 7, and 8), while the average precision (AP) has high variability across queries. For example, the typi-cal standard deviation of AP results on the TREC-8 collection is around 0.22, so the 95% confidence interval around an MAP result is  X  0 . 062 [Webber et al., 2008]. The conclusion that most dif-ferences between methods are not significant is also true of many other published studies using these standard collections. The dif-ferences between log-entropy, BM25, and LSI methods above are statistically reliable.
 An obvious possible reason why LSI underperforms in the exper-iments above is that the required number of dimensions k is large for TREC collections. However, our experiments indicate that this is not the case. The increase in MAP as k increases is modest for all four TREC collections; see Figure 1. For the TREC 2 ad hoc collection, for the standard LSI scoring method s 1 , the optimal is less than 300. On other collections k values up to 1700 were tried in previous research [Jiang and Littman, 2000], but they did not yield much improvement. The results in Figure 1 are consistent with that finding. The influence of dimensionality on LSI has also been investigated for large collections of documents, for the pur-pose of measuring semantic similarity between words [Bradford, 2008]. That paper finds that the optimal dimensionality is different for different word pairs, but not larger than k = 500 . This conclu-sion again indicates that the bad performance of LSI is unlikely to be due to values of k that are much too small. Armstrong, T. G., Moffat, A., Webber, W., and Zobel, J. (2009).
Improvements that don X  X  add up: ad-hoc retrieval results since 1998. In Proceedings of the 18th ACM Conference on Informa-tion and Knowledge Management (CIKM) , pages 601 X 610.

Bradford, R. B. (2008). An empirical study of required dimen-sionality for large-scale latent semantic indexing applications. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM) , pages 153 X 162.

Buckley, C. and Voorhees, E. M. (2000). Evaluating evaluation measure stability. In Proceedings of the 23rd ACM Conference on Research and Development in Information Retrieval (SIGIR) , pages 33 X 40.

Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990). Indexing by latent semantic analy-sis. Journal of the American Society for Information Science , 41(6):391 X 407.

Diaz, F. (2005). Regularizing ad hoc retrieval scores. In Pro-ceedings of the 14th ACM Conference on Information and Knowledge Management (CIKM) , pages 672 X 679.

Dumais, S. T. (1995). Latent semantic indexing (LSI): TREC-3 report. In Overview of the Third Text REtrieval Conference , pages 219 X 230.

Husbands, P., Simon, H. D., and Ding, C. H. Q. (2005). Term norm distribution and its effects on latent semantic indexing. In-formation Processing and Management , 41(4):777 X 787.

Jiang, F. and Littman, M. L. (2000). Approximate dimension equalization in vector-based information retrieval. In Proceed-ings of the 17th International Conference on Machine Learning (ICML) , pages 423 X 430.

Kjems, U., Hansen, L. K., Strother, S. C., et al. (2001). Gen-eralizable singular value decomposition for ill-posed datasets.
In Advances in Neural Information Processing Systems (NIPS) , pages 549 X 555.

Kontostathis, A. (2007). Essential dimensions of latent semantic indexing (LSI). In Proceedings of the 40th Hawaii International
International Conference on Systems Science (HICSS) , pages 73 X 80. IEEE Computer Society.

Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, C., and Lioma, C. (2006). Terrier: A high performance and scal-able information retrieval platform. In Proceedings of the 29th ACM Conference on Research and Development in Information Retrieval (SIGIR) , pages 18 X 24.

Robertson, S. E. and Walker, S. (1994). Some simple effec-tive approximations to the 2-Poisson model for probabilistic weighted retrieval. In Proceedings of the 17th ACM Conference on Research and Development in Information Retrieval (SIGIR) , pages 232 X 241.

Vogt, C. C. and Cottrell, G. W. (1999). Fusion via a linear com-bination of scores. Information Retrieval , 1(3):151 X 173.
Webber, W., Moffat, A., and Zobel, J. (2008). Score standard-ization for inter-collection comparison of retrieval systems. In
Proceedings of the 31st ACM Conference on Research and De-velopment in Information Retrieval (SIGIR) , pages 51 X 58.
Yan, J., Yan, S., Liu, N., and Chen, Z. (2009). Straightforward feature selection for scalable latent semantic indexing. In Pro-ceedings of the SIAM International Conference on Data Mining , pages 1159 X 1170.

