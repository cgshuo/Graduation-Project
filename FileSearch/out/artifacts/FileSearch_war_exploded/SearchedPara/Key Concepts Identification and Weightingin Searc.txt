 As the search queries are becoming closer to natural language, discovering key concepts from search queries and assign ing reasonable weighting to them is an important way to understand user X  X  sear ch goals. Key concepts identification and weighting are key points in a serious of query analysis issues. Not only can it be directly used by search engine, but also it acts an important role in several research issues, such as query rewriting, query expansion, etc. It X  X  the foundation of many query processing problems.

Current commercial search engines process billions of queries per day [1], most of them are free text. There are a lot of useless terms in those queries. For example, for the query  X  X hat X  X  the weather of Chicago in Sunday X , it is not friendly to search engine. If the search e ngine could identify the key concept of this query is  X  X hicago weather X  and there X  X  no need to retrieve documents use other terms. The relevance of results would be remarkably improved. There are often several concepts in a query. We use noun phrases extracted from the queries as concepts. Our goal is assigning 3 different levels weight to them according to their importance. There is no need to a ssign different weight to different key concepts of a query, for they are all the most important parts of the query. We will illustrate it in the following example.

In the TREC topic of Table 1, we treat the &lt;desc&gt; part as a raw query. We expect it can generate the followi ng results after processing.

Weight 3 means the most important concepts of the query. Weight 2 is assigned to the concepts which are important to the query, but not the necessary parts. We identify the concepts with Weight 2 in order to guarantee the recall of the retrieved documents. Meanwhile, we assign Weight 1 to the useless ones.
There are three primary contributions in this paper: (1)we propose a super-vised machine learning technique for automatic extracting key concepts; (2)dis-covering key concepts from natural language queries with anchor analysis, since anchor and queries are organized by similar language; (3)not only discovering the most important concepts in queries, but also classifying them into 3 levels. It can guarantee the reca ll rate of documents.

The rest of this paper is organized as fo llows: In section 2 we discuss previ-ous work on key concepts identification and weighting. We describe our proposed model X  X  details in Section 3 and our experimental results are shown in Section 4. And finally, section 5 summarizes the conclusions and gives a look to future work. Most traditional information retrieval models, such as language modeling, treat query terms as independent and of uniform importance [2]. For example, inverse document frequency( idf ) is thought of as a simple query term weighting model, however, it hardly contains much information. Meanwhile, it is not clear if idf is a proper standard to judge the importance of phrases and other generic concepts [3]. Furthermore, most of concepts det ection work treat all concepts matches in queries equally, rather than weighting them [16,17,18,19,4,8,12]. As queries are becoming more complex, it is clearly m uch unreasonable for these queries.
Some previous work on key concept detect ion use linguistic and statistical methods to discover core terms in &lt;desc&gt; queries and use name entity recog-nition to convert &lt;desc&gt; queries into structured INQUERY queries [20]. While our proposed model towards all kinds of general search queries.

Key concept detection, focused on verbo se queries, has been a subject in recent work. Bendersky et. al [6] proposed a supervised machine learning technique for key concept detection. In their model, nea rly all features were statistical count in documents( tf,idf,etc. ). However, documents and search queries are organized by so much different language. Kumaran et al. [9] use learning to rank all sub-sets of the original query (sub-queries) based on their predicted quality for query reduction. Although similar to our work, the weighting techniques discussed in these papers are based on document counts, while our model is based on anchor analysis.

In sum, most of previous work on key con cept detection via statistical methods (such as inverse document frequency) [10, 11, 13] or machine learning methods are based on direct web counts in documents [6]. However, queries and docu-ments are organized by so much different language [14]. Even the different parts of a document are organized differently, such as title, body, anchor, etc. Both anchor text and queries seem to be composed in a way that how users summarize documents. In contrast to previous work, we do not treat all parts of documents as the same. We use a supervised machin e learning technique for key concepts detection and use a diverse mix of features which extracted via anchor analysis. In this section we present our model for k ey concepts detect ion in natural lan-guage queries. First we describe the details of the supervised machine learning technique with anchor analysis. Then we detail the feature selection method that we used. 3.1 Anchor Analysis Most of text in documents and queries are organized by different languages. Main text in documents try to detail problems or describe something, while queries often ask for information by s ummarized words. A query can often be regarded as a very brief summary of a document which may be the best fit of user X  X  expectation. Anchor texts are so similar to queries on this point. So we try to extract some significant features via anchor analysis. We extract anchor texts and the relationship between anchor and documents from billions of web pages 1 . The following are features we considered in our model.
 Concept Frequency( cf ( c i )). Concept Frequency( cf ( c i )) in all anchor texts. It is supposed that key concepts will tend to have a higher concept frequency in anchor text than non-key concepts. Inverse Host Frequency( ihf ( c i )). Concept ihf in the corpus. It inspired by the inverse document frequency. The inverse document frequency is a measure of the general importance of the term (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient). The inverse host frequency tend to filter out common concepts. In our paper, the form of ihf ( c i )weusedis where | H | is the total number of hosts in the corpus. |{ h : c i  X  h }| is the number of hosts which the concept c i points to.
 Concept Frequency-Inverse Anchor Frequency( cf iaf ( c i )). The concept frequency only considers the raw concept frequency in all anchor texts and treats different anchor-url pair equally even if some urls are very heavily linked. We define ( a i ,d j ) = 1 when there are at least one link between anchor a i and url d . Intuitionally, more general urls would be linked with more anchors. Using information theory, we define the entropy of a url d j as wherewesuppose p ( a i | d j )= 1 to url d j . So the maximum entropy of url d j is Then the inverse anchor frequency is defined as where | D | is the total number of urls in our corpus. iaf ( d j ) is inspired by Entropy-biased model [23]. The most important function of iqf ( d j ) is lessen the impact of some very popular urls and trying to balance the bias of them. Finally, cf iaf ( c i ) is defined as where cf ( c i ) is the number of urls which the concept c i linked. The number of concept X  X  different destination host( hf ( c i )). We define the source host of anchor is the host it appears. Destination host of anchor is the host it point to. The host frequency is defined as where sh j is the source host of concept c i , dh k is the destination host of concept c .( sh j ,c i ,dh k )=1,if sh j = dh k . 3.2 Other Features Inverse document frequency idf ( c i ). IDF is commonly used in information retrieval as a weighting function [13]. It is defined as where | D | is the number of documents 2 and |{ d : t i  X  d }| is the number of documents where the concept c i appears.
 Residual IDF ridf ( c i ). Residual IDF [6] is the difference between IDF and the value predicted by a Poisson model [22]. documents. It is supposed that only the non-content concepts X  distribution fits the Poisson distribution. ( c i ) . [6] qp ( c i ) is the number of a concept c i was used as a part of some queries in the query log. qe ( c i ) is the number of a concept was used as a query in the query log. We extract these two features from billions of queries from the query log 3 . It is supposed that the ratio of the key concepts would be higher. 3.3 Feature Selection Method Pairwise Comparison. Pairwise Comparison [21] measures the similarity of two ranking results. Let E is the set of all elements. A and B are two ranking results of E. The Pairwise Comparison of A and B is defined as where  X  a i ,a j  X  A ,if i&gt;j ,then a i  X  A a j .
 Feature Selection with Pairwise Comparison. The Feature Selection method is based on greedy algorithm. It worked well in some similar machine learning issues [15]. It is defined as follows: Algorithm 1. Feature Selection with Pairwise Comparison Bas is a ranking result as the basis of the algorithm. For each feature f in C, we calculate the Pairwise Comparison between Bas and S { f } .Welet Pairwise Comparison ( A,  X  )=0,if A =  X  . In this section, we describe the details about our experiment results of our work. First, we provide a brief introduction of the corpora we used in our experiments. Then we assess our model with 3 experim ents. Section 4.2 assess the effective-ness of our supervised machine learning model with a non-supervised weighting approach. Section 4.3 analyze the utility of various features used in our model with Feature Selection Method outlined in Section in 3.3 and a feature contribu-tion experiment. Section 4.4 compare the result of our model with another well known supervised machine learning model [6]. 4.1 Experiments Preparation The data source we used in our experiments are provided in Table 3. ROBUST04 is a newswire collection and W10g is a web collection. We also extract 5000 queries from search log 4 for assessing the effectiveness of our supervised machine learning model.

We employ Support Vector Machine as our machine learning algorithm. The details of the our supervised machine learning approach are outlined in Section 3. 4.2 Concept Weighting Results In this section, we assess our supervised machine learning model via comparing with a non-supervised weighting method. In the first step of this experiment, we examine how well our proposed model cl assify the concepts into 3 sets outlined in Section 1. Then we test whether our p roposed model outperforms a simple non-supervised weighting approach where idf ( c i ) weighting are directly used to classify the concepts. All &lt;desc&gt; queries in TREC collections and queries from search log are labeled by professional surfers.

In the first step of the experiment, we use our proposed model to classify the concepts into 3 sets according their importance.

We use a cross-validation approach to get the final test results. For the TREC collections, all &lt;desc&gt; queries are divided into subsets of 50 queries. For queries of search log , each subset contains 1000 queries. Each subset will be regard as a test set in turn, while the rest of subsets s erve as a training set. Experiments are run independently for each data collection. At last, average results on all test sets will be reported as our final results.

In the second step of the experiment, we use a simple non-supervised ap-proach, idf ( c i ) directly, to classify the concepts . We assign each concepts to the corresponding result set according to the idf ( c i ) ranking results. Each subset of collection is tested and the average results are reported.
Table 4 is the comparison of accuracy and recall results for ROBUST05, W10g collections and queries of search log, when using svm algorithm with the features detailed in Section 3 and a single idf for concept weighting.

Table 4 shows that for test results of all collections our proposed model out-performs idf ( c i ) ranking. We note that most of misclassification cases are the queries contain multiple key concepts. It X  X  the reason that the effectiveness on ROBUST04 collection is not so good as other two collections. For the topics of ROBUST04 collection tend to contain more verbose &lt;desc&gt; queries, multiple key concepts are more common in it. 4.3 Feature Analysis In this section, we analyze the utility of the various features we used in our supervised machine learning model. Fir st, we use feature selection method de-tailed in Section 3.3 to analyze the features. Then, in order to assess the feature contribution, for each feature, we evaluate the influence to the effectiveness of the model with the rest of features.
 Feature Selection. Pairwise Comparison and Greedy algorithm has been proven worked well in some similar machine learning problems [15]. We employ them as our feature selection method. Th e experiments described in previous section are repeated with different featur es combination. In ea ch iteration, the best feature will be moved to selected feat ures set. The details of the selection algorithm has been described in Section 3 .3. Table 5 reports the feature selec-tion experimental results. We can get seve ral conclusions from the results of the feature selection.

First, The results in Table 5 indicates that all features have positive impact to overall accuracy. However, we note tha t not all features are positive to every collection. For example, ridf ( c i ) have a little negative impact for the ROBUST04 collection. Second, for each feature, it h as different contribution to each collec-tion. It depends on the types of collectio ns and features. For example, the W10g pact on ROBUST04 collection. Third, though some features in our model are biased towards different collections, our proposed model is basically not biased towards any specific query type. It more general than simple non-supervised approaches.
 Feature Contribution. In order to assess the contribution and the impact of each feature, we repeat the experimen ts with different features set. In each iteration, a single feature is moved from the full feature set and the rest of features used to assess the effectiveness of the model. Table 6 reports the feature contribution experiments result. The po sitive value indicates the accuracy is increased after removing the feature, wh ile negative value indicates the accuracy is decreased after removing the feature.

From the results, we can get some similar conclusions in Feature Selection experiments. First, features are biased t oward different data collections. Same conclusion was also got in Feature Sel ection Experiments . Second, most of non-biased features do not perform very well, while the biased features are not general enough. It the restriction of non-supervised approach. However, super-vised machine learning approach as both biased features and non-biased features the input and get a general model for key concepts detection. 4.4 Key Concepts Identification Results In this section, we evaluate the effectiv eness of our proposed model via compar-ing with another supervised machine learning approach [6]. Bendersky et. al [6] proposed a supervised machine learning approach for discovering key concepts in verbose queries. They assign weighting to each concept and select the con-cept with the highest weight as the key c oncept of the query. They employ the AdaBoost.M1 meta-classifier with C4.5 decision trees as base learners [7]. They extract just one key concept from each verbose query, however, there are often not only one key concept in a natural language query. We try to extract all key concepts from queries. It X  X  the differ ence between our work and Bendersky et. al X  X  work [6]. Bendersky et. al X  X  work [6] are similar to us and can significant improve retrieval effectiveness for verb ose queries in their experiments. So we choose the method as the baseline.

In the experiments, we select top k co ncepts with baseline approach as the key concepts set. Our proposed model is designed to extract the key concepts set directly. We use the cross-validation approach outlined in Section 4.2 to get the experiment results.

Table 7 reports the accuracy and the mean average precision (MAP) results when either the baseline AdaBoost.M1 or our model. For the key concepts set may contain more than one key concept, so we use MAP to assess the effec-tiveness. The results in Table 7 and Fig. 1 show that for all tested collections our proposed approach outperforms the baseline method. We note that most of misclassification cases are ambiguous concepts. For the queries that contain several key concepts, it is hard to judge whether an ambiguous concept belongs to the key concepts set.
 In this paper we proposed a supervised machine learning approach for detect-ing the key concepts from natural language queries. We use 2 standard TREC collections and 1 Query co llection to assess the effectiveness of our model.
For modern search engines, one of the mai n issues in query analysis is detecting the key concepts from queries. It X  X  the foundation of many query processing prob-lems. The most difficult part of the problem is the results would be highly amplified in the retrieved documents. If the key con cepts were detected wrong, it would lead a lack of focus on the main topics of the query in the retrieved documents.
We use features with anchor analysis and other common features as the in-puts of our supervised machine learning algorithm. It is known that queries and documents are organized by different language, while anchor language is so much similar to query language. Both of them try to summarize documents. Moreover, the link relationship between anchors and urls are similar to the relationship be-tween queries and clicked urls in sea rch engine log. It has been proved that the anchor analysis approach performed well for key concepts identification and weighting problem in our experiments.

There are still some issues needed to be addressed as future work. First, we plan to extract more query-dependent features as the input of our proposed model. In the experiments, we observe d that some concepts are always non-key concepts. Intuitively, concepts in different queries should be assign different weight. However, most of features of machine learning algorithm are global fea-tures. They do not perform well on th ese queries. We need to extract more significant query-dependent features as the input of machine learning algorithm. Second, besides key concepts set, we pl an to pay more attention to second grade concepts in verbose queries, such as  X  download  X  in the former query. It is not the key concept, however, it indicates that the user want a download page, but not a introduction page. These second grade concepts are not the most important concepts for query, however, it also provide a lot of information indi-cate the users search goals.
 We are grateful to Honglei Zeng X  X  work on data preparation. Gui-Rong Xue also provided many very useful suggestions. We would also like to thank for Xin Fu and Prof. Bing Liu X  X  encouragement.

