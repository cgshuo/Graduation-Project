 Sida I. Wang sidaw@cs.stanford.edu Christopher D. Manning manning@stanford.edu Recent work (Hinton et al., 2012) has shown that pre-venting feature co-adaptation by dropout training is a promising method for regularization. Applied to neu-ral network training, the idea is to dropout (zero) ran-domly sampled hidden units and input features dur-ing each iteration of optimization. Dropout played an important role in the systems that won recent learning competitions such as ImageNet classification (Krizhevsky et al., 2012) and the Merck molecular ac-tivity challenge at www.kaggle.com , and improves per-formance on various tasks. Dropout can be considered another approach to regularization in addition to the widely used parameter shrinkage methods and model averaging. This process lowers the trust in a feature that is only helpful when other specific features are present, since any particular feature may be dropped out and cannot be depended on. Alternatively, the procedure can be seen as averaging over many neural networks with shared weights.
 Other observations of harmful co-adaptation and ways to address them exist in the literature. Naive Bayes, by completely ignoring co-adaptation, performs better than discriminative methods when there is little data (Ng &amp; Jordan, 2002), and continues to perform bet-ter on certain relatively large datasets (Wang &amp; Man-ning, 2012). In (Sutton et al., 2006), it is observed that training involves trade-offs among weights, where the presence of highly indicative features can cause other useful but weaker features to undertrain. They propose feature bagging: training different models on subsets of features that are later combined, an idea further pursued under the name logarithmic opinion pools by (Smith et al., 2005). Improved performance on Named Entity Recognition and Part-of-Speech Tag-ging was demonstrated.
 While the effectiveness of these methods in preventing feature co-adaptation has been demonstrated, actually sampling, or training multiple models, make training slower. Moreover, with a dropout rate of p , the pro-portion of data still not seen after n passes is p n (e.g., 5 passes of the data are required to see 95% of it at p = 0 . 5). If the data is not highly redundant, and if we make the relevant data only partially observable at random, then the task becomes even harder, and training efficiency may reduce further.
 In this paper, we look at how to achieve the benefit of dropout training without actually sampling, thereby using all the data efficiently. The approach uses a Gaussian approximation that is justified by the cen-tral limit theorem and empirical evidence. We show the validity of this approximation and how it can pro-vide an order of magnitude speed-up at training time, while also giving more stability. Fast dropout fits into the general framework of integrating out noise added to the training data (Matsuoka, 1992; Bishop, 1995). See (van der Maaten et al., 2013) for an alternative approach to integrating out noise and a survey of re-lated work from that angle. Their approach is exact for loss functions decomposable by the moment gen-erating function of the independent noise such as the exponential loss and squared error loss. Our approach does not require independence: it can integrate out small transformations that an image classifier should be invariant to. We begin with logistic regression for simplicity, then extend the idea to other loss functions, other noise, and neural networks. Code is provided at the author X  X  website. 2.1. The implied objective function We illustrate the idea with logistic regression (LR) given training vector x , and label y  X  X  0 , 1 } . To train LR with dropout on data with dimension m , first sam-ple z i  X  Bernoulli( p i ) for i = 1 ...m . Here p i is the probability of not dropping out input x i . After sam-pling z = { z i } i =1 ...m we can compute the stochastic gradient descent (sgd) update as follows: where D z = diag( z )  X  R m  X  m , and  X  ( x ) = 1 / (1 + e is the logistic function.
 This update rule, applied over the training data for multiple passes, can be seen as a Monte Carlo approx-imation to the following gradient: The objective function with the above gradient is the expected conditional log-likelihood of the label given the data with dropped out dimensions indicated by z , for y  X  Bernoulli(  X  ( w T D z x ))). This is the implied objective function for dropout training: L ( w ) = E z [log( p ( y | D z x ; w )] (2) = E z [ y log(  X  ( w T D z x )) + (1  X  y ) log(1  X   X  ( w T Since we are just taking an expectation, we still have a convex optimization problem provided that the neg-ative log-likelihood is convex.
 Evaluating the expectation in (1) naively by summing over all possible z has complexity O (2 m m ). Rather than directly computing the expectation with respect to z , we propose a variable transformation that allows us to approximately compute the expectation with re-spect to a simple random variable Y  X  R , instead of z  X  { 0 , 1 } m . In the next subsection, we describe an efficient O ( m ) approximation that is accurate for ma-chine learning applications where w i x i usually come from a unimodal or bounded distribution. 2.2. The Gaussian approximation We make the observation that evaluating the objective function L ( w ) involves taking the expectation with re-spect to the variable Y ( z ) = w T D z x = P m i w i a weighted sum of Bernoulli random variables. For most machine learning problems, { w i } typically forms a unimodal distribution centered at 0, { x i } is either unimodal or in a fixed interval. In this case, Y can be well approximated by a normal distribution even for relatively low dimensional data with m = 10. More technically, the Lyapunov condition is generally sat-isfied for a weighted sum of Bernoulli random vari-ables of the form Y that are weighted by real data (Lehmann, 1998). Then, Lyapunov X  X  central limit the-orem states that Y ( z ) tends to a normal distribution as m  X   X  (see figure 1). We empirically verify that the approximation is good for typical datasets of mod-erate dimensions, except when a couple of dimensions dominate all others (see figure 3). Finally, let S be the approximating Gaussian ( Y d  X  S ) where  X  N (0 , 1), E z [ Y ( z )] = P m i =1 p i w i x i Var [ Y ( z )] = P m i =1 p i (1  X  p i )( w i x i ) 2 . In the following subsections, based on the Gaussian as-sumption above, we present several approximations at different tradeoff points between speed and accuracy. In the end, we present experimental results showing that there is little to no performance loss when using the faster, less accurate approximations. 2.3. Gradient computation by sampling from Given good convergence, we note that drawing samples of the approximating Gaussian S of Y ( z ), a constant time operation, is much cheaper than drawing sam-ples of Y ( z ) directly, which takes O ( m ). This effect is very significant for high dimensional datasets. So without doing much, we can already approximate the objective function (2) m times faster by sampling from S instead of Y ( z ). Empirically, this approximation is within the variance of the direct MC approximation of (2) by taking 200 samples of z .
 Approximating the gradient introduces a complication when using samples from the Gaussian. The gradient (1) involves not only Y ( z ) d  X  X  X  S , but also D z x directly: Let f ( Y ( z )) = y  X   X  ( Y ( z )) and let g ( z ) = D Naively approximating E z [ f ( Y ( z )) g ( z )] by either E
S [ f ( S )] E z [ g ( z )], or worse, by f ( E s [ S ]) E poorly in terms of both approximation error and final performance. Note that g ( z ) is a linear function and to approximate (4) is by analytically taking the ex-pectation with respect to z i and then using a linear approximation to the conditional expectation. More precisely, consider dimension i of the gradient:  X  X  ( w ) where z  X  i is the collection of all other z s except z  X  , X  S is defined in (3),  X   X  i = (1  X  p i ) x i w i ,  X   X   X  p i (1  X  p i ) x 2 i w 2 i are the changes in  X  S ,  X  2 ditioning on z i . Note that the partial derivatives as once per training case, since they are independent of i .  X , X , X  can be computed by drawing K samples from S , taking time O ( K ) (whereas K samples of Y ( z ) take time O ( mK )). Concretely, computed by differentiating inside the expectation. One can combine (5) and what we do in (7) below to obtain a more accurate yet relatively cheap approxi-mation to the derivative. However, in practice, using only  X  approximates the derivative to within the vari-ance of successive MC computations of the objective L (see figure 4). Empirically, this is 2 X 30 times faster compared to MC dropout (see figure 5 and table 1). At a slightly higher loss in accuracy, we can get rid of z completely by re-parameterizing the problem in  X  s and  X  s and taking derivatives with respect to them instead of approximating the derivative directly. So the objective function (2) becomes 2.4. A closed-form approximation In the binary classification case, we can avoid sam-pling by tabulating  X , X , X  , and their partial deriva-tives (they are just functions of 2 arguments). Inter-estingly, an accurate closed-from approximation is also possible by using the Gaussian cumulative distribution function  X ( x ) = 1  X  logistic function. It can be shown by parameter differ-entiation with respect to  X  and then integrating with respect to  X  that Substituting in  X  ( x )  X   X ( p  X / 8 x ), we get This is an approximation that is used for Bayesian pre-diction when the posterior is approximated by a Gaus-sian (MacKay, 1992). As we now have a closed-form approximation of  X  , one can also obtain expressions for  X  and  X  by differentiating.
 Furthermore, by substituting x =  X  + st , differentiating with respect to  X  , and (7), we can even approximate the objective function (6) in a closed-form: E  X  p 1 +  X s 2 / 8 log  X  The actual objective as defined in (2) can be obtained from the above by observing that 1  X   X  ( x ) =  X  (  X  x ). The gradient and Hessian with respect to w can be found by analytically differentiating. 3.1. Least squares regression In contrast to all the approximations so far, dropout training of regression with squared error loss can be computed exactly. Let y be the true label and  X  Y = P i w i x i z i be the predicted label with  X  = E By the bias-variance decomposition, the expected squared error loss is Since (9) is completely determined by the mean and variance of  X  Y , it does not matter which distribution  X  Y comes from as long as  X  and s 2 are matched. As a result, (9) is also the exact loss function of the original dropout objective if we summed over z i instead. So over the whole dataset of size n , dropout regression has the following equivalent objective: This is a form of L 2 regularization depending on c i = regularized more strongly.
 Alternatively, let X  X  R n  X  m be the design matrix, then the normal equations for dropout training and ridge regression are, respectively, where diag( A ) represents the diagonal matrix with the same diagonal as A . The diagonal of X T X is stronger by a multiplicative factor 1 +  X  for dropout instead of the additive  X I for L 2 . The equivalent value for  X  determined by dropout is (1  X  p ) /p . 3.2. Hinge loss and the Maxout unit Our apporach can be applied to the classical hinge loss and the recently proposed maxout network (Goodfel-low et al., 2013). The structured SVM loss is where Y is the set of possible predictions and ` ( y,y 0 ) is the loss incurred by predicting  X  y when the true label is y . The maxout unit computes Under the fast dropout approach, both of these reduce to the problem of computing the maximum of Gaus-sians max i X i for X i  X  X  (  X  ( x,w i ) , X  2 ( x,w i )) not nec-essarily indepedent. Several approaches to this prob-lem is presented in (Ross, 2010). 3.3. Softmax and general loss Unfortunately, the best way to compute the cross-entropy loss for softmax seems to be sampling from the input Gaussian directly with S  X  R |Y| where Y is the set of possible predictions.
 The required partial derivatives can again be com-puted by differentiating inside the expectation. This is also the general way to do fast dropout training on output units that may be vector-valued functions of vectors. 3.4. Transformation invariance as noise More image data can be generated by applying trans-formations like small translations, rotations, shearing etc. to the original training data. A transformation of magnitude can be approximated locally by its Lie derivative as T  X  ( x ) = x + L T,x (Simard et al., 1996). For translation, rotation, shearing, we can generate more data by randomly sampling i  X  N (0 , X  2 i ) and computing X = x + P i i L i . Notice that w T X is again normally distributed and the techniques presented in this paper can be used to integrate out these transfor-mations without actually generating the transformed data. Here we do not need the central limit theorem and the noise is not independent.
 3.5. Other noise Like the exact approach in (van der Maaten et al., 2013), the Gaussian approximation can be applied to other noise models (Poisson, Gaussian, etc). We just need to characterize the noise in terms of its mean and variance and rely on the central limit theorem. Dropout training, as originally proposed, was intended for neural networks where hidden units are dropped out, instead of the data. Fast dropout is directly ap-plicable to dropping out the final hidden layer of neu-ral networks. In this section, we approximately extend our technique to deep neural networks and show how they apply to several popular types of hidden units. For the last layer of a neural network, any output unit outlined in section 3 can be used. 4.1. The hidden layers Under dropout training, each hidden unit takes a ran-dom variable as input, and produces a random vari-able as output. When the number of hidden units is more than 10 or so, we may again approximate their inputs as Gaussians and characterize their outputs by the output means and variances. A complication is that the inputs to hidden units have a covariance as shown in figure 2.
 Consider any hidden unit in dropout training. We may approximate its input as a Gaussian variable X  X  N ( x |  X ,s 2 ), and let its output mean and variance be  X  and  X  2 . E.g., for the commonly used sigmoid unit This integral can be evaluated exactly for the rectified linear unit f ( x ) = max(0 ,x ). Let r =  X /s , then  X  = The rectified linear unit is a special case of the maxout unit, for which techniques in (Ross, 2010) can be used to compute its mean and variance.
 With dropout training, each hidden unit also has an output variance. Sigmoid squared can be approxi-mated by a translatedscaled version of the sigmoid: a , b can be found by matching the values and deriva-tives ( a = 4  X  2 4.2. Training with backpropagation The resulting neural network can be trained by back-propagation with two sets of partial derivatives. In normal backpropagation, one only needs to keep  X  X   X  X  for each hidden unit i with input  X  i . For fast dropout training, we need  X  X   X  X  2 Where  X  i = p P j w ij  X  0 j and s 2 i = P j p (1  X  p )  X  p X  ance of the previous layer. In practice, the method still works well if we ignore the output variance  X  , so the input variance to the next layer is generated by dropout alone. Once we make the Gaussian approximation, there is an alternative interpretation of where the variance comes from. In the dropout framework, the variance comes from the dropout variable z . Under the alternative interpretation where w is a random variable, we can view dropout training as maximizing a lower bound on the Bayesian marginal likelihood among a class of models M  X  indexed by  X   X  R m . Concretely, let  X  i = pw i , then the dropout objective where M  X  = R p ( D| v ) p ( v |  X  ) dv is the Bayesian ev-idence. p ( v i |  X  i ) = N ( v i |  X  i , X  X  2 i ) and p ( y | v dropout training,  X  = w/p and  X  = (1  X  p ) /p . Here the variance of v is tied to its magnitude, so a larger weight is only beneficial when it is robust to noise. While  X  can be determined by the dropout pro-cess, we are also free to choose  X  and we find empir-ically that using a slightly larger  X  than that deter-mined by dropout often performs slightly better. 6.1. Evaluating the assumptions and speed For logistic regression (LR), figure 4 shows that the quality of the gradient approximation using Gaussian samples is comparable to the difference between dif-ferent MC dropout runs with 200 samples. Figure 5 shows that, under identical settings, the Gaussian ap-proximation is much faster than MC dropout, and has a very similar validation error profile. Both Gaussian dropout training and real dropout training reduce val-idation error rate by about 30% over plain LR when trained to convergence, without ever overfitting. 6.2. Experiments on document classification We show the performance of fast dropout LR on sev-eral sentiment and topic document classification tasks, both accuracy and time taken, in the top half of table 1. Sampling from the Gaussian is generally around 10 times faster than MC dropout and performs compara-bly to NBSVM in (Wang &amp; Manning, 2012), which is a method specifically engineered for document classi-fication. Further speedup is achieved by directly opti-mizing the objective in (8) and that is only 30% slower than plain logistic regression. While each iteration of fast dropout is still slower than LR, fast dropout some-times reaches a better validation performance in less time as seen in figure 5. Note that for the MPQA dataset where the average number of non-zero dimen-sions is m  X  4, the Gaussian assumption is unjustifi-able, but the derived method works empirically any-ways. We compare to other papers in the bottom half of the table 1, using either a test/train split or N -fold cross validation, depending on what is the most standard for the dataset. With the right regularization parameters and bigram features, our plain LR baseline is itself quite strong relative to previous work. 6.3. Experiments on MNIST Experimental results on MNIST using 2-hidden-layer neural networks are shown in table 2 and the valida-tion error curves with a slight smaller net are shown in figure 6. Here is a case where the data is fairly re-dundant so that dropping out input features does not make the problem much harder and MC dropout on minibatches converges fairly quickly. We replicate the original experiment using the exact settings described in (Hinton et al., 2012) with a 20% dropout of the inputs, an exponentially decaying learning rate, a mo-mentum schedule, and minibatch stochastic gradient descent. Under the learning schedule in the original experiment, no improvement resulted from doing fast dropout in the minibatch setting. In fact, each mini-batch of fast dropout takes 1.5 times as much time as real dropout with 1 sample. However, the fast dropout objective is suitable for standard optimization technol-ogy, and we were able to train faster using L-BFGS where it converged in less than 100 epochs as opposed to over 500 epochs (see figure 6). 160 errors is the previous best result without pre-training or weight-sharing or enhancement of the training data.
 6.4. The test time utility of fast dropout For the case of real dropout, at test time, Hinton et al. (2012) propose using all features, with weights scaled by p . This weight scaling heuristic does not exactly match the training objective being optimized, but greatly speeds run time performance. If we are not concerned about run time, we can still apply dropout at test time.
 In contrast, the test time procedure for fast dropout is exactly the same as the training time procedure. One shortcoming of fast dropout is that the implementation of training does become more complicated, mainly in the backpropagation stage, while the forward compu-tation of the network function is still straightforward. One compromise here is to use fast dropout at test time, even if we want to train with real dropout for simplicity. Table 3 compares several test time meth-ods on neural networks trained for MNIST and CIFAR using real dropout. Multiple real dropout samples and fast dropout provide a small but noticeable improve-ment over weight scaling. 6.5. Other experiments The normal equations (10) show the contrast be-tween additive and multiplicative L 2 regularization. For linear regression, L 2 regularization outperformed dropout on 10 datasets from UCI that we tried. 1 Re-sults on 5 of them are shown in table 4.
 Classification results using neural networks on small UCI datasets are shown in table 5 where fast dropout does better than plain neural networks in most cases. We presented a way of getting the benefits of dropout training without actually sampling, thereby speeding up the process by an order of magnitude. For high dimensional datasets (over a few hundred), each it-eration of fast dropout is less than 2 times slower than normal training. We provided a deterministic and easy-to-compute objective function approximately equivalent to that of real dropout training. One can optimize this objective using standard optimization Test 129 108 199 118 105 103 Train 4 1 36 1 1 1 Time(s) 10 10 13 110 1.1K 16 Test 53 47 51 45 43 44 Train 42 35 42 33 32 33 Time(s) 17 23 25 230 2.2K 29 Autos 0.25 0.51 0.41 0.57 Cardio 109.24 117.87 140.93 188.91 House 23.57 21.02 65.44 56.26 Liver 9.01 9.94 9.69 9.88 Webgrph 0.17 0.20 0.19 0.21 methods, whereas standard methods are of limited use in real dropout because we only have a noisy measure-ment of the gradient. Furthermore, since fast dropout is not losing any information in individual training cases from sampling, it is capable of doing more work in each iteration, often reaching the same validation set performance in a shorter time and in less iterations. Acknowledgments We thank Andrej Karpathy, Charlie Tang and Percy Liang for helpful discussions. Sida Wang was partially funded by a NSERC PGS-M scholarship and a Stan-ford School of Engineering fellowship.
 Bishop, Christopher M. Training with noise is equiv-alent to Tikhonov regularization. Neural Computa-tion , 7(1):108 X 116, 1995.
 Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron C., and Bengio, Yoshua. Maxout networks. CoRR , abs/1302.4389, 2013.
 Hinton, Geoffrey E., Srivastava, Nitish, Krizhevsky,
Alex, Sutskever, Ilya, and Salakhutdinov, Rus-lan. Improving neural networks by prevent-ing co-adaptation of feature detectors. CoRR , abs/1207.0580, 2012.
 Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geof-frey E. ImageNet classification with deep convo-lutional neural networks. In Proceedings of NIPS , 2012.
 Lehmann, Erich L. Elements of Large-Sample Theory . Springer, 1998. ISBN 03873985956.
 Maas, Andrew L., Daly, Raymond E., Pham, Peter T.,
Huang, Dan, Ng, Andrew Y., and Potts, Christo-pher. Learning word vectors for sentiment analysis. In Proceedings of the ACL , 2011.
 MacKay, David J.C. The evidence framework applied to classification networks. Neural Computation , 5 (4):720 X 736, 1992.
 Matsuoka, Kiyotoshi. Noise injection into inputs in back-propagation learning. IEEE Transactions on Systems, Man, and Cybernetics , 22(3):436 X 440, 1992.
 Nakagawa, Tetsuji, Inui, Kentaro, and Kurohashi,
Sadao. Dependency tree-based sentiment classifica-tion using CRFs with hidden variables. In Proceed-ings of ACL:HLT , 2010.
 Ng, Andrew Y. and Jordan, Michael I. On discrim-inative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Proceedings of NIPS , volume 2, pp. 841 X 848, 2002.
 Ross, Andrew. Computing bounds on the expected maximum of correlated normal variables. Methodol-ogy and Computing in Applied Probability , 12:111 X  138, 2010. ISSN 1387-5841.
 Simard, Patrice, LeCun, Yann, Denker, John, and Vic-torri, Bernard. Transformation invariance in pattern recognition-tangent distance and tangent propaga-tion. In Neural Networks: Tricks of the Trade , pp. 239 X 27, 1996.
 Smith, Andrew, Cohn, Trevor, and Osborne, Miles.
Logarithmic opinion pools for conditional random fields. In Proceedings of the ACL , pp. 18 X 25, 2005. Socher, Richard, Pennington, Jeffrey, Huang, Eric H., Ng, Andrew Y., and Manning, Christopher D. Semi-Supervised Recursive Autoencoders for Predicting
Sentiment Distributions. In Proceedings of EMNLP , 2011.
 Sutton, Charles, Sindelar, Michael, and McCallum,
Andrew. Reducing weight undertraining in struc-tured discriminative learning. In Proceedings of
HLT-NAACL) , 2006. van der Maaten, Laurens, Chen, Minmin, Tyree,
Stephen, and Weinberger, Kilian. Learning with marginalized corrupted features. In Proceedings of ICML , 2013.
 Wang, Sida and Manning, Christopher. Baselines and bigrams: Simple, good sentiment and topic classifi-
