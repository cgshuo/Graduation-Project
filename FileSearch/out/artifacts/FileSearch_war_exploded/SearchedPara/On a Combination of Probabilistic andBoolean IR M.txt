 MASAHARU YOSHIOKA and MAKOTO HARAGUCHI Hokkaido University 1. INTRODUCTION
Large quantities of textual data can now be accessed through the Internet and many search engines have been implemented for commercial use. Most such systems mainly use simple Boolean query operators such as  X  terms that should be included in retrieved documents and  X  should be excluded. However, most users have great difficulty specifying ap-propriate queries in Boolean format [Hearst 1999; Young and Shneiderman 1993]. As a result, according to an analysis of real queries used in the
AltaVista search engine, most users only use implicit AND, and do not use such Boolean features [Spink et al. 2001]. In addition, Eastman and Jansen [2003] analyzed the impact of these query operators for different search en-gines (e.g., Google, AOL, and MSN); they found that most user-defined opera-tors do not improve search engine results, except for some PHRASE operator cases.

Because of the difficulties in constructing effective Boolean queries, these search engines use Boolean queries only as filters that select document sets for ranking and use information other than content information, such as link structures, for ranking to find highly relevant pages as highly scored ones. PageRank [Brin and Page 1998] is a famous algorithm for that purpose pages.

In contrast, there are professional users who would like to retrieve exhaus-tive (high-recall) documents for particular tasks (e.g., compiling surveys). An
IR system that uses a Boolean query as a filter for selecting a document set for ranking may miss many relevant documents when the given Boolean query is not appropriate enough. To utilize a Boolean IR model, it is, therefore, desirable to have a support mechanism for constructing an appropriate Boolean query that represents his/her information need. Several studies have considered sup-port of Boolean query formulation [Anick et al. 1990; Young and Shneiderman 1993; Jones 1998]. In those studies, Boolean query reformulation process was supported by showing interactively how retrieved results would change with a revised query, but they did not focus on how to reformulate a query according to relevant documents.

In the current study, we propose a method for modifying a given Boolean query by using information from a relevant document set. This method is based on the assumption that the (pseudo-) relevant document set should satisfy the newly constructed Boolean query. However, some important key-words may be excluded as a result of this query reformulation process, thereby causing difficulties when searching for relevant documents that con-tain the excluded keywords. To compensate for this difficulty, we also propose a new method that combines the probabilistic IR model and the Boolean IR model.

Based on these two methods, a new IR system,  X  X ppropriate Boolean query reformulation for information retrieval X  (ABRIR) is introduced. This system uses a modified version of the Okapi system as a probabilistic IR engine. It uses both a word index and an index of phrases comprising combinations of two adjacent words.

The rest of this paper is divided into five sections. Section 2 introduces our baseline probabilistic IR model based on the Okapi system and discusses its characteristics. In Section 3, two methods that combine the probabilistic and
Boolean IR models are proposed. In Section 4, our new IR system, ABRIR is described and evaluated by using the NTCIR-4 Web test collection [Eguchi et al. 2004]. Section 5 compares our approach with related work; Section 6 concludes the paper.
 2. AN IR SYSTEM BASED ON THE PROBABILISTIC IR MODEL
Our baseline system is mostly similar to Okapi BM25 [Robertson and Walker 2000] with pseudo-relevance feedback and query expansion and to the IR sys-tem proposed by Toyoda et al. [2002], which was the highest-performing system in the NTCIR-3 workshop. However, we have modified several aspects of these existing systems, so these modifications in our system are briefly explained in this section.

Our system is designed to handle mainly Japanese documents. It uses BM25 [Robertson and Walker 2000] as a basic probabilistic IR model and ChaSen [Matsumoto et al. 2000] as a morphological analyzer to extract index terms.
It uses a word index and a phrase index comprising combinations of adjacent words [Toyoda et al. 2002]. Like Uchiyama and Isahara X  X  system [Uchiyama and Isahara 2001], it employs pseudo-relevance feedback and query expan-sion by using the five top-ranked documents retrieved initially. The generic engine for transposable association (GETA) tool is used gine. Indexes for documents in other languages totally depend on the results of
ChaSen. 2.1 Indexing Each Document Japanese text can have different coding systems such as Shift-JIS, EUC, and UTF-8, so before applying the indexer we converted all texts into EUC code. We also removed HTML tags from documents that contained them.

After these preprocessing steps, we applied the following procedure to extract the word and phrase indexes from the text. 1. Morphological analysis X  X SCII text characters are converted into 2-byte
EUC codes by using KAKASI 2 as a code converter and ChaSen as a morpho-logical analyzer. 2. Extraction of index terms X  X oun words (nouns, unknowns, and symbols) are extracted as index terms. We excluded numbers, prefixes, postfixes, and pronouns from the index terms. We removed  X  X  X  from the end of a term when the length of the term was longer than two katakana characters. All alphabetical letters were then normalized to 1-byte ASCII codes and stored in lower case. 3. Extraction of phrasal terms X  X he aim was to use compound nouns as phrasal terms extracted from pairs of adjacent nouns. In addition, prefixes, postfixes, and numbers were used for extracting phrasal terms. 2.2 Pseudo-Relevance Feedback and Query Expansion
The five top-ranked documents were used for pseudo-relevance feedback. How-ever, when the score was normalized by the number of terms existing in the document, some texts with fewer terms tended to score highly. For example, when the single query term was  X  X OEIC, X  a document containing only the term  X  X OEIC X  scored highly. This may occur, for example, when the title of an HTML page is  X  X OEIC X  and the contents are Macromedia Flash or image objects. Be-cause these documents are not useful for query expansion, we excluded docu-ments containing fewer than four terms from the relevant documents list. Pseudo-relevant documents are also used as a source of query expansion.
The computational cost increases when large numbers of expanded terms are added, so expansion was restricted to 300 terms. If there were more than 300 different terms in a pseudo-relevant document set, the 300 different terms with the highest mutual information content between a relevant document set and a term would be selected [Yoshioka and Haraguchi 2003]. 2.3 Term Weighting The BM25 weighting formula was used to calculate the score for each document:
Here, w (1) is the weight of a (phrasal) term T which is a term or a phrasal term in query Q , and is calculated using Robertson-Sparck Jones weights: where N is the count of all documents in the database, n is the count of all documents containing T , R is the given number of relevant documents, and r is the count of all relevant documents containing T . In addition, tf and qt f are the number of occurrences of T in a document and in a query, respectively, and k , k 3 , and K are control parameters.

The results of term extraction in our system may vary because of the results of the morphological analyzer. The effect of this difference must, therefore, be minimized.

For example, suppose a phrasal term  X  (information science) X  ( X  (information) X  +  X  (science) X ) exists. When  X  (information sci-ence) X  is registered in the dictionary of the morphological analyzer, the term  X  (information science) X  is extracted. When  X  (information) X  and  X  (science) X  are registered separately, but  X  (information science) X  is not registered, the terms  X  (information) X  and  X  (science) X  and a phrasal term  X  (information science) X  are extracted. In the latter case, in ad-dition to  X  (information science), X  the terms  X  (information) X  and  X  (science) X  are also used to calculate the score.

Phrasal terms should, therefore, have lower weights than regular terms. For this purpose, we introduced a parameter c (0  X  c  X  1) for counting the phrasal terms in a query, where qtf is incremented by c , rather than one when a phrasal term is found.
 For the query expansion, Rocchio-type feedback was used [Uchiyama and
Isahara 2001]: where qtf 0 and qtf i are the number of times T appears in the query and in relevant document i , respectively.
 To estimate parameters, we conducted retrieval experiments using the
NTCIR-3 Web test collection, and we set k 1 = 1, K = dl avdl
Here, dl is the length of a document (the number of terms and phrasal terms) and avdl is the average length of all documents. We set k retrieval and k 3 = 7 for final retrieval. 2.4 Retrieval Procedure
The retrieval procedure used in our IR system is as follows. 1. Morphological analysis X  X n identical morphological analysis process was applied to generate an index of each document and to extract terms and phrases for the query. 2. Initial retrieval X  X he query was applied to obtain the top-ranked docu-ments. We set R = r = 0 to calculate the score of each document. 3. Pseudo-relevance feedback and query expansion X  X he five top-ranked doc-uments were selected as the relevant documents. When this set included documents that had fewer than four terms, it removed them from the rele-vant documents list and included the next higher-ranked documents. be too specific for use with pseudo-relevance feedback [Toyoda et al. 2002].
When there were many terms in the relevant documents, the 300 terms that shared the highest mutual information were selected [Yoshioka and
Haraguchi 2003]. 4. Final retrieval X  X he expanded query was applied to obtain the final results. 2.5 Implementation
We implemented the baseline IR system using the generic engine for transpos-able association (GETA) tool.

This system has almost equivalent retrieval performance in terms of mean average precision to the highest-performance IR system in NTCIR-3 [Toyoda et al. 2002], which is based on an Okapi BM25.

Because GETA cannot handle all documents as a single database, the docu-ments were divided into eight subsets. To obtain an equivalent score from all databases N , n , and avdl were shared. A given query was applied to all eight databases and the results were merged. 2.6 Evaluation
We used the NTCIR-4 Web test collection [Eguchi et al. 2004] to evaluate the system. This collection contains 100 gigabytes of document data, 35 queries for survey-type retrievals, and 45 queries for target-type retrievals. Figure 1 shows a sample topic of this test collection. &lt; TITLE &gt; includes 1 X 3 terms with Boolean expressions. Attribute  X  X ASE X  in &lt; TITLE &gt; , &lt; ALT0 &lt; ALT3 &gt; has the following meanings.
 (a) All of the terms have relationships with one another that can be used as an (b) All of the terms have relationships with one another that can be used as an (c) Only two of the terms have a relationship that can be used as an OR operator
For the sample topic described in Figure 1, we can construct the Boolean query ( (offside) and ( (soccer) or (rule))) from TITLE and ( (offside) and (soccer) and (rule)) from ALT3.

In this test collection, assessors judged the  X  X ultigrade relevance X  of doc-uments, i.e., highly relevant (S), fairly relevant (A), partially relevant (B) or irrelevant (C). Rigid relevance levels, where  X  X  X  or  X  X  X  documents were classi-fied as relevant, were used for overall evaluation in this paper.
Table I lists the evaluated results from our IR system. Survey-type exper-iments were conducted with 35 topics selected by the organizers and target types with another 45 topics selected by the organizers. A thousand documents were retrieved for every topic.

In most cases, our system DBLAB-tt-02, DBLAB-ds 02 in [Eguchi et al. 2004] has one of the highest retrieval performances. However, in several cases, it has poorer performance than average.

We assume that the quality of phrasal terms used in a query may affect the retrieval performance. For example, topic 0058 uses the terms  X  (ontol-ogy) X  =  X  (onto-) X  +  X  (-logy) X  in the title.

In contrast, it uses  X  (from the philo-sophical aspect, (find documents that explain)  X  X hat is (onto-, existence) X  that includes  X  (philosophical aspect) X  =  X  (philosoph-) X  (-cal) X  +  X  (aspect) X  in the description.

Because  X  (ontology) X  is a technical term in philosophy and artificial intelligence,  X  (ontology) X  is a more appropriate word than  X  (onto-, existence). X  On the other hand, because  X  (philosophical aspect) X  is more important than  X  (onto-, existence), X  which is a common word, our system tends to neglect  X  (onto-, existence). X 
The difference between these terms causes the quality of the initially re-trieved results to vary, so the final results for retrieving the description are worse than average, but the final results for retrieving the title are better than average.

Another problem arises from pseudo-relevance feedback with irrelevant and similar document sets. In topic 0006, the system retrieves quite similar doc-uments (NW002999258, NW002999245, NW002999257, NW002999256, and
NW002999253) that contain formatted record data. Because these five docu-ments have a similar term list, our query expansion method generates a bad query. To reduce the effect of irrelevant documents, we believe that it is better to check for similarity among the top-ranked documents and to remove similar documents from the query expansion. A further problem arises from our index-ing method. Topic 0034 uses the following three terms  X  (cooking), X   X  (cutting method), X  and  X  (name) X  in the title.

Because we do not use verbs for indexing, we do not identify  X  (cut -[t]ing method) X  as an index term in our system, so the retrieved results for topic 0034 are poor. There are two possibilities for including  X  (cutting method) X  as an index term. The first is to include verbs as index terms; the second is to include phrasal terms made with noun postfixes. Because  X  (method) X  is a noun postfix,  X  (cutting method) X  can be included as the phrasal term  X  (cut) X  +  X  (-[t]ing method) X .
 3. COMBINATION OF A PROBABILISTIC IR MODEL AND A BOOLEAN IR
There are three major IR model types: a probabilistic model such as the one on which our proposed IR system is based, a vector-space model, and a Boolean model [Baeza-Yates and Ribeiro-Neto 1999]. The most distinctive differences between the Boolean model and the other models are the assumptions about the appropriateness of selected IR query terms.

For example, a probabilistic model and a vector-space model may retrieve documents that do not contain the user-specified query terms. In contrast, a
Boolean model assumes that the user will select appropriate terms and it re-trieves only documents that contain the user-specified required query terms, but do not contain the user-specified query terms with a NOT operator.
However, it is not easy to formulate an appropriate Boolean query. For ex-ample, some user-formulated Boolean queries defined in this test collection are not precise enough for retrieving all relevant documents, as we have shown in the retrieval results for the Boolean query (see Section 4 for detail).
In this research, we, therefore, propose a new IR system ABRIR based on the following two new proposed methods.

A method for formulating a Boolean query that includes more relevant doc-uments, by using information about relevant documents.
 A method for combining a probabilistic IR model and a Boolean IR model.
Each approach is discussed in details in the following sections. 3.1 Reformulation of a Boolean Query Based on Relevant Documents
Because we assume that all relevant documents contain words that the user intends to retrieve, words that exist in all relevant documents were selected. To remove common words, only words that exist in the original query were used. The original Boolean query was reformulated by using these words.
The following procedure is used to reformulate a Boolean query. Figure 2 shows an example of this process. 1. Selection of Boolean candidate words. All terms used in the original query 2. Reformulation of the Boolean query based on the initial query. When an
Because the description query does not have an original Boolean query, the first step only is applied to generate a new Boolean query.

We think that the methodology proposed here is applicable not only for find-ing the N top-ranked pseudo-relevant documents, but also for user-selected relevant documents. However, the meaning of this reformulation procedure varies with the nature of the relevant documents.

When we use user-selected relevant documents, the meaning is simple. Be-cause user-selected documents should be included in the retrieved set, it is nec-essary to reformulate a Boolean query so that it can be satisfied, at least, by all the selected documents. However, because this Boolean query is not compared with all relevant documents, it may not be sufficient for all.

In contrast, when the N top-ranked pseudo-relevant documents are used, the meaning is different. In this case, the method deals with the co-occurrence patterns of the given query terms. When term co-occurrences described by an initial Boolean query are frequent, the N top-ranked pseudo-relevant docu-ments may satisfy the Boolean query. However, if such term co-occurrences are rare, we must modify the Boolean query. Because we use BM25 term weighting for initial retrieval, n in Eq. (2) (the count of all documents containing the term  X  T  X ) affects the score of each term. Therefore, when we assume that query terms are independent of each other, this algorithm tends to exclude high-occurrence terms (common terms) from the new Boolean query. 3.2 Modification of the Score Based on the Boolean Query
A query expansion technique based on relevance feedback improves retrieval performance, because the expansion may include terms that can specify a de-sired document domain and/or terms that may help to find similar terms based on co-occurrences. These terms are not necessarily included in relevant docu-ments but are useful for finding new relevant documents that are difficult to find with the original query.

However, when a query X  X  terms are expanded by using relevant documents in the probabilistic IR model, there is a chance that documents without all the re-quired query terms will receive a higher score than documents with these terms, although the original query includes terms that are necessarily included in relevant documents. In such cases, conventional query expansion may degrade retrieval performance.

Because we assume that documents that do not satisfy the Boolean query may be less appropriate than documents that do satisfy it, a penalty score is subtracted from documents that do not satisfy the Boolean query.

The penalty is applied according to the importance of the word. For a proba-bilistic IR model, the BM25 weighting formula was used to calculate the score of each document (Eq. 1). In this equation, w (1) ( k 3 + the word in the query. A control parameter  X  is used to calculate the penalty score.

For the OR operator, we use the highest penalty from all the OR terms as the overall penalty.

We describe how to calculate the penalty, using, as an example, the Boolean query ( X  X  X  and ( X  X  X  or  X  X  X )) given in Figure 2. First, we calculate the penalty score for all words ( X  X , X   X  X , X  and  X  X  X ). In this case, we assume Penalty ( C ) Penalty ( D ). Documents not possessing terms  X  X , X   X  X , X  or  X  X  X  receive the penalty Penalty ( A ) + Penalty ( C ). Documents possessing only the  X  X  X  term receive
Penalty ( A ). 4. ABRIR (APPROPRIATE BOOLEAN QUERY REFORMULATION FOR 4.1 Implementation We implement ABRIR based on our baseline IR system discussed in
Section 2 (BM25 + pseudo-relevant feedback + query expansion by using terms in pseudo-relevant documents (max 300).

The GETA tool has a mechanism for applying the Boolean AND operator, but not for applying the Boolean OR operator by itself. In previous experi-ments, when the system retrieved the number of top-ranked documents for each database, we could find the desired number of top-ranked documents for the total database. However, if we apply the Boolean OR operator to the retrieved results and reject documents from them, the resulting number of top-ranked documents for each database may not be large enough to retrieve the desired number of documents from the entire database. To reduce the effect of this problem, we, therefore, add a margin for retrieved document numbers. 4.2 Evaluation
We also applied ABRIR to the NTCIR-4 Web test collection. That is, we con-structed initial Boolean queries from the topic descriptions for the title-retrieval task. When given terms were split into two or more index words by ChaSen, the last phrase was used for an initial Boolean query in order to avoid constructing complicated Boolean queries. For example ( X  (use)-(-er)) X  or initial Boolean query is ( X  (user) X  or  X  (researcher) X ). 3
Tables II and III list the results of this experiment. Eighteen Boolean queries from 35 survey-type topics and 19 Boolean queries from 45 target-type topics were modified. We used the following three types of reformulation: Removal of terms . Remove term(s) (13 from survey and 13 from target);
Break phrase into word . Remove phrase(s) and add terms that are parts of the phrase(s) (four from survey and six from target) Add terms . Add new phrases and/or terms that are excluded from the initial Boolean construction process (five from survey and five from target)
The first two reformulations relax the initial Boolean query, the last one strengthens the initial Boolean query.

Since the Boolean query obtained by the proposed method retrieves more doc-uments that the original user-constructed Boolean query does, it is confirmed that the original Boolean query is stricter than the constructed query. There were 158 more  X  X etrieved X  documents from the survey task (1843 35 topics: 3893 relevant documents in all), and 61 more from the target task (1451  X  1390 from 45 topics: 2891 relevant documents in all).

In the baseline system, 1000 documents are retrieved for each topic. How-ever, because we restrict the retrieved results by using a Boolean query, there is a chance that the system cannot retrieve 1000 documents. The  X  X otal X  column in Tables II and III shows the number of retrieved documents for all topics.
Comparing the increased number of total retrieved documents and that of re-trieved relevant documents, shows that this Boolean reformulation works well
We think that this result comes from the method for constructing relevant document data. Relevant document data of the target-type retrieval task are generated by using pools that are constructed by the top 20-ranked documents from each run results and that of survey-type one uses top 100-ranked docu-ments [Eguchi et al. 2004]. There is, therefore, less chance of including relevant documents with complementary terms (e.g., synonyms) in target-type than in survey-type retrieval.

In addition, the precision of higher-ranked documents (Prec@5 and Prec@10) for target-type retrieval is no better than that of the original query. In target-type retrieval, the task is to find a limited number of appropriate pages from the query, and the precision of higher-ranked documents is important. Because our
Boolean query reformulation method mostly relaxes the original Boolean query to achieve higher recall, it may reduce the precision of the original Boolean query, thus reducing the precision of higher-ranked documents. We, therefore, think that this strategy is useful mainly for survey-type retrievals and not for target-type ones.

When we compare the above results with those from the probabilistic IR model only (Table I), it becomes clear the developed system performs worse for  X  X verage Precision X  and  X  X Prec X  values. This problem arises because of the difference in the number of relevant retrieved documents [for our baseline system: tt (s) 2166, ds (s) 2177, tt (t) 1843, and ds (t) 1616], and implies that the given Boolean query is not precise enough to represent the user X  X  information need.

Figures 3 and 4 show the recall-precision graph of the retrieved results when using different Boolean queries for survey-type retrieval. This Boolean query re-formulation method improves the performance precision, especially for smaller recall values.

We assume that this improvement is a result of removing documents that have expanded query terms with higher Robertson-Sparck Jones weights and do not have initial query terms with smaller Robertson-Sparck Jones weights.
However, in collecting all relevant documents, this restriction is too strict and this is one reason why we have poor performance for higher recall values.
We also conducted retrieval experiments using the score-modification method. Because the constructed Boolean queries perform better than the orig-inal queries, we use them for calculating the penalties. Tables IV and V show the results for this method with different  X  values.

These results confirm that the penalty calculation improves the retrieval results.

Because we aim to make a recall-oriented system, we mainly use  X  X vePrec X  and  X  X Prec X  for the survey-type task. For the target-type task, we mainly use  X  X rec@5 X  and  X  X rec@10, X  because this task mainly focuses on the precision of the lower recall value.

In the title-only experiment, the best performance was obtained when 2 . 0. In contrast, the results for  X  = 1 . 0 in the description-only experiment show better performance, in most cases, than that when  X  = 2 . this difference comes from the quality of the given Boolean query, because our constructed Boolean query used for the description gives worse performance than the titles query in terms of the relevant retrieved document sizes. We, therefore, think that the estimation of an appropriate value for based on a user model that has information on how a user may describe a Boolean query correctly.

Figures 5 and 6 show recall-precision graphs of the retrieved results for differ-ent values of  X  for survey tasks. This method improves performance, especially for recall values of 0.1 to 0.6.

Unlike the strict Boolean model, because this model does not remove docu-ments that do not have the required initial terms, there is a chance that the system will find relevant documents that have complementary terms (e.g., syn-onyms) of the initial required query terms that do not exist in the document.
On the other hand, documents that do not satisfy a Boolean query have less chance of being considered as relevant documents, so the penalty works well in reducing the importance of such documents. 5. RELATED WORKS
There have been several studies on Boolean query formulation [Anick et al. 1990; Young and Shneiderman 1993; Jones 1998] with graphical user inter-faces. Their main support methods for Boolean query formulation consisted of showing how retrieved results changed as the user interactively modified the Boolean query. They did not focus on how to reformulate a Boolean query according to relevant documents.

The extended Boolean information retrieval model [Salton et al. 1983] is another approach to finding documents that do not satisfy a given Boolean query with a ranking algorithm. This method uses a metric space defined by the given query terms. However, this method itself did not distinguish query terms that are useful for Boolean-type operations from those that are useful for finding related documents. Shaw and Fox [1994] proposed a method that combines this extended Boolean IR model and a vector-space IR model. This approach is similar to our approach. However, they did not discuss how to reformulate a Boolean query into a more appropriate one.

Kekalainen and Jarvelin [1998] analyzed how query structure, which means the use of operators to express the relations between search keys and query expansion, affect retrieval performance. They found that query expansion was not very effective for a Boolean-structured query, while strong structures (syn-onym operator: SYN of INQUERY) with many query expansions, based on a thesaurus, achieved the highest performance. This research is similar to ours in that it combines Boolean operators with a probabilistic IR model. Their find-ings were similar to ours. For example, both studies confirmed that the Boolean
IR model itself cannot achieve higher performance and query expansion also improves retrieval performance as a combination of a Boolean IR and a prob-abilistic IR model. However, there are two points of difference between this research and ours. One is that our system divides query terms into two groups: terms that are used to formulate a Boolean query and ones that are used only in the probabilistic IR model. The other point is that our methodology includes a
Boolean query-reformulation process that is based on relevant documents and is different from thesaurus expansion.

Several studies have tried to find useful terms from retrieved results and/or information from relevant documents. Xu and Croft [1996] proposed an automatic query expansion method that used relevant document informa-tion. RU-INQUERY [Koenemann and Belkin 1996] and DualNavi [Takano et al. 2001] are interfaces for selection of keywords that are useful for characterizing relevant documents. Scatter/Gather X  X  approach [Cutting et al. 1992] is to select useful terms by using text clustering information.

These systems are good for adding new query terms that are useful in finding new relevant documents. However, because they do not focus on Boolean query reformulation, they do not consider whether those terms should be strictly contained in relevant documents or not. As a result, their query expansion corresponds to the simple query expansion method used in our baseline system.
This means that we could employ this method to improve the quality of query expansion in our system. 6. SUMMARY
We developed an information retrieval (IR) System X  X alled appropriate Boolean query reformulation for information retrieval (ABRIR) X  X hat modifies a given Boolean query by using relevant documents and combines the probabilistic
IR model and the Boolean IR model. We confirmed that ABRIR improves the retrieval performance of our baseline system, which was one of the highest-performing retrieval systems among the NTCIR-4 Web task participants. We also confirmed that a user-constructed Boolean query is not precise enough to represent the information need and we proposed a method for Boolean query reformulation by using relevant documents to improve the retrieval perfor-mance. We confirmed that calculating a penalty based on the Boolean query improves the retrieval performance. In future work, we plan to use a thesaurus for constructing more expressive Boolean queries.

We thank the organizers of the NTCIR Web Task for their efforts in constructing this test data. This research was partially supported by a Grant-in-Aid for Scientific Research on Priority (2), 15017202 from the Ministry of Education, Culture, Sports, Science, and Technology, Japan.

