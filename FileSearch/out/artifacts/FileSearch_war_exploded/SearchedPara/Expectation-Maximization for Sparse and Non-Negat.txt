 Christian D. Sigg chrsigg@inf.ethz.ch Joachim M. Buhmann jbuhmann@inf.ethz.ch Principal component analysis (PCA) provides a lower dimensional approximation of high dimensional data, where the reconstruction error (measured by Eu-clidean distance) is minimal. The first principal com-ponent (PC) is the solution to where C  X  R D  X  D is the positive semi-definite covari-ance matrix of the data. It is straightforward to show that the first PC is the dominant eigenvector of C , i.e. the eigenvector corresponding to the largest eigen-value. The first PC maximizes the variance of the projected data, while the second PC again maximizes the variance, under the constraint that it is orthogonal to the first, and so on.
 Constrained PCA and its Applications. We con-sider problem (1) under two additional constraints on w : Sparsity k w k 0  X  K 1 and non-negativity w 0 . Constraining PCA permits a trade-off between maxi-mizing statistical fidelity on the one hand, and facil-itating interpretability and applicability on the other (d X  X spremont et al., 2007). Although it is often the case that PCA provides a good approximation with few PCs, each component is usually a linear com-bination of all original features. Enforcing sparsity facilitates identification of the relevant influence fac-tors and is therefore an unsupervised feature selec-tion method. In applications where a fixed penalty is associated with each included dimension (e.g. trans-action costs in finance), a small loss in variance for a large reduction in cardinality can lead to an over-all better solution. Enforcing non-negativity renders PCA applicable to domains where only positive in-fluence of features is deemed appropriate (e.g. due to the underlying physical process). Moreover, the to-tal variance is explained additively by each compo-nent, instead of the mixed sign structure of uncon-strained PCA. Often non-negative solutions already show some degree of sparsity, but a combination of both constraints enables precise control of the car-dinality. Sparse PCA has been successfully applied to gene ranking (d X  X spremont et al., 2007), and non-negative sparse PCA has been compared favorably to non-negative matrix factorization for image parts ex-traction (Zass &amp; Shashua, 2006).
 Related Work. Problem (1) is a concave program-ming problem, and is NP-hard if either sparsity or non-negativity is enforced (Horst et al., 2000). Although an efficient global optimizer is therefore unlikely, lo-cal optimizers often find good or even optimal solu-tions in practice, and global optimality can be tested in O ( D 3 ) (d X  X spremont et al., 2007), where D is the dimensionality of the data. As is evident from writing the objective function of (1) as setting w k to zero excludes the k -th column and row of C from the summation. For a given sparsity pattern S = { i | w i 6 = 0 } , the optimal solution is the dominant eigenvector of the corresponding submatrix of C . For sparse PCA, the computationally hard part is there-fore to identify the optimal sparsity pattern, and any solution can potentially be improved by keeping S only and recomputing the weights, a process called varia-tional renormalization by Moghaddam et al. (2006). Sparse PCA methods can be characterized by the fol-lowing two paradigms: 1. Relaxation of the hard cardinality constraint 2. Direct combinatorial optimization of S . Due Cadima and Jolliffe (1995) proposed thresholding the ( D  X  K ) smallest elements of the dominant eigenvec-tor to zero, which has complexity O ( D 2 ). Better re-sults have been achieved by the SPCA algorithm of Zou et al. (2004), which is based on iterative elastic net regression. Combinatorial optimization was intro-duced by Moghaddam et al. (2006), who derived an exact branch-and-bound method and a greedy algo-rithm, that computes the full sparsity path 1  X  K  X  D in O ( D 4 ). Based on a semi-definite relaxation of the sparse PCA problem, d X  X spremont et al. (2007) pro-posed PathSPCA, which reduces the complexity of each greedy step to O ( D 2 ), and renders computation of the full regularization path possible in O ( D 3 ). Fi-nally, Sriperumbudur et al. (2007) formulate sparse PCA as a d.c. program (Horst et al., 2000) and provide an iterative algorithm called DC-PCA, where each it-eration consists of solving a quadratically constrained QP with complexity O ( D 3 ).
 Non-negative (sparse) PCA was proposed by Zass and Shashua (2006). In contrast to the methods dis-cussed so far, their algorithm (called NSPCA) opti-mizes the cumulative variance of L components jointly, versus a sequential approach that computes one com-ponent after another. Orthonormality of the compo-nents is enforced by a penalty in the objective function (see section 4 for a discussion about orthogonality for non-negative components), and the desired sparsity is again expressed in terms of the whole set of L compo-nents.
 Our Contribution. To our knowledge, there is no al-gorithm either for sparse or non-negative sparse PCA that achieves competitive results in less than O ( D 3 ). In this paper, we propose an O ( D 2 ) algorithm that enforces sparsity, or non-negativity or both constraints simultaneously in the same framework, which is rooted in expectation-maximization for a probabilistic gener-ative model of PCA (see next section). As for the combinatorial algorithms, the desired cardinality can be expressed directly as K = |S| , instead of a bound B on the l 1 norm of w (which requires searching for the appropriate value). Although computing the full regularization path is also of order O ( D 3 ), our method directly computes a solution for any K in O ( D 2 ), in contrast to forward greedy search which needs to build up a solution incrementally. As is the case with SPCA, our method works on the data matrix X  X  R N  X  D ( N is the number of samples), instead of the covariance matrix C . To summarize, the low complexity com-bined with an efficient treatment of the D N case enables an application of our method to large data sets of high dimensionality.
 Notation. Vectors are indexed as w ( t ) , and elements of vectors as w i . k w k 1 = P i | w i | and k w k 0 = |S| , where S = { i | w i 6 = 0 } . k w k 0 is also called the car-dinality of w . I is the identity matrix, 0 a vector of zero elements, and w 0  X   X  i : w i  X  0. x  X  y denotes element-wise multiplication of x and y , and tr( X ) = P i X ii is the trace of matrix X . E [ . ] is the expectation operator, and N denotes a Gaussian dis-tribution. Tipping and Bishop (1999) and independently Roweis (1998) proposed a generative model for PCA, where the full covariance matrix  X   X  R D  X  D of the Gaussian distribution is approximated by its first L eigenvectors (in terms of magnitude of the respective eigenvalues). The latent variable y  X  R L (in the principal compo-nent subspace) is distributed according to a zero mean, unit covariance Gaussian The observation x  X  R D , conditioned on the value of the latent variable y , is linear-Gaussian distributed according to where the matrix W  X  R D  X  L spans the principal sub-space, and  X   X  R D is the mean of the data. To sim-plify the presentation, we will assume centered data from now on.
 The EM equations for probabilistic PCA have the fol-lowing form. The E-step keeps track of where M  X  R L  X  L is defined as The M-step equations are W In order to efficiently incorporate constraints into the EM algorithm (see next section), we make three sim-plifications: take the limit  X  2  X  0, consider a one-dimensional subspace and normalize k w ( t ) k 2 to unity. The first simplification reduces probabilistic PCA to standard PCA. Computing several components will be treated in section 4, and the unity constraint on k w ( t ) k 2 is easily restored after each EM iteration. The E-step now amounts to and the M-step is These two equations have the following interpretation (Roweis, 1998): The E-step orthogonally projects the data onto the current estimate of the subspace, while the M-step re-estimates the projection to minimize squared reconstruction error for fixed subspace coordi-nates. We summarize this result in algorithm 1, which iteratively computes the solution to eq. (1). Due to the fact that so far only k w k 2 = 1 is enforced, con-vergence to the global optimum doesn X  X  depend on the initial estimate w (1) . This will no longer be the case for additional constraints.
 Algorithm 1 Iterative Computation of First PC Input: Data X  X  R N  X  D , initial estimate w (1) ,  X 
Algorithm: t  X  1 repeat
Output: w Consider the minimization step in algorithm 1, which can be written as a quadratic program (QP), and is convex due to the non-negativity of h . Furthermore, because the Hes-sian is a scaled identity matrix, the problem is also isotropic. The unique global optimum is found by an-alytical differentiation of the objective function which of course is identical to eq. (11). 3.1. Sparsity It is well known (Tibshirani, 1996) that solving a QP under an additional constraint on k w k 1 favors a sparse solution. This constraint corresponds to restricting the feasible region to an l 1 diamond: where the upper bound B is chosen such that w  X  has the desired cardinality. The l 1 constrained QP is again convex, and because the objective function is isotropic, it implies that w  X  is the feasible point minimizing l 2 distance to the unconstrained optimum w  X  .
 We derive an efficient and optimal algorithm for eq. (14), where the desired cardinality can be specified di-rectly by the number K of non-zero dimensions. Ob-serve that w  X  must have the same sign structure as f , therefore we can transform the problem such that both w  X  and w  X  come to lie in the non-negative orthant. The algorithm (illustrated in fig. 1) approaches w  X  with axis-aligned steps in the direction of the largest element of the negative gradient until the boundary of the feasible region is hit or the gradient vanishes. Because the elements of w become positive one after another, and their magnitude in-creases monotonically, B is set implicitly by terminat-ing the gradient descent once the cardinality of the so-lution vector is K . Finally, the solution is transformed back into the original orthant of w  X  .
 Proposition 3.1 Axis-aligned gradient descent with infinitesimal stepsize terminates at the optimal feasible point w  X  .
 Proof. Optimality is trivial if w  X  lies within the feasi-ble region, so we consider the case where the l 1 con-straint is active. The objective function in eq. (14) is equivalent to The gradient descent procedure invests all available co-efficient weight B into decreasing the largest term(s) of this sum, which follows from eq. (15). We show equivalence of w  X  to the gradient descent solution v by contradiction. Suppose the computation of w  X  fol-lows a different strategy, so at least one summation term ( w  X  l  X  w  X  l ) 2 is larger than max d ( w  X  d  X  v ever, subtracting a small amount from w  X  s ( s 6 = l ) and adding it to w  X  l doesn X  X  change k w  X  k 1 but decreases the objective, which is a contradiction.
 Implementation of axis-aligned gradient descent amounts to sorting the elements of  X  X  X  J ( w ) in de-scending order (an O ( D log D ) operation), and iter-ating over its first K elements. At each iteration k  X  { 1 ,...,K } , the first k elements of w are manip-ulated, resulting in complexity O ( K 2 ) for the whole loop. Algorithm 2 provides a full specification of the method. Because EM is a local optimizer, the ini-tial direction w (1) must be chosen carefully to achieve good results. For sparse PCA, initialization with the unconstrained first principal component gave best re-sults (see section 5). Initialization is therefore the most expensive operation of the algorithm with its O ( D 2 ) complexity. For the D N case, it can be reduced to O ( N 2 ) by working with XX &gt; instead of X &gt; X . As initialization is independent of K , w (1) can be cached and re-used when varying the sparsity parameter. The number of EM iterations t until convergence also de-pends on D and K , but our experiments (see section 5) suggest that dependence is weak and sub-linear. On average, t &lt; 10 iterations were sufficient to achieve convergence. 3.2. Non-Negativity Enforcing non-negativity is achieved in the same way as sparsity. Here, the the feasible region is constrained to the non-negative orthant, which is again a convex domain: Eq. (17) implies that choosing w i = 0 for f i &lt; 0 is optimal. The non-negativity constraint can then be dropped, and optimization for the other elements of w proceeds as before.
 The first PC is invariant to a change of sign. How-ever, this symmetry is broken if the non-negativity constraint is enforced. As an extreme example, non-negative EM fails if the initial projection w (1) is a dom-inant eigenvector that only consists of non-positive el-ements -the minimum of eq. (17) is the zero vector. But changing the sign of w (1) implies that the non-negativity constraint becomes inactive, and the algo-rithm terminates immediately with the optimal solu-tion. We choose to initialize EM for non-negative PCA with a random unit vector in the non-negative orthant, which exploits the benefit of random restarts. For non-negative sparse PCA, the feasible region is defined as the intersection of the non-negative orthant Algorithm 2 EM for Sparse PCA Input: X  X  R N  X  D , K  X  X  1 ,...,D } ,  X 
Algorithm: t  X  1 w ( t )  X  first principal component of X repeat
Output: w and the l 1 diamond. As the intersection of two convex sets is again convex, the combined constraints can be treated in the same framework. We establish conver-gence of our method in the following proposition: Proposition 3.2 EM for sparse and non-negative PCA converges to a local minimum of the l 2 recon-struction error.
 Proof. Given a feasible w ( t ) (either by proper initial-ization or after one EM iteration), both the E-step and the M-step never increase l 2 reconstruction error. Orthogonal projection y = Xw in the E-step is the l optimal choice of subspace coordinates for given w . Error minimization w.r.t. w in the M-step either re-covers w ( t ) as it is feasible, or provides an improved w A full eigen decomposition of the covariance matrix C provides all r PCs, where r is the rank of C . Sorted in descending order of eigenvalue magnitude, each eigen-vector maximizes the variance of the projected data, under the constraint that it is orthogonal to all other components considered so far. For sparse PCA, we compute more than one component by means of iter-ative deflation: having identified the first component w (1) , project the data to its orthogonal subspace using re-run EM to identify w (2) , and so on. Although defla-tion suffers from numerical errors that accumulate over each iteration, this inaccuracy is not a serious problem as long as the desired number of components L is small compared to r (which is true in many applications of PCA).
 Desiring non-negativity and orthogonality implies that each feature can be part of at most one component: for m 6 = l , i.e. the sparsity patterns have to be dis-joint: S l T S m =  X  , for l 6 = m and S l = { i | w ( l ) This constraint might be too strong for some applica-tions, where it can be relaxed to require a minimum angle between components. This quasi -orthogonality is enforced by adding a quadratic penalty term to eq. (17), where V = w (1) w (2)  X  X  X  w ( l  X  1) contains previously identified components as columns, and  X  is a tuning parameter. Because VV &gt; is also positive semi-definite, the QP remains convex, but the Hes-sian is no longer isotropic. We have used the standard Matlab QP solver, but there exist special algorithms for this case in the literature (Sha et al., 2007). We report performance and efficiency of our method in comparison to three algorithms: SPCA 2 and Path-SPCA 3 for cardinality constrained PCA, and NSPCA 4 for non-negative sparse PCA. SPCA was chosen be-cause it has conceptual similarities to our algorithm: both are iterative methods that solve an l 1 constrained convex program, and both use the data matrix instead of the covariance matrix. PathSPCA was chosen be-cause it is (to our knowledge) the most efficient com-binatorial algorithm. We are not aware of any other non-negative PCA algorithm besides NSPCA.
 The data sets considered in the evaluation are the fol-lowing: 1. CBCL face images (Sung, 1996): 2429 gray scale 2. Leukemia data (Armstrong et al., 2002): Expres-The two data sets cover the N &gt; D and D N case and are large enough such that differences in computa-tional complexity can be established with confidence. Both were standardized such that each dimension has zero mean and unit variance. 5.1. Sparse PCA Figure 2 (left) plots explained variance versus cardinal-ity for SPCA, PathSPCA and our algorithm (called emPCA) on the face image data set. Variational renormalization is necessary for SPCA and emPCA to close the performance gap to PathSPCA, which com-putes optimal weights for a specific sparsity pattern by construction. Figure 2 (middle) shows analogous re-sults for the gene expression data. As a reference, we have also plotted results for simple thresholding (after renormalization).
 To complement theoretical analysis of computational complexity, we have also measured running times of reference Matlab implementations, provided by their respective authors (SPCA is a re-implementation of the author X  X  R code in Matlab). CPU time was mea-sured using Matlab X  X  tic and toc timer constructs, running on an Intel Core 2 Duo processor at 2.2GHz with 3GB of RAM. Our focus is not to report abso-lute numbers, but rather demonstrate the dependency on the choice of K . Figure 2 (right) plots the run-ning times versus cardinality on the gene expression data. The PathSPCA curve is well explained by the incremental forward greedy search. SPCA is harder to analyze, due to its active set optimization scheme: at each iteration of the algorithm, active features are re-examined and possibly excluded, but might be added again later on. emPCA is only marginally affected by the choice of K , but shows an increased number of EM iterations for 10  X  K  X  25, which was observed on other data sets as well. 5.2. Non-Negative PCA The impact of the non-negativity constraint on the explained variance depends on the sign structure of w  X  . Because the first principal component for the face image data happens to lie in the non-negative orthant, we projected the data onto its orthogonal subspace such that the constraint becomes active. Figure 3 (left) shows the variance versus cardinality trade-off curves for non-negative sparse PCA. For NSPCA, the sparsity penalty  X  was determined for each K using bisection search, which was aborted when the relative length of the parameter search interval was below a threshold of 10  X  5 . Both the variance achieved and the number of cardinalities for which a solution was found strongly depend on the value of  X  , which corresponds to a unit norm penalty (for the case of a single component). For smaller values of  X  the performance of NSPCA is comparable to emPCA, but only solutions close to the full cardinality are found. Increasing the magnitude of  X  makes it possible to sweep the whole cardinality path, but the performance degrades.
 Because both algorithms are initialized randomly, we chose the best result after ten restarts. Running times for both methods showed no strong dependency on K . Average times for K  X  { 1 ,..., 100 } were 0 . 4 s for em-PCA (0 . 15 s standard deviation) and 24 s for NSPCA (14 . 7 s standard deviation).
 We already motivated in section 4 that requiring or-thogonality between several non-negative components can be restrictive. If the first PC happens to lie in the non-negative orthant, the constraints have to be mod-ified such that more than one component can satisfy them. We have explored the following two strategies: 1. Enforcing orthogonality, but constraining the car-2. Relaxing the orthogonality constraint, by enforc-There is a methodological difficulty in comparing the performance of NSPCA and emPCA. The former max-imizes cumulative variance of all components jointly, while our algorithm computes them sequentially, max-imizing the variance under the constraint that subse-quent components are orthogonal to previous ones (see section 4). We therefore expect emPCA to capture more variance in the first components, while NSPCA is expected to capture larger cumulative variance. Fig-ure 3 (middle) shows the results of applying the first strategy to the face image data. The NSPCA sparsity penalty  X  was tuned to achieve a joint cardinality of 200 for all components. For emPCA we distributed the active features evenly among components by set-ting K = 20 for all of them. As in figure 3 (left), emPCA captures significantly more of the variance, suggesting that the way NSPCA incorporates sparsity seriously degrades performance. This observation was confirmed for various values of K and L .
 Finally, figure 3 (right) reports results for the second strategy, where a minimum angle of 85 degrees was enforced between components. Here, the complemen-tary objectives of NSPCA and emPCA match with our prior expectations. Again, various values for L and minimum angle lead to essentially the same behavior. 5.3. Unsupervised Gene Selection We apply emPCA to select a subset of genes of the leukemia data, and measure subset relevance by fol-lowing the evaluation methodology of Varshavsky et al. (2006). For each gene subset, we cluster the data us-ing k -means ( k = 3), and compare the cluster assign-ments to the true labeling of the data, which differenti-ates between three types of leukemia (ALL, AML and MLL). Agreement is measured using Jaccard scores (Varshavsky et al., 2006), where a value of one signi-fies perfect correspondence between cluster assignment and label. We compare emPCA to simple ranking of the CE criterion as proposed by the authors, which has shown competitive performance to other popular gene selection methods. Figure 4 shows that selecting 70 genes according to the first non-negative sparse PC results in a significantly better Jaccard score than a clustering of the full data set. We have presented a novel algorithm for constrained principal component analysis, based on expectation-maximization for probabilistic PCA. Our method is applicable to a broad range of problems: it includes sparsity, non-negativity or both kinds of constraints, it has an efficient formulation for N &gt; D and D N type of data, and it enforces either strict or quasi-orthogonality between successive components. Desired sparsity is directly specified in the number of non-zero elements, instead of a bound on the l 1 norm of the vector. We have demonstrated on popular data sets from biology and computer vision that our method achieves competitive results for sparse prob-lems, and that it shows significant improvements for non-negative sparse problems. Its unmatched compu-tational efficiency enables a constrained principal com-ponent analysis of substantially larger data sets and lower requirements on available computation time. Although our algorithm is rooted in expectation-maximization for a generative model of PCA, con-straints are added at the optimization stage. In the fu-ture, we will study how to include them in the model it-self, which would enable a Bayesian analysis and data-driven determination of the proper sparsity and num-ber of components. Secondly, we intend to examine whether our algorithm can be extended to the related problem of constrained linear discriminant analysis. A Matlab implementation of emPCA is available at http://www.inf.ethz.ch/personal/chrsigg/ icml2008 .
 We thank Wolfgang Einh  X auser-Treyer, Peter Orbanz and the anonymous reviewers for their valuable com-ments on the manuscript. This work was in part funded by CTI grant 8539.2;2 ESPP-ES.
 Armstrong, S., Staunton, J., Silverman, L., Pieters,
R., den Boer, M., Minden, M., Sallan, S., Lan-der, E., Golub, T., &amp; Korsmeyer, S. (2002). MLL translocations specify a distinct gene expression pro-file that distinguishes a unique leukemia. Nature Genetics , 30 , 41 X 47.
 Cadima, J., &amp; Jolliffe, I. (1995). Loadings and correla-tions in the interpretation of principal components.
Applied Statistics , 203 X 214. d X  X spremont, A., Bach, F., &amp; El Ghaoui, L. (2007).
Full regularization path for sparse principal compo-nent analysis. Proceedings of the International Con-ference on Machine Learning .
 Horst, R., Pardalos, P., &amp; Thoai, N. (2000). Introduc-tion to global optimization . Kluwer Acad. Publ. Moghaddam, B., Weiss, Y., &amp; Avidan, S. (2006). Spec-tral bounds for sparse PCA: Exact and greedy algo-rithms. Advances in Neural Information Processing Systems .
 Roweis, S. (1998). EM algorithms for PCA and sensi-ble PCA. Advances in Neural Information Process-ing Systems .
 Sha, F., Lin, Y., Saul, L., &amp; Lee, D. (2007). Multi-plicative Updates for Nonnegative Quadratic Pro-gramming. Neural Computation , 19 , 2004 X 2031. Sriperumbudur, B., Torres, D., &amp; Lanckriet, G. (2007).
Sparse eigen methods by d.c. programming. Pro-ceedings of the International Conference on Machine Learning .
 Sung, K.-K. (1996). Learning and example selection for object and pattern recognition . Doctoral disser-tation, MIT, Artificial Intelligence Laboratory and Center for Biological and Computational Learning, Cambridge, MA.
 Tibshirani, R. (1996). Regression shrinkage and selec-tion via the LASSO. Journal of the Royal statistical society, series B , 58 , 267 X 288.
 Tipping, M., &amp; Bishop, C. (1999). Probabilistic prin-cipal component analysis. Journal of the Royal Sta-tistical Society, Series B , 21 , 611 X 622.
 Varshavsky, R., Gottlieb, A., Linial, M., &amp; Horn, D. (2006). Novel Unsupervised Feature Filtering of Bi-ological Data. Bioinformatics , 22 .
 Zass, R., &amp; Shashua, A. (2006). Nonnegative sparse PCA. Advances in Neural Information Processing Systems .
 Zou, H., Hastie, T., &amp; Tibshirani, R. (2004). Sparse principal component analysis. Journal of Computa-
