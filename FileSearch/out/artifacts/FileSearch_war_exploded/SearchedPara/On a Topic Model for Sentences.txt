 Probabilistic topic models are generative models that de-scribe the content of documents by discovering the latent topics underlying them. However, the structure of the tex-tual input, and for instance the grouping of words in coher-ent text spans such as sentences, contains much information which is generally lost with these models. In this paper, we propose sentenceLDA , an extension of LDA whose goal is to overcome this limitation by incorporating the structure of the text in the generative and inference processes. We illus-trate the advantages of sentenceLDA by comparing it with LDA using both intrinsic (perplexity) and extrinsic (text classification) evaluation tasks on different text collections. Text Mining; Topic Modeling; Unsupervised Learning
Statistical topic models are generative unsupervised mod-els that describe the content of documents in large textual collections. Prior research has investigated the application of topic models such as Latent Dirichlet Allocation (LDA) [2] in a variety of domains ranging from image analysis to political science. Most of the work on topic models assumes exchangeability between words and treats documents in a bag-of-words fashion. As a result, the words X  grouping in coherent text segments, such as sentences or phrases, is lost.
However, the inner structure of documents is generally useful, when identifying topics. For instance, one would expect that in each sentence, after standard pre-processing steps such as stop-word removal, only a very limited number of latent topics would appear. Thus, we argue that coherent text segments should pose  X  X onstraints X  on the amount of topics that appear inside those segments.

In this paper, we propose sentenceLDA ( senLDA ), whose purpose is to incorporate part of the text structure in the  X  Also affiliated with: Coffreo, Clermont Ferrand, France Figure 1: The senLDA model. The words w of a sentence share the same topic z . topic model. Motivated by the argument that coherent text spans should be produced by only a handful of topics, we propose to modify the generative process of LDA. Hence, we argue that the latent topics of short text spans should be consistent across the units of those spans. In our approach, such text spans can vary from paragraphs to sentences and phrases depending on the task X  X  purpose. Also, note that in the extreme case where words are the coherent text seg-ments, the standard LDA model becomes a special case of senLDA .

In the remainder of the paper we present the senLDA and we derive its collapsed Gibbs sampler in Section 2, we illus-trate its advantages by comparing it with LDA on intrinsic ( in vitro ) and extrinsic ( ex vivo ) evaluation experiments us-ing collections of Wikipedia and PubMed articles in Section 3, and we conclude in Section 4.
A statistical topic model represents the words in a col-lection of D documents as mixtures of K  X  X opics X , which are multinomials over a vocabulary of size V . In the case of LDA, for each document d i a multinomial over topics is sam-pled from a Dirichlet prior with parameters  X  . The probabil-ity p ( w | z = k ) of a term w , given the topic k , is represented by  X  k,t . We refer to the complete K  X  V matrix of word-topic probabilities as  X . The multinomial parameters  X  k again drawn from a Dirichlet prior parametrized by  X  . Each observed term w in the collection is drawn from a multino-mial for the topic represented by a discrete hidden indicator variable z i . For simplicity in the mathematical development and notation, we assume symmetric Dirichlet priors but the extension to the asymmetric case is straightforward. Hence, the values of  X  and  X  are model hyper-parameters.
We extend LDA by adding an extra plate denoting the co-herent text segments of a document. In the rest, without loss of generality we use sentences as coherent segments. A finer level of granularity can be achieved though, by analysing the structure of sentences and using phrases as such seg-ments. The graphical representation of the senLDA model is shown in Figure 1 and the generative process of a docu-ment collection using senLDA is described in Algorithm 1. For inference, we use a collapsed Gibbs sampling method [5]. We now derive the Gibbs sampler equations by estimating the hidden topic variables.

In senLDA the joint distribution can be factored: because the first term is independent of  X  and the second from  X  . After standard manipulations as in the paradigm of [6] one arrives at: mensional extension of the beta function used for notation convenience, and ~n m , ~n z refer to the occurrences of top-ics with documents and topics with terms respectively. To calculate the full conditional we take into account the struc-ture of the document d and the fact that ~w d = { ~w d  X  s ~z = { ~z d  X  s ,~z  X  s } . The subscript s in ~w s ,~z s and the topic respectively of sentence s . For the full condi-tional of topic k we have: p ( z s = k | ~z  X  s , ~w ) = p ( ~w,~z ) For the first term of equation Eq. (3) we have: = Here, for the generation of A and B we used the recursive property of the  X  function:  X ( x + m ) = ( x + m  X  1)( x + m  X  2)  X  X  X  ( x + 1) x  X ( x ); w is a term that can occur many times in a sentence and n ( w ) k,s denotes w  X  X  frequency in sentence s given that the sentence s belongs to topic k ; N ( w ) k,s how many words of sentence s belong to topic t .
 The development of the second factor in the final step of Eq. (3) is similar to the LDA calculations with the difference that the counts of topics per document are calculated given
Algorithm 1: Text collection generation with senLDA the allocation of sentences to topics and not the allocation of words to topics. This yields:  X  where n ( w ) m,  X  s denotes the number of times that topic k has been observed with a sentence from document d , excluding the sentence currently sampled. Note that Eq. (5) reduces to the standard LDA collapsed Gibbs sampling inference equations if the coherent text spans are reduced to words.
The idea of integrating the sentence limits in the LDA model has been previously investigated. For instance, in [9] in the context of summarization the authors combine the unigram language model with topic models over sentences so that the latent topics are represented by sentences instead of terms. In [4] the notion of sentence topics is introduced and they are sampled from separate topic distributions and co-exist with the word topics. Also, Boyd et al. [3] propose an adaptation of topic models to the text structure obtained by the parsing tree of a document. Our method resembles these works in that it integrates the notion of sentences to extend LDA. In our case though, we directly extend LDA maintaining the association of words to topics, we retain its simplicity without adding extra hyper-parameters thus allowing a fast, gibbs sampling inference, and we do not require any language-dependent tools such as parsers.
We conduct experiments to verify the applicability and evaluate the performance of senLDA compared to LDA. The process is divided into two steps: (i) the training phase, where the topic models are trained to learn the their pa-rameters, and (ii) the inference phase that is for new, unseen documents their topic distributions are estimated. We use the Gibbs sampling inference approach given by Eq. (5). The hyper-parameters  X  and  X  are set to 1 K , with K be-ing the number of topics. Table 1 shows the datasets we used. They come from the publicly available collections of Wikipedia [7] and PubMed [8]. The first four datasets (Wik-iTrain* and PubMedTrain*) were used for learning the topic model parameters; they differ in their respective size. Also, the vocabulary of the PubMed datasets is significantly larger due to the medical terms that appear. During preprocessing Table 1: Description of the data used after pre-processing.  X  X iming X  refers to the 25 first training iterations with the left (resp. right) values corre-sponding to senLDA (resp. LDA). Figure 2: The ratio of perplexities of senLDA and LDA calculated on Wiki37 and PubMed25. we only applied lower-casing, stop-word removal and lemma-tization using the WordNet Lemmatizer. 1 The rest of the document collections of Table 1 are used for classification purposes and are discussed later in the section.
 Intrinsic evaluation Topic model evaluation has been the subject of intense research. For intrinsic evaluation we re-port here perplexity [1], which is probably the dominant measure for topic models evaluation in the bibliography. The perplexity of d held out documents given the model parameters ~  X  is defined as the reciprocal geometric mean of the token likelihoods of those data, given the parameters of the model: Note that senLDA samples per sentence and thus results in less flexibility at the word level where perplexity is calcu-lated. Even though, the comparison between senLDA and LDA, at word level using perplexity, gives insights in the relative merits of the the proposed model.

Figure 2 depicts the ratio of the perplexity values be-tween senLDA and LDA. We set K = 125 after grid search-ing K  X  { 25 , 75 , 125 , 175 } for perplexity with 5-fold cross-validation on the training data. Values higher (resp. lower) than one signify that senLDA achieves lower (resp. higher) perplexity than LDA. The figure demonstrates that in the first iterations before convergence of both models, senLDA performs better. What is more, senLDA converges after
The code and the data are publicly available at https:// github.com/balikasg/topicModelling/ only around 30 iterations, whereas LDA converges after 160 iterations on Wikipedia and 200 iterations on the PubMed datasets respectively. We define convergence as the situ-ation where the model X  X  perplexity does not any more de-crease over training iterations. The shaded area in the figure highlights the period while senLDA performs better. It is to be noted, that although competitive, senLDA does not outperform LDA given unlimited time resources. However, that was expected since for senLDA the training instances are sentences, thus the model X  X  flexibility is restricted when evaluated against a word-based measure.

An important difference between the models however, lies in the way they converge. From Figure 2 it is clear that senLDA converges faster. We highlight this by providing exact timings for the first 25 iterations of the models (col-umn  X  X iming X  of Table 1) on a machine using an Intel Xeon CPU E5-2643 v3 at 3.40GHz. For both models we use our own Python implementations with the same speed optimisa-tions. Using  X  X ikiTrain2 X  and 125 topics, for 25 iterations the senLDA needs 332 secs, whereas LDA needs 434 sec., an improvement of 30%. Furthermore, comparing the con-vergence, senLDA needs 332 secs (25 iterations) whereas LDA needs more than 2770 secs (more than 160 iterations) making senLDA more than 8 times faster. Similarly for the  X  X ubMedTrain2 X  dataset which is more complex due to its larger vocabulary size, senLDA converges around 12 times (an order of magnitude) faster. Note that senLDA  X  X  fast convergence is a strong advantage and can be highly ap-preciated in different application scenarios where unlimited time resources are not available.
 Extrinsic evaluation Previous studies have shown that perplexity does not always agree with human evaluations of topic models [1] and it is recommended to evaluate topic models on real tasks. To better support our development for senLDA applicability we also evaluate it using text classifica-tion as the evaluation task. For text classification, each doc-ument is represented by its topic distribution, which is the vectorial input to Support Vector Machines (SVMs). The classification collections are split on train/test (75%/25%) parts. The SVM regularization hyper-parameter  X  is se-lected from  X   X  [10  X  4 ,..., 10 4 ] using 5-fold cross-validation on the training part of the classification data. The PubMed testsets are multilabel, that is each instance is associated with several classes, 1.4 in average in the sets of Table 1. For the multilabel problem with the SVMs we used a binary rel-evance approach. To assess the classification performance, we report the F 1 evaluation measure, which is the harmonic mean of precision and recall.

The classification performance on F 1 measure for the dif-ferent classification datasets is shown in Figure 3. First note that in the majority of the classification scenarios, senLDA outperforms LDA. In most cases, the performance differ-ence increases when the larger train sets ( X  X ikiTrain2 X  and  X  X ubMedTrain2 X ) are used. For instance, in the second line of figures with the PubMed classification experiments, in-creasing the topic models X  training data benefits both LDA and senLDA , but senLDA still performs better. More im-portantly though and in consistence with the perplexity ex-periments, the advantage of senLDA remains: the faster senLDA convergence benefits the classification performance. The senLDA curves are steeper in the first training iterations and stabilize after roughly 30 iterations when the model con-verges. We believe that assigning the latent topics to coher-ent groups of words such as sentences results in document representations of finer level. In this sense, spans larger than single words can capture and express the document X  X  content more efficiently for discriminative tasks like classification.
To investigate the correlation of topic model representa-tions learned on different levels of text, we report the classi-fication performance using as document representations the concatenation of a document X  X  topic distributions output by LDA and senLDA . For instance, the concatenated vecto-rial representation of a document when K = 125 for each model is a vector of 250 dimensions. The resulting concate-nated representations are denoted by  X  X enLDA+ X  in Figure 3. As it can be seen,  X  X enLDA+ X  performs better compared to both LDA and senLDA . Its performance combines the advantages of both models: during the first iterations it is as steep as the senLDA representations and in the later it-erations benefits by the LDA convergence to outperform the simple senLDA representation. Hence, the concatenation of the two distributions creates a richer representation where the two models contribute complementary information that achieves the best classification performance. Achieving the optimal performance using those representations suggests that the relaxation of the independence assumptions be-tween the text structural units can be beneficial; this is also among the contributions of this work.
We proposed senLDA , an extension of LDA where top-ics are sampled per coherent text spans. This resulted in very fast convergence and good classification and perplexity performance. LDA and senLDA differ in that the second as-sumes a very strong dependence of the latent topics between the words of sentences, whereas the first assumes indepen-dence between the words of documents in general. In our future research, our goal is to investigate this dependence and further adapt the sampling process of topic models to cope with the rich text structure.
This work is partially supported by the CIFRE N 28/2015. [1] L. Azzopardi, M. Girolami, and K. van Risjbergen. [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] J. L. Boyd-Graber and D. M. Blei. Syntactic topic [4] R.-C. Chen, R. Swanson, and A. S. Gordon. An [5] T. L. Griffiths and M. Steyvers. Finding scientific [6] G. Heinrich. Parameter estimation for text analysis. [7] I. Partalas, A. Kosmopoulos, N. Baskiotis, T. Artieres, [8] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas, [9] D. Wang, S. Zhu, T. Li, and Y. Gong. Multi-document
