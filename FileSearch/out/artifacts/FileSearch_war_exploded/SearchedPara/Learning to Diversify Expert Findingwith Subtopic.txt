 Given a coauthor network, how to find the top-k experts for a given query q ?Howto diversify the ranking list so that it captures the whole spectrum of relevant authors X  ex-pertise? Expert finding has long been viewed as a challenging problem in many different domains. Despite that considerable research has been conducted to address this prob-lem, e.g., [3,17], the problem remains largely unsolved. Most existing works cast this problem as a web document search problem, a nd employ traditional relevance-based retrieval models to deal with the problem.

Expert finding is different from the web document search. When a user is looking for expertise collaborators in a domain such a s  X  X ata mining X , she/he does not typically mean to find general experts in this field. Her/his intention might be to find experts on different aspects (subtopics) of data mining (e.g.,  X  X sso ciation rules X ,  X  X lassification X ,  X  X lustering X , or  X  X raph mining X ). Recently , diversity already becomes a key factor to address the uncertainty and ambiguity problem in information retrieval [12,21]. How-ever, the diversification problem is still not well addressed for expert finding. In this paper, we try to give an explicit diversity-based objective function for expert finding, and to leverage a learning-based algorithm to improve the ranking performance. Motivating Examples. To illustrate this problem, Figure 1 gives an example of di-versified expert finding. The list on the left is obtained by using language model, a state-of-the-art relevance-based ranking model [2]. We see that all the top five experts are mainly working on information retrieval models. The right list is obtained using the proposed diversified ranking method with four subtopics. The top two experts are working on information retrieval models, but the third one is working on multimedia retrieval, the fourth is about digital library, and the fifth is about information retrieval using natural language processing. The diversified ranking list is more useful in some sense: the user can quickly gain the major subtopics of the query, and could refine the query according to the subtopic that she/he is interested in. Additionally, the user can have the hint about what the other users are r ecently interested in, as the ranking list is obtained by learning from the user fee dback (e.g., users X  click data).

We aim to conduct a systematic investigation into the problem of diversifying expert finding with subtopics. The problem is non-trivial and poses a set of unique challenges. First, how to detect subtopics for a given query? Second, how to incorporate the diver-sity into the relevance-based ranking score? Third, how to efficiently perform the expert ranking algorithm so that it can be scaled up to handle large networks? Contributions. We show that incorporating diversity into the expert finding model can significantly improve the ranking performance (+15.3%-+94.6% in terms of MAP) compared with several alternative methods using language model, topic model and ran-dom walk. In this work, we try to make the following contributions:  X  We precisely formulate the problem of diversified expert finding and define an ob- X  We present a learning-based algorithm to solve the objective function.  X  We evaluate the proposed method in a real system. Experimental results validate its Organization. Section 2 formulates the problem. Section 3 explains the proposed method. Section 4 presents experimental results that validate the effectiveness of our methodology. Finally, Section 5 reviews related work and Section 6 concludes. In this section, we formulate the problem in the context of academic social network to keep things concrete, although adaption of this framework to expert finding in other social-network settings is straightforward.

Generally speaking, the input of our problem consists of (1) the results of any topic modeling such as predefined ontologies or topic cluster based on pLSI [9] or LDA [5] and (2) a social network G =( V,E ) and the topic model on authors V ,where V is a set of authors and E  X  V  X  V is a set of coauthor relationships between authors. More precisely, we can define a topic distr ibution over each author as follows. Topic distribution: In social networks, an author usually has interest in multiple topics. Formally, each user v  X  V is associated with a vector  X  v  X  R T of T -dimensional topic distribution ( z  X  vz =1 ). Each element  X  vz is the probability (i.e., p ( z | v ) ) of the user on topic z .

In this way, each author can be mapped onto multiple related topics. In the meantime, for a given query q , we can also find a set of associated topics (which will be depicted in detail in  X  3). Based on the above concepts, the goal of our diversified expert finding is to find a list of experts for a given query such that the list can maximally cover the associated topics of the query q . Formally, we have: Problem 1. Diversified Expert Finding. Given (1) a network G =( V,E ) ,(2) T -dimensional topic distribution  X  v  X  R T for all authors v in V , and (3) a metric function f ( . ) , the objective of diversifie d expert finding for each query q is to maximize the following function: where f ( k | z,G, X ,q ) measures the relevance score of top-k returned authors given topic z ; we can apply a parameter  X  to control the complexity of the objective function by selecting topics with larger probabilities (i.e., minimum number of topics that satisfy z p ( z Please note that this is a general formulation of the problem. The relevance metric f ( k | z,G, X ,q ) can be instantiated in different ways and the topic distribution can also be obtained using different algorithms. Our formulation of the diversified expert finding is very different from existing works on expert finding [3,16,17]. Existing works have mainly focused on finding relevant experts for a given query, but ignore the diversi-fication over different topics. Our problem is also different from the learning-to-rank work [11,23], where the objective is to combine different factors into a machine learn-ing model to better rank web documents, which differs in nature from our diversified expert finding problem. 3.1 Overview At a high level, our approach primarily consists of three steps:  X  We employ an unified probabilistic model to uncover topic distributions of authors  X  We propose an objective function which incorporates the topic-based diversity into  X  We present an efficient algorithm to solve the objective function. 3.2 Topic Model Initialization In general, the topic information can be obtained in many different ways. For exam-ple, in a social network, one can use the predefined categories or user-assigned tags as the topic information. In addition, we can use statistical topic modeling [9,5,20] to automatically extract topics from the social networking data. In this paper, we use the author-conference-topic (ACT) model [20] to initialize the topic distribution of each user. For completeness, we give a brief introduction of the ACT model. For more de-tails, please refer to [20].

ACT model simulates the process of writing a scientific paper using a series of prob-abilistic steps. In essence, the topic model uses a latent topic layer Z = { z 1 ,z 2 , ..., z T } as the bridge to connect the different types of objects (authors, papers, and publication venues). More accurately, for each object it es timates a mixture of t opic distribution which represents the probability of the object being associated with every topic. For ex-ample, for each author, we have a set of probabilities { p ( z i | a ) } and for each paper d ,we have probabilities { p ( z i | d ) } . For a given query q , we can use the obtained topic model to do inference and obtain a set of probabilities { p ( z i | q ) } . Table 1 gives an example of the most relevant topics for the query  X  X atabase X . 3.3 DivLearn : Learning to Diversify Expert Finding with Subtopics Objective Function. Without considering diversification, we can use any learning-to-rank methods [11] to learn a model for ranking experts. For example, given a training data set (e.g., users X  click-through data), we could maximize normalized discounted cumulative gain (NDCG) or Mean Average Precision (MAP). In this section, we use MAP as the example in our explanation. Basically, MAP is defined as: where Q is a set of queries in the training data; Prec ( a ji ) represents the precision value obtained for the set of top i returned experts for query q j ; rel ( a ji ) is an indicator function equaling 1 if the expert a ji is relevant to query q j , 0 otherwise. The normalized inner sum denotes the average precision for the set of top k experts and the normalized outer sum denotes the average over all queries Q .
 Now, we redefine the objective function bas ed on a generalized MAP metric called MAP-Topic, which explicitly incorporates the diversity of subtopics. More specifically, related experts for query q ( j ) , we can define the following objective function: Linear Ranking Model. To instantiate the expert ranking model, we define differ-ent features. For example, for expert findi ng in the academic network, we define fea-tures such as the number of publications, h -index score of the author, and the language model-based relevance score. For the i -th feature, we define  X  i ( a, q ) as the feature value of author a to the given query q . Finally, without loss of generality, we consider the lin-ear model to calculate the score for ranking experts, thus have where w i is the weight of the i -the feature. Given a feature weight vector w , according to the objective function described above , we can calculate a value, denoted as O ( w ) , to evaluate the ranking results of that model. Thus our target is to find a configuration of w to maximize O ( w ) . 3.4 Model Learning Many algorithms can be used for finding the optimal w in our model, such as hill climbing [15], gene programming(GP) [10], random walk, gradient descent [4]. For the Algorithm 1. Model learning algorithm.
 purpose of simplicity and effectiveness, in this paper, we utilize the hill climbing algo-rithm due to its efficiency and ease of implementation. The algorithm is summarized in Algorithm 1.

Different from the original random start hill climbing algorithm which starts from pure random parameters, we add our prior knowledge empiricalV ector to the initial-ization of w , as we know some features such as BM25 will directly affect the relevance degree tends to be more important. By doing so, we could reduce the CPU time for training. We evaluate the proposed models in an online system, Arnetminer 1 . 4.1 Experiment Setup Data Sets. From the system, we obtain a network consisting of 1,003,487 authors, 6,687 conferences, and 2,032,845 papers . A detailed introduction about how the aca-demic network has been constructed can be referred to [19]. As there is no standard data sets available, and also it is difficult to create such an data sets with ground truth. For a fair evaluation, we construct a data set in the following way: First, we select a num-ber of most frequent queries from the query log of the online system; then we remove the overly specific or lengthy queries (e.g.,  X  X  Convergent Solution to Subspace Learn-ing X ) and normalize similar queries (e.g.,  X  X eb Service X  and  X  X eb Services X  to  X  X eb Service X ). Second, for each query, we identif y the most relevant (top) conferences. For example, for  X  X eb Service X , we select ICWS an d for  X  X nformation Retrieval X , we select SIGIR. Then, we collect and merge PC co-chairs, area chairs, and committee members of the identified top conferences in the past four years. In this way, we obtain a list of candidates. We rank these candidates accord ing to the appearing times, breaking ties using the h-index value [8]. Finally, we use the top ranked 100 experts as the ground truth for each query.
 Topic Model Estimation. For the topic model (ACT), we perform model estimation by setting the topic number as 200, i.e., T = 200 . The topic number is determined by empirical experiments (more accurately, by m inimizing the perplexity [2], a standard measure for estimating the performance of a probabilistic model, the lower the better). The topic modeling is carried out on a server running Windows 2003 with Dual-Core Intel Xeon processors (3.0 GHz) and 4GB m emory. For the academic data set, it took about three hours to estimate the ACT model.
 We produce some statistics for the selected queries (as shown in Table 2). Entropy ( q ) measures the query X  X  uncertainty and #(  X  =0 . 2) denotes the minimum number of topics that satisfy P ( z | q )  X   X  .
 Feature Definition. We define features to capture the observed information for ranking experts of a given query. We consider two types of features: 1) query-independent fea-tures (such as h-index, sociability, and longev ity) and 2) query-dependent features (such as BM25 [13] score and language model with recency score). A detailed description of the feature definition is given in Appendix.
 Evaluation Measures and Comparison Methods. To quantitatively evaluate the pro-posed method, we consider two aspects: relevance and diversity. For the feature-based ranking, we consider six-fold cross-validation(i.e. five folds for training and the rest for testing) and evaluate the approaches in terms of Prec@5, Prec@10, Prec@15, Prec@20, and MAP. And we conduct evaluation on the entire data of the online system (including 916,946 authors, 1,558,499 papers, and 4,501 conferences). We refer to the proposed method as DivLearn and compare with the following methods:
RelLearn : A learning-based method. It uses the s ame setting (the same feature defini-tion and the same training/test data) as that in DivLearn , except that it does not consider the topic diversity and directly use MAP as the objective function for learning.
Language Model : Language model(LM) [2] is one of the state-of-the-art approaches for information retrieval. It defines the relevance between an expert (document) and a query as a generative probability: p ( q | d )=
BM25 [13]: Another state-of-the-art probabilistic retrieval model for information re-trieval. pLSI : Hofmann proposes the probabilistic Latent Semantic Indexing(pLSI) model in [9]. After modeling, the probability of generating a word w from a document d can be use the EM algorithm[9].

LDA : Latent Dirichlet Allocation (LDA) [5] also models documents by using a topic layer. We performed model estimation with the same setting as that for the ACT model.
ACT : ACT model is presented in  X  3. As the learned topics is usually general and not specific to a given query, only using it alone for modeling is too coarse for academic search [22], so the final relevance score is defined as a combination with the language model p ( q | a )= p ACT ( q | a )  X  p LM ( q | a ) .

ACT+RW : A uniform academic search framework proposed in [17], which combines random walk and the ACT model together. 4.2 Performance Comparison Table 3 lists the performance results of the different comparison methods. It can be clearly seen that our learning approach significantly outperforms the seven comparison methods. In terms of P@5, our approach achieves a +23% improvement compared with the (LDA). Comparing with the other expert finding methods, our method also results in an improvement of 8-18%. This advantage is due to that our method could combine multiple sources of evidences together. From Table 3, we can also see that the learning-based methods (both RelLearn and DivLearn) outperform the other relevance-based methods in terms of all measurements. Our DivLearn considers the diversity of topics, thus further improve the performance. 4.3 Analysis and Discussion Now, we perform several analysis to examine the following aspects of DivLearn :(1) convergence property of the learning algorithm; (2) effect of different topic threshold; and (3) effect of recency impact function in Eq. 7. Convergence Property. We first study the convergence property of the learning al-gorithm. We trace the execution of 72 random hill climbing runs to evaluate the con-vergence of the model learning algorithm. On average, the number of iterations to find the optimal parameter w varies from 16 to 28. The CPU time required to perform each iteration is around 1 minute. This suggests that the learning algorithm is efficient and has a good convergence property.
 Effect of Topic Threshold. We conduct an experiment to see the effect of using dif-ferent thresholds  X  to select topics in the objective function (Eq. 3). We select the min-imum number of topics with higher probabilities that statisfy z p ( z | q )  X   X  ,then re-scale this sum to be 1 and assign 0 to other topics. Clearly, when  X  =1 , all topics are counted. Figure 2a shows the value of MAP of multiple methods for various  X  .It shows that this metrics is consistent to a certain degree. The performance of different methods are relatively stable with differen t parameter setting. This could be explained by Figure 2b, which depicts the cumulated P ( z | q ) of top n topics. As showed, for a given query, p ( z | q ) tends to be dominated by several top related topics. Statistics in Table 2 also confirm this observation. All these observations confirm the effectiveness of the proposed method.
 Effect of Recency. We evaluate whether expert finding is dynamic over time. In Eq. 7, we define a combination feature of the language model score and the recency score (Func 1). Now, we qualitatively examine how different settings for the recency impact function will affect the performance of DivLearn . We also compared with some other performance of MAP with different parameter  X  . The baseline denote the performance without considering recency. It shows that recency is an important factor and both im-pact functions perform better than the base line which does not consider the recency. We can also see that both impact function perform best with the setting of  X  ! 5 .On average, the first impact function (Func 1, used in our approach) performs a bit better than Func 2. Previous works related to our learning to diversify for expert finding with subtopics can be divided into the following three aspects: expert finding, learning to rank, search result diversification. On expert finding, [17] propose a topic level approach over het-erogenous network. [3] extended language m odels to address the expert finding prob-lem. TREC also provides a platform for researchers to evaluate their models[16]. [7] present a learning framework for expert finding, but only relevance is considered. Other topic model based approaches were proposed either[17].

Learning to rank aims to combining multiple sources of evidences for ranking. Liu [11] gives a survey on this topic. He categorizes the related algorithms into three groups, namely point-wise, pair-wise and list-wise. To optimize the learning target, in this paper we use an list-wise approach, which is similar to [23].

Recently, a number of works study the problem of result diversification by taking inter-document dependencies into consideration [1,25,6,18]. Yue and Joachims [24] present a SVM-based approach for learning a good diversity retrieval function. For evaluation, Agrawal et al. [1] generalize classical information retrieval metrics to ex-plicitly account for the value of diversification. Zhai et al. [25] propose a framework for evaluating retrieval different subtopics of a query topic. However, no previous work has been conducted for learning to diversify expert finding. In this paper, we study the problem of learning to diversify expert finding results us-ing subtopics. We formally define the problem in a supervised learning framework. An objective function is defined by explicitly incorporating topic-based diversity into the relevance based ranking model. An efficient algorithm is presented to solve the ob-jective function. Experiment results on a real system validate the effectiveness of the proposed approach.

Learning to diversify expert finding represents a new research direction in both infor-mation retrieval and data mining. As future work, it is interesting to study how to incor-porate diversity of relationships between experts into the learning process. In addition, it would be also interesting to detect user intention and to learn weights of subtopics via interactions with users.
 This section depicts how we define features in our experiment. In total, we defined features of two categories: query-independent and query-dependent .  X  h-index: h-index equals h indicates that an author has h of N papers with at least h  X  Longevity: Longevity reflects the length of an author X  X  academic life. We consider  X  Sociability: The score of an author X  X  sociability is defined based on how many co- X  Language Model with Recency: We consider the effect of recency and impact factor  X  BM25 with Recency: It defines a similar relevance score as that in Eq. 6, except that
