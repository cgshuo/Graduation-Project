 Trust networks, where people leave trust and distrust feed-back, are becoming increasingly common. These networks may be regarded as signed graphs, where a positive edge weight captures the degree of trust while a negative edge weight captures the degree of distrust. Analysis of such signed networks has become an increasingly important re-search topic. One important analysis task is that of sign inference, i.e., infer unknown (or future) trust or distrust re-lationships given a partially observed signed network. Most state-of-the-art approaches consider the notion of structural balance in signed networks, building inference algorithms based on information about links, triads, and cycles in the network. In this paper, we first show that the notion of weak structural balance in signed networks naturally leads to a global low-rank model for the network. Under such a model, the sign inference problem can be formulated as a low-rank matrix completion problem. We show that we can perfectly recover missing relationships, under certain conditions, us-ing state-of-the-art matrix completion algorithms. We also propose the use of a low-rank matrix factorization approach with generalized loss functions as a practical method for sign inference  X  this approach yields high accuracy while being scalable to large signed networks, for instance, we show that this analysis can be performed on a synthetic graph with 1.1 million nodes and 120 million edges in 10 minutes. We further show that the low-rank model can be used for other analysis tasks on signed networks, such as user segmenta-tion through signed graph clustering, with theoretical guar-antees. Experiments on synthetic as well as real data show that our low rank model substantially improves accuracy of sign inference as well as clustering. As an example, on the largest real dataset available to us (Epinions data with 130K nodes and 840K edges), our matrix factorization ap-proach yields 94.6% accuracy on the sign inference task as compared to 90.8% accuracy using a state-of-the-art cycle-based method  X  moreover, our method runs in 40 seconds as compared to 10,000 seconds for the cycle-based method. H.2.8 [ Database Management ]: Database Applications X  Data Mining Signed Networks, Structural Balance, Low Rank Model
Social network analysis has received a lot of attention re-cently. Traditionally, online networks such as Facebook or World Wide Web can be viewed as graphs, with nodes rep-resenting entities, and edges representing relationships be-tween entities. Recently, trust networks have also become in-creasingly common where two opposite kinds of relationships exist between entities. For example, online review websites such as Epinions allow users to either like or dislike others X  reviews. Such networks can be modeled as signed networks , where edge weights are +1 or  X  1, representing positive or negative relationships respectively.

Perhaps the most basic yet significant belief in signed net-works is structural balance [11, 4]. Structural balance states that people in signed networks tend to follow patterns such as X  X n enemy of my friend is my enemy X  X nd X  X n enemy of my enemy is my friend X , and so on [4]. It is important to note that since balance notion applies only to signed networks, therefore, algorithms for signed networks can be somewhat different from algorithms for unsigned networks. Structural balance has been shown to be useful for analysis tasks for signed networks. For instance, the sign inference problem , which aims to infer the unknown relationship between two entities, can be achieved by learning from balance informa-tion of signed networks [17, 5]. Nevertheless, these state-of-the-art methods for sign inference problem mainly consider structural balance, while a more general notion -weak bal-ance [7] -is not taken into account. Therefore, it is natural to ask what can be further inferred from weak balance.
In this paper, we propose a low rank model by observ-ing that complete weakly balanced networks have a low-rank structure. Therefore, many analysis tasks such as sign inference and clustering can be posed as low-rank matrix completion. The advantages of taking a matrix completion approach are as follows. First, many matrix completion al-gorithms provide theoretical recovery guarantees under cer-tain conditions [19, 12]. Moreover, many algorithms such as Alternating Least Squares (ALS) and Stochastic Gradi-ent Descent (SGD) can efficiently find effective solutions for problems with billions of nonzero entries [14]. We will em-pirically demonstrate that our proposed low rank model is both accurate and scalable in many applications.
We summarize the contributions of this paper:
The paper is organized as follows. In Section 2, we review some recent work related to this paper. In Section 3, we pro-pose our low rank model, and show that the sign inference problem can be modeled as a low-rank matrix completion problem. We present two approaches; matrix completion and matrix factorization. In Section 4, we show how to do clustering using the low rank model. In Section 5, we conduct experiments which convincingly demonstrate that our low rank model improves sign inference accuracy as well as clustering results. Finally, we present our conclusions in Section 6.
Signed network analysis has a rich history dating back to the 1950s  X  the notion of structural balance was formulated and analyzed by Harary and Carwright [11, 4], who formally defined balanced triads and proved global structural results for signed balanced networks (stated as Theorem 1 in Sec-tion 3). Davis [7] further generalized the notion of balance to weak balance, by allowing triads where all edges are neg-ative. In Section 3, we will elaborate on weak balance and show that it naturally leads to our proposed low-rank model.
An important analysis task on signed networks is the sign inference problem. This problem was first considered by Guha et al.[9]. More recently, Kunegis et al.[15, 16] recon-sidered this problem by using varied similarity functions and kernels such as matrix exponential and signed Laplacian, on the signed link structure of the network. Leskovec et al.[17] proposed a machine learning formulation of this problem, arguing that learning from only local triangular structure of edges can achieve high accuracy. Chiang et al.[5] general-ized [17] by showing that longer cycles in the signed network reveal balance information in the network  X  using these ad-ditional  X  X eatures X  for learning led to an improvement in in-ference accuracy. Our modeling approach in this paper is distinct from existing work, as we first show that the global viewpoint of structural balance (as opposed to the local triad structure) naturally leads to a low-rank model for the net-work, and then show that the sign inference problem may be regarded as a low-rank matrix completion problem. Our approach is much more scalable than previous approaches, and leads to higher inference accuracy as well. Some recent work also considers the link inference problem as network completion [10, 13]. The goal in [10, 13] is to reconstruct the underlying (unsigned) network topology given partially observed links and/or nodes. In contrast, the main goal of our work is to infer the (unobserved) signed relationships between all pairs of entities. Moreover, our low-rank matrix completion approach arises from the notion of weak struc-tural balance, which only applies in signed networks.
Clustering or community detection is another important task in network analysis, and has been well studied for un-signed social networks using many varied approaches [18, 8]. However, extending these algorithms to signed networks is not obvious since it has been shown that clustering on signed networks is highly related to (weak) balance the-ory [11, 4, 7]. Thus several tailored approaches have been proposed for clustering of signed networks [20]. Recently, Kunegis et al.[16] proposed a spectral approach using the so-called  X  X igned X  Laplacian, and showed that partitioning signed networks using the signed Laplacian kernel is anal-ogous to considering ratio cut on unsigned networks. Our approach is somewhat similar to [16] in that we also con-sider the spectra of signed graphs. However, our clustering algorithm proceeds by first completing the underlying graph using low-rank matrix completion, and then performing the clustering. This important difference makes our clustering results much more reliable, especially on graphs where the observed signed relationships are sparse.

Sign inference using our low rank model is closely related to matrix completion problem. In the last five years, there has been substantial research that has studied exact recov-ery conditions for this problem [19, 3, 2], and algorithms with theoretical guarantees have also been proposed [1, 12]. Matrix factorization is another approximation technique for matrix completion. Though this approach is notoriously hard to analyze, it is very competitive in practice [14]. While the matrix completion problem is considered mostly in col-laborative filtering, our low rank model arises naturally from weak balance of signed networks. We will discuss the details of matrix completion and matrix factorization in Section 3.
In this section, we investigate the structure of signed net-works and show that a low-rank structure intuitively emerges when we consider so-called weakly balanced networks. First, we introduce a few preliminaries before going into details.
We will consider a signed network to be a graph G = ( V, E, A ), where V is the vertex set of size n , E is the edge set of size m , and A  X  R n  X  n is the signed adjacency matrix associated with G . The entries of A are as follows: A For now we restrict ourselves to symmetric relationships, i.e., A is a symmetric matrix, although asymmetric relationships are possible. Note that for us, A ij = 0 does not imply no relationship between i and j ; it just means we do not currently observe the relationship. We denote the set of known entries by  X , i.e. ( i, j )  X   X  iff A ij is observed. A network is complete if all entries in A are observed.
We first review some basics of structural balance. A com-plete network is called (strongly) balanced if all triads in the graph have (i) all positive edges or (ii) only one posi- X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  tive edge. With this local definition, it can be shown that complete balanced networks have a special global structure: Theorem 1 (Global  X  X trong Balance X  Structure [11]) A complete network is balanced iff all edges are positive, or the vertices can be divided into two different groups such that all edges within the same group are positive, and all edges between the two groups are negative.

The assumption that strong structural balance exists in a real signed network might be too extreme, so a more relaxed notion of balance is the so-called  X  X eak X  structural balance theory. The formal definition of weak balance is as follows: Definition 1 (Weak Balance for Complete Graphs [7]) A complete signed network is weakly balanced iff there is no triad in the network that contains two positive edges and one negative edge.

Similar to Theorem 1, this local definition implies a simple global structure of weakly balanced networks: Theorem 2 (Global  X  X eak Balance X  Structure [7]) A complete signed network is weakly balanced iff all edges are positive, or the vertices can be divided into several groups such that within-group edges are positive and between-group edges are negative.

Thus we can say that a network is k -weakly balanced iff it can be perfectly divided into k groups, k  X  N . Note that Theorem 1 can be regarded as a special case of Theorem 2 with k  X  2. We now show that the adjacency matrix A of a complete k -weakly balanced network is low rank. With a suitable reordering of nodes, A can be represented as a block-diagonal matrix where all entries within the diagonal blocks are +1 X  X , and all entries within off-diagonal blocks are all  X  1 X  X . The following theorem proves that the adjacency matrix of a complete k -weakly balanced network has rank up to k .
 Theorem 3 (Low Rank Structure of Signed Networks) The adjacency matrix A of a complete k -weakly balanced network has rank 1 if k  X  2 , and has rank k for all k &gt; 2 .
Proof. Since A is k -weakly balanced, the nodes can be divided into k groups, say S (1) , S (2) , . . . , S ( k ) S tors A (after suitable reordering of nodes): and so the column space of A is spanned by { b 1 , . . . , b
First let us consider k  X  2, i.e., the network is strongly balanced. If k = 1, it is easy to see that rank( A ) = 1. If k = 2, then b 1 =  X  b 2 . Therefore, rank( A ) is again 1. Now consider k &gt; 2. In this case, we argue that rank( A ) = k by showing that b 1 , . . . , b k are linearly independent. We consider the following k  X  k square matrix: It is obvious that 1 = [1 1 1] T is an eigenvector of M with eigenvalue  X  ( k  X  2). We can further construct k  X  1 linearly independent eigenvectors all with eigenvalue 2: where e i  X  R k is the i th column of the k  X  k identity matrix. These k  X  1 eigenvectors are clearly linearly independent. Therefore, rank( M ) = k .

From the above we can show that rank( A ) = k . Suppose that b 1 , . . . , b k are not linearly independent, then there ex-ists  X  1 , . . . ,  X  k , with some  X  i 6 = 0, such that P Using this set of  X   X  X , it is easy to see that P k i =1  X  but this contradicts the fact that rank( M ) = k . Therefore, rank( A ) = k .
 Figure 1 is an example of a complete 3-weakly balanced n etwork. As shown, its adjacency matrix can be expressed as a product of two rank-3 matrices, indicating its rank is no more than three. By Theorem 3, we can conclude that rank( A ) = 3.

The above reasoning shows that complete weakly balanced graphs are low rank, however, most real networks are not complete graphs. One way to define balance on graphs that are not complete is to try to fill in the unobserved or missing edges(relationships) so that balance is obtained: Definition 2 (Weak Balance for General Graphs) A signed network is weakly balanced iff it is possible to add missing edges to the network, with appropriate sign, so that the resulting complete graph is weakly balanced.

Hence, sign inference in trust networks can be thought of as a low-rank matrix completion problem. Specifically, given a signed network with observed edges A ij , ( i, j )  X   X , we want to find a complete matrix by assigning  X  1 to every unknown entry, such that the resulting complete graph is (nearly) weakly balanced and hence, the completed matrix is low rank. Thus, our missing value estimation problem can be formulated as: Once we obtain the minimizer of (1), which we will denote b y X  X  , we can infer the missing relationship between i and j by simply looking up the sign of the entry X  X  ij . However, it is known that solving (1) is NP-hard in general. Recent research on low-rank matrix completion has shown the sur-prising result that in many cases, problem (1) can be solved to yield the global optimal in polynomial time [3]. In the following subsections, we identify such conditions as well as approaches to approximately solve (1) for real-world signed networks.
One possible approximate solution for (1) can be obtained by dropping the discrete constraints and replacing rank( X ) by k X k  X  , where k X k  X  denotes the trace norm of X , which is the tightest convex relaxation of rank. Thus, a convex relaxation of (1) is:
It turns out that, under certain condition, by solving (2) we can recover the exact missing relationships from the un-derlying complete signed network. This surprising result is the consequence of recent research [3, 2] which has shown that perfect recovery from the observations is possible if the observed entries are uniformly sampled and X  X  has high incoherence, which may be defined as follows: Definition 3 (Incoherence) An m  X  n matrix X  X  with singular value decomposition X  X  U SV T is -incoherent if
Intuitively, higher incoherence (smaller ) means that large entries of X  X  are not concentrated in a small part of the matrix, and so uniform sampling is sufficient to recover X The following theorem summarizes the exact recovery con-dition that we will use in this paper: Theorem 4 (Recovery Condition [2]) Let X  X  be a matrix of bounded rank ( k = O (1) ) with sin-gular value decomposition X  X  = U SV T . Assume X  X  is -incoherent and more than C 4 n log 2 n entries are uniformly sampled, then with probability at least 1  X  n  X  3 , X  X  is the unique optimizer of (2) .

Based on Theorem 4, we now show that the notion of inco-herence can be connected to the relative sizes of the clusters in signed networks. As a result, by solving (2), we can re-cover the underlying signed network with high probability if there are no extremely small groups. More precisely, we define the group imbalance of a signed network as follows: Definition 4 (Group Imbalance) Let X  X  be the adjacency matrix of a complete k -weakly balanced network with n nodes, and let n 1 , . . . , n sizes of the groups. Group imbalance  X  of X  X  is defined as By definition, k  X   X   X  n . Intuitively, larger group imbalance  X  indicates the presence of a very small group, which would make recovery of the underlying network harder (under uni-form sampling). For example, consider an extreme scenario that a k -weakly balanced network contains n nodes, with two groups containing only one node. Then the adjacency matrix of this network has group imbalance  X  = n with the following form: ble to determine whether the last two nodes are in the same cluster, or each of them belongs to an individual cluster. When n is very large, the probability of observing one of these two entries will be extremely small. Therefore, no ma-trix completion algorithm can exactly recover this network under uniform sampling.

Motivated by this example, we now analytically show that group imbalance  X  determines the possibility of recovery. We first show the connection between  X  and incoherence . Theorem 5 (Incoherence of Signed Networks) Any complete k -weakly balanced network is  X  -incoherent where  X  is the group imbalance measurement.

Proof. Let X  X  be the adjacency matrix of a k -weakly balanced complete network. Recall from Definition 3 that is defined as the maximum absolute value in the (normal-ized) singular vectors of X  X  , which are the same as eigen-vectors of X  X  since the adjacency matrix is symmetric.
Let u be any eigenvector of X  X  ( k u k 2 = 1) with eigenvalue  X  . Suppose i and j are in the same group, namely X  X  i, : X the following form: Since k u k 2 = 1, P k i =1 n i v 2 i = 1, and so n i v 2 implies | v i | X  1 /  X  n i ,  X  i . Thus, max T herefore, X  X  is  X  -incoherent.

Putting together Theorems 4 and 5, we now have the main t heorem of this subsection: Theorem 6 (Recovery Condition for Signed Networks) Suppose we observe edges A ij , ( i, j )  X   X  , from an underlying k -weakly balanced signed network X  X  , and suppose that the following assumptions hold: A. k is bounded ( k = O (1) ), B. the set of observed entries  X  is uniformly sampled, and C. number of samples is sufficiently large, i.e. |  X  | X  C X  Then X  X  can be perfectly recovered by solving (2) , with probability at least 1  X  n  X  3 .
 In particular, if n i /n is lower bounded so that  X  is a constant, then we only need O ( n log 2 n ) observed entries to exactly recover the complete k -weakly balanced network.

It is known that the convex problem (2) can be exactly solved by an SDP. However, the computational cost of SDP might be too prohibitive in practice. Recent research pro-vi des more efficient algorithms to approximately solve (1) [1, 12]. In our experiment, we use the SVP algorithm proposed by Jain et al.[12] which attempts to solve matrix comple-tion problem in an efficient manner. Experimental evidence in [12] shows that all iterates of the SVP algorithm are -incoherent, in which case the matrix completion problem (1) can be exactly solved by SVP. In Section 5, we will see that SVP performs well in recovering weakly balanced networks.
Though matrix completion algorithms can guarantee re-covery for weakly balanced networks under certain condi-tions, convex relaxation (2) does not work very well in real-life applications, where observed values are not uniformly distributed, which violates one of the assumptions in The-orem 6. In addition, the methods for solving (2) cannot scale to very large datasets. Thus, we use a gradient based matrix factorization approach as an approximation to the signed network completion problem. In Section 5, we will see that a matrix factorization approach can not only boost the accuracy of estimation but also scale to large real net-works.

In the matrix factorization approach, we consider the fol-lowing problem:
Although problem (6) is non-convex, it is widely used in practical collaborative filtering applications as the perfor-mance is competitive or better as compared to trace-norm minimization, while scalability is much better. For example, to solve the Netflix problem, (6) has been applied with a fair amount of success to factorize the dataset with 100 million ratings [14].

Nevertheless, there is an issue when modeling signed net-works using (6): the square loss in the first term of (6) tends to force entries of W T H to be either +1 or  X  1. However, what we care about in this completion task is the consistency between sign(( W T H ) ij ) and sign( A ij ) rather than their dif-ference. For example, ( W T H ) ij = 10 should have zero loss when A ij = +1 if only the signs are important.

To resolve this issue, instead of using the squared loss, we use a loss function that only penalizes the inconsistency in sign . More precisely, objective (6) can be generalized as: In order to penalize inconsistency of sign, we can change the loss function to be the sigmoid or squared-hinge loss: In Section 5, we will see that applying sigmoid or square-hinge loss functions slightly improves prediction accuracy. Time complexity. There are two main optimization tech-niques for solving (7) for large-scale data: Alternating Least Squares (ALS) and Stochastic Gradient Descent (SGD) [14]. ALS solves the squared loss problem (6) by alternately min-imizing W and H . When one of W or H is fixed, the op-timization problem becomes a least squares problem with respect to the other variable, so that we can use well devel-oped least squares solvers to solve each subproblem. Given a n n  X  n observed matrix with m observations, the time com-plexity for each subproblem requires O ( mk 2 ) operations to form the Hessian matrices, and O ( nk 3 ) to solve the least squares problem. Therefore, the time complexity of ALS is O ( t 1 ( mk 2 + nk 3 )) where t 1 is the number of iterations.
However, ALS can only be used when the loss function is square loss. To solve the general form (7) with various loss functions, we use stochastic gradient descent (SGD).In SGD, for each iteration, we pick an observed entry ( i, j ) at random, and only update the i th column of W and the j th column of H , denoted by w i and h j , respectively. The update rule for w i is given by: where  X  is a small step size. The update rule for h j is similar to (9). Since each SGD update (9) costs O ( k ) time, after a sweep through all known entries it will take O ( mk ) time. Therefore, the time complexity for SGD is O ( t 2 mk ), where t is the number of iterations taken by SGD to converge. Notice that although the complexity of SGD is linear in k , it usually takes many more iterations to converge compared with ALS, i.e., t 2 &gt; t 1 .

On the other hand, all previous link or cycle-based sign inference algorithms [17, 5] require time at least O ( nm ) be-cause all of them contain some n  X  n sparse matrix multipli-cation steps in model construction. Moreover, for all length-l paths, the number of features is exponential in l . Therefore, assuming the number of features in consideration is d , the time complexity for various methods will be O ( dnm ). The time complexity is summarized in the following table:
Since in real large-scale social networks, m &gt; n  X  t 1 this shows our alternative minimization approach is much more efficient for sign inference.
We now show that real networks tend to exhibit low-rank structure to a much greater extent than random net-works. We consider three large-scale online social networks  X  Wikipedia, Epinions[17], and Slashdot[15] 1 . Table 1 shows the statistics of these datasets. We compare these real net-works with random networks as the baseline. The random network is created using the Erd  X  os-R  X enyi model with spar-sity equal to the Wikipedia network.

To measure the closeness of observed entries between the original network and the completed matrix, we first de-rive the low-rank complete matrix A  X  by conducting ma-trix completion using the observed entries A ij . Then, we look at the relative error on the observed set  X : err  X  = k W  X  ( A  X   X  A ) k F / k A k F , where W ij = 1 if ( i, j )  X   X  and W ij = 0 otherwise, and  X  denotes element-wise multiplica-
A ll the three data sets can be downloaded from SNAP ( http://snap.stanford.edu ) tion. Clearly, smaller err  X  i ndicates better approximation for the observed entries.

In our experiment, we choose matrix factorization ap-proach for matrix completion, with ranks k = 1 , 2 , 4 , 8 , 16 and 32. For each network (three real datasets and the ran-dom network), we complete the network with different k and compute err  X  . The result is shown in Figure 2. Compared with the purely random network, the three real-life networks achieve much smaller err  X  for each small k . This suggests that low-rank matrices provide a better approximation of the observed entries for each real-life network, as compared to random Erd  X  os-R  X enyi graphs. Figure 2: Relative error between adjacency matrix and com-p leted matrix with respect to observed entries, for real-life networks versus a random network. Real-life networks achieve much smaller relative error for every k as compared with the random network.
In this section, we see how to take advantage of the low-rank structure of signed networks to find clusters. Based on weak balance theory, the general goal of clustering for signed graphs is to find a k -way partition such that most within-group edges are positive and most between-group edges are negative. One of the state-of-the-art clustering algorithms [16] extends the notion of Laplacian to signed networks, and proposes a spectral clustering algorithm based on a signed Laplacian matrix. Given a partially observed signed net-work A , the signed Laplacian is defined as  X  D  X  A , where  X  D is a diagonal matrix in which  X  D ii = P j 6 = i | A ij definition, the ratio cut of signed networks can be derived by computing the top k eigenvectors of  X  L , say U  X  R n  X  k subsequently running the k -means algorithm on U to get the clusters. This procedure is analogous to the standard spectral clustering algorithm on unsigned graphs; the only difference being that the usual graph Laplacian is replaced by the signed Laplacian.

However, there is no theoretical guarantee that the use of the signed Laplacian can recover the true groups in a weakly-balanced signed network. To overcome this theoret-ical defect, we now give an algorithm which, under certain conditions, is able to recover the real structure even with partial observations. The key idea is that since in Theorem 3 we proved that the k -weakly balanced graphs have rank up to k , we can obtain good clustering by first running a matrix completion algorithm, say trace-norm minimization, on A . The following theorem shows that the eigenvectors of the completed matrix possess a desirable property: Theorem 7 Let A ij , ( i, j )  X   X  , be entries observed from a complete k -weakly balanced network X  X  , and assume that the solution of (2) is X with eigenvectors U = [ u 1 , u 2 , , u k ] . If the Algorithm 1 : Clustering with Matrix Completion Input : Adjacency matrix A , number of clusters k
Output : Cluster indicators assumptions in Theorem 6 are all satisfied, then with high p robability U i, : = U j, : iff i and j are in the same cluster in X Proof. From Theorem 6, we know the recovered matrix X will be X  X  with high probability. Suppose u 1 , . . . , u are the k eigenvectors of X  X  . From the proof of Theorem 5, the eigenvectors will have the form in (5), which means U i, : = U j, : if i and j are in the same cluster. Furthermore, when i and j are in different clusters, X  X  i, : 6 = X  X  j, : cannot equal to U j, : . This proves the theorem.
Following this theorem, the true clusters can be identified f rom the eigenvectors of X when the assumptions in The-orem 6 hold. Therefore, perfect clustering is guaranteed in this scenario.

More generally, we can use any matrix completion method discussed in Section 3 to complete A . For example, if we take SVP as the matrix completion approach, we can derive perfect clustering result if all iterates of the algorithm are -incoherent. This is because under this condition, SVP can recover X  X  exactly, so the property of eigenvectors in Theorem 7 can again be used. Our clustering algorithm that uses matrix completion is summarized in Algorithm 1.
It should not be surprising that our clustering algorithm is superior to (signed) spectral clustering. In some sense, our approach can be viewed as a spectral method, except that it first generates missing links from the training data by doing matrix completion. This step is simple yet crucial in signed networks as it overcomes the sparsity of the network. We will see that our clustering algorithm outperforms the (signed) spectral clustering method in Section 5.
In this section, we perform experiments on synthetic and real networks, and show that our proposed low-rank model for signed networks outperforms other methods on the tasks of sign inference and clustering. To ensure that our results are reliable, we conduct all experiments 10 times, and aver-age the result from all of the trials.
Recall that given a partially observed signed graph A , the sign inference task is to predict the signs of the missing links. Although in Section 3 we focused on undirected graphs when introducing our low rank model, we can easily extend our sign inference algorithms to directed graphs since matrix completion and matrix factorization algorithms are easily adapted to the directed case. Therefore, our sign inference experiments are all on networks that are directed.
We consider two low-rank modeling approaches proposed in Section 3: Matrix Completion (MC) and Matrix Factor-ization (MF). For MC methods, we use Singular Value Pro-j ection (MC-SVP) since it is efficient and effective in prac-tice [12]. For MF methods, we mainly consider Alternating Least Square (MF-ALS) which uses the squared loss in (7). In real datasets, we also use Stochastic Gradient Descent (SGD) to solve (7) with sigmoid and square-hinge losses, denoted as MF-SGDSIG and MF-SGDSH respectively.

We compare the performance of our low rank model to state-of-the-art approaches, such as cycle-based methods, for the sign inference problem [17, 5]. The first cycle-based methods are the so-called measure of social imbalance (MOI), which predict the sign of an edge so that more cycles become balanced [5]. If we consider cycles of arbitrary length, and exponentially damp their importance based on length, we get a measure we call (MOI- X  ), which infers the sign of ( i, j ) as sign (( I  X   X A )  X  1  X  I  X   X A ) ij . The second cycle-based ap-proach we consider is a supervised learning approach based on high order cycles (HOC), with features derived from cy-cles of length 3 (i.e. triangles) [17], length 4 and length 5 (see [5] for more details). As in [5], we use HOC-3, HOC-4 and HOC-5 to denote these methods.

Some of these models require parameter setting before-hand, such as the regularization parameter  X  in MF (see (7)) and  X  in MOI- X  . To select these parameters, we con-duct a 3-fold cross validation on the training set. With the selected parameters, we then construct the model based on the whole training set and conduct sign inference on the testing data.
We first compare all categories of approaches on synthetic datasets. We choose MC-SVP, MC-ALS, MOI- X  and HOC-3 as representatives of MC, MF, MOI-based and HOC-based methods respectively. We fix the underlying signed network X  X  to be a complete 5-weakly balanced network, where the five clusters have sizes 100, 200, 300, 400 and 500. Instead of observing all of X  X  , we assume that we only observe a par-tial network by sampling some entries from X  X  using three sampling procedures: uniform sampling, uniform sampling with noise, and sampling with power-law distribution. For each inference algorithm, we input the observed entries as training data and calculate the sign inference accuracy on the rest of elements.
 Uniform sampling: In this scenario, we randomly sample n p entries from X  X  , where p  X  (0 , 1) is the fraction of ob-served entries. We vary p from 0 . 001 to 0 . 1 and plot the in-ference accuracy in Figure 3(a). Clearly, MC-SVP and MF-ALS outperform the cycle-based methods. MOI- X  performs the worst with accuracy only 50%-70%. This is because MOI uses cycle-based measurements to make more cycles become balanced . This inference policy can work only when k = 2 (that is, the underlying network has strong balance), but performs poorly when the underlying network is weakly balanced. HOC-3 works much better than MOI- X  since it learns a classifier from cycle-based features rather than simply making cycles balanced, but its accuracy drops dra-matically when p is less than 0 . 05. On the other hand, both MC-SVP and MF-ALS show high accuracy for all p  X  0 . 01. In particular, MC-SVP can achieve 100% accuracy when p &gt; 0 . 07, which reconfirms the theoretical recovery guar-antee stated in Theorem 6. Moreover, although MF-ALS has no theoretical guarantee, it can still recover the ground truth, an observation that is consistent with previous results. Uniform sampling with noise: To make the synthetic data more similar to real data, we further add noise into ob-servations. Specifically, in this scenario, each observed entry A ij has sign that is opposite to the true value X  X  ij with prob-ability r . For clarity, we fix the fraction of observed entries p = 0 . 1, and increase r from 0 . 01 to 0 . 25. The result is shown in Figure 3(b). We can see that our low-rank modeling ap-proaches are still clearly better than cycle-based methods when noise level becomes higher. Moreover, MC-SVP can still perfectly recover X  X  when the noise level r &lt; 0 . 05, and MF-ALS can also achieve perfect recovery with a smaller r . Sampling with power-law distribution: As Section 3 mentioned, the good performance of matrix completion cru-cially relies on the assumption that observed entries are uni-formly sampled. However, in most real networks (for exam-ple, Slashdot in [15]), the degree distribution follows power law. Therefore, we examine how all approaches perform on power-law distributed networks. We generate power-law distributed networks using the Chung-Lu-Vu (CLV) model proposed in [6], which allows one to generate random graphs with arbitrary expected degree sequence. Similar to the uni-form sampling case, we vary the fraction of observed en-tries and plot the inference accuracy in Figure 3(c). We can see MOI- X  still has poor performance for weakly bal-anced graphs. However, distinct from uniform sampling case, MC-SVP has lower accuracy rate compared to HOC-3 when p &lt; 0 . 1. This shows that MC-SVP cannot work well given non-uniform distributed observations. On the other hand, MF-ALS still performs better than all other methods in power-law distributed graphs.

From all experiments on synthetic data shown in Figure 3, we can conclude that low rank modeling approaches gen-erally do better than cycle-based methods, and the matrix factorization approach (MF-ALS) performs the best in most cases, even in non-uniform distributed networks. This indi-cates MF approach should be superior than others in real networks. This will be confirmed in the following subsection.
Next we demonstrate that our low rank model is more ef-fective than existing methods in real datasets also. Already in Figure 3(c) we have seen that MC-SVP fails to perform well under power-law distributed networks, so we consider the more robust MF approaches, including MF-ALS, MF-SGDSIG and MF-SGDSH, for experiments on real datasets. We compare these proposed methods with the best cycle-based methods, HOC-3, HOC-4 and HOC-5. Again we use Wikipedia, Slashdot and Epinions to examine sign inference algorithms on real networks. These three datasets have pre-viously been used as benchmarks on sign inference [17, 5].
To make the comparison fair, we conduct a 10-fold cross validation and report the average inference accuracy for each dataset in Table 2. We observe that MF-based algorithms clearly outperform cycle-based methods. In particular, we observe that HOC-5 only improves HOC-3 by less than 1.5%, while MF-based algorithms consistently improve the accu-racy of HOC-5 by more than 2% over all datasets. In ad-dition, MF-SGDSIG and MF-SGDSH further improve the accuracy of MF-ALS slightly. This shows that the sigmoid loss and square-hinge are more suitable for sign inference, which supports the discussion in Section 3.2.

In Figure 4, we further examine the performance of these algorithms with different levels of edge embeddedness. Em-beddedness of edge ( i, j ) is defined as the number of common neighbors of the nodes i and j , and can be thought as a mea-observations are sampled from a power-law distribution. Table 2: The sign inference accuracy for MF-based algo-rithms and cycle-based algorithms. We can see that the MF-based algorithms are better than cycle-based algorithms. sure of the proximity between i and j . One might expect that cycle-based approaches should perform better on edges with higher embeddedness because more cycle information is available. However, surprisingly MF-ALS achieves higher inference accuracy regardless of the embeddedness. The per-formance of MF-SGDSIG and MF-SGDSH is similar to MF-ALS so they are not shown in Figure 4 for clarity.
In addition to inference accuracy, we now compare the running time required by the different methods. As dis-cussed in Section 3.2, matrix factorization methods are more efficient than cycle-based algorithms in terms of time com-plexity. Here, we further show that MF-based methods are empirically much faster than cycle-based algorithms. The running times are summarized in Table 3. To conduct tim-ing tests on a large signed network, in addition to the three real datasets as described in Table 1, we further construct a large-scale synthetic dataset called Cluster10 where number of edges is 100 times more than Epinions. Cluster10 is gen-erated from a 10-weakly balanced network, in which clusters have sizes 20000, 40000,. . . , 200000 respectively. There are totally 1.1 million nodes and 120 million edges uniformly observed from the complete graph. We construct this syn-thetic data to show that our matrix factorization approach can easily scale up to massive graphs compared to HOC-3 and HOC-5. For matrix factorization approach, we report the time needed to solve the model by SGD (with sigmoid and square-hinge) and ALS (with square loss). For HOC methods which build classifiers from cycle-based features, since the time for training phase depends on the classifier, we only report the time for computation of features. Thus the reported time for HOC is an underestimation for con-structing the HOC model; even then we can see that the time required by MF-based algorithms is much lower than HOC methods.
 In conclusion, for the sign inference problem, we can see Table 3: Running time (in seconds) for our MF approach and HOC on real datasets and a 1.1 million node synthetic data Cluster10. For HOC methods, we only consider the time for feature computation before the model training, while for MF-based methods we report the total time for constructing the model. We can see that MF-based methods are clearly more efficient than cycle-based algorithms.
 Slashdot 133.4 1936.0 &gt; 1 0,000 17.4 24.7 Epinions 560.64 6156.8 &gt; 1 0,000 28.67 37.2
Cluster10 &gt; 1 0,000 &gt; 10,000 &gt; 10,000 455.1 1152 that our low rank model outperforms other traditional in-ference methods. In particular, the matrix factorization ap-proach is clearly the most competitive method in terms of accuracy and scalability.
In this subsection, we show that our proposed clustering approach, which completes the low-rank structure of signed networks before performing clustering, outperforms spectral clustering based on the signed Laplacian [16]. Similar to Sec-tion 5.1.2, we conduct experiments on synthetic data gener-ated from weakly balanced networks (note that we do not have ground truth for clustering in the real-life datasets). We consider a 10-weakly balanced network X  X  where size of each group is 100. We then observe entries from X  X  with two sampling procedures: uniform sampling and uniform sampling with noise.

To measure the performance of clustering, we calculate the number of edges that satisfy the ground-truth clustering, which is defined by where s 1 , . . . , s n denote the ground-truth clustering assign-given by the clustering algorithm.

Following the procedure outlined in the previous subsec-tion, in the uniform sampling case, we draw pn 2 i.i.d. sam-ples from all the n 2 edges. Similarly, in sampling with noise, we flip the sign of each observed edge with probability r . The results of these two scenarios are shown in Figure 5. In both scenarios, our proposed clustering approach is signifi-cantly better than clustering based on the signed Laplacian. This shows that recovering the low-rank structure of signed networks leads to improved clustering results. Figure 5: Clustering partially observed synthetic data. Fig-ure 5(a) is the result without noise and Figure 5(b) is the result with noise. In both cases, clustering with MC-SVP performs significantly better than using signed Laplacian.
In this paper, we have proposed a low rank modeling ap-proach for signed network analysis. We have shown that the low-rank structure of signed networks naturally emerges from weak balance theory. The sign inference problem in such networks can thus be modeled as a low-rank matrix completion problem. We first showed that missing links in a signed network can be exactly recovered by matrix com-pletion algorithms under certain conditions, and then intro-duced a more efficient matrix factorization approach for sign inference. Furthermore, we showed that the low rank model can also be used for clustering. Experiments conducted on both synthetic data and real networks show that our low rank model improves sign inference significantly, in terms of both accuracy and speed. Clustering results also become more favorable by making use of the low rank model. This research was supported by NSF grants CCF-0916309, CCF-1117055 and DOD Army grant W911NF-10-1-0529. [1] J.-F. Cai, E. J. cand  X es, and Z. Shen. A singular value [2] E. J. Cand  X es and T. Tao. The power of convex [3] E. J. Can  X es and B. Recht. Exact matrix completion [4] D. Cartwright and F. Harary. Structure balance: A [5] K.-Y. Chiang, N. Natarajan, A. Tewari, and I. S. [6] F. Chung, L. Lu, and V. Vu. Spectra of random [7] J. A. Davis. Clustering and structural balance in [8] I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph [9] R. V. Guha, R. Kumar, P. Raghavan, and A. Tomkins. [10] S. Hanneke and E. P. Xing. Network completion and [11] F. Harary. On the notion of balance of a signed graph. [12] P. Jain, R. Meka, and I. Dhillon. Guaranteed rank [13] M. Kim and J. Leskovec. The network completion [14] Y. Koren, R. M. Bell, and C. Volinsky. Matrix [15] J. Kunegis, A. Lommatzsch, and C. Bauckhage. The [16] J. Kunegis, S. Schmidt, A. Lommatzsch, J. Lerner, [17] J. Leskovec, D. Huttenlocher, and J. Kleinberg. [18] A. Y. Ng, M. Jordan, and Y. Weiss. On spectral [19] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed [20] B. Yang, W. Cheung, and J. Liu. Community mining
