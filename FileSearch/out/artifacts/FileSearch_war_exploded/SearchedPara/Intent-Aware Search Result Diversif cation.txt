 Search result diversification has gained momentum as a way to tackle ambiguous queries. An effective approach to this problem is to explicitly model the possible aspects underly-ing a query, in order to maximise the estimated relevance of the retrieved documents with respect to the different as-pects. However, such aspects themselves may represent in-formation needs with rather distinct intents (e.g., informa-tional or navigational). Hence, a diverse ranking could ben-efit from applying intent-aware retrieval models when esti-mating the relevance of documents to different aspects. In this paper, we propose to diversify the results retrieved for a given query, by learning the appropriateness of different re-trieval models for each of the aspects underlying this query. Thorough experiments within the evaluation framework pro-vided by the diversity task of the TREC 2009 and 2010 Web tracks show that the proposed approach can significantly im-prove state-of-the-art diversification approaches. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models Algorithms, Experimentation, Performance Web Search, Relevance, Diversity
User queries often carry some degree of ambiguity [38]. On the one hand, genuinely ambiguous queries (e.g.,  X  zeppelin  X ) have multiple interpretations (e.g.,  X  airship  X ,  X  band  X ). On the other hand, even those queries with a single, clearly de-fined interpretation might still be underspecified, as it is not clear which aspects of this interpretation the user is actually interested in (e.g.,  X  led zeppelin  X ...  X  website  X ?  X  downloads  X ?  X  biography  X ?  X  albums  X ?  X  reunion  X ?) [13].

Search result diversification has recently gained attention as a means to tackle query ambiguity. Instead of trying to identify the  X  X orrect X  interpretation behind a query, the idea is to diversify the search results, in the hope that different users will find at least one of these results to be relevant to their information need [1]. Differently from the traditional assumption of independent document relevance [30], diver-sification approaches typically consider the relevance of a document in light of the other retrieved documents. For in-stance, once users have found the information they are seek-ing, it is questionable whether documents with the same (or very similar) information will still be of any use [13].
An effective approach to diversifying search results is to explicitly account for the various aspects 1 underlying an am-biguous query [1, 7, 33, 36]. By doing so, the problem be-comes to select a ranking of documents that collectively pro-vide the maximum relevance with respect to these different aspects. In a real scenario, however, the actual aspects of a query are not known, nor is the relevance of each retrieved document to each query aspect determined with certainty. Moreover, the relevance of a document to a query aspect may depend on the intent 2 underlying this aspect (e.g., informa-tional or navigational [4, 31]). Additionally, different aspects could feasibly represent information needs with different in-tents. For instance,  X  website  X ,  X  downloads  X , and  X  biography  X  arguably represent navigational, transactional, and informa-tional aspects of the query  X  led zeppelin  X , respectively. Simi-larly,  X  albums  X  could exemplify a typed query aspect, repre-senting an information need for a list of entities. In the same vein,  X  reunion  X  might denote a question-answering aspect of the query  X  led zeppelin  X , regarding whether the legendary rock band have any plans of reuniting in the near future.
Queries with different intents have been shown to benefit from different retrieval models [22]. Likewise, we hypothe-sise that explicit diversification approaches may benefit from accounting for the intents of different query aspects. For instance, relevance estimations with respect to the  X  web-site  X  aspect of the query  X  led zeppelin  X  could be arguably improved by applying a retrieval model suitable for naviga-tional queries. In this paper, we propose a novel diversifi-cation approach, aimed at learning the appropriateness of
Unl ess otherwise noted, through the rest of this paper, we refer to query  X  interpretations  X  and  X  aspects  X  indistinctly.
Agrawal et al. [1] use  X  intents  X  in the sense of what we call  X  interpretations  X . We believe our choice is more appropriate in light of the established nomenclature in the literature. multiple intent-aware retrieval models for each aspect. As a res ult, the relevance of a document to multiple aspects X  X .e., its diversity X  X an be estimated more effectively.

We thoroughly evaluate our approach in the context of the diversity task of the TREC 2009 and 2010 Web tracks [11, 12]. In particular, we investigate learning strategies that ei-ther select the most appropriate retrieval model or merge multiple retrieval models for each query aspect. The results of our investigations attest the effectiveness of both strate-gies within our proposed intent-aware approach for diversi-fying Web search results, with significant improvements on top of state-of-the-art diversification approaches.
The remainder of this paper is organised as follows. Sec-tion 2 describes related work on search result diversification and search intents. Section 3 further details our main con-tributions. Section 4 describes our approach for leveraging intent-aware models for diversification. Sections 5 and 6 detail the experimental setup and the evaluation of our ap-proach. Finally, Section 7 presents our conclusions.
In this section, we provide background on the search result diversification problem and related approaches to this prob-lem. We then set the grounds for our proposed approach by reviewing related work on the use of search intents.
Most of the existing diversification approaches are some-how inspired by the work of Carbonell and Goldstein [5]. The basic idea of their Maximal Marginal Relevance (MMR) method is to iteratively select a document with the highest similarity to the query and lowest similarity to the already selected documents, in order to promote novelty. Subse-quent implementations of this idea include the approach of Zhai et al. [41] to model relevance and novelty within a risk minimisation framework. In particular, they promote doc-uments with highly divergent language models from those of the already selected documents. Chen and Karger [10] proposed a probabilistic approach to the related problem of finding at least one relevant result for a given query, by choosing documents under the assumption that those al-ready chosen are not relevant to the query. More recently, Wang and Zhu [39] proposed to diversify a document rank-ing as a means to reduce the risk of overestimating its rel-evance. In their work, two documents are compared based on the correlation of their relevance scores.

By assuming that similar documents will cover similar as-pects, the aforementioned approaches only consider the as-pects underlying a query implicitly . An alternative approach consists of explicitly modelling these aspects [36]. For in-stance, Agrawal et al. [1] proposed the IA-Select algorithm for search result diversification. It employs a classification taxonomy over queries and documents to iteratively pro-mote documents that share a high number of classes with the query, while demoting those documents with classes al-ready well represented in the ranking. Similarly, Carterette and Chandar [7] proposed a probabilistic approach to max-imise the coverage of the retrieved documents with respect to the aspects of a query, by modelling these aspects as topics identified from the top ranked documents. Recently, Santos et al. [33] introduced the xQuAD probabilistic framework for search result diversification, which explicitly represents different query aspects as  X  X ub-queries X . They defined a di-versification objective based on the estimated relevance of documents to multiple sub-queries, as well as on the relative importance of each sub-query in light of the initial query.
Since our goal is to produce intent-aware relevance esti-mations given an explicit representation of query aspects, our approach is also set in the context of explicit diversifi-cation. Accordingly, in Section 6, we use both IA-Select [1] and xQuAD [33] as a basis for evaluating our approach. In particular, these two approaches represent the state-of-the-art in explicit search result diversification.
Different information retrieval tasks have benefited from taking into account the intent of a query (e.g., informational, navigational, or transactional [4, 31]). These approaches can be generally categorised based on whether or not they rely on the classification of queries into predefined intents.
Query intent detection approaches first classify a query with respect to a predefined set of intents. A retrieval model specifically trained for the predicted intent is then applied to retrieve documents for the query. For instance, Kang and Kim [22] showed that queries of different intents can benefit from the application of intent-specific retrieval models. A major shortcoming of this approach, however, is the limited accuracy of existing intent detection mechanisms [17].
Instead of classifying a query into a predefined target in-tent, an alternative is to identify similar queries from a train-ing set, and then to apply a retrieval model appropriate for this set. This approach has an advantage over a classification of queries based on a fixed set of intents, as queries of the same intent often benefit from different retrieval models [17]. For example, Geng et al. [20] proposed an instance-based learning approach using k -nearest neighbour ( k -NN) classi-fication to improve Web search effectiveness. In their ap-proach, a k -NN classifier is used to identify training queries similar to an unseen query. A retrieval model is then learned based on the identified queries and applied to the unseen query. A more general approach was proposed by Peng et al. [27]. In their work, multiple ranking functions are chosen from a pool of candidate functions, based on their perfor-mance on training queries similar to an unseen query.
Our approach is similar in spirit to the approaches of Kang and Kim [22], Geng et al. [20], and Peng et al. [27]. However, while these approaches focused on inferring the intent of a query , we target the problem of inferring the intent under-lying different aspects of this query. Besides this difference in granularity, our intent-aware approach tackles a different search scenario, namely, search result diversification.
In a similar vein, Santos et al. [34] proposed a selective diversification approach, aimed at tailoring a diversification strategy to the ambiguity level of different queries. In partic-ular, given an unseen query, their approach learns a trade-off between relevance and diversity, based on optimal trade-offs observed for similar training queries. As a result, their ap-proach effectively determines when to diversify the results for an unseen query, and also by how much. Our proposed approach also differs from the approach of Santos et al. [34], in that ours focuses on selecting appropriate retrieval mod-els for different query aspects , as opposed to the query itself. More importantly, their approach is orthogonal to ours. In essence, instead of determining when to diversify the results for a given query, we tackle the problem of how to diversify these results given the identified aspects of this query.
The major contributions of this paper are: 1. A novel aspect intent-aware diversification approach, 2. A thorough evaluation of the proposed approach within
As discussed in Sections 1 and 2, different aspects of an ambiguous query can have rather different intents. To illus-trate this, consider topic #1 from the diversity task of the TREC 2009 Web track [11], as shown in Figure 1. In this example, different aspects of the query  X  obama family tree  X  are represented as a set of sub-topics, identified from the query log of a commercial search engine. Moreover, these sub-topics represent aspects with an informational ( X  X nf X ) or a navigational ( X  X av X ) intent, as judged by TREC assessors. Figure 1: TREC 2009 Web track, topic #1, along with its corresponding sub-topics.

Inspired by related work in Section 2.2, we hypothesise that diversification approaches can benefit from retrieval models targeted to the intents of different query aspects. For instance, for the query exemplified in Figure 1, a di-versification approach could leverage a navigational intent-aware model for the first query aspect, and an informational intent-aware model for the second and third aspects.
In this work, we propose a supervised learning approach for estimating the appropriateness of multiple intent-aware retrieval models for each query aspect. Given a query q , our goal is to maximise the diversity of the retrieved documents with respect to the aspects underlying this query. Without loss of generality, following an explicit diversification strat-egy, we can quantify the diversity of a document d given a query q and the other retrieved documents S as the expected relevance of d with respect to the aspects of q , denoted A ( q ): where P( a | q ) captures the relative importance of each aspect a given the query q , and P( d |S , q, a ) denotes the probability of the document d being relevant to this aspect, given how well the documents in S already satisfy this aspect.
Equation (1) can be seen as a canonical formulation of the objective functions deployed by different explicit diversifica-tion approaches in the literature [34]. In particular, these approaches differ primarily in how they represent the set of aspects associated with a query, and in how they estimate the relevance of each document to every identified query aspect. For instance, Agrawal et al. [1] rely on the top-level categories from the Open Directory Project (ODP) 3 for representing query and document classes, and integrate relevance and classification scores for ranking documents. Santos et al. [33] exploit query reformulations from commer-cial search engines as representations of the different aspects of a query, and directly estimate the relevance of documents to these aspects. Our proposed approach is agnostic to any particular mechanism for generating explicit query aspect representations. Indeed, our only assumption is that differ-ent aspects may convey different user intents.

In the interest of keeping the description of our approach general, in the remainder of this section, we adopt an ab-stract view of aspects and intents. 4 In particular, to for-malise our approach, we further derive Equation (1), by marginalising P( d |S , q, a ) over a target set of intents I : where P( i | a ) denotes the probability that the aspect a of the initial query q conveys the intent i . Accordingly, P( d |S , q, a, i ) denotes the relevance of the document d in light of the other retrieved documents S , the query q , the aspect a , and the intent i . Once again, without loss of generality, assuming that different aspects are equally probable (i.e., P( a | q ) = 1 |A ( q ) | ,  X  a  X  A ( q )) , 5 our task becomes two-fold: 1. To infer the probability P( i | a ) of each intent i  X  I 2. To learn an appropriate retrieval model P( d |S , q, a, i )
In Section 4.1, we propose a classification approach for the first task. For the second task, as described in Section 4.2, we resort to learning-to-rank.
In order to infer the probability of different intents for a query aspect, we propose a linear classification approach. In particular, given a query aspect a , our goal is to estimate the probability of an intent i  X  I as: where x a is a feature vector representing the aspect a , and w is a weight vector, which is learned from labelled training data. The function f maps the dot product of the weight and feature vectors into the desired prediction outcome. Alter-native regimes for instantiating the function f are described in Section 4.1.1. Section 4.1.2 describes our choices for la-belling training data. Lastly, Section 4.1.3 defines the clas-sification space considered in this paper, and describes the query aspect features leveraged for this classification task. htt p://www.dmoz.org
A concrete instantiation of query aspects and their possible intents is discussed in Section 4.1.3.
For alternatives on how to estimate the likelihood of differ-ent query aspects, we refer the reader to [33, 36].
We p ropose two alternative regimes for instantiating the function f in Equation (3): model selection and model merg-ing . The model selection regime employs a hard classifica-tion approach [40]. In particular, this approach treats dif-ferent intents as mutually exclusive, hence assigning each aspect a single (i.e., the most likely) intent. For instance, outcome could be: P( i 1 | a ) = 1 , P( i 2 | a ) = 0 , P( i this example, the aspect a would be associated with its most likely intent, i 1 , and only the retrieval model P( d |S , q, a, i would have an impact on the estimated relevance of docu-ment d to the aspect a . This classification regime resembles the selective retrieval approaches described in Section 2.2, except that the most appropriate model is selected at the aspect level (as opposed to the query level).

Our second regime, model merging, provides a relaxed al-ternative to model selection. In particular, it deploys a soft classification approach, in order to obtain a full probability distribution over the considered intents [40]. For the above example, a possible outcome of the model merging classifi-cation could be P( i 1 | a ) = 0 . 6 , P( i 2 | a ) = 0 . 3 , P( i In this case, the estimated relevance of a document d to the aspect a would be determined by a linear combination:
Different classifiers can be deployed to implement both the model selection and model merging regimes. Further details about the specific classifiers that enable both regimes in our investigations are provided in Section 5.3.
In order to determine the ground-truth intent for different query aspects, we investigate two alternatives. The first one is based on the direct judgement by humans, who base their assessment solely on the observed aspects. However, the differences between query aspects may go beyond their ap-parent characteristics. For instance, aspects with the same judged intent could still benefit from leveraging different re-trieval models [17]. Additionally, judging the intent of dif-ferent aspects may be costly for large training datasets.
To overcome these limitations, we propose a second alter-native for automatically labelling training aspects. Given a training query q with aspects A ( q ) and a set of target intents I , with |A| = k and |I| = p , we devise an oracle selection mechanism. In particular, this oracle mechanism always chooses the best out of the p k possible selections for the k aspects of q , according to a diversity evaluation met-ric (e.g., ERR-IA [9], or any of the metrics described in Section 5.4). Although estimating this oracle may be in-feasible for large values of k , it can be easily estimated for most practical settings. For instance, the maximum number of aspects per query in the TREC 2009 and 2010 Web tracks is k = 8. Moreover, if many more aspects were available for a particular query, less plausible aspects could be discarded without much loss. Indeed, this is precisely what leading Web search engines do when displaying only the top sugges-tions for a user query, which have been shown to deliver an effective diversification performance [33]. Finally, it is worth noting that this entire labelling process is conducted offline.
So far, we have intentionally described our approach with-out a strict definition of the classification space. This ab-stract view of classification instances as query aspects demon-strates the generality of our proposed approach, and its ap-plicability to different explicit diversification approaches in the literature (e.g., [1, 7, 33, 36]). In particular, our pro-posed approach is not bound to any particular query as-pect representation. In fact, any aspect representation that portrays the multitude of information needs underlying an ambiguous query [35] is potentially applicable, as different information needs can convey different user intents.
Nonetheless, to enable our investigations in Section 6, we follow Santos et al. [33, 36] and adopt a concrete represen-tation of query aspects as  X  sub-queries  X . In particular, a sub-query can be seen as a keyword-based representation of the information need expressed by a query aspect. In our experiments, we consider two mechanisms for generat-ing sub-queries, as described in Section 5.1. Additionally, limited by the TREC Web test collection used in our ex-periments [11, 12], we restrict the space of target intents to navigational and informational ones. Based on this repre-sentation of aspects and intents, and inspired by research on related query analysis tasks, we devise a large feature set for sub-query intent classification. In particular, these include features computed from the words in the sub-query itself, as well as from the top documents retrieved for this sub-query. In total, we devise 838 features, based on 21 different fea-ture classes. These features are described on the left side of Table 1, and organised into three groups: Query log features (LOG). Query logs provide valuable evidence for discriminating between informational and navi-gational intents. To exploit such evidence, we compute sev-eral sub-query features based on the 15-million query MSN Search 2006 Query Log. For instance, we count the raw fre-quency of sub-queries, as navigational sub-queries are gen-erally more popular than informational ones. Likewise, in-formational sub-queries intuitively require more effort from the users while inspecting the retrieved results. We quantify this in terms of the number of examined results and the time spent in doing so, as well as the click entropy [15]. Query performance predictors (QPP). The intent of a sub-query may be reflected not only on the sub-query itself, but also on the documents retrieved for this sub-query. For instance, a low coherence of the top-retrieved documents could indicate a sub-query with an informational intent. This, in turn, can reflect on the performance of this sub-query when used in a retrieval system. To exploit this in-tuition, we build upon a large body of research on query performance prediction [6] and leverage both pre-and post-retrieval predictors as sub-query features. In particular, the former are solely based on statistics of the sub-query terms, while the latter also leverage information from the docu-ments retrieved for the sub-query.
 Taxonomy-based features (TAX). Informational needs are intuitively broader than navigational ones, in terms of the concepts they cover. To quantify this intuition, we de-vise different features based on concepts from two different taxonomies derived from Wikipedia: categories and named entities. For the latter, we consider entities of four types: people, organisations, products, and locations. In partic-ular, we represent the documents retrieved for each sub-query in the space of the concepts from these different tax-onomies. 6 Based on this representation, we compute vari-ous distributional features, such as the average number of retrieved concepts, the average distance between pairs of documents, and the concept entropy of the entire retrieved list. Additionally, we also quantify the number of ambiguous entities among the top documents retrieved for a sub-query. Our intuition is that the presence of such entities further indicates the broadness of the sub-query [37].
 Most of these features are extracted in multiple variants. For instance, retrieval-based features are computed based on five different approaches, as implemented by the Terrier IR platform [25]: Okapi BM25, the Divergence From Random-ness (DFR) DPH and PL2 models, a language modelling (LM) approach with Dirichlet smoothing, and a count of the number of matching query terms. Additionally, these features are estimated at six rank cutoffs: 1, 3, 5, 10, 50, and 100. Finally, distributional features (e.g., the number of concepts across the retrieved documents) are summarised using up to four different statistics: mean, standard devia-tion, median, and maximum. Altogether, these amount to the grand total of 838 features.
In Section 4.1.1, we proposed two regimes for inferring an intent distribution P( i | a ) for each aspect a . In this sec-tion, we propose a learning-to-rank approach for producing suitable intent-aware retrieval models for each intent of a .
In order to produce an intent-aware model P( d |S , q, a, i ) for each intent i underlying the aspect a , we once again re-sort to machine learning. In particular, we deploy a large set
Ca t egory features are computed from documents retrieved from Wikipedia for each sub-query, while entity features are based on documents retrieved from the target collection. of document features, and leave it to a learning-to-rank al-gorithm to generate retrieval models optimised for different intents. To achieve this goal, each model is learned using the entire feature set, but with a different training set of queries for each target intent. Given the intents considered in our investigation (i.e., informational and navigational), we use two intent-targeted query sets from the TREC 2009 Mil-lion Query track [8]. The first set contains 70 informational queries and the second set contains 70 navigational queries, as judged by TREC assessors. As a learning algorithm, we use Metzler X  X  Automatic Feature Selection (AFS) [24]. In particular, AFS learns effective ranking models by directly optimising an IR evaluation metric. In our experiments, it is deployed to optimise mean average precision (MAP).
To enable the generation of effective intent-aware retrieval models, we deploy a total of 61 document features, sum-marised on the right portion of Table 1, and organised into six groups: standard weighting models (WM), field-based models (FM), term dependence models (DM), link analysis (LA), spam (SP), and URL features. As these are all stan-dard features traditionally used in the learning-to-rank liter-ature [29], 7 we refer the interested reader to the descriptions and pointers provided in the table. In particular, each fea-ture is computed for a sample of 5000 documents retrieved by the DFR DPH weighting model for each query. Standard weighting models and term dependence models are deployed with their commonly suggested parameter settings in the lit-erature. Field-based models are trained through simulated annealing [23]. The remaining (query-independent) features are optimised using FLOE [18]. Finally, all feature scores are normalised to lie between 0 and 1 for each query.
Table 2 lists the top 10 features selected by AFS for each of our produced intent-aware models. For each feature, we also show its attained performance in terms of MAP when com-
We l eave the investigation of features that exploit the de-pendencies between d and the documents in S for the future. Table 2: Top 10 selected features in the two intent-awar e retrieval models used in this paper. bined with the features selected before it. From the table, we observe that the top selected features are generally intu-itive. For instance, DPH (which is used to generate the ini-tial sample of documents for learning) is the top feature for both models. Likewise, as expected, various URL and link analysis features (e.g., URLWiki, URLLength, Absorbing, InvPageRank, Inlinks) are ranked high in the navigational model. Besides producing intuitive intent-aware models, we believe that our data-driven approach based on a large set of features provides a more robust alternative to hand-picking features traditionally associated with a particular intent.
In this section, we have introduced a novel supervised learning approach for diversifying the search results in light of the intents of different aspects of an ambiguous query. To enable our investigations in Section 6, we have instan-tiated our intent-aware diversification approach in light of two target aspect intents, namely, informational and naviga-tional. Given these intents, we have described large feature sets for both inferring the intent distribution of different as-pects (Section 4.1), as well as for learning the corresponding intent-aware retrieval models (Section 4.2). Although the choice of appropriate feature sets naturally depends on how learning instances (i.e., aspects) and labels (i.e., intents) are represented [40], it is worth reiterating that our approach is agnostic to these representations. While instantiating it for a different aspect representation or a different set of intents may require devising different features, no modification to the approach itself would be necessary. Moreover, although motivated by the learning tasks at hand, both feature sets in Table 1 comprise features deployed for a variety of different purposes in the literature. As a result, we believe they might be useful for deploying our approach with target intents be-yond the two considered in our current investigations.
In the next section, we investigate our intent-aware diver-sification approach proposed in Section 4. In particular, we aim to answer two main research questions: 1. Can we improve diversification performance with our 2. Can we improve diversification performance with our
These questions are investigated in Sections 6.1 and 6.2, respectively. In the remainder of this section, we detail the experimental setup that supports these investigations.
Our analysis is conducted within the standard experi-mentation paradigm provided by the diversity task of the TREC 2009 and 2010 Web tracks [11, 12] X  X enceforth de-noted WT09 and WT10 tasks, respectively. As a document collection, we consider the category-B ClueWeb09 dataset, as used in these tasks. This collection comprises 50 million English documents, aimed to represent the first tier of a commercial search engine index. In our experiments, we in-dex this collection using Terrier [25], after applying Porter X  X  English weak stemmer and without removing stopwords.
The WT09 and WT10 tasks comprise 50 and 48 queries, respectively. As mentioned in Section 4.1.3, for each of these 98 queries, we generate two sets of sub-queries, in order to provide alternative aspect representations for our investi-gations. The first sub-query set is based on the official sub-topics identified by TREC assessors for each of these queries. In particular, each WT09 query has an average of 3.54 informational and 1.32 navigational aspects, as judged by TREC assessors. For the WT10 queries, these numbers become 2.84 and 1.50, respectively. As TREC only provides a natural language description for each sub-topic, we obtain a shorter, keyword-like version using Amazon X  X  Mechanical Turk. This step was necessary to make these sub-topics bet-ter resemble real Web search requests, so as to enable their matching in our query log. Note that this procedure by no means interfere with our conclusions, as these keyword-like sub-topics are uniformly deployed for all tested approaches.
Using the official TREC Web track sub-topics as a sub-query set has two main advantages. Firstly, as discussed in Section 4.1.2, they provide judged intent labels for each sub-query, which can be contrasted to our proposed performance-oriented labelling of training data. Secondly and most im-portant, they provide a controlled environment for evalu-ating the effectiveness of our approach while isolating the impact of any particular aspect representation. In addition to this  X  X round-truth X  sub-query set, we also evaluate our approach using an alternative sub-query set. Following San-tos et al. [33], for each of the 98 queries, we obtain up to 13 query suggestions from a commercial search engine.
In Section 6, we apply our intent-aware model selection and model merging regimes to two diversification approaches: IA-Select [1] and xQuAD [33]. In particular, both approaches instantiate the general explicit diversification objective de-scribed in Equation (1), and hence can directly leverage our intent-aware aspect relevance estimations. Additionally, as discussed in Section 2.1, these approaches are representa-tive of the state-of-the-art in search result diversification. Indeed, a variant of xQuAD was among the top perform-ing approaches in the diversity task of both TREC 2009 and 2010 [11, 12]. In our investigations, both IA-Select and xQuAD diversify the top 1000 documents retrieved by DPH.
In Section 4.1, we introduced two regimes for leveraging the inferred intents of different aspects: model selection and model merging. The model selection regime builds upon a hard classification of intents. To enable this regime, we de-ploy two alternative classifiers. Firstly, we train a support vector machine (SVM) classifier with a polynomial kernel through a sequential minimal optimisation [40]. Our second classifier performs a multinomial logistic regression with a ridge estimator [40]. In both cases, the single most likely in-tent is chosen for each aspect, in a typical selective fashion. In order to enable our second regime, model merging, we fit the output of the SVM classifier to a logistic regression model, hence obtaining a full probability distribution over intents for each aspect underlying the query [40]. In order to cope with the high dimensionality of our sub-query feature set, classification is performed after a dimensionality reduc-tion via principal component analysis [40]. All classification tasks are performed using the Weka suite. 8
We report our results based on the official evaluation met-rics in the diversity task of the TREC 2010 Web track [12]: ERR-IA [9],  X  -nDCG [13], NRBP [14], and MAP-IA [1]. The first three metrics implement a cascade user model, which penalises redundancy by assuming an increasing prob-ability that users will stop inspecting the results as they find their desired information. The fourth metric is based on a simpler model, which rewards a high coverage of query as-pects, without directly penalising redundancy.

Our evaluation ensures a complete separation between training and test settings. In particular, we use the WT09 and WT10 queries interchangeably as training and test, in a cross-year evaluation fashion (i.e., we train on WT09 and test on WT10, and vice versa). This training procedure renders our results on the WT10 queries directly compa-rable to those of participant systems in TREC 2010. For the reported results on the WT09 queries, however, we note that TREC 2009 participant systems naturally did not have access to WT10 queries for training. This training proce-dure is used for the classification approaches described in ht t p://www.cs.waikato.ac.nz/ml/weka/ Section 5.3, as well as for xQuAD X  X  diversification trade-off parameter  X  [33]. As for IA-Select, it is a parameter-free diversification approach, and hence requires no training.
In this section, we evaluate our intent-aware diversifica-tion approach, in order to answer the two research ques-tions stated in Section 5. In particular, Section 6.1 investi-gates the effectiveness of our model selection regime, while Section 6.2 analyses the effectiveness of the model merging regime. Both regimes were described in Section 4.1.1.
Our primary goal in this experiment is to assess the ef-fectiveness of our model selection regime for search result diversification. As described in Section 4.1.1, this regime selects the most likely between an informational and a nav-igational intent-aware retrieval model for each aspect. As a baseline, we consider a simple regime that uniformly de-ploys one of the informational or navigational models for all aspects, regardless of the intents of these aspects. To vali-date our findings, as described in Section 5.2, we test both our model selection regime as well as the baseline uniform regime applied to two diversification approaches: IA-Select and xQuAD. Additionally, these diversification approaches are deployed using two different aspect representations: the official TREC Web track sub-topics and query suggestions from a search engine, as discussed in Section 5.1. Moreover, we test variants of our model selection regime. Each variant is denoted Sel ( C , L ), where C and L denote a classifier and a set of classification training labels, respectively. In par-ticular, C can be one of three classifiers: an oracle ( ora ), which simulates a perfect classification accuracy, and the lo-gistic regression ( log ) and support vector machine ( svm ) classifiers described in Section 5.3. As for the classification labels L , as described in Section 4.1.2, we consider both hu-man judgements ( judg ) as well as the selection with best diversification performance ( perf ) on the training data.
Tables 3 and 4 compare the aforementioned variants of our model selection regime to the baseline uniform regime on the WT09 and WT10 queries, respectively, in terms of the four metrics described in Section 5.4: ERR-IA@20,  X  -nDCG@20, NRBP, and MAP-IA. In parentheses, we show the percent improvement of each variant of our model se-lection regime compared to the uniform regime that uses the informational ( Uni(inf) ) or the navigational ( Uni(nav) ) model, respectively. Significant improvements are measured by the Wilcoxon signed-rank test. In particular, the symbols N ( H ) and  X  (  X  ) denote a significant increase (decrease) at the p &lt; 0 . 01 and p &lt; 0 . 05 levels, respectively. Lastly, the bottom row in Tables 3 and 4 shows the performance of the top performing category-B system in the WT09 and WT10 tasks [11, 12], respectively, hence providing a further refer-ence value for evaluating our intent-aware approach.
From Tables 3 and 4, we first note that the uniform appli-cation of an informational or a navigational model provides a strong baseline performance. The uniform application of the navigational model, in particular, performs at least com-parably to the best performing TREC system in both the WT09 and WT10 tasks. To see whether our model selection regime can improve upon this strong baseline, we first look at the performance of this regime using an oracle classifier. The results show that a massive improvement can be attained by selecting the most appropriate model for each aspect (as op-posed to uniformly using a single model for all aspects) for both IA-Select and xQuAD, when the performance-oriented labels ( perf ) are used for training. On the WT09 queries (Table 3), compared to the strongest uniform setting (i.e., Uni(nav) ) in terms of ERR-IA@20, improvements can be as high as 29.1% for IA-Select and 19.4% for xQuAD using the Web track sub-topics as sub-queries, and are always signifi-cant. When using query suggestions, the potential improve-ments are 38.3% and 30.8%, respectively. On the WT10 queries (Table 4), similar figures are observed: 32.3% and 24.5% gain for IA-Select and xQuAD, respectively, using the Web track sub-topics; 30.1% and 37.8% using query sugges-tions. Once again, all improvements are statistically signif-icant. Human judgements, in contrast, provide a supotimal labelling criterion, as denoted by the lower performance at-tained when using the judg labels. Indeed, even an oracle classifier, which always choses the correct intent according to these judgements (i.e., the Sel(ora,judg) regime), cannot improve over applying the navigational model uniformly. As discussed in Section 4.1.2, this further confirms our intuition that the appropriateness of an intent-aware retrieval model for a given aspect cannot be effectively judged purely on the basis of the apparent characteristics of this aspect.
Besides showing a strong potential for improving diversi-fication performance, as demonstrated using an oracle clas-sifier (i.e., the Sel(ora,perf) regime), our intent-aware ap-proach is also effective in a practical deployment based on standard classifiers. Indeed, Tables 3 and 4 show that our model selection regime using both logistic regression ( log ) and support vector machine ( svm ) classifiers with perf la-bels always improves compared to a uniform regime, of-ten significantly. For instance, on the WT09 queries (Ta-ble 3) and considering the Web track sub-topics as aspect representations, compared to the stronger Uni(nav) base-line, improvements in terms of ERR-IA@20 are as high as 11.1% for IA-Select ( Sel(log,perf) ) and 7.9% for xQuAD ( Sel(log,perf) ). On the WT10 queries (Table 4), improve-ments are as high as 8.8% for IA-Select ( Sel(log,perf) ) and 6.6% for xQuAD ( Sel(svm,perf) ). Similar improve-ments across the other reported metrics are consistently ob-served. When query suggestions are used as aspect rep-resentations, although improvements are less pronounced, they are consistent and can still be significant.
Overall, the results in this section answer our first research question, by showing that diversification performance can be significantly improved by leveraging the most appropri-ate intent-aware retrieval model for each query aspect. Our model selection regime using performance-oriented classifi-cation labels is particularly effective, significantly improv-ing upon a uniform regime comparable to the top perform-ing systems of the TREC 2009 and 2010 Web tracks [11, 12]. Furthermore, the consistency of our observations for two state-of-the-art diversification approaches and accord-ing to multiple evaluation metrics attests the robustness of the model selection regime. In the next section, we contrast this regime against the alternative model merging regime.
After demonstrating the effectiveness of selecting a single model for each query aspect, in this experiment, we investi-gate whether deploying a model merging regime could bring further improvements. For this investigation, we focus our attention to the TREC Web track sub-topics as an aspect representation. As discussed in Section 5.1, this represen-tation allows for assessing the effectiveness of our merging regime across the two proposed training labelling alterna-tives, judg and perf . The results based on query sugges-tions using perf labels lead to identical conclusions and are hence omitted for brevity. In particular, Table 5 shows the diversification performance of IA-Select and xQuAD under the model merging regime ( Mrg ), in contrast to their perfor-mance under the model selection regime ( Sel ), which serves as our baseline in this investigation. Similarly to Tables 3 and 4, percent differences between these two regimes are shown in parentheses, alongside one of the aforementioned symbols to denote the significance (or lack thereof) of such differences. As discussed in Section 5.3, both regimes are based on predictions made by an SVM classifier. In particu-lar, the model merging regime is enabled by fitting the SVM predictions to a logistic regression model.

From Table 5, we observe that the model merging regime can improve upon the model selection regime in most cases, particularly on the WT10 queries (bottom half of Table 5). However, the merging regime can also underperform com-pared to the selection regime, when perf labels are used for IA-Select and xQuAD, on WT09 and WT10, respectively. Nevertheless, significant differences are only observed when IA-Select is deployed under the Mrg(svm,judg) regime on the WT10 queries. These results answer our second research question, by showing that merging multiple intent-aware models can be at least as effective as selecting the single most likely model. Moreover, we believe that the merging regime can offer additional benefits for an intent-aware diversifica-tion. For one, it can help attenuate the harm of selecting the wrong model for a particular sub-query. Additionally, it provides a natural upper-bound for the selection regime. In-deed, model selection is a special instance of model merging, with a mutually exclusive probability distribution.
In this paper, we have introduced a novel intent-aware approach for search result diversification. Given the possi-ble intents underlying the aspects of a query, our approach learns the appropriateness of retrieval models targeted to each of these intents. These models are then leveraged selec-tively or combined in a merging fashion in order to refine the estimation of the relevance of the retrieved documents with respect to each query aspect. In particular, our approach builds upon a general explicit diversification model, which makes it seamlessly deployable by existing approaches in the literature. Indeed, thorough experiments in the context of the TREC 2009 and 2010 Web tracks demonstrate that our approach is general and significantly improves the effective-ness of two state-of-the-art diversification approaches.
Our data-driven approach for learning both intent-aware retrieval models and their appropriateness for a given aspect opens up promising directions. In particular, the full poten-tial of our approach could be further exploited by building upon a larger pool of intent-aware retrieval models (e.g., with, say, a transactional model [4]), as well as features cap-turing dependencies between the retrieved documents. [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [2] G. Amati, C. Carpineto, G. Romano, and F. U.
 [3] L. Becchetti, C. Castillo, D. Donato, S. Leonardi, and [4] A. Broder. A taxonomy of Web search. SIGIR Forum , [5] J. Carbonell and J. Goldstein. The use of MMR, [6] D. Carmel and E. Yom-Tov. Estimating the query [7] B. Carterette and P. Chandar. Probabilistic models of [8] B. Carterette, V. Pavluz, H. Fangx, and E. Kanoulas. [9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [10] H. Chen and D. R. Karger. Less is more: Probabilistic [11] C. L. A. Clarke, N. Craswell, and I. Soboroff. [12] C. L. A. Clarke, N. Craswell, I. Soboroff, and G. V. [13] C. L. A. Clarke, M. Kolla, G. V. Cormack, [14] C. L. A. Clarke, M. Kolla, and O. Vechtomova. An [15] P. Clough, M. Sanderson, M. Abouammoh, [16] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. [17] N. Craswell and D. Hawking. Overview of the TREC [18] N. Craswell, S. Robertson, H. Zaragoza, and [19] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [20] X. Geng, T.-Y. Liu, T. Qin, A. Arnold, H. Li, and [21] B. He and I. Ounis. Query performance prediction. [22] I.-H. Kang and G. Kim. Query type classification for [23] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. [24] D. A. Metzler. Automatic feature selection in the [25] I. Ounis, G. Amati, V. Plachouras, B. He, [26] L. Page, S. Brin, R. Motwani, and T. Winograd. The [27] J. Peng, C. Macdonald, and I. Ounis. Learning to [28] V. Plachouras, I. Ounis, and G. Amati. The static [29] T. Qin, T.-Y. Liu, J. Xu, and H. Li. LETOR: A [30] S. E. Robertson. The probability ranking principle in [31] D. E. Rose and D. Levinson. Understanding user goals [32] M. Sanderson. Ambiguous queries: Test collections [33] R. L. T. Santos, C. Macdonald, and I. Ounis. [34] R. L. T. Santos, C. Macdonald, and I. Ounis. [35] R. L. T. Santos and I. Ounis. Diversifying for multiple [36] R. L. T. Santos, J. Peng, C. Macdonald, and I. Ounis. [37] R. Song, Z. Luo, J.-Y. Nie, Y. Yu, and H.-W. Hon. [38] K. Sp  X  arck-Jones, S. E. Robertson, and M. Sanderson. [39] J. Wang and J. Zhu. Portfolio theory of information [40] I. H. Witten and E. Frank. Data Mining: Practical [41] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond [42] Y. Zhou and W. B. Croft. Query performance
