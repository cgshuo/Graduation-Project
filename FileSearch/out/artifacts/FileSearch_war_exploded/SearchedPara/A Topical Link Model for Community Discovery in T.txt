 This paper is concerned with community discovery in tex-tual interaction graph, where the links between entities are indicated by textual documents. Specifically, we propose a Topical Link Model(TLM), which leverages Hierarchical Dirichlet Process(HDP) to introduce hidden topical variable of the links. Other than the use of links, TLM can look into the documents on the links in detail to recover sound com-munities. Moreover, TLM is a nonparametric model, which is able to learn the number of communities from the data. Extensive experiments on two real world corpora show TLM outperforms two state-of-the-art baseline models, which ver-ify the effectiveness of TLM in determining the proper num-ber of communities and generating sound communities. H.3.3 [ Information Search and Retrieval ]: Clustering; H.2.8 [ Database Applications ]: Data mining Algorithms, Experimentation Community discovery, Nonparametric statistical model, Top-ical link model
Community discovery is one of the important research top-ics in multiple disciplines. Traditionally, it is performed on an entity link graph in which the vertices represent the en-tities and the edges indicate links between pairs of entities. Several methods have been proposed to discover communi-ties in previous work. Most approaches, including graph cut based methods[6], modularity based methods[4], flow based methods[1] and spectral based methods[5], typically choose an objective function which captures the above intuition of a community and then try to optimize the objective func-tion[2]. In [3], Mei et al. propose NetPLSA for discovering smoothing topics over network. These methods only model the links by assigning a certain weight to each link between pairs of entities, as the co-authorship network shown in Fig-ure 1(a). Figure 1: Difference between interaction graph and textual interaction graph
In particular, this paper explores community discovery in textual interaction graph . In our setting, the links between entities are indicated by text documents. We refer to such kind of data as textual interaction graph . Figure 1(b) gives a sample of a research proceeding corpus. In this paper, we propose a Topical Link Model(TLM). Given the textual interaction graph, TLM leverages Hierarchical Dirichlet Pro-cess(HDP) to introduce hidden topic variables of the links. Moreover, TLM can look into the documents on the links in detail to recover sound communities. We first generate the hidden topic variables of the links and then documents are generated by these according to the hidden variables. Be-sides, TLM is a nonparametric model. Experimental results on two real world corpora show the effectiveness of TLM in determining the proper number of communities and gener-ating sound communities.
De nition 1. (Textual Interaction Graph) : A textual interaction graph is viewed as G 0 = ( V 0 , E 0 ) with associated documents D , where V 0 represents the set of users (vertices), E 0 represents the set of interactions (edges) between users and D represents the set of documents that record those interactions. If vertex v i and v j have interacted once which is recorded by document d k 2D , there is an edge e v i ;v to indicate this interaction , and the document representing this interaction is d k . Figure 2(a) shows one example of interaction graph.

De nition 2. (Topical Community) : A topical com-munity is a soft partition of the users with a multinomial Figure 2: An interaction graph and its correspond-ing participation graph word distribution over the vocabulary V . We denote the topical community space as  X  1 .

De nition 3. Task (Community Discovery) : Given an participation graph G with interaction corpus D , the task of Community Discovery is to find a set of topical com-munities f c 1 , c 2 , ..., c K g , where the community number K is detected automatically and to calculate the community distribution of each user u , i.e. u .
In this paper, in order to consider the content of the inter-actions, we transform G 0 to an equivalent form G = ( V, E ) with corpus D where V = V 0 , D is the set of interaction documents generated among the interactions of the users and each e 2 E represents a user X  X  participation in a doc-ument. If vertex v i and v j have interacted once and the corresponding interaction document is d , there is an edge e ;d connecting v i and d indicating that v i participates in the interaction represented by d , and the same with v j . Fig-ure 2(b) shows the equivalent form of Figure 2(a). Figure 3: Separated graphical model of TLM. (a) models the participation graph, (b) models the doc-uments.

Participation Graph Modeling. Figure 3(a) shows the graphical model of our Participation Graph Modeling, where M is the number of users and j D ( i ) j is the number of documents linked to user i . The outside global Dirichlet Process(DP) provides a shared infinite number of variables Figure 4: Combined graphical model of TLM for the sample graph in Figure 2 (b) for all the users X  evidence variables. Formally, the Dirichlet process is
The inside local DP models all participations of a single user in her documents. This models the clustering property of a user X  X  participation in documents. Formally,
Then, every user X  X  evidence variables are drawn from the infinite space  X  1 as follows: where  X  i;j h denotes the h th evidence variable of U i , and  X  is the atom function. This actually forms a two-layer HDP.
Document Modeling. Figure 3(b) shows the graph-ical model of our Document Modeling, where j d j j is the number of words in d j and j U ( j ) j denotes the number of topic variables connected to  X  j . On the top,  X  s are the topic variables of all the authors of the document, which are draws from the infinite space  X  1 . Then,  X  is the weighting vector parameters for the topic selection process, satisfying  X  h =1  X  j;h = 1. The relation of these variables is as where  X  ( h ) ;j denotes the h th evidence variable that connects  X  . Then, we assume that all the words in that document are drawn from the topic model, as the lower part of the Figure 3(b). The joint probability of a document model  X  j and its generated words W j is: where  X  j is a draw from an infinite semantic space  X  1 . Combination of Participation Graph Modeling and Document Modeling. Considering the community space  X  1 and the document topic space  X  1 are both semantic spaces, we unite them into a single space,  X  1 . See Fig-ure 4 for the complete model. Figure 4 just gives a sample according to the participation graph in Figure 2(b).
We employ Gibbs sampling for model inference.Firstly, we derive two likelihood expressions. The conditional density of W j under mixture component k given all other observed documents is: where M ul () denotes multinomial distribution and the se-lection probability distribution is s ( d j j k t  X  ;j ) = We have set all  X   X  X  to be uniform so that these parameters can be omitted. There are three sets of hidden variables to be sampled: table indices t of customers, mixture compo-nent indices d of documents, and dish indices k of tables.
Sampling t . The variable set t should be split to two sets because they are different in sampling. Firstly, if the document j is a single user interaction document, t i;j is sam-pled by combing the likelihood of generating the observed documents. where p ( W j j t i;j , t i;j = t new , k ) is the liklihood for t t
If the sampled value of t i;j is t new , we need to obtain a
Secondly, for t of multi-user interaction documents, we have a similar sampling process but the likelihood function is re-placed by the selection function. where p ( d j j t i;j , t i;j = t new , k ) is:
And in the case of choosing t new :
Sampling d . These variables relate only to the selection processes and the multi-user document likelihood. They are thus sampled as
Sampling k . Since changing k i;t will change the mixture components of all the t i; , the sampling relates to both the selection likelihood and the document likelihood.
After model training, the mixture components can be es-timated as
And the community distribution  X  i of a user i is estimated from the community assignment variables d j , j 2 D m ( i ) and t i;j , j 2 D s ( i ) as
We mainly use the research proceeding corpus to evaluate the performance of TLM. The dataset contains the abstracts from 7 research conferences, i.e. ACL, ICML, SIGGRAPH, SIGIR, SIGKDD, SIGMOD, and WWW, from 2005 to 2009. We call this corpus PAPER. We also collect a set of com-panies 1 and their news articles from New York Times. The dataset consists of all the articles that mention about at least 3 companies. And hereafter we refer to it as NYT. Table 2 shows some statistics of PAPER and NYT.
We use the Categorical Clustering Distance(CCD)[7] to compare the similarity between the computed community http://topics.nytimes.com/topics/news/business/ companies/index.html Table 4: Evaluation result of NetPLSA on PAPER distribution and the ideal community distribution. For PA-PER, we treat each conference as a community and the pro-portion of the number of papers one author published in each conference as the ideal probability the author belongs to that community. On PAPER, The CCD of NCut is 1351.02, and we list the evaluation results of NetPLSA regarding to its parameter  X  in Table 4.

From Table 4, the best CCD value of NetPLSA is 1408.9, which outperforms NCut, so from now on we choose the best evaluation result of NetPLSA as comparison baseline for TLM. In our experiments, we also try different sets of parameters of TLM with  X  l varying from 0.1 to 0.5,  X  g from 1.0 to 5.0 and  X  from 0.01 to 0.20. We show part of the detailed evaluation results of TLM with respect to  X  l ,  X   X  in Figure 5. With  X  l = 0 . 2,  X  g = 5 . 0 and  X  = 0 . 05, we get the minimum value of CCD = 1219 . 65 giving a maximum
With  X  l = 0 . 2,  X  g = 5 . 0 and  X  = 0 . 05, TLM detects 8 communities and we make statistics of papers from each con-ference as shown in Figure 6. Note here that the community membership of a paper is set to be community assignment in the last iteration of Gibbs sampling. Figure 6: Paper number percentage distribution
From Figure 6, we see that our model discovered 6 ma-jor communities and 2 minor communities. We examine the 6 major communities first. Considering both Figure 6 and Table 5, it is easy to see that c 1 well corresponds to infor-mation retrieval, c 2 is closely related to computer graphics, c is mainly about data mining, c 4 covers the database com-munity, c 5 mainly concerns about computer linguistics, c closely related to machine learning. Note that papers from WWW scatter around several communities, which is quite reasonable because the WWW conference covers multiple topics. As to the 2 minor communities, c 7 and c 8 both con-tain only 1 paper. After an investigation of the data, we find that the authors of the above 2 papers have no co-authorship with the rest authors and that the contents of these papers are very dissimilar from others.
Although community discovery techniques have been de-veloped for decades, there is no much work done in devel-oping general algorithms for textual interaction graph. This paper proposes a principle solution. In the future, we will try some other document modeling e.g. LDA, and apply TLM to large-scale datasets.
We would like to thank the three anonymous reviewers for their elaborate and helpful comments.
