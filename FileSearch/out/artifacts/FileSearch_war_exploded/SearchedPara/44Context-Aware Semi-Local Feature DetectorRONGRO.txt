 PENGFEI XU, XIAOSHUAI SUN, and XIANMING LIU, Harbin Institute of Technology Recent years have witnessed great advances in local feature representation for many computer vision tasks, such as object categorization, location recognition, image retrieval, video analysis, an d scene reconstruction. In general, state-of-the-art local feature extraction follows a typical detector-descriptor procedure: the detector phase aims to discover and locate salient regions (or interest points), for example, corners and junctions, which are local areas that contain strong signal changes in more than one dimension. To provide repeatable a nd discriminative detection, many local feature detectors have been proposed over the past decade, for instance Harris [Harris and Stephens 1988], Harris-Affine [Mikolajczyk and Schmid 2001], Hessian-Affine [Mikolajczyk and Schmid 2004], MSER [Matas et al. 2004], and DoG [Lowe 1999]. The descriptor phase aims to robustly characterize the salient regions to offer invariance to different scales, rotations, and affine image transformations. There are also many representative descriptors in the literature, such as SIFT [Lowe 2004], PCA-SIFT [Ke and Sukthankar 2004], GLOH [Mikolajczyk and Schmid 2005], Shape Context [Belongie et al. 2002], RIFT [Lazebnik and Ponce 2005], MOP [Brown et al. 2005], and the learning-based MSR descriptors [Hua et al. 2007; Simon and Winder 2007].
Until recently, the detector phase of each local feature has been treated in iso-lation from each other [Harris and Stephens 1988; Lowe 1999; Matas et al. 2004; Mikolajczyk and Schmid 2001, 2004]. Each salient region is detected and located in a separative manner, paralleling to the other salient regions. On the contrary, in many computer vision tasks such as object recognition, the extracted local features are proceeded as a whole in the subsequent classification procedure. In such cases, there are more emphases on discovering meaningful and discriminative local features rather than repeatable detection. Research on human visual systems [Hubel 1995] has also revealed that contextual statistics of simple cell responses in the V1 cortex (which can be simulated by local filters such as Gabor [Gilbert and Wiesel 1983]) are integrated into complex cells in V2 [Gazzaniga et al. 2002] to produce semi-local stimuli in visual representation. Consequently, a natural question is  X  X an we integrate the context of correlative local features to detect more meaningful and discriminative features? X 
In this article, we explore the context issue of local feature detectors, which refers to both spatial and inter-image concurrence statistics of local features. Former works in context-based visual representation and recognition can be categorized into two groups depicted in Figure 1 (blue lines), all of which handled spatial contexts. The first group integrates spatial contexts in order to build local descriptors [Belongie et al. 2002; Bileschi and Wolf 2007; Bruckstein et al. 1997; Lazebnik et al. 2004; Lin et al. 2006; Paletta et al. 2005]. For instance, the Shape Context proposed by Belongie et al. [2002] used spatially nearby shape primitives to codescribe local shape features by radial and directive bin division. Lazebnik et al. [2004] presented a semi-local affine part description, which adopted geometric construction and supervised grouping of local affine parts to model 3D objects. Bileschi and Wolf [2007] introduced a contextual description with continuity, symmetry, closure, and repetition to low-level descriptors with a C1 feature as its basic detector part. However, this Gestalt descriptor [Bileschi and Wolf 2007] operates on the global image statistics without the flexibility of tuning different detection and des cription scales. The second group incorporates spatial contexts to refine the recognition classifiers. The most representative work comes from that of Torralba et al. [Oliva and Toralba 2007; Torralba 2003; Torralba and Oliva 2006; Torralba et al. 2005] in context-aware recognition aiming to integrate spatial cooccurrence statistics into recognition classifier outputs to codetermine object labels. In addition to these two groups, there are also recent works on context-aware similarity matching [Jegou et al. 2007, 2009], which adopted similarities from spa-tial neighbors to filter out contextually dissimilar patches. There is also a recent work on Ni and Yan [2009] that exploits the contextual concurrence cues into a histogram-like structure, which shows the contextual statistics of the local descriptors per image. However, this serves as a global-level contextual representation which is not suitable for our scenario, as we aim to provide a generalized local (semi-local) feature detector that can be easily integrated into the current bag-of-features representations.

Biology inspired approaches have also been proposed to discover meaningful and discriminative local features. Human eye tracking experiments [Hubel 1995] have demonstrated that the human visual systems can rapidly focus on foreground objects by discriminating global contrasts from local stimuli. For simulation, Walther and Koch [2006] introduced an attention-based prefiltering before local feature detection to filter out noninformative regions with limited saliency [Itti et al. 1998]. Serre et al. [2006] presented a biology inspired model to mirror the mechanisms of V1 and V2 cor-tices, in which a S1-C1-S2-C2 framework is pr oposed for extracting features. However, its global representation [Serre et al. 2006] restricts its usage in patch-based applica-tions. Also, its S2-C2 learning part is ind ispensable, and hence cannot be effectively extended to different scenarios.

To the best of our knowledge, contextual cues have not yet been introduced into the interest point detection phase: only local information is used to guide the interest point detection, without regard to the contextual statistics from neighborhood points. Consequently, it is hard to detect the real interest points at a higher scale. Further-more, the state-of-the-art detector-descriptor procedure still lacks recurrent feedbacks from the statistics of local descriptors in order to guide the operations of local detectors.

In this article, we present a context-aware semi-local (CASL) feature detector framework which employs extreme focus on exploring local feature context in detector construction. Going beyond state-of-the-art works, we provide contextual statistics feedback of interest points to recurrently optimize the detection and location of inter-est points. This optimization boosts our interest point detector from the traditionally local scale to a semi-local scale, which enables the detection of more meaningful and discriminative features. In particular, our CASL feature detector consists of two phases.  X  Our first phase models the local detector correlations with Gaussian smoothing into a contextual Gaussian estimation at each scale then aggregates their differences between different scales into a difference of contextual Gaussians (DoCG) field. We show that the DoCG field can highlight contextually salient regions, as well as discriminate foreground regions from background clutter to reveal visual attention [Hou and Zhang 2007; Itti et al. 1998]. We have further uncovered the correlations between the DoCG field and saliency map analysis [Hou and Zhang 2007; Itti et al. 1998], as well as human eye tracking results in Section 3.1.  X  Our second phase discovers semi-local salient regions over the DoCG field, which is achieved by locating contextual peaks using Comaniciu and Meer [2002] Mean Shift search. Each located peak is described with a context-aware semi-local descriptor (CASE), which meanwhile ensures the invariance to scales, rotations, and affine transformations.

Our second contribution is to enable learning-based interest point detection. Dif-ferent from previous works in visual pattern mining [Jamieson et al. 2006; Quack et al. 2007; Yuan et al. 2007] or learning-based descriptors [Hua et al. 2007; Simon and Winder 2007], we introduce the first step of learning-based detector Figure 1 red lines. We achieve this goal by integrating category learning into our mean shift search kernels and weights in order to discover ca tegory-aware discriminative features.
The rest of this article is organized as follows. Section 2 presents the building pro-cedure of our CASL detector. We discuss the mechanism of our CASL detector with bi-ological evidence in Section 3.1 and introduce the learning-based CASL in Section 3.2. Section 4 gives three groups of experimental evaluations, including near-duplicated image search (UKBench dataset), object recognition (Caltech 6 and Caltech 101) and detector repeatability evaluation (INRIA detector evaluation sequences [Mikolajczyk et al. 2006]), with comparisons to state-of-the-art local feature detectors (DoG [Lowe 1999], MSER [Matas et al. 2004], Harris-Affine [Mikolajczyk and Schmid 2001], and Hessian-Affine [Mikolajczyk and Schmid 2004]), descriptors (SIFT [Lowe 2004], Shape Context [Belongie et al. 2002]), saliency preprocessing (Saliency Map [Itti et al. 1998; Walther and Koch 2006]), and biology-inspired (global) contexts [Serre et al. 2006]. This section presents the two-phase procedure to build our CASL detector, in which Section 2.1 introduces the first phase in constructing the local detector context 1 and Section 2.2 introduces the second phase in detecting semi-local contextually salient regions. This phase constructs the local detector context based on their spatial cooccurrence statistics at multiple scales. In principle, the initial detector can be any of the current approaches [Mikolajczyk and Schmid 2004]. Here, we adopt difference of Gaussians [Lowe 2004] as an exemplar detector. However, we should note that this phase does not depend on a specific detector to achieve the principle of our contextual Gaussian con-struction (MSER [Matas et al. 2004] is also evaluated in our subsequent experiments.). Indeed, arbitrarily shaped and sampled patches could be used as well.

Local Feature Extraction. First, for a given image, a scale space L at scale  X  is defined by applying a Gaussian convolution kernel to the intensity component in its HSI color is a Gaussian kernel applied to ( x , y ) with scale parameter  X  .
 Second, the difference of Gaussians at the k th scale is calculated by subtracting the Gaussian convolutions between the k th scale k  X  and the original scale  X  (which produce scale spaces L ( x , y , k  X  )and L ( x , y , X  ), respectively) such that Then, local feature candidates are points that are local maxima within their space and scale neighbors. (For a given ( x , y ) location, there are (3  X  9  X  1) neighbors in total.). For each candidate, we construct a Hessian matrix to investigate the ratio between its trace square and determinant. This rat io removes candidates coming from edge points with only signal changes in one dimension. We use the SIFT [Lowe 2004] de-scriptor to describe each remaining candidate as a 128-dimension local feature vector S . Similarly, other state-of-the-art descriptors [Mikolajczyk and Schmid 2005] can be also adopted.

Context Construction. Second, the distribution of loca l feature context is quantita-tively measured using a proposed difference of contextual Gaussians (DoCG) field. Our main idea is to estimate the contextual intensity of local features (contex-tual gaussian) based on their spatial distributions with another Gaussian kernel smoothing at different scales. The differences between contextual Gaussians at dif-ferent scales are then aggregated to produce the DoCG field to quantize local feature context.

Formalizing the preceding principle, for a given local feature at location ( x , y ), its contextual Gaussian strength is estimated u sing its neighborhood local feature distri-butions with Gaussian smoothing at contextual scale  X  context : where S CL ( x , y , X  context ) is the estimated contextual Gaussian density at location ( x , y ) with contextual scale  X  context ; i =1to n represents the n local features falling within  X  descriptor.

The difference of contextual Gaussians ( S DoCG )atthe k th scale ( k  X  context )isthen the  X  context contextual scales.
 Unlike the DoG operation [Lowe 1999] that searches for the neighborhood local peaks within consecutive scales, we additively accumulate the differences of contextual Gaussians between different scales (there are in total ( K  X  1) accumulations for K scales) to produce a final DoCG field, denoted as S DoCG ( x , y ) such that Similar to DoG, this accumulated strength S DoCG ( x , y ) among all scales can indicate the robustness of location ( x , y ) to the variances caused by scale variances. This phase detects the semi-local salient regions over local detector context, which is achieved by mean shift search [Comaniciu and Meer 2002] over the DoCG field to discover semi-local contextual peaks. The mean shift vector for each location ( x , y )is calculated as In Equation (6), w ( S i DoCG ) 0 is the weight given to the i th local feature,; i =1to n denotes the n local features that fall within G H () of ( x , y ); and G H () is a Gaussian ker-nel that differentiates contributions of different local features based on their distances from ( x , y ). We denote G H ( S DoCG  X  S DoCG )as H is a positive definite d  X  d bandwidth matrix. We simplify H as a diagonal matrix ( diag [ h 2 1 , ..., h 2 d ]).
 Equation (6) can be further converted into We denote the first term in the second line of Equation (8) as m h ( S DoCG ( x , y )), which represents the gradient of the DoCG field at which the point ( x , y ) is conducted. Sub-sequently, the mean shift search for contextual peaks over the DoCG field is conducted according to the following.  X  Step 1. Compute m h ( S DoCG ( x , y )) to offer certain robustness in order to ensure fea-ture locating precision.  X  Step 2. Assign the nearest SIFT point as a new ( x , y ) position to m h ( S DoCG ( x , y )for the next-round iteration.
  X  X tep3. If || m h ( S DoCG ( x , y ))  X  S DoCG ||  X   X  , stop its mean shift operation; otherwise, repeat Step 1.

The final convergence is achieved at local maximal or minimal locations in the DoCG field, which are defined as the detected locations of our CASL detector.

Figure 2 further shows the effect of tuning both contextual and mean shift scales to achieve local-to-global context detections. (In each contextual scale, we settle the mean shift scale by adding 30 additional pixels.) Note that tuning larger contextual and mean shift scales (e.g., to include all SIFT features into contextual representation) would result in focusing attention on viewing images. On the contrary, small contex-tual and mean shift scales degenerate CASL into a traditional local feature detector (e.g., SIFT [Lowe 2004]).

The computational cost of CASL detection is given as follows. Suppose there are n extracted local features for a given image; the original nearest neighbor estimations for this image are conducted in a brute force manner with o ( n 2 ) time cost. These near-est neighbor search results are further stored into a table (to be used in the subsequent mean shift search). Also, the mean shift vector at each point is estimated and stored with o ( nm )timecost( m is the number of included neighbors). In the mean shift locat-ing procedure, the computation cost is o ( n  X  k ), in which k is the maximum number of iterations. Thus, the overall computation cost is o ( n 2 + nm + nk ).

We intuitively use DoCG to simulate the con struction procedure of DoG (difference of Gaussian), which models well the local distribution statistics and is scale invariant in local feature detection. Our simulation (difference of contextual Gaussian) can also handle scale changes in describing contextual statistics well. Meanwhile, the Gaussian smoothing operation can address well the outliers in modeling the local feature context distributions.

In our DoCG construction, similar to that o f the DoG construction, we incorporate two operations: (1) the Gaussian smoothing operation, which can suppress the outliers of inappropriate local feature context and (2) the Gaussian difference accumulation operation, which can unify the influences of different contextual scales. Subsequently, we present a novel context-aware semi-local descriptor (CASE) to de-scribe these semi-local peaks. Similar to the Gradient Location-Orientation Histogram [Mikolajczyk and Schmid 2005], we provide a polar grid division for context-aware description.

A visualized example of our CASE descriptor is shown in Figure 3. For each semi-local interest point, we include the local features in its  X  Context neighbor into its descrip-tion. We subdivide this  X  Context neighbor into log-polar grids with three bins in radial directions (radius: 6, 11, and 15 pixels) and six in angular directions (angle: 0  X  ,60  X  , 120  X  , 180  X  , 240  X  , and 300  X  ). It results in 17-bin division. The central bin is not divided in angular directions in order to offer certain robustness against feature-locating pre-cisions, which differs from GLOH [Mikolajczyk and Schmid 2005]. In each grid, the gradient orientations of SIFT points are averaged and quantized in eight bins, produc-ing a (3  X  6  X  8) 144-bin histogram for each CASE descriptor. Different from the traditional local salient region detectors [Harris and Stephens 1988; Lowe 1999; Matas et al. 2004; Mikolajczyk and Schmid 2001, 2004], our semi-local detector can discover salient regions based on the DoCG filed construction, which is one of our key advantages, compared with st ate-of-the-art works. Figure 4 shows some exemplar comparisons in the Fixdens dataset [Bruce and Tsotsos 2006] of (1) human eye tracking results [Bruce and Tsotsos 2006], (2) saliency map [Itti et al. 1998], and (3) our DoCG field, in which the normalized gray-scale value denotes the focus of attention estimated by (1) tracking eye movements of observers (for the human eye tracking column), (2) the saliency strength (for the saliency map column), and (3) the contextual strength (for the DoCG field column). In most cases, the locations with high responses in the DoCG field correspond to high saliency results in this region, which can reveal, in some cases, the focus of attention when viewing this image. Figure 5 shows more exemplar results for psychological pattern images and man-made paintings. While it is common sense that the saliency maps [Hou and Zhang 2007; Itti et al. 1998] highlight image regions with high visual attention, our DoCG construction X  X n extension from local interest points to semi-local interest points X  X stonishingly can also reveal such phenomenon.

We attribute this phenomenon to the construction process of the DoCG field: for a given location, our semi-local contextual measurement is derived by accumulating differences of cooccurrence strengths among K contextual scales. Similarly, the prin-ciple of the spatial saliency map [Itti et al. 1998] adopts a center-surround difference operation to discover highly contrasting points at different scales with Gaussian smoothing which are then fused together to evaluate point saliency.

From another viewpoint, the DoCG field also shares certain similarities with the spectral saliency map [Hou and Zhang 2007] built by the Fourier transform, which retains the global spectrum phase to evaluate pixel saliency. Based on spatial convo-lution and difference accumulation, our DoCG field also preserves global character in general, for which the high-frequency fluctuations are discarded by spatial Gaussian convolutions over local feature densities, and the influences of frequency variances are further weakened by the difference operations of contextual strengths.

Our Differences. However, such similarities cannot result in simple replacement of the DoCG field with the saliency map in our CASL detector framework. On one hand, both spatial saliency and spectral saliency are designed to reveal pixel-level  X  X ense X  responses, while our DoCG field aims to evaluate semi-local features as  X  X parse X  re-sponses with a different calculation mechanism. On the other hand we emphasis the contextual-level detector discriminability (rather than the direct saliency map analy-sis), which can better suit its subsequent mean shift search to locate contextual peaks.
Similar to many center-surround saliency detection operations [Itti et al. 1998], center-surround operation is the basic and fundamental form of context in this arti-cle. However, we highlight two additional advantages. First, our context-aware detec-tor has a two-stage operation: (1) the initial context measurement is judged based on blobs of similar SIFT descriptors in the center, and they are different to those in the periphery; and (2) the eventual locating of salient points goes into a higher semi-local level, in which we adopt mean shift to find points that are sensitive within given con-textual and mean shift scales. As shown in Figure 2, this procedure is flexible for dif-ferent levels of context definitions (such as finding globally salient points and finding semi-local salient points). Second, our mean shift search enables further refinement of context measurements based on introducing TF/IDF statistics (from multiple images) to weight the contribution of initial contextual measurement as follows. Differing from traditional local feature detectors that perform in an unsupervised manner, our CASL detector also enables learning-based detection in the case that there are multiple images available from an ident ical category. By introducing supervised learning into interest-point detection, our approach differs from state-of-the-art works in the literature that carry out learning steps no earlier than the subsequent recogni-tion/classification phase (see Figure 1).

The Algorithm. In the settlement of our learning-based CASL, multiple images from the same category are available which are treated in detection as a whole. For each image within this given category, we integrate semi-local statistics from other images to teach the CASL detector to locate category-aware semi-local peaks which removes the features that are useless in discriminating this category from the others. This is achieved by refining mean shift kernels and weights in the second phase (Section 2.2) of our CASL detector as follows.  X  First, we adopt k-means clustering to group local features extra cted from (1) the cat-egory images or (2) the entire image dataset containing categories (if other images are also available).  X  Subsequently, we build a Bag-of-Visual-Word model [Nister and Stewenius 2006] from the clustering results to produce a visual word signature for each local feature.  X  For each visual word, its term frequency (TF) weighting [Salton and Buckley 1988] is calculated to describe its importance ( within this category) in constructing S DoCG .
This importance modifies the weight w ( S i DoCG )andkernel G H ( S i DoCG  X  S DoCG )inthe mean shift search (Equation (6)) as follows. (1) For the mean shift search weight in Equation (6), the following modification is (2) For the mean shift search kernel in Equation (6), the following modifications are
Theoretical Comparisons. Former works in learning-based local feature representation can be categorized into two groups. The first group aims to learn local feature de-scriptors [Hua et al. 2007; Simon and Winder 2007] which use a ground truth set of matching correspondences (pairs of matched features) to learn descriptor parame-ters. Due to the requirement of exact matching correspondences, works in Simon and Winder [2007] and Hua et al. [2007] can be viewed as a strongly supervised learning scenarios. On the contrary, our learning-based CASL only requires image category in-formation rather than exact matching correspondences and can hence be viewed as a weakly supervised learning scenario.
 The second group derives from frequent visual pattern mining [Jamieson et al. 2006; Quack et al. 2007; Yuan et al. 2007], which adopts spatial association rules to mine frequent patterns (defined as certain spatial combinations of visual words in Jamieson et al. [2006], Yuan et al. [2007], and Quack et al. [2007]). Our main advantage lies in efficiency: works in Jamieson et al. [2006], Yuan et al. [2007] and Quack et al. [2007] require time-consuming frequent itemset mi ning, such as A-prior co-location pattern mining [Huang et al. 2004], to find meaningful patterns.

Last and most important, our approach puts forward the learning mechanism into the feature detection step (mean shift over DoCG) which enables us to dis-cover category-aware locations during feature detection. This differs from all related works in learning-based description [Hua et al. 2007; Simon and Winder 2007] or fea-ture selection [Jamieson et al. 2006; Quack et al. 2007; Yuan et al. 2007] (as shown in Figure 1).

Similarities also exists between our learning-based CASL detector and the top-down saliency of Gao and Vasconcelos [2005], which presents a way to design object-oriented local descriptors that can be seen as descriptors with object context. However, there are two main differences: from the leaning perspective, the contents or attributes that are learned by discriminant saliency [Gao and Vasconcelos 2005] and by CASL are different. Discriminant saliency [Gao and Vasconcelos 2005] learns the importance (weight) of each spatial feature using task-relevant knowledge, while the proposed CASL learns the importance of a group of features (quantized SIFT feature). So the scale of the learned contents is totally different between previous work and ours. Gao and Vasconcelos [2005] used the principle of winner-takes-all to find peaks in a single saliency map, whereas our proposed CASL utilizes the mean shift algorithm to find local peaks among SIFT points, which is out of the scope of traditional local context. That X  X  why we call our CASL a semi-local detector. We provide an interesting simulation as well as an informal explanation for our CASL detector from the human visual cognition mechanism. The first phase of our CASL detector is only sensitive to local regions. It offers similar functionality to local re-sponses of simple cells in the V1 cortex (similar to Gabor filters [Hubel 1995]) or to local response of retina (similar to DoG [Gilbert and Wiesel 1983]). To a certain de-gree, this phase shows similarities to selective spatiotemporal filters (such as Gabor or DoG [Gilbert and Wiesel 1983]) for detecting spatial frequency, orientation, and di-rection features [Hubel 1995] from local receptive fields [Daugman 1985]. In these cases [Daugman 1985; Hubel 1995], their spatial locations are well preserved in V1 for further processing in higher level visual cortices. This shows certain similarities to our contextual gaussian field, which integrates spatial statistics to construct the local detector context.

The V2 cortex produces stimuli based on the context of simple cells from V1 and has been discovered to show certain attentional modulation (less than that of V4) [Treue 2001]. As an informal and imprecise analogy, our semi-local detector phase receives stimuli from local feature context (difference of contextual Gaussians), which shows certain similarities to the complex cells in V2 in integrating contextual responses from local stimuli. This section presents the quantitative experimental evaluations of our CASL detector with comparisons to state-of-the-art approaches. First, to demonstrate our effective-ness in discovering discriminative and meaningful interest points, we evaluate our CASL detector in two challenging computer vision tasks: (1) near-duplicated image re-trieval and (2) object categorization. Second, to prove that our CASL detector can still maintain good detection repeatability without regard to photometric variations, we give a series of standardized evaluations in the INRIA detector evaluation sequences [Mikolajczyk et al. 2006]. This group of experiments quantizes our robustness against scale, viewpoint, blur, compression, and illumination variances. We also show the com-putation efficiency in Table III, in which we compare the time cost of our CASL detector in detecting an image (size 800  X  600) with different contextual and mean shift scales (measured by number of pixels).

Baseline Methods. There are five groups of baselines in our experimental comparisons.
 (1) Local Detectors . First, we offer four baselines of local detectors, including DoG (2) Local Descriptors . Second, we offer two baselines of local descriptors, that is, SIFT (3) Saliency Map Prefiltering . In both comparisons, we also compare with the saliency (4) Integration of Category Learning . Third, we compare our learning-based CASL (5) Context-Aware Features . Finally, we compare our framework with two context-Evaluation Database. We adopted the UKBench benchmark database [Nister and Stewenius 2006] to evaluate the effectiveness of our CASL detector in the near-duplicated image retrieval task. The UKBench database contains 10,000 images with 2,500 objects. There are four images per object to offer sufficient variances in view-points, rotations, lighting conditions, scales, occlusions, and affine transforms. We built our retrieval model over the entire UKBench database (see Figure 7). Then we selected the first image per object to form a query set to test our performance. This experimental setup is identical to that of Nister and Stewenius [2006], which directly offers us the base line performance of MSER + SIFT as reported in Nister and Stewenius [2006]. Identical to Nister and Stewenius [2006], we ensured that the top returning photo would be the query itself; hence, the performance depends on whether the system can return the remainding three photos of this object as early as possible. This criterion was measured as correct returning in Nister and Stewenius [2006]. In Figure 11, the y-axis represents this corre ct returning, and the x-axis denotes the number of photos we used to build the retrieval model. The test set is increased correspondingly (by only adding photos with in current categories) to derive Figure 11. Note that in all methods, the correct retu rning is larger than one, since the query would definitely find itself in the database.
 Retrieval Model. We built a ten-branch, four-layer vocabulary tree (VT) [Nister and Stewenius 2006] for the UKBench database, which produced approximately 10,000 visual words. Nearly 450,000 CASL features and nearly 1,370,000 MSER + SIFT fea-tures were extracted from the entire database to build two vocabulary tree models [Nister and Stewenius 2006] respectively (with inverted document indexing), each of which gave a bag-of-visual-words (BoW) vector for each image. In the vocabulary tree model, if features within a node are less than a given threshold (100 for CASL features, 250 for SIFT features), we stopped the K-means division of this node, whether it had reached the deepest level or not. For an a -branch VT with m words, the search time for one feature point is alog a ( m ), which is proportional to the logarithm of the branch number and is independent of the database volume.

Parameter Tunings. In parameter turning, we evaluated the (1) different context scales in the DoCG field construction which determine the trade-off between scale invariance and context sensitivity, the (2) different scales in the mean shift search kernel which determine context sensitivity, and the (3) different numbers of detected salient regions, which are codetermined based on the first two parameters.

Indeed, there are only two parameters to tune the performance of our CASL de-tector: the contextual scale and the mean shift scale. Figure 8 gives their parameter tunings based on cross validations. Left: fix the mean shift scale (100 pixels) to tune the contextual scales; middle: fix the contextual scale (60 pixels) to tune the mean shift scale; right: best tuning results at different contextual scales. It is worth mentioning that within each identical contextual and mean shift scale settings, the number of scale divisions ( K in Equation (5)) does not significantly affect the performance of our CASL detector.

The left-hand graphs of Figure 8 present the affects of different contextual scales, for each of which we tune the mean shift scale to get its best performance. The right-hand graphs are obtained by the same procedure. We have discovered that the best fit contextual scale is approximately 65 pixels, much larger than the scales of tradi-tional local feature detectors. Meanwhile, the best fit mean shift scale falls into ap-proximately 100 pixels, larger than the contextual scale. In the right-hand graphs of Figure 8, for a given contextual scale, its best fit mean shift scale is found to be larger than its current contextual scale X  X pproximately 30 to 50 pixels larger.

As shown in Figure 9, the number of semi-local salient regions actually depends on the contextual scales as well as the mean shift scales. More specifically, by increasing both scales, we obtain fewer salient regions with a more global definition. For a given contextual scale, the number of detected salient regions would decrease as the mean shift scale increased. We also discovered that, in the case that the mean shift scale is smaller than the contextual scale, there would be limited or no salient regions. This phenomenon can be expressed through the mean shift clustering procedure [Cheng 1995] of our DoCG search: if the kernel is truncated so that it will not cover more than one point in this dataset, then the current mean shift position will not change in the process.

Figure 10 demonstrates the variances of corrected returning by changing the num-ber of detected regions. It is interesting that neither too dense nor too sparse features can produce the best performance. In contrast, the best number of salient regions is a moderate number, corresponding to the optimal contextual and mean shift scales. Indeed, for our CASL detector, its number is approximately 50% of the baseline MSER detector.

Quantitative Comparisons. Figure 11 shows the performance of CASL + CASE in com-parison with the following state-of-the-a rt local feature detectors (DoG, MSER), de-scriptors (SIFT), and the improvement of Walther and Koch [2006] based on saliency map prefiltering.
 (1) DoG [Lowe 1999] + SIFT [Lowe 2004] is a widely adopted approach in building (2) MSER [Matas et al. 2004] + SIFT [Lowe 2004], is the implementation of a vocabu-(3) Saliency map [Itti et al. 1998] + MSER [Matas et al. 2004] + SIFT [Lowe 2004] (4) CASL + SIFT [Lowe 2004] quantizes the performance of our CASE descriptor. (5) DoG + MOP [Brown et al. 2005] is an alternative approach for our implemental (6) MSER [Matas et al. 2004] + MOP [Brown et al. 2005] is another alternative ap-
All these methods are based on bag-of-visual-words quantization, in which we adopt inverted document search to find the near-duplicated images to the query example (in our query set) on the UKBench database.

From Figure 11, it is obvious that our CASL feature outperforms baseline methods that are based solely on local features and their variation [Walther and Koch 2006] with saliency map prefiltering. Meanwhile, compared with MSER [Matas et al. 2004], DoG [Lowe 1999] performs much better in building our local feature context.
Higher precision also indicates two merits of our CASL detector: (1) more repeata-bility over rotation, scale, and affine transformations, which are common in the UK-Bench database; and (2) more discrimination within different object appearances. The photos in UKBench usually contain differe nt objects, such as CD covers with identi-cal or near-duplicated backgrounds. Hence, the capability to discriminate foreground object from background clutter is essential for high performance.
Evaluation Database. We selected a ten-category subset of the Caltech 101 database to evaluate our CASL detector in the object categorization task, including face , car side , snoopy , elephant , wheelchair , cellphone , motorbike , butterfly , pyramid ,and buddha (see Figure 12). Each image category contains approximately 60 to 90 images. We randomly selected ten images from each category to form a test set and used the rest of the images to train the classifiers.

We evaluated our categorization performa nce by measuring the percentage of cor-rected categorizations in each category (averaged hit/miss of its ten test examples) to draw a categorization confused matrix.
 Categorization Model. We built a two-layer, 30-branch vocabulary tree [Nister and Stewenius 2006] for image indexing, which contains approximately 900 visual words for this categorization task. For each category, the bag-of-visual-words vectors (approx-imately 900 dimensions for each image) are extracted for training. We built an SVM for every two categories offline. In the online recognition, we adopted an one-vs.-one strategy to vote for the category membership for a test image: if one category won an SVM between this category and another category, we increased the voting score for the winning category by one. The category with the highest scores was assigned to the test image as its final label. Since we aimed to compare CASL with other detectors, we simply adopted a one-vs.-one SVM in the classifier phase which can be easily replaced by other sophisticated methods, for example, SVM-KNN [Zhang et al. 2006]. We used the same parameter tuning approach described in near-duplicated image retrieval to tune the best contextual and mean shift scales.

Evaluation Results. Figures 13 X 14 present the confused matrix of different combina-tion schemes, including (1) saliency map + MSER + SIFT + SVM, (2) CASL (local features: MSER + SIFT) + CASE + SVM, (3) CASL (local features: DoG + SIFT) + CASE + SVM, and (4) learning-based CASL (local features: DoG + SIFT) + CASE + SVM. Generally speaking, CASL features perform much better than the approach of Walther and Koch [2006] that adopted saliency map prefiltering to integrate semi-local cues. Meanwhile, the integration of the learning part into CASL detector can largely boost the categorization perfor mance in our current settlement.

Comparing with Context-Aware Global Features [Serre et al. 2006]. We provide quantitative comparisons to the context-aware global features of Serre et al. [2006]. Identical to the settlement of Serre et al. [2006], the Caltech 5 is adopted in comparison. The per-formances of SIFT and C2 are directly from Serre et al. [2006], the former of which adopted approximately 1,000 dimension features for categorization. To offer compa-rable evaluations, we built an CASL-based BoW vector containing about 900 visual words and adopted linear SVM for classifier training. Note that we also used learning-based CASL in feature extraction.

Based on the comparison in the Caltech 5 database (as shown in Table I), our CASL detector achieved almost identical performance compared with the best performance reported in Serre et al. [2006]. Since C2 +SVM already achieves very high (nearly 100 percent) performance, it is hard to obtain m uch better performances with a large mar-gin. On the contrary, in addition to (slightly) better performance than the C2 features, our CASL detector also shows much better results compared to SIFT +SVM with a large margin. Nevertheless, the C2 feature is better at computational cost. However, there are two major differences between our approach and the S1-C1-S2-C2-like fea-tures [Serre et al. 2006].

First, the S1-C1-S2-C2-like feature extr action and classification framework [Serre et al. 2006] produces one feature vector per image with fixed feature dimensions. This is different from our CASL detector that outputs patch-based features to produce bag-of-(semi-local)-words representations.

Second, the S2-C2 part in Serre et al. [2006] needs training for prototype learning, which is indispensable in feature construction. In contrast, our CASL feature can also perform in an unsupervised manner (learning is an optional choice), which can be easily reapplied in other unsupervised scenarios and can be further combined with more complicated classifiers in the subsequent categorization step.

Comparing with Shape Context [Belongie et al. 2002]. Regarding the polar-bin division strategy, there exist similarity-between our CASE descriptor and the shape context feature [Belongie et al. 2002]. Hence, it is a natural through to replace our CASE descriptor with the shape co ntext descriptor. Table II presents the experimental com-parisons of our CASL + CASE feature with the CASL + shape context feature [Belongie et al. 2002]. We should note that the shape context is a feature descriptor which is not competitive but comprehensive to our CASL detector. However, we have found that the direct replacement of the shape context feature to our CASE descriptor cannot achieve satisfactory results to describe our CASL detections. Its similarity-matching mechanisms are originally designed for shape primitives.

We further leveraged the PASCAL VOC 05 database to evaluate our CASL detec-tor under the classification and localization framework in Winn et al. [2005], which achieved state-of-the-art results in that database. We split PASCAL into two equal-sized sets for training and testing, which is identical to the settlement in Winn et al. [2005]. The PASCAL training set provides bounding boxes for image regions with an-notations: what we try to decide is whether the test bounding box is a given object. We extract DoG + SIFT as a baseline implementation of the CASL + CASE detector.
In the PASCAL VOC 05 database, we build a ten-branch, three-level vocabulary tree based on either DoG + SIFT features on learning-based CASL + CASE features. If features in a node are fewer than a given threshold (2,000 features for DoG + SIFT and 1,000 features for CASL + CASE), we stop the K-means division in this node, whether or not it has achieved the deepest level. A document list (approximately 1,000 words) is built to record which photo contains each word which forms an inverted index file. In the online part, each SIFT feature extracted from the query image is sent to its nearest visual word in which the indexed images are picked out to rank the similarity scores to the query.

In classifier learning, each object category is viewed as a class with a set of BoW vectors extracted from the bounding box with its label. Identical to Winn et al. [2005], the classification of the test bounding box (we know its label beforehand as ground truth) is a nearest neighbor search process. Figure 16 shows that in almost all cate-gories, CASL + CASE features give better precision than DoG + SIFT features within the identical ten PASCAL categories.
Evaluation Database. Despite detecting meaningful and discriminative salient re-gions, we should still ensure that our CASL detector can retain the detector repeata-bility requirement [Mikolajczyk et al. 2006] well, which is crucial for many computer vision applications, such as image matching and 3D scene modeling. We evaluated our detector repeatability in the INRIA detector evaluation sequences [Mikolajczyk et al. 2006]. Using this database, we directly got the results reported in Mikolajczyk et al. [2006] as our baselines. In Mikolajczyk et al. [2006], the repeatability rate of detected regions among a test image t and the i th image in the test sequence are calculated as in which | R I ( ) | denotes the number of repeatab le salient regions between t and i ,and n t and n i represent the number of salient regions detected in the common part of images t and i , respectively. Please refer to Mikolajczyk et al. [2006] for detailed experimental setups.

Repeatability Comparisons. Figure 15 shows the quantitative evaluations of the detec-tor repeatability comparisons in the sequences of different scales , viewpoints , blurs , compressions ,and illuminations . We can see that our CASL detector produces more repeatable detection results in the repeatability comparisons of compressions, illumi-nations, and blurs and we obtain comparable performances in the repeatability com-parisons of viewpoints and scales. Although our CASL detector produces generally fewer salient regions compared with the alterative approaches, we still achieve com-parable performance by including semi-local spatial layouts into detection. Such spa-tial layout prevent unstable detections that are frequently found in traditional local features detectors.

We can also see that in many cases, our CASL detector produces more repeatable de-tection results in the repeatability comparisons of illuminations (fifth subfigure) and viewpoints (sixth subfigure); in all cases better in the repeatability comparisons of compressions (forth subfigure); and in some cases better in the repeatability compar-isons of blurs (third subfigure). However, we should note that we rank almost persis-tently worse than the Hessian-Affine in viewpoints (first subfigure) and scales (second subfigure). (Hessian-Affine is shown as one of the most effective detectors in the liter-ature.). One reason for these performances is that our CASL detector generally pro-duces fewer salient regions compared with alternative approaches. In sum, our CASL detector achieves comparable performance to state-of-the-art detectors with generally fewer features per image, thanks to the inclusion of semi-local spatial layouts into detection which prevent unstable detections that are frequently found in traditional local features detectors. However, as shown in Figure 15, in some cases (such as view-points or scales changes), Hessian-Affine and MSER would be better choices due to their simplicity.

We further compare the computational time cost of our CASL detector with respect to different contextual scales and mean shi ft scales in Table III. One interesting ob-servation here is that by increasing both contextual and mean shift scale, the overall computational cost will subsequently increase.
 Based on the preceding three groups of experiments with comparisons to state-of-the-art detectors, we make the following statements and guidelines about our application scenarios.

What vision tasks are more suitable for CASL than traditional local feature detectors?  X  Vision tasks that emphasize discovering meaningful and discriminative features rather than repeatable detection. For instance, generalized or specialized ob-ject recognition, image annotation, semantic understanding, and video concept detection.  X  In the case that more attentional focus is demanded, such as to describe an image based solely on its most salient objects or to discriminate foreground objects from backgrounds from a set of training images. In the later case, we should also know their category labels before carrying out our learning-based CASL.

What scenarios are not so suitable for using CASL instead of local feature detectors?  X  When the vision tasks emphasize repeatable detection more, rather than seman-tic or attentional discriminability, for example, image matching and wide baseline matching.  X  When the target image contains large amounts of local features and there are no demands to differentiate foreground objects from background clutter. For instance, scene matching and near-duplicated scene identification.

What our CASL detector tries to find is the high contrast regions in the semi-local scale. High contrast is well digested in both local scale detectors and global features. In local scale detectors, high contrast us ually corresponds to locally salient regions, such as local peaks in DoG and MSER regions; in global features, high contrast can also be regarded as the focus of attention (FOA) in saliency map detection.
Our work takes effect on a semi-local scale (between global and local scales), which takes advantage of both local detectors and global features while avoiding their draw-backs. On one hand, the local scale is too local and cannot digest the intrinsic object character well; on the other hand, the global scale is too sensitive to photographing variances, such as partial occlu sions and different backgrounds.

Rather than simple weighted combinations of different local feature scales, the mean shift search serves as an extensible step in finding discriminative semi-local regions. The category learning can further teach our DoCG construction by incorpo-rating feature category discriminability in contextual strength evaluation.
Our CASL detector works extremely well for the task of discriminative detection, as well as for the task that needs focus of attention in order to distinguish foreground objects from background clutter. In these cases, our effectiveness lies in contextual discriminability, in which our CASL detector has promoted the feature extraction from the traditional local scale to a semi-local s cale. It enables us to detect more discrim-inative locations at a robust and discriminative scale for image representation and discrimination.

Our learning-based CASL detector serves as another important step to boost detec-tor discriminability. It can largely filter out meaningless local regions to improve the subsequent bag-of-features representation and classifier training.
 This article gives a first and systematic exploration of context information in design-ing an interest point detector. Going beyond state-of-the-art approaches, we integrate contextual cues to enhance interest point detectors from the traditional local scale to a semi-local scale, which enables us to discover more meaningful and discriminative salient regions without losing detector repeatability. Our other contribution is to intro-duce a learning-based detector mechanism. It brings forward category learning (tradi-tionally in subsequent classifier phases) into the feature detection phase, which locates category-aware interest points to improve p erformances of the subs equent recognition classifiers.

All these contributions are integrated within a novel context-aware semi-local (CASL) feature detector framework, which is a two-phase procedure. The first phase builds the local feature context based on a pro posed difference of contextual Gaussians (DoCG) field, which offers the capability to highlight attentional salient regions, shar-ing good similarity to the saliency map detection results. The second phase adopts the mean shift search to locate semi-local DoCG peaks as context-aware salient regions. This phase naturally enables learning-based detection by integrating category learn-ing into the mean shift weights and kernels. We have conducted quantitative compar-isons on image search, object categorization, and detector repeatability evaluations. We compare our performances with state-of-the-art approaches, based on which we further discuss the suitable and unsuitable scenarios for deploying our CASL detector.
Two interesting questions remain open. First, we would further integrate category learning into the DoCG construction phase , in which we can adopt category-aware dis-tributions to supervise the contextual repr esentation of local features. Second, we are interested in extending our CASL feature to detect context-aware 3D salient regions within videos, in which we would investigate whether the sequential characteristics of successive frames can be used to construct the s patiotemporal contextual statistics in our first phase.

