 Machine learning is everywhere in today X  X  NLP, but by and large machine learning amounts to numerical optimization of weights for human designed repre-sentations and features. The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appro-priate for complex interpretation tasks. This tuto-rial aims to cover the basic motivation, ideas, mod-els and learning algorithms in deep learning for nat-ural language processing. Recently, these methods have been shown to perform very well on various NLP tasks such as language modeling, POS tag-ging, named entity recognition, sentiment analysis and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed re-sources or time-intensive feature engineering. De-spite these advantages, many researchers in NLP are not familiar with these methods. Our focus is on insight and understanding, using graphical illustra-tions and simple, intuitive derivations. The goal of the tutorial is to make the inner workings of these techniques transparent, intuitive and their results in-terpretable, rather than black boxes labeled  X  X agic here X .

The first part of the tutorial presents the basics of neural networks, neural word vectors, several simple models based on local windows and the math and algorithms of training via backpropagation. In this section applications include language modeling and POS tagging.

In the second section we present recursive neural networks which can learn structured tree outputs as well as vector representations for phrases and sen-tences. We cover both equations as well as applica-tions. We show how training can be achieved by a
