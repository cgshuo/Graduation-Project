 Pontificia Universidad Cat X lica de Chile, Regi X n Metropolitana, Chile 1. Introduction Techniques to divide a set of N data instances into K groups are known as clustering algorithms. Clustering algorithms are commonly used in an unsupervised learning framework where the goal is to minimize an error function with respect to a given distance metric, for example, intra-cluster distance. A robust model for clustering is to use a mixture of Gaussians [4] that has the ability to capture complex relationships among the data using a sound statistical approach. Despite its advantages, this technique tends to be slow, mainly due to the calculation of a covariance matrix. A faster and simpler clustering technique is the K-Means algorithm [21] that uses a hard assignment of data points to clusters and assumes a spherical covariance. While the simplicity of the K-Means algorithm is one of the main reasons for its popularity [38], its lower computational complexity with respect to alternative clustering visual recognition [17].

In contrast to traditional clustering, supervised clustering is applied to labeled data. Here, the goal is cluster that belongs to its most frequent class. Figure 1 shows an illustrative toy example corresponding to a 2D dataset with three spatial clusters and two classes. After applying both unsupervised and super-vised clustering, it can be observed that unsupervised clustering ignores class labels (Fig. 1(a)), while supervised clustering generates clusters that focus on a particular class (Fig. 1(b)).
Eick et al. [11] enumerate several applications of supervised clustering, such as dataset compression, distance metric learning, or classification refinement, among others. As an example, supervised cluster-ing can be used to identify customers profiles according to ordinal measures (e.g. age, salary, marital status) by identifying clusters that are homogeneous with respect to their buying behavior in terms of particular product categories (labels). Further uses of supervised clustering can be found in the areas of genetics and finance [32].

Current algorithms for supervised clustering usually have the form of a K-Medoids algorithm. Due to the use of medoids, this type of methods is more resistant to outliers than schemes based on a K-Means strategy, however, they have the drawback of being considerably slower. In particular, assuming a fixed number of iterations, the K-Medoids algorithm has a quadratic complexity in terms of the number of data instances, while in the case of K-Means this complexity is only linear [4].

In this work we present a new method for supervised clustering that is based on two main hypotheses: i) For a wide variety of applications a combination of supervised and unsupervised information can lead us to more informative clusters, and ii) A supervised clustering method based on a K-Means type of algorithm can allow us to overcome the speed limitations of current methods based on a K-Medoids clustering strategy. Following these hypotheses, the main contributions of this paper are: i) Presenting LK-Means, a new supervised clustering algorithm that extends the K-Means algorithm to incorporate instance labels, ii) Empirical evidence showing that LK-Means outperforms the K-Means algorithm and a K-Medoid supervised clustering method, as measured by several popular metrics commonly used to access clustering quality, and iii) Empirical evidence showing that, in terms of execution time, our method is more efficient than a supervised clustering technique based on a K-Medoid clustering strategy. The rest of this paper is organized as follows. Section 2 describes two baseline methods, K-Means and K-Medoids algorithms, and relevant previous works. Section 3 presents LK-Means, the proposed super-vised clustering approach. Section 4 presents and discusses experimental results using several bench-mark datasets and an application to the case of object recognition. Finally, Section 5 presents our main conclusions and future avenues of research. 2. Background 2.1. K-Means and K-Medoids K-Means algorithm is one of the most popular clustering techniques. This algorithm partitions N data instances into K clusters, where the number of clusters K has to be known a priori. Specifically, given a dataset X with N data instances x i  X  R d , i  X  [1 ...N ] , K-Means algorithm partitions X into K cluster C where the indicator function  X  nk is given by:  X  refers to L2-norm and u
Optimal parameters u k are found by minimizing Eq. (1) using a gradient descent approach. This re-sults in an iterative procedure that alternates between assigning data instances to cluster centers, and re-estimating cluster centers given the new assignations. Convergence to a local minimum of Eq. (1) is granted by the gradient descent type of exploration and the finite set of possible assignations of data instances to clusters. In particular, assuming a fixed number of iterations and dimensions, the compu-tational complexity of K-Means is O ( NK ) . Algorithm 1 summarizes the main steps of the K-Means algorithm.
 Algorithm 1 : K-Means algorithm. 1. Randomly select K data instances as initial means. 2. Associate each data instance with the cluster of its nearest mean and calculate the cost function 3. Calculate the new means as the centroids of the K new partitions. 4. Repeat steps 2 and 3 until there is no change in the cost evaluation (or the cost change is below a
While K-Means for fixed numbers of iterations and dimensions has a linear computational complexity To alleviate this problem, the K-Medoids algorithm uses a more robust procedure to find the cluster centers, but this procedure has a quadratic complexity with respect to the number of data instances. In particular, K-Medoids minimizes a score that is similar to the one used by K-means, but it considers a more general distance metric  X  ( x, x ) between data instances x and x , as shown in Eq. (2). An example of metric  X  ( x, x ) is the Euclidean distance, used in K-Means, or the Jaccard distance, commonly used in applications related to transactional databases [23].

In contrast to K-means, K-Medoids minimizes Eq. (2) with respect to parameters u k by calculating a matrix that stores the distances between all pairs of data instances. Specifically, initially K-Medoids randomly chooses a set of K data instances as the initial set of K medoids and calculates the distance matrix between all the data instances. It then replaces each medoid with all non-medoid points and calcu-lates all possible configurations costs according to Eq. (2). Next, it chooses as the new medoids the ones corresponding to the configuration with the lowest cost. Finally, the method repeats the search over the non-medoids elements until the medoids do not change. The procedure is summarized in Algorithm 2.
Assuming a fixed number of iterations and dimensions, the computational complexity of K-Medoids is
O ( K ( N  X  K ) 2 ) . This implies that in general the K-Medoids algorithm is slower than K-Means. Algorithm 2 : K-Medoid algorithm. 1. Randomly select k data instances as initial medoids. 2. Associate each data instance to its most similar medoid and calculate the cost using Eq. (2). 3. for each medoid m do 4. for each non-medoid o do 5. Swap m and o and compute the cost of the configuration. 6. end for 7. end for 8. Select the set of elements corresponding to the configuration with the lowest cost. 9. Repeat 3 through 8 until there is no change in the set of medoids. 2.2. Related work
Semi-supervised clustering uses labeled and unlabeled data to find clusters that maximize a score related to cluster purity with respect to known class labels. Semi-supervised clustering methods can be divided into two groups: Similarity based methods and search-based methods [3]. Similarity-based methods use a modified distance function that considers the labels of classified examples and then uses a traditional clustering algorithm. On the other hand, search-based methods modify clustering algorithms themselves to accommodate for labeled instances, but do not change the distance function [5].
In terms of supervised clustering, all available records have labels. Tishby et al. propose an agglom-erative clustering algorithm [35] using the notion of  X  X nformation bottleneck X  [34]. This technique min-imizes the information loss of the clustering related to a class conditional distribution. Embrechts et al. [10] propose a genetic algorithm for a version of K-Means where the goal of the search process is to obtain clusters that minimize cluster dispersion and cluster impurity. Cohn et al. [7] modify the popular EM algorithm for incorporating similarity and dissimilarity constraints. They assume the presence of a human oracle that guides the clustering process. Basu et al. [3] modify the K-means algorithm to cope with class knowledge. They use a careful initialization based on the neighborhood of the data instances.
Sinkkonen et al. [32] propose a method called discriminative clustering that minimizes distortion within clusters. In their work, distortion is related to the loss of mutual information among classes and clusters, which is caused by representing each cluster by a prototype. This technique seeks to produce clusters that are internally as homogeneous as possible with respect to a class conditional distribution. The resulting minimization is complex and they have to resort to approximations or simulated annealing methods to find suitable solutions.

Jordan et al. [39] (and similarly Shental et al. [2]) transform training examples into constraints based Then, they derive a modified distance metric that minimizes the distance between data instances consid-ering the constraints. Finally, they use a K-Means algorithm in conjunction with the modified distance metric to compute clusters.

Eick et al. [11] formally introduce the term supervised clustering. Their work proposes supervised ver-sions of some clustering algorithms, such as K-Medoids and divisive clustering. In particular, the SRID-HCR algorithm (Single Representative Insertion/Deletion Steepest Decent Hill Climbing with Random-ized Restart) shows good performance in their experiments when compared to alternative techniques, thus, we choose this method as the baseline for comparison in our work.

Ye et al. [40] present a discriminative version of K-Means. They simultaneously solve linear discrim-inant analysis (LDA) and K-Means optimization using matrix algebra. An advantage of this method is that it makes a feature transformation using LDA properties. For each iteration, their method needs to solve an optimization problem using linear search. Unfortunately, they do not show any measure of the speed of their method.

In relation to extensions of K-Means, Deelers and Auwatanamongkol [9] propose a scheme to ini-tialize the K-Means algorithm using a recursive strategy that, considering the data axis with highest variance, progressively divides the data until they obtain a suitable number of clusters. Shanmugasun-daram and Sukumaran [31] introduce a related scheme to initialize the K-Means algorithm, where they divide the data into two smaller cells considering the data axis with highest variance and keeping the two cells as far apart as possible. This procedure is repeated until one can obtain a prefixed number of clusters. Kumar et al. [18] enhance the K-Means algorithm by considering particular data structures (red-black tree and min-heap) that allow them to reduce computational time. These previous works are valuable in terms of improving the initialization and time processing capabilities of the traditional k-means algorithm, however, these works do not consider labeled data as in our technique. In this sense, these techniques can be considered as complementary to our work.

In a related research task, Lasserre et al. [19] propose the idea of a convex combination of unsupervised and supervised information in machine learning. They introduce a Bayesian framework to combine unlabeled and labeled data, where they find that under limited training data, the best performance is given by a combination of both views. Here, we also follow a similar idea but in the context of a supervised version of the K-Means algorithm leading to a different optimization problem and solution. As shown by our experiments, our proposed strategy provides several advantages with respect to alternative techniques for supervised clustering. 3. Labeled K-Means
Following Eick et al. [11], several supervised clustering methods follow a K-Medoids approach that is very time consuming. Inspired by [19], we propose LK-Means, a K-Means like algorithm with a modified cost function that considers a convex combination of both, a class-dependent and non-class-dependent cost functions.

We assume a labeled dataset X with N training instances ( x i ,y i ) ,where x i  X  R d , y i  X  [1 ,...,L ] , and i  X  [1 ...N ] . We assume that the clustering problem requires K clusters. LK-Means replaces the tradicional K-Means cost function in Eq. (1) with the following function: where  X  l nk refers to the supervised indicator function that assigns instance x n to mean u l k ,whichin data instances with label l inside cluster k ,  X  nk refers to the unsupervised indicator functions, and u k corresponds to the mean of all data instances in cluster k . Equation (3) represents a convex combination, where parameter  X  in the range [0 , 1] manages the balance between the supervised and unsupervised clustering scores.

In particular, prior factor  X  l k for data instances with label l inside cluster k is defined as: near one, cluster k tends to contain only elements with label l . In the opposite case, when this weight is near zero, cluster k tends to contain no elements with label l .

The unsupervised indicator function  X  nk for data instance x n and cluster C K is defined as:
In terms of each unsupervised mean u k , it is defined as the weighted mean over all supervised means u k for the corresponding cluster
To find the optimal parameters:  X  l nk and u l k , we minimize Eq. (3) using a block coordinate descent approach that resembles the operation of the K-Means algorithm. Specifically, we alternate optimizations call these steps assignment and update -steps, respectively. We refer now to each of these steps.
In terms of the assignment-step , cost function J in Eq. (3) considers each data instance n in separate terms of the main sum, therefore, we can independently optimize J with respect to each indicator  X  l nk . Furthermore, in the assignment-step we fix the value of the supervised means u l k and, as a consequence, we also fix the values of the unsupervised means u k . As a result, the assignment that minimizes the cost function J is given by: a Laplace smoothing. We use this procedure to avoid empty values for the supervised means vectors which can appear in the case of clusters without elements of the corresponding class. In this case, the supervised mean of a missing class is given by the unsupervised cluster because all elements have a value near to zero.

Specifically, we apply a Laplace smoothing according to: where  X  l nk is defined as:
We apply a Laplace smoothing [26] with a constant  X  =0 . 001 . This small constant can be interpreted as the global uncertainty about the label of an element.

In terms of the update-step , we only need to find the optimal supervised means since each unsuper-vised mean u k is a function of the corresponding supervised means u l k . Applying the corresponding partial derivatives to Eq. (3), we have: By rearranging components in Eq. (8) and setting the derivative to zero, we obtain:
Assuming iteration t and that we are computing the optimization for the supervised mean of a given class label l , we use the previous supervised means u l maximized component u l k and fixing the rest. Then, u u k and l = l ,wehave:
Then, by rearranging the components, we have:
Finally we obtain:
Equation (12) has a straightforward interpretation. If we have  X  =1 , then only supervised information is considered. On the other hand, if  X  =0 then the resulting clusters correspond to the unsupervised solution provided by the traditional K-Means algorithm. We summarize our procedure in Algorithm 3. Algorithm 3 Labeled K-Means Algorithm 1. Initialize K initial means randomly. 2. Associate each data instance with its nearest mean and consider its class. 3. Compute supervised means u l k using Eq. (12). 4. Compute unsupervised means u k using Eq. (5). 5. Compute indicatrices  X  l nk considering Equation (6). 6. Compute the cost J using Eq. (3). 7. Repeat 3 to 6 until there is no change in the cost evaluation (or cost change is below a threshold).
In terms of convergence, the assignment-step given by Eq. (6) can only decrease the value of the relevant cost function in Eq. (3). Similarly, the update step provides new parameter values that also decrease this cost function. Furthermore, given that set of possible assignments of training instances to clusters is finite, the procedure in Algorithm 3 can not decrease forever. As a consequence, it is possible to guarantee that the proposed algorithm will converge to a local or global optimum of the relevant cost function.

In our model, we do not consider specific strategies to deal with noisy or missing data. However, stan-dard preprocessing strategies do exist to deal with these problems, and they can be used to complement our technique [16]. 4. Experiments and results 4.1. Experiments in general datasets
In this Section, we test the performance of LK-Means using diverse datasets. In particular, we use 8 real data sets from the UCI Machine Learning Repository [1]: Iris, Heart, Glass, Diabetes, Silhouttes, Segment, Ionosphere, and Sonar. Table 1 shows the main details for these datasets. We normalize all these datasets to the range [0 , 1] . Following the regular implementation of K-means, we use Euclidean distance as the similarity metric. All experiments are performed on a PC with 2.0 Ghz Pentium IV processor with 2 GB of RAM memory.

We compare our algorithm against classical K-Means and SRIDHCR. SRIDHCR is a K-Medoids al-gorithm based on a discriminative metric with random re-initialization if it detects a local minimum. We choose SRIDHCR because it shows good performance in relation to other supervised clustering methods [11]. We compare these algorithms in terms of clustering quality and computational time. In gorithms. There are alternative metrics for evaluating clustering quality, such as F-measure [6], Jaccard index [28], or Fowlkes Mallows index [14], however, we follow the metrics suggested in [24]. Con-sequently, we assess clustering quality using 4 different metrics commonly used to validate clustering results [24]: Adjusted Mutual Information (AMI) [36], Adjusted Variation of Information (AVI) [36], Mirkin distance (MD) [25], and Adjusted Rand Index (ARI) [15]. AMI and AVI are variations of mutual information (MI), while MD and ARI are variations of Rand Index (RI). All these metrics do not make any assumption about the form of the clusters. Also, they are in the range [0 , 1] , where higher values indicate a better clustering, except in the case of MD where small values indicate better results.
In our experiments, we use cross-validation with ten folds (10-CV) to validate the results of each proposed in the context of the K-Means algorithm [22]. Also, it is possible to relate the selection of K to the number of known classes. Here, we do not focus in proposing new strategies to choose this value, and we run our experiments testing different numbers of clusters. For each dataset, we select values for K equally spaced according to 4 intervals beginning from the number of classes L to the upper bound
For each of the 4 clustering metrics considered here, we test the performance of LK-Means using performance for these 3 values. In the case of SRIDHCR, we choose the best parameter  X  (see [11] for details) according to 3-CV in a grid with 11 values: 0 to 2.0 with a step of 0.2. K-Means does not require more parameters than the number of clusters. It is important to note that to be fair with K-Means, we do not optimize the value of parameter  X  in LK-Means. This is because when  X  approaches zero LK-Means behaves exactly like K-Means, therefore, by optimizing  X  , LK-Means can always at least match the performance of K-Means. Consequently, in all tests, we just consider high values of  X  to stress the we use a paired Student X  X  t-test (Behrens-Fisher problem [29]) to compare the results of LK-Means against the performance of each of the alternative techniques.

Table 2 shows our results using AMI metric. Considering the average AMI results for all values of K under test, our method outperforms K-Means and SRIDHCR in most of the cases, with the exceptions of the Silhouttes and Ionosphere datasets where K-Means shows better performance. Considering only cases with confidence 75%, a paired t-Student test shows that for Iris, Heart, Glass, Segment, and Sonar datasets, LK-Means has better AMI than the nearest competitor with 84%, 100%, 88%, 91%, and 75% of confidence, respectively. On the other hand, K-Means shows the best performance in Ionosphere and Silhouttes datasets with 95% and 84% confidence, respectively.
Table 3 shows our results using AVI metric. By considering all datasets, we can observe that, on average, again LK-Means outperforms the other algorithms in most of the cases. Similarly to the results with AMI metric, the worst relative results for LK-Means is given for the case of the Ionosphere dataset. Considering only cases with confidence 75%, a paired t-Student test finds that LK-Means in Iris, Heat, Glass, Segment, and Sonar datasets has greater AV I than the nearest competitor with 87%, 100%, 94%, 75% and 80% of confidence, respectively. On the other hand, K-Means shows the best performance in Ionosphere and Silhouttes datasets with 96% and 95% confidence, respectively.

Table 4 shows results using MD metric. In terms of average results, in half of the 8 datasets LK-Means is the winner, while K-Means shows best performance in the rest of the datasets. In general, we notice that under MD metric there is not a clear winner between LK-Means and K-Means, and results depend on the type of dataset. Considering only cases with confidence 75%, a paired t-Student test shows that LK-Means in Heart, Diabetes, and Sonar datasets has lower MD than the nearest competitor with 100%, 92% and 87% of confidence, respectively. On the other hand, K-Means shows the best performance in Glass, Silhouttes, and Ionosphere datasets with 97%, 98%, and 95% of confidence, respectively.
Table 5 summarizes results using ARI metric. In average LK-Means is the winner in 5 of the 8 datasets. while K-Means shows best performance in 2 datasets, and SRIDCHR in one. Considering only cases with confidence 75%, a paired t-Student test finds that LK-Means in Iris, Heart, Glass, Segment, and Sonar datasets has better ARI than the near est competitor with 75%, 100%, 95%, 98% and 93% of confidence, respectively. On the other hand, K-Means shows the best performance in Ionosphere dataset with 99% confidence.

Considering the different metrics and datasets used to evaluate clustering quality, the previous results indicate that in general LK-Means outperforms the alternative techniques under consideration. However, the superior performance of LK-Means depends on the type of dataset and the validation metric under consideration. In terms of the different datasets, in general LK-Means shows superior performance in most of them with the exception of Silhouttes and Ionosphere, where the unsupervised clustering strat-egy of K-Means leads to better clusters. We believe that, in general, the performance of LK-Means is closely related to the pertinence of our hypothesis that homogeneity in class information leads to more informative clusters. Clearly, the validity of this hypothesis depends of the application under considera-tion, particularly, the semantic of the data labels under consideration. In terms of the 4 metrics used to evaluate clustering quality, LK-Means outperforms clearly the alternative algorithms in the case of AMI and AVI metrics, and to a lesser degree in the case of ARI metric. In the case of MD metric, for the values of  X  under consideration, LK-Means is unable to improve the results of K-Means. Following the observations in [37], MD metric and, to a lesser extend, ARI metric are affected by cluster size, there-fore, they have a bias that affect their performance. As recommended in [37], AMI and AVI produce more stable and suitable results. Coincidentally, in our case AMI and AVI produce similar results and they provide stronger support to the superiority of LK-Means with respect to the alternative techniques.
Additionally, we test the sensibility of performance respect to  X  by measuring adjusted mutual infor-the analysis of results, we consider two representative datasets. Specifically, we choose Diabetes and Glass datasets because in our experiments they represent cases where, under the AMI metric, LK-Means and K-Means alternate the best performance for different values of K . In both cases, we use a fixed number of clusters. We choose the number of clusters using the classical silhouette method [30], where the cardinality of the set of clusters is selected to maximize the average silhouette of the clusters. Figure 3(a) shows the relationship between  X  and AMI for Diabetes dataset. The best result for LK-Means is obtained when  X  = 0.9 with a corresponding value of AMI =0 . 092 . The worst result is for alpha =0 . 1 with a corresponding value of AMI =0 . 051 . For this dataset, K-Means obtains a value of AMI =0 . 050 , therefore, there is a big advantage in favor of LK-Means. On the other hand, Fig. 3(b) shows the relationship between  X  and AMI for dataset Glass. In this case, the best results are obtained with low values of  X  (0.1 and 0.2). In particular, the best result for LK-Means is obtained when  X  = 0.1 with a corresponding values of AMI =0 . 171 . For this dataset, K-Means obtains a value of AMI =0 . 168 . Consequently, both algorithms show a similar behavior. This is expected because, according to Eq. (3), for values of  X  near zero LK-Means behaves like K-Means.

The processing time for the different algorithms is summarized in Table 6. As expected, K-Means is faster than the other methods, however, it is relevant to see that LK-Means is visibly faster than SRIDHCR. For example, for 12 clusters in the Silhouttes dataset, K-Means, LK-Means and SRIDHCR use approximately 0.1, 5, and 715 seconds, respectively. The reason for the slowness of SRIDHCR is that K-Medoids requires a distance matrix to be calculated between all the records, while K-Means and LK-Means only require the distance measures from the means to all records. Considering the good results of the AMI score, we can see that LK-Means is capable of combining the speed of K-Means and the semantic gain to incorporate data labels during the clustering process. 4.2. Experiments in object recognition In this Section we apply LK-Means to the task of codebook generation for a visual recognition task. Currently, the Bag-of-Visual-Words (BoVW) scheme is one of the most popular approaches for visual object recognition [ 33]. Under this approach, the generation of a suitabl e codebook plays a key role. In general, most BoVW approaches build the codebook using a clustering algorithm, mainly K-Means. Interestingly, although class labels are usually available, these are not considered during the codebook such as LK-Means, can offer to provide more discriminative codebooks.

Following the previous intuition, we compare the performance of LK-Means against K-Means for the task of codebook generation in object recognition applications. As a testbed, we select 4 object recog-nition datasets that are commonly used to benchmark object recognition techniques. These datasets are: UIUC, DARMSTADT, VEH-CALT, and OUTDOOR. UIU C contains 2 obj ect classes: cars and back-gorund. DARMSTADT contains 3 object classes: motorbike, cow, and cars [20]. VEH-CALTECH is a subset of CALTECH-101 (VEH-CALT) dataset [12], including 4 object classes: airplane, car, heli-copter, and motorbike. Finally, OUTDOOR contains images of 8 types of outdoor scenes [27]. Table 7 shows relevant details for all these datasets. Following a standard implementation of K-means, we use Euclidean distance as the main similarity metric for all our test.

Following standard procedures for BoVW schemes [33], we use Histogram of Gradients (HoG) as a basic visual feature [8]. In particular, we obtain the HoG descriptors using patches of 32  X  32 pixels. These patches are selected using a sliding window process over a fix grid on each input image. In particular, we use the variant UOCTTI of HoG proposed by Felzenswalb et al. [13]. UOCTTI considers a compressed representation of HoG given by 31 dimensions. For each dataset, we use the HoG descriptors of a set of training patches to build codebooks using K-Means and LK-Means. In the case of LK-Means, we assign to each patch the label of the object class that generates the patch. We evaluate the discriminative properties of the resulting codebooks using them to train a category-object classifier. As a classifier, we use the popular linear Support Vector Machine (SVM), as in [8]. In relation to the training process, we use 15 random images for training and 15 images for testing. In order to evaluate the sensibility of our results in terms of the number of clusters, we consider the following number of codewords: K = { 50 , 100 , 150 , 200 , 250 } .

Table 8 shows the average accuracy achieved by the resulting classifier. These results are obtained using a 20-hold-out scheme and a fixed value of  X  =0 . 8 . We select this value of  X  extrapolating the results of the previous Section, and as a good compromise between the supervised and unsupervised terms in Eq. (3). In Table 8, we can observe that LK-Means outperforms K-Means in almost all cases; and in the few cases where K-Means shows superior performance the difference in accuracies is less than 1.0%. Furthermore, we observe that the positive difference in favor of LK-Means increases with the number of clusters, indicating that LK-Means benefits more that K-Means from a greater flexibility in the search for relevant patterns.

Finally, Fig. 2 shows some visual codewords resulting from the VEH-CALT dataset. We present the top-six most discriminative words according to the Fisher discriminant score [4]; and considering K = 200 for both algorithm. In Fig. 2, each visual codeword is represented by its four nearest patches. In Fig. 2, it is possible to observe in each row that, in general, LK-Means provides more discriminative codewords than K-Means. 5. Conclusions
In this paper we introduce LK-Means, an extension of the classical K-Means algorithm to the case of supervised clustering. As a main search strategy, LK-Means optimizes a convex combination of class de-pendent and non-class dependent cost functions. Experiments using a set of standard benchmark datasets and 4 different metrics to assess clustering quality, show that, on average, LK-Means outperforms clas-sical K-Means and SRIDCHR algorithms. In the cases of AMI and AVI metrics, in most of our tests LK-Means outperforms the alternative algorithms by a large margin. In the case of ARI metric, on aver-age, LK-Means also outperforms K-Means and SRIDCHR but by a narrower margin. In the case of MD metric, LK-Means and K-Means present mixed results. As it has been noticed in previous works, MD is negatively affected by clustering size, and it is in general less robust than metrics such as AMI and AVI. Additionally, we show an application of LK-Means as a codebook generator for object recognition applications. We consider several common benchmark datasets, and in all cases LK-Means outperforms K-Means, demonstrating the relevance of considering class information to find meaningful clusters.
Interestingly, our results indicate that the advantages of LK-Means over K-Means depends on the type of dataset. This is closely related to our hypothesis that homogeneity in class information leads to more informative clusters, which depend on the semantic of the data labels. For example, in the case of the object recognition application, where one expects a high correlation between clusters of visual features and object categories, the advantages of using LK-Means instead of regular K-Means are more clear. This observation offers a  X  X ule of thumb X  to set the value of the parameter  X  . For a dataset where it is expected a high correlation between class information and cluster composition  X  should be close to 1, increasing the relevance of class information.

In relation to time, our experiments show that LK-Means presents an attractive computational perfor-mance, being considerably faster than the alternative supervised clustering method considered in this work. In relation to future work, we plan to increase the reliability of the model by modifying the cost function of LK-Means to accommodate cluster shape. We also plan to extend this work to manage fuzzy labels inside of our model. Finally, we also plan to extend the idea behind LK-Means to the case of subspace clustering, which can provide a suitable extended search space to find relevant class-dependent clusters.
 References
