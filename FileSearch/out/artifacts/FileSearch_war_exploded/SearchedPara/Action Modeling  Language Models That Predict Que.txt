 We present a novel language modeling approach to capturing the query reformulation behavior of Web search users. Based on a framework that categorizes eight different types of  X  X ser moves X  (adding/removing query terms, etc.), we treat search sessions as sequence data and build n -gram language models to capture user behavior. We evaluated our models in a prediction task. The results suggest that useful patterns of activity can be extracted from user histories. Furthermore, by examining prediction performance under different order n -gram models, we gained insight into the amount of history/context that is associated with different types of user actions. Our work serves as the basis for more refined user models. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  query formulation, search process H.1.2 [ Models and Principles ]: User/Machine Systems  X  human factors. General Terms: Experimentation, Human Factors, Languages, Theory Keywords: Web search, user models, query modeling, query reformulation Information-seeking is a fundamentally iterative process. During search, user X  X  queries are generally a compromised expression of what they are looking for X  X  trade-off between a tacit information need and what the user knows about the solution sp ace [3]. Our analysis of AOL Search logs reveals that 28% of all search queries are reformulations of previous queries. Therefore, accurate models of search behavior will require a component for query reformulation [1,4]. Users X  query behaviors are linear in nature. As such, they can be modeled as sequence data using n -grams. We applied a simple language modeling technique to predict types of query changes. Our work tests a framework of user X  X  information-seeking behavior and can inform user models of interactive search. When a user issues a query against a collection of documents, the system returns a set of results and the user typically examines only some subset of those results (e.g., the top 10). When the examined subset does not contain enough relevant documents, the user usually modifies the query. The initial query and the reformulated queries that follow can be conceptualized as a sequence of signs intended by the user to signify what that user knows about the information search sp ace, and how the user is trying to change that knowledge. Beyond the semantics of the query terms, there is a syntagmatic relationship between two queries in a sequence; one query follows the other as a particular type of action to adjust the original expression. We constructed a typology of user actions, independent of the semantic content of the queries. We identified a set of eight  X  X ser moves X  that capture the generic ways Web users modify their queries. These can be seen as a vocabulary of move types: Similar taxonomies have been used elsewhere [5]. Naturally, semantic features must also be modeled, but they should be integrated into a higher level syntactic framework. Our work provides a first step toward that framework. We explore the use of n -gram models of query moves and demonstrate that they can account for much of the observed patterns of query behavior. We took a sample of 8.3 million queries issued by AOL Search users over a period of three months and labeled them with the  X  X ove types X  described above. We trained n -gram models on the sequences of moves within users X  query sessions. Users were randomly sorted into 10 groups. We trained our models on 8 groups of users at a time, held one group out, and evaluated our models using the remaining group. This  X  X eave one out X  approach reduces over-fitting and allows for parameter tuning in the future. We repeated our process 10 times to cross-validate our results. Models were evaluated by predicting next moves based on preceding moves. To assess the effect of using different am ounts of history/context we evaluated performance with n -gram models of varying length. We used the SRILM toolkit [2] to build Bayesian n -gram mixture models of move sequences. The models were smoothed using the Turing method and Katz back-off weighting. Thus each model was a mixture of n -grams of a maximum size and smaller order n -grams. For a baseline comparison, we also implemented a weighted random guess model. This baseline is essentially a context-free unigram model that  X  X uesses X  moves with a frequency distribution directly proportional to moves in the training set. We evaluated each n -gram mixture model for its ability to predict the next move in a series of moves, given the context of previous moves. From each of the 10 ra ndom groups we randomly selected 10,000 user sessions that contained 10 or more queries. From each of theses sessions we ra ndomly picked one of the moves in the session as a target for prediction. This gave us 10 evaluation test sets of 10,000 prediction targets each, for a total of 100,000 prediction targets. We then evaluated each test set against five language models: 2-gram, 3-gram, 4-gram, 5-gram and the unigram baseline. Using the sequence of moves leading up to each target move, we predicted what the next move (the target) would be based on the perplexity of the sequence of moves when compared to the each of the five language models. To quantify the accuracy of our prediction models we use traditional IR measures of precision and recall. In our case, precision is calculated as the number of times a move was correctly predicted, divided by the number of times it was predicted overall. Recall is calculated at the number of times a move was correctly predicted divided by the number of times that move should have been predicted. Results are presented in Tables 1 and 2. We compared the results of the four different context-based models to the results that would be obtained from the baseline. For all but two of the moves, our language modeling technique achieves results that are significantly better than the baseline (using a 2-tailed paired t-test,  X  =0.01). The gray cells in Tables 1 and 2 indicate the max values for each row; i.e., they indicate the best model for that move. Nearly all of the non-zero values for n -gram models X  precision and recall are statistically significant when compared to the random baseline. With only two exceptions (2-gram/ edit_longer and 3-gram/ return ) all of the n -gram models make consistent successful predictions for the moves repeat, new, edit_longer, return, and edit_same_length . 2-grams models successfully predicted edit_longer but with less consistency across the 10 folds of the cross validation. Predictions for add_to_prev are better than the baseline in two conditions (4-gram and 5-gram) but the difference is not statistically significant. The same is true of 3-gram models and the return move. The data shows that for most moves, a bi-gram model achieves the highest precision. For some moves, a trigram model is more accurate. For all eight moves, 4-and 5-gram models are overkill. Language modeling techniques are useful for predicting types of changes applied to web search queries. Our perplexity based prediction models outperformed a weighted random guess for six out of eight query move types. These results indicate that sequences of previous moves are predictive of the query changes users will make. The fact that shorter n -gram models out performed longer ones suggests that the predictive influence of preceding context is very local. For some move types, a little bit of context goes a long way toward prediction, but for most, adding a lot more context just adds noise. We anticipate that adding semantic features to the model(s) will improve performance. However, we also believe that syntagmatic abstractions such as query moves will be an important component in a larger framework. [1] Shen, X., Tan, B., and Zhai, C. Implicit user modeling for [2] Stolcke, A. SRILM X  X n extensible language modeling [3] Taylor, R. Question negotiation and information seeking in [4] Teevan, J., Dumais, S. T., and Horvitz, E. Personalizing search [5] Vakkari, P., Pennanen, M., and Serola, S. Changes of search 
