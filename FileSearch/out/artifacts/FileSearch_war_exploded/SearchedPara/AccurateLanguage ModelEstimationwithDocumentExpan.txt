 H.3.3 [ Inf ormation Sear ch and Retrie val ]: Retrie val models Information retrie val, algorithms Document expansion, language models, corpus structures, smooth-ing, pseudo feedback
Smoothing is a very critical factor in statistical language model-ing for information retrie val due to the insuf cient sampling prob-lem [4, 1]. In [3, 2], cluster information is exploited to essentially help better smooth a document model with promising results. Ho w-ever, although document clusters are intuiti vely helpful for aug-menting the data, the y are not optimal in the sense that the aug-mentation would all be biased toward the centroid of a cluster . We illustrate this problem in Figure 1(a). Clearly , a generati ve model estimated based on the cluster D is more suitable for smoothing documents close to the centroid, such as a , but is inaccurate for smoothing a document at the edge, such as d . To optimize sample augmentation for d , one would need a cluster centered at d itself, i.e. d 's direct neighborhood sho wn in Figure 1(b). In this work, we exploit such neighborhood information to augment the sample of each document, which we refer to as document expansion . The augmented document samples can lead to more accurate estimate of document models. Since neighborhood is a vague concept, we associate a condence value with every document in the collection to indicate our belief of how lik ely each document could have been sampled from the same underlying document model as a given doc-ument under consideration. The closer a document is to the given document, the more we would trust it. In this way, we establish a probabilistic neighborhood potentially consisting of all documents.
In this section, we present our document expansion models. We rst dene the follo wing notation: d is a document and its neigh-borhood is represented as N ( d ) ( d = 2 N ( d ) ). We use c ( w; d ) to represent the count of a term w (from vocab ulary V ) in doc-ument d . C is the collection(corpus), and document model and the collection model, respecti vely . Here, where is a parameter to control the balance between the original document d and its neighborhood N ( d ) . Intuiti vely , the optimal setting of would be some where between 0 and 1, but not exactly 0 or 1. We hypothesize that estimating ^ rate than based on d itself because d 0 potentially has more complete information.

Since the far away neighbors all have low condence values, their contrib ution to document expansion can be negle gible. Thus, to impro ve the efcienc y, we may compute d 0 using only the top M closest documents to d as long as M is suf ciently lar ge. In our experiments, we set M = 100 .

With the augmented document d 0 to replace the original docu-ment d , we may use any existing smoothing methods, such as JM or Dirichlet smoothing [4], to estimate a smoothed document lan-guage model ^ used to score document d using the query lik elihood method or any other language modeling retrie val method.
The data sets used in our experiments are sho wn in Table 1. We choose the rst four TREC data in order to compare our results with those reported in [3]. In addition, we use TREC8 and DOE data. TREC8 includes 528K documents and represents a relati vely lar ge data set. DOE is selected because it represents a collection with very short documents. We use both the non-interpolated average precision and precision at 10 for our evaluation.

We rst examine whether our document expansion method can indeed address the sampling insuf cienc y problem. We experi-mented with the two most popular smoothing methods, JM and Dirichlet. For each smoothing method, we use the original docu-ment with no neighborhood information as our baseline run, which is essentially the standard query lik elihood method. We compare this baseline with the corresponding document expansion run in Table 2.  X JM X  and  X Dirichlet X  indicate the baseline standard language models with JM smoothing and Dirichlet smoothing, respecti vely .  X DELM+ X  runs are the corresponding document expansion runs. For both baselines, we tune their parameters( for JM, and for Dirichlet) to the optimal setting. We use the same values for or in our model directly without further tuning. Note that this means our performance is probably not at the optimal point since and are possibly not at their best for our model. Ne vertheless, we can still clearly see that our models signicantly outperform their cor -responding baselines. It is very encouraging to see that the lar gest impro vement is more than 15% .

We then compare our model with the best method (CBDM) of [3]. To mak e the comparison fair , we use identical sources and queries 1 . Furthermore, we also set Dirichlet prior parameter = 1000 , as is done in [3], to rule out any potential inuence of Dirich-let smoothing.
We use exactly the same data, queries, stemming and all other preprocessing techniques. The baseline results in [3] are conrmed.
