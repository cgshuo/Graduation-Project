 One of the key tasks in natural language understanding (Hemphill et al., 1990a; He and Young, 2003; De Mori, 2007; Dinarelli et al., 2008; Wang et al., 2005) is slot tag-ging that labels user queries with semantic tags. It is a sequence labeling problem that transcribes a sequence of observations X =[ x ( 1 ) , x ( 2 ) ,..., x ( M )] to a sequence of the query  X  X how me flights from Seattle to Boston X , the words  X  X eattle X  and  X  X oston X  should be labeled, respec-tively, as the from-city-name slot and the to-city-name slot.

Recently recurrent neural networks ( RNN s) and their variants achieved state-of-the-art performances on slot tagging tasks (Yao et al., 2013; Yao et al., 2014b; Yao et al., 2014a; Graves, 2012; Shi et al., 2015; Mesnil et al., 2015; Peng and Yao, 2015). One direction to improve the sequence labeling is to strengthen the model memoriza-tion capability by designing dedicated special structures, for example, using long-short-term-memory ( LSTM ) net-works (Hochreiter and Schmidhuber, 1997; Graves et al., 2013; Yao et al., 2014a), gated RNN and RNN with ex-ternal memory ( RNN -em) (Peng and Yao, 2015). The other direction is to optimize the sequence-level discrim-ination criterion. For example, recurrent conditional ran-dom fields ( RCRF s) (Yao et al., 2014b) is trained to opti-mize the sequence conditional likelihood rather than min-imizing frame level cross-entropy applied in conventional RNN based sequence labeling (Prez-ortiz et al., 2001; Yao et al., 2013; Mikolov et al., 2010; Shi et al., 2015; Mesnil et al., 2015).

In this paper, we propose recurrent support vector ma-chines ( RSVM s) to improve the discrimination ability of RNN s. Different from RCRF s and conventional RNN s that in essence apply the multinomial logistic regression on the output layer, RSVM s optimize the sequence-level max-margin training criterion used by structured support vector machines (Tsochantaridis et al., 2005) on the out-put layer of RNN s. There are several advantages of using sequence-level max-margin training over maximum like-lihood or minimum cross-entropy. Firstly, the sequence-level max-margin criterion is a global un-normalized cri-terion in which there is no computation cost for normal-ization. Secondly, using max-margin training, only train-ing samples from support vectors generate non zero er-rors. In other words, model training can be sped up by skipping the weight updating for non-support vector training samples. Finally, as proven in (Vapnik, 1995), margin maximization is equivalent to minimization of an upper bound on the generalization errors. Max-margin training has no assumption about the model distribution. To use maximum likelihood or minimum cross-entropy, it assumes that the model distribution is peaked. How-ever, especially in natural language processing where the ambiguity is ubiquitous, this assumption does not hold. For example,  X  X even eleven X  can be labeled as time tag or place name (super market name) tag. The conditional probability of tag given  X  X even eleven X  should not be sharp for time or place name.

Recently, SVM is also applied on top of a deep neu-ral network for speech recognition (Zhang et al., 2015). In their work, a cutting-plane algorithm (Joachims et al., 2009) is used, which is computationally expensive for speech recognition tasks. In this paper, we use the stochastic gradient descent algorithm ( SGD ) (Pana-giotakopoulos and Tsampouka, 2013) for model train-ing. The loss function is critical to the sequence level max-margin training criterion, which defines the mar-gin. In this paper, we apply the sequence level hard loss function rather than traditional Hamming loss function (Nguyen and Guo, 2007). In sequence level hard loss function, the wrong sequence is assigned loss one with-out considering the number of wrong slot labels in the se-quence. In the experiments on two bench mark datasets, namely the ATIS (Airline Travel Information Systems) dataset (Hemphill et al., 1990b; Yao et al., 2014b) and the CoNLL 2000 Chunking dataset 1 , and private Cortana live log dataset, RSVM s outperformed previous results. In this section, we propose RSVM that uses the struc-tured SVM algorithm (Tsochantaridis et al., 2005) to es-timate the weights for RNN and label transition probabil-ities based on the entire training sequence. The training objective in RSVM is the following constrained optimiza-tion. represents the slot label sequence y ( k ) ( 1 : T ) for train-trix A , representing the weight for the slot label transition loss function of a possible slot label sequence for a train-separate the score f ( Y ( k ) ) for ground truth slot label se-quence with all other possible slot sequences in Eq. (1). quence that violates the margin constraint.

The constrained optimization problem can be trans-formed to an unconstrained optimization problem as where [ x ] + is the Hinge function that maps x to zero when x is smaller than zero, otherwise [ x ] + = x . The loss function L ( Y ( k ) ) is critical to the structured SVM training. The following two types of loss functions have been investigated: Eq. (6) is Hamming loss that is applied by (Nguyen and Guo, 2007) for structured SVM sequence labeling. Eq. (7) is sequence level hard loss function that always give loss one to wrong slot label sequences no matter how many words are labeled with wrong slot labels. In our experi-ment, we find that the margin defined by Eq. (7) gives the best performance. 2.1 Training Procedure For Recurrent Support Fig. 1 depicts the architecture of RSVM that can be viewed as the conventional RNN unrolled over the sequence x ( 1 : T ) . For each single training sample x ( 1 : T ) , a forward in-ference and a backward learning are carried out to sweep over the network shown in Fig. 1.

In the forward inference, an unnormalized slot score vector y ( t ) is computed based on each word input x ( t ) and its corresponding auxiliary feature Cx ( t ) . The word input and auxiliary feature are encoded in one-hot representa-tion. As shown in Fig. 1, a slot label lattice is generated for the training sample x ( 1 : T ) . Using Viterbi algorithm, rived from the lattice. In the decoding phase, only the best slot label sequence is computed.

In backward learning, the sub-gradient (Ratliff et al., 2007) are calculated to update the weights for RSVM s. sub-gradient is subgradient is zero. Our experiment show that the RSVM training can be substantially sped up by skipping the backward weight updating for non-support vector train-ing samples that obtain zero subgradient.
 In Eq. (8) and (9),  X  represents the weights in RSVM s. Specifically, the weights W , A , U and V are updated using sequence level mini-batch method. The weights O con-necting hidden layers are updated using Backpropagation Through Time ( BPTT ) (Werbos, 1990). 3.1 Data To evaluate performances of the proposed model, three sets of experiments were conducted. The first set of ex-periments are based on ATIS dataset (Hemphill et al., 1990b; Yao et al., 2014b). There are 893 queries from ATIS-III, Nov93 and Dec94 for testing, and 4978 utter-ances from the rest of ATIS-III and ATIS-II used for train-ing. The training data contains 127 unique slot tags.
The second dataset used in the experiment is CoNLL 2000 Chunking dataset. Chunking is also called shallow parsing that assigns syntactic labels to segments of a se-quence of words. Chunking and slot tagging are typi-cal sequence labeling problems. In this paper, we use the chunking task to further verify the performance of the proposed RSVM model. In the CoNLL 2000 Chunk-ing task, the training data are from sections 15-18 of WSJ data and the test data are from section 20. In the training data, there are 220663 tokens with 19123 unique words and additional 45 different types of Part-Of-Speech ( POS ).

The last dataset is Cortana live log dataset which is constituted by 8 domains, namely alarm, calender, communication, note, ondevice, places, reminder and weather. In total, there are 71 slots. There are 42506 queries used for training and 5290 queries for testing. The data distribution is described in Table. 1. The last column of Table. 1 shows the average query length (the number of words in one query) on different domains. 3.2 Settings In this paper, we use a predefined maximum iteration number to terminate the training. The learning rate is dy-namically adjusted using AdaGrad (Duchi et al., 2011).
In all the experiments, we set the hidden layer size to 300 and initial learning rate to 0 . 1. In RSVM s, the sur-rounding two words of the current word are used as auxil-iary feature which is represented as bag of words. We set the maximum iteration to 20 for ATIS and 30 for Chunk-ing and Cortana live log. For each dataset, we trained 10 models with the same parameter settings except using different random initialization. 3.3 Results on ATIS ATIS is a well studied benchmark dataset. Table. 2 gives the slot tagging F1 scores achieved by different models in the literature, using the same data settings. There are three blocks in Table. 2. The top block gives the F1 score obtained by CRF and simple RNN . The middle block gives the results obtained by applying advanced RNN architec-tures such as LSTM , Gated RNN and RNN with external memories ( RNN -em) (Peng and Yao, 2015). These ad-vanced RNN s improves RNN by enhancing its memory (sequence representation capability). The bottom block gives results using the proposed RSVM s.

The bottom part gives F1 scores of the proposed RSVM method. We show results generated by 10 models with different random seeds. The average F1 score of RSVM is similar to the best score of RNN -em. F1 score distri-bution of 10 RSVM models gets significant improvement over the average score of RNN -em (z-test p value = 0 . 0002 &lt;&lt; 0 . 05). Fundamentally, the proposed RSVM is based on simple RNN . Comparing LSTM and RNN -em, the proposed model has simpler topology. Note that in (Yao et al., 2014a) and (Peng and Yao, 2015), their advanced models are trained using local normalization method without using sequence level optimization. So the superiority of the proposed RSVM may come from the sequence training and the powerful discriminant ca-pability of SVM . Applying the proposed RSVM method to
LSTM or other advanced RNN can be a promising di-rection for future work. 3.4 Results on CoNLL 2000 Chunking Table. 3 gives the F1 scores of different models on CoNLL 2000 chunking experiment. To our best knowl-edge, the first neural network ( NN ) based chunking model is proposed in (Collobert et al., 2011). Using four basic natural language processing tasks, namely POS tagging, chunking, name entity recognition and semantic role la-beling, they demonstrate the ability of NN to discover hid-den representations. In their work, only simple input fea-ture is used. There is not any task-specific feature en-gineering work in their proposed system. Their model purely relies on the NN feature representation that are learned from large amount of unlabeled data. As shown in Table. 3, their system performs better than all the pre-vious systems on CoNLL 2000 chunking dataset.
 The performance of Bidirectional LSTM ( BLSTM ), RCRF and the proposed RSVM on chunking task fur-ther confirms the conclusion in (Collobert et al., 2011) that NN is able to discover the internal representations that are useful for different natural language processing tasks. Additionally, the results of BLSTM , RCRF and RSVM , indicate that RNN s have better capabilities to dis-cover the sequence representation than NN . The average F1 score of RCRF and RSVM are 94.9% and 95.0%, re-spectively. Comparing the F1 score distribution, RSVM achieves the significant improvement over RCRF (paired t-test p value = 0 . 012 &lt; 0 . 05). As shown in Table. 3, replacing the CRF objective function with structured SVM max-margin criterion could generate further improve-ment. The average performance of RSVM is better than the best result of RCRF shown in the table. 3.5 Results on Internal Live dataset In this section, we compare different slot models on dif-ferent domains based on Cortana live log data.
Table. 4 compares the F1 score on CRF , RNN , RCRF , joint-RNN and the proposed RSVM on alarm, calendar, communication, and note. Table. 5 presents the F1 score of different models on the rest domains.  X  RNN  X  denotes the Elman type of RNN for slot tagging which uses current word information, previous slot output information and context window information (surrounding four words) (Yao et al., 2013).  X  RCRF  X  represents the RCRF slot tag-ging models that use the same feature as  X  RNN  X  (Yao et al., 2014b).  X  X oint-RNN  X  (Shi et al., 2015) also uses the same features as  X  RNN  X  and  X  RCRF  X . However,  X  X oint-RNN  X  implicitly makes use of query domain, intent and slot information by training the domain classifier, intent classifier and slot labeling jointly via multi-task learning.
Overall, the proposed RSVM obtains significant im-provement over CRF , RNN , RCRF and joint-RNN on alarm, communication, note and reminder (z-test p value &lt; 5 E 5). On the calendar, places and weather, RSVM achieves similar performance as joint-RNN . Even joint-RNN is built on the basis of conventional RNN using local normalization, it actually takes the sequence representa-tion information implicitly from domain and intent clas-sification. However, in ondevice domain, RCRF performs the best and the proposed RSVM model performs even worse than CRF . We notice that, in ondevice model, user queries tend to be short, with on average 2.4 words in a query, shown in Table. 1. Also the loss function in the proposed model only uses the top and the second most hypothesis, which may be less informative, especially with short sentences, as compared to using all hypothe-sis in RCRF .
Table. 6 gives the overall performance comparison of different models in internal live log dataset using the weighted average F1 score over all domains. In this ta-ble, we can find that the proposed RSVM on average can achieve 0 . 6% and 0 . 7% F1 score improvement over joint-RNN and RCRF , respectively. 3.6 Training Speed Up In RSVM Using max-margin criteria, backward weight updating only happens to support vector samples. While using cross-entropy or maximum likelihood based training cri-teria, backward weight updating has to sweep over the whole training data. Fig. 2 shows that RSVM can substan-tially speed up the model training by skipping the back-ward weight updating for non-support vector samples. As depicted in Fig. 2, RSVM only executes backward weight updating for 337 training samples (7% of whole training data) at epoch 20. We have proposed a recurrent support vector machine (
RSVM ) which applies the structured SVM on top of the conventional RNN for slot tagging. Different from pre-vious RNN sequence training approaches that use max-imum conditional likelihood as objective function, the proposed method uses sequence level max-margin crite-rion with hard loss function. The model is trained to dis-criminate the score of ground-truth slot sequences with respect to other competing slot sequences by a margin. Viterbi algorithm is used in decoding to select a slot se-quence that gives the largest score. To verify the perfor-mance of the proposed method, three datasets, namely ATIS dataset, CoNLL 2000 Chunking dataset and Cor-tana live log dataset, were used. The proposed RSVM achieved a new state-of-the-art performances on these datasets. In addition, RSVM showed substantial training speed up by skipping the weight updating for non-support vector training samples. On ATIS data, after 20 epoches, backward weight updating only happened for almost 7% of whole training samples.
 The proposed RSVM is built on top of conventional RNN structure. Though RSVM doesn X  X  have advanced topology used in LSTM and RNN -em, it achieves com-parable or better performances. Therefore, the improve-ment comes from its sequence level max-margin crite-rion. For future works, we plan to apply the structured SVM on top of other advanced models.
