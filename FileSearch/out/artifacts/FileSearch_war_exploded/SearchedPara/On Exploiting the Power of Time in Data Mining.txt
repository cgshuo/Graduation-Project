 We introduce the new paradigm of Change Mining as data mining over a volatile, evolving world with the objective of understanding change. While there is much work on incremental mining and stream mining, both focussing on the adaptation of patterns to a changing data distribution, Change Mining concentrates on understanding the changes themselves. This includes detecting when change occurs in the population under observation, describing the change, predicting change and pro-acting towards it. We identify the main tasks of Change Mining and discuss to what ex-tent they are already present in related research areas. We elaborate on research results that can contribute to these tasks, giving a brief overview of the current state of the art and identifying open areas and challenges for the new research area. Data mining has traditionally concentrated on the analy-sis of a static world, in which data instances are collected, stored and analyzed to derive models and take decisions ac-cording to them. More recent research on stream mining has put forward the need to deal with data that cannot be col-lected and stored statically but must be analyzed on the fly. At the same time, the need to store, maintain, query and update models derived from the data has been recognized and advocated [30]. However, these are only two aspects of the dynamic world that must be analyzed with data mining: The world is changing and so do the accummulating data and, ultimately, the models derived from them. The chal-lenge does not only lay in adapting the models to the chang-ing world but also to analyze how the models change and when they do so. In this paper, we propose a new paradigm for data mining over the evolving world: Change Mining encompasses methods that capture the process of change, analyze how models have changed and predict changes that will emerge.
 The need for a paradigm shift is not only dictated by new applications that deal e.g. with streams of data [15] but is also incubent in traditional applications of business (e.g . Customer-Relationship-Management and fraud detection), medicine (e.g. healthcare and epidemiology), natural sci-ences (e.g. astronomy and meteorology) and many more. Taking an example from the business world, companies reg-ularly carry out surveys to analyze the relation between past experiences of customers and their attitude towards the business. Association rule discovery is commonly used in this scenario, usually accompanied by clustering or clas -sification methods for the establishment of customer seg-ments, upon which decision makers design the segment-specific products, marketing actions or recommendations. Nowadays, this course of action is a quite straightforward business intelligence task. Yet it suffers from several draw -backs: (i) Since the datasets used to this purpose are very large and grow at fast pace, decision makers are often forced to refrain from powerful but poorly scaling data mining methods or to perform data sampling, thus risking the con-struction of a non-representative data subset. (ii) Some methods such as association rule discovery almost always re -turn a vast number of rules, many of which are obvious and already known. It requires a strenuous manual inspection to detect those few rules which are of interest because they were not known before. (iii) The segments built by clus-tering or derived by classification are affected by external factors, such as demographics, but also by the production and the marketing actions of the company itself and of its competitors. So, before exploiting models that may have become obsolete, decision makers need to learn how their earlier actions may have affected/changed them.
 All these shortcomings can be alleviated by taking a time-oriented perspective and placing the understanding of changes in the centre of knowledge discovery: A mechanism that re-members old association rules, compares them to new ones and highlights those that have emerged or have changed sig-nificantly guarantees the distinction between already know n and new -interesting or interestingly changed ones. The study of the changes occurring upon customer segments in regular intervals or after the launching of marketing actio ns provides insights on how customers X  behaviour is affected by the actions and sets the basis for pro-active marketing ac-tivites and for the prediction of further segment evolution . The study of change does not require the analysis of whole datasets, nor sampling over them. In fact, this would be counterproductive since the underlying population is all b ut stationary.
 In this paper, we discuss Change Mining as a paradigm that encompasses mechanisms that monitor models and patterns over time, compare them, detect changes and quantify them on their interestingness. In Section 2 we define the notion of Change Mining and elaborate on the methodology as-sociated with this paradigm. In Section 3 we show that Change Mining is already present though implicit in mod-ern research and we discuss associated research fields. Sec-tion 4 describes the tasks of Change Mining and elaborates on research studies that have contributed to each task. In the last Section we present a research agenda for Change Mining as a new way of thinking about data mining. A terminological note: Here and in the following, we use the term  X  X odel X  for both models and patterns, notwithstanding that a pattern, differently from a model, describes only part of the data. Hence, we term classifiers, clusterers and sets of association rules over a dataset as models. When we refer to individual clusters, to single association rules or deci sion tree nodes that describe only part of the data population we use the terms model component or local model . Conventional data mining methods observe one dataset (or data stream) and learn (local) models upon it. In the Change Mining paradigm, we generalize into a temporal sequence of datasets , for which we want to derive the changes effected upon their models during the elapsed time.

Definition 1 (Change Mining). Change Mining is a data mining paradigm for the study of time-associated data. Its objective is the discovery, modelling monitoring and in -terpretation of changes in the models that describe an evolv -ing population.
 Change Mining is thus a subdomain of Higher Order Min-ing [37], which encompasses methods for the discovery of knowledge by processing models (instead of data), such as meta-learning, model adaptation, model comparison, tem-poral mining, mining models (e.g. clustering of associatio n rules) and Change Mining  X  the discovery of changes in evolving models. Formally, let T = &lt; t 0 , . . . , t n &gt; be a sequence of timepoints and let D i be the dataset accummulated during the interval ( t i  X  1 , t i ], where D i may be a static dataset, whose records do not possess timestamps themselves, or a stream of records. Furthermore, let f () be a decay function which determines which data contribute in the learning process and with which weights. For example, f () can express a sliding window of length w , so that all data in the interval [ t 0 , t i  X  w ] are forgotten, or f () may be an exponential function of ageing, which assigns weights to the individual records on the basis of their age.
 Hence, similarly to conventional data mining, Change Min-ing has a descriptive and a predictive subcategory of algo-rithms. The description of changes among models involves two core tasks. First, it must be decided whether models are indeed different and their differences must be quantified. Second, their differences must be described semantically and inter-preted. While there is much work on identifying differences between two models, there is less work on the process of monitoring differences in a sequence of models and also less work on the semantic interpretation of such differences. The prediction of changes in a sequence of models implies building a higher order model, which will decide whether the next member of the sequence will be different from the mem-bers seen thus far. A more elaborate aspect of change pre-diction involves describing in what aspects the next model will be different from already seen ones. In this context, change prediction involves the description of model changes , as mentioned above.
 In general, the spectrum of Change Mining approaches to be researched is vast: For each type of model used in con-ventional data mining there is a manifold of ways to analyse change. We propose four generic tasks that constitute a methodological process for Change Mining: 1. Determining the goals of Change Mining: 2. Specifying a model of time: 3. Specifying the objects of change: 4. Designing a monitoring mechanism: The process formed by these tasks partially reflects the tra-ditional process of data mining, paying emphasis on the specification of the problem to be solved, i.e. the goal and the objects of change and the types of change that are of interest, and on the appropriate modeling of the data, i.e. the models and their changes.
 Similarly to conventional data mining, Change Mining is fol -lowed by the task of identifying suitable actions to be taken in response to the discovered changes. This task depends strongly on the application domain and thus lacks the gen-erality of the other tasks which we will discuss in more detai l in Section 4. We nonetheless discuss the nature of this task in the example hereafter.
 Example: (Change Mining over a Web Log) . In con-ventional Web usage mining, we derive models that reflect user behaviour and preferences. Assume an online shop that acquires monthly reports from their web hoster. Further, as -sume that the web hoster does not simply deliver rudimen-tary statistics but also association rules on products acce ssed together, as well as clusters of users with similar purchase preferences.
 Task 1 of Change Mining: Example objectives for the on-line shop are (a) the discovery and description of differ-ences among the purchases of adjacent and/or non-adjacent months, (b) the discovery and description of the evolution o f the user clusters over the last few months and (c) the pre-diction of rules that are expected to hold in the next month, given the models seen thus far.
 Web hosters deliver activity reports in regular intervals, e.g. months. Then, in terms of Task 2 , the time axis is parti-tioned in monthly intervals, i.e. ( t i  X  1 , t i ] is a month and t denotes the last reported day of the month. At the end of each month t i , the web hoster delivers a clustering of users  X  i and a set of association rules R i . Both the reports and the clustering have been derived on dataset c D i . If the web hoster has been instructed to build the models upon all data, i.e. from the moment when hosting started, then f () is the identity function and no data are forgotten. If the web hoster deletes the web server log every two months, then f is a sliding window of 2 , i.e. two months.
 Task 3: The objects of change are individual clusters of users with similar preferences and individual association rules on items being purchased together.
 Task 4: According to the objectives in Task 1 , the expert responsible for Change Mining must specify in what ways a cluster of preferences or an association rule has changed an d which of those changes are of interest. Mechanisms for these tasks are described in Section 4. The Change Mining paradigm, as defined in Section 2, fo-cusses on change in the models or patterns that describe the evolution of data over time. The ultimate objective is the discovery and the understanding of changes. The study of temporal data is a mature field with a veritable amount of valuable findings, but there are also further research ar-eas with a close relationship to Change Mining. We provide a brief overview of these research areas here and determine the scope of Change Mining within and beyond them. Many methods in these areas refer explicitly to patterns, so we us e the term  X  X atterns X  instead of  X  X odels X  here. Incremental mining methods are designed for the updating of patterns as data are inserted, modified or deleted  X  promi-nently in a data warehouse. This implies dealing with a very large initial dataset  X , which should be updated with a batch of insertions  X  + or deletions  X   X  . The bottleneck of pattern actualization is the size of  X . Accordingly, the goa l of incremental methods is to update the patterns in a fast and reliable way, minimizing the accesses to  X .
 Pattern updating may involve the detection of changes in the original patterns. For example, the incremental densit y-based clustering algorithm IncrementalDBSCAN [17] iden-tifies changes in the neighbourhood of newly inserted or of deleted data points, such as a cluster split or dissolution o r the emerging of a bridge that connects two initially discon-nected regions (so called: neighbourhoods of data points) into the same cluster. However, the objective is the efficient updating of the clusters with a minimal number of opera-tions, rather than the capturing and understanding of pat-tern change. For example, the insertion of a data point may cause a cluster to grow, the insertion of the next data point may cause the cluster to merge with another cluster, while the insertion of a third point may cause growth of a different region than the first one. The only information considered by IncrementalDBSCAN in association to these changes is whether the set of cluster members needs to be updated or not. Whether or not a cluster merger is incidentally caused by some noise data and therefore should not be reported as a change in the underlying population is beyond the scope of such algorithms.
 A further characteristic of incremental mining methods tha t disagrees with the nature of Change Mining is the treatment of old patterns. An incremental miner updates a pattern by overwriting its old state. It is obviously trivial to store t he old patterns instead of replacing them. However, as men-tioned already, the understanding of the differences betwee n an old and a new pattern is beyond the scope of an incre-mental miner. A substantial body of research has been devoted to the de-tection of differences between two datasets. In most cases, the objective is to decide whether the two datasets belong to the same population or, equivalently, have been generated by the same distribution. Some methods of this category like FOCUS [18] and PANDA [6] provide both a qualitative and quantitative description of the differences between two datasets or between two derived models.
 The discovery of differences between two datasets is also studied in the field of so-called emerging patterns : Goal of these methods is to discover patterns, in particular itemse ts, whose support significantly differs between the two datasets [16; 44].
 Very recently, Liu and Tuzhilin stressed the need to store, query and analyze models, discover models that underper-form and figure out whether models are missing and should be built [30]. They use the term modelbase for the venue where models are maintained and propose a first approach for the identification of underperforming models. The theoretical underpinnings of such methods are obviousl y relevant for Change Mining, although the methods them-selves lack the temporal aspect, which we consider funda-mental for Change Mining. On the other hand, combina-tions of these methods with a utility that deals with tem-poral data do belong to the field of Change Mining. For ex-ample, FOCUS can be combined with the module DEMON [20] developed by the same research group: DEMON mon-itors evolving data, while FOCUS measures the differences between patterns captured at different timepoints. As a fur-ther example, one of the followup frameworks of PANDA [6], the PSYCHO framework [32], has utilities for the treatment of temporal data and for the detection of changes in patterns -prominently of association rules. Novelty detection is a special area of Change Mining. The objective of novelty detection methods is to decide whether newly arriving data instances agree (are represented by) an existing model or deviate strongly from it. This issue has been studied among else in [21; 24; 31]. Novelty detection methods start with an existing model, which is assumed to be representative of the population and may have been derived from historical data. They detect events , i.e. new data instances, which do not fit the model or even invalidate it. These events can signal something not seen before, hence the term  X  X ovelty X . In general, the focus is on detecting deviations from the model, i.e. abnormalities.
 Change Mining encompasses more than novelty detection or abnormality detection. First, Change Mining also covers advances on the monitoring of an evolving population and the identification of changes in individual models or model components, e.g. in an association rule or a cluster; a com-plete model is not imperative. Second, Change Mining takes a more long-term perspective over the data and includes methods for the regular comparison of derived models and the discovery of trends in their changes (e.g. topic evolution, community dynamics). In this section, we investigate the steps of the methodologi -cal process of section 2 in greater detail. As pointed out at the beginning of Section 2.2, the objective of Change Mining may be to capture and semantically inter-pret changes, i.e. change description , or the establishment of a higher order model that foresees the changes to come, i.e. change prediction . Change prediction obviously builds upon change description.
 There are two possible notions of change. Both have their own relevance and both should be accounted for by Change Mining: change may denote the process of change , such as the evolution of a customer segment and its responsiveness to a marketing strategy that must be aligned again and again to keep it profitable, and the outcome of change , such as a sales collapse for a particular product in a certain custome r segment.
 These notions of change are associated with two intuitive questions in Change Mining:  X  X ow is the world changing? X  and  X  X hen did/will the change occur? X  The answer to the first question refers to the evolution of a model across sever al timepoints t i , . . . , t j . It describes the nature of the change, such as the shrinking of a cluster or the increase in the sup-port or confidence of an association rule, it captures the properties that have changed and quantifies the importance of the changes, and, ultimately results in a model of change. This higher order model can be a descriptive one, corre-sponding to the objective of change description or be used for proactive change prediction . The second question on the timepoint of change is important for the study of the past ( X  X hen did a change occur? X ) and no less for the prediction of the future, i.e. for foreseeing when change as outcome will occur and what it will look like.
 The nature and frequency of change as well as the distribu-tion of time points t 0 , . . . , t n has an effect on which of the two objectives should be put forward. In a slowly evolv-ing domain, transitions from model  X  i to  X  j are expected to be smooth. Next to describing them, prediction of the next change and its impact can be attempted. Contrary to it, if data are captured sparsely and sudden changes are ob-served, then change description may be the sole accomplish-able task. Inducing forthcoming changes from the changes seen thus far may be less appropriate at first. As with pre-diction in conventional KDD, the description of the data is expected to provide clues about a predictive model at the long term.
 Change description is directly or indirectly practised in a ll methods that study change in models or patterns: Agrawal et al use queries to capture differences between rules seen at different timepoints [4], while Liu et al study capture the interestingness of rules that change [12; 29], next to their support and confidence [29; 5; 8]. Cluster evolution methods first start by describing the types of cluster change that may be observed (cf. Section 4.4 below), including the distinct ion between clusters and background in a noisy environment [33; 22].
 Change prediction is practised less often [33]. Quite intu-itively, it presumes that change occurs smoothly rather tha n abruptly: Aggarwal measures the velocity of change in clus-ters [1]; evolving clusters are detected against a stationa ry background in [22]. A stationary background is also as-sumed in [33], where a HMM is designed for prediction over a document stream. Change Mining assumes a partitioning of the time axis T = &lt; t , . . . , t n &gt; , such that a model  X  i is derived at each t This model can be discovered by a conventional static min-ing method that is applied on the data seen thus far and possibly weighted with help of a decay function f (), e.g. of a sliding window. Alternatively,  X  i may be the result of adaptation, as is done by stream mining methods which adopt the previous model  X  i  X  1 to the dataset D i arriving in ( t i  X  1 , t i ] while forgetting older records, again by means of a decay function. In all cases, the model observed at t depends on the partitioning of the time axis.
 Several aspects must be considered by the specification of the partitioning T = &lt; t 0 , . . . , t n &gt; . On the one hand, a long period leads to large datasets and thus enhances the reliability of the estimated model. However, long periods imply a coarse-grain partitioning, in which interesting sh ort-duration changes might be overseen and the precise location of change on the temporal axis gets blurred. On the other hand, short periods force a more frequent re-learning of the model, implying that the model may become less robust. This makes model evaluation more difficult, because the dis-tinction between interesting models and incidental noise g ets more challenging [22].
 To our knowledge, the approach of Chakrabarti et al [13] on change discovery upon association rules is the only one which computes the timepoints t 0 , . . . , t n . Rather than as-suming pre-defined time periods, Chakrabarti et al deter-mine the best partitioning for each itemset , so that the cor-relation between its constituting items is maximally homo-geneous within each partition and inhomogeneous between partitions. The method is inspired by the Minimum Descrip-tion Length principle: The authors define a binary coding scheme for the time partitions, so that the code length de-creases as homegeneity increases. Then, for each itemset they choose the partitioning that minimizes the sum of the code lengths of the partitions. An elegant aspect of this approach is that the code length also reflects volatility  X  a potential measure of interestingness: Large code lengths i n-dicate higher volatility of the correlation among the items and thus a potentially more interesting itemset.
 The disadvantage of the method in [13] is that it derives one partitioning per itemset . Beyond the scalability restric-tions thus implied, a juxtaposition of the evolution of diffe r-ent rules (e.g. overlapping ones) is more complicated. This disadvantage holds also for traditional methods applied on time series to distinguish between signal and noise, such as Fourier and wavelet transforms. To make the task even more challenging, Change Mining may require the study of multi-ple properties of the same rule (e.g. confidence, support and lift) or cluster (e.g. cardinality, intra-cluster distanc e and centroid position), as well as the study of the evolution of properties of multiple objects, e.g. the pairs of inter-clu ster distances within a clustering. Hence, single ideal partiti on-ing of the time axis is desirable but is not to be expected in this context.
 More pragmatic approaches are often feasible though. For example, many applications are inherently designed around regular time intervals like days, months or years: Customer surveys in marketing are often conducted regularly, so mode l evolution can be designed across the same intervals. An-other option is to fix the size of the dataset D i studied at each timepoint t i into a constant c . This is appropriate for applications like the mining of rapid streams that operate o n buffers of given size. Datasets of fixed size also imply that model perturbations caused by variations in the dataset siz e cannot occur and thus require no corrective preprocessing. The objects studied in change mining are themselves models in the traditional sense, i.e. a  X  i , i = 1 . . . n may be a clas-sifier (e.g. a decision tree, a neural network, or a support vector machine), a set of clusters (a  X  X lustering X ) or a set o f association rules.
 One crucial decision in Change Mining refers to whether a model is observed as a monolithic black box that gives no further insights or as the composite of individual submod-els. Intuitively, the monolithic approach makes sense when the model cannot be decomposed into interpretable compo-nents, as e.g. for a neural network, where a decomposition is possible but a neuron-by-neuron interpretation is not. The compositional approach is more appropriate when the com-ponents of the model are themselves meaningful and of po-tential interest when studied separately and in relation to each other, as holds e.g. for individual rules or clusters. Under the monolithic approach, the object of study for chang e mining is the model as an atomic, non-decomposable entity. Change Mining involves then monitoring the model X  X  evo-lution over time and the detection, quantification and in-terpretation of differences between models encountered at different timepoints.
 Figure 1: Deriving time series for support and confidence of association rules Change Mining methods adhering to the monolithic approach devise measures that quantify and assess the model X  X  be-haviour or performance at each timepoint. For example, Rissland et al propose the structural-instability metric [36] for decision trees: Two decision trees  X  i  X  1 and  X  i , built at t i  X  1 , t i respectively, are juxtaposed by pairwise comparing their tree nodes and counting (and weighting) (mis)matches . Yang et al propose the conceptual equivalence metric , used to decide whether the model presently observed is equivalen t to any previously seen model: The equivalence is computed as the count of agreements between the two models when labeling a testset.
 In general, the quantification and comparison of the be-haviour of two models can be based on measures of the models X  predictive power towards a testset, e.g. by monitor -ing the accuracy, F-measure, precision &amp; recall of a predic-tive model. Alternatively, Change Mining can concentrate on internal validity measures on homogeneity, separation o r stability. Such measures are traditionally used for the eva l-uation of descriptive models [41]. Since they usually range over a continuous interval of real numbers like [0 , 1], the evolution of the model under study can be mapped to the timeseries of each measure X  X  values, as shown in Figure 1 for the case of an association rule. Changes may then be peaks or significant perturbations across the time axis, indicati ng changes in the analyzed population. Such approaches con-centrate mainly on detecting change rather than describing the change in greater detail.
 The monolithic approach lends itself quite intuitively for models that are hard to interpret, such as neural networks or SVMs. Other models, such as decision trees, can be de-composed easily into interpretable components; for exampl e, a decision tree can be decomposed into a set of overlapping decision rules. For such models, it is appealing to consider monitoring the components rather than the whole model. However, the underlying graph theoretic problem to solve, like the inexact graph matching, common subtree and tree editing distance problem, are known to be NP-hard and hard to approximate [7; 43]. Another difficulty is the instability of many learning algorithms: Small changes in input train-ing samples may cause dramatic changes in the produced models. For instance, multi-layer perceptrons and decisio n trees are known to be unstable [10; 26]. This means that even if the underlying population does not change at all we may observe large changes in the model X  X  structure. Over-coming these difficulties with appropriate methods is still a topic of ongoing research.
 Under the compositional approach, the objects of study for Change Mining are the components that jointly constitute the global model. Such components may be the decision rules or the tree nodes in a decision tree classifier, individ -ual clusters of a clustering or individual rules found by as-sociation rules discovery. The objective of Change Mining is to study the evolution of local models over time, to de-tect, quantify, localize and predict changes among such loc al models.
 Under Change Mining for clustering we find approaches that detect the emergence of new clusters and the decay/dissipat ion of old ones, the shrinking or expansion of a given cluster in size or with respect to some homogeneity indicator or the movement of a cluster in a topological space (cf. among else [1; 23; 40]), as well as methods that study the relationship among multiple clusters such as cluster split or merge (cf. [17; 33; 40]). In the context of association rules discovery , there are methods that compute the validity interval of in-dividual association rules [35] and methods that focus on (interesting) changes in the statistics of individual rule s (cf. [12; 28; 5]). The comparison of classifier components in-volves comparison of their statistics and of their properti es, i.e. (attribute, value)-pairs, as is done in the FOCUS frame -work of [18]. Kim et al [25] propose a method based on sim-ilarity and difference measures for discovering three types of changes from rules derived from a decision tree: the emerg-ing model, the unexpected change and the added/perished rule.
 Quite naturally, Change Mining for local models involves the study of the local model X  X  statistical properties, such as the cardinality, homogeneity or distribution (mean, sta n-dard deviation, curtosis etc) of a cluster, the accuracy of a decision tree node or a decision rule and the support, confi-dence or lift of an association rule.
 Under the compositional approach we find methods that deal with the object of change (the local model) in fun-damentally different ways: Aggarwal proposes a dedicated method that both builds and monitors evolving clusters in a spatiotemporal domain [1]. Other methods take as input the models built by an independent, conventional knowledge discovery algorithm [27; 5; 40; 22], sometimes posing certa in requirements on the statistics made available for the local models [18]. Frameworks for the comparison of models and their components [6; 32] also delegate the process of model building to an external independent algorithm. Incremen-tal methods [17; 34] and stream miners stand between these ends by adapting a dedicated method with change detection. Many advances on stream mining use change detection to adapt local models to changes in the population [2; 34; 42; 11; 45].
 With respect to interpretability, the compositional appro ach seems advantageous over the monolithic one, since the hu-man expert can understand change in the context of the application domain, e.g. retail marketing [14], exception de-tection [5] and customer segmentation [9]. However, the compositional approach is more demanding: The model of study must be interpretable itself and decomposable. If thi s is guaranteed, then different types of change can be be moni-tored and studied, as proposed e.g. in the change taxonomies of [18; 40]. Next to the types of change in statistics of ob-jects, in their interplay with other objects (e.g. cluster s plit or rule merge) and the evolution in structure of the objects themselves (e.g. cluster movement), it is important to cap-ture also the case where no changes have occured at all. In fact, one aspect of change mining is the distinction between evolution/change and stability of the underlying populati on [36; 28; 22]. According to the change mining methodology outlined in subsection 2.2, once the time axis is properly partitioned and the objects of change are defined, mechanisms are due for monitoring the models or their components. They should encompass discovery and interpretation of change and may be descriptive or predictive in nature.
 The discovery of change in models is more challenging for the compositional than for the monolithic approach because it involves tracing the components (the local models of Sec-tion 4.3.2 ) across time. In particular, if a model  X  i discov-ered at timepoint t i contains a component X , change mining involves tracing X in the model  X  i +1 built at the next time-point and deciding that X has disappeared if it cannot be matched to any of the components in  X  i +1 .
 For some types of model  X  i and for some forms of decom-position, tracing a component X  X   X  i may be trivial. For example, association rules have an unambiguous symbolic description, so that tracing e.g. a rule X = ( A  X  B ) in a later model  X  i +1 is a simple task that can be solved on the syntactic level. Once this task is solved, comparisons on th e statistics of the old and the new rule can easily take place. In other cases, the local models may have a more ambigu-ous symbolic description that makes tracing a challenge: As pointed out in [40], a cluster X (as component of a cluster-ing  X  i ) may be observed as a distribution, a formation/area in a feature space or as a set of objects. If it is set that a cluster is a distribution, then it must be decided to what extend a change of the distribution X  X  parameters is tolerat ed as inherent to the same cluster . For example, if cluster X has a mean  X  and if we find a cluster Y  X   X  i +1 with mean  X   X  , how close should  X ,  X   X  be to each other to decide that Y is the same as X ? Solutions to the challenge of tracing local model are of two types: First, one may define a match -function that returns the closest approximate to a given local model X according to the local model X  X  definition. One may thus specify that two clusters X, Y defined as distributions are the same if their means are within  X  1 of each other and their standard deviations with  X  2 of each other. Similarly, if X, Y are de-fined as sets of objects, the match -function may determine that they should share at least n % objects to be consid-ered the same cluster (as e.g. in [23; 40; 22]). If X, Y are dense areas in a feature space, then they may be expected to overlap for more than  X  to be considered identical (as e.g. in [1]). Examples of such match -functions for clusters can be found in [40], while model comparison frameworks like FOCUS [18], PANDA [6] and PSYCHO [32] provide both example match -functions and generic mechanisms for build-ing user-defined ones.
 The second solution to the matching challenge is the asso-ciation of each local model with a unique identifier. Quite intuitively, this approach is taken in incremental data min -ing, where each component of the model is adjusted to the arriving new records and to the deleted/forgotten ones. For example, IncrementalDBSCAN checks whether a cluster has lost some of its dense areas (called: neighbourhoods) after data record deletion and may thus be split to more than one clusters or even disappear altogether [17].
 The task of tracing a local model, i.e. a component of a model is interwoven with the task of identifying the changes that may have occurred upon it. Most change mining al-gorithms have an embedded set of changes or transitions that they can trace. For example, Kalnis et al trace loca-tion shifts in clusters over a feature space [23], while Ag-garwal X  X  mechanism traces additionally contractions, dis -sipations and growth of individual clusters and can fur-ther capture the velocity of change across different dimen-sions/features [1]. Additionally, cluster splits and merg es can be traced e.g. by the mechanisms of [17; 40]. Moreover, there are mechanisms that map a local model to a condensed representation (a summary) and focus on transitions of this summary, including disappearance, split or merge [19; 33; 3].
 The monitoring of change encompasses also the interpreta-tion of change. Methods that map local models into sum-maries and associating such a summary with interpretable semantics do a great step in this direction. For example, cluster evolution in text stream clustering can be mapped to the evolution of human-understandable topics (cluster sum -maries as weighted vectors of terms), as done among else in [33; 38; 39]. Then, change monitoring can be used to de-scribe change of existing topics and the emergence of new ones [38; 39] or to predict further changes [33]. In many cases, change interpretation is greatly assisted by pictorial or even visual representations. For example, the naming scheme used for the cluster transitions captured in [1; 23] do an excellent service in helping the human expert into a pictorial understanding of change: A cluster may  X  X ove X ,  X  X row X ,  X  X hrink X ,  X  X isappear X  etc. As another ex-ample, the framework FOCUS incorporates a representation of changes in the nodes of a decision tree classifier [18] whic h can be visualized in an intutive way.
 Change interpretation incurs particular challenges in the context of association rules X  discovery. As already men-tioned, matching of individual rules across different time-points is trivial. However, rules may overlap and the change s encountered for one rule may lead to a cascade of changes upon other rules. Liu et al attempt to solve this issue by detecting so-called  X  X undamental changes X  in a set of assoc i-ation rules, i.e. changes that are responsible for all chang es seen in the set [28].
 A final challenge associated with change monitoring and in-terpretation is the interplay between change in the popula-tion and stability of the mining algorithm used to discover the models (cf. [22]). Some algorithms are very sensitive to small variations of the dataset, among them decision tree classifiers. Others are sensitive to outliers, among them K-Means for clustering. Many algorithms are sensitive to in-herent properties of the dataset such as the existence of clu s-ter formations of particular shape. Further, almost all alg o-rithms are sensitive to the selection of the values for their initialization parameters, including random settings (li ke the initialization seeds for center-based partitioning metho ds in clustering). For this reason, for any type of change, there is still the open issue of distinguishing between real change in the data and change inherent to the instability of a poor model. We have presented change mining , a new perspective on knowledge discovery upon data, which brings forward the volatility and the dynamics of real world applications. The object of study in change mining are models and patterns learned from a non-stationary population. Its objective is to detect and analyze when and how changes occur, including the quantification, interpretation and prediction of chang e. Change mining builds upon research advances on incremen-tal mining, temporal mining, novelty detection, stream min -ing, pattern maintenance and pattern evolution monitoring . Much research work in all but the last two fields concen-trates on the adaptation of patterns and models as the data generating process changes. The capturing of the changes themselves is sometimes a by-product of the adaptive pro-cess, the interpretation and tracing of change is beyond the scope of adaptive methods. In contrast, change mining ob-serves change as an inherent and indispensable aspect of dynamic environments. Accordingly, it encompasses meth-ods that model and trace patterns across time, detect and quantify changes and provide the underpinnings for change interpretation and prediction.
 In this study, we have proposed a fundamental framework for change mining. We have specified an abstract method-ological process, described the building blocks for this pr o-cess and identified exemplary research advances that can be used to implement these blocks.
 Despite the wealth of research results that can be used for change mining, a fair amount of research effort is required in this new field, mostly due to the demand of studying evolving patterns rather than static data. In the previous sections, we have identified the need to model patterns as objects across the time axis and to devise mechanisms for monitoring them. We have stressed the difference between studying the evolution of a model (e.g. a classifier) as a whole and studying how a single cluster or classification rul e changes within a global model. We have brought forward the necessity of designing methods and measures for capturing, quantifying and predicting change. We have finally stressed the need for distinguishing between real change and artefac t, where artefacts can be caused by noise in the data or by instability and evolution of the data mining process itself . A major objective of knowledge discovery from data is the understanding and prediction of the population which gen-erates the data. Institutions pursue this objective by accu m-mulating data in an ever increasing pace. Data proliferatio n makes the need for maintaining patterns rather than the data themselves more and more pressing. This implies that knowledge discovery from data should extend to knowledge discovery from patterns , of which change mining is a central component.
 Change Mining aims at identifying changes in an evolving domain by analyzing how models and patterns change. How-ever, often the data mining process evolves too and the ef-fects of this evolution on a model superimpose those of the underlying domain. Two reasons why the data mining pro-cess evolves can be frequently encountered: Firstly, many businesses steadily improving the quality of gathered data . Secondly, the parameters that drive the data mining algo-rithms are adjusted and tweaked over time. With no doubt, such interventions into the data mining process are impor-tant, necessary and useful. Still, they raise the challenge of seperating changes imposed by an evolving domain from those of an evolving data mining process. [1] C. Aggarwal. On change diagnosis in evolving data [2] C. Aggarwal, J. Han, J. Wang, and P. Yu. A framework [3] C. C. Aggarwal and P. S. Yu. A Framework for Clus-[4] R. Agrawal and G. Psaila. Active data mining. In [5] S. Baron, M. Spiliopoulou, and O. G  X unther. Efficient [6] I. Bartolini, P. Ciaccia, I. Ntoutsi, M. Patella, and [7] P. Bille. A survey on tree edit distance and related prob-[8] M. Boettcher, D. Nauck, D. Ruta, and M. Spott. To-[9] M. Boettcher, D. Nauck, D. Ruta, and M. Spott. A [10] L. Breiman. The heuristics of instability in model se-[11] F. Cao, M. Ester, W. Qian, and A. Zhou. Density-Based [12] S. Chakrabarti, S. Sarawagi, and B. Dom. Mining Sur-[13] S. Chakrabarti, S. Sarawagi, and B. Dom. Mining sur-[14] M.-C. Chen, A.-L. Chiu, and H.-H. Chang. Mining [15] G. Dong, J. Han, and L. Lakshmanan. Online min-[16] G. Dong and J. Li. Efficient mining of emerging pat-[17] M. Ester, H.-P. Kriegel, J. Sander, M. Wimmer, and [18] V. Ganti, J. Gehrke, and R. Ramakrishnan. A Frame-[19] V. Ganti, J. Gehrke, and R. Ramakrishnan. CACTUS: [20] V. Ganti, J. Gehrke, and R. Ramakrishnan. DEMON: [21] V. Guralnik and J. Srivastava. Event detection from [22] F. H  X oppner and M. B  X ottcher. Matching partitions over [23] P. Kalnis, N. Mamoulis, and S. Bakiras. On Discovering [24] E. Keogh, S. Lonardi, and B. Y. chi X  Chiu. Finding [25] J. K. Kim, H. S. Song, T. S. Kim, and H. K. Kim. [26] R.-H. Li and G. G. Belford. Instability of decision tree [27] B. Liu, W. Hsu, H.-S. Han, and Y. Xia. Mining changes [28] B. Liu, W. Hsu, and Y. Ma. Discovering the set of fun-[29] B. Liu, Y. Ma, and R. Lee. Analyzing the interesting-[30] B. Liu and A. Tuzhilin. Managing large collections [31] J. Ma and S. Perkins. Online novelty detection on tem-[32] A. Maddalena and B. Catania. Towards an interop-[33] Q. Mei and C. Zhai. Discovering Evolutionary Theme [34] O. Nasraoui, C. Cardona-Uribe, and C. Rojas-Coronel. [36] E. L. Rissland and M. T. Friedman. Detecting change [37] J. F. Roddick, M. Spiliopoulou, D. Lister, and [38] R. Schult and M. Spiliopoulou. Discovering emerging [39] S. Schulz, M. Spiliopoulou, and R. Schult. Topic and [40] M. Spiliopoulou, I. Ntoutsi, Y. Theodoridis, and [41] M. Vazirgiannis, M. Halkidi, and D. Gunopoulos. Un-[42] H. Yang, S. Parthasarathy, and S. Mehta. A generalized [43] K. Zhang, J. T. L. Wang, and D. Shasha. On the editing [44] X. Zhang, G. Dong, and R. Kotagiri. Exploring con-[45] A. Zhou, C. Feng, W. Qian, and C. Jin. Tracking clus-
