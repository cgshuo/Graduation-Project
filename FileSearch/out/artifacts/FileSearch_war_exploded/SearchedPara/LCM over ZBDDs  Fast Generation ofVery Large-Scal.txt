 Considerable attention in the last decade has been placed on discovering useful information from large-scale databases. Frequent itemset mining is one of the fundamental data mining problems. Since the pioneering paper by Agrawal et al. [1] various algorithms have been proposed to solve the frequent pattern min-ing problem (cf., e.g., [3,5,16]. Among those state-of-the-art algorithms, Linear time Closed itemset Miner (LCM) [15,13,14] by Uno et al. has a feature of the theoretical bound as output linear tim e. Their open source code [12] is known as one of the fastest implementations of a frequent itemset mining program.
LCM and most of the other itemset mining algorithms focus on only enu-merating or listing the itemsets that satisfy the given conditions, and how to store and index the result of itemsets for a more efficient data analysis was a different matter. If we want to postprocess the mining results by setting various conditions or restrictions, we have to dump the frequent itemsets into storage at least once. Even though LCM is an output linear time algorithm, it may re-quire impracticable time and space if th e number of frequent itemsets gets too large. We usually control the size of the output by using the minimum support threshold in the ad hoc setting, but we unsure if this may cause some important information to be lost.

For representing very large-scale fr equent itemsets, S. Minato proposed a method using Zero-suppressed Binary Decision Diagrams (ZBDDs) [7], an ef-ficient graph-based data structure. ZBDD is a variant of a Binary Decision Di-agram (BDD) [2], which was originally developed in the VLSI logic design area, but has recently been applied to data minin g problems [9,6,8]. Last year, Mi-nato et al. presented the ZBDD-growth [11] algorithm for computing all/closed/ maximum frequent itemsets based on ZBDD operations, and that generates a compressed output data structure on the main memory. Unfortunately, the over-head of ZBDD-based frequency computation is not small when using their algo-rithm, so the computational advantage is limited to only the examples where the ZBDD-based data compression rate is extr emely high. Otherwise, for example, when the number of frequent itemsets is not large, an ordinary LCM algorithm is much faster than the ZBDD-growth one.
 In this paper, we propose a nice combination of an LCM algorithm and a ZBDD-based data structure. Our method,  X  X CM over ZBDDs, X  can generate very large-scale frequent itemsets o n the main memory that uses a very small overhead of computational time when compared with the original LCM algo-rithm. The mining result can be efficien tly postprocessed by using algebraic ZBDD operations. The original LCM is an output linear time algorithm, but our new method requires a sub-linear tim e for the number of frequent itemsets when the ZBDD-based data compression works well. Our method will greatly accelerate the data mining process and this will lead to a new style of on-memory processing for dealing with knowledge discovery problems. Let E = { 1 , 2 ,...,n } be the set of items .A transaction database on E is a multiset T = { T 1 ,T 2 ,...,T m } where each T i is included in E .Each T i is called a transaction (or tuple ). We denote the sum of sizes of all transactions in T , with ||T|| that is, the size of database T .Aset P  X  X  is called an itemset (or pattern ). The maximum element of P is called the tail of P , and is denoted by tail ( P ). An itemset Q is a tail extension of P if and only if both Q \ P = { e } and e&gt;tail ( P ) hold for an item e .Anitemset P =  X  is a tail extension of Q if and only if Q = P \ tail ( P ), and therefore, Q is unique, i.e., any non-empty itemset is a tail extension of a unique itemset.

For itemset P , a transaction including P is an occurrence of P .The denotation of P , which is denoted by Occ ( P ), is the set of the occurrences of P . | Occ ( P ) | is the frequency of P , and is denoted by frq ( P ). In particular, for an item e , frq ( { e } ) is the frequency of e . For a given constant  X  , called a minimum support , itemset P is frequent if frq ( P )  X   X  . If a frequent itemset P is not included in any other frequent itemset, P is maximal . We define the closure of itemset P in T , denoted by clo ( P ), with T  X  Occ ( P ) T .Anitemset P is closed if P = clo ( P ). We briefly explain LCM algorithm and ZBDD-based techniques for representing frequent itemsets in this section. 3.1 LCM Algorithm LCM is a series of algorithms for enumerating frequent itemsets, which was developed by Uno et al. These algorithms feature that the computation time is theoretically bounded as an output linear time. The first LCM algorithm was presented at FIMI2003 [15], and the second version of LCM demonstrated its remarkable efficiency at FIMI2004 [13]. The original LCM was developed for enumerating closed itemsets, and the n LCMfreq and LCMmax were presented for mining all frequent itemsets and maximal itemsets 1 . Now the three variants are integrated into one program. These implementations are available on the developer X  X  web page [12] as open source software.

In general, frequent itemset mining algorithms are classified into two cate-gories: apriori-like (or level-by-level ) algorithms [1] and backtracking (or depth-first ) algorithms [16,5]. LCM algorithms belong to the backtracking style.
Backtracking algorithms are based on recursive calls. The algorithm inputs a frequent itemset P , and generates new itemsets by adding one of the un-used items to P . Then, for each itemset being frequent among them, it gen-erates recursive calls with respect to it. To avoid duplications, an iteration of the backtracking algorithms adds items with indices larger than the tail of P . The following information is a description of the framework of the backtracking algorithms.
 ALGORITHM Backtracking ( P :itemset) Output P For each e  X  X  , e&gt;tail ( P ) do
If P  X  X  e } is frequent then call Backtracking ( P  X  X  e } )
LCM algorithms are base d on backtracking algor ithms, and use accelera-tion techniques for the frequency counting, called occurrence deliver and any-time database reduction . Therefore, LCM algorithms efficiently compute the fre-quency. Here, we omit the detailed techni ques used in LCM, as they are described in references [13,14].
Although LCM can efficiently enumerate large-scale frequent itemsets, how to store and index the result of itemsets for efficient data analysis is a different matter. Even though LCM is an output linear time algorithm, it may require im-practicable time and space if the number of frequent itemsets becomes too large. We usually control the output size by using the minimum support threshold in the ad hoc setting, but we do not know if it may lose some of the important information that needed to be discovered. 3.2 ZBDDs A Binary Decision Diagram (BDD) is a graph representation for a Boolean function. An example is shown in Fig. 1 for F ( a, b, c )= a bc  X  ab c .Givena variable ordering ( a, b, c in our example), we can use Bryant X  X  algorithm [2] to construct the BDD for any given Boolean function. For many Boolean functions appearing in practice this algorithm is quite efficient and the resulting BDDs are much more efficient representatio ns than binary decision trees.

BDDs were originally invented to represent Boolean functions. However, we can also map a set of combinations into the Boolean space of n variables, where n is the cardinality of E (Fig. 2). So, we could also use BDDs to represent sets of combinations. However, we can even ob tain a more efficient representation by using Zero-suppressed BDDs (ZBDDs) [7].
If there are many similar combinations then the subgraphs are shared resulting in a smaller representation. In addition, ZBDDs have a special type of node deletion rule. As shown in Fig. 3, all of the nodes whose 1-edge directly points to the 0-terminal node are deleted. As the result, the nodes of items that do not appear in any sets of combinations are automatically deleted (Fig.1). This ZBDD reduction rule is extremely effective for handling a set of sparse combinations. If the average appearance ratio of each item is 1%, ZBDDs are possibly more compact than ordinary BDDs, even up to 100 times more.

ZBDD representation has another good property, which is that each path from the root node to the 1-terminal node corresponds to each combination in the set. Namely, the number of such paths in the ZBDD equals the number of combinations in the set. This attractive property indicates t hat, even if there are no equivalent nodes to be shared, the ZBDD structure explicitly stores all the items of each combination, as well as uses an explicit linear linked list data structure. In other words, (the order of) the size of the ZBDD never exceeds the explicit representation. If more nodes are shared, the ZBDD is more compact than the linear list.

Table 1 summarizes most of the primitive operations of the ZBDDs. In these operations,  X   X  , X   X  1 , X  and P.top can be obtained in a constant time. P.offset ( v ), P.onset ( v ), and P.change ( v ) operations require a constant time if v is the top variable of P , otherwise they consume linear time for the number of ZBDD nodes located at a higher position than v . The union, intersection, and difference operations can be performed in almost linear time to the size of the ZBDDs. 3.3 ZBDD-Growth Algorithm Using a ZBDD-based compact data structure, we can efficiently manipulate large-scale itemset data bases on the main memory. R ecently, Minato et al. have developed a ZBDD-growth algorithm to generate all/closed/maximal frequent itemsets for given databases. The details of the algorithm are written in the article referenced in this paper [11]. The ZBDD-growth is based on the back-tracking algorithm using recursive calls as well as the LCM. This algorithm has two following technical features: (i) Uses ZBDDs for the internal data structure, and (ii) Uses ZBDDs for the output data structure.

In the first feature, the internal data structure means that the given transac-tion database is converted to a ZBDD-based representation on the main memory. On each recursive step of the backtracking, frequency counting for the condi-tional (or restricted) database is performed by the ZBDD operations. This is similar to the FP-growth [5] algorithm, which manipulates the FP-tree in the backtracking algorithm.

Since ZBDDs are representations of sets of combinations, a simple ZBDD only distinguishes the existence of each itemset in the database. In order to count the integer numbers of frequency, the ZBDD-growth algorithm uses the m -digits of the ZBDD vector { F 0 ,F 1 ,...,F m  X  1 } to represent the integers up to (2 m  X  1), as shown in Fig. 4. The numbers are encoded into a binary digital code, as F 0 represents a set of itemsets appearing at odd times (LSB = 1), F 1 represents a set of itemsets whose appearance number  X  X  second lowest bit is a one, and which is similar to the way we define the set of each digit up to F m  X  1 . Notice that this ZBDD vector is used only for the internal data structure in the ZBDD-growth algorithm. The output data is represented by a simple ZBDD, because the result is just a set of frequent itemsets. (It does not keep the frequency of each itemset.)
ZBDD-growth algorithm manipulates the ZBDDs for both the internal and output data structures, so the advantage of the ZBDD-based data compression is fully employed. There are examples where billions of frequent itemsets can be represented by only thousands of ZBDD nodes. The mining result can be efficiently postprocessed by using algebraic ZBDD operations.

However, ZBDD-growth has an frequency computing overhead for using ZBDD vectors. The arithmetic operations of the ZBDD vectors are performed by a series of ZBDD operations on each binary digit, and this requires more steps than or-dinary 32-or 64-bit arithmetic operations in the CPU normally use. Unless the ZBDD-based data compression rate is very high, the overhead becomes obvious. There are the two typical cases when the ZBDD is not very effective.  X  The number of itemsets is small enou gh to be easily handled in anyway.  X  The database is completely random and no similar itemsets are included. In many practical cases, the ZBDD-growth algorithm is no faster than previous algorithms. As shown in the experimental results outlined in this paper, the ZBDD-growth is 10 to 100 times slower than ordinary LCM when the output size is small. ZBDD-growth wins only wh en a huge number of frequent itemsets are generated. In this section, we discuss the combination of the LCM and ZBDDs. It is for-tunate that we can observe a number of common properties in LCM algorithms and ZBDD manipulation, and they are listed as follows:  X  Both are based on the backtracking (depth-first) algorithm.  X  All the items used in the database have a fixed variable ordering.  X  In the algorithm, we choose items one by one according to the variable  X  In the current LCM implementation, the variable ordering is decided at the These common properties indicate that LCM and ZBDDs may be a really good combination. Our algorithm,  X  X CM over ZBDDs, X  does not touch the core algo-rithm of LCM, and just generates a ZBDD for the solutions obtained by LCM. In this way, we aim to efficiently generate very large-scale frequent itemsets with a very small overhead of ZBDD manipulation. We will now describe the techniques used in the new method. 4.1 ZBDD Construction in LCM Procedure We recall the basic structure of the original LCM algorithm shown in Fig. 5. However, we omit the detailed techniques used in checking the frequency of each itemset, but basically the algorithm explores all the candidates of the itemsets in a backtracking (or depth-first) manner, and when a frequent itemset is found, they are appended one by one to the output file. On the other hand,  X  X CM over ZBDDs X  constructs a ZBDD that is the union of all the itemsets found in the backtracking search, and finally returns a pointer to the root node of the ZBDD. A naive modification can be described using in Fig. 6. However, this naive algorithm has a problem with its efficiency.

In the LCM procedure, a ZBDD grows by repeating the union operations of the frequent itemsets found in the depth-first search. If we look at the sequence of itemsets generated by the algorithm, the consecutive itemsets are quite similar to each other in most cases, namely, only a few items near the tail are different and the other top items are completely identical. The ZBDD union operations look similar to those shown in Fig. 7, although only a few of the bottom levels are different, but almost all the other parts are the same. Since the procedures for the ZBDD operations are recursively executed from the top node to the bottom one, the computation of a union operation requires O ( n ) steps, while only a few bottom items are meaningful. Namely, this algorithm will become n times slower. This is an unacceptabl e loss of efficiency, because n may be more than a hundred in practical datasets.

To address this problem, we improved the algorithm (Fig. 8). On each recur-sive step, we construct a ZBDD only for the lower items, and after returning from the subsidiary recursive call, we stack the top item up on the current result of ZBDD. In this way, we can avoid redundant traversals in the ZBDD union operation, as shown in Fig. 9. If we use the variable ordering of ZBDDs that is the same as the LCM X  X  item ordering, each ZBDD operation requires only a constant time, and the total overhead of the ZBDD generation can be bounded by a constant factor compared with the original LCM.
 4.2 Employing Hypercube Decomposition The original LCM finds a number of frequent itemsets all at once to reduce the computation time by using the technique of hypercube decomposition [15] (or, also called equisupport ). For a frequent itemset P ,let H ( P )bethesetofitems e satisfying e&gt;tail ( P )and Occ ( P )= Occ ( P  X  X  e } ). Then, for any Q  X  H ( P ), Occ ( P )= Occ ( P  X  Q ) holds, and P  X  Q is frequent. The original LCM avoids duplicated backtracking with respect to the items included in H ( P ), by passing H ( P ) to the subsidiary recursive calls. This algorithm is shown in Fig. 10.
Current LCM implementations have two output options: (i) printing out all the solutions to the output file, or (ii) just counting the total number of so-lutions. When counting the number of itemsets, we accumulate a 2 X  X  power to the hypercube size for each solution, wi thout generating all the candidates de-rived from the hypercube. This technique greatly reduces the computation time because the LCM algorithm is dominated by the output size.

Also in LCM over ZBDDs, we can employ the hypercube decomposition tech-nique. The algorithm is described in Fig. 11. A remarkable advantage of our method is that we can efficiently generate a ZBDD that includes all of the so-lutions, within a similar computation time as the original LCM when counting only the number of solutions. The original LCM is known as an output linear time algorithm, but our method can generate all the solutions in a sub-linear time for the number of solutions if the hypercubes appear often. 4.3 Closed/Maximal Itemset Mining The original LCM can also generate close d/maximal itemsets. Our method does not touch the core algorithm of LCM, and just generates ZBDDs for the solutions obtained by LCM. Therefore, a ZBDD for closed/maximal itemsets as well as the original LCMs can be generated. The technique for hypercube decomposition should be slightly modified to generate a closed/maximal one, but it is a similar technique as to one used in the original LCMs.
 Based on the above ideas, we implemented LCM over ZBDDs by modifying the open software, LCM ver. 5 [12]. We composed about 50 line modifications or additions to the main file of the original LCM, and compiled it with our own ZBDD package, which consists of about 2,300 lines of C codes. We used a 2.4GHz Core2Duo E6600 PC, 2 GB of main memory, with SuSE Linux 10 and a GNU C++ compiler. On this platform, we can manipulate up to 40,000,000 nodes of ZBDDs with up to 65,000 different items.

To evaluate the performance of our method, we applied it to a practical size of the datasets chosen from FIMI2003 repository [4] with various minimum support thresholds. We compared our results with those of the original LCM [12] and the ZBDD-growth [11]. In the datasets, a  X  X ushroom X  is known as an example where the ZBDD-growth is effective because the ZBDD-based data compression works well.  X  X 10I4D100K X  is known as the opposite, an artificial database consists of randomly generated combinations. In this case, ZBDD-based data compression is quite ineffective.  X  X MS-WebView-1  X  has an intermediate property between the two.

Table 2 shows our experimental results. In this table, | ZBDD | represents the number of ZBDD nodes representing all the frequent itemsets. The column  X  X CM-count X  shows the computational time of the original LCM when counting only the number of solutions, and  X  X CM-dump X  represents the time for listing all the itemset data to the output file (using /dev/null).  X  X CMoverZBDD X  and  X  X BDD-growth X  show the time for generating the results of the ZBDD on the main memory, including the time for counting the ZBDD nodes.

From the experimental results, we can clearly see that LCM over ZBDDs is more efficient than ZBDD-growth in most cases. The advantage of our method can be observed when a smaller number of solutions are generated. ZBDD-growth shows comparable performances to our method only in the  X  X ushroom X  with very low minimum support, but for all the other cases, our method overwhelms the ZBDD-growth.

We can also observe that LCM over ZB DDs is more efficient than the origi-nal LCM-dump. The difference becomes significant when very large numbers of itemsets are generated. The original LCM-dump is known as an output linear time algorithm, but our LCM over ZBDDs requires a sub-linear time for the number of itemsets. The computational time of our method is almost the same as executing an LCM-count. We must empha size that LCM-count does not store the itemsets, but only counts the number of solutions. On the other hand, LCM over ZBDDs generates all the solutions and stores them on the main memory as a compact ZBDD. This is an important point.

After executing LCM over ZBDDs, we can apply various algebraic operations to the ZBDD to filter or analyze the frequent itemsets [11]. Storing the results as a ZBDD will be more useful than having a large dump file of all the frequent itemsets.

Finally, we show the experimental results for generating closed itemsets in Ta-ble 3. We compared our results with the original LCM and ZBDD-growthC [10], a variation of ZBDD-growth to generate closed itemsets. Since the closed (or maxi-mal) itemsets are a very small subset of all the frequent itemsets, in this case, the performances of LCM-count and LCM-dump were not so different. Anyway, LCM over ZBDDs can efficiently generate clos ed itemsets using a very small overhead of the ZBDD manipulation. As well as the ZBDD of all the frequent itemsets, var-ious postprocessing is applicable to the ZBDD of closed itemsets. For example, we can easily obtain all the  X  X on-closed X  itemsets by using a ZBDD-based difference operation between all the freque nt itemsets and closed itemsets.
 We proposed our  X  X CM over ZBDDs X  algorithm for efficiently generating very large-scale all/closed/maximal freque nt itemsets using ZBDDs. Our method is based on LCM, one of the most efficient state-of-the-art algorithms previously proposed. The algorithm not only enumerates the itemsets but also generates a compact output data structure on the main memory. The result can efficiently be postprocessed by using algebraic ZBDD operations.

The original LCM is known as an output linear time algorithm, but our new method requires a sub-linear time for the number of frequent patterns when the ZBDD-based data compression works we ll. Our experimental results indicate that the ZBDD-based method will greatl y accelerate the data mining process and will lead to a new style of on-memory processing for dealing with knowledge discovery problems.
 Acknowledgment. This research was partially supported by the Ministry of Education, Science, Sports and Culture (MEXT), Grant-in-Aid for Scientific Research on Priority Area:  X  X yber Infrastructure for the Information-explosion Era. X 
