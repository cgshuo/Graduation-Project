 Previous studies in computerized deception de-tection have relied only on shallow lexico-syntactic cues. Most are based on dictionary-based word counting using LIWC (Pennebaker et al., 2007) (e.g., Hancock et al. (2007), Vrij et al. (2007)), while some recent ones explored the use of machine learning techniques using sim-ple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags (Mihalcea and Strapparava (2009), Ott et al. (2011)). These previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges. For instance, the work of Ott et al. (2011) in the hotel review domain results in very insightful observations that deceptive re-viewers tend to use verbs and personal pronouns (e.g.,  X  X  X ,  X  X y X ) more often, while truthful re-viewers tend to use more of nouns, adjectives, prepositions. In parallel to these shallow lexical patterns, might there be deep syntactic struc-tures that are lurking in deceptive writing?
This paper investigates syntactic stylometry for deception detection, adding a somewhat un-conventional angle to prior literature. Over four different datasets spanning from the product re-view domain to the essay domain, we find that features driven from Context Free Grammar (CFG) parse trees consistently improve the de-tection performance over several baselines that are based only on shallow lexico-syntactic fea-tures. Our results improve the best published re-sult on the hotel review data of Ott et al. (2011) reaching 91.2% accuracy with 14% error reduc-tion. We also achieve substantial improvement over the essay data of Mihalcea and Strapparava (2009), obtaining upto 85.0% accuracy. To explore different types of deceptive writing, we consider the following four datasets spanning from the product review to the essay domain: I. TripAdvisor X  X old: Introduced in Ott et al. (2011), this dataset contains 400 truthful re-views obtained from www.tripadviser.com and 400 deceptive reviews gathered using Amazon Mechanical Turk, evenly distributed across 20 Chicago hotels. II. TripAdvisor X  X euristic: This dataset contains 400 truthful and 400 deceptive reviews harvested from www.tripadviser.com , based on fake review detection heuristics introduced in Feng et al. (2012). 1 III. Yelp: This dataset is our own creation using www.yelp.com . We collect 400 filtered re-views and 400 displayed reviews for 35 Italian restaurants with average ratings in the range of [3.5, 4.0]. Class labels are based on the meta data, which tells us whether each review is fil-tered by Yelp X  X  automated review filtering sys-tem or not. We expect that filtered reviews roughly correspond to deceptive reviews, and displayed reviews to truthful ones, but not with-out considerable noise. We only collect 5-star reviews to avoid unwanted noise from varying degree of sentiment.
 IV. Essays: Introduced in Mihalcea and Strapparava (2009), this corpus contains truth-ful and deceptive essays collected using Amazon Mechanic Turk for the following three topics:  X  X bortion X  (100 essays per class),  X  X est Friend X  (98 essays per class), and  X  X eath Penalty X  (98 essays per class). Words Previous work has shown that bag-of-words are effective in detecting domain-specific deception (Ott et al., 2011; Mihalcea and Strap-parava, 2009). We consider unigram, bigram, and the union of the two as features.
 Shallow Syntax As has been used in many previous studies in stylometry (e.g., Argamon-Engelson et al. (1998), Zhao and Zobel (2007)), we utilize part-of-speech (POS) tags to encode shallow syntactic information. Note that Ott et al. (2011) found that even though POS tags are effective in detecting fake product reviews, they are not as effective as words. Therefore, we strengthen POS features with unigram features. Deep syntax We experiment with four differ-ent encodings of production rules based on the Probabilistic Context Free Grammar (PCFG) parse trees as follows:  X  r : unlexicalized production rules (i.e., all  X  r  X  : lexicalized production rules (i.e., all  X   X  r : unlexicalized production rules combined  X   X  r  X  : lexicalized production rules (i.e., all For all classification tasks, we use SVM classi-fier, 80% of data for training and 20% for test-ing, with 5-fold cross validation. 2 All features are encoded as tf-idf values. We use Berkeley PCFG parser (Petrov and Klein, 2007) to parse sentences. Table 2 presents the classification performance using various features across four different datasets introduced earlier. 3 4.1 TripAdvisor X  X old We first discuss the results for the TripAdvisor X  Gold dataset shown in Table 2. As reported in Ott et al. (2011), bag-of-words features achieve surprisingly high performance, reaching upto 89.6% accuracy. Deep syntactic features, en-coded as  X  r  X  slightly improves this performance, achieving 90.4% accuracy. When these syntactic features are combined with unigram features, we attain the best performance of 91.2% accuracy, yielding 14% error reduction over the word-only features.

Given the power of word-based features, one might wonder, whether the PCFG driven fea-tures are being useful only due to their lexi-cal production rules. To address such doubts, we include experiments with unlexicalized rules, r and  X  r . These features achieve 78.5% and 74.8% accuracy respectively, which are signifi-cantly higher than that of a random baseline (  X  50.0%), confirming statistical differences in deep syntactic structures. See Section 4.4 for concrete exemplary rules.

Another question one might have is whether the performance gain of PCFG features are mostly from local sequences of POS tags, indi-rectly encoded in the production rules. Compar-ing the performance of [shallow syntax+words] and [deep syntax+words] in Table 2, we find sta-tistical evidence that deep syntax based features offer information that are not available in simple POS sequences. 4.2 TripAdvisor X  X euristic &amp; Yelp The performance is generally lower than that of the previous dataset, due to the noisy nature of these datasets. Nevertheless, we find similar trends as those seen in the TripAdvisor X  X old dataset, with respect to the relative performance differences across different approaches. The sig-nificance of these results comes from the fact that these two datasets consists of real (fake) reviews in the wild, rather than manufactured ones that might invite unwanted signals that can unexpectedly help with classification accu-racy. In sum, these results indicate the exis-tence of the statistical signals hidden in deep syntax even in real product reviews with noisy gold standards. 4.3 Essay Finally in Table 2, the last dataset Essay con-firms the similar trends again, that the deep syn-tactic features consistently improve the perfor-mance over several baselines based only on shal-low lexico-syntactic features. The final results, reaching accuracy as high as 85%, substantially outperform what has been previously reported in Mihalcea and Strapparava (2009). How ro-bust are the syntactic cues in the cross topic set-ting? Table 4 compares the results of Mihalcea and Strapparava (2009) and ours, demonstrat-ing that syntactic features achieve substantially and surprisingly more robust results. 4.4 Discriminative Production Rules To give more concrete insights, we provide 10 most discriminative unlexicalized production rules (augmented with the grand parent node) for each class in Table 1. We order the rules based on the feature weights assigned by LIB-LINEAR classifier. Notice that the two produc-tion rules in bolds  X  [SBAR X NP  X  S] and [NP  X VP  X  NP SBAR]  X  are parts of the parse tree shown in Figure 1, whose sentence is taken from an actual fake review. Table 3 shows the most discriminative phrasal tags in the PCFG parse
M&amp;S 2009 58.7 58.7 62.0 trees for each class. Interestingly, we find more frequent use of VP, SBAR (clause introduced by subordinating conjunction), and WHADVP in deceptive reviews than truthful reviews. Much of the previous work for detecting de-ceptive product reviews focused on related, but slightly different problems, e.g., detecting dupli-cate reviews or review spams (e.g., Jindal and Liu (2008), Lim et al. (2010), Mukherjee et al. (2011), Jindal et al. (2010)) due to notable dif-ficulty in obtaining gold standard labels. 4 The Yelp data we explored in this work shares a sim-ilar spirit in that gold standard labels are har-vested from existing meta data, which are not guaranteed to align well with true hidden la-bels as to deceptive v.s. truthful reviews. Two previous work obtained more precise gold stan-dard labels by hiring Amazon turkers to write deceptive articles (e.g., Mihalcea and Strappa-rava (2009), Ott et al. (2011)), both of which have been examined in this study with respect to their syntactic characteristics. Although we are not aware of any prior work that dealt with syntactic cues in deceptive writing directly, prior work on hedge detection (e.g., Greene and Resnik (2009), Li et al. (2010)) relates to our findings. We investigated syntactic stylometry for decep-tion detection, adding a somewhat unconven-tional angle to previous studies. Experimental results consistently find statistical evidence of deep syntactic patterns that are helpful in dis-criminating deceptive writing.
