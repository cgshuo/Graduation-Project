 News websites give their users the opportunity to partici-pate in discussions about published articles, by writing com-ments. Typically, these comments are unstructured making it hard to understand the ow of user discussions. Thus, there is a need for organizing comments to help users to (1) gain more insights about news topics, and (2) have an easy access to comments that trigger their interests. In this work, we address the above problem by organizing comments around the entities and the aspects they discuss. More speci cally, we propose an approach for entity and aspect extraction from user comments through the following contri-butions. First, we extend traditional Named-Entity Recog-nition approaches, using coreference resolution and external knowledge bases, to detect more occurrences of entities in comments. Second, we exploit part-of-speech tag, depen-dency tag, and lexical databases to extract explicit and im-plicit aspects around discussed entities. Third, we evaluate our entity and aspect extraction approach, on manually an-notated data, showing that it highly increases precision and recall compared to baseline approaches.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; I.2.7 [ Arti cial Intelligence ]: Natural Language Processing entity extraction; aspect extraction; text mining
Media platforms, like CNN 1 and Al Jazeera 2 , deliver the latest breaking news on various topics about everyday www. cnn.com www.aljazeera.com c  X  events. Moreover, they provide the possibility to write com-ments about any published article and engage in discussions with other users. Figure 1 shows an example of a news arti-cle about the Scottisch independence referendum, published at Al Jazeera on September 19, 2014. On this article, users started several threads of discussions about \Scots origin" , \Bene t of terrorists from Scottisch independence" , \The re-sults of the vote" , and other subjects. Although user com-ments form such well-de ned discussions, they are displayed in an unstructured way, listed based on time and date in-formation, as shown in Figure 1. Consequently, it is hard for the reader to catch the ow of discussions and to un-derstand their main points of agreement and disagreement. Thus, there is a need for organizing user comments to (1) have a better understanding of the viewpoints related to each topic and (2) facilitate the participation in discussions and thus increase the chance of acquiring new viewpoints. A natural way to summarize and organize comments is to clus-ter those that contain similar discussions. In other words, they talk about the same entities and argue about the same aspects of those entities. To achieve this task, we need to extract entities and aspects from user comments.

The core problem of our work is entity and aspect extrac-tion from unstructured text. Several techniques for Named Entity Recognition (NER) have been proposed in the lit-erature [1, 3, 5{7, 11, 12, 19{21], including supervised, semi-supervised, and unsupervised techniques. Similarly, several approaches have been proposed for aspect extraction from user reviews about products [22]. Exploiting existing ap-proaches to extract entities and aspects from user comments brings new challenges: (1) a user comment can refer to other comments when talking about entities, which requires the integration of coreference resolution strategies and tailored techniques for identifying the context of a comment; (2) user comments are informal and might include noisy information such as abbreviations and misspellings, which make entity extraction dicult; (3) aspect extraction techniques limit aspects to attributes and components of entities and thus do not cover other forms of aspects related to entities that are not objects, such as people or events.

In this paper, we propose an approach for entity and as-pect extraction from user comments on news media plat-forms, tackling the problems above. The main contributions of this paper are summarized as follows: 1. We extend traditional Named Entity Recognition 2. We propose an aspect extraction approach to detect 3. We evaluate our entity and aspect extraction approach, The rest of this paper is organized as follows. Section 2 presents related work. Section 3 introduces the problem we are tackling in this work. Section 4 presents the extensions we propose for entity extraction tools. Section 5 describes our approach for extracting explicit and implicit aspects. Section 6 presents and discusses experiment results, and -nally Section 7 concludes the paper.
Our work falls into the category of Text Mining. To the best of our knowledge, we are the rst to touch the area of organizing opinionated texts from user-generated content us-ing a domain-independent approach for entity and aspect ex-traction. One of the most similar works to ours was done by Hu et al. [8,10]. They extract product features that are com-mented on by the user, then they group up review sentences commenting on the same features. This work was later im-proved by Poria et al. [15] who developed an unsupervised technique of product feature extraction and summarization using handcrafted rules. The di erence between these ap-proaches and our work is that they completely break down user comments into small sentences and they lose connection to the original user comments, whereas we want to preserve this information. Moreover, they are domain-speci c while our work is more general. In terms of beautiful visualization of text summary, the Word Cloud 3 may be one of the best work available. It emphasizes important terms, however, it does not preserve semantic relations between terms. Some other similar works are those focusing on the area of topic mining [9,14]. Pons-Poratta et al. [14] explored a topic dis-covery system to browse and select topics of interest. Sim-ilarly to topic classi cation approaches, our work aims at maintaining relationships between entities and aspects. The main di erence is that the relationships we build are not only semantic but they depend also on the co-occurences of terms in user comments.
 Directly related to our work is research in Named Entity Recognition (NER) [20]. The rst class of approaches is supervised and consists in learning classi ers from training data for entity recognition [1,3,5,6,11,19]. One of the most well-known open tools for supervised learning of NER is the StanfordNLP 4 , which is based on the CRF classi er [7]. To address the diculty of obtaining manually-annotated data for training, a second class of approaches was proposed. It relies on a semi-supervised learning technique called \boot-straping" [12]. Typically, this approach uses a small ini-tial set of seeds as a start of the learning process, and then it searches for the occurrence of the seed in the sentences and identi es common contextual clues of the initial seeds. Then, the technique proceeds to nd new instances of the seeds that have similar context.

The third class of approaches to NER uses unsuper-vised learning techniques, typically by making use of open knowledge bases such as DBpedia. 5 Some of the unsu-pervised NER tools are available online, such as AIDA, 6 AlchemyAPI, 7 NERD, 8 Zemanta, 9 and Wiki er. 10 In ad-dition to detecting entities, they also employ entity link-ing/disambiguation, which is the ability to correctly disam-biguate entities that have the same name (to some extent). These tools have been evaluated to have good performances [17]. However, their application in our work raises several problems. First, they do not have a common agreement of what an entity is and thus they provide, in some cases, un-reliable results. For example, AIDA does not recognize \To-ries", the british political party, as an entity, while NERD returns\true freedom"as an entity, which is subjective. This problem makes it hard to decide which NER tool to use for our purpose. Second, they do not cover all forms of occur-rences of entities, including coreferences, which is crucial for user comments. In our work, we address these issues to dis-cover more occurrences of entities and reduce the amount of noise involved in the process. h ttp://www.wordle.net/ http://nlp.stanford.edu/ http://dbpedia.org/ https://gate.d5.mpi-inf.mpg.de/webaida/ http://www.alchemyapi.com/ http://nerd.eurecom.fr/ http://www.zemanta.com/ http://cogcomp.cs.illinois.edu/page/demo vi ew/Wiki er
Asp ect extraction has also gained attention in the prod-uct review domain [22]. These approaches de ne aspects as the components and the attributes of a given entity. More-over, they rely on the presence of sentiments to detect as-pects. Thus, the process of identifying and extracting as-pects and entities is limited to evaluative texts. However, in our work, we want to extend the de nition of aspects so that (1) it includes other forms than attributes or com-ponents and (2) the extraction does not rely solely on the presence of sentiments.
User comments talk about entities and argue about spe-ci c aspects of those entities. An entity can be either a person, an organization, a location, or any well-de ned con-cept such as languages, nationalities, or wars. By contrast, an aspect is all what is arguable about an entity. Consider the following example: \I 'm all for Scottish independence. This vote is about continuing to depend on the UK without having to bear the responsibilities." This comment talks about the entities \Scottish" and \UK" , and argues about the aspect \independence" of the entity \Scottish" and the aspect \responsibility" related to the entity \UK" . In this work, our task is to extract from each comment the set of entities it talks about, and the aspects related to each entity.

Entity extraction from user comments brings new chal-lenges to traditional NER. First, existing NER tools pro-vide, in some cases, results that do not correspond to our entity de nition, such as \true freedom" . Note that an entity must refer to an objective concept since subjective concepts are subject to discussion among users and they need to be extracted as aspects. Distinguishing entities from aspects is important for an accurate representation of the various view points of users about a given entity. Second and more impor-tantly, a comment is typically a part of a discussion which implies that entities are not always explicitly mentioned but appear as coreferences. Note that limiting the entity ex-traction process only to occurrences that mention the name will not allow having a wide coverage of the aspects of an entity. Third, comments are informal in nature and might contain grammatical errors, misspellings, abbreviations, and unreliable capitalization.

To address the above problems, we need rst to lter the outcome of NER tools to have only entities of the types we have de ned. Second, we need to extract the context of each comment from its related comments and eventually from the news article itself. The underlying structure of comments in news sites is a tree, where users can post a standalone comment, a reply to a standalone comment, or a reply to another reply. Figure 2 shows the two largest sub-trees of the full tree of comments on the Scottish independence news article. The largest contains 28 comments discussing about terrorism, and the other one has 17 comments discussing about Scots origin. Generally, the full tree has a news article as a root, and comments as either intermediate nodes or leaves. Based on this structure, the context of a comment is given by its ancestors. Having context information would then help resolving entities that occur as coreferences and ambiguity related to abbreviations and missspellings.
After extracting entities from comments, we need to ex-tract for each entity its corresponding aspects. Aspects oc-cur in the text as noun phrases. Thus, aspect extraction consists in nding noun phrases that have a grammatical relationship with the extracted entities. Consider the fol-lowing example: \Sc ots love to spout nonsense about independence." In this comment, there is a dependency between the noun phrase \independence" and the nominal subject of the sen-tence which is the entity \Scots" . Thus, the noun phrase \independence" is extracted as aspect for the entity \Scots" . The main challenge is that aspects are not always explicitly mentioned in the text. Consider the following example: \Y ou live in the British Isles. The largest is Great Britain. Some of you are Irish, Scottish, and English. Some of you claim to be Welsh." The sentence \The largest is Great Britain" talks about the aspect \Isle" of the entity \Great Britain" even though it is not explicitly mentioned. In other cases, aspects can be semi-implicit. Consider the following example: \Sc otland can vote however it wants, it's the Scottish peoples right." This comment talks about the aspect \voting" of the entity \Scotland" . In this case, the aspect can be derived from ex-plicitly mentioned words in the sentence such as verbs and adjectives. To tackle the problem of aspect extraction from user comments, we propose in this paper tailored techniques for each case of explicit, implicit, and semi-implicit aspects This involves the exploitation of context information to nd frequent co-occurrences of noun phrases with verbs and ad-jectives. .
Our aim is to exploit an existing NER tool for extracting entities from user comments. However, there are a number of limitations raised by the structure and the nature of user comments as described earlier. In this section we tackle these limitations through the following strategies.
We have de ned an entity to be either a person, an or-ganisation, a location, or any well-de ned concept such as la nguages. According to this de nition, an entity is an indi-vidual of a class. A class is not an entity because it describes a very general concept. The issue with existing tools is that they do not have an agreement on what an entity is. As a consequence, entity ltering is not implemented in any of them. Consider the following example: \I say let the Scots have their true freedom. Don't be afraid of Rasmussen or NATO, this is none of their business. If he ends up sending USS Ronald Reagan aircraft carrier to the coast of Scotland, then he should have done the same to Crimea." From this comment, AIDA[21] does not recognize \Crimea" as an entity whereas NERD[17] does. Moreover, NERD ex-tracts \aircraft carrier" and \true freedom" as entities. How-ever, \aircraft carrier" is a class and \true freedom" is not an instance of a class but an aspect which is arguable.
Our entity ltering approach consists in exploiting a knowledge base to check whether a given noun phrase is an entity or not. To this end, we choose DBpedia [2], a large scale knowledge base extracted from Wikipedia. DB-pedia is a graph database that uses the RDF 11 format. It represents Wikipedia categories as resources and uses the rdf:type predicate to state whether a resource is a class or an individual of a class. Using this property, entity lter-ing removes all results produced by NER tools that have no property rdf:type in DBpedia.

In the previous example, NERD extracts as entities: \Scots" , \true freedom" , \NATO" , \USS Ronald Reagan" , \aircraft carrier" , \Scotland" , and \Crimea" . It provides, for each extracted entity, a link to Wikipedia. We then convert the Wikipedia link to its corresponding DBpedia URI , and check whether the object represented in the URI has prop-erty rdf:type other than owl:class and owl:thing . In this example, because \true freedom" and \aircraft carrier" do not have any rdf:type property, they are removed. All other en-tities have types other than owl:class and owl:thing and thus they are kept.
When writing comments, users tend not to write the full name of entities. They use abbreviations, or only last names of people, and sometimes they misspell names. We call all variations of the name of an entity an \alias" . Interestingly, traditional NER tools are not always able to recognize enti-ties from aliases. Thus, we introduce a name normalization technique that converts all aliases to normalized names to facilitate entity extraction. To begin, we extract entities from the news article and all its related comments using the entity ltering technique described earlier. For entities of type Person , we set as aliases rst names, middle names, and last names. For other types, we nd possible aliases us-ing a knowledge base. In DBpedia, there are two properties that are very useful for nding aliases. 1. dbpedia-owl:wikiPageDisambiguates : represents the disambiguation page of Wikipedia. For example, by using this property, we know that \UK" is an alias for \United Kingdom" . 2. dbpedia-owl:wikiPageRedirects : stores less common aliases than the rst property does, including some frequent typos of the entity, which can be useful in detecting typos to h ttp://www.w3.org/RDF/ some extent. For example, the Wikipedia URL correspond-ing to \United Kingdon" is directed to the Wikipedia page of \United Kingdom" and thus we can resolve the misspelling.
Once we have all entities with their aliases, we proceed as follows. For each unresolved alias s in comment c , we check if c contains an entity e that has s as alias. If e exists then we replace s by e . Otherwise, we look for e in the parent comment of c . We recursively run this procedure until we nd the entity that has s as alias. Formally the algorithm is given as follows. In some cases (although rare), we Data : s : unresolved alias, c : comment containing s Result : e : entity for which s is an alias
FindEntity(s,c) begin end may encounter an alias that refers to multiple normalized names. In this work, we use a coin toss to decide which normalized name to choose. Considering that \Rasmussen" is not extracted as an entity in the previous example, we run our algorithm and if \`Anders Fogh Rasmussen" is mentioned in one of the ancestors of the comment, then we replace the string \Rasmussen" by \Anders Fogh Rasmussen" .
NER approaches are not designed to detect entities that appear as coreferences. This is a problem for our work since we need to extract aspects related to each entity and thus all types of occurrences of an entity should be taken into account. Let us consider the following example: \I f the Scots pull out it leaves the rest of the UK to themselves where the Tories will be the dominant party.
They have promised an IN/Out referendum on EU membership." From this comment, NER does not recognize the pronoun \they" as an occurrence of the entity \Tories" . Consequently, \IN/Out referendum" and \EU membership" cannot be ex-tracted as aspects related to \Toris" . To solve this problem we apply the Stanford Deterministic Coreference Resolution System [16] to map coreferences to their corresponding en-tities. The di erence that our work makes is that comments are not independent from each other. In this case, a corefer-ence can be related to an entity mentioned in another com-ment or in the news article itself. To handle this problem we propose an Intertextual Coreference Resolution approach that nds the context of comments to resolve coreferences.
The approach works as follows. For each unresolved coref-erence, we take the phrase that contains it and append it to the parent of the comment. Then, we apply the Stan-ford Coreference Resolution[16]. If the coreference is not resolved, we recursively extend the context to the ancestors until we nd the referent or reach the root of the tree. The algorithm of intertextual coreference resolution is formally given by:
Data : r : unresolved coreference, p : phrase containing c : comment that gives the context for r , initially set to the parent of the comment where r occurs.

ICR(r,p,c) begin end
Note that in practice, the referent often belongs to the parent comment or the news article. The reason is that a comment either replies to its parent or comments directly on the content of the news article.
Comments might contain aliases that refer to entities which are not mentioned in the news article. If we take the previous example: \Do n't be afraid of Rasmussen or NATO, this is none of their business." and we suppose that the name \Anders Fogh Rasmussen" is not mentioned in the news article, then applying the name normalization would not work. We call entities mentioned in the news article explicit entities , while we call implicit entities those that are not mentioned in the news article but brought up by users in their comments. To be able to de-tect implicit entities, we proceed with a context-related en-tity search. We start by extracting all explicit entities using a traditional NER tool and the name normalization tech-nique. Further, for each explicit entity, we search the set of entities that are related to it by some arbitrary property in Dbpedia. Note that we use the entity ltering method to remove all implicit entities that do not have the property rdf:type . In this work, we do the search only by one step to reduce noise and the running time. However, it might be the case that in the knowledge base implicit entities are connected to explicit entities multiple steps away. The re-sult of this search is then used to extend the entity alias mapping by including the aliases of implicit entities. If we Fi gure 3: Example of entities extracted by context search take the previous example, we nd that the entity \NATO" is related to the entity \Anders Fogh Rasmussen" via the relationship dbprop:leaderName as shown in Figure 3. Note that \Anders Fogh Rasmussen" was the Secretary General of \NATO" . After nding the entity \Anders Fogh Rasmussen" , we set all its possible aliases including \Anders" , \Fogh" and \Rasmussen" . Thus, when \Rasmussen" is encountered in a given comment, it is normalized using the name of the im-plicit entity. In case a comment mentions \McGarr" , we need to do a two-step search to nd the entity \Pearson Center" , then from that entity we nd the entity \Kevin McGarr" as shown in Figure 3.
We divide aspects into three categories: (1) Explicit As-pects , (2) Implicit Aspects , and (3) Semi-Implicit Aspects . Explicit aspects appear explicitly as noun phrases, implicit aspects are derived from the context, while semi-implicit aspects are inferred from other part-of-speech tags. In the following, we describe in more detail each type of aspect and present our approach to extracting it.
An explicit aspect of an entity e appears as a noun phrase that has a relationship with e . We use the dependency gram-mar to identify this relationship. Explicit aspect extraction parses comments using the part-of-speech tagging and the dependency grammar, and then selects noun phrases that have a speci c grammatical dependency to e . We exploit the following types of dependencies:
Prepositional Dependency. We extract as explicit as-pects, for an entity e , all noun phrases that have a relation-ship with e using prepositions, such as \of" , \at" , or \in" .
I n the example above, the aspect \coast" is extracted for the entity \Scotland" using the propositional dependency \of" .
 Possessive Dependency. We extract as explicit aspects, for an entity e , all noun phrases that have a possessive de-pendency to e .
 In the example above, the aspect of \freedom" is extracted for the entity \Scots" by applying possessive dependency, com-bined with the coreference resolution described in section 4.3. We can see that \their" is detected as a coreference for the entity \Scots" . Moreover, the noun phrase \freedom" has a possessive dependency to the coreference `their" . Thus, \freedom" has a possessive dependency to \Scots" . Verb Dependency. We extract as explicit aspects, for an entity e , all noun phrases connected to e using a verb that has dependencies to both of the aspects and the entity e . In the following example the aspect \Queen" is extracted for the entity \Scots" using verb dependency. The clausal complement of the verb \want to keep" has a dependency on both \Scots" and \Queen" .

Th e dependencies described above are very precise and thus they rarely fail to extract explicit aspects. However, th ey only cover a small portion of possible occurrences of aspects in a comment because most of them appear implic-itly. Therefore, implicit aspect extraction is needed.
To deal with the problem of extracting aspects that are implicit in the text, we propose two approaches. The rst approach exploits mappings between adjectives and aspects that co-occur frequently. By contrast, the second approach deals with word-sense disambiguation and lemmatization over implicit aspects.
The extraction rules described in Section 5.1 do not rely on the occurrence of any opinionated word, which makes them di erent from previous work of explicit aspect extraction [22]. However, the occurrence of an opinionated word is in fact still useful to help extracting some implicit aspects. In this work, we identify the occurrence of words containing sentiment, typically adjectives 12 , to build a mapping from pairs of entity and adjective to some explicit aspects.
From all comments, we extract the set of pairs P = { X  e i ; adj k  X  X  where e i is an entity, and adj k is an adjective that has a dependency to entity e i . To each pair  X  e i ; adj we associate the set of aspects S ik = { a 1 ; :::; a n } a dependency to entity e i and adjective adj k . Whenever a pair of entity e i and adjective adj k is found in a com-ment without any occurrence of an explicit aspect, we use adjective-to-aspect mapping. We extract as implicit aspect of e i , the aspect a j of S ik which has the highest frequency of occurrence with the pair  X  e i ; adj k  X  . In the case of multi-ple aspects that share the value of highest frequency, we use word-sense disambiguation to choose the most appropriate aspect as described in the next section. \T esco is large! Of course many of the jobs it o ers are in shops, transport etc. Those would stay regardless of `yes' or `no'." In the rst sentence of the above comment, \Tesco" is an entity and \large" is an adjective associated to it. Moreover there is no explicit aspect related to \Tesco" . In this case, we check the adjective-to-aspect mapping and we nd three occurrences of explicit aspects with the adjective \large" and the entity \Tesco" : { employer (2), back oce (1), call cen-tre operation (1) } . Among these three aspects, we assign to the entity \Tesco" the aspect \employer" , which has the highest number of occurrences. The aspect \employer" was implicitly referred to by the sentence.
Adjective-to-aspect mapping might lead to ambiguity when there are multiple aspects that have the highest fre-quency with respect to a given entity and adjective. To address this issue, we exploit a lexical database, more specif-ically WordNet 13 for word-sense disambiguation. We do this in two steps. First, we increase the coverage of adjectives in the mapping by including other similar adjectives. Take the following example: \T esco is big!"
W e consider adjective to be the only form of the opinion-ated words, as we do not focus on investigating a sophisti-cated sentiment analysis. https://wordnet.princeton.edu Because \big" is an adjective similar to \large" , the aspect \employer" was implicitly referred to by the sentence. The similarity between adjectives in WordNet is captured by the similar to lexical relation, which is symmetric. In WordNet, adjectives \huge" and \great" are also similar to \large" . To reduce noise, we do the lexical relation search only by one step.

Second, if a pair of an entity and an adjective is related to multiple aspects having the highest frequency of occur-rences, then we use the similarity measures of WordNet to choose the aspect which is most similar to the context of the entity and its adjective. The context is de ned by the sentences around the entity and the adjective. Formally, we de ne a context c as a set of words { w 1 ; w 2 ; :::; w m similarity between an aspect a and a context c is computed as follows: wh ere Wordnet :: Similarity is a quantitative measure of the degree to which two word senses are related, provided by WordNet. Basically, for each aspect a we compute the aver-age similarity between a and the words of context c . Then, we choose the aspect with the highest similarity to c .
We have seen so far how to extract explicit and implicit aspects from comments. Explicit aspects are extracted from prepositions, possessions, and verbs, while implicit aspects are inferred based on adjectives. An explicit aspect ex-tracted using verb dependency is typically the subject or the object of the verb. By contrast, an implicit aspect is always something that is described by an adjective. Let us take the previous example on verb dependency: \Sc ots want to keep the Queen as Monarch." We note that \Queen" is an explicit aspect of \Scots" and it is also the object of the verb \keep" . Similarly, we take the example of implicit aspects: \T esco is large!" Recall that the implicit aspect \employer" was extracted for the entity \Tesco" , and in fact this aspect is described by the adjective \large" .

The two types of aspects described above can be enhanced with complementary information. The goal is to give more meaning to the aspects discussed about entities. The rst comment talks about the aspect \Queen" of the entity \Scots" and more speci cally about \Keeping the Queen" . In the same way, the second comment talks about \Tesco" being \large" and more speci cally about being a \large employer" . We present in the following our approach for enhancing ex-plicit and implicit aspects.
We focus here on aspects extracted using verb depen-dency. To have complimentary information about an ex-plicit aspect, we convert to a noun the verb to which the explicit aspect is related. In the previous example we con-vert the verb \keep" to \keeping" . Then, we concatenate the resulting noun to the explicit aspect, so it becomes \Keeping the Queen" .
To have complementary information about an implicit as-pect, we convert to a noun the adjective which is related to the aspect. In the example above, \large" is converted to \largeness" . Then we concatenate the resulting noun to the implicit aspect, so it becomes \employer largeness" .
In some cases, entities are not related to any explicit or implicit aspect, but they are related to an action or described as a whole without referring to a speci c side. Let us take the following example: \S cotland can vote however it wants, it's the Scottish peoples right." The rst sentence of this comment talks solely about \Scot-land" voting. Consider another example: \I would love to visit Scotland again|hopefully an in-dependent one|it is a very beautiful country and the Scots are so honest." Here we can see that the entity \Scots" as a whole is de-scribed as honest. To generalize, if an entity (1) has a depen-dency to a verb and (2) the verb is not related to an explicit or implicit aspect, then we convert the verb to a noun and use it as an aspect for the entity. The same thing applies for adjectives. Applying this rule, we would have from the two previous examples the aspect \voting (or vote)" assigned to \Scotland" and the aspect \honesty" assigned to \Scots" . To convert verbs and adjectives into nouns, we use Word-Net, which returns for each word a list of nouns that are lexically related. There are di erent relations that can be exploited such as \synonym" , \similar to" , and \derivation-ally related form" to nd the noun form of a verb or an ad-jective. Among the returned nouns, we choose the one most similar to the input, using the Wordnet :: Similarity function.
In this section, we describe the setup of our experiments, then we present and discuss the results.
Dataset. We have crawled comments on 10 di erent news articles from the Al Jazeera and CNN websites. For each news article, we have selected the 100 longest comments. For each comment we have added the set of its ancestor comments to keep track of its context. This process resulted in 1087 comments on the 10 news articles. We have chosen the articles from three di erent categories: Politics , Sport , and Techs . The reason of this choice is to re ect the vari-ous types of user comments. Comments on sport category have a high tendency to introduce entities that are not men-tioned in the article. Comments on techs category have the same characteristic of sport comments and in addition they have the tendency to use slang language. Finally, comments on politics are highly controversial and discuss a variety of aspects regarding the same entity. Besides the ancestor com-ments, the distribution of comments on the three categories is: 400 comments in politics, 300 comments in techs, and 300 in Sport Annotation. The collected dataset of 1087 comments was annotated by ve rst-year bachelor students who are not involved in this project. Their task was to extract from each comment the set of entities and aspects using the following guidelines: 14 1. An entity is either a person, an organisation, a loca-2. An explicit aspect is a noun phrase that has a gram-3. An implicit aspect is referred to by an adjective that 4. A semi-implicit aspect is the noun form of a verb or It is important to mention that the students had the freedom to extract the implicit aspects based on what they under-stood from the comment, decide the suitable noun form of the verb or the adjective, and decide whether something is a well-de ned concept or not. The results of these annota-tions were use as our ground truth data.
 Baselines. We have used three baseline approaches for en-tity extraction: NERD and Zemanta, which are two of the best NER tools [17,18], and AIDA, which has been demon-strated to have an accuracy competitive to the very best named-entity disambiguation (NED) systems [4]. We do not use the Stanford NER as our baseline because it does not provide us with the necessary information, that is URI for extracted entities, to run our improvements.

We have used the baseline approaches to extract entities from user comments. Then, we applied the di erent ap-proaches we have proposed to assess their impact on the results. Regarding aspect extraction, there is no existing work that is really comparable to ours, because all work on aspect extraction has mainly focused on product reviews [15]. More importantly, they assume the input text to be always about a single entity, whereas texts in our dataset may contain multiple entities. Thus, they perform poorly. Metrics. To assess the performance of our approach, we use three evaluation metrics: precision , recall , and F 1 Precision is given by: recall is given by: and F 1 score is given by:
We have tested separately entity extraction and aspect extraction techniques and the results are described in the following.
The overall results are shown in Table 1. We can see the approach we have proposed for extending existing NER improves substantially both precision and recall. We note that precision is improved by 22% for Zemanta, and 30% for NERD. Similarly, the recall is improved by 34% for Ze-manta and 25% for NERD. Although we slightly improved t hey are also provided with an extended, comprehensive guidelines Prec. R ecall F 1 the precision of AIDA, we have signi cantly improved its re-call by 15% mainly by detecting more forms of occurrences of entities. More details about the impact of each proposed technique to enhance existing NER are shown in Tables 2, 3, and 4. They report the overall precision and recall together with the average values for each category of user comments including politics, techs, and sport. The di erent techniques we have proposed, in this paper, are applied incrementally from the leftmost one (traditional NER tool as baseline) to the rightmost one (coreference resolution). Therefore, the numbers listed in the coreference resolution are obtained af-ter applying all the techniques in order. The results for each technique are presented in the following.
 Entity Filtering. After applying entity ltering, the pre-cision of all baselines is boosted for a little drawback on recall. If we use Zemanta, the entity ltering increases the precision by 27%, but reduces the recall by 0 : 5% for all cat-egories, while when we use NERD, the precision increases by 64% and the recall is reduced by 0 : 4%. Note that the precision of NERD increases more than the one of Zemanta because NERD produces more noise. Regarding AIDA, it returns as entities instances of classes thus the improvement is very small.
 Name Normalization. The improvement that name nor-malization brings is on the recall. This is natural since its goal is to detect more occurrences of entities. While preci-sion values are almost the same for entity ltering and after applying name normalization, recall has an overall increase of 4% for both Zemanta and NERD and less than 3% for AIDA. We note that the increase depends on the category of comments. For techs, it is around 4% and for politics is two times higher. This is due to the fact that typically techs entities are not mentioned using aliases while in politics they are. We note that AIDA behaves di erently because it al-ready performs labeling people, organization, and country names in abbreviation. However, we provide an additional improvement of 3% on its precision and recall values Context Search. The same observation holds for context search which improves the recall keeping the values of preci-sion stable. We can note an additional improvement of 11% in overall recall for both Zemanta and NERD. It is inter-esting to see that the improvement depends on the category of comments. For example, in the techs category, the recall improves by 2% at most while in sports is of 30%. This huge di erence is due to the fact that sport comments often talk about entities that are not mentioned in the news article. For example, a comment may mention members of a football team while the article talks on the team as a whole, without using speci c names. In this case, context search is more evident helps extracting these implicit entities and thus we can see the impact clearly in sport comments. This strat-egy penalizes slightly the precision which is natural, since we collect from the knowledge base extra information that can contain some noise. However, the decrease in precision is negligible. Although AIDA performs also context search, our approach provides additional improvement of 4% on the recall.
 Coreference Resolution. After applying coreference res-olution, we complete the integration of all the methods we have proposed in the entity extraction process. We provide an additional improvement in recall of more than 9% for Ze-manta, NERD, and AIDA while we have a slight decrease in precision of less than 3%. This is because coreference reso-lution extracts more occurrences of entities, but sometimes it infers the wrong entity which penalizes the precision.
We also evaluate the Stanford Coreference Resolution on our annotated data using the pairwise metric [23]. It has precision of 74.12% and recall of 46.86%. This result is sim-ilar to the one presented in [16].
In this section, we present the experiment results for our aspect extraction approach based on our annotated dataset. This means that we take the annotated entities as starting point. The reason is to analyze the performance of our ap-proach independently from the in uence of external tools. The results are shown in Table 5. We can see that the com-bination of explicit, implicit, and semi-implicit techniques provide the best trade-o between precision and recall. The details of the performance of each technique are presented in the following.
 Explicit Aspect Extraction. This technique achieves a very high precision of 90 : 88% because the extraction rules are designed to correctly capture all explicit aspects. How-ever it fails, in very few cases, when gurative language is used. For example, consider the sentence \This game is a piece of cake for Arsenal? Our approach will extract \piece of cake" as an aspect of \Arsenal" because of the presence of a prepositional dependency between them, while in fact, the term \piece of cake" is an idiom that suggests an aspect of \easiness" . In contrast, the recall of this technique is very low because most aspects appear implicitly.
 Explicit + Implicit Aspect Extraction. We can see in Table 5 that adjective-to-aspect mapping is not very useful when tested on our annotated data. It increases the recall by less than 3% while it decreases the precision by 14%. The reason is that choosing implicit aspect based on the highest frequency of occurrences may create a bias which negatively in uences precision and recall. However, when we use the lexical adjective-to-aspect mapping, we improve both precision and recall values because the use of aspect disambiguation decreases the likelihood of bias.
 Explicit + Semi-implicit Aspect Extraction. Using semi-implicit aspect extraction together with explicit aspect extraction increases the recall from 23 : 50% to 65 : 62%, which is substantial. The precision is deceased because of the in-accuracies of choosing the best aspects using the lexical re-lations and similarity measures. The Wordnet :: Similarity itself is not optimized for comparing similarity between synsets other than nouns and verbs [13], therefore the draw-back in precision is understandable.
To assess the e ectiveness of our aspect extraction ap-proach, we use the set of entities provided by the baseline approaches after applying the extension techniques as shown in section 6.2.1. The set of entities would contain noisy in-formation and the goal is to analyze the impact of this noisy information on the precision and recall of aspect extraction. The results are shown in Table 6. We can observe that using NER tools decreases both precision and recall compared to ground truth data which is obvious. The interesting obser-vation is that AIDA is the one performing worse causing a decrease of 12% in precision and 22% in recall. The best in terms of precision is Zemanta, and the best in term of recall is NERD. These results are very consistent with the entity extraction ndings where AIDA is the worst, while Zemanta and NERD provide the best precision and recall values.
We have shown that our proposed techniques for extend-ing NER tools improve signi cantly precision and recall val-ues. Interestingly, AIDA performs best when we do not ap-ply any extension. This is because it already provides similar strategies to what we have proposed for entity ltering, name normalization, and context search. However, Zemanta and NERD, which are relatively much simpler, perform better than AIDA when we extend them with our proposed tech-niques. There are two bene ts from this result: (1) we do not depend on one NER tool but can use di erent tools to cover diverse types of entities while still guaranteeing high quality results; and (2) we can improve the performance of a simple NER tool using our light-weight extension techniques and outperform more sophisticated tools such as AIDA. The simplicity of our approach makes the results more e ective and the computation time faster.
 Another important point to consider is that AIDA uses Stanford NER which is supervised, while NERD and Ze-manta are unsupervised. In our work, we aim at giving more room to unsupervised techniques because in news do-main, new types of entities continue to emerge and we want to have a system that can handle that. For example, AIDA does not detect entities of type product, so we need to be able to use another NER to extract this type of entities.
Regarding aspect extraction, we have seen that we achieve 73% on recall and precision on the ground truth data. This result is very encouraging. We can also see that if the in-put is noisy, aspect extraction provides less accurate results because of the decrease in recall and precision of extracted entities. This is also due to some speci c reasoning that NER tools perform on entities, such as AIDA that labels \Scotland" as \United Kingdom" , \ISIS" as random entities, and \Ukraine as Russian Empire.Inaccurate labelling would then lead to inaccurate aspects.
We have presented, in this paper, a new approach for en-tity and aspect extraction from user comments. We have introduced light-wight techniques to extend existing NER tools using coreference resolution and comment context in-formation. We have de ned new forms of aspects that can be either explicit or implicit and proposed extraction tech-niques for each type. The experiments results have shown that our approach is promising giving new insights about in-formation extraction from user-generated content. In future work, we aim at testing our approaches on larger datasets, which is a tedious task because of manual annotation. More-over, we would like to investigate concept extraction to also be able to handle news articles talking about general con-cept, like vaccination or human rights. Furthermore, we need to investigate aspect extraction techniques to improve their performance. Some of our ideas include aspect normal-ization and gurative language analysis in aspect extraction. This work has been funded by the to-KNOW project and the European Master's Program in Computational Logic. [1] M. Asahara and Y. Matsumoto. Japanese named entity [2] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, [3] D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel. [4] C. Bizer, T. Heath, S. Auer, and T. Berners-Lee, [5] A. Borthwick, J. Sterling, E. Agichtein, and [6] H. L. Chieu and H. T. Ng. Named entity recognition: a [7] J. R. Finkel, T. Grenager, and C. Manning.
 [8] M. Hu and B. Liu. Mining and summarizing customer [9] D. Kitayama, N. Oda, and K. Sumiya. Organizing user [10] B. Liu, M. Hu, and J. Cheng. Opinion observer: [11] A. McCallum and W. Li. Early results for named [12] D. Nadeau and S. Sekine. A survey of named entity [13] T. Pedersen, S. Patwardhan, and J. Michelizzi. [14] A. Pons-Porrata, R. Berlanga-Llavori, and [15] S. Poria, E. Cambria, L.-W. Ku, C. Gui, and [16] K. Raghunathan, H. Lee, S. Rangarajan, [17] G. Rizzo and R. Troncy. Nerd: evaluating named [18] G. Rizzo, R. Troncy, S. Hellmann, and M. Bruemmer. [19] S. Sekine, R. Grishman, and H. Shinnou. A decision [20] S. Sekine and C. Nobata. De nition, dictionaries and [21] M. A. Yosef, J. Ho art, I. Bordino, M. Spaniol, and [22] L. Zhang and B. Liu. Aspect and entity extraction for [23] J. Gosh. Scalable clustering methods for data mining.
