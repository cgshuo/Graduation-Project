 The choice of indexing terms used to represent documents crucially determines how effective subsequent retrieval will be. IR systems commonly use rule-based stemmers to nor-malize surface word forms to combat the problem of not find-ing documents that contain words related to query terms by inflectional or derivational morphology. But such stemmers are not available in all languages. In this paper we explore the effectiveness of unsupervised morphological segmenta-tion as an alternative to stemming using test sets in thirteen European languages. We find that unsupervised segmen-tation is significantly better than unnormalized words, in several cases by more than 20%. However, rule-based stem-ming, if available, is better in low complexity languages. We also compare these methods to the use of character n-grams, finding that on average n-grams yield the best performance. H.3.1 [ Information Systems ]: Content Analysis and In-dexing X  linguistic processing, indexing ; H.3.3 [ Information Systems ]: Information Search and Retrieval Experimentation Tokenization, Stemming, Unsupervised Morphological Seg-mentation, Character N-grams
Stemming is an intentional conflationary technique de-signed to group together morphological variants of a word. This is frequently desirable because if a user query contains a gerund such as kayaking , it is likely that documents that have the words kayak or kayaker are also of interest. Even a relatively benign tokenization procedure like case-folding can create errors, and unfortunately all stemming algorithms will produce errors. For example, the popular Porter algo-rithm [8] conflates arm and army , words that do not belong in the same equivalence class.
 On early English collections (e.g., Cranfield, Medlars, and CACM) Harman found little advantage in stemming [3]; however Krovetz found larger differences using CACM, NPL, TIME, and WEST [5] which he ascribed to addressing de-riviational morphology. In English, where the number of inflectional forms is low, the observed differences are not large. Hull reported average improvements of 1 to 3% [4].
Since stemmers are not available in all languages, and even when available, require work to integrate with retrieval engines, we address the question of how effective unsuper-vised morphological analysis is for information retrieval. In Sections 2 and 3 we describe a recent evaluation that moti-vated this study and one of the best performing algorithms, Morfessor . In Section 4 we present experiments in thirteen languages comparing different ways of tokenizing documents and queries. We present our conclusions in Section 5.
Recent evaluations in unsupervised morphological analy-sis were held in 2005 and 2007. The first evaluation focused on word segmentation while the second evaluation had two tasks: (1) to analyze words, for example, determining that cats has the root cat and the +plural attribute; and (2) to apply unsupervised morphological analyses for informa-tion retrieval. For this IR task one year of CLEF data was used in English, Finnish, and German. Using the Lemur toolkit with Okapi term weighting nearly all of the anal-yses produced by competing systems, which could contain both segmentations and attributes, outperformed the base-line condition where words were left unaltered. Selected results (from Kurimo et al. [6]) are shown in Table 1; unsu-pervised segmentation led to 25-50% relative improvements over unprocessed words. One of the leading approaches was the Morfessor algorithm, which we describe next.
Morfessor 1 is designed to accomodate languages with con-catenative morphology. The algorithm does not restrict the number of morphemes that can be present in a single word and requires no explicit knowledge of a language; it takes as input only a list of words in the language, possibly with frequencies of occurrence. The output is a segmentation for each vocabulary word (e.g., affectionate represented as affect+ion+ate . The algorithm minimizes a cost function composed of two parts; one piece is based on how well the model represents the observed data, and the other part mea-sures the combined lengths of the segments (or codewords) that make up the model X  X  vocabulary [1].
Available from http://www.cis.hut.fi/projects/morpho/ Table 1: Selected results from Morpho Challenge 2007. Values are mean average precision.
 Top result 0.3943 0.4915 0.4729 Morfessor 0.3882 0.4412 0.4571
No analysis 0.3123 0.3274 0.3228
We used test sets from the CLEF ad hoc tasks between 2002 and 2007 [2]. The data consist of newspaper corpora that generally contain about fifty queries per year. In each language we used two years worth of data except for Czech where only one year was available.

The test collections were indexed using five separate meth-ods of tokenization: space delimited words; segments pro-duced by the Morfessor algorithm; stems produced by Porter X  X  Snowball software 2 ; and overlapping character n-grams with n = 4 or n = 5 as described by McNamee and Mayfield [7]. Common to each method was conversion to lower case, re-moval of punctuation, and truncation of long numbers. With Morfessor we used the default parameters and removed all tokens from the input vocabulary that contained digits.
Mean average precision (MAP) was used to assess perfor-mance and tests of statistical significance were conducted using the Wilcoxon test.
We directly compare plain words and the segments pro-duced by Morfessor in Table 2. Morfessor X  X  segments led to gains in 9 of 13 languages; 8 of these were significant improvements with p &lt; 0.05. The languages where words outperformed segments were English (dramatically), French, Italian, and Spanish  X  each is relatively low in morphologi-cal complexity. The differences in French and Spanish were less than 0.004 in absolute terms. Segments achieved more than a 20% relative improvement in Bulgarian, Finnish, and Russian, and over 40% in Czech and Hungarian.
Snowball does not support Bulgarian, Czech, or Russian and we had trouble using it in Portuguese and Hungarian. In the eight remaining languages we compared words, stems, segments, and n-grams. Due to space restrictions we re-port mean average performance averaged across the eight languages in Table 3. Without the higher complexity lan-guages, segments only have a 3.0% relative advantage over plain words instead of the 9.6% seen in Table 2. The use of stemming and n-grams is clearly advantageous.
The segmentations provided by Morfessor were statisti-cally superior to unnormalized words in 8 of 13 languages and significantly worse in 1 language, English. The gains were greatest in morphologically richer languages. In lower complexity languages, notably Romance languages where mature stemmers are widely available, rule-based stemming beat out unsupervised segmentation. Parameter tweaking might improve the unsupervised segmentation, but would Available from http://snowball.tartarus.org/ Table 2: Effectiveness of Morfessor Stemming. Bold values indicate statistically significant changes. Lang Data Queries Words Morfessor Average 0.3376 0.3701 (+9.64%) Table 3: Comparing 5 tokenization methods in 8 languages using macro-averaged MAP.

Words Snowball Morfessor 4-grams 5-grams 0.3943 0.4329 0.4060 0.4371 0.4450 conflict with the goal of language neutrality. Character n-grams did better than both segmentation with Morfessor and stemming with Snowball; however n-grams suffer from greater disk space requirements and longer query times.
Based on these results our suggestion is to use a stemmer, if feasible. When none is available and when the language is rich in morphology, unsupervised segmentation may be the most pragmatic alternative, unless the computational cost of character n-grams is acceptable. Both stemming al-ternatives techniques significantly improve on unlemmatized words.
 [1] M. Creutz and K. Lagus. Unsupervised discovery of [2] G. M. Di Nunzio, N. Ferro, T. Mandl, and C. Peters. [3] D. Harman. How effective is stemming? JASIS , [4] D. A. Hull. Stemming algorithms: A case study for [5] R. Krovetz. Viewing morphology as an inference [6] M. Kurimo, M. Creutz, and V. Turunen. Overview of [7] P. McNamee and J. Mayfield. Character n-gram [8] M. F. Porter. An algorithm for suffix stripping.
