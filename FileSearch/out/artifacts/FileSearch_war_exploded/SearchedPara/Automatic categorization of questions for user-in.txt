 1. Introduction
User-interactive question answering (also known as, community question answering ( Agichtein, Castillo, Donato, Gionis, &amp; Mishne, 2008 )) is becoming a very popular Web-based service with the emergence of Web 2.0. Unlike the traditional ques-tion answering (QA) systems which totally obtain answers automatically, the user-interactive QA systems serve as interac-tive platforms for users to help each other with human-provided answers, which overcome the shortcoming of poor quality of the automatic answers. Some well-known user-interactive QA systems, such as Yahoo! Answers serving hundreds of thousands of users X  requests each day. With the number of accumulated questions increases rapidly, it is necessary to organize these questions in a good way for users to browse and navigate them more conveniently.
Question categorization, which assigns one of the predefined categories to a new question according to the question X  X  topic or content, is a technique used for this purpose. For example, when the question  X  X  X ow many matches does a team play in a NBA league season? X  is posted, the QA system will automatically recommend a category  X  X  X asketball X  to the user because the question is talking about something about basketball. In this way, users could put their questions into appropriate cat-egories/boards and the system could also effectively organize a large numbers of questions. In this paper, each question is only assigned with one category even though there may be multiple categories suitable for it.

However, manually selecting a suitable category from a long list of categories is time-consuming and inconvenient for a user when asking a new question. Hence, an automatic method for question categorization is necessary to save users X  time and efforts, in which some (e.g., 3 X 5) candidate categories ranked by their relevancy can be recommended to the user auto-matically. If the user finds a suitable one, he only needs to click a button to confirm it. Otherwise, he can still choose the suitable one from the entire category list. Hence, it can save much effort if the system can recommend the categories accu-rately, which is exactly the motivation of this paper.

Currently, most text categorization techniques focus on the document-level texts, which usually consist of at least hun-dreds of words. In most of these techniques, statistical information such as Term Frequency (TF) is utilized to explore the importance of terms in documents based on Vector Space Model (VSM) ( Salton, Wong, &amp; Yang, 1975 ). However, questions, consisting of only a few words, do not necessarily share a lot of common words. If we use a high dimensional space to rep-resent questions, the vectors will be very sparse, which may leads to poor performance. Moreover, in a question, different words may contribute differently. For example, the words implying the question focus ( Moldovan et al., 2000 ) may play more important roles than other words. These words should be given a higher weight for categorization task.
In this paper, we propose an automatic method for question categorization in user-interactive QA systems. In the pro-posed method, we select the important terms extracted from accumulated questions as features to construct a feature space.
Each category is represented by a vector in the feature space. These feature vectors only need to be calculated once and loaded into memory for the later categorization task. For each new question, we firstly represent it as an initial word vector using the Term Frequency weighting method. For the words which directly indicate the information about the question X  X  sidered to play more important roles in question and can be extracted using the semantic pattern ( Hao, Hu, Liu, &amp; Zeng, structed by calculating word similarity between each word in the question and each word of the feature space based on
WordNet ( Fellbaum, 1998 ). The initial question vector is then mapped into the feature space to enrich its semantic repre-sentation by multiplying the initial question vector with the word similarity matrix. To categorize a new question, we cal-culate the similarity between the new question vector and each category vector. The similarity scores are sorted in the descending order and the top k (which is three in this paper) ranked categories are suggested to the user. Experimental re-sults show our proposed method achieves good precision and outperforms the traditional categorization methods on se-lected test questions.

The rest of the paper is organized as follows. Section 2 briefly reviews related work. In Section 3 , we describe our pro-posed method in detail, which includes feature space construction, topic-wise words identification and weighting, semantic mapping, and similarity calculation. Section 4 shows our experimental results and Section 5 concludes this paper and pro-poses future work. 2. Related work
Question topic categorization can be considered as a particular type of (short) text categorization problem, which has been investigated in many literatures. Most of the text categorization methods, including Na X ve Bayesian (NB) classifier ( McCallum machine learning to construct a classifier and use it to categorize new documents. However, most of these categorization techniques are based on some underlying models or assumptions. For instances, SVM is based on the Structural Risk Minimi-zation principle and Na X ve Bayesian classifier is based on the assumption that the probabilities of words are independent. When the data fit the model or satisfy the assumption, the performance is usually good; otherwise, it may be quite poor.
Some techniques have been proposed to improve the performance of text categorization. A refinement approach to hand-ing model misfit is proposed by Wu, Phang, Liu, and Li (2002) , which makes use of training errors to successively refine the classification model on the training data. Based on the prediction errors on the training data, a sub-classifier is retrained using the training examples of each predicted class with the same learning method. By forcing the classifiers to learn from refined regions in the training data, the model becomes more and more strong and fits the training data better. Shehata, Kar-ray, and Kamel (2007) propose a novel concept-based model to enhance text categorization, which combines the work ( Shehata, Karray, &amp; Kamel, 2006a, 2006b ) into one concept-based model. This concept-based model analyzes terms at the sentence and document levels instead of document level only. It can effectively discriminate between important and non-important terms with respect to sentence semantics and terms which hold the concepts of the sentence. Scott and Matwin (1999) , as well as Wermter and Hung (2002) , use WordNet ( Fellbaum, 1998 ) to transform document representation from a bag of words to a bag of synsets by using the hypernymy relation to generalize word senses. However, in some categorization task, using the synsets only is not enough and some words in the synsets may be useless. Keywords weighting has also been studied ( Hulth &amp; Megyesi, 2006 ), which does comparison in different ways of key words representation.
Question categorization is a bit different from normal text categorization. This is mainly due to the special properties of questions compared with normal text. First, questions contain much less statistic information than long documents. If we use traditional statistic method, the question vector may be very sparse. Second, questions have some additional information important role than other information of the question.

There have been some research works ( Li &amp; Roth, 2002; Zhang &amp; Lee, 2003 ) on question type categorization, which cat-egorize questions into some semantic classes that impose constraints on potential answers, like  X  X  X erson X ,  X  X  X ocation X ,  X  X  X ef-inition X , and so on. Such constraints can be utilized to find the right answers. In question type classification, the about time. Thus, some heuristic rules can be constructed to assist the categorization. However, in question topic categori-zation for a user-interactive QA system, the situation is different. The categories are predefined according to the question sion. For example, two questions  X  X  X ho invented computer? X  and  X  X  X hen did human invent computer X  belong to the same category  X  X  X omputer X . Hence, the content of the question plays an important role in question topic categorization and the interrogatives may be not as important as in question type classification. 3. The proposed method
Our proposed method for question categorization includes four important steps: feature space construction, topic-wise words identification and weighting, semantic mapping, and similarity calculation. In feature space construction, we use the features of all categories to construct a feature space and obtain the vector representation of each category from the question database. The feature space is later used for semantic mapping and similarity calculation. Topic-wise words iden-tification and weighting is used to identify some topic-related words in a question and give them a higher weight in the vec-tor representation. Semantic mapping maps the initial question vector into the feature space constructed in the first step to obtain its new vector representation which is semantically enriched. The similarity between the question and categories is then calculated based on their vectors in the feature space and the categories which have the highest similarities are recom-mended to users. Fig. 1 shows the workflow of our method. 3.1. Feature space construction
We select the important terms extracted from accumulated questions as features to construct the feature space. The accu-mulated questions have been posted previously and each of them has been manually assigned to one of the predefined cat-egories. We assume that these questions are always correctly assigned. Since a question contains only a few words, we use word-level features. Before feature selection, stop words are removed and the remaining words are stemmed to their roots.
Information gain (IG) is chosen as the selection criteria, which has been proved to be quite effective in previous research ( Sebastiani, 2002 ). The higher information gain a feature gets, the more strongly the feature indicates the presence or ab-sence of a category. The information gain of a term t is defined as follows: where, IG( t ) is the information gain of feature t ; P(c to the number of questions containing t divided by the number of all questions; P  X  t  X  the probability that term t does not appear in a question; P(c i | t ) the probability of category c c given that term t does not appear.

All the extracted words are sorted by their information gain scores defined in Eq. (1) in the descending order and the top n words are selected as features.

For each category, we use a vector to represent it. The vector of each category consists of the weights of the selected fea-tures for the category. We take into account the impact of the features in a specific category and the whole corpus. We define feature as the number of categories in which it occurs. Each category is represented by a vector as follows: where, v c i is the vector representation of category c i tioned and wc ij is the weight of the j th feature in category c where, M is the total number of categories in the corpus, TF
Category Frequency of the j th feature. If we consider a category as a document, our model is exactly the traditional TF-IDF model for text retrieval. 3.2. Topic-wise words identification and weighting
Topic-wise words in a question are the words which directly indicate the question topic. From the topic-wise words, users can easily infer the question category. For example, in the question  X  X  X hat book did Rachel Carson write in 1962? X , the word  X  X  X ook X  and  X  X  X achel Carson X  are two topic-wise words because from these two words we can infer that the question category is the name of a writer. The reason why we do not select  X  X 1962 X  as topic-wise word is that time (year) usually cannot deter-mine the category of a question. For example,  X  X 1962 X  can appears in any kind of questions, such questions may be about political events, sports or other categories. Since topic-wise words are more relevant to question category, they play more important roles than other words in the process of categorization. For each question, we identify the topic-wise words and give them a higher weight than other words during the categorization process.

We use the semantic pattern ( Hao et al., 2008 ) to identify the topic-wise words in a user X  X  question. This semantic pattern is defined based on surface pattern, in which semantic information is added for knowledge acquisition and machine under-standing. The semantic pattern is mainly composed of five components: Question_Target , Question_Type , Concept , Event and
Constraint . Question_Target is used to represent the exact information of what the question is asking for. Question_Type is used to represent the type of question, for example,  X  X  X hen X ,  X  X  X hat X , and  X  X  X hy X . Concept is used to label meaningful noun of question, which is represented by a two-level concept hierarchy, such as  X  X  X uman/individual X . The second level is a con-an instance of this class in a real question. Event is used to represent something happening or any specific behavior. Con-
Among these components, the Concept part contains the main information of a question. Hence, the words in this part are select a pattern from the ones our system suggests or from the entire pattern list to ask this question. He may also not use and the contents of these blanks can be identified as topic-wise words.

Take the above question as example, its semantic pattern is shown in as follows: &lt;Target: Entity n Product&gt;&lt;Type: What&gt;[Entity n Product] did [Human n Individual] write in 1962? where, [Entity n Product] and [Human n Individual] are labels of two Concept parts. They are two blanks and their contents are filled in by the user with the word  X  X  X ook X  and  X  X  X achel Carson X . These two words can be extracted as topic-wise words for the question categorization task.

Since the topic-wise words are important to a question, we give them a higher weight than other words in the question each word in the question is weighted using its Term Frequency. The topic-wise words are then extracted using the above semantic pattern method. For each topic-wise word in the question, we give it a higher weight by multiplying its original weight with a coefficient k which is greater than 1. In this way, when we calculate the similarity, the word with higher weight plays a more important role as expected. 3.3. Semantic mapping After topic-wise words identification and weighting, the question vector is mapped into the feature space constructed in
Section 3.1 to enrich its semantics. Let q denote a new question, the vector where, m is the number of words in q , tf i is the Term Frequency of the i th word in question q . w It is equal to k if the i th word is a topic-wise word; otherwise, it is equal to 1.
 Then we construct a word similarity matrix S between the words in question q and all words in the feature space: where, n is the number of words in the feature space, s ij similarity s ij is calculated based on WordNet ( Fellbaum, 1998 ).

WordNet ( Fellbaum, 1998 ) is a semantic dictionary which is structured as a hierarchical net where words are organized into synonym sets (synsets) with semantic pointers to other synsets. Each synset is denoted as a node in the net. A word in the net can reach any other word in the net through one or more path. With the structure of WordNet, we can see that if two each other. We use the minimum length of path ( Rada, Mili, Bichnell, &amp; Blettner, 1989 ) to measure the similarity of two words, which has been proved to be a simple and effective method by Fellbaum (1998) and Okazaki, Matsuo, Matsumura, and Ishizuka (2003) . The length of a path ranges from 0 to infinite while the range of similarity is [0, 1]. When the path the similarity should monotonically decrease to 0. We use the following formula to calculate the similarity between two words: where, Dis ij is minimum path length between the two compared words in WordNet and c is a non-negative threshold. We can see that if the two words are the same or in one synset in WordNet, Dis in WordNet, they are treated as totally dissimilar.

After the word similarity matrix S is constructed, we can map the question vector the vector v q with the matrix S : where, v 0 q is the question vector in the feature space after mapping. In this way, we convert the m -dimensional vector the n -dimensional vector v 0 q . It is obvious that after the mapping, the question vector representation is enriched by the semantic relationships between the words in question and the words in feature space. 3.4. Similarity calculation
The similarity between the question and a category can be calculated based on their vectors in the feature space using the cosine measure: where, sim  X  q ; c i  X  is the similarity between the question q and the i th category c used to judge the relevancy between the question and each category. When a new question is posted, the similarity between the question and each category is calculated and the categories with the highest scores are suggested to the users. 4. Experiments
In this section, we firstly describe our dataset and then present the evaluation method and the experimental results. 4.1. Experimental setup
The dataset in our experiment is taken from the pattern-based user-interactive QA system X  BuyAns ( Liu et al., 2009 ), in the ones the system suggests or the entire pattern base. For example, when a user wants to ask the question  X  X  X hat book did Rachel Carson write in 1962? X , the process of operations are as follows: Step 1: Type the question content and click a confirm button to go to next step.

Step 2: Select a pattern from the ones the system suggests to ask the question. For example, for the above question, the system may suggest the following three patterns: (1) What did [Entity n Term]? (2) What [Human n Title] [Event n Action]? (3) What [Entity n Product] did [Human n Individual] write in 1962? son X  in the second blank. We ask the user to manually check/confirm and/or fill in the content because the content automat-ically filled in by the system may be incorrect sometimes. If the blanks are filled in accurately by the system, the user only needs to click a button to confirm it. If the user does not want to use pattern, he can also choose the option  X  X  X se original question X .
 Step 3: Submit the question.

We use two data collections in our experiment. One is obtained from BuyAns ( Liu et al., 2009 ), which contains 6000 train-ing questions and 500 testing questions from 60 categories. may have as few as 30 questions. The other question collection is selected from a data set about 200,000 questions from 100 categories. We select 10,000 questions from it, 9000 for training and 1000 for testing. The questions selection is conducted by two steps. The first step is to randomly select a set of questions from each category and the second step is to discard the noise questions by manual inspection. The proportion of questions for training and testing Answers data set respectively based on extensive testing.

In BuyAns ( Liu et al., 2009 ), when a user asks a question, the question is automatically categorized and a few categories with higher relevancy are recommended to the user to choose. However, it is not suitable to suggest too many categories for one or several highly relevant categories for the user to confirm or to choose. In our system, we suggest at most three cat-egories for each new question. Hence, we use precision at k (P@ k ) as the performance metrics in our experiment, which means the proportion of questions whose correct category is within the top k categories our system suggests. For example,
P@3 = 0.5 means that for half of the questions, their correct categories are among the top three categories our system sug-programs available for download? X  could belong to either  X  X  X oftware X  or  X  X  X nternet X . In this situation, categorizing the ques-tion into either of the two categories should be considered as correct when calculating the precision. However, to simplify the evaluation, we do not consider multi-label categorization and hold only one valid category for each question in the ground-truth. 4.2. Experimental results
We need to set 2 parameters during the experiment, which are the coefficient k for topic-wise words, and the threshold c for distance. The optimal parameter setting is obtained through a cross-validation process on the training data. We divide the average performance of the six rounds is used to evaluate each setting of parameters. Such process can be also applied if retraining is conducted on a different dataset for another problem domain.

Table 1 shows the optimal k settings as c increases. We can see that k increases from 3 to 5 or 6 (5 for the BuyAns data set and 6 for the Yahoo! Answers data set) as c increases from 0 to 6. When c is larger than 6, k keep unchanged. This indicates that giving topic-wise words a higher weight than other words in a question is reasonable since the topic-wise words play more important roles. There are two reasons why k keeps unchanged when c is larger than 6. The first reason is when k is too question become trivial compared to the topic-wise words because the weight of the latter is too high. The second reason is word have little impact on word similarity calculation. In other words, if two words are far away from each other in WordNet ( Fellbaum, 1998 ), they can be treated as totally dissimilar.

BuyAns data set and 2 for the Yahoo! Answers data set). This indicates that the most useful words around the current word are within a distance 4 or 2 when calculating word similarity based on WordNet ( Fellbaum, 1998 ). The words whose distance is farther than 4 or 2 can be ignored.

Figs. 2 and 3 show the categorization precision with different values of c . The other parameter k is set to the optimal value in Table 1 . We can see that our method achieves the best P@1 and P@3 when c is 4 and the best P@2 when c is 5 for the
BuyAns data set. For the Yahoo! Answers data set, our method achieves the best P@2 and P@3 when c is 2 and the best P@1 when c is 1. This indicates that the words near the current word (within distance 4 for the BuyAns data set and 2 for the
Yahoo! Answers data set) are useful for word similarity calculation in WordNet ( Fellbaum, 1998 ). This is because when c increases, the larger area around the current word is considered during the process of word similarity calculation. For exam-ple, when c = 0, we only consider the synonymous words of the current word; when c = 1, any word which is within distance 1 to the current word is counted. Hence, more semantic information is obtained when c increases. Another observation is that when c is larger than a certain value (5 for the BuyAns data set and 3 for the Yahoo! Answers data set), the precision decreases as c increases. This may be due to the reason that when c becomes larger and larger, more and more dissimilar words are considered during the process of similarity calculation. These dissimilar words may cause ambiguity between the current word and other words, which may cause negative effect to word similarity calculation. Hence, the precision decreases.

Figs. 4 and 5 show the categorization precision with different values of k . The other parameter c is set to the optimal value to 8 or 9 (8 for the BuyAns data set and 9 for the Yahoo! Answers data set). Finally, the precision keep unchanged when k is larger than 8 or 9. This indicates that giving topic-wise words a higher weight than other words is helpful to improve the categorization precision and this proves that topic-wise words indeed plays a more important role than other words. The reason why the precision decreases a little when k increases from 4 (5) to 8 (9) is mainly due to that when the weight of topic-wise words is too high, other words X  impact becomes trivial and the useful information other words contain is missed.
Hence, topic-wise words should not be over-weighted. When k is larger than 8 or 9, other words have little impacts which can be ignored compared to topic-wise words. Hence, the precision keeps stable though the weight of topic-wise words increases.

We also can see that the discovered optima of the two experimental corpora differ from each other. The two datasets are different in the aspect of the tested categories and the words/languages used. For example, BuyAns  X  users are just a few hun-dred Chinese users who may not be good at using English. Hence, the vocabulary used is much smaller and may be even different from the vocabulary used in the Yahoo! Answers data set (which is much larger and more complicated). Hence, the optimal c in BuyAns data set is larger than Yahoo! Answers data set. However, we did not check the underlying relation between the vocabulary and c , which is considerably very complicated. We will investigate it in our future work. The optimal our future work.

We compare our method with different classifiers: NB, KNN and SVM. In KNN of vector is TF-IDF weighting plus topic-wise words identification, weighting and semantic mapping. In our method, each test-ing question is also represented by the two kinds of vector respectively. We conduct two experiments. The two parameters ( k and c ) are set to the optimal values. Tables 3 and 4 show the comparison precision of these methods on BuyAns and Yahoo!
Answers data set respectively. From the two tables, we can see that our method with expanded vector achieves the best pre-cision among all the methods. By making comparison between the basic vector and expanded vector, we can see that topic-wise words identification, weighting and semantic mapping are helpful to improve question categorization performance of our meth-od. The reason why it hurts the performance of KNN and SVM may be that the expanded features in each vector cause a negative words in questions are related to each other and they usually occur together. Hence, the assumption of independence of NB is not appropriate here. 5. Conclusions and future work
In this paper, we propose an automatic method for question categorization in a user-interactive QA system, which uses the semantic pattern and semantic relationship between words to enrich the question representation. At first, a feature space is constructed and the vector of each category is calculated based on the accumulated questions. For a new question, we use the semantic pattern to extract the topic-wise words which are considered important and given a higher weight. After that, a similarity matrix is constructed based on WordNet ( Fellbaum, 1998 ) which is used to map the question into the feature space. Finally, the similarity between the question and each category is calculated and the question is suggested to be as-signed to the categories which have the highest similarities. The experimental results show that both the topic-wise words identification and weighting, and semantic mapping are helpful to improve the categorization precision.

In our future work, we will improve the feature selection method to select more suitable features for each category. We will also investigate the underlying relation between the discovered optima and corpora characteristics. The user activities which are very important to a user-interactive QA system will also be considered. For instance, we will make use of the user feedback to revise our categorization method. Both positive and negative feedbacks will be considered to refine our method. Acknowledgements The work described in this paper was fully supported by a grant from City University of Hong Kong (Project No. 7002488). References
