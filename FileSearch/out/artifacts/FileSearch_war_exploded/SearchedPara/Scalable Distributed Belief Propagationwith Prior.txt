 Belief propagation (BP) is a popular method for perform-ing approximate inference on probabilistic graphical mod-els. However, its message updates are time-consuming, and the schedule for updating messages is crucial to its running time and even convergence. In this paper, we propose a new scheduling scheme that selects a set of messages to up-date at a time and leverages a novel priority to determine which messages are selected. Additionally, an incremental update approach is introduced to accelerate the computa-tion of the priority. As the size of the model grows, it is desirable to leverage the parallelism of a cluster of ma-chines to reduce the inference time. Therefore, we design a distributed framework, Prom, to facilitate the implementa-tion of BP algorithms. We evaluate the proposed scheduling scheme (supported by Prom) via extensive experiments on a local cluster as well as the Amazon EC2 cloud. The eval-uation results show that our scheduling scheme outperforms the state-of-the-art counterpart.
 I.2.6 [ Artificial Intelligence ]: Learning Belief Propagation; Distributed Framework; Prioritized Block Updates; Incremental Updates
Probabilistic graphical models have been used for reason-ing in a wide range of application domains [9, 13, 23, 29, 32]. Inference in these models, including marginalization and maximum a posteriori estimation, forms the basis of many statistical methods in knowledge management. Usually, ex-act inference in a probabilistic graphical model is NP-hard. As a result, there have been many approaches on introducing both variational and sampling approximations to inference. Among them, loopy belief propagation (BP) and its vari-ants [12, 19, 21, 25] are popular message passing methods for performing approximate inference.

It has been shown that the schedule for updating mes-sages can make a huge difference to the running time of BP algorithms. Specifically, dynamic scheduling schemes, which determine the order of updating messages by the changes of message values, can significantly speedup BP algorithms [6 X 8, 22]. Although dynamic scheduling schemes have po-tential to speedup BP algorithms, existing ones cannot fully utilize the potential. Most of them typically select one mes-sage for updating each time, e.g., the message with the high-est priority value. As a result, many operations need to be performed so as to select next message. That is, the cost of realizing such a dynamic scheduling scheme is high.
In this paper, we propose to select a set of messages in-stead of a single one to update at a time. Hence, the amor-tized cost of selecting one message is low. Moreover, a novel priority is leveraged to determine which messages are se-lected. In other words, we present a prioritized block schedul-ing scheme, which selects a block of messages to update via a priority. The priority allows messages that are more useful towards achieving convergence to be selected, and the com-putation cost of the priority is low. To this end, we introduce an efficient incremental update mechanism, which propa-gates only the changes of original messages. The change of a message is efficiently computed using the changes of orig-inal incoming messages. Also, the change can be directly utilized to calculate the priority. We refer to this mecha-nism as an incremental-update approach.

As the probabilistic graphical models are applied to model large and complex applications, such as image restoration for high-resolution images, it is desirable to leverage the paral-lelism of a cluster of machines to reduce the inference time. Therefore, we design and implement a distributed frame-work, Prom , which facilitates the implementation of BP and other graph algorithms in a distributed environment. Prom uses the proposed scheduling scheme as its built-in scheduling and supports the incremental-update approach. We evaluate two BP algorithms, the sum-product algorithm and the max-product algorithm on Prom, on a local cluster of machines as well as the Amazon EC2 cloud [1].

More specifically, our main contributions are as follows:
Probabilistic graphical models, such as Bayesian networks, factor graphs, and pairwise Markov Random Fields (MRFs), are popular tools to capture uncertainty in real-world ap-plications. Without loss of generality, we consider factor graphs, since any other graphical models can be converted to factor graphs [13]. A factor graph is a bipartite graph with two types of vertices: variable vertices and factor ver-tices. Each variable vertex represents a single random vari-able (e.g., x i ). Each factor vertex (e.g., f j ) denotes a func-tion that maps a subset of random variable values (e.g., X to a non-negative real-valued number so as to capture the compatibility of an assignment to those variables. The argu-ments are graphically represented by edges, which connect a particular function vertex with its variable vertices. There-fore, a factor graph is a factored representation of a joint probability distribution: P ( x 1 ,x 2 ,...,x n ) = 1 Z Q where Z is the normalization constant.

We next briefly review two BP algorithms, the sum-product algorithm and the max-product algorithm, and then discuss asynchronous BP algorithms.
Marginal probabilities of the distribution represented by a factor graph are central to inference. The sum-product al-gorithm provides an efficient way to compute marginal prob-abilities on a factor graph. It propagates messages in both directions along edges. Each vertex sends and receives mes-sages till reaching a stable situation, and then the incoming messages are used to estimate the marginal probabilities of the vertex. Let m i  X  a ( x i ) and m a  X  i ( x i ) denote the message sent from variable vertex x i to factor vertex f a and the mes-sage sent from f a to x i , respectively. They can be updated by the following equations: where N ( i ) \ a denotes the set of neighbors of a given vertex i ( x ) excluding vertex a ( f a ), and  X  is a normalization factor to ensure all elements of the messages sum to 1.

The belief at a variable vertex (e.g., i ) is proportional to the product of all the messages coming to the vertex: b probability is P ( x i )  X  b i ( x i ). While the sum-product algo-rithm converges to the exact marginal probabilities in acyclic graphs, there are no guarantees of convergence or correct-ness for graphs with loops. Nonetheless, the sum-product algorithm is widely applied on cyclic graphs for approximate inference with great success [4, 17, 26].
In some cases, we are interested in determining which valid configuration has the largest probability, rather than deter-mining the marginal probabilities for the individual vari-ables. The max-product algorithm addresses this problem efficiently. Message updates in the max-product algorithm are similar with those in the sum-product algorithm. In fact we only need to replace P with max in computing factor-to-variable messages. The message updates in the max-product algorithm are as follows:
We can represent each message as a vector in the vector space S  X  R d , and represent an entire set M of messages as a vector in S | M | . The BP algorithm can be considered as the iterative algorithm with an update function F : S | M | S
BP aims to find a fixed point m  X  where m  X  = F ( m  X  ). BP is guaranteed to converge to a unique fixed point m  X  , if the update function F is a contraction under a message norm, where the message norm k X k measures the distance between messages. If F is a max-norm contraction, then we have k F ( m )  X  m  X  k  X   X   X  k m  X  m  X  k  X  , where the max-norm k X k is defined as the maximum of the individual message norms, k m use the max-norm to measure the convergence of BP. Mooij and Kappen [18] present sufficient conditions for F to be a contraction under the max-norm.

Function F can also be viewed as a set of individual func-tions, and each individual function F i applies to one mes-sage. These individual update functions can be used to de-fine synchronous BP and asynchronous BP. In synchronous BP , the functions compute the new values of all messages simultaneously at every iteration using their values from last iteration. In asynchronous BP , the functions update mes-sages using the most recent values. The convergence rate of asynchronous BP (with a pre-defined update order) is proven to be at least as good as that of synchronous BP [6].
For asynchronous BP, it has been shown that the dy-namic scheduling, which uses a priority to determine the order of updating messages, converges much faster than the static scheduling [6 X 8,22]. The intuition behind the dynamic scheduling is that sending a message whose current value is very different from its previous value is perhaps more useful, and thus leads to more rapid transfer of information across the graph, while sending a message whose value does not change is useless.
The general techniques of incremental updates have shown efficiency in many algorithms, such as Nonnegative Matrix Factorization [27] and Expectation-Maximization [28]. In this section, we present an incremental update mechanism for BP algorithms, referred to as an incremental-update ap-proach. In contrast, the traditional way of updating mes-sages (described in the previous section) is referred to as a basic-update approach. The incremental-update approach propagates only the incremental part (change) of the original message. The message update in the incremental-update ap-proach can be performed by accumulating incoming changes instead of computing from scratch, and thus is much more efficient than that in the basic-update approach. Further-more, since it usually calculates the priority value using the changes of messages, the dynamic scheduling can benefit from the incremental-update approach.

The basic idea of the incremental update is inspired by the Hugin architecture [5], an approach proposed for the ex-act inference. It uses an efficient way to update messages, which computes the marginal of a vertex as the product of messages once and then divides a message out from the marginal when one needs to update a message. However, the incremental update we proposed aims to support asyn-chronous computation. The order of asynchronous compu-tations is based on a priority-based scheduling. The mes-sage computed by our incremental update can be directly used to derive priority, while there is no concept of prior-ity in the Hugin architecture. Furthermore, our incremental update performs log-space calculations, so it can use addi-tion/subtraction to update messages, while the Hugin archi-tecture uses more expensive multiplication/division.
To derive an incremental update mechanism for a BP al-gorithm, we treat messages in log-space. A message in log-space is the logarithmic equivalent of the original message, i.e., m ( x i ) = ln m ( x i ).
When the messages are in log-space, the message compu-tation for the sum-product algorithm is as follows: where m ( x i ) = ln m ( x i ),  X  = ln(  X  ), and g t  X  1 a  X  i P
We can make a slight modification to Eq. (5) in which we omit normalization factor  X  . As Pearl [19] pointed out, normalizing the messages is only for avoiding numerical un-derflow and makes no differences to the final beliefs. Since we still keep the normalization factor in Eq. (6) and mes-sages are in log-space, there is no numerical underflow prob-lem. Then, the message computation can be performed in-crementally. The message m t i  X  a ( x i ) can be incrementally computed as follows: where m 0 i  X  a ( x i ) = 0, and  X  m 0 k  X  i ( x i ) = m initial message.

In our incremental-update approach, a vertex sends the incremental part of the original message instead of the mes-sage itself. For example, vertex x i sends message  X  m t i  X  a to factor vertex f a . In order to compute the belief, variable vertex x i also accumulates the messages received from its
The function g a  X  i ( x j ) in Eq. (6) can be also incrementally computed. We have
Then, the incremental message sent from factor vertex f a to variable vertex x i can be computed as follows: where m 0 a  X  i ( x i ) is the initial message. Factor vertex f
Since the incremental-update approach uses only new in-coming incremental messages to compute outgoing incre-mental messages, the complexity of computing an outgoing message for a vertex depends on the number of new incoming messages the vertex has received (since last update) rather than the vertex X  X  degree. This is highly useful especially in the asynchronous communication model (e.g., under the dy-namic scheduling), in which only part of a vertex X  X  incoming messages may be updated when the algorithm computes its outgoing messages. In contrast, the basic-update approach always computes messages from scratch no matter how many incoming messages are updated. Its computation complex-ity is determined by the vertex X  X  degree.
When the messages are in log-space, the message compu-tation for the max-product algorithm is as follows: m P
The only difference in computing messages between the max-product algorithm and the sum-product algorithm is that the former one replaces P with max in computing factor-to-variable messages. As a result, the message update for the max-product algorithm can be performed incremen-tally as well. Computing the incremental variable-to-factor message is the same with that in the sum-product algorithm (so is g a  X  i ( x j )). Here, we only show how to incrementally compute the factor-to-variable message. The incremental message sent from factor vertex f a to variable vertex x i be computed as follows: where m 0 a  X  i ( x i ) is the initial message. Factor vertex f
Using mathematical induction, it is straightforward to ver-ify that performing message updates traditionally and per-forming message updates incrementally are equivalent.
In this section, we present our scheduling scheme, which is inspired by the residual scheduling [6]. The residual schedul-ing leverages the difference in values of the message before and after the update as the residual of the message. By giv-ing the message with high residual a high execution priority, the BP algorithm can potentially converge fast. The resid-ual scheduling uses a priority queue to order all outgoing messages X  residuals. Every time it sends out the outgoing message with the largest residual in the priority queue and then updates the queue.

The issue of the residual scheduling is that it has high overhead. It always selects one message to update at a time. Once the message is updated, it needs to recompute the pri-orities of the messages that have been affected and maintain the priority queue so as to select next message. Moreover, the residual scheduling determines a message X  X  priority by actually computing the message. Many messages are com-puted only for the purpose of obtaining their priority values, and are never sent out. As a result, in order to select one message, many operations have to be performed.

Our scheduling scheme selects a set of messages instead of a single one to update each time so as to reduce the cost. It utilizes a priority to determine which messages are selected. In addition, we also present a novel priority, which allows messages that are more useful towards achieving conver-gence to be selected (without actually computing the mes-sages in advance).
Our scheduling scheme is over vertices. That is, when a selected vertex is updated, all its outgoing messages will be computed and sent out. Scheduling over vertices rather than messages can reduce the cost of selecting messages, since a vertex usually has at least several messages. Updating a vertex always uses the most recently available data (i.e., incoming messages). Our scheduling scheme selects a block of k vertices to update each time. Once the block of selected vertices are updated, it selects another block of vertices to update. A priority is used to determine which vertices are selected. Every time our scheduling scheme selects the top-k vertices in terms of the priority value. Since our scheduling scheme selects a block of vertices to update via a priority, we refer to it as the prioritized block scheduling .
The size of the block (i.e., k ) balances the tradeoff between the gain from the prioritized block scheduling and the cost of selecting the k vertices. Setting k too small may incur considerable cost, e.g., when k = 1, the prioritized block scheduling can be in principle seen as a vertex-based version of the residual scheduling (since it selects one vertex to up-date at a time). Setting k too large may degrade the effect of the prioritized block scheduling, e.g., if setting k as the number of vertices, it degrades to the round-robin schedul-ing. We will show in experiments (Section 6.3) that a quite large range of k can allow the prioritized block scheduling to have better performance than the round-robin scheduling.
The prioritized block scheduling uses an efficient way to select the top-k vertices. The naive way is to first sort all the vertices by their priority values and then pick the top ones. However, sorting all the vertices can be expensive and time consuming (at least O ( n log n ) time). Instead, the prioritized block scheduling first finds the vertex with the k -th largest priority value. Then, it utilizes the k -th largest priority value as a threshold to filter the vertices. That is, it scans all the vertices once and picks only the vertices with larger or equivalent priority values. Randomized-Select [3] is utilized to find the k -th largest value. It has an expected running time of O ( n ). In this way, the prioritized block scheduling takes O ( n ) time (including the time in scanning all the vertices) in extracting the top-k vertices.
Our prioritized block scheduling has much lower cost of selecting one message than the residual scheduling. Updat-ing one message in the residual scheduling needs to reset the message X  X  residual and adjust the dependent messages X  residuals (the messages sent from the message X  X  destination vertex). Assuming the degree of the message X  X  destination vertex is d , there are ( d  X  1) dependent messages. We know that adjusting an element X  X  priority value in a pri-ority queue with n elements typically needs O (log n ) time. Given a factor graph with | V | vertices and | E | edges, there are O ( | E | ) messages in the priority queue. Hence, select-ing a message to update in the residual scheduling needs d  X  O (log | E | ) time, O (log | E | ) for the selected message itself and ( d  X  1)  X  O (log | E | ) for the ( d  X  1) dependent messages. In our prioritized block scheduling, selecting k vertices to update only needs O ( | V | ) time. Suppose the averaged de-gree of these k vertices is d 0 . Then, ( k  X  d 0 ) messages will be updated once the k vertices are selected. As a result, the amortized cost of selecting one message to update in our k (e.g., k is one tenth of | V | ), the cost is low and much lower than that in the residual scheduling.
We define the residual of an incremental message  X  m ( x i as its L 1 -norm (in log-space), Next, we derive the priority utilized in our prioritized block scheduling for the sum-product algorithm and for the max-product algorithm, respectively. The priority is vertex-based, and the priority of a vertex is directly computed from the residuals of its incoming messages.
For any outgoing message sending from a variable vertex (e.g., i ), its residual can be computed as follows: Therefore, we use the summation over all assignments of incoming messages in log-space, as the priority of a variable vertex ( i ), which well approx-imates the residual of each individual outgoing message of the variable vertex.

For any outgoing message sending from a factor vertex (e.g., a ), its residual can be computed as follows: Applying the fact for any y 1 &gt; 0 ,y 2 &gt; 0 ,z 1
Applying the fact for any y 1 &gt; 0 ,y 2 &gt; 0 ,z 1 &gt; 0 ,z
We have derived the lower bound and the upper bound for r ( X  m a  X  i ). Then, we use a value between these two bounds to approximate r ( X  m a  X  i ). Let v a  X  i = P x states of X j \ x i . We can see that (since v a  X  i age) v a  X  i is between those bounds. Therefore, we use v to approximate r ( X  m a  X  i ), and use the summation of av-eraged values over all assignments of incoming messages in log-space, as the priority of a factor vertex ( a ). Intuitively, this prior-ity well captures the importance of new incoming messages available to the factor vertex.
The message update for a variable vertex in the max-product algorithm is the same with that in the sum-product algorithm. Accordingly, the priority for a variable vertex defined in the sum-product algorithm also applies to the max-product algorithm. Next, we derive the priority for a factor vertex in the max-product algorithm.

For any outgoing message sending from a factor vertex (e.g., a ), its residual can be computed as follows: Applying the fact for any y 1 &gt; 0 ,y 2 &gt; 0 ,z 1 equations:
Applying the fact for any y 1 &gt; 0 ,y 2 &gt; 0 ,z 1 inequations:
From the above inequations, we can see that the max-product algorithm has the same bounds for the residual of an outgoing message sending from a factor vertex as the sum-product algorithm. Accordingly, the priority for a factor vertex defined in the sum-product algorithm applies to the max-product algorithm as well.

The defined priority uses summation to aggregate incom-ing messages, and thereby we call it the sum priority . From the above derivation, we can see that the sum priority has strong connections with the residuals of its outgoing mes-sages and thus well captures the gain of updating the vertex. That is, updating a vertex with large sum priority will send out highly useful outgoing messages. In contrast, updating a vertex with zero sum priority will waste a update, since the outgoing messages will not change.
The prioritized block scheduling guarantees that BP al-gorithms converge if update function F is a max-norm con-traction. It has been shown that when F is a max-norm contraction, if a scheduling scheme can guarantee that ev-ery message is updated infinitely often (until convergence), the BP algorithm will converge [6]. We first show that our prioritized block scheduling can fulfill this requirement.
Lemma 4.1. If update function F is a max-norm contrac-tion, the prioritized block scheduling guarantees that every message is updated infinitely often.

Proof. We prove this lemma by contradiction. Assume there are a set of messages that belong to (sent from) a set of vertices, C , which are updated only before a time point t . We use pr i to denote the priority value of vertex i . Since update function F is a contraction, the messages that are updated will move towards their fixed points. Consequently, at some time point after t , for any vertex that does not belong to C (i.e., i  X  ( V  X  C ), where V is the whole set of vertices), its outgoing messages can reach the fixed points (since they are always being updated). At that time, for any i  X  ( V  X  C ), we have pr i = 0; if we also have pr i for any i  X  C , the BP algorithm has converged; otherwise, a vertex in C (e.g., j , pr j &gt; 0) must be selected to update, which contradicts with the assumption that any vertex in C is updated only before time point t .
 Therefore, we have the following theorem.

Theorem 4.2. If update function F is a max-norm con-traction, BP algorithms with the prioritized block scheduling converge.
BP algorithms and its variants are commonly used to perform inference on large real-world probabilistic graphi-cal models. It is desirable to leverage the parallelism of a cluster of machines to reduce the completion time, and to have a general framework to facilitate the implementation in a distributed environment. BP algorithms (and its many ex-tensions) are graph algorithms. Actually, graph algorithms have become an essential component in knowledge discov-ery, since graphs can capture complex dependencies and in-teractions. Therefore, we propose Prom , an asynchronous distributed framework for graph algorithms.

Prom provides several high-level APIs to users for im-plementing BP or other graph algorithms without worry-ing about the complexity of parallel computation. Prom supports asynchronous executions on graphs, in which ver-tices are updated using the latest available values, and lever-ages the proposed prioritized block scheduling as its default scheduling in order to efficiently order vertex updates.
Prom is built upon Maiter [31], an open-source graph pro-cessing framework. Maiter has shown good performance for several graph algorithms. In Maiter, users specify the appli-cation logic simply through a vertex update function. How-ever, Maiter assumes that each vertex (or message) has only one scalar value (e.g. a floating-point number), and thus cannot support algorithms with vector values, such as BP and Personalized PageRank [10]. Additionally, Maiter as-sumes that the update function has only one operation (e.g., addition) with commutative and associative properties, but there are many graph algorithms with more than one opera-tions in the update function (e.g., sum-product has addition and multiplication). These limitations need to be removed so as to accommodate more graph algorithms. To this end, Prom extends Maiter to support a broader class of graph algorithms efficiently. Prom makes two basic assumptions: (1) the graph structure is static and will not change during execution; (2) asynchronous execution with dynamically or-dering vertex updates does not affect the correctness of the algorithm. Graph algorithms satisfying these two assump-tions can be implemented on Prom and can benefit from the efficient prioritized block scheduling.

A vertex-centric programming model (which has been shown to be efficient for many graph algorithms) is adopted by Prom. That is, each vertex is considered as an independent computing unit, and the operations are performed over ver-tices until termination. Vertex updates are performed on workers, and there is a master controlling the flow of com-putation. All workers (and the master) run in parallel and communicate through MPI.
The input graph is split into partitions and each worker is responsible for one partition. Each partition consists of a set of vertices and all their (outgoing) edges. Each worker leverages an in-memory table, info table , to store the ver-tices in its partition. For graph algorithms under the vertex-centric programming model, storing the following informa-tion is typically sufficient for a vertex: ID, incoming mes-sages, outgoing messages, priority, state, and edges (with edge data associated with each edge). Hence, as shown in Figure 1, Prom represents a vertex by a tuple with six fields, { v,im,om,pr,st,sd } , where field v for the vertex ID, im for the incoming messages, om for the outgoing messages, pr for the priority value, st for the state, and sd for the static data (e.g., edges and their associated data).
Prom allows users to define each field of the info table. For example, to implement the incremental-update approach for BP, we can define the incoming message field ( im ) of a ver-tex with [ X  m v a ,  X  m v b ,...,  X  m v l ,m v a ,m v b ,...,m item can be a vector), where  X  m v a stores the new incoming incremental message from neighbor v a , and m v a accumu-lates the incoming messages already received from v a . The static data ( sd ) is usually defined to contain edges and the data associated with edges (e.g., factor functions of the fac-tor graph). Each tuple is stored in one entry of the info table, which is indexed by the vertex ID ( v ).
Each worker has two main operations for its stored ver-tices: the catch operation and the update operation. The catch operation uses a user-defined function ( c fun ()) to ag-gregate a new incoming message for a vertex (say v j ) to its stored incoming messages. That is, function c fun () needs to update the incoming message field ( im j ) of vertex v upon receiving a new incoming message. Also, it needs to update the priority field ( pr j ) to aggregate the importance of the new incoming message. By defining function c fun () in different ways, users can realize different update approaches (e.g., incremental-update or basic-update) and priorities.
The update operation uses another user-defined function ( u fun ()) to compute outgoing messages (and the state) for scheduled vertices. When it is performed on a vertex, function u fun () computes outgoing messages and updates the state (e.g., the belief distribution of the vertex) by in-corporating the latest incoming messages, and modifies the incoming message field if necessary as well as resets the pri-ority value to zero.
 Prom uses MPI to transmit messages between workers. All messages during transmission are in the format ( dst , src , cnt ), where dst denotes the message X  X  destination vertex, src indicates the source vertex, and cnt denotes the message X  X  content. The catch operation and the update operation are realized in two threads for asynchronous execution.
Prom leverages the prioritized block scheduling (described in Section 4.1) as its default scheduling scheme. Since a cen-tralized ordering is inefficient in a distributed environment, Prom allows each worker to build its own prioritized block scheduling. Round by round, each worker selects its local top-k vertices in terms of the priority value as a block to update. All workers selects vertices independently.
A worker puts the block of selected vertices into a list, prioritized list . To minimize the copy cost, only vertex IDs are put in the prioritized list, as shown in Figure 1. Vertex IDs are used to locate corresponding vertices in the info table. All the vertices in the prioritized list will be updated by the update operation during the round. In the first round, all vertices are put into the prioritized list to guarantee that each vertex is updated at least once before convergence.
Prom adopts a passively monitoring model to perform ter-mination check. Each worker utilizes a user-defined func-tion ( m fun ()) to periodically measure its local progress by scanning the info table (typically looking at the incoming message field), and reports the progress to the master. The master aggregates the local progress reports from workers (in the way that a user specifies) so as to obtain the global progress, and in turn determines whether the termination condition is satisfied. If yes, the master sends termination signals to all workers. Upon receiving the terminate signal, a worker stops updating its info table and dumps the table to a distributed file system (i.e., HDFS) so as to reliably store the converged results.

We use the following convergence criterion (max-norm) for BP algorithms (where  X   X  0 is a small constant):
In this section, we evaluate the proposed prioritized block scheduling and the priority. Both the sum-product algo-rithm and the max-product algorithm are implemented on Prom. For the comparison purpose, both the incremental-update approach and the basic-update approach are used. To show the performance of the prioritized block schedul-ing, we compare it with the round-robin scheduling (static scheduling). We also compare the prioritized block schedul-ing with the state-of-the-art dynamic scheduling.
The experiments are performed on a local cluster and a large-scale cluster on Amazon EC2 [1]. The local cluster consists of 4 machines, and each of them has Intel E8200 dual-core 2.66GHz CPU, 4GB of RAM, and 1TB of hard disk. These 4 machines are connected through a Gbit switch. The large-scale cluster consists of 50 medium instances.
Both synthetic and real-world factor graphs are used. We generate one type of pairwise MRFs, random grids with bi-nary variables (parameterized by the Ising model) [6], and convert them into factor graphs. Random grids are cho-sen because they are standard benchmarks for evaluating BP algorithms. For real-world graphs, we consider Markov Logic Networks (MLNs) [20]. Alchemy is leveraged to com-pile the MLNs from the UW-CSE data collection [2] into factor graphs. After compiling, the factor functions will be adjusted if BP algorithms on the compiled graphs do not converge. The factor graphs are summarized in Table 1. In order to load the strongly connected vertices to the same worker and thus reduce across-worker communication, we utilizes METIS [11] to split a graph into partitions.
Each worker by default sets k as 10% of the number of its local vertices. The convergence criterion is set to  X  = 10 Running times are averaged over 10 runs. (a) Sum-Product on grid-200 Figure 2: BP algorithms with different scheduling schemes and update approaches.

We first show the running time of BP algorithms with the prioritized block scheduling on the local cluster. The running time is measured as the wall-clock time that BP uses to reach the convergence criterion. The round-robin scheduling is also evaluated as a reference point. For the sum-product algorithm as well as the max-product algo-rithm, the prioritized block scheduling is faster than the round-robin scheduling with either the incremental-update approach or the basic-update approach, as presented in Fig-ure 2. For example, the prioritized block scheduling is 1 . 9x faster for the sum-product algorithm on grid-200 when the incremental-update approach is utilized. In addition, the incremental-update approach is always superior to the basic-update approach. Note that, in all figures,  X  X -B X  indicates the prioritized block scheduling;  X  X -R X  represents the round-robin scheduling;  X  X ncr X  and  X  X asic X  denote the incremental-update approach and the basic-update approach, respec-tively.
 uw-theory 3 . 8 55 . 7 uw-systems 3 . 8 78 . 8
To further show the advantage of the prioritized block scheduling, we evaluate both scheduling schemes for the sum-product algorithm on real-world factor graphs. The performance comparison for the max-product algorithm is similar and therefore omitted here due to space limitations. As plotted in Figure 3, the speedup of the prioritized block scheduling over the round-robin scheduling is up to 2 . 1x on real-world factor graphs (when the incremental-update ap-proach is used). Moreover, compared with the basic-update approach, the incremental-update approach allows the pri-oritized block scheduling to achieve up to 4x speedup, much higher than that on the synthetic factor graphs (Figure 2a). The different speedups can be attributed to different struc-tures of the factor graphs. For instance, the real-world fac-tor graphs have much higher degrees for variable vertices, as shown in Table 2. Figure 3: Prioritized block scheduling on real-world graphs.

We also measure the convergence speed of the different scheduling schemes (when the incremental-update approach is used). The test is performed on the real-world factor graph, uw-theory , and the max-norm (max i,j |  X  m i  X  j ( x is used to measure the convergence progress. As shown in Figure 4, the prioritized block scheduling converges much more rapidly than the round-robin scheduling.
The block size (i.e., k ) balances the tradeoff between the gain from the prioritized block scheduling and the cost of preparing the prioritized list. Figure 5 shows the conver-gence speedup results with different k . The speedup is mea-sured over the running time when k is the number ( n ) of a worker X  X  local vertices (i.e., the round-robin scheduling). From the figures, we can see that a quite large range of k can allow the prioritized block scheduling to have better per-formance than the round-robin scheduling (when either the incremental-update approach or the basic-update approach is used), and that the optimal speedup happens at around k/n = 0 . 1. This is also why we set k/n = 0 . 1 by default. (a) Sum-Product on grid-200
To further demonstrate the efficiency of its built-in pri-oritized block scheduling, Prom is also compared with an-other distributed implementation of the sum-product algo-rithm, MPI Splash [8], on the local cluster. MPI Splash uti-lizes the DBRSplash scheduling , a distributed version of the ResidualSplash scheduling [7]. The ResidualSplash schedul-ing applies a variation of the residual scheduling in a sin-gle machine (multiple-core) environment, and it has been shown that ResidualSplash is more efficiently than the orig-inal residual scheduling. By recognizing the high overhead of the residual scheduling, ResidualSplash also defines the residual over vertices instead of messages and selects a set of vertices to update at a time via a Splash operation. The Splash operation uses the vertex with the largest residual as a root and updates vertices around the root. However, not all vertices covered by the Splash operation have large residuals, and thus some updates might not be useful. Resid-ualSplash defines a vertex X  X  priority as the maximum of the residuals of its incoming messages. To differentiate this pri-ority with our sum priority, we refer to it as the max priority . The DBRSplash scheduling is the state-of-the-art dynamic scheduling for BP in a distributed environment. (a) Different schedules Figure 6: Performance comparison with the state-of-the-art dynamic scheduling.

For fairness, Prom uses the same priority and termination condition as MPI Splash. To compare scheduling schemes only, we leverage the basic-update approach to implement the sum-product algorithm on Prom. As presented in Figure 6a, Prom can be up to 2x faster than MPI Splash, indicat-ing that the prioritized block scheduling outperforms DBR-Splash. In order to verify that the superiority of Prom over MPI Splash stems from its scheduling scheme, we implement both the prioritized block scheduling and the ResidualSplash scheduling (single machine version of DBRSplash) in a sin-gle machine environment and evaluate them with the same settings. The prioritized block scheduling is 1.8x faster on grid-200 and 2.3x faster on uw-theory than the Residual-Splash scheduling.

In order to show the performance of our sum priority, we compare it with the max priority. We evaluate these two priorities (when both are utilized by the prioritized block scheduling) for the sum-product algorithm on real-world graphs. As presented in Figure 6b, the prioritized block scheduling with our sum priority is 1 . 2x faster on uw-theory and 1 . 5x faster on uw-systems than that with the max priority.
We also assess accuracy of the beliefs computed by Prom (using the prioritized block scheduling with the incremental-update approach) for the sum-product algorithm. We first compare with the exact result. Since exact inference is in-tractable on large graphical models, we here use a small fac-tor graph, grid-10 . The beliefs (of all variable vertices) com-puted by Prom are compared against the exact beliefs com-puted by the junction tree algorithm [14]. We use MPI Splash as a reference point. Kullback-Leibler (KL) divergence is leveraged to measure the difference. From Figure 7, we can see that both Prom and MPI Splash achieve high accuracy. For example, for more than 90% variable vertices, the KL divergence of the beliefs computed by Prom from the exact beliefs is less than 0 . 01. Figure 7: Cumulative percentage of variable vertices as a function of the KL divergence.

For large graphs, since exact inference is intractable, we only compare Prom with MPI Splash. We evaluate both Prom and MPI Splash on grid-200 . Beliefs from both sys-tems are compared by calculating the L 1 difference averaged over all variable vertices. The difference in beliefs computed by the two systems is less than 0 . 02 in terms of averaged L per variable vertex.
Figure 8 presents the scaling performance of the priori-tized block scheduling (for the sum-product algorithm) on Prom as the number of workers increases from 10 to 50 on the Amazon EC2 cloud. The real-world factor graph, us-systems , is used. The speedup is calculated over the run-ning time of 10 workers. We can see that the prioritized block scheduling exhibits nearly linear speedup, and that it always converges faster when the incremental-update ap-proach is utilized than when the basic-update approach is utilized.
Several works [6 X 8, 22] have shown that BP algorithms with the dynamic scheduling converge faster than those with the static scheduling. The earliest work [6] proposes the residual scheduling , which selects the outgoing message with largest residual to update each time. It uses a priority queue to order messages. Besides the large priority queue main-tenance overhead, the problem of the residual scheduling is that it determines an outgoing message X  X  residual by actu-ally computing it. Later, Sutton and McCallum [22] propose to approximate the residual of an outgoing message rather than compute it in order to reduce the computation over-head. However, the cost of ordering messages so as to select the one with the largest residual is still high. Our prioritized block scheduling scheme selects a set of messages to update each time in order to reduce the cost.

The ResidualSplash scheduling [7] applies a variation of the residual scheduling in the multiple-core environment. It defines the residual over vertices instead of messages. The residual of a vertex is used to determine the Splash ordering, and a Splash operation uses the vertex with the largest resid-ual as a root and propagates messages around the root (i.e., among the neighbors within fixed number hops). That is, it selects a set of messages to update at a time. The Residual-Splash scheduling outperforms the residual scheduling, since it reduces the cost of selecting one single message. However, not all vertices covered by the Splash operation have large residuals, and thus some updates might not be useful. The DBRSplash scheduling [8] extends the idea of the Residual-Splash scheduling to a distributed environment. In contrast, our prioritized block scheduling selects vertices with high residuals uniformly, and therefore all scheduled updates are potentially useful.

Since massive graphs become increasingly popular, a se-ries of parallel frameworks have emerged to scale graph pro-cessing. Among them, Priter [30], Maiter [31], GRACE [24], and GraphLab [15, 16] support prioritized execution. Priter is a MapReduce-based framework, which requires syn-chronous iterations. Maiter presents asynchronous execu-tion but assumes that each vertex (or message) has only one scalar value. As a result, none of them supports BP with dy-namic scheduling. GRACE and GraphLab can support BP. GRACE relies on users to implement their own scheduling schemes and its prototype is built on a shared-memory ar-chitecture. GraphLab is the first framework to use a general asynchronous model for graph algorithms and provides the Splash scheduling (based on ResidualSplash) for BP. In com-parison, Prom provides a more efficient scheduling scheme, the prioritized block scheduling.
In this paper, we propose an efficient dynamic scheduling scheme, the prioritized block scheduling, with a novel prior-ity for BP algorithms. In order to efficiently compute the priority and update messages, we introduce an incremental-update approach, which is much more efficient than the tra-ditional basic-update approach. In addition, to facilitate the implementation of BP algorithms and other graph algo-rithms in a distributed environment, we design and imple-ment an asynchronous distributed framework, Prom. Prom uses the prioritized block scheduling as its default scheduling scheme. We implement two BP algorithms, the sum-product algorithm and the max-product algorithm, on Prom. With both synthetic and real-world data, the evaluation results show that the prioritized block scheduling outperforms the state-of-the-art dynamic scheduling scheme, and that the incremental-update approach can further accelerate the pri-oritized block scheduling.
 We would like to thank anonymous reviewers for their in-sightful comments and suggestions. This work is partially supported by NSF grants CNS-1217284 and CCF-1018114. Any opinions, findings, conclusions or recommendations ex-pressed in this paper are those of the authors and do not necessarily reflect the views of the sponsor. [1] Amazon Elastic Compute Cloud (Amazon EC2). [2] UW-CSE MLN. [3] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and [4] C. Crick and A. Pfeffer. Loopy belief propagation as a [5] A. Darwiche. Modeling and Reasoning with Bayesian [6] G. Elidan, I. McGraw, and D. Koller. Residual belief [7] J. E. Gonzalez, Y. Low, and C. Guestrin. Residual [8] J. E. Gonzalez, Y. Low, C. Guestrin, and [9] J. Ha, S.-H. Kwon, S.-W. Kim, C. Faloutsos, and [10] G. Jeh and J. Widom. Scaling personalized web [11] G. Karypis and V. Kumar. Multilevel k-way [12] K. Kersting, B. Ahmadi, and S. Natarajan. Counting [13] D. Koller and N. Friedman. Probabilistic Graphical [14] S. L. Lauritzen and D. J. Spiegelhalter. Local [15] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, [16] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, [17] R. McEliece, D. J. C. MacKay, and J.-F. Cheng. [18] J. Mooij and H. Kappen. Sufficient conditions for [19] J. Pearl. Probabilistic reasoning in intelligent systems: [20] M. Richardson and P. Domingos. Markov logic [21] L. Song, A. Gretton, D. Bickson, Y. Low, and [22] C. Sutton and A. McCallum. Improved dynamic [23] D. Z. Wang, E. Michelakis, M. N. Garofalakis, and [24] G. Wang, W. Xie, A. Demers, and J. Gehrke.
 [25] J. S. Yedidia, W. T. Freeman, and Y. Weiss.
 [26] J. S. Yedidia, W. T. Freeman, and Y. Weiss.
 [27] J. Yin, L. Gao, and Z. M. Zhang. Scalable nonnegative [28] J. Yin, Y. Zhang, and L. Gao. Accelerating [29] X. Yu, W. Lam, and B. Chen. An integrated [30] Y. Zhang, Q. Gao, L. Gao, and C. Wang. PrIter: A [31] Y. Zhang, Q. Gao, L. Gao, and C. Wang. Accelerate [32] J. Zou and F. Fekri. A belief propagation approach for
