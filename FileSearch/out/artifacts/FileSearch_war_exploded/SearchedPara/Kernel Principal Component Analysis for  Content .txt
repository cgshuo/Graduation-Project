 Content-based image retrieval (CBIR) supports image searches based on visual fea-tures such as color, texture, and shape. In a CBIR system, these features are extracted and stored as feature vectors. During the retrieval process, the feature vector of the query image is computed and matched against those in the database. The returned images should be similar to the query image. This similarity (or nearest neighbor ) indexing/retrieval problem can be solved efficiently when the feature vectors have low or medium dimensionalities (e.g., less than 10) by the use of existing indexing methods such as the R*-tree [1] and the HG-tr ee [2]. So far, however, there has been no efficient solution to this problem when the feature vectors have high dimensional-ities, say over 100 [3]. So the issue is to overcome the curse of dimensionality . Motivated by this phenomenon, the approach to reduce the dimensinoality of image feature vectors has been attempted by the use of some dimensionality reduction techniques such as principal component analysis [4, 7]. 
Principal component analysis (PCA) [5] has been widely used for re-expressing multidimensional data. It allows researchers to reorient the data so that the first few dimensions account for as much of the available information as possible. If there is substantial redundancy present in the data set, then it may be possible to account for most of the information in the original data set with a relatively small number of di-mensions. In other words, PCA finds out for the original data set the new structure given by the linear combination of the original variables. However, one cannot assert that linear PCA will always detect all structure in a given data set. By the use of suit-able nonlinear features, one can extract more information. In this paper, we investi-gate the potential of a nonlinear form of PCA for dimensionality reduction and feature extraction in content-based image retrieval. PCA is an orthogonal basis transformation. The new basis is found by diagonalizing the covariance matrix C of a centered data set { x i  X  R N | i = 1, ... , m }, defined by The coordinates in the Eigenvector basis are called principal components . The principal components are given by the linear combination of the original variables. The size of an Eigenvalue  X  corresponding to an Eigenvector v of C equals the amount of variance in the direction of v . Furthermore, the directions of the first n Eigenvectors corresponding to the biggest n Eigenvalues cover as much variance as possible by n orthogonal direc-tions. In many applications, they contain the most interesting information. 
Clearly, one cannot assert that linear PCA will always detect all structure in a given data set. Moreover, it can be very sensitive to  X  X ild X  data ( X  X utliers X ). By the use of suitable nonlinear features, one can extract more information. Kernel PCA is very well suited to extract interesting nonlinear structures in the data [13]. 
The purpose of our work is therefore to consider the potential of kernel PCA for dimensionality reduction and feature extraction in CBIR. Kernel PCA first maps data into some feature space F via a (usually nonlinear) function  X  and then performs linear PCA on the mapped data. As the feature space F might be very high dimen-sional, kernel PCA employs Mercer kernels instead of carrying out the mapping  X  explicitly. A Mercer kernel is a function k ( x , y ) which for all data sets { x i } gives rise k ( x , y ) = (  X  ( x )  X   X  ( y )). 
To perform PCA in feature space F , we need to find Eigenvalues  X  &gt; 0 and Eigen-vectors v  X  F  X  {0} satisfying  X  v = C  X  v with the covariance matrix C  X  in F , defined as Substituting C  X  into the Eigenvector equation, we note that all solutions v must lie in the span of  X  -images of the sample data. This implies that we can consider the equivalent equation and that there exist coefficients  X  1 , ... ,  X  m such that = k ( x i , y j ), we arrive at a problem which is cast in terms of dot product. Solve for nonzero Eigenvalues  X  and Eigenvectors  X  = (  X  1 , ... ,  X  m ) t subject to normalization condition  X  k (  X  k  X   X  k ) = 1. To extract nonlinear principal components for the  X  -image of a test point x , we compute the projection onto the k -th component by In this section, we consider the measures to assess the performance of kernel PCA. In widely used in content-based image retrieval and pattern recognition [9, 14]. In tradi-tional (document) information retrieval, performance is often measured by using pre-cision and recall [10, 12]. Recall measures the ability of the system to retrieve useful items, while precision measures its ability to reject useless items. For a given query, retrieved, and T r the total number of retrieved items. Then precision is defined as R r / T , and recall as R r / T . 
Precision and recall can also be applied to image retrieval. In IBM QBIC that per-forms similarity retrieval as opposed to exact match, normalized precision and recall have been suggested [6]. These reflect the positions in which the set of relevant items appear in the retrieval sequence (ordered by some similarity measure). If there are T relevant images in the database, then for an ideal retrieval, all T relevant items occur ideal AVRR (average rank of all relevant, retrieved images). It is the maximum when all relevant images are retrieved on the top: IAVRR = (0 + 1 + ... + ( T  X  1)) / T (where the first position is the 0-th). The ratio of AVRR to IAVRR gives a measure of the effectiveness of the retrieval. In an ideal case of retrieval, this ratio would be 1. For example, if the relevant images for the query image A are defined as: so that T = 6, and a CBIR system returns, in order: then relevant items appear at 0, 3, 5, 6, 9 and 13. The AVRR for this is therefore (0 + 3 + 5 + 6 + 9 + 13)/6 = 6. The IAVRR would be (0 + 1 + 2 + 3 + 4 + 5)/6 =2.5. Thus AVRR / IAVRR is 2.4. 
If the order of retrieval matters, Kendall X  X  tau can be used to provide measures of association between two sets of data [8]. Kendall X  X  tau can be viewed as a coefficient of disorder. For example, consider the following two rankings, where both have se-lected the same four images, but have placed them in a different order: Tau is calculated as (no. of pairs in order  X  no. of pairs out of order) / (total no. of possible pairs) For this example, 2 in the bottom row is followed by 1, 4, and 3, 2-1 is out of order, and 3. Both are in order, scoring +1 each. Finally, 4 is followed by 3, scoring -1. The +2, divided by the maximum number of in-order pairs, N ( N -1)/2, which here is 6, since N = 4. The value of tau is therefore 2/6, or 0.3333. This gives a measure of the  X  X isarray X  or difference in ranking, between the two. It ranges from -1, which repre-sents complete disagreement, through 0, to +1, complete agreement. 
For each experiment in our work, we report the average of the above measures over 100 k nearest neighbor ( k -NN) queries. For k -NN queries, precision and recall are the same because T = T r , i.e., the total number of relevant items and the total number of retrieved items are the same. There we compute only the precision measure as a representative. The ratio of AVRR to IAVRR gives a measure that how much the results are close to the top. Kendall X  X  tau provides a measure of the order for the k -NN search results. To demonstrate the effectiveness of kernel PCA, we performed an extensive experi-mental evaluation for kernel PCA and compared it to linear PCA. For our experiments we used 13,724 256-color images of U.S. stamps and photos. To obtain feature vec-tors for experiments, we used four MPEG-7 visual features: (1) color structure de-scriptor (256 dimensions), (2) homogeneous texture descriptor (30 dimensions), (3) edge histogram descriptor (80 dimensions), and (4) region-based shape descriptor (35 dimensions). These descriptors are general descriptors that can be used in CBIR. We applied kernel PCA and linear PCA to those four data sets consisting of MPEG-7 visual descriptors, respectively, in order to reduce their dimensionality. We posed k nearest neighbor queries to 3 kinds of data sets, i.e., (1) the original data set, (2) the data set whose dimensionality is reduced by kernel PCA, and (3) the data set whose dimensionality is reduced by linear PCA. In all experiments, the numbers of nearest neighbors to find were 20, 40, 60, 80 and 100 and we averaged their results. 100 random k -NN queries were processed and the results were averaged. Tables 1  X  4 show that the experimental results for four MPEG-7 visual features. The first column of each table represents the dimensionality of the original data and the dimensionalities of the transformed data after the dimensionality reduction. In addition, the percentages of the variance in each original variable we retain are also provided in the first column of each table. The performance of kernel PCA is better than that of linear PCA with respect to all three performance parameters, i.e., preci-sion, the ratio of AVRR to IAVRR, and Kendall X  X  tau. In terms of precision and AVRR/IAVRR, kernel PCA is 10%  X  20% better than linear PCA. With respect to Kendall X  X  tau, kernel PCA is better than linear PCA more than 50%. These experi-mental results indicate that kernel PCA can be successfully employed as a generalized nonlinear extension of linear PCA. In this paper, we described the potential of kernel PCA for dimensionality reduction and feature extraction in content-based image retrieval. Through the use of Gaussian kernel, a kernel PCA was able to work effectively within the feature space of image data sets, thereby producing a good performance. Compared with linear PCA, kernel PCA showed better performance with respect to the retrieval quality as well as the retrieval precision in content-based image retr ieval. Therefore, we can conclude that kernel PCA can be successfully employed as a generalized nonlinear extension of linear PCA. This work was supported by Korea Research Foundation Grant (KRF-2003-041-D20438). 
