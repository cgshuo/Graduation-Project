 Many websites have large collections of pages generated dy-namically from an underlying structured source like a datab ase. The data of a category are typically encoded into similar pages by a common script or template. In recent years, some value-added services, such as comparison shopping and vertical search in a specific domain, have motivated the re-search of extraction technologies with high accuracy. Almo st all previous works assume that input pages of a wrapper in-duction system conform to a common template and they can be easily identified in terms of a common schema of URL. However, we observed that it is hard to distinguish different templates using dynamic URLs today. Moreover, since ex-traction accuracy heavily depends on how consistent input pages are, we argue that it is risky to determine whether pages share a common template solely based on URLs. In-stead, we propose a new approach that utilizes similarity between pages to detect templates. Our approach sepa-rates pages with notable inner differences and then generate s wrappers, respectively. Experimental results show that ou r proposed approach is feasible and effective for improving ex -traction accuracy.
 H.3.m [ Information Storage and Retrieval ]: Miscella-neous X  Data Extraction, Wrapper Generation, Web Algorithms, Experimentation, Performance Wrapper, Template Detection, Information Extraction  X  Work was done when the authors were visiting Microsoft Research Asia.
 Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
Websites like Amazon.com are data-intensive, and infor-mation on them comes from structured sources. Often the data are encoded into semi-structured HTML pages that employ templates for rendering. Some value-added services , such as comparison shopping, are emerging to query or ma-nipulate the data, such as products and reviews, from sev-eral websites. To achieve high accuracy, the task of extract -ing structured information from Web pages is usually im-plemented by programs called wrappers. Manually writing wrappers for Web sources [9] is a tedious, time-consuming, and error-prone job, thus the study of automatic wrapper induction using machine learning techniques has been a sub-ject of research in recent years [12, 11, 17, 16, 3, 6, 5, 18, 4, 21, 10]. This paper also focuses on wrapping Web sources in an automatic manner.

Although different wrapper induction systems employ var-ious techniques and strategies to generate wrappers, they a ll separate template detection from learning wrappers. The detection groups training pages into several clusters or cl asses, based on cues like URLs. For example, [7] assumes that pages belonging to the same template are located at the same sub-directory of a website. Thus, pages are considered to share a common template if their URLs fit a common schema. Grouped pages are then fed into an induction mod-ule. The module generally assumes that a group of pages conforms to a common template, and generates a wrapper per class.

The separated template detection strategy has at least two limitations: 1. With the popularity of dynamic URLs, it is no longer 2. Even if URLs can group pages that share a template,
To solve the problems above, we propose detecting tem-plate solely based on the similarity among page represen-tations that are also used in wrapper generation. In our system, tree structures are used as representations for pag es and wrappers. Based on a distance metric between a page and a wrapper, a clustering algorithm is employed to clus-ter similar enough pages into a class and induces a central wrapper for the class at the same time. Such a joint ap-proach makes it possible to optimize the final performance of extraction by involving template detection in the traini ng process.

Compared with prior works, our approach has two advan-tages: 1. Our approach is more stable because it does not rely 2. Given a set of pages, the number of wrappers is de-
The rest of this paper is organized as follows. We ex-plain our main idea by clarifying two concepts on template in Section 2. Section 3 provides the basic representations. Section 4 formally states the addressed problem and briefly overviews our solution. In Section 5, we describe wrapper induction algorithm that is implemented in our system, and propose a new wrapper-oriented page clustering algorithm that joins template detection with wrapper generation to-gether. Section 6 reports some experimental results, while Section 7 describes related works. Section 8 concludes the paper with directions for future work.
Before going to the details of our approach, in this section we will first describe the main idea of this paper.
What X  X  a ground-truth template? In previous related work, the concept of template has been presented by var-ious descriptive definitions. Most of them associate a tem-plate with a script that encodes the data of a category into a group of HTML pages, called a page class. For example, we can guess that both page (a) and page (b) in Figure 1 are generated dynamically by a script for the category of computer while page (c) and page (d) generated by another script for the category of cap. This kind of templates does exist and it is indispensable for wrapper induction systems to generate effective wrappers in reverse. We call these kind of templates Ground-Truth Templates because they denote the original relations among pages of a website. The corre-sponding page classes are called Ground-truth Page Classes .
However, the ultimate purpose of template detection is not to guess which pages are encoded by a ground-truth tem-plate, but to generate more effective wrappers that can cor-rectly extract data. What we propose is to cluster pages into several groups based on how similar they are. In general, a page is less different from the pages in the same group than those pages in other groups. Each group is corresponded to a template that is called Similarity-based Template and the group itself is a Similarity-based Page Class .

For a particular page set P , since the ground-truth tem-plates are invisible to us, it is difficult and not necessary to ensure that detected similarity-based templates are exact ly same to ground-truth templates. For example, for pages like page (c) and page (d) in Figure 1, a ground-truth page class may be divided into two similarity-based page classes because the color attribute is optional so that some pages like page (d) have such attributes while others like page (c) do not. It is very likely that extracted results by using two similarity-based templates are good and even better than those by inducing one ground-truth template because the complexity of these templates becomes lower than that of one ground-truth template.

In addition, we found that the definition of similarity is highly related to alignment in the stage of wrapper induc-tion. We propose to use consistent representations in both template detection and wrapper generation and jointly op-timize these two stages to achieve better extraction perfor -mance. In our system, tree-structures are used as represen-tations for pages and wrappers, although the representatio n is not restricted to tree-structure.
We describe the representations of a Web page and a wrapper in this section.
DOM tree is the representation of a HTML page in our system. Each DOM node of a DOM tree represents an HTML tag pair (e.g., &lt;TABLE&gt; and &lt;/TABLE&gt; ). The nested structure of HTML tags corresponds to the parent-child re-lationship among DOM nodes. Thus, a DOM tree is formed naturally. More information about DOM specification can be found at [1].

In our experiments, DOM trees used for wrapper gener-ation are manually labeled so that the generated wrappers can extract values and assign labels in one step. Labels are only assigned to leaf nodes of a DOM tree. DOM nodes with different labels are considered different no matter whether they have the same tag or not. In the rest of this paper, for a given DOM node  X  , we use T (  X  ) and L (  X  ) to denote its tag and label.
In our system, a wrapper is also presented in tree structure that can be regarded as extended DOM trees with Sign for each node.

Definition 1. (Node Sign) Given a wrapper node  X  , its sign S (  X  ) indicates its matching rule in the alignment be-tween its owner wrapper and a DOM tree. S (  X  ) can be 1 or an integer N ( N  X  2) or one of the following wildcards: ? , + ,  X  .

Rule 1. Given a wrapper node  X  , Such wrapper node signs are similar to the wildcards used in other works like [6, 18].

Another difference between a wrapper and a DOM trees is that a wrapper may have a kind of special nodes that act like pairs of parentheses, called Parentheses Nodes . These nodes have no corresponding tags and must be inner nodes with at least one child. For the sake of convenience, we call other DOM nodes or wrapper nodes as Tag Nodes .
In this section we formally define the extraction problem and briefly overview our solution. We define the problem as follows:
Given a set of labeled DOM trees D parsed from pages of a particular website, a group of wrappers ( w 1 , w 1 , ..., w should be learned from D . And the target is to maximize the overall extraction accuracy P when generated wrappers are tested on another DOM-tree set D 0 that comes from the same website.

In this paper, we use manually labeled training data to explain and verify our ideas. Although the idea of joint op-timization of wrapper induction and template detection is not constrained to labeled data, we do so for several reasons . First, our main focus is not on the algorithm of wrapper induction but on how to detect similarity-based templates and how the detection influences extraction performance. Labeled data can simplify the evaluation of extraction re-sults. Second, using labeled data to generate wrappers is commonly used in some scenarios such as comparison shop-ping. As the accuracy of price is required to be close to 100 percent, automatic attributes labeling methods cannot mee t the requirement. Furthermore, inducing wrappers based on labeling data is selectively used for only a few of head sites . For each site, as few as tens of pages are enough to train a robust wrapper set. Thus, the cost of labeling is acceptable . A flowchart of our system is shown in Figure 2.

To begin with, training pages are parsed into DOM trees before they are processed by our system. We will not discuss the HTML parsing technique since it is beyond the scope of this paper.

Second, the DOM trees will be fed to the wrapper-oriented page clustering module that combines template detection and wrapper generation into one step and outputs a set of wrappers. A by-product in the step is that the train-ing DOM trees are also clustered into similarity-based page classes.
When a new Web page is introduced, it will be parsed into a DOM tree first. Then, our system can automatically select a wrapper from the generated wrapper set, which makes a best match with the DOM tree. At last, data is extracted and saved in a structured format like a relational database.
In this section, we present the idea of joint optimization of wrapper generation and template detection in detail. We first introduce the wrapper generation algorithm that is im-plemented in our system. We then describe how template detection is combined with wrapper generation by a pro-posed algorithm called wrapper-oriented page clustering.
In Section 5.1.1, we describe how to convert a DOM tree to a wrapper tree. This is the first step for a page before it is evolved in wrapper generation in our system. In Sec-tion 5.1.2 and Section 5.1.3, we implement a cost-driven al-gorithm to perform wrapper induction. This algorithm syn-thesizes several state-of-the-art techniques [6, 4, 18], e .g., regular expression inference.
Given a source DOM tree T d , supposing that the con-verted wrapper is T w , we use T d  X  T w to indicate this con-version.

In T d  X  T w , we need to perform a repeat pattern combi-nation algorithm to make T w more compact than T d . This combination algorithm is similar to the work in [15]. If N ( N  X  2) identical consecutive sub-trees are detected in T , they will be merged as one sub-tree rooted at a tag node  X  in T w , where S (  X  ) = N . If N ( N  X  2) identical consecutive sub-forests are detected in T d , they will be merged as one sub-forest rooted under a parentheses node p in T w , where S ( p ) = N . Node labels are considered in the algorithm. Fig-ure 3 illustrates this procedure, where letters indicate no des X  tags and subscripts indicate nodes X  labels.

Tree alignment is a frequently used algorithm in our sys-tem. There are two types of tree-alignment algorithms: one is for aligning two different wrappers, called WW-alignment , and the other is for aligning a wrapper and a DOM tree, called WD-alignment. We employ cost-driven dynamic pro-gramming for both algorithms.

In the tree-alignment algorithm, DOM nodes and wrapper nodes are the basic units for matching. Mismatched nodes will cause cost in the alignment. To calculate the cost, we need to assign weight to each node before aligning.
Definition 2. (DOM-Node Weight) Given a DOM node  X  , its weight W (  X  ) equals the number of nodes in the sub-tree rooted at  X  , including itself.

Definition 3. (Wrapper-Node Weight) Given a wrapper node  X  , its weight W (  X  ) can be calculated as follows:
The reason we set a wrapper node  X   X  X  weight as 0 if S (  X  ) = ? or S (  X  ) =  X  is that this kind of wrapper node is allowed to be mismatched without causing any cost.

In this sub-section, we only describe WW-alignment and leave WD-alignment to Section 5.2.2.
 Given two wrappers T w 1 and T w 2 , the basic procedure of WW-alignment is to align two forests: A ( F w 1 , F w 2 ). It is performed in a top-down order layer by layer. Only nodes at the same layer of T w 1 and T w 2 can be aligned with each other.

Rule 2. Given two wrapper nodes  X  w 1 and  X  w 2 , we say  X  1 matches  X  w 2 , iff all the following rules are satisfied, 1.  X  w 1 and  X  w 2 are either both inner nodes or both leaf 2. T (  X  w 1 ) = T (  X  w 2 ) 3. If  X  w 2 and  X  w 2 are both leaf nodes, L (  X  w 1 ) = L (  X 
At each layer, A ( F w 1 , F w 2 ) performs a sequence alignment between the array of F w 1  X  X  root nodes and that of F w 2 namic programming is adopted here to minimize the cost. All mismatched root nodes in F w 1 and F w 2 contribute their weight as cost to A ( F w 1 , F w 2 ). For a pair of matched nodes  X  1 and  X  w 2 that are inner nodes, A ( childF w 1 , childF will be invoked recursively, where childF w 1 and childF w are the sub-forests consisting of sub-trees rooted at the ch ild nodes of  X  w 1 and  X  w 2 . The cost caused by A ( childF w ) will be counted in the cost calculation of A ( F w 1 , F w cause our WW-alignment algorithm works in such a top-down recursive way, it attempts to align nodes in two wrap-pers only if their parent nodes are aligned with each other. Such a mechanism saves some unnecessary alignment.
Figure 4 shows an example of WW-alignment. In this ex-ample, label difference is ignored for statement convenienc e. The alignment algorithm works as follows. First, A ( A , A ) recursively invokes A B ( C 3 DE  X  ) ? , BC 3 E . Then, according to the matching rule of wildcards ? (Rule 1), A B ( C 3 DE  X  ) Obviously, the former one costs less by now. Then, A ( F 2 to calculate the cost of these two solutions. In this exam-ple, WW-alignment algorithm can find an optimal result as Figure 4 shown with the cost of 2.
After WW-alignment obtains an optimal result between two wrappers, a new wrapper can be constructed according to the sign-inference function I : where NULL represents a mismatch of a wrapper node. For example, I (1 , NULL ) is applied for D node because D has the sign 1 in T w 1 while it is mismatched in T w 2 .
Given two source wrappers T w 1 and T w 2 , supposing that the generated wrapper is T w 3 , we use T w 1 + T w 2  X  T denote this induction procedure. Figure 4 also illustrates how to construct a new wrapper based on the alignment result.
In Section 5.2.1, we describe the clustering algorithm that combines template detection and wrapper generation to achi eve joint optimization. For clustering, a distance metric is de -fined in Section 5.2.2.
Wrapper-oriented page clustering (WPC) is the most novel part of our system. Given a set of DOM trees D , our WPC algorithm clusters DOM trees in D and generates a wrapper for each cluster. Actually, templates are detected one by on e in WPC. After a template X  X  clustering process is completed, all clustered DOM trees of this template will be removed and then clustering for another template will start. The cycles will stop when no DOM tree is left.

Here, we use Figure 5 to illustrate the clustering process of one template. In Figure 5,  X  X  X  represents a wrapper for this template, and positive points represent DOM trees that belong to the same template as the centered wrapper. Each (c) Level-3 wrapper gray point represents a chosen DOM tree that will be used to refine the centered wrapper.

In the WPC algorithm, we use Wrapper Level to indicate a wrapper X  X  complexity and generality. It is defined as the number of training DOM trees used to learn this wrapper.
First, a level-1 wrapper T w is converted from a randomly chosen DOM tree and taken as the center for this template (Figure 5(a)). DOM trees whose distance to T w is less than a given threshold (dashed circle in Figure 5) are considered belonging to the same template of T w and will be used to refine T w . Refining T w with a DOM tree T d includes three steps: converting T d to a level-1 wrapper T 0 w ; generating a new wrapper from T w and T 0 w ; and replacing T w with the new wrapper.

After T w is refined, it is upgraded by one level and be-comes more general. Actually, for any DOM tree T d , the recalculated distance between T w and T d is expected to de-crease. For the DOM trees that match the wrapper perfectly (e.g., central DOM tree in Figure 5(c)), we will not use them to refine the centered wrapper because they will not bring any changes to it. For those DOM trees whose distance to the centered wrapper is less than threshold , T d will be employed to refine T w (Figure 5(a) and Figure 5(b)).
Finally, the WPC algorithm stops for one template when no DOM tree is within the given threshold (Figure 5(d)). The full algorithm of WPC is listed in Figure 6.
Our proposed WPC algorithm has only one parameter, i.e., the distance threshold. Fortunately, there is a wide threshold to assure high performance. Please refer to exper -imental results shown in Section 6.
In our system, instead of measuring the similarity between two DOM trees directly, we derive a Wrapper-DOM Distance (WD-Distance) to measure the distance between a wrapper and a DOM tree. This distance is used in both the wrapper-oriented page clustering module and the wrapper selection module.

WD-Distance is calculated based on the WD-alignment X  X  cost. WD-alignment algorithm is similar to WW-alignment. Thus, we will not describe it in detail but only present the difference between them. In WW-alignment, nodes are aligned in a one-to-one manner; while in WD-alignment a wrapper node whose sign is +, * or N can be aligned with multiple DOM nodes (Figure 7).

For WD-alignment between a wrapper T w and a DOM tree T d , we use C w ( T w , T d ) to denote the total cost caused by mismatched wrapper nodes and use C d ( T w , T d ) to denote the total cost caused by mismatched DOM nodes. When Algorithm : WPC( D : DOM tree set, : threshold) 1. begin 2. R := page cluster set; 3. W := wrapper set; 4. while D is not empty 5. create a new page cluster C ; 6. select a DOM tree T d 1 from D randomly; 8. move T d 1 from D to C ; 9. for each T d in D 10. if  X ( T w 1 , T d ) = 0 11. move T d from D to C ; 12. endif 13. endfor 14. while  X  T d 2  X  D :  X ( T w 1 , T d 2 ) &lt; 18. move T d 2 from D to C ; 19. for each T d in D 20. if  X ( T w 1 , T d ) = 0 21. move T d from D to C ; 22. endif 23. endfor 24. endwhile 25. add C to R , add T w 1 to W ; 26. endwhile 27. return R and W ; 28. end Figure 6: Wrapper-oriented page clustering algo-rithm calculating the WD-Distance, it is necessary to normalize tree because the larger the number of nodes in the tree, the greater is the likelihood of the tree having nodes that are mismatched. Thus, Wrapper-DOM Distance is defined as follows:
Definition 4. (Wrapper-DOM Distance) Given a wrap-per T w and a DOM tree T d , the wrapper-DOM distance
Here, weight of a DOM tree and weight of a wrapper are defined as below: Definition 5. (DOM-Tree Weight) Given a DOM tree T d whose root node is  X  , then W ( T d ) = W (  X  ) Figure 7: Alignment of a wrapper with a DOM tree
Definition 6. (Wrapper Weight) Given a wrapper T w whose root node is  X  , then W ( T w ) = W (  X  )
According to Definition 4, WD-distance is the arithmetic mean of the normalized cost caused by the wrapper side and that caused by the DOM-tree side. Thus, values are normal-ized in the range between 0 and 1.  X ( T w , T d ) = 0 means T perfectly matches T d without any cost, and  X ( T w , T d ) = 1 means none of the nodes in T w and T d match in the align-ment.
We test the performance of our approach through exper-iments.
We use a dataset of 1,700 product pages from Amazon.com and a dataset of mixed 1,000 pages from 10 shopping web-sites. We call the former dataset Amazon and the latter M10 hereinafter. The reason we choose these datasets is that they are real-life large-scale Web sites and their Web pages are relatively more complex than datasets used in ear-lier works. Although we can also gain very good results on simple datasets, i.e., all pages are similar of each site, us -ing complex real-life datasets is a better choice to show the effectiveness of our joint approach.

In each page, product records and their three attributes, namely product name, product image, and product price, are manually labeled. For each website, twofold cross val-idation is conducted. We use precision, recall, and F1 as measures in evaluation of data extraction results. These measures are calculated based on whether a labeled node is correctly extracted.

All experiments were run on a PC, with a 3.06 GHz Pen-tium 4 processor and 3.87 GB RAM. On the Amazon data, our joint optimization approach achieve s as high F1 as 94.88% by setting the threshold as 0.3 (Fig-ure 8) with 44 wrappers generated. For comparison, we im-plement the separated template detection strategy based on URLs. For specific, training pages are divided into several templates by their URLs. Then, each template generates a wrapper using the same wrapper induction technique as that used in our WPC algorithm. The experimental result shows that these wrappers can only achieve 78% accuracy in terms of F1. Therefore, our approach outperforms the traditional method by about 17 points.

To further evaluate the performance of the WPC algo-rithm, we run the experiment on M10 data. Table 1 shows the evaluation results for each site. As we see, the average F1 is as high as 97.2%. For seven sites out of 10, the pro-posed WPC algorithm achieves F1 higher than 98%. The lowest F1 is got on pages from ftd.com. By case study, we find that the number of templates are unbalanced between the training set and the test set. When some templates are unseen in the stage of training, the generated wrappers reject those pages and extract nothing in testing. But for those seen templates, the wrappers can handle them well. That is why we get high precision and low recall.
Website Wra. # Pre. Rec. F1 ashford.com 1 1.0000 1.0000 1.0000 circuitcity.com 2 1.0000 1.0000 1.0000 costco.com 23 0.9667 0.9153 0.9403 diamond.com 1 0.9875 0.9975 0.9925 ebags.com 2 0.9976 1.0000 0.9988 ftd.com 2 0.9833 0.7528 0.8527 officedepot.com 1 0.9850 1.0000 0.9924 overstock.com 6 0.9224 0.9979 0.9587 pricegrabber.com 1 0.9970 0.9773 0.9870 sears.com 3 0.9960 1.0000 0.9980 Average 4.2 0.9835 0.9640 0.9720
We notice that pages in site Costco.com are clustered into 23 similarity-based templates. It is surprising because in the viewpoint of a human, training pages of this site share only one template. Then, we use all training pages of this site to generate one wrapper. The wrapper can only achieve 87% accuracy in terms of F1, which is lower than the 23 wrappers generated by our approach.
 We evaluate the performance of WPC algorithm as the thresh-old changing on Amazon data. We run WPC 18 times under different thresholds: from 0 to 0.85 with a 0.05 interval. Fig -ure 8 shows three curves on performance when the thresh-old increases from 0 to 0.85. There is no result with greater thresholds because pages chosen are too diverse to learn a wrapper. Figure 8: WPC performance under different thresh-olds
When the threshold is set as 0, only exactly matched pages can be absorbed by the wrapper. Thus, we got the highest precision while the recall is the lowest. It means that a generated wrapper is specific for the small number of pages although the wrapper is precise in its scope.

As we increase the threshold, the precision drops down and the recall goes up. In terms of F1, the peak value of 94.88% is achieved by setting the threshold to 0.3. Af-ter that, F1 stays above 94% and becomes stable until the threshold is set to 0.85. The stable range, from 0.3 to 0.8 in-dicates that it is not hard to set an appropriate fixed thresh-old in the approach.

We also list comparison of the number of wrappers gen-erated with different thresholds in Figure 9. The number of wrappers or similarity-based templates decreases quick ly from 832 to 44 as the threshold increases from 0 to 0.3. Then the wrapper number decreases slowly. There is an ob-vious drop if the threshold is set to 0.85. All training pages are clustered into just four. Such wrappers can cover more pages by sacrificing the effectiveness of extraction. Figure 9: Template detection under different thresh-olds
The impact of different thresholds is also presented by the runtime of our algorithm in the wrapper generation process. When the threshold is set to 0, it takes 13,197 seconds (3.67 hours) to generate 832 wrappers. Then it drops to 1,424 (0.40 hours) seconds when the threshold increase to 0.15 and keeps stable around 2,000 seconds (0.56 hours) until the threshold reaches 0.8. After that, the runtime increases dr a-matically to 24,666 seconds (6.85 hours) when the threshold is set to 0.85. The runtime gets too much to tolerate when the threshold is greater. So we treat the situations as if the y fail to learn a wrapper.
 Since our algorithm chooses the initial DOM tree for clus-tering in a random way, we evaluate how the initial choice impacts performance of our approach in the experiment. We conducted WPC algorithm five times with the threshold set to 0.2 on Amazon dataset. Table 2 lists the number of tem-plate, extraction precision and recall of each run.
As Table 2 shows, in terms of F1, the mean is 94% while the standard variance is smaller than 4E-5. It indicates tha t our proposed approach is quite stable with the random strat-egy to select an initial DOM tree.
 As stated earlier, our approach requires manually labeling for wrapper generation. This experiment was designed to show how many training pages are required for learning wrappers to achieve an accuracy higher than 95% in terms of F1. Table 3 shows the results for all sites in M10. Actu-ally, most websites only need a handful of labeled pages to meet the demand of accuracy. That proves that the cost of manually labeling in our approach is acceptable. officedepot.com 19 overstock.com 16 pricegrabber.com 7 sears.com 27 Our work is in the area of Web Information Extraction . It is highly related to previous works on wrapper induc-tion. Several automatic or semi-automatic wrapper learnin g methods have been proposed. For example, WIEN [12] is the earliest method as we know on automatic wrapper induc-tion. Other representative ones are SoftMeley [11], Stalke r [17], RoadRunner [6], EXALG [2], TTAG [4], works in [18] and ViNTs [21]. Here, we only discuss RoadRunner, TTAG, and works in [18] and refer the reader to two surveys [13, 8] and two tutorials [19, 14] for more studies related to infor-mation extraction and wrapper induction.

In previous work, page clustering for wrapper induction is considered a trivial task in most previous wrapper induc-tion systems. Among them, only RoadRunner [6, 7] and [18] proposed automatic approaches to implement page cluster-ing for wrapper generation. Other methods all manually collect training pages, template by template.
 In [7], page clustering for RoadRunner system is discussed. They use four types of page features to calculate the similar -ity between two pages: (1) distance from the home page; (2) url similarity; (3) tag probability; (4) tag periodicity. B ased on page similarity, they adopt a popular non-supervised clustering algorithm MiniMax to conduct the page cluster-ing process. This process is isolated from the wrapper gen-eration process, which is the primary difference compared with our WPC algorithm. Wrapper selection problem is also discussed in [7].

In [18], tree edit distance is used to measure the distance between two pages. They use traditional hierarchical clus-tering techniques [20] in which the distance measured is the output of a restricted top-down tree mapping algorithm (RTDM). The RTDM algorithm does not distinguish node tag and it is designed only for finding the main contents in news pages. This restricts that method from being applied to general information extraction problems. Similar to our system, works in [18] can also derive a similarity between a wrapper (called extraction patterns) and a page when se-lecting a proper wrapper for extracting data from a new page. However, template detection is still isolated from th e wrapper generation process.

We need to mention TTAG because wrappers in TTAG are also presented as a tree structure with wildcards. The authors also employ a top-down layer-by-layer alignment, but the alignment in any layer is isolated from that in other layers. As a result, child nodes can still be aligned even when their parent nodes do not match. That is a differ-ent strategy from ours. Moreover, like most other previous systems, template detection is not discussed in TTAG.
In this paper, we propose a novel wrapper induction sys-tem that expresses a different opinion regarding the relatio n between template detection and wrapper generation. Our system takes a miscellaneous training set as input and con-ducts template detection and wrapper generation in a single step. By the criterion of generated wrappers X  extraction accuracy, our approach can achieve a joint optimization of template detection and wrapper generation. Experimental results on real-life shopping websites prove the feasibili ty and effectiveness of our approach. The preliminary compar-ison demonstrates that our approach significantly outper-forms the separated template detection strategy.
Our wrapper induction algorithm only leverages the HTML tag-tree structure and does not involve any content. As fu-ture work, we will try to extend the approach to handle the templates that contain some content strings.
We are grateful to Dwight Daniels for detailed edits on writing this paper. Comments from the three anonymous referees are invaluable for us to prepare the final version. [1] http://www.w3.org/dom/. [2] A. Arasu and H. Garcia-Molina. Extracting structured [3] C.-H. Chang and S.-C. Lui. Iepad: information [4] S.-L. Chuang and J. Y.-j. Hsu. Tree-structured [5] W. W. Cohen, M. Hurst, and L. S. Jensen. A flexible [6] V. Crescenzi, G. Mecca, and P. Merialdo. Roadrunner: [7] V. Crescenzi, G. Mecca, and P. Merialdo.
 [8] S. Flesca, G. Manco, E. Masciari, E. Rende, and [9] J. Hammer, H. Garcia-Molina, J. Cho, A. Crespo, and [10] A. Hogue and D. Karger. Thresher: automating the [11] C.-N. Hsu and M.-T. Dung. Generating finite-state [12] N. Kushmerick, D. S. Weld, and R. B. Doorenbos. [13] A. H. F. Laender, B. A. Ribeiro-Neto, A. S. da Silva, [14] B. Liu. Web content mining (tutorial). In Proceedings [15] B. Liu, R. Grossman, and Y. Zhai. Mining data [16] L. Liu, C. Pu, and W. Han. Xwrap: an xml-enabled [17] I. Muslea, S. Minton, and C. Knoblock. A hierarchical [18] D. C. Reis, P. B. Golgher, A. S. Silva, and A. F. [19] S. Sarawagi. Automation in information extraction [20] P. Willett. Recent trends in hierarchic document [21] H. Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu.
