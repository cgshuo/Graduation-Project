 In this paper, we look at the  X  X ocial tag prediction X  prob-lem. Given a set of objects, and a set of tags applied to those objects by users, can we predict whether a given tag could/should be applied to a particular object? We inves-tigated this question using one of the largest crawls of the social bookmarking system del.icio.us gathered to date. For URLs in del.icio.us, we predicted tags based on page text, anchor text, surrounding hosts, and other tags applied to the URL. We found an entropy-based metric which captures the generality of a particular tag and informs an analysis of how well that tag can be predicted. We also found that tag-based association rules can produce very high-precision predictions as well as giving deeper understanding into the relationships between tags. Our results have implications for both the study of tagging systems as potential information retrieval tools, and for the design of such systems. H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; H.1.2 [ Models and Principles ]: User/Machine Systems X  Human information processing Design, Experimentation, Human Factors, Measurement
Social tags (keyword annotations) have recently emerged as a popular way to allow users to contribute metadata to large and dynamic corpora. Standard taxonomies force ob-jects into predefined categories. By contrast, tags have no such requirement. This makes tags appropriate for corpora like the web and user contributed video and photo collec-tions where the distribution or type of content may change rapidly. However, despite increased interest in tagging, tags are still poorly understood. In particular, little is known about the predictability of tags. Given a set of objects, and a set of tags applied to those objects by users, can we predict whether a given tag could/should be applied to a particular object? We call this the  X  X ocial tag prediction X  problem. In this paper, we look at how effective different types of data are at predicting tags in a tagging system.
 Solving the social tag prediction problem has two benefits. At a fundamental level, we gain insights into the  X  X nforma-tion content X  of tags: that is, if tags are easy to predict from other content, they add little value. At a practical level, we can use a tag predictor to enhance a social tagging site. These enhancements can take a variety of forms: Increase Recall of Single Tag Queries/Feeds Many, Inter-User Agreement Many users have similar inter-Tag Disambiguation Many tags are polysemous, that is, Bootstrapping Sen et al. [16] find that the way users use System Suggestion Some tagging systems provide tag
In this paper, we use one of the largest tagging datasets available, the Stanford Tag Crawl 2007 dataset based on the del.icio.us social bookmarking site. We examine whether tags are predictable based on the page text, anchor text, and surrounding domains of pages they annotate. We find that there is a high variance in the predictability of tags, and we look at metrics associated with predictability. One such metric, a novel entropy measure, captures a notion of generality that we think might be helpful for other tasks in tagging systems. Next, we look at how to predict tags based on other tags annotating a URL. We find that we can expand a small set of tags with high confidence. We conclude with a summary of our findings and their broader implications for tagging systems and web search.
A social tagging system consists of users u  X  U , tags t  X  T , and objects o  X  O . We call an annotation of a set of tags to an object by a user a post . A post is made up of one or more ( t i , u j , o k ) triples . We imagine that every object o has a vast set of tags that do not describe it, a smaller set of tags which do describe it, and an even smaller set of tags which users have actually chosen to input into the system as applicable to the object. We say that the first set of tags negatively describes the object, the second set of tags positively describes the object, and the last set of tags currently annotates the object. We model each of these three relationships as relations or tables: R p A set of ( t, o ) pairs where each pair means that tag t R n A set of ( t, o ) pairs where each pair means that tag t R a A set of ( t, u, o ) triples where each triple means that user In practice, the system owner only has access to R a .
We manipulate the relations R p , R n , and R a using two standard relational algebra operators with set semantics. Selection, or  X  c selects tuples from a relation where a par-ticular condition c holds. Projection, or  X  p projects a rela-tion into a smaller number of attributes.  X  c is equivalent to the WHERE c clause in SQL whereas  X  p is equivalent to the SELECT p clause in SQL.  X  c can be read as  X  X elect all tuples satisfying c . X   X  p can be read as  X  X how only the attributes in p from each tuple. X 
Suppose a tagging system had only two objects, a web page o bagels about a downtown bagel shop and a web page o pizza about a pizzeria next door. We might have: R R If we want to know all of the tags which positively describe o bagel , we would write  X  t (  X  o bagel ( R p )) and the result would Suppose also that a user u sally has annotated the pizzeria web page with the tag t pizzeria : If we want to know all users who have tagged o pizza , we would write  X  u (  X  o pizza ( R a )) and the result would be ( u Figure 1: Average new tags versus number of posts.
The Stanford Tag Crawl Dataset consists of one contigu-ous month of data starting May 25th 2007. This data was gathered from the del.icio.us recent feed. The recent feed is filtered by del.icio.us, so it is likely that the recent feed is only a significant portion of all posts during the period. For each URL posted to the recent feed, the dataset also contains a crawl of that URL within 2 hours of its posting, pages linked from that URL, and inlinks to the URL. The dataset is likely to have within 1% of all of the posts that were present in the recent feed during the month long pe-riod. This dataset consists of 3 , 630 , 250 posts, 2 , 549 , 282 unique URLs, 301 , 499 active unique usernames and about 2 TB of crawled data. We call the set of the top 100 tags in the dataset by frequency T 100 for short (shown in Figure 2). For more details about the dataset, how it was gathered, and tradeoffs in gathering it, see Heymann et al. [8].
Based on the Stanford Tag Crawl dataset, we wanted to construct a dataset approximating R p and R n for our predic-tion experiments. However, we only know R a . In previous work, Heymann et al. [8] suggest that if ( t i , o k )  X   X  then ( t i , o k )  X  R p . In other words, annotated tags tend to be accurate. However, the reverse is not true. The case where ( t ten that measures of precision, recall, and accuracy can be heavily skewed. In early experiments on a naively created dataset, we found that as many as 3 4 of false positives were erroneous according to manual reviews we conducted. Our classifiers had accurately predicted for a given ( t i , o that ( t i , o k )  X  R p , but ( t i , o k ) 6 X   X  ( t,o )
When comparing systems, it is reasonable to use a par-tially labeled dataset, because the true relative ranking of the systems is likely to be preserved. Pooling [11], for ex-ample, makes this assumption. However, for this work, we wanted to give absolute numbers for how accurately tags can be predicted, rather than comparing systems.

We decided to filter our dataset by looking at the total number of posts for a given URL: As postcount( o k ) increases, we expect the probability for any given t i that ( t i , o k ) 6 X   X  ( t,o ) ( R a ) and ( t decrease. 1 We chose a cutoff of 100, which leads us to ap-
Using postcount( o k ) for filtering relies to a certain ex-tent on evaluating popular tags. While we do not examine it here, for lower frequency tags we suggest vocabcount( t i , o k ) = P u # Tag # Tag # Tag 4225 reference 3469 technology 3012 web 3794 toread 3366 tools 2996 web2.0 3788 resources 3365 internet 2879 online 3677 cool 3205 computer 2759 free 3593 work 3016 blog 2661 software # Tag # Tag # Tag 396 politics 328 mp3 226 fashion 396 mobile 326 health 216 rails 351 game 310 environment 135 food 343 jobs 266 finance 74 recipes 341 wordpress 233 ruby 5 fic Table 1: The top 15 tags account for more than 1 3 of top 100 tags added to URLs after the 100th book-mark. Most are relatively ambiguous and personal. The bottom 15 tags account for very few of the top 100 tags added to URLs after the 100th bookmark.
 Most are relatively unambiguous and impersonal. proximate R p and R n as: ( t i , o k )  X  R p iff 100  X  postcount( o k ) &lt; 3000 ( t i , o k )  X  R n iff 100  X  postcount( o k ) &lt; 3000 This results in a filtered set of |  X  o ( R p  X  R n ) | X  62 , 000 URLs and their corresponding tags.

Our reasoning for the 100 post minimum is based on the rate at which new unique tags from the top 100 tags, T 100 are added to a URL. Figure 1 shows the average number of new tags t i  X  T 100 that are added to a URL by the n th post. This information is computed over all URLs which oc-cur at least 200 times in our dataset. On average, the first person to post a URL adds one tag from T 100 , the second adds 0.6 of a tag from T 100 , and so on. By the 100th post, the probability of a post adding a new tag from T 100 is less than 5% and remains relatively flat. Furthermore, the top tags which are added later tend to be much more ambiguous or personal. Table 1 shows the fifteen tags which most and least commonly get added after the 100th post. Tags like  X  X p3 X  and  X  X ood X  are relatively clear in meaning, whereas tags like  X  X nternet X  and  X  X oread X  are much more ambiguous and personal. While we cannot completely eliminate the possibility of erroneous tuples in R p and R n , our approach is most accurate for unambiguous or impersonal tags and does not require creating a gold standard based on human annotation. Such a gold standard would be especially diffi-cult to create for subjective tags like  X  X ool X  or  X  X oread. X 
In this section, we look at the predictability of tags given two broad types of data. In Section 4.1 we look at the pre-dictability of the tags in T 100 given information we have about the web pages in our dataset. We look at page text, anchor text, and surrounding hosts to try to determine whether particular tags apply to objects in our dataset. This task is specific to social bookmarking systems because the which should behave similarly to postcount( o k ) but relies on users X  vocabularies rather than raw number of posts. data we use for prediction is specific to web pages. However, the predictability of tags for web pages may also be impor-tant for web search, which may want to determine if tags provide information above and beyond page text, anchor text, and surrounding hosts, and to vertical (web) search, which may want to categorize parts of the web by tags. Heymann et al. [8] provides some initial answers to these questions, but does not address predictability directly, nor does it look specifically at anchor text.  X  X redictability X  is approximated by the predictive power of a support vector machine. While classifiers differ, we believe our results en-able qualitative conclusions about the machine predictabil-ity of tags for state of the art text classifiers.
In Section 4.2 we look at the predictability of tags based on other tags already annotating an object. This task has many potential applications within tagging systems, as dis-cussed in Section 1. Unlike the task in Section 4.1, our work in Section 4.2 is applicable to tagging systems in general (including video, photo and other tagging systems) rather than solely social bookmarking systems because it does not rely on any particular type of object (e.g., web pages).
We chose to evaluate prediction accuracy using page infor-mation on the top 100 tags in our dataset (i.e., T 100 ). These tags collectively represent 2 , 145 , 593 of 9 , 414 , 275 triples, meaning they make up about 22 . 7% of the user contributed tags in the full Stanford Tag Crawl dataset. The dataset contains crawled page text and additional information for about 60 , 000 of the URLs in  X  o ( R p )  X   X  o ( R n ) (about 95%).
We treated the prediction of each tag t i  X  T 100 as a binary classification task. For each tag t i  X  T 100 , our positive ex-amples were all o k  X   X  o (  X  t i ( R p )) and our negative examples were all o k  X   X  o (  X  t i ( R n )). For each task, we defined two different divisions of the data into train/test splits. In the first division, which we call Full/Full, we randomly select of the positive examples and 11 16 of the negative examples to be our training set. The other 5 16 of each is our test set. For each Full/Full task, the number of training, test, positive, and negative examples varies depending on the tag. How-ever, usually the training set is between 30 , 000 and 35 , 000 examples and the test set is about 15 , 000 examples. The proportion of positive examples can vary between 1% and 60% with a median of 14% and a mean of 9%. In the sec-ond division, which we call 200/200, we randomly select 200 positive and 200 negative examples for our training set, and the same for our test set.

How well we do on the Full/Full split implies how well we can predict tags on the naturally occurring distribution of tagged pages. (We call it Full/Full because the union of positive and negative examples is the full set of URLs in R and R n .) However, we can get high accuracy (if not high precision) on Full/Full by biasing towards guessing negative examples for rare tags. For example, because  X  X ecipes X  only naturally occurs on 1.2% of pages, we could achieve 98.8% accuracy by predicting all negative on the  X  X ecipes X  binary classification task. One solution to this problem is to change metrics to precision-recall break even point (PRBEP) or F1 (we report the former later). However, these measures are still highly impacted by the proportion of positive exam-ples. We provide 200/200 as an imperfect indication of how predictable a tag is due to its  X  X nformation content X  rather than the distribution of examples in the system.
Each example represented one URL and had one of three different feature representations depending on whether we were predicting tags based on page text, anchor text, or surrounding hosts. Page text means all text present at the URL. Anchor text means all text within fifteen words of inlinks to the URL (similar to Haveliwala et al. [7]). Sur-rounding hosts means the sites linked to and from the URL, as well as the site of the URL itself. For both page text and anchor text, our feature representation was a bag of words. We tokenized pages and anchor text using the Penn Tree-Bank Tokenizer, dropped infrequent tokens (less frequent than the top 10 million tokens) and then converted tokens to token ids. For anchor text tasks, we only used URLs as examples which had at least 100 inlinks. 2 The value of each feature was the number of times the token occurred. For surrounding hosts, we constructed six types of features. These features were: the hosts of backlinks, the domains of backlinks, the host of the URL of the example, the do-main of the URL of the example, the hosts of the forward links, and the domains of the forward links. The value of each feature was one if the domain or host in question was a backlink/forwardlink/current domain/host and zero if not.
We chose to evaluate page text, anchor text, and host structure rather than just combining all text of pages linked to or from the URL of each example because Yang et al. [20] state that including all surrounding text may reduce accu-racy. For all representations (page text, anchor text, and surrounding hosts), we engineered our features by apply-ing Term Frequency Inverse Document Frequency (TFIDF), normalizing to unit length, and then feature selected down to the top 1000 features by mutual information. We chose mutual information due to discussion in Yang and Peder-sen [19]. In previous experiments, we found that the impact of more features was negligible, and reducing the feature space helped simplify and speed up the training process. 3
For our experiments, we used support vector machines for classification. Specifically we used Thorsten Joachims X  SVMlight package with a linear kernel and the default reg-ularization parameter (see [10]) and his SVMperf package with a linear kernel and regularization parameters of 4 and 150 (see [9]). With SVMlight, we trained to minimize aver-age error, with SVMperf, we trained to minimize PRBEP. Given that we had 100 tags, 2 splits (200/200 and Full/Full), and 3 feature types for examples (page text, an-chor text, and surrounding hosts), we conducted 600 binary classification tasks total. Assuming only a few evaluation metrics for each binary classification task, we could have thousands of numbers to report. Instead, in the rest of this section, we ask several questions intended to give an idea of the highlights of our analysis. Apart from the questions answered below, Figure 2 gives a quick at-a-glance view of which tags are more or less predictable in T 100 ranked by the sum of PRBEP (Full/Full), Prec@10% (Full/Full) and Ac-curacy (200/200). 4 See discussion below for description of
We found that the difference between 10 and 100 inlinks as the cutoff was negligible. More data about a particu-lar URL improves classification accuracy for that URL, but more URLs improves classification accuracy for in general.
Gabrilovich and Markovitch [5] actually find that aggres-sive feature selection is necessary for SVM to be competitive with decision trees for certain types of hypertext data.
Two tags are missing,  X  X ystem:imported X  (a system gener-Figure 2: Tags in T 100 in increasing order of pre-dictability from left to right.  X  X ool X  is the least pre-dictable tag,  X  X ecipes X  is the most predictable tag. each metric. In the analysis below, when we give the mean of the values of tags, we mean the macro-averaged value. For applications like vertical search (or search enhanced by topics), one natural question is what our precision-recall curve looks like at reasonably high recall. PRBEP gives a good single number measurement of how we can tradeoff precision for recall. For the Full/Full split, we calculated the PRBEP for each of the 600 binary classification tasks. On average, the PRBEP for page text was about 60%, for anchor text was about 58%, and for surrounding hosts was about 51% with a standard deviation of between 8% and 10%. This suggests that on realistic data, we can get about of the URLs labeled with a particular tag with about 1 3 erroneous URLs in our resulting set. This is pretty good X  we are doing much better than chance given that a majority of tags in T 100 occur on less than 15% of documents. For applications like bootstrapping or single tag queries (see Section 1), we may care less about overall recall (because the web is huge), but we may want high precision. We used the Full/Full split to look at this question. For each binary classification task, we calculated the precision at 10% re-call (e.g., Prec@10%). With all of our feature types (page text, anchor text, and surrounding hosts), we were able to get a mean Prec@10% value of over 90%. The page text Prec@10% was slightly higher, at 92.5%, and all feature types had a standard deviation of between 7% and 9%. This suggests that whatever our feature representation, if we have many more examples than we need for our system, we can get high precision by reducing the recall. Furthermore, it suggests that there are some examples of most tags that our classifiers are much more certain about, rather than a relatively uniform distribution of certainty.
 According to all evaluation metrics, we found a strict order-ing among our feature types. Page text was strictly more informative than anchor text which was strictly more infor-mative than surrounding hosts. For example, for PRBEP, ated tag) and  X  X ic X  (which is common in the full dataset but uncommon for top URLs and was removed as an outlier). Figure 3: When the rarity of a tag is controlled in 200/200, entropy and occurrence rate are negatively correlated with predicability (first/second graphs). However, in Full/Full, additional examples are more important than the vagueness of a tag, and more common tags are more predictable (third graph). the ordering is (60 , 58 , 51), for Prec@10% it is (92 . 5 , 90 , 90), for accuracy on the 200/200 split, it is (75 , 73 , 66). Usu-ally, page text was incrementally better than anchor text, while both were much better than surrounding hosts. This may have been due to our representation or usage of our surrounding hosts, or it could simply be that text is a par-ticularly strong predictor of the topic of a page. One common complaint about tags is that they should be highly predictable based on anchor text, because both serve as commentary on a particular URL. While both page text and anchor text are predictive of tags, we did not find anchor text to be more predictive on average than page text for any of our split/evaluation metric combinations.
 A more general question than those above is what makes a tag predictable. Predictability may give clues as to the  X  X nformation content X  of a tag, but it may also be practi-cally useful for tasks like deciding which tags to suggest to users. In order to try to quantify this, we defined an entropy measure to try to mirror the  X  X enerality X  of a tag. Specif-ically, we call the distribution of tag co-occurrence events with a given tag t i , P ( T | t i ). Given this number, we define the entropy of a tag t i to be: For example, if the tag t car co-occurs with t auto 3 times, say its entropy was equal to:
Because the relative rarity of a tag heavily impacts its predictability, we used the 200/200 split to try to evaluate predictability of tags in the abstract. For this split, we found a significant correlation between our entropy measure H ( t and accuracy of a classifier on 200/200 (see Figure 3 for the plot of accuracy versus entropy and accuracy versus occur-rence rate which follows). For page text, we had a Pearson product-moment correlation coefficient of r =  X  0 . 46, for an-chor text r =  X  0 . 51, and for surrounding hosts r =  X  0 . 54. All p-values were less than 10  X  5 . 5 However, for the same split, we also found that the popularity of a tag was highly negatively correlated with our accuracy. Specifically, for page text, we had r =  X  0 . 53, for anchor text r =  X  0 . 51, and for domains r =  X  0 . 27. In other words, the popularity of a tag seems to be as good a proxy for  X  X enerality X  as a more complex entropy measure. The two are not exclusive X  X  lin-ear model fit to accuracy based on both popularity and en-tropy does better than a model trained on either one alone.
For the Full/Full split, we found that the commonality of a tag (and hence the commonality of positive examples) was highly positively correlated with high PRBEP. How-ever, perhaps because the recall was relatively low, we found no correlation between the commonality of a tag and our performance on Prec@10% (though we did find some low but significant correlation between PRBEP and Prec@10%). The entropy measure was uncorrelated with PRBEP or Prec@10% for the Full/Full split.
Between about 30 and 50 percent of URLs posted to del.icio.us have only been bookmarked once or twice. Given that the average bookmark has about 2.5 tags, the odds that a query for a particular tag will return a bookmark only posted once or twice are low. In other words, our recall for single tag queries is heavily limited by the high number of rare URLs with few tags. For example, a user labeling a new software tool for Apple X  X  Mac OS X operating system might annotate it with  X  X oftware, X   X  X ool, X  and  X  X sx. X  A second user looking for this content with the single tag query (or feed)  X  X ac X  would miss this content, even though a human might easily realize that  X  X sx X  implies  X  X ac. X  The question in this section is given a small number of tags, how much can we expand this set of tags in a high precision manner? The better we do at this task, the less likely we are to have situations like the  X  X sx X / X  X ac X  case because we will be able to expand tags like  X  X sx X  into implied tags like  X  X ac. X 
A natural approach to this problem is market-basket data mining. In the market-basket model, there are a large set of items and a large set of baskets each of which contains a small set of items. The goal is to find correlations between Though we do not quote them here, we also computed Kendall X  X   X  and Spearman X  X   X  values which gave similarly strong p-values. Table 2: Association Rules: A selection of the top 30 tag pair association rules. All of the top 30 rules appear to be valid, these rules are representative. sets of items in the baskets. Market-basket data mining produces association rules of the form X  X  Y . Association rules commonly have three values associated with them: Support The number of baskets containing both X and Y . Confidence P ( Y | X ). (How likely is Y given X?) Interest P ( Y | X )  X  P ( Y ), alternatively P ( Y | X ) Given a minimum support, Agrawal et al. [1] provide an algorithm for computing association rules from a dataset. In our case, the baskets are URLs, and the items are tags. Specifically, for each o k  X   X  o ( R p ), we construct a basket  X  (  X  o k ( R p )). We constructed three sets of rules: rules with support &gt; 500 and length 2, rules with support &gt; 1000 and length 3, and rules with support &gt; 2000 of any length. We merged these three rule sets into a single association rule set for our experiments.
 We found a surprising number of high quality association rules in our data. Table 2 shows some of the top associ-ation rules of length two. The rules capture a number of different relationships between tags in the data. Some rules correspond to a  X  X ype-of X  style of relationship, for exam-ple,  X  X raphic-design X  is a type of  X  X esign. X  Others corre-spond to different word forms, for example,  X  X hotographer X  and  X  X hotography. X  Some association rules correspond to translations of a tag, for example,  X  X isenoweb X  is Spanish for  X  X ebdesign. X  Some of the relationships are surprisingly deep, for example, the  X  X 3c X  is a consortium that develops  X  X eb X  standards. Arguably, one might suggest that if both t  X  t j and t j  X  t i with high confidence and high interest, t and t j are probably synonymous.

Depending on computational resources, numbers of asso-ciation rules in the millions or billions can be generated with reasonable support. However, in practice, the most intuitive rules seem to be rules of length four or less. In order to give an idea of the rules in general, rather than picking the top rules, we give a random sampling of the top 8000 rules of
Int. Conf. Supp. Rule 0.81 0.989 1097 open source &amp; source  X  opensource 0.55 0.979 1003 downloads &amp; os  X  software 0.42 0.967 1686 free &amp; webservice  X  web 0.73 0.964 1134 accessibility &amp; css  X  webdev 0.84 0.952 1305 app &amp; osx  X  mac 0.40 0.950 2162 webdesign &amp; websites  X  web 0.47 0.947 2269 technology &amp; webtools  X  tools 0.40 0.945 1662 php &amp; resources  X  web 0.63 0.937 2754 html &amp; tips  X  webdesign 0.50 0.934 1914 xp  X  software 0.45 0.928 1332 freeware &amp; system  X  tools 0.62 0.919 1513 cool &amp; socialsoftware  X  web2.0 0.61 0.915 1165 business &amp; css  X  webdesign 0.61 0.912 2231 tips &amp; webdevelopment  X  development 0.35 0.900 6337 toread &amp; web2.0  X  web 0.69 0.897 1010 fotografia &amp; inspiration  X  art 0.33 0.895 1723 help &amp; useful  X  reference Table 3: Association Rules: A random sample of association rules of length  X  3 and support &gt; 1000 . length three or less. This information is shown in Table 3. There is sometimes redundancy in longer rules, for example, one might suggest that rather than  X  X ebdesign &amp; websites  X  web X  we should instead have  X  X ebdesign  X  web X  and  X  X ebsites  X  web X . This is a minor issue however, and it is relatively rare for a rule with high confidence to be outright incorrect. Furthermore, given their ease of interpretation, it would not be unreasonable to have human moderators look over low length, high support and high confidence rules. For our evaluation, we simulate rare URLs with few book-marks. We separated  X  o ( R p ) into a training set of about 50 , 000 URLs and a test set of about 10 , 000 URLs. We generated our three sets of association rules based only on baskets from the training set. We then sampled n book-marks from each of the the URLs in the test set, pretending these were the only bookmarks available. Given this set of sampled bookmarks, we attempted to apply association rules in decreasing order of confidence to expand the set of known tags. We stopped applying association rules once we had reached a particular minimum confidence c .

For example, suppose we have a URL which has a recipe for oven-cooked pizza bagels with three bookmarks corre-sponding to three sets of tags: For n = 1, we might sample the bookmark (pizza, bagels) . Assuming we had two association rules: We would first apply the confidence 0 . 9 rule and then the confidence 0 . 8 rule. Applying both rules (i.e., two applica-tions), would result in (pizza, bagels, food, bagel) . We ran a simulation as described above for each number of sampled bookmarks n  X  X  1 , 2 , 3 , 5 } and for each minimum ble 4. Each row represents one setting of n and c . We asked two initial questions:  X  X ow many tags were added? X  and
B-marks Conf. 0 1 2 3 4 5+ Est. Actual Orig. Expd. Mean Median tag expansions, recall, and precision.  X  X ow accurate were our applications of tags? X  The column labeled  X # Tag Expansions X  shows, for each simulation, the number of URLs to which we were able to add 0, 1, 2, 3, 4 or 5+ tags. The column labeled  X  X ctual Precision X  shows the percentage of tag applications which were correct (given the other information we had about each URL in R p ). For each simulation, we also compute our estimate ( X  X stimated Precision X ) of what our precision should have been based on the confidence values of applied rules. Our estimate is the average of the confidence of all applied rules. For example, in our oven-cooked pizza bagel example above (assuming the URL was the only URL in our simulation), we would have an actual precision of 0 . 5 because  X  X ood X  is a tag which appears in other bookmarks annotating the URL, whereas  X  X agel X  is not. Our estimate of our precision would be 0 . 9+0 . 8 2 We would also increment the  X 2 X  column of  X # Tag Expan-sions X  because the URL was expanded twice.

Thus, the first ten columns of the first row of Table 4 say that we ran a simulation with n = 1, c = 0 . 5 and 10 , 937 URLs. In 2 , 096 cases, we were not able to add any tags (anecdotally, this usually happens when a bookmark only has one tag). In 7 , 667 cases, we were able to add five or more tags. Our estimate of our precision was 0 . 650 while our actual precision was a little lower, 0 . 633.
The results in the first ten columns of Table 4 show that with only a single bookmark, we can expand anywhere from 10 to 80 percent of our URLs by at least one tag depend-ing on our desired precision. With larger numbers of book-marks, we can do better, though the most pertinent tags for a URL are applied quickly. Also, as the number of book-marks increases, the difference between estimated and actual precision increases. This means that as a URL receives more and more annotations, we become increasingly unsure of the effectiveness of association rules for unapplied tags. As we argued in Section 1, predicted tags can be used by a system in many ways. Here we briefly explore one such use: increasing recall for single tag queries. For instance, if the user searches for  X  X ood, X  the system can return ob-jects annotated with  X  X ood X  as well as objects which we predict  X  X ood X  annotates. Using term co-occurrence to ex-pand query results is a well known IR technique; here we want to know how well it works for tags.

For evaluation, we consider each tag t i  X  T 100 to be a query q t i . For each query q t i , the result set s contains the URLs annotated with the tag, and the result set s  X  contains the URLs annotated with the tag plus URLs which we pre-dict are annotated with the tag using association rules. We then compare the recall and precision achieved by s and s For example, suppose five objects are positively described by  X  X ood. X  In our simulation suppose only two of the ob-jects are known to have  X  X ood. X  Suppose that we correctly predict that one of the remaining three objects is labeled  X  X ood X  (perhaps using our  X  X agels X   X   X  X ood X  rule above), and we incorrectly predict that two other objects are labeled  X  X ood. X  Without expansion, query q retrieves s which has two known bagel objects, so recall is 2/5. With expansion, s returns three additional objects, one of which was correct, for a recall of 2+1 5 and a precision of 2+1 2+3 .
The rightmost 4 columns of Table 4 show the results for the experiment. For each simulation (row), we give (a) the mean recall before expansion (macro average over all tags in T 100 ); (b) the mean recall after expansion; and (c) the mean and median of the precision after expansion. (Note that without expansion precision is always 1.) For instance, if we sample one bookmark per URL ( n = 1) and use 50% confidence ( c = 0 . 5), we see that tag expansion improves mean recall from 0 . 099 to 0 . 271, a factor of 3 improvement! Of course, our average precision drops from 1 to 0 . 629. For both the one and two tag cases (i.e., n = 1 and n = 2) with confidence c = 0 . 75 we can increase recall by 50% while keeping precision above 90%.
Previous work has looked at the nature of tags chosen by users [6, 16]. We do not know of any work explicitly looking at how to construct a reasonable dataset for prediction in tagging systems as we do in Section 3. While our hypertext classification task in Section 4.1 is inspired by a long line of work, usefully surveyed by Yang et al. [20], we believe the application to tags is new. Chakrabarti et al. [3] suggest a different way to use local link information for classification that might prove more effective than our domain features, however, we do not evaluate this possibility here. Our use of an entropy measure for tagging systems is inspired by Chi and Mytkowicz [4]. Other work has looked at tag suggestion, usually from a collaborative filtering and UI perspective, for example with URLs [18] and blog posts [13, 17].
Our work in Section 4.2 is similar to work by Schmitz et al. [14]. However, Schmitz et al. is primarily concerned with theoretical properties of mining association rules in tri-partite graphs. Schwarzkopf et al. [15] extend Schmitz X  X  association rules work to build full ontologies. However, neither Schmitz et al. nor Schwarzkopf et al. appear to evaluate the quality of the rules themselves aside from gen-erating ontologies. Lastly, there is also much previous work in IR studying query expansion and relevance feedback try-ing to address similar questions of cross-language and cross-vocabulary queries (see for example a general reference such as Manning et al. [12]). However, we believe that association rules may be the most natural approach to these problems in tagging systems due to user interface issues (for example, feeds, browsing).
We are in the midst of a large scale experiment to de-termine what kind of metadata scales to millions of users, what kind gives the most information for IR, and how to maximally leverage that metadata for IR. In this paper, we looked at tags, which make several tradeoffs to try to scale the web. Our tag prediction results suggest three insights.
First, many tags on the web do not contribute substantial additional information beyond page text, anchor text, and surrounding hosts. All three types of data can be quite pre-dictive of different tags in our dataset, and if we only want a small recall (e.g., 10%) we can have a precision above 90%. The predictability of social bookmarking tags may influence web search (by suggesting ways to use tagging information or whether to use it at all), as well as to system designers who might bootstrap tagging systems with initial quality data (by making it possible to predict such initial data).
Second, the predictability of a tag when our classifiers are given balanced training data is negatively correlated with its occurrence rate and with its entropy. More popular tags are harder to predict and higher entropy tags are harder to predict. When considering tags in their natural (skewed) distributions, data sparsity issues tend to dominate, so each further example of a tag improves classifier performance. To the extent to which predictability is correlated with the  X  X enerality X  of a tag, these measures may serve as build-ing blocks for tagging system designers to produce new fea-tures that rely upon understanding the specificity of tags (for example, system suggestion and tag browsing). Both of our measures of tag predictability are object type indepen-dent. This suggests that they may be applicable to tagging systems based on photos or video rather than only social bookmarking systems.

Third, association rules can increase recall on the single tag queries and feeds which are common in tagging systems today. This suggests that they may serve as a way to link disparate vocabularies among users. We found association rules linking languages, super/subconcepts, and other rela-tionships. These rules may also indicate synonymy and pol-ysemy, two issues that have plagued tagging systems since Golder and Huberman X  X  seminal work [6].
The authors acknowledge Varun Ganapathi X  X  help in ex-perimental design. Heymann is supported by an NSF GRFP Fellowship. Ramage is supported by an NDSEG Fellowship. [1] R. Agrawal, T. Imieli  X nski, and A. Swami. Mining [2] M. Aurnhammer, P. Hanappe, and L. Steels.
 [3] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced [4] E. Chi and T. Mytkowicz. Understanding the [5] E. Gabrilovich and S. Markovitch. Text [6] S. Golder and B. A. Huberman. Usage Patterns of [7] T. Haveliwala, A. Gionis, D. Klein, and P. Indyk. [8] P. Heymann, G. Koutrika, and H. Garcia-Molina. Can [9] T. Joachims. A Support Vector Method for [10] T. Joachims. Making Large-scale Support Vector [11] K. Jones and C. van Rijsbergen. Information Retrieval [12] C. D. Manning, P. Raghavan, and H. Schutze.
 [13] G. Mishne. AutoTag: a collaborative approach to [14] C. Schmitz, A. Hotho, R. Jaschke, and G. Stumme. [15] E. Schwarzkopf, D. Heckmann, D. Dengler, and [16] S. Sen, S. K. Lam, A. M. Rashid, D. Cosley, [17] S. Sood, K. Hammond, S. Owsley, and L. Birnbaum. [18] Z. Xu, Y. Fu, J. Mao, and D. Su. Towards the [19] Y. Yang and J. O. Pedersen. A Comparative Study on [20] Y. Yang, S. Slattery, and R. Ghani. A Study of
