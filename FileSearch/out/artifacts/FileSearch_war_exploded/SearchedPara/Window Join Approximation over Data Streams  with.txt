 Load shedding techniques generate approximate sliding window join results when memory constraints prevent exact computation. The previously proposed random load shedding method drops input tuples without consideration for the number of outputs created, while the recently proposed semantic load shedding technique aims to produce the la rgest possible result set. We consider a new model in which data stream tuples contain numerical importance values relevant to the query source and seek to maximize the importance of the approximate join result. We show that both random load shedding and semantic load shedding are sub-optimal in this situation, while the techniques presented in this paper satisfy the objective function by considering both tuple importance and join attribute dist ributions. We extend the existing offline semantic approximation technique to make it compatible with our objective function and show that it is less space and time efficient than our new optimal offline algorithm for small and large join memory allotments. We also introduce four efficient online algorithms, which are quite promising in maximizing the importance of the approximate join result without foreknowledge of input streams. H.2.4 [ Database Management ]: Query Processing Algorithms, Management, Performance Data streams, importance sema ntics, sliding window join, approximation algorithms, load shedding Research in data stream processi ng is motivated by the important application domains in which data naturally occurs in the form continuous streams. Examples of these applications include weather monitoring via sensor networks [4], life signs monitoring in hospitals, vehicle tracking via a global positioning system (GPS) or through a digital radi o service, internet traffic monitoring [12], or trans action log analysis [7]. overcome by directly applying traditional DBMS techniques. Firstly, it is impossible to store unbounded streams in their entirety. Secondly, recently arrived data stream elements may be more relevant than older data . Thirdly, standard blocking operators cannot be used, as they may indefinitely delay output production. Further, data stre am management systems (DSMSs) are subject to real time processing constraints. To satisfy these requirements, DSMSs may pr oduce approximate results. operator. The familiar blocking join operator must be adapted to operate in a streaming environment because it would require infinite time and storage to comput e the join result over a pair of unbounded streams. continuously, with new result tupl es being generated and streamed away as matching input tuples arrive [17]. To bound a streaming join X  X  memory requirement, the cr iterion of exactness is altered to require that the join operates on a finite prefix of the input streams. While several variations are possible, such as fixed and landmark windows [13], we consid er sliding windows (which we refer to simply as windows), where both endpoints conceptually move over the input stream, allowing in the newest element and displacing the oldest. Sliding wi ndows may be time (i.e. holding tuples from the last 20 minutes) or count-based. We consider the time-based windows in this paper. the join operator and bound its processing and memory requirements, operating conditi ons may still overwhelm a DSMS X  resources. In reality, stream arrival rates fluctuate over time and may exceed the DSMS X  processing capability. In addition, the DSMS X  resources may become constrained when it simultaneously performs multiple continuous queries. In these situations, there is no recourse to generating approximate query results. In this paper, we consider approximation by load shedding, where tuples are prematurely dropped either before or after entry into a sliding window. network of battery-powered sensors monitoring environmental conditions. Sensors, which have limited processing, storage, and communications capabilities, transmit gathered data to proxies, which are terminals to which users pose queries. Rather than sending raw tuple data to proxies as in [8], sensors append importance metadata before transmitting tuples. A tuple X  X  importance metadata may be a function of its frequency, its presence in a predetermined range, its degree of statistical aberrance, or its distance to a cluster point [7], [18], for instance. A proxy accepts input streams and streams output tuples to the query source. The importance of an output tuple, which provides * domain information to the query source beyond the raw tuple data, is a function of the importance of the input tuples which compose it. at a proxy, and the objective is to minimize the approximation error to the greatest extent possible. In the former case, an approximation arises if power constraints prevent the sensor from transmitting all its tuples. In this ev ent, the sensor aims to transmit the most relevant tuples to the proxies. Approximations in the latter case occur when the number of queries being posed or the volumes of incoming data exceed a proxy X  X  computational resources. sliding window join operator where such a system has sufficiently fast CPUs but lacks enough memory to compute the exact join result. Since all load shedding approximations are subsets of the exact result [8], our techniques are aimed at producing the approximation with the least error. Random load shedding techniques [10], [5], [17], which drop tuples randomly with respect to join attribute values, are known to produce sub-optimal approximations because the join result is composed of pairs of matching tuples from the input streams. In the extreme, a bad load shedding strategy can produce few or no output tuples, even though the largest possible approximate result is large. Semantic load shedding [8] improves upon random load shedding by taking into account join attribute value correlations between the two input streams with the aim of maximizing the size of the approximate join result under the memory constraint. However, semantic approximation depends only upon join attribute distributions and not importance semantics. relevant to the query source diff erentiates our work from previous load shedding techniques. Our objective is to compute the approximate result having the largest aggregate importance , given the memory constraint. Unlike those previously mentioned, our techniques process data streams whose tuples have different explicit importance values and ev aluate these tuples on two independent criteria: importance and the expected number of matches. information to the query source than output tuples with lower importance. In this way, tuple im portance semantics assist in QoS-based approximation. Dropping an input tuple randomly or because of its relatively low number of matches ignores the fact that the tuple may provide valuable information to the query source if it creates an output. For example, semantic approximation techniques will li kely drop a highly anomalous tuple prematurely because its join attribute value occurs infrequently. Yet, despite its infrequent occurrence, this anomalous value might be actually be a  X  X eedle in the haystack X  that provides the application with valuable information whenever it joins, especially in the case of an approximation. It may be desirable to retain this tuple in the join memory for as long as possible with the expectation th at it will produce at least one important output tuple. many matches creates more aggregate importance in the join result than a tuple with fewer ma tches. Consequently, retaining input tuples solely because of their high importance may run counter to the objective function. Because it, in effect, regards all input and output tuples as having equal explicit importance, semantic load shedding deals with a special case of the problem we consider. Indeed, domain im portance together with join semantics constitute a novel, non-trivial extension to the problem of load shedding in sliding window join approximation. elements contain numerical importance values relevant to the query source. We propose different strategies of incorporating tuples X  importance attributes into the priority formulation of online algorithms. In addition, we propose various methods of formulating the importance of output tuples from matching inputs and establish the objective function of maximizing the approximate join result X  X  aggreg ate importance. We demonstrate that this new objective function is a non-trivial extension of load shedding by showing that random and semantic load shedding solutions previously proposed in the literature yield sup-optimal approximations. We extend the existing offline semantic load shedding algorithm [8] to compute the maximum importance under memory constraints and also present a new optimal offline join approximation algorithm with superior space efficiency and time efficiency at small and large join memory allotments. Furthermore, we propose four efficient online join approximation algorithms with superior efficacy to online random and semantic load shedding techniques. review related work in Section 2. In Section 3, we formally state our problem and describe different methods of formulating join memory priorities and output tuple importance. We present our offline and online algorithms in Section 4 and experimentally evaluate them in Section 5. Fina lly, we conclude the paper and outline directions for future work in Section 6. Golab et al. [14] investigate the problem of join ordering in queries with multiple sliding window joins and explore the tradeoffs of eager and lazy re-e valuation and expiration. Data stream summary structures [3] produce approximate results when a sliding window is too large to fit in available memory or when a streaming version of a blocking operator does not exist or is too inefficient. These techniques also provide an efficient way to maintain data stream statistics used in computing tuple priorities for online join memory maintenance. process joins over unbounded streams, XJoin [10] was the first to shed load when stream arrival rates exceed the available join processing capacity. Processing two streams, XJoin randomly sheds load by spilling tuples to disk and processes backlogged inputs when the DSMS is once again able to cope with stream arrival rates. Viglas et al. apply their optimization framework for maximizing output production rate [17] to MJoin [5], which extends XJoin by aggregating or decomposing query plans containing multiple join operators. We consider the load shedding scenario in which tuples are dropped permanently. problem of optimal resource allocation with the aim of maximizing window join processing efficiency or output size and propose a per-unit-time cost model to evaluate their random load shedding techniques. Das et al. [8] show that random load shedding in general yields join approximations of sub-optimal size. Their semantic load shedding techniques aim to compute the join approximation of maximum size over input streams without the importance semantics that we consider here. proposed in the literature. Punctu ations provide in formation about the remainder of an input stream which can be used to unblock an operator. However, work in this area does not address load shedding or sliding window join approximation. query processing systems which deal with resource fluctuations by dynamically re-ordering operators. Though promising for providing reliable query performance in changing environments, they do not address tuple importance semantics or join approximation via load shedding. to load shedding in continuous queries. Aurora continuously monitors the frequency distribution of output tuples from streaming operators and compares it to the distributions of tuples in the input streams. It augments input tuples with QoS values from its monitoring statistics to ensure that load shedding reasonably preserves the distributions of the input tuples in the outputs. However, its techniques do not consider optimizations taking into account importance semantics embedded within data stream tuples at their source. To our knowledge, our work is the first to consider offline and online optimization of window join approximation in this context. In this section, we define the problem space and discuss the formulation of join memory priorities in online approximation algorithms and the importance of output tuples. We process a sliding window equi -join between two data streams, R and S. Tuples in a stream are identified as &lt; ts , sch , imp &gt;, where ts  X  N , the set of natural numbers, is the tuple X  X  arrival timestamp; sch is the conventional schema of the stream; imp  X  {x | x  X  R and 0 &lt; x &lt; U } is the tuple X  X  importance, where R is the set of real numbers and U is an upper bound for numerical importance. We employ time-based windows where one tuple arrives in each input stream per time instant, though our discussion extends to count-based windows or asynchronous tuple arrival. size w over stream R, where w is also the lifetime of tuples in window R . Let r(i) be a tuple that arrives in stream R at time i . For convenience, this tuple X  X  join attribute also has a value r(i) . At the end of time t , window R contains tuples r(i) such that 0 &lt; t  X  w +1 &lt; i &lt; t . The description of tuple s(i) arriving in window S over stream S is analogous. tuples are not dropped before reaching the join operator, with eager re-evaluation and expiration [14]. The join memory, M , which is fixed, is bounded above by 2 w , the amount required for exact computation. When input tuples r(t) and s(i) join at time t to create output tuple o(t) , the importance of o(t) is a function of r(t).imp and s(i).imp . Given that streams R and S begin at t = 0 and end at t = N , the result multiset of the sliding window join is and has importance where the inner summation is the total importance of the set of output tuples, Y (t) = { o(t) }, created at time t . Any load shedding strategy produces a subset of the tuples in the exact answer whose importance is, consequently, no larger than that of the exact result. computation is not possible. We seek to compute the approximate result with the maximum importance. We first consider the formulation of tuple priorities, the basis of join memory retention and eviction in online algorithms. Assigned by the join algorithm, a tuple X  X  prio rity is its estimated worth with respect to the objective function re lative to the other tuples in the constrained join memory. The tuple priority is determined by importance, imp , the (expected) number of matches, m , and the arrival timestamp (i.e. age), ts . There is no explicit or implied correlation among these three parameters. Note that semantic approximation [8] does not factor importance into its priority formulation. In our case, tuple r(i) has priority for some function f . The priority of s(i)  X  S is defined similarly. A tuple X  X  priority is either assigned once when it arrives or is updated at each re-evaluation interval. The following are some general possibilities for priority formulation. 1. Additive : r(i)  X  X  priority may be a linear combination of its 2. Multiplicative : alternatively, P( r(i) ) can be formulated 3. Cumulative : in this case, a r(i)  X  X  priority is a function of its We will evaluate special cases of a ll three priority formulations in our experiments. importance from the importance of the input tuples that join to create it. Specifically, we consider how to formulate o(t).imp , must address the issue of output tuple importance because matching input tuples convey their importance to the monitoring application or query source through o(t).imp . Moreover, o(t) can conceivably be an input to another join operator, where its importance is once again used to de termine its priority within that join memory. Intuitive ways to derive an output tuple X  X  importance from the inputs are additive (i.e. o(t).imp = r(i).imp + s(t).imp ), multiplicative, maximal, minimal, and average. Without loss of generality, we use the  X  X inimum X  output importance formulation, where In practice, the output importance formulation is application-dependent. Nevertheless, the real world meaning of importance semantics is irrelevant to our priority and output importance formulations beyond the ability to order priorities and to propagate importance values from input tuples to the outputs. We first introduce our offline approximation algorithm, which employs its foreknowledge of the input streams to generate an approximate join result with the maximum importance. This establishes the baseline against which we measure the efficacy of online algorithms, which have no knowledge of future inputs. We formulate the offline window join approximation as a directed graph. Vertices correspond to snapshots of the join memory at different points in time, while edges, which model transitions between sequential memory states, represent choices to retain or drop tuples. The join memory state graph, which can be constructed for arbitrary combinations of tuple lifetime ( w ), join memory size ( M ), and input stream length ( N ), models all possible ways to retain and evict tuples. Our goal is to determine the tuple eviction and retention strategy corresponding to the approximate join result with the largest importance. graph. In this example, both data streams have length N =6. From Subsection 3.1, data stream tuples are of the form &lt; ts , sch , imp &gt;. &lt;5,1,1&gt;. Only the join attributes of sch are shown. stream arrives at time t =0, the second tuple arrives at t =1, and so on. Tuples are identified by a combination of their stream and arrival time. For example, tuple &lt;1,9,20&gt; in stream R is referred importance values are 9 and 20, respectively. The importance of an output tuple is the minimum of those of the matching tuples that create it (see Subsection 3.2). For instance, r(2) and s(2) join and an importance of 1. Assuming that each tuple occupies one unit of memory, 2 w =8 memory units are required to compute the exact result. The available join memory, M =4, is half of this amount. In this example, M is divided evenly between windows R and S . subgraphs: the R , S , and SA subgraphs. An output tuple in the SA subgraph is created when two matching tuples in opposite streams arrive at the same time. In subgraphs R and S , however, an output tuple is created when a tuple in a memory state joins with the new tuple arriving in the opposite stream. Because of symmetry, subgraphs R and S share the same properties. tuples they contain. The start vertex represents the join memory before any data stream tuple arrives, while the stop vertex represents the time instant after the final tuple has arrived. An edge X  X  vertex of origin represen ts the join memory state at the instant when a new tuple arrives. When it arrives, the tuple is either (1) admitted into the join memory which is not full, or (2) admitted at the expense of an expired tuple, or (3) admitted at the expense of an active tuple in the join memory, or (4) dropped before entering the join memory. An edge is created from the current memory state to a subsequent state in the next time instant for each of these events. For example, r(0) and r(1) are admitted into the join memory which is not yet full. When r(2) arrives, to a full join memory ( M /2 tuples in this example), since neither r(0) nor r(1) has expired, new memory states must be created from vertex { r(0) , r(1) } at t =2, each representing a decision to drop r(0) , r(1) or r(2) prematurely. resulting from the decision to admit, expire, retain, or drop tuples. create output tuple ( r(0) , s(2) ) which has importance 1. Thus, the two memory states at t =2 in which r(0) survive have incident edges with weight 1. On the other hand, edge not shown.) Note that an edge X  X  weight represents the total importance from the output tuples from the originating vertex. If multiple tuples in the join memory state match the new tuple in S, the edge weight is the sum of the importance of the created output two output tuples of importance 1. represents a complete path. Each path represents a set of decisions to retain or evict tuples and the states resulting from those decisions. Paths respect correct temporal modeling because they represent a set of trans itions from one time instant to the next. In addition, paths in a subgraph respect the memory constraint because vertices within all paths do not accommodate more than the allotted number of tuples. For example, each vertex in the R and S subgraphs of Figure 1 does not accommodate more than M /2 tuples. Moreover, paths only contain valid transitions between memory states, meaning that a tuple that is dropped within a path cannot re-enter a join memory state. For example, no path in Figure 1 contains the edge { r(0) , r(1) }  X  { r(1) , r(2) } at t =4. once dropped, cannot possibly re-enter a join memory state at a future time. Thus, each path from the start vertex to the stop vertex represents a valid sequence of join memory states adhering to the memory constraint. In other words, each complete path from the start vertex to the stop vertex represents a join approximation. The optimization goal is to find a path from the start vertex to the stop vertex such that the sum of the edge weights is maximal. those in the S subgraph because join memory states in R are determined solely by tuples arriving in R . Furthermore, edge weights in the R subgraph depend on the arrival of tuples in stream S and not on the contents of window S  X  X  join memory. The same is true of the corresponding structures in the S subgraph in relation to those in the R subgraph. Subgraph SA is independent of both R and S because it alone captures output tuples from matching input tuples that arrive at the same time. Because subgraphs R , S , and SA are independent, the optimal approximation consists of the output tuples in the SA subgraph in union with the optimal paths in subgraphs R and S . contains nine output tuples and has an importance of 32. For clarity, the only non-zero edge weights shown are those of paths 0 (bold) and 1 ( X  x  X ). Path 0 corresponds to the join approximation for M =4 with the maximum importance. This approximation contains seven output tuples and has an importance of 30. For comparison, the largest approximate join result for M =4 (i.e. the optimal semantic load shedding approximation [8]), represented by Path 1, contains eight output tuples but has an importance of only 12. First, different paths have ove rlapping subpaths. For example, three different subpaths starting at t =4 contain subpath path corresponding to the opti mal approximate join result necessarily contains within it op timal subpaths, which means that the graph formulation possesses optimal substructure. The presence of both properties, which will be discussed in more detail in Subsection 4.2.2, leads us to a dynamic programming optimal offline algorithm, which we present next. Our optimal offline approximation algorithm consists of several procedures, which are described in the following subsection. In the following procedures for c onstructing the join memory state graph and determining the optimal approximation, variable OJI represents the importance of the optimal join approximation. Variable MS[ t ] contains all the join memory states (i.e. vertices) at time t . A memory state X  X  MI field represents the maximum importance of all paths from the start vertex to that memory state, and a state X  X  prev field is the immediate predecessor vertex in this optimal path. Both are initialized to zero for new vertices. Function Importance() returns the sum of the importance of the output tuples produced by the joined inputs for a given memory state at a time instant. produced by the above procedures, we can print the input tuples in the states corresponding to the optimal join approximation in the reverse order of time. The procedure starts from the stop vertex and iteratively uses the prev field to traverse the predecessor in the optimal path. different objective functions. For example, modifying Importance() to return the number of output tuples from a memory state would cause the algorithm to produce the optimal semantic approximation [8]. In addition, changing the procedure so that the vertices X  MI fields store the smallest importance of all the paths leading to them allows the algorithm to compute the approximation with the lowest importance. Alternatively, the algorithm can support a combination of positive and negative edge weights, where desirable output tuples have positive importance and undesirable outputs tuples have negative importance. The correctness of the algorithm follows from the preservation of the following invariant: at the end of each time step, every vertex contains information to determ ine the maximum importance from the start vertex to itself. Thus, the stop vertex in the R and S subgraphs contains the maximum importance from the start vertex to itself. The optimal substructure and overlapping subpath properties present in the join memory state graph formulation allow for the preservation of this invariant. Let G(V,E) represent a jo in memory state graph. Importance : E  X  R is the weight function for edges as described in Subsection 4.1. Let P be a path in the join memory state graph from the start vertex to the stop vertex which yields the join result with the greatest importance. P consists of vertices {v 0 ,v 1 ,...,v {e 0,1 ,e 1,2 ,...,e N -1, N }, where edge e i,j connects vertices v &lt; i &lt; j &lt; N . If P i,j is a subpath in P starting at vertex v at vertex v j , then P i,j yields the greatest importance between v v . If not, then decompose P as P 0,i  X  P i,j  X  P j,N importance Importance (P) = Importance (P 0,i ) + Importance (P Importance (P j,N ). Since there is an alternate path P  X  such that Importance (P  X  i,j ) &gt; Importance (P i,j path into P in place of P i,j to form path P  X  , whose importance, Importance (P 0,i ) + Importance (P  X  i,j ) + Importance (P than Importance (P). This contradicts the assumption that P has the greatest importance from the start to the stop vertex. property of overlapping subpaths and how the algorithm uses the property to generate the optimal approximation. Let v memory state in MS[ t ]. As a result of r(t)  X  X  arrival, v between one and M /2+1 subsequent states (inclusive) in MS[ t +1] corresponding to each of the following events:  X  Join memory is not full. r(t) is admitted.  X  Join memory is full. r(t) displaces the expired tuple in v  X  Join memory is full. r(t) displaces any active tuple in v  X  Join memory is full. r(t) is dropped. Let v i+1 be a subsequent state of v i . v i+1 may also be reached independently through memory state v h in MS[ t ]. Updating v MI and prev fields enables the algorithm to select which of the paths from the start vertex ending at v i+1 importance measure, while storing only one copy of v i+1 . In Figure 1, for example, vertex { r(1) , r(4) } at t =4 can be reached from { r(0) , r(1) }, { r(1) , r(2) }, or { r(1) , r(3) } at t =3. By maintaining non-duplicate vertices, the algorithm allows these paths to share subpath { r(1) , r(4) }  X  { r(4) , r(5) }  X  stop and decides which of them maximizes the importance measure without the need to store a separate copy of the subpath for each. allocations, the following time complexity analysis assumes that each window holds at most M /2 tuples. Each subgraph contains N time instants, and there are at most memory states in a time instant. Each state generates at most M /2+1 new states of size M /2. Each new memory state is generated in O( M /2) [19]. The calculation of Importance() for each output tuple is absorbed into this step. The algorithm uses the overlapping subpaths property by maintaining non-duplicate memory states in a time instant. Duplicate avoidance is performed in O( M /2). Each MI and prev update is performed in O(1). Thus, the algorithm determines the sliding window join approximation of maximum importance in O . The time complexity approaches O( Nw ) as M /2  X  1 and approaches O( Nw 4 ) as M /2  X  w  X 1. The printing procedure runs in  X  ( N ), since there are N time instants and the Locate() step takes constant time. memory graph to produce an optimal approximation. Thus, its space complexity,  X  , is independent of N . The printing procedure only requires O(1) space since only vertex v is stored. Semantic approximation [8] is sub-optimal for maximizing the importance measure because it only computes the largest approximate join result. We extend the original semantic approximation technique to make it compatible with our objective function (omitted due to space constraints). We modify the flow graph creation algorithm to accept tuple importance semantics by allowing arc weights of arbitrary (absolute) magnitude and employ an efficient implementation of the minimum cost linear flow algorithm [11] to calculate the optimal join approximation X  X  numerical importance in O(( wN ) 2 ( wN + M )log( wN )). Unlike our offline algorithm, the space efficiency of this algorithm, O( wN + M ), is a function of M , w , and N . The extended semantic approximation technique and our optimal offline algorithm will be evaluated empirically in Section 5. Unlike the optimal offline algorithm, online approximation algorithms have no knowledge of future input tuples. Therefore, they have no recourse but to greedily hold on to tuples they consider valuable for the objective function according to pre-determined heuristics, which include: 1. Using past usefulness (i.e the importance of the outputs 2. Using current or past join attribute distributions to estimate Such statistical and historical information is maintained automatically by DSMSs as histograms and sketches [3]. Our techniques employ these general guidelines while also incorporating tuple importance. We present four efficient online algorithms in the Fast CPU framewo rk [8] with either static or dynamic priorities. The Static IMPortance heuristic (SIMP) fixes the priority of each tuple as its importance. This is a special case of the additive (or multiplicative) priority formulation in Subsection 3.2. SIMP prefers high importance tuples over those with lower importance, regardless of estimated match pr obabilities or the relative ages. The tuple of lowest priority (including the new tuple) is dropped whenever a tuple arrives at a full window. If a tie in priorities occurs, the oldest tuple is dropped. For simplicity, suppose that the join memory is maintained as two priority queues, each holding M /2 tuples. Using the cost model in [17], SIMP X  X  per-tuple processing cost is evaluated as Figure 2. Importance vs. memory size When a new tuple arrives, th e Static IMPortance PROBability heuristic (SIMPPROB) fixes its priority as the product of its importance and the number of ma tches in the opposite window, the latter number being the new tuple X  X  estimated match probability. This is a special case of the multiplicative priority formulation. Since a new tuple may have no matches, any tuple can be prioritized to zero, regard less of its importance. Ties are resolved by dropping the tuple with the lowest importance, then the tuple with the fewest number of matches, followed by the oldest tuple. Because priorities are assigned once, SIMPPROB X  X  per-tuple processing cost is the same as that of SIMP. Note that the online semantic approximation heuristic [8] also has the same per-tuple processing cost. The Dynamic IMPortance PROBab ility (DIMPPROB) heuristic also calculates a new tuple X  X  priority as the product of its importance and the number of matches. However, expected match probabilities are updated at each time step. Ties are resolved exactly as they are in SIMPPROB. Assuming M /2 tuples per window, the per-tuple processing cost, is greater than SIMP PROB X  X , where match probability estimates become inaccurate over time as the opposite window changes. The Dynamic Gain Loss heuristic (DGL) employs the cumulative priority formulation. At arrival, a tuple X  X  priority is its importance. If it produces an output during a time instant, the tuple X  X  priority is increased by an amount proportional to the product of its importance, its current expected number of matches, and its remaining lifetime. A tuple X  X  prior ity is decreased by a constant factor during time instants when it does not produce an output. Thus, high importance tuples generally make priority gains more quickly with each matching partner than low importance tuples, though gains are reduced as tuples age. DGL X  X  analytical per-tuple processing cost is the same as DIMPPROB X  X , as all priorities are modified at every time step. We verify the feasibility of tuple importance semantics in window join approximation and evaluate the efficacy and efficiency of our techniques under different c onditions using independently generated synthetic input stream s. Normal importance tuples dominate the input streams, while tuples with high importance, which correspond to aberrant or special values, occur infrequently. M is divided evenly between windows R and S , though our techniques work for uneven allocations. Experiments comparing approximation quality begin counting outputs at t =2 w , since all algorithms accept the first M /2 input tuples from each input stream, the last of which may still be in memory at t = M /2+ w . The starting time for counting is adju sted to reflect the maximum w or M /2 in experiments where these parameters are varied. The platform is a 2.0 GHz AthlonXP machine with 1.0 GB of RAM running SUSE Linux 9.3. Each re ported running time is the average of three trials. We compare our offline algorithms to semantic and random load shedding with respect to efficacy, and time and space efficiency. Figure 2 depicts the approximatio n qualities of the aforementioned load shedding strategies as a function of increasing join memory size ( M ) given a fixed tuple lifetime, w =10, and input stream length, N =5600. Henceforth, EXACT is the exact result, RAND is random load shedding, and SJA and ESJA, respectively, represent the semantic and extended semantic (see Subsection 4.2.3) load shedding techniques. ODJA is our dynamic programming approximation technique. to the exact result (EXACT) as join memory increases. For example, at M /2=1, SJA produces a 75.3% approximation error, while ODJA/ESJA X  X  error is 29.7%. At M /2=5, the respective approximation errors shri nk to 28% and 1%. RAND X  X  approximation error decreases from 98% at M /2=1 to 80.4% at M /2=5. RAND is least effective because it drops input tuples without regard for importance or the number of output tuples produced. ODJA and ESJA produce the maximum possible importance under the memory constraint and converge upon EXACT more quickly than other me thods. This result is expected because both methods recognize important input tuples and those which produce many outputs. SJA generates the largest result set but is consistently sub-optimal because it only favors input tuples for their ability to produce many output tuples. At M/2 =1 and M/2 =2, for example, SJA generates 164 and 97 more output tuples than ODJA/ESJA. Since ODJA and ESJA always yield the same (optimal) approximation quality, we examine the running times of the two techniques in the first special cas e discussed in Subsection 4.2.2, namely M /2=1. from 800 to 5600. ODJA is 481% more efficient at N =800 and becomes 700% more efficient at N =5600. In Figure 4, where N Figure 5. Running times for M/2 = w  X 1 is fixed at 5600 tuples and w varies from 100 to 800, ODJA X  X  time efficiency advantage over ESJA progressively increases from 596% to 689%. ODJA X  X  running time increases less rapidly with N and w because ODJA X  X  running time approaches O( Nw ) as M /2  X  1, while ESJA X  X  is in O(( wN ) 3 (log( wN ))) (see Subsections 4.2.2 and 4.2.3). We now consider ODJA and ESJA X  X  relative running times in the second case described in Subsection 4.2.2, M /2= w  X 1. As Figure 5 shows, ODJA and ESJA require almost the same amount of time to compute the optimal approximation at N =800, but OJDA becomes increasingly more efficient as N alone is increased to 5600. This is because ODJA X  X  running time, which is in O( Nw when M /2= w  X 1, grows more slowly with increasing input stream length than ESJA X  X , whose running time is in O(( wN ) 3 (log( wN ))). relative running times of the two offline algorithms at M /2= w  X 1, given a fixed input stream length of N =5600. ODJA becomes more efficient relative to ESJA as w is reduced. Even though ODJA X  X  running time increases more rapidly with increasing tuple lifetime than ESJA X  X  (i.e. quadric vs. cubic), ODJA outperforms ESJA when N is sufficiently large because ESJA X  X  running time is influenced more by the size of the input streams. Note that the when join memory is neither small nor large (i.e. M /2 is not close to 1 or w  X 1, respectively), ESJA is more efficient (results are omitted due to space constraints). We investigate the effect of increasing input stream length on the memory requirement of ODJA and ESJA. Memory use, in kilobytes, is reported by top , the resource usage tool available in UNIX and LINUX operating systems. Figure 7, where w =400 and M /2=1, shows that ODJA X  X  memory usage increases very slightly as N increases, whereas ESJA X  X  memory use increases at a higher rate. ODJA should have a constant space requirement, regardless of the size of the input stream (see Subsection 4.2.2), yet our results show an increase from 1944 KB to 9704 KB as N is increased from 800 to 5600. This discrepancy arises because measurements of memory use include the input stream tuples and the algorithm state. Consequently, we conclude that ODJA X  X  space requirement is constant, regardless input stream length. ESJA X  X  memory requirement incr eases from 25.3 times ODJA X  X  at N =800 to 45.3 times ODJA X  X  at N =5600. This difference in memory utilization arises because ESJA must store all state (i.e. vertices and edges) in memory for all time instants, whereas ODJA only stores memory states for two time instants. A similar result (not shown) is obtained when M /2 is fixed at w  X 1 and the input stream length is increased from N =800 to N =5600. OSJA X  X  memory requirement is i nvariably independent of N , so its space efficiency will be superior whenever N is sufficiently large. Before evaluating our online heuristics, we note that the online semantic approximation heuristic in [8], PROB, estimates match probabilities using the join attribute value distributions of the entire input streams. We maintain that no online heuristic can have knowledge of future tuples and, as a result, estimate match probabilities solely from the sliding windows. We determine the effect of increasing tuple lifetime ( w ) on the relative approximation qualities of the online algorithms. Figure 8 shows the result set importance as a function of w , where M =200 (divided evenly between the windows) and N =5600. The two input streams have Zipf distributi ons of 1.0 and 0.0, respectively, and have a domain size of 100. The algorithms X  relative efficacy is unaffected by increasing tuple lifetime. As expected, the maximum possible importance under the memory constraint, OPT (generated by either ESJA or ODJA), grows as tuple lifetime increases. This is because tuples that produce outputs can be retained longer, which increases their chances of finding matches in the opposite stream. For example, OPT X  X  importance is 3.21 times greater at w =800 than at w =200. Among the online algorithms, DGL degrades most gracefully and yields the best approximation quality, foll owed by DIMPPROB and SIMPPROB. PROB X  X  quality does not improve consistently because it pursues frequently occurring join attributes and ignores tuple importance. SIMP X  X  quality improves little because it retains high importance tuples longer, without regard to whether they produce any output tuples. RAND  X  X  quality is, constant and poorest of all, since its eviction policy does not consider importance or match probability. We consider the effect of increasing join memory size on the algorithms X  approximation quality. In this experiment, N =5600, and the domain size is set to 100. w is fixed at 400, as it does not alter the algorithms X  relative efficacy (see Subsection 5.2.1). The input streams have Zipf distributions of 1.0 and 0.0 as in Subsection 5.2.1. The results, shown in Figure 9, indicate that, all the heuristics approach EXACT as the join memory size increases. DGL produces the best approximations and degrades most gracefully. For instance, at M =100, the lowest memory setting in the experiment, DGL is 3.2% more efficacious than DIMPPROB, 20.6% more efficacious than SIMPPROB, 47.6% more efficacious than PROB, 52.9% more efficacious than SIMP, and 77.8% more efficacious than RAND. Predictably, RAND performs most poorly. It s approximation quality scales linearly with M because its join memory can hold an increasing number of tuples. PROB X  X  quality also increases linearly, though at a higher rate than does RAND X  X . PROB does not quickly converge upon OPT, since it ignores the importance attribute. In general, SIMP is a poor heuristic because it completely ignores match probabilities. It performs we ll only when the allotted join memory is close to the exact amount. SIMPPROB and DIMPPROB perform well, converging quickly upon EXACT. DIMPPROB X  X  error remains somewhat less than SIMPPROB X  X  because its match probability estimates are always updated. DGL outperforms DIMPPROB because skewness in the Zipf 1.0 input stream implies that some tuples occur with greater frequency than others. By inflating their priorities, DGL retains these tuples longer than DIMPPROB, giving them a better opportunity to find matches in the opposite stream. Figure 10 shows the effect of increasing the join attributes X  domain sizes on approximation quality. M is fixed at 200 tuples, w =400, N =5600, and the Zipf parameters are 1.0 and 0.0. Figure 10 shows that the resu lt set importance varies inversely with the domain size. The online algorith ms perform worse relative to EXACT as the domain size increases, while OPT very quickly converges upon exact. These results are explained as follows. Increasing the domain size in the Zipf 1.0 stream effectively reduces the number of join attrib ute values occurring with a high frequency and, simultaneously, increases the number of join attribute values occurring with a low frequency. Increasing the domain size in the stream with uni formly distributed join attribute values reduces the frequency of all join attributes values by an equal amount. One consequence of increasing  X  X parseness X  in the distribution of the join attribute values of both streams is that tuples are increasingly unlikely to find matches. This lowers the importance of EXACT. Another result of increasing domain size is that, on average, tuples spend more time in the join memory before encountering matches, which explains the online algorithms X  progressively worse performance relative to OPT, which relies on its foreknowledge to find a good retention strategy. The online algorithms X  relative efficacy is the same as that obtained in Subsection 5.2.2: DGL yields the best approximation quality and degrad es the least, followed by DIMPPROB, SIPPROB, PROB, SIMP, and RAND. Previous arguments account for their relative efficacies. In this experiment, the join attribute values of both streams are increased from Zipf 0.0 to Zipf 1. 0 in a correlated fashion, while w =400, M =200, and the streams X  domain size is fixed at 100. As Figure 11 shows, result set importance becomes progressively larger with increasing skewness , approaching OPT. Because the frequently and infrequently occurring join attribute values in both streams are correlated, input tuples, on average, have a greater expected number of matches and a reduced amount of time between matches as the skew parameter increases. Unlike previous scenarios, DIMPPROB and SIMPPROB are, on average, 2.1% more efficacious than DGL. Th is interesting result is due to the effects of correlation and skewness. The large number of matches causes priorities in DGL become inflated so that subsequently arriving tuples are le ss likely to be admitted into the join memory as compared to DIMPPROB. DIMPPROB more readily replaces old tuples that don X  X  match with new tuples that do. DGL X  X  favoritism of ol d high priority tuples over new tuples also causes it to perform worse than SIMPPROB, whose static match estimates prove accurate in this situation because of the large number of matches and the relatively small amount of time between matches. We compare the running times of the online algorithms using large, uncorrelated (Zipf 1.0 and 0.0) input streams of N =56,000 tuples, with w =2000, M =800, and a domain size of 2000. The results, shown in Table 1, confirm our previous approximation quality results. DGL provides the best approximation, followed DIMPPROB, SIMPPROB, and PROB , whose relative quality is not as low as in previous experi ments because of the large domain size. Once again, the strategy of retaining high importance input tuples (i.e. SIMP) proves ineffec tive. The relative running times corroborate the cost modeling in Subsection 4.3, with DIMPPROB and DGL requiring more time than the static priority heuristics, including PROB. DGL X  X  greater running time, offset by its superior efficacy, is attributed to its more complex tuple priority formulation. We compared our sliding window join approximation algorithms to semantic and random load shedding techniques in memory-constrained situations. Input data consisted of synthetic data streams with tuple level importan ce semantics. Offline and online semantic and random load shedding produce poor join approximations when dealing with importance semantics, especially when very little join me mory is available. However, our optimal offline algorithm, ODJA, and our extension to semantic load shedding, ESJA, compute an optimal join result and quickly converge upon the exact result as the amount of join memory is increased. ODJA is up to seven times more time efficient than ESJA at small and large join me mory allotments. In addition, ODJA X  X  memory requirement is i ndependent of the input streams X  size. It is 45.3 times more space efficient than ESJA for w =400 and M /2=1. Similar results hold for join memory allotments nearing EXACT. ESJA has superior space and time efficiency when M is neither very small nor large. On the other hand, our online algorithms are consistently superior to PROB, the online semantic load shedding heuristic, in situations where we vary tuple lifetime, memory size, domain size, and skewness. By properly detecting important tuples and those with high match probabilities, our online algorithms produce high quality approximations and quickly converge upon the exact result. DGL degrades most gracefully with decreasing join memory size in uncorrelated data streams and is most efficacious for joins over streams with large domain sizes and long tuple lifetimes, while DIMPPROB and SIMPPROB are mo st efficacious for correlated streams. The ineffectiveness of the SIMP heuristic and the consistency of our results on a large dataset give us confidence that the data sets do not inherently favor our techniques and, respectively, that that the efficacy of our methods is unaffected by specific choices of input stream size, domain size, tuple lifetime, and join memory size. Overall, we discovered that join approximation error diminishes with increasing memory size. The greatest optimization opportunity exists for small join memory sizes, and it is in this situation that our methods show the largest gains over the previous state of the art in terms of efficacy, space efficiency, and time efficiency. This paper examined the problem of computing memory-constrained sliding window join approximations over data streams. We motivated the inclusion of importance semantics within input tuples and the objective function of maximizing the importance of the approximate join result. We introduced efficient optimal offline and effective, lightweight online algorithms and showed that previous load she dding techniques are insufficient for the objective function. Avenues of future work include exploring other useful QoS-based objective functions for window join approximation and incorporating our methods into complex continuous query processing frameworks. [1] Abadi, D. J., Carney D., Centintemel, U., Cherniack, M., [2] Apers, P. and Wilschut, A. Dataflow query execution in a [3] Babcock, B., Babu, S., Datar, M., Motwani, R., and Widom, [4] Bonnet, P., Gherke, J., and Seshadri, P. Towards Sensor [5] Burger, J., Naughton, J., and Viglas, S. Maximizing the [6] Chen, J., DeWitt, D. J., Tian, F., and Wang, Y. NiagaraCQ: [7] Cortes, C., Fisher, K., Pregibon, D., Rogers, A., and Smith, [8] Das, A., Gerkhe, J., and Riedewald, M. Semantic [9] Fegaras, L., Maier, D., Sheard, T., and Tucker, P. Exploiting [10] Franklin, M. J. and Urhan, T. XJoin: A Reactively-Scheduled [11] Goldberg, A. V. An Efficient Implementation of a Scaling [12] Guha, S., Indyk, P., Muthukrishnan, S., and Strauss, M. [13] Golab, L. and Ozsu, M. T. Issues in Data Stream [14] Golab, L. and Ozsu, M. T. Processing Sliding Window [15] Hellerstein, J., Madden, S., Raman, V., and Shah, M. [16] Kang, J., Naughton, J., and Viglas, S. D. Evaluating [17] Naughton, J. and Viglas, S. Rate-Based Optimization for [18] O X  X allaghan L., Mishra N., Meyerson A., Guha S., and [19] Takaoka, T. O(1) Time Algorithms for Combinatorial 
