 in reduced error for a series of experiments. 
Since that earlier work, very strong classification meth-ods have been developed that use ensembles of solutions and voting [2, 1, 5, 14]. In light of the newer methods, we reconsider solving a regression problem by discretizing the continuous output variable using k-means and solving the resultant classification problem. The mean or median value for each class is the sole value to be stored as a possible answer when that class is selected as an answer for a new example. 
To test this approach, we use a recently developed, light-weight rule induction method [14]. It was developed strictly for classification, and like other ensemble methods performs exceptionally well on classification applications. However, classification error can diverge from distance measures used for regression. Hence, we adapt the concept of margins in voting for classification [11] to regression where, analogous to nearest neighbor methods for regression, class means for close votes are included in the computation of the final pre-diction. 
Why not use a direct regression method instead of the in-direct classification approach? Of course, that is the main-stream approach to boosted and bagged regression [9]. Some methods, however, are not readily adaptable to regression in such a direct manner. Some rule induction methods, such as our lightweight method, generate rules sequentially class by class and cannot be applied to a continuous output with-out major revisions to the induction method. Why not try a trivial preprocessing step to discretize the predicted con-tinuous variable? Moreover, if good results can be obtained with a small set of discrete values, then the resultant solu-tion can be far more elegant and possibly more interesting to human observers. Lastly, just as experiments have shown that discretizing the input variables may be beneficial, it may be interesting to gauge experimentally the effects of discretizing the output variable. 
In this paper, we review a recently developed rule induc-tion method for classification. Its use for regression requires an additional data preparation step to discretize the con-tinuous output. The final prediction involves the use of marginal votes. We compare its performance on large pub-lic domain data sets to those of other logic-based methods: single and bagged regression trees. We show that strong predictive performance, in these example better than single regression trees and sometimes better than bagged regres-sion trees, may be achieved with simple rule-based solutions having relatively few unique output values. Input: t, a user-specified threshold (0 &lt; t &lt; 1) Output: C, the number of classes Mt :----mean absolute deviation (MAD) of yi min-gain := t. M1 i:----1 repeat until Mi/2 -Mi &lt;_ rain-gain output C percentage of the MAD from using the median of all val-ues. This percentage is adjusted by the threshold, t. In our experiments, for example, we fixed this to be 0.1 (thereby requiring can that the reduction in MAD be at least 10%). Besides the predicted variable, no other information about the data is used. If the number of unique values is very low, it is worthwhile to also try the maximum number of po-tential classes. In our experiments, we found that this was beneficial when there were not more than 30 unique values. 
Besides helping decide the number of classes, Table 1 also provides an upper bound on performance. For example, with 16 classes, even if the classification procedure were to produce 100% accurate rules that always predicted the cor-rect class, the use of the class median as the predicted value would imply that the regression performance could at best be 0.3505 on the training cases. This bound can be also be a factor in deciding how many classes to use. 
Once the regression problem is transformed into a classi-fication task, standard classification techniques can be used. Of particular interest is a recently developed new ensem-ble method for learning compact disjunctive normal form (DNF) rules [14] that has proven to give excellent results on a wide variety of classification problems and has a time com-plexity that is almost linear in time relative to the number of rules and cases. This Lightweight Rule Induction (LRI) procedure is particularly interesting because it can rival the performance of very strong classification methods, such as boosted trees. 
Figure 2 shows an example of a typical DNF rule gener-ated by LR/. The complexity of a DNF rule is described with two measurements: (a) the length of a conjunctive term and (b) the number of terms (disjuncts). In this example, the rule has a length of three with two disjuncts. Complexity of rule sets generated is controlled within LRI by providing upper bounds on these two measurements. 
The LRI algorithm for generating a rule for a binary clas-sification problem is summarized in Figure 3. FN is the number of false negatives, FP is the number of false pos-itives, and TP, the number of true positives, e(i) is the 289 additive ailerons2 census16 compact elevator kinematics 
Table 3 summarizes the data characteristics. The num-ber of features describes numerical features and categorical variables decomposed into binary features. For each dataset, the number of unique target values in the training data is listed. Also shown is the mean absolute distance (MAD) from the median of all values. For classification, predictions must have fewer errors than simply predicting the largest class. To have meaningful results for regression, predictions must do better than the average distance from the median. This MAD is a baseline on a priori performance. 
For each application, the number of classes was deter-mined by the algorithm in Figure 1 with the user-threshold t set to 0.1. When the number of unique values was not more than 30, solutions were induced with the maximum possible classes as well. 
LPd has several design parameters that affect results: (a) the number of rules per class (b) the maximum length of a rule and (c) the maximum number of disjunctions. For all of our experiments, we set the length of rules to 5 conditions. For almost all applications, increasing the number of rules increases predictive performance until a plateau is reached. In our applications, only minor changes in performance oc-curred after 100 rules. The critical parameter is the number of disjuncts, which depends on the complexity of the un-derlying concept to be learned. We varied the number of with a single conjunctive term. The optimal number of dis-juncts is obtained by validating on a portion of the training data set aside at the start. 
An additional issue with maximizing performance is the use of margins. In all our experiments we included classes having vote counts within 80% of the class having the most votes. 
Performance is measured in terms of error distance. The error-rates shown throughout this section are the mean ab-solute deviation (MAD) on test data. Equation 4 shows how this is done, where yi and y~ are the true and predicted val-ues respectively for the i-th test case, and n is the number of cases in the test set. 
Table 4 summarizes the results for solutions with only 10 rules. The solutions are obtained for variable numbers of disjuncts, and the best rule solution is indicated with an asterisk. Also listed is the optimal tree solution (rain-tree), which is the pruned tree with the minimum test error found by cost-complexity pruning. The tree size shown is the number of terminal nodes in the tree. Also shown is the tree solution where the tree is pruned by significance Figure 5: Relative errors for LRI, Single ~ X ee, and 
Bagged Trees parameter estimation. Thus, we have included results that describe the minimum test error. With big data, it is easy to obtain more than one test sample, and for estimating a single variable, a large single test set is adequate in practice [3]. For purposes of experimentation, we fixed almost all parameters, except for maximum number of disjnncts and the number of rules. The number of disjuncts is clearly on the critical path to higher performance. Its best value can readily be determined by resampling on the large number of training cases. variable, producing pseudo-classes, creates another task for estimation. What is the proper number of classes? The ex-perimental results suggest that when the number of unique values is modest, perhaps 30 or less, then using that number of classes is feasible and can be effective. For true continuous output, we used a simple procedure for analyzing the trend as the number of classes is doubled. This type of estimate is generally quite reasonable and trivially obtainable, but occasionally, slightly more accurate estimates can be found by trying different numbers of classes, inducing rules, and testing on independent test data. tial sources of error beyond those found for other regression models. For a given number of classes less than the number of unique values, the segmentation error, measured by the 
MAD of the median values of the classes, is a lower bound on predictive performance. For pure classification, where the most likely class is selected, the best that the method can do is the MAD for the class medians. In the experimental results, we see this limit for the artificial additive data gen-erated from an exact function (with additive random noise). 
With a moderate number of classes, the method is limited by this approximation error. To reduce the minimum error implied by the class medians, more classes are needed. That in turn leads to a much more difficult classification problem, also limiting predictive performance. ing deviation from the true value. This difference introduces cess. This error is most obvious when we predict using the 291 [6] J. Dougherty, R. Kohavi, and M. Sahami. Supervised [7] U. Fayyad and K. Irani. Multi-interval discretization [8] J. Friedman. Multivariate adaptive regression splines. [9] J. Friedman, T. Hastie, and R. Tibshirani. Additive [10] J. Hartigan and M. Wong. A k-means clustering [11] R. Schapire, Y. Freund, P. Bartlett, and W. Lee. [12] L. Torgo and J. Gama. Regression using classification [13] S. Weiss and N. Indurkhya. Rule-based machine [14] S. Weiss and N. Indurkhya. Lightweight rule 
