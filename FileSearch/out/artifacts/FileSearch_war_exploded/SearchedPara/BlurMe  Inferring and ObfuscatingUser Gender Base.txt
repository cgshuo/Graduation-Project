 User demographics, such as age, gender and ethnicity, are routinely used for targeting content and advertising prod-ucts to users. Similarly, recommender systems utilize user demographics for personalizing recommendations and over-coming the cold-start problem. Often, privacy-concerned users do not provide these details in their online profiles. In this work, we show that a recommender system can infer the gender of a user with high accuracy, based solely on the rat-ings provided by users (without additional metadata), and a relatively small number of users who share their demograph-ics. We design techniques for effectively adding ratings to a user X  X  profile for obfuscating the user X  X  gender, while having an insignificant effect on the recommendations provided to that user.
 Categories and Subject Descriptors: H.2.8 Database Applications: Data Mining Keywords: Recommender Systems, Privacy.
Profiling users through demographic information, such as gender, age, or ethnicity, is of great importance in targeted advertising and personalized content delivery. Recommender systems too can benefit from such information to provide personalized recommendations. However, users of recom-mender systems often do not volunteer this information. This may be intentional  X  to protect their privacy, or unin-tentional  X  out of laziness or disinterest. As such, traditional collaborative filtering methods eschew using such informa-tion, relying instead solely on ratings provided by users.
At a first glance, disclosing ratings to a recommender sys-tem may appear as a rather innocuous action. There is cer-tainly a utility users accrue from this disclosure  X  namely, the ability to discover relevant items. Nevertheless, there has been a fair amount of work indicating that user demo-graphics are correlated to, and thus can be inferred from, user activity on social networks [9], blogs [2], and micro-blogs [12] etc. It is thus natural to ask whether demographic information such as age, gender, ethnicity or even political orientation can also be inferred from information disclosed to recommender systems. Indeed, irrespective of a rating value, the mere fact that a user has interacted with an item ( e.g. , viewed a specific movie or purchased a product) may be correlated with demographic information.

The potential success of such an inference has several im-portant implications. From the recommender X  X  perspective, profiling users not only improves their own recommenda-tions, but also enables targeted advertising. From the user X  X  perspective, the success of such an inference raises serious privacy concerns. A privacy-conscious user cannot simply withhold all information as this would come at the cost of foregoing the utility gained from using the recommender sys-tem in the first place  X  namely, finding relevant content. Ex-plicitly withholding the user X  X  demographic information does not ensure privacy either, as it may be possible to uncover it through inference. Because the approach of withholding in-formation is often impractical, we believe a more promising approach is that of adding ratings into a user profile with the intent of creating ambiguity. In this paper, we thus explore both the questions of how demographic information can be inferred  X  from ratings data alone  X  and that of how to hin-der such inference via obfuscating the information disclosed to the recommender system.

In general, any obfuscation mechanism employed by the user strikes a tradeoff. This is between (a) the user X  X  pri-vacy, as captured by the recommender X  X  ability to infer her demographic information, and (b) the utility to the user, captured by the accuracy of recommendations she receives. Understanding the nature of such a tradeoff is thus a funda-mental question. In this work, we study the above issues in a comprehensive manner, making the following contributions:  X  We evaluate several gender inference algorithms on two  X  We find that the act of watching a movie, regardless of  X  Based on these observations, we propose several obfus- X  We further evaluate these mechanisms with respect to the  X  We establish that quite favorable tradeoffs are feasible;
To the best of our knowledge, we are the first to study and quantify demographic inference methods that rely solely on rating data. Moreover, we are the first to design and analyze obfuscation mechanisms aiming to preserve gender privacy while maintaining recommendation accuracy.
Inferring demographics of users has been widely studied in different contexts, and for various types of user-generated data. In the context of interaction networks, the graph structure has been shown to be useful for inferring demo-graphics using link-based information for blog [2] and social network [9] data from Facebook. Other works rely on the textual features derived from writings of users to infer de-mographics. For instance, Rao et al. [12] use an SVM clas-sifier on Twitter data, and Otterbacher et al. use logistic regression on movie reviews from IMDB [11]. It is useful to note that the prediction accuracy obtained using logistic regression on movie reviews is about 73.7%, lower than that obtained using the same algorithm on movie ratings, albeit for a different dataset. In our setting, the input to the gen-der inference mechanism is only the movie ratings provided by users, with no metadata about movies or users. While there has been work on collective matrix factorization [13] to take into account attributes of movies and users in addition to ratings for making recommendations, the work does not explore the specific task of inferring user demographics.
Rather than focusing solely on inference, our goal is to be able to use insights gained to design mechanisms that obfuscate users X  demographics. Injecting noise for privacy was recently studied in [15, 14] for search privacy, where the goal is to obfuscate search engine queries rather than a user X  X  demographics.

Our work is also related to studies of robustness in rec-ommender systems [10, 3, 1]. The goal of such studies is to evaluate how an attacker can manipulate a recommender system by injecting adversarially selected ratings. In con-trast, our study is based on the interaction of a user, whose ratings are not necessarily added in the training set, with the recommender system. Although the user may submit altered ratings, her interest is still in receiving relevant rec-ommendations, albeit without disclosing her gender.
An elegant and formal approach to privacy in recommender systems has been made through differential privacy [8]. Nev-ertheless, differential privacy guarantees aim at a different goal, which is to ensure that the output of a recommender depends only marginally on the input of any single user. In contrast, we aim at not protecting ratings per se, but the demographic information of each user; this notion cannot be captured within the formalism of differential privacy.
For the sake of concreteness, we assume throughout the paper that the information users wish to protect is their gen-der; nevertheless, our algorithms are generic, and apply also when different demographic features (age, ethnicity, politi-cal orientation, etc. ), expressed as a categorical variable, are to be protected.

Our setup is summarized in Figure 1. A user, indexed by 0, views and rates items which, for concreteness, we refer to as movies. We assume that the universe of movies the user can rate comprises a catalog of M movies; the user rates a subset S 0 of the catalog M = { 1 , 2 ,...,M } . We denote by r 0 j  X  R the rating of movie j  X  X  0 and define the user X  X  rating profile as the set of (movie, rating) pairs H { ( j,r 0 j ) : j  X  S 0 } . The user submits H 0 to an obfuscation mechanism , which outputs an altered rating profile H { ( j,r 0 0 j ) : j  X  S 0 0 } , for some S 0 0 6 = S 0 this obfuscation aims at striking a good balance between the following two conflicting goals : (a) H 0 0 can be used to provide relevant recommendations to the user, and (b) it is difficult to infer the user X  X  gender from H 0 0 .

More specifically, we assume that the obfuscated rating profile H 0 0 is submitted to a recommender mechanism that has a module that implements a gender inference mecha-nism. The recommender mechanism uses H 0 0 to predict the user X  X  ratings on M\S 0 0 , and potentially, recommend movies that might be of interest to the user. The gender inference module is a classification mechanism, that uses the same H to profile and label the user as either male of female.
Though the implementation of the recommender mecha-nism might be publicly known, the obfuscation and gender inference mechanisms are not. As a first step in this prob-lem, we take the simple approach that both recommendation and gender inference are oblivious to the fact that any kind of obfuscation is taking place. Both mechanisms take the profile H 0 at  X  X ace value X  and do not reverse-engineer the  X  X rue X  profile H . (We leave for future work the case when the obfuscation mechanism is known.)
We assume that the recommender and inference mecha-nisms have access to a training dataset . This dataset com-prises a set of N = { 1 ,...,N } users each of which has given ratings to a subset of the movies in the catalog M . We denote by S i  X  M the set of movies for which the rating of a user i  X  N is in the dataset, and by r ij , j  X  S i rating given by user i  X  N to movie j  X  M . Moreover, for each i  X  N the training set also contains a binary variable y  X  X  0 , 1 } indicating the gender of the user (we map bit 0 to male users). We assume that the training set is unadulter-ated: neither ratings nor gender labels have been tampered with or obfuscated.

The obfuscation mechanism may also have a partial view of the training set. In the extreme case, the training dataset is public, and the obfuscation mechanism has full access to it. It is interesting however to consider weaker obfuscation mechanisms, that can only access limited statistics (or other queries over the dataset), such as, the average rating of a movie. Though the mechanisms we propose can, a fortiori , be implemented when the dataset is public, we will state the training set statistics required in their implementation.
The main focus of this paper is the design and analysis of mechanisms for gender inference and obfuscation. As such, we fix the recommender mechanism throughout the paper to be matrix factorization [6], since this is commonly used in commercial systems. In short, given the rating profile H we generate ratings for the set M\S 0 by appending the provided ratings to the rating matrix of the training set and factorizing it.

More specifically, we associate with each user i  X  X   X  X  0 } a latent feature vector u i  X  R d . We also associate with each movie j  X  X  a latent feature vector v j  X  R d . We define the regularized mean square error to be where  X  is the average rating of the entire dataset. We construct the vectors u i , v j by minimizing the MSE through gradient descent. We use d = 20 and  X  = 0 . 3. Having profiled thusly both users and movies, we predict the rating of user 0 for movie j  X  X \S 0 0 through  X  u 0 ,v j  X  +  X  . Flixster. Flixster is an online social network for rating and reviewing movies. Flixster allows users to enter demographic information into their profiles and share their movie ratings and reviews with their friends and the public. The dataset collected by Jamali et al. [5] has 1M users, of which only 34.2K users share their age and gender. We evaluate our techniques on this subset of 34.2K users, who have rated 17K movies and provided 5.8M ratings. The 12.8K males and 21.4K females have provided 2.4M and 3.4M ratings, re-spectively. Flixster allows users to provide half star ratings, however, to be consistent across the evaluation datasets, we round up the ratings to be integers from 1 to 5.
 Movielens. Our second dataset is Movielens from the Grou-plens 1 research team. The dataset consists of 3.7K movies and 1M ratings by 6K users. The 4331 males and 1709 fe-males provided 750K and 250K ratings, respectively.
In this section, we investigate whether inferring a user X  X  gender based on her ratings is indeed possible. We study sev-eral different classifiers and evaluate them using the Flixster and Movielens datasets. We use the results of this analysis to inform our design of obfuscation mechanisms (Section 5).
To train our classifiers, we associate with each user i  X  X  in the training set a characteristic vector x i  X  R M such that x ij = r ij , if j  X  S i and x ij = 0, otherwise. Recall that the binary variable y i indicates user i  X  X  gender, which serves as the dependent variable of our classification. We denote www.grouplens.com/node/73 by X  X  R N  X  M the matrix of characteristic vectors, and by Y  X  X  0 , 1 } N the vector of genders.

We use three different types of classifiers: Bayesian classi-fiers, support vector machines (SVM) and logistic regression. In the Bayesian setting, we studied several different gener-ative models; for all models, we assume that points ( x i are sampled independently from the same joint distribution P ( x,y ). Given P , the predicted label  X  y  X  { 0 , 1 } attributed to characteristic vector x is the one with maximum likeli-hood, i.e. , Class Priors. The class prior classification serves as a base-line method for assessing the performance of the other classifiers. Given a dataset with unevenly distributed gen-der classes of the population, this basic classification strat-egy is to classify all users as having the dominant gender. This is equivalent to using (1) under the generative model P ( y | x ) = P ( y ), estimated from the training set as: Bernoulli Na  X   X ve Bayes. Bernoulli Na  X   X ve Bayes is a simple method that ignores the actual rating value. In particular, it assumes that a user rates movies independently and the decision to rate or not is a Bernoulli random variable. For-mally, given a characteristic vector x , we define the rating indicator vector  X  x  X  R M to be such that  X  x j = 1 x captures the movies for which a rating is provided. Assum-ing that  X  x j , j  X  M , are independent Bernoulli, the genera-tive model is given by P ( x,y ) = P ( y ) Q j  X  X  P (  X  x P ( y ) is the class prior, as in (2), and the conditional P (  X  x is computed from the training set as follows:
P (  X  x j | y ) = |{ i  X  X  :  X  x ij =  X  x j  X  y i = y }| / |{ i : y Multinomial Na  X   X ve Bayes. A drawback of Bernoulli Na  X   X ve Bayes is that it ignores rating values. One way of incor-porating them is through Multinomial Na  X   X ve Bayes, which is often applied to document classification tasks [7]. Intu-itively, this method extends Bernoulli to positive integer values by treating, e.g. a five-star rating as 5 indepen-dent occurrences of the Bernoulli random variable. Movies that receive high ratings have thus a larger impact on the classification. Formally, the generative model is given by P ( x,y ) = P ( y ) Q j  X  X  P ( x j | y ) where P ( x j | y ) = P (  X  x and P (  X  x j | y ) is computed from the training set through (3). Mixed Na  X   X ve Bayes. We propose an alternative to Multi-nomial, which we refer to as Mixed Na  X   X ve Bayes. This model is based on the assumption that, users give normally dis-tributed ratings. More specifically, For each movie j , we estimate the mean  X  yj from the dataset as the average rating of movie j given by users of gen-der y , and the variance  X  2 y as the variance of all ratings given by users of gender y . The joint likelihood used in (1) is then given by P ( x,y ) = P ( y ) Q j  X  X  P (  X  x j where P ( y ), P (  X  x j | y ) are estimated through (2) and (3), respectively. The conditional P ( x j |  X  x j ,y ) is given by (4) P ( x j = 0 |  X  x j = 0 ,y ) = 1, when it is not. Logistic Regression. A significant drawback of all of the above Bayesian methods is that they assume that movie rat-ings are independent. To address that, we applied logistic regression. Recall that linear regression yields a set of coeffi-cients  X  = {  X  0 , X  1 ,..., X  M } . The classification of a user i  X  N with characteristic vector x i is performed by first calculat-user is classified as a female if p i &lt; 0 . 5 and as a male oth-erwise. The value p i also serves a confidence value for the classification of user i . One of great benefits of using logistic regression is that the coefficients  X  capture the extent of the correlation between each movie and the class. In our case, the large positive  X  j indicates that movie j is correlated with class male, whereas small negative  X  j indicates that movie j is correlated with class female. We select the regularization parameter so that we have at least 1000 movies correlated with each gender that have a non-zero coefficient.
 SVM. Intuitively, SVM finds a hyperplane that separates users belonging to different genders in a way that minimizes the distance of incorrectly classified users from the hyper-plane (for a thorough explanation on SVMs see [4]). SVM holds many of the advantages of logistic regression  X  it does not assume independence in the feature space and produces coefficients. Since our feature space (number of movies) is already quite large, we use linear SVMs in our evaluations. We performed a logarithmic search over the parameter space ( C ) and found that C = 1 gave the best results.
We evaluate all algorithms on both the Flixster and Movie-lens datasets. We use 10-fold cross validation and compute the average precision and recall for the two genders across all folds. Additionally, we compute the Area Under the Curve (AUC) using the mean Receiver Operating Characteristic (ROC) curve computed across the folds. For the ROC, the true positive ratio is computed as the ratio of males correctly classified out of the males in the dataset, and the false posi-tive ratio is computed as the ratio incorrectly classified males out of the females in the dataset. The ROC curves are given in Figure 2a and Figure 2b. Table 1 provides a summary of the classification results for 3 metrics: AUC, precision and recall. Table 2 shows the same results separated per-gender.
We see from the ROC curves that SVM and logistic regres-sion perform better, across both datasets, than any of the Bayesian models since the regression curves for SVM and lo-gistic dominate the others. In particular, logistic regression performed the best for Flixster while SVM performed best for Movielens. The performance of the Bernoulli, mixed and multinomial models do not different significantly from one another. These findings are further confirmed via the AUC values in Table 1. This table also shows the weakness of the simple class prior model that is easily outperformed by all other methods.

In terms of precision and recall, Table 2 shows that logis-tic regression outperforms all other models for Flixster users and both genders. For the Movielens users, SVM performs better than all other algorithms, while logistic regression is second best. In general, the inference performs better for the gender that is dominant in each dataset (female in Flixster and male in Movielens). This is especially evident for SVM, which exhibits very high recall for the dominate class and low recall for the dominated class. The mixed model im-proves significantly on the Bernoulli model and results sim-ilarly to the multinomial. This indicates that the usage of a Gaussian distribution might not be a sufficiently accurate estimation for the distribution of the ratings.
 Impact of user ratings. We assess the importance of the rating value itself (number of stars) versus the simple bi-nary event  X  X atched or not X  by applying logistic regression and SVM on a binary matrix, denoted by  X  X , in which rat-ings are replaced by 1. Table 1 shows the performance of these two methods on X and  X  X . Interestingly, SVM and lo-gistic regression performed only slightly better when using X rather than  X  X as input, with less than 2% improvement on all measures. In fact, Table 2 indicates that although using X performs better than using  X  X for the dominant class, it is worse for the dominated class. Similarly, the Bernoulli model, which also ignores the rating values, per-formed relatively close to Multinomial and Mixed. This im-plies that whether or not a movie is included in one X  X  profile is nearly as impactful as the value of star rating given for the movie. This has important ramifications for obfusca-tion mechanisms that need to do two things: decide which movies to add to a user profile, and decide which rating to give a movie. This finding suggests that the choice of which movies to add could have a large impact on impeding gen-der inference. However if the actual ratings do not impact gender inference much, then we could select a rating value that helps maintain the quality of recommendations.
We focus on logistic regression to further understand the classification results, since it provides us with coefficients for the movies and confidence in the gender inference. We note that a similar analysis can be done using SVM, which we omit for brevity.
 Effect of training set size. Since we use 10-fold cross vali-dation, our training set is large relative to the evaluation set. We use the Flixster data to assess the effect that the number of users in the training set size has on the inference accu-racy. In addition to the 10-fold cross validation giving 3000 users in the evaluation set, we performed a 100-fold cross validation using a 300-user evaluation set. Additionally, we incrementally increased the training set, starting from 100 users and adding 100 more users on each iteration.

Figure 2c plots the precision of the logistic regression in-ference on Flixster for the two evaluation set sizes. The figure shows that for both sizes, roughly 300 users in the training set are sufficient for the algorithm to reach above 70% precision, while 5000 users in the training set reaches a precision above 74%. This indicates that a relatively small number of users are sufficient for training.
 Movie-Gender Correlation. The coefficients computed by logistic regression expose movies that are most correlated with males and females. Table 3 lists the top 10 movies cor-related with each gender for Flixster; similar observations as the ones below hold for Movielens. The movies are or-dered based on their average rank across the 10-folds. We use average rank since the coefficients can vary significantly between folds, but the order of movies does not. The top gender correlated movies are quite different depending on whether X or  X  X is used as input. For example, out of the top 100 most female and male correlated movies, only 35 are the same for males across the two inputs, and 27 are the same for females; the comparison yielded a Jaccard dis-tance of 0.19 an 0.16, respectively. We saw that many of the movies in both datasets align with the stereotype that action and horror movies are more correlated with males, while drama and romance are more correlated with females. However, gender inference is not straightforward because the majority of popular movies are well liked by both genders.
Table 3 shows that in both datasets some of the top male correlated movies have plots that involve gay males, (such as Latter Days, Beautiful Thing, and Eating Out); we observed the same results when using  X  X . The main reason for this is that all of these movies have a relatively small number of ratings, ranging from a few tens to a few hundreds. In this case it is sufficient for a small variance in the rating distri-Table 3: Top male and female correlated movies in Flixster butions between genders with respect to the class priors, to make the movie highly correlated with the class.

Further, we evaluate whether the distribution of movie ratings are different for males and females, and whether movies that are correlated with a gender tend to have more ratings from users of that gender. Figure 2d shows that the rating distribution is similar for females and males in the two datasets. In Figure 2e each dot corresponds to a movie; the x-axis plots the normalized difference in the number of ratings between females and males, and the y-axis (scaled to -1 to 1) shows the difference in the value of the ratings between the sexes. Green circles in the upper-right por-tion (blue crosses in lower left) of the plot indicate that the pair of features (captured on the two axes) can explain the highly female (male) correlated movies, respectively. While this pair of features explains some of the observed gender correlation, they are not nearly sufficient to explain all of it since more than half of the gender correlated movies lie in the middle of this plot.
 Confidence in classification. Finally, the confidence value of the classifier is the obstacle that an obfuscation mecha-nism needs to overcome when trying to hide the gender from the classifier. The higher the confidence of the classifier in its prediction, the more effort the obfuscation method needs to apply, possibly increasing the impact on the recommen-dations. Therefore, we evaluate whether the classifier has different confidence values when it outputs a correct or in-correct classification. Figure 2f plots the CDF of the confi-dence value for correct and incorrect classifications, showing that the confidence is higher when the classification is cor-rect, with a median confidence for incorrect classifications of 0.65, while for correct classification it is 0.85. Moreover, nearly 20% of correct classifications have a confidence of 1.0, which holds for less than 1% of incorrect classifications.
The obfuscation mechanism takes as input a user i  X  X  rat-ing profile H i , a parameter k that represents the number of permitted alterations, and information from the training set to output an altered rating profile H 0 i such that it is hard to infer the gender of the user while minimally impacting the quality of recommendations received. In general, such a mechanism can alter H i by adding, deleting or changing movie ratings. We focus on the setting in which the obfusca-tion mechanism is only allowed to add k movie ratings, since deleting movies is impractical in most services and chang-ing ratings is more suspicious than adding ratings. Because users have different numbers of movies rated in their profiles (and some may have a small number), we do not use a fixed number k but rather we add a number that corresponds to a given percentage of movies in a user X  X  rating profile. In order to add movies into a user X  X  profile, the obfuscation mechanism needs to make two non-trivial decisions:  X  Which movies should be added?  X  What should be the rating assigned to each movie?
We refer to these added movie ratings as extra ratings . We note that the rating values assigned are not  X  X oise X  but have some useful value. For example, if this rating corresponds to the average rating over all users, or the predicted rat-ing (using matrix factorization) for a specific user, then the rating value is a reasonable predictor of how the user may have rated had he watched the movie. In this work, we do not aim to provide an exhaustive list of obfuscation mecha-nisms, instead our goal is to design mechanisms informed by observations from our gender inference study (Section 4).
To simplify the discussion, we first assume that the obfus-cation mechanisms have full access to the training dataset, and can use it to derive information for selecting movies and ratings to add. Later in this section, we amend the assump-tion of full access to the dataset.
 Movie selection. We design three intuitive strategies for selecting movies. Each strategy takes as input a set of movies S i rated by the user i , a the number of movies k to be added that corresponds to p % of i  X  X  existing profile, and or-dered lists L M and L F of male and female correlated movies, respectively, and outputs an altered set of movies S 0 i , where S  X  X  0 i . The lists L M and L F are stored in decreasing order of the value of a scoring function w : L M  X  L F  X  R where w ( j ) indicates how strongly correlated a movie j  X  L M is with the associated gender. A concrete example of the scoring function is to set w ( j ) =  X  j , where  X  j is the coef-ficient of movie j obtained by learning a logistic regression model from the training dataset. We will use this instantia-tion of the scoring function in our evaluation. Additionally, we assume that k &lt; min( | L M | , | L F | )  X  X S i | and L
The movie selection process is as follows. For a given female (or, male) user i , we initialize S 0 i = S i . Each strategy repeatedly picks a movie j from L M (or, L F ), and if j 6 X  X  it adds j to S 0 i , until k movies have been added. The set S is the desired output. The three strategies differ in how a movie is picked from the ordered lists of movies. 1. Random Strategy. For a given female (male) user i , 2. Sampled Strategy. Sample a movie based on the dis-3. Greedy Strategy. Pick the movie with the highest Rating assignment. In Section 4.2, we made a key ob-servation that the binary event of including or excluding a movie in a profile (indicating watched or not) was a signal for gender inference nearly as strong as the ratings. Given that, we aim to assign ratings to the extra movies that have a low impact on the recommendations provided to a user. We propose and evaluate two rating assignments: 1. Average movie rating. The obfuscation mechanism 2. Predicted rating. The obfuscation mechanism com-Access to Dataset. Earlier we assumed the obfuscation mechanism had unrestricted access to the training set. We point out now that our mechanisms described above require access only to the following quantities: (a) for movie selec-tion: ordered lists of male and female correlated movies, and (b) for rating assignment: average movie ratings, and movie latent factors to predict user movie ratings. Note that this information can be found from publicly available datasets, such as the Netflix Prize dataset 2 . Assuming that users in such public datasets are statistically similar overall to those in a particular recommender systems, then we no longer need the assumption of access to the training set.
We evaluate all the permutations of movie selection and rating assignment strategies proposed above. We evaluate values of k corresponding to 1%, 5% and 10% |S i | for each user i . The movie scores in lists L M and L F are set to the corresponding logistic regression coefficients.
 Impact on privacy. We capture the privacy gain that obfuscation brings via the reduced performance in gender inference. Table 4 shows the accuracy of inference for all three movie selection strategies (i.e., random, sampled and http://www.netflixprize.com/index Table 4: Accuracy of gender inference for different strategies, when rating assignment is average movie rating greedy) when the rating assigned is the average movie rat-ing. The accuracy is computed using 10 fold cross valida-tion, where the model is trained on unadulterated data, and tested on obfuscated data.

Since the accuracy of inference is the highest for the logis-tic regression classifier, it would be the natural choice as the inference mechanism for a recommender system. Figures 3a and 3d show the drop in inference accuracy for adding noisy ratings for the two datasets. On adding just 1% extra ratings using the greedy strategy, the accuracy drops to 15% (that is an 80% decrease) and with 10% extra ratings the accuracy is close to zero for the Flixster dataset, as compared with the accuracy of 76.5% on the unadulterated data. Therefore, if the obfuscation mechanism selects movies according to the greedy strategy, adding a small number of movies is suffi-cient to obfuscate gender. Even when the movies are chosen using the random strategy (which ignores movie scores and hence, the logistic regression coefficients), just 10% addi-tional movies correlated with the opposite gender are suffi-cient to decrease the accuracy of gender inference by 63% (from 76.5% to 28.5% accuracy). Similar trends are observed for the Movielens dataset.

Our obfuscation mechanism above is using ordered lists that correspond well to the inference mechanism X  X  notion of male or female correlated movies. However, in general, the obfuscation mechanism does not know which inference algo-rithm is used and thus lists such as L M and L F may have a weaker match to such a notion interior to the inference algorithm. We evaluate our obfuscation under such a sce-nario, with Multinomial Na  X   X ve Bayes and SVM classifiers. Our obfuscation still performs well as we see in Table 4, the inference accuracy of the Multinomial classifier drops from 71% to 42.1% for Flixster, and from 76% to 60% for the Movielens dataset (with 10% extra ratings and the greedy strategy). Obfuscation results in a similar decrease in gender inference accuracy of SVMs, results omitted for brevity. Impact on recommendations. Next, we evaluate the im-pact on the recommendation quality that the user will ob-serve if she obfuscates her gender. We measure this impact by computing the RMSE of matrix factorization on a held-out test set of 10 ratings for each user. Again, we perform 10 fold cross validation, where the data for users in 9 folds is unadulterated, and one of the folds has users with additional noisy ratings. That is, we use H 0 for a tenth of the users, and H for the rest. This is equivalent to evaluating the change in RMSE for 10% of the users in the system who obfuscate their gender. Figures 3b and 3e show the change in RMSE Table 5: Accuracy of gender inference for different strategies, when rating assignment is users X  predicted ratings due to obfuscation for Flixster and Movielens, respectively, when the ratings added were the same as in Table 4. Over-all, we see that obfuscation has negligible impact on RMSE. For Flixster, we see that compared to the case of no extra ratings ( X  X one X ) the RMSE increases with additional rat-ings, although negligibly. For Movielens, we observe a slight decrease in RMSE with extra ratings. We conjecture that this may occur because by adding extra ratings we increase the density of the original rating matrix which may improve the performance of matrix factorization solutions. Another explanation could be that the extra ratings are not arbi-trary, but somewhat meaningful (i.e., the average across all users). The key observation is that for both datasets, the change in RMSE is not significant, a maximum of 0.015 for Flixster (with random strategy and 10% extra ratings), and 0.058 for Movielens (with sampled strategy and 10% extra ratings).
 Analyzing privacy-utility tradeoff. We now take a com-prehensive look at the privacy-utility tradeoff of the pro-posed obfuscation, where the desired high privacy corre-sponds to a low accuracy of gender inference, and a high utility corresponds to a low RMSE which is often used as a proxy for high quality recommendations. Figures 3c and 3f show the privacy (inference accuracy) on the x-axis and util-ity (RMSE) on the y-axis. Each point on the curve corre-sponds to the amount of extra ratings, where the rightmost point corresponds to no additional ratings, and the follow-ing points moving left are 1%, 5% and 10% extra ratings. For the Flixster dataset, as we move towards higher privacy the utility decreases. As described above, for Movielens as we move towards higher privacy, the utility increases how-ever only slightly. These plots illustrate the clear trend that our obfuscation mechanism can lead to a substantial reduc-tion in gender inference accuracy yet only incurs very small changes to the quality of the recommendations.
 Preserving recommendation quality. We now evaluate the tradeoff when the rating assignment corresponds to the  X  X redicted ratings X  approach (Section 5.1). The motivation behind this rating assignment is that, in principle, this ob-fuscation results in no change in RMSE as compared with the RMSE on unaltered data. In other words, there is no tradeoff to be made on the utility front with this choice of rating assignment. Table 5 shows the accuracy of gender in-ference when this rating assignment is used. The results are similar those in Table 4 where the rating assignment is the average movie rating. For the Movielens data, the accuracy of gender inference is slightly lower with predicted ratings; (a) Inference on obfuscation -Flixster (d) Inference on obfuscation -Movie-lens for example, for the greedy strategy with 1% extra ratings, the accuracy of the logistic regression classifier reduces from 57.7% to 48.4% -and this benefit comes without sacrificing the quality of recommendations.

In conclusion, our experimental evaluation shows that with small amount of additional ratings, it is possible to protect a user X  X  gender by obfuscation, with an insignificant change to the quality of recommendations received by the user.
In this work we show that a user X  X  rating profile alone can be used to infer her gender with high accuracy. Given a rel-atively small training set, our inference algorithms correctly predict the gender of users with a precision of 70%-80%. We use the insights from inferring gender to design obfuscation mechanisms that add ratings to a user X  X  profile with the goal of making it hard to infer the user X  X  gender, while posing a minimal impact on recommendation quality. We evaluate the tradeoff in the privacy and utility for different obfusca-tion mechanisms and show that just 1% additional ratings to a user X  X  profile decreases the inference accuracy by 80%.
Although the focus of this paper is on a relatively simple binary inference of the gender, it raises a red flag regard-ing the possibility to infer private information about users based on the apparently non-revealing act of rating items for purpose of recommendations, unlike the more explicit actions performed in social networks. We plan to further study the accuracy of more sensitive private information in future work.
 Acknowledgments. The authors would like to thank Mohsen Jamali for sharing the Flixster dataset.
