 John Paisley 1 jpaisley@berkeley.edu David M. Blei 3 blei@cs.princeton.edu Michael I. Jordan 1 , 2 jordan@eecs.berkeley.edu
Department of EECS, 2 Department of Statistics, UC Berkeley Department of Computer Science, Princeton University Mean-field variational Bayesian (MFVB) inference is an optimization-based approach to approximating the full posterior of the latent variables of a Bayesian model (Jordan et al., 1999). It has been applied to many problem domains, for example mixture model-ing (Blei &amp; Jordan, 2006), sequential modeling (Beal, 2003) and factor analysis (Paisley &amp; Carin, 2009). In addition, recent development of the theory has ex-tended the method to online inference and stochastic optimization settings, making variational Bayes a vi-able approach for Bayesian learning with massive data sets (Hoffman et al., 2010; Wang et al., 2011). Variational Bayes approximates the full posterior by attempting to minimize the Kullback-Leibler diver-gence between the true posterior and a predefined fac-torized distribution on the same variables. Minimiz-ing this divergence is equivalent to maximizing the fa-miliar variational objective function. To review, let  X  = {  X  i } represent the set of latent variables (ran-dom effects and parameters) in the model and X rep-resent the data. The joint likelihood of X and  X  is P ( X,  X  |  X ), with  X  the set of hyperparameters. Varia-tional inference approximates the posterior P ( X  | X,  X ) with a Q distribution that takes a set of variational parameters  X  = {  X  i } . This distribution is factorized, Q ( X  |  X ) = Q i q i (  X  i |  X  i ), and the values of  X  are opti-mized to maximize the objective function, The solution is only locally optimal when L is not convex, which is usually the case. Most variational inference algorithms optimize L by coordinate ascent, which repeatedly cycles through and optimizes with respect to each variational parameter  X  i . Often the locally optimal value of  X  i has a closed-form solution, for example in conjugate exponential models.
 The log of the joint likelihood results in a sum of terms; a major issue that often arises in MFVB is that not all expectations in this sum are in closed form. A typical solution in this case is to replace the problematic func-tion with another function of the same variables (plus auxiliary variables) that is a point-wise lower bound. This new function is selected such that the expectation is tractable. While inference can now proceed, a draw-back of introducing bounds is that the true variational objective function is no longer being optimized, which may lead to a significantly worse posterior approxi-mation. Therefore, much attention has been paid to developing tight bounds of commonly occurring func-tions (e.g., Jaakkola &amp; Jordan (2000), Marlin et al. (2011), Leisink &amp; Kappen (2001)).
 We present a method for directly optimizing Eq. (1) for models in which not all expectations are tractable; we show how a stochastic approximation of  X   X  i L can allow for optimization of L when the expecta-tion E q i [ln P ( X,  X  |  X )] is not in closed form. The ap-proximation is unbiased, and so by using the proposed stochastic method we are directly optimizing L . Our stochastic approximation is based on Monte Carlo integration, for which the number of samples heavily depends on the variance of this approximation. We in-troduce a control variate (Ross, 2006) to significantly reduce the variance of this stochastic approximation. A control variate is a tractable function g that is highly correlated with the intractable function f . The func-tion g replaces f in Eq. (1), and the gradient is then stochastically corrected for bias.
 Existing lower bounds have properties that make them ideal as control variates, and thus can improve the speed of the algorithm. However, a major advantage of the control variate methodology is that it does not require the tractable function g to bound f , but only to correlate well with it (i.e., to approximate it well mod-ulo a scaling). This opens the door to many more func-tions that may give better approximations than a lower bound. One of these possible functions is the second-order Taylor expansion, which often gives a very good approximation, while also allowing for closed-form ex-pectations. We show the potential performance gain using this function as a control variate, which we de-note the control variate delta method for MFVB. Related work. Recent work by Knowles &amp; Minka (2011) has also addressed the problem of intractable expectations in MFVB inference in the context of de-veloping a more general variational message passing al-gorithm. Our solution arises from a different perspec-tive and results in a new algorithm based on stochastic optimization. Graves (2011) considers a similar prob-lem for neural networks, but a lack of control variates limits the algorithm to significantly simpler variational approximations. Stochastic search algorithms have also been developed for models of Evolution Strate-gies (see, e.g., Yi et al. (2009)). Mean-field variational Bayesian (MFVB) inference ap-proximates the full posterior of the latent variables of a Bayesian model with a factorized distribution. As motivated in the introduction, let  X  = {  X  i } be these variables, X the data and  X  all hyperparameters of the prior distributions on  X . We define the factor-ized distribution on  X  to be Q ( X  |  X ) = Q i q i (  X  where  X  i are the parameters of the q i distributions. The variational objective function arises by bounding the marginal likelihood using the Q distribution, Maximizing this lower bound (denoted L ) with re-spect to  X  is equivalent to minimizing the Kullback-Leibler divergence between Q ( X ) and P ( X  | X,  X ), which makes up the difference in Eq. (2).
 To facilitate our discussion, we write the functions ap-pearing in the log joint likelihood as ln P ( X,  X  |  X ) = P j f j ( X A j ,  X  B j ), where A j indexes the data appear-ing in function j and B j indexes the latent variables appearing in function j . We note that the index j does not correspond to variables or distributions, but to the terms of the log joint likelihood. Using this notation, the variational lower bound in Eq. (1) becomes For each function f j , those  X  i /  X   X  B j will have their corresponding q i removed from the expectation. For those  X  i  X   X  B j , the expectation of f j results in a new function of variational parameters  X  i  X   X  B j . Ideally, all expectations will be in closed form, allowing for the optimization of  X  to proceed.
 In the case where an expectation in Eq. (3) is not tractable, a nicer functional lower bound can replace the problematic function. That is, let E q i [ f j (  X  intractable. 1 A common approach to dealing with this issue is to introduce a function g (  X  i , X  ) that replaces f and is a point-wise lower bound: f j (  X  i )  X  g (  X  i , X  ) for all  X  i . The function g usually takes auxiliary variables  X  , which determines how tightly g approximates f j and is tuned along with the other parameters during infer-ence. The expectation E q i [ g (  X  i , X  )] has a closed-form solution, and gives a lower bound on the variational objective that can be optimized.
 To illustrate, consider the case where f j is convex in  X  Then a bound g could be a first-order Taylor expansion of f j about the point  X  , which has a closed-form ex-pectation. Significantly tighter tractable bounds have also been developed for various frequently occurring functions (e.g., Marlin et al. (2011), Knowles &amp; Minka (2011)). In general, the looser the bound the further one is from optimizing the variational objective, and learning of  X  i can suffer as a result. We next present a method based on stochastic search for directly optimizing the variational objective func-tion L in cases where some expectations cannot be computed in the log joint likelihood. This method uses a stochastic approximation of the gradient with respect to the variational parameters of the associated q distribution. To further simplify notation, we drop all indices; f is the intractable function of  X  (plus other variational parameters), and  X  has a variational distri-bution q taking parameters  X  .
 We separate the lower bound L into two functions, E f and h , where h ( X,  X ) contains everything in L except for E f . Notably, h contains all other functions of  X  resulting from expectations calculated with respect to q . In coordinate ascent variational inference, the first step in optimizing q with respect to its parameters  X  is to take the gradient of the variational objective, This gradient contains a tractable term resulting from  X   X  h , and an intractable term  X   X  E q f . Our goal is to make a stochastic approximation of this gradient. To this end, assuming the necessary regularity conditions, we rewrite this function as can stochastically approximate this expectation using Monte Carlo integration, where  X  ( s ) iid  X  q (  X  |  X  ) for s = 1 ,...,S . We can there-fore replace  X   X  E q [ f (  X  )] with the unbiased stochastic approximation of this gradient in Eq. (6). Denote this approximation as  X  . At iteration t , we update the vari-ational parameter  X  by taking a gradient step, By decreasing the step size  X  t such that P  X  t =1  X  t =  X  and P  X  t =1  X  2 t &lt;  X  , convergence to a local optimal so-lution of L is guaranteed. For example,  X  t = ( w + t )  X   X  with  X   X  (0 . 5 , 1] and w  X  0 satisfies this requirement. A practical issue with the stochastic approximation proposed in Sec. 3 is that the variance of the gradient approximation may be very large. Given S samples of a random vector X , the covariance of its unbiased sample mean  X  X is known to be Cov(  X  X ) = Cov( X ) /S . When the diagonal values of Cov( X ) are large, many samples will be required to bring this variance below a desired level for approximating the expectation. As our experiments will show in Sec. 6, the value of S can be very large in practice and lead to a slow algorithm. We therefore seek a variance reduction method to re-duce the number of samples needed to construct the stochastic search direction.
 We introduce a control variate (Ross, 2006) to reduce the variance of the stochastic gradient constructed in Eq. (6). A control variate is a random variable that is highly correlated with an intractable variable, but for which the expectation is tractable. In this case the random variable is f (  X  ), for which we introduce a control variate g (  X  ). Control variates are ideal for MFVB because they can leverage the existing bounds, though they also admit a larger class of functions. We next review this variance reduction technique for E f , and discuss the modifications needed to account for the stochastic vector f (  X  )  X   X  ln q (  X  |  X  ). 4.1. A control variate for f (  X  ) Generally speaking, variance reduction works by mod-ifying a function of a random variable such that its ex-pectation remains the same, but its variance decreases. Toward this end, we introduce a control variate g (  X  ), which approximates f (  X  ) well in the highly probable regions as defined by q (  X  ), but also has a closed-form expectation under q . Using g and a scalar a  X  R , we first form the new function  X  f , This function has the same expectation as f and there-fore can replace it in L in Eq. (3).
 The next step is to set the value of a to minimize the variance of  X  f . A simple calculation shows that Taking the derivative with respect to a and setting to zero gives the optimal value, As is usual, this covariance and variance is unknown in the functions we encounter. We can approximate Algorithm 1 Variational Bayes with stochastic search Goal To calculate  X   X  L =  X   X  E q [ f (  X  )] +  X   X  h ( X,  X ). Approximate  X   X  L using stochastic search. input Variance reduction parameter . 1: Introduce the function g (  X  ) as a control variate 2: Sample an initial (small) collection  X  ( s )  X  q (  X  |  X  ). 3: Sum the sample variances and covariances 4: Set  X  a =  X / X  and S = (  X   X   X  2 / X  ) /K . 5: Sample  X  ( s )  X  q (  X  |  X  ) i.i.d. for s = 1 ,..., d S e . 6: Construct the stochastic search vector 7: Step in the direction of the stochastic gradient a with  X  a , found by plugging the sample variance and covariance into Eq. (10) using samples from the algo-rithm.
 The potential reduction in variance is seen by plugging Eq. (10) into Eq. (9) and taking the ratio of the two variances, Therefore, the greater the correlation between f and g , the greater the variance reduction. Tight lower bounds of f by construction have this high correlation, but we note that tight upper bounds work as well, as do well-approximating functions that do not bound f .
 Using the control variate g , we now write the stochastic approximation to the gradient as  X   X  E q [  X  f (  X  )]  X   X  a  X   X  E q [ g (  X  )] (12) where  X  ( s ) iid  X  q (  X  |  X  ) for s = 1 ,...,S . Writing the stochastic approximation this way allows for a more intuitive understanding of the algorithm. By separating the tractable and stochastic parts as done in Eq. (12), we first replace the intractable func-tion f with a tractable approximation g . (This resem-bles the standard method when g lower bounds f .) The gradient of E g is then corrected by a stochastic vector. The variance of the correction is smaller than that of the original stochastic approximation in Sec. 3, since the function f (  X  ) is close to  X  ag (  X  ). The gradient of E g can be thought of as an initial guess, followed by a stochastic correction which ensures that we are optimizing the variational objective function. 4.2. The stochastic search case We have introduced a control variate for f (  X  ), but in fact we would like to minimize the variance of the vec-tor f (  X  )  X   X  ln q (  X  |  X  ) in Eq. (6). In this case, the con-trol variate becomes g (  X  )  X   X  ln q (  X  |  X  ) and we have the following modification.
 Let  X  k be the k th dimension of  X  . Then for each dimension the discussion in Sec. 4.1 carries through, variance of each dimension again follows Eq. (9), and we seek an a to minimize the sum of these equations. This results in the optimal value which we approximate using samples. We summarize stochastic search variational Bayes in Algorithm 1. We next illustrate stochastic search variational infer-ence on logistic regression and a finite approximation to the hierarchical Dirichlet process (Teh et al., 2007). For logistic regression, we will consider two control variates, one of which is a lower bound and the other of which is not a bound. For the finite HDP, we will consider a piecewise control variate, one part being an upper bound on the original function. 5.1. Logistic regression Binary logistic regression takes in d -dimensional data vectors x n and predicts the class y n  X  X  X  1 , 1 } to which each belongs. The parameter is  X   X  R d and the predic-sigmoid function,  X  ( b ) = (1+ e  X  b )  X  1 . Bayesian logistic regression places a prior distribution on the coefficient vector,  X   X  Normal(0 ,cI ). For inference we define a Gaussian variational q distribution The variational lower bound for this model is The expectations of f n ( y n ,x n ;  X  ) := ln  X  ( y n intractable. One approach to avoiding this issue is to forgo variational inference and use Laplace X  X  method to approximate q . This method sets  X  to the MAP so-lution, and  X   X  1 to the negative Hessian of the log joint likelihood evaluated at  X  . Another is to lower bound f n with the bound in, e.g., Jaakkola &amp; Jordan (2000), which allows for closed-form variational inference. We consider this bound as a control variate.
 A lower bound control variate. The lower bound for f n developed by Jaakkola &amp; Jordan (2000) is a useful control variate for variational logistic regression. For each pair ( x n ,y n ), this bound takes an auxiliary parameter  X  n &gt; 0 and has the form g n ( y n ,x n ;  X , X  n ) = ln  X  (  X  n ) + We have  X  (  X  n ) = (2  X  (  X  n )  X  1) / (4  X  n ). We select this bound for illustrative purposes, but any lower bound will work in principle. For a multivariate Gaussian q distribution, having a quadratic term in g is essential for stochastically learning a full covariance matrix. In general, tighter bounds will require fewer samples, but for some functions finding tight bounds may require much effort. We next consider a general purpose con-trol variate that can help in this case.
 Control variate delta method. We also consider the second-order Taylor expansion of f as a control variate. The second-order Taylor expansion often ac-curately approximates a function of interest, and when used alone is known as the delta method. In addition to accuracy, the quadratic approximation of the delta method results in a function for which the expectation with respect to q is very likely to be analytic. The delta method arguably should not be used for mean-field variational inference because the second-order Taylor expansion is not a lower bound. On the other hand, the first-order Taylor expansion often is a lower bound. Therefore, though their bounds are typ-ically loose, first-order approximations are commonly employed for MFVB. An advantage of the proposed stochastic search algorithm is that second-order meth-ods can now be used as a control variate to ( i ) more accurately approximate the function of interest, and ( ii ) significantly reduce the variance of the stochastic gradient. We call this approach of using Taylor expan-sion control variates the control variate delta method . We consider a second-order Taylor expansion at  X   X  , the current mean of q , for approximating ln  X  ( y n x T n Letting  X  n :=  X  ( y n x T n  X   X  ), this control variate is g ( y n ,x n ;  X ,  X   X  ) = ln  X  n + y n (1  X   X  n )(  X   X   X   X  ) As with the Jaakkola &amp; Jordan (2000) bound, this control variate contains a quadratic term that helps in learning the covariance matrix of q .
 We compare these control variates in Figure 1. In these plots we show the difference f n  X  g n for two specific q distributions, and with x = 1. We also show 100 sam-ples from q , which indicates the regions where these functions would be evaluated (for a = 1). The plots show that the second-order Taylor expansion approxi-mates f n significantly better where it matters; we sup-port this conclusion with the experiments in Sec. 6. 5.2. Hierarchical Dirichlet processes We also investigate a stochastic search VB algorithm for an approximation to the hierarchical Dirichlet pro-cess (Teh et al., 2007). We focus on the two-level generative structure using finite dimensional Dirichlet priors as an approximation to the infinite dimensional process X  X n the limit the HDP is recovered. In this finite process, a top-level Dirichlet-distributed proba-bility vector  X  parameterizes the Dirichlet distribution for d = 1 ,...,D second-level probability vectors, In topic models, these  X  d vectors are often used as distributions on word distributions. In this section, we focus solely on the generic hierarchical structure in Eq. (17). We define the approximate posterior of  X  as The part of the lower bound associated with  X  is L  X  = P k  X  E q [  X  k ] P d E q [ln  X  dk ]  X  P k D E q [ln  X (  X  X  The expectation E q [ln  X (  X  X  k )] is intractable for each k . We use a stochastic approximation, and introduce two control variates for this function, depending on the current expected value of  X  X  k .
 As  X  X  k approaches zero, the function f k (  X  X  k ) =  X  ln  X (  X  X  k ) diverges to  X  X  X  . By construction of the Dirichlet prior, many values of  X  k will be very small. (In the infinite limit, there are an infinite number of such values smaller than any  X  &gt; 0.) The variance in this region is massive X  X hen computer precision be-comes an issue it can be infinite (see Figure 2). We propose the control variate g k (  X  X  k ) = ln  X  X  k , with E [ln  X  k ] =  X  ( c k )  X   X  ( P j c j ) where  X  (  X  ) is the digamma function. This control variate not only correlates well with f k , but if a = 1, lim  X  X  in Figure 2. This results from the equality For all other values of a , this equality does not hold, and the difference f k  X  ag k diverges as  X  X  k  X  0. For this model, we can thus give the optimal value of a in advance, and we set a = 1.
 From Figure 2, we also see that the approximation gets worse when  X  X  k gets large, which can occur for a few highly probable dimensions when  X  is large. Since f k is approximately linear in this regime, we use a first-order Taylor expansion of f k about the mean  X   X  k = E [  X  k ] as a control variate. This gives the following two control variates, g k = ln  X  X  k , 0 &lt;  X   X   X  k &lt;  X  1 , (21) g k =  X  ln  X (  X   X   X  k )  X   X  (  X  k  X   X   X  k )  X  (  X   X   X  k Since f k is concave, this second control variate is an upper bound on L  X  without the stochastic correction. We discuss the boundaries  X  1 and  X  2 in Sec. 6. Thus far, we X  X e focused mainly on reducing the vari-ance induced by f k , but in Sec. 4.2 we noted that  X  ln q introduces variance to the Monte Carlo integral as well. This suggests that we should look at other parts of the integral for potential variance reduction. We briefly show how this can be done for the HDP. The lower bound in Eq. (19) contains a sum of K in-tractable integrals over the probability simplex  X  K We perform separate stochastic approximations of each gradient. Using the fact that each gamma func-tion is over a single dimension of the simplex, for a function of  X  k the variables  X  i 6 = k will integrate out. In this case, marginalizing a Dirichlet distribution to a single dimension yields a beta distribution. That is,
Z where q 0 k (  X  k | c ) = Beta(  X  k | c k , P i 6 = k c i We can choose which of these integrals to stochas-tically approximate for gradient ascent. However, the stochastic gradient using q 0 k results in signifi-cantly less variance than for q since  X  ( s ) k will be near zero; the vector  X  c ln q 0 k has K  X  1 entries containing ln(1  X   X  ( s ) k )  X  E q [ln(1  X   X  k )], while these values will be ln  X  ( s ) i  X  E q [ln  X  i ] for i = 1 ,...,K when using  X  We perform experiments using stochastic search VB for binary classification with logistic regression and for topic modeling with the approximate HDP. We next give the details of the experiments we perform and the data sets and algorithms used for comparison.
 Data and set-up. For logistic regression, we use five data sets from the UCI repository: Iris, Pima, SPECTF, Voting and WDBC. These data sets range from 150 to 768 labeled examples living in 5 to 45 di-mensions, including a dimension of all ones to account for offset. We perform experiments with stochastic search variational inference using the two control vari-ates discussed in Sec. 5.1. We compare with two ad-ditional methods for posterior approximation: varia-tional inference with the Jaakkola &amp; Jordan (2000) bound and Laplace X  X  method. We evaluate perfor-mance on the true variational objective function in Eq. (14) using each posterior approximation.
 For the HDP topic model, we use 8,000 documents with 3,012 vocabulary size from The New York Times . We compare with ( i ) a point estimate of the top-level probability vector using a delta q distribution, and ( ii ) fixing the top-level distribution to the uniform vector, which is equivalent to LDA (Blei et al., 2003). We perform experiments for different corpus sizes, differ-ent values of  X  , and we set K = 200.
 model | data iris pima spectf vote wdbc Taylor CV -7.9 -3974 -165 -67.8 -74.6 J&amp;J CV -7.9 -3974 -165 -67.6 -74.8 Laplace -11.9 -3985 -170 -70.5 -80.0 J&amp;J bnd -11.5 -3976 -173 -74.6 -86.2 model | data iris pima spectf vote wdbc Taylor CV 0.33s 1.7m 20s 17s 11s J&amp;J CV 0.42s 18m 1.2m 1.2m 2.3m Laplace 21ms 29ms 94ms 20ms 0.10s J&amp;J bnd 64ms 88ms 0.13s 97ms 0.15s SS no CV 2.4s &gt; 2yr 6.6hr 9hr 1.4hr Logistic regression results. In Table 1 we show the variational lower bound for each model on each data set. Since all algorithms return an approxima-tion of the posterior distribution on the vector  X  , this comparison is meaningful and gives a measure of how close each posterior is to the true posterior. We see a considerable improvement for the stochastic algo-rithms (denoted by their control variate). Since both stochastic algorithms optimize the same objective, the performance should be the same.
 We show performance details of the stochastic search VB algorithm in Figure 3 and Table 2. In Figure 3, we show the number of samples, the variance reduction factor and the scaling  X  a as a function of iteration. We see that the control variates provide a major reduction in variance. Also, the Taylor expansion control vari-ate (i.e., control variate delta method) requires signif-icantly fewer samples than the bound control variate, which benefits the running time (see Table 2). While the non-sampling methods are faster, control variates make stochastic search VB a viable inference method when compared to the base algorithm of Sec. 3.
 Hierarchical Dirichlet process results. We fit topic models to The New York Times using different numbers of documents ( D = 1000 to 8000) and concen-tration parameter values  X   X  { 1 , 5 , 10 , 15 } . As switch points for the two control variates, we set  X  1 = 1 and  X  2 = 2. We summarize our results in Table 3. In general, fitting a variational posterior on the top-level Dirichlet vector yielded a better posterior approxima-tion than a point estimate and a  X  fixed as uniform. However, this improvement was not as dramatic as for logistic regression.
 In Figure 4, we show the number of samples required from the Dirichlet q distribution to approximate the stochastic integral. We compare the two methods dis-cussed in Sec. 5.2 for reducing the variance of the stochastic vector  X  c ln q by instead using  X  c ln q 0 k . We see a significant reduction in the number of samples. Experiments without control variates were not possi-ble due to computer precision issues and the massive variance of ln  X (  X  X  ) near zero. We have presented stochastic search variational Bayes, a method for optimizing intractable variational ob-jective functions such as those arising from non-conjugacy. The algorithm relies on a stochastic ap-proximation of the gradient; we showed how control variates can significantly reduce the variance of this Monte Carlo integral. Since existing lower bounds can be recast as control variates, our approach is relevant to many existing MFVB algorithms. However, a lack of restrictions on control variates allows for other types of function approximations when a good bound is not readily available. We introduced the control variate delta method toward this end.
 Acknowledgements J.P. and M.J. are supported by
