 Graph clustering is a fundamental problem that partitions vertices of a graph into clusters with an objective to opti-mize the intuitive notions of intra-cluster density and inter-cluster sparsity . In many real-world applications, however, the sheer sizes and inherent complexity of graphs may render existing graph clustering methods inefficient or incapable of yielding quality graph clusters. In this paper, we propose gSparsify , a graph sparsification method, to preferentially retain a small subset of edges from a graph which are more likely to be within clusters, while eliminating others with less or no structure correlation to clusters. The resultant simplified graph is succinct in size with core cluster struc-tures well preserved, thus enabling faster graph clustering without a compromise to clustering quality. We consider a quantitative approach to modeling the evidence that edges within densely knitted clusters are frequently involved in small-size graph motifs, which are adopted as prime features to differentiate edges with varied cluster significance. Path-based indexes and path-join algorithms are further designed to compute graph-motif based cluster significance of edges for graph sparsification. We perform experimental studies in real-world graphs, and results demonstrate that gSparsify can bring significant speedup to existing graph clustering methods with an improvement to graph clustering quality. I.5.3 [ Clustering ]: Algorithms; G.2.2 [ Graph Theory ]: Graph Algorithms Graph Sparsification; Graph Clustering; Graph Motif
Recent years have witnessed a growing trend in business intelligence and scientific exploration that models and in-terprets structured data as graphs [1 , 9]. As a ubiquitous c  X  abstraction depicting relationships among entities, graphs have become an increasingly common focus of data sciences, and have fused a wide range of applications in social net-works, biological networks, and the Web. Out of many graph analytical and mining tasks, graph clustering is a fundamen-tal building block with extensive applications in community detection [13 ], network visualization [ 31], ranking [ 38], and keyword search [ 11 ], to name a few. The objective of graph clustering is to partition vertices of a graph into clusters such that vertices within the same cluster are densely connected, while those in different clusters are sparsely interlinked. As a result, there have been an array of graph clustering meth-ods toward optimizing the intuitive notions of intra-cluster density and inter-cluster sparsity for graph clusters [ 14, 33 ].
However, the problem of graph clustering remains chal-lenging due in particular to the following reasons. First, graphs drawn from real-world applications become massive in scale. The sheer sizes of graphs may hinder a direct appli-cation of existing graph clustering methods that are mainly applicable to small or medium-size graphs. Second, and more interestingly, when graphs get larger and more com-plex, a great percentage of interactions (edges) have been witnessed accompanying redundant and spurious informa-tion [ 32]. The existence of extremely tangled, noisy edges can easily obfuscate intrinsic properties of graphs, thus lead-ing to low-quality graph clusters on the one hand, and in-ducing fruitless computation on the other.

To address the aforementioned issues, we design in this paper a new graph sparsification method, gSparsify . The goal of gSparsify is to simplify a graph G in a way that the cluster-significant edges ( w.r.t. the graph clustering objec-tives) are well retained, while edges with little or no clus-ter structure insight can be filtered without sacrificing the clustering quality significantly. This way, the salient struc-tures of G are approximately preserved, or even enhanced, in the sparsified graph G  X  , which is much smaller and more amenable to effective graph clustering. gSparsify can be em-ployed as a pre-processing step for existing graph clustering methods with significant speedup (2x to 37x in our experi-mental studies) and no compromise on the quality of graph clusters. gSparsify is also of practical interest as an indi-vidual tool in many graph analytical tasks, such as graph summarization, graph backbone detection, and large-scale graph visualization.

Example 1. Figure 1a illustrates the famous karate club social network with 34 vertices and 78 edges. We further add random edges to complicate the graph structure with 127 edges in total. The graph exhibits a  X  X air-ball X  structure Figure 1: Sparsifying the Karate Club Network with 34 Ver-tices and 127 Edges into a Simplified Graph with 48 edges Exhibiting 4 Clusters. which is hard to cluster. After applying the proposed graph sparsification method, gSparsify , we get a sparsified graph (Figure 1b) with 48 edges ( 62 . 2% edge reduction). It is im-mediately clear that there exist 4 clusters in the graph.
To accurately identify cluster-significant edges that need to be retained during graph sparsification, we propose a quantitative approach to associating each edge e = ( u, v ) with structure-aware weights denoting the degrees of rel-evance of two constituent vertices, u and v , to be in the same cluster. We start our reasoning with an important observation that vertices of an intra-cluster edge e are usu-ally densely connected within a cluster. As a result, e is frequently involved in many basic, localized graph primi-tives, termed as graph motifs . Graph motifs are small, con-nected subgraphs occurring in significantly higher frequen-cies than would be expected in random graphs [ 28], and thus play a key role in uncovering structural design princi-ples from real-world graphs. We examine a series of small-size graph motifs, and select short-length cycles as prime features to quantify cluster significance for edges. Specif-ically, a vector of cluster-significance scores is associated with each edge e  X  E , with each component representing the number (or ratio) of every pre-selected cycle motif e lies in. Cluster-significance scores can be further aggregated to syn-thesize the collective significance of e in participating in and forming all specified graph motifs. The higher the cluster-significance scores of e , the more occurrences of e witnessed in graph motifs, and thus more probably e is an intra-cluster edge that should be preserved in graph sparsification.
To compute the cluster-significance scores of an edge e = ( u, v ) in terms of short-length cycle motifs, we need enumer-ate all cycles encompassing e as a constituent edge, which turns out to be very time-consuming. We design a path-based indexing approach to maintaining short-length paths emanating from vertices. This way the cycle-motif enumer-ation problem boils down to a set of path-join operations on all indexed paths originated from u and v , respectively. We further use another level of inverted index to facilitate path-join evaluation, thus resulting in fast enumeration of cycle motifs in large graphs. Specifically, the contribution of our work is summarized as follows, 1. We propose a graph sparsification principle based on 2. We design a path-based indexing approach to enumer-3. We design the motif-based graph sparsification algo-
The remainder of this paper is organized as follows. We first elaborate on the related work in Section 2, then define preliminary concepts and notations in Section 3. Section 4 examines graph motifs, more specifically short length-cycles, in quantifying cluster significance of edges in graphs. In Sec-tion 5 we discuss a path-based indexing approach to enumer-ating short-length cycles for computing cluster significance of edges. Section 6 summarizes the graph sparsification al-gorithm, gSparsify . We discuss experimental studies in Sec-tion 7, and provide concluding remarks in Section 8.
In this section, we discuss related work for graph cluster-ing, graph sparsification, and graph motifs. We then brief an existing graph sparsification method, L-Spar [32 ], with similar goals as gSparsify .

Graph clustering , also referred to as community detec-tion in the social network community, has been extensively studied, and there exist rich literature on a wide range of graph clustering methods, including network flow based ap-proaches [ 29 ], spectral clustering approaches [ 25], modular-ity based approaches [ 34 ], hierarchical graph partitioning approaches [21 , 12], and local clustering approaches [ 10, 36, 16] (see surveys [ 14 , 33] for a comprehensive list of graph clustering algorithms). Most of existing graph clustering so-lutions aim to optimize intra-cluster density, or inter-cluster sparsity, or both, based on various definitions of  X  X ensity X  and  X  X parsity X  of graph clusters. Note that gSparsify is not restricted to any specific graph clustering method. Instead, it is to complement existing graph clustering methods.
Graph sparsi cation is to approximate a graph G = ( V, E ) by another sparse graph G  X  = ( V, E  X  ) in achieving desired graph metrics within reasonable error bounds. As |
E  X  |  X  | E | , the computation cost of related problems upon G , as opposed to G , is expected to be cheaper. Cut spar-sifiers [ 15] are simplified graphs in which the weight of ev-ery cut agrees up to a multiplicative factor of (1  X   X  ) with that of the corresponding cut in the original graph. Graph spanners [ 7] are sparsified graphs to approximate pairwise distances of vertices. Spectral sparsifiers [ 3, 35 ] are to ap-proximate eigenvalues to an arbitrarily small multiplicative error. SPINE [26] is to identify backbones of graphs for infor-mation propagation. However, existing graph sparsification methods are not primarily proposed to optimize graph clus-tering, which is the goal of our work.

Graph motifs , also known as graphlets , are small sub-graphs defined as interaction patterns occurring at numbers significantly higher than those in randomized graphs [ 28 ]. As elementary structures of complex networks, graph motifs carry out key functionalities and represent a broad range of natural phenomena. Triangles [ 4] and related clustering co-efficients [ 23 ] are used to detect the presence of spamming ac-tivities in large-scale Web graphs. The distribution of graph motifs in which a vertex is part of is used as an indicator for network classification [ 27]. Graph motifs are also used to analyze protein-protein interaction networks [ 8]. How-ever, to the best of our knowledge, there is no existing work that considers graph motifs for promoting the efficiency and effectiveness of graphs clustering.
 Enumerating graph motifs is nontrivial in large graphs [ 18 ]. Both exact and approximate solutions have been proposed under various paradigms including exact counting [ 41 ], sam-pling [ 28], frequent pattern mining [ 8], and color coding [ 17]. Specifically, triangle enumeration has been extensively stud-ied, including memory-resident algorithms [ 24] with the op-timal complexity of O ( | E | 3 = 2 ) in the worst case, and disk-resident algorithms [ 30 , 22 ] for massive graphs. Counting the number of short-length cycles can be accomplished in O ( | V | ! ) where  X  &lt; 2 . 376 is the exponent for matrix mul-tiplication [ 2]. Enumerating maximal cliques from a graph has proven to be optimal in O (3 | V | = 3 ) in the worst-case [ 39].
L-Spar [ 32] (short for L ocal Spar sifier) is most similar to our method, gSparsify , for graph sparsification. L-Spar takes advantage of a similarity-based heuristic that an edge e = ( u, v ) that is a part of many triangles is probably an intra-cluster edge [ 19]. L-Spar adopts Jaccard coefficient, | of a given vertex, to model this heuristic and uses minwise hashing [ 5] to approximate Jaccard coefficient, thus achiev-ing excellent scalability for graph sparsification. The key difference between gSparsify and L-Spar is two-fold. First, instead of using a single triangle motif, gSparsify considers a group of short-length cycle motifs including triangles (a special kind of cycle motif of length 3), to model cluster significance of edges. Evaluation based on multiple graph motifs can help identify intra-cluster edges that are not nec-essarily involved solely in triangles, and will significantly enhance the effectiveness of graph sparsification. From this perspective, L-Spar is just a special case of gSparsify . Sec-ond, the modeling of Jaccard coefficient and minwise hash-ing in L-Spar fails to generalize if non-triangle motifs are considered for graph sparsification. As a result, we propose a path-based indexing approach in gSparsify to enumerate all short-length cycles based on path-join operations. This path-join method can also be extended to enumerate other graph motifs in large graphs.
We consider in this paper the graph sparsification prob-lem on simple, undirected, and connected graphs, while the proposed algorithms can be easily extended to other types of graphs. Fi g ure 2: Graph Motifs with 3 and 4 Vertices. Different Non-
Given a graph G = ( V, E ), we consider some basic struc-tures that are of special interest for graph motifs and graph sparsification. A path p = ( v 1 , . . . , v l +1 ) is a sequence of vertices where ( v i , v i +1 )  X  E, 1  X  i  X  l . When the con-text is clear, we consider p a simple path in which all ver-tices are distinct and can be indexed and represented by p [ i ] , 1  X  i  X  l +1. A path containing l +1 vertices, or l edges, is of length l , denoted as an l -path for brevity. If v 1 the path p is closed and it turns out to be a cycle . A cycle containing l vertices (edges) is denoted as an l -cycle. The shortest cycles are 3-cycles, also known as triangles . Table 1 summarizes the key notations in this paper.
The goal of graph sparsification towards optimizing graph clustering is to preferentially retain intra-cluster edges of a graph, such that cluster structures can be well preserved in the resultant spersified graph. The critical problem is to accurately identify edges that are more likely to be intra-cluster edges. In this section, we discuss how graph motifs can be used as prime features to encode cluster structures of graphs and help identify intra-cluster edges from a graph.
In comparison to global features of graphs, such as de-gree distributions, diameters, and community structures [ 6], graph motifs are small, connected graphs captured in the lo-cal vicinity of vertices or edges, and are primarily used as el-ementary features representing key functionalities of graphs. Figure 2 illustrates graph motifs with 3 and 4 vertices, which can be broadly classified into two categories. The position-sensitive graph motifs contain vertices in different equiv-alence classes w.r.t. graph isomorphism. For example, in Figure 2, paths ((a) and (c)), tree (d), paw (f), and kite (g) are position-sensitive motifs, and the alphabet letters denote different equivalence classes of vertices w.r.t. graph isomorphism. In contrast, in position-insensitive graph mo-tifs, all vertices are isomorphic to each other. For example, in Figure 2, cycles ((b) and (e)) and clique (h) are position-insensitive motifs.

Based on the intuitive definition of graph clustering, we note that clusters of a graph are relatively dense subgraphs. Therefore, the chances of small-size graph motifs occurring in a graph cluster are high. Equivalently, an intra-cluster edge e  X  E is very likely to lie frequently in different graph motifs. As a result, we evaluate the cluster significance of e by examining the distribution of graph motifs that include e as a constituent edge. The higher the cluster-significance value of e is, the more probably e is involved in a closely knitted local structure, and hence e is more likely to be an intra-cluster edge in the graph. Figure 3: Cluster Significance Scores of Edges in the Karate Club Network.

However, not all graph motifs play an equal role in repre-senting local cluster structures of graphs. Path motifs (Fig-ure 2(a) and (c)) are often used to identify cuts in sparse regions of graphs [ 20]. Similarly, all the edges of tree mo-tifs (Figure 2(d)) and the singular edge ( A, C ) of the paw motif (Figure 2(f)) are likely to be cuts straddling different clusters. We therefore consider short-length cycle motifs as prime features for cluster significance evaluation in that: 1. Graph clusters are characterized by a high density of 2. Cycles are the simplest position-insensitive graph mo-3. For other complex position-insensitive graph motifs
Example 2. We consider the four clusters in the karate club network in Example 1 (Figure 1). Each edge in the orig-inal graph G can be classified as intra-cluster edges if two end-vertices are in the same cluster, or inter-cluster edges otherwise. We quantify the cluster significance of an edge e in two different ways: ( 1 ) Count: we compute the absolute number of 3 -cycles (triangles), 4 -cycles (quadrangles) and 5 -cycles (pentagons) in the graph containing e in these cycles; ( 2 ) Ratio: we compute the accumulative ratio of 3 -cycles, 4 -cycles, and 5 -cycles containing e relative to 3 -paths, 4 -paths, and 5 -paths, respectively, which go through e . For each ver-tex in G , its incident edges are sorted non-increasingly in terms of cluster-significance scores, and results are shown in Figure 3a and Figure 3b, respectively. Note that intra-cluster edges (blue circles) are consistently ranked higher than, and well separated from, inter-cluster edges (red di-amonds), because intra-cluster edges are more frequently in-volved in short-length cycles. Therefore, short-length cycle motifs lend themselves well as representative candidates to model cluster significance of edges in graphs.

Hen ceforth, we focus on short-length cycles as prime graph motifs to evaluate the cluster significance of edges. We first define the notion of  X  X luster significance X  upon which edges can be ranked w.r.t. short-length cycle motifs. Given a graph G = ( V, E ) and an edge e  X  E , if c l ( e ) denotes the number of l -cycles ( l  X  3), each of which includes e as a constituent edge, we define the cluster significance of e w.r.t. the l -cycle motif as Here the subscript C stands for absolute counting . Given a user-specified cycle length threshold l 0  X  3, we can further maintain a cluster-significance vector for the edge e as CSV C ( e ) is a vector of ( l 0  X  2) cluster-significance scores of e w.r.t. a set of cycles of lengths ranging from 3 up to l 0 further define the cluster significance of e w.r.t. counting as Definition 1 ( CS w.r.t. COUNTING ). Given e  X  E and a cycle length threshold l 0 , the cluster significance of e w.r.t. counting is an aggregation of CSV C ( e ) : where F (  X  ) is an aggregate function like SUM or AVG .
Alternatively, if p l ( e ) denotes the number of l -paths, each of which passes through the edge e , and we allow closed paths , i.e. , vertices within paths can occur repeatedly, we define the cluster significance of e w.r.t. l -cycle motif as Here the subscript R stands for ratio . Analogously, we can define the cluster-significance vector CSV R ( e ) as The cluster-significance w.r.t. ratio can be defined as
Definition 2 ( CS w.r.t. RATIO ). Given e  X  E and a cycle length threshold l 0 , the cluster significance of e w.r.t. ratio is an aggregation of CSV R ( e ) :
Example 3. We consider a sample graph shown in Fig-ure 4, the cycle motif length threshold l 0 = 5 , and the ag-gregate function is SUM(  X  ) . For the edge (1 , 2) , the cluster-significance vector w.r.t. counting is CSV C (1 , 2) = (2 , 4 , 4) , meaning that the edge (1 , 2) is contained in two 3 -cycles, four 4 -cycles, and four 5 -cycles. So the cluster significance of (1 , 2) w.r.t. counting, CS C (1 , 2) , is 10 . Similarly, the cluster-significance vector w.r.t. ratio is CSV R (1 , 2) = (0 . 167 , 0 . 1 , 0 . 036) , and the cluster significance of (1 , 2) w.r.t. ra-tio, CS R (1 , 2) , is 0 . 303 . If we consider the edge (5 , 6) , both CS C (5 , 6) and CS R (5 , 6) are 0 . It means that this edge is not contained in any cycle motifs, and most probably it is an inter-cluster edge.
In this section, we discuss the computation of cluster-significance scores for edges in a graph. For either the count-based model, CS C ( e ), or the ratio-based model, CS R ( e ), we need enumerate all short-length cycles that contain e as a constituent edge. However, even enumerating the sim-plest triangle motifs turns out to be time-consuming in large graphs. We thus design an efficient path-based indexing ap-proach to facilitating the computation.

Consider an edge e = ( u, v )  X  E , any cycle including e as a constituent edge can be decomposed into three sub-parts: a path p 1 = ( u, . . . , w ) originating from u , another path p = ( v, . . . , w ) originating from v , and the edge e = ( u, v ), where p 1 and p 2 are two vertex-disjoint paths except that they share one common end-vertex w  X  V . Specifically, we choose w as the median point to u and v in the cycle, such that contains the edge e = ( u, v ) can be identified as follows. We enumerate every path p 1 of length  X  l  X  1 2  X  ema nating from u , and every path p 2 of length  X  l  X  1 2  X  eman ating from v , respectively. if p 1 and p 2 share no common vertices except the end-vertex other than u and v , a cycle is identified from the graph. We formulate this idea based on the notion of path join , as follows,
Definition 3 ( PATH JOIN ). For an edge e = ( u, v ) , l -cycles containing e can be identified by joining two paths p 1 and p 2 , p 1  X  X  X  p 2 , where the join condition  X  is 2. p 1 [1] = u , p 2 [1] = v ( p 1 and p 2 originate from u and
Theorem 1. All cycle motifs of length l containing the edge e = ( u, v )  X  E as an constituent edge can be enumerated based on path join, as defined in Definition 3.

Example 4. Figure 5 illustrates two examples for enu-merating cycle motifs that contain the edge ( u, v ) (in thick black color), based on path join. In Figure 5a, we enumerate all 4 -cycles. The 2 -paths emanating from u (color in red), and 1 -paths enumerating from v (color in blue) are joined together, thus resulting in three 4 -cycles. In Figure 5b, we enumerate all 5 -cycles. The 2 -paths emanating from u and Figure 5: Enumerating Cycle Motifs for the Edge ( u, v ) Al gorithm 1: Computing CS l C ( e ) Input : An edge e = ( u, v )  X  E , the motif length l
Output : The cluster-significance score CS l C ( e ) 1 begin 2 fo r w  X  V do 3 H 1 ( w )  X  X  X  ; H 2 ( w )  X  X  X  ; 8 CS l C ( e )  X  0; 9 for w  X  V do 10 if H 1 ( w )  X  =  X  and H 2 ( w )  X  =  X  then 11 fo r i  X  X  1 ( w ) do 12 for j  X  H 2 ( w ) do 13 if  X  : ( p i  X  X  X  p j ) then 14 C S l C ( e )  X  CS l C ( e ) + 1; 15 return CS l C ( e ); v , r espectively, are considered for path join, thus resulting in seven 5 -cycles. Note that the paths ( u, d, w ) and ( v, d, w ) fail to join as both share a common intermediate vertex d , so the path-join condition does not hold.

To this end, we design a path-based indexing approach to computing cluster significance of edges in G . Given the motif length threshold, l 0  X  3, we maintain, for each vertex v  X  V , all distinct l -paths emanating from v into a vector, P adjacency list of v . If the maximum degree of vertices in G is denoted d , both the time and space complexity for path-world graphs often follow power-law degree distributions, so in practice a majority of vertices have degrees that are sig-nificantly smaller than d . Another important factor is that, the length threshold of cycle motifs, l 0 , is often set small, as longer cycles in cluster-significance evaluation will bring marginal improvement for graph sparsification. Therefore, this path-based index can be pre-built offline effectively in the index construction phase.

Based on Theorem 1, in order to enumerate all cycle motifs of length l ( l  X  l 0 ) containing an edge e = ( u, v ), we employ all pre-indexed paths p i  X  P  X  l 1 P  X  l 1 2  X  ( v ) to perform a path join, p i  X  X  X  p j , as detailed in Algorithm 1. To facilitate the evaluation of path join, we maintain an inverted index H 1 (  X  ) : V 7 X  X  X  2 N , such that for each vertex w  X  V , if w is the last end-vertex (other Alg orithm 2: gSparsify
Input : Graph G = ( V, E ), the motif length threshold
Output : Sparsified graph G  X  1 begin 2 G  X   X  X  X  ; 3 for e = ( u, v )  X  E do 4 CS C ( e )  X  0; CS P ( e )  X  0; 5 foreach l : 3  X  l  X  l 0 do 6 com pute CS l C ( e ) (Algorithm 1); 7 compute CS l R ( e ) (Equation 7); 8 CS C ( e )  X  F ( CS 3 C ( e ) , . . . , CS l 0 C ( e )); 9 fore ach u  X  V do 10 So rt all incident edges e = ( u, v )  X  E by CS C ( e ) 11 Add top d u edges to G  X  ; 12 retu rn G  X  ; tha n u ) of a path p i  X  X   X  l 1 path index i of p i are stored into H 1 ( w ). This way, all the grouped together. Analogously, we build another inverted index H 2 (  X  ), in which all paths of P  X  l 1 end-vertices (other than v ) are grouped together. Both in-verted indexes are first initialized (Lines 2-3), and updated in terms of the last end-vertex of each path in P  X  l 1 P  X  l 1 2  X  ( v ), resp ectively (Lines 4-7). To compute the cluster significance w.r.t. counting, CS l C ( e ), we consider the paths p  X  X  common end-vertex (Line 10). If p i and p j satisfy the path-join condition  X  , as defined in Definition 3, we successfully identify an l -cycle from the graph (Lines 13-14).
Assume paths originating from u may end at any vertex of G , then |H 1 (  X  ) | = |P  X  l 1 |P out to be O ( |P  X  l 1 the term (( l  X  1) / 2) 2 ) denotes the time to evaluate the path-join condition  X  for p i and p j . Note that |P  X  l 1 n umber of (  X  l  X  1 2  X  )-pa ths originating from u , bounded up by d  X  l 1 2  X  , so the worst-case complexity of Algorithm 1 is
Once the cluster significance w.r.t. counting, CS l C ( e ), has been computed, it is straightforward to derive the cluster sig-nificance w.r.t. ratio, CS l R ( e ), which is to normalize CS by considering all possible paths passing through e : No te that all (  X  l  X  1 2  X  )-pa ths originating from u are main-tained in the index P  X  l 1 pa ths originating from v are maintained in P  X  l 1 CS l C ( e ) is known, CS l P ( e ) can be computed in O (1).
In this section, we detail our graph motif-based sparsifica-tion method, gSparsify , which uses short-length cycle motifs to model the cluster significance of edges. The edges with highly ranked cluster-significance scores are preferably re-tained. This way, the core cluster structures can be well preserved in the sparsified graph.

Algorithm 2 describes the whole process for graph spar-sification. The sparsified graph, G  X  , is first initialized as an empty graph (Line 2). For each edge e  X  G , we compute its cluster-significance scores, CS C ( e ) and CS R ( e ), based on Algorithm 1 (Lines 3-8). We further choose an aggregate function, F , to synthesize cluster-significance scores of edges based on a series of cycle motifs. After that, a localized, vertex-centric sparsification approach is employed to retain edges with high cluster significance in the neighborhood of vertices. Specifically, for each vertex u of the graph G , we select the top d u (0  X   X  &lt; 1) incident edges, ranked on either CS C ( e ) or CS R ( e ), where d u is the degree of u , into G  X  for sparsification (Lines 9-11). In principle, we sparsify high-degree vertices more aggressively than low-degree ones, because high-degree vertices are more likely to be hubs that tend to connect multiple clusters. As a result, their incident edges are likely to be inter-cluster edges. In implementation, we choose a strictly concave function, d u , for graph sparsifi-cation. When  X   X  0, the sparsified graph G  X  contains very few edges but each vertex u still preserves at least one edge, making the resultant cluster containing u connected. When  X   X  1, the sparsified graph G  X  is almost identical to G as most edges are retained.

The complexity of Algorithm 2 is formulated with two components. The first is to compute cluster significance of edges in the graph, which is O ( d l 0 | E | / | V | ). The sec-ond is to sort, for each vertex, its incident edges based on cluster-significance scores, which is  X  u O ( d u log d ) = O (log d of Algorithm 2 is O ( d l 0 | E | / | V | + log d  X | E | ).
Example 5. We apply the graph sparsification algorithm, gSparsify , on the sample graph G shown in Figure 4 by set-ting l 0 = 5 ,  X  = 0 . 1 , and F = SUM . The sparsified graph G is illustrated in Figure 6. G  X  contains 11 edges, so 5 edges of G are filtered out during graph sparsification. It can be witnessed that there are two clusters (colored in red and blue, respectively) in the sparsified graph G  X  .
I n this section, we present our experimental studies for gSparsify . We make use of gSparsify as a pre-processing step to sparsify a series of real-world graphs, and examine both the benefit and cost of gSparsify toward improving the ef-fectiveness and efficiency of state-of-the-art graph clustering methods. We implement gSparsify in C and compile it with GCC 4 . 4 . 7. All experiments were carried out in a Linux ma-chine running RedHat Enterprise Server 6 . 5, with 12 AMD Opteron 2 . 3GHz CPUs and 96GB of memory.
W e consider three real-world graphs in our experimental studies, including a protein-protein interaction network, a co-authorship network, and a social network: 1. Yeast PPI Network: This is a protein-protein in-2. DBLP 1 : This is a co-authorship graph where two au-3. Orkut 2 : This is an online social network where ver-
We consider three well-known graph clustering methods in our experimental studies: 1. METIS[ 21] is a high-quality graph partitioning toolkit 2. Graclus[ 12] is a fast graph clustering tool that com-3. MCL[ 40] is a scalable graph clustering method based
It is worth noting that gSparsify is not restricted to the above-mentioned graph clustering algorithms. Any graph clustering technique toward optimizing the notions of intra-cluster density and inter-cluster sparsity of graph clusters can benefit from the proposed method, gSparsify . Tab le 2: Index Construction Cost of gSparsify (Time in Sec-onds, and Space in Megabytes).
We evaluate the graph sparsification method from multi-ple perspectives. The foremost evaluation is to examine to what extent gSparsify can help improve the graph clustering effectiveness. We apply graph clustering algorithms on the original graph G , and the sparsified graph G  X  , respectively, and assess the clustering quality w.r.t. ground truth or graph clustering metrics.

If we have ground truth for clusters, C = { c 1 , c 2 , . . . , c ( e.g. , in DBLP and Orkut), we evaluate clustering quality based on the average F-score of graph clusters. The F-score of a predicted graph cluster  X  c w.r.t. a ground-truth cluster c  X  C , denoted F (  X  c ) | c sion and recall. Furthermore, the F-score of  X  c is It is the F-score of the predicted cluster  X  c w.r.t. the ground truth cluster c  X   X  C to which  X  c approximates best. The average F-score of all the predicted clusters is the weighted average of all F-scores, each of which is weighted by the cluster size. Empirically, the higher the average F-score, the better the clustering quality.

We also consider the average graph conductance to evalu-ate the quality of graph clusters in all graph datasets. Given a cluster c  X  X  of G , the graph conductance of c is where I ij is an indicator function that equals 1 if there exists an edge ( i, j ) between vertices i and j , and 0 otherwise, and Graph conductance lies between 0 and 1 with lower val-ues indicating better clustering quality. The average graph conductance is the average of  X  ( c ) for all clusters c  X  C Note that the evaluation of clustering quality based on graph conductance should be performed on the original graph G . That is, for clusters derived from the sparsified graph G need map all such clusters back into G and compute graph conductance in G . This way, it will reflect how well graph sparsification can retain the cluster structures of G .
Another evaluation metric is the speedup gSparsify offers for graph clustering. We apply different graph clustering methods on the original graph G and the sparsified graph G , respectively, and examine the runtime performance gain gSparsify provides. As G  X  is much smaller than G , clustering on G  X  is expected to be significantly faster.

There exist several critical parameters for gSparsify , in-cluding the cycle length threshold l 0 , the local sparsification exponent  X  , and the aggregate function F . We also exam-ine how these parameters regulate the performance of graph sparsification. If not specified otherwise, we choose the clus-ter significance w.r.t. ratio, CS R , as the default model for graph sparsification, and set the key parameters with the following default values: l 0 = 5,  X  = 0 . 5 and F = AVG ( Meanwhile, as an indexing approach, gSparsify needs to build path-based indexes and inverted indexes, we also evaluate the time and space cost of index construction in the experi-mental studies.

We compare gSparsify with L-Spar [32] that only uses the triangle motif for graph sparsification. The results will demon-strate that gSparsify is a generalized method where a series of graph motifs can be incorporated together to improve the clustering quality of real-world graphs.
We first report the index construction cost of gSparsify in different graph datasets, shown in Table 2. We choose dif-ferent lengths of cycle motifs ranging from 3 to 5. When l = 3, only the one-level neighborhood index P 1 , i.e. , the adjacency list, is built. When l 0 = 4, we need to build the path index P 2 and use the inverted index H 1 , and when l = 5, both the path index P 2 and inverted indexes of all vertices are required. When the graph is small, for example, in the Yeast dataset, the index can be constructed efficiently with very small memory consumption. When the graph be-comes excessively large, for example, in the Orkut dataset, the index construction needs more time and memory. How-ever, this path-based index is constructed offline only once. Such cost is much affordable compared with the performance gain obtained for graph sparsification.
We consider both gSparsify and L-Spar to sparsify the three graph datasets, and examine the clustering quality and speedup by applying three graph clustering algorithms on the original graph G and the sparsified graph G  X  , respec-tively. As to the clustering quality, we evaluate both average F-scores, if graphs have ground truth for clusters, and the average graph conductance. For speedup, we consider the runtime cost of graph clustering on G  X  against G . The graph sparsification ratio, | E  X  | / | E | , is controlled by tuning the lo-cal sparsification exponent,  X  .
 The experimental results of using METIS are shown in Table 3. For all the three datasets, gSparsify provides bet-ter clustering quality results than the plain graph cluster-ing method applied on the original graph, in terms of both F-scores and the graph conductance. gSparsify also outper-forms L-Spar by offering better clustering quality results. For example, in the largest Orkut dataset, the F-score is enhanced from 10 . 97 to 15 . 45, and the graph conductance is further reduced from 0 . 79 to 0 . 72, both indicating that gSparsify preserves more intra-cluster edges during graph sparsification. The main reason is that gSparsify leverages short-length cycle motifs for cluster significance quantifica-tion, such that intra-cluster edges can be effectively iden-tified from within large graphs. However, L-Spar only uses the 3-cycle motif, thus leading to less effective results. An-other benefit of graph sparsification is the speedup it of-fers for graph clustering. We apply METIS on the original graph G , and the sparsified graph G  X  , which is about 1 / 4 to 1 / 5 the size of G . The speedups for graph clustering are 21x, 18x, and 37x, respectively, for the Yeast, DBLP and Orkut datasets. This indicates that, gSparsify can sig-nificantly speedup graph clustering on large graphs while preserving, or even enhancing, the clustering quality.
We perform the same experimental studies using Graclus and MCL clustering algorithms, and the results are pre-sented in Table 4, and Table 5, respectively. Note that Gra-clus cannot finish successfully in the largest Orkut dataset, so the results are not included. Our graph sparsification method, gSparsify , can improve clustering quality in differ-ent graph datasets. Specifically, gSparsify results in better clustering quality than L-Spar , in terms of both F-scores and the graph conductance. Meanwhile, gSparsify brings signifi-cant speedup for these two clustering methods. This is im-portant because clustering in large graphs is very resource-intensive and time-consuming, while graph sparsification is an effective means to shorten the gap of the application of many graph clustering solutions that are designed mainly for small or medium-size graphs.
The graph sparsification algorithm, gSparsify , is critical to a series of algorithmic parameters. We then perform exper-imental studies to examine how these key parameters affect the overall graph sparsification performance.
We first examine the motif length threshold, l 0 . By tun-ing the values of l 0 , we adopt different short-length cycle motifs for graph sparsification. For example, if l 0 = 3, we only use 3-cycles in graph sparsification, while if l 0 = 4, both 3-cycles and 4-cycles will be employed. As shown in Figure 7a and Figure 7b, we choose l 0 ranging from 3 to 5 and evaluate the clustering quality w.r.t. F-scores and the graph conductance, respectively, using the METIS method. When more short-length cycle motifs are considered, intra-cluster edges are more likely to be identified during graph sparsification, and thus improve the graph clustering qual-ity. We perform the same experiments and witness similar evidences using the MCL method, and the results are illus-trated in Figure 7c and Figure 7d, corresponding to F-scores and the graph conductance, respectively.

However, when l 0 is set high, we have to take more time for graph sparsification. Figure 8 illustrates the sparsifica-tion time for three different datasets, in terms of l 0 . Here a common trade-off needs to be made between sparsification time and effectiveness. Considering graph sparsification can be performed offline, we still can afford more time for graph sparsification to trade better graph clustering quality, which is the goal of our work.
We then examine the local sparsification exponent,  X  , which controls the number of top ranked edges to be re-tained during graph sparsification. By tuning 0  X   X  &lt; 1, we can derive a series of sparsified graphs with varied sizes. As shown in Figure 9a, we generate five different sparsified graphs for the graph datasets by choosing different  X  rang-ing from 0 . 3 to 0 . 7, and evaluate clustering quality using METIS. We note that, in terms of F-scores, if we retain more edges during graph sparsification (  X  = 0 . 7), the clustering quality can be slightly improved. If we sparsify the graph avidly be setting  X  = 0 . 3, more edges will be filtered thus resulting in a slightly worse clustering result. However, the F-score for  X  = 0 . 3 is still better than that generated from the original graph. It indicates that although we filter out most edges from G and retain about 10% of the edges into the sparsified graph G  X  , the core cluster structures are still largely preserved with most of the noisy edges eliminated.
Similarly, we evaluate the clustering quality w.r.t. the graph conductance, and results are shown in Figure 9b . When  X  is set low (  X  = 0 . 3), we filter out many edges that may hurt the clustering effectiveness. However, when  X  is set high (  X  = 0 . 7), many noisy edges are retained in G which also bring side effects for graph clustering and lead to inferior results. We conduct the same experiments using MCL, and results are shown in Figure 9c and Figure 9d , re-spectively. The patterns between the clustering quality and  X  are very similar to the cases where METIS is adopted.
If multiple short-length cycle motifs are adopted to quan-tify the cluster significance of edges, we use an aggregate function F to synthesize cluster-significance scores. In this experiment, we choose two aggregate functions, SUM and AVG , and examine the effect of such aggregate functions for graph sparsification. Note that with different aggregate functions, the top ranked edges that are to be retained dur-ing graph sparsification will be different, and thus the final sparsified graph G  X  may vary. We test the clustering quality results, in terms of both F-scores and the graph conduc-tance, on the DBLP dataset, as shown in Figure 10. For different graph clustering methods, AVG offers better clus-tering quality results than SUM in most cases. Note that it is inappropriate to choose MIN or MAX as the aggregate function in our framework, as they only choose the cluster-significance score based on one type of cycle motifs, which usually offers inferior sparsification results.
When modeling cluster significance of edges in a graph, we consider two approaches: CS C and CS R . The first one counts the absolute number of motifs that contain the edge to be examined, while the second one computes the rela-tive ratio w.r.t. the total number of possible paths passing through the edge. We perform another experiment to eval-uate these two models, each of which is chosen as the un-derlying method for the computation of cluster-significance scores of edges. We apply gSparsify with CS C , termed as COUNT, and gSparsify with CS R , termed as RATIO, in the DBLP dataset and examine the clustering quality of differ-ent graph clustering methods. The results are shown in Fig-ure 11 . We notice that RATIO provides consistently better quality results than COUNT, in terms of both F-scores and the graph conductance. This is especially true when dealing with edges incident to high-degree vertices.
In this paper, we designed a new graph sparsification method, gSparsify , toward enabling efficient and cost-effective graph clustering on real-world graphs. The goal of gSparsify is to identify and preferentially retain cluster-significance edges of a graph G into a sparsified graph G  X  , such that edges with little or no cluster structure insight can be ef-fectively pruned before costly graph clustering is performed. The main idea of gSparsify is to make use of a group of short-length cycle motifs to model cluster significance of edges. To facilitate the computation of cluster significance, we devised a path-based indexing approach such that the costly graph-motif enumeration can be cast into a systemic path-join pro-cess. Our experimental studies demonstrated that, gSparsify generalizes and outperforms the state-of-the-art graph spar-Figure 11: Graph Clustering Quality w.r.t. Cluster-significance Modeling sification method, L-Spar . More importantly, gSparsify en-ables more efficient graph clustering without a compromise of clustering quality, and therefore can be effectively em-ployed to cluster real-world large graphs.
