 1. Introduction number of categories, because training of a single, binary SVM requires O  X  n a  X  time for 1 : 7 and decision trees, are not so accurate as SVMs ( Yang &amp; Liu, 1999; Yang &amp; Pederson, 1997 ). tivefeaturetransformationaseigenvectorsassociatedwiththelargesteigenvaluesofmatrix T  X  S covariancematrixand S b istheinter-classcovariancematrix. the null space of S w would eliminate important discriminant information when the projections of S Zhu, &amp; Ogihara, 2003 ).
 shows experimental results. Finally, Section 8 provides conclusions and discussions. 2. Related work Based on this principle, it estimates the conditional distribution of the class label given a document. ering correlations between classes.
 input and then finding the class with the codeword closest to the output codeword. class classification and does not require reduction to binary classification problems.
Insummary,aspointedoutin Yangand Liu(1999);ScholkopfandSmola(2002) ,itisfairtosaythatthereisprobablynomulti-3. Classical linear discriminant analysis The notations used through the discussion of this paper are listed in Table 1 . sponds to a particular term, we consider finding a linear transformation G 2 R  X  1
R optimal transformation matrix G such that the class structure is preserved. More details are given below.
Assume there are k classes in the data set. Suppose m i , S of the i th class, respectively, and m is the total mean. For the covariance matrix S
S Define the matrices
Then the between-class scatter matrix S b , the within-class scatter matrix S follows ( Fukunaga, 1990 ): ces become An optimal transformation G would maximize Trace  X  S L b  X  and minimize Trace  X  S optimal G is
The solution can be readily obtained by solving a eigenvalue decomposition problem on S most k 1 discriminant vectors. 4. Generalized discriminant analysis is very high. Usually, this problem is overcome by using a non-singular intermediate space of S null space of S w and then computing eigenvectors. However, the removal of the null space of S of S w is guaranteed to contain useful discriminant information when the projections of S with it is to use generalized eigenvalue decomposition ( Howland &amp; Park, 2003; Li et al., 2003 ). 4.1. The basics of GSVD GSVD, shown below, was first introduced in Loan (1976) .

Theorem 1 (GSVD diagonal form, Loan, 1976 ). If A 2 R m p matrices, U 2 R m m and V 2 R n n , and a non-singular matrix, X 2 R where C and S are non-negative diagonal and of dimension m k and n k, respectively, 1 P S C C  X  S T S  X  I k .
 sions on computing GSVD, please refer to ( Bai et al., 1992 ). 4.2. Generalized discriminant analysis matrices U 2 R k k , V 2 R n n , and a non-singular matrix X 2 R where satisfying
From Eq. (2) , we have
Note that entries on the main diagonal of R 1 are non-increasing while entries on the main diagonal of R terion as in Section 5 . 4.3. The GDA algorithm metrics, such as Euclidean distance and cosine measure
A new instance, z , it is classified to where x k is the centroid of k th class.
 The pseudo codes of the training and prediction procedures are described as follows: Algorithm 1 ( Training procedure G = Train ( x  X  X )).
 Algorithm 2 ( Prediction procedure T = Predict ( G , x )).
 Input: the transformation G generated by the training procedure; and a new instance x ;
Output: the label T of the new instance; begin end 5. Connections mization criterion. 5.1. Optimizing the determinant ratio the determinant of the intra-class scatter matrix of the projected samples: maximize j G T S b G j minimize j G T S w G j . Using GSVD, H To maximize J  X  G  X  , j G T S b G j should be increased while decreasing j G
This implies the matrix G satisfying would simultaneously maximize j G T S b G j and minimize j G In the case where we must weight the transformation with the generalized singular, is the transformation we want. 5.2. A new criterion 5.2.1. The criterion inter-class and intra-class distances by inter-class and intra-class covariance matrices: where k X k F is the Frobenius norm of the matrix X , i.e., term is used to minimize the intra-class covariance.

The Eq. (7) can be rewritten as and this is a least squares problem with the solution 5.2.2. Stable solution solution thus obtained is stable.
 By plugging the GSVD matrices of H w and H b , e.g., in (9) , we have Since V is orthogonal, we can drop it without changing the squared distance. So, we have number of columns and B is of full column rank. Let A  X  A
F  X  F 1 D 2 be perturbations of A and B, respectively, such that for all x there exist some g
The above theorem gives a bound on the relative error of the generalized eigenvalues ( C is bounded by the relative error of estimated intra-and inter-class covariance matrices. ucts H b H T b and H w H T w , which causes roundoff errors, is not required. 6. Text classification via GDA: examples tributed systems.
 After removing words (terms) that occurs only once, we have the document-term matrix as shown in Fig. 2 . using each of the three methods. LSI.
 7. Experiments 7.1. The datasets of the datasets. 7.1.1. 20Newsgroups skipped, and all header fields except subject and organization of the posted article were ignored. 7.1.2. WebKB sets, we used stemming or stop lists. 7.1.3. Industry sector
MIME and HTML headers and using a standard stop list. We did not perform stemming. 7.1.4. Reuters which we call Reuters-2. Reuters-2 had approximately 9000 documents and 50 categories. 7.1.5. TDT2 documents. 7.1.6. K-dataset stripping algorithm. 7.1.7. CSTR tions on the remaining words. 7.2. Data preprocessing The feature selection is done with the Rainbow package ( McCallum, 1996 ).
Here we use classification accuracy for evaluation. Different measures, such as precision-recall graphs and F were carried out on a P4 2GHz machine with 512M memory running Linux 2.4.9 X 31. 7.3. Experimental results involving SVM we used SVMTorch ( Collobert &amp; Bengio, 2001 ), imental comparisons are performed on the same datasets with exactly same testing and training settings. CSTR, Reuters-Top10, and K-dataset).
 most counts. We can say that GDA is a viable, competitive algorithm in text categorization.
H w and H b which are smaller in size than the original scatter matrices S 1997 ).

In summary, these experiments have shown that GDA provides an alternate choice for fast and efficient text categorization.
 8. Discussions and conclusions avan, &amp; Vempala, 1998 ).
 Acknowledgements This work is supported in part by NSF Grants EIA-0080124, DUE-9980943, and EIA-0205061, and NIH Grant P30-AG18254.
 References
