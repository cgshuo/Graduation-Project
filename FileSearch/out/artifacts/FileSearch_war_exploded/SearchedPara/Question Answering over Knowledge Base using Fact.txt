 Open-domain question answering (Open QA) is a longstanding problem that has been studied for decades. Early systems took an information retrieval approach, where question answering is reduced to returning passages of text containing an answer as a substring. Recent advances in constructing large-scale knowledge bases (KBs) have enabled new sys-tems that return an exact answer from a KB.

A key challenge in Open QA is to be robust to the high variability found in natural language and the many ways of expressing knowledge in large-scale KBs. Another challenge is to link the natural language of questions with structured semantics of KBs. In this paper, we present a novel architecture based on memory networks (Bordes et al., 2015) that can be trained end-to-end using (question, answer) pairs as training set, instead of strong supervision in the form of (question, associated facts in KB) pairs.
The major contributions of this paper are two-fold: first, we introduce factual memory networks, which are used to answer questions in natural lan-guage (e.g  X  X here was Bill Gates born? X  ) using facts stored in the form of (subject, predicate, object) triplets in a KB (e.g ( X  X ill Gates X ,  X  X lace of birth X ,  X  X eattle X ) ). We evaluate our system against current baselines on various benchmark datasets. Since KBs can be extremely large, making it computationally inefficient to search over all entities and paths, our second goal of this paper is to increase the efficiency of our model in terms of various performance mea-sures and provide better coverage of relevant facts, by intelligently selecting which nodes to expand. The state-of-the-art methods for QA over a knowl-edge base can be classified into three classes: se-mantic parsing, information retrieval and embedding based.

Semantic parsing (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Fader et al., 2014) based approaches aim to learn semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. Even though these approaches are difficult to train at scale be-cause of the complexity of their inference, they tend to provide a deep interpretation of the question.
Information retrieval based systems retrieve a set of candidate answers and then conduct further analy-sis to rank them. Their main difference lies in select-ing correct answers from the candidate set. Yao and Van Durme (2014) used rules to extract question fea-tures from dependency parse of questions, and used relations and properties in the retrieved topic graph as knowledge base features.

Embedding based approaches (Bordes et al., 2014b; Bordes et al., 2014a) learn low-dimensional vectors for words and knowledge base constitutes, and use the sum of these vectors to represent ques-tions and candidate answers. However, simple vec-tor addition ignores word order information and higher order n-grams. For example, the question representations of  X  X ho killed A? X  and  X  X ho A killed? X  are same in the vector addition model. (Bordes et al., 2015) used strong supervision sig-nal in form of supporting facts for a question during training to improve their performance. 3.1 Processing FREEBASE Freebase : Freebase (Bollacker et al., 2008) is a huge and freely available database of factual infor-mation, organized as triplets (subject Entity, Rela-tionship, object Entity). All Freebase entities and relationships are typed and the lexicon for types and relationships is closed. Each entity has an internal id and a set of alternative names (called aliases, e.g. JFK for John Kennedy) that can refer to that entity in text.

The overall structure of Freebase is a hypergraph, in which more than two entities can be linked to-gether in a n-ary fact. The underlying triple stor-age involves dummy entities to represent such facts, effectively making actual entities involved linked through paths of length 2, instead of 1. For exam-ple, a statement like  X  X  starred as character B in movie C X  is represented in Freebase as (A,  X  X tar-ring X , dummy entity), (dummy entity,  X  X ovie X , C), (dummy entity,  X  X haracter X , B), where dummy entity has same internal id in all three facts.

To obtain direct links between entities in such cases, we modify these facts by removing the dummy entities and using the second relationship as the relationship for the new condensed facts. In our example, we condense aforementioned facts into two : (A,  X  X haracter X , B) and (A,  X  X ovie X , C). Also, we label these condensed facts as siblings of each other. Therefore, (A,  X  X haracter X , B) is sibling of (A,  X  X ovie X , C) and vice versa. Moving forward, we use the term  X  X act X  to refer to a triplet in Freebase, containing actual entities only (no dummy entities).
After above preprocessing, we represent each en-tity and relationship in KB as a vector  X  R d . Each such entity/relationship vector is computed as the average of its word vectors, where each word vector  X  R d . In case of entities, we also include word vec-tors of all its aliases when calculating the average. Such a scheme has the additional advantage that we can benefit from pre-trained unsupervised word vec-tors (e.g. from Word2Vec), which in general capture some distributional syntactic and semantic informa-tion. 3.2 Processing Question Let a question be given as sequence of words ( x 1 ,x 2 ,...,x | q | ) . Each word x i mapped to its word vector. We experimented with two different ways to compose the question embedding q out of these word-vectors:
Bag-of-words (BOW): It is sum of individual word vectors i.e. q =
Position Encoding (PE): This heuristic take in account order of words in the question. The em-bedding is of form q = is an element-wise multiplication. For each i , l i is a column vector  X  R d with the structure l ij = embedding and j runs from 1 to d . This type of function ensures that initial part of summed vector is weighed higher for initial word vectors and vice versa. Thus, a statement like  X  X ho is parent of Darth Vader? X  will map to a different embedding than statement like  X  X ho is Darth Vader parent of? X . 4.1 Fact Extraction To begin, we generate an initial list of facts (called candidate fact list) which is fed as input to our net-work. To generate this list, we match all possible n-grams of words of the question to aliases of Freebase entities and discard all n-grams (and their matched entities) that are a subsequence of another n-gram. All facts having one of the remaining entities as sub-ject are added to candidate fact list. 4.2 Factual Memory Network A L -Hop Factual Memory Network consists of L Computation Layers C 1 ...C L connected one after another in a linear chain, and an Output Function. The initial Computation Layer C 1 takes as input the candidate fact list (from previous step) and the initial question embedding (from Section 3.2). The Output Function takes as input the output of final Compu-tation Layer C L and generates a list of answers (in form of Freebase entities). Layer C i takes as input the output of layer C i  X  1 . 4.2.1 Computation Layer
A Computation layer accesses two inputs : 1) a fact list (our  X  X emory X ) F = { f 1 ...f | F | } , where each fact f is of form ( s,R,o ) , s and o being the en-tity vectors and R the relationship vector, 2) a ques-tion embedding q  X  R d . For each fact f  X  F , we calculate an unnormalised score g ( f ) and a normal-ized score h ( f ) .

We visualize a fact f = ( s,R,o ) as a question-answer pair with ( s,R ) forming the question and o the answer. Therefore, g ( f ) calculates similarity be-tween the given question embedding and a hypothet-ical question of form q 0 = ( s,R ) . For example, given a question q =  X  X here was Bill Gates born? X  and a fact f = ( X  X ill Gates X ,  X  X lace of birth X ,  X  X eattle X ), g ( f ) will compute similarity be-tween the question q and a hypothetical question q 0 of form  X  X ill Gates place of birth? X  .

In case a fact f has some siblings b 1 ,b 2 ,...,b k (Section 3.1), we re-calculate its g ( f ) as follows: g ( f ) = average ( g ( f ) ,g ( b 1 ) ,g ( b 2 ) ,...,g ( b where g ( f ) on RHS is calculated using Eq.(1). For each sibling b i , we calculate g ( b i ) using Eq.(1), but with b i  X  X  subject replaced by its object in the for-mula.

Continuing with example in Section 3.1, if f = (A, X  X haracter X ,B) and b 1 = (A, X  X ovie X ,C) is sibling of f , then this kind of processing is helpful in an-swering questions like  X  X hat character does A play in movie C? X  , where fact f alone would not be enough to answer the question. Thus, above pro-cessing corresponds to a hypothetical question of form  X  X  character C movie ? X  for the fact f .
The normalized score h ( f ) for a fact f is calcu-lated using softmax function over all facts to deter-mine their relative importance.
Next we modify the fact list and the question em-bedding on basis of above calculated scores.
Step 1. Fact Pruning: We choose a threshold value and remove facts from fact list with h ( f ) &lt; . We found in our experiments that setting = seems to remove non-relevant facts from subsequent computation, significantly improving our response time and allowing us to explore larger search space around the subgraphs of our question entities.
Step 2. The question embedding is modified as follows: Each such modification allows us to incorporate knowledge gathered (in form of hypothetical ques-tions ( s,R ) , weighted by their relevance h ) at a par-ticular layer about the question and pass it on to sub-sequent layer.

Step 3. Fact Addition: For each object entity o belonging to a fact f , we find all the facts ( o,R 0 ,o 0 ) connected to it in KB and assign a score h ( f ) q 0 T ( o + R 0 ) to each of them. If the new fact X  X  score is &gt; , it is added to the fact list, effectively increasing path-length of our search space by 1.

The modified fact list along with the new question embedding( q 0 ) form the outputs of this layer, which are fed as input to next Computation Layer or the Output function. 4.2.2 Output Function
Output Function takes as input the output of fi-nal Computation Layer C L (i.e. its output fact list and q 0 ) and calculate scores h ( f ) . The answer set is formed by the object entity of highest scoring fact as well as object entities of all those facts that have same s and R as the highest scoring fact .

This simple heuristic can increase utility of our model when there are multiple correct answers for a question. For example, a question like  X  X ho is Anakin Skywalker father of? X  has more than one an-swer entities i.e. [  X  X eia Organa X  ,  X  X uke Skywalker X  ], and they are all linked by same s ( X  X nakin Sky-walker X ) and R ( X  X hildren X ) in Freebase. Of course, this follows the assumption that all such facts sur-vive till this stage and atleast one of them is highest scoring fact. 4.3 Training Objectives Let the QA training set D be set of question-answer pairs ( q,A ) , where q is the question with list of cor-rect answers A , e.g. ( q =  X  X ho is Anakin Skywalker father of? X  , A = [  X  X eia Organa X  ,  X  X uke Skywalker X  X  ). To train our parameters, we minimize the following loss function: L Here n refers to n-th Computation Layer in our net-work (with F n as its input fact list) and L is total number of Computation layers/hops in the network. This loss function defines the degree to which the object entities in fact list of a given layer are near to given answer list, weighted by h ( f ) , by taking pairwise difference between entities in answer list and objects in fact list. It was observed that mini-mizing this function gives higher weights to facts in which object entities are similar to answer entities as well as allow our network to generate shorter paths to reach answers from the question.

Paraphrasing Loss: Following previous work such as (Fader et al., 2013; Bordes et al., 2015), we also use the question paraphrases dataset WikiAn-which are unseen in the training set of question-answer pairs. We minimize hinge loss so that a ques-tion and its paraphrases should map to similar rep-resentations. For the paraphrase dataset P of set of tuples ( p,p 0 ) where p and p 0 are paraphrases of each other, the loss is defined as: where p 00 is random example in P that is not a para-phrase of p and 0.1 is the margin hyper-parameter.
Backpropagation is used to calculate gradients while Adagrad was used to perform optimisation us-ing max-norm regularisation. At each time step, a sample is drawn from either P with probability 0.25 or D with probability 0.75. If sample from P is chosen, gradient of L P P is calculated. Otherwise, gradient of L QA is calculated. The only parameters optimised in our model are the word vectors. 5.1 Baselines We evaluate our model on following datasets:
WebQuestions 2 : This dataset, introduced in (Be-rant et al., 2013), contains 5,810 question-answer pairs where answer can be a list of entities, simi-lar to ( q,A ) pairs described before. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Me-chanical Turk. WebQuestions is built on Freebase since all answers are defined as Freebase entities.
On WebQuestions , we evaluate against follow-ing baselines : (Berant et al., 2013; Berant and Liang, 2014; Yih et al., 2015) (semantic parsing based methods), (Fader et al., 2013) (uses a pattern matching scheme), (Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015) (Embedding based approaches). Results of the baselines have been ex-tracted from respective papers, except for (Berant et al., 2013; Berant and Liang, 2014) where we use the
We compare our system in terms of F1 score as et al., 2013), which is the average, over all test ques-tions, of the F1-score of the sets of predicted an-swers.

SimpleQuestions 3 : The SimpleQuestions dataset, introduced in (Bordes et al., 2015), consists of a to-tal of 108,442 questions written in natural language by human English-speaking annotators each paired with a corresponding Freebase fact. Our model only use the answer entity during the training, instead of whole fact. For example, { q =  X  X hich forest is Fires Creek in? X , Fact =  X (fires creek, contained by, nan-tahala national forest) X  } could be data point in Sim-pleQuestions but we only use { q =  X  X hich forest is Fires Creek in? X , A = [ X  X antahala national forest X  X  } for training.

On SimpleQuestions , we evaluate against previ-ous result (Bordes et al., 2015) in terms of path-level accuracy, in which a prediction is correct if the sub-ject and the relationship of highest scoring fact were correctly retrieved by the system. 5.2 Experimental Setup The current dump of Freebase data was down-contained 1.9B triplets. We used following splits of each evaluation dataset for training, validation and testing, same as (Bordes et al., 2015).
 WebQuestions (WQ) : [3000, 778, 2032] SimpleQuestions (SQ) : [75910, 10845, 21687] We also train on automatic questions generated from the KB, which are essential to learn embeddings for the entities not appearing in either WebQuestions or SimpleQuestions. We generated one training ques-tion per fact following the same process as that used in (Bordes et al., 2014a).

The embedding dimension d was chosen 64 and max-norm cutoff was chosen as 4 using validation dataset. We pre-train our word vectors using method described by (Wang et al., 2014) to initialize our em-beddings.

We experimented with variations of our model on both test sets. Specifically, we analyze the ef-fect of question encoding (PE vs BOW) , number of Hops and inclusion of pruning/fact-additions (P/FA) in our model. In subsequent section, the word  X  X ig-nificant X  implies that the results were statistically significant ( p &lt; 0 . 05 ) with paired T-test The results of our experiments are presented in Ta-ble 1. It shows that our best model outperforms considered baselines by about 3% in case of We-bQuestions and even comparable to previous results in case of SimpleQuestions. Note that the best per-forming system for SimpleQuestions used strong su-pervision (question with supporting fact) while our model used only answer entities associated with a question for training.
We also give the performance for the variations of our model. Position Encoding improves our perfor-mance by 7% on WQ and by 5% on SQ, validating our choice of heuristic. Also, most answers in both datasets can be found within path length of two from candidate fact list, thus a 3-Hop network shows only 2% improvement over 2-Hop network.

Top-K Performance: In Table 2, we present the top-k results on both datasets. A large majority of questions can be answered from the top two or three candidates. By providing these alternative results (in addition to the top-ranked candidate) to the user, many questions can be answered correctly. 6.1 Efficiency and Error Analysis Efficiency: All experiments for this paper were done on an Intel Core i5 CPU @ 2.60GHz, 8GB RAM with average HDD Access time of 12.3 ms. We calculated the Average Response time / Query (ART/Q), defined as average time taken by the sys-tem from input of query till the generation of an-swer list, including both computational and search-and-access time for KB. ART/Q for 1, 2 and 3 Hop Networks was 200, 350 and 505 ms respectively. Also the training time (including time to search for hyper-parameters) for each of these networks was 740, 1360 and 2480 min respectively.

The major bottleneck in ART/Query for our net-work was the search-and-access of large amount of KB Data, therefore we implemented efficient search procedures using Prefix Trees for String Matching and pipelined different stages of the model using multi-threading (i.e. Fact Extraction, Computation and Back-Propagation were performed on individual threads) to improve our response and training time.
Effect of Pruning/Fact-Additions : From Table 1, we can see that pruning and fact-additions have significantly improved scores on all datasets. We analyzed 200 random data points from set of exam-ples that were correctly answered only when P/FA was used (for each test set). In 97.5% of these sam-ples, we observed that pruning allowed our model to remove spurious facts generated during initial fact extraction, making soft-max calculation in Eq. (3) more robust.

We also observed that the set of correctly an-swered questions using model without P/FA was proper subset of one with P/FA on each evaluation dataset, signifying that if relevant facts were scored higher in previous Computation layers, their scores are not reduced as more facts are added in subse-quent layers. Removing pruning alone didn X  X  im-prove our performance by more than 0.4% on any dataset while exponentially increasing the response time, signifying that pruning itself didn X  X  remove rel-evant/correct fact in majority of examples.

On the computational end, we tried to determine the effect of pruning on our model response time (excluding search and access time). Including prun-ing improved our ART/Q by approximately 44% during test phase and by 21% during training phase for our 3 Hop network.

Manual Error Analysis : We sampled 100 ex-amples from each test set to identify major sources of errors made by our model. Following classes of errors were determined :
Complex Questions (55%) : These types of questions involved temporal or superlative qualifier like  X  X irst X ,  X  X fter X ,  X  X argest X , etc. This problem oc-curred in both test sets. We may be able to solve this problem using small set of rules for comparison on final answer set or better semantic representations for numerical values and qualifiers.

Question Ambiguity (20%) : This error class contains those questions that may have ambiguity in their interpretation. For example, a question like  X  X here is shakira from? X  generated answer as  X  X lace of birth X  (Baranquilla) while ground truth is  X  X ationality X  (Colombia). This occurred mostly in WebQuestions dataset.

Ground truth Inconsistency (10%) : This type of error occurred when ground truth differed from correct entity present in Freebase KB (even though both are correct in many cases). For example, the question  X  X here did eleanor roosevelt died? X  have ground truth as  X  X ew York City X  whereas KB deliv-ers the entity  X  X anhattan X , even though both are en-tities in Freebase. It occurred only in WebQuestions dataset.

Miscellaneous (15%) : This error class con-tains bad entity/relationship extraction (for exam-ple, mapping Anakin Skywalker to Darth Vader), bad question/answer pairs (e.g. q =  X  X hat time does american horror story air? X  A = [Tom Selleck]), Ty-pos in Question, etc. This paper presents a Factual Memory Network model that aims to perform question-answering us-ing facts from Knowledge bases like Freebase. The system uses a multi-hop neural network that can perform reasoning over facts generated from named entities in a given question as well as traverse the knowledge graph to include more information.
In future, we hope to extend our system so that it can work better with n-ary relations present in Free-base to deal with qualifiers, improve entity disam-biguation mechanism in our model as well as in-clude a mechanism to involve user interaction with system to improve our rates. Another goal is to add support for KBs with noisy data generated through automated relation extraction from unstructured data (for example OLLIE, etc) as well as for unstructured sources of knowledge (like Wikipedia) in our model, to extend and improve its utility.

