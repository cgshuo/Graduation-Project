 Microsoft Research, New York, NY Columbia University, New York, NY Yahoo! Labs, New York, NY Microsoft Research, New York, NY Microsoft Research, Redmond, WA Princeton University, Princeton, NJ In the contextual bandit problem, an agent collects rewards for actions taken over a sequence of rounds; in each round, the agent chooses an action to take on the basis of (i) con-text (or features) for the current round, as well as (ii) feed-back , in the form of rewards, obtained in previous rounds. The feedback is incomplete : in any given round, the agent observes the reward only for the chosen action; the agent does not observe the reward for other actions. Contextual bandit problems are found in many important applications such as online recommendation and clinical trials, and rep-resent a natural half-way point between supervised learn-ing and reinforcement learning. The use of features to en-code context is inherited from supervised machine learn-ing, while exploration is necessary for good performance as in reinforcement learning.
 The choice of exploration distribution on actions is im-portant. The strongest known results (Auer et al., 2002; McMahan &amp; Streeter, 2009; Beygelzimer et al., 2011) pro-vide algorithms that carefully control the exploration dis-tribution to achieve an optimal regret after T rounds of O ( p KT log( |  X  | / X  )) , with probability at least 1  X   X  , rela-tive to a set of policies  X   X  A X mapping contexts x  X  X to actions a  X  A (where K is the number of actions). The regret is the difference between the cumulative reward of the best policy in  X  and the cumulative reward collected by the algorithm. Because the bound has only a logarithmic dependence on |  X  | , the algorithm can compete with very large policy classes that are likely to yield high rewards, in which case the algorithm also earns high rewards. How-ever, the computational cost of the above algorithms is lin-ear in |  X  | , which is tractable only for simple policy classes. A sub-linear in |  X  | running time is possible for policy classes that can be efficiently searched. In this work, we use the abstraction of an optimization oracle to capture this property: given a set of context/reward vector pairs, the oracle returns a policy in  X  with maximum total reward. Using such an oracle in an i.i.d. setting (formally defined in Section 2.1), it is possible to create -greedy (Sutton &amp; Barto, 1998) or epoch-greedy (Langford &amp; Zhang, 2007) algorithms that run in time O (log |  X  | ) with only a single call to the oracle per round. However, these algorithms have suboptimal regret bounds of O (( K log |  X  | ) 1 / 3 because the algorithms randomize uniformly over actions when they choose to explore.
 The Randomized UCB algorithm of Dud  X   X k et al. (2011a) achieves the optimal regret bound (up to logarithmic fac-tors) in the i.i.d. setting, and runs in time poly( T, log |  X  | ) with  X  O ( T 5 ) calls to the optimization oracle per round. Naively this would amount to  X  O ( T 6 ) calls to the oracle over T rounds, although a doubling trick from our analy-sis can be adapted to ensure only  X  O ( T 5 ) calls to the oracle are needed over all T rounds in the Randomized UCB al-gorithm. This is a fascinating result because it shows that the oracle can provide an exponential speed-up over previ-ous algorithms with optimal regret bounds. However, the running time of this algorithm is still prohibitive for most natural problems owing to the  X  O ( T 5 ) scaling. In this work, we prove the following 1 : Theorem 1. There is an algorithm for the i.i.d. contex-tual bandit problem with an optimal regret bound requir-ing  X  O q KT ln( |  X  | / X  ) calls to the optimization oracle over T rounds, with probability at least 1  X   X  .
 Concretely, we make  X  O ( p KT/ ln( |  X  | / X  )) calls to the or-improving over the complexity of Randomized UCB . The major components of the new algorithm are (i) a new co-ordinate descent procedure for computing a very sparse distribution over policies which can be efficiently sampled from, and (ii) a new epoch structure which allows the dis-tribution over policies to be updated very infrequently. We consider variants of the epoch structure that make differ-ent computational trade-offs; on one extreme we concen-trate the entire computational burden on O (log T ) rounds with  X  O ( p KT/ ln( |  X  | / X  )) oracle calls each time, while on the other we spread our computation over  X  O ( p K/ ln( |  X  | / X  )) oracle calls for each of these rounds. We stress that in either case, the total number of calls to the oracle is only sublinear in T . Finally, we develop a more efficient online variant, and conduct a proof-of-concept ex-periment showing low computational complexity and high reward relative to several natural baselines. Motivation and related work. The EXP4-family of al-gorithms (Auer et al., 2002; McMahan &amp; Streeter, 2009; Beygelzimer et al., 2011) solve the contextual bandit prob-lem with optimal regret by updating weights (multiplica-tively) over all policies in every round. Except for a few special cases (Helmbold &amp; Schapire, 1997; Beygelzimer et al., 2011), the running time of such measure-based algo-rithms is generally linear in the number of policies. In contrast, the Randomized UCB algorithm of Dud  X   X k et al. (2011a) is based on a natural abstraction from supervised learning: efficiently finding a function in a rich function class that minimizes the loss on a training set. This abstrac-tion is encapsulated in the notion of an optimization oracle, which is also used by -greedy (Sutton &amp; Barto, 1998) and epoch-greedy (Langford &amp; Zhang, 2007).
 Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle &amp; Li, 2011). Such algorithms, as well as the closely related upper-confidence bound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the posterior distribution over policies can be effi-ciently maintained or approximated. In our experiments, we compare to a strong baseline algorithm that uses this approach (Chu et al., 2011).
 To circumvent the  X ( |  X  | ) running time barrier, we restrict attention to algorithms that only access the policy class via the optimization oracle. Specifically, we use a cost-sensitive classification oracle, and a key challenge is to de-sign good supervised learning problems for querying this oracle. The Randomized UCB algorithm of Dud  X   X k et al. (2011a) uses a similar oracle to construct a distribution over policies that solves a certain convex program. However, the number of oracle calls in their work is prohibitively large, and the statistical analysis is also rather complex. 2 Main contributions. In this work, we present a new and simple algorithm for solving a similar convex program as that used by Randomized UCB . The new algorithm is based on coordinate descent: in each iteration, the algorithm calls the optimization oracle to obtain a policy; the output is a sparse distribution over these policies. The number of itera-tions required to compute the distribution is small X  X t most  X  O ( p Kt/ ln( |  X  | / X  )) in any round t . In fact, we present a more general scheme based on epochs and warm start in which the total number of calls to the oracle is, with high probability, just  X  O ( p KT/ ln( |  X  | / X  )) over all T rounds ; we prove that this is nearly optimal for a certain class of optimization-based algorithms. The algorithm is natural and simple to implement, and we provide an arguably sim-pler analysis than that for Randomized UCB . Finally, we report proof-of-concept experimental results using a vari-ant algorithm showing strong empirical performance. In this section, we recall the i.i.d. contextual bandit setting and some basic techniques used in previous works (Auer et al., 2002; Beygelzimer et al., 2011; Dud  X   X k et al., 2011a). 2.1. Learning Setting Let A be a finite set of K actions, X be a space of possible contexts ( e.g. , a feature space), and  X   X  A X be a finite set of policies that map contexts x  X  X to actions a  X  A . 3 Let  X 
 X  := { Q  X  R  X  : Q (  X  )  X  0  X   X   X   X  , P be the set of non-negative weights over policies with total weight at most one, and let R A + := { r  X  R A : r ( a )  X  0  X  a  X  A } be the set of non-negative reward vectors. Let D be a probability distribution over X  X  [0 , 1] A , the joint space of contexts and reward vectors; we assume ac-tions X  rewards from D are always in the interval [0 , 1] . Let D X denote the marginal distribution of D over X .
 In the i.i.d. contextual bandit setting, the context/reward vector pairs ( x t ,r t )  X  X  X  [0 , 1] A over all rounds t = 1 , 2 ,... are randomly drawn independently from D . In round t , the agent first observes the context x t , then (ran-domly) chooses an action a t  X  A , and finally receives the reward r t ( a t )  X  [0 , 1] for the chosen action. The (ob-servable) record of interaction resulting from round t is the here, p t ( a t )  X  [0 , 1] is the probability that the agent chose action a t  X  A . We let H t  X  X  X  A  X  [0 , 1]  X  [0 , 1] denote the history (set) of interaction records in the first t rounds. We use b E x  X  H t [  X  ] to denote expectation when a context x is uniformly chosen at random from the t contexts in H t . Let R (  X  ) := E ( x,r )  X  X  [ r (  X  ( x ))] denote the expected (in-stantaneous) reward of a policy  X   X   X  , and let  X  ? arg max  X   X   X  R (  X  ) be a policy that maximizes the expected reward (the optimal policy ). Let Reg(  X  ) := R (  X  ? )  X  X  (  X  ) denote the expected (instantaneous) regret of a policy  X   X   X  relative to the optimal policy. Finally, the (empirical cu-mulative) regret of the agent after T rounds is defined as P 2.2. Inverse Propensity Scoring An unbiased estimate of a policy X  X  reward R (  X  ) may be obtained from a history of interaction records H t using in-verse propensity scoring ( IPS ; also called inverse proba-bility weighting ): the expected reward of policy  X   X   X  is estimated as This technique can be viewed as mapping H t 7 X  IPS ( H t ) of interaction records ( x,a,r ( a ) ,p ( a )) to context/reward vector that assigns to the chosen action a a scaled reward other actions zero rewards. We may equivalently write b R agent X  X  probability (conditioned on ( x,r ) ) of picking ac-tion a . This implies b R t (  X  ) is an unbiased estimator for any history H t .
 Let  X  t := arg max  X   X   X  b R t (  X  ) denote a policy that max-imizes the expected reward estimate based on inverse propensity scoring with history H t (  X  0 can be arbitrary), gret relative to  X  t . Note that d Reg t (  X  ) is generally not an unbiased estimate of Reg(  X  ) , because  X  t is not always  X  2.3. Optimization Oracle One natural mode for accessing the set of policies  X  is enu-meration, but this is impractical in general. In this work, we instead only access  X  via an optimization oracle which cor-responds to a cost-sensitive learner. Following Dud  X   X k et al. (2011a), we call this oracle AMO 4 .
 Definition 1. For a set of policies  X  , the arg max oracle ( AMO ) is an algorithm, which for any sequence of context and reward vectors, ( x 1 ,r 1 ) , ( x 2 ,r 2 ) ,..., ( x R + , returns arg max  X   X   X  P 2.4. Projections and Smoothing In each round, our algorithm chooses an action by ran-domly drawing a policy  X  from a distribution over  X  , and then picking the action  X  ( x ) recommended by  X  on the current context x . This is equivalent to drawing an ac-For keeping the variance of reward estimates from IPS in check, it is desirable to prevent the probability of any ac-tion from being too small. Thus, as in previous work, we also use a smoothed projection Q  X  (  X | x ) for  X   X  [0 , 1 /K ] , Q  X  ( a | x ) := (1  X  K X  ) P Every action has probability at least  X  under Q  X  (  X | x ) . For technical reasons, our algorithm maintains non-negative weights Q  X   X   X  over policies that sum to at most one, but not necessarily equal to one; hence, we put any remaining mass on a default policy  X   X   X   X  to obtain a legitimate probability distribution over policies  X  Q = Q + 1  X  P  X   X   X  Q (  X  ) 1  X   X  . We then pick an action from the smoothed projection  X  Q  X  (  X | x ) of  X  Q as above. Our algorithm ( ILOVETOCONBANDITS ) is an epoch-based variant of the Randomized UCB algorithm of Dud  X   X k et al. (2011a) and is given in Algorithm 1. Like Randomized UCB , ILOVETOCONBANDITS solves an optimization problem (OP) to obtain a distribution over policies to sample from (Step 7), but does so on an epoch schedule , i.e. , only on certain pre-specified rounds  X  , X  2 ,... . The only requirement of the epoch schedule is that the length of epoch m is bounded as  X  m +1  X   X  m = O (  X  m ) . For simplicity, we assume  X  m +1  X  2  X  m for m  X  1 , and  X  1 = O (1) .
 The crucial step here is solving (OP). Before stating the main result, let us get some intuition about this problem. The first constraint, Eq. (2), requires the average estimated regret of the distribution Q over policies to be small, since b  X  is a rescaled version of the estimated regret of policy  X  . This constraint skews our distribution to put more mass on  X  X ood policies X  (as judged by our current information), and can be seen as the exploitation component of our algo-rithm. The second set of constraints, Eq. (3), requires the distribution Q to place sufficient mass on the actions cho-sen by each policy  X  , in expectation over contexts. This can be thought of as the exploration constraint, since it requires the distribution to be sufficiently diverse for most contexts. As we will see later, the left hand side of the constraint is a bound on the variance of our reward estimates for policy  X  , and the constraint requires the variance to be controlled at the level of the estimated regret of  X  . That is, we require the reward estimates to be more accurate for good policies than we do for bad ones, allowing for much more adaptive exploration than that of -greedy style algorithms. (2011a), and our coordinate descent algorithm in Sec-tion 3.1 gives a constructive proof that the problem is fea-sible. As in Dud  X   X k et al. (2011a), we have the following regret bound: 5 Theorem 2. Assume the optimization problem ( OP ) can be solved whenever required in Algorithm 1. With probability at least 1  X   X  , the regret of Algorithm 1 ( ILOVETOCONBANDITS ) after T rounds is Algorithm 1 Importance-weighted LOw-Variance Epoch-Timed Oracleized CONtextual BANDITS algorithm ( ILOVETOCONBANDITS ) input Epoch schedule 0 =  X  0 &lt;  X  1 &lt;  X  2 &lt;  X  X  X  , allowed 1: Initial weights Q 0 := 0  X   X   X  , initial epoch m := 1 . 2: for round t = 1 , 2 ,... do 3: Observe context x t  X  X . 5: Select action a t and observe reward r t ( a t )  X  [0 , 1] . 6: if t =  X  m then 7: Let Q m be a solution to (OP) with history H t and 8: m := m + 1 . 9: end if 10: end for
Given a history H t and minimum probability  X  m , define b 3.1. Solving (OP) via Coordinate Descent We now present a coordinate descent algorithm to solve (OP). The pseudocode is given in Algorithm 2. Our analy-sis, as well as the algorithm itself, are based on a potential function which we use to measure progress. The algorithm can be viewed as a form of coordinate descent applied to this same potential function. The main idea of our analy-sis is to show that this function decreases substantially on every iteration of this algorithm; since the function is non-negative, this gives an upper bound on the total number of iterations as expressed in the following theorem.
 Theorem 3. Algorithm 2 (with Q init := 0 ) halts in at most 3.2. Using an Optimization Oracle We now show how to implement Algorithm 2 via AMO (c.f. Section 2.3).
 Lemma 1. Algorithm 2 can be implemented using one call to AMO before the loop is started, and one call for each iteration of the loop thereafter.
 Proof. At the very beginning, before the loop is started, Algorithm 2 Coordinate Descent Algorithm Require: History H t , minimum probability  X  , initial 1: Set Q := Q init . 2: loop 3: Define, for all  X   X   X  , 4: if P  X  Q (  X  )(2 K + b  X  ) &gt; 2 K then 5: Replace Q by cQ , where 6: end if 7: if there is a policy  X  for which D  X  ( Q ) &gt; 0 then 8: Add the (positive) quantity 9: else 10: Halt and output the current set of weights Q . 11: end if 12: end loop we compute the best empirical policy so far,  X  t , by calling AMO on the sequence of historical contexts and estimated reward vectors; i.e. , on ( x  X  ,  X  r  X  ) , for  X  = 1 , 2 ,...,t . Next, we show that each iteration in the loop of Algorithm 2 can be implemented via one call to AMO . Going over the pseudocode, first note that operations involving Q in Step 4 can be performed efficiently since Q has sparse support. Note that the definitions in Step 3 don X  X  actually need to be computed for all policies  X   X   X  , as long as we can identify a policy  X  for which D  X  ( Q ) &gt; 0 . We can identify such a policy using one call to AMO as follows.
 First, note that for any policy  X  , we have V  X  ( Q ) = t historical contexts and reward vectors, ( x  X  ,  X  r  X  1 , 2 ,...,t , where for any action a we define Observe that D  X  ( Q ) = (  X  X  )  X  1 P t  X  =1  X  r  X  (  X  ( x stant independent of  X  . Therefore, arg max  X   X   X  D  X  ( Q ) = once on the sequence ( x  X  ,  X  r  X  ) for  X  = 1 , 2 ,...,t , we ob-tain a policy that maximizes D  X  ( Q ) , and thereby identify a policy for which D  X  ( Q ) &gt; 0 whenever one exists. 3.3. Epoch Schedule Recalling the setting of  X  m in Algorithm 1, The-orem 3 shows that Algorithm 2 solves (OP) with  X  O ( p Kt/ ln( |  X  | / X  )) calls to AMO in round t . Thus, if we use the epoch schedule  X  m = m ( i.e. , run Algorithm 2 in every round), then we get a total of  X  O ( p KT 3 / ln( |  X  | / X  )) calls to AMO over all T rounds. This number can be dramatically reduced using a more carefully chosen epoch schedule.
 Lemma 2. For the epoch schedule  X  m := 2 m  X  1 , the total number of calls to AMO is  X  O ( p KT/ ln( |  X  | / X  )) . Proof. The epoch schedule satisfies the requirement  X  m +1  X  2  X  m . With this epoch schedule, Algorithm 2 is run only O (log T ) times over T rounds, leading to  X  O ( p KT/ ln( |  X  | / X  )) total calls to AMO . 3.4. Warm Start We now present a different technique to reduce the number of calls to AMO . This is based on the observation that prac-tically speaking, it seems terribly wasteful, at the start of a new epoch, to throw out the results of all of the preceding computations and to begin yet again from nothing. Instead, intuitively, we expect computations to be more moderate if we begin again where we left off last, i.e. , a  X  X arm-start X  approach. Here, when Algorithm 2 is called at the end of epoch m , we use Q init := Q m  X  1 (the previously computed weights) rather than 0 .
 We can combine warm-start with a different epoch sched-ule to guarantee  X  O ( p KT/ ln( |  X  | / X  )) total calls to AMO , spread across O ( Lemma 3. Define the epoch schedule (  X  1 , X  2 ) := (3 , 5) and  X  m := m 2 for m  X  3 (this satisfies  X  m +1  X  2  X  m ). With high probability, the warm-start variant of Algo-rithm 1 makes  X  O ( p KT/ ln( |  X  | / X  )) calls to AMO over T rounds and O ( 3.5. Computational Complexity So far, we have only considered computational complex-ity in terms of the number of oracle calls. However, the reduction also involves the creation of cost-sensitive clas-sification examples, which must be accounted for in the net computational cost. With some natural bookkeeping of probabilities, the computational complexity of our algo-rithm, modulo the oracle running time, can be made to be  X  O ( p ( KT ) 3 / ln( |  X  | / X  )) . Details are given in the full ver-sion of the paper.
 3.6. A Lower Bound on the Support Size An attractive feature of Algorithm 2 is that the number of calls to AMO is directly related to the number of policies in the support of Q m . For instance, with the doubling sched-ule of Section 3.3, Theorem 3 implies that we never have epoch m . The support size of the distributions Q m in Al-gorithm 1 is crucial to the computational cost of sampling an action.
 We now demonstrate a lower bound showing that it is not possible to construct substantially sparser distributions that also satisfy the low-variance constraint (3) in the optimiza-tion problem (OP). To formally state the lower bound, for a given an epoch schedule (  X  m ) , define the following set of non-negative vectors over policies: (The distribution Q m computed by Algorithm 1 is in Q m .) Recall that supp( Q ) denotes the support of Q (the set of policies where Q puts non-zero entries). We have the fol-lowing lower bound on | supp( Q ) | .
 Theorem 4. For any epoch schedule 0 =  X  0 &lt;  X  1 &lt;  X   X  X  X  and any M  X  N sufficiently large, there exists a distri-bution D over X  X  [0 , 1] A and a policy class  X  such that, with probability at least 1  X   X  , In the context of our problem, this lower bound shows that the bounds in Lemma 2 and Lemma 3 are unimprovable, since the number of calls to AMO is at least the size of the support, given our mode of access to  X  . In this section, we outline the regret analysis for our algo-rithm ILOVETOCONBANDITS .
 The deviations of the policy reward estimates b R t controlled by (a bound on) the variance of each term in Eq. (1): essentially the left-hand side of Eq. (3) from (OP), discrepancy is handled using deviation bounds, so Eq. (3) holds with E x  X  X  X [  X  ] , with worse right-hand side constants. The rest of the analysis, which deviates from that of Randomized UCB , compares the expected regret Reg(  X  ) of any policy  X  with the estimated regret d Reg t (  X  ) using the variance constraints Eq. (3): Lemma 4 (Informally) . With high probability, for each m such that  X  m  X   X  O ( K log |  X  | ) , each round t in epoch m , and each  X   X   X  , Reg(  X  )  X  2 d Reg t (  X  ) + O ( K X  m ) . This lemma can easily be combined with the constraint Eq. (2) from (OP): since the weights Q m  X  1 used in any round t in epoch m satisfy P obtain a bound on the (conditionally) expected regret in round t using the above lemma: with high probability, P terms up over all T rounds and applying martingale concentration gives the bound in Theorem 2. In this section, we give a sketch of the analysis of our main optimization algorithm for computing weights Q m on each epoch as in Algorithm 2. As mentioned in Section 3.1, this analysis is based on a potential function.
 Since our attention for now is on a single epoch m , here and in what follows, when clear from context, we drop m from our notation and write simply  X  =  X  m ,  X  =  X  m , etc. Let U
A be the uniform distribution over the action set A . We define the following potential function for use on epoch m :  X  m ( Q ) =  X  X  This function is defined for all vectors Q  X   X   X  . Also, RE ( p k q ) denotes the unnormalized relative entropy be-tween two nonnegative vectors p and q over the action space (or any set) A : RE ( p k q ) = P a  X  A ( p a ln( p q  X  p a ) , which is always nonnegative. Here, Q  X  (  X | x ) de-notes the  X  X istribution X  (which might not sum to 1 ) over A induced by Q  X  for context x as given in Section 2.4. Thus, ignoring constants, this potential function is a combination of two terms: The first measures how far from uniform are the distributions induced by Q  X  , and the second is an esti-mate of expected regret under Q since b  X  is proportional to the empirical regret of  X  . Making  X  m small thus encour-ages Q to choose actions as uniformly as possible while also incurring low regret  X  exactly the aims of our algo-rithm. The constants that appear in this definition are for later mathematical convenience.
 For further intuition, note that, by straightforward calcu-lus, the partial derivative  X   X  m / X  X  (  X  ) is roughly propor-tional to the variance constraint for  X  given in Eq. (3) (up to a slight mismatch of constants). This shows that if this constraint is not satisfied, then  X   X  m / X  X  (  X  ) is likely to be negative, meaning that  X  m can be decreased by increasing Q (  X  ) . Thus, the weight vector Q that minimizes  X  m sat-isfies the variance constraint for every policy  X  . It turns out that this minimizing Q also satisfies the low regret con-straint in Eq. (2), and also must sum to at most 1 ; in other words, it provides a complete solution to our optimization problem. Algorithm 2 does not fully minimize  X  m , but it is based roughly on coordinate descent. This is because in each iteration one of the weights (coordinate directions) Q (  X  ) is increased. This weight is one whose corresponding partial derivative is large and negative.
 To analyze the algorithm, we first argue that it is correct in the sense of satisfying the required constraints, provided that it halts.
 Lemma 5. If Algorithm 2 halts and outputs a weight vector Q , then the constraints Eq. (3) and Eq. (2) must hold, and furthermore the sum of the weights Q (  X  ) is at most 1 . What remains is the more challenging task of bounding the number of iterations until the algorithm does halt. We do this by showing that significant progress is made in reduc-ing  X  m on every iteration. To begin, we show that scaling Q as in Step 4 cannot cause  X  m to increase.
 Lemma 6. Let Q be a weight vector such that P  X  Q (  X  )(2 K + b  X  ) &gt; 2 K , and let c be as in Eq. (4) . Then  X  m ( cQ )  X   X  m ( Q ) .
 Next, we show that substantial progress will be made in reducing  X  m each time that Step 8 is executed.
 Lemma 7. Let Q denote a set of weights and suppose, for some policy  X  , that D  X  ( Q ) &gt; 0 . Let Q 0 be a new set of weights which is an exact copy of Q except that Q (  X  ) = Q (  X  ) +  X  where  X  =  X   X  ( Q ) &gt; 0 . Then  X  m ( Q )  X   X  m ( Q 0 )  X   X  X  2 / (4(1  X  K X  )) .
 So Step 4 does not cause  X  m to increase, and Step 8 causes  X  m to decrease by at least the amount given in Lemma 7. This immediately implies Theorem 3: for Q init = 0 , the initial potential is bounded by  X  X  ln(1 / ( K X  )) / (1  X  K X  ) , and it is never negative, so the number of times Step 8 is executed is bounded by 4 ln(1 / ( K X  )) / X  as required. 5.1. Epoching and Warm Start As shown in Section 2.3, the bound on the number of iter-ations of the algorithm from Theorem 3 also gives a bound on the number of times the oracle is called. To reduce the number of oracle calls, one approach is the  X  X oubling trick X  of Section 3.3, which enables us to bound the total combined number of iterations of Algorithm 2 in the first T rounds is only  X  O ( p KT/ ln( |  X  | / X  )) . This means that the average number of calls to the arg-max oracle is only  X  O ( p K/ ( T ln( |  X  | / X  ))) per round, meaning that the oracle is called far less than once per round, and in fact, at a van-ishingly low rate.
 We now turn to warm-start approach of Section 3.4, where in each epoch m + 1 we initialize the coordinate descent al-gorithm with Q init = Q m , i.e. the weights computed in the previous epoch m . To analyze this, we bound how much the potential changes from  X  m ( Q m ) at the end of epoch m to  X  m +1 ( Q m ) at the very start of epoch m + 1 . This, combined with our earlier results regarding how quickly Algorithm 2 drives down the potential, we are able to get an overall bound on the total number of updates across T rounds.
 Lemma 8. Let M be the largest integer for which  X  M +1  X  epoch-to-epoch increase in potential is
X where M is the largest integer for which  X  M +1  X  T . This lemma, along with Lemma 7 can be used to further establish Lemma 3. We only provide an intuitive sketch here, with the details deferred to the appendix. As we ob-serve in Lemma 8, the total amount that the potential in-creases across T rounds is at most  X  O ( p T ln( |  X  | / X  ) /K ) . On the other hand, Lemma 7 shows that each time Q is updated by Algorithm 2 the potential decreases by at least  X   X (ln( |  X  | / X  ) /K ) (using our choice of  X  ). Therefore, the total number of updates of the algorithm totaled over all T rounds is at most  X  O ( p KT/ ln( |  X  | / X  )) . For instance, if we use (  X  1 , X  2 ) := (3 , 5) and  X  m := m 2 for m  X  3 , then the Q is only updated about of those rounds, Algorithm 2 requires  X  O ( p K/ ln( |  X  | / X  )) iterations, on average, giving the claim in Lemma 3. In this section we evaluate a variant of Algorithm 1 against several baselines. While Algorithm 1 is significantly more efficient than many previous approaches, the overall com-putational complexity is still at least  X  O (( KT ) 1 . 5 total cost of the oracle calls, as discussed in Section 3.5. This is markedly larger than the complexity of an ordinary supervised learning problem where it is typically possible to perform an O (1) -complexity update upon receiving a fresh example using online algorithms.
 A natural solution is to use an online oracle that is stateful and accepts examples one by one. An online cost-sensitive classification (CSC) oracle takes as input a weighted ex-ample and returns a predicted class (corresponding to one of K actions in our setting). Since the oracle is stateful, it remembers and uses examples from all previous calls in an-swering questions, thereby reducing the complexity of each oracle invocation to O (1) as in supervised learning. Using several such oracles, we can efficiently track a distribution over good policies and sample from it. We detail this ap-proach (which we call Online Cover) in the full version of the paper. The algorithm maintains a uniform distribution over a fixed number n of policies where n is a parameter of the algorithm. Upon receiving a fresh example, it updates dim, minibatch-10 cover n = 1 nothing all n policies with the suitable CSC examples (Eq. (5)). The specific CSC oracle we use is a reduction to squared-loss regression (Algorithms 4 and 5 of Beygelzimer &amp; Langford (2009)) which is amenable to online updates. Our imple-mentation is included in Vowpal Wabbit. 6 Due to lack of public datasets for contextual bandit prob-lems, we use a simple supervised-to-contextual-bandit transformation (Dud  X   X k et al., 2011b) on the CCAT docu-ment classification problem in RCV1 (Lewis et al., 2004). This dataset has 781265 examples and 47152 TF-IDF fea-tures. We treated the class labels as actions, and one minus 0/1-loss as the reward. Our evaluation criteria is progres-sive validation (Blum et al., 1999) on 0/1 loss. We compare several baseline algorithms to Online Cover; all algorithms take advantage of linear representations which are known to work well on this dataset. For each algorithm, we report the result for the best parameter settings (shown in Table 6). 1. -greedy (Sutton &amp; Barto, 1998) explores randomly 2. Explore-first is a variant that begins with uniform ex-3. A less common but powerful baseline is based on bag-4. LinUCB (Auer, 2002; Chu et al., 2011) has been quite 5. Finally, our algorithm achieves the best loss of 0 . 0530 . All baselines except for LinUCB are implemented as a sim-ple modification of Vowpal Wabbit. All reported results use default parameters where not otherwise specified. The contextual bandit learning algorithms all use a doubly ro-bust reward estimator instead of the importance weighted estimators used in our analysis (Dud  X   X k et al., 2011b). Because RCV1 is actually a fully supervised dataset, we can apply a fully supervised online multiclass algorithm to solve it. We use a simple one-against-all implementation to reduce this to binary classification, yielding an error rate of 0 . 051 which is competitive with the best previously re-ported results. This is effectively a lower bound on the loss we can hope to achieve with algorithms using only partial information. Our algorithm is less than 2.3 times slower and nearly achieves the bound. Hence on this dataset, very little further algorithmic improvement is possible. In this paper we have presented the first practical algorithm to our knowledge that attains the statistically optimal re-gret guarantee and is computationally efficient in the set-ting of general policy classes. A remarkable feature of the algorithm is that the total number of oracle calls over all T rounds is sublinear X  X  remarkable improvement over pre-vious works in this setting. We believe that the online vari-ant of the approach which we implemented in our experi-ments has the right practical flavor for a scalable solution to the contextual bandit problem. In future work, it would be interesting to directly analyze the Online Cover algorithm. Acknowledgements We thank Dean Foster and Matus Telgarsky for helpful dis-cussions. Part of this work was completed while DH and RES were visiting Microsoft Research.
 Agarwal, Alekh, Hsu, Daniel, Kale, Satyen, Langford, John, Li, Lihong, and Schapire, Robert E. Taming the monster: A fast and simple algorithm for contextual ban-dits. CoRR , abs/1402.0555, 2014.
 Auer, Peter. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Re-search , 3:397 X 422, 2002.
 Auer, Peter, Cesa-Bianchi, Nicol ` o, Freund, Yoav, and
Schapire, Robert E. The nonstochastic multiarmed ban-dit problem. SIAM Journal of Computing , 32(1):48 X 77, 2002.
 Beygelzimer, Alina and Langford, John. The offset tree for learning with partial labels. In KDD , 2009.
 Beygelzimer, Alina, Langford, John, Li, Lihong, Reyzin,
Lev, and Schapire, Robert E. Contextual bandit algo-rithms with supervised learning guarantees. In AISTATS , 2011.
 Blum, Avrim, Kalai, Adam, and Langford, John. Beating the holdout: Bounds for k-fold and progressive cross-validation. In COLT , 1999.
 Chapelle, Olivier and Li, Lihong. An empirical evaluation of Thompson sampling. In NIPS , 2011.
 Chu, Wei, Li, Lihong, Reyzin, Lev, and Schapire, Robert E. Contextual bandits with linear payoff functions. In AIS-TATS , 2011.
 Dud  X   X k, Miroslav, Hsu, Daniel, Kale, Satyen, Karampatzi-akis, Nikos, Langford, John, Reyzin, Lev, and Zhang, Tong. Efficient optimal learning for contextual bandits. In UAI , 2011a.
 Dud  X   X k, Miroslav, Langford, John, and Li, Lihong. Doubly robust policy evaluation and learning. In ICML , 2011b. Helmbold, David P. and Schapire, Robert E. Predicting nearly as well as the best pruning of a decision tree. Ma-chine Learning , 27(1):51 X 68, 1997.
 Langford, John. Interactive machine learning, Jan-uary 2014. URL http://hunch.net/  X  jl/ projects/interactive/index.html .
 Langford, John and Zhang, Tong. The epoch-greedy algo-rithm for contextual multi-armed bandits. In NIPS , 2007. Lewis, David D, Yang, Yiming, Rose, Tony G, and Li,
Fan. Rcv1: A new benchmark collection for text cate-gorization research. The Journal of Machine Learning Research , 5:361 X 397, 2004.
 Li, Lihong. Generalized Thompson sampling for contex-tual bandits. CoRR , abs/1310.7163, 2013.
 Li, Lihong, Chu, Wei, Langford, John, and Schapire,
Robert E. A contextual-bandit approach to personalized news article recommendation. In WWW , 2010.
 McMahan, H. Brendan and Streeter, Matthew. Tighter bounds for multi-armed bandits with expert advice. In COLT , 2009.
 Sutton, Richard S. and Barto, Andrew G. Reinforcement learning, an introduction . MIT Press, 1998.
 Thompson, William R. On the likelihood that one unknown probability exceeds another in view of the evidence of
