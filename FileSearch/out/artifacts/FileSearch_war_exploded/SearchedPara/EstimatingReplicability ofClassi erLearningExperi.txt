 Mac hine learning researc h on classi ers relies to a large exten t on exp erimen tal observ ations. It is widely rec-ognized that there are man y pitfalls in performing ex-perimen ts [3, 6, 8]. But, so far, most researc h in this area concen trates on undesirable high lev els of Type I error, the situation where the exp erimen t indicates that one classi er outp erforms another, while in real-ity it does not. An often overlo oked issue with exp eri-men tal researc h is that the particular randomizations used in the exp erimen t can have a ma jor impact on the outcome of the exp erimen t. This e ect can be so large that for some exp erimen tal designs only in 2 out of 3 cases rep etition of the exp erimen t pro duces the same outcome [2].
 In this pap er, we try to get a better insigh t in this issue of replicabilit y and whic h factors in an exp er-imen t in uence replicabilit y. In order to do so, we need a practical de nition of replicabilit y and a way to measure replicabilit y of an exp erimen t. Once this is established we can actually perform exp erimen ts on various set-ups. In the follo wing section, we consider a num ber of exp erimen tal designs. We con tin ue in Sec-tion 3 with ways to estimate replicabilit y and perform a theoretical analysis of their performance. Section 4 presen ts empirical results where we measure replica-bilit y for the various exp erimen tal set-ups. We nish with some concluding remarks. The problem we want to address is, giv en two learn-ing algorithms A and B that generate classi ers and a small data set D , how to mak e a decision whic h of the two algorithms performs best based on classi cation accuracy for the giv en data set. A general metho d to mak e suc h a decision is to split D into a training set D t and a test set D n D t . Then, train algorithm A and B on D t and register the classi cation accuracy on the D n D t . This way, we obtain two classi cation accura-cies P A and P B and the di erence x = P A P B giv es an indication whic h algorithm performs better. A formal way to mak e suc h a decision is to apply a hy-pothesis test. Ho wever, suc h hypothesis test typically requires more than a single outcome x . Unfortunately , for small datasets D , we have to split D rep eatedly in training and test sets to obtain multiple outcomes P A;i and P B;i with asso ciated di erences x i = P A;i P B;i , 1 i n obtaining a sample of size n .
 So, an exp erimen t has two comp onen ts. Firstly , a sam-pling scheme for obtaining a sample x 1 ;:::;x n , and secondly , a hypothesis test to mak e a decision based on the sample. There are various ways to obtain sam-ples and to perform hypothesis tests. 2.1. Sampling metho ds We consider six di eren t sampling schemes.
 Resampling: Resampling consist of splitting the data n times in a randomly selected test set D t;i con tain-ing a fraction of the data (typically 10% to 33%) and a training set D n D t;i . The algorithms A and B learn on the training set and accuracies P A;i and P B;i , 1 i n are obtained by classifying instances on the accompan ying test set giving n accuracy di erences x i = P A;i P B;i for the sample. Resampling used to be an accepted way for applying the t-test on the sam-ple x 1 ;:::;x n till it was discredited by Dietteric h [3] due to its extremely high Type I error. Nadeau and Bengio [6] sho wed how this problem can be solv ed by correcting the variance.
 K-fold cross validation: Cross validation splits the data D into k appro ximately equal parts D 1 ;:::;D k , and learns on the data D n D i , 1 i k with one part left out. The part D i left out is used as test set, giving n = k accuracy di erences x i = P A;i P B;i . Dietteric h [3] observ ed a sligh tly elev ated Type I error for cross validation with a t-test and its replicabilit y is rather low [2].
 Use all data: To obtain more samples, we can rep eat k -fold cross validation r times with di eren t random splits into folds for eac h of the runs. This giv es us r k accuracy di erences. Let ^ x i;j , 1 i r , 1 j k denote the di erence in accuracy of algorithms A and B in the i th run on the j th fold. Here A and B are trained on the k 1 remaining folds in the i th run. We obtain a sample of size n = r k by using all of the accuracy di erences x i;j (formally by setting x Av erage over folds: In averaging over folds, the rec-ommended metho d for Weka [9], we tak e the result in a rep eated cross validation exp erimen t. We obtain one sample value per run by taking the average di er-ence over all results for a single run, x i = (where ^ x i;j as for the use all data scheme). Av erage over runs: Av eraging over folds can be interpreted as an impro ved way of doing resampling. The natural extension is performing an impro ved way of k-fold cross validation, and instead of averaging over folds, average over runs. We obtain one sam-ple value per fold de ned as the average di erence x = runs sho w a very high Type I error when applying a t-test [2].
 Av erage over sorted runs: Av eraging over runs com bines results from di eren t runs rather arbitrar-ily. One gets better estimates of a k-fold cross valida-tion exp erimen t by rst sorting the results for the in-dividual k-fold cross validation exp erimen ts and then taking the average. This way, the estimate for the minim um value is calculated from the minim um val-ues in all folds, the one but lowest from the one but lowest results in all folds, etc. Let ^ x ( i;j ) be the j th highest value of accuracy di erence ^ x i 0 j 0 of run i . Then, the sample consisting of k values is de ned by x i = Figure 1 illustrates the di erence between the data used for the sampling schemes. The gure sho ws an example of 3x3 fold cross validation outcomes in the box at the left half (though in practice a 10x10 fold cross validation is more appropriate). All the data in the box in Figure 1 is used for the "use all data" scheme. For resampling, essen tially only the rst col-umn is required when performing a 2/3-1/3 split of training and test data. Cross validation uses only the rst run, that is, the rst row of a 3x3 fold cross val-idation outcome. Av eraging over folds and runs is es-sen tially summing over columns and rows resp ectiv ely . For getting sorted means, rst the results have to be sorted over folds, giving the table at the righ t of Fig-ure 1. Then the means are obtained by summing over rows. 2.2. Hyp othesis tests In our exp erimen t, we want to test the null hypothesis H 0 that A and B perform the same. More formally , we want to test whether the sample x 1 ;:::;x n has zero mean. There are di eren t metho ds to test suc h hy-pothesis, all of whic h are based on sligh tly di eren t assumptions. We consider the popular t-test, the sign test and the rank sum test, also kno wn as Wilco xon's test. All these tests assume that the outcomes x i in the sample are mutually indep enden t, an assumption that is obviously violated.
 These hypothesis tests follo w a similar pro cedure. First, we calculate a statistic Z from the sample. Dif-feren t tests have di eren t metho ds of calculating Z (see belo w). Then, we calculate the probabilit y p ( Z ) that the value Z or less is observ ed assuming H 0 is true. We choose a signi cance lev el and accept H 0 if p ( Z ) is higher than = 2 but less than 1 = 2. If p ( Z ) &lt; = 2, the test indicates B outp erforms A and if p ( Z ) &gt; 1 = 2, the test indicates A outp erforms B . Paired t-test: The assumption underlying the paired t-test is that the outcomes x i are normally dis-tributed. If this is true, then the mean can be esti-mated using ^ m = 1 n ( df = n 1) we have a statistic Z = ^ m p is distributed according to Studen ts t-distribution P t with df degrees of freedom. The probabilit y that the data x 1 ;:::;x n is observ ed assuming the null hypoth-esis is true is obtained by nding P t ( T;df ). Sign test: The attractiv eness of the sign test is that it is simple and mak es no assumptions about the under-lying distribution of the sample. Instead, it only looks at the signs of x 1 ;:::;x n and statistic Z is the num-ber of pluses. When accuracies P A;i and P B;i are the same, whic h occurs quite often when two algorithms perform very similarly , x i = 0 and we coun t this as half a plus. If the null hypothesis is true, the probabilit y of generating a plus or a min us is 0 : 5, in other words H 0 : p = 0 : 5. The probabilit y of observing Z pluses in n comparisons is P ( Z ) = with p = 0 : 5 is P ( Z ) = Rank sum test: Lik e the sign test, the rank sum test mak es no assumption about the underlying dis-tribution of outcomes x i . Ho wever, the rank sum test does exploit the size of the values of x i , whic h con tains poten tially valuable information. The rank sum test sorts the outcomes x i on its absolute value, giving a When accuracies are the same (i.e. outcomes for whic h x i = 0) they are remo ved from the sample, lea ving n 0 items. No w, we add the ranks of outcomes that are positiv e, r = is appro ximately normally distributed. So, we use Z = r m , whic h is normally distributed with mean 0 and variance 1. 2.3. Qualit y of exp erimen ts There are essen tially three metho ds to judge the qual-ity of an exp erimen t:
The Type I err or is the probabilit y that the conclu-sion of an exp erimen t is there is a di erence between algorithms, while in realit y there is not. In theory , the Type I error equals the signi cance lev el chosen for the hypothesis test if none of the assumptions of the test are violated. In practice, the indep endence assump-tion is often violated resulting in an elev ated Type I error.

The Type II err or is the probabilit y the conclusion of an exp erimen t is there is no di erence between algo-rithms, while in realit y there is. The power is de ned as 1 min us the Type II error. The power is not directly con trollable like the Type I error is. Ho wever, there is a trade-o between power and Type I error and a higher power can be obtained at the cost of a higher Type I error. The exact relation between the two de-pends on the exp erimen tal design.

Replic ability of an exp erimen t is a measure of how well the outcome of an exp erimen t can be repro duced. The most desirable exp erimen t has a low Type I error, a high power an high replicabilit y. In the follo wing section we will have a closer look at replicabilit y. In [2], an ad hoc de nition for replicabilit y was pro-posed as follo ws. When an exp erimen t is rep eated ten times with di eren t randomizations of a giv en data set, the exp erimen t is deemed replicable if its outcome is the same for all ten exp erimen ts. If one or more outcomes di er, it is not replicable. An impression of the replicabilit y of an exp erimen t can be obtained by averaging over a large num ber (sa y 1000) of data sets. This de nition is useful in highligh ting that repli-cabilit y of exp erimen ts is indeed an issue in mac hine learning. Ho wever, the disadv antage is that replica-bilit y measured this way cannot be compared with re-sults for doing the exp erimen t another num ber than ten times. Also, replicabilit y de ned this way would not distinguish between having 1 out of 10 outcomes being di eren t and 5 out of 10 outcomes being di er-ent. Further, increasing the num ber of exp erimen ts to say 100 increases the likeliho od that one of the exp eri-men ts di er and thus decreases replicabilit y according to the de nition of [2]. A de nition of replicabilit y that does not su er from these issues is the follo wing. De nition: Replic ability of an exp erimen t is the prob-abilit y two runs of the exp erimen t on the same data set, with the same pair of algorithms and the same metho d of sampling the data pro duces the same out-come.
 This de nition applies both in the situation where the algorithms perform the same and when one outp er-forms another. Note the di erence between Type I error and replicabilit y. When the algorithms perform the same, the Type I error expresses the probabilit y over all data sets that a di erence is found. Replica-bilit y only expresses that error for one data set . By de ning replicabilit y in terms of probabilities, one can compare replicabilit y of di eren t exp erimen ts with di eren t exp erimen tal set-ups and num ber of runs. Furthermore, an exp erimen t that pro duces 9 same out-comes out of 10 has a higher replicabilit y this way than when it only pro duces 5 same outcomes out of 10. Note that replicabilit y alw ays lies between 50% and 100%. Normalize d replic ability is replicabilit y linearly scaled to the range 0% to 100%. So, if replicabilit y is r , normalized replicabilit y is 2( r 1 2 ). 3.1. A simple estimator The only way to determine the replicabilit y of an ex-perimen t is to measure it empirically . So, we need an estimator of replicabilit y. A simple approac h is to ob-tain pairs of runs of an exp erimen t on a data set D and just interpret those as the outcome of Bernoulli trial with probabilit y r that the outcomes are the same. The outcome e of an exp erimen t on data set D is 'ac-cept' or 'reject'. When the outcome is 'accept' the null hypothesis that the two learning algorithms per-form the same on D is accepted, otherwise they are not.
 De nition Let e = e 1 ;:::;e n ( n &gt; 0 and n even) be the outcomes of n exp erimen ts with di eren t random-izations on data set D . The estimator ^ R 1 of replica-bilit y r is where I is the indicator function, whic h is 1 if its ar-gumen t is true, and 0 otherwise.
 We write ^ R 1 if it is clear from the con text what the argumen t e of ^ R 1 ( e ) is.
 Lemma 3.1 ^ R 1 is an unbiase d estimator of replic abil-ity r with varianc e r r 2 n= 2 .
 Pro of: The bias of ^ R 1 is E ( ^ R 1 ) r . No w, E ( ^ E ( the exp ectation giv es E ( ^ R 1 ) = 1 n= 2 E ( e 2 i 1 )). Distributing the sum results in E ( e e r and P ( I ( e 2 i 6 = e 2 i 1 )) = 1 r so we get E ( I ( e e 2 i 1 )) = r 1 + (1 r ) 0 = r . Substituting in E ( above giv es E ( ^ R 1 ) = 1 n= 2 bias of ^ R 1 = E ( ^ R 1 ) r = r r = 0, whic h sho ws that ^ R 1 is an unbiased estimator of r .
 The variance of ^ R 1 is var ( ^ R 1 ) = E ( ^ R 2 1 ) E ( ^ E ( ^
R 1 ) 2 where ^ R 1 = 1 n= 2 . From the deriv a-tion above, we have E ( ^ R 1 ) 2 = r 2 . Further, ob-serv e that P ( i same pairs out of n= 2) follo ws the bi-nomial distribution with probabilit y r . So, we have var ( ^ R 1 ) = r ing Lemma A.1 (see App endix), r ) 3.2. An adv anced estimator The simple estimator ^ R 1 uses exp erimen t e 1 only to compare with e 2 . Since e 3 is indep enden t of e 1 , one could compare e 1 with e 3 as well. Lik ewise, the pair ( e 1 ;e k ) for any k &gt; 1 could be compared and used in the estimate of replicabilit y. In fact, we can use all pairs of outcomes and estimate replicabilit y as the fraction of pairs with the same outcome. This de nes a new estimator ^ R 2 .
 De nition Let e = e 1 ;:::;e n and n as before, then we de ne estimator ^ R 2 ( e ) of r as According to the follo wing lemma, we can actually cal-culate ^ R 2 directly from coun ting the num ber of ac-cepted tests out of the n exp erimen ts. So, ^ R 2 can be calculated ecien tly in linear time of the num ber of exp erimen ts.
 Lemma 3.2 Let e = e 1 ;:::;e n and n as befor e and i out of n tests be accepting the nul l hyp othesis, then ^
R 2 ( e ) = ^ R 2 ( i;n ) = Pro of: The numerator of ^ R 2 in (1) is the num ber of pairs with equal outcomes. If i (0 i n ) tests accept the null hypothesis and the remaining n i do not, then 2 pairs of rejecting pairs and jecting pairs can be formed. This giv es an estimate of No w, we will examine the bias and variance of ^ R 2 . It turns out that ^ R 2 is an unbiased estimator of replica-bilit y and its variance can be expressed in closed form. Theorem 3.1 ^ R 2 is an unbiase d estimator of replic a-bility r with varianc e 1 n ( n 1) (2( n 2)( n 3) F ( p; 4) + (4 2( n 3))( n 2) F ( p; 3)+( n 2)( n 3)+2) F ( p; 2) r 2 wher e F ( p;x ) = p x + (1 p ) x and p = 1 = 2 + 1 = 2 The pro of that ^ R 2 is unbiased closely follo ws that of Lemma 3.1. The pro of establishing the variance of ^ R 2 is rather tec hnical and is omitted here. A full pro of is available in the rep ort version of this pap er. Unfortunately , the closed form expression for the vari-ance of ^ R 2 is hard to interpret and compare with that of ^
R 1 . Figure 2 sho ws the variance of ^ R 1 and ^ R 2 for various values of replicabilit y r and num ber of exp eri-men ts n . It sho ws that the variance of ^ R 2 is equal to that of ^ R 1 when r = 1. This is when there is full repli-cabilit y and in this case the variance is zero. Ho wever, for other values of r , the variance of ^ R 2 is alw ays be-low that of ^ R 1 , indicating that ^ R 2 is a more ecien t estimator of replicabilit y than ^ R 1 . 3.3. Is there a better estimator? Is there an unbiased estimator of replicabilit y with lower variance than ^ R 2 based on exp erimen ts e = e ;:::;e n on a single database? We will consider the class of estimators based on linear functions of I ( e i = e j ).
 De nition: Let e = e 1 ;:::;e n ( n &gt; 0 and n even) be the outcomes of n exp erimen ts with di eren t ran-domizations on data set D . Then estimator ^ R k of r is Note that ^ R 1 is in this class with k i;i +1 = 1 n= 2 for odd i and k i;j = 0 otherwise. Lik ewise, ^ R 2 is in this class demand that ^ R k is unbiased, we put a restriction on the coecien ts k i;j expressed in the follo wing lemma. Lemma 3.3 ^ R k is an unbiase d estimator of replic abil-ity r i X In the pro of, we use the prop ert y that if r is the repli-cabilit y of an exp erimen t for a giv en data set, then, by de nition, r is the probabilit y two exp erimen ts pro-duce the same outcome. No w, two exp erimen ts use di eren t indep enden t randomizations. So, if p is the probabilit y that the outcome of a single exp erimen t is accept, then the replicabilit y is the probabilit y that two outcomes are accept ( p p ) plus the probabilit y that two outcomes are reject ((1 p ) (1 p )). So, r = p p + (1 p ) (1 p ), whic h can be solv ed for p giving p = 1 2 1 2 Pro of: For ^ R k to be an unbiased estimator of replica-bilit y r , we must have E ( ^ R k ) = r . No w, E ( ^ R inition of exp ectation is equals the order of sums, we get e ). Note that and e j only with probabilit y p 2 (both accept) and (1 p ) 2 (both rejects). So, ( p 2 + (1 p ) 2 ) k i;j = rk i;j . Summing over i and j giv es P equalit y follo ws from the condition that the estimator is unbiased. Consequen tly, So, ^ R 1 and ^ R 2 being unbiased (Lemma 3.1 and The-orem 3.1) can be pro ven observing ^ R 1 and ^ R 2 are in-stances of ^ R k and noting that the coecien ts k i;j add to 1.
 Theorem 3.2 var ( ^ R estimator ^ R k .
 Pro of: We determine the minim um of var ( ^ R k ) and sho w that ^ R 2 realizes the minim um. By de nition, var ( ^ R k ) equals E ( ^ R 2 k ) E ( ^ R k ) 2 . Since biased, E ( ^ R k ) = r so var ( ^ R k ) = E ( ^ R 2 k ) r P e P ( e ) At the minim um, dvar ( ^ R k ) =dk i;j = 0 for all 1 i &lt; j n . Taking deriv ativ es w.r.t. k a;b for any a;b suc h that ( a;b ) 6 = (1 ; 2) giv es dvar ( ^ R k ) =dk a;b = d computes as We can write ^ R k = e ) + k 1 ; 2 I ( e 1 = e 2 ) and use (3) to ing ^ R k = (1 the term d ^ R k ( e ) =dk a;b can be written as d P I ( e a = e b ) I ( e 1 = e 2 ). So dvar ( ^ R k ) =dk a;b P e P ( e )2 We need to distinguish two cases, namely a 2 and a &gt; 2. If a &gt; 2, dvar ( ^ R k ) =dk a;b is P e P ( e )2 e )) reduces to 2 k a;b ( p (1 p ) 3 + p 3 (1 p )) 2 k 1 ; 2 p ) 3 + p 3 (1 p )) where p = 1 this to equal zero, we have p = 0 or p = 1 coinciding with replicabilit y of r = 1, or k a;b = k 1 ; 2 . Lik ewise, if a 2 dvar ( ^ R k ) =dk a;b reduces to 2 k a;b ( p (1 p ) p (1 p )) 2 k 1 ; 2 ( p (1 p ) 2 + p 2 (1 p )). And again, we have r = 1 or k a;b = k 1 ; 2 .
 So, var ( ^ R k ) reac hes an optim um at k a;b = k 1 ; 2 a;b , whic h means all coecien ts are equal. And since they sum to 1, we have k a;b = 1 n ( n 1) = 2 since there are n ( n 1) = 2 coecien ts.
 The optim um is a minim um, as Figure 2 sho ws. In summary , Theorem 3.2 states that ^ R 2 is indeed an ecien t (i.e. unbiased with lowest variance) estimator in the class of unbiased estimators ^ R k . First, we establish whic h sampling scheme results in acceptable exp erimen ts based on Type I error and power. Then, we look at factors that impact replica-bilit y. To measure Type I error and power, algorithm A (naiv e Ba yes [5] as implemen ted in Weka 3.3 [9]) and algorithm B (C4.5 [7] as implemen ted in Weka with default parameters) were compared on syn thetic data and UCI data sets. The syn thetic data sets was generated using four data sources based on four ran-domly generated Ba yesian net works ([2] for more de-tails). The data sets con tained 10 binary variables and 50% class probabilit y. Eac h of the data sources were used to generate 1000 data sets with 300 instances. Data source 1 has mutually indep enden t variables, so there is no performance di erence between naiv e Ba yes and C4.5, whic h allo ws us to measure the Type I error. For sources 2, 3 and 4, C4.5 outp erforms naiv e Ba yes with increasing margin (on average 2.77%, 5.83% and 11.27% resp ectiv ely as measured on 10.000 instance test sets), whic h allo ws us to measure the power of tests. The sampling metho ds men tioned in Section 2.1 were performed 10 times with 10 folds and 10 runs at 5% signi cance lev el.
 Table 1 sho ws the results on the syn thetic data with num bers in brac kets indicating a 95% con dence inter-val. The rst six rows are for the rank sum test. Note that the use all data, average over folds and over runs sampling schemes have a Type I error over 50%, while a 5% Type I error is desired. The resampling scheme has an elev ated Type I error as has the 10 fold cross validation scheme. Only the sorted runs scheme sho ws an appropriate lev el of Type I error. This comes at the cost of decreased power compared to most of the other schemes.
 Table 1 also sho ws the minim um of the average repli-cabilit y over Set 1 to 4. It sho ws that resampling and 10-fold cv has a lev el of replicabilit y whic h is not ac-ceptable (belo w 50%). The schemes based on rep eated cross validation do sho w acceptable replicabilit y. In particular, the sorted runs scheme has a replicabilit y of over 80%. Results for the sign test and t-test are similar to the results for the rank sum test. Table 1 also sho ws Type I error and power for sorted runs with sign test and t test. Those gures are very close to the ones for the rank sum test, taking in ac-coun t that a sligh tly higher Type I error should lead to sligh tly better power. The replicabilit y for sorted runs with the sign test is 75.2% and with the t-test is 81.6%. Compared to the 80.6% for the rank sum test, the sign test performs considerably worse. This can be explained by the lac k of exploiting sizes of di erences in the sample by the sign test. The replicabilit y of the t-test is only sligh tly better.
 Further exp erimen ts were performed using the sorted runs sampling scheme while varying various parame-ters of the exp erimen t, namely signi cance lev el (1%, 2.5%, 5% and 10%), num ber of runs (10, 20, 30, 40, 50, 60, 70, 80, 90 and 100), class probabilit y for binary data (0.1, 0.2, 0.3, 0.4 and 0.5), class cardinalit y (2, 3 and 4), di eren t pairs of algorithms (out of Naiv e Ba yes, C4.5, nearest neigh bor, tree augmen ted naiv e Ba yes, decision stump and supp ort vector).
 Though space limitations prev ent us from presen ting the complete set of outcomes here, we can rep ort that the exp erimen ts resulted in a Type I error not exceed-ing the signi cance lev el by more than 1% with the sorted runs sampling scheme for all three tests con-sidered. Decreasing the class probabilit y increased replicabilit y. The explanation for this beha vior can be found in realizing that learners tend to predict the ma jorit y class the more this class dominates the data. Increasing the num ber of runs consisten tly increased replicabilit y. It app ears that the sorted runs sampling scheme results in a sample for whic h the indep endence assumption is not hea vily violated, so that no correc-tion in variance [6] or degrees of freedom [2] is required. Table 2 sho ws results for 27 data sets from the UCI rep ository [1] 1 using the sorted runs sampling scheme with the three di eren t types of tests. We compared naiv e Ba yes, C4.5 and nearest neigh bor (NB, C4.5 and NN resp ectiv ely in Table 2). Eac h algorithm was run ten times. The middle three columns sho w the num-ber of times that the exp erimen t decides that the null hypothesis is acceptable (so algorithms perform equal on a data set) as num bered in the footnote 1 . When the null hypothesis is 0 or 10 times accepted only a dot is sho wn, since both situations indicate perfect repli-cabilit y. The rst observ ation is that replicabilit y is an issue for non-syn thetic data sets, and thus a ects man y mac hine learning researc hers. Further, the sign test performs much worse than the other two tests, while the t-test sho ws marginally higher replicabilit y than the rank sum test. So, not only the sampling metho d, but also the hypothesis test has an impact on the replicabilit y of the exp erimen t. We de ned replicabilit y of mac hine learning exp eri-men ts in terms of probabilit y. This has the bene t that it allo ws for comparison over di eren t exp erimen-tal designs, unlik e a previous rather ad hoc de nition [2]. For example, replicabilit y measured on n rep eats of an exp erimen t can be compared with replicabilit y measure on 2 n rep eats. Furthermore, threshold e ects presen t in the ad hoc de nition are not presen t in our de nition.
 The main theoretical result of this pap er is the presen-tation of an estimator for replicabilit y that was sho wn to be unbiased and whic h has the lowest variance in its class. Using this estimator, we gathered empirical data to gain new insigh ts in how exp erimen tal designs in uence replicabilit y and found that the hypothesis test, the sampling scheme, and the class probabilit y impact replicabilit y. In our exp erimen ts, replicabil-ity consisten tly increased with sampling metho ds that dra w more samples from the same data set. Replica-bilit y app ears to be an issue both with syn thetic data sets as well as with UCI data sets. This indicates that mac hine learning researc her and data analysts should be wary when interpreting exp erimen tal results. The main practical outcome of the exp erimen ts is that judged on replicabilit y the sorted runs sampling scheme with the widely used t-test sho wed sup erior prop erties compared to the sign test and performed marginally better than the rank sum test. The sorted runs scheme is based on com bining accuracy estimates in a way that pro duces a represen tativ e sample of ac-curacy di erences of learning algorithms. Surprisingly , the sorted runs sampling schemes is the only scheme out of a set of popular schemes we considered that also sho wed acceptable Type I errors and reasonable power for a wide range of parameters using the three hypoth-esis tests considered. Consequen tly, exp erimen ts based on sorted runs sampling schemes do not require vari-ance corrections [6] or calibration of degrees of freedom [2]. In summary , based on replicabilit y, Type I error, power and theoretical considerations, we recommend using the sorted runs sampling scheme with a t-test for comparing classi ers on a small data set. One would exp ect that replicabilit y ceases to be an is-sue with larger data sets. In the future, we would like to perform larger scale exp erimen ts to get a better in-sigh t in the relation between replicabilit y, the num ber of samples tak en in an exp erimen t and data set size. This should also giv e a better insigh t in the relation between replicabilit y and Type I and II error. In this pap er, we considered mac hine learning exp eri-men ts in whic h we choose the best of two classi ers for a giv en data set. In practice, more than two classi ers are available. Also, mac hine learning researc hers rou-tinely compare algorithms over a large num ber of data sets. This leads to new replicabilit y issues and mul-tiple comparison problems, issues that require further researc h.
 Ac kno wledgemen ts I would like to thank the Mac hine Learning Group of Waik ato Univ ersit y for stim ulating discussions and the anon ymous review ers for their helpful commen ts.
