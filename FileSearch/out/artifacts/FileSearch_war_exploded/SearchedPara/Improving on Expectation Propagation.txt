 types of Bayesian models are considered.
 be trusted. algorithms converge.
 theory, we will skip them in our notation. It is assumed that p ( x ) factorizes into a product of terms f where the normalising partition function Z = R d x Q an approximation to p ( x ) in the form where the terms g the optimal parameters of the g via Here a single approximating term g placement leaves q and all q in one of the applications in the last section.
 by solving for f Hence Z = R d x Q This shows that corrections to EP are small when all distribu tions q the optimality criterion of EP. For related expansions, see [2, 3, 9]. sions are be presented in this section. 3.1 Expansion I: Clusters The most basic expansion is based on the variables  X  correction to the partition function brackets h . . . i P n h  X  n ( x ) i q = 0 of corrections is feasible when q of In a similar way, we get 3.2 Expansion II: Cumulants where K is the kernel matrix. In this case we have N +1 terms f g observation which depends only on a single component x tributions q expand with respect to the higher order cumulants of the dist ributions q To derive this expansion, we simplify (6) using the fact that q ( x ) = q ( x the marginals of q ( x ) and q Since q ( x higher cumulants of the q c Expressing the Gaussian marginals q ( x the variances S which contains the contributions of all higher order cumula nts, we get where in the last equality we have introduced a shift of varia bles  X  An expansion can be performed with respect to the cumulants i n the terms g R to the partition function. 3.2.1 Correction to the partition function z Assuming that the g ln R = ln Here we have repeatedly used the fact that each factor z (by Wick X  X  theorem) with a factor z nonzero contributions only, when l = s and there are l ! ways for pairing. 2 ances S where N is the number of variables x tions may not scale with N . 3.2.2 Correction to marginal moments include a new latent variable x  X  with E [ x  X  x ] = k  X  and E [ x 2 likelihood term over the posterior marginal of x  X  .
 averaging the conditional distribution p ( x  X  | x ) = N ( x  X  ; k  X  Using the expression (15) we obtain (where we set R = 1 in (6) to lowest order) p ( x  X  ) = where the parameters in the Gaussian terms g expectation over the complex Gaussian variables  X  we obtain p ( x  X  ) = N ( x  X  ; x where h density where the only occurrence of x  X  is through ( x  X   X  over (21). 4.1 Mixture of Gaussians Consider N observed data points  X  Wisharts densities so that f chosen to be of the same exponential family form as f normalizable.
 to the log marginal likelihood ln Z for  X  original approximation might be inadequate. 4.2 Gaussian Process Classification The GP classification model arises when we observe N data points  X  true correction ln R , using the USPS data set from [4]. likelihood terms for y Normal density.
 characteristic function of q where v  X  2 = 1 /S The characteristic function of q with expectations h i being with respect to q tives of the characteristic function, i.e. h x j from the derivatives of ln  X  2 h x n i 3  X  3 h x n ih x 2 n i + h x 3 n i ), such that where  X  = v 2 / used by [4]. We used the same kernel k (  X  ,  X   X  ) =  X  2 exp(  X  1 of EP for GPC. are positive and large compared to the posterior variance, n on-Gaussian terms f for almost all values of x is proportional to q ( x ) /g for | x variables  X  increase with sample size. 4.3 Ising models a  X  X aussian X  f definite), makes this GP model an Ising model with binary vari ables x The tilted distributions q m styles are the same as in the left plot. described in detail in [8]. In the first scenario, with N = 10 , the J at random according to J deviation (MAD) of the estimated covariance matrices from t he exact one max neighbors in a 4 X  X y X 4 grid. The external field (observation) strengths  X  distribution  X  considered: repulsive (anti-ferromagnetic) J and attractive (ferromagnetic) J situation, the correction might worsen the results.
 only one spanning tree approximating term (EP tree).
 References
