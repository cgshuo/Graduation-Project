 Hashing techniques have been extensively investigated to boost similarity search for large-scale high-dimensional data. Most of the existing approaches formulate the their objec-tive as a pair-wise similarity-preserving problem. In this paper, we consider the hashing problem from the perspec-tive of optimizing a list-wise learning to rank problem and propose an approach called List-Wise supervised Hashing (LWH). In LWH, the hash functions are optimized by em-ploying structural SVM in order to explicitly minimize the ranking loss of the whole list-wise permutations instead of merely the point-wise or pair-wise supervision. We evalu-ate the performance of LWH on two real-world data sets. Experimental results demonstrate that our method obtains a significant improvement over the state-of-the-art hashing approaches due to both structural large margin and list-wise ranking pursuing in a supervised manner.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval Hashing; Learning to rank; Structural SVM
With the rapid development of the Internet and social networks, an increasing number of multimedia data are pro-duced at every moment, e.g., images and videos. Given a query example, retrieving relevant samples from large-scale database has become an emergent need in many practical applications. An effective way to speed up the similarity search is the hashing technique. The hashing techniques often make a tradeoff between accuracy and efficiency and therefore relax the nearest neighbor search to approximate nearest neighbor (ANN) search. The underlying motivation of the hashing techniques is to map high-dimensional data into compact hash codes so that similar data have the same or similar hash codes.
 One of the well-known hashing methods is the Locality Sensitive Hashing (LSH) [1], which uses random projections to obtain the hash functions. However, due to the limitation of random projection, LSH usually needs a quite long hash code to guarantee good retrieval performance. To alleviate this weakness, several data-dependent learning based meth-ods are proposed. Spectral Hashing (SH) [11] exploits the distribution of the training data and uses eigenfunction to obtain the hash functions. Compared with LSH, SH achieves better performance since the learned hash functions better disentangle the manifold structure of the data set.
Since semantic similarity is usually given in terms of la-beled information (e.g., annotated tags or categories), semi-supervised or supervised learning algorithms have been adapt-ed to devise hash functions in order to make the learned hash functions preserve the semantic similarity of the data set [9]. From the learning to rank (LTR) point of view, these afore-mentioned methods can be regarded as the point-wise super-vised hashing since the supervision is on the instance-level labels. However, it is observed that the point-wise hashing methods are limited by the data distribution and the out-lier data points may deteriorate the retrieval performance. To overcome this limitation, Norouzi et al. introduced a triplet supervision into hashing [6]. In this method, each triplet { x i , x j , x k } is conducted to indicate the data point x is more relevant to the data point x j than the data point x . This kind of relative similarity in terms of triplets less rely on the data distribution and thus generates more ro-bust hash functions. Their method can be classified into the category of pair-wise supervised hashing. Although the point-wise and pair-wise hashing methods achieve promis-ing performance, their objectives are sub-optimal for the retrieval task since the ranking loss of the whole list-wise permutation is disregarded. Therefore, we attempt to in-troduce the list-wise ranking supervision (i.e., given queries and their corresponding true ranking lists) into hash func-tion learning, which corresponds to the list-wise supervised hashing. Since the main target of hashing is to retrieve relevant results, the list-wise supervised hashing method is more straightforward when comparing with point-wise and pair-wise hashing approaches. To the best of our knowledge, there are only several approaches focus on list-wise hashing. Wang et al. proposed a hashing method supervised with the list-wise ranking information when learning hash func-tions [10]. However, in their optimization, the total ranking lists are transformed into various triplets, and the hashing function is in fact learned in a pair-wise supervised hashing m anner. As a result,this transformation potentially weakens the ranking information. Furthermore, when the size of the ranking list is large, the tremendous combinations of triplets will make the problem intractable.

In this paper, we propose a list-wise supervised hashing approach named LWH according to the given total ranking list. Unlike [10] which transform the total ranking list into triplets, we directly learn the hash functions to optimize the ranking evaluation measures of the total ranking list (e.g., MAP [12], NDCG[2]). In LWH, the hash functions are learned by structural SVM [8] in order to explicitly minimize the ranking loss of the whole list-wise permutation.
In this section, we first define notations that will be used in this paper and briefly describe the problem to be solved. Then we propose a list-wise supervised framework to learn hash functions. Finally, we overview the entire framework and clarify some implementation details.
Assume the training set X = { x i } N i =1 has N data points with each x i  X  R d corresponds to a d -dimensional feature vector. The training set we used is in the query vs. retrieved ranking list format. We have a query set Q = { q m } M m =1 consisting of M queries. For each query q  X  X  , we can derive its true ranking y  X   X  X  over X , where Y denotes the set of all possible rankings. We formulate a ranking as a matrix of pair orderings as [12] does and Y  X  X  +1 , 0 ,  X  1 } N  X  N any y  X  X  , y ij = +1 if x i is ranked before x j , and y ij if x j is ranked before x i , and y ij = 0 if x i and x j rank. The true ranking y  X  for each query q is assumed to be a weak ranking in this paper, which indicates two levels of relevancy (relevant or irrelevant) w.r.t. q . Therefore, for any query q , its true ranking list consists of two disjoint sets X q and X  X  q representing the relevant and irrelevant results, respectively.

After given the true ranking y  X  (in terms of semantic sim-ilarity ) of each query q in training set, we attempt to learn hash functions H : R d  X  { X  1 , 1 } k , where k is the dimen-sionality of Hamming space with k &lt; d , to make the ranking generated by the hash codes consistent with y  X  .
Similar with most of the hashing approaches, we adopt a linear formulation for designing hash functions. Assume that the data have been zero-centered, the hash functions H ( ) are defined as follows: where W = [ w 1 , ..., w k ]  X  R d  X  k is a linear projection matrix and B ( ) is a binarization function that maps real values into binary values. Following the strategy which is common used by [11][9], we simply use the sgn function with threshold at 0, i.e., B ( x ) = sgn( W T x ).
 To measure the similarity of two hash codes H ( q ) and H ( x ), we use the following cosine-similarity based function which has been proved to generate the identical ranking with one based the Hamming distance [10].
B y adapting the partial order feature representation, we define a scoring function F parameterized by the hash func-tions H ( ) to measure how compatible the ranking based on hash codes are with the current predicted ranking y :
Here, the scoring function F ( q, X, y ) is a summation over the differences of all pairs of relevant-irrelevant results. More-over, the scoring function F has a property that if the hash functions H ( ) (i.e., W ) is fixed, the ranking y that maxi-mizes F ( q, X, y ) is simply obtained in descending order of rule exactly satisfies the hash functions we expect to learn the ascending order of the Hamming distance for each hash code H ( x i ) from the query hash code H ( q ).

However, due to the binary constraint of the hash func-tions, Eq.(1) is non-differentiable and hard to optimize. Fol-lowing the strategy commonly used in the hashing approach-es such as [11, 5], we relax the binarization function. In this way, Eq.(1) can be rewritten as: where &lt; A, B &gt; F = tr( A T B ) and
By representing the scoring function F as a Frobenius inner product of W T W and  X ( ), we find that it is straight-forward to conduct structural SVM [8] to learn F as well as the projection matrix W of the hash functions.
 For the purpose of listwise learning to rank, the structural SVM takes a set of vector-valued features which character-ize the relationship between the input query and a set of retrieved data as the input, and predicts a ranking y  X  X  of the retrieved data. The structural SVM is applied to max-imize the margins between the true ranking y  X  and all the other possible rankings y .

For each q  X  X  , we have: where  X F ( q, X, y  X  , y ) = F ( q i , X, y  X  i )  X  F ( q for simplicity.  X ( y  X  , y )  X  [0 , 1] is the non-negative loss func-tion to reflect the similarity of two ranking y  X  and y . Nat-urally, we can use many criteria to define the loss function  X , such as  X  map [12],  X  ndcg [2], etc. Empirically, we choose  X  map due to its superior effectiveness and robustness over the other criteria observed by [2].

Following the structural SVM framework, our LWH learn-s the optimal W which maximizes the margins between the true ranking and all other possible rankings. Specifically, w e i n structural SVM with  X  2 k W k 2 F t o obtain a better general-ization performance of the learned hash functions.
The overall objective function of the proposed LWH is formulated as follows: where  X  is a slack variable over all the constrains. Compared with the traditional SVM with n -slacks, the 1-slack method shares a single slack variable  X  across all constraint batches, which leads to less computation time.
For each triplet ( q i , X, y  X  i ) in the training set, there are |Y| possible permutations which is super-exponential w.r.t. N , thus can not be exactly optimized. To make this problem tractable, we adopt the cutting-plane algorithm [3] to solve the objective problem in Eq.(2).

The objective function in Eq.(2) is optimized by an it-erative mechanism between the following two steps alterna-tively: 1) optimizing W with the current constraints set con-sisting of batches of rankings ( y 1 , ..., y M ) which most violate the current constraints. 2) updating the constraints set and adding a new batch which is mostly violated by current W . The iterative procedure terminates once the empirical loss on the new constraint batch is within a pre-defined tolerance  X  on the current constraints set. The overall optimization of LWH is listed in Algorithm 1.
 Algorithm 1 T he Optimization of LWH 1: Initialize the constraints set W  X  X  X  2: repeat 3: 4: for i = 1 to M do 5:  X  y i  X  argmax 6: end for 7: W  X  X  X  X  (  X  y 1 , ...,  X  y M ) } 8: until Output: The optimized projection matrix W
The main parts of the optimization in Algorithm 1 is the m inimization in step 3 and the searching of the most violated constraints in step 5. For the problem in step 5, different loss functions  X ( y  X  , y ) lead to different solutions. Recall that we use  X  map in this paper, the work of [12] can be easily applied to solve our problem of the step 5.
For the minimization problem of step 3, we implement a sub-gradient descent solver adapted from Pegasos algorithm [7], which is very efficient to solve the primal SVM problem.
The experiments show that the optimization yields a fast convergence rate. For a fixed tolerance  X  = 0 . 01, Algorithm 1 always terminates within 50 iterations in our experiments.
Since our LWH is derived from structural SVM, we do not further analyze the complexity in this paper. The detailed complexity analysis can be referred to [8].
In this section, we conduct experiments and comparison WH, we compare LWH with the state-of-the-art hashing approaches.
The CIFAR-10 data set contains 60,000 tiny images that have been manually grouped into 10 concepts (e.g., airplane, bird, cat, deer). The images are 32  X  32 pixels and we represent them with 512-D GIST descriptors. Given a query image, the images sharing the same concept with the query image are regarded as the relevant ones.

The NUS-WIDE data set contains 269,648 labeled images and is manually annotated with 81 concepts. The images are represented with 500-D Bag of Visual Words (BOVW). Giv-en a query image, the images sharing at least one common concept with the query image are regarded as the relevant ones.
We compare the proposed LWH with six stat-of-the-art hashing methods including four unsupervised methods LSH [1], SH [11], AGH [5], KLSH [4], one supervised(semi) method SSH [9], and one list-wise supervised method RSH [10].
Except for the LSH and KLSH method which do not need training samples, for the unsupervised methods (i.e., SH and AGH), we randomly sample 3000 data points as the training set; for the point-wise supervised method SSH, we addition-ally sample 1000 data points with their concept labels; for the list-wise supervised methods (i.e., RSH and LWH), we randomly sample 300 query samples from the 1000 labeled samples to compute the true ranking list. In the test stage, we use 2000 random samples as queries and the rest samples as the database set to evaluate the retrieval performance. To evaluate the performance, we adopt four criteria, i.e., Mean Average Precision (MAP), Precision within Hamming distance 2 (PH2), Precision vs. Recall (PR), Recall vs. top retrieved examples (Recall), respectively.

The parameters of all the comparing methods are tuned on the validation set to achieve the best performance. For LWH, the only parameter is the trade-off parameter  X  in for both data sets in our experiments. h ttp://www.cs.toronto.edu/~kriz/cifar.html http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm
Figure 1 and 2 demonstrate the performance of LWH a-gainst the other comparative methods on CIFAR-10 and NUS-WIDE data sets, respectively.

From the results, we have the following observations: 1) the results on both data sets show that LWH outperform-s the other methods significantly; 2) supervised or semi-supervised approaches such as SSH, RSH and LWH outper-form the unsupervised approaches. The observation may be explained as hashing function learned by the unsuper-vised approaches only preserve the similarity of original da-ta, but do not actually preserve the semantic similarity; 3) among the supervised approaches, the methods incorporate list-wise supervision (i.e., LWH and RSH) have a remarkable enhancement due to their aptitude to minimize the listwise ranking loss.
In this paper, we propose a hashing approach leveraging the list-wise supervision in a max-margin learning manner. In LWH, the hash functions are optimized by employing structural SVM in order to explicitly minimize the rank-ing loss of the whole list-wise permutation. The results over two real-world data sets demonstrates the superiority of the proposed LWH over the existing state-of-the-art hashing ap-proaches.
This work was supported in part by 973 program (No. 2010CB327904), Chinese Knowledge Center of Engineer-ing Science and Technology (CKCEST), NSFC (No. 61105074, 61103099), 863 program(2012AA012505) , Program for New Century Excellent Talents in University and Zhejiang Provin-cial Natural Science Foundation of China (No. LQ13F020001, LQ14F010004). [1] A. Andoni and P. Indyk. Near-optimal hashing [2] S. Chakrabarti, R. Khanna, U. Sawant, and [3] T. Joachims, T. Finley, and C.-N. J. Yu.
 [4] B. Kulis and K. Grauman. Kernelized [5] W. Liu, J. Wang, S. Kumar, and S. Chang. Hashing [6] M. Norouzi, D. Fleet, and R. Salakhutdinov.
 [7] S. Shalev-Shwartz, Y. Singer, N. Srebro, and [8] I. Tsochantaridis, T. Joachims, T. Hofmann, and [9] J. Wang, S. Kumar, and S. Chang. Semi-supervised [10] J. Wang, W. Liu, A. Sun, and Y. Jiang. Learning hash [11] Y. Weiss, A. Torralba, and R. Fergus. Spectral [12] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A
