 A recent trend in statistical machine translation (SMT) has been the use of synchronous grammar based formalisms, permitting polynomial algorithms for exploring exponential forests of translation options. Current state-of-the-art synchronous grammar translation systems rely upon heuristic rel-ative frequency parameter estimates borrowed from phrase-based machine translation[1, 2]. In this work we draw upon recent Bayesian models of monolingual parsing [3, 4] to develop a generative synchronous grammar model of translation using a hierarchical Dirichlet process (HDP) [5]. There are two main contributions of this work. The first is that we include sparse priors over the model parameters, encoding the intuition that source phrases will have few translations, and also ad-dressing the problem of overfitting when using long multi-word translations pairs. Previous models have relied upon heuristics to implicitly bias models towards such distributions [6]. In addition, we investigate different priors based on standard machine translation models. This allows the perfor-mance benefits of these models to be combined with a principled estimation procedure.
 Our second contribution is the induction of categories for the synchronous grammar using a HDP prior. Such categories allow the model to learn the latent structure of translational equivalence be-tween strings, such as a preference to reorder adjectives and nouns when translating between French to English or to encode that a phrase pair should be used at the beginning or end of a sentence. Au-tomatically induced non-terminal symbols give synchronous grammar models increased power over single non-terminal systems such as [2], while avoiding the problems of relying on noisy domain-specific parsers, as in [7]. As the model is non-parametric, the HDP prior will provide a bias towards parameter distributions using as many, or as few, non-terminals as necessary to model the training data. Following [3] we optimise a truncated variational bound on the true posterior distribution. English, showing improvements over a maximum likelihood estimate (MLE) model. We focus on modelling the generation of a translation for a source sentence, putting aside for further work integration with common components of a state-of-the-art translation system, such as a language model and minimum error rate training [6].
 While we are not aware of any previous attempts to directly induce synchronous grammars with more than a single category, a number of generatively trained machine translation models have been Figure 1: An example SCFG derivation from a Chinese source sentence which yields the English sentence:  X  X tanding tall on Taihang Mountain is the Monument to the Hundred Regiment Offensive. X  (Cross-bars indicate that the child nodes have been reordered in the English target.) proposed. [8] described the ITG subclass of SCFGs and performed many experiments using MLE training to induce translation models on small corpora. Most subsequent work with ITG grammars has focused on the sub-task of word alignment [9], rather than actual translation, and has continued to use MLE trained models. A notable recent exception is [10] who used Dirichlet priors to smooth an ITG alignment model. Our results clearly indicate that MLE models considerably overfit when used to estimate synchronous grammars, while the judicious use of priors can alleviate this problem. previously dismissed for under-performing heuristic approaches, should be revisited. A synchronous context free grammar (SCFG, [13]) describes the generation of pairs of strings. X  X  X   X , X ,  X  X  , where X is a non-terminal,  X  and  X  are strings of terminals and non-terminals and  X  specifies a one-to-one alignment between non-terminals in  X  and  X  . In the context of SMT, by assigning the source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence, while generating the target translation [2]. In this paper we only consider binary normal-form SCFGs which allow productions to rewrite as either a pair of a pair of non-terminals, or a pair of non-empty terminal strings (these may span multiple words). Such grammars are equivalent to the inversion transduction grammars presented in [8]. Note however that our approach is general and could be used with other synchronous grammar transducers (e.g., [7]). The binary non-terminal productions can specify that the order of the child non-terminals is the same in both languages (a monotone production), or is reversed (a reordering production). Monotone and reordering rules are written: respectively, where X,Y and Z are non-terminals and the boxed indices denote the alignment. Without loss of generality, here we add the restriction that non-terminals on the source and target sides of the grammar must have the same category. Although conceptually simple, a binary normal-form SCFGs can still represent a wide range of linguistic phenomena required for translation [8]. Figure 1 shows an example derivation for Chinese to English. The grammar in this example has non-terminals A and B which distinguish between translation phrases which permit re-orderings. A sequence of SCFG rule applications which produces both a source and a target sentence is referred to as a derivation , denoted z . The generative process of a derivation in our model is described in Table 1. First a start symbol, z 1 , is drawn, followed by its rule type. This rule type determines if the symbol will rewrite as a source-target translation pair, or a pair of non-terminals with either monotone or reversed order. The process then recurses to rewrite each pair of child non-terminals.  X  |  X   X  GEM (  X  ) (Draw top-level constituent prior distribution)  X  S |  X  S , X   X  DP (  X  S , X  ) (Draw start-symbol distribution)  X  T z |  X  Y  X  Dirichlet (  X  Y ) (Draw rule-type parameters)  X  M z |  X  M , X   X  DP (  X  M , X  X  T ) (Draw monotone binary production parameters)  X  R z |  X  R , X   X  DP (  X  R , X  X  T ) (Draw reordering binary production parameters)  X  E z |  X  E ,P 0  X  DP (  X  E ,P 0 ) (Draw emission production parameters) z 1 |  X  S  X  Multinomial (  X  S ) (First draw the start symbol)
For each node i in the synchronous derivation z with category z i : Table 1: Hierarchical Dirichlet process model of the production of a synchronous tree from a SCFG. This continues until no non-terminals are remaining, at which point the derivation is complete and the source and target sentences can be read off. When expanding a production each decision is drawn from a multinomial distribution specific to the non-terminal, z i . This allows different non-terminals to rewrite in different ways  X  as an emission, reordering or monotone production. The prior distribution for each binary production is parametrised by  X  , the top-level stick-breaking weights, thereby ensuring that each production draws its children from a shared inventory of category labels. The parameters for each multinomial distributions are themselves drawn from their corresponding prior. The hyperparameters,  X , X  S , X  Y , X  M , X  R , and  X  E , encode prior knowledge about the sparsity of each distribution. For instance, we can encode a preference towards longer or short derivations using  X  Y , and a preference for sparse or dense translation lexicons with  X  E . To simplify matters we assume a single hyperparameter for productions, i.e.  X  P  X  =  X  S =  X  M =  X  R . In addition to allowing for the incorporation of prior knowledge about sparsity, the priors have been chosen to be conjugate to the multinomial distribution. In the following sections we describe and motivate our choices for each one of these distributions. 3.1 Rule type distribution The rule type distribution determines the relative likelihood of generating a terminal string pair, a monotone production, or a reordering. Synchronous grammars that allow multiple words to be translation pairs, i.e. if a training set sentence pair can be explained by many short translation pairs, or a few long ones the maximum likelihood solution will be to use the longest pairs. This issue is manifested by the rule type distribution assigning a high probability to emissions versus either of the binary productions, resulting in short flat derivations with few productions. We can counter this tendency by assuming a prior distribution that allows us to temper the model X  X  preference for short derivations with large translation pairs. We do so by setting the concentration parameter,  X  Y , to a number greater than one which smooths the rule type distribution. 3.2 Emission distribution The Dirichlet process prior on the terminal emission distribution serves two purposes. Firstly the prior allows us to encode the intuition that our model should have few translation pairs. The trans-lation pairs in our system are induced from noisy data and thus many of them will be of little use. Therefore a sparse prior should lead to these noisy translation pairs being assigned probabilities close to zero. Secondly, the base distribution P 0 of the Dirichlet process can be used to include sophisticated prior distributions over translation pairs from other popular models of translation. The two structured priors we investigate in this work are IBM model 1, and the relative frequency count estimators from phrase based translation: IBM Model 1 ( P m 1 0 ) IBM Model 1 [14] is a word based generative translation model that assigns a joint probability to a source and target translation pair. The model is based on a noisy channel in which we decompose the probability of f given e from the language model probability of e . The conditional model assumes a latent alignment from words in e to those in f and that the probability of word-to-word translations are independent: where e 0 represents word insertions. We use a unigram language model for the probability P ( e ) , and Section 3.4.
 Model 1 allows us to assign a prior probability to each translation pair in our model. This prior suggests that lexically similar translation pairs should have similar probabilities. For example, if the French-English pairs (chapeau, cap) and (rouge, red) both have high probability, then the pair (chapeau rouge, red cap) should also.
 Relative frequency ( P RF 0 ) Most statistical machine translation models currently in use estimate the probabilities for translation pairs using a simple relative frequency estimator. Under this model the joint probability of a translation pair is simply the number of times the source was observed to be aligned to the target in the word aligned corpus normalised by the total number of observed pairs: where C (  X  ,  X  ) is the total number of translation pair alignments observed. Although this estimator doesn X  X  take into account any generative process for how the translation pairs were observed, and by extension of the arguments for tree substitution grammars is biased and inconsistent [15], it has proved effective in many state-of-the-art translation systems. 1 3.3 Non-terminal distributions We employ a structured prior for binary production rules inspired by similar approaches in mono-lingual grammar induction [3, 4]. The marginal distribution over non-terminals,  X  , is drawn from a stick-breaking prior [5]. This generates an infinite vector of scalars which sum to one and whose rameters of the start symbol distribution are drawn from a Dirichlet process parametrised by the stick-breaking weights,  X  . In addition, both the monotone and reordering production parameters are drawn from a Dirichlet process parameterised by the matrix of the expectations for each pair of non-terminals,  X  X  T , assuming independence in the prior. This allows the model to prefer grammars with few non-terminal labels and where each non-terminal has a sparse distribution over productions. 3.4 Inference Previous work with monolingual HDP-CFG grammars have employed either Gibbs sampling [4] or variational Bayes [3] approaches to inference. In this work we follow the mean-field approximation presented in [16, 3], truncating the top-level stick-breaking prior on the non-terminals and optimising a variational bound on the probability of the training sample. The mean-field approach offers better scaling and convergence properties than a Gibbs sampler, at the expense of increased approximation. First we start with our objective, the likelihood of the observed string pairs, x = { ( e , f ) } : where  X  = (  X , X  S , X  M , X  R , X  E , X  T ) are our model parameters and z are the hidden derivations. We bound the above using Jensen X  X  inequality to move the logarithm (a convex function) inside the integral and sum, and introduce the mean-field distribution q (  X , z ) . Assuming this distribution factorises over the model parameters and latent variables, q (  X , z ) = q (  X  ) q ( z ) , obtain sub-normalised summary weights for each of the factorised variational distributions: W where C ( z  X   X  X  X  ) is the expected count of rewriting symbol z using the given production. The starred rewrites in the denominators indicate a sum over any monotone or reordering production, respectively. The weights for the rule-type and emission distributions are defined similarly. The variational training cycles between optimising the q (  X  ) distribution by re-estimating the weights W and the stick-breaking prior  X  , then using these estimates, with the inside-outside dynamic program-ming algorithm, to calculate the q ( z ) distribution. Optimising the top-level stick-breaking weights has no closed form solution as a dependency is induced between the GEM prior and production distributions. [3] advocate using a gradient projection method to locally optimise this function. As our truncation levels are small, we instead use Monte-Carlo sampling to estimate a global optimum. 3.5 Prediction The predictive distribution under our Bayesian model is given by: p ( z | x , f ) = where x is the training set of parallel sentence pairs, f is a testing source sentence and z its deriva-tion. 2 Calculating the predictive probability even under the variational approximation is intractable, therefore we bound the approximation following [16]. The bound can then be maximised to find the best derivation, z , with the Viterbi algorithm, using the sub-normalised W parameters from the last E step of variational Bayes training as the model parameters. We evaluate our HDP-SCFG model on both synthetic and real-world translation tasks.
 Recovering a synthetic grammar This experiment investigates the ability of our model to recover a simple synthetic grammar, using the minimum number of constituent categories. Ten thousand training pairs were generated from the following synthetic grammar, with uniform weights, which includes both reordering and ambiguous terminal distributions: Figure 2: Synthetic grammar experiments. The HDP model correctly allocates a single binary production non-terminal and three equally weighted emission non-terminals.
 Figure 2 shows the emission and production distributions produced by the HDP-SCFG model, 3 as well as an EM trained maximum likelihood (MLE) model. The variational inference for the HDP model was truncated at five categories, likewise the MLE model was trained with five categories. The hierarchical model finds the correct grammar. It allocates category 2 to the S category, giving the emission distribution the HDP model assigns category 1 to A , 3 to B and 5 to C , each of which has a posterior probability of 1 3 . The stick-breaking prior biases the model towards using a small set of categories, and therefore the model correctly uses only four categories, assigning zero posterior probability mass to category 4 .
 The MLE model has no bias for small grammars and therefore uses all available categories to model the data. For the production distribution it creates two categories with equal posteriors to model the S category, while for emissions the model collapses categories A and C into category 1 , and splits category B over 3 and 5 . This grammar is more expressive than the target grammar, over-generating but including the target grammar as a subset. The particular grammar found by the MLE model is dependent on the (random) initialisation and the fact that the EM algorithm can only find a local maximum, however it will always use all available categories to model the data.
 Chinese-English machine translation The real-world translation experiment aims to determine whether the model can learn and generalise from a noisy large-scale parallel machine translation corpus, and provide performance benefits on the standard evaluation metrics. We evaluate our model on the IWSLT 2005 Chinese to English translation task [17], using the 2004 test set as development data for tuning the hyperparameters. The statistics for this data are presented in Table 2. The training data made available for this task consisted of 40k pairs of transcribed utterances, drawn from the travel domain. The translation phrase pairs that form the base of our grammar are induced using the standard alignment and translation phrase pair extraction heuristics used in phrase-based translation models [6]. As these heuristics aren X  X  based on a generative model, and don X  X  guarantee that the target translation will be reachable from the source, we discard those sentence pairs for which we cannot produce a derivation, leaving 33,164 sentences for training. Model performance is evaluated using the standard Bleu4 metric [18] which measures average n -gram precision, n  X  4 . Figure 3: Tuning the Dirichlet  X  parameters for the emission and rule type distributions (develop-ment set).
 Table 3: Test results for the model with a single non-terminal category and various emission priors ( B Table 4: Test set results for the hierarchical model with the variational distribution truncated at five non-terminal categories ( B LEU ).
 We first evaluate our model using a grammar with a single non-terminal category (rendering the hierarchical prior redundant) and vary the prior P 0 used for the emission parameters. For this model we investigate the effect that the emission and rule-type priors have on translation performance. Figure 3 graphs the variation in Bleu score versus the two free hyperparameters for the model with a simple uniform P 0 , evaluated on the development corpus. Both graphs show a convex relationship, with  X  Y being considerably more peaked. For the  X  E hyperparameter the optimal value is 0 . 75 , indicating that the emission distribution benefits from a slightly sparse distribution, but not far from the uniform value of 1 . 0 . The sharp curve for the  X  Y rule-type distribution hyperparameter confirms our earlier hypothesis that the model requires considerable smoothing in order to force it to place probability mass on long derivations rather than simply placing it all on the largest translation pairs. The optimal hyperparameter values on the development data for the two structured emission distri-bution priors, Model 1 ( M 1 ) and relative frequency ( RF ), also provide insight into the underlying models. The M 1 prior has a heavy bias towards smaller translation pairs, countering the model X  X  inherent bias. Thus the optimal value for the  X  Y parameter is 1 . 0 , suggesting that the two biases balance. Conversely the RF prior is biased towards larger translation pairs reinforcing the model X  X  bias, thus a very large value ( 10 6 ) for the  X  Y parameter gives optimal development set performance. Table 3 shows the performance of the single category models with each of the priors on the test set. 4 The results show that all the Bayesian models outperform the MLE, and that non-uniform priors help considerably, with the RF prior obtaining the highest score.
 In Table 4 we show the results for taking the best performing RF model from the previous experi-ment and increasing the variational approximation X  X  truncation limit to five non-terminals. The  X  P was set to 1 . 0 , corresponding to a sparse distribution over binary productions. 5 Here we see that the HDP model improves slightly over the single category approximation. However the baseline MLE model uses the extra categories to overfit the training data significantly, resulting in much poorer generalisation performance. We have proposed a Bayesian model for inducing synchronous grammars and demonstrated its effi-cacy on both synthetic and real machine translation tasks. The sophisticated priors over the model X  X  parameters address limitations of MLE models, most notably overfitting, and effectively model the nature of the translation task. In addition, the incorporation of a hierarchical prior opens the door to the unsupervised induction of grammars capable of representing the latent structure of translation. Our Bayesian model of translation using synchronous grammars provides a basis upon which more sophisticated models can be built, enabling a move away from the current heuristically engineered translation systems.

