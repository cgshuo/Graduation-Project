 Support Vector Machine (SVM) is a practical implementation of structural risk mini-related to Vapnik-Chervonenkis dimension of the learning machine. The minimization space of SVM is only an approximation to the hypothesis space because of statistical, computational and representational reasons. Ensemble learning is a promising method ensemble is a set of classifiers whose individual decisions are combined in some way direction of current machine learning [1] [2 ]. The idea of ensemble has been applied for SVM by Kim [3] [4]. Now, SVM ensemble is widely used in many fields, such as news audio classification [5], gene expressi on analysis [6], cancer recognition [7] and fault diagnoses [8], etc. 
In order to constructing a good SVM ensemble, two main problems should be learning, so this paper mainly focuses on it. Many methods of constructing ensembles have been developed, such as Bagging [9], Adaboost [10] and k-fold cross validation method. These techniques are very effective for unstable learning algorithms, such as neural network, decision tree and rule-based learning algorithm. However, these methods have some randomness and unrationality because they don X  X  consider the these drawbacks, we put forward a new SVM ensemble based on clustering analysis. In this method, the training samples are firstly been clustered with subtractive cluster-components are combined with majority voting method to classify new examples. training subsets generated by this method may bitterly reflect the actual sample distri-bution comparing to existing methods. So the SVM ensemble constructed by our method has higher accuracy. 
The rest of this paper is organized as follows: section 2 provides a brief review for subtractive clustering algorithm; the main idea and detailed steps for SVM ensemble results and analysis of our method on synthetic and UCI datasets; finally, we draw a conclusion in section 6. ( , ), ( , ), , ( , ) nn xy x y x y L .Each training sample m i xR  X  belongs to either of two samples, a SVM will search for a separating hyper-plane with the largest margin. The optimal hyper-plane problem is then regarded as the solution to the following problem rameter can make balance between margin maximization and classification violation. The solution of this problem can be obtained by solving its dual formulation: where not linear separable in the input space, we need to map them into a high dimensional For this purpose, we only need to change (2) into the forms as following: where (, ) k  X  X  X  is the kernel function. Clustering analysis is an unsupervised learning method, by which we can master some algorithms, it is a hard problem to decide the number of k or the number of competi-ples based on the density of surrounding data points. The algorithm goes as follows: into a unit hyper box to make each dimension identical. of each point. cluster center. Note the first cluster center as * 1 x and its potential is * 1 P . where 0 &gt; b r defines the neighborhood of a cluster center with which the existence of other cluster centers are discouraged. To avoid closely space centers, usu-is the next candidate cluster center. when the current highest potential * k P is far smaller than * 1 P . 4.1 The SVM Ensemble Method Based on Clustering Analysis Aiming at aforesaid problems, we put forward a new support vector machine ensem-algorithm. And the clustering centers are denoted as as
The main idea of SVMECA is as follows: 1) Generating individual training subset and constructing SVM components 
Using Bagging, Adaboost, k-fold cross-validation or any other methods to choose certain number of examples from of them to obtain base classifiers. 2) Aggregating the base classifiers X  outputs fusing their outputs in some way to obtain the final decision. Many classifier combi-theory, evidence theory, fuzzy integral, LSE-based weighting, double-layer hierarchi-SVM components X  performance, majority voting method may be a good selection in classifiers X  outputs. The details about SVMECA are shown as follows: Input: L : number of SVM Components Num: Size of training subsets by which to generate SVM 
Component and it is smaller than the number of examples included in the original training set algorithm for positive and negative examples, these pa-rameters are set by the user manually. 
Output: 
Notation: 
Svm :The support vector machine generated by subset B
Svm x : Classification result of x by is -1 or 1 Procedure SVMECA 
Begin begin for j=1: p end for m=1: ' n end Training i Svm on B i TR ; end some representative property and can reflect the actual distribution. 
Through constructing smaller training sets and combining different classifier com-greatly decreased. However, th is is obtained on the cost of the classification accuracy. 1) SVMECA can improve the generalization ability through ensemble of different base classifiers; 2) The classifier componen ts of ensemble are not constructed by the set of cluster centers but by the representative examples, so the ensemble X  X  accuracy whole training dataset. Adaboost and k-fold cross validation algorithms. Here the majority voting is used to machine with Pentium 2.0 CPU and 256M memory. 
Sonar, ionoshpere and handwritten digits recognition datasets from UCI machine learning repository [16] are used to verify our algorithms. Sonar dataset includes 208 samples from two classes and each sample includes 60 features. One half of them are Ionosphere dataset includes 351 samples and each sample includes 34 features. Here 2/3 of them are randomly chosen as training set, and the remainders are used as test-ing dataset. Handwritten digits recognition dataset includes 3823 training samples and 1797 testing samples from 10 classes with 64 features. And the samples of digits  X 6 X  Bagging, Adaboost, k-fold cross validation and out algorithm. For Bagging and and 200. The statistical results of classification accuracy are shown in table 1. It can be obviously found that the SVM ensembles generated by SVMECA has better per-formance than Bagging, Adaboost and k-fold cross validation algorithms. This paper presented a novel construction method for support vector machine ensem-ble based on clustering analysis. The experiments results on synthetic and UCI data-algorithms, the superiority of SVMECA is that the instances distribution information ensemble generated in this way has better performance since the samples in the train-ing subsets have good representative property. 
