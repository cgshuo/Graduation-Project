 in lems arise if there is more than one connected component.
 R learning and which differs from the one usually given in the c omputer graphics community. measure P where  X   X  P in concentrated isotropic noise should work. The law P the true data-generating probability measure P Now the Gaussian measure is equivalent to the heat kernel p of view on P P reverse this diffusion process. In practice we have only an i.i.d. sample X corresponding set of points i (  X  X is to find corresponding points Z we are facing several problems. Since we are only given a finit e sample, we do not know P even P due to the high dimension of the ambient space R d .
 Instead we solve the diffusion process directly on a graph ge nerated by the sample X Laplacian  X  R community and takes into account the probabilistic nature o f the problem. 3.1 Structure on the sample-based graph We would like to define a diffusion process directly on the sam ple X undirected graph. The graph vertices are the sample points X neighbor ( k -NN) distances the weights of the k -NN graph are defined as w ( X i , X j ) = exp and w ( X loops. Further we denote by d the degree function d ( X we introduce two Hilbert spaces H products are defined as  X  f, g  X  H Introducing the discrete differential  X  : H Laplacian is defined as H 3.2 The denoising algorithm differential equation on the graph: timestep in Equation 3 can then be computed as: X Algorithm 1 Manifold denoising 1: Choose  X t , k 2: while Stopping criterion not satisfied do 3: Compute the k -NN distances h ( X i ) , i = 1 , . . . , n , 4: Compute the weights w ( X i , X j ) of the graph with w ( X i , X i ) = 0 , 5: Compute the graph Laplacian  X  ,  X  =  X  D  X  1 W , 6: Solve X ( t + 1)  X  X ( t ) =  X   X t  X  X ( t + 1)  X  X ( t + 1) = ( +  X t  X )  X  1 X ( t ) . 7: end while regularization problem on the graph: where Z  X  denotes the  X  -component of the vector Z  X  R d . With k X  Z  X  k 2 minimizer of the above functional with respect to Z  X  can be easily computed as so that Z = ( +  X t  X )  X  1 X 3.3 k -nearest neighbor graph versus h -neighborhood graph scales which leads for a fixed h to either disconnected or over-connected graphs. Lemma 1 Let x, y  X  R d and  X  the noise term if 2 d X  2 &gt; max h that the linear equation in each time step can be solved effici ently. 3.4 Stopping criterion based on a k -NN graph. Our conjecture 5 is that the result carries over to k -NN graphs. Theorem 1 [7, 8] Let { X x  X  M \  X  X  , then if h  X  0 and nh m +2 / log n  X  X  X  , where  X  4.1 The noise-free case Doing now the limit h  X  0 and  X t  X  0 such that the diffusion constant D = h 2 Lemma 2 ([9], Lemma 2.14) Let i : M  X  R d be a regular, smooth embedding of an m -dimensional manifold M , then  X  Using the equation  X  (4) to a generalized mean curvature flow. The equivalence to the mean curvature flow  X  an additional part if one has a non-uniform probability meas ure on M . 4.2 The noisy case large sample limit n  X  X  X  of the graph Laplacian  X  at a sample point X where k k to main contribution of  X   X  X with a Gaussian, see [7], using the explicit form of p Z Now define the closest point of the submanifold M to X : i (  X  is an approximation of  X   X  have i (  X  the submanifold. In the experiments we observe this as the sh rinking phenomenon. preprocessing method for clustering or dimensionality red uction. 5.1 Denoising t purpose the correlation dimension estimator of [4].
 and the scale has changed as can be seen from the histogram of d istances shown to the right 5.2 Denoising as pre-processing for semi-supervised learn ing of the denoising process as an additional parameter in the SS L algorithm. [15]. It can be formulated as the following regularized leas t squares problem. where y is the given label vector and  X  f,  X  f  X  following weights: w ( X parameter  X  and the regularization parameter  X  were selected from { 1 limited to the employed SSL-algorithm but should also apply to other graph-based methods. No MD 47.9  X  2.67 47.2  X  4.0 14.1  X  5.4 19.2  X  2.1 66.2  X  7.8 50.0  X  1.1 41.9  X  7.0
MD 29.0  X  14.3 26.6  X  17.8 13.8  X  5.5 20.5  X  5.0 66.4  X  6.0 49.8  X  1.5 33.6  X  7.0  X  Iter. 12.3  X  3.8 11.7  X  4.4 9.6  X  2.4 7.3  X  2.9 4.9  X  2.7 8.2  X  3.5 5.6  X  4.4 No MD 38.9  X  6.3 34.2  X  4.1 3.0  X  1.6 6.2  X  1.2 15.5  X  2.6 46.5  X  1.9 27.0  X  1.9
MD 16.1  X  2.2 7.5  X  0.9 3.2  X  1.2 5.3  X  1.4 16.2  X  2.5 48.4  X  2.0 24.1  X  2.8  X  Iter. 15.0  X  0.8 14.5  X  1.5 8.0  X  3.2 8.3  X  3.8 1.6  X  1.8 8.4  X  4.3 6.0  X  3.5
