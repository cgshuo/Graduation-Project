 One of the basic problems in modeling controlled, partially observable, stochastic dynamical sys-interacting with the system. It observes a series of observa tions o h observations: p ( O distribution directly models observable quantities in the system.
 the distribution p ( F n | h changes because h this, the Predictive Linear-Gaussian (PLG) model [8] assum es that p ( F n | h consistent parameter estimators are available for PLGs. Thus, as part of capturing state in a dynamical system in our m ethod, p ( F n | h dealing with high-dimensional density estimation, and in p articular, graphical models. In this paper, we introduce the Exponential Family PSR (EFPSR) which assumes that p ( F n | h carefully, we can impose graphical structure on p ( F n | h graphical models, maximum entropy modeling, and Boltzmann machines. The EFPSR inherits both the advantages and disadvantages of graphical exponential family models: inference and parameter is applicable (in particular, work on approximate inferenc e).
 Selecting the form of p ( F n | h lem. We must also model the dynamical component, which descr ibes the way that the parameters vary over time (that is, how the parameters of p ( F n | h method called  X  X xtend-and-condition, X  which generalizes many state update mechanisms in PSRs. such as HMMs, CRFs [3], or Maximum-entropy Markov Models (ME MMs) [5], for example. In all statistics of interest are directly related to observab le quantities. We refer the reader to [11] for an extended version of this pap er. We now present the Exponential Family PSR (EFPSR) model. The next sections discuss the specifics of the central parts of the model: the state representation, and how we maintain that state. 2.1 Standard Exponential Family Distributions to maximum entropy modeling and graphical models. We refer t he reader to Jaynes [2] for detailed form of the maximum entropy distribution under certain cons traints.
 features  X  ( x ) , graphical structure may be imposed on the distribution. 2.2 State Representation and Dynamics with both {  X  ( f n ) , s Maintaining State . In addition to selecting the form of p ( F n | h given the parameters of p ( F n | h of p ( F n | h t , o t +1 ) ? Our strategy is to extend and condition , as we now explain. Extend. We assume that we have the parameters of p ( F n | h tion of F n | h distribution p ( F n , O dom variables. In order to add the new variable O O and define the vector s + have the following form for the extended distribution: of the extended distribution. We call this the extension function : s + vector of parameters controlling the extension function (a nd hence, the overall dynamics). a non-linear extension allows the model to capture non-line ar dynamics [11]. we then condition on the actual observation o over observations from t + 1 through t + n + 1 : s the statistics representing p ( F n | h many choices of features and extension function, the overal l extend-and-condition operation does not involve any inference, mean that tracking state is compu tationally efficient. ditioning the distribution, the resulting distribution ca n be expressed as: p ( F n = f n | h ture vector  X  did not change between timesteps, which means the form of the distribution does not change. For example, if p ( F n | h 2.3 Representational Capacity under the umbrella of the general EFPSR: for example, every P SR can be represented as an EFPSR (implying that every POMDP, MDP, and k -th order Markov model can also be represented as an EFPSR); and every linear dynamical system (Kalman filter) an d some nonlinear dynamical systems as multinomials and Gaussians) are exponential family dist ributions [11]. We now choose specific features and extension function to gen erate an example model designed to time. Because of this, we will now discuss how we can use a grap h whose form is independent of time to help define structure on our distributions.
 We construct the feature vectors  X  () and  X  + () as follows. Let each O where V = { 1 , ..., nd } are the nodes in the graph (one for each F n | h graph (one for each ( F n +1 | h such that  X  ( f some feature k in the vector such that  X  ( f copies and conditioned versions of each feature exist in the graphs (seen pictorially in Fig. 1). to be a matrix which transforms s + generality, we can permute the indices in our state vector su ch that s Note that although this is linear in the state, it is nonlinea r in the observation. the question of learning the model from data. There are two th ings which can be learned in our each in the next two subsections. We assume we are given a sequ ence of T observations, [ o which we stack to create a sequence of samples from the F n | h 4.1 Structure Learning To learn the graph structure, we make the approximation of ig noring the dynamical component of the model. That is, we treat each f a is always followed by observation b , this fact will be captured within the f The problem therefore becomes one of inducing graphical str ucture for a non-sequential data set, features and adds the one with highest expected gain in log-l ikelihood. To enforce the temporal that feature, as well as the conditioned versions of that fea ture. 4.2 Maximum Likelihood Parameter Estimation ters of our model are not needed, simply because there are no u nobserved variables over which to take expectations. Second, when trying to learn a sequence o f states ( s of futures ( f model. Given a parameter estimate, an initial state s of s t  X  X  is completely determined. This will be a key element to our proposed maximum-likelihood learning algorithm.
 Although the sequence of state vectors s they are not the model parameters  X  that is, we cannot freely select them. Instead, the model pa-modeling a dynamical system, rather than just density estim ation.
 The likelihood of the training data is p ( o nient to measure the likelihood of the corresponding f likelihoods are not the same because the likelihood of the f the approximate equality is because the first n and last n are counted fewer than n times). The expected log-likelihood of the training f Our goal is to maximize this quantity. Any optimization meth od can be used to maximize the log-likelihood. Two popular choices are gradient ascent and qua si-Newton methods, such as (L-)BFGS. of the likelihood with respect to the parameters, which we wi ll now compute. First, we compute the derivative of the log-likelihood with respect to each state: where E servations closer to the observed features however, we cann ot adjust s adjust it implicitly by adjusting the transition parameter s A and B .
 We now compute the gradients of the state with respect to each parameter: the state with respect to B are given by These gradients are temporally recursive  X  they implicitly depend on gradients from all previous timesteps. It might seem prohibitive to compute them: must a n algorithm examine all past t can be computed in a recursive fashion as the algorithm walks through the data. Figure 2: Results on two-state POMDPs. The right shows the ge neric model used. By varying the transition and observation probabilities, three differen t POMDPs were generated. The left shows E[  X  ( F n | h t )] at each timestep must be computed (see Eq. 4): s of valid mean parameters is uniquely determined by one set of canonical parameters [9]. Computing these marginals is an inference problem. This is r epeated T times (the number of sam-must be repeatedly performed in our model, computational ef ficiency is a more stringent require-propagation, (loopy) belief propagation, MCMC methods, an d contrastive divergence [1]. to determine an optimal policy. Evaluating the reward achie ved becomes an objective measure of model quality, even though approximate likelihood is the le arning signal. We used exact inference to compute the E[  X  ( F n | h the likelihood using BFGS. For each dataset, we computed the log-likelihood of the data under the true model, as well as the log-likelihood of a  X  X aive X  model, which assigns uniform probability log-likelihood under the learned and true models.
 for a moderately noisy POMDP; again, the learned model is alm ost perfect. The third panel shows prediction is difficult, even with a perfect model.
 see that the learned models are quite accurate, and generali ze well.
 Second set. We also tested on a two more complicated POMDPs called Cheese maze and Maze VMF), and log-determinant relaxations (LDR) [10]. Since th e VMF and LDR bounds on the log-appeal to likelihood. Instead, we opted to evaluate the mode ls based on control performance. We used the Natural Actor Critic (or NAC) algorithm [6] to tes t our model (see [11] for further experiments). The NAC algorithm requires two things: a stoc hastic, parameterized policy which softmax function of a linear projection of the state: the pro bability of taking action a given the policy parameters  X  is: p ( a  X  are to be determined. For comparison, we also ran the NAC plan ner with the POMDP belief true POMDP in place of the EFPSR X  X  state ( s assumption (or reactive policy) and a totally random policy .
 mance obtained (average reward per timestep) as a function o f steps of optimization. The  X  X OMDP X  the  X  X andom X  line shows the reward obtained with a random pol icy, and the  X  X eactive X  line shows the best reward obtained by using the observation as input to the NAC algorithm. The lines  X  X MF, X   X  X BP, X  and  X  X DR X  correspond to the different inference meth ods. at 0.187. The EFPSR policy closes about 94% of the gap between a random policy and the policy improvement over the first order Markov model is not as strong : the EFPSR closes about 77.8% of the gap between a random policy and the optimal policy. We con clude that the EFPSR has learned methods, and even in cases where we cannot compare to the exac t likelihood. We have presented the Exponential Family PSR, a new model of c ontrolled, stochastic dynamical a specific member of the EFPSR family, the Linear-Linear EFPS R, and a maximum likelihood learn-ing algorithm. We were able to learn almost perfect models of several small POMDP systems, both from a likelihood perspective and from a control perspectiv e. The biggest drawback is computa-models which can be accurate in terms of likelihood and usefu l in terms of control performance. David Wingate was supported under a National Science Founda tion Graduate Research Fellowship. Satinder Singh was supported by NSF grant IIS-0413004. Any o pinions, findings, and conclusions the views of the NSF.

