 Christos Faloutsos Carne gie Mellon Univ . christos@cs.cmu.edu Given a lar ge collection of medical ima ges of sever al con-ditions and treatments, how can we succinctly describe the char acteristics of eac h setting? For example , given a lar ge collection of retinal ima ges from sever al dif fer ent experi-mental conditions (normal, detac hed, reattac hed, etc.), how can data mining help biolo gists focus on important regions in the ima ges or on the dif fer ences between dif fer ent exper -imental conditions?
If the ima ges wer e text documents, we could nd the main terms and concepts for eac h condition by existing IR meth-ods (e.g., tf/idf and LSI). We propose something analo gous, but for the muc h mor e challenging case of an ima ge col-lection: We propose to automatically develop a visual vo-cab ulary by breaking ima ges into n n tiles and deriving key tiles ( X V iVos X ) for eac h ima ge and condition. We exper -iment with numer ous domain-independent ways of extr act-ing featur es from tiles (color histo grams, textur es, etc.), and sever al ways of choosing char acteristic tiles (PCA, ICA).
We perform experiments on two dispar ate biomedical datasets. The quantitative measur e of success is classica-tion accur acy: Our  X V iVos X  achie ve high classication ac-cur acy (up to 83 % for a nine-class problem on feline retinal ima ges). Mor e importantly , qualitatively , our  X V iVos X  do an excellent job as  X visual vocab ulary terms X : the y have bi-olo gical meaning , as corr obor ated by domain experts; the y help spot char acteristic regions of ima ges, exactly like text vocab ulary terms do for documents; and the y highlight the dif fer ences between pair s of ima ges. We focus on the problem of summarizing and disco ver-ing patterns in lar ge collections of biomedical images. We would lik e an automated method for processing the images and constructing a visual vocab ulary which is capable of describing the semantics of the image content. Particularly , we are interested in questions such as:  X What are the in-ter esting regions in the ima ge for further detailed investi-gation? X  and  X What chang es occur between ima ges from dif fer ent pair s of classes? X  As a concrete example, consider the images in Figure 1. The y depict cross-sections of feline retinas X specically , sho wing the distrib utions of two dif ferent proteins X under the experimental conditions  X normal X  and  X 3 days of de-tachment.  X  Ev en a non-e xpert human can easily see that each image consists of several vertical layers, despite the fact that the location, texture, and color intensity of the pat-terns in these layers vary from image to image. A trained bi-ologist can interpret these observ ations and build hypothe-ses about the biological processes that cause the dif ferences.
This is exactly the goal of our effort: We want to build a system that will automatically detect and highlight patterns dif ferentiating image classes, after processing hundreds or thousands of pictures (with or without labels and times-tamps). The automatic construction of a visual vocab ulary of these dif ferent patterns is not only important by itself, but also a stepping stone for lar ger biological goals. Such a sys-tem will be of great value to biologists, and could pro vide valuable functions such as automated classication and sup-porting various data mining tasks. We illustrate the power of our proposed method on the follo wing three problems: Pr oblem 1 Summarize an ima ge automatically .
 Pr oblem 2 Identify patterns that distinguish ima ge classes. Pr oblem 3 Highlight inter esting regions in an ima ge.
Biomedical images bring additional, subtle complica-tions: (1) Some images may not be in the canonical orienta-tion, or there may not be a canonical orientation at all. (The latter is the case for one of our datasets, the Chinese ham-ster ovary dataset.) (2) Ev en if we align the images as well as possible, the same areas of the images will not always contain the same kind of tissue because of indi vidual varia-tion. (3) Computer vision techniques such as segmentation require domain-specic tuning to model the intricate texture in the images, and it is not kno wn whether these techniques can spot biologically interesting regions. These are subtle, but important issues that our automatic vocab ulary creation system has to tackle.

We would lik e a system that automatically creates a vi-sual vocab ulary and achie ves the follo wing goals: (1) Bi-olo gical interpr etations : The resulting visual terms should have meaning for a domain expert. (2) Biolo gical process summarization : The vocab ulary should help describe the underlying biological process. (3) Gener ality : It should work on multiple image sets, either color or gray-scale, from dif ferent biological domains.
 The major contrib utions of this paper are as follo ws:
The paper is organized as follo ws. Section 2 describes related work. In Section 3, we introduce our proposed method for biomedical image classication and pattern dis-covery . Classication results are presented in Section 4. Ex-periments illustrating the biological interpretation of ViVos appear in Section 5. Section 6 concludes the paper . Biomedical images have become an extremely important dataset for biology and medicine. Automated analysis tools have the potential for changing the way in which biologi-cal images are used to answer biological questions, either for high-throughput identication of abnormal samples or for early disease detection [7, 18, 19]. Two specic kinds of biomedical images are studied in this paper: confocal microscop y images of retina and uorescence microscop y images of Chinese Hamster Ov ary (CHO) cells.

The retina contains neurons that respond to light and transmid electrical signals to the brain via the optic nerv e. Multiple antibodies are used to localize the expression of specic proteins in retinal cells and layers. The antibodies are visualized by immunohistochemistry , using a confocal microscope. The images can be used to follo w a change in the distrib ution of a specic protein in dif ferent exper -imental conditions, or visualize specic cells across these conditions. Multiple proteins can be visualized in a single image, with each protein represented by a dif ferent color .
It is of biological interest to understand how a protein changes expression and how the morphology of a specic cell type changes across dif ferent experimental conditions (e.g., an injury such as retinal detachment) or when dif fer -ent treatments are used (e.g., oxygen administration). The ability to discriminate and classify on the basis of patterns (e.g., the intensity of antibody staining and texture produced by this staining) can help identify dif ferences and similari-ties of various cellular processes.

The second kind of data in our study are uorescence microscop y images of subcellular structures of CHO cells. These images sho w the localization of four proteins and the cell DN A within the cellular compartments. This informa-tion may be used to determine the functions of expressed proteins, which remains one of the challenges of modern biology [1]. A textual vocab ulary consists of words that have distinct meanings and serv e as building blocks of lar ger semantic constructs lik e sentences or paragraphs. To create an equi v-alent visual vocab ulary for images, pre vious work applied transformation on image pix els to deri ve tok ens that can describe image contents effecti vely [22 , 4]. Ho we ver, an image usually has tens of thousands of pix els. Due to this high dimensionality , a lar ge number of training images is needed by pix el-based methods to obtain a meaningful vo-cab ulary . This has limited the application of these methods to databases of small images.

One way to deal with this dimensionality curse is to ex-tract a small number of features from image pix els. The vocab ulary construction algorithm is then applied to the ex-tracted features to disco ver descripti ve tok ens. A feature is usually extracted by ltering and summarizing pix el in-formation. In man y applications, these tok ens have been sho wn useful in capturing and con veying image properties, under dif ferent names such as  X blob,  X   X visterm,  X   X visual keyw ords,  X  and so on. Examples of applications include object detection [20 ] and retrie val [21 ], as well as image classication [22 , 4, 14] and captioning [5, 9].

Clustering algorithms or transformation-based methods are other defenses against the curse of dimensionality . K-means clustering has been applied to image segments [5, 9] and the salient descriptor [21 ] for vocab ulary construction. Examples of transformation-based methods include prin-cipal component analysis (PCA) [10 , 22, 14] and wavelet transforms [20 ]. Recently , independent component analysis (ICA) [8] has been used in face recognition [4], yielding fa-cial templates. Lik e the feature extraction approaches, these methods also have problems with orientation and registra-tion issues, as the y rely on global image features.
In this paper , we present a method that disco vers a mean-ingful vocab ulary from biomedical images. The proposed method is based on  X tiles X  of an image, and successfully avoids issues such as registration and dimensionality curse. We use the standard MPEG-7 features color structur e de-scriptor (CSD), color layout descriptor (CLD) and homo-geneous textur e descriptor (HTD) [16 ]. The CSD is an n -dimensional color histogram ( n is 256, 128, 64, or 32), but it also tak es into account the local spatial structure of the color . For each position of a sliding structural element, if a color is present, its corresponding bin is incremented. The CLD is a compact representation of the overall spatial lay-out of the colors, and uses the discrete cosine transform to extract periodic spatial characteristics in blocks of an im-age. The HTD characterizes region texture using mean en-ergy and ener gy deviation of the whole image, both in pix el space and in frequenc y space (Gabor functions along 6 ori-entations and 5 scales).

Alternati vely , there is work on constructing visual vo-cab ulary [17 , 15] with a human in the loop, with the goal of constructing a vocab ulary that better captures human per -ception. Human experts are either ask ed to identify criteria that the y used to classify dif ferent images [17 ], or directly give labels to dif ferent patterns [15 ]. The vocab ulary is then generated according to the given criteria and labels. These approaches are supervised, with human feedback as input to the construction algorithms. In contrast, our proposed method presented in this paper is unsupervised: The image labels are used only after the ViVos are constructed, when we evaluate them using classication. In this section, we introduce our proposed method for trans-forming images into their symbolic representations. The al-gorithm is given in Figure 2, and uses the symbols listed in Table 1. The algorithm consists of ve steps.

The rst step partitions the images into non-o verlapping tiles. The optimal tile size depends on the nature of the im-ages. The tiles must be lar ge enough to capture the charac-teristic textures of the images. On the other hand, the y can-not be too lar ge. For instance, in order to recognize the red Input: A set of n images I = f I 1 ; : : : ; I n g . Output: Visual vocab ulary (ViVos) V = f v 1 ; : : : ; v m Algorithm: 1. Partition each image I i into s i non-o verlapping tiles 2. For each tile j 2 f 1 ; : : : ; s i g in each image I i 3. Generate visual vocab ulary V = gen vv ( [ n i
Also, compute P , the PCA basis for all  X  t i ; j 's. 4. For each tile j 2 f 1 ; : : : ; s i g in each image I i 5. For each image I i , compute the ViVo-v ector of I i : Symbol Meaning v (  X  t i ; j ) m -dimensional ViVo-v ector of tile  X  t i v k ( I i ) The k -th element of v ( I i ) T ( v i ) Set of repr esentative tiles of ViVo v i R ( c i ) Set of repr esentative ViVos of condition c i layer in Figure 1(a), the tile size should not be much lar ger than the width of the layer . We use a tile size of 64-by-64 pix els, so each retinal image has 8 12 tiles, and each sub-cellular protein localization image has 8 6 or 8 8 tiles.
In the second step, a feature vector is extracted from each tile, representing its image content. We have conducted experiments using features such as the color structure de-scriptor (CSD), color layout descriptor (CLD), and homo-geneous texture descriptor (HTD). The vector representing a tile using features of, say CSD, is called a tile-vector of the CSD. More details are given in Section 4.

The third step deri ves a set of symbols from the feature vectors of all the tiles of all the images. In text processing, there is a similar issue of representing documents by topics. The most popular method for nding text topics is latent se-mantic inde xing (LSI) [3], which is based on analysis that resembles PCA. Given a set of data points, LSI/PCA nds a set of orthogonal (basis) vectors that best describe the data distrib ution with respect to minimized L 2 projection error . Each of these basis vectors is considered a topic in the doc-ument set, and can be used to group documents by topics. Our approach is similar: We deri ve a set of symbols by ap-plying ICA or PCA to the feature vectors. Each basis vector found by ICA or PCA becomes a symbol. We call the sym-bols ViVos and the set of symbols a visual vocab ulary .
Figure 3(a) sho ws the distrib ution of the tile-v ectors of the CSD, projected in the space spanned by the two PCA basis vectors with the highest eigen values. The data dis-trib ution displays several characteristic patterns X  X arms X  X  on which points are located. None of the PCA basis vectors (dashed lines anchored at h 0 ; 0 i : P 1 ; P 2 ) nds these charac-teristic arms. On the other hand, if we project the ICA basis vectors onto this space (solid lines: I 1 ; I 2 ; I 3 ), the y clearly capture the patterns in our data. It is preferable to use the ICA basis vectors as symbols because the y represent more precisely the dif ferent aspects of the data. We note that only three ICA basis vectors are sho wn because the rest of them are approximately orthogonal to the space displayed.
Relating Figure 3 to our algorithm in Figure 2, each point is a  X  t i ; j in step 2 of the algorithm. Function gen vv() in step 3 computes the visual vocab ulary which is dened according to the set of the ICA basis vectors. Intuiti vely , an ICA basis vector denes two ViVos, one along the positi ve direction of the vector , another along the negati ve direction.
Formally , let T 0 be a t -by-d matrix, where t is the num-ber of tiles from all training images, and d is the number of features extracted from each tile. Each row of T 0 corre-sponds to a tile-v ector  X  t i ; j , with the overall mean subtracted. Suppose we want to generate m ViVos. We rst reduce the dimensionality of T 0 from d to m 0 = m = 2, using PCA, yield-ing a t -by-m 0 matrix T . Ne xt, ICA is applied in order to de-T = HB . The rows of B are the ICA basis vectors (solid lines in Figure 3(a)). Considering the positi ve and negati ve directions of each basis vector , the m 0 ICA basis vectors would dene m = 2 m 0 ViVos, which are the outputs of the function gen vv() .

Ho w do we determine the number of ViVos? We follo w the rule of thumb, and mak e m 0 = m = 2 be the dimensionality which preserv es 95 % spread/ener gy of the distrib ution.
With the ViVos ready , we can use them to represent an image. We rst represent each d -dim tile-v ector in terms of ViVos by projecting a tile-v ector to the m 0 -dim PCA space and then to the m 0 -dim ICA space. The positi ve and neg-ative projection coef cients are then considered separately , yielding the 2 m 0 -dim ViVo-v ector of a tile. This done by comp vivo() in the fourth step of the algorithm in Figure 2. The m = 2 m 0 coef cients in the ViVo-v ector of a tile also indicate the contrib utions of each of the m ViVos to the tile.
In the fth and nal step, each image is expressed as a combination of its (reformulated) tiles. We do this by sim-ply adding up the ViVo-v ectors of the tiles in an image. This yields a good description of the entire image because ICA produces ViVos that do not  X interfere X  with each other . That is, ICA mak es the columns of H (coef cients of the basis vectors, equi valently , contrib ution of each ViVo to the im-age content) as independent as possible [8]. Denition 1 summarizes the outputs of our proposed method.
 Denition 1 (V iVo and ViVo-v ector) A ViVo is dened by either the positive or the negative dir ection of an ICA ba-sis vector , and repr esents a char acteristic pattern in im-age tiles. The ViVo-vector of a tile  X  t i ; j is a vector v f ; : : : ; f m ] , wher e f i indicates the contib utions of the i-th ViVo in describing the tile . The ViVo-vector of an ima ge is dened as the sum of the ViVo-vectors of all its tiles. Repr esentati ve tiles of a ViVo. A ViVo corresponds to a direction dened by a basis vector , and is not exactly equal to any of the original tiles. In order to visualize a ViVo, we represent it by a tile that strongly expresses the characteris-tics of that ViVo.

We rst group tiles that are majorly located along the same ViVo direction together as a  X tile group X . Formally , let the ViVo-v ector of a tile  X  t i ; j be v (  X  t i ; j say that the tile  X  t i ; j belongs to ViVo v k , if the element with lar gest magnitude is f k , i.e., k = arg max k 0 j f k 0 group of a ViVo v k is the set of tiles that belong to v k ure 3(b) visualizes the tile groups of two ViVos on the 2-D plane dened by the PCA basis vectors ( P 1 ; P 2 ).
The repr esentative tiles of a ViVo v k , T ( v k ) , are then se-lected from its tile group (essentially the tiles at the  X tip X  of the tile group). The top 5 representati ve tiles of the two ViVos in Figure 3(b) are sho wn in light triangles. The top representati ve tile of ViVo v k has the maximum j c k j value among all tiles in v k 's tile group. In Section 5.1, we sho w the representati ve tiles of our ViVos and discuss their bio-logical interpretation. The experiments in this section evaluate the combinations of image features and ViVo generation methods for ViVo construction. In these experiments, our goal is to nd the Feature Dim. Accurac y Std. dev.

Original CSD 512 0.838 0.044 14 ViVos from CSD 14 0.832 0.042 12 ViVos from CSD 12 0.826 0.038
Original CLD 24 0.346 0.049 24 ViVos from CLD 24 0.634 0.023
Original HTD 124 0.758 0.048 12 ViVos from HTD 12 0.782 0.019 best representation of the images in the symbolic space and ensure that classication accuracies obtained using these symbols are close to the best accurac y that we could obtain with the raw features.

Biologists have chosen experimental conditions which correspond to dif ferent stages of the biological process. Thus, a combination that successfully classies images is also lik ely to be a good choice for other analyses, such as the qualitati ve analyses described in Section 5, where we investigate the ability of the visual vocab ulary to reveal bi-ologically meaningful patterns.

Classication experiments were performed on two datasets: one dataset of 433 retinal micrographs, and an-other dataset of 327 uorescence micrographs sho wing sub-cellular localization patterns of proteins in CHO cells. In the follo wing, we refer to the datasets by their cardinality: the 433 dataset and the 327 dataset. The 433 dataset contains retinal images from the UCSB BioImage database (http://bioimage.ucsb .edu/), which con-tains images of retinas detached for either 1 day (label 1d ), 3 days ( 3d ), 7 days ( 7d ), 28 days ( 28d ), or 3 months ( 3m ). There are also images of retinas after treatment, such as reattached for 3 days after 1 hour of detachment ( 1h3dr ), reattached for 28 days after 3 days of detachment ( 3d28dr ), or exposed to 70 % oxygen for 6 days after 1 day of detachment ( 1d6dO2 ), and images of control tissues ( n ) [6, 13, 12].

We experimented extensi vely with dif ferent features and vocab ulary sizes. Features are extracted separately for the red and green channels and then concatenated. The chan-nels sho w the staining by two antibodies: anti-rod opsin (red) and anti-GF AP (green). The number of ViVos should be small, as lar ge vocab ularies contain redundant terms and become dif cult for domain experts to interpret. Preserv-ing 95 % of the ener gy resulted in 14, 24, and 12 ViVos for CSD, CLD, and HTD, respecti vely . The classication accu-racies, reported in Table 2, are from 5-fold cross-v alidation using SVM [2] with linear kernels. SVM with polynomial kernels and a k -NN ( k = 1, 3, or 5) classier produced results that were not signicantly dif ferent. ViVos from CSD perform signicantly better than ViVos from CLD ( p &lt; 0 : 0001) and also signicantly better than ViVos from HTD ( p = 0 : 0492). Further , manual inspection of HTD ViVos did not reveal better biological interpretations.
Two of the 14 CSD ViVos were remo ved because none of the images had high coef cients for them. Those two ViVos had no interesting biological interpretation either . As expected, remo ving these two ViVos (using only 12 ViVos) resulted in insignicantly ( p = 0 : 8187) smaller classica-tion accurac y compared to the 14 CSD ViVos (Table 2). The dif ference from the original CSD features is also insigni-cant ( p = 0 : 6567). We therefore choose to use the 12 CSD ViVos as our visual vocab ulary . In order to assess the generality of our visual vocab ulary approach, we also applied our method to classify 327 uo-rescence microscop y images of subcellular protein localiza-tion patterns [1]. Example micrographs depicting the cell DN A and four protein types are sho wn in Figure 4. We par -titioned the data set into training and test sets in the same way as Boland et al. [1].

We note that although these images are very dif ferent from the retinal images, the combination of CSD and ICA still classies 84 % of the images correctly . The 1-NN classier achie ves 100 % accurac y on 3 classes: Giantin, Hoechst, and NOP4. The training images of class LAMP2 in the data set have size 512-by-512, which is dif ferent from that of the others, 512-by-382. Due to this discrepanc y, class LAMP2 is classied at 83 %, and around half of Tubu-lin images are classied as LAMP2. (a) ViVo 1 (b) ViVo 2 (c) ViVo 3 (d) ViVo 4 (e) ViVo 5 (e) ViVo 6 (f) ViVo 7 (g) ViVo 8 (i) ViVo 9 (j) ViVo 10 (k) ViVo 11 (l) ViVo 12
To summarize, our classication experiments sho w that the symbolic ViVo representation captures well the contents of microscop y images of two dif ferent kinds. Thus, we are condent that the method is applicable to a wider range of biomedical images. Deri ving a visual vocab ulary for image content description opens up man y exciting data mining applications. In this section, we describe our proposed methods for answering the three problems we introduced in Section 1. We rst discuss the biological interpretation of the ViVos in Sec-tion 5.1 and sho w that the proposed method correctly sum-marizes a biomedical image automatically (Problem 1). An automated method for spotting dif ferential patterns between classes is introduced in Section 5.2 (Problem 2). Several observ ations on the class-distinguishing patterns are also discussed. Finally , in Section 5.3, we describe a method to automatically highlight interesting regions in an image (Problem 3). The representati ve tiles of ViVos 2, 3, 4, 7, and 12 sho wn in Figure 5 demonstrate the hypertrophy of M  X  uller cells. These ViVos correctly discriminate various morphological changes of M  X  uller cells. The green patterns in these repre-sentati ve tiles is due to staining produced by immunohisto-chemistry with an antibody to GF AP , a protein found in glial cells (including M  X  uller cells). Our visual vocab ulary also captures the normal expression of GF AP in the inner retina, represented by ViVo 1. The M  X  uller cells have been sho wn to hypertrophy follo wing experimental retinal detachment. Understanding how the y hypertrophy and change morphol-ogy is important in understanding how these cells can ulti-mately form glial scars, which can inhibit a reco very of the nerv ous system from injury .

Also, our ViVos correctly place tiles into dif ferent groups, according to the dif ferent anti-rod opsin staining which may due to functional consequences follo wing in-jury . In an uninjured retina, anti-rod opsin (sho wn in red) stains the outer segments of the rod photoreceptors, which are responsible for con verting light into an electrical signal and are vital to vision. ViVos 5 and 10 sho w a typical stain-ing pattern for an uninjured retina, where healthy outer seg-ments are stained. Ho we ver, follo wing detachment or other injury to the retina, outer segment degeneration can occur (ViVo 9). Another consequence of retinal detachment can be a re-distrib ution of rod opsin from the outer segments of these cells to the cell bodies (ViVo 8).

As described abo ve, both the re-distrib ution of rod opsin and the M  X  uller cell hypertrophy are consequences of reti-nal detachment. It is of interest to understand how these processes are related. ViVo 11 captures the situation when the two processes co-occur . Being able to sample a lar ge number of images that have these processes spatially over-lapping will be important to understanding their relation-ship. ViVo 6 is rod photoreceptor cell bodies with only background labeling. We are interested in identifying ViVos that sho w dif ferences between dif ferent retinal experimental conditions, including treatments. Let images f I 1 ; : : : ; I n g be the training images of condition c i . Suppose that our analysis in Section 3 sug-gests that m ViVos should be used. Follo wing the algorithm outlined in Figure 2, we can represent an image I as an m -dimensional ViVo-v ector v ( I ) . The k -th element of a ViVo-vector, v k ( I ) , gives the expression level of ViVo v k image I . Let S ik = f v k ( I 1 ) ; : : : ; v k ( I n ) g the k -th elements of all image ViVo-v ectors in condition c
To determine if a ViVo v k is a discriminati ve ViVo for two conditions c i and c j , we perform an analysis of variance (ANO VA) test, follo wed by a multiple comparison [11 ]. If the 95% condence interv als of the true means of S ik and jk do not intersect, then the means are not signicantly dif ferent, and we say that ViVo v k discriminates conditions c and c j , i.e., v k is a discriminati ve ViVo for c i and c separation between S ik and S jk indicates the  X discriminating power X  of ViVo v k .

Figure 6 sho ws the conditions as box es and the discrimi-nati ve ViVos on edges connecting pairs of conditions that are of biological interest. ViVos 6 and 8 discriminate n from 1d and 1d from 3d . The two ViVos represent rod photoreceptor cell bodies with only background labeling and with redistrib ution of rod opsin, respecti vely , indicat-ing that the redistrib ution of rod opsin is an important effect in the short-term detachment. Note also that ViVo 8 dis-tinguishes 1d6dO2 from 7d . This suggests that there are cellular changes associated with this oxygen treatment, and the ViVo technique can be used for this type of comparison.
The ViVos that represent M  X  uller cell hypertrophy (ViVo 2, 3, 4, 7, and 12) discriminate n from all other conditions. We note that ViVo 1, which represents GF AP labeling in the inner retina in both control ( n ) and detached conditions, is present in all conditions, and therefore cannot discriminate any of the pairs in Figure 6. In addition, several ViVos dis-criminate between 3d28dr and 28d , and 1h3dr and 3d , suggesting cellular effects of the sur gical procedure. Inter -estingly , there are no ViVos that discriminate between 7d and 28d detachments, suggesting that the effects of long-term detachment have occurred by 7 days.

Although these observ ations are generated automatically by an unsupervised tool, the y correspond to observ ations and biological theory of the underlying cellular processes. In this section, we propose a method to nd class-rele vant ViVos and then use this method to highlight interesting re-gions in images of a particular class.

In order to determine which condition a ViVo belongs to, we examine its representati ve tiles and determine the most popular condition among them (majority voting). We dene the condition of a tile to be that of the image from which it the more its representati ve tiles are present in images of a condition, the more rele vant the ViVo is to that condition.
Formally , the set R ( c k ) of representati ve ViVos of a con-dition c k is dened as ( c k ) = where t is a tile, and I ( p ) is an indicator function that is 1 if the predicate p is true, and 0 otherwise. The representati ve ViVos of a condition c k can be used to annotate images of that particular condition in order to highlight the regions with potential biological interpretations.

Figure 7(a) sho ws an annotated image of a retina de-tached for 28 days. The GF AP labeling in the inner retina is highlighted by ViVo 1 (see Figure 5(a)).

Figure 7(b) sho ws an annotated image of a retina de-tached for 3 days and then reattached for 28 days. The an-notation algorithm highlighted the outer segments of the rod photoreceptors with ViVo 10 (see Figure 5(j)). As pointed out in Section 5.1, ViVo 10 represents healthy outer seg-ments. In the retina depicted in Figure 7(b), the outer seg-ments have indeed reco vered from the degeneration caused by detachment. This reco very of outer segments has pre vi-ously been observ ed [6], and conrms that ViVos can rec-ognize image regions that are consistent with pre vious bio-logical interpretations. Mining biomedical images is an important problem because of the availability of high-throughput imaging, the applica-bility to medicine and health care, and the ability of images to reveal spatio-temporal information not readily available in other data sources such as genomic sequences, protein structures and microarrays.

We focus on the problem of describing a collection of biomedical images succinctly (Problem 1). Our main con-trib ution is to propose an automatic, domain-independent method to deri ve meaningful, characteristic tiles ( ViVos ), leading to a visual vocab ulary (Section 3). We apply our technique to a collection of retinal images and validate it by sho wing that the resulting ViVos correspond to biological concepts (Section 5.1).

Using ViVos, we propose two new data mining tech-niques. The rst (Section 5.2) mines a lar ge collection of images for patterns that distinguish one class from another (Problem 2). The second technique (Section 5.3) automati-cally highlights important parts of an image that might oth-erwise go unnoticed in a lar ge image collection (Problem 3). The conclusions are as follo ws:
