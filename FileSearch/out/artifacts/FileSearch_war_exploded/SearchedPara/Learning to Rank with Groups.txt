 An essential issue in document retrieval is ranking, and the documents are ranked by their expected relevance to a given query. Multiple labels are used to represent different level of relevance for documents to a given query, and the cor-responding label values are used to quantify the relevance of the documents. According to the training set for a given query, the documents can be divided into several groups. Specifically, the documents with the same label are assigned to the same group. If the documents in the group with higher relevance label can always be ranked higher over the ones in groups with lower relevance label by a ranking model, it is reasonable to expect perfect ranking performance. In-spired by this idea, we propose a novel framework for learn-ing to rank, which depends on two new samples. The first one is one-group constituted by one document with higher level label and a group of documents with lower level la-bel; the second one is group-group constituted by a group of documents with higher level label and a group of documents with lower level label. A novel loss function is proposed based on the likelihood loss similar to ListMLE. We demon-strate the advantages of our approaches on the Letor 3.0 data set. Experimental results show that our approaches are effective in improving the ranking performance. H.3.3 [ Information Search and Retrieval ]: Search and Retrieval X  Retrieval models Algorithms, Experimentation, Performance Information Retrieval, Learning to Rank, ListMLE, Group Ranking Framework
Learning to rank [1] is an effective method that applies the machine learning approaches to improve the performance of information retrieval. It aims to learn a rank function with the relevance judgments from training set. The features for learning algorithms are usually derived from traditional feature selection approaches. Many learning methods have been proposed during last decade. They are usually divided into three categories: the pointwise, pairwise and listwise approaches, which are based on the types of input samples for learning. Most of the effective approaches of learning to rank are based on these three frameworks. However, there are few studies to improve learning to rank methods using other patterns.

The purpose of ranking is to list the relevant documents on the top positions of the permutations sorted by a rank-ing model. Most of the previous studies treat the documents with the same level label as a single object respectively. It may not achieve the perfect performance. If we view the document with the same level label as a group, the rank-ing task is reduced from ranking lots of documents to rank several groups with same label. In this paper we try to im-prove a classic learning to rank method (ListMLE) based on this idea. We present a new framework, named group wise framework to improve the existing approach of learning to rank, which is defined by different level labels.
The rest of this paper is organized as follows. Section 2 introduces some related studies of our research. Section 3 makes a brief analysis of ListMLE based on the framework of listwise and top-k ranking, and gives a description on the new framework we used to improve the ListMLE. Then Section 4 presents the experimental results and analysis. Fi-nally, we conclude this work and point out some directions for future research.
ListMLE [4] is an effective ranking approach of listwise, which formalizes learning to rank as a problem of minimiz-ing the likelihood function of a probability model. Specially, Xia et al. [3] presented a new framework based on ListMLE, which is used to optimize the top-k ranking. They demon-strated it is effective to improve the ranking performance of top-k and also derive sufficient conditions for a listwise ranking method to be consistent with the top-k true loss. To expend their work, we develop a new framework for rank-ing. In this paper we divide the list of documents according to a query into several groups based on their relevance la-bels. And there are two patterns to construct the groups: one is constituted of one document with higher relevance la-bel and a group of documents with the same label which is lower than the previous document; the other is constituted of a group of documents with the same label and a group of documents with the same label which is lower than previous group. We take these as the training samples and define the new loss function of group based on likelihood loss to examine whether it is effective to improve the ranking per-formance of ListMLE. ListMLE [4] as a permutation-level ranking framework of Listwise can improve ranking performance effectively. It is a feature-based ranking algorithm that minimizes a prob-abilistic listwise loss function which is the likelihood loss, defined by the permutation probabilities in the Luce model. The loss function of ListMLE is defined as:
L ( f ; x q ; y q ) = In the function, y is a randomly selected optimum permuta-tion which satisfies the condition for any two documents x x , if label( x i ) &gt; label( x j ), the x i is ranked before x However there are still some problems with the algorithm. Suppose we have a hypothesis space with three hypothesis functions f 1 , f 2 and f 3 , as shown in Table 1. There are 6 documents with respect to one query in the Table 1 with their relevance labels. The indices of docu-ments also construct optimum permutation for ListMLE, and the f 1 , f 2 and f 3 are generated by ListMLE in the pro-cess of iterations. We make a hypothesis that f 2 and f 3 are got after f 1 . The loss of f 1 calculated by likelihood loss 5.9915 . Calculating loss in the same way, it can be obtained that the loss of f 2 is 5.8579 and the loss of f 3 is 5.7991. The loss of ListMLE may be decreased by the relevance scores of document 1 and 2 decided by f 2 compared with f 1 . As matter of fact that the relevance labels of document 1 and 2 are equal, the descent loss may be not necessary. However it is more serious problem caused by document 4, the loss decreased by f 3 compared with f 2 . It seems that as some scores of irrelevant documents scores increasing, it may re-sult in descending performance of ranking.
In practical application of information retrieval, correct ranking at top k positions is much more important. In order to improve the top k ranking performance, Xia et al. [3] develop a top-k ranking framework. The actual value of k is determined by application. When k equals the length of the entire ranked list, the top-k true loss will become permutation-level loss.

ListMLE algorithm can be indeed improved by the frame-work of Top-k ranking. The only difference of loss function between listwise and top-k ranking framework is that the permutation probability is changed from whole list prob-ability to the top-k subgroup probability, Mapping to the function: n-1 is changed to k. The framework has indeed improved the performance of top k ranking of ListMLE by setting k from 1 to 10, and it achieved best performance at 10, we can see the ranking accuracies of ListMLE and top-10 ListMLE on the OHSUMED collection of Letor 3.0 data set in Table 2.

From Table 2, we can see that the top-k ranking perfor-mance is improved evaluated by NDCG. But it is not just the top-k ranking performance improved, the ranking per-formance of the whole list is also be improved shown by MAP. In the paper [3] they use the statistical consistency to explain the superior performance for the top-k ranking, but there is no such experiments and theories to interpret why it is still helpful to improve the whole list ranking performance.
In this paper, we interpret the improvement caused by ig-noring the affect of increasing scores of irrelevance, which decreases the loss of ListMLE, we have taken a toy example in section 3.1 to explain that the loss function can be de-creased by the increasing the score of irrelevant documents. For the top-k ranking framework, the true loss only defined on the top-k documents on the optimum permutation which are usually the relevance documents. It is quite natural to improve the whole list ranking performance.
In fact, the top-k ranking loss function is based on the documents on the top-k positions. As the true loss only depends on top-k documents, the effect of irrelevant docu-ments is smaller than that in the whole list loss. So top-k ranking loss can achieve a good performance. With k from 1 to 10, the performance of the frame gradually becomes better [3]. It reveals that the more relevance document is considered by the loss, the better performance the ranking model can achieve.

It is natural to set the value of k equal to the number of relevant documents. However, the relevance of document is not just only relevant or irrelevant. There may be multiple relevance judgments as 2 (definitely relevant), 1 (possibly relevant), 0 (irrelevant) used in the OHSUMED collection of Letor data set [2]. And the loss function can not consider the relevant documents with the same relevance as the same ones. It may affect the performance of ranking model. In this paper, we use the idea of grouping to solve the problem.
The documents with respect to a given query in training set can be divided into several groups in which the docu-ments with same labels are gathered together. The groups are the basis of two new samples. We make the pair of groups as the pairwise method. The first pattern is constructed of a document with higher level label and a group with lower level label, which is called one-group sample. The second pattern is constructed of a group with higher level label and a group with lower level label, which is called group-group sample.

Group ranking framework is similar to the top-k ranking framework. However, the samples of the two methods are different, and the meaning of k is different.The true loss of group ranking is defined as: l ( f ( x ) ; y ) = where the r is decided by the number of documents with the higher label in the group ranking samples, and the expected risk of group ranking loss becomes where X is the input space whose elements are the group samples to be ranked, Y is the output space whose elements are permutations of groups. and the optimal ranking func-tion with respect to the group ranking true loss is: where G r ( j 1 ; j 2 ; : : : ; j r ) = y 2 Y j y ( t ) = j notes a group sample in which all the permutations have the top-r,which is decided by the number of documents with higher label, true loss. Our loss function is based on the group samples, which is described as follows:
L ( f ; x g ; y g ) = The function only describes the loss of a group sample, and there are two loss function for two types of group samples; x g is a group sample. y g is a ranked list of x g , which ranks the documents with higher label upon the lower label group; r represents the positions where the relevant documents oc-cupy. In the one-group loss function r equals to 1, and in the group-group loss function r equals to the number of doc-uments in the group with higher label. The parameter n is the length of optimum ranked list. Algorithm 1 shows the learning algorithm of group ranking. It is clear that the loss becomes greater when increasing the scores of irrelevant doc-uments.

Group sample can be taken as a ranked list with respect to a virtual given query. Therefore, the theory analysis of the top-k ranking framework [3] also works for the group ranking framework.
We evaluate our methods on the Letor3.0 data set re-leased by Microsoft Research Asia. This data set contains two collections: the OHSUMED collection, the .Gov col-lection. The collections we use to evaluate our experiment Algorithm1 Group ranking algorithm
Input: a set of listwise examples f Iteration T , learning rate
Constructing group samples from listwise samples: f ( X g ; Y g ) j ( x 1 ; y 1 ) ; ( x 2 ; y 2 ) ; : : : ; ( x Initialize parameter ! For t=1 to T do
Compute gradient  X  ! : Update ! = !  X  ! End For Output: parameter vector of the ranking function: ! . are OHSUMED and TD2003, TD2004 which are the sub-sets from .Gov collection. We adopt NDCG@N and MAP to evaluate the performance of the learned ranking function.
The basic information about relevant documents to a given query in three collections is shown in Table 3.

From Table 3 , we can obtain that 10 is not the perfect setting for the top-k ranking method, and it can increase likelihood loss due to the irrelevant documents by setting k to 10. Even though, the top-10 method still better than original ListMLE method. In the collection OHSUMED, the relevance judgments are represented by multiple labels, Top-k loss function can not distinguish multiple labels, while the group sample is easily to solve the problem. It can list the group with label 2 before the ones with label 0 and 1; also list the group with label 1 before the one with label 0. The labels for .Gov collection are only two types: relevant and ir-relevant, the group-group sample equal to the top-k ranking sample, in which k is the number of relevant documents.
In this section, we examine the effectiveness of group rank-ing methods. The results of group ranking methods on OHSUMED, TD2003 and TD2004 collection are shown in Tables 4, 5 and 6.

From the tables, we can see that all the methods with modifying ListMLE algorithm significantly boost the rank-ing accuracies. In addition, the group ranking methods achieve best performance, which clearly validates our argu-ment that it can improve the ranking performance by adopt-ing the true loss based on relevance documents only. One-group method performs better than ListNet and top-10 ranking method on average. Especially, it achieves the best performance on the TD2004 collection. The improve-ment shows that it is effective to adopt group loss function. However, compared with the top-10 method, it seems that the effectiveness is not obvious to avoid randomly selection of optimum ranked list. In some cases, the accuracies may be decreased. The possible explanation is that the probabil-ity space which decided by the ranked list in the collection is well formed. So we expend one relevant document to a group of documents with the same relevant label to construct the group sample. The ranked list for the loss function is also selected randomly.

The group-group samples are constructed of a group with higher label and a group of with lower label. The results show that the group-group methods outperform the base-line of Top-10 ListMLE and One-group method in most cases. The reason why group-group method is superior to one-group may be the number of samples. It is clear that the number of one-group samples is far larger than that of group-group samples. More samples may bring greater loss. It may play a decisive role in most cases.

In this section, we also examine the performance of our methods compared with the existing methods. We select several representative ranking algorithms, and OHSUMED and TD2003 as the test collections, since they have differ-ent types of relevance judgments. The performance of the ranking methods is shown in Table 7 and Table 8.
The results on the test collections show that methods of group ranking are useful in enhancing performance of IR sys-tems. Although the one-group method only achieve an aver-age performance of ranking methods, the group-group rank-ing method gains a notable improvement over existing rank-Table 7: Ranking accuracies of ranking methods on OHSUMED collection Table 8: Ranking accuracies of ranking methods on TD2003 ing methods by learning a suitable model based on group loss function and group samples. From the above experi-mental results, one important conclusion can be drawn that group ranking methods are feasible to enhance the ranking performance.
In this paper, we has proposed two novel approaches to improve the ranking performance on the framework of group ranking. The results of our experiments demonstrate that the group ranking methods, especially the group-group rank-ing method, can significantly outperform their original ver-sions, and also many other ranking methods.

For future work, we will continue to study the theory ba-sis of the group ranking framework to examine whether the ranking performance can be improved further. In this pa-per our work is based on ListMLE, we will also exploit the odds to using other existing method to implement the group ranking methods.
This work is supported by grant from the Natural Sci-ence Foundation of China (No.60673039 and 60973068) , the National High Tech Research and Development Plan of China (No.2006AA01Z151), National Social Science Foun-dation of China (No.08BTQ025), the Project Sponsored by the Scientific Research Foundation for the Returned Over-seas Chinese Scholars, State Education Ministry and The Research Fund for the Doctoral Program of Higher Educa-tion (No.20090041110002). [1] T. Y. Liu. Learning to rank for information retrieval. [2] T. Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: [3] F. Xia, T. Y. Liu, and H. Li. Statistical consistency of [4] F. Xia, T. Y. Liu, J. Wang, W. Zhang, and H. Li.
