 While there has been much research in sentiment analysis on the tasks of subjectivity detection and polarity classification, there has been less work on other types of categorizations that can be imposed upon subjective expressions.

In this paper, we focus on the views that an opin-ion expression evokes. By views, we understand the perspective of the holder of some opinion. We dis-tinguish between the two most common types: ex-pressions conveying sentiment of the entities partic-ipating in the event denoted by the opinion word, referred to as actor views (e.g. disappointed in (1) or praised in (2)), and expressions conveying sen-timent of the speaker of the utterance, referred to as speaker views (e.g. excelled in (3) or wasted in (4)). The distinction between those categories is relevant for related tasks in sentiment analysis, most impor-tantly, opinion holder and target extraction. This has already been demonstrated for verbs (Wiegand and Ruppenhofer, 2015). For example, even though the noun Peter has the same grammatical relation to the opinion verb in (5) &amp; (6), in the former sentence it is a holder but in the latter it is a target. Similar cases can be observed for opinion nouns (7) &amp; (8) and opinion adjectives (9) &amp; (10). Only the knowl-edge of sentiment views helps us to assign opinion roles correctly. While the distinction of sentiment views is not new, we put a different emphasis on this task. Our focus is on the prior meaning that opinion words evoke. Hence we consider this as a word-level task. Every opinion word from a sentiment lexicon is to be cat-egorized as conveying either an actor or a speaker view. Our aim is to find comprehensive methods to automatically categorize opinion words of various parts of speech (verbs, nouns, adjectives). The re-sulting lexical resources are indispensable for open-domain categorization. Previous work focused on contextual classification of sentiment views (Johans-son and Moschitti, 2013). Wiegand and Ruppen-hofer (2015) showed that while prior lexical knowl-edge of sentiment views is effective in transferring opinion role extractors to other domains, this does not apply to contextual classifiers.

In this work, we focus on linguistic properties for predicting sentiment views. We examine in how far morphological information can be used. Distribu-tional and syntactic information is also considered. In terms of lexical resources, we examine WordNet and FrameNet. We show that information from a sentiment lexicon can give some additional clues.
In order to combine the different features to pre-dict the sentiment views evoked by opinion words we employ supervised classification. As a classi-fier, we use Markov Logic Networks (Richardson and Matthew, 2006) since they do not only allow us to define features for instances (i.e. opinion words) but also to formulate global constraints between dif-ferent instances. The latter cannot be expressed by traditional classifiers (e.g. SVM). We examine two types of constraints: consistency between instances that are distributionally similar and consistency be-tween morphologically related instances.

Finally, we also examine the relationship between prior lexical information (i.e. our approach) and contextual annotation in the MPQA corpus. The annotation scheme of the MPQA corpus (Wiebe et al., 2005) was the first work to address the dis-tinction between different sentiment views. The two sentiment views are referred to as direct subjectivity ( = actor view) and expressive subjectivity ( = speaker view). In subsequent research, some approaches have been proposed to distinguish these two cate-gories in the MPQA corpus. The most extensive work is Johansson and Moschitti (2013). Since MPQA provides annotation regarding sentiment in context, sentiment views are exclusively considered in contextual classification. The fact that it is the opinion words that convey those views, as we do in this paper, is not addressed. Unlike in this paper, the focus of Johansson and Moschitti (2013) is also on optimizing a machine-learning classifier, in particu-lar to model the interaction between different sub-jective phrases within the same sentence.
 Some of the lexical resources we examine, i.e. WordNet ( X 4.1) and FrameNet ( X 4.2), have also been employed in Breck et al. (2007) who, like Jo-hansson and Moschitti (2013), also deal with con-textual (sentiment) classification. However, the au-thors do not examine in how far these individual re-sources separate speaker and actor views.

Maks and Vossen (2012b) link sentiment views to opinion words as part of a lexicon model for senti-ment analysis. Maks and Vossen (2012a) also exam-ine a corpus-driven method to induce opinion words for the different sentiment views. The authors, how-ever, conclude that their approach, which sees news articles as a source for actor views and news com-ments as a source for speaker views, is not suffi-ciently effective.
 The work most closely related to our research is Wiegand and Ruppenhofer (2015). Opinion words are categorized according to their sentiment view. Our work substantially goes beyond that previous research: Firstly, Wiegand and Ruppenhofer (2015) only consider distributional similarity for inducing opinion views. In this work, we consider various linguistic features and also compare this with distri-butional information. Secondly, Wiegand and Rup-penhofer (2015) only consider opinion verbs, while we also consider opinion nouns and opinion adjec-tives.

Wiegand and Ruppenhofer (2015) distinguish be-tween two types of actor views, agent views and pa-tient views . The former take their opinion holder as an agent and their target as a patient (typical verbs are criticize , love , believe ), while the latter align their roles inversely (typical verbs are disappoint , please , interest ). Since this distinction between ac-tor views does not exist among nouns or adjectives, we consider one merged (actor-view) category for all three parts of speech in this paper. We manually annotated all verbs, nouns and adjec-tives contained in the Subjectivity Lexicon (Wilson et al., 2005) for view type. The dataset comprises 2502 adjectives, 1676 nouns and 1175 verbs. Since Wiegand and Ruppenhofer (2015), we adhere to the annotation process proposed in that paper. That is, the basis of the annotation were online dictionaries (e.g. Macmillan Dictionary ) which provide both a word definition and example sentences. Each word is either labeled as primarily conveying an actor or a speaker view. (Our categorization is binary.) On a subset of 250 words for each part of speech, we com-puted an interannotation agreement (Cohen X  X   X  ) of 61 . 9 , 71 . 9 and 60 . 1 for verbs, nouns and adjectives, respectively. This agreement can be considered sub-stantial (Landis and Koch, 1977). Table 1 shows the distribution of the different sentiment views among the different parts of speech.

The expressions comprising our gold standard do not represent anywhere near the full set of English subjective words with these parts of speech. Other-wise, an automatic categorization would not be nec-essary in the presence of our gold standard. The classification approach that we propose in this pa-per, which works well with few labeled training data, would also be helpful for categorizing senti-ment views on much larger sets of subjective expres-sions. 4.1 WordNet WordNet (Miller et al., 1990) is the largest lexical ontology for the English language. It is organized in synsets. However, we want to assign categories to words. Due to the lack of robust word sense disam-biguation, in order to use this resource, we consider the union of synsets in which a word with the same part of speech to be categorized is contained. 4.1.1 Gloss Information (GLOSS)
One common way to harness WordNet is by tak-ing into account its glosses. A gloss represents some explanatory text for each synset, usually some defi-nition of the concept. We use the words from those glosses as features in a supervised classifier. We as-sume that opinion words conveying the same senti-ment view also contain similar glosses.

Glosses are a special type of feature. It is basically a bag-of-words feature set, i.e. a low-level feature set, which is known to be sparse yet effective when sufficient training data are used. All the other fea-tures presented in this paper are high-level features, i.e. more frequently occurring features already being effective if only few labeled data are used. Glosses are one of the most frequently used features for lexi-con induction tasks in sentiment analysis (Esuli and Sebastiani, 2005; Andreevskaia and Bergler, 2006; Gyamfi et al., 2009; Choi and Wiebe, 2014; Kang et al., 2014). We will consider them as a baseline, showing that our proposed high-level features are more suitable for our task. 4.1.2 Lexicographer Files (LEX)
Lexicographer files organize the synset inventory of WordNet into a coarse-grained set of semantic categories. In total, there are 45 categories for the of such a coarse-grained inventory is that it should require only few labeled training data in supervised classification. 4.2 FrameNet (FN) FrameNet (Baker et al., 1998) is a semantic resource that has been found useful for subtasks of sentiment analysis related to ours, i.e. opinion holder/target extraction (Bethard et al., 2004; Kim and Hovy, 2006). It includes a large set of more than 1 , 200 semantic frames that comprise words with similar semantic behaviour. As a feature we use the frame-membership of the opinion words, assuming that different frames are associated with different senti-ment views. We use FrameNet version 1.5. 4.3 Subcategorization Frames (SUB) Subcategorization frames could also be predictive. For example, actor views demand the presence of an explicit entity that utters some opinion, i.e. the opin-ion holder. For a speaker view, this entity remains implicit. This should be reflected in the argument valence of the respective opinion words. We employ the subcategorization frames encoded in COMLEX (Grishman et al., 1994) for verbs and adjectives, and NOMLEX (Macleod et al., 1998) for nouns. 4.4 Morphological Information (MORPH) As morphological information, we consider deriva-tional affixes. Table 2 lists our choice of prefixes and suffixes. We only included affixes that occurred at least 10 times in our dataset.

We distinguish between sentiment and neutral af-fixes. The sentiment affixes are affixes which, due to their meaning, suggest a sentiment view. For exam-ple, mis-as in misinterpret indicates that the speaker believes that a given interpretation is incorrect. -able as in admirable has the meaning of capable of which corresponds to an evaluation of the speaker. We could only find sentiment affixes for speaker views.
The neutral affixes that we use specify which kinds of bases they can combine with. For example, the noun suffix -ness as in foolishness indicates that the word originates from an adjective (i.e. foolish ). Even though this knowledge is syntactic, it may give us some clue as to what sentiment view an opinion word conveys. Table 1 shows that adjectives pre-dominantly carry speaker views. Therefore, a noun ending in -ness (thus originating from an adjective) may be similarly likely to convey a speaker view. 4.5 Context Patterns (PATT) Wiegand and Ruppenhofer (2015) proposed patterns for actor-view and speaker-view verbs . For actor views (PATT actor), they rely on prototypical opin-ion holders (protoOHs) , i.e. common nouns, such as opponents or critics , that act like opinion hold-ers (Wiegand and Klakow, 2011). If a verb often co-occurs with an opinion holder  X  Wiegand and Ruppenhofer (2015) take protoOHs as a proxy  X  then this is a good indicator of being an actor view (speaker views, per definition, do not have any opin-ion holder as their dependent). ProtoOHs can simi-larly be used to extract actor-view nouns and adjec-tives. For speaker views (PATT speaker), Wiegand and Ruppenhofer introduced reproach patterns , e.g. blamed for X as in (11). These patterns can also be applied to nouns (12) but not to adjectives. For the latter, we did not find any pattern. The patterns were applied to the North American News Text Corpus (LDC95T21). 4.6 Polarity Information (POLAR) We also investigate in how far polarity information correlates with sentiment views. This information is obtained from the Subjectivity Lexicon (Wilson et al., 2005). Each opinion word is assigned a polarity type, i.e. positive , negative or neutral . Markov Logic Networks (MLNs) are a supervised classifier combining first-order logic with probabili-ties. MLNs are a set of pairs ( F i ,w i ) where F i is a first-order logic formula and w i a real valued weight associated with F i . They build a template for con-structing a Markov network given a set of constants C . The probability distribution that is estimated is a log-linear model where n i ( x ) is the number of groundings in F i in x and Z is a normalization constant. As an implemen-tation, we use thebeast (Riedel, 2008).

We employ MLNs since they allow us (in addi-tion to including ordinary features, i.e.  X 4.1- X 4.6) to formulate constraints holding between individual instances. Such global constraints have been effec-tively exploited with MLNs in related tasks, such as semantic-role labeling (Meza-Ruiz and Riedel, 2009), anaphora resolution (Hou et al., 2013), ques-tion answering (Khot et al., 2015) and discourse-based sentiment analysis (Zirn et al., 2011). We formulate three such constraints. Two of them are based on the two most effective types of word simi-larities from Wiegand and Ruppenhofer (2015). The first word similarity measures the cosine of word vectors representing opinion words produced by Word2Vec -embeddings (Mikolov et al., 2013). The second word similarity is represented by the met-ric of Lin (1998), which exploits the rich set of dependency-relation labels in the context of distri-considers morphological relatedness by which we understand two words deriving from two different parts of speech but belonging to the same lexical root and therefore carrying similar meaning (e.g. happi-ness.noun and happy.adj ). We obtain that type of relatedness from WordNet (Miller et al., 1990).
Table 3 lists our constraints. They state that if for two opinion words some similarity or morpho-logical relatedness holds, then these words should convey the same sentiment view. For the two types of word-similarity consistencies we considered the top 3 most similar words for each noun, and the top 5 most similar words for each verb and adjective. These values were determined empirically. For the generation of word vectors, we used 200 dimensions along the default configuration of Word2Vec . Word similarity and word vectors were generated from the North American News Text Corpus. For our evaluation of supervised classification, we focus on a setting in which only few labeled training data are available. We sampled from our gold stan-dard 20% of the labeled training data. The remain-ing 80% are used as test data. This process was re-peated five times. We report performance averaged over these five (test) samples. We focus on small training sizes since we think that for the given lex-icon induction task, we should pursue an approach that requires little human annotation. Moreover, we show that our approach yields good results despite the absence of large amounts of training data. 6.1 High-Precision Features Before we evaluate supervised classification, we look for each part of speech at the 10 features with the highest precision (for each of the two views) as displayed in Table 4. This provides a good overview of the quality of different features. Since we do not have an equal class distribution, we also list a baseline-precision that always predicts the senti-ment view under consideration. Since this is just an exploratory experiment, we measure precision on the entire dataset. We exclude the WordNet glosses ( X 4.1.1) from our analysis as we found individual words from glosses too difficult to interpret.
Table 4 shows that features from all feature groups ( X 4.1- X 4.6) achieve a high precision. Sub-categorization features ( X 4.3) are very predictive for verbs conveying actor views. The frame types that are predictive mostly have in common that one of their arguments is some proposition (13)-(17). This is also true for adjectives (18). FrameNet-frames ( X 4.2) achieve high precision; but the only frame with good coverage is Stimulus-focus for adjectives conveying a speaker view.
 There are fewer lexicographer files ( X 4.1.2) than FrameNet-frames in Table 4, but some of them have high coverage, most notably LEX person for speaker-view nouns and LEX feeling for actor-view nouns. Given the strength of LEX person , we con-clude that most opinion nouns denoting persons tend to be speaker views (e.g. idiot or loser ). There are also several predictive lexicographer files whose la-bel seems fairly unintuitive, e.g. LEX weather for speaker-view verbs or LEX animal for speaker-view nouns. These are not errors, however. They actually concern words that convey opinions in metaphorical usage. For instance, cloud (a typical weather verb) conveys a speaker view if it is used metaphorically as in The stroke clouded memories of her youth. Nouns denoting animals, such as bull and dragon , convey a speaker view if they are meant to describe a human being ( She is a real dragon! ). Other noun classes follow this pattern, e.g. body (parts) with terms such as backbone or bum .

Simple morphological features ( X 4.4) also seem to be meaningful. In particular, the noun suffix -ity (occurring 132 times in our set of opinion nouns) is indicative of speaker views. The relevant nouns are derived from adjectives (Table 2) and the set of ad-jectives predominantly conveys speaker views (Ta-ble 1).

Even plain polarity information ( X 4.6) has some significance. Neutral sentiment verbs often convey an actor view, such as opinion , utterance or view .
The fact that the pattern-feature ( X 4.5) also ap-pears on the list of actor-view nouns and adjectives suggests that it is not only effective for verbs as shown in Wiegand and Ruppenhofer (2015) but also for nouns and adjectives.

Finally, we performed an ablation experiment in which we trained a classifier with all of these fea-tures in MLNs and compared it to another classifier in which each of the feature groups ( POLAR , LEX , MORPH etc.) was removed, one by one. We com-puted statistical significance (t-test), testing whether the classifier trained on a feature set in which one feature group was removed performs significantly worse than a classifier with all features. We found that, at a significance level p&lt; 0 . 05 , this is always the case, with the exception of LEX (here, the sig-nificance level is p = 0 . 0552 ). This is proof that features from most feature groups contain informa-tion that is to some extent complementary. 6.2 Classification Table 5 lists the different types of classifiers we consider. As one baseline, we consider the graph-based approach graph from Wiegand and Ruppen-hofer (2015) which starts with the seeds gained by agation (Talukdar et al., 2008) based on a distri-butional similarity graph (using the metric by Lin (1998)). graph is the only classifier not depending on manually labeled training data. So far, it has only been examined on verbs. As a further baseline, we consider our features from  X 4 on an SVM. (We use ered as a state-of-the-art classifier that, unlike mln , cannot incorporate global constraints (Table 3).
Table 6 shows the results. Both graph and svm are significantly outperformed. graph performs bet-ter on verbs (in terms of F-score) than on nouns and adjectives. It is also for these parts of speech that the global constraints w2v and lin notably improve the performance of mln . Global constraints have a lesser impact on verbs. However, a combination of global constraints is effective, as well as a combi-nation of graph and mln . The best overall results are obtained by the combination of mln with global constraints and graph . These results suggest that our new features (including global constraints) are use-ful and complementary to previous work, i.e. graph . Figure 1 compares the feature derived from Word-Net glosses ( X 4.1.1), a standard feature for lexicon induction, with the remaining features we use on a learning curve. This feature performs poorly if only few labeled training data are used. Our proposed feature set is consistently better. The combination of glosses and our proposed features is only helpful if many labeled training instances are used ( &gt; 60% ). 6.3 Prior Labels and Context Labels So far, we have considered sentiment views as prior information of words. Now we relate those labels to sentiment views annotated in context. For that, we consider the view annotation in the MPQA corpus.
Table 7 shows that prior labels of opinion words largely coincide with the respective context labels. This proves that it is a valid approach to compile lex-icons with sentiment views, which can subsequently be used in contextual sentiment-view classification.
However, in Table 7, we still observe mismatches between prior and contextual labels. This mostly concerns actor-view words in speaker-view con-texts. We examine this mismatch more closely on nouns (highlighted in gray ) where this confusion is greatest. In MPQA, most subjective expressions that are annotated are sequences of tokens rather than individual words. We found that the largest set of disagreements derives from the nature of MPQA X  X  contextual annotation. The annotators were asked to label spans that expressed opinions that are salient in the document context. Often these are larger spans composed of multiple smaller subjective ex-pressions. The component expressions were not kept track of because the opinion expressed by the larger span was more salient on the document level.
For example, the annotation of the subjective phrase this must be a warning as a speaker view (containing the actor noun warning ), in our opin-ion is primarily triggered by the epistemic modal verb must , which signals that the speaker feels com-pelled to come to the conclusion that this is a warn-ing . The actor view of warning is not invalidated by this: it is just backgrounded relative to the speaker view introduced by the modal verb, which, going in parallel with its greater prominence, is also the syn-tactic governor of the verb phrase be a warning , of which the actor view warning is part. Our evalua-tion scheme might thus detect a match between our prior annotation for the modal must and the MPQA X  X  larger phrase. But since the less prominent actor view was not picked up by the MPQA annotators, our prior annotation has no counterpart. Copula con-structions similarly represent instances, where the speaker performs a speech act (e.g. a warning) by using the copular construction (e.g. This is a warn-ing ). Here, the speaker is identical to the actor of the warning. Practically, it makes no difference whether we call such a case speaker or actor view, as long as we can recognize that the actor is the speaker.
In order to show that the annotation of MPQA fo-cuses on the more salient opinions and thus senti-ment views as conveyed by less prominent expres-sions are not considered (and largely account for the mismatches in Table 7), we designed a supervised classifier whose features indicate whether a mention of an actor-view expression in a subjective phrase is salient. The features are displayed in Table 8.
The key salience features regarding speaker views, i.e. modal and copular , were already dis-cussed above. Features indicating the salience of the actor-view word address the subcategorization frame of the word. Cases in which there is a per-son as some subcategorized argument ( personArg ) often imply an opinion holder. The presence of an (explicit) opinion holder indicates an actor view. A proposition as argument of an opinion word is typically the proposition of some opinion holder ( propArg ) and not of the speaker of the utterance.
For this experiment we take the detection of sub-jective phrases as given. (Only the information regarding contextual sentiment views is withheld.) This allows us to define features that explicitly look into the entire text span constituting the subjective phrase in which each opinion word is contained. The two length features ( shortPhrase and longPhrase ) make use of this information. If a phrase is long, chances are high that there are other more salient opinion words contained than the one under con-sideration. In short subjective phrases, the pres-ence of other salient words in it is unlikely. This is supported by the fact that the average length of subjective phrases with a speaker view (in which an actor-view opinion noun occurs) are 5 . 4 tokens while actor-view phrases (that include an actor-view noun) only have an average length of 2 . 3 tokens.
The majority features ( majActor and majSpeaker ) also exploit the information of the entire subjective phrase. We argue that the sentiment view of the phrase is likely to coincide with the view of the ma-jority of the opinion words contained in that phrase.
Table 9 shows how these features separate the mentions of an actor-view opinion noun into contex-tual actor views and speaker views. We report clas-sification using an SVM (10-fold cross-validation). With only those few features, we largely outperform the baseline always classifying an instance as an ac-tor view (i.e. the majority class). Table 10 displays the precision of each individual feature, supporting that these features are effective. These experiments show that there is indeed a systematic relationship between salience and contextual sentiment views. We examined different types of features and classi-fiers for the categorization of sentiment views that opinion words convey. We found that many features are effective for this task. A detailed feature anal-ysis provided linguistic insights into the nature of sentiment views. As a classifier, MLNs performed best. This classifier has the advantage that global constraints can be incorporated, which raises classi-fication performance on nouns and adjectives. Our approach outperforms a previously proposed graph-based approach evaluated on opinion verbs. We also demonstrated that prior sentiment views correlate with contextual sentiment views on MPQA.

