 Hui Yang  X  Minjie Zhang
Abstract As the number and diversity of distributed Web databases on the Internet expo-
Given database language models that describe the content of each database, database selec-users. In this paper, we propose a database selection approach based on statistical language the databases are ranked by the similarity to the query according to the estimated language model. Two-stage smoothed language models are presented to circumvent inaccuracy due to word sparseness. Experimental results demonstrate that such a language modeling approach is competitive with current state-of-the-art database selection approaches.
Keywords Database language model . Text database selection retrieval . Hierarchical topics . Statistical language modeling 1. Introduction to determine which databases to search for desired information. To reduce network traffic to those potentially useful databases has become more and more attractive to both users and information science researchers.
 a structured hierarchy of topics, provides a useful and efficient way to organize and manage for database classification and have received encouraging achievements. Gauch et al. (1996) on the number of matches that each query probe generates from the databases. The formation of queries comes from document classifiers. More recently, Ipeirotis and Gravano (2004) of a topic hierarchy to compensate for incomplete content summaries. In Meng et al. (2002) concept description is treated as a query that is submitted to the database. The documents retrieved from the database are used to calculate the similarity between the concept and framework for the hierarchical classification of distributed databases with a set of Naive Baysian classifiers.

A number of different algorithms for database selection have been proposed during the last decade, including GLOSS (Gravano et al. 1999), CORI (Callan 2000), RDD (Voorhees et al. 1995), CVV (Yuwono and Lee 1997), query probing (Hawking and Thistlewaite 1999, Meng et al. 1998), lexicon inspection (Zobel 1997), a probability model (Baumgarten 1999), and a KL-divergence algorithm (Xu and Croft 1999). Moreover, extensive comparative studies among these algorithms have also been performed (French et al. 1999, Craswell et al. 2000,
D X  X ouza et al. 2000, Powell and French 2003). In general, database selection algorithms do not directly have access to each database. Instead, they mainly interact with a database and their frequency occurrence, and the number of documents containing each index term.
The statistical information indicates the approximate content of databases. With database language models, selection algorithms compute a ranking score for each database which characterizes its relative usefulness to a query. Thus database language models are perhaps the most important component of database selection.

The use of statistical language models for information retrieval has been studied exten-sively for decades and has generated encouraging results (David et al. 1999, Hiermstra 2001,
Song and Croft 2002, Lafferty and Zhai 2002). However, most existing language models are only involved in document indexing and document retrieval. The work in statistical language models for database selection seems relatively new. However, there are several works that have looked at development of the database language models (Xu and Croft 1999, Si and
Callan 2003, Si et al. 2002). Xu and Croft (1999) proposed a KL-divergence algorithm that used the Kullback-Leibler (KL) divergence between the word frequency distribution of the query and the database to measure the relevance of the database to the query. This algorithm is represented in a language modeling framework consisting of a database language model and a query language model, respectively. Si et al. (2002, 2003) proposed a language model (LM) algorithm that was the extension of the KL-based database selection method. The LM algorithm incorporated the database and query language models into an integrated language selection.

In this paper, we present a novel way of looking at the problem of database selection from the viewpoint of statistical language modeling. The statistical language model explored in this paper is, in practice, a two-stage database language model which is the combination of a class-based language model and a term-based language model. This work is an extension of our previous research (Yang and Zhang 2004) which introduced a clustering method for conceptually divided into two distinct steps: (1) First, with a class-based language model, the search focuses on databases in confined (2) Second, the selection algorithm computes the likelihood of separate databases to the
Obviously, the two-stage language model approach explicitly captures the different influ-settings of selection parameters. Our experimental results reported here have demonstrated traditional selection methods, such as the state-of-the-art CORI method. Our study has three important contributions:
First, we acquire database models independently according to different search stages, and intentionally keep these models separate.

Second, we consider general ways of combining different database models together by introducing suitable parameters. The synthetic model can be further individualized and optimized for database selection.

Third, we propose a query expansion method to alleviate the problem of query ambiguity using a query translation model.

It is believed that using such simple and effective language models as a solid baseline method opens up the door to the improvement in database selection performance.
The paper is organized as follows: in Section 2 we discuss the language modeling approach to IR, and briefly review previous work in this area; Section 3 lays out the basic theory of the two-stage database language model and develops the formulas for its simple realization; experimental methodology and a series of experiments to evaluate our language model are presented in Section 4 and 5; and finally, conclusions and contributions of this work are summarized in Section 6. 2. The language models in information retrieval documents, the formulation of a user query, and the construction of a ranking function which compares the likelihood between the document and the user X  X  information need. Language models explicitly define how documents and queries should be analyzed and they reasonably model the way that the ranking is produced. So research on language models has received of language models have been developed. Among them, three commonly used traditional model (Salton and McGill 1983) and the classic probabilistic model (Robertson and Jones 1976, Van Rijsbergen 1992).
 The Boolean model is a simple retrieval model based on set theory and Boolean algebra.
The relevance of documents is specified as a Boolean expression which has a strict logic implication (Salton and McGill 1983). Due to the exact matching strategy which is unable to recognize uncertain matches, the Boolean model is considered as the weakest classic method.
The vector-space model (Salton and McGill 1983) assumes that a query and a document are both represented by term vectors. The similarity between a query Q and a document D ={ d 1 , d 2 ,..., d n } can be measured by of the two vectors. The similarity values are assumed to reflect the degree of relevance of individual document with respect to a user query. Similarity measures and term-weighting methods used in the vector-space model strongly depend on the type of query and the charac-great challenge.

The classic probabilistic modeling approaches (Robertson 1977, Robertson and Jones relevance of a document to a user query. In essence, the probabilistic model is an adap-vance of a document to a query is simply the product of the individual term probabilities
P ( Q | D ) = difficult task.

In recent years, there have been controversies about which model, the vector-space model own advantages and disadvantages in different situations. Nevertheless, they both play an important role in the development of information retrieval.
 models mentioned above, several researchers attempt to develop some hybrid models that combine the strengths of these three models within a unified framework. For example, Turtle and Croft (1990) introduced an inference network model in which the indexing and retrieval models are integrated by making inferences of concepts from features by means of a Bayesian probabilistic models. Other studies on mixture retrieval models can be found in Wong et al. (1987) and Van Rijsbergen (1989).
 models.
 The statistical language modeling approach was first proposed in Ponte and Croft (1998).
Ponte et al. presented a simple unigram document language model, which estimated the probability of producing the query for each document, and then ranked the document ac-word generation mechanisms are incorporated within the same model using Hidden Markov
Models (HMM) theory. The retrieval parameters are trained and optimized through a learning procedure. In a linguistically motivated model proposed by Hiemstra (1998), documents and queries are defined by an ordered sequence of single terms rather than unordered collec-tions of terms or phrases. The probability estimation of df statistical natural language processing.
 has been developed in a recent study by Lafferty and Zhai (2001). A probabilistic retrieval model is motivated to unify document models and query models in a framework based on Bayesian decision theory. Kullback-Leibler divergence is introduced as a loss function for collection, are used for estimating expanded query models.

There are other statistical language modeling approaches to information retrieval including
The statistical language models are often approximated by a N -gram model for estimating the probability of query terms.

The simplicity and robustness of the statistical language modeling approach make it a new important research direction for text retrieval. 3. Two-stage database language models for database selection
One important advantage of the two-stage language models over traditional database language general ways to represent them.

The basic idea of the statistical language modeling approach to information retrieval is to define a N -gram language model for each document in a collection (Ponte and Croft, 1998).
For each document, the language model computes the conditional probabilities of generating a sequence of query terms, and then multiplies these probabilities in order to estimate the likelihood value of the document with respect to the query.

Our work is originally motivated by the fact that a textual database can be regarded as a consists of a collection of text documents. Therefore, a document language model can be easily borrowed to model the likelihood measure of the database to a user query.
In this section, we extend recent advance in the use of statistical language models to information retrieval and explore two-stage database language models based on statistical language modeling for database selection. A brief overview of this language model and its application to database selection is described as follows.

As we know, database selection is difficult partly because database selection algorithms scriptions about the database contents. Resource descriptions usually come from cooper-tive approach named query-based sampling . The basic idea behind the query-based sam-pling methods is that a set of simple queries are submitted to each searchable database, and a small document sample (e.g., 300 documents) is extracted via querying. The doc-ument sample is used to construct an approximate resource description for this database.
Sampling (QBS) method (Callan and Connell 2001) and the Focused-probing Sampling (FPS) method (Ipeirotis and Gravano 2002). Moreover, Ipeirotis and Gravano (2004) pro-posed a shrinkage method to create category content summaries of databases using a topic hierarchy.

However, as described earlier, the emphasis of our work is on building effective database selection models to choose the useful databases for search. Therefore, the discussion about the acquisition of resource descriptions has been beyond the scope of this paper. For more
Hawking and Thistlewaite (1999) for the  X  X ooperative X  databases, and Callan and Connell (2001), Ipeirotis and Gravano (2002, 2004), Xu and Croft (1999) for the  X  X ncooperative X  databases. Here, we only focus on the construction of the database selection model, assuming that the resource descriptions for the databases have been known. 3.1. Hierarchical structure of topics for text databases recently, several large-scale commerce Web search services such as Yahoo and Infoseek have also adopted such hierarchies to manage the Wide World Web. By browsing these categories in the topic hierarchy, the users can conveniently find the appropriate topic that they are interested in. A simple hierarchical structure of topics is shown as Figure 1. We can give some formal definitions of a hierarchical structure of topics as follows: node corresponds to a topic.

In general, in such a topic hierarchy, topics are ordered from general topics at the higher cover different aspects of the parent class.

Definition 2. In each parent-child pair, the weight of the connection between parent class c and child class c k is denoted by w c p a given parent class, c p k ={ c 1 ,..., c k ,..., c T } , the weight w where N indicates the number of classes in the level in which the parent class c
T is the number of child classes in the parent class c p k hierarchy.
 feature vector in the feature space F c k .
 The features are the words that are strongly associated with one specific category (class). in the hierarchical classification scheme. Due to computational cost which is exponential in classes is out of the scope of this paper). For a given class c hierarchy.

Definition 4. Cluster c p is a parent class that consists of a number of child classes, c { c , c cluster c p , which is described as F c p ={ f 1 , f 2 ,..., of all the child classes, namely, F c p = F c 1  X  F c 2  X  X  X  X  X  feature vector in the feature space F c p .

Definition 5. With a hierarchical classification scheme with categories, c database S can be classified into one or more classes, which is denoted by a class set C C i  X  K ), where t S represented by the posterior probability P ( c i | S ) of class c is normalized into the range from 0 to 1.

It is possible for a large-scale general-purpose database to be assigned to multiple topics to categorize databases in a hierarchical classification scheme can be found in our previous work (Yang and Zhang 2004).

It is easily noted that if a database is assigned to a topic (a leaf node or a node of lower  X  software  X  must be in the class set C S of database S (see Figure 1).

The topic hierarchy and its associated Web databases could be used in a database selection select the more appropriate databases from those chosen databases based on the degree of relevance to the query.
 3.2. The statistical language modeling approach model.
 In automatic speech recognition, language models are capable of assigning a probability in English text can be characterized by a set of conditional probabilities P ( can be expressed as a product
P ( W ) = where P ( w i | w i  X  1 1 ) is the probability that the word
P ( w i | w i  X  1 1 ). At present, the dominant technology in language modeling used in IR is the
Consequently, the probability of a word string becomes the product of the probabilities of the individual words. Therefore, Eq. (1) can be simplified as
P ( W ) = distributions over words in V , which has V  X  1 independent parameters.

The basic idea of the statistical language modeling approach for database selection is to treat each database S as a language sample, and to estimate the conditional probability
P ( Q | S ), i.e., the probability of generating a query Q ={ q tion of a database S :
P ( Q | S ) = The databases are ranked by the probability P ( Q | S ).

Applying Bayes X  rule of probability, the posterior probability of the database S to the query Q can be written as
P ( S | Q ) = P ( Q
The P ( Q ) term represents the probability that query Q is generated from a document-
Q , it does not affect the ranking of the databases and can be ignored for the purpose of ranking databases. Therefore, Eq. (4) can be rewritten as
P ( S | Q )  X  P ( Q | S ) P ( S ) (6)
As to the P ( S ) term, it is a query-independent term, which measures the quality of the can be explored to capture database characteristics, e.g., the length of a database. In this work, since a user query probably involves in a variety of topics and we cannot predict its does not affect database ranking. A similar assumption has been made in most existing work (Zhai and Lafferty 2001, Ponte and Croft 1998, Song and Croft 1998). Therefore, in practice, the query.

Just as in the use of language model for speech recognition, one of the most obvious practical problems in applying statistical language modeling to IR is the problem of sparse more of the query terms.

The sparseness problem can be avoided by data smoothing methods. In most data smooth-ing methods, two distributions are used for smoothing. One is high probability for  X  X een X  words that occur in the database, and the other is low probability such as zero probabil-ity for  X  X nseen X  words. Smoothing methods tend to make distributions more uniform by discounting high probability and adjusting low probability upward. Not only do smoothing methods prevent zero probability, but they have also the potential to improve the accuracy and reliability of the models. 3.3. The smoothed two-stage language models
The generic language model for the two-stage database selection procedure can be refined match the query, and a concrete term-based model P ( Q | S ) for generating the most likely
Next, we present the generic two-stage models with data smoothing methods. 3.3.1. The smoothed class-based language model.

Assume that we have successfully assigned a feature space F topic hierarchy. Given a user query Q , it may be possible to make reasonable predictions of determining the appropriate topic(s) that the user is interested in by using the feature vectors in the feature space. Let F c k ={ f 1 , f 2 ,...,
Q ={ q vocabulary of all the classes in the topic hierarchy. Obviously, the feature space F class c k only contains a subset of features of the feature vocabulary F a certain topic class c k , the likelihood function of class c
P ( Q | c k ) = where P ( q i | c k ) is the probability of query term q i
As discussed earlier, due to computation cost, the feature space F describe a smoothing technique called the Jelinek-Mercer (JM) method (also called linear interpolation) (Jelinek and Mercer 1980) that combines the probability estimates of several language models within a unified model in order to alleviate the suffering from the data sparseness problem. The probability estimates of this linearly interpolated language model will become more reliable and more accurate.

In order to capture the common and non-discriminating words in a query, we assume that a query is generated by sampling words from a three-component mixture of language models with one component being P ( q | c k ), and the two others being the parent-class language model P ( q | c p k ) and the class-corpus language model P ( q P ( Q | c k ) =  X  ,  X  2 and and  X 
With reasonably smoothed class models, we can now consider the process of query genera-two steps: (1) First, it is smoothed with the correlation information about parent-child pairs. Since
Definition 4), the size of the feature space F c p k of the parent class c the feature space F c k . Therefore, such a parent-class model P ( q the contribution of different missing query terms in the feature space F relationship between  X  1 and  X  2 :  X  = w where w c p P ( Q | c k ) =  X  1 P ( Q | c k ) +  X  1 w c p (2) Second, in order to avoid the probability that some query terms may still be missing in with a class-corpus model. Note that, in the class-corpus model, the probability of term q
P ( q | C ), is much smaller than the probabilities in the first two feature spaces F
P ( q | c k ) and P ( q i | c p of all classes in the topic hierarchy.

In the use of the linearly interpolated language model, the coefficient in information loss and the improvement of selection performance. In our work, an automatic procedure, called EM (Expectation Maximization) (Dempster et al. 1977) is executed to learn feature spaces, the probability P JM ( Q | c k ) will be maximized with the optimum The EM-algorithm can be defined as follows.

First, the evaluation of the expectation is called the E-step of the algorithm, which is described as m
Second, the second step (the M -step) of the EM algorithm is to maximize the expectation that we computed in the first step. That is, These two steps are iterated as necessary to maximize the probability of the query given function.

With the smoothed class-based language model, one or more classes, whose likelihood probabilities P JM ( Q | c k ) are greater than the threshold at the category-specific stage depends on the threshold  X  empirically by a number of experiments.
 model, the likelihood of the database associated with the chosen topics to the query Q can be calculated as
P ( Q | S ) = where P ( c k | S ) is the posterior probability of class c
The databases will be ordered by the likelihood probabilities, and only the top K databases in the ranking list will be chosen as preliminary relevant databases to the query. 3.3.2. The smoothed term-based language model.

Once a number of relevant databases are chosen from the category-specific search stage, the of a database to a query is related to relevant documents in the database. Two important parameters, document quality and document quantity , are considered as selection criteria.
Document quality refers to the similarity degree of the most likely similar documents in the database, while document quantity is the number of relevant documents whose similarity scores are greater than a threshold. This approach is explored in the works of (Yu et al. 2000a, Ipeirotis and Gravano 2001). The second technique is a term-based method, which uses to characterize the usefulness of a database. The terms associated with the database provide approximately the content of the database. The ranking score based on term distributions reflects the relative usefulness of the database to a query. Related works are employed in Yang and Zhang (2004), Manber and Bigot (1997).

In our previous work (Yang and Zhang 2004), we learned that the term-based method has similar performance on database classification to that of the document-based method, but as for the computation time and resource consumption, the term-based method is far better than the document-based method. Hence, the language model that we propose here is based on the terms associated with the database. 3.3.2.1. A maximum likelihood estimation (MLE) language model smoothed with dirichlet prior. Given a database as a language sample, a straightforward method to estimate the prob-abilities of individual query terms is the Maximum Likelihood Estimation (MLE) (Moode and Graybill 1963). The Maximum Likelihood Estimation of the probability of term t the term distribution for database S is P ( t i | S ) = where C ( t i | S ) is the frequency occurrence of term t sum of the occurrence times of all terms in database S . The MLE assigns the probability of the observed data in the database as high as possible and assigns zero probability to the missing data.

Our term-based language model is a unigram language model based on the MLE. Given a user query Q, the probability P ( Q | S ) of a database S to the query can be expressed as P ( Q | S ) = For a database with a large number of documents, there is enough data for P to be robustly estimated. However, the MLE may still assign zero probabilities to query terms that do not appear in the database. Since the probability of the query is computed by multiplying the probability of individual query terms, these zeros are propagate and give a of the smoothing techniques for language models is the maximum a prior (MAP) estimator with Dirichlet Prior (DP) (Zaragoza et al. 2003). With a Dirichlet Prior with parameters ( m ={ m P ( Q | S ) = where  X  is a positive scalar; and m i is the posterior probability of query term q of training data C , which contains the average probability of the term through a geometric distribution of query terms. For a term t i , P ( t i | C ) is normalized, i.e., 3.3.2.2. Query expansion with translation models. Users usually submit short queries that function of the database to the query will suffer from the problem of query ambiguity. The use of query expansion can alleviate this problem. The basic idea of query expansion is to discover related terms or concepts, along with their relationships with query terms in the user X  X  query. The terms in the expanded query help disambiguate the meanings of query terms in the original query, which in turn make database selection more accurate.
In this paper, we propose a query expansion method which makes use of the relationships of the user query, and the topic classes of interests for the user. The intuition behind our approach is that once relevant topic classes are determined, the query can be mapped to a number of features associated with the identified topic classes. Assume that a set of topic classes, c 1 , c 2 ,..., c u , are relevant to the query Q . For each topic class c a query translation model T ( Q | F c k Q ) is used to map a query Q to a feature set F { f c ). These features are more specific (or related) terms in the relevant class c which provide sufficient content to clear up the ambiguity. Thus, an expanded query consists terms from the relevant topic classes. Using translation models, the term-based language model for the expanded query can be rewritten P
QE ( Q | S ) = P ( Q | S ) class c k for the database S , P ( c k | S )(recall Definition 5).

For computative simplification and analysis convenience, the likelihood function can be performed in the form of logarithm, which is described as P
QE ( Q | S )  X  log P ( Q | S ) + ranking remains identical to the original ranking.

As described previously, the key component in the query expansion model is the query translation model T ( Q | F c k Q ). This model is related to the feature set F which can be denoted as
T ( Q | F c k Q ) = T ( Q | c k ) P ( c k | F c k Q ) = T ( Q where T ( Q | c k ) is the degree of relevance of the topic c the weight of the expanded terms. To avoid the over-influence of the expanded query terms on the result of database selection, the weight of the expanded terms are usually diminished compared with the original query terms. Here, T ( Q | c k T ( Q | c k ) = where P JM ( Q | c k ) is known at the category-specific stage (recall Section 3.3.2.1). c (Kullback and Leibler 1951) which is an information theoretic measure of the divergence f ,  X  which information loss Q = f
Intuitively, we use a greedy algorithm to obtain the feature set F follows: 1. Initially, we begin with a current feature set F c k Q 2. At each state, we eliminate a feature f i in a way that allows as possible to F c k 3. Repeat step 2 until Q is increased to the desired information loss.

Once the feature set F c k Q is selected from the feature space F described as 4. Experimental design 4.1. The data sets
In order to demonstrate the effectiveness of our modeling techniques, we conduct a number of experiments to evaluate the selection performance of two-stage database language models. For our experiments, we do not use the TREC data set which is commonly used in distributed form of a hierarchical structure. Since our database language modeling approach is based on structured data sets: the Reuters 21578 data set and the LookSmart Web data set.
The Reuters 21578 data set (http://www.daviddlewis.com/resources/testcollections/  X  classification (D X  X lessio et al. 1995, Weighend et al. 1998, Kohler and Sahami 1997). This data set consists of 21578 articles labeled with 135 topics with no hierarchy structure. To second-level, and 96 leaf-level categories. 4 top-level categories come from meta-categories codes (economic indicators, currency, commodities and energy). 18 second-level categories
However, Reuters-21578 is a small and homogeneous collection, which makes it problem-atic to understand how our approach is applicable to large, complex and heterogeneous Web collections. To overcome this problem, we investigate the use of hierarchies for searching very heterogeneous Web contents. For our experiments, we use a large collection of heteroge-neous Web pages from LookSmart X  X  Web directory (www.Looksmart.com). The LookSmart of 370, 597 unique pages (from a May 1999 Web snapshot) as reported in (Dumais and Chen 2000), which have been hierarchically classified by trained professional Web editors.
Each Web page had been assigned to zero or more categories. Pre-processing is executed to extract plain text from each Web page by removing HTML markup tags. In addition, the title description and keyword fields from the META tag were also extracted because they provide useful descriptions of the Web page. After pre-processing, a pseudo document was generated as a description of the original Web page. 4.2. Evaluation metrics
At the category-specific search stage, we draw the 11 point average precision-recall curve to compare the selection performance between the JM model and non-interpolated model precision-recall curve plots the average precision values at each of the 11 recall points 0, 0.1, 0.2, . . . , 1.0, where precision P Class and recall R equations: P R At the term-specific search stage, we use the  X  R k ( E , of variations of three types of language models, namely, the MLE model, the DP model and the DP + QE model. Note that the names of all the models are abbreviated with the two initial letters (e.g., DP for Dirichlet Prior smoothing).

To compare the selection performance of our proposed approach, we provide a widely-used keyword-based technique X  X he CORI database selection algorithm (Callan 2000) as the experimental baseline. The CORI algorithm uses a variant of tf 1983) adapted for ranking databases. The relevance score of a database S to the query Q is calculated as: relevance score ( Q | S ) = where df is the document frequency of the query term q i in database S , cw is the number of indexing term occurrences in database S, avg cw is the average of cw in database set; C is the number of databases, and cf is the collection frequency of query terms q collection. Databases are then ranked based on the relevance score.

As to the performance metrics of database selection, we choose the comparison, which had been used by several researchers in database selection (Gravano et al. 1999, French et al. 1999, Callan and Connell 2001, Ipeirotis and Gravano 2004). For each by the relevance score calculated by the database selection algorithm. The measures the percentage of relevant documents contained in the k top-ranked database, which is defined as  X 
R where E k is the estimated rank of the k top-ranked database, and B all the databases that are useful for the query. The primary objective of database selection 4.3. Experimental setup
In this paper, our experiments, in practice, are made up of two phases: (1) Training phase : tuning parameters in the language models
In this phase, we consider a number of variations on language models and evaluate the impact that these variations had on the database selection results. These variations are:  X  the effects of a wide range of parameter values, the JM coefficient parameter  X  ;  X  the effects of query expansion size of the DP + QE model on database selection. (2) Test phase : the comparison of different language models on selection performance
In this phase, we conduct a series of experiments to compare the selection performance for different language models. These comparisons are:  X  the comparison of The JM model and the non-interpolated model on the selection of relevant topics at the class-specific search stage;  X  the comparison of 3 different language models: the MEL model, the DP model, the DP + QE model on database selection at the term-specific search stage;  X  the comparison of the two-stage selection approach and the general one-stage selection approach X  X he state-of-the-art CORI model on database selection.

In order to carry out the above experiments, the datasets are divided into two parts: the is decomposed into 130 smaller databases for each collection. Among these databases, 30 databases are used for the tuning of the model parameters in the language models at the training phase, and the rest of the 100 databases are for performance evaluation at the test the training and test databases for the Reuters-21578 and LookSmart datasets. Note that for than 96 topics. The reason is that we found there are three topics that only have fewer than 5 documents. The number of documents is not enough for the training and test phase. So we have to discard these three topics in the construction of the topic hierarchy.
As we known, the performance of a selection system may vary significantly according to the test datasets used. To fairly compare the behavior of our language modeling method with that of other methods, we perform our experiments on databases with different database LookSmart documents, and the small-scale databases with a few topics that consist of the
Reuters-21578 documents. Each test dataset was divided into 130 smaller databases that were of different sizes (between 0.01 X 1.2 megabytes per Reuters-21578 database, and between 7.2 X 36.4 megabytes per LookSmart database), and varied in the number of documents they contain. The documents inside a database are from the same source or Web site (for the
LookSmart databases) or the same time-frame (for the Reuters-21578 databases). Table 2 depicts the summarized statistics for the test databases in these two datasets. especially for the small Reuters-21578 dataset. To lessen the problem of document overlap, the document number of most of the databases in the Reuters-21578 dataset, in practice, is only within the range of 50 X 200. This is the main reason why the smallest size of Reuters databases is only 0.01 M (see Table 2). According to the statistical information obtained from the Reuters-21578 and LookSmart database collections, the percentage of document construction.

For our experiments, 50 trials (25 for short queries and 25 for long queries) were conducted from the 50 trials.

To avoid the sensitivity of selection performance to query length, in this paper, we exper-imented with both short and long queries. Unlike the TREC data where these two types of queries can be created based on the title , description and narrative fields from the TREC-query, and the title concatenated with a group of frequent terms occurring in the dateline section of relevant documents in a particular topic domain as the long query. The reason for the use of frequent terms as query terms is that these frequent terms would return the most random sample of documents from the test datasets. In order to obtain long queries with the frequent terms for a specific topic, we use the query selection strategy reported in (Callan and Connell 2001). For each topic, a collection of relevant documents is chosen from the dataset (note that these documents are excluded from the training and test datasets used for the experiments). The selection of frequent terms is measured by average term frequency ( avg tf = ctf / df ), where ctf is collection term frequency and df is document frequency.
Note that all the documents and the queries are preprocessed by removing stop words and employing the Porter stemming algorithm (Porter 1980). We hired undergraduate students to produce long queries. We provide them a list of frequent words for reference when they generated long queries. Table 3 below lists query set statistics for these two datasets.
As described earlier in Section 4.1, the documents in the two datasets had been labeled label associated with each document in the database. The documents with no category label are treated as irrelevant. For example, each Reuters-21578 document contains the field that denotes the class label of this document. With the information provided by the &lt; topic &gt; field of the document, it is easy to know the number of relevant documents in a database with respect to a given query of a particular topic. 5. Experimental results two-stage language model for database selection. 5.1. The effects of various values of smoothing parameters in individual language models
To examine the effect of the smoothing parameter in each specific smoothing method on selection effectiveness, we vary the value of the smoothing parameters in a wide range in order to observe all possible difference in smoothing. In each run, we set of the smoothing parameters the same value across different data sets to report the average selection perfor-mance at each parameter value. Then, we compare selection performance by plotting the
Jelinek-Mercer (JM) Smoothing : We compare the precision of the JM model at different search on the parameter space of  X  1 , we firstly tried 10 values 0 . 7 , 0 . 8 , 0 . 9 , 0 . 95 } . We note that, when  X  1 varies in the range between 0.5
JM model performs reasonably well, we had better set  X  1 at some desired range with the value of 0.5 X 0.8. However, it is hard to determine the optimal To further obtain the optimal  X  1 on separate datasets, we used the EM algorithm (Recall
Section 3.3.1) to learn the optimal value over the desired parameter space with the value of 0.5 X 0.8.

The plots in Figure 2 show that the average precision for different values of datasets. It is easily noted that the choice of the appropriate smoothing parameter have a large impact on selection performance. We can see that the optimal value of little high (0.67 for the LookSmart and 0.75 for the Reuters), which suggests that although the parent-class model and the class-corpus model can help smooth the word sparseness, the basic class model P ( q | c k ), in practice, plays a major role in class selection.
Dirichlet Prior (DP) Smoothing : To see the detail change, we experimented with a wide range of value of the DP smoothing parameter  X  Recall Eq. (17). The plots in Figure 3 show the average  X  R k ( E , B ) value for different settings of the prior sample size chance for  X  to be at a small value. This is probably because parameter, which is affected by the length of the data collection. When the data collection occurrence frequency of query term C ( t i | S ) on selection process. The effect of to the smoothing parameter  X  1 in the JM model. The optimal value of range between 1000 X 3000. 5.2. The JM model vs the non-interpolated model on relevant topic selection
The class-based language model using the JM smoothing method is expected to perform better chart of the JM model with that of the non-interpolated model over two datasets. The 11 point precision-recall curves are shown in Figures 4 and 5. Note that the figure for the JM model uses the best choice of the smoothing parameter  X  1 guage modeling approach achieved better precision at all levels of recall. The precision is improved significantly by 7.54% and 9.45% on average respectively. This means that the linear interpolated approach is an attractive way in which the parent-class model and the class-corpus model help effectively smooth the word sparseness and therefore improve the selection effectiveness.

It is interesting to notice that the behavior of the JM model and the non-interpolated model varies in different data sets. For small-scale Reuters databases, the improvement of the JM model over the non-interpolated model is not as significant as that for the LookSmart data set. The performance gain in the Reuters data set achieves 7.54% improvement in contrast to 9.45% in the LookSmart data set. One possible reason for this difference might be because of selection performance. However, this is only a conjecture, the verification of which we leave for future work.
 5.3. The effects of different term-based language models on selection performance discussed in Section 3.3.2. These language models, the MLE model, the DP model and the
DP + QE model are evaluated in our experiments. The selection performances of these three models are shown in Figures 6 and 7.

Compared with the behavior of the MLE model, the DP model and the DP improve selection performance significantly and consistently across both datasets. This is understandable because some query terms missed at the database lead to a bad probability estimate for the query in the MLE model.

As would be expected, the DP + QE model (Expansion terms are 50) performs the best and 41.23% on average against the MLE model in Figures 6 and 7. It suggests that query expansion does help improve the selection effectiveness if the number of expansion terms is chosen properly.

Note that the DP model has not performed as well as the DP experiments. However, the results of the DP model are still very competitive when compared with the performances of the MLE model. As we see, in all results, the performance of the
DP model smoothed with the optimal parameter value  X  (  X  = than the best performance of the MLE model, even though it is sometimes very close to the MLE model in specific cases.
 5.4. The effects of query expansion size on selection performance
Obviously, the quality of query expansion might well be influenced by the quality of the features associated with topic classes. When expansion terms are selected, how many of them are actually relevant to the original query? To investigate this, we conducted experiments number of expansion terms used affects selection performance (e.g., QE 10 refers to the query expansion with 10 terms). To observed this more clearly, we plot the curves in Figures 8 and 9.
 the translation model with query expansion are comparable to that of the base-query model.
This is a clear indication that query expansion can be helpful in improving the selection performance using the translation modeling approach. Second, the greatest improvement can be seen when 50 terms are used for selection in both datasets. When more expansion terms take part in selection, the performance begins to deteriorate. The possible explanation for this phenomenon is that the most distinguished features for the class could be contained in does not bring any larger benefit. 5.5. The comparison of the two-stage database selection approach with the one-stage two-stage database selection approach on selection performance. As mentioned earlier, one most current one-stage database selection approaches is that the search focuses on a small different search stages, and compare their effectiveness of selection. Here, we deliberately conduct a set of experiments to compare the overall selection performance of the estimated two-stage language models with the best results achievable using the one-stage model X  X he CORI model in both test datasets.

Three types of language models are measured in the experiments: one two-stage model X  the JM + QE + DP model, and two one-stage models X  X he QE +
CORI model. To compare the selection performance of these three models, we select a group of best runs for each model on each testing dataset and compare the average  X 
R ( E , B ) value for different database selection models.

In all the results, it seems to be a clear ordering among the three selection approaches the JM+QE + DP model and the QE + DP model compared to the baseline model X  X he CORI model. Figure 10 shows that, the overall  X  R k ( E , crease greatly with 51.67% and 31.54%, respectively, on the Reuters-21578 dataset. The same effect is also observed in the LookSmart dataset, as illustrated in Figure 11. It ap-selection.

We also notice that, the performance of the JM + QE + DP approach is consistently better than the best performance of the QE + DP approach. Indeed, some results in Figures 10 and effectiveness on relevant databases. 6. Conclusions and future works
We have presented a novel way of looking at the problem of database selection based on two-processing. For databases categorized into a topic hierarchy, instead of searching them in an ad hoc manner, we intentionally keep the database language models separated according to individual natures of different search stages. It makes our models easy to be understood and extended. We also conducted a number of experiments to prove the effectiveness and robustness of our language models. The experimental results have demonstrated that this approach holds great promise of improving search performance. Due to the simplicity of our models, we believe that our model can be easily extended to incorporate with any new language-based techniques under a general, well-ground framework.

We plan to investigate the following matters in the future work. First, due to time con-additional knowledge added to the models will further improve our selection system. For example, using relevant feedback techniques, a user can provide more meaningful words which facilitate the formation of better queries.

Second, as mentioned previously, we simply take all the words occurring in the databases independently. Such unigram models completely ignore word phrases that are likely to be bility is to combine bigram or trigram models into our current language models. However, parameter estimation of these complex language models remains a major difficulty. Thus, we
For example, in the kernel-based SVM, Cancedda et al. proposed the use of word-sequence kernels, a novel way of computing document similarity based on matching sequences of words (Cancedda et al. 2003). The idea of sequence kernels to process document as a se-we could combine this technique into our current database selection model.
Third, we only employ simple smoothing methods in our current work. We are planning to explore other more elaborate smoothing methods to extend our models. It is also possible that this will improve selection results.

Fourth, in our experiments, we used the frequently occurring words as long queries. This in database selection. To avoid this problem, one of the possible appropriate methods is the model used in the real-world applications. We will leave this for further research.
Finally, in our current datasets used for the experiments, we use the Reuters 21578 dataset which contains a very small set of homogeneous documents. This dataset is not enough for simulating a huge, complex, and heterogeneous real environment. Recently, a new Reuters dataset -Reuters Corpus Volume 1 (RCV1) is introduced by Lewis et al. (2004), which is a collection of over 800,000 documents manually categorized newswire stories made available this collection is constructed with the hierarchical category taxonomies. This feature makes 21578 dataset with the RCV1 dataset to further investigate the performance of our proposed two-stage approach in a more complex environment.
 Acknowledgment
This research was supported by a university grant from Wollongong University under con-tract SDRG03-227381010. We thank Peter Eklund for his help and advices in the written expression of this paper. Finally, we would like to thank the anonymous reviewers for many valuable comments and suggestions that are useful for improving the quality of this paper. References
