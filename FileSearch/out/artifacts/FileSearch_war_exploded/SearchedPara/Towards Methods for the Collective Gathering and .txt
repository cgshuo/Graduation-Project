 Growing interest in online collections of digital books and video content motivates the developmen t and optimization of adequate retrieval systems. However, tr aditional methods for collecting relevance assessments to tune system performance are challenged by the nature of digital items in such collections, where assessors are faced with a considerable effo rt to review and assess content by extensive reading, browsing, and within-document searching. The extra strain is caused by the length and cohesion of the digital item and the dispersion of topics within it. We propose a method for the collective gathering of relevance assessments using a social game model to instigate participants X  engagement. The game provides incentives for assessors to follow a predefined review procedure and makes provisions for the quality control of the collected relevance judgments . We discuss the approach in detail, and present the results of a pilot study conducted on a book corpus to validate the approach. Our analysis reveals intricate relationships between the affordances of the system, the incentives of the social game, and the behavior of the assessors. We show that the proposed game design achieves two designated goals: the incentive structure mo tivates endurance in assessors and the review process encourages truthful assessment. H.3.4 [ Systems and Software ]: Performance evaluation (efficiency and effectiveness): Test collection construction Performance, Design, Experime ntation, Human Factors. Test collection construction, rele vance assessments, social game. Most approaches for evaluating in formation retrieval (IR) systems are based on test collections that can be used in repeated experiments to assess and compar e retrieval results and optimize system performance. A typical test collection consists of a corpus of documents, a set of search t opics, and relevance assessments collected from human judges. The construction of such a test set requires a great deal of effort in which gathering relevance judgments is especia lly time consuming and expensive. This is further amplified in focused retrieval tasks, where the aim is to point users directly to releva nt parts of documents. There, relevance assessments need to be rendered at the passage level [4][6]. Current practice to reduce assessment load is to apply a pooling method to select a subset of documents to be judged [15]. In our research we focus on challenges that are presented by the growing use of on-line collections of digital items, such as digitized text books, audio books , and video and mixed media content 1 , which require adequate browsing and search support. Digital items of this type repres ent cohesive semantic units that may be substantial in size, requiri ng extensive effort to assess for relevance. Each book, for example, may take a considerable time to review, particularly when collecting passage level relevance assessments. While pooling may also be applied at the passage level, the scale of the task is still beyond what is possible for a single judge. In addition, in order to build reusable test collections, one needs to encourage assessors to look for sections of content that might be relevant, but were not retrieved by any of the systems contributing to the pool [6]. In this paper we introduce a new method for gathering relevance assessments through a collective effort of human assessors who, as a group, are engaged in a social game . We implemented a system that supports the review of digitized books and piloted the method by running the Book Explorers X  Competition, but the same approach can be applied to other media content. From the data collected, we can then extract relevance assessments by a specific user profile and use them accordingly to support the tuning of search systems. The uniqueness of our approach is in the relevance assessment method that: -Actively promotes communicati on between assessors during -Introduces self re-assessment of the relevance judgments by -Uses the distribution of relevance opinions as the form of In the following sections we di scuss related work (Section 2), describe our approach (Section 3), detail the pilot study we conducted to validate our approach (Section 4), and present an analysis of our findings (Section 5). We conclude with a discussion of the research achie vements and open questions for future work. The most common approach to test collection construction follows the Cranfield methodology, which has been widely adopted by evaluation initiatives, like TREC [15]. To tackle the issue of increasing sized document corpora, which renders exhaustive judgment unfeasible, a range of pooling methods have been developed for selecting a pr omising subset of the documents for judgment [3][17][7]. Alterna tive approaches for optimizing retrieval systems aim to reduce or to avoid altogether the need for relevance assessments [8]. They focus on observing properties of the result sets and defining perfo rmance prediction measures that are highly correlated with traditiona l IR measures [16]. However, these methods are typically used as complementary rather than stand-alone approaches to evaluation. Established methods for collecti ng relevance judgments, e.g., at TREC or INEX [4], are based on the model of employing a single judge, usually the topic author, to assess the relevance of documents to the given topic. Assessments from multiple judges on a topic may also be collected with the aim to measure the reliability of the evaluations with respect to changes in the relevance set. When assessments from multiple judges are available, these are typically converted to a single score per document, e.g., based on minimum or majority rules [14]. Recent practices diversified the ways in which relevance judgments are collected and used. In Web search the tendency is to use judgments from a repres entative sample of the user population, where assessments of search results for a single topic may be collected from multiple judges. Furthermore, user logs are often mined for indicators of user satisfaction with the retrieved documents. The idea of using multiple judges for a single topic has also been experimented with in the context of INEX, where the assessment process involved hi ghlighting relevant passages in Wikipedia documents. Trotman et al. [11] conducted experiments with synthesized multi-assessor re levance sets, constructed from assessments for 15 topics by 3 to 5 assessors. The main objective was to distribute the assessment cost over a larger number of participants and thus reduce th e individual user X  X  cost, which originally amounted to 7 hours per topic in INEX 2006. Their findings showed that assessment variations made no significant difference in the ranking of systems. Contrary to this, Bailey et al. [1] reported consistent but small differences in system orderings based on relevance assessments by three distinct groups of assesso rs: topic authors, topic experts, and non-experts. Using the test collection from the TREC 2007 Enterprise track, they conclude d that non-experts X  assessments may not be a reliable substitute for judgments from topic authors or topic experts. At the same time, assessments from the topic experts X  yielded results close to those from the topic authors. Assessors X  background knowledge is, however, just one of many aspects that affect the assessors  X  behavior and criteria for judging the relevance of documents (e.g., [9]). In our approach, we extend the use of multiple assessors per topic with a process that facilitates the review and re-assessment of relevance judgments and enable s communication between judges. Furthermore, we preserve and pr omote the diversity of opinions, offering an enriched collection of relevance labels that incorporates different user profiles and user needs. From such data we can then extract relevance assessment by a specific user profile or a need and use them accordingly to support the tuning of the system. Since its launch in 2002, INEX demonstrated a compelling case of community effort in collec ting relevance judgments from its participants. For example, in 2003, assessors had to spend 21 hours on average to provide passage level judgments for a single topic, on a corpus of computer science articles [6]. The incentive for contribution was to gain access to the collected data. In general, the literature distinguishes between material and immaterial incentives, i.e., tangible versus intangible rewards. The latter is further broken dow n to  X  X elf X  focused incentives, such as raising one X  X  social status, and community wide incentives such as achieving the collective goals of an organization [2]. In terms of engagement and quality control, the intangible incentives are the most likely to l ead to quality data when the data is generated by the same group who will then benefit from it. Naturally, individuals or members of the community have a stake in the process and thus are likely to be motivated to engage as well as to control quality. Entertainment as a form of self incentive may lead to varying quality output, dependent on the design of the game and its entert ainment value [12]. Engagement is a function of this entertainm ent value to the user. The most popular game of this type is the ESP game used in the Image Labeler project by Google [13]. Games which include leader boards also promote a self interest through increased visibility and social status of the contributors. Games based on monetary incentives are the most susceptible to fraud since the participants do not have a stake in the quality of the output, but only care about obtaining their rewards. This necessitates the introduction of quality assessment measures, upon which payment can be conditioned. An example of such a service is Amazon X  X  Mechanical Turk, wher e it is the responsibility of the  X  X mployer X  to incorporate checks for quality assurance. Hybrid structures can incorporate elements from a number of these strategies, and may lead to increased quality output. For example, a game may be designed that builds on aspects of social status, but also offers monetary rewards to winners. In addition, the game may be deployed to a specific community who will own the collected data. The Book Explorers X  Competition that we designed to collect relevance judgments for digitized books, adopts such a hybrid approach. In this section we describe the collective relevance assessment (CRA) method that we devised to gather relevance assessments for collections of digital books, vide os, and alike. The approach is to involve a group of assessors in a social game that is designed to promote a desired assessor behavior . Thus, our main concern is to create the rules and the incentives that can lead participants to produce the expected outcomes, e.g., desired amount of relevance assessments, as a by-product of th e game. This approach is most likely to succeed if the players X  winning strategies, or their most likely strategies, are aligned with the goals we wish to achieve [12]. While the design of an adequate game is far from trivial, the very fact that the game can be tailored to pre-defined goals makes our CRA model extendible and app licable to different contexts and objectives. The CRA method involves the following three phases: -Preparation of data and setting CRA objectives -Design of the game and the re levance assessment system -Conducting the game. We describe these phases using a specific assessment task for a collection of digitized books. Given a set of topics and a collection of documents, the key issue is to identify a promising samp le of candidate documents or document segments for assesso rs to inspect and judge. Typically, candidate documents are pooled from the results of search engines. To apply CRA on books requires search results comprising the book titles as well as relevant passages within the books. Adopting the pooling method used at INEX [4], for each topic we consider retrieval results from r runs and build the corresponding topic assessment pool by applying book level top k round robin method, adding books from the ranked lists until we arrived at k unique books in the pool. Then, for each selected book, we create a passage assessment pool by including all the passages retrieved in the r runs. review all candidate books. Thus, to increase the likelihood that they will review books that are relevant, we order the topic pool by a score that reflects the documents X  median rank across r runs. We apply the same ranking for the candidate passages in books. This approach is supported by th e findings of Soboroff et al. [8] who showed that using authority of participating systems has high reliability for predicting document relevance. As an illustration of assessment objectives we use two that are typically expected when reviewing digital books: (a) Identification of coherent rele vant passages that may span (b) Consistent assessment of possibl y dispersed relevant content. To achieve these, it is of param ount importance that assessors are motivated to review long stretche s of texts. This has immediate implications on the game design. The game incentives need to promote participants X  endurance during review and assessment. Furthermore, the expectations for coherence and consistency require that we design facilities for browsing, searching, and annotating relevant segments effectively and reliably. In addition to these objectives we have to ensure that the resulting relevance assessments are not tainted by the assessors X  objective to win the game. We want to encourage truthful assessment that reflects assessors X  true beliefs in the document X  X  relevance based on their level of expertise, info rmation acquired by reviewing the content, by interacting with othe r game participants, or by using external resources. This section presents details of the Book Explorers X  game that we designed to reward endurance a nd truthfulness of digital book assessments. In order to attract participants, we offered hardware or software rewards to the winners of the game. For playing according to the rules, participants collected points and the player with the highest number of points by the close of the competition was pronounced the winner. Roles . The game is modeled as a tw o-player game with competing roles: explorer and reviewer . An explorer X  X  task is to locate and mark relevant content inside books by highlighting relevant passages on a page, or marking an entire page as relevant. Reviewers have the task of checking the quality of the explorers X  work by providing their own relevance assessment for each page that has been judged by at least one explorer. During this process, the reviewers can see the relevance assessments of all the explorers who assessed a give n page, but cannot see other reviewers X  judgments. In addition to the passage level exploration, both explorers and reviewers are required, independe ntly (i.e., information is not shared), to assign the degree of relevance to the book as whole. Note that a book is rendered releva nt if it contains at least one relevant passage, but the degree of relevance is decided by the assessors (on a scale from 0 to 5, with 5 designating the highest degree of relevance). In addition, judges are asked to indicate how familiar they were with the book before and after exploration.
 Rules. For each topic, participants can choose either to explore or review, but they cannot assume both roles on the same topic. Participant can compete as individuals or form teams to help one another. In teams, participants can benefit from seeing each others X  annotations, i.e., relevan ce labels and notes, and dividing the work on a topic or a book. However, they can also affect each other X  X  opportunity to earn points. For example, if one team member selects to explore a gi ven topic then other members cannot review the same topic. A t eam X  X  score is derived simply by summing up all the individual cont ributions from the members. Scoring and Incentives. Players gain points for both roles. Each judgment by an explorer is wort h 10 points, regardless of whether the page is judged relevant or irrelevant. Each judgment by a reviewer is worth 5 points. The di fference in the scores is meant to reflect greater effort required for discovering relevant content. The reviewing task is considered lighter and faster. While the scoring scheme does not differentiate between relevant and non-relevant judgments, there ar e several incentives that skew the explorer X  X  attention towards relevant documents. First, the maximum number of non-re levant pages that an explorer can gain points for in a book cannot exceed the total number of recommended candidate pages (i.e., re trieved by one of the search systems contributing to the pool). Furthermore, explorers are awarded 100 bonus points for every relevant book, 1500 bonus points for every 100 relevant pages, and another 2000 bonus points for every 100 undisputed re levant pages. Reviewers are also encouraged to review relevant rather than irrelevant content. Each relevant book brings them a bonus of 50 points. They also gain 500 points for every 50 new relevant pages that went undiscovered by explorers, and a bonus of 1000 points for every 50 newly discovered relevant page which is then agreed upon by explorers. A further bias towards relevant content is added through the user interface: we use a toggle button to judge a page, toggling first to  X  X elevant X , then  X  X rrelevant X , then  X  X njudged X . Thus the user effort for irrelevant judgment is double that of a relevant judgment. Game Phases . The Book Explorers X  game consists of three main phases: (1) exploration, (2) review , and (3) dispute resolution. For each topic, there is a fixed set of K books, from which explorers can choose from. Once a book has been explored for a given topic, it is removed from the explorer X  X  list (other explorers can who selected to review that topic. If the reviews result in a disagreement between explorers and reviewers (based on majority opinion, which can flip with new reviews), then the book is moved to the disputed books list and is subject to the dispute process. First, the explorer is given a chance to respond to the challenge from reviewers. After that, the reviewers can respond to the explorer X  X  rebuttal. Game Dynamics . As mentioned above, the Book Explorers X  game is based around the two roles with competing but at the same time shared interests. For example, the reviewer role depends on the explorer X  X , since a book for a given topic cannot be reviewed until at least one person explores it. However, in contrast to many simple two-role games, the Book Explorers X  game incorporates dependencies that stem from the collective behavior of participants. For example, if the majority of reviewers disagree with an explorer X  X  assessment, the explorer is penalized through a deduction of 15 points from his or her score. Similarly, reviewers can lose 5 points per page level assessment when the majority of explorers disagree with them. However, the penalty is lifted once the disputed page is re-assessed by the explorer or the reviewer, regardless of whether they stood by their original judgment or changed their minds. This design decision is in accordance with our objective to support the assessor X  X  true belief and opinion. We treat relevance as subjective by nature and do not force individuals to agree in order to conform to the group judgment. However, we motivate them to re-assess their judgments by re-visiting pages that are disputed and ensure that disagr eement is not due to an error. Thus, the incentive structure of the game encourages engagement through exploration and reviewi ng, rewarding persistence in searching and assessment. This forms a good basis for achieving exhaustive relevance judgments over time. Furthermore, the incentives for re-assessing one X  X  own judgments are a good foundation for eliminating errors and adjusting the assessment based on evidence brought forward by reviewers. With no inhibitions for changing one X  X  mind, this game presents a solid basis for arriving at truthful opini ons and eliminates possible bias from the gaming strategy. Accumu lation of points and winning is only related to the endurance and not to the nature of the relevance assessment. We implemented the CRA social game in an assessment system based on a client-server architecture that comprises: (1) an indexed and searchable collection of OCR text from digitized books, images of the digitized books, and search results from r retrieval runs compiled from k retrieval systems, (2) a user interface for presenting content, and game play. The latter involves four main modules: game administration, exploration, review, and disputes. The admini stration includes registration, login, team selection, and topic selection. Searchable Collection of Content. We indexed the full texts of the digitized books (at book and page level) using Indri and built two types of search capabilities on top of the index, enabling search over the set of books as well as search inside a given book. Using the former, we added an additional run to the INEX run set, containing book-level search results. Searching inside a book was incorporated into the online system to aid assessors in exploring the contents of a book. To display pages of the books, we used the PDF versions that we crawled from the Internet Archive. User Interface . The assessment system was developed for Internet Explorer 7 or higher with a SQL Server and the search engine backend built on top of Indri. While the system could also be used with Mozilla and Opera, some functionality, e.g., passage highlighting, was not supported in these browsers. Judges could explore or review any of the books listed in the pools of their chosen topics. Th e list of books showed metadata information on each book, a segment of the table of contents (if available), and a snippet from the top ranked page inside the book. Selecting a book from the list opened the Book Viewer window (see Figure 1), which supported various forms of browsing and searching inside the book. Judges could browse a book sequentially or jump to a page, browse using the hyperlinked table of contents, search inside the book, and visit the recommended candidate pages liste d on the Assessment tab. By in the Search tab. Judges could judge the displayed page of a book using the toggle button below the pa ge image. Alternatively, they could judge pages directly in the Assessment or Search list without actually viewing the pages. Members of a team could see each others X  relevance judgments and comments. Similarly, review ers were displayed aggregate relevance information across all explorers as well as all comments. Once judges finished e xploring/reviewing a book, they were asked to rate the book on the Book tab before marking it as complete. A modified Book Viewer, with reduced functionality, was used for the dispute resolution phase. It did not have search and only showed a listing of the pages that had conflicting relevance labels and any comments that related to these pages. We conducted a pilot study by deploying the Book Explorers X  game in the context of the INEX 2008 Book Track 2 , with the aim to collect book relevance assessments. The study included two rounds. The first round lasted for 2 weeks and resulted in 3 winners. One of them participated as an individual assessor while the other two formed a team. The second round spanned for 4 weeks and yielded 4 winners. All f our assessors were on the same team; one among them also achieving the highest individual score. For the study we used data that consists of the book corpus, the set of topics, and the search results from the systems participating in the INEX 2008 Book Track [5]. The INEX book corpus comprises over 50,000 digitized out-of-copyright books, with 17 million s canned pages in total, and covers a broad range of genre, from history books, biographies, literary studies, religious texts and teachings to encyclopedias, proceedings, poetry, essays, and novels. The OCR content of the books is marked up in BookML, an XML based format, while book metadata is provided by the standard MARC records used by libraries. The topic set contains 70 TREC style topics that include a title, description, and narrative. These topics were created collectively by the INEX participants. For each topic, we pooled the search results from 18 runs by 4 groups that participated in the Book Retrieval (BR) task and provided book level results only. We also included results from 13 runs from 2 groups that participated in the Page in Context (PiC) task and returned lists of XML elements, e.g., pages or small book parts, clustered by books. On average, a BR run contained 946 books per topic 3 (  X  =174), while a PiC run contained 735 books (  X  =281) and 2,423 (  X  =2707) within-book results, i.e., XML elements, per topic 4 . Participation in the study was open to anyone over the age of 16. We sent a call for participation to a digital library mailing list and attracted 48 participants. In addition, we pre-registered 81 INEX 2008 Book Track participants. From the total 129 registered participants, 17 took active part in the competition and provided relevance assessments. With the exception of one participant, who incidentally assessed only one book, all the assessors were either INEX participants or in some way affiliated with INEX. 14 were participants of the Book Track, and 2 were from other INEX tracks. Participation was, thus, primarily driven by the interest of the specific community of practitioners who would directly benefit from the outcome, i.e., the collected data. Participation by others, who were motivated by the competition or general interest in books, has proven to be short lived. This illustrates the importance of the community incentive for the type of task we are considering. Through the relevance assessment system we collected several types of data 5 : document regions that the assessors highlighted on a page, the binary relevance labels they associated with a page, notes and comments they recorded for a page, and the relevance degree they assigned to books. In total, assessors judged 3,478 books and 23,098 pages across 29 topics, and marked a total of 877 highlighted regions. The system recorded 32,112 navigational events, excluding direct page jumps, 45,126 relevance judgment events, and 2,970  X  X earch inside a book X  events. The pilot study enabled us to assess properties of the CRA method. In this section, we pres ent an analysis of the collected relevance assessments and the behavioral patterns of the assessors. We discuss the feasibility of the CRA approach by comparing the scale of the assessment task with the levels of engagement that assessors exhibited during the pilot study. From the system logs we see that 17 assessors actively engaged with the system on 7.2 distinct days (  X  =7.8, median=5, min=1, max=35 days). Over the course of the study assessors spent, on average, 11.4 hours (median=6) judging a total of 3,741 books across 29 topics, where 239 books were assessed by multiple judges. On average, assessors created 4,718 log events (  X  =6,147, median=2,615) and judged 220 books (  X  =383, median=90, min=1, max=1,385). Judging rele vant books took approximately 7.3 minutes per book while for non -relevant ones it took about 2.7 minutes per book. Similarly, it took 37 seconds to judge a relevant page and 22 seconds to judge a non-relevant page. The total relevance assessment effort that participants accumulated amounts to 8.1 days of work or equivalent of 195 hours. Extrapolating these statistics, th e average time to judge a topic with 1000 books is 52.7 hours, with the ratio of 1:9 relevant to non-relevant books. With the observed rate of spending on average 95 minutes a day assessing, it would take 33.3 days for a single judge to assess one topic. We could reduce this time by decreasing the pool size and speeding up the interaction with the system or by focusing on relevant content only. With a pool of 200 books per topic and 20 judges, each spending an hour a day, a test collection with 70 topics could be built in 36.9 days. 737 judges would complete the whole task in one hour. The above observations are comp arable with the level of assessors X  engagements in the ad hoc track of INEX 2003 [6]. There it took assessors 8 minutes to judge a relevant document and 1 minute to judge a non-relevant one. This level of engagement is, however, not ch aracteristic of typical human computation tasks or games, which tend to be more fast-paced and require less cognitive effort [12][13]. In relevance assessment efforts that rely on pooling, it is common to consider the completeness of relevance assessments as the percentage of search results that are covered by the judged documents. Our assessment pool comprises 31 runs for 29 judged topics, each with 1000 candidate books per topic, with 79.4% overlap. That resulted in 158,442 unique book results, out of a total of 768,488 books, and the maximum completeness level of 18.3% for unique book results and 29.4% for all book results. The 3,478 judged books represent 2.2% of the unique book results (12% of the achievable completeness level), and 5.18% of all book results (17.6% of the maximum co mpleteness level). Completeness, however, varies greatly across topics, with 120 out 1000 books judged per topic, on average, (  X  =258, median=18, min=1, max=1000). By removing 5 topics with just 1 judged book each, the relative completeness increases fro m 17.6% to 20.9%. This can be improved with additional time committed to the assessment effort and by encouraging the review of distinct books among assessors. It is also useful to consider how exhaustive assessors X  efforts were in considering evidence for book relevance, either from the recommended list of pages or from browsing and searching books. From the logs we observe that judges viewed a total of 3,541 books and judged 3,478 across the 29 topics . Only 306, i.e., 10.6 books per topic, were labeled as relevant by at least one assessor. Considering the relevance assessment at the passage level, we note that assessors viewed 32,298 pages, i.e., only 2.33% out of the total of 1.38 million associated with the viewed 3,541 books. Number of Books 306 3 , 162 Total Pa g es 146 , 200 1 , 240 , 395 Viewe d 8.91 % 1.84% Total Recomm. p a g es 13 , 497 39 , 383 Viewe d 67.80% 26.76% If, however, we look at the review s of relevant and non-relevant books separately we note that th e relevant books were explored more thoroughly. As shown in Tabl e 1, assessors looked at almost 9% of the book content if they found the book relevant. This is in contrast to reviewing non-releva nt books where they looked through less than 2% of the book content. With regards to the recommended pages, in case of relevant books a ssessors reviewed almost 68% of the suggested content. For non-relevant books the percentage is down to 27%. We note that none of these statistics take into account implicit assessments that are reflected in the absence of user actions, e.g., viewing but not judging a page. Indeed, rapid content viewing and skipping could be taken as non -relevant judgments and count towards the level of review and exhaustiveness of content assessments. In our sample, judge s viewed but did not judge 9,091 pages. An important aspect of book collections is the perception of each book as a cohesive semantic unit. This suggests that even passage level judgments should be supported by offering easy access to the whole context of the book. Similar c onclusions were drawn at the ad hoc track of INEX in [10]. We i nvestigate this issue by observing assessors X  browsing during relevan ce assessment and examining the distribution of relevant pages w ithin their browsing patterns. From the logs, we found that, on average, 13 pages (  X  =38.5, median=4) per relevant book (avera ge 495 pages in length) were marked relevant. The frequency dist ribution is shown in Figure 2. We see that most of the time, relevant information forms only a minor theme within a book, consis ting between 1-4 relevant pages per book. This suggests that the focused retrieval approach is essential for supporting assessors in discovering relevant content. Looking at the sequences of relevant pages, we find that 27.45% of all relevant pages appear as singletons, while the rest form sequences of varying length. Fi gure 3 shows the distribution of relevant page sequences. We found 192 occurrences of sequences of 2 consecutive relevant pages (11.36% of all relevant pages), and 64 triples (5.68%). The average sequen ce length is 2.55 relevant pages. Most of the relevant pages are pa rt of a sequence between 2 and 9 pages in length (34.69%). However, the average distance in pages between clusters of relevant cont ent is 41 across all relevant books, highlighting that relevant content is not necessarily concentrated in one part of a book, but is dispersed. Browsing and Relevance Decision . The user logs contain a total of 32,112 navigational user actions, ex cluding direct page jumps, and 26,033 judgment actions (after rem oving duplicates due to use of the toggle relevance button). Figure 4 shows the frequency of observed browsing patterns of vari ous lengths before judgments were made. We see that most of the time (60%) assessors simply navigated to a page and assessed it, i.e., viewed only the one page they judged. However, we also found evidence for extensive browsing: judges viewing between 2 to 10 pages in 27% of the browsing patterns before making a judgment. Furthermore, 53.6% of time the page they viewed and judged next was either the next or the previous page in the book. The observed browsing behavior suggests that assessors often require contextual information in order to decide about the relevance of a passage in a book, supporting the need for proving easy access to the entire book during relevance assessment.
 Of the 17 participants, 12 only explored, 1 only reviewed, and 4 followed a mixed strategy of expl oring some topics and reviewing others. The average rate of indicated topic familiarity was 3.4 for explored topics, and 2.3 for review ed topics. With the exception of 2 topic authors, participants chose to explore the topics they authored. The 2 exceptions chose to explore topics they were most familiar with while review topics they knew less about. This reflects on participants X  expectations on the re lative difficulty of the two tasks, indicating expected higher cognitive effort for discovering relevant content by searching and browsing than for reviewing already judged content. In terms of the methodology, the fewer uptake of the reviewing role and the lower levels of familiarity could present a problem in reducing aspects of quality control. This may, however, be addressed by increasing the incentives for judges to become reviewers. Another issue to consid er, which also contributed to the low number of reviews, is the initial delay in the availability of explored content that can be reviewed. 98% of the reviews were completed only after 2/3 rd into the duration of the competition. We profiled judges X  assessment st rategies based on their user behavior and judgment patterns. We found two clear types of judges: those who focused on releva nt content ( X  X ocused X ) and those who judged most content presente d to them ( X  X xhaustive X ). Both types reflect mixed incentives. On the one hand, both the community interest and the bonus points system promote the marking of relevant content over irrelevant. On the other hand, points can easily be accumulated by judging high volumes of pages, most of which are irrelevant. A lthough the latter strategy does lead to exhaustive assessments, it does mean that an assessor X  X  time is used for creating potentially less useful labels. 7 of the 17 judges followed the exhaustive strategy, 5 the focused method and 3 showed mixed behavior (2 outliers who judged a single relevant and irrelevant book, respectively, were excluded). The ratio of relevant to total number of judged books were significantly different across these groups: 10%, 41%, and 30% for exhaustive, focused and mixed, respectively. This clearly reflects the different behaviors to judge a ll or only selected books. The ratio of relevant to total judged page s were, on the other hand, more comparable: 25%, 27% and 29%, respectively. The latter suggests that once committed to a book, judges would carry out an exhaustive assessment of it. In terms of winning strategy, alt hough the focused strategy provided the biggest rewards (on average, 195 points per judged book, and 18 points per judged page), judges who followed this strategy only achieved an average rank of 12.4. Since relevant books took longer to judge, focused assessors spent on average longer per book (5.2 minutes), which means that thei r reward per book and per minute was 37 points (and 38 points pe r page minute). Judges who employed the mixed strategy ranked 5 on average, and enjoyed the best rewards: gaining on averag e 188 points per books and 15 points per page but at the cost of less time (4 minutes per book), which translates to 46 points per minut e per book (and 40 points per page minute). The exhaustive strategy was the most taxing on judges X  efforts with an average reward of 63 points per book and 11 points per page (23 points per book per minute, and 38 points per page minute). The average rank position of exhaustive judges was 6.4, although the winners of both comp etitions were judges from this group. The above suggests that while the incentive structures in the scoring system have appropriately balanced rewards for a unit of effort, the level of engagement was the hi ghest for exhaustive (enduring) judges and least for focused assesso rs. A reason for this could be that the experience of a focused judge was significantly different from that of an exhaustive j udge: focused judges ended up only completing a small number of books, each of which required substantial effort. Their perception of the assessment task was thus likely to have been one that is rather laborious and cognitively demanding. Exhaustive judging on the other hand requires substantial stamina and enduran ce as judging endless number of irrelevant pages is a mundane task . The mixed strategy promises the best of both worlds, satisfaction on the completion of a larger number of books, but limiting frustration by placing more focus on relevant content. Despite this, from the study we found that only 3 participants followed this strategy. A total of 239 books were judged by multiple assessors (between 2-4) across 18 topics. The level of pairwise agreement between judges, based on binary releva nce, was relatively high, around 80.7%. Out of 239 books, judges disagreed on the relevance of 24 books. Their opinion differed on the degree of relevance for 34 books (71% by 1 degree, 20% by 2 degrees, 6% by 3 degrees and 3% by 4 degrees). At the page level, 4,622 pages were judged by multiple assessors with an agreement level of 57%. The observed levels of agreement are relatively high in comparison to those reported at TREC, i.e., in the range of 33-49% for document assessment, and at INEX , between 27-57% for documents and 16-24% for document elements [6 ]). This is a result of several factors. First, the majority of the multiple judgments were results of reviewers checking the explorers X  work (74%). This means that they were re-assessing a limited subset of documents in the pool rather than the entire set. Second, in the case of 15 books the conflicts were resolved through the dispute resolution dialogue between explorers and reviewers. During c onflict resolution, judges changed their assessments 12% of cases and maintained their original judgment in 88% of cases. This sugge sts that the review was useful for correcting potential errors in judgments but that judges did not feel forced to agree with one another. Explorers and reviewers were anonymous to each other which reduced the impact of possible other factors on their assessment. Annotations were used by 9 of the assessors, adding comments to 227 pages in 98 books. The distribu tion of annotations varied greatly with an average of 25 comments per judge (  X  =37, min=1, max=102). Two of them, in particul ar, made a frequent use of this feature adding 102 and 75 comments, respectively. We manually categorized all 227 comments into 5 groups: 76% as explanation of relevance decision or summary of content (e.g.,  X  X rong period. X ,  X  X ythagoreanism and buddhism X ), 15% as qualitative statement on the content (e.g.,  X  X reat! X ,  X  X arginally relevant X ), 6% as comment on technical issues (e.g.,  X  X annot r ead this page. X , or  X  X age is not showing. X ), 2% as direct addresse s between explorers and reviewers (e.g.,  X  X hat is relevant here to Titanic? Nothing in my opinion. X ), and 1% as other. We expect that both the explanatory notes and the quality statements may have acted as indirect messages, purposefully added by explorers to preempt possible challenges from reviewers. Furthermore, the a nnotations lead to a richer form of relevance assessment related to the content, the user background and the user task. The deployment of the CRA method in the pilot study enabled us to validate some of the aspects of the approach on a real relevance assessment task and observe the effectiveness of the Book Explorers X  game in engaging a ssessors and producing relevant outcomes. The study shows that it is f easible to achieve a productive environment in which individuals a nd teams are incited to engage in the exhaustive assessment of re levant documents. The study confirms that the assessment pro cess cannot be simply reduced to viewing individual pages in isola tion. Due to content cohesion and dispersion, assessment may requi re reviewing multiple pages throughout the book. Consequently, a system for collective relevance assessment requires s earch and browsing support. The self-regulating aspect of the B ook Explorers X  game, through re-assessment, provides a good founda tion for achieving high inter-assessor agreements, should that be desired. Finally, the CRA approach is amenable to diverse search scenarios with different objectives. The problem of extendib ility is reduced to implementing the rules and incentives of the so cial game that are in accordance with the objectives of the assessment process. The study findings reveal several areas for potential improvement. First, the analysis of the particip ants X  behavior shows that attracting judges who do not share in the community incentive bears little fruit, at least when the topics are predefined. Because of the frequent engagement of assessors with topics they created themselves, we believe that opening up the system to allow users to contribute their own topics would provide the right model for expanding the population of assessors and growing the test collection. Second, the highly skewed distribution of judge s X  roles suggests that we may need to adjust the rules. We can adjust the incentive levels in the scoring scheme to promote the pe rceived gain from the reviewing activity. Alternatively we can separate the exploration and reviewing phase and make them c onsecutive rather than parallel. We could also adopt a game design closer to ESP to involve a real-time dialogue between explorers a nd reviewers. This may also encourage increased interaction between the assessors, which can promote quality judgments. Completeness and exhaustiveness may be improved by introducing a separate game first to identify potentially relevant books for wh ich to gather passage-level judgments. We also note that the Book Explorers X  game design promoted assessor endurance, but failed to encourage the more desired, focused assessment of relevant content. Addressing this issue requires a careful review of the incentive structure and quality control of the assessment proce ss to suppress fraudulent behavior that may possibly result from new award schemes. In this paper we propose and valid ate various aspects of a method for the collective gathering of relevance assessments (CRA). The approach expands on the traditional methods by engaging a group of assessors in a social game that reinforces the specific behavior and produces the desired outcomes. The core of the approach is the design and modification of the social game to adapt the CRA approach to the particular rele vance assessment scenario. The CRA method introduces several new concepts into the assessment process. It involves a community of practitioners who can actively engage and collaborate. Furtherm ore, it encourages personalized and diverse perspectives on the t opics and promotes the collection of rich contextual data that can assist with interpreting the relevance assessments and their use for system optimization. In future work we will evaluate the quality of the collected relevance data by comparing to labels collected through other assessment modes. We also aim to evaluate the reliability of the constructed test collections using established methods for testing the effects of topic and judgment va riations on systems rankings and evaluation. [1] Bailey, P., Craswell, N., Soboroff, I., Thomas, P., de Vries, A. P., and [2] Clark, P. B. and J. Q. Wilson. 1961.  X  X ncentive Systems: A Theory of [3] Cormack, G. V. and Lynam, T. R. 2007. Power and bias of subset [4] Fuhr, N., Kamps, J., Lalmas, M., Malik, S., and Trotman, A. 2007. [5] Kazai, G., Doucet, A., Landoni, M. 2009. Overview of the INEX 2008 [6] Piwowarski, B., Trotman, A., and Lalmas, M. 2008. Sound and [7] Sanderson, M. and Joho, H. 2004. Fo rming test collections with no [8] Soboroff, I., Nicholas, C., and Cahan, P. 2001. Ranking retrieval [9] Spink, A. and Greisdorf, H. 2001. Regions and levels: measuring and [10] Trotman, A., Pharo, N. &amp; Lehtonen (2006). XML-IR users and use [11] Trotman, A. and Jenkinson, D. 2007. IR evaluation using multiple [12] von Ahn, L. and Dabbish, L. 2008. De signing games with a purpose. [13] von Ahn, L. and Dabbish, L. 2004. Labeling images with a computer [14] Voorhees, E. M. and Tice, D. M. 2000. Building a question answering [15] Voorhees, E. M. and Harman, D. K. 2005 TREC: Experiment and [16] Yilmaz, E., Kanoulas, E., and Aslam, J. A. 2008. A simple and efficient [17] Zobel, J. 1998. How reliable are the results of large-scale information 
