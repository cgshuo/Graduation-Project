 Google, Inc.
 Oregon Health &amp; Science University Oregon Health &amp; Science University Google, Inc.
 semirings, one involving a pair of tropical weights, and the other a tropical weight paired with model semiring allows for off-line optimization of exact models represented as large weighted present empirical results demonstrating that, even in simple intersection scenarios amenable to in terms of time of intersection. The second of these lexicographic semirings is applied to the best-scoring part of speech tagging for each word sequence. We do this by incorporating the tags determinizing the resulting word lattice acceptor in that semiring, and then mapping the tags back as output labels of the word lattice transducer. We compare our approach to a competing method due to Povey et al. (2012). 1. Introduction
Applications of finite-state methods to problems in speech and language processing have grown significantly over the last decade and a half. From their beginnings in the 1950s and 1960s to implement small hand-built grammars (e.g., Joshi and Hopely 1996) through their applications in computational morphology in the 1980s (Koskenniemi 1983), finite-state models are now routinely applied in areas ranging from parsing (Abney 1996), to machine translation (Bangalore and Riccardi 2001; de Gispert et al. 2010), text normalization (Sproat 1996), and various areas of speech recognition includ-ing pronunciation modeling and language modeling (Mohri, Pereira, and Riley 2002). 2002; Mohri 2009) has made it possible to implement models that can rank alternative analyses. A number of weight classes X  semirings  X  X an be defined (Kuich and Salomaa 1986; Golan 1999), though for all practical purposes nearly all actual applications use the tropical semiring , whose most obvious instantiation is as a way to combine negative log probabilities of words in a hypothesis in speech recognition systems. With few exceptions different semirings, in particular structured semirings consisting of tuples of weights. are tuples of weights where the comparison between a pair of tuples starts by comparing the first element of the tuple, then the second, and so forth until unequal values are found X  X ust as lexicographic order is determined between words. We investigate two such lexicographic semirings, one based on pairs of tropical weights, and the other that uses a tropical weight paired with a novel string weight that we call the categorial semiring . The latter is based loosely on the operations of categorial grammar. ond we apply to the problem of selecting only the single-best tagging for each word sequence in a tagged lattice. In each case we formally justify the application and demon-strate the correctness and efficiency on real domains. 1.1 Definitions
Adopting the notations often used in the speech and language literature (Mohri 2009), operations are defined, namely, the semiring plus  X  and semiring times  X  , such that: of a well-known semiring and is defined as ( &lt; X  X  X  X  , min , + ,  X  , 0).
 tively; Q is a finite set of states of which I and F are initial and final subsets of states, respectively; E is a finite set of transitions between pairs of states with an input and an output alphabet as well as a semiring weight E  X  Q  X  (  X   X  )  X  (  X   X  )  X  K  X  Q ; is an empty element in the alphabet; and  X  and  X  are semiring weights associated with initial and final states, respectively. A weighted finite-state acceptor can be regarded as a special case where either the input or the output alphabet is an empty set. 734 if no two transitions leaving the same state have the same input label. A generic deter-minization algorithm can transform a weighted finite-state acceptor or transducer into its deterministic form if such a form exists. For details on the algorithm and conditions for determinization, see Section 6.2 in Mohri (2009). The condition most relevant for our purpose is that the algorithm works with any weakly divisible semiring. Briefly, a semiring ( K ,  X  ,  X  ,  X  0,  X  1) is said to be divisible if all non-is, ( K  X   X  0) is a group. A semiring is weakly divisible if for any x and y in K such that unique and can be written as z = ( x  X  y )  X  1 x . The non-unique case is not relevant here. 1.2 Lexicographic Semirings
The notion of weight can be extended to complex tuples of weights, and semirings over those tuples. Of interest to us here is a tuple-based semiring, the lexicographic semiring . weight classes W 1 , W 2 ... W n , must observe the path property (Mohri 2002). The path property of a semiring K is defined in terms of the natural order on K such that: a &lt; a  X  b = a . The tropical semiring mentioned above is a common example of a semiring that observes the path property, since and therefore if w 1 &lt; K w 2 , then w 1  X  w 2 = w 1 , and vice versa.
 of in this article, involves a pair of tropical weights, which we will notate the  X  T , T  X  -lexicographic semiring. For this semiring the operations  X  and  X  are defined as follows (Golan 1999, pages 223 X 224):
The term lexicographic is an apt term for this semiring because the comparison for  X  is like the lexicographic comparison of strings, comparing the first elements, then the second, and so forth. Lexicographic semirings can be defined with other underlying semirings or tuple lengths. 1.3 An Example Application of Lexicographic Semiring: Implementing Ranking
As an example of a lexicographic semiring that has a tuple length (usually) greater than 2, consider one way in which one might implement constraint ranking in Optimality Theory.
 patterns are explained by a rank-ordered set of violable constraints. Actual forms are generated via a function G EN , and selected by considering which of the forms violates the lowest-ranked constraints. Each constraint may have multiple violations, but a single violation of a higher-ranked constraint trumps any number of violations of a lower-ranked constraint.
Optimality_theory#Example : It accounts for the form of the regular noun plural suffix ( dishes ), and /z/ otherwise. Quoting directly from the Wikipedia example, the following constraints in the order given account for the phenomena: generates a range of possible forms, including those in the lefthand column in the following table:
Asterisks indicate violations, and exclamation marks indicate the critical violation that rules out the particular form. Both dishs and dishz have violations of *SS , and because none of the other forms violate *SS , and *SS is highest ranked, those two violations are critical. Concomitantly, any other violations (e.g., dishs violation of Ident ) are irrelevant for determining the fate of those forms. Moving down the constraint hierarchy, dish violates Max , because the suffix does not appear in this form; again this violation is critical, because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi) , which dishis violates because the underlying z is changed to an s . This violation is therefore critical, and the winning form is dishiz , indicated by the right-pointing hand.
 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 to show that an appropriately defined lexicographic semiring can readily model the constraint ranking.
 just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as  X  V , V , ...  X  , namely, a lexicographic tuple over V . The number of elements of the tuple is the same rank-ordered constraints, as above, then  X  V , V , ...  X  is a 5-tuple over V . for a word, and a set of n constraints, we need a set of constraint acceptors C each of which matches individual violations of the constraints, and where each weight. So in the given example, *SS would be a finite-state acceptor that allows sibilant-sibilant sequences, but only at a cost  X  1, 0, 0, 0, 0  X  per sequence. Assuming that when G EN deletes an element (as in the form dish ), it marks the deletion (e.g., dish* ), then we can implement Max as an acceptor that accepts the deletion symbol with cost per instance. Finally, Ident(Voi) assumes that a change in voicing is marked somehow (e.g., dishis &lt; ), and this marker will be accepted with cost  X  0, 0, 0, 0, 1  X  per instance. each of the constraints, and then computing the shortest path to select the form with the best overall cost. Formally:
In the case at hand, the cost of each of the paths will be as follows, ranked from worst to best, from which it immediately can be seen that the optimal form is dishiz :
Hence a lexicographic semiring designed for Optimality Theory would have as many dimensions as constraints in the grammar. 1 In what follows, we discuss two specific binary lexicographic semirings of utility for encoding and performing inference with sequence models encoded as weighted finite-state transducers. 2. Paired Tropical Lexicographic Semiring and Applications
We start in this section with a simple application of a paired tropical-tropical lexico-language model. Although  X  -transitions can be represented exactly, as we shall argue in the following, there are limitations on their use, limitations that can be overcome by representing them instead as arcs and lexicographic weights. 2.1 Lexicographic Language Model Semiring
Representing smoothed n -gram language models as weighted finite-state transducers of the  X  X therwise X  formulation of smoothing (Allauzen, Mohri, and Roark 2003). For example, the typical backoff formulation of the probability of a word w given a history h is as follows: where P is an empirical estimate of the probability that reserves small finite probability for unseen n -grams;  X  h is a backoff weight that ensures normalization; and h backoff history typically achieved by excising the earliest word in the history h . The n -gram transitions explicitly for observed n -grams X  X hat is, counts greater than zero, as opposed to all possible n -grams of the given order, which would be infeasible in, for example, large vocabulary speech recognition. This is a massive space saving, and such an approach is also used for non-probabilistic stochastic language models, such as those trained with the perceptron algorithm (Roark, Saraclar, and Collins 2007), as the means to access all and exactly those features that should fire for a particular sequence in a deterministic automaton. Similar issues hold for other finite-state sequence processing problems, for example, tagging, bracketing, or segmenting, as with the POS tagger that we use for experimental results in Section 3.4.
 automaton X  X n the case of n -gram models, all possible n -grams for that order. Dur-ing composition with the model, the failure transition must be interpreted on the fly, keeping track of those symbols that have already been found leaving the original state, and only allowing failure transition traversal for symbols that have not been found (the semantics of  X  X therwise X ). This compact implicit representation cannot generally be preserved when composing with other models, for example, when combining a language model with a pronunciation lexicon as in widely used FST approaches to speech recognition (Mohri, Pereira, and Riley 2002). Moving from implicit to explicit representation when performing such a composition leads to an explosion in the size of the resulting transducer, frequently making the approach intractable. In practice, an off-line approximation to the model is made, typically by treating the failure transitions as epsilon transitions (Mohri, Pereira, and Riley 2002; Allauzen, Mohri, and Roark 2003), allowing large transducers to be composed and optimized off-line. These complex approximate transducers are then used during first-pass decoding, and the resulting pruned search graphs (e.g., word lattices) can be rescored with exact language models encoded with failure transitions. Failure transitions can be used to exactly encode a wide range of language models, including class-based language models (Allauzen, Mohri, and Roark 2003) or discriminatively trained n -gram language models (Roark, Saraclar, and Collins 2007) X  X llowing for full lattice rescoring rather than n -best list extraction. tag sequence will have been observed during training, hence failure transitions will achieve great savings in the size of models. Yet discriminative models may include 738 complex features that combine both input stream (word) and output stream (tag) sequences in a single feature, yielding complicated transducer topologies for which effec-tive use of failure transitions may not be possible. An exact encoding using other mech-anisms is required in such cases to allow for off-line representation and optimization. 2.1.1 Standard Encoding. For language model encoding, we will differentiate between two classes of transitions: backoff arcs (labeled with a  X  for failure, or with using our new semiring); and n-gram arcs (everything else, labeled with the word whose probabil-ity is assigned). Each state in the automaton represents an n -gram history string h and each n -gram arc is weighted with the (negative log) conditional probability of the word w labeling the arc given the history h . We assume that, for every n -gram hw explicitly represented in the language model, every proper prefix and every proper suffix of that n -gram is also represented in the model. Hence, if h is a state in the model, then h suffix of h of length | h | X  1) will also be a state in the model. For a given history h and n -gram arc labeled with a word w , the destination of the arc is the state associated with the longest suffix of the string hw that is a history in the model. This will depend on the
Markov order of the n -gram model. For example, consider the trigram model schematic shown in Figure 1, in which only history sequences of length 2 are kept in the model.
Thus, from history h i = w i  X  2 w i  X  1 , the word w i transitions to h longest suffix of h i w i in the model.

We call the destination state a backoff state. This recursive backoff topology terminates at the unigram state (i.e., h = , no history).
 n -gram of order k + 1 or via an n -gram arc from a lower order n -gram of order k  X  1.
This means that no n -gram arc can enter the zeroeth order state (final backoff), and full-order states (history strings of length n  X  1 for a model of order n ) may have n -gram arcs entering from other full-order states as well as from backoff states of history size n  X  2. 2.1.2 Exact Encoding of a Backoff Model with Lexicographic Language Model Semiring. For an LM machine M on the tropical semiring with failure transitions, we can simulate  X  -arcs in a standard LM topology by a topologically equivalent machine M lexicographic  X  T , T  X  semiring, where  X  has been replaced with epsilon, as follows. Let s and s 0 i be equivalent states in M and M 0 , respectively. For every n -gram arc with label w and weight c , source state s i and destination state s w label w , weight  X  0, c  X  , source state s 0 i , and destination state s is constructed as follows. If the state is non-final, the cost is  X  X  X  ,  X  X  . Otherwise if it is final with exit cost c , it will be  X  0, c  X  .
 graphic language model semiring is enumerated in Figure 2 and illustrated in Figure 3. (backoff) weight c , source state s i , and destination state s k , construct an -arc with source state s 0 i , destination state s where  X  &gt; 0 and  X   X  ( n  X  k ) takes  X  to the ( n  X  k ) are backing off from a bigram state h (history length = 1) to a unigram state, n  X  k = 2  X  where the  X  -arc has weight  X  , which can happen in some language model topologies, the corresponding  X  T , T  X  weight will be  X  X  X  ,  X  X  . 740 need to also convert those models to the  X  T , T  X  semiring. For these automata, we simply use a default transformation such that every transition with weight c is assigned weight  X  0, c  X  . For example, given a word lattice L , we convert the lattice to L semiring using this default transformation, and then perform the intersection L
By removing epsilon transitions and determinizing the result, the low cost path for any given string will be retained in the result, which will correspond to the path achieved with  X  -arcs. Finally we project the second dimension of the  X  T , T  X  weights to produce a lattice in the tropical semiring, which is equivalent to the result of L  X  M , namely, where C 2 denotes projecting the second-dimension of the  X  T , T  X  weights, det (  X  ) denotes determinization, and eps-rem (  X  ) denotes -removal. 2.2 Proof of Equivalence
We wish to prove that for any machine N , ShortestPath( M equivalent states in M 0 to those passed through in M for ShortestPath( M  X  N ). Therefore determinization of the resulting intersection after -removal yields the same topology as intersection with the equivalent  X  machine. Intuitively, because the first dimension traverse the fewest possible backoff arcs; further, because higher-order backoff arcs cost less in the first dimension of the  X  T , T  X  weights in M n -gram arcs at their earliest possible point.
 state s i / s 0 i in the respective machines M / M 0 .

Base case: If p / p 0 is of length 0, and therefore the states s respective machines, the proposition clearly holds.

Inductive step: Now suppose that p / p 0 visits s 0 ... s reached s i / s 0 i in the respective machines. Suppose the cumulated weights of p / p
W and  X   X  , W  X  , respectively. We wish to show that whichever s the path becomes s 0 ... s i s j ), the equivalent state s s ... s 0 i s 0 j ).
 to consider:
In cases (1) and (2), there is only one possible transition to take in either M or M based on the algorithm for construction of M 0 given in Section 2.1.2, these transitions machine. This leaves case (4) to consider. In M , because there is a transition leaving state s labeled with w , the backoff arc, which is a failure transition, cannot be traversed, hence the destination of the n -gram arc s j will be the next state in p . However, in M n -gram transition labeled with w and the backoff transition, now labeled with , can be traversed. What we will now prove is that the shortest path through M the backoff arc in this case.
 ( ) transitions must be taken, followed by an n -gram arc labeled with w . Let k be the  X  ( n  X  k )  X  ,  X  log(  X  s 0 w , the first dimension of our accumulated cost will be m ( n  X  k + algorithm for the construction of M 0 given in Section 2.1.2. Let s after traversing m backoff arcs followed by an n -gram arc labeled with w . Note that, by definition, m  X  k , and k  X  m + 1 is the order of state s algorithm, the state s 0 l is also reachable by first emitting w from state s followed by some number of backoff transitions, as can be seen from the paths between reach state s 0 l , one fewer than the path to state s a total cost of ( m  X  1)( n  X  k + m  X  1 2 )  X  , which is less than m ( n  X  k + is of order k + 1, there will be m backoff arcs to reach state s
Hence the state s 0 l can always be reached from s 0 i with a lower cost through state s by first taking the backoff arc from s 0 i . Therefore the shortest path on M s ... s 0 i s 0 j . 2 2.3 Experimental Comparison of ,  X  , and  X  X  , T  X  Encoded Language Models
For our experiments we used lattices derived from a very large vocabulary contin-uous speech recognition system, which was built for the 2007 GALE Arabic speech recognition task, and used in the work reported in Lehr and Shafran (2011). The lexicographic semiring was evaluated on the development set (2.6 hours of broadcast news and conversations; 18K words). The 888 word lattices for the development set were generated using a competitive baseline system with acoustic models trained on about 1,000 hours of Arabic broadcast data and a 4-gram language model. The language model consisting of 122M n -grams was estimated by interpolating 14 components. The pronunciations.
 using OpenFst (Allauzen et al. 2007), and represented in three ways: (1) as an approxi-mation of a failure machine using epsilons instead of failure arcs; (2) as a correct failure machine; and (3) using the lexicographic construction derived in this article. Note that all of these options are available for representing language models in the OpenGrm library (Roark et al. 2012).
 lattices of the development set. The overall error rate for the systems was 24.8% X  742 and lexicographic machines always produced identical lattices (as determined by FST equivalence); in contrast, 78.6% of the shortest paths from the epsilon approximation are different, at least in terms of weights, from the shortest paths using the failure LM. For full lattices 6.1% of the lexicographic outputs differ from the failure LM outputs, due to small floating point rounding issues; 98.9% of the epsilon approximation outputs differ. alent  X  T , T  X  -lexicographic LM requires 120 Mb, due to the doubling of the size of the weights. 4 To measure speed, we performed the intersections 1,000 times for each of our 888 lattices on a 2993 MHz Intel Xeon CPU, and took the mean times for each of our methods. The 888 lattices were processed with a mean of 1.62 seconds in total (1.8 msec per lattice) using the failure LM; using the  X  T , T  X  -lexicographic LM required 1.8 seconds (2.0 msec per lattice), and is thus about 11% slower. Epsilon approximation, where the failure arcs are approximated with epsilon arcs, took 1.17 seconds (1.3 msec per lattice). The slightly slower speeds for the exact method using the failure LM, and  X  T , T  X  are due to the overhead of (1) computation of the failure function at runtime for the failure LM, and (2) determinization for the  X  T , T  X  representation. After intersection from any of the three methods.
 a finite-state language model topology can be exactly represented using arcs, and weights in the  X  T , T  X  lexicographic semiring.
 time involving a novel string semiring as one of the components. 3. Tagging Determinization on Lattices
In many applications of speech and language processing, we generate intermediate results in the form of a lattice to which we apply finite-state operations. For example, we might POS tag the words in an ASR output lattice as an intermediate stage for detecting out-of-vocabulary nouns. This involves composing the lattices with a POS tagger and will result in a weighted transducer that maps from input words to tags.
 each word sequence just the single-best tagging. One obvious way to do this would be to extract sublattices containing all possible taggings of each word sequence, compute the shortest path of each such sublattice, and unite the results back together. There are various ways this might be accomplished algorithmically, but in general it will be an expensive operation.
 the problem we have just described involves determinization. That is, the result is deterministic in the sense that for any input, there is a unique path through the lattice. But one cannot simply apply transducer determinization because, for one reason, any given input may have multiple outputs and thus is non-functional and not even p -subsequential (Mohri 2009).
 solution that involves determinization on an acceptor in that semiring. One, due to Povey
Shafran et al. (2011), is presented in Sections 3.2 and 3.3. In Section 3.4 we compare the approaches for efficiency. 3.1 Povey et al. X  X  Approach
Povey et al. (2012) define an appropriate pair weight structure such that determinization yields the single-best path for all unique sequences. In their pair weight ( T , S ), T is the original (tropical) weight in the lattice, and S is a form of string weight representing the tags. Using here the more formal  X   X   X  to denote concatenation, they define  X  and  X  operations as: determinization does not work on this semiring. They change the standard determiniza-tion of a lexicographic semiring by defining a new  X  X ommon divisor X  operation for their pair weight. In the standard determinization,  X  operation finds the common divisor of the weights.
They create a state-level lattice during ASR decoding and determinize it to retain only the best-scoring path for each word sequence. They invert the state-level lattice, encode it as an acceptor with its input label equal to the input label of lattice (word), and the pair weight equal to the weight and output label of the lattice, and finally determinize the acceptor to get the best state-level alignment for each word sequence.
 this particular type of weight) are done simultaneously in their method. For the string part, they use a data structure involving a hash table which enables string concatenation in linear time. 3.2 Categorial Semiring terminization, using instead the standard definition already provided in the OpenFst 744 library. To this end, we designed a lexicographic weight pair that incorporates a tropical weight as the first dimension and a novel form of string weight for the second dimen-sion to represent the tags. Note that the standard string weight (e.g., that implemented in the OpenFst library) will not do. In that semiring, w 1 and w 1  X  w 2 is defined as the longest common prefix of w general equal to either w 1 or w 2 . Thus the string weight class does not have the path property, and hence it cannot be used as an element of a lexicographic semiring tuple. cording to some definition of string ordering) of w 1 and w the semiring has the path property. But now we need a way to make the semiring weakly divisible , so that when weights are pushed during the determinization operation, the  X  X oser X  can be preserved. For a string weight, this can be achieved by recording the division so that a subsequent  X  operation with the appropriate (inverse) string is cancellative . Thus if x  X  y = y , then there should be a z = y \ x , such that ( x  X  y )  X  z = ( x  X  y )  X  y \ x = y  X  y \ x = x .
 mar, there are a set of primitive categories, such as N , V , NP , as well as a set of complex denotes a category that, when combined with an X on its left, makes a Y . For example, a verb phrase in English could be represented as a NP \ S , because when combined with an NP on the left, it makes an S . Similarly, a determiner is NP / N , because it combines with an N on the right to make an NP .
 ourselves in this discussion to the left-categorial semiring, the right-categorial version being equivalently defined. Thus we define the left-categorial semiring (  X  over strings  X   X  with and  X  s as special infinity and null string symbols, respec-tively (as in the normal string semiring). The  X  operation accumulates the symbols along a path using standard string concatenation. The  X  operation simply involves a string comparison between the string representations of (possibly accumulated versions of) the output symbols or tags using lexicographic less-than ( &lt; records the left-division in the same sense as categorial grammar. Finally, we introduce a function R EDUCE , which performs reductions on any string, so that for example R
EDUCE ( a  X  a \ b ) = b :
We further define grouping brackets  X  and  X  as part of the notation so that, for example, a complex weight a \ bc divided into d is  X  a \ bc  X  X  d .
 because with that definition,  X  is not distributive over  X  . As stated in Section 1.1, a semiring must be defined in such a way that w 1  X  ( w 2  X  w
To see that this is not in general the case with the above definition, let w and w 3 = b . Using  X   X  to indicate concatenation of two weights, and assuming that a &lt; b &lt; L c , then: whereas
To solve this problem requires modifying our semiring definition slightly to distinguish the concatenations involved in creating the particular weight instance, without any concomitant reductions, and the value is the actual value of the weight, including the reductions. We redefine the left categorial semiring as follows: ( c  X  c \ a ), on the other hand, has a value of a as before, but a history of c c \ a . The sum of these weights is determined by the lexicographic comparison c b &lt; ( c  X  c \ a )  X  ( c  X  b ) = c b .
 histories rather than the concatenated weight values, in order to guarantee that  X  is associative: for semiring  X  it must be the case that w 1  X  ( w on the basis of the values of the weights, we have but
However, the histories in both cases are given as: rial grammar is that the categorial semiring division may involve complex categorial weights that are themselves concatenated, as we have already seen. For example, one may need to left-divide a category NN by a complex category that itself involves a divi-sion and a multiplication. We might thus produce a category such as  X  VB \ JJ NN  X  X  NN .
We assume division has precedence over multiplication (concatenation), so in order to represent this complex category, the disambiguating brackets  X  X  are needed. The 746 interpretation of this category is something that, when combined with the category
VB \ JJ NN on the left, makes an NN . 3.3 Implementation of Tagging Determinization Using a Lexicographic Semiring
Having chosen the semirings for the first and second weights in the transformed weighted finite-state automaton, we now need to define a joint semiring over both the weights and specify its operation. For this we return to the lexicographic semiring.
Specifically, we define the  X  T , C  X  lexicographic semiring (  X &lt; X  X  X  X  ,  X  a tuple of tropical and left-categorial weights, inheriting their corresponding identity elements. The  X  0 and  X  1 elements for the categorial component are defined the same way as in the standard string semiring, namely, respectively, as the infinite string, and as the empty string , discussed previously.

A Sketch of a Proof of Correctness : The correctness of this lexicographic semiring, combined with determinization, for our problem could be shown by tracing the results of operation in a generic determinization algorithm, as in Mohri (2009). Instead, here we provide an intuition using the example in Figure 4. The two input strings fine me and fine mead share the prefix fine . In the first case, fine is a verb (VB), whereas in the second it is an adjective (JJ). When two outgoing arcs have the same input symbols, the determinization algorithm chooses the arc with the lowest weight,  X  1, JJ  X  . For potential is arithmetic subtraction.) When processing the next set of arcs, the determinization algorithm will encounter two paths for the input fine mead . The accumulated weight on
The accumulated weight computed by the determinization algorithm through 0-1-3 arc weight for mead from 1-3. Thus, the accumulated weight for 0-1-3 for fine mead is  X  1, JJ  X  X  X  X  1, JJ \ VB  X  X  X  X  7, NN  X  =  X  9, VB _ NN  X  . From the two possible paths that terminate at node 3 with input string fine mead , the determinization algorithm will pick one with the lowest accumulated weight,  X  7, JJ _ NN  X  X  X  X  9, VB _ NN  X  =  X  7, JJ _ NN  X  , the expected re-sult. Similarly, the determinization algorithm for the input fine me will result in picking the weight  X  5, VB _ PRP  X  . Thus, the determinization algorithm will produce the desired result for both input strings in Figure 4 and this can be shown to be true in general. mulate in certain paths, as in the earlier example. These weights need to be mapped back to associated input symbols (words). This mapping and the complete procedure for computing the single-best transduction paths for all unique input sequences for a given few sections. Note that our categorial semiring allows for synchronizing the resulting output labels with their associated input labels, which the Povey et al. (2012) approach in general does not. 3.3.1 Lattice Representation. Consider a lattice transducer where input labels are words part-of-speech tagger), and weights in the tropical semiring represent negative log probabilities. For example, the toy lattice in Figure 5 has four paths, with two possible sequence, there may be many paths in the lattice with that word sequence, with different costs corresponding to different ways of deriving that word sequence from the acoustic input, as well as different possible ways of tagging the input.
 sequence is as follows. We convert the weighted transducer to an equivalent acceptor in the  X  T , C  X  -lexicographic semiring as in the algorithm in Figure 6. This acceptor is then determinized in the  X  T , C  X  -lexicographic semiring, to yield a lattice where each distinct sequence of input-labels (words) corresponds to a single path. The result of converting the lattice in Figure 5 to the  X  T , C  X  semiring, followed by determinization, and conver-sion back to the tropical semiring, is shown in Figure 7. Note now that there are three paths, as desired, and that the tags on several of the paths are complex categorial tags. complex categorial weights in the second component of the weight pair. It is now necessary to simplify these categorial weight sequences down to sequences of simplex categories, and reconstruct a transducer that maps words to tags with tropical weights. Figure 8 presents the result of such a simplification.
 volves pushing  X  T , C  X  -lexicographic weights back from the final states, splitting states as needed, and then reconstructing the now simple categorial weights as output labels
Figure 6. The second approach involves creating a transducer in the tropical semiring with the input labels as words, and the output labels as complex tags. For this approach we need to construct a mapper transducer which, when composed with the lattice, will reconstruct the appropriate sequences of simplex tags. 3.3.2 State Splitting and Weight Pushing. In the first approach we push weights back from 748 C The categorial weights of each arc are split into a prefix and a suffix, according to the S
PLIT W EIGHT function of Figure 9. The prefixes will be pushed towards the initial state, but if there are multiple prefixes associated with arcs leaving the state, then the state will need to be split: For k distinct prefixes, k distinct states are required. The P algorithm in Figure 10 first accumulates the set of distinct prefixes at each state subsequently modified. For each prefix, a new state is created (lines 22 X 25), although the first prefix in the set simply uses the state itself. Note that any categorial weight associated with the final cost yields the first prefix, meaning that it would be assigned the already existing state; hence all newly created states can be non-final. Each state is thus associated with a distinct single prefix, and each must be reachable from the same set of previous states as the original state. Thus, for each new state, any arc that already has the original state as its destination state must be copied, and the new arc assigned the new destination state and weight, depending on the prefix associated with the new state (lines 26 X 30). The prefix associated with the original state must then be pushed onto the appropriate arcs (line 29). Finally, because all the prefix values have been pushed, each arc from the original state must be updated so that only the suffix value remains in the weight, now leaving the state associated with the original weight X  X  prefix (lines 31 X 34). 3.3.3 Mapper Approach. In the second approach, we build a mapper FST ( M ) that con-verts sequences of complex tags back to sequences of simple tags. The algorithm for constructing this mapper is given in Figure 11, and an illustration can be found in Fig-ure 12. In essence, sequences of observed complex tags are interpreted and the resulting simplex tags are assigned to the output tape of the transducer. Simplex tags in the lattice are mapped to themselves in the mapper FST (line 6 of the function B
Figure 11); complex tags require longer paths, the construction of which is detailed in the M AKE P ATH function. The complex labels are parsed, and required input and output labels are placed on LIFO queues (lines 3 X 7). Then a path is created from state 0 in the mapper FST that eventually returns to state 0, labeled with the appropriate input and output sequences (lines 9 X 15).
 posed with the mapper X  L 0  X  M  X  X o yield the desired result, after projecting onto output labels. Note, crucially, that the mapper will in general change the topology of the deter-minized acceptor, splitting states as needed. This can be seen by comparing Figures 7 and 8. Indeed, the mapping approach and P USH S PLIT are completely equivalent, and, as we shall see, have similar time efficiency.
 750 tains the words flies like meat , which has the categorial tag sequence in Figure 7. The cancellation, working from right to left, first reduces with yielding This then is concatenated with the initial simplex category to yield the sequence
NNS VB NN . The actual cancellation is performed by the mapper transducer in Figure 12; the cancellation just described can be seen in the path that exits state 0, passes through state 3, and returns to state 0.
 it operates on the determinized lattice before it is converted back to the tropical semiring ; after which the simplex categories are reconstructed onto the output labels to yield a transducer identical to that in Figure 8.
 4 while  X  6 = 1 7  X   X   X  3 while |  X   X  | &gt; 0 3 Let  X  =  X  \  X  for rightmost (non-embedded) backslash denominator \ numerator 752 3.4 Experimental Comparisons Between Povey et al. X  X  and 3.4.1 POS-Tagging Problem. Our solutions were empirically evaluated on 4,664 lattices from the NIST English CTS RT Dev04 test set. The lattices were generated using a state-of-the-art speech recognizer, similar to Soltau et al. (2005), trained on about 2,000 hours of data, which performed at a word error rate of about 24%. The utterances were decoded in three stages using speaker independent models, vocal-tract length normalized models, and speaker-adapted models. The three sets of models were similar in complexity with 8,000 clustered pentaphone states and 150K Gaussians with diagonal covariances.
 The tagger was trained on the Switchboard portion of the Penn Treebank (Marcus,
Santorini, and Marcinkiewicz 1993). Treebank tokenization is different from the recog-nizer tokenization in some instances, such as for contractions ( X  X on X  X  X  becomes  X  X o n X  X  X ) or possessives ( X  X aron X  X  X  becomes  X  X aron  X  X  X ). Further, many of the words in the recognizer vocabulary of 93k words are unobserved in tagger training, and are mapped to an OOV token  X   X  unk  X   X . Words in the treebank not in the recognizer vocabulary are also mapped to  X   X  unk  X   X , thus providing probability mass for that token in the tagger.
A tokenization transducer T was created to map from recognizer vocabulary to tagger vocabulary.

Markov model (HMM), estimated and encoded in tagging transducers P . In the first-order HMM model, the transition probability is conditioned on the previous word X  X  tag, whereas in the third-order model the transition probability is conditioned on the previous three words X  tags. The transition probabilities are smoothed using Witten-Bell smoothing, and backoff smoothing is achieved using failure transitions. For each word in the tagger input vocabulary, only POS-tags observed with each word are allowed for that word; that is, emission probability is not smoothed and is zero for unobserved tag/word pairs. For a given word lattice L , it is first composed with the tokenizer T , then with the POS tagger P to produce a transducer with original lattice word strings on the input side and tag strings on the output side.
 board treebank. The first-order model achieved 91.4% tagging accuracy, and the third-order model 93.8% accuracy, which is competitive for this particular task: Eidelman,
Huang, and Harper (2010) reported accuracy of 92.4% for an HMM tagger on this task  X   X  unk  X   X  category, which is relatively coarse and does not capture informative suffix and prefix features that are common in such models for tagging OOVs. For the purposes of this article, these models serve to demonstrate the utility of the new lexicographic semiring using realistic models. A similar WFST topology can be used for discrimina-tively trained models using richer feature sets, which would potentially achieve higher accuracy on the task.
 ger, were then converted to  X  T , C  X  -lexicographic semiring, determinized in this lexico-graphic semiring, and then converted back using the mapper transducer as discussed in
Section 3.3.1. Note that the computational cost of this conversion is proportional to the number of arcs in the lattice and hence is significantly lower than the overhead incurred in the conventional approach of extracting all unique paths in the lattice and converting the paths back to a lattice after tagging.
 best paths through the original lattice, and removing any path where the path X  X  word sequence had been seen in a lower-cost path. This generally resulted in a rank-ordered set of paths with n &lt; 1, 000 members.
 identical to the n-best paths produced by the method just described. The only differ-ences were due to minor floating-point number differences (expected due to weight-pushing in determinization), and cases where equivalent weighted paths were output in different orders. 3.4.2 Results. Despite large overall commonalities between Povey et al. X  X  approach (henceforth Povey), and  X  T , C  X  lexicographic approaches (henceforth TC), there are some interesting differences between the two. One difference is that the highly struc-tured categorial weights used in TC are more complex than the string weight used in Povey. Another important difference in the approaches is the synchronization issue. In
TC, the original input symbols are synchronized with determinized output symbols, whereas in Povey they are not. TC uses the semantics of the categorial grammar to keep the history of the operations while determinizing a lattice, whereas Povey lacks this semantics. Although POS-tagging is a task that by definition has one tag per input token, many other tasks of interest (e.g., finding the most likely pronunciation or state sequence) will have a variable number of output labels per token, making syn-chronization in the absence of such semantics more difficult. Hence, these differences may affect time and space complexity, feasibility, and ease of use of the approaches in various tasks.
 situations on the same data. We ran the experiments detailed in Section 3.4.1 in three conditions: Povey in the Kaldi toolkit (Povey et al. 2011) (with specialized determiniza-tion); and both Povey and TC in the OpenFST library (with general determinization).
This allows us to tease apart the impact of the differences in the approaches that are due to the specialized determinization versus differences in the weight definitions.
There would be nothing in principle to prevent the simultaneous epsilon removal being implemented in OpenFst for use with general determinization in  X  T , C  X  lexicographic semiring, although this is not the focus of this article.
 disk space. Tables 1(a) and 1(b) show efficiency results of determinizing lattices tagged 754 using the first-order HMM tagger, and Tables 2(a), and 2(b) show those results for the third-order HMM tagger.
 with TC. However, results using Povey with general determinization show that the memory demands between the two approaches are similar in the absence of the spe-cialized determinization. We also see that the average number of intermediate tags produced during determinization in Povey is larger, whereas the average length of intermediate tags is smaller, than those in TC. This is due to the fact that the categorial semiring keeps a complete history of operations by appending complex tags. We do not perform any special string compression on these tags, which may yield performance improvements (particularly with the larger POS-tagging model, as demonstrated in Table 2).
 gorithm in TC. The outputs were equivalent in both cases and the time and space complexities were comparable. The P USH S PLIT algorithm was slightly more efficient than the mapper approach, although the difference is not significant.

Table 1(b) that the output lattices resulting from TC are smaller than the output lattices in Povey in terms of number of states, transitions, input/output epsilons, and required disk space. Because the lattices produced by Povey are not synchronized, they contain many input/output epsilons, and therefore an increased number of states and transi-differences are even larger between the two approaches when both are using general determinization. order HMM tagger follow the same pattern as those using the first-order HMM tagger, although the differences are more pronounced than in the former. We report these results on a subset of 4,000 out of 4,664 test lattices, chosen based on input lattice size so as to avoid cases of very high intermediate memory usage in general determinization.
This high intermediate memory usage does argue for the specialized determinization, and was the rationale for that algorithm in Povey et al. (2012). The non-optimized string representation within the categorial semiring makes this even more of an issue for TC than Povey. Again, though, the size of the resulting lattice is much more compact when using the lexicographic  X  T , C  X  semiring. We leave investigation of an optimized string representation, such as storing the history only if it is different from the value , using the hash table data structure, or memory caching, to future work. 4. Combining the Semirings
In this article, we have described two lexicographic semirings, each consisting of a weight pair. Suppose one wished to combine these two in a system that tags a lattice, and then selects the single best tagging for each word sequence. An obvious way to do this would be to implement a two-stage process. Apply the n -gram Markov model of the tagger with the backoff strategy implemented using the paired tropical semiring in
Section 2 with tags as acceptor labels. Then, convert the resulting transducer into the lexicographic  X  T , C  X  semiring with words as acceptor labels and determinize to obtain the correct results.
 756 the two semirings into a single  X  T , T , C  X  lexicographic triple where, for example, the first third dimension holds the tags represented in the categorial semiring. One might then compose the tagging model with the lattice, and then determinize in one step in the triple semiring.
 semiring and determinize in it, it yields the wrong results. The reason for this is that the lexicographic semirings for the two tasks (the tagging task and the subsequent determinization of the tagged lattice) involve determinization with respect to different labels. In the first task, the backoff models are defined with respect to the Markov chain or n -grams of the tags and the labels on the resulting acceptor are tags. In the second task, the determinization needs to be performed with respect to the word labels to obtain unique tags for all word sequences. A cross product of the two types of labels would not accomplish the task either, because the determinization would then produce unique paths for all word and tag combinations, and not the best tag sequences for all word sequences. There is no obvious or easy way to determinize with respect to both sets of labels simultaneously.
 understanding how each of the semirings functions. The simple example involves a a can take two possible tags A or B . We will assign variables to model costs, so that we can illustrate the range of scenarios where the use of the triple semiring will yield an incorrrect answer, and why. Let c ( a : A ) be the cost of the tag A with word a , which sequence model) of transitioning from state x to state y in the model. See Figure 13 for our example L , T , L  X  T , and G . All costs in the example are in the  X  T , T  X  semiring for ease of explication; the first dimension of the cost is zero except for backoff arcs in G . epsilon removal and conversion from a transducer in the  X  T , T  X  semiring to an acceptor in the  X  X  T , T  X  , C  X  semiring. In the second and third WFSTs, we highlight the paths that have zero cost in the first dimension of that semiring, which are the only paths that can result from determinization (whatever the model costs). These paths only include tag lower (second dimension) cost than a:B a:A, despite having taken a backoff arc. Because using a backoff arc is the only way to produce the tag sequence A A , then that path should be the result. In order to get the correct result, one must first determinize with x:Y labels as unit (using fstencode) in the  X  T , T  X  semiring; then project into the  X  T , C  X  semiring and determinize again. 5. Conclusions
In this article, we have introduced two applications of lexicographic semirings to speech and language processing problems. The first application used the  X  T , T  X  lexicographic semiring to provide an exact encoding of failure arcs in an n -gram language model using an epsilon representation for the failure arc. This lexicographic language model semiring allows much more flexibility in combining the language model with other linguistic models without danger of prohibitive blow-up in the size of the resulting transducers: for example, precomposing the language model with a lexicon and a context model in a CLG model of speech recognition (Mohri, Pereira, and Riley 2002).
 determinizing a tagged word lattice so that each word sequence has the single best tag sequence. This was accomplished by encoding the tags as the second dimension of the  X  T , C  X  semiring, then determinizing the resulting acceptor. Finally we map the second dimension categorial weights back as output labels. This latter stage generally requires that we push complex categorial weights back to reconstruct a sequence of simplex categories, an operation that can be performed in two distinct and equally efficient ways. As part of this work we developed a novel string semiring, the categorial semiring, which we have described in detail for the first time here.
 competitive in terms of efficiency with alternative approaches.
 further applications. For example, one might use an Optimality Theory X  X nspired model with ranked constraints implemented using a lexicographic semiring as part of a pro-nunciation modeling system that ranks pronunciations according to the degree to which they violate various constraints of the language.. The  X  T , C  X  -lexicographic semiring introduced in Section 3 can be generalized to compute the single-best transduction path 758 in multi-tape weighted transducers. For instance, by encoding the arc likelihoods, the phone sequence, the clustered allophone sequence, acoustic state sequence, and acoustic segmental duration associated with word sequence as a  X  T , C , C , C , T  X  lexicographic semiring and determinizing the resulting automaton, we can extract the tags corre-sponding to the single-best word sequence. Thus, our method is much more flexible and powerful than algorithms developed specifically for determinizing POS-tagged word in Shugrina (2010).
 graphic semiring is distributed as part of the core OpenFst distribution at http://www .openfst.org . The categorial semiring is available in the contributed section at http://www.openfst.org/twiki/bin/view/Contrib/FstContrib . The categorial rescoring methods including both the mapping and push-split approaches are available from http://www.opengrm.org .
 Acknowledgments References 760
