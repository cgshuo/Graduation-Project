 The estimation of semantic similarity between words is one of the longest-established tasks in Nat-ural Language Processing and many approaches to the problem have been proposed. The two domi-nant lexical similarity paradigms are distributional similarity, which compares words on the basis of their observed co-occurrence behaviour in corpora, and semantic network similarity, which compares words based on their position in a graph such as the WordNet hierarchy. In this paper we consider measures of network similarity for the purpose of supervised classification with kernel methods. The utility of kernel functions related to popular distribu-tional similarity measures has recently been demon-strated by  X  O S  X  eaghdha and Copestake (2008); we show here that kernel analogues of WordNet simi-larity can likewise give good performance on a se-mantic classification task. Kernel-based classifiers such as support vector ma-chines (SVMs) make use of functions called kernel functions (or simply kernels ) to compute the similar-ity between data points (Shawe-Taylor and Cristian-ini, 2004). Valid kernels are restricted to the set of positive semi-definite (psd) functions , i.e., those that correspond to an inner product in some vector space. Kernel methods have been widely adopted in NLP over the past decade, in part due to the good perfor-mance of SVMs on many tasks and in part due to the ability to exploit prior knowledge about a given task through the choice of an appropriate kernel function. In this section we consider kernel functions that use spectral properties of a graph to compute the sim-ilarity between its nodes. The theoretical founda-tions and some machine learning applications of the adopted approach have been developed by Kondor and Lafferty (2002), Smola and Kondor (2003) and Herbster et al. (2008).

Let G be a graph with vertex set V = v 1 ,...,v n and edge set E  X  V  X  V . We assume that G is connected and undirected and that all edges have a positive weight w ij &gt; 0 . Let A be the symmetric n  X  n matrix with entries A ij = w ij if an edge exists between vertices v i and v j , and A ij = 0 otherwise. Let D be the diagonal matrix with entries D ii = P as The normalised Laplacian is defined as  X  L = D  X  1 2 LD  X  1 2 . Both  X  L and L are positive semi-definite, but they are typically used as starting points for the derivation of kernels rather than as kernels themselves.

Let  X  1  X  X  X  X  X  X   X  n be the eigenvalues of L and u ,...,u n the corresponding eigenvectors. Note that u n = 0 for all graphs. L is singular and hence has no well-defined inverse, but its pseudoinverse the resistance distance between points in an elec-trical circuit (Herbster et al., 2008) and to the av-erage commute-time distance , i.e., the average dis-tance of a random walk from one node to another and back again (Fouss et al., 2007). The similar-ity measure defined by L + hence takes information about the connectivity of the graph into account as well as information about adjacency. An analogous pseudoinverse  X  L + can be defined for the normalised Laplacian.

A second class of graph-based kernel functions are the diffusion kernels introduced by Kondor and Lafferty (2002). The kernel H t is defined as H t = e where t&gt; 0 , and  X   X  1  X  X  X  X  X  X   X   X  n and  X  u 1 ,...,  X  u are the eigenvalues and eigenvectors of  X  L + respec-tively. H t can be interpreted in terms of heat diffu-sion or the distribution of a lazy random walk ema-nating from a given point at a time point t . 3.1 Graph construction WordNet (Fellbaum, 1998) is a semantic network in which nodes correspond to word senses (or synsets ) and edges correspond to relations between senses. In this work we restrict ourselves to the noun com-ponent of WordNet and use only hyponymy and in-stance hyponymy relations for graph construction. The version of WordNet used is WordNet 3.0.
To evaluate the utility of the graph-based kernels described in Section 2 for computing lexical sim-ilarity, we use the dataset developed for the task on Classifying Semantic Relations Between Nom-inals at the 2007 SemEval competition (Girju et al., 2007). The dataset comprises candidate exam-ple sentences for seven two-argument semantic rela-tions, with 140 training sentences and approximately 80 test sentences for each relation. It is a particularly suitable task for evaluating WordNet kernels, as the candidate relation arguments for each sentence are tagged with their WordNet sense and it has been pre-viously shown that a kernel model based on distribu-tional lexical similarity can attain very good perfor-mance (  X  O S  X  eaghdha and Copestake, 2008). 3.2 Calculating the WordNet kernels The noun hierarchy in WordNet 3.0 contains 82,115 senses; computing kernel similarities on a graph of this size raises significant computational issues. The calculation of the Laplacian pseudoinverse is com-plicated by the fact that while L and  X  L are very sparse, their pseudoinverses are invariably dense and require very large amounts of memory. To circum-vent this problem, we follow Fouss et al. (2007) in computing L + and  X  L + one column at a time through a Cholesky factorisation procedure. Only those columns required for the classification task need be calculated, and the kernel computation for each relation subtask can be performed in a mat-ter of minutes. Calculating the diffusion kernel in-volves an eigendecomposition of  X  L , meaning that computing the kernel exactly is infeasible. The so-lution used here is to approximate H t by using the m smallest components of the spectrum of  X  L when computing (3); from (2) it can be seen that a similar approximation can be made to speed up computation of L + and  X  L + . 3.3 Experimental setup For all kernels and relation datasets, the kernel ma-trix for each argument position was precomputed and normalised so that every diagonal entry equalled 1. A small number of candidate arguments are not annotated with a WordNet sense or are assigned a non-noun sense; these arguments were assumed to have self-similarity equal to 1 and zero similarity to all other arguments. This does not affect the pos-itive semi-definiteness of the kernel matrices. The per-argument kernel matrices were summed to give the kernel matrix for each relation subtask. The ker-Kernel Acc F Acc F Acc F B 72.1 68.4 ----
L + 73.3 69.4 73.2 70.5 73.6 70.6  X  L + 72.5 70.0 72.7 70.0 74.1 71.0 H t --68.6 64.7 69.8 65.1 nels described in Section 2 were compared to a base-line kernel B . This baseline represents each word as a binary feature vector describing its synset and all its hypernym synsets in the WordNet hierarchy, and calculates the linear kernel between vectors.
All experiments were run using the LIBSVM sup-port vector machine library (Chang and Lin, 2001). For each relation the SVM cost parameter was op-cross-validation on the training set. The diffusion kernel parameter t was optimised in the same way, Macro-averaged accuracy and F-score for each ker-nel are reported in Table 1. There is little difference between the Laplacian and normalised Laplacian pseudoinverses; both achieve better performance than the baseline B . The results also suggest that the reduced-eigenspectrum approximations to L + and  X  well as efficiency via a smoothing effect. The best performance is attained by the approximation to  X  L + with m = 1 , 000 eigencomponents. The heat ker-nel H t fares less well; the problem here may be that the optimal range for the t parameter has not been identified.

Comparing these results to those of the partici-pants in the 2007 SemEval task, the WordNet-based lexical similarity model fares very well. All versions of 15 systems in the competition and higher F-score than all but three. Even the baseline B ranks above all but the top three systems, suggesting that this too can be a useful model. This is in spite of the fact that all systems which made use of the sense annotations also used a rich variety of other information sources such as features extracted from the sentence context, while the models presented here use only the graph structure of WordNet. 1 There is a large body of work on using WordNet to compute measures of lexical similarity (Budanit-sky and Hirst, 2006). However, many of these mea-sures are not amenable for use as kernel functions as they rely on properties which cannot be expressed as a vector inner product, such as the lowest com-mon subsumer of two vertices. Hughes and Ram-age (2007) present a lexical similarity model based on random walks on graphs derived from WordNet; Rao et al. (2008) propose the Laplacian pseudoin-verse on such graphs as a lexical similarity measure. Both of these works share aspects of the current pa-per; however, neither address supervised learning or present an application-oriented evaluation.
Extracting features from WordNet for use in su-pervised learning is a standard technique (Scott and Matwin, 1999). Siolas and d X  X lche-Buc (2000) and Basili et al. (2006) use a measure of lexical similar-ity from WordNet as an intermediary to smooth bag-of-words kernels on documents. Siolas and d X  X lche-Buc use an inverse path-based similarity measure, while Basili et al. use a measure of  X  X onceptual den-sity X  that is not proven to be positive semi-definite. The main purpose of this paper has been to demon-strate how kernels that capture spectral aspects of graph structure can be used to compare nodes in a lexical hierarchy and thus provide a kernelised measure of WordNet similarity. As far as we are aware, these measures have not previously been in-vestigated in the context of semantic classification. The resulting WordNet kernels have been evaluated on the SemEval Task 4 dataset and shown to attain a higher level of performance than many more com-plicated systems that participated in that task.
Two obvious shortcomings of the kernels dis-cussed here are that they are defined on senses rather than words and that they are computed on a rather impoverished graph structure (the WordNet hyponym hierarchy is quite tree-like). One of the significant benefits of spectral graph kernels is that they can be computed on arbitrary graphs and are most powerful when graphs have a rich connectiv-ity structure. Some potential future directions that would make greater use of this flexibility include the following:  X  A simple extension from sense-kernels to  X  Incorporating other WordNet relations such as  X  A WordNet graph can be augmented with in-
