 Jens Allwood  X  Loredana Cerrato  X  Kristiina Jokinen  X  Costanza Navarretta  X  Patrizia Paggio Abstract This paper deals with a multimodal annotation scheme dedicated to the study of gestures in interpersonal communication, with particular regard to the role played by multimodal expressions for feedback, turn management and sequencing. The scheme has been developed under the framework of the MUMIN network and tested on the analysis of multimodal behaviour in short video clips in Swedish, Finnish and Danish. The preliminary results obtained in these studies show that the reliability of the categories defined in the scheme is acceptable, and that the scheme as a whole constitutes a versatile analysis tool for the study of multimodal com-munication behaviour.
 Keywords Multimodal annotation Feedback Hand and facial gestures 1 Introduction The purpose of this paper is to describe the MUMIN multimodal scheme for the annotation of multimodal communicative behaviours (Allwood et al. 2004 ), and to illustrate its application to short video clips in different settings and languages.
The creation of a multimodal corpus often reflects the requirements of a specific application, e.g. the creation of a computer interface that accepts speech and pen input in a very limited domain, as in Steininger et al. ( 2002 ), and thus constitutes an attempt at modelling input or output multimodal behaviour to be handled by the application. Consequently, annotation schemes often reflect the specific require-ments that drive the creation of such a corpus. On the contrary the MUMIN coding scheme, developed in the Nordic Network on Multimodal Interfaces MUMIN, is intended as a general instrument for the study of gestures (in particular hand gestures and facial displays) in interpersonal communication, focusing on the role played by multimodal expressions for feedback, turn management and sequencing. It builds on previous studies of feedback strategies in conversations (Clark and Schaefer 1989 ; Allwood et al. 1992 ), research on non-verbal behaviour (Duncan and Fiske 1977 ; Kendon 2004 ; McNeill 1992 ) and work where verbal feedback has been categorised in behavioural or functional terms (Allwood 2001 , 2001b ; Allwood and Cerrato 2003 ; Cerrato 2004 ).

MUMIN distinguishes itself by its emphasis on functional annotation, thus providing a complementary and important perspective to frameworks dedicated to detailed analyses of the physical characteristics of gestures. Examples are frameworks aiming at the registration of facial movements (Ekman and Friesen 1978 ) and hand gestures (Duncan 2004 ); or studying emotions as expressed by facial movements (Ekman and Friesen 2003 ). Some of these schemes can be used to annotate gestures in different scientific settings. The construction of software agents is one example. Another is the recognition of facial expressions in psychopathology (Harrigan et al. 2005 ).

In what follows, we briefly describe the MUMIN annotation categories starting with the functional ones. Then, in order to illustrate the versatility and general usefulness of the scheme, we deal with three case studies. We conclude with a few reflections on the potential applications of the framework. 2 General annotation methodology The main focus of the coding scheme is the annotation of the feedback, turn-management and sequencing functions of multimodal expressions, with important consequences for the annotation process and results. First of all, the annotator is expected to select gestures 1 to be annotated only if they have a communicative function. In other words, gestures are annotated if they are either intended as communicative by the communicator (displayed or signalled) (Allwood 2001 ), or judged to have a noticeable effect on the recipient. This means that not all gestures need be annotated. For example, mechanical recurrent blinking due to dryness of the eye might not be annotated because it does not seem in a given context to have a communicative function.
Moreover, the attributes concerning the shape or dynamics of the observed phenomena are not fine-grained, because they only seek to capture features that are significant with respect to the functional level of the annotation. However, the annotation of gesture shape and dynamics can be extended for specific purposes, and possibly combined with automatically derived measures, for example to construct computer applications, without changing the functional level of the annotation.
Once a gesture has been selected by an annotator because of its communicative role, it is annotated with functional values, as well as features that describe its behavioural shape and dynamics: this is what we call the modality-specific annotation level. An additional, multimodal annotation level concerns the relation that the gesture has either with other gestures or with the speech modality. The scheme provides a number of simple categories for the representation of multimodal relations. However, it does not include tags for the specific annotation of verbal expressions since its focus is on the study of gestures.

The paper focuses on three communicative functions: feedback, turn manage-ment and sequencing. They are studied together, since very often the same behavioural units are multifunctional, i.e. simultaneously have a function in relation to feedback, turn management and sequencing. For instance the word yes after a statement can be used to give feedback, accept the turn and be the second pair part of a sequence of type proposal . Gestures can of course function in a similar, multifunctional way. Feedback, turn management and sequencing, thus, represent three analytically distinct functional perspectives that can overlap behaviourally. 3 Functional annotation categories Three communicative functions are treated in the scheme: feedback, turn management and sequencing. These functions, and the categories we use to model them, were selected on the basis of the theoretical model of communication management described in Allwood ( 2001 , 2001b ).

Feedback is a pervasive phenomenon in human communication. Participants in a conversation continuously exchange feedback as a way of providing signals about the success or failure of the interaction. Both feedback giving and eliciting are annotated by means of the same three sets of attributes: Basic, Acceptance , and Attitudinal emotions/attitudes , as shown in Table 1 . Basic features are:  X  Continuation/contact and perception (CP), where the dialogue participants  X  Continuation/contact, perception and understanding (CPU), where the inter-
The two categories of basic feedback are intended to capture Clark and Schaefer ( 1989 ) X  X  acknowledgement . CP might occur if a person hears, but does not really understand, e.g. repeating a word to have it clarified. CPU might occur when a person hears and understands without necessarily agreeing, e.g. giving negative feedback after a positive statement.

Acceptance, which is a boolean feature, indicates that the interlocutor has not only perceived and understood the message, but also gives or elicits signs of either agreeing with its content or rejecting it. Basic and Acceptance can be compared with the notions of process-related and content-related in Tho  X  risson ( 2002 ).
Finally, feedback annotation can rely on a list of emotions and attitudes that can co-occur with one of the basic feedback features and with an acceptance feature. Emotions and attitudes are not viewed as distinct phenomena, but rather as the result of having a relational (attitude) or a non-relational perspective (emotion) on similar psychological and behavioural phenomena, e.g.  X  X  X nger X  X  in she is angry might be seen as an emotion, while in she is angry at him , it might be seen as an attitude. The list includes the six basic emotions described and used in many studies (Ekman 1999 ) (Cowie 2000 ) plus  X  X  X ther X  X , which allows for an open list. Emotions/attitudes may then be further classified, for example, on the basis of causes or objects towards which they are directed. Different corpora or applications may also expand the list to include more specific types. The European network of excellence HUMAINE ( http://www.emotion-research.net/ ) provides recent models and applications dealing with emotions, several of which build on the six basic emotions.

Turn management regulates the interaction flow and minimises overlapping speech and pauses. It is coded by the three general features Turn gain, Turn end and Turn hold . An additional dimension concerns whether the turn changes in agreement between the speakers or not. Thus, a gain in turn can either be classified as a Turn take if the speaker takes a turn that wasn X  X  offered, possibly by interrupting, or a Turn accept if the speaker accepts a turn that is being offered. Similarly, the end of a turn can also be achieved in different ways: we can have a Turn yield if the speaker releases the turn under pressure, a Turn offer if the speaker offers the turn to the interlocutor, or a Turn complete if the speaker signals completion of the turn and end of the dialogue at the same time. The various features are shown in Table 2 .
Sequencing concerns the organisation of a dialogue in meaningful sequences, or sub-dialogues, i.e. sequences of speech acts which may extend over several turns. Sequencing is orthogonal to the turn system. Open sequence indicates that a new speech act sequence is starting, e.g. in conjunction with  X  X  X y the way... X  X . Continue sequence indicates that the current speech act sequence is going on, e.g. when a gesture is associated with enumerative phrases such as  X  X  X he first... the second... the third X  X . Close sequence indicates that the current speech act sequence is closed, which may be shown by a head turn while uttering a phrase like  X  X  X hat X  X  it, that X  X  all X  X . The features are shown together with those relevant for turn management in Table 2 . 4 Shape and dynamics of gestures In addition to the functional categories, gestures are also coded with features that describe their shape and dynamics. Although the categories proposed here are coarse-grained, they should be specific enough to be able to distinguish and characterise non-verbal expressions that have a feedback, turn management and sequencing function. They are concerned with the movement dimension of gestures, and should be understood as dynamic features that refer to a movement as a whole. Internal gesture segmentation is not considered.

In the studies reported further below, hand gestures and facial displays are the only types of gesture taken into consideration. However, since the studies clearly pointed to the fact that body posture is also relevant to the phenomena we are interested in, categories dealing with it are tentatively introduced.

The term facial displays refers to timed changes in eyebrow position, expressions of the mouth, movement of the head and of the eyes (Cassell 2000 ). The coding scheme includes features describing gestures and movements of the various parts of the face, with values that are either semantic categories such as Smile or Scowl or rough direction indications such as Up or Down . They add up to a total of 36 different features.

The annotation of the shape and trajectory of hand gesture is a simplification of the scheme from the McNeill Lab (Duncan 2004 ). The features, 7 in total, concern Handedness and Trajectory , so that we distinguish between single-handed and double-handed gestures, and among a number of different simple trajectories analogous to what is done for gaze movement. The value Complex is intended to capture movements where several trajectories are combined. Similarly, body posture comprises trajectory indications for the movement of the trunk. The features that describe facial displays, hand gestures and body posture are shown together in Table 3 .

Finally, semiotic categories relevant for all gesture types have also been defined based on Peirce X  X  semiotic types (Peirce 1931 ) as follows:  X  Indexical deictic gestures locate aspects of the discourse in the physical space  X  Indexical non-deictic gestures also indicate via a causal relation between the  X  Iconic gestures (including so-called metaphoric gestures) express some semantic  X  Symbolic gestures (emblems) are gestures in which the relation between form
These categories are mutually exclusive in this version of the scheme to facilitate the annotation work. This means that in case a gesture seems to have more than one semiotic function X  X or instance to be iconic and symbolic at the same time X  X he annotator is asked to select the most noticeable communicative function and note as a comment that more than one value seems necessary. 5 Relations between modalities Under normal circumstances, in face-to-face communication feedback, turn management and sequencing all involve use of multimodal expressions, and are not mutually exclusive. For instance, turn management is partly done by feedback. A turn can be accepted by giving feedback and released by eliciting information from the other party. Within each feature, however, only one value is allowed, since the focus of the annotation scheme is on the explicit communicative function of the phenomenon under analysis. For example, a head nod which has been coded as CPU (continuation/contact, perception and understanding) cannot be assigned accept and non-accept values at the same time. An example of a multifunctional facial display coded with ANVIL (Kipp 2001 ) is shown in the frame in Fig. 1 : the speaker frowns and takes the turn while agreeing with the interlocutor by uttering:  X  X  X a, det synes jeg X  X  (yes, I think so). By means of the same multimodal expression (facial display combined with speech) he also elicits feedback from the interlocutor and encourages her to continue the current sequence.

The components of a multimodal expression can have different time spans. For instance, a cross-modal relation can be defined between a speech segment and a slightly subsequent gesture. To define a multimodal relation, we make a basic distinction between two signs being dependent or independent . If they are dependent, they are either compatible or incompatible . For two signs to be compatible, they must either complement or reinforce each other, while incompat-ibility arises if they express different contents, as it happens in ironic contexts. 6 Empirical results The coding procedure has been iteratively defined in a series of workshops, and annotations have been carried out by means of the three coding tools ANVIL (Kipp 2001 ), MultiTool (Gunnarsson 2002 ) and NITE (Bernsen et al. 2002 ). The purpose of using different tools was to test the versatility of the coding scheme provided by its being defined in terms of a number of attributes and related values.

The annotated material consists of three different video clips: 1. interview of an actress for Danish TV (1 min.); 2. interview of the Finnish finance minister for Finnish TV (1 min.); 3. clip from the Swedish film Show me love (1 min.).

Although this is not a large data collection, we believe it is an interesting one for a first trial of applying an annotation scheme targeted towards the analysis of communicative behaviours. It consists of natural data as opposed to videos collected in the laboratory, and it shows a challenging variation both in setting and language. 6.1 The Danish case study Two independent annotators with linguistic experience but limited experience in video analysis annotated gestures in the Danish clip using ANVIL. They started by annotating the non-verbal expressions of one of the interlocutors together to familiarise themselves with the task. To train the annotators in using a particular scheme before starting the real annotation is a practice in many annotation projects (Sikorski 1998 ). After the first training exercise the two annotators did the annotation task for the other dialogue participant independently and the reliability of the coding scheme was calculated on these independently annotated data.

In order to align the two annotations, it was decided that two segments referred to the same gesture if they covered the same time span, plus or minus 1/4 of a second at the onset or end of the gesture. The first coder annotated 37 facial displays, and the second one 33. Of these, 29 were common to both coders. One gesture was considered a whole segment by one of the coders and split into two by the other.The results of this evaluation are shown in Tables 4 and 5 .

The k -scores obtained on the features concerning gesture shape and semiotic type are all in the range 0.83 X 0.96 with the exception of those concerning Gaze (0.54) and Head (0.2). This low agreement is partly due to the fact that one coder privileged head position over gaze (head up, no gaze), while the other in such cases ignored head movements and annotated gaze. There are also inconsistencies: in some cases the chosen tag is Gaze side with the comment  X  X  X way from the interlocutor X  X , in others Gaze other with the comment  X  X  X way from the interlocutor X  X . Thus, the interaction of head movement and gaze needs better training of the coders. A more detailed explanation has now been added in the coding manual to clarify the interaction between head and gaze.

In the coding of communicative functions, on the other hand, the annotators achieved satisfactory k -scores with the only exception of sequencing, particularly the feature Continue sequence . In general, the interaction of turn management and sequencing is an interesting issue that merits further study. In a sense, any exchange of turns is a sequence, but only some such sequences recur sufficiently often to be called exchange types . In a further study, we will attempt to investigate what sequences recur as conventionalised exchange types. 6.2 The Swedish case study The Swedish video clip consists of a one-minute emotional conversation between two actors who interpret father and daughter. They are mostly filmed in close ups of their faces, so that the hands are rarely in the picture, making it impossible to annotate hand gestures. The actor that speaks is not always in focus, in fact in two cases in which the speaker utters a feedback expression, the face cannot be observed.

Only one expert annotator coded the film scene, so reliability was evaluated only by means of an inter-variance test, which checks whether a coder X  X  judgements vary over time. The coder annotated the material once and then again after about six months. The same 12 facial displays related to feedback were coded both times. The coded facial displays related to turn management functions were 12 the first time and 13 the second time. Table 6 shows the number of features relative to feedback and turn management assigned during the first annotation session.

Not all behaviours are represented in the same degree. Given the fact that it is not possible to predict non-verbal communicative behaviour, it is not easy to collect materials that contain an even number of each communicative phenomenon. Collecting and analysing more material will of course provide a more solid basis for the characterisation of the various behaviours covered by the scheme.

Since the video-clip is extracted from an acted film, all the conversational moves are pre-defined: this may explain the few turn-gain and turn-hold facial displays. Moreover, no sequencing gestures were identified, probably due again to the scripted dialogue. Given the emotional scene, on the other hand, it is not surprising that most of the feedback phenomena annotated have been labelled as F-Give Emotion/Attitude . The fact that F-Elicit Acceptance was used points to the fact that the category is useful, and that its absence from the Danish data is due to the different communicative situation. On the other hand, in the Swedish clip there are no examples of F-Give Basic , which in spontaneous conversation has been found to be one of the most frequent feedback categories (Cerrato 2007 ). This fact can again be explained by the emotional nature of the interaction in a film scene, which exceeds what is customary of spontaneous conversations in emotionally more neutral contexts. The overrepresentation of Turn end compared to the other two Turn features, on the other hand, may be adduced to psychological treats of the characters involved in the scene. In general, it is clear that the interpretation of the kind of multimodal analysis allowed by the MUMIN coding scheme is extremely complex when such an analysis is applied to staged interaction, and data like those produced in this short case study cannot be used to generalise about communicative interaction at large. 6.3 The Finnish case study The Finnish video was taken from the corpus of the Finnish TV broadcasting company programs, hosted by the Centre for Scientific Computing (CSC). The corpus contains interviews conducted in the Morning TV programs, and the selected 1-minute clip deals with the interview of the (then) Finnish finance minister, who is asked about the budget-cuts, economic growth, and taxation concerning the on-going budget negotiations. Both the interviewer and the interviewee are male, sitting in front of each other in the studio.

The annotation of hand gestures was the main object of this annotation study and testing ground for the coding schemes validity and annotator agreement.
 The clip was annotated by four naive annotators and one expert annotator using ANVIL. After a short introductory course and individual annotations, the naive annotators formed two pairs, and both pairs produced a single annotated file by negotiating about the correct tags at each point they had originally disagreed. At the end of the annotation task, a common session was held to discuss the categories and the annotation task with all the coders.

The statistics below is calculated on the basis of three annotated files: the two pairs and the expert file. The coder pairs annotated 9 interviewee gestures and 4 interviewer gestures, while the expert annotated 8 interviewee gestures and 5 interviewer gestures. The segments were aligned by considering two segments as referring to the same gesture if they covered the same time span, plus or minus 0.3 second at the onset or at the end of the gesture. All segments appeared common to all coders, and the missing ones turned out to be included in a corresponding longer segment by the other annotators, i.e. the missing gesture was considered as part of a longer gesture by one annotator but split into two by the other coders. The annotators used the categories that describe hand gestures: Handedness, Trajectory and SemioticType . Therefore, these are also the focus of the intercoder agreement study.

Table 7 presents results of the percent and k agreement on hand gesture annotation. Coders 1 and 2 refer to the coder pairs whereas coder 3 is the expert coder. It is interesting to note that coders 1 and 3 seem to have fairly strong percentage agreement on the annotations on average, while the naive annotators 1 and 2 disagree considerably.Regarding k -agreement, K = 0 means that agreement is not different from chance, while K = 1 means perfect agreement. To assess the significance of all the intermediate scores, we use the scale proposed by Rietveld and Hout ( 1993 ), where values between 0.21 and 0.40 are considered as indicating fair agreement, values between 0.41 and 0.60 indicate moderate agreement, and values between 0.61 and 0.80 indicate substantial agreement. Values below 0.20 indicate slight agreement whereas values above 0.81 are almost perfect.

Again, coders 1 and 2 seem to have on average only fair level of agreement beyond chance level. Of the individual categories, the annotators disagree on SemioticType and Trajectory , whereas agreement for Handedness is almost perfect. Coders 1 and 3 show moderate agreement on average, and substantial agreement on most individual categories with the exception of SemioticType . Coders 2 and 3, on the contrary, show substantial or high moderate agreement on average on all the gesture annotations. Again, SemioticType is agreed upon only on a fair level, indicating that the concept itself may be problematic, or that the different types are difficult to distinguish from each other in reality. On average, over all categories and coder pairs, the agreement is moderate X  X  positive result concerning the annotation exercise as a whole. 7 Discussion Percent agreement is generally considered too liberal a measure for intercoder agreement, and indeed, the results shown above concerning the Finnish case study show that it gives consistently higher values than the k -scores. However, it is interesting to ponder upon the differences between the two agreement measures, since kappa can often be very low, even negative, while percentage agreement is still quite high. Spooren ( 2004 ) discusses those cases which typically occur when there are only few categories and one of them is preferred by the coders. The problem is that strong preference for one category means that the likelihood that the coders agree on that one category by chance is increased. Consequently, the overall agreement measured by means of k -scores decreases. This seems to be the case with our category SemioticType : the annotators preferred the value index-non-deictic , but did not use consistently the other values ( iconic, symbolic ). The expected probability for the kappa calculations was thus biased towards index-non-deictic .
According to Craggs and McGee Wood ( 2004 ), however, low k -scores may still be useful for theoretical argumentation. They may in fact deal with subtle phenomena, and it is thus understandable that the distribution is uneven or skewed. Indeed, when annotating and applying theoretical concepts as categories, it is important to understand the subjective nature of the phenomena that are being coded, and also to accept the fact that it may not be possible to achieve substantial or nearly perfect agreement scores at all. From this perspective, our k -agreement results represent very good results. In future, we would also experiment with a score that makes chance agreement independent of individual preferences. Craggs and McGee Wood ( 2004 ) suggest as an alternative to kappa Krippendorff X  X  alpha (Krippendorff 1980/2004), which takes care of the biased category distributions discussed above. However, it must be noted that alpha has only rarely been used and this can be a problem when comparing results from different studies.

Another issue worth discussing is the relation between form and function of gestures. In the Finnish clip, most of the gestures are  X  X  X rdinary X  X  indexical non-deictic gestures which have a general emphasis function in the dialogue (batonic gestures), whereas some gestures seem to function both as indexical and iconic ones. For instance, the hand movement in Fig. 2 was categorised by one of the coders as two adjacent gestures, e.g. an indexical non-deictic and an iconic one. The gesture is used by the speaker to emphasise the fact that the planned taxation will bring about a small but significant economic increase.

A similar gesture is used twice by the same speaker in the later interactions, but its iconicity is not so clear in these cases; rather, the gesture looks like a batonic gesture (indexical non-deictic) with the particular complex hand shape just being reminiscent of the earlier gesture in the dialogue, as can be seen in Fig. 3 .
The similarity of the gestures tie the speaker X  X  utterances together and make the speaker X  X  communicative behaviour coherent, but it is a matter of definition whether the gestures are considered instances of the same gesture form with two different functions, or whether form and function go together and the gestures are single instances of two different gesture types. The former position, which is supported by the coarse-grained shape analysis allowed for in the MUMIN coding scheme, allows us to attempt to classify gestures into similar form types and to cross-classify the forms with respect to different functions. We believe this is useful for the purposes of automatic gesture recognition and production. The latter position, on the other hand, requires a more detailed analysis of the gesture shape. 8 Conclusion The purpose of the MUMIN coding scheme is to provide a methodology for the annotation of feedback, turn management and sequencing multimodal behaviour in human communication. The preliminary results of the reliability test run in three different case studies on video material in Danish, Swedish and Finnish confirm the general usefulness of the categories defined for the purpose of coding feedback and turn taking functions. However, some of features concerning the shape as well as the semiotic and sequencing functions of gaze, head, and hand gestures, seemed problematic in some cases, and not enough fine-grained in others. In general, however, despite the difficulties encountered by some of the annotators in using specific features, the average agreement is acceptable. The lack of features concerning body posture, which several annotators had noted at the time of the case studies, has now been remedied by adding relevant features.

Another issue raised by the empirical studies is whether form and function are always tightly connected, in the sense that a specific gesture shape always corresponds to a specific function and vice versa. The stance taken in MUMIN is that of abstracting away from too many details in the annotation of gesture shape, and allow for a cross-classification of coarse-grained shape types with several different functional categories. We believe this choice gives the most flexibility and robustness both if the annotation is done manually and if automatic methods are used to recognise or generate gestures.

We believe the MUMIN scheme constitutes an important step towards creating annotated multimodal resources for the study of multimodal communicative phenomena in different situations and cultural settings, and for investigating many aspects of human communication. Examples of issues that can be investigated empirically by looking at annotated data are: to what extent gestural feedback co-occurs with verbal expressions; in what way different non-verbal feedback gestures can be combined; whether specific gestures are typically associated with a specific function; how multimodal feedback, turn management and sequencing strategies are expressed in different cultural settings.

The annotation proposed in the MUMIN scheme can be combined with systems that automatically recognise movements of body parts, as for example the system described in Bailly et al. ( 2006 ) to provide an interpretation of some of the recorded movements. Furthermore, corpora annotated according to the scheme represent useful resources to guide the generation of feedback, turn management and sequencing gestures of multimodal agents.

The MUMIN coding scheme is available from the MUMIN site at http://www.cst.dk/mumin , together with an XML file containing the ANVIL spec-ifications of the scheme. We encourage anybody interested in using the scheme to download the files and send us their comments. It will help us assess our work and further develop the framework.
 References
