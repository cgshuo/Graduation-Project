 on work towards the sample compressibility conjecture of [7] e.g. in [5]. inclusion strate gy from d=n to d D d tion 5. Where best bound on expected risk for k 2 N to-date is O ( log ) for = learners [8, 1, 2, 4]. For lar ge n this is O (log n prediction.
 bolded typef ace as in C;x; v . We define n 2.1 The prediction model of lear ning X . For notational con venience we write sam ( x ;f ) = (( x the probability of f Q (sam ( X of performance is the wor st-case expected risk A mistak e bound for Q with respect to F is an upper -bound on ^ M f given an f -labeled sample, but instead in predicting f ( X ^ 2.2 The capacity of function classes contained in f 0 ;:::;k g X We denote by Lemma 2.4 (Sauer X  s Lemma [9]) For any n 2 N and V f 0 ; 1 g n , j V j n meeting this with equality is called maximum .
 by F if ther e exists a 2 n suc h that f 0 ; 1 g n ( -dim ( F ) = sup f n j9 x 2X n ; 2 n s.t. f 0 ; 1 g n ( x ( F )) g . We next describe three important translation families used in this paper . Example 2.6 The families
N = f N;i;j : i;j 2 f 0 ;:::;k g ;i 6 = j g 2.3 The one-inclusion prediction strategy hamming-1 separ ated vertices.

Gi ven: F f 0 ;:::;k g X , sam (( x
Retur ns: a prediction of f ( x V x ( F ) ;
G G ( V ) ; ! G orient G to minimize the maximum outde gree ;
V space f v 2 V j v 1 = f ( x 1 ) ;:::;v n 1 = f ( x n 1 ) g ; if V space = f v g then retur n v else retur n the n th component of the head of hyperedge V space in ! G ; The one-inclusion graph X  s prediction strate gy Q Theor em 2.8 (Theor em 2.3 [4]) ^ M of Q and let S to decreased from 1 to 0 . The entire family V can be shifted to S shifted verte x-set induces S s 2 I . If V is [ n ] -closed-below then we call it closed-belo w . A number of properties of shifting follo w relati vely easily: finite sequence s below . In particular VC( V Pr oof: a of W Kuzmin and Warmuth [5] introduced D d classes. We begin with properties of D d Definition 4.1 Define D d d closed-below subset of f 0 ; 1 g n equal to the union of all n Lemma 4.2 D d (i) equals the graph density of V d (ii) is strictly upper -bounded by d , for all n ; (iv) is strictly monotonic incr easing in d (with n fixed); (v) is strictly monotonic incr easing in n (with d fixed); and (vi) limits to d as n !1 .
 Pr oof: By counting, for each d n &lt; 1 , the density of G V d pro ving (i). Since for all A;B;C;D &gt; 0 , A Pr oof: If G ( U \ V ) has density less than then For n 2 N and d 2 [ n ] , V d Pr oof: Allo w a permutation g 2 S that a closed-belo w VC-dimension d family V f 0 ; 1 g n satisfies S For the purposes of contradiction assume that V (i.e. a closed-belo w family at least as small and dense as V Thus dens G V not have been selected (i.e. a distinct family at least as dense as V and so V with dens ( G ( V )) = D d D d n =n and ^ M Q For small d , n ( d ) = min n d j d = D d only at i and no w 2 V exists such that u Let S f S s ( u ; V ) ;S s ( v ; V ) g 2 S s ( E 0 ) same number of labels then the y remain connected in S S shift down to S w s &lt; v s f S s ( w ; V ) ;S s ( u ; V ) g Suppose that I [ n ] is to but equal to 0 at s , these are the remaining half of the witnesses of V  X  X  F will be reached. If I [ n ] is is closed-belo w the translation vector ( upper -adjoining vertices we have pro ved that Theor em 5.2 Consider any k;n 2 N and F f 0 ;:::;k g X with class one-inclusion prediction str ate gy satisfies ^ M 5.1 A lower bound noting that X and f all n &gt; for which prediction under a distrib ution P supported by a sider d = then a subset F some i 2 f 0 ;:::;k g on x E ^ M mistak e bound from d=n to D d Peeling compression scheme is kno wn to be false [8]. The symmetrization method of Theorem 4.4 can be extended over subgroups G S density bounds. Just as the S open as to whether this generalizes to n 2 N . While a general fact incomparable X  X either Ackno wledgments We gratefully ackno wledge the support of the NSF under award DMS-0434383. Refer ences
