 Graph-based approaches for multi-document summarization have been widely used to extract top sentences for a sum-mary. Traditionally, the documents X  cluster is modeled as a graph of the cluster X  X  sentences only which might limit the ability of recognizing topically discriminative sentences in re-gard to other clusters. In this paper, we propose StarSum star bipartite graph which models sentences and their topic signature phrases. The approach ensures sentence similarity and content importance from the graph structure. We ex-tract sentences in an approach that guarantees diversity and coverage which are crucial for multi-document summariza-tion. Regardless of the simplicity of the approach in rank-ing, a DUC experiment shows the effectiveness of StarSum compared to different baselines.
 I.2.7 [ ARTIFICIAL INTELLIGENCE ]: Natural Lan-guage Processing X  Text analysis Text summarization; Text graphs; Multi-document summa-rization; Topic signatures
Multi-document text summarization is the process of au-tomatically producing a concise summary version represent-ing a group of related documents. Graph-based text summa-rization algorithms have been widely used for multi-document summarization where sentences are modeled as vertices and their similarities or content overlap are modeled as edges. After constructing the graph, an eigenvector centrality mea-sure, as in PageRank[2], is used to rank the vertices. No-table algorithms as in TextRank[12] and LexRank[5] have shown tremendous success in text summarization. However, it is not clear if they capture important characteristics for multi-document summarization as in coverage and diversity. c  X  Figure 1: StarSum example: S represents a sentence and ph ts represents topic signature phrase(bigram) Moreover, when constructing the graph edges, all words are used to determine the sentences similarity which might over-estimate the importance of longer sentences. Therefore, we propose an approach that addresses such limitations while maintaining simplicity.

In this paper, we propose a new graph-based summarizer that combines topic signature phrases with sentences in a star bipartite graph, called StarSum . Sentences are mod-eled as central nodes while their topic signature phrases are modeled around the sentences. This way we can rank sen-tences not only based on their relations to other sentences but also to how many topic signature phrases do they cover. For example, assume a sentence S i has a relation to three other sentences. It is not clear if the traditional homoge-neous graphs differentiate between whether S i shares a small or large set of words with all three sentences regardless of the edge weight. Imagine for instance that S i shares { w 1 ,...,w with all three sentences, which means an edge weight of 4 between all sentences with S i . Now, if S i shares different sets of words with all three sentences each of which is of 4 words, it would mean that the sentence cover more infor-mation and would probably be more suitable for a summary regardless of the edge weights. It is clear that if the sentence shares more words, then it should be ranked higher than a sentence with fewer important words.

By explicitly representing words in the graph, we over-come the aforementioned problem. Moreover, by only con-sidering topic signatures we have a less condensed graph which makes clustering the graph to components easier and ensures diversity. We show that by using a simple degree centrality of sentences with their topic signature phrases and neighboring sentences we can easily ensure coverages. The experimental work shows the effectiveness of our approach regardless of its simplicity compared to eigenvector central-ity measures.
The proposed approach StarSum combines topic signa-tures bigrams with sentences in a star graph for multi-document summarization. By using topical bigrams, we ensure the se-lection of sentences that are topical in respect to different clusters of documents. We start by describing the extraction of topic signatures; then we explain the proposed StarSum graph-based approach.
Topic signatures[9] are unigrams, or bigrams in our case, that describe the topics of a giving document compared to a background corpus. These descriptive bigrams are found by using the log-likelihood ratios log  X  of two different hy-potheses. We have the input document cluster as D I , and the background corpus as D B which is all the other clus-ters. To calculate the log-likelihood ratio of a given bigram log  X  ( ph ) 3 ph = w i  X  1 w i , we form two different hypotheses. The first one is H 1 which assumes to have the same prob-ability of occurrence of ph in both D I and D B giving us P ( ph | D I ) = P ( ph | D B ) , and is considered the null hypothe-sis. The second hypothesis, H 2 , which assumes that ph has higher probability of occurrence in the input D I compared to D
B giving us P ( ph | D I ) &gt; P ( ph | D B ) which indicates that ph is descriptive. The log-likelihood ratio is calculated as mial distribution from the Bernoulli trials of sequences of bigrams.

To find topic signature bigrams, we use  X  2 log  X  to test the statistical significance of their occurrences, which is asymp-totically approximated to the  X  2 distribution. We classify bi-grams to be descriptive if their R ( ph ) = 1 iff(  X  2 log  X  ( ph ) &gt; 10 . 83) , 0 which is equal to confidence level of 0 . 001. All the significant bigrams are stored for each input cluster sepa-rately as D I = { ph ts 1 ,...,ph ts n | R ( ph ts ) = 1 } .
After explaining the topic signature extraction, we move on to describe our proposed approach StarSum , which mod-els sentences with their topic signature bigrams in a bipar-tite graph. By modeling the relations between sentences and their topical phrases in a graph, we ensure that saliency scores of sentences depend on the topicality of their phrases as well as their connectivity to other sentences. Our ap-proach is designed to cover two main properties of sum-maries in a multi-document settings: diversity and cover-age. First, we describe our approach to ensure diversity of main topics. Second, we describe our ranking approach that ensures coverage.

Before we describe our approach to ensure diversity in summaries, let us define the StarSum graph. Let G ( V 1 V ,E ) be a bipartite star graph where V 1  X  V 2 is a set of fi-nite vertices, and E is a set of finite edges connecting V V . The set of sentences is denoted as V 1 = { S 1 ,...,S n the set of topic signature bigrams as V 2 = { ph ts 1 ,...,ph We are interested on ranking V 1 only. To have a diversi-fied ranking of sentences, traditional approaches use clus-tering algorithms which might be computationally intensive for well connected graphs. However, since we only model sentences with their topic signatures as opposed to all words or phrases, the graph G is loosely connected. We can sim-ply decompose the graph G to different graph components G = { G 1 ,...,G n } that loosely represent different topics of the document cluster. We greedily choose the top ranked sentence from each graph component G i starting with the largest component in number of vertices to the smallest. Therefore, we have a diversified list of sentences that cover the main topics of the document cluster. Next, we explain the ranking approach for sentences and how we measure the saliency scores for them.

Our saliency scores of sentences are measured in a way to ensure coverage of the topic within a given graph com-ponent G i . The rank of sentences in a graph component is measured by the degree centrality of the sentence ver-tices, which chooses the sentence that covers the topic well. However, instead of only using the degree centrality, which is the number of topic signature bigrams in the sentence, we use both the first-order degree which is the number of phrases, and the second-order which are the number of other sentences that share topic signature phrases. Let us define N ( v,d ) to be the neighborhood set of vertices for vertex v of order d . For example, N ( v, 1) is only the set of adjacent ver-tices to v . Therefore, the rank of the sentences is measured as: Where | N ( v, 2) | is the total number of vertices in the neigh-borhood set which contains topic signature phrases and sen-tences that share these phrases with sentence v , we subtract one since we do not want to count v , and length ( v ) is the total number of bigrams in sentence v for penalizing long sentences. The final summary is then constructed as:
We keep picking top sentences from each graph compo-nent, starting from the largest, until we reach the summary length. If we cover all the graph components but have not reached the summary length, we start over in a round robin fashion.

The intuition behind the StarSum approach is that as-sessing sentences X  appropriateness for summaries should not only be about sentence similarity, as in number of common words or cosine similarity. Instead, it should be about both containing topical phrases and sharing them with other sen-tences. Therefore, modeling the topic signature phrases in the graph is important to ensure rewarding sentences that are more topically informative. Additionally, despite its sim-plicity, StarSum is an effective approach for guaranteeing diversity and coverage compared to other eigenvector ap-proaches that uses a random walk.
To evaluate how our proposed approach compares to other baselines, we use Task 2 of the Document Understanding Conference dataset (DUC 2001) which is designated for multi-document summarization. The DUC is an English news benchmark for text summarization. The dataset is clus-tered to 30 news topics where each cluster contains roughly 10 documents 1 . Each cluster has 3 different human writ-ten summaries that are considered as the golden summaries we used the 30 topics in the test set only to compare against all multi-document summarization al-gorithms 2 . All the baseline algorithms are set to produce a 100 word summary, which will be compared against the 100 word summary of the gold set. The topic signatures are calculated by assigning D I to the current cluster, and the D B to the other 29 clusters. For the preprocessing, we remove punctuations, lowercase all characters, and stem the text with Porter stemmer.

We use the well-known recall metric ROUGE [10] for eval-uating the automated summaries. The metric measures the overlap between the generated summaries and the human summaries. We use 5 different options for ROUGE which depends on the size of the textual unit. ROUGE-1 is the unigram measure which shows the most agreement with hu-man judges[10]. Additionally, we use the bigram measure ROUGE-2, the trigram measure ROUGE-3, ROUGE-4, and ROUGE-L for the longest common subsequence of words. We use a number of different baselines to test our Star-Sum approach validity through ROUGE[10]. The baselines are common graph-based approaches that generally use an eigenvector centrality algorithms for ranking vertices and extracting the top sentences. The baselines are chosen to directly examine whether our simple approach for ensuring coverage and diversity is effective. The baselines used for comparison are described as follows:
The total is actually 29 clusters. Given that the organizers made a mistake in cluster d31 X  X  summaries, it was excluded from our experiment
The experimental results in Table 1 shows that regardless of
StarSum  X  X  simplicity, it still outperforms computation-ally intensive approaches. Additionally, it slightly outper-forms a state-of-the-art approach that uses random walks with topic signatures on a hypergraph. The improvement over baselines is noticed in all different options of ROUGE which shows the validity of the approach. Moreover, the us-age of topic signature bigrams has outperformed using uni-grams topic signatures in the StarSum approach in Table 2. This shows that informative bigrams are more important in highlighting salient sentences compared to unigrams in the StarSum algorithm.

To understand why StarSum works well despite its sim-ple algorithmic design, we show how the intuition behind it is slightly different than eigenvector centrality approaches. StarSum is tailored toward two goals: coverage and diver-sity, where the random walk approaches focus on the prestige of vertices. It is not clear if prestige ensures either diversity or coverage. Additionally, modeling the relation between topic signature bigrams and sentences ensures that we pre-fer sentences that both contain many topical bigrams and share many connections with other sentences.

The efficiency of the StarSum ranking comes from the fact that it is simply a degree centrality approach. To get the rank of sentence v , the algorithm only needs to traverse the local neighborhood of that sentence. This means that for a single node v , the running time will be O ( | V n | + | E is linear in v  X  X  local neighborhood n . In algorithms that use PageRank, the ranking of a node is dependent on the entire graph structure and number of iterations until convergence. Additionally, our approach avoids doing any sentence pair similarity calculation as in most of the baseline approaches which is of quadratic time.
Summarizing a cluster of documents through graphs has been extensively researched. The most notable work is Tex-tRank[12] and LexRank[5]. In TextRank[12], the edge weights are defined using the words overlap normalized by the length of the two sentences. A weighted PageRank is then used to rank the sentences. Similarly in LexRank[5], a PageRank approach is used to rank vertices but the edge weights are based on cosine similarity between the two sentences. To en-sure diversity, some enhanced approaches cluster the graph first as in C-LexRank[13], or enhance the random walk pro-cess as in DivRank[11].
Other approaches model both sentences and words in a bipartite graph fashion and approach the task as mutual re-inforcement between keywords and sentences[14, 16]. How-ever, they are different approaches than StarSum since we are using the log likelihood to identify important phrases a priori to enhance sentence selection. Some new approaches have relied on the log likelihood of word association in sen-tences[7] which shows their importance in summarization. Additionally, Yih et al. [15] proposed an approach to find sentences that maximize the coverage of informative words. They recognized informative words by frequency and posi-tion of words. Similarly, Gillick and Favre[6] proposed to use Integer Linear Programming to find sentences that cover im-portant bigrams based on their document frequency. Both approach do not use discriminative weighting approach as in topic signature. A recent survey on text summarization comparing baselines with state-of-the-art approaches can be found in[8].
In this work, we proposed a simple, yet effective, star graph that combines topic signature bigrams and sentences. We ensure diversity by decomposing the StarSum graph into different components and picking top sentences from each different component, and we ensure coverage by rank-ing sentences through their degree connection to other topic phrases and sentences. The experiment showed the effective-ness of our approach by outperforming all other baselines. The results highlighted the shortcoming of using an eigen-vector approach on graphs which ranks sentences based on their prestige as opposed to diversity and coverage. More-over, the simplicity of the StarSum should be appealing to systems that needs to scale to large collection of documents as in the modern Information Retrieval systems.

In the future, we plan to experiment with more datasets and different summary length. Additionally, we will test our approach in related summarization tasks as query-focused summarization. Also, we will experiment with different setup for calculating the topic signature statistics and examine whether it changes the accuracy of summarization. For ex-ample, studying the effect of changing the statistical signifi-cance level for finding topic signatures. Moreover, we plan to test using a larger background dataset for finding topic sig-nature phrases. Additionally, testing the use of trigrams, in addition to bigrams, and whether they improve the quality of the summaries is in our future plans. [1] A. Bellaachia and M. Al-Dhelaan. Multi-document [2] S. Brin and L. Page. The anatomy of a large-scale [3] J. Carbonell and J. Goldstein. The use of mmr, [4] J. M. Conroy, J. D. Schlesinger, and D. P. O X  X eary. [5] G. Erkan and D. R. Radev. Lexrank: graph-based [6] D. Gillick and B. Favre. A scalable global model for [7] O. Gross, A. Doucet, and H. Toivonen. Document [8] K. Hong, J. Conroy, B. Favre, A. Kulesza, H. Lin, and [9] C.-Y. Lin and E. Hovy. The automated acquisition of [10] C.-Y. Lin and E. Hovy. Automatic evaluation of [11] Q. Mei, J. Guo, and D. Radev. Divrank: The [12] R. Mihalcea and P. Tarau. Textrank: Bringing order [13] V. Qazvinian and D. R. Radev. Scientific paper [14] X. Wan, J. Yang, and J. Xiao. Towards an iterative [15] W.-t. Yih, J. Goodman, L. Vanderwende, and [16] H. Zha. Generic summarization and keyphrase [17] B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan,
