 For the task of near-duplicated document detection, both traditional fingerprinting techniqu es used in database community and bag-of-word comparison approaches used in information retrieval community are not sufficiently accurate. This is due to the fact that the characteristics of near-duplicated documents are different from that of both  X  X lmo st-identical X  documents in the data cleaning task and  X  X elevant X  documents in the search task. This paper presents an instan ce-level constrained clustering approach for near-duplicate detection. The framework incorporates information such as document attributes and content structure into the clustering process to form near-duplicate clusters. Gathered from several collections of public comments sent to U.S. government agenci es on proposed new regulations, the experimental results demonstrate that our approach outperforms other near-duplicate detection algorithms and as about as effective as human assessors. H.3.3 [ Information Search and Retrieval ] Clustering Algorithms, Performance, Experimentation. Duplicate detection, clus tering, public comments Near-duplicate detection is the task to identify and organize documents that are  X  X early identical X  to each other. In another word, near-duplicates originated from the same reference copy. Early research on duplicate de tection was done mainly for  X  X lmost-identical X  documents and mostly in the areas of databases, digital libraries, el ectronic publishing and web search tasks [1][3][4][5][7][9][10][12]. Many algorithms consider near-duplicate detection as a  X  X etrieval X  problem, i.e., to find the near duplicates for a single document. However, just looking for near-dup licates for a document neither fulfills the needs of organizing a large collection of documents nor that of identifying the original reference copy for near-duplicates. Moreover, one important characteris tic of near-duplicate detection is that two documents may be cons idered near-duplicates even if they only share a relatively sma ll amount of text. Solely relying on bag-of-word similarity measures , even some sophisticated ones carefully calibrated to the near-duplicate detection problem, are brittle under such situation. Additional knowledge about the documents is sometimes the key to tackle the problem. A more flexible framework is henc e desirable to make use of the additional information available in the document collection to organize the documents into n ear-duplicate clusters with the original reference copy recognize d. In this work, near-duplicate detection is considered as an instance-level constrained clustering problem. Instance-level constr ained clustering is a semi-supervised process which provi des a flexible framework to incorporate constraints on documen t attributes, content structure, and self-defined preferences to guide through the clustering process. We believe that our work is the first such semi-supervised clustering approach developed for the general near-duplicate detection task. A key component of this semi-s upervised clustering algorithm is the use of instance-level constraints that are based on document attributes and editing styles of near-duplicates. Three types of instance-level clus tering constraints 1 are used in our system: must-link (believed to be in the same class), cannot-link (believed to be in different classes) and family-link (possibly in the same class) constraints. Note that the instance-level constraints used in semi-supervised clustering are not the same as labeled data used in classification or partial-labeled data in semi-supervised classification, they are pair-wise constraints which are not sufficient to general class labels. Though they are not as strong conditions as class labels, they pr ovide a valuable guidance for the conventional unsupervised clus tering process. Moreover, instance-level pair-wise constraints are much easier to generate than class labels and the appro ach is still largely unsupervised. A newly-emerging domain for nea r-duplicate detection is notice and comment rulemaking[13], in wh ich U.S. regulatory agencies receive comments about proposed regulations from the general public. Many of the comments are  X  X orm letters X  ( exact duplicates ) and modified copies of form letters ( near duplicates ). U.S. law requires agencies to respond to every substantive issue raised by the public, even if the i ssue is inserted into a form letter. Spotting exact-duplicates is relati vely easy, but identifying near-duplicates and their unique compone nt(s) is a more challenging task that is currently done manua lly. When a proposed regulation attracts a large amount of public interests, an agency may receive hundreds of thousands of comments th at must be processed within Must-link and cannot-link concepts were first introduced in [14]. a few weeks or months, typically requiring significant manual effort and expense. Near-duplicate detection automates the process and largely saves agencies X  efforts. Our system, DURIAN ( DU plicate R emoval I n l A rge collectio N s), is evaluated in experiments with public comments recently collected for U.S. Environmental Protection Agency (EPA) and Department of Transporta tion (DOT) rulemakings. The experimental results show that system-to-human intercoder agreement is comparable to human-to-human intercoder agreement. They also demonstrate that incorporating instance-level constraints boost the accuracy and efficiency for clustering tasks. It shows that our approach is a more promising framework than unsupervised clustering without any constraint. Furthermore, although evaluated in the public comment domain, the technique for nea r-duplicate detection is domain-independent; it could also be applied to n ear-duplicate detection in other domains such as question answer ing, web search, information flow, and plagiarism detection. The rest of the paper is organize d as follows. Section 2 defines the problem. Section 3 reviews relate d research. Section 4 describes the algorithm. Section 5 disc usses evaluation methodology. Section 6 presents experimental results. Section 7 concludes. Near-duplicate detec tion is different from other Information Retrieval (IR) tasks in how it defines what it means for two documents to be  X  X imilar X . In many IR tasks document similarity refers to semantic  X  X elevance  X  among documents, which are could be syntactically very different but still relevant. In contrast, the definition of similarity in duplic ate detection in early database research [1][3][12] is very conser vative, which is mainly to find syntactically  X  X lmost-identical X  documents. As pointed out by Metzler et al.[10], for other task s that need to detect documents with  X  X ntermediate level of similarity X , there has not been much research done. Near-duplicate de tection in notice and public comment rulemaking is one such task and our work is designed to be general enough to apply to ot her detection tasks for documents with  X  X ntermediate level of similarity X . In notice and public comments ru lemaking domain there are two broad categories of documents: co mments that are written more-or-less from scratch (they contai n unique insights and opinions, derivative opinions, spam or viruses), and comments that are written based on a form letter. The former will produce singleton clusters and the later will form near-duplicate clusters. We define two documents to be near-duplicates if they are from the same origin. In particular, several sub categories of nea r-duplicates are defined based on common editing styles:  X  Block Added: Add one or more paragraphs (&lt;200 words) to a  X  Block Deleted : Remove one or mo re paragraphs (&lt;200  X  Key Block : Contains at least one paragraph from a document;  X  Minor Change : A few words altered within a paragraph  X  Minor Change &amp; Block Edit : A combination of minor change  X  Block Reordering : Reorder the same set of paragraphs;  X  Repeated : Repeat the entire document several times in  X  Bag-of-word similar : &gt;80% word overlap (not in above  X  Exact : 100% word overlap. As we mentioned earlier, in the task of near-duplicate detection, even if two documents share a small amount of text, they may still be considered as near duplicates. Solely relying on similarity measures, even some sophisticated ones carefully fitted to the near-duplicate detection problem, for instance, relevant frequency [7], statistical translation-model-based measure[10], and Kullback-Leibler (KL) divergence[16], becomes fragile. The example given in Figure 1 shows that two documents clearly from the same origin are given very di fferent similarity scores (0.30 and 0.05 respectively) by one of state-of-the-art similarity measure, Dirichlet-smoothed KL di vergence, which is used in our baseline unsupervised algorithm as the distance metric (See Section 4.2 for details). It is undesirable that documents which should be grouped together have di ssimilar scores. It is also hard to determine what the correct threshold is for a particular cluster if the similarity measure is not able to represent the true grouping. On the other hand, sometimes the similarity score suggests that they should be grouped together however they should not be, for instance, some other documents wh ich have very close scores but are actually submitted for different proposed rules, i.e., different topics. Therefore, just employi ng a single distance metric in unsupervised algorithm prevents us revealing the near-duplicates in both situations. This paper studies how to employ additional knowledge to tackle the above problem and boost the near-duplicate detection accuracy. In particular, the tasks include:  X  To identify where a particular near-duplicate first originated;  X  To achieve highly effective near-duplicate detection by The problem of finding near-duplicate documents has been a subject of research in the database and web-search communities for some years. The applications range from plagiarism detection in web publishing to redundancy det ection in large datasets. The common duplicate detection techniques are classified into two categories: Fingerprint-based and Fulltext-based. A fingerprint of a document is a set of integers, each of which is the hash value for a substring extracted from the document. In this paper, to be clearer in concept, the term  X  X ingerprint X  refers only to document-level fingerprint while the term  X  X nteger X  or  X  X ash-value X  refers to hash function ou tput, which in many other paper is sometimes also called  X  X ingerprint X . Each integer is stored in an index for fast access during query process. Similarity between two documents is measured by c ounting the number of common integers. Algorithms are different in their choices of hash functions, substring size, subs tring number, and substring selection strategy. Hashing functions is used to generate ha sh values for substrings. Popular hash functions include NIST X  X  SHA1 [11] and Rabin [3]. However, many other hash functions are qualified for this task as long as they are reproducible and with a low rate of hash collision. Substring size is defined by the length of each substrings extracted from a document. Larger size increases the chance of false negatives in duplicate detec tion while smaller size increases that of false positives, e.g., SCAM [1] used a very small substring, word, as the unit for fingerprinting. Prior research suggested that substrings of 3-5 words are good[7]. Substring number is the number of substrings extracted from a document to build a fingerprint. Some techniques used a fixed number of substrings for efficiency , e.g., I-Match presented in [4], while many others used a variab le number of substrings for a more accurate representation of the document, e.g., DSC presented in [3]. A smaller number of substrings have the risk of ignoring short documents and increasing false negatives. Substring selection strategy is the way to pick which substrings to hash. It can be categorized as position-based, hash-value-based, anchor-based and frequency-based strategies . Position-based strategy selects substrings based on their offsets in a document, a sentence or a paragraph. It includes full fingerprinting [1][3][7], non-overlapping fingerprinting [1], and overlapping fingerprinting [3]. It is popular due to the simplicity. Hash-value-based strategy is also popular. The famous shingling approach (or DSC), proposed by Broder, et al. [3] pick s substrings whose hash values are multiples of an integer. Anchor-based strategy extracts substrings that start with special words or character sequences. It was shown to be one of the best substring selection methods[7], however this approach has to be manually tuned to fit a specific collection and hence is not that practical. Frequency-based strategy selects substr ings based on their frequency of occurrences in the document, the entire collection [4][5] and/or external collections[9]. Term frequency with in a document (tf) and inverse document frequency in a collection (idf) are used to select the substrings. Conrad et al. [5] used 30-60 highest idf words. Chowdhury, et al. X  X  I-Match [4] also selected terms with high idf. However term selection based on idf alone can be overly sensitive to small changes in document content and hence the false negative is high. The more recent extended I-Match [9] used external collection statistics to select the lexicon. It achieved a better recall by introducing multiple fingerprints. However, it is computationally expensive. The simplest full-text approach is to adapt methods originally developed for search engines, for example, vector-space model, which treats a document as bag-of-words, with term weights determined by tf.idf values, and similarity determined by cosine similarity. Traditional cosine-similarity measure focuses on finding a semantic relevant document while near-duplicate detection focuses more on syntactic similarity. Several previous works thus have been done in fi nding suitable similarity measures to address syntactic similarity among documents. The identity measure proposed by [7] emphasi zes that the gap between rare words X  term frequency in two do cuments should be smaller than that between common words X  and their best ranking is giving by a term weighting function biased towa rds rare terms. Metzler et al. [10] used statistical translation models to estimate the probability that one sentence in a document is a translation of another sentence in another document. The probability of aligning to a absent term is estimated by th e background language model. The translation probability serves as the basis of the sentence-level and the document-level similarity scores [10]. Clustering is a very useful tool fo r data analysis, especially in the initial process of a large dataset analysis where the data labeling is impractical or unavailable. Howeve r, the usefulness of clustering is questioned by researchers because of its  X  X lindness X  rooted in its nature of  X  X o supervision X . Mo reover, it is always hard to evaluate the performance of a clustering algorithm since there could be many ways of defining what a  X  X ood X  set of clusters should be and hence there is no de finite gold standard to compare with. The semi-supervised clusteri ng approach offers a promising way to incorporate additional knowledge as some kind of supervision.[8][14][15]. The ad ditional knowledge in documents is valuable guidance whic h could greatly improve the performance of clustering process. Some algorithms focus on how to incorporate additional knowledge to modify distance metrics, for example, the Jensen-Shannon divergence updated with gradient descent algorithm, or the Euclidean distance modified by a shortest-path algorithm[8]. Others focus on how to introduce conditions to constrain the clus tering process on the fly, for example, initializing clusters using constraints in k-means algorithm, or controlling clus tering assignments based on constraints[14]. Our focus is on public comments that are submitted through email and Web forms; these represen t the majority of comments submitted for high-profile regulations in recent years. In order to get useful additional knowl edge about the documents automatically, DURIAN employs an information extraction (IE) module. Simple, rule-based heuris tics enable it to identify document attributes (metadata), such as the comment sender/author, the receiver, timest amp, address block, signature block, salutation, docket identi fication number, topic, relayer (email-relaying organizations) and footer block (email signatures attached by email service providers). The documents are analyzed and represented in XML format with extracted metadata as well as the main text. This preprocessing not only normalizes the representation by separating the header, salutation, footer and signature lines from the main text, it also provides ready-to-use additional knowledge about the documents to derive the instance-level constraints automatically. For any clustering algorithm it is important to define a distance metric. Insertion and deletion of a block of text to/from a form letter are common in public comment s, hence it is not surprising that a form letter and its n ear-duplicates have unmatched vocabularies. In [10], the authors used a statistical translation model to get document similarity and claimed that their measure for the detection of documents with  X  X ntermediate level of similarity X  is the best among the current technologies. The statistical translation model turns out to be equivalent to the Dirichlet-smoothed KL-divergence (S ee [10] for more details). In our work, Dirichlet-smoothed KL-d ivergence is employed as the distance metric in the baseline unsupervised clustering algorithm. When comparing the similarity of two documents, the problem of unmatched vocabulary, which is very common in near-duplicate detection, needs to be handled. Instead of assigning zero weights to an absent word, smoothing techniques give the word a probability proportional to its overall probability in a background language model. For any two documents d a and d probability distributions p a and p b respectively, the KL divergence between them is: Here ) ( w p a is the probability of word w occurring in document d , similar for ) ( w p b . The first term in Equation 1 can be dropped since it does not depend on distribution p b and hence is irrelevant for ranking p b . The KL divergence becomes: where  X  is a coefficient provided for each unseen word. Note that all of the probabilities should sum to one, ) | ( b s d w p is the smoothed probability of a word present in the document, and ) | ( C w p is the background language model. Due to space limitation, the derivation is skipped a bit however it can be shown that with such a smoothing method, the KL divergence becomes: Note that the scoring is now based on a sum over all the terms that occur in both documents, i.e., all  X  X atched X  terms. By Dirichlet prior smoothing [17], we have: d d + =  X   X  , (6) Where ) , ( b d w tf is the term frequency of word w in document b d , | | b d is the length of document b d ,  X  in Dirichlet smoothing and is set to 1 in this work. The background language model ) | ( C w p is estimated by maximum likelihood estimation (MLE): For ) ( w p a , there are many ways to estimate it. If MLE is chosen and document a d is used as evidence, ) ( w p a as: By putting Equations (5)(6)(7)(8) into (4), we can see that the KL-divergence scoring formula is e ssentially the same as the query likelihood retrieval formula in [ 17], and also equivalent to the statistical translation model presented in [10]. Since KL-divergence is non-negative and non-symmetric, we define and use the minimum value of two KL-divergences as the distance metric between documents d a and d b : The smoothed KL divergence is t hus the distance metric used in the unsupervised clustering algorithm, which is the baseline in our experiments in section 6. One of the tasks of near-duplicate detection is to find the reference copy from which a set of documents derived. DURIAN employs a simple but effective redundancy-based reference copy detection strategy. The assumption is that form letters are submitted in both unmodified and modified forms. Any document that has many exact duplic ates is considered an instance of a form letter 2 . In practice, when comment volume is high, a significant fraction of the comments is exac t duplicates of form letters, making them relatively easy to find. This fact makes our approach practical. To identity exact duplicates, all words in a document are converted into a single document string , which is a long string of characters with all the words in the document concatenated together (white spaces and punctuations removed). After that, a hash function is applied to the document string to create a unique identifier for this particular document. The hash function used in DURIAN is the NIST X  X  security hash function, SHA1 [11]. It makes sure that the chance of hash value collision is as low as p ( 160 2 ) . It is also designed to be very fast and is good for strings of any length. After hashing, a &lt;has h-value, document id&gt; tuple is created for each document and all the tuples are sorted by their hash values. Documents resulting in the same hash value are neighbors since they are sorted. A simple linear scan to the sorted list is performed to identify documents with same hash values, which are considered exact duplicates. Given a set of exact duplicates whose size is bigger than m (m=5 in our system), the document with the earliest timestamp is selected to be the reference copy. DURIAN also annot ates the reference copy with the number of exact duplicates in the set, and retains the information for further study. Instance-level constraints are generated and incorporated in the clustering process for near-duplica te detection. In many cases, public comments share the same metadata, such as email relayer, Documents with just a few exact duplicates are usually comments that a person accident ally submitted several times. approximate file size, approximate date, and docket identification number, are likely to be near-duplicates. These attributes can be used to create instance-level constraints that indicate that certain pairs of documents must be, cannot be, or are likely to be in the same duplicate cluster. In this work, two hard constraints, must-link and cannot-link , are used, and a new type of instance-level constraint, family-link , is introduced. Each type of constraints is described in more detail below. Must-links are pair-wise constraints specifying that two documents must occur in the same cluster. They are strong connections of two documents. The must-link conditions in near-duplicate detection include the complete containment of the reference copy ( key block ), and word overlap &gt; 95% ( minor change ). By using the must-links, pairs of documents satisfying the conditions are forced to be in the same cluster even if their KL divergence is high, i.e., even if they are dissimilar in bag-of-word comparison. Cannot-links are pair-wise cons traints specifying that two documents must go to different cl usters, i.e., cannot be in the same cluster. They are strong exclusions between two documents. The cannot-link condition in near-duplicate detection only occurs when two documents cite different docket identification numbers in their texts; this happens more frequently than one might expect because people often use the wrong email address or Web form when submitting comments. Family-link constraints are pair-wise constraints specifying that two documents are likely to be in the same cluster. In near-duplicate detection a family-link is created when two documents have the same email relayer, the same docket identification number, similar file sizes, and the same footer block. As mentioned above, an initial se t of must-links, cannot-links and family-links are created between pairs of documents, based upon document attributes, content structure and preferences, A more complete set of constraints is obtained by taking the transitive closures of these three types of instance-level constraints. For any random documents a d , b d and c d the transitive closures of the three types of constraints are shown below. Must-link transitive closure: d = m b d , b d = m c d =&gt; a d = m c d Cannot-link transitive closure: d = c b d , b d = m c d =&gt; a d = c c d Family-link transitive closure: d = f b d , b d = m c d =&gt; a d = f c d ; a d = f b d , d = f b d , b d = f c d =&gt; a d = f c d . = , = c and = f indicate must-link, ca nnot-link and family-link respectively. Transitive closure provides a larger and more complete set of instance-level constraints. For example, initially there are only three family-links among six documents in Figure 2 (I). Assume three of them (a, a X , a X  X ) are derived from the reference copy a, the other three (b, b X , b X  X ) from b. By introducing one cannot-link between two reference copies, a nd two must-links between the reference copies and two edited c opies in Figure 2 (II), many other constraints can be derive d(Figure 2 (III)). Note that once a cannot-link is assigned, some prev ious family-links are removed and changed to cannot-link since cannot-link is a stronger condition. As mentioned earlier, any comment that has more than 5 exact duplicates (after lexical preprocessing) is considered an instance of a form letter. The copy with the earliest timestamp is the reference copy of the form letter. All reference copies are put into a set called ReferenceCopies . Some documents that are not reference copies are put into another set called Non-referenceCopies , which may be also become seeds later in the clustering process. The two sets of documents are inputs to the clustering algorithm detailed in Figure 3. Clustering (ReferenceCopies R , Non-referenceCopies NR ) { A) Initialize the Duplicate Cluster Collection N : N  X  X  X  B) Pick one document d i from R, (See Section 4.3.) C) Retrieve candidate set S i for d i .  X  document s a) if (s ij , d i )  X  Must , /* if must-link constraint holds*/ n d i  X  n d i  X  {s ij } , goto e); /*add s ij b) if (s ij , d i )  X  Family , /* if family-link constraint holds*/ c)  X  cluster centroid d k in N , /* check must-and family-if (s ij , d k )  X  Family, dist(s ij ,d k ) = dist(s d) if dist(s ij , d i ) &lt; if d i  X  N,  X  cluster centroid d k in N , if dist(s ij ,d i ) &lt;= add s ij into n d i : n d i  X  n d i if d i  X  N ,  X  cluster centroid d k in N , if ( dist(s ij ,d i ) &gt; dist(s ij ,d k ) ) &amp; ( (s E) If R  X   X  , go to step B); F) If R =  X  &amp; NR  X   X , let R  X  NR, NR  X   X  , go to step B); G) If R =  X  &amp; NR =  X  , output N as the final set of clusters. } The research was conducted with public comments about the U.S. Environmental Protection Agency X  X  (EPA) proposed National Emission Standards For Hazardous Air Pollutants For Utility Air Toxics rule (Docket USEPA-OAR -2002-0056), and the U.S. Department of Transportation X  X  (DOT) proposed Automobile Fuel Economy Standards (Docket DOT-2003-16128). The EPA dataset contains 536,975 email messages. The DOT dataset contains 39,593 email messages and scanned letters in pdf format. Although the algorithms easily handl e datasets of these sizes, manually obtaining assessments or class labels for the complete datasets would be impractical, t hus we created three samples of 1,000 e-mails (two from EPA, one from DOT). These datasets were called NTF ( Name That Form ), NTF2 and DOT by the human assessors ( X  X oders X ) at the University of Pittsburgh X  X  Qualitative Data Analysis Program, a coding and assessment service operated by the University Center for Social and Urban Research. DURIAN X  X  redundancy-base d reference copy detection identified 28 reference copies of form letters for NTF, 26 for NTF2 and 4 for DOT. Exact duplicates of the reference copies were removed automatically. The coders were asked to identify near-duplicates of the known form letters, and to assign labels such as  X  X lock added X ,  X  X lock delete d X ,  X  X inor change X ,  X  X lock rearrange X ,  X  X ingleton X  (unique comment) and  X  X epeated copies X  , which are near-duplicate subc ategories. The coders also annotated  X  X eader X ,  X  X ignature line X   X  X takeholder X  and  X  X nique text X  (the substantive text that the commenter added to a form letter).  X  X lock added X  and  X  X ey blocks X  are found to be the most common editing styles (&gt;50%).  X  X inor change X  is the next major one (&gt;20%). The number of  X  X ingl eton X  clusters is reasonably large (around 10%), which is of potential interest for social science research. Our experiments used two types of metrics. Precision, Recall, and F1 measure effectiveness objectively. And AC1[6], a modified version of Cohen X  X  kappa intercoder agreement, measure effectiveness relatively. Cohen X  X  kappa is a more common choice, but it suffers from bias and prevalence problem when agreement between assessors is high but skew ed to a few categories, as is common in public comment datasets. AC1 corrects this flaw and is defined as: AC1 = where p(A) is the observed agreement between two assessments x and y , a is the number of pairs in the same groups in both x and y , b is the number of pairs in the same groups in x but different in y , c is the number of pairs in the different groups in x but the same in x , and d is the number of pairs in the different groups in both x and y . p(A) can be calculated as (a+d)/m where m=a+b+c+d . p(E) is the agreement expected by chance, and is calculated as: where P = ((a+b)+(a+c))/2m . The first set of experiments explores the agreement between DURIAN and human assessors. DURIAN is treated as if it is a  X  X oder X . The interceder agreements are measured using AC1. In Table 1, Coder A represents code rs UCSUR 13 for dataset NTF, UCSUR8 for NTF2 and SUPER for DOT; and Coder B represents UCSUR 16 for NTF, UCSUR9 for NTF2 and G for DOT that the human assessors have very high inter-coder agreement (&gt;90%) on what is a near-duplicat e cluster. It shows that for clustering algorithms aiming to solv e a clearly-defined task such as near-duplicate detection, providing a  X  X old standard X  is possible. Under this conditi on, DURIAN shows high system-human intercoder agreements in both macro-averaged (0.82 to 0.94) and micro-averaged AC1 (0.91 to 0.98). Moreover, the system-human intercoder agreements are comparable to human-human intercoder agreements. This is really desirable but we should be careful about the micro-averaged AC1. There are some very large letter-writing campaigns in these datasets, which creates huge near-dupli cate clusters, which tend to dominate the final result of pair-wise comparisons. Figure 4 illustrates the impact of different types of constraints on the clustering algorithm. Note that our baseline is a unsupervised clustering algorithm with smoothed KL divergence as the distance metric. For the NTF dataset, baseline has an average F1 of 0.81. F1 reaches 0.96 after 50 constraints of all three types are used (an UCSUR13, UCSUR8, UCSUR9, UCSUR16, SUPER and G are identification numbers for human assessors Table 1: Near-duplicate Detection Intercoder Agreement improvement of 18.5% over the ba seline. For the NTF2 data set, the baseline has an average F1 of 0.85, and F1 reaches 0.97 after using 50 constraints (an improvement of 14.1% over the baseline). For the DOT data set, the baseline has an average F1 of 0.90. F1 is at 0.98 after 50 constraints are incorporated, (an improvement of 8.9% over the baseline). Our results on these three data sets indicate that instance-level c onstraints have greatly boosted the clustering effectiveness. It can also be observed that some constraints are more effective than others. For both NTF and NT F2, the must-link constraints alone outperform the cannot-link, family-link and even the combination of all three types of constrains. However, on the DOT dataset, the cannot-link works the best of the three types of constraints. This phenomenon reminds us that NTF and NTF2 are drawn from the same collection, in which there are multiple major letter-writing campaigns. In this case, must-links guarantee that two documents with only a small amount of text overlap (low text similarity) are still in the same cluster as long as they satisfy must-link condition. In this case, must-links help to guide many outliners into their correct clusters and thus have a big impact on the overall clustering process. On th e other hand, must-link is less effective for the DOT dataset, in which there are very few letter writing campaigns, and hence few large clusters. In this case, cannot-link constraints are of greatest use because they encourage the creation of more clusters, (correctly) resulting in more singleton clusters. Moreover, family -links speed up the clustering process by introducing bias and clustering documents into the correct clusters at an earlier stage. A set of experiments is conducted in this work to evaluate several well-known duplicate-detection algorithms on the above three datasets. The set of gold standard s used here are the assessments from human coders UCSUR16 for dataset NTF, UCSUR15 for NTF2 and G for DOT. In order to study the effectiveness of duplicate detection techniques on subcategories, the average precision, average recall, and average F1 for each subcategory are studied for competing algorithms. The parameters for the competing methods were tuned us ing parameter sweeps and/or the best values reported in prior publications [5][7]. The contenders are described below. documents is selected and hashed ( s =3 in our experiments). Every hash value and the document id are stored as a &lt;hash-value, document id&gt; tuple. Duplicate detection is performed by 1) sorting the list of tuples; 2) for an y hash-value that also appear in the reference copy, turning this hash-value X  X  flag to 1, keeping the document id information to create another kind of tuple &lt;document id, flag=1&gt;; 3) counting the hash-values with flag=1 for every document, and get &lt;document id, count&gt;. The count is the number of overlap fingerprints between a document and the reference copy. If the overlap is above 80%, the document is considered as a (near) duplicate. Shingling (DSC) [3] : Performed in the same way as described in full fingerprinting except that every 5 overlapping substrings of size s in the documents are selected and hashed. s is set to 3 again. If the count of the overlap fingerprints in a document to form letter is above 80%, it is considered as a (near) duplicate. I-Match [4] : N words with the highest idf values in a document are selected, ( N = 30). Note that the 5 words with the highest idfs are ignored, because they might be mistakes such as misspellings. A single fingerprint is generate d for each document. Duplicate detection is performed by sorting all &lt;fingerprint, document id&gt; tuples. Those agreeing with the fingerprint of the form letter are selected as the (near) duplicates. DURIAN: The algorithm proposed in this paper. Based on results from Table 2, we can see that full fingerprinting and DURIAN are the most effective algorithms. Full fingerprinting gives the largest po ssible set of fingerprints for a document. It provides either the best or the second best F1 value in every subcategory. However, it is also the most computationally expensive method. Every substring is stored as a hash number, so the algorithm requires sorting a large list of tuples. On the other hand, DURI AN consistently performs well in all subcategories, occasionall y beating the full fingerprint approach. However, the retrieva l and detection time is much lower than for full fingerprinting. Table 3 shows that it uses less than 40% time than Full fingerprinting. Two other techniques DSC and I-Match are not as effective as Full fingerprinting and Table 2: Comparison of Duplicate Detection Technologies DURIAN. In general, DSC outperfo rms I-Match. I-Match is very sensitive to both  X  X lock added X  and  X  X lock deleted X ,  X  X inor change X  editing patterns. When the changed words are critical, i.e., appearing in the fingerprint that I-Match selected, the algorithm fails to detect the near-duplicates. In general, I-Match produces fairly low Precision and Recall. As text collections grow in si ze, and are assembled from diverse sources, duplicate and near-duplicat e text becomes an increasingly important problem. Notice a nd comment rulemaking is an extreme of this problem, but the underlying form letters are usually easy to identify, and comments often arrive with metadata that provides additional clues ab out near-duplicate relationships. This paper proposes instance-level constrained clustering as a solution to near-duplicate detection for notice and comment rulemaking. Instance-level co nstrained clustering has the advantage that varied information based upon document attributes, information extracted from the document text, and structural relationships among pairs of documents can be expressed as constraints on cluster contents, which narrows the search space, thus improving accuracy and efficiency. Experiments with EPA and DOT da tasets demonstrate that this approach to near-duplic ate detection is about as effective as high-quality manual assessment, at less computational cost than competing methods. Although notice and comment rulemaking is a problem with distinct characteristics, instance-based constrained clustering is a general solution that can be applied broadly. The ability to incorporate diverse constraints makes it a powerful tool for combining multiple forms of textual and non-textual evidence This research was supported by NSF grants EIA-0327979 and IIS-0429102. We are grateful to th e USDA, US DOT, and US EPA for providing the public comment data that made this research possible. We are grateful to invaluable comments from the anonymous reviewers. [1] S. Brin, J. Davis, and H. Garcia-Molina. Copy detection [2] Y. Bernstein and J. Zobel, A scalable system for identifying [3] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. [4] A. Chowdhury. O. Frieder, D. Grossman, and M. McCabe. [5] J. Conrad and C. P. Schriber. Online duplicate document [6] K. Gwet. Kappa Statistic is not Satisfactory for Assessing the [7] T. Hoad and J. Zobel. Methods for identifying versioned and [8] D. Klein, S. D. Kamvar, and C. D. Manning. From instance-[9] A. Ko  X  cz, A. Chowdhury, J. Alspector. Improved Robustness [10] D. Metzler, Y. Bernstein and W. Bruce Croft. Similarity [11] NIST,  X  X ecure Hash Standa rd X , Federal Information [12] N. Shivakumar and H. Garcia-Molina. SCAM: a copy [13] S.W. Shulman, E-Rulemaking: Issues in Current Research [14] K, Wagstaff and C, Cardie, 200 0. Clustering with instance-[15] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance [16] H. Yang and J. Callan. Near-Duplicate Detection for [17] C. Zhai and Lafferty, J. (2001b). A study of smoothing 
