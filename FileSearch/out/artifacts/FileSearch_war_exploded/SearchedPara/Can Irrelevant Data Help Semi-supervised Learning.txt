 Previous semi-supervised learning (SSL) techniques usually assume unlabeled data are relevant to the target task. That is, they follow the same distribution as the targeted labeled data. In this paper, we address a different and very difficult scenario in SSL, where the unlabeled data may be a mixture of data relevant or irrelevant to the target binary classification task. In our framework, we do not require explicitly prior knowledge on the relatedness of the unla-beled data to the target data. In order to alleviate the effect of the irrelevant unlabeled data and utilize the implicit knowledge among all available data, we develop a novel maximum margin classifier, named the tri-class support vector machine (3C-SVM), to seek an inductive rule to separate the target binary classification task well while finding out the irrelevant data by-product. To attain this goal, we introduce a new min loss function, which can relieve the impact of the irrelevant data while relying more on the labeled data and the relevant unlabeled data. This loss function can therefore achieve the maximum entropy principle. The 3C-SVM can then generalize standard SVMs, Semi-supervised SVMs, and SVMs learned from the universum as its special cases. We further analyze the proper-ty of 3C-SVM on why the irrelevant data can help to improve the model performance. For implementation, we make relaxation and approximate the objective by the convex-concave procedure, which turns the original optimization from integral programming problem to a problem by just solving a finite number of quadratic program-ming problems. Empirical results are reported to demonstrate the advantages of our 3C-SVM model.
 I.2.6 [ Learning ]: Induction; G.1.6 [ Optimization ]: Integer pro-gramming, Quadratic programming methods Algorithms, Experimentation, Performance Semi-supervised learning, Concave-convex procedure
Traditional supervised learning usually needs a large number of labeled training samples to learn the inductive rule. However, la-beling data is usually expansive and time consuming since it needs experts X  knowledge. Due to the limited amount of labeled training samples, researchers have proposed various methods, such as active learning [20], transfer learning [17], and semi-supervised learning (SSL) [33], to resolve this problem. Active learning requires user-s X  additional interaction to label the data during the training pro-cedure. Transfer learning transfers the knowledge learned from related tasks to the target task, where the related tasks may need sufficient labeled data. On the contrary, semi-supervised learn-ing needs the least labeled data. It automatically learns a model based on both labeled and unlabeled data. Currently, a variety of SSL methods, including EM with generative mixture models, co-training, Transductive/Semi-Supervised Support Vector Machines, and graph-based, have been proposed in the literature [5, 13, 31, 32, 35].

Previously proposed semi-supervised learning models usually assume unlabeled data are relevant to the target task, i.e., they fol-low the same distribution as the target labeled data [1, 2]. This assumption implicitly indicates that the unlabeled data are well pre-pared [5, 33]. That is, the unlabeled data has excluded all irrelevan-t data, which follow distributions different from the target labeled data. However, in real world applications, without carefully prepro-cessing, irrelevant data are easy to be included as unlabeled data. For example, when crawling web pages as unlabeled data to help classifying corresponding categories, it is very easy to collect some irrelevant web pages for them. Similarly, as illustrated in Fig. 1(a) and Fig. 1(b), when we classify the digits "5" and "8" with the help of unlabeled digits. It is possible to include other digits into unlabeled data. In these cases, learning from the labeled and the mixed unlabeled data indeed do harm to the previously proposed semi-supervised learning models [15, 21].

Hence, it is important to resolve the effect of the irrelevant da-ta in the mixed unlabeled data. To achieve this goal and to utilize the irrelevant data, in this paper, we propose a novel maximal mar-gin semi-supervised classifier, named the tri-class support vector machine (3C-SVM). The 3C-SVM can find the inductive rule to separate the targeted binary data well while determining the irrel-evant data as the 0 -class. More specifically, we introduce a novel min loss function to measure the empirical risk on the unlabeled data. This loss function can take advantages of the symmetrical hinge loss function and the  X  -insensitive loss function. More im-portantly, our model can achieve the maximum entropy principle, i.e., the decision function can rely more on the labeled data and the relevant data, while maximally ignore the irrelevant data.
We highlight the main contributions of our work as follows:
The rest of the paper is organized as follows: In Section 2, we review the related work on learning from both labeled and univer-sum data. In Section 3, the proposed 3C-SVM with its properties is presented. In section 4, we detail how to solve 3C-SVM algorithm through CCCP. We report the experimental comparison and results in section 5 and conclude the paper in section 6.
In literature, many models have been proposed to learning a bina-ry classifier with the help from other auxiliary data. These models usually only work when the auxiliary data are "clean" and satisfy the models X  assumption [5, 25].

A typical kind of methods is semi-supervised learning [5, 34], including generative methods for SSL [12, 16], graph-based SSL methods [2, 35], maximum margin classifiers [5, 6, 11], etc. These methods utilize the labeled data and the unlabeled data to improve model performance. Usually, they assume that the given auxiliary data follow the same distribution as the labeled data [5, 35]. How-ever, when unlabeled data are mixed with irrelevant data, they will hurt the SSL models [21]. An illustration is shown as the dash line in Figure 1(c).

Another line of work is the U -SVMs [27], which learns from labeled data and the universum data, a third kind of data whose distribution is different from neither of the positive class nor the negative class. The universum can play the role of seeking the sub-space for the decision function [22], but they have to be specified explicitly and chosen carefully before the training. Hence, without prior knowledge on the label of the given auxiliary data, e.g., data may be mixed with universum data and relevant data, the relevant data will also disturb the U -SVM eventually; see e.g., the dash-dot line in Figure 1(c).

The third line of work is similar to what we consider in this pa-per, where the unlabeled data is noisy. In [30], a graph-based semi-supervised learning model is proposed to learn from both labeled and unlabeled data, where the unlabeled data is assumed following the same distribution of the targeted binary classification task and the labeled data contain the universum data. This model needs to explicitly indicate the label of the universum data. In [10], a semi-supervised support vector machine is extended to learn from both labeled and mixed unlabeled data. The proposed model is solved by a Semi-Definite Programming (SDP) problem, whose time com-plexity scales to O (( L + U 2 ) 2 ( L + U ) 2 . 5 ) ( L and U denote the number of labeled and unlabeled data, respectively.), the same as that of the relaxed transductive SVM by SDP [4]. In [14], the safe semi-supervised support vector machine method is proposed to al-leviate the effect of the noise in the unlabeled data. The proposed method consists of two steps: 1) to train a SVM and a S 3 multaneously; 2) to determine the label of a data point by the con-fidence of the SVM or the S 3 VM. This method needs a postprocess on the results and do not consider the case of mixture unlabeled data.

In summary, previously proposed methods contain insufficien-cy in solving the problem of semi-supervised learning with mixed unlabeled data. Hence, in this paper, we try to alleviate the effect of those unspecified irrelevant data and utilize them in determining the decision function. (c)  X  -insensitive loss min loss.
In this section, we first define the problem setup of learning with irrelevant data. Next, we formulate the problem and propose the tri-class support vector machine, namely 3C-SVM. After that, we study the properties of 3C-SVM.
Suppose we are given two sets of data, labeled data L and un-labeled data U , where the set of labeled data consists of L labeled of U unlabeled samples, U = { x i } L + U i = L +1 . Here, x and the label is triple, i.e., y i  X  X  X  1 , 0 , 1 } . Hence, the labeled data consist of two sets of data, L  X  1 and L 0 , where data in follows the same distribution in the target task and they are labeled by  X  1 or +1 ; while data in L 0 are irrelevant to the target task. That is, data with distributions different from the labeled target data are all cast into 0 -class to construct the L 0 dataset. Similarly, unlabeled data are a mixture of these data. We denote them as U = U where data in U L follow the same distribution of  X  1 data, and data in
U 0 follow distributions different from the  X  1 data. Normally, the number of unlabeled data is much larger than the number of the labeled target data, i.e., |L  X  1 | U , and given an unlabeled data point, one does not know whether it comes from U L or from
Here, the goal is to seek a decision boundary, f  X  ( x )= b , to classify the  X  1 data well with the help of given labeled and mixed unlabeled data, where  X  =( w ,b ) and  X  : R d  X  R f a feature mapping function often implicitly defined by a Mercer kernel [19, 24]. Hence, we formulate the objective as follows: min where minimizing w 2 corresponds to maximizing the margin width [24] and avoids the overfitting. The parameter  X  is a trade-off constant for the regularization term. L (  X  ,  X  ) is a loss function to measure the empirical risk of the labeled data and U (  X  ) loss function to measure the empirical risk of the unlabeled data. r ,i =1 ,...,L + U , is a ratio penalty to balance the loss on the point x i and the regularization term.

Typically, one may choose different loss functions to measure the empirical risk on the given data. These loss functions include
In our problem setup, the unlabeled data may be a mixture of da-ta relevant or irrelevant to the target task. Actually, this assumption matches natural to normal scenarios when the unlabeled data do not well prepared. However, without prior knowledge, how to dis-tinguish the relevant and irrelevant data correctly is a very difficult task.

Here, in order to relieve the effect of irrelevant unlabeled data, we try to utilize them based on the following two principles. First, from logistic regression perspective [9, 18], when a data point lies farther away from the decision boundary, the data is more likely to be classified as data from  X  1 -class; while data points lie near the decision boundary, they are less confident to be classified correctly. Hence, ideally, data from  X  1 -class should lie on or outside of the margin gap; while other irrelevant data are close to the decision boundary. Second, the maximum entropy principle indicates that a classifier should rely more on the labeled and relevant data, while maximally ignore the irrelevant data. These two principles indicate that irrelevant data should lie around the sought decision boundary.
In order to fulfill the above principles, we adopt a min loss func-tion to measure the risk of unlabeled data, so as to separate the unlabeled data into relevant and irrelevant data. This loss function determines and measures the error of an unlabeled data point by the min value of the symmetric hinge loss and the  X  -insensitive loss (see Figure 2(d)): Hence, for an unlabeled data point, when the error measured by the  X  -insensitive loss is smaller than the error measured by the sym-metric hinge loss, we can deem it as irrelevant data; otherwise, we set it as relevant data.

With this loss function, we can develop a novel maximum margin classifier, named the tri-class support vector machine (3C-SVM), as follows: min
In the above, the first two terms correspond to the formulation of a standard SVM [24]. The third term measures the empirical risk of
L 0 data, the same in U -SVMs [27]. The last term measures the loss of unlabeled data. Hence, we can determine the class of a data point x by the following criterion: The above criterion separate the data into three classes. The 0-class data corresponds to the irrelevant data.
We first list the generalization property of our 3C-SVM, then outline the intuition behind 3C-SVMs through a specific case with r =  X  for unlabeled data and  X  =0 .

Our 3C-SVM provides a framework for the following popular maximum margin classifiers: 1. A standard SVM formulation [24] is a special case of the 3C-2. An S 3 VM formulation [5] is a special case of the 3C-SVM. 3. The 3C-SVM also includes a U -SVM [27]. It can be easily
Hence, our 3C-SVM, a general maximum margin semi-supervised learning formulation, includes standard SVMs, S 3 VMs, and SVMs as its special cases. A summarization is shown in Table 1.
Intuitively, our 3C-SVM may not work for all the cases since it seems that it requires the irrelevant data falls close to the decision function. However, we claim that we can tackle this problem by mapping the original data to a suitable space through the kernel trick. We then study why the 3C-SVM possibly works and give an insight of the model in the following theorem:
T HEOREM 1. A 3C-SVM with r i =  X  for unlabeled data and  X  =0 is equivalent to one of the following two cases: 1) training the margin gap with only one or non of the unlabeled data in the decision boundary; or 2) separating the unlabeled data into two sets, U L and U 0 with |U 0 | X  2 , and training a general S the training data projected onto the orthogonal complement of span {  X  ( x j )  X   X  ( x 0 ) , x j  X  X  0 } ,where x 0 is an arbitrary data point from U 0 , while keeping the unlabeled data in the set of on or out of the margin gap.
 Proof: r i =  X  for U data and  X  =0 imply that the min term in the fourth term of (3) vanish and the optimal solution of w and b in (3) is attained when one of the following conditions is fulfilled: (a) | w  X  ( x j )+ b | X  1 ,or(b) w  X  ( x j )+ b =0 . Hence, the above conditions set up the criterion of separating the unlabeled data into two sets, U L and U 0 , where data in U L satisfy the condition of (a) and data in U 0 satisfy the condition of (b).
 theorem. Here, a general S 3 VM means that it is a generalization of the S 3 VM and the U -SVM.

Next, if U 0 contains at least two samples. For the data x U ,wehave w  X  ( x j )+ b =0 . Hence, picking arbitrary data x from U 0 , we obtain w (  X  ( x j )  X   X  ( x 0 )) = 0 .Thatis, w is orthog-onal to span {  X  ( x j )  X   X  ( x 0 ) , x j  X  X  0 } .Now,let P orthogonal project on the orthogonal complement of the mapped set U ,wehave w = P U  X  w is sought by training a general S 3 VM on the projected labeled data and U L data with projection by P U  X  tion (a) valid, or other unlabeled data falling on or out the margin gap.

Theorem 1 clearly shows that the optimization of our proposed model eventually is to find the most suitable subspace in which the margin is maximized while the overall empirical risk is minimized. The irrelevant data play the role of finding the subspace.
Due to the non-convexity of the min loss function, the formu-lation of the 3C-SVM in (3) is non-convex in general. Moreover, there are two difficulties to be solved in the formulation, the term and the absolute operation on the unlabeled data. In the fol-lowing, we show how to solve these two difficult problems.
First, we introduce decision variables, d k  X  X  0 , 1 } , to remove the min term. This trick is similar to the L 1 -norm S 3 VM in [3]. We then transform the optimization into a mixed integer programming problem as follows: min where kL = k + L , D&gt; 0 is a suitable constant making Q when d k =0 and Q 2 =0 when d k =1 . That means, when d k =0 , the error is counted from Q 2 and the unlabeled data are classified as 0 -class and its error is measured by the  X  -insensitive loss function; when d k =1 , the error is incurred by Q 1 unlabeled data are classified as one of the  X  1 -class, where its error is measured by the symmetrical hinge loss function.

Next, we deal with the absolute terms in the loss function by considering the properties of the loss functions. The shifted sym-metrical hinge loss function in Q 1 can be abstracted as H or H 1 ( | u | + a )=max { 0 , 1  X  X  u | X  a } = H 1  X  a ( | approximated by a symmetrical loss, which is similar to the ramp loss used in [6, 26], as follows:
H 1 ( | u | + a )  X  H 1  X  a ( u )  X  H  X  ( u )+ H 1  X  a (  X 
The shifted  X  -insensitive loss function in Q 2 can be transformed to another symmetrical loss as follows: Due to the symmetry of both loss functions, we introduce new pair-data for the unlabeled data to simplify the expression as [6]. The new pair-data are where kL means k + L and kLU means k + L + U .
Hence, we can transform the problem in (5) into Q  X  (  X  , d which is the summation of two terms, Q vex (  X  , d ) and Q They are defined as follows =  X  + + where  X  1=1  X  D (1  X  d k ) and  X   X  =  X   X   X  Dd k .

Note that the above concave term, Q  X  cav , keeps the non-convexity of the model following from the ramp loss in approximating the Q The optimization in Q  X  (  X  , d ) is a summation of a convex term and a concave term, or difference of convex programming. Hence, it can be solved by the concave-convex procedure (CCCP) [29], a technique has been adopted in large scale transductive SVMs [6] and SVMs on data with missing values [23].

In the CCCP, we need to use the first order Taylor expansion to approximate the concave term of Q  X  cav . Since the variable d does not appear in the concave term, we only need to apply the first order Taylor expansion of Q  X  cav at  X  t . Hence, we can seek the optimal variables by solving a sequence of the following optimization prob-lem: The above optimization is a mixed integer optimization problem since d is an integer vector. Here, we adopt a standard routine to solve the integer programming problem [28]: 1) relaxing the integer variable to a real variable, then solve the whole optimization together; 2) rounding the corresponding variable to get its integer solution.

For 3C-SVM in (6), we relax the decision variable d k from to [0 , 1] and solve the optimization problem in (6) first. We then de-termine the value of d k by its definition, the error incurred is less when the data is assigned to the associated class, as follows where  X  k = H 1 ( | f  X  ( x kL ) | ) and  X   X  k = I  X  ( | f
To simplify the first order approximation of the concave term in (6), we define  X  for those unlabeled samples x k + s with d k =1 ,where k =1 and s is L or L + U . Hence, the first order Taylor expansion of the concave term is then expressed as
Now we turn to solve the relaxed optimization in (6) and sum-marize the result in the following theorem:
T HEOREM 2. The dual problem of the relaxed optimization in (6) is a Quadratic Programming (QP) problem as follows: where the Lagrangian multipliers [  X  ;  X   X  ] consists of an L +4 U -dimensional vector. The matrix  X  on the quadratic term is defined as  X  = and the coefficient for the linear term is A vector containing the label value of all training data including the expanding auxiliary labels, and Y  X  2 U denotes the last 2 in Y .
 The above theorem can be derived based on the standard Lagrangian multiplier method, where Eq. (9) corresponds to the dual form of the optimization on (6).

After solving the QP problem in (9), we obtain w as a linear combination of the dual variables,  X  and  X   X  , w = and the variable b corresponds to the dual variable of the equality constraint. The form of the weight we obtained is similar to that in [2]. We can also define the corresponding support vectors .They are those labeled data x i  X  X  with non-zero  X  i values and unlabeled data x j  X  X  with non-zero (  X  j +  X   X  j  X   X  j ) values. Hence, we obtain Algorithm 1 to solve the 3C-SVM algorithm. Recalling Theorem 1, we can know that, intuitively, the Algorith-m 1 works in the following way: 1) first finding out those unla-beled data which are certainly outside the margin gap, removing them from the training set; 2) then training a U -SVM model on the labeled data with the rest unlabeled data.

Moreover, the convergence of Algorithm 1 is guaranteed by the following theorem:
T HEOREM 3. The algorithm 1 converges in a finite number of iterations.
 Proof: First, we prove that the objective Q  X  decreases in each iteration. From the CCCP, we have where  X  X   X  cav defines the partial derivative of Q  X  cav with respect to  X  . Hence, summing (11) and (12) together, we get Q  X  (  X  t Q  X  (  X  t , d ) for the same d .

After rounding, the objective value Q  X  is Q  X  (  X  t +1 , d may be greater than Q  X  (  X  t , d t ) . In order to avoid this case, we restore d t +1 to d t and seek  X  t +1 again by minimizing Q fixed d . This additional step guarantees to decrease the objective of Q  X  at each step.

Second, the variable  X  can only take a finite number of distinct values. The algorithm converges in several steps since Q  X  es in each iteration and the inequality (12) is strict unless unchanged.
 Algorithm 1 CCCP for 3C-SVMs
Remark Note that the local optimal issue of the 3C-SVM has been alleviated by its initialization and the additional step to avoid increasing the rounded objective function is not needed usually. Our observation from the experimental results shows that our 3C-SVM works well using current initialization and the rounded ob-empirical study in Section 5.
 Complexity Analysis Algorithm 1 has to solve a sequence of QPs in (9). In practice, we find that the number of iteration steps is a constant, usually less than 10; see Figure 4. Thus, training a 3C-SVM is equivalent to solving a constant number of QP problems with |L 0 | + L +4 U variables. Therefore, the 3C-SVM algorith-m has a worst case complexity of O (( |L 0 | + L +4 U ) 3 ) Possible tricks may be applied to speed up the 3C-SVM algorithm in a quadratic scale [6, 19]. Furthermore, by exploring the sparsity structure among the dual variables, we can reduce the number of variables to the number of non-zero variables. This can reduce the computation cost of 3C-SVM largely.
In the formulation of (3), we do not consider the balance con-straint for the unlabeled data. Actually, balance constraint can be easily incorporated in our formulation.

There are two observations: 1) Data from U L need the balance constraint [25]; 2) Data from U L 0 do not need the balance con-straint. By Theorem 1, ideally, the decision values of U L approach to zero. Hence, summarizing the decision values of all unlabeled data, we can obtain the same balance constraint as that used in [7], This constraint can be easily included in the optimization of (6) and rewrite into kernel form in (9) similar to the trick in [7].
It is noted that the balance constraint in (13) is affected by the summation of y i . A possible better setting for the balance con-stant related to the portion of the number of the unlabeled data assigning to the positive class [5]. However, it again introduces another hyperparameter. Actually, our empirical evaluation find-s that balance constraint is insensitive to the model performance. One reason may be that the U 0 data has played the role of balance constrant in the model.
In this section, we evaluate our proposed 3C-SVM algorithm on both synthetic and real-world datasets and compare it with SVM, S
VM [6], and U -SVM [27]. Our 3C-SVM algorithm is imple-mented in Matlab 7.3 and the QP problem is solved by a general optimization toolbox, MOSEK 1 . In the experiments, we try to in-vestigate the following questions: (1) What is the performance of 3C-SVM comparing with other three maximum-margin based al-gorithms? (2) What is the trade-off on the parameters D and  X  on 3C-SVM? (3) What is the convergence of 3C-SVM?
The synthetic data is a 50 -dimensional dataset. The  X  1 -class are generated following the scheme of [22], where the means are c and  X  2 3 ,..., 50 =10 . In this setting, we can generate two Gaussians with the Bayes risk being approximately 5%. Two kinds of U similar to those in [22] are generated: http://www.mosek.com
It is noted that the optimal decision boundary is a linear classifier for the synthetic datasets. Hence, we employ the linear kernel in fitting the data.

As reported in Table 2, we test the number of labeled data from { 20 , 50 , 200 , 500 } and vary the proportion of the mixed unlabeled data by (  X U, (1  X   X  ) U ) ,where  X U data are randomly chosen from  X  1 -class and (1  X   X  ) U data are randomly chosen from U 0 data.  X  the model on a separated test data with 500 data samples.
In the comparison, there are different parameters for different models need to be tuned. They include:
A problem of 3C-SVM is that its parameters are large and they will affect the model performance. These parameters in 3C-SVM not only include C in SVM, C U in U -SVM and S 3 VM,  X  in SVM and  X  in S 3 VM, but also include the parameter D .Itister-rible to tune all the parameters, e.g., by cross validation, togeth-er. To resolve this problem, we adopt a simple way to tune them. More specifically, we first tune the parameters in SVM, U and S 3 VM on the test set, individually. Next, we set the parameter-s of our 3C-SVM based on the obtained optimal parameters from other models. That is,  X  is set to 1 C , r i =1 for labeled data and r corresponds to U -SVM since this set of parameters obtains better performance than that of S 3 VM. The parameters  X  and  X  are set the same as the optimal value from the U -SVM and S 3 VM, respective-ly. D is set to 2 for simplicity since the results have shown that our 3C-SVM can achieve very good performance.

Figure 3 reports the average performance ( 10 runs) of all four algorithms in the above all cases. It is shown that 3C-SVMs con-sistently attain the best results. U -SVM achieves similar perfor-mance as 3C-SVM when the number of U 0 data is large. However, its performance decreases when the number of U 0 data decreases and cannot beat SVM when the size of the labeled training data is 500 . Similar trend is obtained for S 3 VM. On the contrary, our 3C-SVMs keep nearly the same accuracies and outperform U -SVM and S 3 VM when the number of labeled training data is large.
Convergence Study. We also study the convergence of 3C-SVM on the synthetic dataset with different settings ( L =20 / 500 ). Figure 4 shows the one trial result on the objective function value and test errors at each CCCP iteration. The figures show that the 3C-SVM algorithm decreases the objective function value and the test error rates decrease correspondingly at each iteration. At the same time, 3C-SVM tends to converge in only a few iterations, less than 10.
The USPS dataset and the MNIST dataset are two popular bench-mark handwritten digit datasets which have been used in litera-ture to validate the corresponding classification models [6, 19]. As reported in Table 2, each image in USPS was normalized and centered with the size of 16  X  16 , which forms 256 -dimensional data (see examples in Figure 1(a)). This dataset contains 9,298 grayscale handwritten digit images, 7,291 of which are used as the training set, while the remaining 2,007 are used as the test set. The MNIST dataset consists of a training set of 60,000 digits and a test
Objective Values 50 60 70 80 90 100
Objective Values set of 10,000 digits (see examples in Figure 1(b)). The digits are grayscale handwritten images normalized and centered in 28  X  28 which forms 784 -dimensional data. We have normalized each pixel value in an image to the range of  X  1 and 1 .

Similar to the setup in [22, 27], we employ digits " 5 "and" construct the  X  1 -class data, but differently, we utilize all other dig-its as 0 -class. In the evaluation, we test the number of labeled data in 10 and the number of unlabeled data is 100 and 1000, while the proportion of the mixed unlabeled data is set as (  X U, (1  X  where where  X U data are randomly chosen from digits "5" and "8" and (1  X   X  ) U data are randomly chosen from other digits.  X  is test-on the test set of digits " 5 "and" 8 ".

Here, since the data are linearly nonseparable in the original fea-ture space [19], we employ the RBF kernel on all the models. That is, K ( x , y )=exp(  X   X  x  X  y 2 ) , is adopted as the kernel, where  X  is the width of the RBF kernel. Since we also need to tune an additional parameter, this makes it more difficult in tuning the pa-rameters for 3C-SVM, and for other three models. Similar to the procedure in [5], we seek the optimal parameters on a separate val-idation set by maximizing the performance on the test set. More specifically, the parameters are sought as follows: Table 3 reports the average ( 10 runs) accuracies of four algorithms on the two handwritten digit datasets. 3C-SVM consistently attains better results in all cases. By examining the details of the results, we have the following observations:
In the experiment, we also conduct sensitivity analysis on two parameters,  X  and D , in 3C-SVM. The analysis of the hyperparam-eter, e.g., C , C U ,  X  , can be referred to [6, 19]. For  X  , since it is insensitive when it is about 0.1, we do not study its effect in this section.

In the test, we change  X  in { 0 , 0 . 01 , 0 . 1 , 0 . 2 , D in { 1 , 2 , 10 } and test on the case of balance mixed unlabeled Table 3: The average ( 10 runs) accuracies (%) of SVMs, S -SVMs, and 3C-SVMs on the USPS and the MNIST (" 5 "vs" 8 ") . 1  X  =0 . 5  X  =0 . 9 2.6 88.8  X  3.1 88.8  X  3.2 2.7 89.1  X  2.9 89.1  X  3.0 8.4 76.7  X  8.0 76.3  X  7.6 7.4 76.9  X  7.9 76.5  X  8.0 data (i.e.,  X  =0 . 5 ). Figure 5 shows the changing trend with  X  and D , respectively. It should be noted that the best results in Figure 5 also refer to the results in fifth column in Table 3. By examining these results, we have the following observations:
In this paper, we have proposed a novel maximum margin semi-supervised classifier, named the tri-class support vector machine , to learn from mixed unlabeled data. More specifically, we intro-duce a new min loss function to distinguish the mixed unlabeled data into relevant and irrelevant data based on which error occurred is smaller when assigning the data to the associated class. The loss function can therefore achieve the maximum entropy princi-ple and force the irrelevant data close to the decision boundary. In generalization, 3C-SVM includes several popular maximum mar-gin classifiers, such as SVMs, S 3 VMs, and U -SVMs, as its spe-cial cases. Furthermore, we provide detailed theoretical analysis to show the role of irrelevant data and why the model works. More-over, in implementation, we transform 3C-SVM from an integer programming problem to a sequence of QP problems. The approx-imation by the concave-convex procedure has speeded up the model largely and finally yielded the same worst case time complexity as that of S 3 VMs.

This work opens several interesting research issues. One worthy direction is to further speed up the model by warm-starting or by exploiting the sparsity structure of the solution. The other direction is to design an efficient way to tune the model parameters or to de-sign a scheme to automatically learn the model parameters. We will also consider to extend the model to solve multi-class classification tasks and verify its performance.
 This work is supported by two grants from the Research Grants Council of the Hong Kong SAR, China (Project No. CUHK 413210 and Project No. CUHK 415410) and a grant supported by a re-search funding from Google Focused Grant Project "Mobile 2014". [1] R. K. Ando and T. Zhang. A framework for learning [2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [3] K. P. Bennett and A. Demiriz. Semi-supervised support [4] T. D. Bie and N. Cristianini. Convex methods for [5] O. Chapelle, B. Sch X lkopf, and A. Zien, editors.
 Figure 5: Effect on  X  and D for two handwritten digit datasets. The best results of 3C-SVM are obtained when  X  =1 . 0 , D for USPS and when  X  =0 . 5 , D =2 for MNIST, respectively. [6] R. Collobert, F. H. Sinz, J. Weston, and L. Bottou. Large [7] R. Collobert, F. H. Sinz, J. Weston, and L. Bottou. Trading [8] D. Goldfarb and S. Liu. An o( n 3 l ) primal interior point [9] D. W. Hosmer and S. Lemeshow. Applied logistic regression . [10] K. Huang, Z. Xu, I. King, and M. R. Lyu. Semi-supervised [11] T. Joachims. Transductive inference for text classification [12] N. D. Lawrence and M. I. Jordan. Semi-supervised learning [13] Y.-F. Li, J. T. Kwok, and Z.-H. Zhou. Cost-sensitive [14] Y.-F. Li and Z.-H. Zhou. S4vm: Safe semi-supervised [15] M. Mojdeh and G. V. Cormack. Semi-supervised spam [16] K. Nigam, A. McCallum, S. Thrun, and T. M. Mitchell. Text [17] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE [18] J. C. Platt. Probabilistic outputs for support vector machines [19] B. Sch X lkopf and A. Smola. Learning with Kernels .MIT [20] B. Settles. Active learning literature survey. Technical Report [21] A. Singh, R. D. Nowak, and X. Zhu. Unlabeled data: Now it [22] F. H. Sinz, O. Chapelle, A. Agarwal, and B. Sch X lkopf. An [23] A. J. Smola, S. V. N. Vishwanathan, and T. Hofmann. Kernel [24] V. Vapnik. The Nature of Statistical Learning Theory . [25] V. Vapnik and S. Kotz. Estimation of Dependences Based on [26] J. Wang, X. Shen, and W. Pan. On efficient large margin [27] J. Weston, R. Collobert, F. H. Sinz, L. Bottou, and V. Vapnik. [28] L. A. Wolsey. Integer programming . Wiley-Interscience, 1 [29] A. L. Yuille and A. Rangarajan. The concave-convex [30] D. Zhang, J. Wang, F. Wang, and C. Zhang. Semi-supervised [31] Z.-H. Zhou and M. Li. Semi-supervised learning by [32] Z.-H. Zhou, D.-C. Zhan, and Q. Yang. Semi-supervised [33] X. Zhu. Semi-supervised learning literature survey. [34] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised [35] X. Zhu and A. B. Goldberg. Introduction to Semi-Supervised
