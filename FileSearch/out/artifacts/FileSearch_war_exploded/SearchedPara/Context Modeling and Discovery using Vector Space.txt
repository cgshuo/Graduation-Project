 In this paper, context is modeled by vector space bases and its evolution is modeled by linear transformations from one base to another. Each document or query can be associ-ated to a distinct base, which corresponds to one context. Also, algorithms are proposed to discover contexts from doc-ument, query or groups or them. Linear algebra can thus by employed in a mathematical framework to process context, its evolution and application.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval models General Terms: Theory Keywords: Information Retrieval in Context, Vector-Space Model
Information Retrieval (IR) deals with the retrieval of all and only the documents which contain information relevant to any information need expressed by any user X  X  query. A system matching that definition exists in principle. In prac-tice a system is unable to answer any query with all and only relevant information. One of the reasons is that the classical models, e.g. boolean, vector-space or probabilistic, were defined by assuming that there is one user, one infor-mation need for each query, one location where the user is, and no temporal dimensions.

The difficulty of IR is basically due to the fact that rel-evance is context-dependent since information needs evolve with user, place, time. The information provided by a doc-ument would be indistinguishable from noise if the user of the document were out of context  X  the reader of this paper would little understand the meaning represented by these words if he were not placed in a context given by his own ex-pertise, interest or task being carried out. As consequence, the assumption underlying current IR systems that docu-ments and queries are elicited from equivalent contexts is unlikely.

Relevance Feedback (RF) is a variation of the  X  X ono-lithic X  theme of context-unaware systems  X  as an informa-tion need could evolve after the user have seen and assessed some documents answering to a query, the automatic mod-ification of the query could be performed by the system by using the feedback collected from the user. This way, the modified query is  X  X loser X  to relevant documents, and then retrieves, perhaps, other relevant documents, provided that relevant documents are close one to each other and distant from the non-relevant documents. RF could be shown to improve retrieval effectiveness in many situations because it uses information gathered from previous queries, seen doc-uments and relevance judgments. Although it have resulted in good improvements, it has reached its maximum except if some marginal variations are considered. RF remains, how-ever, dependent on a single context.

Beside RF, a common approach to address IR in context may be based on ad-hoc techniques appropriately designed to capture time, space, histories, or profiles. By using some juxtapositions, these data may be then injected into a model which nevertheless does not embody any notion of context. However, the availability of sensors to get space location, log-files to implement history, metadata to describe profiles, clocks, calendars to get time, and the related techniques, are necessary conditions in order to implement context-aware IR systems.
 An important problem hampering the full development of IR is the lack of some sort of embodiment of context the IR models. It is believed in this paper that the classical models have left some reflections and intuitions still unde-veloped despite decades of research. Rather than to attempt to define yet-another model for IR, the Vector Space Model (VSM) is in this paper adopted as an infrastructure for cop-ing with ( i ) the representation of the dimension of context in document authoring and query formulation, ( ii ) the way context evolution reflects on its representation, and ( iii ) the computation of a geometrical representation of context by starting from the available documents or queries.
The formal approach to understanding the VSM capac-ities basically expounded in this paper is appropriate. In-deed, the problem ought presently be investigated at a for-mal level, in our opinion, because every breakthrough in performance ought be preceded by some sort of modeling.
The paper has been structured as follows: After reviewing some related works in Section 2, the Vector Space Model is introduced in Section 3. The approach to context model-ing proposed in this paper is illustrated in Section 4. The description of a method to discover context within this ap-proach is reported in Section 5, whereas RF is viewed in Section 6 as an instance of the presented approach. Finally, an illustrative example is given in Section 7 before the Con-clusions of Section 8.
Fully recognizing the high complexity of IR implies needs to address the fact that IR is context-dependent and the interaction between the system and the environment. This observation dates back to some decades ago since some re-searchers pointed out that context is the key factor affecting retrieval performance. It was then recognized that context implies user-system interaction and that user models should be integrated in system models. [1, 2] Recently the issue has been further investigated by calling attention to identifying  X  X seful abstractions of contextual information X  [3] The need of looking at context is currently even stronger than in the past since classical inverted file-based systems have reached their maximum effectiveness. The contribution of this pa-per is to enhance an IR model with the capacity of modeling context.

The VSM is here adopted as model of reference. It gives an intuitive yet formal view of indexing and retrieval  X  it is a matter of fact that it attracted many researchers and newcomers after it was introduced in [4] and [5]. It proved very effective in retrieving documents of different languages, subjects, size, and media, thanks to a number of proposed and tested weighing schemes and applications which have made it a sound framework.

The potentialities of vector spaces have not been fully ex-ploited in practice, even though some attempts were made in the past with some success. Although the notion of context was ignored, an early effort done to reevaluate the VSM is reported in [6]. Two issues among many were term depen-dence and the role played by the term-document frequency matrix.

Despite its seeming simplicity, the mathematical proper-ties of vector spaces can be leveraged to arrive at further results. In this paper some mathematical constructs are leveraged by starting from some of the issues pointed in [6], by addressing the issue of context, and making a step for-ward toward context-aware IR modeling.

The hypothesis that the term-document frequency matrix encloses information about the correlation among terms and among documents was stated in [6] and further exploited in [7] whose authors proposed Latent Semantic Indexing (LSI). The latter is a technique based on Singular Value Decomposition (SVD) aiming at decomposing the matrix and disclosing the principal components used to represent fewer independent concepts than many inter-dependent in-dex terms. Those authors proposed an approach to discover the hidden concepts of a collection of documents. In this pa-per, a decomposition algorithm is employed as done in [7], though the algorithm is different from SVD, and it is the context of a single document or query that is discovered.
A recent reconsideration of the geometry underlying IR, and indirectly of the VSM, was done in [8]. In that book Hilbert X  X  vector spaces are used to see documents as vectors, relevance as a linear transformation, relevance statuses as the eigenvalues of the linear transformation, and the com-putation of the probability of relevance of a document as the projection of the document vector onto an eigenvector of the linear operator. In other words, the size of the pro-jection of the document vector onto an eigenvector of the operator is the probability that the document is about the relevance state represented by the corresponding eigenvalue. In [8], orthonormal bases are employed to represent the di-verse statuses of relevance, whereas, in this paper, bases are employed to represent the different ways the descriptors are affected by context.
Let V be a vector space in R n and U = { u 1 , . . . , u k a set of k column vectors in V . The theory has been ex-posed in this paper within R n , but nothing prevents us to considering the more general complex case, i.e. C n , where n can be  X  , by thus permitting more general results than the ones reported here. However, the real finite vector spaces are sufficiently broad for what addressed in this paper. Ad-ditionally, the VSM is introduced in this paper by recovering some linear algebra notions which are not always present in the literature.

The matrix U = [ u 1 . . . u k ] is the matrix corresponding to U . A set of vectors U is independent if P k i =1 a i u only if a i = 0 for every i . If k = n and U is independent, U is a base for V . A base for V generates all the vectors of V , that is, v = P n i =1 a i u i = U  X  a for all v  X  V , where a = [ a 1 . . . a n ] &gt; are called coefficients or weights. effective coefficients used to generate document vectors form a family, called TF  X  IDF, of which some instances were re-ported, for example, in [9, 10], whereas some theoretical issues were analyzed in [6].

When a collection of documents is indexed, the IR system yields a set of unique descriptors { t 1 , . . . , t n } . It is worth noting that this representation is valid apart from the me-dia or the languages used to implement the documents. In this model, the set of unique descriptors is modeled as a base T such that there exists a vector of T for each descriptor. Every object, i.e. documents, fragments, or queries are mod-eled as vectors generated by T , that is, z = P n i =1 a i where z models the object z , t i models descriptor t i and a is the coefficient of t i used to generate z .

The retrieved documents are ranked by the inner product between query vectors and document vectors, that is by x &gt; and y = T  X  b are the vectors generated by T which models, respectively, a document x and a query y ; T &gt;  X  T is called descriptor correlation matrix.

The computation of T 0  X  T is resource consuming, therefore descriptors are often assumed in the literature as orthogonal one to each other and T 0  X  T is then a diagonal matrix. Often, T 0  X  T is the identity matrix and the descriptors are mod-eled by the versors e 1 , . . . , e n of V , where e ij = 1 if i = j , 0 otherwise, for every i = 1 , . . . , n . This means that every document or query vector z is generated by the set of ver-sors and the coefficients c 1 , . . . , c n , that is, z = P P incidence z = c is the reason why what has affected the effectiveness of the VSM in the experiments carried out in the past has been the choice of the coefficients, whereas the base has been ignored.

It should be noticed that the notion of orthogonality is stricter than the one of independence. since an orthogonal x &gt; means  X  X ranspose of X  x . set of versors is also independent, but the converse is not true. [11]
From these facts, it is apparent that the notion of base vector is distinct from the one of coefficient. The latter is used to generate object vectors by combining base vectors, which in turn represent descriptor semantics. The distinc-tion between the notion of coefficient and the one of base vector has been leveraged in this paper to model documents and queries by thus associating, for example, multiple bases to the same set of coefficients.
In this Section, the framework for modeling context and its evolution is illustrated. First, the scenario is described in Section 4.1, then modeling is formally introduced in Sec-tion 4.2, and, finally, evolution is formalized in 4.3.
Let a user be either a searcher accessing the system to retrieve documents relevant to his information need, or an author developing his documents. The user employs de-scriptors to express the semantic content of the documents or of the queries depending on whether he is an author or a searcher, respectively.

The descriptors are selected by the user on the basis of their own capabilities in expressing the semantic content of the documents and by their semantic relationships, such as synonymy or relatedness. Of course, it is assumed that the user chooses the best descriptors to express his query or his documents given his own knowledge of the domain. Moreover, the user chooses or does not choose a descriptor depending on the relationships with the other descriptors and on it own Anomalous State of Knowledge (ASK). [1] For instance, a descriptor could be more likely to be used than another descriptor if the former tends to co-occur with other ones already employed in the same query or document.
When using a descriptor to express a query or a docu-ment, the user is giving to the descriptor a semantics which is different from the semantics of the same descriptor used by another user or by the same user in another place, time, need  X  in other words, the use of a descriptor depends on context. Therefore, context influences the selection of the descriptors, their semantics and inter-relationships. Some examples follow: After a series of queries about computers , the next query is likely not to include that key word be-cause it can be assumed the user is refining its information need. If the user is searching for services close to Padua , the query might include Venice as well, since Padua is very close to Venice. If a document is general, highly specific terms are unlikely to be included by the user together with gen-eral terms since specific terms are more frequently used in technical documents. The evolution of queries given above depends on the evolution of the underlying context, and re-flects on the evolution of the vector base.
Despite extensive study and experimentation of the VSM, there are some potentialities to which further study should be deserved. In particular, there is an issue stated below which was taken little seriously in the past, but looks promis-ing, that is why it is addressed in this paper. Basic premises of the paper are: In other words, every document or query is associated to a distinct base and the connection from a base to another is governed by a linear transformation. For example, let n = 2 and t 1 , t 2 be two descriptors, say Padua and Venice , respec-tively. If a user is looking for information about Padua and he is traveling around it, a query q includes t 1 with weight 1; if the vectors representing the descriptors are assumed as orthonormal, then versors can be used and q = [1 , 0] &gt; If some contextual information are available about space, e.g. the user is moving toward Venice, the descriptors can be no longer assumed as orthonormal and the set of vec-tors { [1 , 2] &gt; , [3 , 1] &gt; } can be used instead as base, then, q = [1 , 3] &gt; . One base can be mapped to another by linear transformation.

Thus, context influences not only the choice of a descrip-tor, but also its semantics and the way it relates to other descriptors. This influence cannot be expressed by coef-ficients since these refer to one descriptor at a time and are computed by the system using statistical data about it. Context is actually expressed by the specific instance of the vector base which corresponds to the descriptors employed to express an information need or to author a document  X  this notion is illustrated in the rest of the Section by using matrices.

In general, the vector x of the document x authored in its own context is generated by the base T . The latter is in its turn not necessarily equal to the base U that generates, say a query y , or another document. Therefore, x is represented by x = T  X  a whereas y is represented by y = U  X  b where a and b are the coefficients used to combine the base vectors of T and U , respectively. If relevance is estimated by the usual inner product, documents are ranked by x &gt;  X  y = ( T  X  a ) &gt;  X  ( U  X  b ) = a &gt;  X  ( T &gt;  X  U )  X  b .
The latter reveals that the relationships among the de-scriptors used to express documents and queries depend on two contexts: the one involved in document authoring and the one used in query formulation. In contrast, there is a single base for both the document and query, in the classi-cal formulation of the VSM. In the formulation given in this paper instead, two different contexts are to be considered, and if the descriptor correlation matrix were available, one is able, perhaps, to compute two different bases, i.e. T and U . Thus, the bases embedded in the descriptor correlation matrix ought to be thought as decoupled notions.

Let us give the following numerical example, which is also depicted in Figure 1. Document d is represented by vector d and is generated by base T as follows: T = 2 1 1 2 , the axis, i.e. the base vectors of document d are given by t , t 2 which are not orthogonal, but independent of one to each other. The coefficients a 1 , a 2 linearly combine t which thus generate d . Query q is similarly represented by vector q , but is generated by base U and coefficients b 1 Figure 1: An example of modeling context. Thick lines depict vector bases and thin lines depict gen-erated object vectors. instead: U =  X  1  X  1 2  X  1 , b = 1 2 , 1 &gt; , q = U  X  b =  X  3 2 , 0 &gt; . Query q 0 is instead generated by T yet with the same coefficients as q : q 0 = U  X  b = 1 2 , 2 &gt; . Both query vectors have been generated by the same coefficients, but q is closer to d since it has been generated by the same base, whereas q 0 has been generated by U , which is  X  X istant X  from T . This example explains how the knowledge of the context can be crucial to get the  X  X ight X  document ranking.
In general, the base column vectors of T and U are un-known. Sometimes, the correlation matrices T &gt;  X  T or U are known. A method to compute them starting from cor-relation data is described in Section 5. If correlation data are unknown and only one base is available, changing from a context to another can be possible and the computation of d &gt;  X  q would require the knowledge of the context change matrix, as described in Section 4.3.

It should be also noticed that orthogonality does not mean absence of context, there are on the contrary infinite bases and contexts related to a diagonal matrix. Indeed, if I is the identity matrix used to represent orthonormality between any pair of base vectors, there are infinite bases T such that T
The examples and the approach above presented looks perhaps like query modification. However, there are at least a couple of differences. First, query modification in the VSM is an instance of context change, as shown in Section 6  X  this means that the approach above presented is more gen-eral than query modification. Second, query modification techniques intervene on coefficients, whereas modeling and changing context by using bases means that modifications apply to the hidden structure of queries or documents.
A problem is related to the definition of the elements of the base vectors, i.e. how to decide the scalars which make up the numerical configuration of the base vectors. An option would be that they can be seen as the coefficients which could have been used to combine the versors and generate the base vectors. In that case, a semantics of the versors is needed. With the Generalized VSM, an attempt was made to estimate the concepts, named  X  X interms X , which might be seen as versors generating a vector base. The assumption was that the minterms are the 2 n versors of a vector space and the coefficients are computed by considering document similarities, yet context were not considered. For details, the reader is suggested to refer to [12].

It is important noting that the problem of mapping the data, which are gathered from sensors, log files, cameras, and used to describe a context, to a vector space base is not addressed in this paper. This means that the availability of a mechanism that maps data describing context to a vector space base or to matrices from which bases can be computed is assumed. Actually, a similar assumption has been made in the past as regards to the IR models; the VSM does not tell how weights should be computed. This issue is, however, not ignored completely  X  a method to discover the context which generates a document or a query is illustrated in Section 5.
As descriptors are represented as base vectors, different contexts should reflect on different bases. If, for instance, the user is expressing a query using the context represented by t 1 = [1 , 2] &gt; , t 2 = [3 , 1] &gt; , a change of context should reflect as a different base, say t 1 = [1 , 0] &gt; , t 2 means that a base changes as context changes, i.e. there are more than one base for a collection of documents. Similarly there does not exist a unique base for all the queries, but there are as many bases as contexts are, even for each single query.

Context changes can be modeled as matrix transforma-tion. Let suppose a change of the context T leads to a new context U . The corresponding matrix T is transformed to the matrix U .

Proposition 1. Given two matrices T and U , there ex-ists a unique matrix C such that U = T  X  C . [11] In other words, one needs to only find the right matrix out to compute context change and only one context matrix is computed from another context matrix and a context change matrix. However, there is no unique representation of con-text, i.e. several matrices can represented a context. Propo-sition 1 states uniqueness in the field of matrices.
It is indisputable that this sort of representation of context and its evolution would be simplistic, if one were attempting to regard base vectors as detailed representation of context features. On the other hand, this approach has the advan-tage of being fairly easily mappable to data structures and algorithms, the latter being important issues in automated IR.

Whenever one has to deal with matrices, some computa-tional problems naturally arise because of the  X  X uadratic X  yet simple nature of the data. In this paper, moreover, it has been proposed to assign to every object a context ma-trix of which dimension is of quadratic order in the number of its own unique descriptors. Nevertheless, the computa-tional problem can be still managed since ( i ) the number of unique descriptors of an object may be small, ( ii ) many of these descriptors are unrelated so there are many zeroes which do not need any storage, ( iii ) one context can  X  X ouse X  more than one object like clusters do, by thus making the number of context matrices needed lower than the number of objects, and ( iv ) an interesting form of context matrix shall be introduced in Section 5 so that a further save of computational resources can be obtained.
As shown above, the computation of the elements forming the base vectors allows to employ context in indexing and retrieval. In fact, document ranking changes radically if a context is represented by a base rather that another base, as exemplified above, since different base vectors may lead to different correlation matrices or rotations. In order to exploit the role played by context, the computation of the base vectors is necessary.

The numerical configuration of the base vectors which rep-resent a context is generally unknown. One approach to compute base vectors is to develop a theory which maps a description of a context to a base vector. Of course, this approach requires further study and an answer will be pro-vided by future research. Alternatively, one can start from the correlation matrix which is involved in document rank-ing and find a base which has led to that correlation matrix. The latter is the approach taken in this paper, under the assumption that such correlation matrix is known or can be estimated.

In the rest of this Section, a methodological framework is illustrated in order to discover the unknown matrix T repre-senting the unknown base T which underlies the generation of a document or of a query within the context represented by T .

The definitions used in the rest of the paper can be found, for example, in [11]  X  a couple of less standard def-initions are here reported. A matrix A is strictly diago-nally dominant when the module of every diagonal element is greater than the sum of the modules of the non-diagonal elements of the same row, that is, if | a ii | &gt; P j 6 = i for all i . A Gerschgorin X  X  circle with center s ii is the set { z  X  R  X | z  X  s ii |  X  r i } , where r i = P j 6 = i s ij Gerschgorin X  X  circle is the circle placed along the real axis, with center at s ii and radius r i .
A matrix is non-singular if, and only if, its determinant is not zero, as well as if, and only if, its columns are indepen-dent.

The determinant of A is the product of its eigenvalues, i.e. det( A ) =  X  1  X  X  X   X  n where  X  i  X  C .

If S = T &gt;  X  T is the correlation matrix of n column vectors, then S is a symmetric matrix. Indeed, S &gt; = ( T &gt;  X  T ) T &gt;  X  ( T &gt; ) &gt; = T &gt;  X  T = S . The fact that S is symmet-ric means that S aims at representing relationships among descriptors t i , t j of which strength measured by s ij is sym-metric, i.e. if descriptor i is related to j , then j is related to i as well and with the same strength s ij = s ji .
The symmetry of S is a useful property; if S is symmetric, then det( S )  X  0. Indeed, it is a fact that det( A  X  B ) = det( A )  X  det( B ) and det( A &gt; ) = det( A ). By combining the latter fact with the symmetry of S , det( S ) = det( T &gt; det( T &gt; )  X  det( T ) = det( T )  X  det( T ) = det( T )
The symmetric matrix S is positive definite if, only if, its determinant det( S ) &gt; 0. Moreover, S = B &gt;  X  B , where B is a non-singular matrix if, and only if, S is positive definite. Also, B is unique. It follows that a one-to-one correspon-dence between S and B exists, by thus assuring that a ma-trix B can be always found, and that that matrix is unique. This fact shall be important under some quite general con-ditions, as illustrated in the following.

A Gerschgorin X  X  theorem states that every eigenvalues of a matrix lies in a Gerschgorin X  X  circle. This fact provides a region in which every eigenvalue is located, and will be a useful property in the following when a class of matri-ces which satisfy that theorem is introduced to implement context discovery.

The determinant of a triangular matrix is the product of the diagonal elements; its inverse is a triangular matrix too. Also, the product of two triangular matrices is a triangular matrix. As it shall be shown, these two facts assures that the manipulation of triangular matrices is performed within the space of triangular matrices, by thus providing advantages at computational level when modeling contexts and describing context changes.
The following simple propositions combine the previously introduced facts and provides the methodological framework to discover the base representing a context:
Proposition 2. Given a symmetric matrix S with inde-pendent column vectors, there exists a non-singular matrix B such that S = B &gt;  X  B and B is unique.
 In fact, let S be a matrix of which column vectors are inde-pendent, therefore det( S ) 6 = 0. Moreover, det( S )  X  0 since S is a symmetric matrix. Therefore, det( S ) &gt; 0. From the latter fact, it follows that the matrix S is positive definite; from the preliminary facts, one concludes that S = B &gt; where B is a non-singular matrix and is unique.

The latter results tell that, if S can be built in a way that independence of the column vectors is preserved, then it is possible to compute one non-singular matrix B . Since S = T &gt;  X  T , the matrix B can be employed as an expression of the unknown matrix T which represents the base T cor-responding to the context of a document or of a query. This way, a method to discover the hidden base which represents a context would be devised.

However, the column vectors of the matrix S does not form in general an independent set, that is, the computa-tion may lead to an instance of S of which determinant is zero by thus making the computation of B impossible. A straightforward example proves this fact; if S = 1 1 1 1 the column vectors are dependent yet S is symmetric.
To avoid dependent sets of column vectors of S , the follow-ing proposition provide a method to define a symmetric ma-trix B with independent column vectors. That method also preserves the order of the measures of relationships among the descriptors of the base represented by the discovered matrix B .

Proposition 3. The column vectors of a diagonally dominant matrix S form an independent set.
 not zero because every circle does not contain 0 and every eigenvalues lies in a Gerschgorin X  X  circle. Since every eigen-value of S is not zero, det( S ) =  X  1  X  X  X   X  n is not zero. Since det( S ) 6 = 0, the column vectors are independent. This way, a method to have a matrix whose column vectors are in-dependent has been defined as follows: Define a diagonally dominant matrix S , thus its column vectors are certainly independent. for k = 1 to n  X  1 do end for Figure 2: Pseudo-code of Cholesky X  X  decomposition algorithm.

To build such a matrix, some data can be used to es-timate S . To estimate S , say, for a document, a couple of approaches are available. For the domain of the docu-ment, ontologies are sometimes available  X  these ontologies give a representation of the domain in terms of classes and specialization hierarchies. Document annotation can be au-tomatically performed by associating document descriptors to classes. Thus, s ij can be computed using, for example, the frequencies of correlation within the classes. Alterna-tively, document text can be segmented into text windows or passages, and descriptor correlation within them can be computed using traditional correlation measures such as co-sine or Dice X  X  coefficient. This way, the element s ij is the measure of correlation of t i , t j in the same segment.
If S were computed by using one of the possible ap-proaches in a way it is not diagonally dominant, then its diagonal elements could be varied until the module is not greater than the sum of the modules of the non-diagonal el-ements. Or, the non-diagonal elements could be equivalently multiplied by an appropriate constant factor.

It is worth noting that the procedure to increase the dis-tance between the s kj  X  X  k 6 = j and the s ii  X  X  is reasonable since the order of the measures of relationship among the descriptors is preserved. Indeed, s ii is a measure of the strength of a relationship between descriptor i with itself. Intuition suggests that the strength of the relationship be-tween i and another descriptor j is less than the one between i and itself. The procedure increases s ii without changing the relative measures of strength among the descriptors be-ing different from i . In other words, the order of the s i 6 = k, j is preserved.
A criteria is then available to estimate S such that its column vectors are independent. There exists an algo-rithm, called Cholesky X  X  decomposition, which computes the matrix B such that S = B &gt;  X  B , as coded in Figure 2. The algorithm returns a triangular matrix with independent columns. Indeed, let S be a diagonally dominant matrix  X  its elements are non-negative without losing of generality; therefore, s ii &gt; 0 for all i . The determinant of a triangular matrix is the product of the diagonal elements; det( B ) &gt; 0 because b kk &gt; 0 for all k by construction. It follows that the column vectors of B are independent. This way, the matrix B can be used as a representation of a vector space base which leads to S . The matrix B describes the context in which the object, e.g. document or query from which S has been computed. The use of triangular matrices as constructs to implement bases has some useful implications summarized by the following
Proposition 4. The context change matrix C is a trian-gular matrix, where A = B  X  C and both A , B are triangular matrices.
 In fact, B  X  1 is a triangular matrix and B  X  1  X  A = C is a triangular matrix, since the product and the inversion of triangular matrices yield triangular matrices. C is not a triangular matrix if A is triangular and B is not.
Triangularity of A , B and C is an important property because leads to useful consequences. First, it is easy to compute thanks to the presence of zeros below the diago-nal. Second, there might be many zeros above the diagonal. Indeed, the object from which S has been computed is de-scribed by a few descriptors out of the set of descriptors used for the collection of objects. Therefore, the matrix S is likely sparse. This way, data structures based on linked lists or trees can be employed to implement efficient algorithms.
Cholesky X  X  decomposition algorithm can be modified so that the columns of the input matrix are normalized to their respective diagonal values. This way, the columns of the re-sulting matrix B are normalized to their norm and fall in the range [  X  1 , +1]. As consequence, the columns of B can be seen as cosine values, by thus displaying a context matrix as a rotation, non-orthogonal matrix. Accordingly, a con-text matrix instructs on how the coefficient vector, which is used to generate a document or a query vector, has to be  X  X otated X  from the position based on the versors to the position in the new coordinate system. Similarly, a context change matrix operates a rotation of current context coor-dinate system to a new one.

It should be noticed that the column vectors of context matrices can also refer to non-textual descriptors  X  the lat-ter being a useful feature for image, sound or video retrieval. Moreover, vector spaces can be in principle of infinite dimen-sions. Also, orthogonality of bases is not compulsory, only independence is required.

Proposition 1 recalls matrix similarity  X  two matrices are similar when represent the same linear transformation. It is a fact that two matrices L , M are similar if and only if M = C  X  L  X  C  X  1 , where L transform vectors generated by T and M transform vectors generated by U . [11] As linear transformations represent many retrieval operations, the latter fact tells us that context change matrices applies to those operations and to operands in a dual way. In this case, duality means that the transformation represented by M of one vector to another in the context represented by T is linearly related to the transformation represented by L of the vectors in the context represented by U . Matrix similarity is then rigorously defined in terms of linear transformation. The fact that properties of similar matrices are invariant w.r.t. similarity leaves room for further developments.
Relevance Feedback (RF) is a form of query context change and a triangular context change matrix can be built. Let us describe RF in the VSM. Provided a query q , the generation of vector q by base T is given by T  X  b . RF computes a new query vector after observing 0  X  r  X  N relevant documents { d 1 , . . . , d r } and of N  X  r non-relevant documents { d r +1 , . . . , d n } . Let q + be the new query and q + be the vector which represents it. By using the classical relevance feedback formulation provided with the VSM, the vector is expressed as: where  X  and  X  are two weights whose values are decided after some training experiments. After substituting docu-ment and query vectors with the corresponding generation formulas: s a matrix notation, one can write b + = b + r  X  s where b = [ b 1 , . . . , b n ] &gt; , r = [ r 1 , . . . , r n ]
Proposition 5. There exists C such that b + = C  X  b such that the column vectors of C form an independent set. Such a matrix can be built as follows: The elements of b are permuted so that the n -th element is not zero. The elements of the other vectors are permuted accordingly. For each k , if b k 6 = 0, then c kk = 1 + ( r k  X  n k ) /b k and the elements of the same row of c kk are set to zero. If b k = 0, then let h be the index of the first non-zero element b h such that h &gt; k ; such an element exists because the b n 6 = 0 after permutation. Then, c kh is set to ( r k  X  n k ) /b h , c kk is set to an arbitrary constant 6 = 0 and the elements of the same row of c kk are set to zero. The result is a triangular matrix C with non-null diagonal elements. Thus, b + k = P n i =1 c ki b i = P n i = k ( b k + r k  X  n k ) if b k 6 = 0, and b + k = b k + ( r k  X  n ( r k  X  n k ) /b h , otherwise.

The s in the diagonal of C correspond to the null elements of b and, therefore, do not affect the computation of b + However, the  X  X  are necessary to make the column vectors independent. Indeed, since 6 = 0 and b k = 0 or c kk 6 = 0 and b 6 = 0 for every k , then det( C ) 6 = 0, because C is triangular, and the column vectors are independent.

Note that if q is generated by T , then q + is generated by U = T  X  C because q + = T  X  b + = ( T  X  C )  X  b = U  X  b . The context is provided by the partition of the collection in the set of relevant documents and its complement. Context change is represented by C which shows that RF equals to re-computing a query in the context of the partition relevant/non-relevant documents. In particular, this context change matrix is obtained by increasing the weight of  X  X osi-tive X  descriptors and by decreasing the one of  X  X egative X  de-scriptors. Every technique, such as query translation-based Cross-Language IR or word sense disambiguation, which is implemented as a transformation of a query to another might be seen as a context change.
Let d 1 , d 2 , d 3 , d 4 be the following four documents d discovery science , d 2 = learning space , d 3 = science of astrophysics , d 4 = learning space , which are de-science , learning , space , computer and astrophysics . Let us suppose that there exists a device providing two correlation matrices S 1 , S 2 corresponding to two contexts, respectively, a context is related to Computer Science, the other is related to Astrophysics: and
The matrices B 1 , B 2 of the bases which represent the two contexts are computed using Cholesky X  X  decomposition: and
Let q be the query space expressed in the context of Computer Science, and let the query coefficients be b = [0 , 0 , 1 , 0 , 0 , 0]. The query vector q 1 = B 1  X  b is [0 , 0 , 0 . 34 , 0 . 94 , 0 , 0] &gt; . If the context were Astrophysics, the query vector would be q 2 = [0 . 17 , 0 . 12 , 0 . 17 , 0 . 96 , 0 , 0] &gt; .
Let us consider the coefficients a 2 = [0 , 0 , 1 , 1 , 0 , 0] generate the vector d 2 of document d 2 : If d 2 ere generated by B 1 , e.g. in the context of Computer Science, the inner generated by B 2 , instead, the inner product would be 1 . 30. The difference is due the different context used to generate the document vector. Something similar would happen if B 2 were used to generate q instead  X  in that case, a &gt; 2  X  B b = 1 . 17; the inner product would be even smaller if the set of versors were used to generate q . It is useful noting that the use of B 1 is not saying that the context of Computer Science is more relevant than the context of Astrophysics, but only that d 2 is closer to q 1 if it is generated by B
The explanation for the different outcomes stems from the strength of relationship between space and learning , which is greater in the context of Computer Science than in the context of Astrophysics, as showed by S 1 and S 2 . This fact can be also seen by looking at the base vector t 4 of B 1 and B 2 . In the former base, t 4 shows that space , e.g. t is  X  X xplained X  by learning with 0 . 33 in the context of Computer Science, whereas it is  X  X xplained X  by the other descriptors with lower values in the context of Astrophysics. Let us suppose that is the correlation matrix that describes the relationships among the descriptors in the context of Distance Learning which generated d 2 ; it can be noted the non-positive correla-tion between learning , space and the other descriptors, in order to implement the contrast between Distance Learning and Computer Science. The base is given by
If B 1 generates the vector q 0 that represents the query q = discovery science space computer , the inner product d between two contexts leads to a larger distance between the document and the query vectors; if B 0 had generated d 3 as well, the inner product would have been 2, i.e., the distance would have been noticeably reduced.

In this illustrative example, descriptor correlations are in-strumentally exploited to explain how different bases can lead to different rankings. The example is based on the method to discover context from correlation matrices, as long as some devices have computed such matrices. The reader ought not stray too far away from that correlation matrices is not the unique way to express descriptor seman-tics in context  X  the exposed example wanted to be only a possible approach. Our contention is that context dis-covery can be performed by using other approaches. Al-ternative approaches can be hinged on the definition of the bases represented by matrices like B by using some theory which directly maps context to the scalars of the base vec-tors. Therefore, it is thought the framework proposed in this paper is far more general than that the example illustrated in this Section would lead to think.
In this paper it is argued that the notion of context can be embodied in vector spaces. A base of a vector space can operate at this aim by modeling context directly. The correspondence between Relevance Feedback, which is an in-stance of context change, and linear transformation between bases witness the potentialities of the proposed approach. The major strength of this proposal derives from the fact that it naturally employ a sound mathematical framework.
A method to compute a vector space base which repre-sents a context has also been devised in this paper. The method needs only that the correlation matrix is diagonally dominant  X  the latter is a sufficient condition, but it is not necessary. Further study will be devoted to find other meth-ods, but the triangularity of the computed base matrices is a useful property which makes computation and interpreta-tion easier than non-triangular matrices.

What makes the proposal in this paper different from the previous work is the realization that each document or query can be associated to a distinct base, which corresponds to one context. This way, document ranking can take advan-tage of the diversity of contexts. The  X  X ersonalization X  of the base distinguishes the proposal from LSI which aimed at computing latent descriptors by assuming one base. The de-velopments provide a proposal not only for the computation of a measure of relevance in case of context-aware search, but also for the incorporation of context into the model.
Some issues are not pertaining to this paper and shall be addressed in the future research: The computational issues are still unexplored and need further investigation given the quadratic nature of the data. The mechanism that maps the data observed about a context to the corresponding vector space base has been left out of this discussion, yet a method to discover the base has been devised. The role of rotators played by context (change) matrices opens a direction for further study. The dependence of matrix B on the method used to compute S leads to different bases even for the same document; this issue will be also explored. The impact of matrix similarity will be also studied. I am grateful to the anonymous reviewers for the valuable comments and suggestions.
