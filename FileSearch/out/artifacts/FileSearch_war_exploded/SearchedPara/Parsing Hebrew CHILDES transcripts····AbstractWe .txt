 Shai Gretz  X  Alon Itai  X  Brian MacWhinney  X  Bracha Nir  X  Shuly Wintner Abstract We present a syntactic parser of (transcripts of) spoken Hebrew: a dependency parser of the Hebrew CHILDES database. CHILDES is a corpus of child X  X dult linguistic interactions. Its Hebrew section has recently been morpho-logically analyzed and disambiguated, paving the way for syntactic annotation. This paper describes a novel annotation scheme of dependency relations reflecting constructions of child and child-directed Hebrew utterances. A subset of the corpus was annotated with dependency relations according to this scheme, and was used to train two parsers (MaltParser and MEGRASP) with which the rest of the data were parsed. The adequacy of the annotation scheme to the CHILDES data is established through numerous evaluation scenarios. The paper also discusses different anno-tation approaches to several linguistic phenomena, as well as the contribution of morphological features to the accuracy of parsing.
 Keywords Parsing  X  Dependency grammar  X  Child language  X  Syntactic annotation 1 Introduction Child X  X dult interactions are a basic infrastructure for psycholinguistic investigations of child language acquisition and development. The corpora available through the CHILDES database (MacWhinney 2000 ), consisting of spoken transcripts in over twenty five languages, have been an indispensable source for this line of research with raw data from monologic, dyadic, and multi-party interactions (all following a unified and established transcription scheme) but also with tools for the application of theoretically-motivated and well-tested analyses. The most developed feature of the system is the MOR program, an automatic Part-of-Speech tagger with functionality in thirteen varied languages X  X ncluding Cantonese, Japanese, and Hebrew (in addition to Romance and Germanic languages). More recent work has focused on the automatic syntactic parsing of these data, most notably with the parser that was developed by Sagae et al. ( 2010 ) for the English section of CHILDES.

The current paper reports on a similar endeavor, focusing on automatic syntactic parsing of (a subset of) the Hebrew section of the CHILDES database. This subset includes two corpora with full, reliable, and disambiguated Part-of-Speech and morphological annotation (Albert et al. 2014 ): the Berman longitudinal corpus (Berman and Weissenborn 1991 ) and the Ravid longitudinal corpus. Each of these corpora includes naturalistic data collected on either a weekly or a monthly basis from six Hebrew-speaking children and their care-takers, yielding approximately 110,000 utterances in total (Nir et al. 2010 ).

Similarly to the syntactic structure that Sagae et al. ( 2010 ) induce on the English section of CHILDES, the parser for Hebrew makes use of dependency relations , connecting the surface tokens in the utterance through binary, asymmetric relations of head and dependent . In this we join a growing body of automatic parsers that rely on dependency-based syntactic representations (Ku  X  bler et al. 2009 ). These representations are particularly adequate for the annotation of Hebrew child X  X dult interactions for a number of reasons. Hebrew is a language with relatively complex morphology and flexible constituent structure; morphologically-rich languages such as Arabic have been successfully analyzed using a dependency-based parser (Hajic  X  and Zema  X  nek 2004 ). More importantly, a dependency-based parser relies on the explicit specification of dependencies (i.e., the name of the functional relation), which provides a representation of the relations assumed to be learned by the child, unlike parsers based on constituent structure, in which such information is implicit and requires further derivation (Ninio 2013 ). Finally, this choice makes our annotation more consistent with the syntactic structure of the English section of CHILDES, which may be useful for cross-linguistic investigations.

This work makes several contributions. First, we defined the first dependency annotation scheme for spoken Hebrew (Sect. 4 ). Second, we developed a parser for the Hebrew section of the CHILDES database, annotating utterances with syntactic dependency relations (Sect. 5 ). The parsed corpus will be instrumental to researchers interested in spoken Hebrew, language acquisition and related fields. We evaluated our parser in different scenarios (Sect. 6 ); the results demonstrate that the parser is both accurate and robust. Furthermore, we experimented with alternative annotation methods of several constructions, as well as with different approaches to tokenization of the input and with different sets of morphological features (Sect. 7 ), showing the impact of such choices on the accuracy of parsing. The parser, the manually annotated corpus, and the automatically parsed corpora are all available for download from the main CHILDES repository. 2 Related work To the best of our knowledge, the only parser of Hebrew was introduced by Goldberg ( 2011 ). It focuses on written Hebrew and has two versions, one that produces constituent structures and one that generates dependency relations. The scheme presented by Goldberg ( 2011 ) was generated from the original format of a constituent structure treebank (Sima X  X n et al. 2001 ; Goldberg and Elhadad 2009 ), a relatively small corpus that includes around 6,200 sentences taken from an Israeli daily newspaper.

Several features set our work apart from Goldberg ( 2011 ): first, we focus on spoken , colloquial Hebrew, rather than on written, journalistic language. In particular, our corpus includes samples produced by adult, non-expert speakers as well as by children who are still acquiring their grammar. The study of spoken language requires dealing with utterance complexity at varying levels. On the one hand, spoken language implies short utterances [as witnessed by Sagae et al. ( 2010 )] that are thus generally simpler and easier to parse. On the other hand, utterances are partial and lack any standard formal structure, and some may be ungrammatical (in the traditional sense), especially those produced by children. Spoken language is also characterized by repetitions, repairs, and interruptions within the same utterance and even across utterances. Furthermore, a single utterance may contain more than one clause. These characteristics raise issues that to date have not been addressed in the automatic syntactic analysis of Hebrew.
 Second, the parser described by Goldberg ( 2011 ) relies on data represented via Hebrew orthography. Hebrew orthography is ambiguous, since vowels are represented only partially (if at all), so that many words are homographic (Wintner 2004 ). Our transcription explicitly encodes vowels as well as other phonological information that is highly relevant for ambiguity resolution, such as lexical stress between similar-sounding phonemes that are written differently in the standard Hebrew script, e.g.,  X   X  X et X  and t  X  X av X , or k  X  X af X  and q  X  X uf X . This approach significantly reduces morphological ambiguity, and renders moot some of the problems that restrict the capabilities of Goldberg X  X  parser. Moreover, functional elements (the definite article, prepositions, and conjunctions) that are concatenated with the subsequent token in the standard orthography are separated in our transcription; e.g., we-ha-yeled  X  X nd the child X . This allows us to make sure that each lexeme is treated separately and consistently at the lexical, morphological, and syntactic levels. In addition, multi-word expressions are systematically transcribed night X , etc. Consequently, the level of morphological ambiguity of each transcribed token is very low. Any remaining ambiguity is subsequently resolved in an automatic process (akin to part-of-speech tagging), so that the input for parsing is completely disambiguated. Albert et al. ( 2014 ) show that the automatic morpho-logical disambiguation module achieves an accuracy of 96.6 %. This is in contrast to written Hebrew which suffers from high morphological ambiguity, and for which automatic morphological disambiguation is less accurate (Lembersky et al. 2014 ).
Furthermore, Goldberg X  X  scheme itself is partial, i.e., not all relations existing in the data are labeled. In contrast, the scheme we developed provides fully annotated data (Sect. 4 ). As such, our work is similar in spirit to Sagae et al. ( 2010 ), who, motivated by the same goals, constructed a parser for the English section of the CHILDES corpus. Sagae et al. ( 2010 ) developed a scheme of 37 distinct grammatical relations which was used for annotating the Eve corpus (Brown 1973 ). The annotation included both manual and automatic analyses of 18,863 utterances, 10,280 adult and 8,563 child. During the annotation procedure, the corpus was used for training a data-driven parser which was then tested on an independent corpus of child X  X dult interactions. Cross-validation evaluation of the parser X  X  performance showed a low error rate, of between 6 and 8 %. Both the English and the Hebrew data sets follow the CHAT transcription guidelines (MacWhinney 2000 ), and attempt to reflect the flow of the conversation as accurately as possible. The transcription standard also marks tokens that should be ignored by morphological and syntactic analyses, for example in the case of false starts or repetitions. 1
Tsarfaty et al. ( 2012 ) also address Hebrew; they are concerned with joint evaluation of morphological segmentation and syntactic parsing, specifically in morphologically rich languages such as Hebrew. The motivation is that standard evaluation metrics for syntactic parsing do not take into account the underlying morphological segmentation that may produce errors which do not reflect actual parsing errors. Their proposed method allows for precise quantification of performance gaps between the use of gold morphological segmentation and that of predicted (non-gold) morphological segmentation. This work is representative of an emerging research program whose main goal is to develop techniques for parsing morphologically-rich languages (Tsarfaty et al. 2010 , 2013 ; Seddah et al. 2011 ).
While this line of works is of course relevant to Hebrew parsing in general, it does not seem to be as relevant to parsing CHILDES data. The transcription we use (Sect. 3.2 ) allows high accuracy of morphological segmentation and part-of-speech tagging, as many of the ambiguity issues that normally arise in processing Hebrew are already resolved. In addition, for evaluation purposes we manually tag all remaining morphological ambiguity, rendering the morphological analysis disam-biguated. Thus, our data do not reflect the rich morphology and high level of ambiguity of standardly written Hebrew, which motivate such a joint evaluation metric.
 For parsing (and evaluation) we compare MaltParser (Nivre et al. 2006 ) and MEGRASP (Sagae and Tsujii 2007 ). MaltParser is an architecture of transition-based parsers that can support various learning and parsing algorithms, each accompanied with its feature set and parameters, which can be directly optimized. The feature set is derived from the surface forms, the base forms and the morphological information of a subset of the tokens in the data structures (i.e., the queue and the stack) that comprise the state of the parser.
 Marton et al. ( 2013 ) evaluate the effect of morphological features on parsing Arabic, a sister-language to Hebrew, using MaltParser. This study shows that the most important features for parsing written Arabic utterances are  X  X ase X  and  X  X tate X , while  X  X ender X ,  X  X umber X  and  X  X erson X  did not improve the accuracy of parsing Arabic when gold morphological information was present. Case is irrelevant for Hebrew; the state of nouns is, but construct state nouns are not common in spoken language, and in our corpus in particular. However,  X  X ender X ,  X  X umber X  and  X  X erson X  may prove to be more helpful when the training set is enhanced with corpora which are automatically annotated for morphological features. We explore this direction in Sect. 6.5 .
 MEGRASP was chosen because it was used for dependency parsing of the English section of CHILDES (Sagae et al. 2010 ). The algorithm is a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie ( 2006 ). The parser is compatible with the format of the CHILDES database since CHILDES syntactic annotations are represented as labeled dependencies. 3 Characteristics of the Hebrew CHILDES corpus 3.1 Representation The corpora in the CHILDES database include three levels of data, each referred to as a tier (MacWhinney 2000 ). The main tier contains (a transcription of) actual utterances; the mor tier contains disambiguated lexical and morphological analyses of the utterances; and the gra tier lists the syntactic analyses of the utterances. The data are organized in a one-to-one format, where every token in the main tier has exactly one counterpart in the mor tier and the gra tier. 2
In the CHILDES database and throughout this work, dependency structures are represented linearly as triplets i | j | REL , where i is the index of a token in the Root marker is given the index 0), and REL is the label of the relation between them.
Example 1 shows an utterance with all three tiers. The elements in each of the three tiers are vertically aligned. The utterance consists of two tokens,  X  at  X  X ou X  and mes  X  aq  X  ret  X  X ie X . Below each token, the mor tier specifies the morphological analysis. The first token is analyzed as a personal pronoun ( pro:pers ) of feminine gender and singular number; the second token is a participle ( part ), again in feminine singular. The third line specifies the gra tier: the first token is a dependent of the second, and the dependency relation is Aagr (agreeing argument, Sect. 4 ); and the second token is the root, depending on the special index 0. For convenience, we also add a fourth line, with a full English translation of the utterance. (1) 3.2 Tokenization Tokenization is a crucial aspect of data preparation required for successful syntactic analysis, particularly for morphologically complex languages. Hebrew, which tends to be synthetic, is rich in bound morphology, and tokens that consist of more than a single morpheme are quite common. One such example is the class of simple article, as in the case of ba- X  X n the X . Two other common examples are complex prepositions with suffixed personal pronouns, for example, bis  X  vil  X   X   X  X or you X , and nouns that systematically take pronominal pronouns as bound inflectional suffixes marking possession, for example,  X  axot  X   X  X y sister X . These cases make it difficult to determine how many morphemes are to be counted in the analysis of one token (Dromi and Berman 1982 ).

Previous studies on dependency-based parsers for other morphologically complex languages have considered similar issues, as in the case of the Prague Arabic Dependency Treebank (Hajic  X  and Zema  X  nek 2004 ). Arabic, even more than Hebrew, is rich in bound, inflected morphemes, not only in the nominal system (as in the case of nouns with possessive suffixes) but also in the verbal system, where verbs can take accusative suffixes. The morphological analysis of the Prague Arabic Treebank is based on the idea of splitting words into morphs or segments , each receiving its own morphological analysis (Smrz  X  and Pajas 2004 ). Thus, prepositions and suffixed pronouns are considered separate tokens (e.g., biduni  X  X ithout me X  is split into bi-duni ), even if they are orthographically attached in Modern Standard Arabic script. Nouns with possessive suffixes are also split into two tokens (the noun and the pronominal suffix).

A similar approach is presented by the dependency-based parser for Turkish are specified between morphemes (referred to as in fl ectional groups or IGs) rather than between words. This morpheme-driven analysis resulted in more accurate parsing. However, since the mechanisms for bound morphology are much less productive in Hebrew compared to Arabic (let alone the agglutinative Turkish), we resorted to the following solution: the data were pre-processed and particular tokens were split to allow for a partial morpheme-bound representation. Thus, definite prepositions such as ba- X  X n the X  are split to two tokens, as in be- X  X n X  ha- X  X he X ; prepositions fused with personal pronouns are treated similarly, for example, bis  X  possession; we split such cases to three morphemes: for example,  X  axoti  X   X  X y sister X  is split into  X  ax  X  t  X  X ister X  s  X  el  X  X f X   X  ani  X   X  X  X .

Our motivation is largely computational rather than linguistic; presumably, such a split representation reduces data sparseness. Certain fused forms occur only rarely, possibly making it more difficult for the parser to identify and analyze them correctly. However, these changes are only aimed at improving syntactic analysis. In order to avoid any theoretically controversial decision as well as more practical implications, for example to the analysis of Mean Length of Utterance (Dromi and Berman 1982 ), we merge the split tokens back together once parsing is complete, omitting any intra-word relations. Unsurprisingly, with only a single exception, none of the split morphemes is involved in non-local dependency relations, so this merge operation is well-defined. 4 Annotation scheme for Hebrew CHILDES As noted above, our scheme is inspired by the grammatical relations defined for the annotation of the English section of CHILDES (Sagae et al. 2010 ). This is done mostly for issues that are not language-specific but rather represent general characteristics of spoken language, such as repetitions and repairs, on the one hand, and the inclusion of vocatives and communicative elements, on the other. In addition, some of the relations that were defined similarly in the two schemes relate to specific features of child X  X dult interactions, including onomatopoeias, enumer-ations, serialization of verbs, and topicalizations.

Moreover, our scheme remained consistent with the issue of how to treat coordinate constructions, a relation that poses challenges for dependency annotation in general. In adopting the English scheme, the coordinating conjunction was defined as the head of a coordination construction and the coordinated elements were defined as the dependents. This relation is labeled Coord . Also, we took into consideration the work of Goldberg ( 2011 ) on dependency parsing of written Hebrew, specifically in Sect. 7 where we evaluate alternative approaches for specific relations.

In contrast to the English scheme, we distinguish between three types of dependents for Hebrew: arguments [A], modifiers [M] and others. Arguments are dependents that are typically semantically required by the head, their properties are determined by the head, and they can occur at most once (often, exactly once). Modi fi ers , on the other hand, are non-obligatory dependents: typically, they select the head that they depend on and, consequently, they may occur zero or more times. The Others group includes relations in which the dependents are neither arguments nor modifiers of the heads, or relations in which the dependents do not relate specifically to any other token in the utterance. For example, the Com label marks the relation in which a communicator is the dependent. A communicator is generally related to the entire utterance, and so we mark the root of the utterance as the head of the communicator. The Others group also contains relations for two special cases where we present two optional analyses for a construction: the copula construction (Sect. 7.1 ) and constructions containing the accusative marker (Sect. 7.2 ). The second approach for both of these linguistic issues is marked with a relation whose name starts with X .

Typically in dependency-based parsers, the root of an utterance is an inflected verb or a copula in verbless copula utterances, carrying the tense marking in the clause. In utterances with no verb and no copula, where there is no element carrying a tense, the head is the predicating element. Copulas and existential markers, as well as other forms of the verb hay  X   X  X e X , are discussed elaborately in Sect. 7.1 When an utterance is lacking any of the above, the root is the element on which the other elements depend (such as the noun with respect to its modifiers). In single word utterances, the single token is by default the root.

The annotation scheme is comprised of 24 basic dependency relations and a few more complex dependency relations (combinations of two basic dependency relations; see Sect. 4.4 ). The complete list of the basic dependency relations is given in Appendix 1 . We discuss below some of the main constructions covered by our scheme. 4.1 Verb arguments, agreeing and non-agreeing Two main relations are defined between verbs and their arguments. One relation, Aagr , requires the verb and its argument to agree; at most one argument can stand in this relation with any given verb. The other relation, Anonagr , imposes no agreement constraints, and the number of such arguments can be zero or more. 3
In Example 2, the verb roc  X   X  X ant X  is the head of its agreeing argument  X  an  X   X  X  X  plural noun). (2)
Other non-agreeing relations include the relation between the verb and its indirect (or oblique ) objects, where the nominal element is preceded by a preposition. The Anonagr dependency is marked on the prepositional element and the nominal element is marked as the argument of that preposition, Aprep ; see Example 3. An alternative representation of prepositional phrases is discussed in Sect. 7.3 (3) Non-agreeing arguments can also occur as finite clausal dependents of the verb. In such cases, the subordinating conjunction is marked as Anonagr. In addition, it is treated as the head of the subordinate clause, and the finite verb of the subordinate clause is dependent on it in a SubCl relation, as in Example 4. (4)
When the subordinate clause is introduced by something other than a subordinating conjunction, the finite verb of the clause is directly dependent on the finite verb of the main clause, again in a Anonagr relation, such as in Example 5, where the verb qar  X   X  X appen X  (the head of the subordinate clause) is directly dependent on the matrix verb tir  X   X   X  X ook X . (5)
When the non-agreeing argument is an infinitival verb phrase, the relation between the head of the verb phrase and its (verbal or nominal) head is Ainf ; see Example 6. (6) 4.2 Modifiers Modification in Hebrew may occur for both nouns and verbs. Several relations specify nominal modifiers; these include Mdet for the relation between a determiner and a noun, Mquant for quantifiers, and Madj for adjectival modification. Another type of nominal modification is represented in noun-noun compounds, which in Hebrew are constructed by combining a morphologically-marked noun (said to be in the construct state) with another noun (recall that when such compunds are idiomatic they are represented as a single token). We mark the relation between the two nouns as Mnoun , as in Example 7. (7)
Verbal modifiers include Madv for adverbs (Example 8) and Mneg for negation (Example 2), as well as Mpre for prepositional phrase modifiers (Example 9). (8) (9)
When a subordinate clause is a modi fi er (rather than an argument) of a verb or a noun, the relation between the verb or noun and the subordinating conjunction is labeled Msub . If the clause is a relative clause, the relation between the relativizer and the head of the relative clause is labeled RelCl , as in Example 10. (10) 4.3 Other relations Vocatives are named entities that refer to another speaker in the conversation, most commonly followed by a question or request in the second person. Vocatives depend on the root of the utterance in a Voc relation (Example 11). (11)
Communicators include discourse markers such as  X  av  X  l  X  X ut X ,  X  az  X  X o X , ken  X  X es X , etc., as well as verbs such as tir  X   X   X  X ook X  and b  X   X  i  X  X ome_here X . Like Voc , the root of the utterance is the head of the relation and the communicator is the dependent. The main difference between the two relations is that Com does not include named entities. See Examples 5, 6.

The relation Coord specifies coordination, relating between conjuncts and conjunctions, most commonly we- X  X nd X . As noted above, we follow Sagae et al. ( 2010 ) in dealing with these constructions: the head is the coordinating conjunction and the dependents are the conjuncts. If there are two or more conjuncts with multiple coordinators, the coordinators are linked from left to right (the rightmost coordinator is the head of the others) by a Coord relation. In the absence of a coordinator the rightmost conjunct is the head of the relation. See Example 12. (12) 4.4 Elision relations Spoken language often includes missing elements, whether as a result of true ellipsis or of interruptions and incomplete utterances. In the English section of CHILDES, Sagae et al. ( 2010 ) decided to mark missing elements as elided and to relate to them in the analysis using elision relations . Such relations combine two basic relations: one between the elided element and its presumed head, and one between the elided element and its dependent. Following the scheme for English, we also mark missing elements with elision relations.

In Example 13, ha- X  X he X  is marked with the Mdet-Aprep relation. Mdet stands for the relation between ha- X  X he X  and a missing element, presumably a noun; Aprep stands for the relation that would have held between the missing noun and the preposition ley  X  d  X  X ear X . (13) 4.5 Child invented language As the CHILDES corpus is comprised of child and adult interactions, child-specific forms and constructions are rather frequent. These include neologisms, babbling, and incoherent speech. Such forms can be detached from the utterance, labeled with the Unk relation which marks unknown relations (Example 14); or, when the syntactic function of such forms is known to the annotator, they can take the place of a known relation (e.g., the neologism bdibiyabi in Example 15). (14) (15) 5 Methodology 5.1 Parsing We manually annotated a subset of the Hebrew CHILDES corpus described in Sect. 3 according to the schema of Sect. 4 . The data were annotated by two lexicographers; all disagreements were resolved by a third annotator, a linguist who specializes in syntactic analysis.

This manually annotated corpus consists of 12 files: 8 files from the Ravid corpus and 4 from the Berman corpus. The 8 files of the Ravid corpus contain transcriptions of the same child at different ages (ranging from 1;11 to 2;05). The 4 files of the Berman corpus reflect 4 different children (all different from the child in the Ravid corpus) at different ages (2;04, 3;00, 3;03 and 3;06). Statistical data of the corpora are given in Table 1 . The data presented here refer to the corpora after splitting fused morphemes (Sect. 3.2 ) and exclude punctuation.
 We then trained two dependency parsers on the manually-annotated texts, MEGRASP (Sagae and Tsujii 2007 ) and MaltParser (Nivre et al. 2006 ). MEGRASP works directly on the CHILDES format in which the corpora are stored. MaltParser supports a number of formats, including the CoNLL-X shared task format (Nivre et al. 2007 ). An advantage of using MaltParser is that it also supports costume-made formats, allowing variation in the lexical and morphological information available for the learning algorithm. We used a format similar to CoNLL, but added columns to represent independent morphological attributes (instead of the concatenated FEATS column). Using MaltParser, we examined the effect of adding morpholog-ical features (e.g., number and person) to the default feature set (Sect. 6.5 ).
To achieve the best possible results using MaltParser we used the recently developed MaltOptimizer (Ballesteros and Nivre 2012 ). MaltOptimizer analyzes the training data in a three-phase process and outputs the recommended configuration under which to run MaltParser (e.g., a certain parsing algorithm or a feature set that yield the best results). Since MaltOptimizer is restricted to the CoNLL format and does not support custom formats, we used it as follows. We concatenated the morphological features into the FEATS column, to adapt the input to the CoNLL format. We ran this version of the parser with MaltOptimizer, and converted the files back to our custom format as suggested by MaltOptimizer. For example, MaltParser supports a Split function that splits the values of a certain column according to a delimiter. If MaltOptimizer suggested to split the FEATS column, we did so by placing the morphological information in the separate morphological columns. In the following sections, there is practically no difference between using our format and the CoNLL format. The main difference is when we evaluated the contribution of the morphological data to parsing (Sect. 6.5 ); there we examined various kinds of subsets of features, not all of which are supported by the regular CoNLL format. 5.2 Evaluation We conducted both In-domain evaluation , where training is on parts of the Ravid corpus and testing is on other parts of the same corpus (held out during training), and Out-of-domain evaluation , where training is done on the files of the Ravid corpus and testing is done on the files of the Berman corpus. We did not explore any domain adaptation techniques (Nivre et al. 2007 ; Plank 2011 ), we merely evaluated the robustness of the parser when tested on a different domain. We ran both MEGRASP and MaltParser on these evaluation scenarios. We also ran a fivefold cross-validation on the Ravid corpus and on both corpora combined.

The evaluation metrics that we used are unlabeled attachment score (UAS) and labeled attachment score (LAS). In UAS a token is considered correctly annotated if its head is the same head that is marked in the gold-standard X  X egardless of the grammatical relation. In LAS a token is considered correctly annotated if both the head and the grammatical relation are the same as in the gold-standard. In addition we report Exact Match (EXM), the percentage of utterances that are parsed without any errors. These are standard metrics in the evaluation of dependency parsing (Ku  X  bler et al. 2009 ).

To examine the quality of the parsers and the annotation scheme on individual relations, we used further metrics that are relation specific X  URecall r (unlabeled recall), LRecall r (labeled recall), UPrecision r (unlabeled precision) and LPrecision r head that token x is attached to in the gold file and h ( x ) the head that token x is attached to by the parser. Then:
The first two metrics are refinements of the recall metric for each relation as the analysis is with respect to the appearances of the relation in the gold standard files. The Recall r measures compute the percentage of tokens labeled r in the gold data that were correctly parsed. The other two metrics are refinements of the precision metric for each relation as the analysis is with respect to the appearances of the relation in the parsed model files. The Precision r measures compute the percentage of tokens labeled r in the parsed data that were correctly parsed. For each of the UAS and LAS precision and recall pairs we report also the (balanced) F -score, the harmonic mean of precision and recall.

In addition to testing the corpus as a whole we show results that relate separately to two types of data: child-directed speech (CDS) and child speech (CS). First, we trained and tested on both types of data (All X  X ll); to investigate whether children learn primarily from the language spoken to them or from their peers, we trained on CDS and tested on CS (CDS X  X S), and then trained and tested on CS only (CS X  X S); for completion, we also trained and tested on CDS (CDS X  X DS).

In the in-domain evaluation scenario we built the training set and test set for each of these configurations separately, using 8 files of the Ravid corpus. The files of the Ravid corpus are chronologically ordered by the age of the target child and thus in the in-domain evaluation scenario the held-out set always contains utterances of the same child at an older age. In this configuration, the training set is comprised of 80 % of the utterances of the relevant data type in the corpus, holding out 20 % for the test set. The training set of the All X  X ll configuration contains 3,286 utterances (11,155 tokens), the CS training set contains 1,237 utterances (3,246 tokens) and the CDS training set contains 2,066 utterances (7,946 tokens). The 80 % of CS and CDS were derived from the set of utterances (of their respective data types) in the corpus, and not from the training set of both data types. Consequently, the sum of the sizes of the CS and CDS training sets does not necessarily equal the size of the training set of the All X  X ll configuration. In the CDS X  X S configuration the training set and test set are comprised of utterances of different data types so the entire set of utterances of each data type in the corpus was used, and not just 80 % of it.
In the out-of-domain evaluation scenario the training sets and test sets of the different configurations were taken from different sets of files, so the entire set of utterances of the respective data type was used. For all evaluation scenarios, we excluded punctuation and single-token utterances, to avoid artificial inflation of scores. 6 Results 6.1 In-domain evaluation We first present the results for the in-domain scenario. Recall that we ran MaltOptimizer in order to achieve the best parser configuration with respect to the training set. In the All X  X ll configuration, according to MaltOptimizer, the training set contained approximately 3.9 % utterances with non-projective trees. 4 MaltOptimizer recommended using the Stack parsing algorithm in its non-projective eager version (Nivre 2009 ; Nivre et al. 2009 ). See Appendix 2.1 for a full description of the parameters chosen by the optimizer.

Table 2 shows the accuracy of parsing obtained by both parsers, in all four evaluation scenarios. Considering the relatively small training set, both parsers achieve reasonable results. Evidently, MaltParser proves to be better than MEGRASP on this domain. The difference in the All X  X ll configuration is statistically significant for all three metrics ( p \ 0.05).

To show the contribution of MaltOptimizer, we also ran MaltParser with its default parameters, which allows only projective dependency trees. The settings of the default parsing algorithm are discussed in Appendix 2.2 . In the All X  X ll configuration, the UAS was 84.5 and the LAS was 80.5 X  X ower than the results obtained by both the optimized MaltParser and MEGRASP. Thus, the adaptation of the parsing algorithm and feature set to our corpora using MaltOptimizer was clearly instrumental for improving parsing accuracy.

In general, the Exact Match accuracy is high, mostly due to the relatively short length of the utterances in our corpora. It is interesting to compare these results to Exact Match results of other tasks. In the CoNLL-X shared task (Nivre et al. 2007 ), different parsers were evaluated for token-based measures, such as LAS and UAS, by parsing 13 test-sets of various languages. Ballesteros et al. ( 2012 ) expanded this evaluation of parsers by calculating not only token-based but also sentence-based measures, such as Exact Match. They also drew a correlation between average sentence length and Exact Match accuracy. The test-set of Arabic had the highest average sentence length (37.2 tokens) and the lowest Exact Match score (9.6 with MaltParser, 6.2 averaged across parsers). On the other hand, the test-sets of Chinese and Japanese had the shortest average sentence length (5.9 and 8.9 tokens, respectively) and the highest Exact Match scores (68.1 with MaltParser and 49.5 averaged across parsers for Chinese, 75.3 with MaltParser and 59.6 averaged across parsers for Japanese). These results are in accordance with our results, as both the Ravid and Berman corpora exhibit a short average utterance length and high Exact Match scores, arising from the fact that they reflect adult-child interactions at early stages of language development.

Note also the low EXM when testing on CDS as opposed to the high EXM when testing on CS. Recall that the utterances in CDS are longer on average (Table 1 ) and so there is a higher chance that one of the tokens in an utterance is tagged erroneously.

To see which relations are more difficult for the parsers to predict, we evaluated the accuracy of the parsers on specific relations. Table 3 shows the relation-specific metrics for interesting individual relations, in the All X  X ll configuration. Relations that occur with a small set of tokens as dependents (such as Mdet , where the dependent is mainly the token ha- X  X he X ), or after a specific type of token (such as Aprep , occurring after a preposition) achieved a score of 97 % or above in all the four metrics. The frequent relations Aagr and Root reached high scores of over 92 % unlabeled recall and precision, 89 % labeled. Also accurate were the relations Mneg and Aexs . The more problematic relations were Com and Voc and modifiers such as Madv , Mquant and Mpre , which can sometimes be ambiguous even for human annotators. Amongst the modifiers the labeled scores of Mpre were especially low, due to the confusion between it and Anonagr when deciding whether a preposition is an argument or a modifier of a verb, in certain cases a decision that could be hard for a human annotator.
 Figure 1 shows the learning curves of MEGRASP and MaltParser on this task. We trained the parsers on an increasingly larger training set, from 400 utterances up to 3,200 utterances with increments of 400, and tested on a fixed test set of 590 utterances (2,474 tokens) in the All X  X ll configuration. We plotted UAS and LAS scores as a function of the number of utterances in the training set. The curves suggest that more training data could further improve the accuracy of the parser. 6.2 Out-of-domain evaluation We also evaluated the parsers on a different domain than the one they were trained on. For the All X  X ll configuration, according to MaltOptimizer, the training set contained approximately 3.8 % utterances with non-projective trees. Similarly to the in-domain scenario, MaltOptimizer suggested the Stack algorithm (Nivre 2009 ; Nivre et al. 2009 ), but in contrast to the in-domain scenario, it recommended the Stack non-projective version. This algorithm postpones the SWAP transition of the Stack algorithm as much as possible. The parameters selected for this configuration are discussed in Appendix 2.3 .

We trained the parsers on the 8 files of the Ravid corpus and tested on the 4 files of the Berman corpus. Table 4 lists the results.

Unsurprisingly, the accuracy of the parser in the out-of-domain evaluation scenario is considerably lower than in the in-domain evaluation scenario. The decrease in accuracy when parsing the CS data type can be explained by the fact that the test set of the Berman corpus contains utterances by four different children, all different from the child who is recorded in the training set. They are also children of different ages, and three of the four children in the test set are recorded at an older age than the child in the training set.

Another point to notice is that also in this scenario, MaltParser performed better than MEGRASP, but the differences between the parsers are slightly smaller in some metrics than in the in-domain evaluation scenario. One possible explanation is that MaltParser was run with optimized parameters as suggested by MaltOptimizer (e.g., parsing algorithm and feature set) that are configured according to the training set. In the out-of-domain evaluation scenario the differences in the types of utterances between the training set and the test set are more substantial than in the in-domain evaluation scenario. As a result the optimized parameters are less effective and hence the accuracy is poorer. Still, the advantage of MaltParser over MEGRASP in the All X  All configuration is significant for all three metrics ( p \ 0.05).

As in the in-domain evaluation scenario, we present a learning curve of the parsers when parsing the same out-of-domain dataset on training sets varying in size (Fig. 2 ). The size of the test set is 1,614 utterances (8,750 tokens). Here, too, the learning curves of both parsers suggest that there is room for improvement with more training data. 6.3 Learning from child-directed speech versus child speech Is it better to train the parser on child speech or on child directed speech? The in-domain and out-of-domain tests yield conflicting evidence. The in-domain data suggest that for parsing child speech it is better to learn from child speech than from child-directed speech. This is despite the fact that in the CDS X  X S configuration the training set is larger.

To examine the possibility that the specific CS test set used in both configurations contributes to this difference, we evaluated the CDS X  X S configuration with a training set similar in size to the CS X  X S training size (i.e., 1,237 utterances) and with an identical test set to the one used in the CS X  X S configuration. Table 5 shows the results of the modified CDS X  X S evaluation (line 2) compared to the CS X  X S evaluation (line 1) and the original CDS X  X S evaluation (line 3).

When running the modified CDS X  X S configuration, accuracy was considerably higher than the original CDS X  X S configuration, possibly due to this CS test set being easier to parse than the 969 utterances of the test set of the CDS X  X S configuration presented in line 3. This could be contributed also to the fact that the test set was taken from the recordings of the child at an older age, thus it is perhaps more similar to CDS data than the CS test set of the original CDS X  X S configuration which consists of the entire CS data. The scores of the modified CDS X  X S configuration were slightly lower than the CS X  X S scores, but the differences are not statistically significant.

The fact that training on CS has some advantage over training on CDS when parsing CS can be partially explained by the fact that the age range of the files of the Ravid corpus is rather small, the difference between the first file and the eighth file being only 7 months. Note that in the CDS X  X DS configuration the scores are also relatively low. It is apparent that training on CDS confuses the parser to some degree. This can be explained by the richer structure of CDS compared to CS and by the different constructions and relations uttered by the same adults when the child matures.

However the out-of-domain data (Table 4 ) suggest that when parsing child speech it is better to learn from child-directed speech than from child speech. To further examine this result we trained the parsers on a CDS dataset similar in size to the CS dataset (i.e., the training set consists of 1,541 CDS utterances). Table 6 shows the results of the modified CDS X  X S evaluation (line 2) compared to the CS X  CS evaluation (line 1) and the original CDS X  X S evaluation (line 3). The results suggest that there is some advantage to training on child-directed speech when parsing child speech, in contrast to the trend that emerged from the in-domain task.
The best scores for out-of-domain training, and the closest scores to the in-domain case, are obtained in the CDS X  X DS configuration. This should most probably be attributed to the smaller variance that is expected in CDS between different adults, in contrast to the relatively substantial differences in CS. 6.4 Cross-validation In addition to evaluating our annotation scheme on the same domain and on a different domain, we also tested it on the corpora as a whole without any distinction to participants or ages. The cross-validation process allows for a more robust evaluation of the entire data. To this end we evaluated the entire set of 12 files (concatenated into one large file) using fivefold cross-validation. Similarly to previous evaluations, each fold of the cross-validation analysis had its parsing algorithm and feature set selected using MaltOptimizer. The results, presented in Table 7 , clearly show the advantage of MaltParser over MEGRASP (the differences are statistically significant). They also underline the robustness of both parsers across domains and speakers.
 In addition, we performed a similar fivefold cross-validation on the 8 files of the Ravid corpus, thereby restricting the cross-evaluation to a single domain (Table 8 ). Here we retained the domain, but ignored the age factor of the participant in the interaction, since, unlike in the regular in-domain scenario, the test set is not made of conversations in which the participant is necessarily older than in the training set. This scenario should be compared to the results of the evaluation of CHILDES in English (Sagae et al. 2010 ). Cross-validation on the Eve corpus of the English section of CHILDES (using MEGRASP) yielded an average result of 93.8 UAS and 92.0 LAS. However, the English training set was considerably larger (around 60,000 tokens compared to around 15,000 in the training set of each fold of our in-domain cross-validation evaluation).

The advantage of MaltParser over MEGRASP is statistically significant ( p \ 0.05) for both the Berman and the Ravid corpora, across all three measures. 6.5 Adding morphological features to improve parsing Several morphological features are relevant for parsing. The gender, number and person of tokens are crucial for determining agreement. The argument of a verb can be either an agreeing argument (specified by the Aagr relation) or a non-agreeing argument (specified by the Anonagr relation). The  X  X orm X  feature of a token can indicate whether a verb is in the imperative mood or the infinitive mood. More specifically, the  X  X orm X  feature can help determine the Ainf relation, which only holds for infinitival verbs. Awareness of such features was proven useful for parsing of Arabic (Marton et al. 2013 ). In this section we investigate the impact on parsing accuracy of using such features. To this end, we modify the feature set of MaltParser (such functionality is currently limited in MEGRASP).

In some cases the optimized parameters for MaltParser, suggested by MaltOp-timizer, already include morphological features. In this section we start with a feature set that does not include any of the four morphological features mentioned above (we refer to this set of features as NoMorph ). We then add different subsets of features to the feature set and evaluate the accuracy of MaltParser using these features. The subsets that we test include adding up to three tokens from the top of the data structures used by the selected parsing algorithm, with references to the following data custom columns:  X  VERBFORM, indicating the  X  X orm X  feature described above  X  NUM, indicating the  X  X umber X  feature of the token  X  PERS, indicating the  X  X erson X  feature of the token  X  GEN, indicating the  X  X ender X  feature of the token
As described in Sect. 6.1 , the configuration suggested by MaltOptimizer for the in-domain All X  X ll scenario included using the Stack parsing algorithm. The morphological features may appear in the various data structures of the algorithm, and the parser may use their values to aid its decisions.

Table 9 shows the accuracy of parsing (the in-domain task in the All X  X ll configuration) with the features whose addition to NoMorph yields the highest improvement. The test set consisted of 590 utterances (2,474 tokens). Although VERBFORM provided some improvement in itself (second row), it did not provide further improvement when added to the combination of PERS, NUM and GEN (line 3). None of the improvements is statistically significant ( p [ 0.1).
 Table 10 depicts the changes in the scores of some specific relations when PERS, NUM, and GEN of the elements in the three top positions of the stack were added to the NoMorph features. 5 This set of features improved the scores of these relations (except Anonagr ) in almost every metric. Specifically, the big improvement in Ainf is clearly attributed to the verb form information that was made available to the parser. Note also that for some relations the increase in the labeled scores is higher than in the unlabeled scores, indicating the contribution of the features to identifying the grammatical relation correctly.
 7 Linguistic issues Different frameworks of dependency-based parsers produce different analyses for existing linguistic controversies (Nivre 2005 ). In addition to testing for feature improvement, our work aims to investigate whether contrasting approaches to actual syntactic annotation yield different accuracy rates. Several syntactic constructions that frequently occur in our data can be annotated in two distinctly-motivated ways. In this section, we check empirically these different approaches to syntactic analysis. All evaluations used MaltParser and were conducted on the in-domain task, in the All X  X ll configuration; the size of the training set was thus 3,286 utterances (11,155 tokens) and the size of the test set was 590 utterances (2,474 tokens).

In the following sections, we use two terms to refer to alternative analyses. The term Approach A refers to the annotation scheme described in Sect. 4 , while the term Approach B refers to an alternative approach that we present for the first time in this section. 7.1 Copula constructions and other forms of hay  X   X  X e X  First, we examine utterances with some form of the functional verb hay  X   X  X e X , which we term hay  X  constructions . In Hebrew, hay  X  constructions function in a variety of contexts, mainly copula and existential constructions (Rosen 1966 ; Berman 1978 ). For both types of constructions, which are quite common in Hebrew, the verbal form appears in either past or future tense. In present tense, the two constructions diverge. For copula constructions, the realization of the tense-carrying element is in some cases optional, and it usually takes the form of a pronoun when it is explicitly expressed. Thus, the same clause can occur without a copula, as in  X  eitan gav  X  ah  X  X itan tall X , or with a copula in the form of a pronoun, as in  X  eitan hu  X  gav  X  ah  X  X itan he tall X . For existential constructions, the verbal form alternates with the suppletive (non-verbal) forms yes  X   X  X here_is X  or  X  eyn  X  X here_is_not X .
Previous dependency-based parsers have suggested different ways to deal with copula constructions. 6 The scheme used for annotating English data within CHILDES (Sagae et al. 2010 ) views the verbs  X  X e X ,  X  X ecome X ,  X  X et X , etc., like other verbs, as the heads, and their nominal predicates as the dependents, as in Example 16: (16)
In Hebrew, however, there is no consistent paradigm of copula verbs. Moreover, the optionality of the copula in present-tense constructions requires consideration (Haugereid et al. 2013 ). The Stanford Parser English scheme (de Marneffe et al. 2006 ), for example, is motivated by the need for adaptablity to languages in which the copula is not necessarily explicitly represented. In this scheme, the nominal predicate is viewed as the head and the copula as its dependent. The subject is also dependent on the nominal predicate (Example 17). According to de Marneffe and Manning ( 2008 ), an additional motivation for this decision was to help applications extract the semantic information of the clause through the direct relation between the subject and the predicate. (17) Alternatively, while the Prague Arabic Dependency Treebank (Hajic  X  and Zema  X  nek 2004 ) treats a group of verbs which may act as copulas (referred to as  X  kana  X  X e X  and her sisters X ) as a subset of the entire verbal group and thus as heads, in clauses with zero copula the nominal element is analyzed as the head, as in Example 18. (18)
The scheme for annotating written Hebrew presented by Goldberg ( 2011 )is similar in this respect to the Prague Arabic dependency scheme. In a non-verbal clause with zero copula, the predicate is the head and the subject is its dependent (Example 19); when the copula is present, the predicate is also the head and the copula is its dependent (Example 20). (19) (20)
The past and future forms of hay  X  , in contrast, are viewed as ordinary verbs, and form the root of the sentence they appear in (Example 21). (21)
Our scheme for spoken Hebrew uses the label Acop to mark the relation between the copula and its argument, the copula being the head (Approach A). Alternatively, we use the label Xcop to mark the relation in which the copula is the dependent (Approach B). Similarly, we use the labels Aexs and Xexs for the two approaches of the existential marker. We automatically converted the annotation of Approach A to Approach B. Example 22 depicts an utterance containing an existential marker annotated according to Approach A, where the head is the existential element. The result of its conversion to Approach B is shown in Example 23. (22) (23)
We trained MaltParser on data annotated with both approaches and evaluated the accuracy of parsing. The test set included 590 utterances (2,474 tokens) out of which 45 utterances (271 tokens) included at least one occurrence of either Acop (
Xcop in Approach B) or Aexs ( Xexs in Approach B) according to the gold standard annotation. Table 11 shows the accuracy of parsing with the two alternatives of hay  X  constructions. Table 12 shows the accuracy when evaluating the alternative approaches only on the 45 utterances (271 tokens) that contain the Acop ( Xcop )or Aexs ( Xexs ) relations. Evidently, Approach A yields slightly better results, but the differences between the two approaches are not statistically significant ( p [ 0.1).
Copula-less constructions are rather common in Hebrew, and are far more common than utterances with a pronominal copula. Still, the training set of the in-domain evaluation scenario includes only 45 of them, just above 1 % of all utterances. Since nominal predicates are more often dependent on verbs, it is inconsistent to mark them as the root of utterances when they contain a hay  X  form. 7.2 The accusative marker Another form that presents a challenge for dependency-based parsing is the Hebrew accusative marker,  X  et . This morpheme behaves much like a preposition: it can either introduce a lexical noun phrase or inflect with a pronominal suffix, and it expresses Verb-Patient relations, similarly to other prepositions in Hebrew. Although the analysis of  X  et as a preposition is conventional (Danon 2001 ), its expressed on the surface if and only if the following noun is definite. The syntactic status of  X  et is thus unclear, and two types of analysis are possible: one option is to treat the dependency between  X  et and the noun following it similarly to the relation specified for all other prepositions in our scheme, with the noun functioning as the argument of  X  et ; the alternative is to treat the accusative marker as a dependent of the noun. In the first type of analysis we label the relation between the verb and the accusative marker Anonagr and between  X  et and the nominal element Aprep ,asin Example 24. (24)
In the second analysis, the nominal element is viewed as directly dependent on the verb (in a relation labeled Anonagr ), with a relation labeled Xacc assigned to  X  et , as shown in Example 25. (25)
The implication of the first analysis is that all constructions containing a verb followed by a preposition are treated systematically. This representation, however, results in inconsistency between definite and indefinite direct object constructions: as Anonagr , and so these two parallel constructions are structurally distinct (Example 26). While the second analysis reflects consistency between definite prepositional phrases (e.g., Example 9 above). 7 (26)
We automatically converted our original annotation scheme of  X  et as the head of the following nominal element (Approach A, Example 24) to the alternative scheme, where  X  et is a dependent of the nominal head (Approach B, Example 25). We trained MaltParser on data annotated in accordance with both approaches and evaluated the accuracy of parsing, again for the in-domain evaluation task in the All X  X ll configuration. Table 13 shows the accuracy of parsing for the two alternatives. The test set contained 590 utterances (2,474 tokens) out of which 41 utterances (215 tokens) contained at least one occurrence of  X  et . We also show (Table 14 ) the accuracy when parsing only the 41 utterances that contain  X  et . While there is a small advantage to Approach A, the differences between the two approaches are not statistically significant ( p [ 0.1).

The small difference in accuracy between the two approaches is supported by the distribution in the training data of prepositional arguments of verbs (consistent with Approach A) and of indefinite nominal arguments of verbs (consistent with Approach B). Both are relatively common in the training data, perhaps explaining why neither approach has a significant advantage over the other. 7.3 Prepositions as dependents In the annotation scheme we presented, prepositional phrases are headed by the preposition, labeled with the Aprep relation. An alternative analysis views prepositional phrases as headed by the nominal element, with the preposition depending on this head. In order to examine this alternative, we reversed the direction of all occurrences of the Aprep relation. In Approach A, this relation is headed by the preposition (including the accusative marker  X  et and the possessive marker s  X  el ). In Approach B, the nominal element is the head and the preposition depends on it in an Xprep relation. As a result, in Approach B the nominal element is directly dependent on the verb or noun that the preposition was dependent on in Approach A. Since the accusative marker is also affected by this transformation, this is an extension of the approach discussed in Sect. 7.2
Example 27 presents an utterance containing a prepositional phrase annotated according to Approach A, where the head is the preposition. The result of the conversion is shown in Example 28. (27) (28)
We trained the parser (again, on the in-domain task in the All X  X ll scenario) with the alternative approaches and evaluated the accuracy of parsing. Table 15 compares the results of the original scheme (Approach A) with the alternative representation (Approach B).

There is a significant advantage to Approach A in both UAS and LAS ( p \ 0.05). This advantage can be explained by the decrease in accuracy of the Anonagr and Mpre relations, especially the latter. Recall that there is a direct relation between a verb and a direct object labeled Anonagr , regardless of the annotation approach. With Approach B, nominal elements in prepositional phrases are directly attached to verbs as well, with no intervening preposition that could indicate whether this nominal element is a modifier or an argument. Thus, nominal elements are possibly mistaken for arguments when they are in fact modifiers. 7.4 Prepositional arguments of verbs A prepositional phrase following a verb can be either the verb X  X  argument or its modifier. The decision is hard, even for human annotators, and the phenomenon is ubiquitous, with over 1,100 cases in our corpora. For example, the preposition le- X  X o X  is considered a modifier of the verb (with the Mpre relation) in Example 29, but an argument (with the Anonagr relation) in Example 30. (29) (30)
These subtle differences between prepositional arguments and modifiers of verbs lead to poor (labeled) recall and precision of the Mpre relation, as is evident in Table 3 . In order to improve the overall accuracy of the parser, we altered the annotation scheme and created a new relation, Averb , that uniformly labels the attachment between a verb and a preposition, independently of whether the prepositional phrase is an argument or a modifier. The Mpre relation remains when a preposition is dependent on a noun, and the Anonagr relation now represents arguments of verbs which are not prepositions. We then trained the parser (again, on the in-domain task in the All X  X ll scenario) and evaluated the accuracy of parsing. Table 16 compares the results of the original scheme (Approach A) with the alternative representation (Approach B).

In the All X  X ll configuration, there seems to be a slight overall decrease in unlabeled accuracy, but the difference is not statistically significant. Closer inspection of the confusion matrices shows that in Approach B, the accuracy of the Averb relation is quite high (over 90 % in all individual metrics), and the accuracy of Anonagr actually improves slightly (compared with Approach A), but the accuracy of Mpre drops dramatically. Indeed, Mpre is confused with both Averb and Anonagr . We believe that a larger training corpus may be able to shed more light on this result. 7.5 Token representation The last issue we examined with respect to a potential effect on parsing accuracy is token representation. A morph-based approach calls for the split of words into morphemes, the atomic units that are combined to create words, whereas a word-based approach refers to words as the minimal units of the language (Blevins 2006 ; Tsarfaty and Goldberg 2008 ). Recall that in order to reduce sparseness of data, we pre-processed the transcripts, splitting pronominal suffixes and inflected preposi-tions to separate tokens; this renders the representation of the corpora partially morph-based. Using our annotated data and a simple conversion script we can investigate the differences between the word-based approach and the morph-based approach. More specifically, we examine the accuracy of parsing on data in which pronominal suffixes and inflected prepositions were not split.

We trained MaltParser on a version of the data that reflects such a word-based approach to token representation (Approach B). We compared the accuracy of parsing in this case to the accuracy obtained by MaltParser on the split data (Approach A), in the in-domain evaluation task and the All X  X ll scenario. Table 17 shows the results. The morph-based representation is better, but the differences are not significant ( p [ 0.1). Evaluation restricted to utterances that include a split token reveals similar results. 7.6 Summary In this section we examined various alternatives for relations in our annotation scheme, where both annotation approaches are linguistically plausible. Most of our evaluations showed no significant difference between the alternatives, except one (preposition as dependents) which showed a significant advantage to Approach A. The reason for these results could be that the data set is too small for any significant advantage for either approach. It is possible that the characteristics of these specific corpora X  X specially the relatively short utterances and the lack of morphological ambiguity X  X ad an effect on the outcome, preventing any significant advantage. Most likely, which annotation approach of the two is selected is less important, as long as it is plausible (both linguistically and computationally) and consistent. Thus, we conclude that the annotation scheme proposed originally (as described in Sect. 4 ) is as suitable for our corpora as the alternative annotation approaches. This is the scheme we use for the released version of our corpora. 8 Conclusions We presented a new annotation scheme for Hebrew spoken language, as part of the Hebrew section of the CHILDES database. The scheme handles some of the unique linguistic characteristics of Hebrew spoken language in general and child and child-directed language in particular. We showed that a parser trained on data annotated using this scheme achieves good results when parsing the same domain and is also adaptable the transcriptions were sometimes erroneous or ill-formed; this had an effect on the quality of the syntactic analysis, and future acquired data should help in this respect.
We showed that both MaltParser and MEGRASP produced relatively good accuracy. In both evaluation scenarios, MaltParser proved to be the better of the two, thanks to parameter tuning done by MaltOptimizer. In future work, it would be interesting to experiment with more parsers, especially transition-based ones that may be more adequate for a corpus of short utterances such as ours, but also graph-based ones. Several such parsers exist that could be trained on our corpus (McDonald et al. 2005 ; Bohnet 2010 ; Zhang and Clark 2011 ).

We examined the differences between learning from CDS and CS. Within the same domain there was no significant difference and both configurations yielded relatively high accuracy. However, when parsing out-of-domain (and, crucially, on different children) there was a clear advantage to training on CDS. We attribute this to the simplicity of the CS in the in-domain scenario as well as to differences in CS between the training set and the test set (and within the test set) in the out-of-domain scenario. We conclude that, as expected, there is some difficulty to adapt CS from one domain to another (also recalling the age gap between the domains) whereas CDS is more stable and less varied across domains.

Working with MaltParser allowed us to evaluate the impact of features derived from the morphological tier of the corpora. Although the accuracy of parsing using the feature set without extended morphological data is quite high, due to the fact that the basic feature set was optimized by running MaltOptimizer and to the presence of a gold standard morphological tier, when we used detailed morpho-logical information we were able to improve the accuracy of parsing even more. The best accuracy was exhibited using the morphological attributes  X  X ender X ,  X  X erson X  and  X  X umber X . Future work in this area can embark on a more systematic approach that has the sole purpose of examining the contribution of morphological features. This includes extracting more morphological attributes other than those that were used in this work, as well as a more elaborate search for subsets of features that are derived from MaltParser data structures.

We examined different annotation approaches for a few linguistic constructions, such as hay  X  constructions and accusative marker constructions. In most cases, significant advantages to either approach were not revealed. This can be attributed explanation may very well be that as long as the annotation is consistent it can produce reasonable results, regardless of the specific annotation approach. It would be interesting to see if this is a cross-linguistic phenomenon, e.g., for copula constructions that are challenging in several languages.

We utilized the fact that the input to the syntactic process is a fully disambiguated gold standard morphological tier. An interesting extension is to evaluate the parser on data with a morphological tier that was created automatically. Apart from an obvious decrease in accuracy we expect that this may also introduce some different effects when examining feature sets or linguistic issues. Another extension to this work is parsing of Hebrew spoken language from other domains. We leave these research directions for future research.
 Appendix 1: Dependency relations Table 18 summarizes the basic dependency relations we define in this work. We list below all the relations, providing a brief explanation and a few examples. AgreementArgument (Aagr) Specifies the relation between an argument and a predicate that mandates agreement.
 Non-agreementArgument (Anonagr) Specifies any argument of a verb which need not agree with the verb, including indirect arguments.
 Subordinate Clause (SubCl) Specifies the relation between a complementizer and the main verb of a subordinate clause.

ArgumentOfPreposition(Aprep) Specifies the relation between a preposition and NonFiniteArgument (Ainf) This relation is specified between a verb or a noun in the main clause and its non-finite verbal argument.
 ArgumentOfCopula (Acop) Specifies the relation between a copula and its predicate (either nominal or adjectival). See Sect. 7.1 for further discussion regarding this relation.
 ArgumentOfExistential (Aexs) Specifies a relation between an existential element and a nominal or adjectival predicate. See Sect. 7.1 for further discussion regarding this relation.
 Mdet Specifies a relation between a determiner and a noun.
 Madj Specifies a relation between an adjective and a noun.
 Mpre Specifies a relation between a dependent preposition and a head noun or a verb.
 Mposs Specifies a relation between a noun and a subsequent possessive marker, noted by the token  X  s  X  el  X , headed by the noun.
 Mnoun Specifies a noun X  X oun relation, where the first noun, the head, is in the construct state.
 Madv Specifies a relation between a dependent adverbial modifier and the verb it modifies. Mneg Specifies a negation of a verb or a noun.
 Mquant Specifies a relation between a noun and a nominal quantifier, headed by the noun.
 Msub Specifies a relation between a nominal element and a relativizer of a relative clause, headed by the nominal element. The main predicate of the subordinate clause is marked as the dependent of the relativizer with a RelCl relation. Voc Specifies a vocative.
 Com Specifies a communicator.
 Coordination (Coord) Specifies a coordination relation between coordinated items and conjunctions, most commonly we- X  X nd X , headed by the conjunction.
 Serialization (Srl) Specifies a serial verb.
 Enumeration (Enum) Specifies an enumeration relation.
 Unknown (Unk) Specifies an unclear or unknown word X  X ost commonly a child invented word X  X hich appears disconnected from the rest of the utterance and often functions as a filler syllable.
 Punctuation (Punct) Specifies a punctuation mark, always attached to the root. Appendix 2: The effect of MaltOptimizer 2.1 The features chosen by MaltOptimizer The Stack non-projective eager algorithm uses three data structures: a stack Stack of partially processed tokens; a queue Input which holds nodes that have been on Stack; and a queue Lookahead which contains nodes that have not been on Stack. This algorithm facilitates the generation of non-projective trees using a SWAP transition which reverses the order of the top two tokens on Stack by moving the top token on Stack to Input. The recommended feature set for the All X  X ll configuration is depicted in Table 19 . The features reflect positions within these data structures, where  X 0 X  indicates the first position. For example, the feature  X  X OSTAG (Stack[0]) X  Stack data structure. The NUM , GEN , PERS and VERBFORM features are short for the number, gender, person and verb form morphological features, respectively. Merge and Merge3 are feature map functions which merge two feature values and three feature values into one, respectively. ldep returns the leftmost dependent of the given node; rdep return the rightmost dependent; head returns the head of the node. For definitions of the rest of the features, refer to Nivre et al. ( 2007 ). 2.2 MaltParser X  X  default features MaltParser X  X  default parsing algorithm is Nivre arc-eager (Nivre 2003 ), which uses two data structures: a stack Stack of partially processed tokens and a queue Input of remaining input tokens. The feature set used by Nivre-arc is depicted in Table 20 . 2.3 The features chosen by MaltOptimizer for the out-of-domain configuration The feature set for the out-of-domain configuration suggested by MaltOptimizer is depicted in Table 21 . The similarities between the suggested MaltOptimizer configurations of the in-domain and out-of-domain scenarios are not surprising, as domain scenario.
 References
 Shai Gretz  X  Alon Itai  X  Brian MacWhinney  X  Bracha Nir  X  Shuly Wintner Abstract We present a syntactic parser of (transcripts of) spoken Hebrew: a dependency parser of the Hebrew CHILDES database. CHILDES is a corpus of child X  X dult linguistic interactions. Its Hebrew section has recently been morpho-logically analyzed and disambiguated, paving the way for syntactic annotation. This paper describes a novel annotation scheme of dependency relations reflecting constructions of child and child-directed Hebrew utterances. A subset of the corpus was annotated with dependency relations according to this scheme, and was used to train two parsers (MaltParser and MEGRASP) with which the rest of the data were parsed. The adequacy of the annotation scheme to the CHILDES data is established through numerous evaluation scenarios. The paper also discusses different anno-tation approaches to several linguistic phenomena, as well as the contribution of morphological features to the accuracy of parsing.
 Keywords Parsing  X  Dependency grammar  X  Child language  X  Syntactic annotation 1 Introduction Child X  X dult interactions are a basic infrastructure for psycholinguistic investigations of child language acquisition and development. The corpora available through the CHILDES database (MacWhinney 2000 ), consisting of spoken transcripts in over twenty five languages, have been an indispensable source for this line of research with raw data from monologic, dyadic, and multi-party interactions (all following a unified and established transcription scheme) but also with tools for the application of theoretically-motivated and well-tested analyses. The most developed feature of the system is the MOR program, an automatic Part-of-Speech tagger with functionality in thirteen varied languages X  X ncluding Cantonese, Japanese, and Hebrew (in addition to Romance and Germanic languages). More recent work has focused on the automatic syntactic parsing of these data, most notably with the parser that was developed by Sagae et al. ( 2010 ) for the English section of CHILDES.

The current paper reports on a similar endeavor, focusing on automatic syntactic parsing of (a subset of) the Hebrew section of the CHILDES database. This subset includes two corpora with full, reliable, and disambiguated Part-of-Speech and morphological annotation (Albert et al. 2014 ): the Berman longitudinal corpus (Berman and Weissenborn 1991 ) and the Ravid longitudinal corpus. Each of these corpora includes naturalistic data collected on either a weekly or a monthly basis from six Hebrew-speaking children and their care-takers, yielding approximately 110,000 utterances in total (Nir et al. 2010 ).

Similarly to the syntactic structure that Sagae et al. ( 2010 ) induce on the English section of CHILDES, the parser for Hebrew makes use of dependency relations , connecting the surface tokens in the utterance through binary, asymmetric relations of head and dependent . In this we join a growing body of automatic parsers that rely on dependency-based syntactic representations (Ku  X  bler et al. 2009 ). These representations are particularly adequate for the annotation of Hebrew child X  X dult interactions for a number of reasons. Hebrew is a language with relatively complex morphology and flexible constituent structure; morphologically-rich languages such as Arabic have been successfully analyzed using a dependency-based parser (Hajic  X  and Zema  X  nek 2004 ). More importantly, a dependency-based parser relies on the explicit specification of dependencies (i.e., the name of the functional relation), which provides a representation of the relations assumed to be learned by the child, unlike parsers based on constituent structure, in which such information is implicit and requires further derivation (Ninio 2013 ). Finally, this choice makes our annotation more consistent with the syntactic structure of the English section of CHILDES, which may be useful for cross-linguistic investigations.

This work makes several contributions. First, we defined the first dependency annotation scheme for spoken Hebrew (Sect. 4 ). Second, we developed a parser for the Hebrew section of the CHILDES database, annotating utterances with syntactic dependency relations (Sect. 5 ). The parsed corpus will be instrumental to researchers interested in spoken Hebrew, language acquisition and related fields. We evaluated our parser in different scenarios (Sect. 6 ); the results demonstrate that the parser is both accurate and robust. Furthermore, we experimented with alternative annotation methods of several constructions, as well as with different approaches to tokenization of the input and with different sets of morphological features (Sect. 7 ), showing the impact of such choices on the accuracy of parsing. The parser, the manually annotated corpus, and the automatically parsed corpora are all available for download from the main CHILDES repository. 2 Related work To the best of our knowledge, the only parser of Hebrew was introduced by Goldberg ( 2011 ). It focuses on written Hebrew and has two versions, one that produces constituent structures and one that generates dependency relations. The scheme presented by Goldberg ( 2011 ) was generated from the original format of a constituent structure treebank (Sima X  X n et al. 2001 ; Goldberg and Elhadad 2009 ), a relatively small corpus that includes around 6,200 sentences taken from an Israeli daily newspaper.

Several features set our work apart from Goldberg ( 2011 ): first, we focus on spoken , colloquial Hebrew, rather than on written, journalistic language. In particular, our corpus includes samples produced by adult, non-expert speakers as well as by children who are still acquiring their grammar. The study of spoken language requires dealing with utterance complexity at varying levels. On the one hand, spoken language implies short utterances [as witnessed by Sagae et al. ( 2010 )] that are thus generally simpler and easier to parse. On the other hand, utterances are partial and lack any standard formal structure, and some may be ungrammatical (in the traditional sense), especially those produced by children. Spoken language is also characterized by repetitions, repairs, and interruptions within the same utterance and even across utterances. Furthermore, a single utterance may contain more than one clause. These characteristics raise issues that to date have not been addressed in the automatic syntactic analysis of Hebrew.
 Second, the parser described by Goldberg ( 2011 ) relies on data represented via Hebrew orthography. Hebrew orthography is ambiguous, since vowels are represented only partially (if at all), so that many words are homographic (Wintner 2004 ). Our transcription explicitly encodes vowels as well as other phonological information that is highly relevant for ambiguity resolution, such as lexical stress between similar-sounding phonemes that are written differently in the standard Hebrew script, e.g.,  X   X  X et X  and t  X  X av X , or k  X  X af X  and q  X  X uf X . This approach significantly reduces morphological ambiguity, and renders moot some of the problems that restrict the capabilities of Goldberg X  X  parser. Moreover, functional elements (the definite article, prepositions, and conjunctions) that are concatenated with the subsequent token in the standard orthography are separated in our transcription; e.g., we-ha-yeled  X  X nd the child X . This allows us to make sure that each lexeme is treated separately and consistently at the lexical, morphological, and syntactic levels. In addition, multi-word expressions are systematically transcribed night X , etc. Consequently, the level of morphological ambiguity of each transcribed token is very low. Any remaining ambiguity is subsequently resolved in an automatic process (akin to part-of-speech tagging), so that the input for parsing is completely disambiguated. Albert et al. ( 2014 ) show that the automatic morpho-logical disambiguation module achieves an accuracy of 96.6 %. This is in contrast to written Hebrew which suffers from high morphological ambiguity, and for which automatic morphological disambiguation is less accurate (Lembersky et al. 2014 ).
Furthermore, Goldberg X  X  scheme itself is partial, i.e., not all relations existing in the data are labeled. In contrast, the scheme we developed provides fully annotated data (Sect. 4 ). As such, our work is similar in spirit to Sagae et al. ( 2010 ), who, motivated by the same goals, constructed a parser for the English section of the CHILDES corpus. Sagae et al. ( 2010 ) developed a scheme of 37 distinct grammatical relations which was used for annotating the Eve corpus (Brown 1973 ). The annotation included both manual and automatic analyses of 18,863 utterances, 10,280 adult and 8,563 child. During the annotation procedure, the corpus was used for training a data-driven parser which was then tested on an independent corpus of child X  X dult interactions. Cross-validation evaluation of the parser X  X  performance showed a low error rate, of between 6 and 8 %. Both the English and the Hebrew data sets follow the CHAT transcription guidelines (MacWhinney 2000 ), and attempt to reflect the flow of the conversation as accurately as possible. The transcription standard also marks tokens that should be ignored by morphological and syntactic analyses, for example in the case of false starts or repetitions. 1
Tsarfaty et al. ( 2012 ) also address Hebrew; they are concerned with joint evaluation of morphological segmentation and syntactic parsing, specifically in morphologically rich languages such as Hebrew. The motivation is that standard evaluation metrics for syntactic parsing do not take into account the underlying morphological segmentation that may produce errors which do not reflect actual parsing errors. Their proposed method allows for precise quantification of performance gaps between the use of gold morphological segmentation and that of predicted (non-gold) morphological segmentation. This work is representative of an emerging research program whose main goal is to develop techniques for parsing morphologically-rich languages (Tsarfaty et al. 2010 , 2013 ; Seddah et al. 2011 ).
While this line of works is of course relevant to Hebrew parsing in general, it does not seem to be as relevant to parsing CHILDES data. The transcription we use (Sect. 3.2 ) allows high accuracy of morphological segmentation and part-of-speech tagging, as many of the ambiguity issues that normally arise in processing Hebrew are already resolved. In addition, for evaluation purposes we manually tag all remaining morphological ambiguity, rendering the morphological analysis disam-biguated. Thus, our data do not reflect the rich morphology and high level of ambiguity of standardly written Hebrew, which motivate such a joint evaluation metric.
 For parsing (and evaluation) we compare MaltParser (Nivre et al. 2006 ) and MEGRASP (Sagae and Tsujii 2007 ). MaltParser is an architecture of transition-based parsers that can support various learning and parsing algorithms, each accompanied with its feature set and parameters, which can be directly optimized. The feature set is derived from the surface forms, the base forms and the morphological information of a subset of the tokens in the data structures (i.e., the queue and the stack) that comprise the state of the parser.
 Marton et al. ( 2013 ) evaluate the effect of morphological features on parsing Arabic, a sister-language to Hebrew, using MaltParser. This study shows that the most important features for parsing written Arabic utterances are  X  X ase X  and  X  X tate X , while  X  X ender X ,  X  X umber X  and  X  X erson X  did not improve the accuracy of parsing Arabic when gold morphological information was present. Case is irrelevant for Hebrew; the state of nouns is, but construct state nouns are not common in spoken language, and in our corpus in particular. However,  X  X ender X ,  X  X umber X  and  X  X erson X  may prove to be more helpful when the training set is enhanced with corpora which are automatically annotated for morphological features. We explore this direction in Sect. 6.5 .
 MEGRASP was chosen because it was used for dependency parsing of the English section of CHILDES (Sagae et al. 2010 ). The algorithm is a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie ( 2006 ). The parser is compatible with the format of the CHILDES database since CHILDES syntactic annotations are represented as labeled dependencies. 3 Characteristics of the Hebrew CHILDES corpus 3.1 Representation The corpora in the CHILDES database include three levels of data, each referred to as a tier (MacWhinney 2000 ). The main tier contains (a transcription of) actual utterances; the mor tier contains disambiguated lexical and morphological analyses of the utterances; and the gra tier lists the syntactic analyses of the utterances. The data are organized in a one-to-one format, where every token in the main tier has exactly one counterpart in the mor tier and the gra tier. 2
In the CHILDES database and throughout this work, dependency structures are represented linearly as triplets i | j | REL , where i is the index of a token in the Root marker is given the index 0), and REL is the label of the relation between them.
Example 1 shows an utterance with all three tiers. The elements in each of the three tiers are vertically aligned. The utterance consists of two tokens,  X  at  X  X ou X  and mes  X  aq  X  ret  X  X ie X . Below each token, the mor tier specifies the morphological analysis. The first token is analyzed as a personal pronoun ( pro:pers ) of feminine gender and singular number; the second token is a participle ( part ), again in feminine singular. The third line specifies the gra tier: the first token is a dependent of the second, and the dependency relation is Aagr (agreeing argument, Sect. 4 ); and the second token is the root, depending on the special index 0. For convenience, we also add a fourth line, with a full English translation of the utterance. (1) 3.2 Tokenization Tokenization is a crucial aspect of data preparation required for successful syntactic analysis, particularly for morphologically complex languages. Hebrew, which tends to be synthetic, is rich in bound morphology, and tokens that consist of more than a single morpheme are quite common. One such example is the class of simple article, as in the case of ba- X  X n the X . Two other common examples are complex prepositions with suffixed personal pronouns, for example, bis  X  vil  X   X   X  X or you X , and nouns that systematically take pronominal pronouns as bound inflectional suffixes marking possession, for example,  X  axot  X   X  X y sister X . These cases make it difficult to determine how many morphemes are to be counted in the analysis of one token (Dromi and Berman 1982 ).

Previous studies on dependency-based parsers for other morphologically complex languages have considered similar issues, as in the case of the Prague Arabic Dependency Treebank (Hajic  X  and Zema  X  nek 2004 ). Arabic, even more than Hebrew, is rich in bound, inflected morphemes, not only in the nominal system (as in the case of nouns with possessive suffixes) but also in the verbal system, where verbs can take accusative suffixes. The morphological analysis of the Prague Arabic Treebank is based on the idea of splitting words into morphs or segments , each receiving its own morphological analysis (Smrz  X  and Pajas 2004 ). Thus, prepositions and suffixed pronouns are considered separate tokens (e.g., biduni  X  X ithout me X  is split into bi-duni ), even if they are orthographically attached in Modern Standard Arabic script. Nouns with possessive suffixes are also split into two tokens (the noun and the pronominal suffix).

A similar approach is presented by the dependency-based parser for Turkish are specified between morphemes (referred to as in fl ectional groups or IGs) rather than between words. This morpheme-driven analysis resulted in more accurate parsing. However, since the mechanisms for bound morphology are much less productive in Hebrew compared to Arabic (let alone the agglutinative Turkish), we resorted to the following solution: the data were pre-processed and particular tokens were split to allow for a partial morpheme-bound representation. Thus, definite prepositions such as ba- X  X n the X  are split to two tokens, as in be- X  X n X  ha- X  X he X ; prepositions fused with personal pronouns are treated similarly, for example, bis  X  possession; we split such cases to three morphemes: for example,  X  axoti  X   X  X y sister X  is split into  X  ax  X  t  X  X ister X  s  X  el  X  X f X   X  ani  X   X  X  X .

Our motivation is largely computational rather than linguistic; presumably, such a split representation reduces data sparseness. Certain fused forms occur only rarely, possibly making it more difficult for the parser to identify and analyze them correctly. However, these changes are only aimed at improving syntactic analysis. In order to avoid any theoretically controversial decision as well as more practical implications, for example to the analysis of Mean Length of Utterance (Dromi and Berman 1982 ), we merge the split tokens back together once parsing is complete, omitting any intra-word relations. Unsurprisingly, with only a single exception, none of the split morphemes is involved in non-local dependency relations, so this merge operation is well-defined. 4 Annotation scheme for Hebrew CHILDES As noted above, our scheme is inspired by the grammatical relations defined for the annotation of the English section of CHILDES (Sagae et al. 2010 ). This is done mostly for issues that are not language-specific but rather represent general characteristics of spoken language, such as repetitions and repairs, on the one hand, and the inclusion of vocatives and communicative elements, on the other. In addition, some of the relations that were defined similarly in the two schemes relate to specific features of child X  X dult interactions, including onomatopoeias, enumer-ations, serialization of verbs, and topicalizations.

Moreover, our scheme remained consistent with the issue of how to treat coordinate constructions, a relation that poses challenges for dependency annotation in general. In adopting the English scheme, the coordinating conjunction was defined as the head of a coordination construction and the coordinated elements were defined as the dependents. This relation is labeled Coord . Also, we took into consideration the work of Goldberg ( 2011 ) on dependency parsing of written Hebrew, specifically in Sect. 7 where we evaluate alternative approaches for specific relations.

In contrast to the English scheme, we distinguish between three types of dependents for Hebrew: arguments [A], modifiers [M] and others. Arguments are dependents that are typically semantically required by the head, their properties are determined by the head, and they can occur at most once (often, exactly once). Modi fi ers , on the other hand, are non-obligatory dependents: typically, they select the head that they depend on and, consequently, they may occur zero or more times. The Others group includes relations in which the dependents are neither arguments nor modifiers of the heads, or relations in which the dependents do not relate specifically to any other token in the utterance. For example, the Com label marks the relation in which a communicator is the dependent. A communicator is generally related to the entire utterance, and so we mark the root of the utterance as the head of the communicator. The Others group also contains relations for two special cases where we present two optional analyses for a construction: the copula construction (Sect. 7.1 ) and constructions containing the accusative marker (Sect. 7.2 ). The second approach for both of these linguistic issues is marked with a relation whose name starts with X .

Typically in dependency-based parsers, the root of an utterance is an inflected verb or a copula in verbless copula utterances, carrying the tense marking in the clause. In utterances with no verb and no copula, where there is no element carrying a tense, the head is the predicating element. Copulas and existential markers, as well as other forms of the verb hay  X   X  X e X , are discussed elaborately in Sect. 7.1 When an utterance is lacking any of the above, the root is the element on which the other elements depend (such as the noun with respect to its modifiers). In single word utterances, the single token is by default the root.

The annotation scheme is comprised of 24 basic dependency relations and a few more complex dependency relations (combinations of two basic dependency relations; see Sect. 4.4 ). The complete list of the basic dependency relations is given in Appendix 1 . We discuss below some of the main constructions covered by our scheme. 4.1 Verb arguments, agreeing and non-agreeing Two main relations are defined between verbs and their arguments. One relation, Aagr , requires the verb and its argument to agree; at most one argument can stand in this relation with any given verb. The other relation, Anonagr , imposes no agreement constraints, and the number of such arguments can be zero or more. 3
In Example 2, the verb roc  X   X  X ant X  is the head of its agreeing argument  X  an  X   X  X  X  plural noun). (2)
Other non-agreeing relations include the relation between the verb and its indirect (or oblique ) objects, where the nominal element is preceded by a preposition. The Anonagr dependency is marked on the prepositional element and the nominal element is marked as the argument of that preposition, Aprep ; see Example 3. An alternative representation of prepositional phrases is discussed in Sect. 7.3 (3) Non-agreeing arguments can also occur as finite clausal dependents of the verb. In such cases, the subordinating conjunction is marked as Anonagr. In addition, it is treated as the head of the subordinate clause, and the finite verb of the subordinate clause is dependent on it in a SubCl relation, as in Example 4. (4)
When the subordinate clause is introduced by something other than a subordinating conjunction, the finite verb of the clause is directly dependent on the finite verb of the main clause, again in a Anonagr relation, such as in Example 5, where the verb qar  X   X  X appen X  (the head of the subordinate clause) is directly dependent on the matrix verb tir  X   X   X  X ook X . (5)
When the non-agreeing argument is an infinitival verb phrase, the relation between the head of the verb phrase and its (verbal or nominal) head is Ainf ; see Example 6. (6) 4.2 Modifiers Modification in Hebrew may occur for both nouns and verbs. Several relations specify nominal modifiers; these include Mdet for the relation between a determiner and a noun, Mquant for quantifiers, and Madj for adjectival modification. Another type of nominal modification is represented in noun-noun compounds, which in Hebrew are constructed by combining a morphologically-marked noun (said to be in the construct state) with another noun (recall that when such compunds are idiomatic they are represented as a single token). We mark the relation between the two nouns as Mnoun , as in Example 7. (7)
Verbal modifiers include Madv for adverbs (Example 8) and Mneg for negation (Example 2), as well as Mpre for prepositional phrase modifiers (Example 9). (8) (9)
When a subordinate clause is a modi fi er (rather than an argument) of a verb or a noun, the relation between the verb or noun and the subordinating conjunction is labeled Msub . If the clause is a relative clause, the relation between the relativizer and the head of the relative clause is labeled RelCl , as in Example 10. (10) 4.3 Other relations Vocatives are named entities that refer to another speaker in the conversation, most commonly followed by a question or request in the second person. Vocatives depend on the root of the utterance in a Voc relation (Example 11). (11)
Communicators include discourse markers such as  X  av  X  l  X  X ut X ,  X  az  X  X o X , ken  X  X es X , etc., as well as verbs such as tir  X   X   X  X ook X  and b  X   X  i  X  X ome_here X . Like Voc , the root of the utterance is the head of the relation and the communicator is the dependent. The main difference between the two relations is that Com does not include named entities. See Examples 5, 6.

The relation Coord specifies coordination, relating between conjuncts and conjunctions, most commonly we- X  X nd X . As noted above, we follow Sagae et al. ( 2010 ) in dealing with these constructions: the head is the coordinating conjunction and the dependents are the conjuncts. If there are two or more conjuncts with multiple coordinators, the coordinators are linked from left to right (the rightmost coordinator is the head of the others) by a Coord relation. In the absence of a coordinator the rightmost conjunct is the head of the relation. See Example 12. (12) 4.4 Elision relations Spoken language often includes missing elements, whether as a result of true ellipsis or of interruptions and incomplete utterances. In the English section of CHILDES, Sagae et al. ( 2010 ) decided to mark missing elements as elided and to relate to them in the analysis using elision relations . Such relations combine two basic relations: one between the elided element and its presumed head, and one between the elided element and its dependent. Following the scheme for English, we also mark missing elements with elision relations.

In Example 13, ha- X  X he X  is marked with the Mdet-Aprep relation. Mdet stands for the relation between ha- X  X he X  and a missing element, presumably a noun; Aprep stands for the relation that would have held between the missing noun and the preposition ley  X  d  X  X ear X . (13) 4.5 Child invented language As the CHILDES corpus is comprised of child and adult interactions, child-specific forms and constructions are rather frequent. These include neologisms, babbling, and incoherent speech. Such forms can be detached from the utterance, labeled with the Unk relation which marks unknown relations (Example 14); or, when the syntactic function of such forms is known to the annotator, they can take the place of a known relation (e.g., the neologism bdibiyabi in Example 15). (14) (15) 5 Methodology 5.1 Parsing We manually annotated a subset of the Hebrew CHILDES corpus described in Sect. 3 according to the schema of Sect. 4 . The data were annotated by two lexicographers; all disagreements were resolved by a third annotator, a linguist who specializes in syntactic analysis.

This manually annotated corpus consists of 12 files: 8 files from the Ravid corpus and 4 from the Berman corpus. The 8 files of the Ravid corpus contain transcriptions of the same child at different ages (ranging from 1;11 to 2;05). The 4 files of the Berman corpus reflect 4 different children (all different from the child in the Ravid corpus) at different ages (2;04, 3;00, 3;03 and 3;06). Statistical data of the corpora are given in Table 1 . The data presented here refer to the corpora after splitting fused morphemes (Sect. 3.2 ) and exclude punctuation.
 We then trained two dependency parsers on the manually-annotated texts, MEGRASP (Sagae and Tsujii 2007 ) and MaltParser (Nivre et al. 2006 ). MEGRASP works directly on the CHILDES format in which the corpora are stored. MaltParser supports a number of formats, including the CoNLL-X shared task format (Nivre et al. 2007 ). An advantage of using MaltParser is that it also supports costume-made formats, allowing variation in the lexical and morphological information available for the learning algorithm. We used a format similar to CoNLL, but added columns to represent independent morphological attributes (instead of the concatenated FEATS column). Using MaltParser, we examined the effect of adding morpholog-ical features (e.g., number and person) to the default feature set (Sect. 6.5 ).
To achieve the best possible results using MaltParser we used the recently developed MaltOptimizer (Ballesteros and Nivre 2012 ). MaltOptimizer analyzes the training data in a three-phase process and outputs the recommended configuration under which to run MaltParser (e.g., a certain parsing algorithm or a feature set that yield the best results). Since MaltOptimizer is restricted to the CoNLL format and does not support custom formats, we used it as follows. We concatenated the morphological features into the FEATS column, to adapt the input to the CoNLL format. We ran this version of the parser with MaltOptimizer, and converted the files back to our custom format as suggested by MaltOptimizer. For example, MaltParser supports a Split function that splits the values of a certain column according to a delimiter. If MaltOptimizer suggested to split the FEATS column, we did so by placing the morphological information in the separate morphological columns. In the following sections, there is practically no difference between using our format and the CoNLL format. The main difference is when we evaluated the contribution of the morphological data to parsing (Sect. 6.5 ); there we examined various kinds of subsets of features, not all of which are supported by the regular CoNLL format. 5.2 Evaluation We conducted both In-domain evaluation , where training is on parts of the Ravid corpus and testing is on other parts of the same corpus (held out during training), and Out-of-domain evaluation , where training is done on the files of the Ravid corpus and testing is done on the files of the Berman corpus. We did not explore any domain adaptation techniques (Nivre et al. 2007 ; Plank 2011 ), we merely evaluated the robustness of the parser when tested on a different domain. We ran both MEGRASP and MaltParser on these evaluation scenarios. We also ran a fivefold cross-validation on the Ravid corpus and on both corpora combined.

The evaluation metrics that we used are unlabeled attachment score (UAS) and labeled attachment score (LAS). In UAS a token is considered correctly annotated if its head is the same head that is marked in the gold-standard X  X egardless of the grammatical relation. In LAS a token is considered correctly annotated if both the head and the grammatical relation are the same as in the gold-standard. In addition we report Exact Match (EXM), the percentage of utterances that are parsed without any errors. These are standard metrics in the evaluation of dependency parsing (Ku  X  bler et al. 2009 ).

To examine the quality of the parsers and the annotation scheme on individual relations, we used further metrics that are relation specific X  URecall r (unlabeled recall), LRecall r (labeled recall), UPrecision r (unlabeled precision) and LPrecision r head that token x is attached to in the gold file and h ( x ) the head that token x is attached to by the parser. Then:
The first two metrics are refinements of the recall metric for each relation as the analysis is with respect to the appearances of the relation in the gold standard files. The Recall r measures compute the percentage of tokens labeled r in the gold data that were correctly parsed. The other two metrics are refinements of the precision metric for each relation as the analysis is with respect to the appearances of the relation in the parsed model files. The Precision r measures compute the percentage of tokens labeled r in the parsed data that were correctly parsed. For each of the UAS and LAS precision and recall pairs we report also the (balanced) F -score, the harmonic mean of precision and recall.

In addition to testing the corpus as a whole we show results that relate separately to two types of data: child-directed speech (CDS) and child speech (CS). First, we trained and tested on both types of data (All X  X ll); to investigate whether children learn primarily from the language spoken to them or from their peers, we trained on CDS and tested on CS (CDS X  X S), and then trained and tested on CS only (CS X  X S); for completion, we also trained and tested on CDS (CDS X  X DS).

In the in-domain evaluation scenario we built the training set and test set for each of these configurations separately, using 8 files of the Ravid corpus. The files of the Ravid corpus are chronologically ordered by the age of the target child and thus in the in-domain evaluation scenario the held-out set always contains utterances of the same child at an older age. In this configuration, the training set is comprised of 80 % of the utterances of the relevant data type in the corpus, holding out 20 % for the test set. The training set of the All X  X ll configuration contains 3,286 utterances (11,155 tokens), the CS training set contains 1,237 utterances (3,246 tokens) and the CDS training set contains 2,066 utterances (7,946 tokens). The 80 % of CS and CDS were derived from the set of utterances (of their respective data types) in the corpus, and not from the training set of both data types. Consequently, the sum of the sizes of the CS and CDS training sets does not necessarily equal the size of the training set of the All X  X ll configuration. In the CDS X  X S configuration the training set and test set are comprised of utterances of different data types so the entire set of utterances of each data type in the corpus was used, and not just 80 % of it.
In the out-of-domain evaluation scenario the training sets and test sets of the different configurations were taken from different sets of files, so the entire set of utterances of the respective data type was used. For all evaluation scenarios, we excluded punctuation and single-token utterances, to avoid artificial inflation of scores. 6 Results 6.1 In-domain evaluation We first present the results for the in-domain scenario. Recall that we ran MaltOptimizer in order to achieve the best parser configuration with respect to the training set. In the All X  X ll configuration, according to MaltOptimizer, the training set contained approximately 3.9 % utterances with non-projective trees. 4 MaltOptimizer recommended using the Stack parsing algorithm in its non-projective eager version (Nivre 2009 ; Nivre et al. 2009 ). See Appendix 2.1 for a full description of the parameters chosen by the optimizer.

Table 2 shows the accuracy of parsing obtained by both parsers, in all four evaluation scenarios. Considering the relatively small training set, both parsers achieve reasonable results. Evidently, MaltParser proves to be better than MEGRASP on this domain. The difference in the All X  X ll configuration is statistically significant for all three metrics ( p \ 0.05).

To show the contribution of MaltOptimizer, we also ran MaltParser with its default parameters, which allows only projective dependency trees. The settings of the default parsing algorithm are discussed in Appendix 2.2 . In the All X  X ll configuration, the UAS was 84.5 and the LAS was 80.5 X  X ower than the results obtained by both the optimized MaltParser and MEGRASP. Thus, the adaptation of the parsing algorithm and feature set to our corpora using MaltOptimizer was clearly instrumental for improving parsing accuracy.

In general, the Exact Match accuracy is high, mostly due to the relatively short length of the utterances in our corpora. It is interesting to compare these results to Exact Match results of other tasks. In the CoNLL-X shared task (Nivre et al. 2007 ), different parsers were evaluated for token-based measures, such as LAS and UAS, by parsing 13 test-sets of various languages. Ballesteros et al. ( 2012 ) expanded this evaluation of parsers by calculating not only token-based but also sentence-based measures, such as Exact Match. They also drew a correlation between average sentence length and Exact Match accuracy. The test-set of Arabic had the highest average sentence length (37.2 tokens) and the lowest Exact Match score (9.6 with MaltParser, 6.2 averaged across parsers). On the other hand, the test-sets of Chinese and Japanese had the shortest average sentence length (5.9 and 8.9 tokens, respectively) and the highest Exact Match scores (68.1 with MaltParser and 49.5 averaged across parsers for Chinese, 75.3 with MaltParser and 59.6 averaged across parsers for Japanese). These results are in accordance with our results, as both the Ravid and Berman corpora exhibit a short average utterance length and high Exact Match scores, arising from the fact that they reflect adult-child interactions at early stages of language development.

Note also the low EXM when testing on CDS as opposed to the high EXM when testing on CS. Recall that the utterances in CDS are longer on average (Table 1 ) and so there is a higher chance that one of the tokens in an utterance is tagged erroneously.

To see which relations are more difficult for the parsers to predict, we evaluated the accuracy of the parsers on specific relations. Table 3 shows the relation-specific metrics for interesting individual relations, in the All X  X ll configuration. Relations that occur with a small set of tokens as dependents (such as Mdet , where the dependent is mainly the token ha- X  X he X ), or after a specific type of token (such as Aprep , occurring after a preposition) achieved a score of 97 % or above in all the four metrics. The frequent relations Aagr and Root reached high scores of over 92 % unlabeled recall and precision, 89 % labeled. Also accurate were the relations Mneg and Aexs . The more problematic relations were Com and Voc and modifiers such as Madv , Mquant and Mpre , which can sometimes be ambiguous even for human annotators. Amongst the modifiers the labeled scores of Mpre were especially low, due to the confusion between it and Anonagr when deciding whether a preposition is an argument or a modifier of a verb, in certain cases a decision that could be hard for a human annotator.
 Figure 1 shows the learning curves of MEGRASP and MaltParser on this task. We trained the parsers on an increasingly larger training set, from 400 utterances up to 3,200 utterances with increments of 400, and tested on a fixed test set of 590 utterances (2,474 tokens) in the All X  X ll configuration. We plotted UAS and LAS scores as a function of the number of utterances in the training set. The curves suggest that more training data could further improve the accuracy of the parser. 6.2 Out-of-domain evaluation We also evaluated the parsers on a different domain than the one they were trained on. For the All X  X ll configuration, according to MaltOptimizer, the training set contained approximately 3.8 % utterances with non-projective trees. Similarly to the in-domain scenario, MaltOptimizer suggested the Stack algorithm (Nivre 2009 ; Nivre et al. 2009 ), but in contrast to the in-domain scenario, it recommended the Stack non-projective version. This algorithm postpones the SWAP transition of the Stack algorithm as much as possible. The parameters selected for this configuration are discussed in Appendix 2.3 .

We trained the parsers on the 8 files of the Ravid corpus and tested on the 4 files of the Berman corpus. Table 4 lists the results.

Unsurprisingly, the accuracy of the parser in the out-of-domain evaluation scenario is considerably lower than in the in-domain evaluation scenario. The decrease in accuracy when parsing the CS data type can be explained by the fact that the test set of the Berman corpus contains utterances by four different children, all different from the child who is recorded in the training set. They are also children of different ages, and three of the four children in the test set are recorded at an older age than the child in the training set.

Another point to notice is that also in this scenario, MaltParser performed better than MEGRASP, but the differences between the parsers are slightly smaller in some metrics than in the in-domain evaluation scenario. One possible explanation is that MaltParser was run with optimized parameters as suggested by MaltOptimizer (e.g., parsing algorithm and feature set) that are configured according to the training set. In the out-of-domain evaluation scenario the differences in the types of utterances between the training set and the test set are more substantial than in the in-domain evaluation scenario. As a result the optimized parameters are less effective and hence the accuracy is poorer. Still, the advantage of MaltParser over MEGRASP in the All X  All configuration is significant for all three metrics ( p \ 0.05).

As in the in-domain evaluation scenario, we present a learning curve of the parsers when parsing the same out-of-domain dataset on training sets varying in size (Fig. 2 ). The size of the test set is 1,614 utterances (8,750 tokens). Here, too, the learning curves of both parsers suggest that there is room for improvement with more training data. 6.3 Learning from child-directed speech versus child speech Is it better to train the parser on child speech or on child directed speech? The in-domain and out-of-domain tests yield conflicting evidence. The in-domain data suggest that for parsing child speech it is better to learn from child speech than from child-directed speech. This is despite the fact that in the CDS X  X S configuration the training set is larger.

To examine the possibility that the specific CS test set used in both configurations contributes to this difference, we evaluated the CDS X  X S configuration with a training set similar in size to the CS X  X S training size (i.e., 1,237 utterances) and with an identical test set to the one used in the CS X  X S configuration. Table 5 shows the results of the modified CDS X  X S evaluation (line 2) compared to the CS X  X S evaluation (line 1) and the original CDS X  X S evaluation (line 3).

When running the modified CDS X  X S configuration, accuracy was considerably higher than the original CDS X  X S configuration, possibly due to this CS test set being easier to parse than the 969 utterances of the test set of the CDS X  X S configuration presented in line 3. This could be contributed also to the fact that the test set was taken from the recordings of the child at an older age, thus it is perhaps more similar to CDS data than the CS test set of the original CDS X  X S configuration which consists of the entire CS data. The scores of the modified CDS X  X S configuration were slightly lower than the CS X  X S scores, but the differences are not statistically significant.

The fact that training on CS has some advantage over training on CDS when parsing CS can be partially explained by the fact that the age range of the files of the Ravid corpus is rather small, the difference between the first file and the eighth file being only 7 months. Note that in the CDS X  X DS configuration the scores are also relatively low. It is apparent that training on CDS confuses the parser to some degree. This can be explained by the richer structure of CDS compared to CS and by the different constructions and relations uttered by the same adults when the child matures.

However the out-of-domain data (Table 4 ) suggest that when parsing child speech it is better to learn from child-directed speech than from child speech. To further examine this result we trained the parsers on a CDS dataset similar in size to the CS dataset (i.e., the training set consists of 1,541 CDS utterances). Table 6 shows the results of the modified CDS X  X S evaluation (line 2) compared to the CS X  CS evaluation (line 1) and the original CDS X  X S evaluation (line 3). The results suggest that there is some advantage to training on child-directed speech when parsing child speech, in contrast to the trend that emerged from the in-domain task.
The best scores for out-of-domain training, and the closest scores to the in-domain case, are obtained in the CDS X  X DS configuration. This should most probably be attributed to the smaller variance that is expected in CDS between different adults, in contrast to the relatively substantial differences in CS. 6.4 Cross-validation In addition to evaluating our annotation scheme on the same domain and on a different domain, we also tested it on the corpora as a whole without any distinction to participants or ages. The cross-validation process allows for a more robust evaluation of the entire data. To this end we evaluated the entire set of 12 files (concatenated into one large file) using fivefold cross-validation. Similarly to previous evaluations, each fold of the cross-validation analysis had its parsing algorithm and feature set selected using MaltOptimizer. The results, presented in Table 7 , clearly show the advantage of MaltParser over MEGRASP (the differences are statistically significant). They also underline the robustness of both parsers across domains and speakers.
 In addition, we performed a similar fivefold cross-validation on the 8 files of the Ravid corpus, thereby restricting the cross-evaluation to a single domain (Table 8 ). Here we retained the domain, but ignored the age factor of the participant in the interaction, since, unlike in the regular in-domain scenario, the test set is not made of conversations in which the participant is necessarily older than in the training set. This scenario should be compared to the results of the evaluation of CHILDES in English (Sagae et al. 2010 ). Cross-validation on the Eve corpus of the English section of CHILDES (using MEGRASP) yielded an average result of 93.8 UAS and 92.0 LAS. However, the English training set was considerably larger (around 60,000 tokens compared to around 15,000 in the training set of each fold of our in-domain cross-validation evaluation).

The advantage of MaltParser over MEGRASP is statistically significant ( p \ 0.05) for both the Berman and the Ravid corpora, across all three measures. 6.5 Adding morphological features to improve parsing Several morphological features are relevant for parsing. The gender, number and person of tokens are crucial for determining agreement. The argument of a verb can be either an agreeing argument (specified by the Aagr relation) or a non-agreeing argument (specified by the Anonagr relation). The  X  X orm X  feature of a token can indicate whether a verb is in the imperative mood or the infinitive mood. More specifically, the  X  X orm X  feature can help determine the Ainf relation, which only holds for infinitival verbs. Awareness of such features was proven useful for parsing of Arabic (Marton et al. 2013 ). In this section we investigate the impact on parsing accuracy of using such features. To this end, we modify the feature set of MaltParser (such functionality is currently limited in MEGRASP).

In some cases the optimized parameters for MaltParser, suggested by MaltOp-timizer, already include morphological features. In this section we start with a feature set that does not include any of the four morphological features mentioned above (we refer to this set of features as NoMorph ). We then add different subsets of features to the feature set and evaluate the accuracy of MaltParser using these features. The subsets that we test include adding up to three tokens from the top of the data structures used by the selected parsing algorithm, with references to the following data custom columns:  X  VERBFORM, indicating the  X  X orm X  feature described above  X  NUM, indicating the  X  X umber X  feature of the token  X  PERS, indicating the  X  X erson X  feature of the token  X  GEN, indicating the  X  X ender X  feature of the token
As described in Sect. 6.1 , the configuration suggested by MaltOptimizer for the in-domain All X  X ll scenario included using the Stack parsing algorithm. The morphological features may appear in the various data structures of the algorithm, and the parser may use their values to aid its decisions.

Table 9 shows the accuracy of parsing (the in-domain task in the All X  X ll configuration) with the features whose addition to NoMorph yields the highest improvement. The test set consisted of 590 utterances (2,474 tokens). Although VERBFORM provided some improvement in itself (second row), it did not provide further improvement when added to the combination of PERS, NUM and GEN (line 3). None of the improvements is statistically significant ( p [ 0.1).
 Table 10 depicts the changes in the scores of some specific relations when PERS, NUM, and GEN of the elements in the three top positions of the stack were added to the NoMorph features. 5 This set of features improved the scores of these relations (except Anonagr ) in almost every metric. Specifically, the big improvement in Ainf is clearly attributed to the verb form information that was made available to the parser. Note also that for some relations the increase in the labeled scores is higher than in the unlabeled scores, indicating the contribution of the features to identifying the grammatical relation correctly.
 7 Linguistic issues Different frameworks of dependency-based parsers produce different analyses for existing linguistic controversies (Nivre 2005 ). In addition to testing for feature improvement, our work aims to investigate whether contrasting approaches to actual syntactic annotation yield different accuracy rates. Several syntactic constructions that frequently occur in our data can be annotated in two distinctly-motivated ways. In this section, we check empirically these different approaches to syntactic analysis. All evaluations used MaltParser and were conducted on the in-domain task, in the All X  X ll configuration; the size of the training set was thus 3,286 utterances (11,155 tokens) and the size of the test set was 590 utterances (2,474 tokens).

In the following sections, we use two terms to refer to alternative analyses. The term Approach A refers to the annotation scheme described in Sect. 4 , while the term Approach B refers to an alternative approach that we present for the first time in this section. 7.1 Copula constructions and other forms of hay  X   X  X e X  First, we examine utterances with some form of the functional verb hay  X   X  X e X , which we term hay  X  constructions . In Hebrew, hay  X  constructions function in a variety of contexts, mainly copula and existential constructions (Rosen 1966 ; Berman 1978 ). For both types of constructions, which are quite common in Hebrew, the verbal form appears in either past or future tense. In present tense, the two constructions diverge. For copula constructions, the realization of the tense-carrying element is in some cases optional, and it usually takes the form of a pronoun when it is explicitly expressed. Thus, the same clause can occur without a copula, as in  X  eitan gav  X  ah  X  X itan tall X , or with a copula in the form of a pronoun, as in  X  eitan hu  X  gav  X  ah  X  X itan he tall X . For existential constructions, the verbal form alternates with the suppletive (non-verbal) forms yes  X   X  X here_is X  or  X  eyn  X  X here_is_not X .
Previous dependency-based parsers have suggested different ways to deal with copula constructions. 6 The scheme used for annotating English data within CHILDES (Sagae et al. 2010 ) views the verbs  X  X e X ,  X  X ecome X ,  X  X et X , etc., like other verbs, as the heads, and their nominal predicates as the dependents, as in Example 16: (16)
In Hebrew, however, there is no consistent paradigm of copula verbs. Moreover, the optionality of the copula in present-tense constructions requires consideration (Haugereid et al. 2013 ). The Stanford Parser English scheme (de Marneffe et al. 2006 ), for example, is motivated by the need for adaptablity to languages in which the copula is not necessarily explicitly represented. In this scheme, the nominal predicate is viewed as the head and the copula as its dependent. The subject is also dependent on the nominal predicate (Example 17). According to de Marneffe and Manning ( 2008 ), an additional motivation for this decision was to help applications extract the semantic information of the clause through the direct relation between the subject and the predicate. (17) Alternatively, while the Prague Arabic Dependency Treebank (Hajic  X  and Zema  X  nek 2004 ) treats a group of verbs which may act as copulas (referred to as  X  kana  X  X e X  and her sisters X ) as a subset of the entire verbal group and thus as heads, in clauses with zero copula the nominal element is analyzed as the head, as in Example 18. (18)
The scheme for annotating written Hebrew presented by Goldberg ( 2011 )is similar in this respect to the Prague Arabic dependency scheme. In a non-verbal clause with zero copula, the predicate is the head and the subject is its dependent (Example 19); when the copula is present, the predicate is also the head and the copula is its dependent (Example 20). (19) (20)
The past and future forms of hay  X  , in contrast, are viewed as ordinary verbs, and form the root of the sentence they appear in (Example 21). (21)
Our scheme for spoken Hebrew uses the label Acop to mark the relation between the copula and its argument, the copula being the head (Approach A). Alternatively, we use the label Xcop to mark the relation in which the copula is the dependent (Approach B). Similarly, we use the labels Aexs and Xexs for the two approaches of the existential marker. We automatically converted the annotation of Approach A to Approach B. Example 22 depicts an utterance containing an existential marker annotated according to Approach A, where the head is the existential element. The result of its conversion to Approach B is shown in Example 23. (22) (23)
We trained MaltParser on data annotated with both approaches and evaluated the accuracy of parsing. The test set included 590 utterances (2,474 tokens) out of which 45 utterances (271 tokens) included at least one occurrence of either Acop (
Xcop in Approach B) or Aexs ( Xexs in Approach B) according to the gold standard annotation. Table 11 shows the accuracy of parsing with the two alternatives of hay  X  constructions. Table 12 shows the accuracy when evaluating the alternative approaches only on the 45 utterances (271 tokens) that contain the Acop ( Xcop )or Aexs ( Xexs ) relations. Evidently, Approach A yields slightly better results, but the differences between the two approaches are not statistically significant ( p [ 0.1).
Copula-less constructions are rather common in Hebrew, and are far more common than utterances with a pronominal copula. Still, the training set of the in-domain evaluation scenario includes only 45 of them, just above 1 % of all utterances. Since nominal predicates are more often dependent on verbs, it is inconsistent to mark them as the root of utterances when they contain a hay  X  form. 7.2 The accusative marker Another form that presents a challenge for dependency-based parsing is the Hebrew accusative marker,  X  et . This morpheme behaves much like a preposition: it can either introduce a lexical noun phrase or inflect with a pronominal suffix, and it expresses Verb-Patient relations, similarly to other prepositions in Hebrew. Although the analysis of  X  et as a preposition is conventional (Danon 2001 ), its expressed on the surface if and only if the following noun is definite. The syntactic status of  X  et is thus unclear, and two types of analysis are possible: one option is to treat the dependency between  X  et and the noun following it similarly to the relation specified for all other prepositions in our scheme, with the noun functioning as the argument of  X  et ; the alternative is to treat the accusative marker as a dependent of the noun. In the first type of analysis we label the relation between the verb and the accusative marker Anonagr and between  X  et and the nominal element Aprep ,asin Example 24. (24)
In the second analysis, the nominal element is viewed as directly dependent on the verb (in a relation labeled Anonagr ), with a relation labeled Xacc assigned to  X  et , as shown in Example 25. (25)
The implication of the first analysis is that all constructions containing a verb followed by a preposition are treated systematically. This representation, however, results in inconsistency between definite and indefinite direct object constructions: as Anonagr , and so these two parallel constructions are structurally distinct (Example 26). While the second analysis reflects consistency between definite prepositional phrases (e.g., Example 9 above). 7 (26)
We automatically converted our original annotation scheme of  X  et as the head of the following nominal element (Approach A, Example 24) to the alternative scheme, where  X  et is a dependent of the nominal head (Approach B, Example 25). We trained MaltParser on data annotated in accordance with both approaches and evaluated the accuracy of parsing, again for the in-domain evaluation task in the All X  X ll configuration. Table 13 shows the accuracy of parsing for the two alternatives. The test set contained 590 utterances (2,474 tokens) out of which 41 utterances (215 tokens) contained at least one occurrence of  X  et . We also show (Table 14 ) the accuracy when parsing only the 41 utterances that contain  X  et . While there is a small advantage to Approach A, the differences between the two approaches are not statistically significant ( p [ 0.1).

The small difference in accuracy between the two approaches is supported by the distribution in the training data of prepositional arguments of verbs (consistent with Approach A) and of indefinite nominal arguments of verbs (consistent with Approach B). Both are relatively common in the training data, perhaps explaining why neither approach has a significant advantage over the other. 7.3 Prepositions as dependents In the annotation scheme we presented, prepositional phrases are headed by the preposition, labeled with the Aprep relation. An alternative analysis views prepositional phrases as headed by the nominal element, with the preposition depending on this head. In order to examine this alternative, we reversed the direction of all occurrences of the Aprep relation. In Approach A, this relation is headed by the preposition (including the accusative marker  X  et and the possessive marker s  X  el ). In Approach B, the nominal element is the head and the preposition depends on it in an Xprep relation. As a result, in Approach B the nominal element is directly dependent on the verb or noun that the preposition was dependent on in Approach A. Since the accusative marker is also affected by this transformation, this is an extension of the approach discussed in Sect. 7.2
Example 27 presents an utterance containing a prepositional phrase annotated according to Approach A, where the head is the preposition. The result of the conversion is shown in Example 28. (27) (28)
We trained the parser (again, on the in-domain task in the All X  X ll scenario) with the alternative approaches and evaluated the accuracy of parsing. Table 15 compares the results of the original scheme (Approach A) with the alternative representation (Approach B).

There is a significant advantage to Approach A in both UAS and LAS ( p \ 0.05). This advantage can be explained by the decrease in accuracy of the Anonagr and Mpre relations, especially the latter. Recall that there is a direct relation between a verb and a direct object labeled Anonagr , regardless of the annotation approach. With Approach B, nominal elements in prepositional phrases are directly attached to verbs as well, with no intervening preposition that could indicate whether this nominal element is a modifier or an argument. Thus, nominal elements are possibly mistaken for arguments when they are in fact modifiers. 7.4 Prepositional arguments of verbs A prepositional phrase following a verb can be either the verb X  X  argument or its modifier. The decision is hard, even for human annotators, and the phenomenon is ubiquitous, with over 1,100 cases in our corpora. For example, the preposition le- X  X o X  is considered a modifier of the verb (with the Mpre relation) in Example 29, but an argument (with the Anonagr relation) in Example 30. (29) (30)
These subtle differences between prepositional arguments and modifiers of verbs lead to poor (labeled) recall and precision of the Mpre relation, as is evident in Table 3 . In order to improve the overall accuracy of the parser, we altered the annotation scheme and created a new relation, Averb , that uniformly labels the attachment between a verb and a preposition, independently of whether the prepositional phrase is an argument or a modifier. The Mpre relation remains when a preposition is dependent on a noun, and the Anonagr relation now represents arguments of verbs which are not prepositions. We then trained the parser (again, on the in-domain task in the All X  X ll scenario) and evaluated the accuracy of parsing. Table 16 compares the results of the original scheme (Approach A) with the alternative representation (Approach B).

In the All X  X ll configuration, there seems to be a slight overall decrease in unlabeled accuracy, but the difference is not statistically significant. Closer inspection of the confusion matrices shows that in Approach B, the accuracy of the Averb relation is quite high (over 90 % in all individual metrics), and the accuracy of Anonagr actually improves slightly (compared with Approach A), but the accuracy of Mpre drops dramatically. Indeed, Mpre is confused with both Averb and Anonagr . We believe that a larger training corpus may be able to shed more light on this result. 7.5 Token representation The last issue we examined with respect to a potential effect on parsing accuracy is token representation. A morph-based approach calls for the split of words into morphemes, the atomic units that are combined to create words, whereas a word-based approach refers to words as the minimal units of the language (Blevins 2006 ; Tsarfaty and Goldberg 2008 ). Recall that in order to reduce sparseness of data, we pre-processed the transcripts, splitting pronominal suffixes and inflected preposi-tions to separate tokens; this renders the representation of the corpora partially morph-based. Using our annotated data and a simple conversion script we can investigate the differences between the word-based approach and the morph-based approach. More specifically, we examine the accuracy of parsing on data in which pronominal suffixes and inflected prepositions were not split.

We trained MaltParser on a version of the data that reflects such a word-based approach to token representation (Approach B). We compared the accuracy of parsing in this case to the accuracy obtained by MaltParser on the split data (Approach A), in the in-domain evaluation task and the All X  X ll scenario. Table 17 shows the results. The morph-based representation is better, but the differences are not significant ( p [ 0.1). Evaluation restricted to utterances that include a split token reveals similar results. 7.6 Summary In this section we examined various alternatives for relations in our annotation scheme, where both annotation approaches are linguistically plausible. Most of our evaluations showed no significant difference between the alternatives, except one (preposition as dependents) which showed a significant advantage to Approach A. The reason for these results could be that the data set is too small for any significant advantage for either approach. It is possible that the characteristics of these specific corpora X  X specially the relatively short utterances and the lack of morphological ambiguity X  X ad an effect on the outcome, preventing any significant advantage. Most likely, which annotation approach of the two is selected is less important, as long as it is plausible (both linguistically and computationally) and consistent. Thus, we conclude that the annotation scheme proposed originally (as described in Sect. 4 ) is as suitable for our corpora as the alternative annotation approaches. This is the scheme we use for the released version of our corpora. 8 Conclusions We presented a new annotation scheme for Hebrew spoken language, as part of the Hebrew section of the CHILDES database. The scheme handles some of the unique linguistic characteristics of Hebrew spoken language in general and child and child-directed language in particular. We showed that a parser trained on data annotated using this scheme achieves good results when parsing the same domain and is also adaptable the transcriptions were sometimes erroneous or ill-formed; this had an effect on the quality of the syntactic analysis, and future acquired data should help in this respect.
We showed that both MaltParser and MEGRASP produced relatively good accuracy. In both evaluation scenarios, MaltParser proved to be the better of the two, thanks to parameter tuning done by MaltOptimizer. In future work, it would be interesting to experiment with more parsers, especially transition-based ones that may be more adequate for a corpus of short utterances such as ours, but also graph-based ones. Several such parsers exist that could be trained on our corpus (McDonald et al. 2005 ; Bohnet 2010 ; Zhang and Clark 2011 ).

We examined the differences between learning from CDS and CS. Within the same domain there was no significant difference and both configurations yielded relatively high accuracy. However, when parsing out-of-domain (and, crucially, on different children) there was a clear advantage to training on CDS. We attribute this to the simplicity of the CS in the in-domain scenario as well as to differences in CS between the training set and the test set (and within the test set) in the out-of-domain scenario. We conclude that, as expected, there is some difficulty to adapt CS from one domain to another (also recalling the age gap between the domains) whereas CDS is more stable and less varied across domains.

Working with MaltParser allowed us to evaluate the impact of features derived from the morphological tier of the corpora. Although the accuracy of parsing using the feature set without extended morphological data is quite high, due to the fact that the basic feature set was optimized by running MaltOptimizer and to the presence of a gold standard morphological tier, when we used detailed morpho-logical information we were able to improve the accuracy of parsing even more. The best accuracy was exhibited using the morphological attributes  X  X ender X ,  X  X erson X  and  X  X umber X . Future work in this area can embark on a more systematic approach that has the sole purpose of examining the contribution of morphological features. This includes extracting more morphological attributes other than those that were used in this work, as well as a more elaborate search for subsets of features that are derived from MaltParser data structures.

We examined different annotation approaches for a few linguistic constructions, such as hay  X  constructions and accusative marker constructions. In most cases, significant advantages to either approach were not revealed. This can be attributed explanation may very well be that as long as the annotation is consistent it can produce reasonable results, regardless of the specific annotation approach. It would be interesting to see if this is a cross-linguistic phenomenon, e.g., for copula constructions that are challenging in several languages.

We utilized the fact that the input to the syntactic process is a fully disambiguated gold standard morphological tier. An interesting extension is to evaluate the parser on data with a morphological tier that was created automatically. Apart from an obvious decrease in accuracy we expect that this may also introduce some different effects when examining feature sets or linguistic issues. Another extension to this work is parsing of Hebrew spoken language from other domains. We leave these research directions for future research.
 Appendix 1: Dependency relations Table 18 summarizes the basic dependency relations we define in this work. We list below all the relations, providing a brief explanation and a few examples. AgreementArgument (Aagr) Specifies the relation between an argument and a predicate that mandates agreement.
 Non-agreementArgument (Anonagr) Specifies any argument of a verb which need not agree with the verb, including indirect arguments.
 Subordinate Clause (SubCl) Specifies the relation between a complementizer and the main verb of a subordinate clause.

ArgumentOfPreposition(Aprep) Specifies the relation between a preposition and NonFiniteArgument (Ainf) This relation is specified between a verb or a noun in the main clause and its non-finite verbal argument.
 ArgumentOfCopula (Acop) Specifies the relation between a copula and its predicate (either nominal or adjectival). See Sect. 7.1 for further discussion regarding this relation.
 ArgumentOfExistential (Aexs) Specifies a relation between an existential element and a nominal or adjectival predicate. See Sect. 7.1 for further discussion regarding this relation.
 Mdet Specifies a relation between a determiner and a noun.
 Madj Specifies a relation between an adjective and a noun.
 Mpre Specifies a relation between a dependent preposition and a head noun or a verb.
 Mposs Specifies a relation between a noun and a subsequent possessive marker, noted by the token  X  s  X  el  X , headed by the noun.
 Mnoun Specifies a noun X  X oun relation, where the first noun, the head, is in the construct state.
 Madv Specifies a relation between a dependent adverbial modifier and the verb it modifies. Mneg Specifies a negation of a verb or a noun.
 Mquant Specifies a relation between a noun and a nominal quantifier, headed by the noun.
 Msub Specifies a relation between a nominal element and a relativizer of a relative clause, headed by the nominal element. The main predicate of the subordinate clause is marked as the dependent of the relativizer with a RelCl relation. Voc Specifies a vocative.
 Com Specifies a communicator.
 Coordination (Coord) Specifies a coordination relation between coordinated items and conjunctions, most commonly we- X  X nd X , headed by the conjunction.
 Serialization (Srl) Specifies a serial verb.
 Enumeration (Enum) Specifies an enumeration relation.
 Unknown (Unk) Specifies an unclear or unknown word X  X ost commonly a child invented word X  X hich appears disconnected from the rest of the utterance and often functions as a filler syllable.
 Punctuation (Punct) Specifies a punctuation mark, always attached to the root. Appendix 2: The effect of MaltOptimizer 2.1 The features chosen by MaltOptimizer The Stack non-projective eager algorithm uses three data structures: a stack Stack of partially processed tokens; a queue Input which holds nodes that have been on Stack; and a queue Lookahead which contains nodes that have not been on Stack. This algorithm facilitates the generation of non-projective trees using a SWAP transition which reverses the order of the top two tokens on Stack by moving the top token on Stack to Input. The recommended feature set for the All X  X ll configuration is depicted in Table 19 . The features reflect positions within these data structures, where  X 0 X  indicates the first position. For example, the feature  X  X OSTAG (Stack[0]) X  Stack data structure. The NUM , GEN , PERS and VERBFORM features are short for the number, gender, person and verb form morphological features, respectively. Merge and Merge3 are feature map functions which merge two feature values and three feature values into one, respectively. ldep returns the leftmost dependent of the given node; rdep return the rightmost dependent; head returns the head of the node. For definitions of the rest of the features, refer to Nivre et al. ( 2007 ). 2.2 MaltParser X  X  default features MaltParser X  X  default parsing algorithm is Nivre arc-eager (Nivre 2003 ), which uses two data structures: a stack Stack of partially processed tokens and a queue Input of remaining input tokens. The feature set used by Nivre-arc is depicted in Table 20 . 2.3 The features chosen by MaltOptimizer for the out-of-domain configuration The feature set for the out-of-domain configuration suggested by MaltOptimizer is depicted in Table 21 . The similarities between the suggested MaltOptimizer configurations of the in-domain and out-of-domain scenarios are not surprising, as domain scenario.
 References
