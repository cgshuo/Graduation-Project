
Automatic classification has been a goal for machine learn-Hi Normal Strong Weak 
Figure 1: A decision tree from the training data set tennis. 5. If outlook is rain and wind is weak, then play tennis. 
We note that all rules include the attribute outlook. Suppose that we have a test data set in which outlook information is unknown. Then these rules cannot make any predictions. 
Hence, this rule set is not robust at all. However, we may have another rule set that can make some predictions in the presence of missing i.e. outlook information in test data. 
In real world applications, missing data in a database is very common, especially in a test database. For example, we may generate a diagnostic rule set from records with the complete check results. But when we apply the rule set, some records may miss one or more check results for some reasons. Hence, a rule set that can make reasonably accurate predictions in the presence of missing attribute values in test data is highly desirable in practice. We say that a rule set is more robust than another rule set if it can make more accurate predictions on incomplete test data than the other rule set. In the following, we will explore this problem and its solutions. 
Classification rule mining algorithms have been mainly developed for category prediction by the machine learning community. They use heuristic methods to find simple rule sets to explain training data well, and are generally cate-gorized into two groups [17], simultaneously covering algo-rithms namely C4.5 [18], and sequential covering algorithms such as AQ15 [15] and CN2 [6]. Most algorithms generate simple and accurate rule sets that cover all training data, hut these rule sets may not be robust in the presence of missing values as we will discuss in this paper. 
There are many proposals for improving predictive ac-curacy of traditional classifiers, among which Bagging [5] and Boosting [8, 91 are significant in reducing predictive er-rors. Both techniques utilize voting (weights are involved in 
Boosting) from a set of classifiers obtained by sampling the training database. However, Bagging and Boosting make predictions hard to understand by users. In this paper, we also consider multiple rule sets, but we disturb a database systematically and use the union of all rule sets. 
The proposal for association rule mining first appeared in [1]. Most research work has been on how to generate frequent itemsets efficiently since this may be the bottleneck of association rule mining. Apriori [2] is a widely accepted 
The requirement of using this accuracy is that the sup-port of a rule is not too small, for example, the absolute support number is not less than 30. If the support of a rule is too small, we need another estimation of test accuracy on very small sample data. Laplace accuracy can then be used number of all classes. 
Usually, the minimum confidence requirement of a class association rule is very high so it is natural to exclude con-flicting rules, such as A :=~ cl and A =-~ e2, in a complete rule set. 
In the practice of rule set based classification, a set of rules is usually sorted by decreasing accuracy, and tailed by a default prediction. This ordered rule set is called a rule based classifier. In classifying an unseen test record (an input record without class attribute information), the first rule that matches the case classifies it. If no rule matches the record, the default prediction is used. In this paper, we ignore the effect of the default prediction since we will con-centrate on the predictive power of a rule set. We formulise this procedure in the following. 
For a rule r, we use cond(r) to represent its antecedent (conditions), and cons(r) to denote its consequence. Given a test record T, we say rule r covers T if cond(r) C T. A rule can make a prediction on its covered record, de-noted by r(T) ~ cons(r). If cons(r) is the class of T, then the rule makes a correct prediction. Otherwise, it makes a wrong prediction. We let the accuracy of a prediction equals the accuracy of the rule making the prediction, denoted by acc(r(T) ~ c). If a rule gives the correct prediction on a record, then we say the rule identifies the record. 
DEFINITION 2. Let T be a record in database D and R a covers T. If two rules cover T we choose the one with the greater accuracy. If two rules have the same accuracy we choose the one with higher support. If two rules have the same support we choose the one with the shorter antecedent. 
Please note that in the above definition, we take the sup-port and the length of antecedent of a rule into consider-ation. This is because they have been minor criteria for sorting rules in a rule based classifier in previous practice, such as in [12]. It is easy to understand the preference of the highest support rule among a number of rules with the same accuracy. The preference for a short rule is consistent with the preference for a simple rule in traditional classification rule generation practice. 
As both accuracy and support are real numbers, in a large database it is very unlikely that a record supports two rules with the same accuracy and support. Therefore, we suppose that each record has a unique predictive rule for a given database and rule set in the rest of paper. 
The prediction of rule set R on record T is the same as that of the predictive rule of R on T with the same accuracy. 
Now we consider how to compare predictive power of rules. We use r2 C rl to represent cond(r2) C cond(rl) and cons(r2) = cons(r1). We call r2 is more general than rl~ or rl is more specific than r2. 
DEFINITION 3. Given two rules rl and r2, we say that r2 is stronger than rl iffr2 C rlAacc(r2) _&gt; acc(rl). We denote complete rule set, from the training database. When we say a predictive rule without mentioning a rule set, then it is with respect to the complete rule set. The test database is the same as the training database without class information. test databases, and hence we first define a k-incomplete database to be a new database with exactly k missing values from every record of the test database. The k-incomplete database Dk = { T' I T' C T, T E D, [T I -databases of D as a set of (~) (n is the number of attributes for D) databases in which each omit exactly k attribute (col-umn) information from D. For example, all 1-incomplete databases contains a set of n databases where each omits one attribute (column) information from D. We note that the 0-incomplete database of D is D itself. plete databases. k-incomplete databases wrt Rc where 0 &lt; k &lt; n. rules from a training database for all incomplete databases. 
Now we consider how to preserve all potentially predictive rules for some incomplete test databases. database is all predictive rules on all k-incomplete databases. dictions as the optimal rule set on all p-incomplete databases for O &lt; p &lt; k. way. A k-optimal rule set is a subset of the optimal rule set that makes prediction as well as the optimal rule set on a test database with k missing attribute value per record. As a special case, 0-optimal rule set makes predictions as well as the optimal rule set on a complete test database. least as robust as the k-optimal rule set. set. (k + 1)-optimal rule sets for D and Re. Then R k C R k+l. and we observe that the following chain always holds these optimal rule sets. Congressional Voting than the optimal rule set and more expensive to generate. The size of a k-optimal rule set is much smaller than that of the optimal rule set and is a little larger than that of a traditional classification rule set. multiple C4.5Rules 46 195 32 1 multiple C4.5Rules 70 20 21 8 Table 4: Size and generation time of different rule sets 
In our experiments, all rule sets are tested without de-fault prediction. This is because the default prediction may disguise the true accuracy. Consider database Hypothyroid: if we set the default prediction as Negative, then a classi-fier without any rule will give 95.2% accuracy. Clearly, this accuracy is misleading. 
We evaluated the predictive power of a rule set by the identification accuracy, which is the accuracy without de-fault prediction. 
Identification accuracy = (the number of identified records) / (the number of all records in a database) 
The identification accuracy is the proportion of identified records by the rule set in a database. The higher the accu-racy, the better the predictive power. Its range is between 0 to 100%. 
We tested all generated rule sets on /-incomplete test identification accuracy in Figure 2. In our experiment, the number of missing values is compared with the training data. When a training database already has missing values, then the missing values are additional. Each point in Figure 2 is the average of ten trials. 
From Figure 2, we can see that when test data is incom-plete, a rule set from single C4.5Rules performs poorly while both "precise" (the optimal rule set approach) and approx-
