 Mladen Kolar mladenk@cs.cmu.edu Han Liu hanliu@princeton.edu Eric P. Xing epxing@cs.cmu.edu In many modern problems, we are interested in study-ing a network of entities with multiple attributes rather than a simple univariate attribute. For exam-ple, when an entity represents a person in a social net-work, it is widely accepted that the nodal attribute is most naturally a vector with many personal infor-mation including demographics, interests, and other features, rather than merely a single attribute, such as a binary vote as assumed in the current literature of social graph estimation based on Markov random fields ( Banerjee et al. , 2008 ; Kolar et al. , 2010 ). In another example, when an entity represents a gene in a gene regulation network, modern data acquisition technolo-gies allow researchers to measure the activities of a single gene in a high-dimensional space, such as an im-age of the spatial distribution of the gene expression, or a multi-view snapshot of the gene activity such as mRNA and protein abundances, rather than merely a single attribute such as an expression level, which is as-sumed in the current literature on gene graph estima-tion based on Gaussian graphical models ( Peng et al. , 2009 ). Indeed, it is somewhat surprising that existing research on graph estimation remains largely blinded to the analysis of multi-attribute data that are preva-lent and widely studied in the network community. Ex-isting algorithms and theoretical analysis relies heavily on covariance selection using graphical lasso, or pe-nalized pseudo-likelihood. They can not be easily ex-tended to graphs with multi-variate nodal attributes. In this paper, we present a study on graph estimation from multi-attribute data, in an attempt to fill the gap between the practical needs and existing method-ologies from the literature. Under a Gaussian graphi-cal model, one assumes that a p -dimensional random vector X  X  R p follows a multivariate Gaussian dis-tribution with the mean  X  and covariance matrix  X  , with each component of the vector corresponding to a node in the graph. Based on n independent and identically distributed observations, one can estimate an undirected graph G = ( V, E ), where the node set V corresponds to the p variables, and the edge set E describes the conditional independence relationships among the variables, that is, variables X a and X b are conditionally independent given all the remaining vari-ables if ( a, b ) /  X  E . Given multi-attribute data, this ap-proach is clearly invalid, because it naively translates to estimating one graph per attribute. A subsequent integration of all such graphs to a summary graph on the entire dataset may lead to unclear statistical in-terpretation.
 We consider the following new setting for estimating a multi-attribute graph. Assume now a  X  X tacked X  long random vector X = ( X  X  1 , ..., X  X  p )  X  where X 1 R 1 , . . . , X p  X  R k p are themselves random vectors that jointly follow the multivariate Normal distribution, where  X  = Without loss of generality, we assume  X  = 0. Let G = ( V, E ) be a graph with the vertex set V = [ p ] E  X  V  X  V that encodes conditional independence re-lationships among ( X a ) a  X  V . That is, each node a  X  V of the graph G corresponds to the random vector X a and there is no edge between nodes a and b in the graph if and only if X a is conditionally independent of X b given all the vectors corresponding to the remain-ing nodes, X  X  ab = { X c : c  X  [ p ] \{ a, b }} . Such a graph is also known as a Markov network (of Markov graph), which we shall emphasize in this paper to con-tract an alternative graph over V known as the asso-ciation network, which is based on pairwise marginal independence. Conditional independence can be read from the inverse of the covariance matrix, as the block corresponding to X a and X b will be equal to zero. Let D n = { x i } i  X  [ n ] be a sample of n independent and iden-tically distributed vectors drawn from N ( 0 ,  X  ). For a vector x i , we denote x i,a  X  R k a the component cor-responding to the node a  X  V . Our goal is to esti-mate the structure of the graph G from the sample D n . Note that we allow for different nodes to have different number of attributes, which may be useful in certain applications, e.g., when a node represents a gene pathway in a regulatory network.
 Using the standard Gaussian graphical model for uni-variate nodal observations, one can estimate a Markov graph for each attribute individually, by estimating the sparsity pattern of the precision matrix  X  =  X   X  1 of the GMM. This is also known as covariance selection ( Dempster , 1972 ). For high dimensional problems, Meinshausen &amp; B  X uhlmann ( 2006 ) propose a parallel Lasso approach for estimating Gaussian graphical models by solving a collection of sparse re-gression problems. This procedure can be viewed as a pseudo likelihood based method. In con-trast, Banerjee et al. ( 2008 ), Yuan &amp; Lin ( 2007 ), and Friedman et al. ( 2008 ) take a penalized likelihood approach to estimate the sparse precision matrix  X  . To reduce estimation bias, Lam &amp; Fan ( 2009 ), Johnson et al. ( 2012 ), and Shen et al. ( 2012 ) devel-oped the non-concave penalties to penalize the like-lihood function. More recently, Yuan ( 2010 ) and Cai et al. ( 2011 ) proposed the graphical Dantzig se-lector and CLIME, which can be solved by linear pro-gramming and have better theoretical properties than the penalized likelihood approach. Under certain reg-ularity conditions, these methods have proven to esti-mate graph structure consistently ( Ravikumar et al. , 2011 ; Yuan , 2010 ; Cai et al. , 2011 ) and scalable soft-ware packages, such as glasso and huge , were de-veloped to implement these algorithms ( Zhao et al. , 2012 ). However, in the case of multi-attribute data, it is not clear how to combine estimated graphs to obtain a single Markov network reflecting the structure of the underlying complex system. This is especially the case when nodes in the graph contain different number of attributes.
 Katenka &amp; Kolaczyk ( 2011 ) proposed a method for estimating association networks from multi-attribute data using canonical correlation as a dependence mea-sure between two groups of attributes. However, as-sociation networks are known to confound the direct interactions with indirect ones as they only repre-sent marginal associations. In contrast, we develop a method based on partial canonical correlation , which give rise to a Markov network that is better suited for separating direct interactions from indirect con-founders. Our work is related to the literature on si-multaneous estimation of multiple Gaussian graphical models under a multi-task setting ( Guo et al. , 2011 ; Varoquaux et al. , 2010 ; Honorio &amp; Samaras , 2010 ; Chiquet et al. , 2011 ; Danaher et al. , 2011 ), however, the model given in ( 1 ) is different from models consid-ered in various multi-task settings and the optimiza-tion algorithms developed to handle the multi-task set-ting do not extend to handle the optimization problem given in ( 3 ) below.
 Unlike the standard procedures for learning the structure of GGMs (e.g., neighborhood selec-tion ( Meinshausen &amp; B  X uhlmann , 2006 ) or glasso ( Friedman et al. , 2008 )), which infer the partial corre-lations between pairs of nodes, our proposed method estimates the partial canonical correlations between pairs of nodes. Under this new framework, the contri-butions of this paper include: (i) computationally, an efficient algorithm is provided to estimate the multi-attribue graphs; (ii) theoretically, we provide sufficient conditions which guarantee consistent graph recovery; and (iii) empirically, a number of simulations are used to illustrate performance of the method. Additional results can be found in Kolar et al. ( 2012 ). Canonical correlation, a classical tool in multivariate statistics, is defined between two multivariate random variables as that is, computing canonical correlation between X a and X b is equivalent to maximization of correlation between two linear combinations u  X  X a and v  X  X b with respect to vectors u and v . Canonical correlation can be used to measure association strength between two nodes with multi-attribute observations. For ex-ample, in Katenka &amp; Kolaczyk ( 2011 ), a graph is es-timated from multi-attribute nodal observations by thresholding the canonical correlation between nodes, which may confound the direct interactions with indi-rect ones, as we describe later.
 In this work, we will use the partial canonical corre-lation to estimate a graph from multi-attribute nodal observations. A graph is going to be formed by con-necting nodes with non-zero partial canonical corre-lation. Let b A = argmin E [ || X a  X  AX ab || 2 2 ] and b B = argmin E [ || X b  X  BX ab || 2 2 ], then the partial canon-ical correlation between X a and X b is defined as  X  ( X a , X b ; X ab ) = max that is, the partial canonical correlation between X a and X b is equal to the canonical correlation between residual vectors of X a and X b after the effect of vari-ables X  X  ab is removed ( Rao , 1969 ).
 Let  X   X  denote the precision matrix under the model in ( 1 ). Using standard results for the multivariate Nor-mal distribution (see also equation (7) in Rao ( 1969 )), a straight forward calculation shows that  X  ( X a , X b ; X  X  ab ) 6 = 0  X  X  X  max This implies that estimating whether the partial canonical correlation is zero or not can be done by estimating whether a block of the precision matrix is zero or not. Furthermore, under model in ( 1 ), vectors X a and X b are conditionally independent given X  X  ab if and only if the partial canonical correlation is zero. A network built on this type of inter-nodal relationship is known as a Markov network , as it captures both local and global Markov properties over all arbitrary sub-sets of nodes in the network even though the network is built based on pairwise conditional (in)dependence properties. In Section 3 , we use the above observations to provide an algorithm that estimates the non-zero partial canonical correlation between nodes from data D n using the penalized maximum likelihood estima-tion of the precision matrix.
 Based on the relationship given in ( 2 ), we can motivate an alternative method for estimating the non-zero par-tial canonical correlation. Let a = { b : b  X  [ p ] \{ a }} denote the set of all nodes minus the node a . Then we observe that a zero block  X  ab can be identified from the regression coefficients when each component of X a is regressed on X a . We do not build an estimation procedure around this observation, however, we note that this relationship shows how one would develop a regression based analogue of the work presented in Katenka &amp; Kolaczyk ( 2011 ). 3.1. Penalized Log-Likelihood Optimization Based on the sample D n , we propose to minimize the penalized negative log-likelihood under the model in ( 1 ), where S = n  X  1 matrix and ||  X  ab || F denotes the Frobenius norm of  X  ab . The Frobenius norm penalty encourages blocks of the precision matrix to be equal to zero, similar to the way that the  X  2 penalty is used in the group Lasso ( Yuan &amp; Lin , 2006 ). Here we assume that the same number of samples is available per attribute. However, the same procedure can be used in cases when some samples are obtained on a subset of at-tributes. Indeed, we can simply estimate each element of the matrix S from available samples, treating non-measured attributes as missing completely at random (see Kolar &amp; Xing , 2012 , for more details). The dual problem to ( 3 ) is max where  X  is the dual variable to  X  . Note that the primal problem gives us an estimate of the precision matrix, while the dual problem estimates the covari-ance matrix. The proposed optimization procedure, described below, will estimate simultaneously the pre-cision matrix and covariance matrix, without explicitly performing an expensive matrix inversion.
 We propose to optimize the objective ( 3 ) using a block coordinate descent procedure, inspired by Mazumder &amp; Agarwal ( 2011 ). The block coordinate descent is an iterative procedure that operates on a block of rows and columns while keeping the other rows and columns fixed. Write and suppose that ( e  X  , e  X  ) are current iterates. With the block partition above, we have log |  X  | = log(  X  a, a ) + of the form and is obtained by minimizing Complete minimization over the variables  X  aa and  X  a, a at each iteration of the block coordinate descent can be computationally expensive. Therefore, we pro-pose to update  X  aa and  X  a, a using one generalized gradient step update (see Beck &amp; Teboulle ( 2009 )) in each iteration. Note that the objective in ( 5 ) is a sum of a smooth convex function and a non-smooth convex penalty, so that the gradient descent cannot be ap-plied. Given a step size t , generalized gradient descent optimizes a quadratic approximation of the objective at the current iterate e  X  , which results in the following two updates and for all b  X  a , where  X  t, X  ( A ) = (1  X  t X / || A || F ) + ( x ) + = max(0 , x ). If the resulting estimator b  X  is not positive definite or the update does not decrease the objective, we half the step size t and find new up-date. Once the update of the precision matrix, b  X  , is found, we update the covariance matrix, b  X  . Updates to the covariance matrix can be found efficiently, with-out performing expensive matrix inversion as follows b b the steps we arrive at the following algorithm: 1. Set the initial estimator e  X  = diag( S ) and e  X  = 2. For each a  X  [ p ] perform the following: 3. Repeat Step 2 until the duality gap Finally, we form a network b G = ( V, b E ) by connecting nodes with || b  X  ab || F 6 = 0.
 Convergence of the above described procedure to the unique minimum of the objective in ( 3 ) does not sim-ply follow from the standard results on the block co-ordinate descent ( Tseng , 2001 ) for two reasons. First, the minimization problem in ( 5 ) is not solved to con-vergence at each iteration, since we only update  X  aa and  X  a, a using one generalized gradient step update in each iteration. Second, blocks of variables, over which the optimization is done at each iteration, are not completely separable between iterations due to the symmetry of the problem.
 Lemma 1. For every value of  X  &gt; 0 , proposed pro-cedure produces a sequence of estimates ( e  X  ( t ) ) the precision matrix that monotonically decrease the objective value given in ( 3 ) , are positive definite and converge to the unique minimizer b  X  of ( 3 ) . 3.2. Efficient Identification of Connected When the target graph b G is composed of smaller, dis-connected components, the solution to the problem in ( 3 ) is block diagonal (possibly after permuting the node indices) and can be obtained by solving smaller optimization problems. That is, the minimizer b  X  can be obtained by solving ( 3 ) for each connected com-ponent independently, resulting in massive computa-tional gains. We give necessary and sufficient condi-tion for the solution b  X  of ( 3 ) to be block-diagonal, which can be easily checked by inspecting the empiri-cal covariance matrix S .
 Our first result follows immediately from the Karush-Kuhn-Tucker conditions for the optimization problem ( 3 ) and states that if b  X  is block-diagonal, then it can be obtained by solving a sequence of smaller optimiza-tion problems.
 Lemma 2. If the solution to ( 3 ) takes the form b  X  = separately for each l  X  = 1 , . . . , l , where S l  X  are subma-trices of S corresponding to  X  l  X  .
 Next, we describe how to identify diagonal blocks of b [ p ] and assume that the nodes of the graph are ordered The following lemma states that the blocks of b  X  can be obtained from the blocks of a thresholded sample covariance matrix.
 Lemma 3. A necessary and sufficient conditions for b  X  to be block diagonal with blocks P 1 , P 2 , . . . , P l || S ab || F  X   X  for all a  X  P j , b  X  P j  X  , j 6 = j  X  . Blocks P 1 , P 2 , . . . , P l can be identified by forming a p  X  p matrix Q with elements q ab = 1I {|| S ab || F &gt;  X  } and computing connected components of the graph with adjacency matrix Q . The lemma states also that given two penalty parameters  X  1 ,  X  2 ,  X  1 &lt;  X  2 the set of unconnected nodes with penalty parameter  X  1 is a subset of unconnected nodes with penalty parameter  X  . The simple check above allows us to estimate net-works on datasets with large number of nodes, if we are interested in networks with small number of edges. However, this is often the case when the networks are used for exploration and interpretation of complex sys-tems. In this section, we provide theoretical analysis of the estimator described in  X  3 . In particular, we provide sufficient conditions for the consistent graph struc-ture recovery under the assumption that, for each 1 rameter  X  , where  X   X  aa is a diagonal element of  X   X  . Re-call that Z is a sub-Gaussian random variable if there exists a constant  X   X  (0 ,  X  ) such that A statement of a general result is given in ( Kolar et al. , 2012 ).
 Our assumptions involve the Hessian of the function f ( A ) = tr SA  X  log | A | evaluated at the true  X   X  , H = covariance matrix  X   X  . The Hessian and the covariance matrix can be thought of as block matrices with blocks of size k 2  X  k 2 and k  X  k , respectively. We will make use of the operator C (  X  ) that operates on these block matrices and outputs a smaller matrix with elements that equal to the Frobenius norm of the original blocks. For example, C (  X   X  )  X  R p  X  p with elements C (  X   X  ) ab { ( a, b ) : ||  X  ab || F = 0 } . With this notation introduced, we assume that the following irrepresentable condition holds; there exists a constant  X   X  [0 , 1) such that We will also need the following quantities to specify the results  X   X   X  = |||C (  X   X  ) |||  X  and  X  H = |||C ( H These conditions extend the conditions specified in Ravikumar et al. ( 2011 ) needed for estimation of net-works from single attribute observations.
 We have the following result that provides sufficient conditions for recovery of the graph structure. Proposition 4. Set the penalty parameter  X  in ( 3 ) as where  X  &gt; 2 . If where s is the maximal degree of nodes in G , C 1 = (48 and ) then P b G = G  X  1  X  p 2  X   X  . In this section, we perform a set of simulation stud-ies to illustrate finite sample performance of our pro-cedure. We demonstrate that the scalings predicted by the theory are sharp. Furthermore, we compare against three other procedures: 1) a procedure that uses the glasso first to estimate one network over each of the k individual attributes and then creates an edge in the resulting network if an edge appears in at least one of the single attribute networks, 2) that of Guo et al. ( 2011 ) and 3) that of Chiquet et al. ( 2011 ) (see also Danaher et al. , 2011 ). We have also tried ap-plying the glasso to estimate the precision matrix for the model in ( 1 ) and then post-processing it, so that an edge appears in the resulting network if the cor-responding block of the estimated precision matrix is non-zero. The results were worse compared to the first baseline, so we do not report them here. The tuning parameters are selected by minimizing the Bayesian in-formation criterion, which balances the goodness of fit of the model and its complexity, over a grid of param-eter values. For our multi-attribute method, it takes the following form BIC(  X  ) = tr S b  X   X  log | b  X  | + Theoretical results given in  X  4 predict the sample size needed for consistent recovery of the underlying graph. In particular, Proposition 4 suggests that we need n =  X s 2 k 2 log( pk ) samples to estimate the graph struc-ture consistently, for some  X  &gt; 0. Therefore, if we plot the hamming distance between the true and recov-ered graph structure against  X  , we expect the curves to reach zero distance for different problem sizes at a same point. We verify this on randomly generated chain and nearest-neighbors graphs.
 We generate data as follows. A random graph with p nodes is created by first partitioning nodes into p/ 20 connected components, each with 20 nodes, and then forming a random graph over these 20 nodes. A chain graph is formed by permuting the nodes and con-necting them in succession, while a nearest-neighbor graph is constructed following the procedure outlined in Li &amp; Gui ( 2006 ). That is, for each node, we draw a point uniformly at random on a unit square and com-pute the pairwise distances between nodes. Each node is then connected to s = 4 closest neighbors. Since some of nodes will have more than 4 adjacent edges, we remove randomly edges from nodes that have de-gree larger than 4 until the maximum degree of a node in a network is 4. Once the graph structure is created, we construct a precision matrix, with non-zero blocks corresponding to edges in the graph. El-ements of the diagonal blocks take values as 0 . 5 | a  X  b | 0  X  a, b  X  k , while off-diagonal blocks have elements with the same value, 0 . 2 for chain graphs and 0 . 3 /k for nearest-neighbor networks. Finally, we add  X  I to the precision matrix, so that its minimum eigenvalue is equal to 0 . 5. Note that s = 2 for the chain graph and s = 4 for the nearest-neighbor graph. Simulation results are averaged over 100 independent runs. Figure 1 shows results of the simulations. Each row in the figure reports results for one procedure, while each column in the figure represents a different simu-lation setting. For the first two columns, we set k = 3 and vary the total number of nodes in the graph p . The third simulation setting sets the total number of nodes p = 20 and changes the number of attributes k . In the case of the chain graph, we observe that for small sample sizes method of ( Chiquet et al. , 2011 ) outperforms all the other procedures. We note that the multi-attribute method is estimating many more parameters, which require large sample size in order to be estimated consistently. However, as the sample size increases, we observe that multi-procedure starts to outperform the other procedures. In particular, for the sample size indexed by  X  = 13 all the graph are cor-rectly recovered, while other procedures fail to recover the graph consistently at the same sample size. In the case of nearest-neighbor graph, none of the methods recover the graph well for small sample sizes. However, for moderate sample sizes, multi-attribute procedure outperforms the other methods. Furthermore, as the sample size increases none of the other method recover the graph exactly. This suggests that the conditions for the consistent graph recovery may be weaker in the multi-attribute setting.
 Next we investigate a situation where the multi-attribute procedure does not perform as well as the procedures that estimate multiple graphical models. One such situation arises when different attributes are conditionally independent. To simulate this situation, we follow the data generating approach as before, how-ever, we make each block  X  ab of the precision ma-trix  X  a diagonal matrix. Figure 2 summarizes re-sults of the simulation. We observe that methods of ( Chiquet et al. , 2011 ) and ( Guo et al. , 2011 ) perform better, since they are estimating much fewer parame-ters than the multi-attribute procedure. Glasso does not utilize any structural information underlying the estimation problem and requires larger sample size to estimate the graph correctly than other procedures. A completely different situation arises when the edges between nodes can be inferred only based on inter-attribute data, that is, when a graph based on any individual attribute is empty. To generate data under this situation, we follow the procedure as before, but with the diagonal elements of the off-diagonal blocks  X  ab set to zero. Figure 3 summarizes results of the simulation. In this setting, we clearly see the advan-tage of the multi-attribute procedure.
 In this paper, we have proposed a solution to the prob-lem of learning networks from multivariate nodal at-tributes, which arises in a variety of domains. Our method is based on simultaneously estimating non-zero partial canonical correlations between nodes in a network. When all the attributes across all the nodes follow joint multivariate Normal distribution, our pro-cedure is equivalent to estimating conditional indepen-dencies between nodes, which is revealed by relating the blocks of the precision matrix to partial canonical correlation. Although a penalized likelihood frame-work is adopted in the current paper for estimation of the non-zero blocks of the precision matrix, other ap-proaches like neighborhood pursuit or greedy pursuit can also be developed. Thorough numerical evalua-tions and theoretical analysis of these methods is an interesting direction for future work.

