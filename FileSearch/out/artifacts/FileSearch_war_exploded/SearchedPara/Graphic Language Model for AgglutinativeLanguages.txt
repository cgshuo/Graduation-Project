
Miliwan Xuehelaiti 1 , 2 , 3  X  , Kai Liu 2 , Wenbin Jiang 2 , and Tuergen Yibulayin 1 Language model is one of the most important models in NLP, it describes proba-bilities of sentences in natural language. Many NLP tasks can be boiled down to the modeling of language model, such as transliteration, speech recognition, part of speech tagging and so on. One of most widely used language model is n-gram language model[3], which models words in sentences with local context environ-ment in an linear way. The n-gram language model is simple and effective, and it have got excellent performance on Chinese, English and other languages with simple morphological form.
 North/South Korea, Japan, Mongolian, Turkey and other countries in Middle East Asia and other areas. Agglutinative languages differ from languages with simple morphological form (such as English and Chinese) in their sentence and word-formation[2], in which the composition of each word in agglutinative lan-guage follows different word-building rules according to simple observation: each word of agglutinative language is composed by a word-stem and any number of affixes, in where constrained relations exist between stem and affixes; and simi-lar relations exist in stems of different words. The former rule lead to the data sparseness of word of agglutinative language, the latter rule makes it hard to seize the relation between stems, for there maybe some affixes between stems in different words.
 those relations can not be simply modeled as linear sequence. As a matter of fact, traditional n-gram language language model which models sentences as linear sequence can not obtain idea results on agglutinative languages. In this paper, we propose a novel graphic language model which can depict those relations more deeply. More specifically, our graphic language model models the generative relations between stem and affixes in a word and the relations between stems in different words, which relations can hardly be modeled by traditional linear language models.
 ed natural language processing tasks (morphological analyzing and statistical machine translation(SMT)) are adopted to verify our graphic language model. In the experiments, both tasks show that graphic language model gets significant improvements compares to n-gram language model. In morphological analyzing, the accuracy gains 0 . 8% improvements due to the new style language model, while in SMT it gains more than 1 . 1 BLEU improvement. Furthermore, the graphic language model is simple, and the complexity of it is approximate to the n-gram language model.
 istics of agglutinative languages; Traditional linear language model is described in Section 3; We propose our graphic language model for agglutinative language in Section 4; And the methods of utilizing proposed graphic language model in two NLP tasks are shown in Section 5; Finally, we present the experiments of the two NLP tasks with graphic language model on Uyghur in Section 6 and conclude in Section 8. Agglutinative language is a kind of language that its words are made up of dis-tinct morphemes by a linear sequence way, and each component of meaning is represented by its own morpheme. Agglutinative languages have many charac-teristic, more specifically, we take Uyghur as our study case. Uyghur is one of typical agglutinative languages, it is a Turkic language which is widely used in Western China by Uyghur people, and it shares some characteristics with other agglutinative languages: { Each word jointed with different affixes will show different meanings. Take { Each word can be jointed with multiple morphemes, and such a word can first morpheme of each word is the stem of the word in Uyghur, and each word should have one and only one stem, which conveys the main semantic meaning of the word. As the example above,  X  X izmet X (job) and  X   X  Olchem  X (standard) are stems. And all morphemes after stems are affixes, which convey minor semantic meanings or grammar information. Furthermore, all stems in a sentence form the skeleton of the sentence. signing the probability P ( w i j ) to the sequence by means of certain probability distribution. And language model is widely used in many NLP applications, such as speech recognizing, morphological analyzing, machine translation and so on. calculate the probability in a linear way. One of the most typical models is n-gram language model, which assigns a given word X  X  probability by means of its previous contiguous words. In n-gram language model, the probability of the sequence w i j is assigned approximated as: where n is the n-gram size of the language model. And P ( w k j w k n +1 k 1 ) can be calculate from its frequency in corpus: where #( ) means the total count of the n-gram in the corpus. And in practical terms, the probability above needs some kind of smoothing, such as  X  X dd-one X , Good-Turing, Kneser-Ney smoothing and so on[4]. 3.1 Linear modeling for Agglutinative Language Because of the characteristic of agglutinative language, common linear modeling for agglutinative language will encounter some problems: { Data sparseness. For each word in agglutinative language can be made of { Ignoring the relations in/between words. Traditional linear language model Since there are several drawbacks of linear modeling agglutinative language, we propose a morpheme based directed graphic language model. And the directed graphic structure can better describe the characteristic of agglutinative language. directed graphic with morphemes as basic elements, where all stems are connect-ed linearly in left-right order and all affixes are connected to previous affixes or stems. And it can be divided into two linear parts: { stem-stem: this part assigns the probability to all stems, similar to n-gram { stem-affix: this part calculates all affixes probabilities by means of preceding sentences X  probabilities as follows: where stems s and affixes a are obtained from morphological analyzing results of the sentence w 1 n . The training method of both parts of this model can refer to n-gram language model. In this way, we design a morpheme based directed graphic language model, which is supposed to be a better language model for agglutinative language. Algorithm 1 Estimating Probability of Sequence Training Firstly, we morphological analyze the training corpus into stemmed one. Secondly, according to the stemmed corpus, we obtain stem-stem and stem-affix corpora by removing affixes and splitting sentences by words respectively. Then, those corpora are utilized to train linear language model for stem-stem and stem-affix respectively.
 Estimating Probability Algorithm 1 outlines the estimation procedure in its entirety. In line 1, we analyze the input sequence by morphological analyzing procedure M A ( ) and obtain the stemmed corpus. The stemmed corpus is uti-lized to obtain stem-stem sequences by removing all affixes in the corpus in line 2. Correspondingly, in line 3 sentences are split according to words and then organized into stem-affix sequences. From line 4-7, we calculate the both parts X  probabilities with corresponding sequences and sub models through procedure LM ( , ). And the final score of the model is combined by scores from sub-models in line 8. 5.1 Morphological Analyzing Morphological analyzing is one of the most important NLP tasks in agglutina-tive language[1]. The quality of morphological analyzing will affect other NLP tasks, which are based on morphological analyzing. There are three sub-task in morphological analyzing, including stemming, restoring the changed letter and POS tagging, in which we select the first sub-task stemming as the application to verify our graphic language model. Stemming is similar to segmentation, it splits each word into morphemes (Figure 2), including a stem and several following af-fixes. According to the characteristic of agglutinative language, stemming needs the contexts of inside or outside the word, where language model is available and important.
 with words from position i to j in sentence. And each word can be segmented into several morphemes m , which contain a single stem s and several following affixes a . In this task, we try to find the most probable morphological segmentation m 1 n  X  of sentence w 1 n .
 with the maximum language model probability: where m denotes a morpheme, and m 1 n  X  is the selected morpheme sequence of the of morpheme m i with context m i n +1 i 1 .
 quence with the maximum model probability with stems and affixes: where s i denotes the stem of the i th word, and a i,j denotes the j th affix of i th word. The first term of Formula 7 is the stem-stem part of our graphic model and the second term is the stem-affix part. 5.2 Machine Translation Machine translation is one of the hardest problems in NLP. The performance of statistical machine translation is highly depended on the quality of the language model (cite), and it is a good task to verify the quality of language model. In this paper, we try to verify the effectiveness of graphic language model with agglutinative language as target side.
 performed on two different SMT system with different granularities: Word Based The SMT model is trained with words in agglutinative language side as the basic translation unit, and this kind of SMT system has several characteristics: { It has large-grained translation unit, which may suffer from data sparseness { Shorter sequences of agglutinative language will be translated while we use { Word based translation system is free to recombination of morphemes into Morpheme Based Correspondingly, the stemmed sentences are used to train the SMT model and stems and affixes are the basic translation unit here: { Smaller-grained translation unit means less data sparseness problem. { Longer sequences have to be translated while stems and affixes are chosen, { Morpheme based translation system have to recombine stems and affixes In this section, we verify our graphic language model through two applications. And there are three different types of language model will be utilized in the experiments: { Word linear LM: the n-gram language model based on words, and it will be { Morphemes linear LM: morpheme based n-gram language model which will { Graphic LM: our graphic language model for agglutinative language, and 6.1 Morphological Analyzing DataSet We make use of an annotated corpus Mega-words Corpus of Morpho-logical Analysis of Uyghur, which is manually annotated by Xinjiang multilingual key laboratory. And it contains about 67 thousands sentences, from which we select 5% as our testing set.
 Training and Evaluation We train a 5-gram language model on morphemes and the linear part of our graphic model by SRI Language Modeling Toolkit[9] with Kneser-Ney smoothing. And we simply evaluate the stemming result by precision and recall of the morphemes.
 Results As the results shown in Table 3, our graphic language model shows advantage on morphological analyzing compared to morpheme linear language model, where precision obtain an improvement of 0 . 5% and more than 1% im-provement on recall. 6.2 Machine Translation In this section, we compare our graphic language model with linear n-gram lan-guage model in SMT task. In SMT task, two different granularities are employed to verify the effectiveness of our graphic language model. One experiment is per-formed on word, while the other is performed on the results of morphological analyzing (stems and affixes).
 DataSets For bilingual training data, we select Chinese-Uyghur corpus with 120 thousand parallel sentence pairs, which includes fifty thousand sentence pairs from corpus provided by CWMT 2011 evaluation task[5]. We obtain mor-phological result of the corpus by performing Uyghur morphological analyzer 1 on the corpus. The parallel corpus X  X  word alignments are obtained by running GIZA++[6] on the corpus in both directions and applying  X  X row-diag-and X  re-finement.
 Training and Evaluation We use the development set provided by CWMT 2011 2 evaluation task as our development set, and we organize 1000 sentence pairs as our own test set. The quality of translation is evaluated by the NIST BLEU-4 metric[7]. We make use of the standard MERT as the tuning algorithm to tune our cascaded translation model X  X  parameters on development set. Baselines and Our model We apply SRI Language Modeling Toolkit to train language models with modified Kneser-Ney smoothing on Uyghur side of the training corpus. The open source SMT decoder Moses[8] is selected as our base-line, which contains implementation of hierarchical phase model (Moses-chart). Correspondingly, our model is based on the same decoding system Moses, and train our graphic language model on the same corpus (training corpus). Results The experiment result is shown in Table 4, which line 2-7 show the results of word based SMT model with different language model respectively. And line 9-10 give the results of morpheme based SMT model with both linear language model and our graphic language model.
 those linear modeling language model. And both parts of our graphic language model (stem-stem, stem-affix) show their effectiveness on experiment, while the whole model shows better performance. Meanwhile, those improvements prove that it is reasonable to model agglutinative language in stem-stem and stem-affix style, and this style of structure can describe some kinds characteristic of agglutinative language. Language Model In addition to n-gram language model, there are much work is devoted into language model. Some kind of structured language model aims at modeling the structures of language and overcoming the locality problem[10] and neural network is employed to improve the work[11]. But so far, there is not any work on the structure of agglutinative language.
 Morphological Analyzing There are a lot of supervised work on morpholog-ical analyzing for each language respectively: Japanese[12], Arabic[13], and so on. Correspondingly, unsupervised ones (e.g.[14]) are also available. And mor-phological analyzing is proved to be an important task for other NLP task (e.g. SMT [16 X 18]).
 Machine Translation So far, most studies of agglutinative related machine translation are base on agglutinative to non-agglutinative translation, such as, for Turkish[15 X 17], Korean[18, 19] and others[20]. And there is also work on alignment between agglutinative language and other languages in translation purpose[21, 22]. While there is less work on translation of non-agglutinative lan-guage to agglutinative language[23]. In this paper, we model agglutinative language with graphic structure on the basis of the characteristics of agglutinative language by observations. The novel language model can better describe the agglutinative language and remit data sparseness, where evidences are provided by the experiments of different NLP tasks. The experiment results show significant improvements on morphological analyzing and SMT tasks with 0.8 F-score improvements and 1.1 BLEU im-provements.
 model the agglutinative language (e.g. indirected graph or all connected graph) and involve more feature of agglutinative language into our model, or directly model the language model as discriminative model.

