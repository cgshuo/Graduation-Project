 In many applications, association rules will only be inter-esting if they represent non-trivial correlations between all constituent items. Numerous techniques have been devel-oped that seek to avoid false discoveries. However, while all provide useful solutions to aspects of this problem, none provides a generic solution that is both flexible enough to accommodate varying definitions of true and false discover-ies and powerful enough to provide strict control over the risk of false discoveries. This paper presents generic tech-niques that allow definitions of true and false discoveries to be specified in terms of arbitrary statistical hypothesis tests and which provide strict control over the experimentwise risk of false discoveries.
 Categories and Subject Descriptors: H.2.8 [Database Management] Database Applications: data mining General Terms: Algorithms, Performance, Reliability, Ex-perimentation Keywords: Association rules, Rule discovery, Statistics
Association rule discovery [1] finds collections of items that co-occur frequently in data. In many applications, such rules will only be interesting if they represent non-trivial correlations between all constituent items. For the purposes of this paper we will call such associations significant rules and all remaining associations false discoveries .Manytech-niques have been developed that seek to avoid false discov-eries [1, 3, 4, 5, 10, 12, 16, 18, 20, 22, 25, 23, 28, 29]. This paper builds upon this body of previous work, presenting two generic techniques that both allow definitions of true and false discoveries to be specified in terms of arbitrary sta-tistical hypothesis tests, while providing strict control over the risk of false discoveries. We show that each has relative strengths and weaknesses and provide analyses of these. We substantiate the need for strict control over the risk of false discoveries, showing that on some real-world tasks there is Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. potential for all  X  X iscoveries X  to be false unless appropriate safeguards are employed.
We consider the problem of finding rules from data D = t ,t 2 ,...,t n ,whereeach transaction or record t i  X  I and I = { item 1 ,item 2 ,...item m } is the set of items of which transactions are composed. For market-basket data items are atomic forms and for attribute-value data items have the form a i = v i,j where a i represents an attribute and v i,j of a i . For attribute value data, no transaction t i ,1  X  may contain two items a i = v i,j and a i = v i,k , j = k .That is, each transaction may contain at most one value for each attribute. Rules take the form x  X  y where x  X  I and y  X  I . Note that we limit the consequent y to a single value. While many association rule techniques allow multiple values in the consequent y , the techniques we present generalize directly to multiple-value consequents and a single rule with multiple elements in the consequent can be represented by multiple rules with single elements in the consequent.

We are potentially interested in a number of properties of a rule x  X  y relative to D , and these properties vary from application to application. In this paper we utilize support [1], confidence [1], lift [16] and leverage [22], defined as follows: conf ( x  X  y, D )= sup ( x  X  y, D ) / |{ i : x  X  t i }| (2) Note that the parameters x  X  y and D will be omitted from these functions where they can be determined from the context.

The original association rule task [1] was to find all rules x  X  y such that sup  X  minsup and conf  X  minconf ,where minsup and minconf are user-specified constraints.
Typically, rules will only be interesting if they represent non-trivial correlations between items. High values of minsup and minconf usually deliver rules for which x and y are cor-related when applied to the sparse data typical of market-basket analysis [20]. However, as will be demonstrated in the experiments below, this is not the case for dense data such as typical attribute-value data. Also, there is a serious problem that x may contain items that are independent of y , and hence potentially misleading. To illustrate this prob-lem consider a rule { pregnant } X  oedema that represents a relationship between pregnancy and oedema. Now con-sider { pregnant , female } X  oedema . Assuming that all cases of pregnancy will be female, this will represent an equally strong correlation to the first rule, but in most contexts re-dundant rules such as this will not be useful so long as the first rule together with the further rule { pregnant } X  female are known. For a different kind of example consider another term dataminer that we will assume is in no way related to oedema .Inthiscase { pregnant , dataminer } X  oedema should represent as strong a correlation between the an-tecedent and consequent as the first rule, the only differ-ence being a reduction in support and random differences in confidence resulting from sampling effects. However, in most contexts unproductive rules such as this will be of no interest so long as the first rule is known.

Such redundant and unproductive rules represent rules x  X  y for which there exists z  X  x such that P ( y | x )= P ( y | x \{ z } ), or, in other words, for which z and y are conditionally independent given x \{ z } .

Apriori, one might expect there to be very large numbers of unproductive rules, as from every single productive rule x  X  y many unproductive rules can be generated by insert-ing into x any arbitrary collections of unrelated items.
Note that for the rest of this paper we will assume that the analytic task of interest is to identify positive rules and hence we will regard as false discoveries any negative rules such as { male } X  pregnant [ confidence = 0 . 0 ]. This seems a reasonable assumption in the context of rule discovery us-ing a minimum support constraint, as strong, and hence presumably interesting, negative associations will have low support and hence are excluded from consideration by the imposition of a minimum-support constraint. However, the techniques that we advance generalize directly to different definitions of false discoveries, requiring only the use of dif-ferent constraints and statistical tests.
Clearly it is desirable to avoid false discoveries and nu-merous techniques have successfully tackled aspects of this problem.

Non-redundant rule techniques [3, 28] identify and discard rules such as { pregnant , female } X  oedema . Specifically, they discard rules x  X  y for which  X  z  X  x : sup ( x  X  y )= sup ( x \{ z } X  y ).

A more powerful filter is provided by a minimum improve-ment constraint [5]. The improvement of rule x  X  y is defined as It represents the difference between the confidence of a rule and the highest confidence of any of its generalizations. A minimum improvement constraint is justified in contexts where only positive associations are of interest. In that case an association is unlikely to be of interest unless it repre-sents a stronger correlation than any of its generalizations. A redundant rule will have improvement no greater than 0.0, as for it to be redundant it must have a generalization with identical confidence. Thus, a minimum improvement con-straint is stronger than a non-redundant rule constraint as it rejects all redundant rules as well as many unproductive rules. The only unproductive rules that such a minimum-improvement constraint will fail to reject are those for which random sampling effects happen to result in raised confi-dence for a rule by chance. However, almost 50% of rules may fall into this category, because confidence is as likely to be raised as lowered through sampling effects and is unlikely to exactly represent the true probability of the consequent given the antecedent unless that probability is 1.0. If the minimum improvement constraint is set high enough to ex-clude the majority of these cases, it is also likely to exclude many productive rules.

An even stronger filter is represented by the use of statisti-cal hypothesis tests, either to test for independence between x and y [10, 17, 18, 29] or to test for unproductive rules [25]. We will focus here on the test for unproductive rules, as it most directly addresses the issue at hand, but the key points also apply to the other approaches. For the sake of compu-tational efficiency this test compares x  X  y only against the global frequency of y and against each of its immediate gen-eralizations x \{ z } X  y ,where z  X  x . We do not test against all generalizations as there are 2 | x |  X  1 of these, and hence for large antecedents the computation would be infeasible.
This test rejects a rule x  X  y if p  X   X  for a Fisher exact test [2] for improvement with respect to any of the rule X  X  immediate generalizations x \{ z } X  y and with respect to {} X  y .The p value for the test for improvement with respect to an immediate generalization x \{ z } X  y can be calculated as follows. Let a = |{ i : x  X  t i  X  y  X  t i }| support for the rule), b = |{ i : x  X  t i  X  y  X  t i }| (the number of transactions that contain x but not y ), c = |{ i :( x t  X  z  X  t i  X  y  X  t i }| (the number of transactions that contain y and all the x values other than z but not z )and d = |{ ( x \{ z } )  X  t i  X  z  X  t i  X  y  X  t i }| (the number of transactions that contain all the x values other than z but neither y nor z ). Here, q ! denotes the factorial of q .Byconvention  X  =0 . 05 is used. Equation (6) is also used to calculate the p value for the test for improvement with respect to {} X  y ,using the same a and b , but with c = |{ i : x  X  t i  X  y  X  t i }| (the number of transactions that contain y but not x )and d = |{ i : x  X  t i  X  y  X  t i }| (the number of transactions that contain neither x nor y ).

The use of this statistical test avoids the problem of set-ting an appropriate minimum improvement constraint, as it rejects all rules for which there is insufficient evidence that improvement is greater than zero. However, it still suffers from a very serious problem known as the multiple compar-isons or multiple tests problem. For each rule considered, the risk of it being accepted if it is not productive is no more than  X  . Now consider a typical market basket task for which more than 10 22 potential rules might be considered. If  X  =0 . 05 and none of these rules is productive it is still possible that as many as 5  X  10 20 rules might be accepted. Clearly this is an undesirable state of affairs.
 An alternative approach is to use shrinkage estimates, or Bayesian smoothing, to provide conservative estimates of the true probability of a set of items [12, 23, 26]. These ap-proaches can be very effective at reducing the overestimates of measures such as support or confidence that can occur for rules with low support. Their use can reduce type-1 error with respect to minimum support or confidence (or simi-lar) constraints. However, they do not allow for the number of alternative rules under consideration, and hence do not address the multiple tests problem. Nor do they provide a general mechanism for assessing arbitrary criteria for defin-ing false discoveries.

One solution that has been proposed is randomization tests [20]. Under this approach the data are randomized to establish the null hypothesis to be tested (for example, that x and y are independent). The rule discovery software is run under multiple such randomizations and settings are identified such that rules would be discovered for no more than  X  proportion of the runs. If the software is subse-quently run on the non-randomized data with these set-tings the probability that any rule discovered satisfies the null hypothesis is no more than  X  . Unfortunately, however, this approach does not solve the problem at hand, as it re-quires that a single randomization of the data establish all null hypotheses that are required. Consider the example of { pregnant , dataminer } X  oedema . Tobeabletorejectthis rule we would need to randomize the data so as to make dataminer independent of pregnant and oedema while re-taining the existing correlations between the latter two val-ues. However, we also need to test whether pregnant is conditionally independent of oedema given dataminer ,and for this purpose we need to randomize the data to make pregnant independent of dataminer and oedema while not altering any correlations between this second pair. Clearly it is not possible to perform a single randomization that satisfies both these requirements.
The classical statistical solution to the multiple tests prob-lem is to employ a procedure such as the well-known Bonfer-roni adjustment that makes explicit allowance for the num-ber of hypotheses tested and either seeks to control the ex-perimentwise risk of false discoveries (the risk that any false discovery will occur) [14] or the false discovery rate (the expected proportion of discoveries that are false discover-ies) [6]. In the current paper we address only control of the experimentwise risk of false discoveries, but it would be straightforward to extend the techniques to control of the false discovery rate by simply substituting an adjustment for the latter in place of the former.

The Bonferroni adjustment replaces  X  in the hypothesis tests with  X  =  X /r ,where r is the number of tests per-formed. This ensures that the experimentwise risk of false discoveries is no more than  X  . This adjustment provides strict control over the experimentwise risk of false discov-eries, even if the hypothesis tests are correlated with one another. This feature is important in the association rule discovery context, as many rules considered are likely to be closely related to one another and hence the hypothesis tests applied are likely to be strongly correlated.

More powerful alternatives ex ist to the Bonferroni adjust-ment [24], such as the Holm procedure [15]. The Holm pro-cedure requires that all hypothesis tests be evaluated and their p values ordered from lowest p 1 to highest p r .Thead-justed  X  is then  X  =max( p i :  X  1  X  j  X  i, p j  X   X / ( r  X  All such alternatives to the Bonferroni adjustment require that all tests be evaluated before the adjusted significance level is determined. Clearly this is infeasible during associa-tion rule discovery, as exploration of the large search spaces involved is only feasible if efficient pruning is able to avoid explicit consideration of the majority of rules.

Before we can apply a Bonferroni adjustment we need an upper bound on the number of hypothesis tests in the search space. For market-basket data it is straightforward to determine the size of the search space. Recall that m is the total number of items and assume that x must contain at least one item and that there is an upper bound maxx on the number of items it may contain. There are m possible values for y ,andforeach y value there are m  X  1itemsfrom which up to maxx x values are selected. where C m  X  1 i is the number of combinations containing i out of m  X  1 items. So, for example, with the Retail dataset, used below, the number of items is 16,470 and hence with x limited to no more than 5 items the size of the rule space is 1 . 66  X  10 23 .

For attribute-value data the situation is a little more com-plex, as no rule containing more than one item for a sin-gle attribute can be productive. Examples of such rules include { sex = male , sex = female } X  occupation = dataminer and { sex = male , occupation = dataminer } X  sex = female .A tighter bound can be calculated if all such rules are excluded from the calculation. To calculate the total s we must first be able to calculate the number of combinations of values of a given subset of i attributes, atts . Todosowefirstor-der the attributes in arbitrary order from 1 to i and refer to the individual attributes using this order as att 1 ...att We use intermediate values c att,j,k that each represent the total number of combinations of up to j items, where items contain only values for attributes att 1 ...att k . c where # att j is the number of values for attribute att j upper bound on the number of hypothesis tests in the search space can then be calculated as follows, where each z repre-sents the use of an attribute in the role of consequent.
It is possible that the Bonferroni adjustment has been overlooked by the association rule discovery community be-cause it has been assumed that the required adjustments are so large that the resulting adjusted significance levels (for example  X  =0 . 05 / 1 . 66  X  10 23 =3 . 01  X  10  X  25 be so low that no rules will be discovered. However, as this paper will show, this turns out not to be the case.
This approach is called a within-search approach, as sta-tistical tests, with an appropriate Bonferroni adjustment, are applied to rules as they are encountered during the search process.
Before investigating the application of a within-search Bon-ferroni adjustment to find significant rules, we should also Exploratory Rule Discovery Candidate Statistical Evaluation consider another alternative. Rather than applying statisti-cal tests during the rule discovery process, we could partition our data into exploratory and holdout sets, discover candi-date rules using the exploratory data and then test those rules using the holdout data, accepting only those rules that pass relevant statistical tests for significance. This process is illustrated in Figure 1. It will be necessary to correct for multiple tests, but only with respect to the number of rules found in the exploratory stage, not the full size of the search space considered. As the former is likely to be much smaller than the latter, the adjustment will be much smaller. Further, because only a constrained number of rules will be tested, it becomes feasible to employ a more powerful al-ternative to the Bonferroni adjustment, such as the Holm procedure. Note that, unlike most similar procedures, the Holm procedure is also safe in the face of correlated hypoth-esis tests. If it were desired to control the false discovery rate rather than the experimentwise risk of error, the Benjamini-Yekutieli procedure [7], which is likewise safe, could also be used.

The use of holdout evaluation in this way is similar to es-tablished holdout evaluation methodology in machine learn-ing, except that whereas it is used there to obtain unbiased estimates of properties of a si ngle model, such as its error, here it is being used to perform unbiased hypothesis tests on multiple models.

There are a number of reasons to believe apriori that hold-out evaluation might be more powerful than applying a Bon-ferroni adjustment during rule discovery. First, more power-ful adjustments such as the Holm procedure can be applied. Second, the scale of the adjustments should be far smaller, as there should be far fewer rules discovered than there are rules in the search space. For example, if 10,000 rules are found then the adjusted significance level for a Bonferroni adjustment would be 5 . 00  X  10  X  6 , irrespective of the size of the search space from which the 10,000 rules were discov-ered. On the other hand, however, the power of the tests must be reduced by the use of less data both for the initial rule discovery and also for the statistical tests.
These experiments seek to answer the following questions: 1. Is there a need for significance tests during association 2. Do the within-search and holdout-evaluation processes 3. How powerful are the within-search and holdout-4. How do the techniques perform on real-world data? A pre-release of version 3.1 of the well-known Magnum Opus rule discovery software was employed [26]. This software implements k -optimal rule discovery [27], whereby the user specifies a maximum number of rules to be discovered k to-gether with a measure to optimize such as support , confi-dence , lift or leverage and any other constraints to impose, such as that the rules must be non-redundant or produc-tive. It also supports the application of a Fisher exact test as described in Section 4 and holdout-evaluation. Using this software the within-search technique can be evaluated sim-ply by calculating and imposing the appropriate adjusted significance level.
The first experiment sought to investigate questions 1 and 2. Random data were generated containing 10,000 transac-tions, each containing values for 100 binary variables, with each value being equiprobable. As each value was gener-ated randomly without reference to any other variable, all variables are independent of one another and all rules are false discoveries. 100 such data sets were generated. Mag-num Opus was applied to each data set using each of the following set of parameters.
 Non-redundant : find the 1000 non-re dundant rules with the highest leverage.
 Productive : find the 1000 productive rules with the high-est leverage.
 Significance=0.05 : find the 1000 rules with the highest leverage that pass a significance test at the 0.05 significance level.
 Bonferroni : find the 1000 rules that pass a significance test at the 1 . 77  X  10  X  08 significance level that results from applying a Bonferroni correction to a raw significance level of 0.05 with a search space of 2 . 82  X  10 06 rules. Non-redundant+holdout : find the 1000 non-redundant rules with the highest leverage from half the data and then validate the rules using the remaining holdout data. Productive+holdout : find the 1000 productive rules with the highest leverage from half the data and then validate the rules using the remaining holdout data.
 For all settings the maximum antecedent ( x )sizewassetto the default value of 4.

The non-redundant, productive and significance=0.05 treat-ments all resulted in discovery of 1000 rules for every dataset. Table 1 shows the minimum, mean and maximum support, confidence and leverage for each of these treatments. As can be seen, some rules had substantial support, confidence and leverage. For this task there were almost no differences in the rules discovered by the non-redundant and produc-tive approaches because almost all rules with the highest leverage were productive.

These results appear to support an affirmative answer to question 1. It seems clear that there is a strong risk of false discoveries unless appropriate allowance is made for the multiple-tests problem.

No rules were found for any dataset under the Bonferroni or either holdout treatment. It might come as a surprise that no rules were found under any of these treatments, whereas the adjustments are supposed to restrict the num-ber of analyses for which any false discoveries are made to no more than 5% and hence one might have expected rules to have been found for up to 5 of the datasets under each of these treatments. With the Bonferroni adjustment this is possibly not so surprising as it is much more strict than the Holm procedure which also guarantees strict control over the experimentwise risk of false discoveries, and hence must be less likely to make false discoveries. The reason the Holm procedure makes no false discoveries may relate to the inter-dependencies between the rules. Both the Holm and Bonferroni procedures control against the most disad-vantageous form of relationship between the hypothesis tests which occurs when the null hypotheses are mutually exclu-sive. In practice, however, many of the hypothesis tests for this rule discovery task will be closely related to one an-other, and may even be equivalent, as for example for the rules pregnant  X  oedema and oedema  X  pregnant .Inthis circumstance the probability of any false discovery occurring is greatly reduced, although if one occurs it is likely several will occur.

These results provide support for an affirmative answer for question 2. Both the within-search and holdout approaches can control the risks of false discoveries. The second experiment sought to investigate question 3. Random data were generated comprising 10,000 transac-tions each containing values for 20 binary variables X55, Y55, X60, Y60, X65, Y65, X70, Y70, X75, Y75, X80, Y80, X85, Y85, X90, Y90, X95, Y95, X100, Y100. Each of the X values was randomly generated with each value being equiprobable. The probability of Y v =1 was v %ifX v =1, otherwise 100-v % and the probability of Y v =0 was v %if X v =0, otherwise 100-v %. For example, the probability of Y55=1 was 55% if X55=1, otherwise 45%. These data give rise to 40 true discoveries, X 55 = 0  X  Y 55 = 0, X 55 = 1 Y 55 = 1, Y 55 = 0  X  X 55 = 0, Y 55 = 1  X  X 55 = 1 and so on. Any other rules found represent false discoveries. The varying confidence levels of the rules (from 0.55 through to 1.00) represent different levels of challenge to a discovery system. 100 such random datas ets were generated and all six treatments used in the previous experiment were applied to each.

All treatments found all true rules relating to X and Y65 and higher for all data sets. Only significance=0.05 and Bonferroni found the remaining 8 rules for any dataset and they consistently did so for all datasets. All of non-redundant, productive and significance=0.05 found as many false dis-coveries as required to fill a quota of 1000 rules. The reason that non-redundant and productive failed to find the 55 and 60 level rules was that they found so many higher leverage rules that these true rules did not fit within the quota of 1000.

Neither the Bonferroni nor either of the holdout treat-ments made any false discoveries. The reason the holdout treatments did not find the 55 or 60 level rules was that those rules were not found during the exploratory rule discovery stage. The highest p -value for any of the rules found un-der the Bonferroni treatment was 6 . 19  X  10  X  26 .Thiswould have been rejected if the search space had contained more than 8 . 095  X  10 23 rules, which would occur, for example, if there were 100 pairs of variables and x were allowed to con-tain up to 15 values (rule space size = 5 . 79  X  10 27 , adjusted p =1 . 73  X  10  X  30 .

These results show that the relative power of the within-search and holdout approaches will vary depending upon the size of the search space and on the capacity under hold-out evaluation of the search technique applied during the exploratory stage to find the true discoveries.
The final experiment investigates question 4, how do the techniques perform on real-world data. The same six treat-ments were used as for the previous experiments. Experi-ments were conducted using eight of the largest attribute-value datasets from the UCI machine learning [21] and KDD [13] repositories together with the BMS-WebView-1 [30] and Retail [9] datasets. These datasets are described in Table 2. We first found for each dataset the minimum even value for minimum-support that results in fewer than 10,000 non-redundant rules given a minimum-confidence of 0.75. This is listed in the minsup column of Table 2. Each treatment was then applied to each dataset five times, once with each max-imum limit maxx on the size of x from 1 to 5. All runs used minimum-confidence=0.75 and the appropriate minimum-support, except for the holdout treatments which only use half the data for rule discovery and for which the minimum-support was therefore halved.

Table 3 presents the number of rules found by each tech-nique for each data set and setting of maxx . The meanings of the columns are as follows: Dataset : The dataset. maxx : The maximum number of items allowed in x . NR : The number of non-redundant rules  X  X iscovered. X  Prod : The number of productive rules  X  X iscovered. X  0.05 : The number of rules  X  X iscovered X  that passed an un-adjusted significance test at the 0.05 level.
 Within-Search Rule Space : The number of rules in the search space. The within-search technique used a signifi-cance level of 0 . 05 divided by this value. Within-Search Disc : The number of rules  X  X iscovered X  that passed the adjusted significance test. This is abbrevi-ated as WS, below.
 Holdout-NR Cand : The number of non-redundant can-didate rules generated from the exploratory data under the holdout approach.
 Holdout-NR Disc : The number of those candidate rules that passed the subsequent holdout evaluation. This is ab-breviated as HN, below.
 Holdout-Prod Cand : The number of productive candi-date rules generated from the exploratory data under the holdout approach.
 Holdout-Prod Disc : The number of those candidate rules that passed the subsequent holdout evaluation. This is ab-breviated as HP, below.
 WS HP : The number of rules  X  X iscovered X  by the within-search technique but not passed by holdout evaluation on productive rules.
 WSHP : The number of rules passed by holdout evaluation on productive rules but not  X  X iscovered X  by the within-search technique.

The relative numbers of rules discovered for each dataset and maxx by within-search adjusted significance tests and by each of the holdout evaluation techniques are plotted in Figure 2.
A number of points are worth highlighting. First, the number of non-redundant rules that are not productive and the number of productive rules that do not pass an unad-justed significance test is in many cases extremely high. In the most extreme case, Covtype, none of the non-redundant rules is productive. This is due to a peculiarity of this par-ticular dataset which uses 40 mutually exclusive binary vari-ables ST01 to ST40 to represent which one of 40 soil types predominates in an area. Thus, the most frequent attribute-values are values of 0 for individual ST?? variables and the most frequent itemsets are sets of these values. Because they are mutually exclusive, for any two of these variables w and z , P ( w =0 | z =1) = 1 . 0. It follows that, so long as P ( z =1) &gt; 0 . 0, P ( w =0 | z =0) &lt;P ( w =0). Hence, all associ-ations between these variables must be unproductive. The fact that all the top 9,988 non-redundant associations for this dataset represent negative associations highlights the dangers of data mining without both a clear definition of what constitutes a false discovery and sound mechanisms for detecting and rejecting such false discoveries. (Indeed, it turns out that all of the 197,183,686 highest support associ-ations for this data are associations between these variables and hence negative associations.)
Holdout evaluation with productive rules usually finds slightly more rules than Holdout evaluation with non-redundant rules. This is because the size of the correction for multiple tests that is performed during holdout evalu-ation is smaller, as there are fewer productive than non-redundant rules. The only circumstance in which the use of non-redundant rules could result in more discoveries is if some rules that were unproductive on the exploratory data turned out to be significantly productive on the holdout data. This rarely occurred in the experiments, a total of four rules being discovered by holdout with non-redundant but not holdout with productive. Only one of these rules is productive with respect to the full data, having by chance proved unproductive with respect to the exploratory sam-ple. The remaining three rules were unproductive with re-spect to both the full data and the exploratory data, but by chance turned out to be productive on the holdout data. This illustrates a potential flaw in the holdout method. Con-sider random data such as that used in Experiment 1, above, where a dataset is divided into exploratory and holdout sets. If one were to take the rules that represented the strongest negative correlations with respect to the exploratory data, the probability of those rules representing positive correla-tions on the holdout data would be increased, as one would be actively selecting for rules for which, by chance, more of the transactions containing both the antecedent and conse-quent were selected into the holdout data than the training data. It is clearly necessary to avoid any such confounding selection process under the holdout evaluation strategy.
Overall, holdout evaluation with productive rules found more rules than within-search significance testing, with the relative performance being more favorable to within-search when the size of the rule space was smaller (small maxx or fewer items) and more favorable to holdout evaluation as the size of the search space increased. This is because of the extremely small significance levels that are employed with large search spaces. The total number of rules discovered by WS often decreased as the search space increased. Such decreases occurred less for holdout evaluation and when they did, the decreases were smaller. Note, however, that these relative results are in part an artifact of the experimental design. If minimum support had been set lower, then WS would have found more rules, as the rules found under the current setting would not have been affected and further rules with lower support could have potentially passed the adjusted significance test. However, increasing minimum support could have reduced the number of rules found by holdout evaluation as it would have increased the number of candidate rules and hence lowered the adjusted significance level that rules had to pass.

It is interesting to consider the causes of holdout evalu-ation failing to find a rule that a within-search correction finds. WS made 16,238 discoveries, where this is the sum of the WS column of Table 3, and multiply counts some rules because a single rule may be discovered repeatedly at different settings of maxx . Of these, 2,554 were not dis-coveredbyHP,asshownincolumnWS HP of Table 3. In 533 cases the rule was found during the exploratory stage but failed holdout evaluation. In the remaining 2,021 cases the rule was not found during the exploratory stage. This latter case occurred primarily because rules failed to attain the required minimum support with respect to the smaller sample of data available for the initial rule discovery stage when holdout evaluation is performed.

It is also interesting to observe that in some cases huge numbers of additional rules were found during the ex-ploratory stage relative to those found from the full data set, the most extreme case being BMS-Webview-1, for which ap-proximately three times the number of candidate rules were found relative to discovery from the full dataset. This illus-trates the disadvantage of working with the smaller samples inherent in the holdout approach.
The effectiveness of holdout evaluation with produc-tive rules relative to that of holdout evaluation with non-10000 No. of rules  X  X iscovered X  No. of rules  X  X iscovered X  No. of rules  X  X iscovered X  No. of rules  X  X iscovered X  No. of rules  X  X iscovered X  redundant rules demonstrates the value of excluding from holdout evaluation rules that are unlikely to pass. On the other hand, however, the small number of rules that were found using non-redundant rather than productive rules demonstrates the danger of performing too strong a filter on the rules to be subjected to holdout evaluation. The devel-opment of effective techniques for performing such filtering provides a promising direction for future investigation.
In the current within-search approach, the adjustment to the significance level takes account of the size of the en-tire rule space under consideration. It takes no account of the fact that the number of candidates must vary as user specified constraints such as minimum-support are varied. It seems credible that the adjustment should get smaller as minimum-support gets higher, as fewer significance tests will be applied. However, it would be incorrect to simply adjust for the number of rules that pass a minimum support con-straint. To understand why this is so, consider the example of random data in Experiment 1, above, which consists of 10,000 transactions each containing values for 100 binary variables with each value being equiprobable. For one such random dataset, 12 non-redundant rules were found with support greater than 2600. If a significance level of 0.05 were adjusted to allow for 12 tests, the resulting adjusted significance level would be 0.00417. Two of the twelve rules would pass a Fisher exact test at this significance level, both obtaining p -values of 0.00112. The reason that it is not ap-propriate to adjust the significance test only for the rules that pass a minimum-support constraint is that sampling effects will exaggerate the support of some rules and the number of such exaggerated support counts experienced re-lates to the total number of rules in the global rule space, not just to the number that pass the minimum support con-straint. It would be valuable if sound techniques could be developed for modifying adjustments in line with relaxation or strengthening of constraints.

It is possible to vary the size of the rule space for the within search strategy by varying constraints such as maxx , or by deleting items from consideration. It is clear that doing so will alter the number of discoveries that are made, and that in some circumstances increasing the size of the rule space will decrease the number of discoveries. It would be valuable to develop techniques for selecting constraints that will maximize the expected number of discoveries. One approach might be to explore a variety of constraints and then select the one that delivers the most discoveries, but care would need to be taken that the selection criteria were in no way selecting the settings that were most likely to have resulted in false discoveries, because doing so could defeat the current strict control over t he risk of false discoveries. It is possible that selecting the settings that deliver the most rules might favor settings that produce false discoveries, as any false discoveries produced will increase the number of discoveries.

The current research has adopted the association rules support-confidence framework. The ability to assess signifi-cance during search holds open the prospect of a new form of rule discovery, in which all statistically significant rules are discovered without the need to specify any arbitrary pa-rameter other than the risk of false discoveries (the global significance level). As search through the massive search spaces involved in rule discovery relies upon efficient pruning from consideration of the majority of the search space, the development of such techniques will depend critically upon whether effective techniques can be developed for identifying sections of the search space that cannot contain rules that could pass a significance test. This is a further intriguing direction for future research.

A somewhat related line of research to that pursued herein involves the discovery of condensed representations [19, 8, 11] for datamining. Like the current research, this is con-cerned with reducing the number of discoveries. However, while we seek here to limit discoveries to those that rep-resent non-trivial interactions between variables, condensed representations seek to limit discoveries to a limited set from which all other discoveries can be inferred. It would be use-ful to develop condensed representations for significant rules, thereby coupling the representational power of condensed representations with the ability to avoid false discoveries.
This research has demonstrated the effectiveness of two alternative techniques for controlling the risk of false discov-eries during association rule discovery. Within-search signif-icance tests adjust the significance level to allow for the size of the search space from which rules have been drawn. De-spite the resulting use of extremely low significance levels (in our experiments as low as 3  X  10  X  25 ), in some cases large numbers of rules can be discovered while providing strict control over the risk of false discoveries. Holdout evaluation can also provide strict control o ver the risk of false discover-ies by first discovering candida te rules from an exploratory dataset and then testing those rules on different holdout data.

Our experimental evaluation has demonstrated that nei-ther of these approaches clearly dominates the other. While in our experiments the holdout approach made substantially more discoveries overall, it is clear that the within-search ap-proach holds an advantage when the the size of the search-space is small or when the number of candidate rules is large.
Both techniques provide considerable flexibility. Most im-portantly, while we have examined in this paper only tech-niques for discarding unproductive rules, the techniques are directly applicable to any statistical hypothesis test, allow-ing users to identify and discard false discoveries using what-ever definition of false discovery is applicable to their specific application.

The problem of false discoveries appears to be a serious one, as evidenced by the large numbers of non-redundant rules that are not even productive on the sample data, let alone in the domain from which the data have been sam-pled. The Covtype dataset is real-world data for which all of the 197,183,686 high est support rules are unproductive. Experiment 1 demonstrated that it is possible to  X  X iscover X  large numbers of association rules from totally random data. These examples starkly illustrate that there is a serious risk of discovering large numbers of spurious rules if we do not perform appropriate statistical evaluation during association rule discovery that takes account of the size of the search space that is explored. We, the members of the data mining community, are doing a serious disservice to ourselves, as well as to the communities we seek to serve, if we present sets of  X  X iscoveries X  to our clients of which the majority are spurious. This research has been supported by Australian Research Council Grant DP0450219. We are grateful to Janice Boughton for assistance in the collation of experimental re-sults. [1] R. Agrawal, T. Imielinski, and A. Swami. Mining [2] A. Agresti. A survey of exact inference for contingency [3] Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, and [4] S. D. Bay and M. J. Pazzani. Detecting group [5] R. J. Bayardo, Jr., R. Agrawal, and D. Gunopulos. [6] Y. Benjamini and Y. Hochberg. Controlling the false [7] Y. Benjamini and D. Yekutieli. The control of the false [8] J.-F. Boulicaut and A. Bykowski. Frequent closures as [9] T. Brijs, G. Swinnen, K. Vanhoof, and G. Wets. Using [10] S. Brin, R. Motwani, and C. Silverstein. Beyond [11] T. Calders and B. Goethals. Mining all non-derivable [12] W. DuMouchel and D. Pregibon. Empirical Bayes [13] S.HettichandS.D.Bay.TheUCIKDDarchive. [14] B.HollandandC.M.D.ImprovedBonferroni-type [15] S. Holm. A simple sequentially rejective multiple test [16] International Business Machines. IBM intelligent [17] C. Jermaine. Finding the most interesting correlations [18] B. Liu, W. Hsu, and Y. Ma. Pruning and summarizing [19] H. Mannila and H. Toivonen. Multiple uses of frequent [20] N. Megiddo and R. Srikant. Discovering predictive [21] D.J.Newman,S.Hettich,C.Blake,andC.J.Merz.
 [22] G. Piatetsky-Shapiro. Discovery, analysis, and [23] T. Scheffer. Finding association rules that trade [24] J. P. Shaffer. Multiple hypothesis testing. Ann. Rev. [25] G. I. Webb. Magnum Opus Version 1.3. Software, G. [26] G. I. Webb. Magnum Opus Version 3.1. Software, G. [27] G. I. Webb and S. Zhang. K-optimal rule discovery. [28] M. J. Zaki. Mining non-redundant association rules. [29] H. Zhang, B. Padmanabhan, and A. Tuzhilin. On the [30] Z. Zheng, R. Kohavi, and L. Mason. Real world
