 base learning algorithms [6][7][11]. It has been shown to do well with algorithms such as C4.5 [9], Decision Stumps [6], and Na X ve Bayes [4]. For example, it was reported that boosting for decision trees has achieved a relative error reduction of 27% on a set of domains [1]. However, some experiments have also revealed that the boosted na X ve Bayesian classifier is not so effective in error rate reduction as the boosted decision tree (or boosted decision stump) [1][12]. One possible reason for this phenomenon is that training data at each round. 
The AdaBoost method uses a relatively simple weight-update rule. At each round, the weights of all the misclassified training examples are multiplied by the same factor which is larger than 1, while the weights of all the correctly-classified are divided by this factor. The factor used at each round is determined only by the global error rate (or global accuracy) of the corresponding base classifier. However, the global accuracy may have different local accuracies in different areas (or instance subspaces). 
Next, we would like to further justify the necessity of adopting local accuracies into the boosting process by considering the meaning of the weight of an instance. In boosting methods, the weight of a training example reflects its  X  X ardness X  for class label prediction. If two training examples have one same weight at the end of the round t and they are both misclassified at the round t +1, then they are of another same weight at the end of the round t +1. This situation is not satisfying in that the base classifier at points. The training example with higher local accuracy is clearly harder for the base classifier to make classification, and its we ight should be increased to a higher value. Taking local accuracies into considera tion may result in a more powerful weight-update rule, and then make versatile changes to the training data. 
Thus, the main thrust of this paper is to design a novel and effective boosting method by integrating estimated local accuracies of the base classifiers into the boosting method. The local accuracies at different instance points provide more detailed per-formance information about a base classifier than its global accuracy, which makes it possible to make a versatile and significant change to the training data. To materialize accuracy of a na X ve Bayesian base classifier at an instance point. This is solved by in-troducing an  X  X valuator X , which is actually a C4.5 decision tree, for each nave Bayesian base classifier in the following steps: firstly, a Boolean meta-attribute is appended to each training example, and its value indicates whether a training example can be cor-rectly classified by the base na X ve Bayesian classifier with the leave-one-out technique; training example labeled by its meta-attribute value. This C4.5 decision tree can output a confidence which is actually an estimated local accuracy of the base classifier for a given test instance. The proposed boosting method depends highly on these estimated local accuracies. During the boosting process, the estimated lo cal accuracies for the training examples are used to determine the multiplying-factors in weight updating. On the other hand, during the decision fusion process for a given test instance, the estimated local accuracy of a base classifier not only determines whether this classifier will take part in process. 
The whole paper is organized as follows. Section 2 introduces the na X ve Bayesian base learning algorithm and describes what the meta-attribute is and how to calculate its value with the current base classifier using the leave-one-out technique. After filling up the meta-attribute value for each training example, section 3 gives out the details about how to use a C4.5 decision tree as the evaluator for estimating the local accuracy of the method works and presents its pseudo-code. Experimental results are shown in section 5. Finally, section 6 summarizes the whole paper. Consider a domain where instances are represented as instantiations of a vector A ={ a 1 , a , ..., a m } of m nominal variables. Here, each instance x takes a value a i ( x ) from do-dataset of size n . The task of classification is to construct a model (or classifier) from instance. 
Na X ve Bayesian learning [3] is a probability-based simplest classification method. It has surprisingly good performance in a wide variety of domains and is robust to noise and irrelevant attributes. The underlying assumption that it takes is that attributes are conditionally mutually independent given the class label. According to the Bayes consisting of m attribute values, where v k = a k ( x ), 1  X  k  X  m , is given by instance x, is used as the predicted class. Note that we do not need to compute the value Bayesian classifier trained on S can be expressed as 
The local accuracy of a classifier at a given test instance is usually measured as its accuracy on a small instance subspace containi ng the instance. In other words, the local accuracy can be estimated as the weighted percentage of the correctly-classified training examples in an instance subspace cont aining the test instance. To calculate this value, a basic job is to determine whether a given training example is classified cor-rectly or not by the classifier. This can be done by simply applying the classifier on this example and comparing the predicted class label with its real label. However, to ensure leave-one-out technique is used, which first updates the classifier by dropping the training example from it before applying the classifier to decision making. The results for all the training examples are recorded in a meta-attribute. Formally speaking, we be classified correctly by the na X ve Bayesian classifier or not: Here, we would like to explain in more detail how this work can be carried out. Let the predicted class label by NB ( S ) for a test instance x . By updating the corresponding be correctly classified by the na X ve Bayes method, with c meta ( x i )= true , if the predicted into the na X ve Bayesian classifier to re-obtain NB ( S ), also in time O ( m ). By repeating c input to the C4.5 decision tree algorithm, an d the resulting decision tree classifier can instance points. measure its local accuracy a given instance, which can usually be estimated as the weighted proportion of correctly-classified training examples in an instance subspace around the instance. This section focuses on a solution by training a C4.5 decision tree correctly classified by the base classifier. This C4.5 decision tree, called the  X  X valuator X  of the base classifier, can output its estimated local accuracy at a given instance. 
In a decision tree, each node corresponds to an instance subspace and thus a training subset. The induction process starts with the root node which corresponds to the whole instance space and thus contains all the training examples. Then at each node, the de-cision-tree induction algorithm tries to select a test attribute on which to partition the smaller subspaces, each of which corresponds to one value in the domain of the at-tribute, while a continuous attribute a together with a threshold partitions the current subspace into two smaller subspaces, one co ntains the instances whose values on a are values on a are greater than the threshold. Finally, the resulting decision tree dt parti-S X corresponds to a weight vector ) , ( j false j true j W W W = , where be classified correctly by the current na X ve Bayesian base classifier, and local accuracy is estimated as the Laplace ratio: where dt denotes the evaluator decision tree corresponding to the base classifier. 
The process described above is under the assumption that the values of each instance on all attributes are known, which may not be the case in reality. The real situation is complicated a little more by the fact that instances may have missing values on some attributes. The C4.5 algorithm [8] adopts a proba bilistic way to deal with this situation. A test instance x with missing values on some attributes will traverse from the root node to a number of leaf nodes, and it belongs to each such leaf node with a certain prob-ability that is estimated from the training set (As to the details of how to calculate this probability, please refer to the Chapter 3 in [8]). Let us use J ( x ) to denote the set of leaf the instance x is estimated as: 
It should be noted that the estimated local accuracies are measured with respect to the boosted weights at each round in the boosting process (not the original weights). The motivation of the proposed boosting method for na X ve Bayesian classifiers is to integrate the estimated local accuracies (instead of the global accuracy, or equivalently global error rate, in original Adaboost method) into the boosting method. As we shall see in this section, estimated local accuracy plays an important role in both the boosting process and the decision fusion process. This paper uses C4.5 decision trees as the evaluators of local accuracies for the base na X ve Bayesian classifiers, which has already been described in section 3. The task of this section is to develop an effective method for boosting na X ve Bayesian classifiers, called EffectiveBoost-NB, with its pseudo-code listed in figure 1. 
In EffectiveBoost-NB, two classifiers are constructed at each round t . One is the na X ve Bayesian base classifier NB ( t ) trained on the set of weighted training examples: training example, the algorithm uses leave-one-out technique to judge whether this local-accuracy evaluator of the base classifier, is trained on the derived dataset S beled by its meta-attribute value c meta ( x i ) (line 7 in Figure 1). 
After these two classifiers have been built, it is the time to update the weights of all the training examples in order to make necessary preparation for the next round. The way that the weight of a training example ( x i , y i ) is updated depends on two factors: (1) what is the local accuracy of the current base classifier (that is, the base classifier at round t ) at the instance point x i ; and (2) whether the instance x i can be correctly clas-sified by the base classifier at round t , with the leave-one-out technique. To answer the first question, the current C4.5 evaluator is used to estimate the local accuracy; while for the second question, the meta-attribute va lue is checked to provide the answer. For each training example ( x i , y i ), the weight-update rule goes as follows: (1) If the estimated local accuracy is larger than 0.5 and it can be correctly (2) If the estimated local accuracy is larger than 0.5 and it cannot be correctly classified (3) Otherwise (that is, when the estimated local accuracy is less than 0.5), the weight 
The weight-update procedure here differs from that of the classical AdaBoost algo-rithm in two aspects: (1) the local accuracies specific to individual training examples are used instead of the global accuracy, which makes the change of weights versatile; (2) we use leave-one-out technique in judging whether a training example can be cor-rectly classifier or not, in order to guarantee the generalization ability, while AdaBoost applies the base classifier without dropping the training example out. On the other hand, it shares some similarity with the boosting with specialist models [10] by the fact their weights are not subject to the weight-update. After the weights of all the training examples have been updated, all these weights are normalized to make them a distri-bution (line 20 in Figure 1). In addition, there are two points to clarify. The first is about the Boolean variable ConfidentEnough , which is operated in the line 8, line 11, and line 19 of the algorithm. If none of the local accuracies (of the current base classifier) at all the instance points of remain unchanged, and the boosting process has to terminate (line 19 in Figure 1). This is analogous to the situation in AdaBoost.M1 th at the current base classifier has an error than err , while those at other points are lower. Therefore, even if the base classifier at the current round satisfies the termination condition (that is, err  X  0.5) of AdaBoost.M1, it may not satisfy the termination condition of EffectiveBoost-NB and the boosting process may continue. ALGORITHM EffectiveBoost-NB INPUT: a data set S of n training examples, S ={( x 1 , y 1 ), ..., ( x n , y n )} 1. Initialize D 1 ( i )=1/ n and w ( x i )= n  X  D 1 ( i ) for all i . 2. FOR t =1, ..., T : 3. 4. FOR each instance x i DO 6. ENDFOR 7. train a C45 evaluator dt ( t ) = C 45( S derived ) on the derived data with weights w 8. ConfidentEnough := false 9. FOR each instance x i DO 1 0 IF 5 . 0 ) ( ) ( &gt; i dt x LocalAcc t THEN 11 ConfidentEnough := true 1 2 IF c meta ( x i )= true THEN 1 4 ELSE 1 6 ENDIF 1 7 ENDIF 1 8 ENDFOR 1 9 IF ConfidentEnough = false THEN T := t ; break; ENDIF 2 0 21 Update the instance weights: w ( x i ):= D t +1 ( i )  X  n for each x i 2 2 ENDFOR 
The second point is about the usage of th e two copies of the instance weight in-formation used in the algorithm. Boosting methods usually require that the weights of Bayesian method and C4.5 learning algorithm traditionally treats the weights as the numbers of instances (i.e. their sum should be equal to the total number of examples). Therefore, two copies ( w and D ) of the weight information are maintained in the algo-is maintained as the line 21 in Figure 1 does at the end of each round..  X  OUTPUT  X  line in Figure 1. For a given test instance x , a na X ve Bayesian base classi-test instance point (estimated by the correspon ding C4.5 evaluator) is larger than 0.5. In other words, the voting committee for the test instance x consists of those base classi-fiers whose local accuracies estimated is high enough. If a base classifier NB ( t ) is se-NB ( t ) ( x ) by the weight  X   X  sponding to the base classifier NB ( t ) . Clearly, this vote weight also depends only on the estimated local accuracy. The class label with the highest sum of votes is the final de-which make it distinct from the classical AdaBoost methods: (1) a selective voting committee of base classifiers are formed dynamically for each given test instance; and (2) the vote weights of the classifiers in the subset are determined dynamically by the given test instance. The proposed EffectiveB oost-NB algorithm in this paper is actually a dynamic classifier ensemble method. To demonstrate and compare the performance of the proposed boosting method for na X ve Bayesian classifiers, we randomly pick out 30 datasets from UCI machine learning repository [2], and use them in the experiments, with the detailed information summarized in the first column of table 1. These datasets have many distinct features, including the number of attributes, the number of classes, and the size of each dataset. entropy-based discretization algorithm in [5] was employed to discretize the numeric attributes in the training sets for each fold, as pre-processing. 
In this study, the proposed EffectiveBoost-NB algorithm is compared with other two algorithms: the na X ve Bayesian algorithm and the AdaBoost.M1 of na X ve Bayesian Compared with standard na X ve Bayesian method, EffectiveBoost-NB has lower error rates than NB on 22 datasets and has higher error rates than NB on 7 datasets (with only 1 tie). Even when compared with AdaBoos t-NB, EffecitveBoost-NB performs better on 20 datasets and worse on 6 datasets (with 4 ties). 
The final row in table 1 shows the mean error rates across all the datasets. Among the three algorithms, EffectiveBoost-NB gets the best result, which is much better than both NB and AdaBoost-NB. nificance level set at 5%). It shows that EffectiveBoost-NB wins significantly on 13 datasets, and loses on none of the datasets when compared with NB. Even when compared with AdaBoost-NB, EffectiveBoost-NB also wins significantly on 9 data-sets, also without loses. When comparin g AdaBoost-NB with NB, we find that AdaBoost-NB significantly wins on 8 datasets and loses on 1 datasets. 
As a conclusion, the proposed EffectiveBoost-NB provides an effective method for boosting na X ve Bayesian classifier, and it has significantly reduced both the error rate of na X ve Bayesian method and that of the AdaBoost.M1 of na X ve Bayesian classifiers. This paper proposes an effective boosting method for na X ve Bayesian classifiers by using C4.5 decision trees as local accuracy evaluators. Experimental results have manifested that it can significantly reduced the error rates of the na X ve Bayesian clas-would be interesting to compare with another boosting method for na X ve Bayesian classifiers presented in [12] which introduces tree structures into na X ve Bayesian clas-sification to form a new kind of base classifier called  X  X eveled na X ve Bayesian tree X . We also plan to investigate the effect of combining the boosting technique in this paper with the leveled na X ve Bayesian trees. Acknowledgements. This work was funded in part by National Natural Science Foundation of China under grant number 60503025.

