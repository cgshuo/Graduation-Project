 Singular Value Decomposition (SVD) has been extensively used in the classification context as a preprocessing step aim-ing to reduce the number of features of the input space. Traditional classification algorithms are then applied on the new space to generate accurate models. In this paper, we propose a different use of SVD. In our approach SVD is the building block of a new classification algorithm, called CMF, and not that of a feature reduction algorithm. In particular, we propose a new classification algorithm where the clas-sification model corresponds to the k largest right singular vectors of the factorization of the training dataset obtained by applying SVD. The selected singular vectors allows rep-resenting the main  X  X haracteristics X  of the training data and can be used to provide accurate predictions.

The experiments performed on 15 structured UCI datasets show that CMF is efficient and, despite its simplicity, it is more accurate than many state of the art classification algorithms.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Experimentation Classification, Singular Value Decomposition
The classification problem consists in generating an ab-stract model of a set of classes, called classifier, which is built from a set of labeled training data. The classifier is then used to assign an appropriate class label to new test data for which the class label is unknown. Many approaches have been proposed to generate accurate classification mod-els [1, 4, 5, 8, 11] in the context of structured (relational) datasets. In this paper, we propose a new classification algo-rithm based on Singular Value Decomposition (SVD). SVD has been extensively used in the classification context, and in other contexts, as a preprocessing step in order to reduce the dimensionality (i.e., number of features) of the analyzed data. Data are mapped from the original space into a more compact space on top of which traditional classification algo-rithms are applied. In this paper, inspired by the use of SVD and matrix factorization as predictive algorithms in recom-mender systems [10], we propose a new classifier where the model is based on the matrix factorization of the training dataset. The new classifier is called CMF ( C lassification by means of M atrix F actorization). CMF exploits SVD to per-form a matrix factorization of the training dataset and then selects the k largest singular values and the related right singular vectors to identify the k most important  X  X harac-teristics X  of the training data. The selected singular vectors represent our classification model. An interesting property of the selected k singular vectors [13] is that they can be used to compute the optimal rank-k approximation of D , denote as f
D k (i.e., the best approximation of D based on the first k components). Given a data d 2 D , singular vectors can be exploited to compute its approximation e d 2 f D k (the differ-ence between e d and d is approximately zero if enough sin-gular vectors are used). We argue that the singular vectors extracted from the training data are representative also of the test data, because test data and training data are char-acterized by a similar behavior. Hence, given an arbitrary test data t , for which the class label is unknown, we can use the singular vectors obtained by factorizing the training data to compute the approximation e t of t . Also for the test data, the difference between e t and t should be close to zero. We exploit this assumption to select the most appropriate class label for new data. The new data t is labeled with the class label c i that allows obtaining the approximation e close to t . The idea is that the class label that allows min-imizing the difference between t and e t is also the one that allows obtaining a tuple more similar to those included in the training dataset. The approach is simple, but the initial set of experiments performed on 15 UCI datasets show that CMF is both efficient and accurate.

Different from the majority of the state of the art classifi-cation algorithms, CMF is characterized by only one param-eter ( k ). Moreover, this parameter can be easily estimated by analyzing the accuracy achieved by CMF on the training d ata. This is an interesting property because usually the parameter tuning operation is a tough task.
In the first part of this section the needed background knowledge is introduced (Section 2.1). Then the proposed algorithm is described.
Given a n f matrix D , its singular value decomposition (SVD) is a factorization of the form where U is a n n unitary matrix,  X  is a n f diagonal matrix, and V is a f f unitary matrix. With V T we denote the transpose matrix of V . The diagonal entries i =  X ( i; i ) of  X  are called singular values of D , while the columns of U and the columns of V are called the left singular vectors of D and the right singular vectors of D , respectively.
It is well known [13] that the optimal rank-k approxima-tion of D , denoted as f D k in the following, can be generated by selecting the k largest singular values and the associated singular vectors. In particular, f D k is computed as follows. where U k is a n k matrix composed of the k left singular vectors associated with the k largest singular values,  X  k a k k diagonal matrix with the k largest singular values of  X  on the diagonal, and V k is a f k matrix composed of the k right singular vectors associated with the k largest singular values.

To generate the model exploited by CMF, we need to ex-press f D k as a function of the original (training data) matrix D and the matrixes composing its factorization. Since V is a unitary matrix, it follows that V T V is equal to the identity matrix I . Hence, Equation 3 trivially follows from Equa-tion 1.
 Equation 4 also follows.

We can exploit Equation 4 to rewrite Equation 2 as fol-lows.
Hence, given an arbitrary data d 2 D , the approximation of d , denoted as e d , can be computable as follows.
If k is set properly only the minor components, which are usually given by noise, are pruned and the obtained matrix f D k is a good approximation of D . Hence, d 2 D .

In Section 2.3, we will show how Equation 6 can be used to compute the approximation of an arbitrary new test data t and how the obtained result can be exploited to label the new data.
Given a n f matrix D (the training dataset), where n is the number of training data and f the number of attributes (including the class attribute), the training model is gener-ated by applying SVD on D and by selecting the value of k (the number of singular vectors). The training model of CMF is equal to the matrix V k obtained by performing the training data factorization (see Equation 2).

Since SVD can be computed only on numerical matrixes, before applying CMF all the categorical attributes (includ-ing the class label) are transformed in numerical attributes by mapping their values to consecutive positive integers.
As discussed in [13], the complexity of the SVD compu-tation of a n f matrix D is O ( nf 2 ). Hence, also the com-plexity of the training step of CMF is O ( nf 2 ).
Equation 6 can be exploited to compute an approximation of any data characterized by the schema of D (i.e., the same set of attributes). Hence, given an arbitrary test data t com-patible with the schema of D , its approximation, denoted as e t , can be computed as
Since t is a test data, its class label is unknown (i.e., the class attribute is set to null). Before applying Formula 7 a valid value must be assigned to the class attribute of t . We argue that the most appropriate value for the class attribute is the class label c i that minimizes the difference between t and its approximation e t . The idea is that the main char-acteristics of t are similar to those of the training data and hence the factorization computed on the training dataset D fits well also on the test data t . Hence, the class label c that allows minimizing the difference between t and its approximation e t is the one that must be selected.
More formally, the classification of an arbitrary new data t is performed as follows. We denote as t c i the data obtained by labeling t with the class label c i (i.e., the value of the class attribute of t is set to c i ). For each generated data t its approximation f t c i is computed as f t c i = t c i V for each pair ( t c i ; f t c i ) the difference between t computed as where A j is the j -th attribute of the considered data. CMF labels the test data t with the class label c i that allows ob-taining the minimum difference. c i is computed by applying Formula 9.
We performed a set of experiments on 15 UCI datasets [7] to analyze the accuracy achieved by CMF and its execution time. The main characteristics of the 15 UCI datasets are reported in the first 4 columns of Table 1. We compared CMF with six classification algorithms: a nearest neighbor classifier (K-NN [1]), Support Vector Machines (SVMs [4]), decision trees (C4.5 [12]), a rule based classifier (Ripper [5]), a bayesian classifier (Bayesian network [8]), and an associa-tive classifier ( L 3 [2]). We used the available Weka [9] imple-mentations of the six algorithms to compute the accuracy of the selected algorithms on the 15 UCI datasets. The CMF algorithm was implemented in Octave [6]. higher/equal/lower accuracy than the classi er in column
All the experiments were performed on a laptop with a 2.2-GHz AMD Turion Dual-Core RM-75 processor and 4.0 Gbytes of main memory, running Kubuntu 10.04.
The accuracy measure is used to analyze the ability of the classifier to correctly classify unlabeled data. In partic-ular, it is the ratio of the number of correctly classified data over the total number of given test data. The experiments on the accuracy measure have been performed by using a 10 fold cross validation test. For all the considered classi-fication algorithms we set the parameters to their standard values, according to the setting reported in the related pa-pers. CMF is characterized by the k parameter (i.e., the number of selected singular values). For each dataset, the appropriate value of k has been obtained by selecting the value of k that achieves the best accuracy value on the train-ing data. Hence, the value of k has been automatically set. This is an interesting property of our algorithm. The selec-tion of the best parameter setting for the other classification algorithms is usually a tough task because they are usually characterized by many parameters and the best setting for the training set is usually not a good setting for the test set. Differently, we have empirically checked that the best value of k , for CMF, is usually the same on both the training and the test set.

The obtained results are reported in Table 1. For each dataset the accuracy of the classifier that achieves the high-est accuracy value is reported in boldface. Based on Table 1, Table 2 summarizes the accuracy comparisons of CMF with respect to the other classification algorithms. For each com-parison, Win/Draw/Loss are the number of datasets where the accuracy of CMF is higher/equal/lower than the com-pared classifier.

The results reported in Table 1 highlight that CMF is, on the average, more accurate than five of the considered algorithms (+1.93% against K-NN, +12.01% against SVMs, +0.97% against C4.5, +1.26% against Ripper, and +0.11% against L 3 ). Only the Bayesian network classifier is, on the average, slightly more accurate than CMF (+0.26% with re-spect to CMF). Hence, despite its simplicity, CMF allows obtaining interesting results. The summary reported in Ta-ble 2 confirms the good results achieved by CMF. Except for Bayesian network and L 3 , the reported results show that CMF wins on the majority of the considered datasets with respect to the other algorithms.
We analyzed both the model generation time and the clas-sification time of CMF on the 15 UCI datasets. The ob-tained model generation time for each dataset is reported in Table 3. We do not report the model generation time of the other algorithms because they are implemented in java, while CMF is implemented in Octave. Hence, execution time comparison will be unfair. The training time of CMF is at most 0.33s for the considered datasets and the average training time is 0.27s. The model generation time of CMF could be further reduced by exploiting ad-hoc SVD libraries such as SVDPACK [3].

As expected the execution time is related to the number of records and attributes. Since the complexity of the singular value decomposition is O (num. of records num. of attributes the dataset with the highest training time is also the one Au stralian 69 0 1 5 1.5 5E+005 0 .29 Ha rberman 30 6 4 4.9 0E+003 0 .26 he patitis 25 5 2 0 1.0 2E+005 0 .25 T ransfusion 74 8 5 1.8 7E+004 0 .25 wit h the highest value of numb. of records*num. of attributes (see Column (4) of Table 3).
 We also measured the average classification time per data. The average classification time per data on the 15 UCI datasets is 0.73ms. The classification step of CMF is very efficient because it is based on the computation of the products be-tween the vectors t c i and the matrix V .
We performed a set of experiments to analyze in detail the model generation time of CMF with respect to the char-acteristics of the training dataset. We generated a set of synthetic datasets by means of the IBM data generator 1 with classification function 2. Figure 1 shows the model generation time when varying the number of records from 10,000 to 100,000 on datasets with ten attributes. Figure 2 reports the execution time with respect to the number of attributes (from 5 to 30 attributes). As expected the execu-tion time increases not linearly with respect to the number of attributes (Figure 2), while, differently from the expected theoretical results, the execution time increases not linearly also with respect to the number of records (Figure 1). This result is probably related to the SVD implementation avail-able in Octave. A linear execution time, with respect to the number of records, could be obtained by using the approach described in [13].
 All the performed experiments need at most 110s. Hence, CMF efficiently handles structured datasets characterized by a large set of training data (up to 100,000 records) and attributes (up to 30 attributes).
In this paper, we proposed a simple but efficient and ac-curate classifier based on SVD. The performed experiments show that CMF achieves interesting results on structured datasets. We are working on the application of the proposed approach to textual classification. However, CMF cannot be straightforwardly applied to textual data because the num-ber of features is significantly larger. h ttp://www.almaden.ibm.com/software/quest/Resources/datasets Fi gure 1: Scalability with respect to the number of records. 10 attributes for each dataset. Fi gure 2: Scalability with respect to the number of attributes. 50,000 records for each dataset. [1] D. Aha and D. Kibler. Instance-based learning [2] E. Baralis, S. Chiusano, and P. Garza. A lazy [3] M. Berry. Large scale singular value computations. [4] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [5] W. W. Cohen. Fast effective rule induction. In [6] J. W. Eaton, D. Bateman, and S. Hauberg. Gnu [7] A. Frank and A. Asuncion. UCI machine learning [8] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian [9] M. Hall, E. Frank, G. Holmes, B. Pfahringer, [10] Y. Koren, R. Bell, and C. Volinsky. Matrix [11] B. Liu, W. Hsu, and Y. Ma. Integrating classification [12] R. Quinlan. C4.5: Programs for Machine Learning . [13] L. N. Trefethen and D. Bau. Numerical linear algebra .
