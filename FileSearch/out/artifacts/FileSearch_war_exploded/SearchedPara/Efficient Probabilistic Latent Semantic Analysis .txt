 Probabilistic latent semantic analysis (PLSA) has been shown to be a competitive tech-nique for document retrieval compared to other methods such as the vector space model (VSM). While the VSM calculates the cosine similarity between query and document vectors, PLSA operates on the co-occurrenc e information between the set of words W and the set of documents D in the collection. By mapping this information on to a set of latent states Z (where | Z | is user-specified and is smaller than both | W | and | D | ), relationships between the words and documents can be discovered. These latent states can be seen as clusters, or  X  X oncepts X .

One of the limiting factors of PLSA is the amount of time and memory required for its execution, making it difficult for practical use. This limitation is related to the sizes of W , D ,and Z . Even with the growth in main memory size, only moderately-sized document collections of 1 to 2 MB can be processed [Deerwester et al., 1990]. Other researchers have proposed vocabulary sampling as a means of processing larger collections [Kim et al., 2003].

In this paper, we investigate alternative techniques for addressing the memory and running time constraints of PLSA without changing its output. We consider three dif-ferent methods which we use in concert: (1) augmenting the data structures for faster access; (2) modifying how the EM algorithm for parameter estimation is implemented; and (3) parallelizing using a shared and distributed memory framework. We assume that our base line system is a na  X   X ve implementation derived from directly translating the description of PLSA.

The remainder of this paper is structured as follows. In the next section, we describe some background related to our work. In Section 3, we report on work related to the efficiency of PLSA. In Section 4, we outline our three suggested improvements. Results from experiments using both real and synthetic data are reported in Section 5 which demonstrate the level of improvement in terms of running time and memory. Section 6 summarizes our findings and provides some directions for future work. Probabilistic latent semantic analysis (PLSA) is an extension of latent semantic analysis (LSA) [Deerwester et al., 1990]. In the context of information retrieval (IR), the latter is also dubbed latent semantic indexing (LSI). The purpose of LSA is to associate two types of data through a set of latent (hidden) states. If the sizes of the two types of objects are m and n , respectively, the starting poi nt of LSA is a co-occurrence table X with m rows and n columns. The cell X ij (where 0  X  i&lt;m and 0  X  j&lt;n )isa non-negative integer indicating the number of times the two objects occur together.
In IR, the two types of data are the words W and the documents D , such that X ij represents the frequency of word i is in document j . Moreover, the matrix X is generally sparse, especially in the IR domain, since a document only contains a small subset of words. LSA then decomposes the matrix by p erforming a singular value decomposition (SVD) on X . If the number of latent states is | Z | , then the | Z | largest singular values and their corresponding vectors from the d ecomposition are chosen to form the next reincarnation X of X .

PLSA [Hofmann, 2001, 1999] re-interprets LSA within a probabilistic framework by defining the joint probability between a word w  X  W and a document d  X  D across Z latent states as: These probabilitie s are obtained by maximizing the log-likelihood: where n ( w, d ) is the co-occurrence matrix X . The parameters of Equation (1) are determined through applying the Expectation-Maximization (EM) algorithm [Dempster et al., 1977]. The EM algorithm is an iterative procedure for finding solu-tions to the maximum likelihood of models with latent variables, such as PLSA. Each iteration consists of two steps: the Expectation step (E-step) and the Maximization step (M-step). In the E-step, the expected value of the log-likelihood is estimated based on the values of the parameters. In the M-step, these parameters are re-estimated in order to maximize L . Thus, the E and M-steps are: E-step: M-step: Since these values are all probabilities, they have to be normalized accordingly. The al-gorithm stops when a condition such as a smal l change in the log-likelihood is obtained or a pre-defined number of iterations has been completed.

Generally, the EM algorithm is computationally intensive and one way of address-ing this problem is through parallelization. Parallelization refers to the distribution of independent sections of an algorithm across multiple processors. The term generally refers to one of two flavors: shared and distributed memory. A comparison of these two paradigms is summarized in Table 1. Shared memory processing operates on a single computer and offers a more fine-grained distribution of work. A common example of it is in a loop structure which perform n iterations, where each iteration is independent of all others. The advantage of this method is that data structures do not need to be distributed between each thread of executio n since they execute on the same computer using the same memory space. As the name suggests, distributed memory processing refers to multiple CPUs each operating more independently at a coarser-grain level, such as functions instead of loops. These CPUs can be all within the same computer or spread across a network. In this case, data structures need to be sent and received explicitly. However, if the CPUs are spread across a network, each CPU will have access to their own disk storage device.

Two standards for these paradigms are OpenMP for shared memory processing [OpenMP Architecture Review Board, 2008] and the Message Passing Interface (MPI) for distributed memory processing [Message Passing Interface Forum, 2008]. The MPI standard has been interpreted by many  X  the application programming interface (API) that we will make use of is Open MPI [Gabriel et al., 2004]. Both paradigms can be used in conjunction in a single implementati on. Further details about this and these two methods in general can be found in other sources [Quinn, 2003]. Implementing PLSA is straightforward giv en Equations (2) to (6). However, a more careful implementation of PLSA could improve its efficiency. Some related work in this area is summarized next.

Hong et al. [2008] recently considered parallelizing PLSA. Their main idea is to partition the co-occurrence matrix into blocks using one of several blocking strategies. Each partitioned block is queued and processed one-by-one by each available CPU in a round-robin fashion. This strategy keeps each CPU busy, and minimizes the overall CPU idle time. Our method of partitioning is different, as we explain below, and we also investigated both shared and distributed memory parallelization, while they only considered the former.
 Another way of improving the efficiency of PLSA is to reduce one dimension of X by removing words from W which do not co-occur with many documents. This process of vocabulary reduction is similar to applying a stop word list in a document retrieval system and, consequently, has the advantage of also improving the quality of the output of PLSA [Kim et al., 2003]. The focus of this work is primarily on improving the efficiency of PLSA computation without altering the output, and hence orthogonal to vocabulary reduction. In fact, both vocabulary reduction and the ideas outlined in this paper can be combined together to handle larger document collections.
 After PLSA has completed, the output is a probabilistic latent semantic index (PLSI) X whose dimensions is equal to that of X . However, the dimensions are the only com-mon characteristics of the two structures. Since X is a table of co-occurrence counts, its values are integers, and, in practice, with a high proportion of zeroes. In contrast, since X is a table of probabilities, storing these floating point values for efficient re-trieval is a more important concern  X  a p roblem which has been addressed recently by Park and Ramamohanarao [2009]. Thus our work and their X  X  refer to two different aspects of PLSA efficiency: calculation of X and subsequent retrieval with it. We investigated three methods for improving the efficiency of PLSA compared to a straightforward implementation. We focus our attention on the data structures used, the EM algorithm, and parallelization of PLSA. 4.1 Data Structures The data structures required by PLSA are summarized in the first column of Table 2. The space requirements depend on the number of words W , documents D , and latent states Z , with this latter value chosen by the user.

In Equations (2) and (4) to (6), note that only the non-zero co-occurrence counts contribute to the total sum. The solution adopted by Hong et al. [2008] was to balance each block by first permuting the rows and columns of X as a pre-processing step.
In our case, due to a different partitioning scheme, we keep only the entries in n ( w, d ) which are non-zero in memory (not shown in Table 2). In the next section, we examine the EM algorithm and consider the sizes of the other data structures. 4.2 The EM Algorithm Our second improvement was initiated by H ong et al. [2008], but c omparatively little attention was given to its importance. The EM algorithm iterates between two steps which are separated for clarity. However, in the interest of space efficiency, the two kept, with the latter ones indicated by the superscript  X  X ew X . This has the advantage of eliminating the largest data structure ( p ( z | w, d ) ) entirely, as illustrated by comparing the two columns of Table 2.
 Replacing p ( z | w, d ) in Equations (4) to (6) with Equation (3) yields the following: E-step + M-step: Algorithm 1. The steps that the master CPU (CPU 0 )andthe k slave CPUs (CPU k ) have to perform under a distributed memory environment. Let P () represent ( p ( w | z ) , tions prefixed by an asterisk (*) have had OpenMP (shared memory parallelization) enabled for their main loops.

While an explicit data structure for p ( w, d ) is now needed, it is still a considerable savings compared to p ( z | w, d ) . Again, since these values are probabilities, normaliza-tion is required. 4.3 Parallelization Our last improvement is the parallelization of PLSA, where we consider both distributed and shared memory models. Of the two, distributed memory is potentially more com-plex since it requires communication between processors to be explicitly taken care of. Among the possible forms of communication between CPUs, two extremes are possi-ble. In the first one, all CPUs are equal in role and communicate directly with all of their peers. In the second model, one of the CPUs is designated as the master and all other CPUs as slaves which communicate directly only with the master and not to each other. While the first variant has the potential advantage of equally dividing the workload, some tasks such as normalization to obt ain probabilities is b est done by one CPU.
Adopting the first model for distributed memory, the steps that the master CPU (CPU 0 )andthe k slave CPUs (CPU k ) have to perform are given in Algorithm 1. For clarity, and P ( new ) () , respectively.

The two main tasks performed by CPU 0 are initializing P () with random values and normalization. During these times, the other CPUs are idle. In our implementation, we prevent CPU 0 from being idle at steps 3 and 6 by giving it an equal unit of work to per-form. All transmissions are blocked so th at CPUs do not proceed until what they need to send or receive has succeeded. This approach reduces concurrency, but facilitates easier management.

An important question is how are the data structures partitioned. We contrast our approach to that of Hong et al. [2008] using Figure 1 by envisioning the three variables ( W , D ,and Z ) as the sides of a cube. Dotted lines indicate the manner in which work is partitioned into work units. Hong et al. elected to divide n ( w, d ) according to W and D (Figure 1(a)). As noted earlier, a consequence of this decision is a pre-processing step to distribute the non-zero co-occurrence counts among blocks. In contrast, we chose to divide the work according to latent states (Figure 1(b)). This gives the same amount of work to each CPU without any pre-processing, provided the number of latent states is evenly divisible by the number of processors.
 In addition to distributed memory, we also consider shared memory parallelization. This is relatively easier than distributed parallelization since only the necessary com-piler instructions are inserted before the loops to be parallelized. The functions which have their main loops parallelized with OpenMP are indicated by asterisks in Algo-rithm 1. We emphasize that how we make use of OpenMP is different from Hong et al. and, because of this, our methods are not compar able. In fact, given th eir blocking strat-egy minimizes CPU idle time, their technique may be more efficient. We implemented two versions of PLSA  X  both with and without the modifications to the EM algorithm. Taking into account parallelization, we obtain five systems, as shown in Table 3. All versions only store the non-zero co-occurrence counts. We have made all attempts to keep the differences between the two versions at a minimum. Both programs are written in C and compiled using GNU gcc v4.1.2 under Linux with the -O3 optimization flag. OpenMP is included with the compiler while v1.2.4 of Open MPI was used. Due to potential underflow problems caused from the large number of multiplications of probabilities, all values are stored in log-space.

The hardware used was a cluster of eight 3.0 GHz Dual-Core AMD Opteron Proces-sor 2222 SE with 18 GB RAM connected by a 16 Port 10M/100M Switching Hub. All computers were generally idle at the time of the experiments.

Times are reported as elapsed time in seconds and averaged across four trials. For every system variation, the same initial values to the EM algorithm were used. The times reported represent just over a single iteration of the EM algorithm from step 3 to step 10 of Algorithm 1 (i.e., steps 3 to 5 are executed twice before exiting). We perform only a single iteration because different initial values may change the number of iterations performed, resulting in greater variations in total running time. The number of latent states is fixed at | Z | =32 .

We considered two sets of experiments. First, we evaluated both versions of PLSA on real data. These experiments are conduc ted first on a single computer and then again on the network of 8 computers. We then used Baseline and the fastest system and applied them to one of the data sets with varying levels of co-occurrence counts added. The purpose of this experiment is to assess to what extent more non-zero co-occurrence counts would affect the overall execution time. 5.1 Real Data We selected four medium-sized data sets that have been used in the PLSA liter-ature [Deerwester et al., 1990, Hofmann, 1999]. They were downloaded from http://www.cs.utk.edu/  X  lsi/corpa.html . In processing the text, we ap-plied case-folding and a simple stemming scheme, but not stopping. Some statistics of the parsed collections are summarized in Ta ble 4. We expect alternative pre-processing steps could change these statistics and affect the size of n ( w, d ) . The table also shows the percentage of the matrix which is non-zero. As this table shows, the matrix is very sparse with well below 1.5% of non-zero elements.

We begin by estimating the amount of memory used by the two versions of PLSA ( Baseline and None ) by using the information from Table 2 and Table 4, including the percentage of non-zeros, with all four data sets shown together. Figure 2 shows that Baseline uses a significant amount of memory, due to the size of p ( z | w, d ) .
Figure 3 presents a set of graphs, one for each of the data files. On the vertical axes, the average elapsed time in seconds is shown. Along the horizontal axes is the number of cores in our computers, ranging from 1 to 4. The changes we have introduced, with-out parallelization, increases the execution time ( None ). Profiling this system indicates that this is due to the calculation of p ( w, d ) (step 3 of Algorithm 1). Fortunately, this additional computational cost is alleviat ed through parallel programming. When only one of OpenMP or MPI is used, the execution times improves with the increase in the number of processors, with MPI performing slightly better in all cases. The best method is All , when both are used together. With All , the number of OpenMP threads is fixed at 4, so when the number of CPUs (MPI processors) increase, the system resources are strained. Hence, this line act ually increases and at 4 CPUs, MPI improves on it slightly. In Figure 4, we extend the experiments to our network of 8 computers, removing OpenMP from consideration and focussing on MPI and All . The results show an in-crease in running time due to the overhead of communicating over a network. This is in contrast to Figure 3, where all MPI pro cesses communicated to each other within the same computer. A faster network switch is expected to reduce this overhead.
If we compare MPI with All , MPI performs worse. The difference between the two lines is due to the use of OpenMP. The best performance of MPI occurs when the number of computers is 3 or 4. 5.2 Synthetic Data As a final experiment, we selected CRAN and increased the percentage of non-zero val-ues from 1.23% (see Table 4). We changed zero values to 1 by selecting cells at random, until a fixed percentage of non-zero values was reached. We then applied Baseline and OpenMP to this data for a single iteration of the EM algorithm.
The results of Figure 5 show that increasing the number of non-zero co-occurrence counts increases the overall running time, producing a linear relation-ship. Moreover, the continued separation of the two lines indicate that the modifi-cations we made continue to hold as the number of non-zero entries increase. Note that this relationship might change when more than one iteration of the EM algo-rithm is taken into account. While PLSA is not difficult to implement from Equations (2) to (6), we have shown that a careful implementation can make a d ifference in both memory usage and over-all running time. We demonstrated the utility of both distributed and shared parallel programming, as well as changing the data structures employed by PLSA. In terms of memory, while Figure 2 shows that our method is an improvement, our work alone seems unable to process data sets larger than CACM.

In order to tackle this problem and to further reduce running time, the techniques reported by others can be coupled with our work. This includes vocabulary reduction, as described earlier, and careful consideration of the number of latent states and iterations of the EM algorithm used.

A possible avenue of future research is extending our work to GPGPU (general-purpose computing on Graphics Processing Units) (see Owens et al. [2008] for details). Though GPGPUs are a recent trend in parallel computing, our work represents a first step in parallelizing PLSA without any specialized hardware.

While PLSA was originally proposed for IR, our work is equally important to other areas which operate on co-occurrence data. Some applications which have found a use for PLSA include machine translation [Kim et al., 2003], analysis of proteins [Chang et al., 2008, Mamitsuka, 2003], and image analysis [Hanselmann et al., 2008]. It is likely that these data sets possess different properties from those of text data (i.e., number of non-zero values, etc.) and it would be interesting to evaluate our techniques to those problems.
 Availability. The source code of all 5 systems in Table 3 are distributed under the GNU Public License and available for download from http://www.cbrc.jp/  X  rwan/ Acknowledgements. Financial support for RW was provided by INTEC Systems In-stitute, Inc. and a post-doctoral fellowship from the Japan Society for the Promotion of Science (JSPS). VNA was supported by a short-term fellowship, also from JSPS, and a grant from the Australian Research Council. This work has been funded in part by BIRD of the Japan Science and Technology Agency (JST).

