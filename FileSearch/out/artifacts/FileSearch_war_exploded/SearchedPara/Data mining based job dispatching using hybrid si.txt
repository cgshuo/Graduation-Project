 Atif Shahzad, Nasser Mebarki n 1. Introduction
Machine scheduling is one of the most important issues in the planning and operation of manufacturing systems. It is aimed at efficiently allocating the available machines to jobs, or operations within jobs and subsequent time-phasing of these jobs on individual machines ( Shaw et al., 1992 ). Traditional approaches to solve scheduling problems use simulation, analytical models, heuristics or combination of these methods.

Scheduling problems in which number of jobs and their ready times are known and fixed are referred as static environment problems in contrast to the dynamic environment problems in which jobs are continually revealed during the execution process ( French, 1982 ). Dynamic scheduling uses priority dispatching rules (pdrs) to prioritize jobs waiting for processing at a resource ( Vieira et al., 2003 ). The general approach for a pdr, according to
Montana (2005) , is to dynamically define a score associated with assigning a given task to a given resource and select the eligible task that minimizes (or maximizes) that score for the chosen resource. Due to their ease of implementation and substantially reduced computational requirement they remained a very popular technique despite of their poor performance in the long run ( French, 1982 ).

The major drawbacks of pdrs include their performance dependence on the state of the system and non-existence of any single rule, superior to all the others for all possible states the system might be in Geiger et al. (2006) . Meta-heuristics (e.g., simulated annealing, tabu search, etc.) have an advantage over pdrs in terms of solution quality and robustness, however, these are usually more difficult to implement and tune, and computa-tionally too complex to be used in a real-time system.
Robust and better-quality solutions provided by meta-heur-istics contain useful knowledge about the problem domain and solution space explored. Such a set of solutions represents a wealth of scheduling knowledge to the domain that can be transformed in a form of decision tree or a rule-set. In this paper, we propose an approach to exploit this scheduling knowledge. In our approach, we seek this scheduling knowledge through a data mining module to identify a rule-set by exploring the patterns in the solution set obtained by an optimization module based on tabu search, a very efficient meta-heuristic for JSSP in particular. This rule-set approximates the output of the optimization module when incorporated in a simulation model of the system. This rule-set is subsequently used to make dispatching decisions in an on-line manner.

The rest of the paper is organized as follows. In the next section, a review of the relevant literature is presented. It follows with a brief description of some necessary background areas. Then the proposed methodological framework, with details of each module in subsections is presented. Section 5 presents the experiments carried out to evaluate the proposed approach in static environment. The results are presented in Section 6 . Finally conclusions follow in Section 7 . 2. Literature review
As there does not exist a pdr that is globally better than all the others ( Lee et al., 1997 ), numerous techniques based on the approach for dynamic selection of pdrs at the right moment according to the systems X  conditions and production objectives were proposed. This approach is referred as multi-pass adaptive scheduling (MPAS) approach ( Shiue and Guh, 2006 ). Priore et al. (2001a) categorized MPAS strategies as look-ahead simulation based and knowledge based.

In the simulation based approach, the rule is chosen at the right moment by simulating a set of pre-established dispatching rules and selecting the one that gives the best performance (see for example, Wu Richard and David, 1988 ; Wu and Wysk, 1989 ; Ishii and Talavage, 1991 ; Yeong-Dae, 1994 ; Pierreval and
Mebarki, 1997 ; Kim, 1998 ). The selected pdr is then used for scheduling periods of shorter lengths that may be fix or dynami-cally sized according to system performance.

The second approach, from the field of artificial intelligence, employs a set of earlier system simulations (training examples) to determine what the best rule is for each possible system state.
These training examples are used to train a machine-learning algorithm to acquire knowledge about the manufacturing system ( Michalski et al., 1986 ). Intelligent decisions are then made in real time, based on this knowledge (see for example, Nakasuka and Yoshida, 1992 ; Shaw et al., 1992 ; Kim, 1998 ).

The main algorithm types in the field of machine learning are case-based reasoning (CBR), neural networks and inductive learn-ing. Aytug et al. (1994) presented a comprehensive review of different machine learning techniques with emphasis on induc-tive learning methods applied in scheduling. Inductive learning in production scheduling has primarily been devoted to issues such as selecting the best dispatching rule using simulation data.
Pierreval and Ralambondrainy (1988) used the induction algorithm GENREG proposed by Ralambondrainy (1988) to obtain a set of rules with best mean tardiness as target concept on simulation data for flow shop environment. They acknowledged that the identified rules may not be strong enough, however, it is mentioned this approach is a promising one especially where there is a lack of knowledge.

Nakasuka and Yoshida (1989) employed a learning algorithm named as Learning Aided Dynamic Scheduler (LADS), capable of automatically generating new useful attributes for on-line rule selection in a production line. The algorithm can handle quanti-tative as well as qualitative type attributes. Selection of useful attributes is done using a local knowledge-base (of each machine). They concluded that the good switching of rules can effectively draw strong features of each individual rule. They also observed that incorporating a mechanism to identify more infor-mative problem sets increases the learning efficiency.
Shaw et al. (1992) developed pattern-directed scheduler (PDS) to monitor the scheduling activity for changes in manufacturing patterns (combinations of various parameters that together represent a given state of the system). Their system performed better than the best dispatching rule for mean tardiness, under similar conditions. However for higher number of machines and lower switching frequency of the selected rules, the performance of system degraded.

Piramuthu et al. (1994) proposed a mechanism based on same principle to select among a given set of heuristics using a well-known data mining algorithm C4.5, proposed by Quinlan (2003) for decision tree generation for a flow shop. They considered both the dispatching at individual machines and releasing jobs into the system. They investigated th e performance of PDS over a range of values for coefficient of variation of process time and identified that their results were superior fo r large process time variations at bottleneck machines. They observe d that pattern-directed schedul-ing based decision trees were not able to improve upon results significantly, however, these results are not generalized.
Priore et al. (2001a) used induction learning to select a heuristic through a set of training examples created by using different values of the control attributes and identifying the relevant manufacturing patterns. They observed that on certain occasions, the obtained heuristic was better by the SPT or the EDD rule. A major cause of such behavior is identified as the transitory changes of the control attributes. This effect is reduced by assigning a weight function to each selected rule and triggering a rule only when its weight reaches a certain lower limit.
Later, Priore et al. (2006) compared inductive learning based on C4.5 algorithm with other machine learning techniques for a selected flexible manufacturing system. They incorporated a generator module to create new control attributes in order to reduce the test error rate.

Li and Olafsson (2005) used decision-tree induction in his proposed approach to discover the key scheduling concepts by applying data mining techniques on historic scheduling data and to generate scheduling rules. Instead of selecting a particular pdr, they used the dominance between two jobs (which job should be dispatched first) as the target concept to be learned. This knowl-edge is then transformed in a form of dispatching lists. They studied a single machine problem with C max as scheduling objective and compared the performance with standard bench-mark pdrs. They, however, did not perform any selection of the subset of data to be used for learning.

Priore et al. (2001b) provided a comprehensive review of main machine learning-based scheduling approaches. The review of the literature on knowledge based approach for MPAS reveals that the usual practice involves the implementation of a predefined set consisting of a number of candidate rules in a discrete-event simulation model of the system under consideration, and comparing their performance using simulation experiments under varying values of system parameters characterizing the system dynamics. A set of best performing rules under varying conditions are taken as training examples to be input to the learning system. Intelligent decisions are then made in real time based on the knowledge obtained through the learning system.
Generally, examination of the simulation results suggests changes to the selected rule-set, requiring repetition of at least a subset of the simulation experiments. This, of course, assumes that all the dispatching rules are known in advance and that the performance of these rules can accurately be simulated. Exception to this usual approach include Koonce and Tsai (2000) ; Geiger et al. (2006) , and Huyet (2006) . In most of these studies, simulation is not used for the generation of knowledge (although it is employed in intermediate steps).

Chiu and Yih (1995) proposed a learning-based methodology to extract scheduling knowledge for dispatching parts to machines for a distributed manufacturing system. A simulation module is used to generate training examples and to evaluate the methodology while genetic algorithm is used for the selection of good training examples. Finally a learning algorithm, capable of adapting to newly observed examples, acquires the scheduling rules from these selected training examples.

Lee et al. (1997) proposed to combine the strengths of genetic algorithm and induction learning for developing a job shop scheduling system. Genetic algorithm used the heuristic space as the search space. However, they limited the learning process only to job release, referring the difficulties to develop dispatching knowledge-bases for the dispatching phase.

Koonce and Tsai (2000) applied data mining on solutions generated by a genetic algorithm (GA) based scheduling and developed a rule set approximating the GA scheduler. The
Attribute Oriented Induction approach was used to characterize the relationship between the operations X  sequences and their attributes. The approach over-performed SPT, however, the obtained rules were unable to match the performance of GA scheduler.

Dimopoulos and Zalzala (2001) used genetic programming (GP) to evolve sequencing policies that combine known sequen-cing rules and problem specific information. They studied a single machine scheduling problem with the objective of minimizing the total tardiness. Various sets of problems are used for training the proposed GP-based algorithm for the evolution of a dispatching rule. Nine dispatching rules were evolved with comparable performance to man-made dispatching rules on training and validation problem set. However, the approach lacks transparency in making decisions. Moreover, it is extremely sensitive to parameter selection making it hard to generalize for other scheduling environments.

Nhu Binh and Joe Cing (2005) made use of genetic program-ming as well for evolving effective composite dispatching rules for solving a flexible JSSP with recirculation, with the objective of minimizing total tardiness. They generated five pdrs for variable due date settings that performed marginally well over the EDD in 74 X 85 of problem instances. Their framework had the same drawbacks of lack of transparency and much higher parameter sensitivity.

Geiger et al. (2006) proposed a genetic programming based system, named SCRUPLES, that combines an evolutionary learning mechanism with a simulation model to discover new priority dispatching rules. They studied single machine scheduling pro-blem with three different objectives of minimizing P C i , L P
T i . A framework for more complex scheduling environments is also presented. They observed that the number of jobs to be scheduled has no significant impact on the performance of the system.

Huyet and Paris (2004) proposed a methodology based on synergistic action of evolutionary optimization and induction graph learning method to search for relevant knowledge of manufacturing system. Optimization process provides increas-ingly efficient solutions during its search. These solutions are then classified by the learning process using certain solution charac-teristics. These characteristics are highlighted in the learning process to characterize the attractive areas of search space as well. They experimented with an assembly kanban system for its optimal configuration.

Later, Huyet (2006) implemented the same approach for configuration of a job shop and compared the results with a classical evolutionary optimization via simulation approach. They generated some profiles for the system that contribute to char-acterization of high-performance solutions.

Mouelhi-Chibani and Pierreval (2010) presented a neural-net-work based approach that uses simulation-optimization instead of using any training set. In the proposed approach neural network can automatically select efficient DRs dynamically.
Optimization driven simulations generate knowledge in an off-line manner, depending on the workshop characteristics and the system state, to construct a neural network, that is to be used online for selection of pdr.

Metan et al. (2010) combined the techniques of simulation, data mining, and statistical process control charts for a job shop problem, with the objective of minimising average tardiness. The knowledge is extracted from the manufacturing environment by constructing a decision tree and a pdr is chosen from the tree for each scheduling period. The process control charts are used as a continuously performance monitoring mechanism of the decision tree. The decision tree is dynamically updated based on the changes observed in manufacturing conditions.

As it is evident from the prior work, the major contribution of learning in production scheduling is focused on the selection of the best dispatching rule for specific conditions. Different learning algorithms are used to extract the desired knowledge, with no algorithm capable of outperforming others for all system condi-tions. The major drawback has been to ignore the question of relevance of the solutions considered for the knowledge generation. The contents of the scheduling knowledge play a major role in improving the efficiency of the learning algorithm. However, it is generally not obvious which solutions would be more relevant with respect to defined scheduling objectives in a particular environment. Simulated data, historical data or data generated by some meta-heuristic like GA remained the source of this scheduling knowledge. Huyet and Paris (2004) considered the problem of relevance of scheduling knowledge while using GA to generate it.

A few studies focused on discovering new rules using GP and data mining. The proposed approach relies on the solutions generated by tabu search. However instead of restricting the focus on the pdr-space, dominance relations of competing jobs is identified, making use of a set of predefined attributes. This differs from the selection of a pdr, where a selected pdr is known to exist in the pool of candidate rules and is then used for dispatching jobs during a certain length of scheduling period. 3. Background
Some background areas must be discussed before presenting the proposed framework. These include tabu search; one of the most effective meta-heuristic to solve JSSP and data mining; a knowledge discovery approach. 3.1. Tabu search
Tabu search (TS) algorithms are among the most effective approaches for solving JSSP ( Jain and Meeran, 1998 ) using a memory function to avoid being trapped in a local optimum ( Zhang et al., 2008 ). However, neighborhood structures and move evaluation strategies play the central role in the effectiveness and efficiency of the tabu search for the JSSP ( Jain et al., 2000 ). In contrast to myopic nature of PDRs, meta-heuristics such as tabu search can attack the problem more rigorously and intelligently due to relatively higher level of domain knowledge. As a consequence, the decisions made by the proposed system reflect the use of this valuable knowledge. 3.2. Data mining
Data mining is an essential step in the process of knowledge discovery from Data (KDD), however, the two terms are often used interchangeably. Data mining is the process of discovering interesting knowledge from large amounts of data stored in databases, data warehouses, or other information repositories ( Han and Kamber, 2002 ). The data mining approach is particularly applicable for large, complex production environments, where the complexity makes it difficult to model the system explicitly.
From the viewpoint of our approach, data mining can specifi-cally be considered as the analysis of a data set, referred as training data set in order to identify previously unknown and potentially useful hidden patterns and to discover relationships among the various elements of this data set. The aim is to classify the cases in another data set, referred as test data set, by mapping the newly discovered relationships on them. The discovery process can be termed as descriptive data mining, while the classification of test data set using the discovered relationships can be viewed as predictive data mining ( Choudhary et al., 2009 ).
We use decision tree based learning as the data mining step as it is simple to understand and interpret, requires little data preparation, able to handle both numerical and categorical data, uses a white box model, possible to validate a model using statistical tests, robust and performs well with large data in a short time. Induction of decision tree, or ID3 (Iterative Dichot-omiser 3) ( Quinlan, 1986 ), is one of the most powerful mining algorithm used in machine learning ( Koonce and Tsai, 2000 ;
Dudas et al., 2009 ; Priore et al., 2006 ). Revised versions of ID3 include GID3 (Generalized ID3), ID4, ID5, C4.5 ( Quinlan, 2003 ) and C5.0 ( Quinlan, 2003 ). 4. Proposed approach
We propose a hybrid simulation-optimization based approach coupled with data mining for job shop scheduling problem. The goal of the proposed system is to generate a set of rules for making dispatching decisions in a job shop scheduling environment.
First of all control module generates number of problem instances relevant to real scheduling system. This may be replaced by historical data of the manufacturing system.These problem instances are stored in an instance database. The optimization module generates solutions for a subset of these instances. This subset of job shop instances is referred as initial training data set. The solutions to these instances generated by optimization module are in fact a set of good scheduling decisions that may be made for the manufacturing system. However, these good decisions are not evident before these are implemented and corresponding performance measures have been computed. In order to identify and extract the characteristics of these good scheduling decisions, a simulation module is employed which associates a performance measure to each decision taken by the optimization module. This refined set of characteristics of sche-duling decisions refers to a relevant scheduling knowledge. The scheduling knowledge is stored in a scheduling database and employed by a learning process to construct a decision tree.
This decision tree is then used to dispatch the jobs-awaiting service in an on-line manner. The decision tree is dynamically updated, whenever necessary through a control module. The control module transmits the knowledge of good scheduling decisions to the optimization module as well to improve its own performance at subsequent levels.

Fig. 1 demonstrates the interaction of these modules in the proposed framework. There are two important aspects of the proposed system besides merely providing a schedule: 1. to provide an insight to the scheduling decisions made by the optimization module and 2. to improve upon the efficiency of the optimization module by utilizing the output of learning module.

Scheduling procedures such as pdrs and meta-heuristics do not provide, in general, any justification of the decisions made, no matter how good or bad they are. This lack of transparency is not only undesirable in practice but also a great hindrance to under-stand the difference between good and bad scheduling decisions.
Identifying the characteristics of good scheduling decisions generated by an optimization process is thus desired. Our pro-posed system is aimed at providing transparency in its scheduling decisions.

The proposed system takes benefit from this white-box effect, by making use of the relevant scheduling knowledge in optimiza-tion process. The meta-heuristics such as tabu search use a neighborhood mechanism to reduce the search space. Optimi-zation process can make use of this knowledge to create more restricted and potentially stronger neighborhoods. In the follow-ing, the structural detail of each module as well as interaction of these modules is explained. 4.1. Optimization module
The optimization module is aimed at providing the most relevant solutions to the scheduling problem. A tabu search ( Nowicki and Smutnicki, 1996 ) based optimization module gen-erates a set of efficient solutions for a set of problem instances.
Since these efficient solutions are obtained through a series of some logical moves of the meta-heuristic, they have some general characteristics that may describe the relationship between opera-tions and their sequential order in a particular solution. These characteristics are a form of schedule knowledge, like dispatching rules. The aggregation of corresponding set of efficient solutions for each problem instance may then be used as the training data set for the data mining algorithm for discovery of scheduling knowledge. However, it is not expected from this module to identify a priori, the reasons of superiority of one solution to another.

The optimization module works in an off-line manner, how-ever, it continuously provides with more and more schedules to the problem instances generated by the control module. 4.2. Simulation module
There are three key functions of the simulation module. Fig. 2 shows the working of simulation module. The main simulator that is in-loop with learning module, takes the schedule gener-ated by optimization module. This schedule is transformed into a set of decisions, i.e. whether a job is dispatched before some other job. Each decision is characterized by a vector of attribute-value pairs, where attributes are the selected attributes and the values are obtained through this simulator. A collection of these vectors is referred as a training block, (see Section 4.2.1 and Chong et al., 2003 ).

Finally, the simulator associates a relative performance mea-sure to a decision made in the schedule. This is done by considering some alternative decisions at each dispatching deci-sion point. These alternative decisions are taken from a multi-pass simulator that measures the performance of a predefined set of pdrs. Each time a dispatching decision is made, it is compared with the best possible decision by the multi-pass simulator. Based on this comparison, a relative value of performance is associated with the decision. In this way, equivalent schedules are incorpo-rated in the training data set. Both these simulators take on the same set of problem instances. 4.2.1. Training block A training block, B i is a matrix of the form B  X  X  xz with x  X  X  x 1 x 2 x 2 ... x p T and z  X  X  z 1 z 2 z 2 ... z q T where p is the number of positive examples in a training block and q is the number of objectives considered. The minimum training block is f x 1 , z 1 g T . Each element  X  x , z  X  example from which we can learn. We refer to each such example or a row in B as a positive training example. Each x is an l -dimensional row vector, i.e., ( k , 1 r k r p , x where l is the number of selected attributes.

For z  X  z 1 , z becomes a p -dimensional column vector, i.e.  X  z 1 z 2 z 3 ... z p , 8 k , z k A f 0 ; 1 g z sing order of two jobs u , v on machine q , i.e. z k  X  1 represents u and vice versa.

Each training block, B k is a chronological sequence of con-secutive x  X  X  and corresponding z  X  X , before another sequence of consecutive x  X  X  follows. A union of these training blocks is the training data set, B used by the learning module B  X  X  B 1 B 2 B 3 ... B n T
There is another simulation model, within the simulation module, that serves the purpose of an observer to the manufac-turing system. It records the performance of the system in response to the scheduling decisions taken. This information is used by a controller module for subsequent generation of training data. Since the actual scheduling decisions are taken on the basis of dispatching list generated by learning module, the performance of the system may differ significantly even for the same training data set. 4.3. Learning module
As the solutions in the solution-set used by learning module are restricted to perform above certain threshold, focus remains on the more relevant search space. Fig. 3 illustrates the working of learning module. The learning module takes the refined relevant scheduling knowledge from the scheduling database. This knowl-edge constitutes the learning set on which the induction process is built. This process aims at predicting a class of scheduling decisions that may be most accurately generalized.

The learning module deciphers the scheduling data so as to make a particular decision as transparent as possible. It is quite easy to understand the process of classification of scheduling decisions when it is represented as a decision tree. This is the key advantage of the decision tree based learning and the major reason to adopt this method in the proposed framework. Learning module uses C4.5 algorithm for mining the implicit information in the solutions found by optimization module.

The relevant scheduling knowledge is provided in the form of training blocks from the scheduling database. The collection of these training blocks is used by the learning module as initial training data. However, considerable amount of transformation is required before it is possible to mine any useful knowledge from the data.

Construction of a proper training data set is a very crucial point in the entire KDD process. From the data mining perspective in JSSP, the target concept to be learned is to determine which job should be dispatched first within a set of schedulable jobs on the same machine at a particular instant. Extracting this knowledge from the training data set would allow us to dispatch the next job at any given time and thereafter to create dispatching lists for any set of jobs.

Finally, the decision tree induced using the learning algorithm can be applied directly to the same JSSP to validate the explored knowledge and as a predictive model to predict the target concept. A set of scheduling problem instances with similar distribution of processing time is to be used as a test data set for the scheduling knowledge discovered. The overall sequence of operations obtained by these rules is translated to a schedule using a schedule generator. Thus, the tree will, given any two jobs, predict which job should be dispatched first and can be thought of as a new, previously unknown rule. In addition to the prediction, decision trees and decision rules reveal insightful structural knowledge that can be used to further enhance the scheduling decision.

The training blocks provide valuable information about the current status of the system. It is the selection of proper attributes that can provide with efficient training blocks to help in generat-ing a better quality decision tree.

Attribute selection is the task of finding the most reasonable subset of attributes for a classifier to seek fewer attributes and maximum class separability ( Kwak and Yih, 2004 ). This process is also critical for the effectiveness of the subsequent model induc-tion by eliminating certain redundant and irrelevant attributes. It is indeed unlikely that the attributes that are recorded as part of the available data are the attributes that are the most relevant or useful for data mining process. Thus, creation of new attributes must be considered.

There exist a strong relation among the sequencing of opera-tions due to precedence constraints, however, considering only two (operations of the) jobs on the same machine among schedulable jobs (the predecessor (if any) of whom are already dispatched) at any instance for the comparison reduces this dependency effect. Proper selection of attributes plays an impor-tant role to reduce this dependency as well.

Both the creation of new attributes and selection of attributes (we call the both process combined as attribute extraction) are primarily linked with the objectives of the JSSP. Tardiness based objectives require different attributes to be taken into account while flow-time based objectives have different requirements. For instance, deadline related statistics and counters are more suited for tardiness based objectives.

Arithmetic combinations of primitive attributes can also be used to generate new useful attributes. However, a large set of attributes is not desirable, as the attributes are generally not independent of each other, making the process computationally impractical. Several heuristics such as backward stepwise heuristic, forward stepwise group heuristic have been proposed to limit the selected subset of attributes while maintaining a certain performance level. Each simulation scenario as a static control rule, and necessary attributes are collected at data collec-tion points and saved in each corresponding file. Initial training data are actually a chronological sequence of x  X  X  and z  X  X  that are collected at the points of job completion, respectively. Each row in the file is called an instance or object. The initial training data are a collection of training blocks. 4.4. Control module
A control module generates the relevant scheduling problem instances. These instances may represent historical data of the manufacturing system. An on-line controller module takes the relevant dispatching decisions from the dispatching database, to be implemented in actual manufacturing system.

As a scheduling decision is implemented in a manufacturing system, an on-line performance monitoring is done, to decide whether a new training block is generated for the recent decisions or not. If the actual performance of the system degrades below a certain threshold, controller triggers the learning process for the updating of dispatching database.

A control sub-module is employed for providing feed-back to the optimization module. Feedback to optimization module serves the purpose of improving its own performance at subse-quent levels. 5. Experiments
Two sets of 6 6 similarly sized instances of a static job-shop problem with different seed values are used as training data-set ( I ) and test data-set ( I ). All jobs are available simultaneously at time zero. Discrete uniform distribution between 1 and 10 is used to generate the operation processing times. The job due dates are determined using two parameters t and r , where t determines the expected number of tardy jobs (and hence the average tightness of the due dates) and r specifies the due date range.
Once these parameters have been specified, the job due dates are generated from the discrete uniform distribution given as d  X  UNIFORM m where m  X  X  1 t  X  E  X  C max is the mean due date. E  X  C max expected makespan for the problem instance and is calculated by estimating the total processing time required by all operations divided by the number of machines. Note that this assumes no idle time on machines, and hence will be an optimistic estimate of
C . We consider t  X  0 : 3 and r  X  0 : 5. L max (Maximum Lateness) is used as the scheduling objective. 5.1. System attributes
Selection of the relevant attributes has a key role in obtaining the appropriate performance level. The selected attributes have the following characteristics:
The attributes are related to tardiness based performance measures.

It is preferred to define attributes in relative values instead of absolute values.
 The attributes with high variation are discretized.
 Table 1 lists the attributes used in the experiments. 6. Results and discussion
The performance of the proposed system is measured in relative terms and is indicated by two measures namely Z 1 . Z 1 indicates performance of obtained rule-set and the set of pdrs relative to tabu search algorithm used and is given as  X  where r
A (see Table 3 for description of these pdrs), TS: tabu search and I : set of training instances used. L max  X  r [ RS , I  X  represents the set of L max values for all the problem instances of set I using the set of pdrs in set r and rule-set RS.

Note that Z 1 is an indication of the performance of TS algorithm relative to pdrs. On the other hand, Z 2 refers to the performance of obtained rule-set relative to considered set of pdrs B R and is given as  X  8 &gt; &lt; &gt; : with I : set of training instances and RS: rule-set obtained.
Lower value of Z 1 for a pdr means that solution by that pdr is close to the one provided by TS algorithm. This means that instances with higher value for Z 1 are more representative of the performance of the proposed system. Higher value for Z this case, indicates good performance of the system and vice versa.

A set of 57 rules are generated by the proposed system for 100 training blocks of different sizes. A partial list of the rules induced is listed in Table 2 .
 A set of five instances are used as test data-set. The measures and Z 2 are plotted for these five test instances for the purpose of comparison. It is observed that the results of mined rule-set are superior or at least comparable to the best performing rule for the
L measure. For the five instances, plot of Z 2 , shown in Fig. 4 (b), reflects the fact that the rule-set is consistently a better performer among all the pdrs used with slack rule as the closest competitor.
This is due to the nature of selected attributes used in the algorithm. There is a considerable room of improvement in performance of rule-set, as can be seen from plot of Z 1 shown in Fig. 4 (a), that may be obtained through better attribute selection. 7. Conclusions and perspectives
We have proposed a data mining based framework for job shop scheduling problems. The structure as well as functionalities of different components of the proposed system are explained. The results for a set of job shop instances are presented at the end.
The approach focuses on the identification of the critical parameters and states of a particular dynamic scheduling envir-onment that contribute to the construction of some efficient solution. It is not always possible to obtain or to implement the CRSI CRSI optimal solutions for a complex dynamic real-world sized JSSP due to constantly varying conditions. However, through this approach several alternative solutions could be proposed that are sufficiently efficient. The proposed methodology is based on the implicit assumption about the ability of tabu search to move intelligently in the solution space while providing the opportu-nity, at the same time, to learn the embedded knowledge about the thinking lines behind these intelligent moves. It is also good to know how the obtained knowledge can be used in-process to reorient the tabu search for some large size instances of the problem. We believe that it is possible to more effectively benefit from it by making an analysis of long-term memory of Tabu Search algorithm.

It is observed that the performance of the system degrades substantially if the system is unable to perform effectively during the earlier period of scheduling in contrast to later scheduling decisions. This is justified due to the nature of the JSSP problem, as the subsequent scheduling decisions are heavily affected by earlier decisions made.

Proper feature selection in regards with scheduling objective under consideration is the key factor for the successful imple-mentation of the proposed framework. Feature selection for different objectives and their combinations has to be rigorously explored to obtain compact and efficient rule set.

The size of the problem used for training as well as test data set plays an important role in the relative performance of the proposed system. As the pdrs are generally used in large dynamic scheduling systems, the true performance of the system relative to pdrs is biased towards pdrs. Using larger problems in optimi-zation modules can reduce this bias, however, this comes at extra (or sometimes impractical) computational overhead.

The performance of the proposed system in different loading conditions is to be explored further. Although it is expected to perform relatively better in heavy shop-load conditions. This is due to the fact that more scheduling decisions would be taken by the mined rule-set in heavy shop-load conditions.
 References
