 Knowledge bases and the Web of Linked Data have become important assets for search, recommendation, and analytics. Natural-language questions are a user-friendly mode of tap-ping this wealth of knowledge and data. However, question answering technology does not work robustly in this setting as questions have to be translated into structured queries and users have to be careful in phrasing their questions. This paper advocates a new approach that allows questions to be partially translated into relaxed queries, covering the essential but not necessarily all aspects of the user X  X  input. To compensate for the omissions, we exploit textual sources associated with entities and relational facts. Our system translates user questions into an extended form of structured SPARQL queries, with text predicates attached to triple pat-terns. Our solution is based on a novel optimization model, cast into an integer linear program, for joint decomposition and disambiguation of the user question. We demonstrate the quality of our methods through experiments with the QALD benchmark.
 H.3.3 [ Information systems ]: Information Search and Re-trieval; I.2.1 [ Artificial Intelligence ]: Natural language in-terfaces question answering; knowledge base; semantic search; dis-ambiguation; usability
Motivation: With the success of IBM X  X  Watson system [23], natural-language question answering (QA) is being revived as a key technology towards coping with the deluge of digi-tal contents. Watson primarily tapped into textual contents, with limited use of structured background knowledge. How-ever, there is a strongly growing wealth of structured data on the Web: large knowledge bases like dbpedia.org , free-base.com , and yago-knowledge.org ; billions of inter-linked RDF triples from these and other sources, together forming the Web of Linked Data [21]; hundreds of millions of Web tables (e.g., accessible via research.google.com/tables ); and a large amount of RDF-style microdata embedded in HTML pages (e.g., using the schema.org vocabulary).
This paper is about conveniently and effectively searching this wealth of structured Web data, specifically, the RDF-based world of Linked Data. Although there are structured query languages like SPARQL that could, in principle, be used to this end, such an approach is impractical for several reasons: i) users are not familiar with the diverse vocabulary of the data, ii) the data itself exhibits high heterogeneity so that even a power-user would struggle with formulating the right queries, iii) even a perfectly formulated query may fail to find answers if the query vocabulary (predicates, classes, entity names) does not match the data vocabulary. To bridge the gap between users X  information needs and the underlying data and knowledge, it has recently been proposed to take natural-language questions as user input and automatically translate these into structured queries, for example, in the SPARQL language [40, 46, 35].

Example: Consider the question  X  X hich music bands covered songs written by the Rolling Stones? X , or in con-veniently short form:  X  X ands covering songs by the Stones X , or in even more telegraphic style:  X  X ands songs Stones X . An ideal QA system for RDF data would automatically trans-late this user input into a SPARQL query with the following triple patterns: where ?x and ?y are variables, and the binding for ?x is the query answer. Getting this translation right is all but an easy task. Moreover, even this seemingly perfect query may not work if the underlying data differs from the vocabulary and structure of the query. For example, the class Song may not be sufficiently populated; instead the data could make m ore frequent use of the type Music . Similarly, the predicate created may be used only for books, paintings, etc., and a different predicate composed could be prevalent for music. Finally, it could be that only individual people are listed as composers (rather than bands), so that one would need additional/alternative triple patterns: Problem: Our goal in this paper is to improve QA over RDF data in terms of robustness, by devising a method for generating good queries that return answers even in such hard situations. The solution must not come at the expense of increasing the burden on the user (e.g., by a tedious pro-cess of providing multiple question formulations). We would like to increase the system X  X  robustness also by allowing users to be informal if not sloppy in the way they express ques-tions. For example, a variant of our example question could be:  X  X oman song-writer covered love ballads by the stones X . A solution must identify the right sub-phrases like  X  X oman song-writer X ,  X  X overed X ,  X  X ove ballad X , and  X  X he stones X  and must automatically map them onto semantic targets of the underlying data: types/classes, relations, and entities.
Approach: This paper presents a solution based on gen-erating a set of query variants that cover the user question to different extents. Thus, we do not need a perfect translation, yet we can execute a query and return good answers. We still face the problem that the noun phrases and verbal phrases of the input are highly ambiguous and can be mapped to different semantic targets. We address this issue by a joint optimization for i) identifying the boundaries of the rele-vant phrases (e.g.,  X  X oman song-writer X  vs.  X  X ong-writer X ) and ii) jointly mapping all or a subset of these phrases onto classes, relations, and entities. The optimization is cast into an integer linear program with an appropriately designed objective function and constraints.

Sometimes a better query is generated by leaving out in-tractable parts of the user question. For example,  X  X ove bal-lad X  X ay not map to any class and even X  X oman song-writer X  may be too hard so that simply mapping it to the class Mu-sician may be the best option. To compensate for such in-complete translations, we tap into textual descriptions that are often available with structured Web data. These could be sources from which RDF triples have been gathered, texts about entities from their Wikipedia articles or homepages, or the contents that surround a table or microdata in a Web page. We extend SPARQL triple patterns into the notion of SPOX quad patterns that consist of the usual subject-predicate-object (SPO) parts and a teXt component with keywords or text phrases that need to be matched by the textual descriptions associated with the SPO triples in the data. For the example question, we could generate a query with the quad patterns: Contribution: This paper makes the following contribu-tions: i) extending the translation of natural-language ques-tions into SPARQL queries by means of quad patterns with textual conditions; ii) a model that allows generating re-laxed queries that capture only parts of the user inputs in SPO conditions and map other parts onto text conditions; iii) an optimization method based on an integer linear pro-gram that identifies the phrases to be mapped and com-putes a joint mapping for disambiguation; iv) a strategy for generating relaxed queries that return many answers and a suitable run-time environment. and v) ultimately, improv-ing the robustness of question-to-query translation without impeding user convenience.
RDF data and text: The Linked Data sources that we operate on consist of subject-predicate-object (SPO) triples following the RDF data model. Example triples are: As such data is often extracted or compiled from text-rich Web pages such as Wikipedia articles or thematic Web por-tals such as last.fm , we associate with each triple a textual context , thus forming SPOX quadruples, quads for short. An example quad could be
Angie type Music {"...a ballad which tells of the For Linked Data and for Web tables, such text extensions of structured data are very natural and can be easily compiled.
We can query this kind of data+text combination by ex-tending SPO triple patterns, the basic building block of the SPARQL query language, with a search condition about key-words or phrases: ?x type song {"love", "ballad"} . The semantics of a quad pattern is that a data triple must sat-isfy the SPO condition and its associated text context should match (at least partially) the specified text condition.
Input questions: We interpret the user X  X  input question as a sequence of phrases, where each phrase is a sub-sequence of consecutive words. We run a part-of-speech (POS) tag-ger and a dependency parser on the question. This way, we generate a set of candidates for sub-phrases that could be mapped to semantic items like classes, relations, or entities. The candidate phrases can overlap; our optimization model takes care of selecting a consistent subset of phrases (see Section 3).

Phrases potentially corresponding to classes and entities are detected using a large dictionary of surface forms. POS tags guide the identification of candidate relation phrases. We rely on an extension of the relational patterns in [16] and extend these with common nouns from a dictionary. This ap-proach produces heavily overlapping phrases. However, we only consider consecutive words, to avoid speculating about user intentions and to be robust against informal formula-tions. We include long phrases, the rationale being that these can sometimes be directly mapped to specific classes in the data (e.g., matching Wikipedia category names).

Predicate-argument dependencies: Some phrases will eventually be mapped to relations, so it is important to identify their arguments. We restrict ourselves to binary re-lations and aim to identify left/right arguments based on the dependency parsing of the input question. For each of t he candidate phrases that could possibly denote a rela-tion, we check for words that have particular dependencies with words in the phrase. The words that these dependen-cies point to are identified using the Stanford dependency parser [29], and all pairs of phrases that contain these words become candidates for left/right arguments of the potential-relation phrase. This is relatively liberal, as the considered dependencies can originate at different words of the phrase. The rationale here is to minimize the risk that we miss rele-vant dependencies; for selecting the correct argument struc-ture, we rely on the optimization model (see Section 3).
Dependency parsing is essential for questions that refer to multiple relationships. An example is  X  X ands from the US who covered songs by the Stones and the Beatles X . It is vital to recognize that the phrase X  X overed X  X as the left-hand argument  X  X ands X  and two right-hand arguments  X  X ongs by the Stones X  and  X  X ongs by the Beatles X .

Because we rely on translating questions to a triple-based language, noun phrases can often stand for a relation and its argument. An example is the question X  X ongs by the Stones X . Conceptually, the phrase X  X ongs by X  X xpresses both a relation and a class (the answer type). If the phrase  X  X ongs by X  is mapped to the relation performed , then we also add a latent concept phrase to stand for one of the two arguments of the relation. This is needed since our optimization model (see Section 3) constrains each phrase to map to at most one semantic target in the data.

Additionally, some patterns, such as adjective forms of countries and regions, and prepositions, can denote the ex-istence of a relation that is unspecified. For example, a ques-tion asking for  X  X wedish skateboarders X  indicates the exis-tence of a relation between Sweden and one or more members of the class Skateboarder . This relation is not specified in any token, so we generate a latent relation phrase to account for this, which, if chosen, would correspond to a wild card relation ?r . During query processing ?r will be bound to relations such as bornIn , livesIn , etc. It is possible that the knowledge has a class SwedishSkateboarders , the opti-mization model would make the decision which of the two interpretations of  X  X wedish skateboarders X  to choose.
Answer type: To determine the output structure (corre-sponding to the Select clause) for the query to be generated, we need to infer the type of the answer(s) that the user ex-pects, based on the question formulation. We use a sequence of heuristics: i) if a question word like  X  X ho X , X  X here X , etc. or  X  X hich X  with a modifier is present, it determines the answer type (i.e., person, location, etc., or the type of the modifier); ii) without such words, the head noun of the question X  X  sub-ject determines the output type (e.g., X  X ands X  X n X  X ands from the US covering . . .  X ) unless this is determined by the dis-ambiguation model to be part of an entity phrase, in which case iii) the first common noun occuring in the sentence that maps to a class determines the answer type.

Output: The main task addressed in this paper is to se-lect a subset of candidate phrases and map them onto seman-tic target items, where possible targets are classes, relations, and individual entities present in the underlying data and knowledge sources. We assume that we have a comprehen-sive dictionary of semantic items compiled from the data. For a given phrase, the candidate targets are those semantic items whose names or textual description (e.g., synonyms, glosses, or salient keyphrases) overlap with the wording in the phrase. For example,  X  X he stones X  would have candidates like gemstones (a class), the Rolling Stones (an entity of type MusicBand ), etc. The candidates for the phrase X  X over X  X ould include book covers (class), album covers (class), perform (relation, between musician and song), treat a subject (re-lation, e.g., between books and events), Thomas M. Cover (entity), etc. Compared to many other disambiguation tasks (e.g., for entity names), a major complexity faced here is the wide range of possible interpretations where not even the kind of semantic item is a priori clear.

More formally, given: (i) a set P = { p 1 , p 2 , . . . } of possibly overlapping candi-(ii) a set S = { s 1 , s 2 , . . . } of semantic target candidates as (iii) a set of phrase dependencies the desired output is: (i) a selected subset P  X  = { p i 1 , p i 2 , . . . }  X  P of phrases, (ii) a functional mapping P  X   X  S , and (iii) a set of semantic target dependencies that satisfy certain constraints.

We will discuss this task in Section 3. It is important to note that the mapping can be partial regarding P : not every phrase needs to be mapped (even if it does not overlap with other phrases).
We design an integer linear program (ILP) to jointly re-solve the question decomposition and disambiguation prob-lems. Our model couples the selection of phrases and their mapping onto semantic targets. Importantly, we introduce constraints that ensure that phrases are selected in a way that preserves their phrase dependencies in the image of the mapping onto semantic targets. This guarantees that we ar-rive at a set of chosen semantic items that are naturally grouped into triples and thus yield a well-formed SPARQL query.

In addition to the sets P , S , D P and D S introduced in the previous section, our model makes use of two kinds of pre-computed weights: (i) s ( i, j ) denotes a prior score for phrase p i mapping to semantic target s j , regardless of the context, and (ii) r ( k, l ) denotes the semantic relatedness be-tween semantic target items k and l , based on co-occurrences in the underlying data and knowledge sources (Yago, DBpe-dia, Wikipedia), to integrate the question context in scoring.
We now define the variables of the ILP, all of which are 0/1 decision variables: The result of the ILP is a 0/1 assignment of the X , Y , Z , Q , and T variables, from which a mapping P  X   X  S and a set of semantic dependencies D S can be read off. The Q and T variables couple the choice of phrases and their mapping to semantic targets with the dependencies among phrases and semantic items. This will ensure that the output consists of meaningful triples, rather than mapping phrases to targets independently. Moreover, the T variables also encode if a se-mantic dependency in the output actually produces answers when the corresponding triple pattern (alone) were executed on the data. The objective function below rewards decisions that lead to non-empty answers.

The following 0/1 constants are also used:
P ( t ) is the set of all phrases that contain the token (word) t . The set of latent concept phrases is P lat , and each latent concept phrase p lat  X  P lat phrase is generated by a relation phrase p r = gen ( p lat ).

Objective function: The objective of the ILP is to max-imize the following function:
The first term aims for good phrase-target mappings if each phrase were mapped separately. This is balanced against the second term which rewards mappings that result in two highly related entities being together in the mapping im-age. The third term reflects the goal of capturing phrase dependencies. The fourth term rewards decisions that lead to triple patterns with non-empty answers. The coefficients  X  ,  X  ,  X  ,  X  are hyper-parameters to be tuned with a small set of withheld training data, that is, pairs of questions and good query translations.

Constraints: The model described until now can be seen as a compact way to encode all possible interpretations of a questions with respect to a knowledge base. Some interpre-tations do not make sense, such as those where a type con-straint is violated or a word is part of multiple phrases (tak-ing into consideration latent ones). Other interpretations, such as those that cannot be answered by the knowledge base, should be less favored, everything else beging equal. The optimization is subject to the following constraints which both forbid nonsensical interpretations (1-12), and down-weigh empty interpretations of a query (13, 14): 1. A phrase maps to at most one semantic target.
 2. If a mapping p i 7 X  s j is chosen, then the target node 3. If a mapping p i 7 X  s j is chosen, then no phrase that 4. Z k,l is 1 iff both Z k and Z l are 1.
 5. X k is 1 and Z l is 1 iff Y k,l is 1.
 6. Each semantic triple can contribute to each role once 7. If Q m,n,d = 1 then the corresponding p n must be se-8. Each chosen phrase dependency (encoded in the values 9. Each semantic triple should have at least one class to 10. If any two Q variables have a token as part of two dif-11. Each relation in a chosen semantic dependency (en-12. A latent concept phrase p l (see Section 2) can be se-13. If a T v ariable is 1 then all variables X , Y , Z and Q 14. T variables corresponding to triple patterns that have
It is worth noting that the last constraint decreases the score of an interpretation in a triple pattern with no results in the KB, without forbidding such interpretations.
By iteratively adding a constraint that prevents the previ-ous solution to the ILP (corresponding to an interpretation of the question), we are able to generate multiple interpre-tations of a question in descending order of their scores.
Much of the query to be generated will naturally fall out from the computed mapping P  X   X  S and the semantic dependencies D S . As we also identify a phrase for the an-swer type (Section 2), we could now directly produce a set of triple-pattern conditions and a Select clause for a full-fledged SPARQL query. To ensure that the triple patterns join in a proper manner (through shared variables), we sub-stitute each class c by a variable ?c and add a triple pattern ?c type c . For example, with the mappings  X  bands  X  7 X  Mu-sicBand ,  X  cover  X  7 X  performed ,  X  songs  X  7 X  Music , we would first obtain the triple pattern MusicBand performed Music and then expand it into: As the mapping of phrases to classes or relations often comes with a semantic generalization step (e.g., mapping  X  X ove bal-lad X  X o Song or Music ), we may even consider already mapped phrases as candidates for additional text conditions. Finally, even if the generated query captures most or all of the input question, it may be overly specific and does not necessarily return answers. Next we discuss how to overcome this issue.
Structured and keyword querying are two paradigms tra-ditionally kept apart. We argue that a combination of both works best in our QA setting. We combine them by generat-ing SPOX quads rather than pure-SPARQL triple patterns only. We start with the SPO query that is generated from the outcome of the ILP-based optimization as explained in Section 4. Then we apply three kinds of extension and re-laxation techniques.

Text extension: The first technique identifies phrases or words in phrases that are not mapped to any of the triple patterns in the generated query. Some words may not be part of any detected phrase. Others may belong to detected phrases that, in the final disambiguation, could not be placed into a semantic triple pattern, and are hence not part of the structured query. Finally, it is possible that triple patterns are generated but are not connected to the output variable (Select clause) of the final query via join operations. To avoid computing Cartesian products and hard-to-interpret results, such triple patterns are discarded leading to  X  X eft-over X  phrases. In all three cases, the words are attached as keywords to the triple pattern for the type of the query out-put.

Empty-result relaxation: The second technique involves triple patterns that lead to empty results when executed against the underlying data. An empty-result SPO condi-tion indicates either a disambiguation error or an overly specific query that lacks coverage in the underlying data due to poorly populated classes or relations. It is also possible that every single SPO condition produces answers, but their combination yields an empty result. An example is: We may have many singers and many women in the data, but no or only very few female singers. In such cases, it is desirable to use only one of the two SPO conditions and cast the other one into a text condition of a SPOX quad pattern:
We compute this form of relaxation in an iterative bottom-up manner. The starting point is the single SPO pattern that encodes the answer type: ?x type &lt;class&gt; . We then itera-tively add triple patterns that have a join variable (initially ?x ) in common with previously added triple patterns. We check if the resulting query has an empty result: if so, the last added pattern is removed. This proceeds until there are no more patterns to be added. Words from phrases corre-sponding to semantic targets that have been removed are then added as keywords to the ?x type &lt;class&gt; pattern.
Extreme relaxation: The third technique that we con-sider is to cast the entire question into a text condition with an additional type filter on the result. The latter is derived from the answer-type heuristics (Section 2). An example would be ?x type Music {"band", "cover", "love", "ballad", The iterative procedure for the result-emptiness relaxation degrades into extreme relaxation if none of the considered queries produces answers. Extreme relaxation is also used when the ILP model does not produce any SPO triple pat-terns at all. Extremely relaxed queries can still be highly beneficial, most notably, when the query expresses a com-plex class in the knowledge base. For example, a question asking for  X  X merican rock bands that . . .  X  can be answered by
Relaxation entails that the query becomes less constrained, returning a larger number of results. This necessitates rank-ing the answers to make the output user-friendly. We employ statistical language models [48] to rank query results. Our s pecific approach, which we describe next, is inspired by El-bassuoni et al. [13, 14].

Formally, a query Q = ( q 1 , ...q n ) consists of keyword-augmented triple patterns of the form Analogously, a result T = ( t 1 , ....t n ) consists of keyword-augmented triples of the form Here, w q i and w t i are bags of keywords associated with the triple pattern and triple, respectively. We further assume that there is a substitution  X  from the variables of Q to resources in the knowledge base such that  X  i,  X  ( q i ) = t
We use a query-likelihood approach to rank results match-ing a query, which factors in salience of contained entities as well as textual relevance. We define the probability of generating the query Q from a result T as thus assuming that triple patterns are generated indepen-dently. The probability of generating the triple pattern q from the corresponding triple t i in the result is defined as
P ( q i | t i ) = P ( s q i , p q i , o q i , | s t i , p We thus assume, for tractability, that the structured and textual part of triple patterns are generated independently. For the generation of the structured part, we define
P ( s q i , p q i , o q i | s t i , p t i , o t i ) = (1  X   X  ) P ( s subject and object. The parameter  X   X  [0 , 1] is set according to whether s q i and/or o q i are variables in the triple pattern.
We use a unigram language model for the generation of the textual part and define as the probability of generating the bag of keywords associ-ated with the triple pattern q i from its counterpart in the keyword-augmented triple t i .

In our concrete implementation, the probabilities P ( s t P ( o t i ) are estimated based on the number of incoming links to the Wikipedia articles of s t i and o t i . The bag of key-words w t i in the keyword-augmented triple t i is a concate-nation of the documents associated with its subject and ob-ject. We associate with every entity from the knowledge base such a document, which consists of its infobox, categories, as well as its keyphrases (i.e., anchor texts of hyperlinks to its Wikipedia article). All probabilities in our model are smoothed taking into account global dataset statistics.
The above model considers a query result as a tuple of triples T that match the query. The final result displayed to the user is a projection of this query result. Duplicates in the final result are filtered out, and we only report the one having the highest query likelihood according to our model.
Intuitively, for a (relaxed) query with keyword-augmented triple patterns, our model returns results that match the (relaxed) structured part of the query in an order that favors results with salient entities and relevant keywords. Data: We used two prominent sources from the Web of Linked Data: the Yago ( yago-knowledge.org ) and DBpedia ( dbpedia.org ) knowledge bases, together comprising several hundred millions of RDF triples. As they link their entities to Wikipedia URLs, we augment each RDF entity with the textual description from the corresponding article. Our im-plementation manages this data by storing the RDF parts in PostgreSQL and the text parts in Lucene (with proper linkage across the two).

As Yago and DBpedia entities reference each other via sameAs links, our slice of Linked Data combines the rich class system of Yago (with more than 300,000 classes in-cluding all of WordNet) with the extensive fact collection of DBpedia (with more than 400 million RDF triples derived from Wikipedia infoboxes). Yago also provides a huge set of name-entity and phrase-entity pairs through its rdfs:label relation. This is the basis for our dictionary driving the gen-eration of possible mappings from phrases to semantic tar-gets. For classes as potential targets, we also incorporate WordNet synsets. For mapping potential relation phrases detected relying on POS tag patterns, we use the PATTY collection [30], which provides paraphrases for the (RDF properties) in DBpedia and Yago.

Benchmark: As our main test case, we adopted the bench-mark from the 2nd Workshop on Question Answering over Linked Data [35]. QALD-2 consists of 100 natural-language test questions of two different flavors: factoid and list ques-tions (plus 100 withheld questions for training). We dis-carded questions that require counting or return literals (e.g., numbers) rather than entities as answers. For test questions, this resulted in: a) 19 factoid questions that are supposed to return exactly one correct result, and b) 30 list ques-tions that produce sets of answers. Examples are  X  X hat is the capital of Canada X  for the former, and  X  X eople who were born in Vienna and died in Berlin X  for the latter. The QALD-2 benchmark comes with manually compiled correct answers. In the case of factoid questions, this is our ground truth. In the case of list questions, this is what we refer to as QALD ground truth . Some of the methods returned ad-ditional answers that are correct but not included in the QALD-2 ground truth. We manually assessed the correct-ness of these extra answers, establishing an extended ground truth answer set.

In addition, we experimented with the 48 telegraphic queries used by Pound et al. [34]. 19 of these queries are not real questions, but are merely entity lookups. They give a de-scription of an entity and ask for disambiguation onto the right entity in Yago/DBpedia/Wikipedia. An example is  X  X uernica picasso X . We excluded these lookup queries from the test set. Among the remaining 29 true questions, we dis-regarded 7 cases that request literals rather than entities as answers. This left us with 22 true questions, further broken down into 16 list questions and 6 factoid questions.
Performance measures: All the methods in our exper-iments return ranked lists of answers. For factoid questions with single correct answers, we use the established notion of Mean Reciprocal Rank (MRR) [8] as our main measure of quality. In addition, we also report on precision at a cut-off r ank of 10.

For list questions, we mainly report the Normalized Dis-counted Cumulative Gain (NDCG) [28] as a measure com-bining precision and recall with geometrically decreasing weights of ranks. Additionally, we give numbers for preci-sion at different cut-off ranks and for Mean Average Preci-sion (MAP).

Measures with a cut-off k are evaluated by taking into consideration that the system might produce less than k results. This can happen when the generated query is unre-laxed (structured only), or the expected result type has few instances. For example, if for question q a list on n results is returned and rel ( e, q ) is an indicator function equal to 1 if e is a correct result and 0 otherwise, then P recision @ k is computed as follows: This is accounted for in all other measures in Tables 1 and 2. Later in this section we will consider recall at a certain cut-off. This is handeled differently, as we will explain.
Methods under comparison: We compare our method against three opponents:
Parameters were tuned using the QALD-2 training set.
Tables 1 and 2 show the results for the QALD-2 list and factoid questions, respectively.

For list questions, the numbers, especially for our main success metric NDCG, show the superiority of the SPOX+Relax method, with an NDCG@10 of 0.51. Both variants with pure QALD ground truth SPO X + Relax 0.51 0.53 0.49 0.46 0.58 SPO+Relax 0.41 0.43 0.46 0.44 0.47 SPO 0.41 0.42 0.46 0.44 0.47 KW+Type 0.24 0.29 0.15 0.10 0.28 Extended ground truth SPO X + Relax 0.60 0.54 0.60 0.48 0.65 SPO+Relax 0.42 0.42 0.49 0.46 0.50 SPO 0.42 0.41 0.49 0.45 0.46 KW+Type 0.30 0.41 0.23 0.13 0.34 Table 2: Results for QALD-2 factoid questions S PO queries, which represent the prior state-of-the-art in QA over Linked Data, perform significantly worse, with an NDCG@10 of 0.41. The results for these two variants are consistently close to each other, which shows that not con-sidering keywords from the question in SPO+Relax limits the quality of results.

These methods often miss important search conditions from the input question, or generate overly specific queries. The SPOX+Relax method, on the other hand, behaves more robustly, because it judiciously chooses between structured and keyword conditions in the query and also avoids quad patterns with insufficient coverage in the data. The KW+Type performed very poorly. Despite the manually specified type constraints, these are still not enough to narrow down the space of possible candidates, more structure was needed for most questions.

As an example, take a list question asking for  X  X wedish professional skateboarders X . We initially map it to the SPO query Although the query captures the question properly, it re-turns no results, as the class Professional is sparsely pop-ulated. Relaxation results in the SPOX query which reurns a satisfactory result.

For factoid questions, the general trends are the same, with SPOX+Relax being the clear winner with an MRR score of 0.72, followed by SPO+Relax which scored 0.54. However, the gains here less pronounced here compared to list questions. SPO already performs well with an MRR of 0.53. For factoid questions, the main issue was answer types that were not properly mapped because a different type, with the same surface form as the one intended, was too List SPO X + Relax@ k = 1 0.50 0.15 0.23 SPO X +Relax@ k = 10 0.49 0.41 0.45 SPO X +Relax@ k = 100 0.46 0.48 0.47 SPO X +Relax@ k = 500 0.44 0.58 0.50 SemSek 0.28 0.29 0.29 M HE 0.26 0.36 0.30 QAKis 0.15 0.16 0.15 Factoid SPO X + Relax@ k = 1 0.68 0.68 0.68 SPO X +Relax@ k = 2 0.61 0.74 0.67 SPO X +Relax@ k = 5 0.58 0.79 0.67 SPO X +Relax@ k = 10 0.55 0.79 0.65 SemSek 0.71 0.78 0.74 M HE 0.52 0.57 0.54 QAKis 0.26 0.26 0.26 Table 3: Comparison to other systems in QALD-2 b ased on the QALD-2 ground truth prominent. An example of a factoid question where our sys-tem does not return satisfactory results is  X  X ho developed Skype? X . The expected answer is an orginization ( SkypeTech-nologies ). Our system favors mapping  X  X ho X  to the class person , which means that even with extreme relaxation, which only preserves the type constraint, the correct answer cannot be retrieved.

We also compared our results against the systems partic-ipating in QALD-2: SemSek [2], MHE [41], and QAKis [9]. QALD-2 adopts set-based measures, whereas our system performs ranked retrieval to compensate for relaxation. To make our results comparable, we consider precision and re-call at various cut-off thresholds k . We computed recall with respect to the size of the ground truth result set, regardless of k . This generally results in penalizing our system as at most 10 relevant results can be returned when k = 10, re-gardless of the total number of relevant results out there. The official QALD-2 evaluation considers  X  X artially right X  answers as good enough [41]. Table 3 shows the results.
For list questions, our system clearly outperforms other systems on all measures. Questions here vary between those that have a couple of answers and those that have more than a hundred. As more results are viewed, there is rapid gain in recall for each cut-off threshold, with little sacrifice of precision, which speaks for the ranking approach.
For factoid questions, our system is outperformed only by SemSek, but the margin is smaller than the gains we make on list questions. The main issue is the same as the one above, namely that of prominent classes taking over the result type. This explains the gap between our results and those of SemSek. Still, overall, our system greatly benefits from falling back onto the excpeted result type for relxation, despite these special cases.
 For the telegraphic query workload of Pound et al. [34], SPOX+Relax again turned out to be the best method in our experiments. For factoid questions we achieved an MRR of 0.83, and for list questions a precision@10 of 0.73. These numbers are similar to those reported in [34].

Note, that the results are not directly comparable, as that prior work used an old, smaller version of Yago as its sole data source and reported only the combined per-formance of all questions regardless of their nature (simple entity lookups, factoid questions, difficult list questions). Table 4 shows the results we obtained for some questions. The first two questions are based on QALD-2, and the third is from the telegraphic query workload of Pound et al. [34].
For the first two, we show the query generated initially, the relaxed query, the number of results in each of the two ground truths we consider, and the number of relevant re-sults in the top 10 answers with respect to each of the two ground truths (both return more than 10 answers). We dis-cussed the first question earlier. The second one results in a SPOX pattern query that includes a keyword component, as the system could not map the verb X  X welt X  X o an appropriate relation. This query returns satisfactory results to the user.
For the last query, despite the fact that the query gen-erated fully captures the user X  X  intention, no results are re-turned by DBpedia, hence the need for relaxation. The Pre-cision@10 (again, this query returns more than 10 results) is equal to 1.0 which means that all returned results are of Grammy awarded guitarists. Question answering (QA): Methods for natural-language QA (see[12, 26, 27, 32, 36, 43] and references given there) have traditionally cast questions into keyword queries in or-der to retrieve relevant passages from the underlying cor-pora and the Web. Then linguistic and statistical methods are used to extract answer candidates from passages, and to aggregate the evidence for candidate ranking and choosing the best answer(s). Some prior work (e.g., the START sys-tem [25]) made use of knowledge sources like the full text of Wikipedia but did not tap into structured data or knowl-edge. The IBM Watson system [23] recently achieved im-pressive improvements in QA, but made only limited use of structured data: 1) knowledge bases were used to answer question fragments that can be mapped into structured form with high confidence [10], and 2) checking lexical types of candidate answers in knowledge bases was used to prune spurious answers[10, 24].

Recently, research on QA over the Web of Linked Data has been pursued, and the QALD benchmark for evaluat-ing progress has been defined [35]. In this setting, natural-language questions are mapped into structured queries, typ-ically using the SPARQL language over RDF data [41]. The translation is performed either based on question/query tem-plates [15, 40, 44] or by using algorithms for word-sense dis-ambiguation [46]. The latter is most closely related to this paper. However, that prior work was limited to generating pure SPARQL queries. In our experiments, we will compare our approach to this baseline.

Keyword search: There is ample prior work on pure keyword queries over structured data, especially, relational databases (e.g., [1, 7, 18, 20, 47]). The semantics of such queries is to find matches in attributes of several relational records and then use foreign-key relationships to compute a connected result graph that explains how the various key-word matches relate to each other. One technique for this model is to compute so-called group-Steiner trees and use data statistics for ranking.

Telegraphic queries: None of the aforementioned keyword-search methods attempts to map the user input into struc-
Question Generated Query Relaxed QALD Extended QALD Extende d  X  X wedish professional s kateboarders X   X  X hich Greek goddesses d welt on Mount Olympus? X   X  X uitarists awarded a X  g rammy X  Award"} .
 t ured predicates. Recent work on telegraphic queries [11, 33, 34, 37] pursues this very goal of translating such user requests into SQL or SPARQL queries. The focus here is on long keyword queries that describe entities or contain rela-tional phrases that connect different entities and/or classes. The main challenge is to decompose the user input and in-fer an adequate query interpretation for often underspeci-fied inputs such as  X  X ands songs Stones X . [34] uses a trained CRF for input decomposition and for mapping phrases onto semantic items; the training is based on query logs. [37] de-vised a probabilistic graphical model with latent variables for this purpose. Although the goal of these works is not really the same as ours, our experiments include the bench-mark queries by [34] and a comparison to the performance of their technique.

Ranking: Statistical language models (LM X  X ) [48] have been used for ranking the results of entity search (e.g., [4, 6, 17, 31, 42]). Also random-walk-based models have been developed for this purpose (e.g., [19, 38]). The typical set-ting here is to take a keyword query as input and return entities rather than documents, as evaluated in the TREC Entity track [5]. Alternatively, the user could enter a struc-tured SPARQL query or a combination of SPARQL and key-word conditions, as currently pursued in the INEX Linked Data track [45]. LM-based ranking was taken one step fur-ther by [13, 14] who addressed the result ranking for full-fledged SPARQL queries (as opposed to mere entity search) with additional keyword conditions. We utilize this work and adapted these LM X  X  to our setting.
With the explosion of structured data on the Web, trans-lating natural-language questions into structured queries seems the most intuitive approach. However, state-of-the-art meth-ods are not very robust as they tend to produce sophisticated queries that are correct translations yet return no answers from the underlying data. To enhance the robustness, we have proposed a new kind of optimization model followed by a suite of query relaxation techniques. Our experimental results with the QALD benchmark show that this approach can significantly enhance the quality of question answering. [1] Sanjay Agrawal, Surajit Chaudhuri, and Gautam [2] Nitish Aggarwal. Cross Lingual Semantic Search by [3] S  X  oren Auer, Christian Bizer, Georgi Kobilarov, Jens [4] Krisztian Balog, Leif Azzopardi, and Maarten de [5] Krisztian Balog, Leif Azzopardi, and Maarten de [6] Krisztian Balog, Yi Fang, Maarten de Rijke, Pavel [7] Gaurav Bhalotia, Arvind Hulgeri, Charuta Nakhe, [8] Stefan B  X  uttcher, Charles L. A. Clarke, and Gordon [9] Elena Cabrio, Julien Cojan, Alessio Palmero Aprosio, [10] Jennifer Chu-Carroll, James Fan, Branimir [11] Marek Ciglan, Kjetil N X rv  X ag, and Ladislav Hluch  X y. [12] Hoa Trang Dang, Diane Kelly, and Jimmy J. Lin. [13] Shady Elbassuoni, Maya Ramanath, Ralf Schenkel, [14] Shady Elbassuoni, Maya Ramanath, and Gerhard [15] Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans [16] Anthony Fader, Stephen Soderland, and Oren [17] Hui Fang and ChengXiang Zhai. Probabilistic Models [18] Vagelis Hristidis and Yannis Papakonstantinou. [19] Vagelis Hristidis, Heasoo Hwang, and Yannis [20] Hao He, Haixun Wang, Jun Yang, and Philip S. Yu. [21] Heath, T., and Bizer, C. Linked Data: Evolving the [22] Johannes Hoffart, Fabian M. Suchanek, Klaus [23] IBM 2012. Special Issue on  X  X his is Watson X  IBM [24] Aditya Kalyanpur, J. William Murdock, James Fan, [25] Boris Katz, Sue Felshin, Gregory Marton, Federico [26] Cody C. T. Kwok, Oren Etzioni, and Daniel S. Weld. [27] Lin, J. J.; An exploration of the principles underlying [28] Christopher D. Manning, Prabhakar Raghavan, and [29] Marie-Catherine de Marneffe, Bill MacCartney, and [30] Ndapandula Nakashole, Gerhard Weikum, and [31] Zaiqing Nie, Yunxiao Ma, Shuming Shi, Ji-Rong [32] Anselmo Pe  X nas, Eduard H. Hovy, Pamela Forner, [33] Jeffrey Pound, Ihab F. Ilyas, and Grant E. Weddell. [34] Jeffrey Pound, Alexander K. Hudek, Ihab F. Ilyas, [35] Second Workshop on Question Answering over [36] Deepak Ravichandran and Eduard H. Hovy. Learning [37] Uma Sawant and Soumen Chakrabarti Learning [38] Pavel Serdyukov, Henning Rode, and Djoerd [39] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard [40] Christina Unger, Lorenz B  X  uhmann, Jens Lehmann, [41] Christina Unger, Philipp Cimiano, Vanessa Lopez, [42] David Vallet and Hugo Zaragoza. Inferring the most [43] Ellen M. Voorhees. Overview of the TREC 2003 [44] Sebastian Walter, Christina Unger, Philipp Cimiano, [45] Qiuyue Wang, Jaap Kamps, Georgina Ramirez [46] Mohamed Yahya, Klaus Berberich, Shady [47] Jeffrey Xu Yu, Lu Qin, and Lijun Chang Keyword [48] ChengXiang Zhai. Statistical Language Models for
