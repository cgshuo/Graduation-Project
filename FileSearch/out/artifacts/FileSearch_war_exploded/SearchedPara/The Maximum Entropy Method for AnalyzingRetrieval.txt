 We present a model, based on the maximum entropy method, for analyzing various measures of retrieval performance such as average precision, R-precision, and precision-at-cutoffs. Our methodology treats the value of such a measure as a constraint on the distribution of relevant documents in an unknown list, and the maximum entropy distribution can be determined subject to these constraints. For good mea-sures of overall performance (such as average precision), the resulting maximum entropy distributions are highly corre-lated with actual distributions of relevant documents in lists as demonstrated through TREC data; for poor measures of overall performance, the correlation is weaker. As such, the maximum entropy method can be used to quantify the over-all quality of a retrieval measure. Furthermore, for good measures of overall performance (such as average precision), we show that the corresponding maximum entropy distribu-tions can be used to accurately infer precision-recall curves and the values of other measures of performance, and we demonstrate that the quality of these inferences far exceeds that predicted by simple retrieval measure correlation, as demonstrated through TREC data.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software  X  Performance evaluation Theory, Measurement, Experimentation Evaluation, Maximum Entropy, Average Precision
We gratefully acknowledge the support provided by NSF grant CCF-0418390.

The efficacy of retrieval systems is evaluated by a num-ber of performance measures such as average precision, R-precision, and precisions at standard cutoffs. Broadly speak-ing, these measures can be classified as either system-oriented measures of overall performance (e.g., average precision and R-precision) or user-oriented measures of specific perfor-mance (e.g., precision-at-cutoff 10) [3, 12, 5]. Different mea-sures evaluate different aspects of retrieval performance, and much thought and analysis has been devoted to analyzing the quality of various different performance measures [10, 2, 17].

We consider the problem of analyzing the quality of vari-ous measures of retrieval performance and propose a model based on the maximum entropy method for evaluating the quality of a performance measure. While measures such as average precision at relevant documents, R-precision, and 11pt average precision are known to be good measures of overall performance, other measures such as precisions at specific cutoffs are not. Our goal in this work is to develop a model within which one can numerically assess the overall quality of a given measure based on the reduction in un-certainty of a system X  X  performance one gains by learning the value of the measure. As such, our evaluation model is primarily concerned with assessing the relative merits of system-oriented measures, but it can be applied to other classes of measures as well.

We begin with the premise that the quality of a list of documents retrieved in response to a given query is strictly a function of the sequence of relevant and non-relevant docu-ments retrieved within that list (as well as R , the total num-ber of relevant documents for the given query). Most stan-dard measures of retrieval performance satisfy this premise. Our thesis is then that given the assessed value of a  X  X ood X  overall measure of performance, one X  X  uncertainty about the sequence of relevant and non-relevant documents in an un-known list should be greatly reduced. Suppose, for exam-ple, one were told that a list of 1,000 documents retrieved in response to a query with 200 total relevant documents con-tained 100 relevant documents. What could one reasonably infer about the sequence of relevant and non-relevant doc-uments in the unknown list? From this information alone, one could only reasonably conclude that the likelihood of seeing a relevant document at any rank level is uniformly 1/10. Now suppose that one were additionally told that the average precision of the list was 0.4 (the maximum possi-ble in this circumstance is 0.5). Now one could reasonably conclude that the likelihood of seeing relevant documents at low numerical ranks is much greater than the likelihood of seeing relevant documents at high numerical ranks. One X  X  uncertainty about the sequence of relevant and non-relevant documents in the unknown list is greatly reduced as a conse-quence of the strong constraint that such an average preci-sion places on lists in this situation. Thus, average precision is highly informative. On the other hand, suppose that one were instead told that the precision of the documents in the rank range [100 , 110] was 0.4. One X  X  uncertainty about the sequence of relevant and non-relevant documents in the unknown list is not appreciably reduced as a consequence of the relatively weak constraint that such a measurement places on lists. Thus, precision in the range [100 , 110] is not a highly informative measure. In what follows, we develop a model within which one can quantify how informative a measure is.

We consider two questions: (1) What can reasonably be inferred about an unknown list given the value of a mea-surement taken over this list? (2) How accurately do these inferences reflect reality? We argue that the former question is properly answered by considering the maximum entropy distributions subject to the measured value as a constraint, and we demonstrate that such maximum entropy models corresponding to good overall measures of performance such as average precision yield accurate inferences about under-lying lists seen in practice (as demonstrated through TREC data).

More specifically, we develop a framework based on the maximum entropy method which allows one to infer the most  X  X easonable X  model for the sequence of relevant and non-relevant documents in a list given a measured constraint. From this model, we show how one can infer the most  X  X ea-sonable X  model for the unknown list X  X  entire precision-recall curve. We demonstrate through the use of TREC data that for  X  X ood X  overall measures of performance (such as average precision), these inferred precision-recall curves are accurate approximations of actual precision-recall curves; however, for  X  X oor X  overall measures of performance, these inferred precision-recall curves do not accurately approximate actual precision-recall curves. Thus, maximum entropy modeling can be used to quantify the quality of a measure of overall performance.

We further demonstrate through the use of TREC data that the maximum entropy models corresponding to  X  X ood X  measures of overall performance can be used to make ac-curate predictions of other measurements. While it is well known that  X  X ood X  overall measures such as average preci-sion are well correlated with other measures of performance, and thus average precision could be used to reasonably pre-dict other measures of performance, we demonstrate that the maximum entropy models corresponding to average pre-cision yield inferences of other measures even more highly correlated with their actual values, thus validating both av-erage precision and maximum entropy modeling.

In the sections that follow, we first describe the maxi-mum entropy method and discuss how maximum entropy modeling can be used to analyze measures of retrieval per-formance. We then describe the results of applying our methodology using TREC data, and we conclude with a summary and future work.
The concept of entropy as a measure of information was first introduced by Shannon [20], and the Principle of Max-imum Entropy was introduced by Jaynes [7, 8, 9]. Since its introduction, the Maximum Entropy Method has been ap-plied in many areas of science and technology [21] including natural language processing [1], ambiguity resolution [18], text classification [14], machine learning [15, 16], and infor-mation retrieval [6, 11], to name but a few examples. In what follows, we introduce the maximum entropy method through a classic example, and we then describe how the maximum entropy method can be used to evaluate measures of retrieval performance.

Suppose you are given an unknown and possibly biased six-sided die and were asked the probability of obtaining any particular die face in a given roll. What would your answer be? This problem is under-constrained and the most seem-ingly  X  X easonable X  answer is a uniform distribution over all faces. Suppose now you are also given the information that the average die roll is 3 . 5. The most seemingly  X  X easonable X  answer is still a uniform distribution. What if you are told that the average die roll is 4 . 5? There are many distribu-tions over the faces such that the average die roll is 4 . 5; how can you find the most seemingly  X  X easonable X  distribution? Finally, what would your answer be if you were told that the average die roll is 5 . 5? Clearly, the belief in getting a 6 increases as the expected value of the die rolls increases. But there are many distributions satisfying this constraint; which distribution would you choose?
The  X  X aximum Entropy Method X  (MEM) dictates the most  X  X easonable X  distribution satisfying the given constraints. The  X  X rinciple of Maximal Ignorance X  forms the intuition behind the MEM; it states that one should choose the dis-tribution which is least predictable (most random) subject to the given constraints. Jaynes and others have derived nu-merous entropy concentration theorems which show that the vast majority of all empirical frequency distributions (e.g., those corresponding to sequences of die rolls) satisfying the given constraints have associated empirical probabilities and entropies very close to those probabilities satisfying the con-straints whose associated entropy is maximal [7].
Thus, the MEM dictates the most random distribution satisfying the given constraints, using the entropy of the probability distribution as a measure of randomness. The entropy of a probability distribution ~p = { p 1 , p 2 , . . . , p a measure of the uncertainty (randomness) inherent in the distribution and is defined as follows Thus, maximum entropy distributions are probability dis-tributions making no additional assumptions apart from the given constraints.

In addition to its mathematical justification, the MEM tends to produce solutions one often sees in nature. For example, it is known that given the temperature of a gas, the actual distribution of velocities in the gas is the maximum entropy distribution under the temperature constraint. We can apply the MEM to our die problem as follows. Let the probability distribution over the die faces be ~p = { p 1 , . . . , p 6 } . Mathematically, finding the maximum entropy distribution over die faces such that the expected die roll is d corresponds to the following optimization problem: Maximize: H ( ~p ) Subject to: 1. 2. The first constraint ensures that the solution forms a distri-bution over the die faces, and the second constraint ensures that this distribution has the appropriate expectation. This is a constrained optimization problem which can be solved using the method of Lagrange multipliers. Figure 1 shows three different maximum entropy distributions over the die faces such that the expected die roll is 3 . 5, 4 . 5, and 5 . 5, respectively.
Suppose that you were given a list of length N correspond-ing to the output of a retrieval system for a given query, and suppose that you were asked to predict the probabil-ity of seeing any one of the 2 N possible patterns of relevant documents in that list. In the absence of any information about the query, any performance information for the sys-tem, or any a priori modeling of the behavior of retrieval systems, the most  X  X easonable X  answer you could give would be that all lists of length N are equally likely. Suppose now that you are also given the information that the expected number of relevant documents over all lists of length N is R ret . Your  X  X easonable X  answer might then be a uniform distribution over all ` N R relevant documents. But what if apart from the constraint on the number of relevant documents retrieved, you were also given the constraint that the expected value of aver-age precision is ap ? If the average precision value is high, then of all the ` N R the lists in which the relevant documents are retrieved at low numerical ranks should have higher probabilities. But how can you determine the most  X  X easonable X  such distribu-tion? The maximum entropy method essentially dictates the most reasonable distribution as a solution to the following constrained optimization problem.

Let p ( r 1 , ..., r N ) be a probability distribution over the relevances associated with document lists of length N , let rel ( r 1 , ..., r N ) be the number of relevant documents in a list, and let ap ( r 1 , ..., r N ) be the average precision of a list. Then the maximum entropy method can be mathematically for-mulated as follows: Maximize: H ( ~p ) Subject to: 1. P 2. P 3. P Note that the solution to this optimization problem is a distribution over possible lists, where this distribution ef-fectively gives one X  X  a posteriori belief in any list given the measured constraint.

The previous problem can be formulated in a slightly dif-ferent manner yielding another interpretation of the problem and a mathematical solution. Suppose that you were given a list of length N corresponding to output of a retrieval sys-tem for a given a query, and suppose that you were asked to predict the probability of seeing a relevant document at some rank. Since there are no constraints, all possible lists of length N are equally likely, and hence the probability of seeing a relevant document at any rank is 1/2. Suppose now that you are also given the information that the expected number of relevant documents over all lists of length N is R ret . The most natural answer would be a R ret /N uniform probability for each rank. Finally, suppose that you are given the additional constraint that the expected average precision is ap . Under the assumption that our distribu-tion over lists is a product distribution (this is effectively a fairly standard independence assumption), we may solve this problem as follows. Let where p ( r i ) is the probability that the document at rank i is relevant. We can then solve the problem of calculating the probability of seeing a relevant document at any rank using the MEM. For notational convenience, we will refer to this product distribution as the probability-at-rank distribution and the probability of seeing a relevant document at rank i , p ( r i ), as p i .

Standard results from information theory [4] dictate that if p ( r 1 , . . . , r N ) is a product distribution, then where H ( p i ) is the binary entropy Furthermore, it can be shown that given a product distribu-tion p ( r 1 , . . . , r N ) over the relevances associated with docu-
H ( p ) ment lists of length N , the expected value of average preci-sion is (The derivation of this formula is omitted due to space con-straints.) Furthermore, since p i is the probability of seeing a relevant document at rank i , the expected number of rel-evant documents retrieved until rank N is P N i =1 p i .
Now, if one were given some list of length N , one were told that the expected number of relevant documents is R ret , one were further informed that the expected average precision is ap , and one were asked the probability of seeing a relevant document at any rank under the independence assumption stated, one could apply the MEM as shown in Figure 2. Note that one now solves for the maximum entropy product distribution over lists, which is equivalent to a maximum en-tropy probability-at-rank distribution. Applying the same ideas to R-precision and precision-at-cutoff k , one obtains analogous formulations as shown in Figures 3 and 4, respec-tively.

All of these formulations are constrained optimization prob-lems, and the method of Lagrange multipliers can be used to find an analytical solution, in principle. When analyti-cal solutions cannot be determined, numerical optimization methods can be employed. The maximum entropy distri-butions for R-precision and precision-at-cutoff k can be ob-tained analytically using the method of Lagrange multipli-ers. However, numerical optimization methods are required to determine the maximum entropy distribution for aver-age precision. In Figure 5, examples of maximum entropy probability-at-rank curves corresponding to the measures average precision, R-precision, and precision-at-cutoff 10 for a run in TREC8 can be seen. Note that the probability-at-rank curves are step functions for the precision-at-cutoff and R-precision constraints; this is as expected since, for example, given a precision-at-cutoff 10 of 0.3, one can only reasonably conclude a uniform probability of 0.3 for seeing a relevant document at any of the first 10 ranks. Note, however, that the probability-at-rank curve corresponding to average precision is smooth and strictly decreasing.
Using the maximum entropy probability-at-rank distribu-tion of a list, we can infer the maximum entropy precision-recall curve for the list. Given a probability-at-rank distri-bution ~p , the number of relevant documents retrieved un-til rank i is REL ( i ) = P i j =1 p j . Therefore, the precision and recall at rank i are PC ( i ) = REL ( i ) /i and REC ( i ) = REL ( i ) /R . Hence, using the maximum entropy probability-at-rank distribution for each measure, we can generate the maximum entropy precision-recall curve of the list. If a mea-sure provides a great deal of information about the under-lying list, then the maximum entropy precision-recall curve should approximate the precision-recall curve of the actual list. However, if a measure is not particularly informa-tive, then the maximum entropy precision-recall curve need not approximate the actual precision-recall curve. There-fore, noting how closely the maximum entropy precision-recall curve corresponding to a measure approximates the precision-recall curve of the actual list, we can calculate how much information a measure contains about the actual list, and hence how  X  X nformative X  a measure is. Thus, we have a methodology for evaluating the evaluation measures them-selves.

Using the maximum entropy precision-recall curve of a measure, we can also predict the values of other measures. For example, using the maximum entropy precision-recall curve corresponding to average precision, we can predict the precision-at-cutoff 10. For highly informative measures, these predictions should be very close to reality. Hence, we have a second way of evaluating evaluation measures.
We tested the performance of the evaluation measures av-erage precision, R-precision, and precision-at-cutoffs 5, 10, 15, 20, 30, 100, 200, 500 and 1000 using data from TRECs 3, 5, 6, 7, 8 and 9. For any TREC and any query, we chose those systems whose number of relevant documents retrieved was at least 10 in order to have a sufficient number of points on the precision-recall curve. We then calculated the maxi-mum entropy precision-recall curve subject to the given mea-sured constraint, as described above. The maximum entropy precision-recall curve corresponding to an average precision constraint cannot be determined analytically; therefore, we used numerical optimization 1 to find the maximum entropy distribution corresponding to average precision.

We shall refer to the execution of a retrieval system on a particular query as a run . Figure 6 shows examples of maximum entropy precision-recall curves corresponding to average precision, R-precision, and precision-at-cutoff 10 for three different runs, together with the actual precision-recall curves. We focused on these three measures since they are perhaps the most commonly cited measures in IR. We also provide results for precision-at-cutoff 100 in later plots and detailed results for all measures in a later table. As can be seen in Figure 6, using average precision as a constraint, one can generate the actual precision-recall curve of a run with relatively high accuracy.

In order to quantify how good an evaluation measure is in generating the precision-recall curve of an actual list, we consider two different error measures: the root mean squared error (RMS) and the mean absolute error (MAE). { 1 /R, 2 /R, . . . , R ret /R } where R ret is the number of rele-vant documents retrieved by a system and R is the number of documents relevant to the query, and let { m 1 , m 2 , . . . , m be the estimated precisions at the corresponding recall lev-els for a maximum entropy distribution corresponding to a measure. Then the MAE and RMS errors are calculated as follows.
 The points after recall R ret /R on the precision-recall curve are not considered in the evaluation of the MAE and RMS errors since, by TREC convention, the precisions at these recall levels are assumed to be 0.

In order to evaluate how good a measure is at inferring actual precision-recall curves, we calculated the MAE and RMS errors of the maximum entropy precision-recall curves corresponding to the measures in question, averaged over all runs for each TREC. Figure 7 shows how the MAE and RMS errors for average precision, R-precision, precision-at-cutoff 10, and precision-at-cutoff 100 compare with each other for each TREC. The MAE and RMS errors follow the same pattern over all TRECs. Both errors are consistently and significantly lower for average precision than for the other measures in question, while the errors for R-precision are consistently lower than for precision-at-cutoffs 10 and 100.
Table 1 shows the actual values of the RMS errors for all measures over all TRECs. In our experiments, MAE and RMS errors follow a very similar pattern, and we therefore omit MAE results due to space considerations. From this table, it can be seen that average precision has consistently lower RMS errors when compared to the other measures. The penultimate column of the table shows the average RMS errors per measure averaged over all TRECs. On average, R-precision has the second lowest RMS error after average precision, and precision-at-cutoff 30 is the third best mea-sure in terms of RMS error. The last column of the table We used the TOMLAB Optimization Environment for Matlab. shows the percent increase in the average RMS error of a measure when compared to the RMS error of average preci-sion. As can be seen, the average RMS errors for the other measures are substantially greater than the average RMS error for average precision.

We now consider a second method for evaluating how in-formative a measure is. A highly informative measure should properly reduce one X  X  uncertainty about the distribution of relevant and non-relevant documents in a list; thus, in our maximum entropy formulation, the probability-at-rank dis-tribution should closely correspond to the pattern of rele-vant and non-relevant documents present in the list. One should then be able to accurately predict the values of other measures from this probability-at-rank distribution. Given a probability-at-rank distribution p 1 , p 2 , . . . , p can predict average precision, R-precision and precision-at-cutoff k values as follows: The plots in the top row of Figures 8 and 9 show how average precision is actually correlated with R-precision, precision-at-cutoff 10, and precision-at-cutoff 100 for TRECs 6 and 8, respectively. Each point in the plot corresponds to a sys-tem and the values of the measures are averaged over all queries. Using these plots as a baseline for comparison, the plots in the bottom row of the figures show the correlation between the actual measures and the measures predicted using the average precision maximum entropy probability-at-rank distribution. Consider predicting precision-at-cutoff 10 values using the average precision maximum entropy dis-tributions in TREC 6. Without applying the maximum en-tropy method, Figure 8 shows that the two measures are correlated with a Kendall X  X   X  value of 0 . 671. However, the precision-at-cutoff 10 values inferred from the average pre-cision maximum entropy distribution have a Kendall X  X   X  value of 0 . 871 when compared to actual precisions-at-cutoff 10. Hence, the predicted precision-at-cutoff 10 and actual precision-at-cutoff 10 values are much more correlated than the actual average precision and actual precision-at-cutoff 10 values. Using a similar approach for predicting R-precision and precision-at-cutoff 100, it can be seen in Figures 8 and 9 that the measured values predicted by using average preci-sion maximum entropy distributions are highly correlated with actual measured values.

We conducted similar experiments using the maximum entropy distributions corresponding to other measures, but since these measures are less informative, we obtained much smaller increases (and sometimes even decreases) in inferred correlations. (These results are omitted due to space con-siderations.) Table 2 summarizes the correlation improve-ments possible using the maximum entropy distribution cor-responding to average precision. The row labeled  X  act gives the actual Kendall X  X   X  correlation between average precision and the measure in the corresponding column. The row labeled  X  inf gives the Kendall X  X   X  correlation between the measure inferred from the maximum entropy distribution corresponding to average precision and the measure in the corresponding column. The row labeled %Inc gives the per-cent increase in correlation due to maximum entropy mod-eling. As can be seen, maximum entropy modeling yields great improvements in the predictions of precision-at-cutoff values. The improvements in predicting R-precision are no-ticeably smaller, though this is largely due to the fact that average precision and R-precision are quite correlated to be-gin with.
We have described a methodology for analyzing measures of retrieval performance based on the maximum entropy method, and we have demonstrated that the maximum en-tropy models corresponding to  X  X ood X  measures of overall performance such as average precision accurately reflect un-derlying retrieval performance (as measured by precision-recall curves) and can be used to accurately predict the val-ues of other measures of performance, well beyond the levels dictated by simple correlations.

The maximum entropy method can be used to analyze other measures of retrieval performance, and we are presently conducting such studies. More interestingly, the maximum entropy method could perhaps be used to help develop and gain insight into potential new measures of retrieval perfor-mance. Finally, the predictive quality of maximum entropy models corresponding to average precision suggest that if one were to estimate some measure of performance using an incomplete judgment set, that measure should be average precision X  X rom the maximum entropy model correspond-ing to that measure alone, one could accurately infer other measures of performance.

Note that the concept of a  X  X ood X  measure depends on the purpose of evaluation. In this paper, we evaluate mea-sures based on how much information they provide about the overall performance of a system (a system-oriented eval-uation). However, in different contexts, different measures may be more valuable and useful, such as precision-at-cutoff 10 in web search (a user-oriented evaluation). R-precision and average precision are system-oriented measures, whereas precision-at-cutoff k is typically a user-oriented measure. Another important conclusion of our work is that one can ac-curately infer user-oriented measures from system-oriented measures, but the opposite is not true.

Apart from evaluating the information captured by a sin-gle measure, we could use the MEM to evaluate the informa-tion contained in combinations of measures. How much does knowing the value of precision-at-cutoff 10 increase one X  X  knowledge of a system X  X  performance beyond simply know-ing the system X  X  average precision? Which is more infor-mative: knowing R-precision and precision-at-cutoff 30, or knowing average precision and precision-at-cutoff 100? Such questions can be answered, in principle, using the MEM. Adding the values of one or more measures simply adds one or more constraints to the maximum entropy model, and one can then assess the informativeness of the combination. Note that TREC reports many different measures. Using the MEM, one might reasonably be able to conclude which are the most informative combinations of measures. [1] A. L. Berger, V. D. Pietra, and S. D. Pietra. A [2] C. Buckley and E. Voorhees. Evaluating evaluation [3] W. S. Cooper. On selecting a measure of retrieval [4] T. M. Cover and J. Thomas. Elements of Information [5] B. Dervin and M. S. Nilan. Information needs and use. [6] W. R. Greiff and J. Ponte. The maximum entropy [7] E. Jaynes. On the rationale of maximum entropy [8] E. T. Jaynes. Information theory and statistical [9] E. T. Jaynes. Information theory and statistical [10] Y. Kagolovsky and J. R. Moehr. Current status of the [11] P. B. Kantor and J. Lee. The maximum entropy [12] D. D. Lewis. Evaluating and optimizing autonomous [13] R. M. Losee. When information retrieval measures [14] K. Nigam, J. Lafferty, and A. McCallum. Using [15] D. Pavlov, A. Popescul, D. M. Pennock, and L. H. [16] S. J. Phillips, M. Dudik, and R. E. Schapire. A [17] V. Raghavan, P. Bollmann, and G. S. Jung. A critical [18] A. Ratnaparkhi and M. P. Marcus. Maximum entropy [19] T. Saracevic. Evaluation of evaluation in information [20] C. E. Shannon. A mathematical theory of [21] N. Wu. The Maximum Entropy Method . Springer, New
