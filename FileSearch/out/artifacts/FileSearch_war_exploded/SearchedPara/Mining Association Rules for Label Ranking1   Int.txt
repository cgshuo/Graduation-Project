 Label ranking is an increasingly popular topic in the machine learning litera-ture [12,7,25]. Label ranking studies the problem of learning a mapping from instances to rankings over a finite numb er of predefined labels. It can be con-sidered as a variant of the conventional cl assification problem [7]. In contrast to a classification setting, where the obj ective is to assign examples to a specific class, in label ranking we are interested i n assigning a complete preference order of the labels to every example.

There are two main approaches to the problem of label ranking. Decomposition methods decompose the problem into several simpler problems (e.g., multiple binary problems). Direct methods adapt existing algorithms or develop new ones to treat the rankings as target objects without any transformation. An example of the former is the ranking by pairwise comparisons [12]. Examples of algorithms that were adapted to deal with rankings as the target objects include decision trees [24,7], k -Nearest Neighbor [5,7] and the linear utility transformation [13,9]. This second group of algorithms can be divided into two approaches. The first one contains methods (e.g., [7]) that are based on statistical distributions of rankings, such as Mallows [17]. The other group of methods are based on measures of similarity or correlation between rankings (e.g., [24,2]).

In this paper, we propose an adaptation of association rules mining for label ranking based on similarity measures. Association rules mining is a very impor-tant and successful task in data mining. Although its original purpose was only descriptive, several adaptations have been proposed for predictive problems.
The paper is organized as follows: sections 2 and 3 introduce the label ranking problem and the task of association rule mining, respectively; section 4 describes the measures proposed here; section 5 pr esents the experimental setup and dis-cusses the results; finally, section 6 concludes this paper. The formalization of the label ranking problem given here follows the one pro-videdin[7]. 1 In classification, given an instance x from the instance space X ,the goal is to predict the label (or class)  X  to which x belongs, from a pre-defined set L = {  X  1 ,..., X  k } . In label ranking the goal is to predict the ranking of the labels in L that are associated with x . We assume that the ranking is a total order over L defined on the permutation space  X  . A total order can be seen as a us also denote  X   X  1 as the result of inverting the order in  X  . As in classification, we do not assume the existence of a deterministic X  X   X  mapping. Instead, every instance is associated with a probability distribution over  X  . This means that, for each x  X  X , there exists a probability distribution P (  X | x ) such that, for every  X   X   X  , P (  X  | x ) is the probability that  X  is the ranking associated with x . The goal in label ranking is to learn the mapping X  X   X  . The training data is a set of instances T = { &lt;x i , X  i &gt; } ,i =1 ,...,n ,where x i are the independent variables describing instance i and  X  i is the corresponding target ranking.
As an example, given a scenario where we have financial analysts making predictions about the evolution of volatile markets, it would be advantageous to be able to predict which analysts are more profitable in a certain market context [2]. Moreover, if we could have bef orehand the full ordered list of the best analysts, this would certainly increase the chances of making good investments.
Given the ranking  X   X  predicted by a label ranking model for an instance x , which is, in fact, associated with the true label ranking  X  , we need to evaluate the accuracy of the prediction. For that, we need a loss function on  X  .Onesuch function is the number of discordant label pairs, which, if normalized into the interval [  X  1 , 1], is equivalent to Kendall X  X   X  coeffi-cient. The latter is as a correlation measure where D (  X ,  X  )=1and D (  X ,  X   X  1 )=  X  1. We obtain a loss function by averaging this function over a set of exam-ples. We will use it as evaluation measure in this paper, as it has been used in recent studies [7]. However, other dista nce measures could have been used, like Spearman X  X  rank correlation coefficient [22]. An association rule (AR) is an implication: A  X  C where A C =  X  , A, C  X  desc ( X )where desc ( X ) is the set of descriptors of instances in X , typically pairs attribute, value .Wealsodenote desc ( x i ) as the set of descriptors of instance x i .

Association rules are typically characterized by two measures, support and confidence. The support of rule A  X  C in T is sup if sup %ofthecasesinit contain A and C . Additionally, it has a confidence conf in T if conf %ofcases in T that contain A also contain C .

The original method for induction of AR is the APRIORI algorithm that was proposed in 1994 [1]. APRIORI identifies all AR that have a support and confi-dence higher than a given minimal support threshold ( minsup ) and a minimal confidence threshold ( minconf ), respectively. Thus, the model generated is a set of AR of the form A  X  C ,where A, C  X  desc ( X ), and sup ( A  X  C )  X  minsup and conf ( A  X  C )  X  minconf . For a more detailed description see [1].
Despite the usefulness and simplicity of APRIORI, it runs a time consuming candidate generation process and needs space and memory that is proportional to the number of possible combinations in the database. Additionally it needs multiple scans of the database and typically generates a very large number of rules. Because of this, many new pruning methods were proposed in order to avoid that. Such as the hashing [19], dynamic itemset counting [6], parallel and distributed mining [20], relational database systems integrated with mining [23].
Association rules were originally proposed for descriptive purposes. However, they have been adapted for predictive tasks such as classification (e.g., [18]). Given that label ranking is a predictive task, we describe some useful notation from an adaptation of AR for classification in Section 3.2. 3.1 Pruning AR algorithms typically generate a large number of rules (possibly tens of thou-sands), some of which represent only small variations from others. This is known as the rule explosion problem [4]. It is due to the fact that the algorithm might find rules for which the confidence can be marginally improved by adding further conditions to the antecedent.

Pruning methods are usually employed to reduce the amount of rules, without reducing the quality of the model. A common pruning method is based on the improvement that a refined rule yields in comparison to the original one [4]. The improvement of a rule is defined as the smallest difference between the confidence of a rule and the confidence of all sub-rules sharing the same consequent. More formally, for a rule A  X  C
As an example, if one defines minImp =0 . 1%, the rule A 1  X  C will be kept, if, and only if conf ( A 1  X  C )  X  conf ( A  X  C )  X  0 . 001, where A  X  A 1 . 3.2 Class Association Rules Classification Association Rules (CAR), were proposed as part of the Classifica-tion Based on AR (CBA) algorithm [18]. A class association rule (CAR) is an implication of the form: A  X   X  where A  X  desc ( X ), and  X   X  X  ,whichisthe class label. A rule A  X   X  holds in T with confidence conf if conf %ofcasesin T that contain A are labeled with class  X  , and with support sup in T if sup % of the cases in it contain A and are labeled with class  X  .

CBA takes a tabular data set T = { x i , X  i } ,where x i is a set of items and  X  i the corresponding class, and look for all frequent ruleitems of the form A,  X  , where A is a set of items and  X   X  X  . The algorithm aims to choose a set of high accuracy rules R  X  to match T . R  X  matches an instance &lt;x i , X  i &gt;  X  T if there is at least one rule A  X   X   X  X   X  ,with A  X  desc ( x i ) ,x i  X  X ,and  X   X  X  .Iftherules cannot classify all examples, a default class is given to them (e.g., the majority class in the training data). We define a Label Ranking Association Rule (LRAR) as a straightforward adap-tation of class association rules (CAR): where A  X  desc ( X )and  X   X   X  . The only difference is that the label  X   X  X  is replaced by the ranking of the labels,  X   X   X  . Similar to what the prediction made in CBA, when an example matches the rule A  X   X  , the predicted ranking is  X  . In this regard, we can use the same basic principle of the ruleitem for CARs in LRARs, which is A,  X  where A is a set of items and  X   X   X  .

This approach has two important problems. First, the number of classes can be extremely large, up to a maximum of k !, where k is the size of the set of labels, L . This means that the amount of data required to learn a reasonable mapping X  X   X  is too big.

The second disadvantage is that this approach does not take into account the differences in nature between label ran kings and classes. In classification, two examples either have the same class or not. In this regard, label ranking is more similar to regression than to classi fication. This property can be used in the induction of prediction models. In regression, a large number of observations with a given target value, say 5.3, increases the probability of observing similar values, say 5.4 or 5.2, but not so much for very different values, say -3.1 or 100.2. A similar reasoning can be made in label ranking. Let us consider the case of a data set in which ranking  X  a = { A, B, C, D, E } occurs in 1% of the examples. Treating rankings as classes would mean that P (  X  a )=0 . 01. Let us further consider that the rankings  X  b = { A, B, C, E, D } , X  c = { B, A, C, D, E } and  X  d = { A, C, B, D, E } occur in 50% of the examples. Taking into account the stochastic nature of these rankings [7], P (  X  a )=0 . 01 seems to underestimate the probability of observing  X  a . In other words it is expected that the observation of  X  ,  X  c and  X  d increases the probability of observing  X  a and vice-versa, because they are similar to each other.

This affects even rankings which are not observed in the available data. For example, even though  X  e = { A, B, D, C, E } is not present in the data set it would not be entirely unexpected to see it in future data. 4.1 Similarity-Based Support and Confidence To take this characteristic into account, we can argue that the support of a rank-ing  X  increases with the observation of similar rankings and that the variation is proportional to the similarity. Given a measure of similarity between rankings s (  X  a , X  b ), we can adapt the concept of support of the rule A  X   X  as follows:
Essentially, what we are doing is assigning a weight to each target ranking in the training,  X  i , data that represents its contribution to the probability that  X  may be observed. Some instances x i  X  X give full contribution to the support count (i.e., 1), while others may give partial or even a null contribution.
Any function that measures the similarity between two rankings or permuta-tions can be used, such as Kendall X  X   X  [16] or Spearman X  X   X  [22]. The function used here is of the form: where s is a similarity function. This general form assumes that below a given threshold,  X  sup , is not useful to discriminate between different similarity values, as they are so different from  X  a . This means that, the support sup of A,  X  a will have contributions from all the ruleitems of the form A,  X  b , for all  X  b where s (  X  a , X  b ) &gt; X  sup ). Again, many functions can be used as s .
The confidence of a rule A  X   X  is obtained simply by replacing the measure of support with the new one.
Given that the loss function that we aim to minimize is known beforehand, it makes sense to use it to measure the similarity between rankings. Therefore, we use Kendall X  X   X  . In this case, we think that  X  sup = 0 would be a reasonable value, given that it separates the negative from the positive contributions. Table 1 shows an example of a label ranking dataset represented following this approach. Algorithm 1. APRIORI-LR -APRIORI for Label Ranking
To present a more clear int erpretation, the example given in table 1, the in-stance ( { A 1= L, A 2= XL,A 3= S } ) (TID=1) contributes to the support count of the ruleitem { A 1= L, A 2= XL,A 3= S } , X  3 with 1. The same instance, will also give a small contribution of 0.33 to the support count of the ruleitem {
A 1= L, A 2= XL,A 3= S } , X  1 , given their similarity. On the other hand, no contribution to the count of the ruleitem X  X  { A 1= L, A 2= XL,A 3= S } , X  2 support is given, which are clearly different. 4.2 APRIORI-LR Algorithm Using the definitions of support and confidence proposed, adaptation of any AR learning algorithm for label ranking is simple. However, for illustration purposes, we will present an adaptation of the APRIORI algorithm, called APRIORI-LR. Given a training set T = { &lt;x i , X  i &gt; } ,i =1 ,...,n , frequent ruleitems are generated with Algorithm 1 and transformed in LRARs.

Let R  X  be the set of all the generated label ranking association rules .The algorithm aims to create a set of high accuracy rules r  X   X  X   X  to cover T .The classifier has the following format:
However, if these are insufficient to rank the given examples, a def ault ranking is used. The default ranking can be the average ranking [5], which is often used for this purpose.

This approach has two problems. The first is that it can only predict rankings which were present in the training set (except when no rules apply and the predicted ranking is the default ranking). The second problem is that it solves conflicts between rankings without takin g into account the  X  X ontinuous X  nature of rankings, which was illustrated earlier. The problem of generating a single permutation from a set of conflicting rankings has been studied in the context of consensus rankings .

It has been shown in [15] that a ranking obtained by ordering the average ranks of the labels across all rankings minimizes the euclidean distance to all those rankings. In other words, it maximizes the similarity according to Spearman X  X   X  [22]. Given m rankings  X  i ( i =1 ,...,m ) we aggregate them by computing for each item j ( j =1 ,...,k ): The predicted ranking  X   X  is obtained by ranking the items according to the value of r j .

We can take advantage of this in the ranker builder in the following way: the final predicted label ranking is the consensus of all the label rankings in the consequent of the rules r  X  triggered by the test example.

To implement pruning based on improvement for LR, some adaptation is re-quired as well. Given that the relation between target values is different from classification, as discussed in Section 4.1, we have to limit the comparison be-tween rules with different consequents, if the similarity function S (  X ,  X  )  X   X  imp . 4.3 Parameter Tuning Due to the intrinsic nature of each different dataset, or even of the pre-processing methods used to prepare the data (e.g., the discretization method), the maximum minsup/minconf needed to obtain a rule set R  X  that matches all or at least most of the examples, may vary significantly. We used a greedy method to define the minimum confidence. As stated earlier, a rule set R  X  matches an example if at least one rule ( A  X   X  )  X  X   X  ,with A  X  desc ( x i ) ,x i  X  X . Then, our goal is to obtain a rule set R  X  that maximizes the number of examples that are matched, here defined as M . Additionally, we want the best rules, the rules with the highest confidence values.

The parameter tuning method (Algorithm 2) determines the minconf that obtains the rule set according to those cr iteria. To set the step value we consider that, on one hand, a suitable minconf must be found as soon as possible. On the other hand, this very same value should be as high as possible. Therefore, 5% seems a reasonable step value.

Theidealvalueforthe minsup , is as close to 1% as possible. However, in some datasets, namely those with a larger number of attributes, frequent ruleitem Algorithm 2. Parameter tuning Algorithm generation can be a very time consuming task. In this case, minsup must be set to a value larger than 1%. In this work, one such example is authorship ,which has 70 attributes.

This procedure has the important advantage that it does not take into account the accuracy of the rule set s generated, thus reducing the risk of over-fitting. The data sets in this work were taken from KEBI Data Repository in the Philipps University of Marburg [7] (Table 2). Continuous variables were discretized with two distinct methods: (1) recursive minimum entropy partitioning criterion ([11]) with the minimum description length (MDL) as stopping rule, motivated by [10] and (2) equal width bins.

The evaluation measure is Kendall X  X   X  and the performance of the method was estimated using ten-fold cross-validat ion. The performance of APRIORI-LR is compared with a baseline method, the default ranking (explained earlier) and RPC [14]. For the generation of frequent ruleitems we used CAREN [3]. The base learner used in RPC is the Logistic Regression Algorithm, with the default configurations of the function Logit from the Stats package of R Programing Language [21].

Additionally, we compare the performance of our algorithm with the re-sults obtained with constraint classification (CC), instance-based label ranking (IBLR) and ranking trees (LRT), that were presented in [7]. We note that we did not run experiments with these methods and simply compared our results with the published results of the other methods. Thus, they were probably obtained with different partitions of the data and can not be compared directly. However, they provide some indication of the quality of our method, when compared to the state-of-the-art.

The value  X  imp was set to 0 in all experiments. This option may not be as intuitive as it is in  X  sup . However, since the focus of this work is the reduction of the number of generated rules, this value is suitable. 5.1 Results Table 3 shows that the method obtains results with both discretization methods that are clearly better than the ones obtained by the baseline method. This means that the APRIORI-LR is identifying valid patterns that can predict label rankings.

Table 4 presents the results obtained with pruned rules using the same minsup and minconf values as in the previous experiments and compares it to RPC using as a base learner Logistic Regression . Rd represents the percentage of the number of rules reduced by pruning. The results presented clearly show that the minImp constraint, set to 0 . 00 and 0 . 01, succeeded to reduce the number of rules. However, there was no improvemen t in accuracy, although it also did not decrease. Further tests are required to understand how this parameter affects the accuracy of the models.

Finally, table 5 compares APRIORI-LR with state of the art methods based on published results [7]. Given that the methods were not compared under the same conditions, this simply gives us a rough idea of the quality of the method proposed here. It indicates that, despite the simplicity of the adapta-tion, APRIORI-LR is a competitive met hod. We expect that the results can be significantly improved, for instance, by implementing more complex pruning methods.
 In his paper we present a simple adaptation of an association rules algorithm for label ranking. This adaptation essentially consists of 1) ensuring that rules have label rankings in their consequent, 2) using variations of the support and confidence measures that are suitable for label ranking and 3) generating the model with parameters selected by a simple greedy algorithm.

These results clearly show that this is a viable label ranking method. It out-performs a simple baseline and competes well with RPC, which means that, despite its simplicity, it is inducing useful patterns.

Additionally, the results obtained indicate that the choice of the discretization method and the number of bins per attribute play an important role in the accuracy of the models. The tests indica te that the supervised discretization method ( minimum entropy ), gives better results than equal width partitioning. This is, however, not the main focus of this work.

Improvement-based pruning was successfully implemented and reduced the number of rules in a substantial number. This plays an important role in gener-ating models with higher interpretability.
The new framework proposed in this work, based on distance functions, is consistent with the classical concepts underlying association rules. Furthermore, although it was developed in the context of the label ranking task, it can also be adapted for other tasks such as regression and classification. In fact, Classifica-tion Association Rules can be regarde d as a special case of distance-based AR, where the distance function is 0-1 loss.

This work uncovered several possibilities that could be better studied in or-der to improve the algorithm X  X  performance. They include: improving the pre-diction generation method; implementing better pruning methods; developing a discretization method that is suitable for label ranking; and the choice of pa-rameters.

For evaluation, we have used a measure that is typically used in label rank-ing. However, it is important to give more importance to higher ranks than to lower ones which can be done, for instance, with the weighted rank correlation coefficient [8].
 Additionally, it is essential to test the methods on real label ranking problems. The KEBI datasets are adapted from UCI classification problems. We plan to test our methods on other problems including algorithm selection and predicting the rankings of financial analysts [2]. In terms of real world applications, these can be adapted to rank analysts, based on their past performance and also radios, based on user X  X  preferences.
 This work was partially supported by project Rank! (PTDC/EIA/81178/2006) from FCT and Palco AdI project Palco3.0 financed by QREN and Fundo Eu-ropeu de Desenvolvimento Regional (FEDER). We thank the anonymous referees for useful comments.

