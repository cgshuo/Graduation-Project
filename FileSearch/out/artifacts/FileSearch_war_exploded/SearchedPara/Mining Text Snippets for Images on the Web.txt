  X  Images are often used to convey many different concepts or illustrate many different stories. We propose an algorithm to mine multiple diverse, relevant, and interesting text snippets for images on the web. Our algorithm scales to all images on the web. For each image, all webpages that contain it are considered. The top-K text snippet selection problem is posed as combinatorial subset selection with the goal of choosing an optimal set of snippets that maximizes a com-bination of relevancy, interestingness, and diversity. The relevancy and interestingness are scored by machine learned models. Our algorithm is run at scale on the entire image index of a major search engine resulting in the construc-tion of a database of images with their corresponding text snippets. We validate the quality of the database through a large-scale comparative study. We showcase the utility of the database through two web-scale applications: (a) augmen-tation of images on the web as webpages are browsed and (b) an image browsing experience (similar in spirit to web browsing) that is enabled by interconnecting semantically related images (which may not be visually related) through shared concepts in their corresponding text snippets. H.2.8g [ Information Systems ]: Database Management: Database Applications: Image Databases Data Mining Algorithms, Applications Text mining for images; Text snippets; Interestingness; Rel-evance; Diversity; Browsing; Semantic image browsing; Web image augmentation  X 
Research conducted while an intern at Microsoft.  X  Research conducted while an intern at Microsoft.

The popular adage  X  X  picture is worth a thousand words X  reflects the effectiveness of an image in conveying many dif-ferent ideas or  X  X tories. X  An image is typically used to illus-trate one of these stories in any given webpage. However, the same image can be used in different webpages to illustrate different stories at different levels of descriptiveness and in-terestingness. By collecting the stories associated with an image together, we can create a host of new applications that seamlessly integrate the image and text modalities.
The focus of our paper is exactly this: We propose a min-ing algorithm to obtain the most relevant and interesting text snippets for images on the web. Using this database of images and their associated text snippets, we present two new applications that we have implemented at web scale. We believe, however, that these applications represent the  X  X ip of the iceberg X : many more applications are possible.
Figure 1 shows a sample of results for a few images. The images are selected to cover many of the important types of images on the web; people, travel, music, etc. For each image, we show two of the many snippets identified for it. The snippets are related to the image in the most general sense: memories or events behind the image, an analogy to establish context, or even descriptions of a historical event in the context of the image. Note, however, that the text generally goes far beyond a simple description of the visual contents of the image. For example, a visual description of Figure 1(b) might be  X  X omeone snorkeling in the Maldives. X  Instead, our mined snippets include interesting information about the monsoon season and the geography of the islands, information that is not apparent in the image itself.
Our first contribution is to propose a scalable solution to mine the web for multiple relevant, interesting and diverse textual snippets about the image. The scale of our solution has two dimensions: we consider all images on the web and for each image we consider all web pages that contain the corresponding image. This scale enables reliable identifica-tion of multiple high quality snippets for images that are discussed widely on the web. Note that our approach differs considerably from prior work in understanding images [ 7, 13 , 12 , 16 , 20 , 22 , 28], where the goal is to synthesize textual de-scriptions of the visual content of the image by recognizing objects, their attributes, and possibly using similar images in image databases to borrow captions.

With the web as a repository of world knowledge, we op-erationalize our solution based on a key insight: if an image is interesting, multiple people will post it in webpages, in-cluding stories related to it. These stories will vary in their content bringing about diversity. Note that these stories are generally not contained in the image captions (which are most often just descriptive), but the captions can help iden-tify the most interesting stories. Concretely, we mine the web by first clustering all the images in the index of a Web search engine into  X  X ear duplicate X  groups. For each group, we identify the top K snippets that are the most diverse, interesting and relevant, where the interestingness and rele-vancy are scored using learned models. The snippet selection problem is posed as the combinatorial optimization problem of subset selection. Since each image set can be processed independently, our algorithm easily parallelizes. We have run our algorithm on the entire web image index of a search engine containing billions of images.

Our second contribution is to demonstrate how the re-sulting database of images along with their snippets can be used to construct two new applications. Both applications are bulit at web scale(  X  5). The first application (Figure 8 and  X  5.1 ) is a plugin to an internet browser than enables an enhanced reading experience on the web by displaying text snippets on top of images as the user passively browses the web. The second application (Figure 9 and  X  5.2 ) is a real time web image browser that provides a seamless browsing experience through the most interesting images on the web, and their associated stories. Images are inter-connected se-mantically through the text snippets, unlike approaches that rely on visual similarity [10 , 24 ].
There are two main bodies of related work. The first set focuses on analysing an image to generate a text description or caption. The second body of work is that of document (i.e. webpage) summarization. We discuss each in turn.
Image caption generation: Possibly the earliest work in this area is [20 ], which focused on associating word tags with an image by analyzing image regions at multiple gran-ularity. Subsequently, in [ 12 , 16 , 28] the semantics of the image pixels are inferred to generate image descriptions. In particular, [ 12 ] detects objects, attributes and prepositional relationships in the image. These detections are composed to generate descriptions of the image. In [ 28 ], the image is decomposed at multiple granularity to infer the objects and attributes. The outputs of this step are then converted into a semantic ontology based representation that feeds into a text generation engine. A related line of work poses caption generation as a matching problem between the input image and a large database of images with captions [7 , 13 , 22 ]. In [7], a learned intermediate semantic space between image features and the tokens in the captions are used to generate sentences for images. In comparison, [ 22] constructs a data set of 1 million images spanning over 87 classes. Given an in-put image, this approach transfers captions from the closest matching image in the data set. In [13 ], phrases extracted from the caption of the matched image are used to generate a descriptive sentence. The goal of all these techniques is to describe the image content while we would like to capture the stories conveyed in the context of the image, and is not restricted by what is contained in the image.
Document Summarization: There is a large body of work in automatic text summarization. See [15, 23 ] for sur-veys. In this literature, the goal is to summarize documents, either by identifying key phrases and sentences that are re-flective of the focus of the document (extractive approaches) [9 , 4, 5] or by paraphrasing the content (abstractive ap-proaches) [3 ]. In [ 9], sentences are extracted from one or more articles about the same topic. In [ 4] key sentences from webpages are extracted for viewing on a mobile device. In another example, [ 3] summarizes a webpage using a prob-abilistic model of the gist of the webpage. The underlying goal of all these methods is to identify the central focus of the document to be summarized. In contrast, we would like to identify content related to the image. One can think of using these techniques if we know the region of text that corresponds to image, which is a hard task by itself [2 ].
Our algorithm is based on the insight that if an image is interesting, multiple people will embed it in a webpage and write about it. For each image (along with near-duplicates), one can mine the web for all the webpages containing it in order to identify text snippets that are relevant and inter-esting and also form a diverse set of text.

With this insight, we have developed a highly scalable solution to mine the web for the interesting text snippets related to an image. Figure 2 provides the overview. The web images are clustered into  X  X ear duplicate X  groups (  X  3.1 ), referred to as  X  X uplicate image set X  or  X  X mage set, X  for short. While each image set corresponds to a single image, these images reside in multiple webpages, PURL (page URL in which the image resides) with a unique media url ( MURL ). Hence, each image set is represented by a collection of triplets { MURL , PURL , HTML } where HTML is the associated HTML source of PURL . Using this representation, we iden-tify candidate snippets for each image set (  X  3.2 ). Then, the top K snippets that are most diverse, interesting and rele-vant to the image (  X  3.3 ) are identified by posing the problem as the combinatorial optimization problem of subset selec-tion. We use machine-learned models of relevance and inter-estingness to score the snippets which are used within the optimization framework (  X  3.3.1 ).
We would like to cluster images such that each cluster con-sists of images that are near duplicate to each other. Im-ages within a near duplicate image set can differ in their sizes, compression ratios, cropping, color transformations and other minor edits. For this purpose, we require a clus-tering algorithm that covers (a) large variation within a du-plicate image cluster while minimizing false positives and (b) is highly scalable for clustering billions of images on the web.

To satisfy both these requirements, we use two-step clus-tering method [26 ] that is scalably implemented using hash-ing techniques within MapReduce framework [ 6]. The tech-nique combines global and local features, with global de-scriptors to discover seed clusters with high precision and the local descriptors to grow the seeds to obtain good recall.
For the global descriptors, we first partition each image into 8  X  8 blocks and compute the mean gray value for each block. The image is then divided further into 2  X  2 blocks, and a histogram of edge distributions in evenly-divided 12 Figure 2: Overview of Snippet Mining Algorithm: We first cluster all the images on the web into  X  X ear duplicate X  groups. For each set of images we extract a set of candidate snippets. We then identify the top K snippets that are relevant, interesting and diverse. directions is extracted from each block, plus one dimension of non-edge pixel ratio. The histograms of mean gray values and edge distributions are cascaded into a 116-dimensional raw feature vector. A pre-learnt Principle Component Anal-ysis (PCA) model is then applied and the first 24 dimensions of the PCA-ed vector are retained. The local descriptor of an image is a truncated set of visual words defined on a vi-sual codebook of 1 miilion codewords. The local features proposed in [ 27 ] are first extracted and then ranked by their average Difference-of-Gaussian (DoG) responses. The top-ranked 40 features are used as the local descriptors of an image. Two images are near duplicates if they have 6 visual words in common.

Seed clusters are generated in three steps. First, each im-age is encoded into a 24-bit hash signature by quantizing each dimension of the global descriptor against the PCA mean. These signatures then serve as keys to divide the images in a MapReduce framework (i.e. the Map step). Sec-ond, images with identical hash signatures are assigned to the same hash buckets. Each bucket is processed within one vertex of the MapReduce machine cluster (i.e. the Re-duce step). Third, pair-wise distance matching is performed within a bucket.

We grow the seed clusters by merging them using local descriptors. For computational tractability, we assume that near duplicate images reside in neighboring buckets in the global feature space, where neighboring buckets are those whose corresponding signatures are different in at most two bits. We randomly select one image from each seed clus-ter and add to the cluster all the images from neighboring buckets that have 6 or more visual words in common. Each duplicate image set is represented by triplets { MURL , PURL , HTML } of the image url, page url and the HTML source of the page 1 . Along with this, we parse HTML to obtain a linear ordering of the text and image nodes and this is maintained as W PURL .

Corresponding to each text node 2 in W PURL , we generate a candidate snippet &lt; s n, PURL ,l n, PURL &gt; , where s the n, PURL th snippet text and l n, PURL is its location in W
PURL . The set of candidate snippets generated is
We restrict triplets to be from English-language non-spam websites. We also exclude social networking and photo shar-ing sites that do not contain enough textual content.
Non-English and/or ill-formed sentences are ignored of snippets.

Similarly, we represent an image as follows: For each im-age node corresponding to the MURL , we extract its as-sociated  X  X lt X  text or  X  X rc X  text. We denote this text &lt; m the location of the image node in W PURL .
For each image, I , let S be the candidate set of snippets identified from the set of triples { MURL , PURL , HTML } (  X  3.2 ). From this large candidate set, we would like to select K top snippets, T  X  S which are not only relevant and interesting to I but also exhibit diversity . This selection can be guided by the objective function that trades-off total gain given by the sum of the product of relevance rel ( s ) and interestingness int ( s ) of each snippet with their diversity as measured by the entropy of the set H ( T ) (larger the entropy, larger the diversity).

In our objective (eq. 1), we consider the product rel ( s ) int ( s ) instead of sum rel ( s )+ int ( s ) because we prefer snippets that are both simultaneously relevant and interesting. We also differentiate between relevance and interestingness and not use a single term since a highly relevant snippet may not be very interesting, and vice versa (See Figure 4).
 The optimal subset satisfies: While, one can solve for the optimal subset through exhaus-tive search over all possible subsets of S , this is clearly pro-hibitive even for a reasonable size of S .

However, for suitable choices of rel ( s ) int ( s ) and H Eq. 1 is submodular. That is, it exhibits the property of  X  X iminishing returns X  so that the difference in the value of the function that a single element makes when added to an input set decreases as the size of the input set increases. Mathematically, if X  X  Y  X  S , then adding an element z  X  X  to both X and Y should satisfy: As long as we choose rel ( s ) int ( s )  X  0 so that P s  X  X  is a monotonically increasing function, Eq. 1 will be submod-ular since entropy is submodular [ 11 ]. We describe one such choice for rel ( s ) int ( s ) in  X  3.3.1 .

The advantage of submodular functions is that while be-ing computationally difficult, we can design greedy solutions that are at most (1  X  1 e ) times worse than the optimal so-lution [21 ]. Using this theoretical guarantee, we design a greedy solution as follows: Initialize with a snippet that has the largest value of rel ( s ) int ( s ). Then, iteratively add snip-pets, one at a time, such that the added snippet (along with previously chosen set of snippets) maximizes Eq. 1.
The top-K snippet selection formulation (Eq. 1) uses sep-arate functions to score a snippet for relevancy rel ( s ) and interestingness int ( s ) to the corresponding image. Here, we describe the instantiation that we use in this paper for these two components. We used separate machine learned models of regularized linear regression.

The regularized linear regressor captures the relationship between features,  X  ( x ), extracted from the snippet x and its scores through the functional form y = w T  X  ( x ). The de-tails of the features used are provided momentarily. Given some training data x i  X  X that has been annotated with the relevance (or interestingness) scores y i , the correspond-ing parameters w are learned by solving the optimization function: where  X  is a regularization parameter. The unique optimum of Eq. 4 can be found via gradient descent [14 ].
 First, we describe the vocabulary for representation: of the image representation and all the candidate snippets for the image set. Let V be the unigrams ( sans stop words) identified in M .

Each snippet s is represented using a |V| vector, s , such that s [ k ] is the number of times that the k th unigram in V occurs in s . Similarly, for image I , m is a |V| vector such that m [ k ] is the sum of the number of times that the k unigram in V occurs in the union of the representations of the images across all PURL ,  X  PURL h m PURL i .

The feature vector,  X  (  X  ), consists of nine features from the following five groups of features: Match Score: While s corresponds to a single snippet, m Context Scores: When the text around a snippet is rel-HTML Parse Distance: Good snippets tend to be closer Figure 3: HIT used to collect training data to learn the snippet scoring functions. We showed image-snippet pairs to the judges and asked them to rate the snippet based on its  X  X elevance X  to the image and on how  X  X nteresting X  it is. Some sentences are highly relevant, but not very interesting (bottom). Measure of SPAM: When a snippet contains a lot of re-Linguistic Features: The interestingness of a sentence of-To learn these functions, we construct a training set as fol-lows: We randomly chose 250 images. For each image, we assembled all the snippets that are within reasonable dis-tance to the MURL in the HTML parse tree (so that clearly irrelevant snippets are not considered). This resulted in 5443 image-snippet pairs. For each pair, we obtain human judg-ments independently for relevance and interestingness. Human judgments: We designed a Human Intelligence Task (HIT) on Amazon Mechanical Turk to label each pair of image and snippet. Figure 3 shows the HIT setup for two example pairs. Given this HIT, the judges were asked to in-dependently score the snippet for relevance and interesting-ness on a discrete scale between zero and three, where zero means not relevant (not interesting) and three corresponds to very relevant (very interesting). Each pair was judged by approximately 10 judges. The average relevance of the snip-pets is 1.57 (about half way between  X  X omewhat Relevant X  and  X  X elevant X ). The average interestingness score is 1.17 (a little bit higher than  X  X omewhat Interesting. X ). Figure 4: An example image with three snippets and their average human labels. (a) illustrates a snippet that is relevant but not very interesting. (b) is much more interesting. (c) is more interesting than rele-vant; it is about the author, not his character.

In Figure 4 we include an image to illustrate the human labeling. We include three snippets together with the hu-man scores. Snippet (a) is provided as an example of why it is important to assess the interestingness of the snippets. Snippet (a) is a fairly factual statement about the image. It is scored as relevant, but as not particularly interesting. In comparison, (b) is significantly more interesting. Snippet (c) is notable in that it has a relatively low relevance (com-pared to the interestingness, which is generally scored more conservatively than relevance.) The sentence is about  X  X r. Seuss X  whereas the image is one of his characters.
The normalized entropy, H 0 ( T ) in Eq. 1 models the diver-sity of the chosen sentences. We would like to favor sentences that are diverse in two respects. First, they differ in their vocabulary usage. Second, they are derived from different webpages. Hence, the entropy H ( T ) of set T is computed over N +1 binary random variables, where N of them consti-tute the vocabulary used in snippets in T and the ( N + 1) variable models the webpage variability among T . The prob-ability of the n th variable to have a value of 1 is given by the fraction of times the value is observed in the set T . The normalized entropy is given by H 0 ( T ) = H ( T ) /log ( N + 1) which normalizes the entropy across different subsets. As we add snippets, the incremental gain in entropy will diminish due to the fact that the vocabulary is fixed.
We have run our snippet mining algorithm on the entire web image index of a major search engine with billions of images. We provide details of scaling up the image cluster-ing along with the details of the clustering in (  X  3.1 ). After the clustering step is performed, the algorithm is embarrass-ingly parallel and is performed independently for each im-age set. In fact, on a 100 core cluster, the entire processing took about 45 hours. We can also run the pipeline for delta updates to introduce clusters of new images or additional snippets for the same image set.
We now provide an experimental validation of our algo-rithm using a quantitative comparison with two reasonable baselines (  X  4.1) and quantitative coverage results on popu-lar web queries for image search retrieval (  X  4.2)
To the best of our knowledge, there is no prior work on extracting a set of text snippets for an image on the web. There are, however, two reasonable baselines: Data set: Web image search retrieval algorithm such as Qbl/WS are most tuned for head (popular) images and their landing pages. Therefore, images that are most popular on web image search will serve as better baselines. We take the top 10,000 textual queries to a popular search engine and randomly selected 50 images that are among the top ranking results for those queries as the test set.
The results for two images are included in Figure 5. We found that the Qbl/WS algorithm always found the correct image and generated plausible results. The results using purely visual features for comparison [22 ] are far less rele-vant. This algorithm never really found the correct topic of the image, due to a mismatch between their database (1 million Flickr images on narrow topics of interest) and the web (billions of images). We therefore dropped the Im2Text algorithm [ 22 ] from the subsequent quantitative comparison.
Mechanical Turk Setup: In order to perform a quan-titative evaluation of our approach with Qbl/WS, we con-ducted pairwise evaluations focused on the overall preference of one ranked list over other. We used Amazon Mechanical Turk with each Human Intelligence Test (HIT) consisting of a pair of ranked lists of snippets, corresponding to our proposed algorithm and Qbl/WS. To remove the presenta-tion bias, we considered both orderings of the lists. Each ordering is a separate HIT. We also had each HIT judged by five judges. Each judge was asked to study the two lists and specify which of the two they preferred. They also had the option of choosing  X  X oth are comparable X . The judges were asked to consider multiple dimensions while making their judgements: relevancy, interestingness, diversity and repet-itiveness of content. They were required to spend at least five minutes on the task (we discarded answers from judges that did not conform to this guideline).

Results: Figure 5(bottom) shows the key results. Of all the judgments, 72% exclusively preferred our approach,
We used the software available at http://vision.cs. stonybrook.edu/~vicente/sbucaptions/ Figure 5: Top and Middle: Qualitative results from a comparison between our algorithm and two baselines. The first baseline is a Query-by-Image/Webpage Summarization algorithm that finds the image on the web and then summarizes each webpage independently. The second baseline is the algorithm of Ordonez, Kulkarni, and Berg [ 22]. Bottom: A quantitative comparison between our al-gorithm and the Query-by-Image/Webpage Summa-rization algorithm. We took the results for 50 im-ages and asked judges which they preferred. No preference was also an option. Bottom Left: 72% of all votes preferred our results, compared to just 10% for the Query-by-Image/Webpage Summariza-tion algorithm. Bottom Right: When we tally up the votes for each image, we found that the judges preferred our results for all 100 images. Figure 6: Results on popular queries to an image search engine. We took the top 10,000 queries and considered the top 50 result images for each query. We plot the number of result images with at least K = 3, 4, 5 text snippets (X-axis) against the page-view weighted percentage of queries (Y-axis) that have at least that number of results images with K text snippets. The results show that most queries have 10-30 results images with snippets. 17.8% felt  X  X oth are comparable, X  while 10.2% preferred the QbI/WS algorithm. We also looked at the individual re-sults. For all images, a (strictly) greater number of judges preferred our approach to the QbI/WS algorithm.

The qualitative results in Figure 5 were chosen to give one example (top) where our results are unanimously preferred, and one example (middle) where a relatively high percentage of judges prefer the Query-by-Image/Webpage Summariza-tion results (20% compared to the average of 10.2%).
Although we ran our algorithm on the entire web index of a major search engine, we do not obtain results for all im-ages. We impose a threshold on the overall score rel ( s ) int ( s ) to remove poor quality results. We now present a set of ex-perimental results to illustrate how likely a snippet will be available for the most popular images; i.e. the ones that are returned as results for the most common queries to Bing image search engine.

Experiment Setup: We consider the top 10,000 most frequent queries issued to Bing image search engine. For each query, we considered the top 50 image results, cor-responding to the first page of image search results. We used our approach to identify the top K snippets. Some of these results may not have K snippets associated with them. Next, we compute a metric that captures this property.
Metric: We use the percentage of queries weighted by the frequency with which they were issued. Each query in the top 10,000 was issued by multiple users over the one month sampling interval. When computing the percentage of queries that meet a criterion, we weight by the number of times the query was issued. We refer to this metric (perhaps slightly confusingly) as  X  X age-view weighted. X 
Results: The results are plot in Figure 6. On the X-axis, we plot the number of result images that have a least K = 3 , 4 , 5 text snippets. On the Y-axis we plot the page-view weighted percentage of queries that have at least that Figure 7: Examples of common types of images for which our algorithm does not find enough high-quality text snippets. (a) Personal photos frequently do not have high quality text, but are often posted with a short caption. (b) We currently only process English language webpages, so foreign language im-ages do not result in any good snippets. (c) Com-mercial images are often repeated across multiple webpages with very similar text and are removed as having too much text duplication. (d) Generic icons can rarely be associated with high quality text. many (X-axis) results images with K text snippets. The results in Figure 6 show that for essentially all 4 queries, there are some results images with text snippets. Most queries have 10 X 30 results images with text snippets.

We qualitatively investigated the nature of the images for which no high quality text snippets were found. Example of the sort of images are shown in Figure 7. (a) Personal photos that are posted to blogs and photo-sharing cites rarely have high quality text associated with them, even if they are sub-sequently reposted multiple times. (b) As we currently only process English webpages, foreign content photos generally do not result in any snippets. (c) Photos from commercial websites often do not result in high quality snippets or are filtered out due to high repetition (often the same image is used on multiple websites with very similar text.) (d) Icons can rarely be associated with high quality text.
The database of text snippets mined by our algorithm has a variety of possible applications. The snippets could be shown to users along with image search results. They can be used to improve image search relevance. It may also be possible to use the snippets to discriminate more interesting images from less interesting ones. Images for which there are a lot of interesting snippets (see Figure 1) are generally far more interesting than images for which not much text is found (see Figure 7).

We now propose two other applications. The first consists of augmenting images on the web when a user views the page (  X  5.1 ). The second is an image browsing application built by connecting images through their snippets (  X  5.2 ).
We investigated the queries for which there are no results with text snippets. These were mainly cases where the top 50 results images are classified as adult content, but the query was not. We restricted our processing to non-adult images and so obtained no results in these cases. Figure 8: A screenshot of the web augmenta-tion application (  X  5.1). The plug-in is part of the Bing Bar toolbar at the top. The text results are populated over the image as text pop-ups that show up on mouse-over. Please see http://research.microsoft.com/en-us/um/people/ sbaker/i/www/augmentation.mp4 for a video.
 These applications both follow a client-server architecture. For both application, we built a Windows Azure [19 ] service that hosts the database of text snippets extracted using our algorithm. This service is used by the client to query for snippets corresponding to an image and/or for images per-taining to key words that appear in a text snippet.
Often when an image is included in a web page, only a brief caption is provided. Enriching these images with addi-tional interesting information can improve the web browsing experience for the user. If the information comes with as-sociated links to its source, the user can easily choose to explore the additional information by clicking on the links. To this end, we developed a  X  X ing Bar X  plug-in for Internet Explorer [ 18 ] that automatically identifies all the images on the current web page, and augments them with any snippets that were found by our algorithm.

Figure 8 shows a screenshot of the application. After the webpage has been rendered, the HTML is passed to the plug-in. The plug-in parses the HTML, extracts the URLs of the images, and queries the Windows Azure service to obtain the snippets for the images. The plug-in then injects Javascript into the HTML that renders the text snippets when the user hovers over the image and then clicks on one of the boxes that are displayed. The plug-in then passes the HTML back to the browser which re-renders the page. Figure 8 shows such an example snippet for an  X  X dele X  image. For a detailed demo, please see the video http://research.microsoft. com/en-us/um/people/sbaker/i/www/augmentation.mp4 .
There are several advantages of this system over other interfaces for viewing query-by-image results. First, the browsing experience is completely passive. The user does not need to initiate a query for every image. The query is initiated automatically, and only when there are interesting results, are they displayed in an un-obtrusive manner. Sec-ondly, this application adds virtual hyperlink on top of the Web, from images to related content on other web pages. This can enrich the browsing experience for the user.
A number of recent works have focused on building effec-tive visualizations of a collection of images that are retrieved in response to a query of interest [ 10 , 24 ]. In this section, we present a system for a never-ending browsing experience that revolves around images and the text snippets associ-ated with them (found using our algorithm). The starting point for the application is either an image or a concept of interest, which can be user-provided or chosen at random. The choice of the next image is based on the text snippets, thereby allowing transitions between images that can be vi-sually unrelated, but semantically related. In particular, we identify concepts in the snippets which are then used as the means for the user to control the choice of the next image in a semantically meaningful manner. See Figure 9.
Browse interface: The interface allows people to browse images on the web. As they browse, the system displays a set of text snippets extracted using our algorithm. These snip-pets provide interesting information about the current image being viewed. We also detect concepts phrases in the snip-pet that map to Wikipedia article titles [8 , 17 , 25 ], further refined using the techniques proposed in [1 ]. These concept phrases are used to hyperlink to other images that share the concept phrases in their snippets. Our algorithm for deter-mining the destination of the hyperlink is randomized; if the viewer returns to the same image twice, we want them to explore a different path the second time around. Using an inverted index, we locate K=10 top images at random that have a text snippet that includes the concept being clicked on. There are typically thousands of possible destinations. We then sort these images by the frequency with which the concept appears across all the snippets associated with the image, and choose the image with the highest frequency. We found this algorithm yielded a compelling experience. More sophisticated approaches are left as future work.

Figure 9 contains an screenshot of our application. The current image is a diagram explaining how neural signals are transmitted from an axon to a dendrite in the brain. The displayed text snippet explains the process. The his-tory of recent images is shown on the left side. We began with an image of Ernest Rutherford, then tapped on Isaac Newton, then Albert Einstein twice, once a photo of Al-bert and his wife, the second time just the famous physi-cist. We then tapped on brain and neuron before arriv-ing at the current image. The right side of the display includes a list of all the concepts for this subset of sci-ence images. The user can tap on any concept to initi-ate a browsing session at a different starting point. An il-lustration of the browsing session in Figure 9 is contained in the web video http://research.microsoft.com/en-us/ um/people/sbaker/i/www/browsing.mp4 .

Evaluation using Mechanical Turk: We evaluated the effectiveness in identifying the next image to browse, based on the semantics conveyed by the snippets. In particular, are two images that are visually unrelated, indeed related through the snippets that describe them? To answer this question, we make use of an evaluation set consisting of 500 pairs, &lt; x i ,y i &gt; , of images (along with the snippets that link x to y i ) constructed as follows: for each pair, we choose an image x i at random. Then, we choose a concept associated with x i (from the snippets of x i ) at random. We then choose the next image, y i using the algorithm described above to compute the destination of a link within our application. Figure 10: We validated our browsing application using Amazon Mechanical Turk. Left: We showed pairs of images that are connected with a transition in the application. The connection here is the word  X  X alcium. X  Half the time we showed the correspond-ing text snippets. The other half we just showed the images. Right: The results show that judges are able to see connections between images more often when they are also shown the text snippets.

We conduct two sets of experiments using Amazon Me-chanical Turk. In the first experiment, each Human In-telligence Task (HIT) consists of only the pair of images, &lt; x i ,y i &gt; . Each judge was asked to specify if they found the pair of images to be  X  X elated X ,  X  X ot related X  or  X  X ot sure X . In the second experiment (a different HIT), the judges re-peated the same experiment, but now, in addition to the pairs of images, they were also shown snippets linking the two images with the connecting concept highlighted.
Results: Figure 10 shows the average number of images that were judged in each of the categories. We can see a huge drop in fraction of  X  X nrelated X  images from 57% to 25% showing the efficacy of our approach in linking semantically related images that are seemingly unrelated. As examples, consider the pair of images shown in Figure 10 . In isolation, the connection between the two images is not obvious. With the addition of the text snippet, the viewer immediately finds both images more interesting and sees the  X  X emantic X  relationship between them.
We have presented a scalable mining algorithm to obtain a set of text snippets for images on the web. The optimal set of snippets is chosen to maximize a combination of rel-evance, interestingness, and diversity. The relevancy and interestingness are scored using learned models.

There are a number of applications of the resulting text snippets. One possibility is to display the snippets along with image search results. In this paper we proposed two others, a plugin to an internet browser that augments web-pages with snippets overlaid on images, and a tablet appli-cation that allows users to browse images on the web via hyperlinks embedded in the text snippets. The snippet data can be useful for improving image search relevance.
One suggestion for future work is to analyze the snippets in more detail, for example by clustering, to find groups of related images. The results could be used to broaden the set of snippets and concepts associated with an image, possibly leading to deeper understanding of the content of the images, and more interesting browsing experiences. Rizwan Ansary (Microsoft, r izwan@microsoft.com) Ashish Kapoor (Microsoft, a kapoor@microsoft.com) Qifa Ke (Microsoft, q ke@microsoft.com) Matt Uyttendaele (Microsoft, m attu@microsoft.com) Xin-Jing Wang (Microsoft, x ywang@microsoft.com) Lei Zhang (Microsoft, l eizhang@microsoft.com) [1] R. Agrawal, M. Christoforaki, S. Gollapudi, [2] R. Angheluta, R. De Busser, and M.-F. Moens. The [3] A. L. Berger and V. O. Mittal. Ocelot: a system for [4] O. Buyukkokten, H. Garcia-Molina, and A. Paepcke. [5] W. T. Chuang and J. Yang. Extracting sentence [6] J. Dean and S. Ghemawat. Mapreduce: Simplified [7] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, [8] E. Gabrilovich and S. Markovitch. Computing [9] J. Goldstein, V. Mittal, J. Carbonell, and [10] Y. Jing, H. A. Rowley, C. Rosenberg, J. Wang, [11] C.-W. Ko, J. Lee, and M. Queyranne. An exact [12] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, [13] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, [14] D. C. Liu and J. Nocedal. On the limited memory [15] I. Mani and M. T. Maybury. Advances in automatic [16] R. Mason and E. Charniak. Annotation of online [17] O. Medelyan, D. Milne, C. Legg, and I. Witten. [18] Microsoft. Internet Explorer. [19] Microsoft. Windows Azure Cloud Services. [20] Y. Mori, H. Takahashi, and R. Oka. Image-to-word [21] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An [22] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: [23] K. Sp  X  arck Jones. Automatic summarising: The state [24] G. Strong, E. Hoque, M. Gong, and O. Hoeber. [25] M. Strube and S. Ponzetto. WikiRelate! Computing [26] X.-J. Wang, L. Zhang, and C. Liu. Duplicate discovery [27] S. Winder and M. Brown. Learning local image [28] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu.
