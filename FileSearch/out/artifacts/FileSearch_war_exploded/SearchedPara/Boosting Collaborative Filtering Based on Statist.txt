 User-based collaborative filtering methods typically predict a user X  X  item ratings as a weighted average of the ratings given by similar users, where the weight is proportional to the user similarity. Therefore, the accuracy of user similar-ity is the key to the success of the recommendation, both for selecting neighborhoods and computing predictions. How-ever, the computed similarities between users are somewhat inaccurate due to data sparsity.

For a given user, the set of neighbors selected for pre-dicting ratings on different items typically exhibit overlap. Thus, error terms contributing to rating predictions will tend to be shared, leading to correlation of the prediction errors.

Through a set of case studies, we discovered that for a given user, the prediction errors on different items are cor-related to the similarities of the corresponding items, and to the degree to which they share common neighbors.

We propose a framework to improve prediction accuracy based on these statistical prediction errors. Two differ-ent strategies to estimate the prediction error on a desired item are proposed. Our experiments show that these ap-proaches improve the prediction accuracy of standard user-based methods significantly, and they outperform other state-of-the-art methods.  X  Another affiliation of this author is Graduate University of Chinese Academy of Sciences, Beijing, 100049, China.  X  This author conducted this work while he was an intern at IBM China Research Lab.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information Filtering Algorithms, Performance, Experimentation Recommender Systems, Collaborative Filtering, Prediction errors, Similarity
Collaborative filtering refers to a process for predicting item preferences based on the preferences of other users with similar behavior. As one of the most successful technologies for recommender systems, it has been widely developed and improved over the past decade [10, 2, 5, 11, 8, 4, 14].
A very common form of collaborative filtering, user-based collaborative filtering, employs two fundamental steps in predicting the rating value of a given item for a particular (active) user based on the ratings from other similar users: first, some users, known as neighbors , are selected based on their similarity to the active user, then, a weighted average of the neighbor X  X  ratings is used to predict the rating value, with the weights proportional to the similarity between each neighbor and the active user. Note that user similarity plays a key role in collaborative filtering approaches both for neighborhood selection and prediction computation.
Traditional measures of user similarity introduce errors due to incomplete sharing of sets of item ratings between users. If two users rate all the items in a given dataset, then we assume that the computed similarity between these two users will be precise. However, when only a subset of the items are co-rated, errors are introduced into the similarity computation. For a given data set, the prediction errors are randomly distributed. However, for a given active user, the neighbor selection process biases these errors. A set of items will tend to share common neighbors, and thus, have correlated errors. We explore these correlations and exploit ways in which they are systematic.
The main contribution of this paper is using known pre-diction errors for items that a user has rated to improve predictions on unrated items. We attempt to improve pre-dictions based on two observations: (1) Given a user, pre-diction errors for different items will tend to be correlated when the items are similar, (2) Given a user, prediction er-rors for different items will tend to be correlated when the neighbor sets used to generate the predictions share com-mon sets of users. Based on these, we compute statistical prediction errors on a training set, and use a combination of these errors to amend the predicted ratings on a testing set, thereby improving prediction accuracy.

The rest of the paper is organized as follows. The next section presents notation used in this paper, and provides a brief review of related work. In Section 3, we present case studies to expose the relationships between predictio n errors and item similarities, and between prediction error s and common neighbors. We then introduce our methods based on statistical prediction errors in Section 4. Empiri cal evaluations of our approaches and comparisons with other collaborative filtering approaches are presented in Sectio n 5. Finally, we present conclusions in Section 6.
Formally, a collaborative filtering algorithm deals with a set of users, S user , and a set of items, S item ; its goal consists of computing a score  X  r ux for each pair &lt;u , x&gt; , u  X  S S item based on a set of item preferences expressed by a set of users. In this paper, the user u is called the active user , and the item x on which the prediction is made is called the desired item . For N users and M items, the user preferences are represented in a N by M user-item matrix R . In R , the user u gives a preference r ux to the item x if the item has been rated, where r ux  X  X  1 , . . . , r } where r is the maximum valid rating. Otherwise,  X  is used to identify an unknown rating. The mean of ratings assigned by user u is denoted by  X  r u . All other parameters derived from the data are iden-tified with a hat notation, for example,  X  r ux is the predicted value for r ux . Given the user-item rating matrix R , collab-orative filtering algorithms automatically make predictio ns for r ux =  X  .
Collaborative filtering algorithms are based on the as-sumption that similar users prefer similar items, or that a user expresses similar preferences for similar items. They fall into two general classes: model-based and memory-based [1, 2]. Model-based algorithms learn a probabilistic model , such as a cluster model or a Bayesian network model [2, 17], from the underlying data using statistical and machine learning techniques, then use the model to make predictions .
Memory-based methods, which are the focus of this pa-per, store raw preference information in computer memory, and access it as needed to find similar users or items, and to make predictions. Memory-based collaborative filtering methods can be further divided into two groups: user-based and item-based [11]. User-based methods look for users (also called neighbors ) similar to the active user, then calculate a predicted rating as a weighted average of the neighbor X  X  ratings on the desired item. Fig. 1 shows an example of a user-based approach. The active user has rated three items, Figure 1: Rating prediction for the active user on desired item. Neighbors are sorted according to their similarities to the active user. y 1, y 2 and y 3, and a predicted rating is desired for an un-rated item. Neighbors who have rated some or all of the items rated by the active user are sorted by similarity; some of them ( u 1, u 3, u 5) who have also rated the desired item are used to create a prediction. Item-based methods look for items similar to the desired item, then calculate a predicte d rating as a weighted average of the active user X  X  ratings of the similar items.

Collaborative filtering recommenders that process a large number of users are unable to maintain real-time perfor-mance if the system adopts rating information from all users in the prediction process. Moreover, including neighbors with low similarity in the calculation may introduce noise. One popular strategy to minimize these effects is to choose only the n nearest neighbors of the active user [6, 10]. This approach is referred to as a top-n strategy. Another com-mon strategy is to set a similarity threshold; only users wit h absolute similarities greater than the threshold are selec ted as neighbors [12].

Cosine similarity (or vector space similarity) and Pear-son X  X  correlation coefficient are popular similarity measur es for collaborative filtering (see [10, 2, 5] for examples). Si m-ilarity can also be learned from training data [8]. For user-based approaches, Pearson correlation (User-Based Pearso n Similarity or UBPS) is believed to perform better than vec-tor space similarity (User-Based Vector Similarity or UBVS ) [2]. In this paper, UBPS is taken as the benchmark method.
The Pearson correlation coefficient to measure the simi-larity of two users is defined as Sim ( u, v ) = where C uv is the set of all items co-rated by both users u and v , i.e. , C uv = { i  X  S item | r ui 6 =  X  &amp; r vi
To generate the prediction value for a desired item from ratings of neighbors, Resnick et al. [10] have proposed a widely-used method for computing predicted ratings. The predicted rating  X  r ux can be calculated as: where N ux represents the subset of selected neighbor users for user u on item x . Generally, only those neighbors who have explicitly rated the desired item are selected.
For item-based methods, it is important to remove the difference in rating scale between users when computing the similarity of different items. The adjusted cosine similari ty accomplishes this by subtracting the user X  X  average rating from each co-rated pair [11]. The adjusted cosine similarit y for a pair of items is defined as: d Sim ( x, y ) = where C xy is the set of all users rating items x and y , referred to as Item-Based Vector Similarity (IBVS) in this paper.

Some examples of user-based methods include [10, 2, 5, 8] and the examples of item-based methods include [11, 4]. Recently, Wang et al. [14] have proposed a fusion framework to unify the user-based and item-based collaborative filter ing approaches. They extract a background model from similar users toward similar items to smooth the rating predictions .
In addition to approaches which restrict selecting nearest -neighbor candidates to users who have rated the given item, are approaches which attempt to replace missing values. They can be replaced by a zero score, which lowers the pre-diction, or the average rating of similar users [2, 5]. Al-ternatively, [15] clusters the user data and replaces missi ng ratings by an interpolation of average rating of the cluster . By comparison, the recursive prediction algorithm propose d by Zhang et al. [16] allows the nearest-neighbor users to be selected from a larger range of candidate users. Based on this algorithm, they propose new techniques for selecting nearest-neighbors to improve the prediction accuracy. lationship between prediction errors and the similarities of items or of neighbors.
The case studies in this section and the experiments in section 5 are based on a popular database, the MovieLens the University of Minnesota. The database contains data on movies collected from a popular recommender system. The data contains 100,000 discrete ratings (on a 1-5 scale) from 943 users on 1682 movies, where each user has rated at least 20 items. This database has been widely exploited as a benchmark for evaluating proposed approaches to rec-ommender systems [14, 13, 16].
 In addition to ratings on items assigned by users, the MovieLens data set includes user-and item-specific infor-mation, such as time stamp, user demographics, movie title, genre, etc. Only the ratings data is used in this paper.
Prediction errors, the differences between the predicted ratings and the true ratings, are unavoidable. For an active of GroupLens Research ( http://www.grouplens.org ). user u , if the predicted rating on a desired item x is  X  r prediction error e ux is defined as the difference between  X  r and the true rating r ux :
A fundamental assumption of our algorithm is that predic-tion error is primarily due to inaccuracy in computing user similarity. If (a) Eqn. (2) is correct, (b) the user X  X  rating s accurately reflect their preferences, and (c) we can compute user similarity accurately, we should be able to correctly generate predictions. We believe the first two of these, but not the third. The computed similarities between users are somewhat inaccurate due to data sparseness. To obtain the similarity of users, using, for example, Eqn. (1), co-rated items are required. In real applications, very few items are co-rated by the active user and other users, leading to a low-ered confidence of computed similarities between users, or o f the weights to be applied to neighbors which contribute to predictions.

Formally, given the set of neighbors of an active user u on the desired item x , N ux , the predicted rating by Eqn. (2) is simplified as where
Since the similarity may be inaccurate, especially when the rating matrix is very sparse, the predicted rating may be inaccurate as well. Based on the assumptions, if the weights contributed by neighbors are accurate, the predicted ratin g is exactly the real rating that the active user would give to the desired item. That is, where w v is the true weight that neighbor v in N ux con-tributes to the prediction.

Thus, we have the prediction error of user u on item x ,
Fig. 2 shows a comparison of similarities with prediction errors for different items rated by the active user. For each item, the rating value from the active user is predicted, and the deviation from the actual value, i.e., the error, is com-puted. Based on this, we can compare similarity between each rated item and the desired item with the associated error. More formally, using Eqn. (8), we can compute the prediction error of user u on a different item y . Assume that user u shares identical neighbors for predictions on items x and y , i.e., N ux = N uy (referred to in this discussion as N then the difference between prediction errors for item x and item y is (b) On the extracted data set with 209 users each of whom rates more than 160 items.
 Figure 2: The relationship between prediction error and item similarity. Prediction error is computed for each item rated by the active user. The similarity between each item and the desired item is compared with the difference in their errors.

Given u and N u , (  X  w v  X  w v ) is constant across items, so  X  e xy depends on ( r vx  X  r vy ) where v  X  N u . If Sim ( x, y )  X  1, i.e., items x and y are similar, then each user will rate both items similarly (based on the assumption of collaborative filtering). So, for each v  X  N u , ( r vx  X  r vy )  X  0). Referring to Eqn. (9), we find that ( r vx  X  r vy )  X  0 implies  X  e Therefore, Sim ( x, y )  X  1 implies  X  e xy  X  0; in other words, there is a negative correlation between Sim ( x, y ) and  X  e To validate this, we conducted a study using the entire MovieLens data set. First, each rating in the data set was predicted with the standard user-based approach (UBPS) for each rating was computed using Eqn. (4). For each user u with k ratings, pairs of ratings ( i, j ) were considered where i = 1 , . . . , k  X  1 and j = i + 1 , . . . , k . The pairs were consid-ered as a vector, and used to form a vector of prediction er-ror differences, remainder are taken as training data to generate a predictio n for the one withheld.
 Figure 4: Common neighbors -for a given item rated by the active user, the set of neighbors that rate both that item as well as the desired item. Here we show common neighbor counts for each item ( y 1 , y 2 , y 3 ) rated by the active user.  X  X  X  X  X  X  X  X  X  Sim ( u ) = ( Sim ( y i , y j )) computed using the adjusted cosine similarity defined in Eqn. (3). The Pearson correlation of these two vectors is then computed for each user. Fig. 3(a) shows the results of this computation for 943 users, with the users sorted by the correlation coefficient. 906 users had a negative correlation. Of these, 752 users had a confidence level [9] of greater than 95%, about 80% of all users.
We note that not all users in the original dataset are neg-atively correlated. Due to data sparsity, some items share very few common neighbors, leading to a lack of correlation of their prediction errors. We conjecture that an increase in the number of items rated by each user would lead to stronger negative correlations by increasing the number of common neighbors per item. To test this, we extracted a subset of data from the original MovieLens data set, con-taining 209 users, each of whom rated more than 160 items. The results are shown in Fig. 3(b). All users show a negative correlation, with greater than 95% confidence.

The MovieLens data set is considered to be representative of most data sets used in collaborative filtering recommende r Figure 5: Statistical results of differences of predic-tion errors in different intervals of common neigh-bors, n = 50 . systems [16]. Thus, we believe that the correlation between prediction errors for similar items is a common phenomenon for most collaborative filtering recommenders.
In the above discussion, we assumed that the neighbors of the active user on different items are identical, i.e., N N uy = N u in Eqn. (9). However, due to missing ratings in real applications, the neighbors of the active user on dif -ferent items are often different i.e., N ux 6 = N uy , or even N ux  X  N uy =  X  . For example in Fig. 4, with respect to the active user, the desired item shares 2 neighbors for item y 1 ( u 1, u 5), 1 neighbor for item y 2 ( u 5) and 3 neighbors for item y 3 ( u 1, u 3, u 5) (note that items not rated by the active user are not considered). Is there any relationship between the number of shared neighbors for different items and the difference of prediction errors between them? Intuitively, the nearest-neighbor users of an active user are also simila r to each other and share similar preferences, i.e., they rate items similarly. For a given active user, if two different ite ms share common neighbors, i.e., the two items are co-rated by a set of similar users, they are somewhat similar. The more common neighbors they share, the more similar two items are. So we can treat the number of neighbors (of the active user) shared by different items as a kind of item similarity measure.

We investigated this idea as follows. First, each rating in the original MovieLens data was predicted by UBPS using the all-but-one method, and the prediction errors calculat ed. Second, for each user, the number of common neighbors for each pair of predictions and the differences of correspond-ing prediction errors were recorded. Fifty nearest neighbo rs were used as the default in the prediction calculation. Fig. 5 plots the differences of prediction errors versus the number of common neighbors (with the neighbors binned). The fig-ure also presents the mean similarity of corresponding item s involved in each interval. Note that the difference of predic -tion errors decreases when the number of common neighbors increases. The exception on the last interval (ranging from 46 to 50 neighbors), may be explained by statistical error due to the limited number of samples (only four samples) in that interval. Based on these results, it seems likely that i n-creased neighbor overlap on a pair of items implies a reduced difference between their prediction errors. It should be not ed that an increase in the number of common neighbors shared by two items implies an increased similarity between items. prediction error for the active user on the desired item. Based on these, a framework is presented to improve rat-ing predictions using estimated prediction errors.
As discussed above, given an active user, the prediction errors on different items are correlated. Intuitively, the p re-diction error of an active user on a desired item could be estimated using other prediction errors in the training set . Then, the original predicted rating could be improved using this predicted error.

Note that such a calculation does not require use of all items in the training set rated by the active user. Analogous to selecting top-n nearest neighbors to make prediction in user-based methods, we introduce a parameter,  X  , to control the number of prediction errors in the training set used to estimate the prediction error of the desired item. The most  X  similar items rated by the active user (computed using Eqn. (3)) are selected and their prediction errors are used.
To estimate the prediction error of the active user u on the desired item x , the following steps are performed: 1. For each item, y i , rated by x , compute a prediction 2. Calculate the estimated prediction error for  X  r ux :
Using this prediction error,  X  e ux , and the original predicted rating based on standard user-based methods,  X  r ux , an im-proved predicted rating can be calculated:
We call this approach the Prediction Error-Based En-hancement method (PEBE). The contribution weight  X  uy i in Eqn. (10) is the key to the estimation of prediction errors . Based on our case studies, we conclude that prediction er-rors are correlated to each other based on item similarities as well as the common neighbors used to make predictions. Thus, we propose the following two strategies to determine the weights  X  uy i 1. Item similarity strategy(IS). Given the desired item, 2. Counting number of common neighbors(CN). The num-
In this section we describe a series of experiments designed to evaluate the two variants of the PEBE. We begin by ex-ploring the effects of two parameters on the performance of the PEBE algorithms. We use these results to select the best parameter values, and then compare the optimally parame-terized PEBE algorithms against benchmark algorithms on a series of data sets with different sparsity values.
We tested the proposed approach, with different weight-ing strategies, on the MovieLens data set. The data set comes with three predefined splits for 5-fold cross valida-each data split, 80% of the original data was included in the training data and 20% of it was included in the test data. The test sets in all cases are disjoint. We refer to this 5-fold cross validation set as ML-4to1 . We generated addi-tional splits on the original data for 2-fold cross validati on and 10-fold cross validation, called ML-1to1 and ML-9to1 respectively. The characteristics of the test data sets are summarized in Table 1. Note that the three datasets allow us to investigate the effects of different levels of sparsity o n the algorithms.

Several techniques have been used to evaluate recommender systems, including Mean Absolute Error (MAE), Mean Squared Error (MSE), Normalized Mean Absolute Error and Re-ceiver Operating Characteristic curves (ROC curves) (see [7] for details). The choice of metrics should be based on the selected user tasks and the nature of the data sets. P-Tango [3], for example, considered the deviation of gener-ated predictions from the user-specified ratings. As a resul t, the selected evaluation metric was inaccuracy, an alternat ive name for MAE. On the other hand, Breese et al. [2] evalu-ated a ranked list of recommended items, and therefore used the expected utility of that list to the user as their metric. We expect our proposed algorithms to derive a predicted peating the experiments with each training and test set and averaging the results. The number of fold is the number of tested sets. score for already-rated items rather than generate a top-n recommendation list. Based on that specific task, MAE is employed as the metric in this paper. It corresponds to the average absolute deviation of predictions from the ground truth data.
 where L denotes the number of tested ratings, and the sum is generated from all test item ratings and test users. A smaller value of MAE indicates a better performance. There are only two parameters to the proposed method. In addition to the previously-discussed parameter  X  -the number of selected similar items, the number of neighbors, n , influences the performance of the proposed method. We first conducted experiments to investigate the influences of n on the three data sets mentioned in Section 5.1. In these tests, we used all available statistical prediction errors to make predictions, that is to say,  X  was set to the number of items in the database, i.e.,  X  = 1682. The benchmark method, UBPS, was also tested in this manner.

The proposed approaches behaved consistently across the three data sets; therefore, only the results on ML-4to1 are reported here. Fig. 6 shows MAE of PEBE with different weighting strategies when  X  is set to the maximum and the number of neighbor ratings is varied. Note that the effect of varying the number of neighbors is similar for the dif-ferent approaches, and the results are relatively invarian t when the number of neighbors exceeds 20. The similarity of results between PEBEs and UBPS may be due to the fact that both the PEBE method, and the statistical prediction errors it employs, are based on UBPS. All methods gave the best result when n was 50; we select that value as the fixed parameter for all approaches in the next experiment.
To study the influence of  X  , we used a fixed value of n (50) for the ML-4to1 dataset and varied the value of  X  .
Fig. 7 shows MAE of PEBE with different weighting strate-gies (this figure does not show UBPS, since UBPS does not depend on the parameter  X  ). Note that we get the best per-formance when  X  = 50 for both weighting strategies (both strategies select  X  most similar items based on item similar-ity as defined in Eqn. (3)). The decrease of MAE when  X  varies from 5 to 50 may be explained by an increase in the ac-curacy of error prediction when more information (i.e., whe n more individual prediction errors) is available. Also note that MAE increases slightly when  X  exceeds 50, probably due to decreased correlation between items as  X  increases.
In any real-life recommendation application, the number of available ratings is usually very small compared to the number of ratings that are to be predicted. Effective predic-tion of ratings from a small number of samples is very impor-tant for real applications. The next experiments investiga te the effect of data sparsity on the performance of our algo-rithm. Also the proposed approaches are compared to the standard user-based collaborative filtering method, UBPS, and the standard item-based collaborative filtering method , IBVS. The comparison results are reported in Table 2. Note that all PEBEs with different weighting strategies exhibit improvement over both UBPS and IBVS for all sparsity lev-els. In particular, PEBE-CN performs best, which indicates Figure 6: Varying the number of neighbors on ML-4to1 with different weighting strategies. In all cases,  X  = 1682 . Figure 7: Varying the number of selected item on ML-4to1 with different weighting strategies. In all cases, n = 50 . that the CN strategy is the best reflection of the relationshi p between prediction errors. Also note that all the standard deviations of prediction errors are consistent with MAE.
Note that PEBE-CN reduces the MAE of UBPS by 2 . 51%, 3 . 49% and 3 . 79% on ML-1to1 , ML-4to1 and ML-9to1 re-spectively. In other words, the proposed approach increas-ingly improves UBPS as the rating data become denser. This can be explained as follows. When the user-item ma-trix is denser, the number of common neighbors of different items with respect to the active user are higher, leading to a higher correlation of prediction errors for correspondin g items. This conjecture is supported by Table 3 which shows the average number of common neighbors for datasets of dif-ferent sparsity. For example, each pair of items for a given user shares about 8 neighbors on average in ML-9to1 , while in ML-1to1 the number decreases to less than 5. In this table, the standard deviations for number of neighbors look somewhat high. And the explanation is that the numbers of neighbors for different users on different desired items ar e very different and vary in a big scale. It is true in most real-life applications. Therefore, it implies that our approach es Table 2: Comparison of MAE with n = 50 ,  X  = 50 on data sets with different sparsity. The values in brackets are standard deviations of prediction er-rors. A smaller value means better performance.
 PEBE-CN 0.737 (0 . 581) 0.718 (0 . 568) 0.710 (0 . 562) PEBE-IS 0.743(0 . 584) 0.727(0 . 573) 0.721(0 . 567) UBPS 0.756(0 . 597) 0.744(0 . 588) 0.738(0 . 584) IBVS 0.791(0 . 612) 0.769(0 . 596) 0.757(0 . 589) Table 3: Numbers of common neighbors on data sets with different sparsity, n = 50 .
 could perform well in most applications regardless of the sparsity of data.

Various authors have attempted to improve on the UBPS and IBVS algorithms. The best we have found, the recursive prediction algorithm proposed by Zhang et al. [16], reporte d results on a data set similar to our ML-4to1 . Their best MAE is 0 . 742 with their CS+ strategy. Our value of 0 . 718 is 3 . 23% lower.
We have shown that the prediction errors of an active user on different items are correlated to the similarities of the corresponding items and to the degree to which they share common neighbors. Therefore, a method based on statisti-cal prediction errors was proposed to improve the predictio n accuracy of standard user-based collaborative filtering al go-rithms.

Two strategies were proposed to estimate the prediction errors on desired items. Experiments on the MovieLens data set were conducted to compare the proposed approaches with other benchmark collaborative filtering methods. The results show that the proposed methods improve the per-formance of the standard user-based approach and perform better than other approaches, such as the recursive predic-tion algorithm. Of the two proposed strategies, the strateg y which considers the number of common neighbors performs best, showing a reduction of 3 . 49% in mean absolute error over the standard user-based collaborative filtering metho d on the predefined MovieLens test set.

To the best of our knowledge, this is the first attempt to improve prediction accuracy in recommendation systems by employing statistical prediction errors. Future work in -cludes presenting better methods to model the prediction errors and utilizing them to boost performance of existing collaborative filtering approaches other than the standard user-based method.
The authors would like to thank the anonymous reviewers for their valuable suggestions. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] J. S. Breese, D. Heckerman, and C. M. Kadie.
 [3] M. Claypool, A. Gokhale, T. Miranda, P. Murnikov, [4] M. Deshpande and G. Karypis. Item-based top-n [5] J. L. Herlocker, J. A. Konstan, A. Borchers, and [6] J. L. Herlocker, J. A. Konstan, and J. Riedl. An [7] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [8] R. Jin, J. Y. Chai, and L. Si. An automatic weighting [9] J. Kiefer. Conditional confidence statements and [10] P. Resnick, N. Iacovou, M. Suchak, P. Bergstorm, and [11] B. M. Sarwar, G. Karypis, J. A. Konstan, and [12] U. Shardanand and P. Maes. Social information [13] M. G. Vozalis and K. G. Margaritis. Using SVD and [14] J. Wang, A. P. de Vries, and M. J. T. Reinders. [15] G.-R. Xue, C. Lin, Q. Yang, W. Xi, H.-J. Zeng, Y. Yu, [16] J. Zhang and P. Pu. A recursive prediction algorithm [17] Y. Zhang and J. Koren. Efficient bayesian hierarchical
