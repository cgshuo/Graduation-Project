 Powerful SIMD instructions in modern processors offer an opportunity for greater search performance. In this paper, we apply these instructions to decoding search engine post-ing lists. We start by exploring variable-length integer en-coding formats used to represent postings. We define two properties, byte-oriented and byte-preserving , that charac-terize many formats of interest. Based on their common structure, we define a taxonomy that classifies encodings along three dimensions, representing the way in which data bits are stored and additional bits are used to describe the data. Using this taxonomy, we discover new encoding for-mats, some of which are particularly amenable to SIMD-based decoding. We present generic SIMD algorithms for decoding these formats. We also extend these algorithms to the most common traditional encoding format. Our exper-iments demonstrate that SIMD-based decoding algorithms are up to 3 times faster than non-SIMD algorithms.
 E.4 [ Coding and Information Theory ]: Data Compaction and Compression; H.3.1 [ Information Storage and Re-trieval ]: Content Analysis and Indexing X  indexing meth-ods ; C.1.2 [ Processor Architectures ]: [Single-instruction-stream, multiple-data-stream processors (SIMD)] Algorithms, Performance, Measurement, Experimentation variable-length integer encoding, SIMD
The central data structure in search engines is the inverted index , a mapping from index terms to the documents that contain them. The set of documents containing a given word is known as a posting list . Documents in posting lists are typ-ically represented by unique nonnegative integer identifiers. Posting lists are generally kept in sorted order to enable fast set operations which are required for query processing.
Because posting lists typically account for a large fraction of storage used by the search engine, it is desirable to com-press the lists. Smaller lists mean less memory usage, and in the case of disk-based indices, smaller lists reduce I/O and therefore provide faster access. A common way to com-press posting lists is to replace document IDs in the list with differences between successive document IDs, known as  X  -gaps (sometimes just gaps or d -gaps ). Since  X -gaps are on average necessarily smaller than raw document IDs, we can use a variable-length unsigned integer encoding method in which smaller values occupy less space. In a sorted list the  X -gaps are always non-negative, so we are only concerned with encoding nonnegative integers; for the remainder of the paper  X  X nteger X  means unsigned integer.

The integer encoding for a posting list is performed in-frequently, at indexing time. The decoding, however, must be performed for every uncached query. For this reason, efficient integer decoding algorithms are essential, as are en-coding formats that support such efficient decoding.
We started by exploring several commonly-used variable-length integer encodings and their associated decoding al-gorithms. Based on their common structure, we defined a taxonomy that encompassed these existing encodings and suggested some novel ones. Our investigation was moti-vated by the desire to incorporate fine-grained parallelism to speed up integer decoding. We were able to develop decoding methods that use SIMD instructions available on many general-purpose processors, in particular current Intel and forthcoming AMD processors. In this paper, we present these methods and evaluate them against traditional tech-niques. Our results indicate significant performance benefit from applying SIMD to these new formats.

The remainder of the paper is organized as follows. We review some related work on encoding formats and decoding algorithms in Section 2. Section 3 presents our taxonomy of byte-oriented encodings. Section 4 describes three particular encoding formats which are well suited to SIMD parallelism in more detail. Section 5 gives an overview of our use of SIMD on Intel-compatible processors. In Section 6 we pro-vide SIMD algorithms for decoding the formats introduced in Section 4. Section 7 contains an evaluation of the results, followed by our conclusions in Section 8. Figure 1: Format known in the literature as  X  X byte X ,  X  X int X , etc. Called varint-SU in the taxonomy of Section 3. Figure 2: Storing the integer 123456 in varint-SU format.
The problem of integer encoding has been studied for sev-eral decades. General compression techniques such as Huff-man coding often utilize analysis of the data to choose opti-mal representations for each codeword. Many applications, however, use techniques that do not depend on data distri-bution. These are called nonparametric codes [4].

Encoded representations of data may also be classified as byte-or bit-aligned, depending on whether codewords are re-quired to end on byte boundaries. In addition, Anh [1, 2] in-troduced an interesting class of word-aligned encodings (e.g. Simple-9). These encodings occupy an intermediate place between bit-aligned and byte-aligned encodings by allowing codewords to end at an arbitrary bit position as long as they do not cross the machine word boundary.

While there are multiple choices for encoding posting lists in information retrieval applications, in this paper we con-centrate exclusively on non-parametric, byte-oriented encod-ings.

The most common such encoding format uses 7 bits of each byte to represent data, and one bit to represent whether the encoded representation of the integer continues into the next byte (see Figure 1). This format has a long history, dating back at least to the MIDI specification in the early 1980s [14], and foreshadowed by earlier work on byte-based vocabulary compression by H.S. Heaps [11]. As early as 1990, Cutting and Pedersen [7] used this format to encode  X -gaps in an inverted index. The information retrieval lit-erature refers to the format by many different names, in-cluding vbyte, vint, and VB. Different authors give slightly different versions that vary on endianness, location of the continuation bit (most significant vs. least significant), and whether 0 or 1 indicates continuation. Figure 2 illustrates this encoding for the integer 123456 with little-endian, most significant bit and 1 for continuation.

Another format BA , introduced by Grossman in 1995 [10] uses 2 bits of the first byte to indicate the length of the encoded integer in binary. In his 2009 WSDM keynote, Dean [8] described a format he calls group varint that ex-tends the BA format (which he calls varint-30 ) and reported significant performance gains obtained at Google by adopt-ing it. A similar but more general format was described by Westmann et al. [19] in the database context. Schlegel et al. [17] applied SIMD-based decompression algorithms to a specialized version of Westmann X  X  format that coincides exactly with group varint, but under the name k -wise null suppression . While they do not precisely describe the decod-ing and table generation, we believe that their algorithms are special cases of the generalized algorithms we describe in Section 6. All of these encodings fall into the taxonomy defined in this paper.

B  X  uttcher et al. [4] have compared performance of several encoding techniques using posting lists from the GOV2 cor-pus on a particular query sample. They originally reported vbyte being the fastest at decoding, with Simple-9 being second. They recently updated their analysis [5] to include the group varint format, reporting that it outperforms both vbyte and Simple-9. Our experiments (presented in Sec-tion 7) show that the SIMD techniques described in this pa-per significantly outperform all of these in decoding speed.
Encoding formats are generally distinguished by the gran-ularity of their representation. We focus on encodings sat-isfying the following definition.

Definition 1. We call an encoding byte-oriented if it satisfies the following conditions: 1. All significant bits of the natural binary representation 2. Each byte contains bits from only one integer. 3. Data bits within a single byte of the encoding preserve 4. All bits from a single integer precede all bits from the A byte-oriented encoding is fixed-length if every integer is encoded using the same number of bytes. Otherwise it is variable-length .
 Since variable-length byte-oriented formats must encode the length of the encoded data, they vary along the following three dimensions: It is evident that for byte-oriented formats, the natural unit of length is a byte. We call the set of bits used to represent the length the descriptor , since it describes how the data bits are organized. (earlier references [14], [7] do not name the format)
We assume that each encoded integer requires at least one byte, so both binary and unary descriptors can represent the length n by recording the value n  X  1. This reduces the number of bits required to represent a given length. 1
The dimensions listed above provide the basis of a tax-onomy of byte-oriented encoding formats for integers that can be encoded in four bytes or less. Selecting one of the possible options for each dimension determines a position in the taxonomy. This taxonomy, shown in Table 1, provides a unifying nomenclature for the encoding formats, several of which have been described previously under various names.
For example, Grossman X  X  BA format becomes varint-PB in our taxonomy, since it is a variable -length encoding of in-tegers with descriptor bits packed together and representing the length in binary .

For the unary formats, we follow the natural convention where the quantity is represented by the number of consec-utive 1 bits, followed by a terminating 0. We start from the least significant bit. Thus 0111 represents the number 3.
Accordingly, the vbyte encoding may be viewed as repre-senting the length  X  1 of the encoded representation in the sequence of continuation bits. For example, a three-byte integer encoding would look like this: Notice that the leading bits form the unary number 2, rep-resenting the length 3. Thus we call this representation varint-SU , since it is a variable -length representation of in-tegers with length information split across several bytes and represented in unary . While unary length representation has been widely used in bit-oriented encodings, for byte-oriented encodings the concept of continuation bits obscured their in-terpretation as unary lengths.

If binary length descriptors are used, the descriptor length must be fixed in advance, or additional metadata would be required to store the length of the descriptor itself. For this reason, all binary formats in the taxonomy use fixed-length descriptors of 2 bits per integer. Furthermore, since split-ting a fixed-length k -bit binary descriptor (one bit per byte) results in a byte-oriented integer encoding that requires at least k bytes, the split binary encoding format does not of-fer a competitive compression rate and we do not consider it further.
Storing length as n would allow the length zero to represent an arbitrary constant with zero data bytes. Such an encod-ing, however, does not in general satisfy the first property of Definition 1.

There are also additional variations of some of these for-mats. Bytes of the encoded data may be stored in little-endian or big-endian order; descriptor bits may be stored in the least significant or most significant bits. While these choices are sometimes described as arbitrary conventions, in practice there are efficiency considerations that make certain variants attractive for certain machine architectures. For example, in varint-SU, representing termination as 0 in the most significant bit allows the common case of a one-byte integer to be decoded without any shifts or masks. While traditional decoding algorithms run more efficiently when the representation preserves native byte ordering, the per-formance of the SIMD algorithms presented in Section 6 does not depend on the ordering. Without loss of general-ity, for the remainder of the paper we restrict our attention to little-endian encodings.

The byte-oriented encoding taxonomy suggests two en-codings, varint-PU and varint-GU , that, to our knowledge, have not been previously described.

Varint-PU is similar to varint-SU, but with the descrip-tor bits packed together in the low-order bits of the first byte rather than being split across all bytes. (The choice of low-order bits to hold the descriptor is appropriate for little-endian encodings on little-endian architectures so that all data bits for one integer are contiguous. For the same reason, on big-endian architectures placing the descriptor in the high-order bits and using big-endian encoding is more efficient to decode.) The compression rate of varint-PU is the same as that of varint-SU, since the bits in each encoded integer are identical but rearranged. The decoding perfor-mance of varint-PU using unaligned reads, masks, and shifts in a table-driven algorithm similar to that for varint-PB is faster than traditional varint-SU, but significantly slower than the group formats such as varint-GU, described in de-tail in the next section.
Within our taxonomy, encoding formats that group sev-eral integers together provide opportunities for exploiting SIMD parallelism. These encodings satisfy the following im-portant property.

Definition 2. We call a byte-oriented encoding byte-preserving if each byte containing significant bits in the original (unencoded) integer appears without modification in the encoded form.

Neither split nor packed formats satisfy this property, since the descriptor bits are intermingled with data bits in some bytes. The separation of descriptor bytes from data Figure 4: Storing the four integers 0xAAAA , 0xBBBBBB , 0xCC , 0xDDDDDDDD in varint-GB format. The value of each pair of bits in the descriptor is one less than the length of the corresponding integer. Byte addresses increase from right to left, matching the order of increasing bit significance. The order of pairs of bits in the descriptor matches the order of the integers. bytes in group formats allows for more efficient decoding. It facilitates the use of tables to simplify the decoding process and avoids bitwise manipulations that are required to elimi-nate interspersed descriptor bits. In particular, we shall see in Section 6 that byte-preserving encodings are especially amenable to decoding with the SIMD techniques described in this paper. 2 There are two classes of group formats, group binary (varint-GB) and group unary (varint-GU).

In the varint-GB format (called group varint in [8] and k -wise null supression (with k = 4) in [17]) a group of four integers is preceded by a descriptor byte containing four 2-bit binary numbers representing the lengths of the corre-sponding integers. Figure 3 illustrates this format for one such group. The actual number of bytes in a group may vary from 4 to 16. Figure 4 shows how the four hexadec-imal numbers 0xAAAA , 0xBBBBBB , 0xCC , 0xDDDDDDDD would be represented. The four integers require, correspondingly, 2 bytes, 3 bytes, 1 byte, and 4 bytes. For each integer, its length n is represented in the descriptor by the 2-bit binary value n  X  1. Therefore, the descriptor byte contains the val-ues 01, 10, 00, and 11 respectively. To maintain a consistent order between descriptor bits and data bytes, we store the first binary length in the least significant bits, and so on. Thus the descriptor byte for these four integers is 11001001.
Varint-GB operates on a fixed number of integers occu-pying a variable number of bytes, storing their lengths in binary. In contrast, the varint-GU format operates on a fixed number of bytes encoding a variable number of inte-gers, storing their lengths in unary.

Varint-GU groups 8 data bytes together along with one descriptor byte containing the unary representations of the lengths of each encoded integer. The 8 data bytes may en-code as few as 2 and as many as 8 integers, depending on their size. The number of zeros in the descriptor indicates the number of integers encoded. This format is shown in
Note that what Anh [1] calls word-aligned is neither byte-oriented nor byte-preserving as defined in this paper. Figure 6: Storing the three integers ( 0xAAAA , 0xBBBBBB , 0xCC ) in varint-G8IU format. The descrip-tor bits express the unary lengths of the integers. Since the next integer 0xDDDDDDDD in our example does not fit in the data block, the block is left in-complete and padded with 0s, while the descriptor is padded with 1s.
 Figure 5. The block size of 8 is the minimal size that can use every bit of the descriptor byte; larger multiples of 8 are possible, but did not improve performance in our exper-iments.

Since not every group of encoded integers fits evenly into an 8-byte block, we have two variations of the encoding: incomplete and complete.
 In the incomplete block variation, which we call varint-G8IU , we store only as many integers as fit in 8 bytes, leav-ing the data block incomplete if necessary. 3 The remaining space is padded with zeros, but is ignored on decoding. When there is no additional integer to decode, the final (most significant) bits of the descriptor will be an unter-minated sequence of 1 bits.

An example is shown in Figure 6. We use the same four integers 0xAAAA , 0xBBBBBB , 0xCC and 0xDDDDDDDD to illus-trate. Encoding these values requires 10 bytes, but we have only 8 bytes in the block. The first three integers fit into the block using 6 bytes, leaving 2 bytes of padding. The final integer 0xDDDDDDDD is left for the next block (not shown). The descriptor contains the three unary values 01, 011, and 0, and two padding bits 11. These are arranged in the same order as the integers, giving the descriptor a binary value of 11001101.
 In the complete block variation, which we call varint-G8CU , we always fill all eight bytes in a data block. before, the number of zero bits in the descriptor indicates the number of complete integers encoded. In situations where an integer exceeds the remaining space in the current block, as much of that integer as fits is placed in the current block. The remaining bytes of that integer are carried over into the
In our notations for the encoding, the number 8 represents the size of the data block.
There is also a variation of this encoding format that uses variable size data blocks and avoids padding. Its perfor-mance characteristics are between those of varint-G8IU and varint-G8CU described later.
As before, the number 8 represents the size of the data block. Figure 7: Storing four integers ( 0xAAAA , 0xBBBBBB , 0xCC and 0xDDDDDDDD ) in varint-G8CU format. The last two bytes of the fourth integer carry over to the subsequent data block, and its descriptor bits carry over to the subsequent descriptor byte. Figure 8: Using the PSHUFB instruction to reverse the byte order of four integers in parallel. The shf vector determines the shuffle sequence used to transform src to dst . next data block. Similarly, the corresponding descriptor bits are carried over to the next block X  X  descriptor byte.
An example is shown in Figure 7. Again we use the same four integers 0xAAAA , 0xBBBBBB , 0xCC and 0xDDDDDDDD . The first three integers and the corresponding descriptor bits are stored exactly as in varint-G8IU. However, varint-G8CU handles the fourth integer differently. Its first two bytes are placed in the first data block, filling it entirely, and the re-maining two bytes go into the following block. The two descriptor bits corresponding to these last two bytes go into the next block X  X  descriptor byte. Although spread across two descriptor bytes, the unary value of the descriptor bits for this fourth integer still equals n  X  1, where n is the length of the encoded integer. Facilities for fine-grained parallelism in the SIMD (Single Instruction, Multiple Data) paradigm are widely available on modern processors. They were originally introduced into general-purpose processors to provide vector processing ca-pability for multimedia and graphics applications. Although SIMD instructions are available on multiple platforms, we restricted our focus to Intel-compatible architectures [12] implemented in current Intel processors in extensive use in many data centers, as well as forthcoming AMD processors.
In these architectures, a series of SIMD enhancements have been added over time. Among the current SIMD capa-bilities are 16-byte vector registers and parallel instructions for operating on them.

The PSHUFB instruction, introduced with SSSE3 in 2006, is particularly useful. 6 It performs a permutation ( X  X huffle X ) of bytes in a vector register, allowing the insertion of zeros in
A similar instruction, vperm , is part of the AltiVec/VMX instruction set for the PowerPC processor family. specified positions. PSHUFB has two operands, a location con-taining the data and a register containing a shuffle sequence. If we preserve the original value of the data operand, we can view PSHUFB as transforming a source sequence of bytes src to a destination sequence dst according to the shuffle se-quence shf , implementing the following algorithm:
In other words, the i th value in the shuffle sequence indi-cates which source byte to place in the i th destination byte. If the i th value in the shuffle sequence is negative, a zero is placed in the corresponding destination byte.

The example illustrated in Figure 8 shows how PSHUFB can be used to reverse the byte order of four 32-bit integers at once.
Since byte-preserving formats remove leading zero bytes while retaining the significant bytes intact, the essence of decoding is reinserting the zero bytes in the right places. Our discovery is that we can make PSHUFB do virtually all of the work for many formats. We construct a shuffle sequence by inserting  X  1s in a sequence { 0, 1, 2, 3, ... } . With this sequence, the PSHUFB instruction will copy the significant data bytes while inserting the missing zeros.

An example of using PSHUFB to decode varint-G8IU is shown in Figure 9. This is the same data represented in Figure 6.

For a given format, we can precompute what the correct shuffle sequence is for a particular data block and its cor-responding descriptor byte. For all possible values of the descriptor (and sometimes additional state) we build a ta-ble of any shuffle sequence that might be needed at decode time.

The table entries also contain a precomputed offset. For the varint-GB format, the offset indicates how many bytes were consumed to decode 4 integers; it always outputs 16 bytes. For the varint-GU formats, the offset indicates how many integers were decoded; it always consumes 8 bytes.
Table construction occurs only once, while table lookup occurs every time a group is decoded. Figure 9: Using the PSHUFB instruction to decode the varint-G8IU format.
Given the availability of these tables, the general strategy of all the decodings is: 1. read a chunk of data and its corresponding descriptor; 2. look up the appropriate shuffle sequence and offset 3. perform the shuffle; 4. write the result; 5. advance the input and output pointers.

This approach allows us to decode several integers simul-taneously with very few instructions. It requires no condi-tionals, and thus avoids performance penalties due to branch misprediction. Two techniques make this possible. First, the logical complexity has been shifted from the code to the table. Second, the algorithm always reads and writes a fixed amount and then relies on the table to determine how much input data or output data it has actually processed. 7
Data blocks are not aligned on any fixed boundary. We depend on the ability of the CPU to perform unaligned reads and writes efficiently, and we have observed this to be true on modern Intel processors.
All of the group formats can be decoded using the gener-alized algorithm shown in Algorithm 1.
 Algorithm 1: decodeBlock
Decodes a block of data using SIMD shuffle. input : src , dst , state output : src , dst , state begin end
Because this algorithm constitutes the inner loop of the decoding process, it is essential to inline the implementation to avoid function call overhead. The algorithm takes three inputs:
This requires that the input and output buffers always have at least this amount available to read or write.
 The algorithm reads encoded values from the input stream, outputs decoded integers to the output stream and returns as its result the new positions of src , dst , and the updated state .

We always read 16 bytes, the size of the vector register used by the PSHUFB operation. The number of bytes cor-responding to a single byte descriptor is 8 for the unary formats and at most 16 for the binary format. While it is possible to read only 8 bytes for the unary formats, in our evaluation we found that doing so did not improve perfor-mance, and in fact made the implementation slightly slower.
A different table is used for each format. There is a table entry corresponding to each possible descriptor value and state value. The table has 256 entries for the varint-GB and varint-G8IU formats. For varint-G8CU format, the table has 4  X  256 = 1024 entries, because we have an entry for each descriptor and state pair, and the state is an integer i , with 0  X  i &lt; 4.

Each table entry logically contains four things: The shuffleSequence, inputOffset, outputOffset, and nextState functions are accessors for these fields. For some of the for-mats, some of these values are constant over all entries in the table, and are not stored explicitly; the accessors simply return constant values.

The shuffleAndWrite operation uses the PSHUFB operation with the provided shuffle sequence to expand the 16 bytes of data, inserting zeros into the correct positions. It then writes its result to the destination.

In the varint-GB case, the shuffle sequence is a 16-byte sequence describing a single PSHUFB operation. A single PSHUFB is sufficient because the group always contains four encoded integers, and thus the output never exceeds 16 bytes.

For decoding the varint-GU formats, the shuffle sequence is a 32-byte sequence specifying two PSHUFB operations. The second PSHUFB is required for the unary formats because an 8-byte data block may encode up to 8 integers, which can expand to 32 bytes. The output of the first PSHUFB is written to locations beginning at dst , and the output of the second PSHUFB to locations beginning at dst + 16. To avoid conditionals, the second shuffle is always performed, even when the output does not exceed 16 bytes. Since PSHUFB rearranges the register in place, the corresponding register needs to be reloaded with the original data before the second PSHUFB .

For unary formats, the input offset, by which we increment the src , is always 8 bytes. For varint-G8IU, the output offset measured in units of decoded integers varies between 2 and 8, except for the last block of a sequence, which may contain only 1 integer. For varint-G8CU, decoding one block may result in writing a portion of a decoded integer, so the output is a byte stream and the offset is measured in byte units. It varies between 8 and 32 bytes, except for the last block of the sequence which may output only 1 byte.

In the case of varint-GB, the output offset is always a constant 4 integers. 8 The input offset varies between 4 and 16 bytes.

For all of the encodings, the input offset needs to account for the additional one byte of the descriptor as well. All variable offsets are precomputed and stored in the format table.

For the varint-G8CU format, the table also contains the new state information indicating the number of bytes in the last integer to be used to decode the subsequent block.
For each of the group formats, the decoding table used by Algorithm 1 is constructed in advance. The construction process takes as input a descriptor byte value and a state value. It builds the shuffle sequence for the entry and com-putes the input offset, output offset, and next state (unless they are constant for the format).

We assume we deal only with valid descriptor values, those which could actually arise from encoding. For varint-GB, all possible byte values are valid. For the group unary formats, a descriptor is valid if and only if the distance between con-secutive zero bits does not exceed 4.

The algorithms for constructing shuffle sequences, offset values, and the next state value depend on the following abstract functions:
Again, the basic idea in constructing a shuffle sequence is to insert  X  1s in a sequence { 0, 1, 2, 3, ... } representing the byte positions in one block of the source data being de-coded. The resulting shuffle sequence is used by the PSHUFB instruction to copy the significant data bytes while inserting the missing leading zeros. The details of the construction are shown in Algorithm 2. The algorithm takes two inputs:
The varint-GB format requires auxiliary information to deal with sequences of length not divisible by 4. This may be done using length information stored separately or the convention that zero values do not appear in the sequence, so terminal zeros can be ignored.
For varint-GB and varint-G8IU, the value of state is always zero, since only complete integers are written in a given data block in these formats.
 The algorithm produces one output, shf , the shuffle sequence to be used for the given descriptor and state. The first loop iterates over every completed integer in the group corre-sponding to the given descriptor. For each completed in-teger in the group, the inner loop sets the shuffle sequence to move the encoded bytes from the source of the shuffle operation, inserting  X  1s to produce the leading zeros nec-essary to complete the decoded integer. Here the variable j advances over the source data positions in the data block, while the variable k advances over the positions in the shuffle sequence, which correspond to destination positions of the shuffle operation.

The concluding loop only executes for varint-G8CU. It sets the remainder of the shuffle sequence to transfer encoded bytes from the source for the last incomplete integer in the group.

Algorithm 2: constructShuffleSequence input : desc , state output : shf begin end
Computing input offsets is easy. For the unary formats the input offset is always 9; we always consume a block of 8 bytes of data and 1 descriptor byte. For the group binary format varint-GB, the input offset for a given descriptor desc is which is the sum of the lengths of the integers in the group plus 1 for the descriptor byte.

The output offset for varint-GB and varint-G8IU is equal to num( desc ) integers (which is always 4 for varint-GB). The output offset is for the varint-G8CU format.

The state value for the subsequent block is always 0 for varint-GB and varint-G8IU (and the state can be ignored for these formats). For varint-G8CU it is rem( desc ).
Although varint-SU is not a byte-preserving encoding, Al-gorithm 1 can be applied as a component for decoding it. By efficiently gathering the descriptor bits from each byte into a single value for a table lookup, we can treat a se-quence of 8 consecutive varint-SU-encoded bytes almost as if they were a varint-GU-encoded block. The Intel instruc-tion PMOVMSKB , which gathers the most-significant bits from 8 bytes into a single byte, provides the needed functionality. After applying a shuffle as in Algorithm 1, the descriptor bits must be  X  X queezed out X  to finish the decoding; this re-moval of the interspersed descriptor bits requires masks and shifts. Despite this additional step, the SIMD implementa-tion still outperforms the traditional method in most cases as shown in Section 7.
We used three corpora in our evaluation: Wikipedia [20] (6M documents 10 , 27 GB), Reuters RCV1 [16] (0.8M docu-ments, 2.5 GB), and GOV2 [15] (25M documents, 426 GB). To satisfy the resource constraints of our test environment, we randomly sampled 50% of the Wikipedia documents and 15% of GOV2. We removed all XML/HTML markup and five common stopwords ( a, an, and, of, the ) and applied stemming to conflate singular and plural nouns.
 The C++ implementation was compiled using gcc 4.5.1. 11 Measurements were done using a single-threaded process on an Intel Xeon X5680 processor (3.3GHz, 6 cores, 12 MB Intel Smart Cache shared across all cores), with 24 GB DDR3 1333 MHz RAM. Measurements are done with all of the input and output data in main memory.

Decoding speed results are shown in Table 2. For each corpus, this table shows the decoding speed measured in millions of integers per second; the fastest result is shown in boldface. In every case, a SIMD algorithm strongly out-performed a conventional algorithm; in all cases, the imple-
This counts all documents from the English Wikipedia dump except empty documents and redirect pages.
It uses the GCC intrinsics __builtin_ia32_pshufb128 , __builtin_ia32_loaddqu , and __builtin_ia32_storedqu to invoke the PSHUFB and unaligned load and store instructions. mentation of our varint-G8IU format was fastest. The  X  X ra-ditional X  implementation of varint-SU shown in the table is our best implementation for this encoding using traditional techniques. The  X  X ask table X  implementation of varint-GB is our implementation of the technique described in Dean [8].
Compression ratios are shown in Table 3. Here the com-pression ratio indicates the ratio between the bytes required for the integers encoded in the format and their original size of 4 bytes each. Compression ratios depend only on the encoding and not on the implementation.
There seems to be no standard benchmark for measur-ing integer decoding implementations in the information re-trieval field. Even with conventional test corpora, there are many variations possible in producing the posting lists. Evaluation methods are also not standardized. Some re-search on compression only reports compression rate but not speed. Speed is reported using different metrics and data.
B  X  uttcher et al. [4] start with the GOV2 corpus and the 10000 queries from the efficiency task of the TREC Terabyte Track 2006. They indicate they used components of the Wumpus search engine to index the GOV2 corpus, and then decoded the posting lists for non-stopword terms contained in the query set. They do in-memory measurements, but also compute a  X  X umulative overhead X  for decoding and disk I/O by estimating the I/O time based on compression rate.
Schlegel et al. [17] use 32MB synthetic data sets con-structed by sampling positive 32-bit integers from the Zipf distribution with different parameter values. They measure the amount of uncompressed data that can be processed per second.

Dean [8] uses millions of integers decoded per second as a performance metric (as we do), but he does not provide details on the data or evaluation method used in his mea-surements.

We measured performance on standard corpora decoding every posting list once, repeating the test to achieve stable timing. While it is easy to reproduce, this method does not account for different term distributions in queries. To account for different frequencies of terms in real queries, one needs a representative query mix. Since GOV2 and Reuters RCV1 are not used in real world search tasks, such a mix is not available.

Creation of a standard benchmark containing several se-quences of integers with distinct but representative statis-tical characteristics would allow meaningful comparisons of different implementations.
We discovered a taxonomy for variable-length integer en-coding formats. This led us to identify some new encodings that offer advantages over previously known formats. We identified the byte-preserving property of encoding formats which makes them particularly amenable to parallel decod-ing with SIMD instructions. The SIMD-based algorithms that we developed outperformed the traditional methods by 300%, and the best previously published methods by over 50%. Furthermore the new group unary formats offer bet-ter compression than the group binary format on all of the corpora tested.

Schlegel et al. [17] also reported success applying SIMD to Elias  X  [9], which is not aligned on byte boundaries at all. Further investigations are needed to see whether SIMD techniques can be similarly applied to other encodings which are not aligned on byte boundaries, such as Simple-9 [2], PForDelta [21] and VSEncoding [18].

We restricted our investigation to integers that can be encoded in four bytes or less. We believe, however, that some of the encodings that we introduced could be easily extended to larger values.

SIMD instructions, a powerful but under-utilized resource, offer the opportunity for significant performance improve-ments in modern search engines.
 We thank Thomas London for suggestions on a draft of this paper and Bill Stasior for supporting this research. [1] V. N. Anh. Impact-Based Document Retrieval . PhD [2] V. N. Anh and A. Moffat. Inverted index compression [3] Apache Software Foundation. Lucene 1.4.3 [4] S. B  X  uttcher, C. L. A. Clarke, and G. V. Cormack. [5] S. B  X  uttcher, C. L. A. Clarke, and G. V. Cormack. [6] W. B. Croft, D. Metzler, and T. Strohman. Search [7] D. Cutting and J. Pedersen. Optimizations for [8] J. Dean. Challenges in building large-scale [9] P. Elias. Universal codeword sets and representations [10] D. A. Grossman. Integrating Structured Data and [11] H. S. Heaps. Storage analysis of a compression coding [12] Intel Corporation. Intel 64 and IA-32 Architectures [13] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [14] MIDI Manufacturers Association. MIDI 1.0 [15] NIST. GOV2 collection. [16] Reuters. Reuters RCV1 Corpus. [17] B. Schlegel, R. Gemulla, and W. Lehner. Fast integer [18] F. Silvestri and R. Venturini. VSEncoding: efficient [19] T. Westmann, D. Kossmann, S. Helmer, and [20] Wikimedia Foundation. Wikipedia database download [21] M. Zukowski, S. H  X eman, N. Nes, and P. A. Boncz.
