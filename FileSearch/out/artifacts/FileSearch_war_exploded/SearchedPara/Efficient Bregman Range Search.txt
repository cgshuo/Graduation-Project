 Range search is a fundamental proximity task at the core of many learning problems. The task of range search is to return all points in a database within a specified distance of a given query. The problem is to do so efficiently, without examining the entire database. Many machine learning algo-rithms require range search. Locally weighted regression and kernel density estimation/regression both require retrieving points in a region around a test point. Neighborhood graphs X  X sed in mani-fold learning, spectral algorithms, semisupervised algorithms, and elsewhere X  X an be built by con-each point. Computing point-correlation statistics, distance-based outliers/anomalies, and intrinsic dimensionality estimates also requires range search.
 A growing body of work uses spatial data structures to accelerate the computation of these and other proximity problems for statistical tasks. This line of techniques, coined  X  n -body methods X  in [11], has showed impressive speedups on a variety of tasks including density estimation [12], gaussian process regression [25], non-parametric classification [17], matrix approximation [14], and kernel summation [15]. These methods achieve speedups by pruning out large portions of the search space with bounds derived from KD or metric trees that are augmented with statistics of the database. Some of these algorithms are direct applications of range search; others rely on very similar pruning techniques. One fairly substantial limitation of these methods is that they all derive bounds from the triangle inequality and thus only work for notions of distance that are metrics.
 The present work is on performing range search efficiently when the notion of dissimilarity is not a metric, but a Bregman divergence . The family of Bregman divergences includes the standard ` 2 2 distance, Mahalanobis distance, KL-divergence, Itakura-Saito divergence, and a variety of matrix dissimilarity measures. We are particularly interested in the KL-divergence, as it is not a metric and is used extensively in machine learning. It appears naturally in document analysis, since documents are often modeled using histograms [22, 5]. It also is used in many vision applications [23], such as content-based image retrieval [24]. Because Bregman divergences can be asymmetric and need not satisfy the triangle inequality, the traditional metric methods cannot be applied.
 In this work we present an algorithm for efficient range search when the notion of dissimilarity is an arbitrary Bregman divergence. These results demonstrate that the basic techniques behind the previously described efficient statistical algorithms can be applied to non-metric dissimilarities including, notably, the KL-divergence. Because of the widespread use of histogram representations, this generalization is important.
 The task of efficient Bregman range search presents a technical challenge. Our algorithm cannot rely on the triangle inequality, so bounds must be derived from geometric properties of Bregman divergences. The algorithm makes use of a simple space decomposition scheme based on Bregman balls [8], but deploying this decomposition for the range search problem is not straightforward. In particular, one of the bounds required results in a non -convex program to be solved, and the other requires comparing two convex bodies. We derive properties of Bregman divergences that imply efficient algorithms for these problems. In this section, we briefly review prior work on Bregman divergences and proximity search. Breg-man divergences originate in [7] and have become common in the machine learning literature, e.g. [3, 4].
 Definition 1. Let f : R D  X  R be strictly convex and differentiable. The Bregman divergence based on f is As can be seen from the definition, a Bregman divergence measures the distance between a func-d ( x,y ) = P i x i log x i y of Bregman divergences.
 Strict convexity of f implies that d f ( x,y )  X  0 , with equality if, and only if, x = y . Though Bregman divergences satisfy this non-negativity property, like metrics, the similarities to metrics end there. In particular, a Bregman divergence need not satisfy the triangle inequality or be symmetric. Bregman divergences do possess several geometric properties related to the convexity of the base function. Most notably, d f ( x,y ) is always convex in x (though not necessarily in y ), implying that the Bregman ball is a convex body.
 Recently, work on a variety of geometric tasks with Bregman divergences has appeared. In [19], geometric properties of Bregman voronoi diagrams are derived. [1] studies core-sets under Bregman divergences and gives a provably correct approximation algorithm for k -median clustering. [13] examines sketching Bregman (and Csisz  X  ar) divergences. [8] describes the Bregman ball tree in the context of nearest neighbor search; we will describe this work further momentarily. As these papers demonstrate, there has been substantial recent interest in developing basic geometric algorithms for Bregman divergences. The present paper contributes an effective algorithm for range search, one of the core problems of computational geometry [2], to this repertoire.
 The Bregman ball tree (BB-tree) was introduced in the context of nearest neighbor (NN) search [8]. Though NN search has a similar flavor to range search, the bounds that suffice for NN search are not sufficient for range search. Thus the utility of the BB-tree for statistical tasks is at present rather seriously limited. Moreover, though the extension of metric trees to range search (and hence to the previously described statistical tasks) is fairly straightforward because of the triangle inequality, the extension of BB-trees is substantially more complex. Several other papers on Bregman proximity search have appeared very recently. Nielsen et al. study some improvements to the BB-tree [21] and develop a related data structure which can be used with symmetrized divergences [20]. Zhang et al. develop extensions of the VA-file and the R-tree for Bregman divergences [26]. These data structures can be adapted to work for Bregman divergences, as the authors of [26] demonstrate, because bounds on the divergence from a query to a rectan-gular cell can be computed cheaply; however this idea appears limited to decomposable Bregman divergences X  X ivergences that decompose into a sum over one-dimensional divergences. 1 Never-theless, these data structures seem practical and effective and it would be interesting to apply them to statistical tasks. 2 The applicability of rectangular cell bounds was independently demonstrated in [9, Chapter 7], where it is mentioned that KD-trees (and relatives) can be used for decomposable Bregman divergences. That chapter also contains theoretical results on the general Bregman range search problem attained by adapting known data structures via the lifting technique (also used in [26] and previously in [19]). In this section, we review the Bregman ball tree data structure and outline the range search algorithm. The search algorithm relies on geometric properties of Bregman divergences, which we derive in section 4.
 The BB-tree is a hierarchical space decomposition based on Bregman balls. It is a binary tree defined over the database such that each level provides a partition of the database points. As the tree is descended, the partition becomes finer and finer. Each node i in the tree owns a subset of the points X i and also defines a Bregman ball B f (  X ,R ) such that X i  X  B f (  X ,R ) . If i is an interior node, it has two children j and k that encapsulate database points X j and X k . Moreover, each point in X i is in exactly one of X j and X k . Each leaf node contains some small number of points and the root node contains the entire database.
 Here we use this simple form of BB-tree, though our results apply to any hierarchical space decom-position based on Bregman balls, such as the more complex tree described in [21].
 To encourage a rapid rate of radius decrease, an effective build algorithm will split a node into two well-separated and compact children. Thus a reasonable method for building BB-trees is to per-form a top-down hierarchical clustering. Since k -means has been generalized to arbitrary Bregman divergences [4], it is a natural choice for a clustering algorithm. 3.1 Search algorithm We now turn to the search algorithm, which uses a branch-and-bound approach. We develop the necessary novel bounding techniques in the next section.
 Suppose we are interested in returning all points within distance  X  of a query q  X  i.e. we hope to retrieve all database points lying inside of B q  X  B f ( q, X  ) . The search algorithm starts at the root node and recursively explores the tree. At a node i , the algorithm compares the node X  X  Bregman ball B x to B q . There are three possible situations. First, if B x is contained in B q , then all x  X  B x are in the range of interest. We can thus stop the recursion and return all the points associated with the node without explicitly computing the divergence to any of them. This type of pruning is called inclusion pruning. Second, if B x  X  B q =  X  , the algorithm can prune out B x and stop the recursion; none of these points are in range. This is exclusion pruning. See figure 1. All performance gains from using the algorithm come from these two types of pruning. The third situation is B x  X  B q 6 =  X  and B x 6 X  B q . In this situation, the algorithm cannot perform any pruning, so recurses on the children of node i . If i is a leaf node, then the algorithm computes the divergence to each database point associated with i and returns those elements within range.
 The two types of pruning X  X nclusion and exclusion X  X ave been applied to a variety of problems with metric and KD-trees, see e.g. [11, 12, 25] and the papers cited previously. Thus though we Figure 1: The two pruning scenarios. The dotted, shaded object is the query range and the other is the Bregman ball associated with a node of the BB-tree. focus on range search, these types of prunings are useful in a broad range of statistical problems. A third type of pruning, approximation pruning, is useful in tasks like kernel density estimation [12]. This type of pruning is another form of inclusion pruning and can be accomplished with the same technique.
 It has been widely observed that the performance of spatial decomposition data structures, degrades with increasing dimensionality. In order to manage high-dimensional datasets, practitioners often use approximate proximity search techniques [8, 10, 17]. In the experiments, we explore one way to use the BB-tree in an approximate fashion.
 Determining whether two Bregman balls intersect, or whether one Bregman ball contains another, is non-trivial. For the range search algorithm to be effective, it must be able to determine these relationships very quickly. In the case of metric balls, these determinations are trivially accom-plished using the triangle inequality. Since we cannot rely on the triangle inequality for an arbitrary Bregman divergence, we must develop novel techniques. In this section we lay out the main technical contribution of the paper. We develop algorithms for determining (1) whether one Bregman ball is contained in another and (2) whether two Bregman balls have non-empty intersection. 4.1 Containment Let B q  X  B f (  X  q ,R q ) and B x  X  B f (  X  x ,R x ) . We wish to evaluate if B x  X  B q . This problem is equivalent to testing whether for all x  X  B x . Simplifying notation, the core problem is determining Unfortunately, this problem is not convex. As is well-known, non-convex problems are in general much more computationally difficult to solve than convex ones. This difficulty is particularly prob-lematic in the case of range search, as the search algorithm will need to solve this problem repeatedly in the course of evaluating a singe range query. Moreover, finding a sub-optimal solution ( i.e. a point x  X  B f (  X ,R ) that is not the max) will render the solution to the range search incorrect. Remarkably, beneath (maxP) lies a geometric structure that allows an efficient solution. We now show the main claim of this section, which implies a simple, efficient algorithm for solving (maxP). We denote the convex conjugate of f by and define x 0  X  X  X  f ( x ) , q 0  X  X  X  f ( q ) , etc. Claim 1. Suppose that the domain of f is C and that B f (  X ,R )  X  relint ( C ) . Furthermore, assume to (maxP). Then x 0 p lies in the set {  X  X  0 + (1  X   X  ) q 0 |  X   X  0 } .
 Proof. Though the program is not concave, the Lagrange dual still provides an upper bound on the optimal solution value (by weak duality). The Lagrangian is where  X   X  0 .
 Differentiating (1) with respect to x and setting it equal to 0, we get which implies that We need to check what type of extrema  X  f ( x p ) = 0 is: Thus for  X  &gt; 1 , the x p defined implicitly in (2) is a maximum. Setting  X   X  X  X   X  1  X   X  gives where  X   X  (  X  X  X  , 0)  X  (1 ,  X  ) ; we restrict attention to  X   X  (1 ,  X  ) since that is where  X  &gt; 1 and hence x p is a maximum. Let x 0  X   X   X  X  0 + (1  X   X  ) q 0 and x  X   X  X  X  f  X  ( x 0  X  ) . The Lagrange dual is Then for any  X   X  (1 ,  X  ) , we have by weak duality. We now show that there is a  X   X  &gt; 1 satisfying d f ( x  X   X  , X  ) = R . One can check that the derivative of d f ( x  X  , X  ) with respect to  X  is Plugging this  X   X  into the dual, we get Combining with (3), we have Finally, since (maxP) is a maximization problem and since x  X   X  is feasible, the previous inequality is actually an equality, giving the theorem.
 Thus determining if B x  X  B q reduces to searching for  X   X  &gt; 1 satisfying and comparing d f ( x  X   X  , X  q ) to R q . Note that there is no obvious upper bound on  X   X  in general, though one may be able to derive such a bound for a particular Bregman divergence. Without such an upper bound, one needs to use a line search method that does not require one, such as Newton X  X  method or the secant method. Both of these line search methods will converge quickly (quadratic in the case of Newton X  X  method, slightly slower in the case of the secant method): since d f ( x  X  , X  x ) is monotonic in  X  , there is a unique root.
 Interestingly, the convex program evaluated in [8] has a similar solution space, which we will again encounter in the next section. 4.2 Non-empty intersection In this section we provide an algorithm for evaluating whether B q  X  B x =  X  . We will need to make use of the Pythagorean theorem , a standard property of Bregman divergences.
 Theorem 1 (Pythagorean) . Let C  X  R D be a convex set and let x  X  C . Then for all z , we have where y  X  argmin y  X  C d f ( y,z ) is the projection of z onto C .
 At first glance, the Pythagorean theorem may appear to be a triangle inequality for Bregman diver-gences. However, the inequality is actually the reverse of the standard triangle inequality and only applies to the very special case when y is the projection of z onto a convex set containing x . We now prove the main claim of this section.
 Claim 2. Suppose that B x  X  B q 6 =  X  . Then there exists a w in such that w  X  B q  X  B x .
 curve .
 Let x be the projection of  X  q onto B x and let q be the projection of  X  x onto B q . Both x and q are on the dual curve (this fact follows from [8, Claim 2]), so we are done if we can show that at least one of them lies in the intersection of B x and B q . Suppose towards contradiction that neither are in the intersection.
 The projection of x onto B q lies on the dual curve between x and  X  y ; thus projecting x onto B q yields q and similarly projecting q onto B x yields x . By the Pythagorean theorem, since q is the projection of x onto B q and since z  X  B q . Similarly, Inserting (5) into (6), we get Rearranging, we get that d f ( q,x ) + d f ( x,q )  X  0 . Thus both d f ( q,x ) = 0 and d f ( x,q ) = 0 , implying that x = q . But since x  X  B x and q  X  B q , we have that x = q  X  B q  X  B q . This is the desired contradiction.
 The proceeding claim yields a simple algorithm for determining whether two balls B x and B q are disjoint: project  X  x onto B q using the line search algorithm discussed previously. The projected and exclusion pruning can be performed. We compare the performance of the search algorithm to standard brute force search on several datasets. We are particularly interested in text applications as histogram representations are com-mon, datasets are often very large, and efficient search is broadly useful. We experimented with the following datasets, many of which are fairly high-dimensional. Figure 2: Approximate search. The y -axis is on a logarithmic scale and is the speedup over brute force search. The x axis is a linear scale and is the average percentage of the points in range returned ( i.e. the average recall). All of our experiments are for the KL-divergence. Although the KL-divergence is widely used, little is known about efficient proximity techniques for it. In contrast, the ` 2 2 and Mahalanobis distances can be handled by metric methods, for which there is a huge literature. Application of the range search algorithm for the KL-divergence raises one technical point: Claim 1 requires that the KL-ball being investigated lies within the domain of the KL-divergence. It is possible that the ball will cross the domain boundary ( x i = 0 ), though we found that this was not a significant issue. When it did occur (which can be checked by evaluating d f (  X ,x  X  ) for large  X  ), we simply did not perform inclusion pruning for that node.
 There are two regimes where range search is particularly useful: when the radius  X  is very small and when it is large. When  X  is small, range search is useful in instance-based learning algorithms like locally weighted regression, which need to retrieve points close to each test point. It is also useful for generating neighborhood graphs. When  X  is large enough that B f ( q, X  ) will contain most of the database, range search is potentially useful for applications like distance-based outlier detection and anomaly detection. We provide experiments for both of these regimes.
 Table 1 shows the results for exact range search. For the small radius experiments,  X  was chosen so that about 20 points would be inside the query ball (on average). On the pubmed datasets, we are getting one to two orders of magnitude speed-up across all dimensionalities. On the rcv datasets, the BB-tree range search algorithm is an order of magnitude faster than brute search except of the the two datasets of highest dimensionality. The algorithm provides a useful speedup on corel, but no speedup on semantic space. We note that the semantic space dataset is both high-dimensional (371 dimensions) and quite small (5k), which makes it very hard for proximity search. The algo-rithm reflects the widely observed phenomenon that the performance of spatial decomposition data structures degrades with dimensionality, but still provides a useful speedup on several moderate-dimensional datasets. For the large radius experiments,  X  was chosen so that all but about 100-300 points would be in range. The results here are more varied than for small  X  , but we are still getting useful speedups across most of the datasets. Interestingly, the amount of speedup seems less dependent of the di-mensionality in comparison to the small  X  experiments.
 Finally, we investigate approximate search, which we consider the most likely use of this algorithm. There are many ways to use the BB-tree in an approximate way. Here, we follow [18] and simply range (perfect precision), but we may not get all of them (less than perfect recall). In instance-based learning algorithms, this loss of recall is often tolerable as long as a reasonable number of points are returned. Thus a practical way to deploy the range search algorithm is to run it until enough points are recovered. In this experiment,  X  was set so that about 50 points would be returned. Figure 2 shows the results.
 These are likely the most relevant results to practical applications. They demonstrate that the pro-posed algorithm provides a speedup of up to four orders of magnitude with a high recall. is an arbitrary Bregman divergence. This is an important step towards generalizing the efficient proximity algorithms from ` 2 (and metrics) to the family of Bregman divergences, but there is plenty more to do. First, it would be interesting to see if the dual-tree approach promoted in [11, 12] and elsewhere can be used with BB-trees. This generalization appears to require more complex bounding techniques than those discussed here. A different research goal is to develop efficient algorithms for proximity search that have rigorous guarantees on run-time; theoretical questions about proximity search with Bregman divergences remain largely open. Finally, the work in this paper provides a foundation for developing efficient statistical algorithms using Bregman divergences; fleshing out the details for a particular application is an interesting direction for future research. [1] Marcel Ackermann and Johannes Bl  X  omer. Coresets and approximate clustering for bregman [2] Pankaj K. Agarwal and Jeff Erickson. Geometric range searching and its relatives. In Advances [3] Katy Azoury and Manfred Warmuth. Relative loss bounds for on-line density estimation with [4] Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. Clustering with [5] David Blei and John Lafferty. A correlated topic model of Science . Annals of Applied Statistics , [6] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. Journal of Machine [7] L.M. Bregman. The relaxation method of finding the common point of convex sets and its [8] Lawrence Cayton. Fast nearest neighbor retrieval for bregman divergences. In Proceedings of [9] Lawrence Cayton. Bregman Proximity Search . PhD thesis, University of California, San Diego, [10] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. Locality-sensitive hash-[11] Alexander Gray and Andrew Moore.  X  X -body X  problems in statistical learning. In Advances [12] Alexander Gray and Andrew Moore. Nonparametric density estimation: Toward computa-[13] Sudipto Guha, Piotr Indyk, and Andrew McGregor. Sketching information divergences. In [14] Michael P. Holmes, Alexander Gray, and Charles Lee Isbell. QUIC-SVD: Fast SVD using [15] Dongryeol Lee and Alexander Gray. Fast high-dimensional kernel summations using the monte [16] D. D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A new benchmark collection for text catego-[17] Ting Liu, Andrew Moore, and Alexander Gray. New algorithms for efficient high-dimensional [18] Ting Liu, Andrew Moore, Alexander Gray, and Ke Yang. An investigation of practical approx-[19] Frank Nielsen, Jean-Daniel Boissonnat, and Richard Nock. On bregman voronoi diagrams. In [20] Frank Nielsen, Paolo Piro, and Michel Barlaud. Bregman vantage point trees for efficient [21] Frank Nielsen, Paolo Piro, and Michel Barlaud. Tailored bregman ball trees for effective [22] Fernando Pereira, Naftali Tishby, and Lillian Lee. Distributional clustering of English words. [23] Jan Puzicha, Joachim Buhmann, Yossi Rubner, and Carlo Tomasi. Empirical evaluation of [24] N. Rasiwasia, P. Moreno, and N. Vasconcelos. Bridging the gap: query by semantic example. [25] Yirong Shen, Andrew Ng, and Matthias Seeger. Fast gaussian process regression using kd-[26] Zhenjie Zhang, Beng Chin Ooi, Srinivasan Parthasarathy, and Anthony Tung. Similarity search
