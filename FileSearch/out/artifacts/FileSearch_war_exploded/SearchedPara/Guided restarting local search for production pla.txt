 1. Introduction
Planning problems exist in many real-world-industrial pro-duction settings and can be solved with a large variety of different approaches. Due to the growing complexity of these real-world planning problems, significant work has been devoted to the automation of planning processes using different kinds of meth-ods. Furthermore, as a result of the increasing deployment of planning and scheduling systems, we often have to deal with very large search spaces, real-time performance demands, and dynamic environments ( Nareyek, 2000 ). A review of the research on integrated process planning and scheduling, along with a discussion on the extent of the applicability of various approaches is presented in Li et al. (2010) and Shaotan et al. (2010) . This paper focuses on the use of specific local search algorithms for real-world production planning based on experiments with real-world data and presents the adapted, local search, evolutionary-based metaheuristics for the real-world industrial problem. It also includes an experimental study of the efficiency of two memetic algorithms, and presents a real-world software implementation for the production planning.
 The motivation of this research is to solve an industrial problem. The goal of our real-world production-planning problem is to find a production plan that satisfies the p roduction time constraints and minimizes the production costs. This involves many specific con-straints that need to be considered. The main problem is the exchange delay, caused by adapting the production lines to different types of products and supplying the appropriate parts. Namely, the manufacturing processes of multiple types of products require many product type. Such a kind of problem can be represented as a job shop scheduling problem ( Brucker, 1998 ).

For solving scheduling problems, many scheduling methods are Guo et al., 2009 ; Chiong and Dhakal, 2009 ; Jarboui et al., 2009 ).
One of the nature-inspired approaches ( Chiong, 2009 ) is the genetic algorithm (GA). A heuristic for the open job shop schedul-ing problem using the GA to minimize makespan was developed by Senthilkumar and Shahabudeen (2006) . On the other hand, a scheduling method based on the GA was developed by considering multiple criteria in Chryssolouris and Subramaniam (2001) . Other implementations of the GA for scheduling can be found in Vazquez and Whitley (2000) .

The advanced local search approaches are usually guided by evolutionary algorithms, simulated annealing, tabu search, min-the successive changing of solutions with moves that alter solutions locally. These moves are defined by the neighborhood, which contains all the solutions that can be reached with one move/change. As the solution quality of the local optima may be unsatisfactory, we need some mechanisms that guide the search to overcome the local optimality. A simple strategy is to iterate/ restart the local search process after a local optimum has been obtained, while more structured ways, e.g., the use of an evolu-tionary search, to overcome the local optimality might be advan-tageous. In Hamiez and Hao (2001) , the problem of sports-league scheduling is addressed, presenting the results achieved by a tabu search method based on a neighborhood of value swaps. Further-more, some works deal with planning systems that are able to incorporate resource reasoning. In Chien et al. (2001) the replan-ning capabilities of local search methods in a continuous planning process clearly outperform a restart strategy. In Engelhardt and
Chien (2001) , learning is used to speed up the search for a plan, where the goal is to find a set of search heuristics that guide the search, while Knight et al. (2001) proposes a technique for aggregating single search moves, so that distant states can be reached more easily.
 memetic algorithms (MAs) ( Ong and Keane, 2004 ). They represent a synergy of the evolutionary approach with separate individual learning or local improvement procedures (local searches) for the problem search. Various MAs were developed ( Hasan et al., 2009 ; and Michalewicz, 2008 ; Ombuki and Ventresca, 2004 )toobtain even better results than the GA for various scheduling applications.
With the use of local search techniques the results were further improved. MAs do not only improve the quality of the solutions, but they also reduce the overall computational time ( Hasan et al., 2009 ). Special attention should also be given to the dynamic nature of the production. Local search is well suited for anytime require-ments because the optimization goal is improved iteratively ( Nareyek, 2000 ). Here, a variety of algorithms have been proposed to solve dynamic optimization problems ( Moser and Chiong, 2010 ; Tfaili and Siarry, 2008 ; Engelhardt and Chien, 2001 ). describe the problem and give its mathematical formulation; in
Section 3, we present preliminary evaluations for the algorithm selection; in Section 4, we describe the evolutionary algorithms; in Section 5, we introduce the used local search procedures; in
Section 6, we show the integration of guiding evolutionary algorithms with local search procedures; in Section 7, we present the experimental environment and the results of the evaluation with the real-world data; in Section 8, we present the implemen-ted application for automatic production planning; and in Section 9, we conclude and propose possible future work. 2. Production planning problem application for planning and monitoring of the production in the company Eta Cerkno d.o.o. The company produces components for domestic appliances, including hot plates, thermostats, and heating elements.
 the production of cooking hot plates. Namely, the production of the components and parts for all the types of plates is more or less similar, but the standard plate models of the current range differ in size (height, diameter), connector type and power character-istics (wattage). There are so many different models due to the various demands of other companies that need those plates for their own cooking appliances. Orders for some particular models vary in quantities and deadlines. Many orders from the same company for different models are also connected with the same deadline. Therefore, their production must be planned very care-fully to fulfill all the demands (quantities and deadlines), to maintain the specified amount of different models in the stock, to optimally occupy their workers, and to efficiently use all the production lines. Furthermore, not all the production lines are equal, since each of them can produce only a few different models. 2.1. General formulation of the problem shop scheduling problem that is NP-hard ( Brucker, 1998 ).
A schedule is an allocation of one or more time intervals for each job to one or more machines. Let us assume we have a finite set of n jobs, where each job consists of a chain of operations. The jobs in our production planning problem correspond to the orders of products. Each order may consist of different type of products.
Every such order is split into more orders that consist of only one type of product. Production process of each product consists of a set of operations. Then, we have a finite set of m machines, where each machine can handle at most one operation at a time.
Machines correspond to production lines. Each operation needs to be processed during an uninterrupted period of a given length on a given production line. The objective is to find a schedule satisfying certain restrictions, while minimizing the overall execution time, i.e., the time for the execution of all the opera-tions. In our planning problem each production line has its own time schedule. Furthermore, each order has its own deadline, which should not be missed, but can be executed anytime before the deadline. Each product can only be made on some production lines and on each of them the execution time is different.
Changing the manufacturing process from one product to another may cause an exchange delay, which also depends on the used production line. There is also a stock. If the product we want to produce is in stock then we use the stock and produce the missing quantity of product. There might be orders with fixed deadlines, which need to be produced on the exact day, but not before.
Nevertheless, the results in this paper do not consider such orders. 2.2. Mathematical formulation of the problem Our production planning problem is defined by a set of orders
J  X f j 1 , j 2 , ... , j n g and a set of production lines M  X f M of order j i and let D i denote its deadline for every i A order j i consists of a set of n i operations O i  X f o i 1
D M are appropriate and the processing time t ik of o ik depends on the production line used. By S [ p ] we denote the number of products p available in the stock. In this paper we consider the problem, where all operations ofaproductaredoneonthesame production line; its processing ti me equals the processing time of the product p on the production line used times the number of p products. In this case the chain of operations is formally unite into one operation. If some products are already in stock, the order processing time is appr opriately shorter.
 Let us assume that N is the number of operations: N  X  A schedule is denoted by where g k 1 is an index on some operation o k A O 1 [[ O n is the production line used to perform the operation o k , for every k and operation o k interchangeably. Furthermore, let F  X  o finishing time of the operation o k and let exd  X  l , o j exchange delay caused by the exchanging of operation o j for the operation o k on the production line M l , for o j , o
The task is to find the schedule that minimizes the number of delayed orders, exchange delays and the time to finish all the orders. The number of delayed orders n orders is n where delay i  X  Let g 1 g 2 denote the schedule g 1 g 2 . Then, the schedule of the production line M l is C  X  " N where d  X  The overall exchange delay t exd is t t  X  max n The sum of squared days of delayed orders n days is n Additionally, some of the orders have to be produced on the exact day: F  X  o ik  X  X  D i , for all o ik A O i and j i A J 0 D J .

The aim is to achieve P n i  X  1 delay i  X  0, but we still allow a schedule with delayed orders. The aim is also to achieve production a line equilibration, which means we would like to minimize the finishing time difference between the lines in M : t  X  min line M l . 3. Preliminary algorithm evaluation
In order to design an efficient memetic optimization algorithm we tested several evolutionary algorithms with respect to their behavior for our industrial case. Before merging the algorithms with case-specific local search procedures we evaluated their performance. The performance check was made with the combi-natorial versions of the basic implementations of the GA ( Goldberg, 1989 ), parameter-less evolutionary search (PLES) ( Papa, 2008 ), and particle swarm optimization (PSO) ( Kennedy et al., 2001 ). Additionally, we also tested the performance of random search (RS) ( Bauer, 1958 ). All algorithms were used without local search procedures to check to see if they are able to tackle the complex search space and various production-based constraints. The control parameters were set and tuned according to the problem type and size, as proposed in Michalewicz and Fogel (2004) and Guo et al. (2009) . The following control para-meters were used for the GA: the population size N P  X  100; the replacement rate r  X  0.2; the crossover probability p c  X  0.7;
No control parameters are needed for the PLES and the RS, while the PSO control parameters were as follows: the number of particles N P  X  100; the inertia coefficient w  X  0.9; the acceleration coefficients c 1  X  0.9, c 2  X  0.9.

Each tested algorithm was run for 1,000,000 evaluations. They tried to find the optimal production plan. Table 1 presents their performance over 30 reruns. For three different tasks (detailed description is in Section 7.4) each algorithm is presented by the best, the mean, the worst result, and its standard deviation. The
PLES has the lowest results for each test case. The GA is slightly worse than the PLES, but with a lower standard deviation. The
PSO has significantly worse results in all three cases, compared to the PLES and GA, while RS returned the worst results of all compared algorithms.

Based on these performances and in accordance with the time constraints for implementing the industrial application we decided to implement, and integrate with local search procedures, only two algorithms, i.e., the GA and the PLES. Their performance with the use of local search was further investigated, as described later in this paper. 4. Search algorithms
Based on preliminary algorithm evaluation we implemented two evolutionary techniques to search for solutions. The first one is the GA, while the second one is the PLES.

If there are several numbers of generations without improve-the control parameters are decreased by 10%, while in the case of the PLES only the population size is reset. In both cases the search is rerun and continues from the currently best solution. The  X  X  X ard X  X  restart occurs if there is no improvement after three consecutive  X  X  X oft X  X  restarts. In this case the whole population is re-initialized (new randomly generated initial population) and all the control parameters are reset to their initial values for both algorithm. The search process starts from the new starting point in the search space.

Such two-staged restarting ensures that in consecutive runs the search can continue close to the optimum, even if there was no improvement for a couple of iterations. This stagnation might be caused by the complex search space. And on the other hand, if the stagnation is too long, then it is very probable, that the local optimum was reached and the search process must be restarted from some other point in the search space. 4.1. Genetic algorithm 1996 ; Goldberg, 1989 ). It is implemented because of the algo-rithm X  X  intrinsic parallelism, which allows a simultaneous search within a broad database of solutions in a search space. The risk of converging to a local optimum exists, but there are several efficient results of the GA for various researches with different optimization problems ( Garbolino and Papa, 2010 ; Korou  X  sic  X 
GA (see Algorithm 1) in order to fully adapt it to the specific problem of production planning. The details of the GA, based on the directions from Michalewicz and Fogel (2004) and B  X  ack et al. (2000) are presented and described in the following sections.
Algorithm 1. GA 4.1.1. Production plan encoding tuples of values. Each tuple (gene) consists of the index of the enumerated order and the production line.
 according to the deadlines into the initial order list. Then, the chromosomes are constructed with variations of the initial list.
Each variation of indexes of orders from the initial list is encoded as a chromosome. A chromosome, which includes the encoded production plan of n orders, is presented in Eq. (1). 4.1.2. Population initialization chromosome the orders are randomly distributed, and also the assigned production line is chosen randomly among the possible lines for each order.
 of orders their values cannot be duplicated and no number can be missed; therefore, both conditions must be considered during the initialization. 4.1.3. Genetic operators found solution by memorizing it. Further, we apply a substitution of the least-fit chromosomes in the population with an equal number of the best-ranked chromosomes in order to ensure that the better solutions have more influence on the new generation.
The ratio of all the chromosomes in the population to be replaced is set by the ratio r .
 positions that store the ordered numbers within the range (order-based crossover). The order-based crossover randomly takes the part of two parents, swaps the genes of the parents in this part and orders the remaining genes in the first parent in accordance with its order in the second parent. During the optimization procedure four types of order-based crossover are used and they are switched every 10 generations. The implemen-ted crossovers are order (OX), cycle (CX), partially mapped (PMX) and PTL crossover. OX, CX and PMX are more precisely explained in Michalewicz and Fogel (2004) , while PTL is explained in
Czogalla and Fink (2008) . In OX, PMX, and PTL the two-point crossover scheme is used, where chromosome mates are chosen randomly.

In the mutation process Mutation( P , p m ) each value of the chromosome mutates with a mutation probability p m . However, since a high mutation rate results in a disruptive random walk through the search space, p m has to be low enough. Three different types of mutation are applied: change of production line  X  a randomly chosen order is moved from one production line to another one (according to the possible production lines for that order); switching of two genes in the chromosome  X  two randomly chosen orders switch their positions, i.e., one is moved back-ward (it is produced later), and the other one is moved forward (it is produced earlier); shifting of a gene into some new position  X  a randomly chosen order is moved to some new position. All orders between the old and new positions of a moved order are shifted. Note that the shifting of a gene into some new position has an effect on a larger part of the chromosome. If an order is moved forward then all the orders between the new and old position are moved backward, but if an order is moved backward then all the orders between the old and new position are moved forward.
 Each new population is generally better than the previous one.
To overcome a possible disruptive effect of mutation at the later stages of the optimization and speed up the convergence to the optimal solution in the final optimization stages, the crossover and mutation probability are decreased with each new restart of the algorithm. Moreover, the lower number of mutated positions in the later stages represents some kind of local search with minor movements around the current solution, i.e., exploitation. 4.2. Parameter-less evolutionary search
The PLES ( Papa, 2008 ) is based on a basic GA, except that it does not need any control parameter, e.g., population size, number of generations, probabilities of crossover and mutation, to be set in advance. They are set virtually, according to the complexity of the problem and according to the statistical proper-ties of the solutions found. In its search process the PLES tries to efficiently explore the whole search space in order to find the optimal solution (see Algorithm 2).

Algorithm 2. PLES 1: SetInitialPopulation( P ) 2: Evaluate( P ) 3: Statistics( P ) 4: while not EndingCondition( ) do 5: ForceBetterSolutions( P ) 6: MoveSolutions( P ) 7: Evaluate( P ) 8: Statistics( P ) 9: end while 4.2.1. Population initialization and termination criterion
The production plan for the PLES was encoded into one chromosome in the same way as for the GA. The initial population P consists of PopSize 0 chromosomes. In each chromosome the orders are randomly distributed, and also the assigned production line is chosen randomly among the possible lines for each order. Since the numbers in the chromosome represent the indexes of orders their values cannot be duplicated and no number can be missed; therefore, both conditions must be considered during the initialization.

The initial population size ( PopSize 0 ) is set according to the following equation: PopSize 0  X  4 where lines i is the number of possible lines of the i -th order. Therefore, it is proportional to the problem X  X  complexity, i.e., to the number of orders to be scheduled inside a plan, and the number of possible combinations (i.e., the number of possible production lines to be placed to) for each order.

The EndingCondition( ) function checks to see if there was no improvement for several generations; then the system is assumed to be in the steady state, and the optimization is then ended. The number of generations depends on the convergence speed of the best solution found. The optimization keeps running while a better solution is found every few generations. But when there is no improvement of the best solution for a few generations ( Limit ), the optimization process stops. The Limit depends on the current size of the population and is defined as follows: Limit  X  10 log 10  X  PopSize i  X  : It is proportional to the population size, where larger populations are used to quickly exploit the search space to find some near-optimal regions, while smaller populations are used to explore this region more precisely. The rough search of the large popula-tion should have enough time, while the fine-tuning within the optimal regions should not take too many generations without improvement. 4.2.2. Variable population size
During the search process, the population size is changed for every few generations ( Limit = 5), based on the average change of the standard deviation of the solutions over a last few genera-tions. When the standard deviation increases, then the population size is decreased, and vice-versa. When the population is shrunk, randomly chosen solutions are removed from the population and when the population is inflated, some randomly chosen solutions are duplicated.
 PopSize i  X  1  X  PopSize i Change where Change i is calculated as Change i  X  StDev i 1  X  StDev i 2 StDev
Moreover, the population size change is limited to 25% per change and is further limited to  X  PopSize 0 = 5 , 1 : 1 PopSize increased standard deviation means that there are more and more chromosomes spread all over the search space, and the algorithm cannot locate any promising region to explore more precisely; therefore the population size needs to be increased to perform a faster exploitation. A smaller standard deviation means that there are many chromosomes inside some optimal region, and less chromosomes are needed to detect small changes in the fitness improvement when fine-tuning is performed. 4.2.3. Force a better solution
In every generation the worse solutions are replaced with better solutions, and up to 25% of the genes in the chromosomes are switched. This operator incorporates the function of elitism, while forcing the replacement of worse solutions with better ones, and the function of crossover, while taking the good solutions and slightly changing them at some positions. 4.2.4. Solution moving
Typically, every chromosome is the subject of a mutation in the basic GA. In the PLES, the mutation is realized by moving some positions in the chromosome according to the population X  X  statistical properties. Namely, the positions are not mutated to some random value, but are moved in a direction towards the best chromosome in the population.

First, only the solutions that were not changed within the  X  X  X orce better X  X  operator are handled here. In other words, the solutions of the previous generation that were better than the average are changed. The number of positions in the chromosome ( Ratio ) to be moved is calculated on the basis of the standard deviation of the solutions in the previous generation and the maximum standard deviation as stated in the following equation:
Ratio where StDev i 1 and StDev i 3 are the standard deviation of the solution fitness of the previous generation, and the standard deviation three generations ago, respectively. Here, the Ratio
A  X  0 , N , and the Ratio positions in the chromosome are selected to be moved. When the standard deviation of the current population is high, i.e., the chromosomes are distributed far from the optimal regions, the number of mutated positions in the chromosome is high so as to allow larger jumps to other, probably better, regions in the search space. When the standard deviation is low, only a few positions are mutated to allow for a finer search inside the regions that are close to the optimal solution. The moves are implemented by changes to the production lines and/ or by switching the positions of the genes in the chromosome (similar to mutation, as presented in Section 4.1.3). 4.2.5. Solution evaluation and statistics
Each population is statistically evaluated. Here the best, the worst, and the average fitness value in the generation are found.
Furthermore, the standard deviation of the fitness values of all solutions in the generation, the maximal standard deviation of the fitness value over all the generations, and the average value of each parameter in the solution are calculated. 4.3. Fitness evaluation
After the variation operators modify the solutions, the modified part of the new population is ready to be evaluated. Here, the evaluator is used to evaluate the solution, p A P , according to the number of delayed orders ( n orders ), the exchange delay times in minutes ( t exchange ), the overall production time in minutes ( t and the sum of squared days of delayed orders ( n days ). The cost function, which is calculated inside Evaluate( P ), was proposed according to company X  X  requirements (production priorities) and is set as follows:
From the cost function it is obvious that the most important item coefficients besides these items make sure that the first two digits of the evaluation function value represent the number of delayed orders, the next three digits represent the exchange delay times in minutes and the last digits represent the influence of t and n 4.4. Ending condition number of generations are without improvement. When that occurs the system is assumed to be in a steady state, and the optimization is ended. The currently best solution is chosen as the final result. 5. Local search maintain a current solution, and explore the search space step by with a better one in the neighborhood, which is used as a current and quickly explore the basin of attraction of optimal solutions, finding an optimum with a high degree of accuracy and within a small number of iterations. In fact, these methods are a key component of metaheuristics that a re the state-of-the-art for many optimization problems ( Garc X   X  a-Mart X   X  nez and Lozano, 2008 ). implemented with four different procedures, which run sequen-tially (see Algorithm 3). The main reason for such an order is that the influence of changes made by, e.g.,  X  X  X tock replacing X  X , are much more degrading to the current solution that the ones done by the  X  X  X imilar product merging X  X . This means that at the begin-ning we allow for major changes, which are then more refined by smaller changes. This enables us to find new solutions, which can be located in some other parts of the solution space and still be better than the current solution. The influence of such a local search, due to the very complex nature of our problem, should be a much improved convergence and quality of the obtained solutions compared to the basic optimization technique (in our case GA and PLES) alone. The examples shown in the following subsections are made up and their only purpose is to show the reader a visualization of the idea behind the presented type of LS.
Algorithm 3. LocalSearch( P ) 5.1. Stock replacing order for the same product is planned for production (see
Algorithm 4). If the order to be produced is delayed, then this order is shifted in front of the order from the stock that was checked. In this case some orders of the same product are possibly moved out of the stock and placed into the production. If the number of delayed orders is increased, then the previously shifted order is moved back to its previous position.

Algorithm 4. StockReplacing( p ) 6: shift order j before order i 7: Evaluate( p ) 8: if number of delayed orders is increased 9: move order j back to its position 10: end if 11: end if 12: end for 13: end if 14: end for 15: end for
The stock replacing example (see Fig. 1 ) is of a one line production process: A and B represent the production of different products and their indexes represent the orders they belong to.
Empty spaces show the unused production capacity due to exchange delays between the different type of products. In reality these exchange delays vary from few minutes up to few hours. In the top row an initial plan is represented with two delayed orders,
B and B 4 . In the middle row the products of the orders B 3 are filled from the stock and the number of delayed orders is reduced (now only B 1 is delayed). In the bottom row the products of the order B 3 and some products of the order B 1 are filled from the stock and the number of delayed orders stays the same (now
B is delayed and B 4 remains delayed). 5.2. Deadline sorting
For each production line, all the orders with delayed deadlines are checked to see if they can be moved before some other order (see Algorithm 5). The delayed order is moved before each of the precedent order, so that it is not delayed anymore, while ensuring that the total number of delayed orders is not increased (see Fig. 2 ).

Algorithm 5. DeadlineSorting( p ) 1: for each production line do 2: for all orders i do 3: if order i is delayed then 4: for all orders j before order i do 5: if deadline of order i after (start of order j  X  6: shift order i before order j 7: Evaluate( p ) 8: if number of delayed orders is increased then 9: move order i back to its position 10: end if 11: end if 12: end for 13: end if 14: end for 15: end for
In the top row of the deadline sorting (see Fig. 2 ) two delayed orders are presented ( B 3 and B 4 ). In the middle row the order B moved after order B 2 , which causes a reduction in the number of delayed orders. In the bottom row the order B 4 is moved before order B 2 and the number of delayed orders stays the same, because B 2 becomes delayed. 5.3. Production line changing
For each production line, all the orders are checked if they can be placed on any other feasible production line (see Algorithm 6). If they can be placed on some other production line, then it is further checked to see if they can be merged with some other order on that new production line (see Fig. 3 ). The movement to another production line should not increase the exchange delay on the new production line.
 Algorithm 6. ProductionLineChanging( p ) 1: for all orders i do 2: if order i type can be placed on more production lines 3: for all feasible lines l do 4: for all orders j on line l do 5: if type of i equal to type of j then 6: move order i after order j on line l 7: Evaluate( p ) 8: if exchange delay is increased then 9: move order i back to its position 10: end if 11: end if 12: end for 13: end for 14: end if 15: end for
In the top row of the production line changing (see Fig. 3 )an initial plan is represented for two production lines. In the middle row order B 3 is successfully moved from line 2 to line 1 and merged with two other similar products B . In the bottom row the moved product D 1 from line 2 to line 1 causes an additional exchange delay, and cannot be merged with any similar products. 5.4. Similar product merging
For each production line there is a check to see if several orders can be merged together (see Fig. 4 and Algorithm 7). This merging is performed in four steps, according to the different properties of the products. First, the orders for the products with the same height are merged, then those with the same size are merged, after that the orders with the same connectors, and finally those with the same power characteristics. The idea of merging is to decrease the production time on each line, due to a decrease of the exchange delay on the line.
 In the top row of the product merging procedure example (see
Fig. 4 ) an initial plan is represented by three different types of the exchange delays (arrows below t he spaces between orders). In the bottom row the products of the orders A and B are merged together, but there is a product C between them that causes longer exchange delays, since the change to/from C takes more time.

Algorithm 7. SimilarProductMerging( p ) 1: for each production line do 2: for four properties p of products do 3: for all orders i do 4: for randomly determined number of orders j after 5: if property p of j equal to property p of i then 6: move order j after order i 7: end if 8: end for 9: Evaluate( p ) 10: if exchange delay is increased then 11: revert last sequence of moving 12: end if 13: end for 14: end for 15: end for 6. Guided restarting local search
Two previously presented evolutionary techniques (the GA and the PLES) were implemented to guide the local search procedures. Additionally, we also implemented the random search algorithm (RLS) to allow local search procedures to restart their search at random positions.

Algorithm 8. GA with local search 6.1. Genetic algorithm of the GA search. Such a MA possesses the ability of the GA to find a good (near-optimal) solution in a reasonable time, and the power of local search to move quickly from the near-optimal solution to the optimal one. The Evaluation( P ) procedure of the Algorithm 1 was replaced with the LocalSearch( P ) procedure (see
Algorithm 8). Evaluation of each solution is now performed inside the local search procedures. 6.2. Parameter-less evolutionary search possesses the ability of the PLES to find a near-optimal solution in a reasonable time, and the power of local search to move quickly towards the optimal one. Again, the Evaluation( P ) procedure of the Algorithm 2 was replaced with the LocalSearch( P ) procedure (see Algorithm 9), and the evaluation of each solution is per-formed inside the local search procedures.

Algorithm 9. PLES with local search 6.3. Random local search space. First, an initial solution for the LS is randomly selected.
Next, the LS is iteratively run; with every next run starting from the previous best solution. This makes sense since the LS is not deterministic. When there is no improvement for three consecu-tive runs, a new random initial solution is selected. This proce-dure is repeated until some ending condition is met. The decision for only three consecutive runs and not more came from some preliminary testing, which showed that with the increased number of consecutive runs the probability of finding new, better solutions decreases exponentially. In fact, if we allow, e.g., 50 consecutive runs, perhaps only one or two new, better solutions were found, but at the expense of a very large number of evaluations. Since the solution space is so complex and we are limited by the number of evaluations, it turned out that it was much better to allow a larger number of restarts instead of allowing a larger number of consecutive runs without an improvement. 7. Performance evaluation 7.1. The experimental environment
The computer platform used to perform the experiments is based on an AMD Athlon II TM 2.9-GHz processor, 4 GB of RAM, and the Microsoft s Windows s 7 operating system. The algo-rithms are implemented in Sun Java 1.6. 7.2. The test cases
The algorithms were tested on three different real order lists from the production company. The first task (Task 1) consists of n  X  409 orders for 176 products, the second task (Task 2) consists of n  X  428 orders for 185 products, while the third task (Task 3) consists of n  X  512 orders for 204 products. The number of orders n represents the problem dimension. In all the tasks m  X  5 produc-tion lines are available. Each product can only be put on certain production lines (depending on the product characteristics).
We made 30 runs with each algorithm for 1,000,000 evalua-tions. The optimization time is approximately 10 min per run. 7.3. Parameter settings
We must point out that during the experimentation we did not fine-tune the parameters of the GA, but only carried out a limited number of experiments to find satisfactory settings.

The following control parameters were used: the population size N P  X  10; the number of generations depends on the progress of the search. After there is no improvement for three generations the search is restarted, and lasts until the number of evaluations limit is reached. With each restart the crossover and mutation probabilities were decreased by 50% of the current value; the replacement rate r  X  0.2; the crossover probability p c  X  0.7;
There was no need to set any parameters for the PLES and RLS algorithms. 7.4. Results
In Tables 2 X 4 , the best, mean, worst, and the standard devia-tions of the solutions are presented. It is obvious that the GA achieved consistently better solutions than the PLES and RLS in all tasks. From the Table 2 , it is clear that the mean solutions obtained by the PLES and RLS consist of 23 delayed orders, while the solutions obtained by the GA consist of 22 delayed orders for
Task 1. Of the evolutionary guided algorithms the PLES found equally good best solution as the GA, but it found a poorer worst solution, which is even worse than the worst solution of the RLS.
The results of the optimization for Task 2 are even more pronounced in favor of the GA than the results of the optimization for Task 1 according to the difference between the best, mean, worst and the standard deviation of the solutions. Here the GA outclasses all the algorithms by a large margin in all aspects. In task 3 we notice the same dominance of the GA as in Task 2. In all three tasks the influence of a local search is observed as a much improved convergence and quality of the obtained solutions compared to the basic optimization technique alone (see Table 1 ).
In Tables 5 X 7 , we can see the overall efficiency, e overall production plan over all three tasks. The efficiency is calculated using the following equation: e where e  X  t production , l represents the actual production time on line l and t interesting information for the user, because it shows the effi-ciency of the work done in production. Here we can see that the usage of the LS, with its specialized routines, has a great impact on making the production lines more efficient. So the influence of using different guided approaches has a really small impact. This shows that the LS alone is doing a really good job.

To show the behavior of the cost function during the search process, we present Figs. 5 X 7 . Here, only the changes when a new, better solution was found are shown. It is clear that the PLES and RLS have a quicker convergence in all the tasks in the early stages of the optimization than the GA, which in all test cases has a longer  X  X  X edge X  X  where there was no improvement for a number of evaluations. This  X  X  X edge X  X  is especially noticeable in Task 3. But after that the GA is able to converge much more efficiently in all tasks.

Although the RLS approach was very competitive compared to the evolutionary approaches, it is obvious that its success was caused by the quality of the LS. Namely, in order to get good results in a relatively short time, as requested by the company, we had to implement very powerful LS. This led to good perfor-mances of all guided approaches; however the GA was the most stable and with the best results in all tasks.

The PLES algorithm found the best solutions in a number of runs, but also several worst solutions. This is caused by the behavior of the PLES algorithm, which is able to quickly find quite good solutions. Due to its statistically based search proce-dures the standard deviation of solutions is also quite high.
It might be questionable as to whether 1,000,000 evaluations are enough for the algorithms to converge. Namely, in some cases the algorithms were just starting to find better solutions and they did not converge completely. But, since the application was intended for industrial use, one of the company X  X  constraints was to find the solution in 10 min. This actually corresponds to 1,000,000 evaluations.

When comparing with the previous approach of the produc-tion planning, the expert X  X  manual plan for those tasks had 42 delayed orders in Task 1, 24 delayed orders in Task 2 and 51 delayed orders in Task 3. This is significantly worse than the results obtained by all of the algorithms. The details are presented in Tables 5 X 7 . 8. Implemented application
Since the application is implemented for a real-world indus-trial problem, we have to consider many other aspects/compo-nents besides the algorithm X  X  performance. The application is used by engineers and other staff on a daily basis. Therefore, it is important to make the optimization program usable and user friendly without having a background knowledge of the optimization algorithms. Since the company requested that the application is accessible from a web browser, we have chosen Java as our implementation language. However, due to intellectual properties issues the application access is restricted and the application is only reachable on some internal level of the company. The application is customized in terms of the profile of the user. One profile has full access to the application and is used by the expert who has control over the database of the coming orders. The other has access to the application with no algorithm control button, where only the current production plan is visible and access to the database of orders is restricted. application (see Fig. 8 ), which can be easily modified to run from any web browser on any platform. The GUI provides a consistent appearance and intuitive controls like push buttons, text fields, sliders, and so forth. The GUI behaves in an understandable and predictable manner.

The GA was implemented to optimize the plan of a given list of orders. The GA proved to be the most stable approach for the guided restarting. It was implemented into the application with the presented, stable set of the algorithm control parameters.
There is no need for a user to set any algorithm control parameter, and the algorithm starts to run after just a press of the start button. Every time a new batch of orders is added to the list, the algorithm is being rerun by the staff. The GUI enables the user to visually attend to the improvement process of the algorithm. By pushing the start button the problem is loaded and the algorithm starts. The current best plan is drawn on the panel in the lower part of the GUI. The plan refreshes every time the algorithm finds a better solution. It stops when the user interrupts the optimiza-tion process by pushing the stop button. During the optimization process, the information about the currently best solutions is displayed in a text list in the upper-right corner of the GUI.
The plan is visualized by production lines with the products shown as colored boxes. Each type of the product box has its own color and the size corresponding to the duration of its production process. The product information window opens with a mouse click on the product box. It displays the production start and end times, the deadline, and other product information. The products, which exceed the deadline, are colored white with only borders in the proper color. To emphasize the fact that the production line exchange delay downtime is minimized similar products are represented with different shades of the same basic color. The slider above the time scale enables the user to visualize up to 31 days of the plan at once. The GUI also allows moving forwards or backwards through the plan. 9. Conclusion and future work
In this paper we have tested a guided local search algorithm on some real-world test cases of a pro duction planning problem. Such a problem is a member of the family of job shop scheduling problems, which are known to be NP-hard. Due to the problem X  X  complexity with many constraints, we have used some specialized local searches and guided them with the GA, PLES and random selection.
We have shown that the use of all the stochastic approaches greatly improved the quality of the production plans in respect to the expert X  X  manual solution. There was also a noticeable difference between the evolutionary and the random guided approach in favor of the evolutionary.
 competitively close to the results of the evolutionary approaches, it was obvious that its success was caused by the quality of the LS.
Namely, to get good results in a relatively short time we had to implement a very powerful LS. This led to good performances for all the guided approaches; however, the GA was the most stable among all. Furthermore, we are planning to analyze the perfor-mance of the PLES to find out possible reasons for its behavior. Its drawback might be in an automatic creation of the population size, which seems to be too large for these kinds of problems. constraints, which are already mentioned in the mathematical formulation of the problem, into the plan construction, i.e., fixed-order production days (the exact day when the order has to be carried out). Another aspect of improvement of the application is to put the results into a database, in order to allow the analysis of the plans and the evaluation of their realization in the production. References
