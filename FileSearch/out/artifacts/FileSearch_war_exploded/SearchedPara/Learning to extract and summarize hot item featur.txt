 Tak-Lam Wong  X  Wai Lam Abstract It is difficult to digest the poorly organized and vast amount of information contained in auction Web sites which are fast changing and highly dynamic. We develop a unified framework which can automatically extract product features and summarize hot item features from multiple auction sites. To deal with the irregularity in the layout format of Web pages and harness the uncertainty involved, we formu-late the tasks of product feature extraction and hot item feature summarization as a single graph labeling problem using conditional random fields. One characteristic of this graphical model is that it can model the inter-dependence between neighbouring tokens in a Web page, tokens in different Web pages, as well as various information such as hot item features across different auction sites. We have conducted extensive experiments on several real-world auction Web sites to demonstrate the effectiveness of our framework.
 Keywords Information extraction  X  Web mining  X  Conditional random fields 1 Introduction The fast development of the Internet technology can effectively vanish the geograph-ical barrier of different communities. Especially, the easily accessible World Wide Web creates a profit-generating market place and convenient shopping environment for sellers and customers respectively. One example is the online auction business such as the Web site ebay.com . In auction sites, sellers can place their items such as a brand new digital camera, or a second hand MP3 player for bidding. Potential buyers can then browse auction sites and bid their favorite items by asking a price that they are willing to pay. Once the bidding period ends, the potential buyer who has asked for the highest bidding price can eventually purchase the item. In the past decade, online auction Web sites have been becoming increasingly popular. According to the press release from ebay.com , they currently have 147 million community members and approximately 50 million items for sale at any given time. 1 There are several reasons accounting for the popularity of the online auction business. One reason is that sellers can reduce their cost by making use of the online auction environment. Since a huge number of visitors browse online auction sites per day, sellers do not need to set up their own Web sites and promote their products. On the other hand, potential buyers can ask the price of items depending on their budgets and have a chance to successfully purchase an item at a lower price if they can bid the item with the right asking price and at the right time.

In every minute, a huge number of sellers and potential buyers are attracted to par-ticipate in online auction Web sites. These auction sites contain a tremendous amount of items from different categories listed for bidding with continuously changing price. Moreover, there exists mutual influence among different participants and items in auction sites. For example, the number of bids for a particular item can be seriously affected if another similar product is listed for bidding with a lower bidding price. As a result, online auction sites become fast changing, highly dynamic, and complex systems. For instance, a digital camera may receive a large number of bids ranging from few US dollars to few hundred US dollars in just 1 or 2 days. Therefore, acquiring the up-to-date and accurate information in auction Web sites offers many benefits to both sellers and potential buyers.

Though online auction Web sites bring much benefit and convenience to sellers and potential buyers, the massive amount of continuously changing information contained in auction sites poses a serious difficulty for sellers and potential buyers to digest and analyze. For example, when a seller intends to place an item for bidding, he/she is required to set a start bidding price. Some sellers may set the start bidding price with their subjective expectations. This can easily result in either that the start bidding price is set too high and hence the chance of the item being sold may be very slim, or that the start bidding price is set too low and hence the return may decrease. Some other sellers may manually analyze the items currently listed for bidding and their price before setting the start bidding price. However, this manual process for analyz-ing the vast amount of information is time-consuming and tedious. Besides sellers, it is also beneficial for potential buyers to obtain up-to-date, detailed, and accurate information to assist the decision. For example, before bidding for a particular item, the potential buyer may study the description of the item, and other similar items listed for bidding. After certain investigation, he/she can then decide on the amount of money for this bid. Due to the highly dynamic and fast changing nature of online auction Web sites, rapid decision is essential. If a potential buyer spends too long time for analysis, he/she may either lose the opportunity for successfully buying the item, or need to pay a higher cost.

We develop a framework which can automatically extract and summarize hot item features across different auction Web sites to assist sellers and buyers in decision making. One objective of our framework is to characterize the popularity of an item listed for bidding. Intuitively, a hot item is the item which attracts many potential buyers for bidding. However, such measurement of popularity, which only considers the number of bids, for an item is not appropriate. As described before, the number of bids of a particular item can be severely affected by other similar items listed for bidding but with a lower price. Although the former item attracts less interest from potential buyers, both of them should be considered as hot items because they are actually similar to each other. Another reason is that potential buyers like to bid a listed item just before the end of the bidding period. According to Auction Software Review, about one-fourth of the items receive only one bid at the end of the auction period and potential buyers like to ask the bid in the last minute [ 2 ]. Consequently, if hot items are only characterized by the number of bids, it is likely that some actual hot items will be ignored. Therefore, we characterize the popularity based on product features of the items. For example, a product feature of a digital camera can be  X 4 megapixel resolution X . Normally, items listed for bidding come with descriptions pro-vided by sellers. Figure 1 shows a sample of Web page containing an item collected from an auction site. It contains a list describing the features of the digital camera. Our approach can automatically discover product features from different descriptions provided by sellers. However, the diversified format of descriptions can range from regular format such as tables to unstructured free texts, making the extraction task difficult. For example, Fig. 2 depicts another Web page collected from the auction site same as the one in Fig. 1 . Though these two Web pages are about digital camera, the layout format of descriptions provided by sellers is very different.

To deal with the irregularity of Web pages and harness the uncertainty involved, we formulate the product feature extraction task and the hot item feature summari-zation task as a single graph labeling problem using conditional random fields [ 19 ]. One characteristic of this graphical model is that it can model the inter-dependence between neighbouring tokens in the Web page, as well as tokens in different Web pages. As a result, Web pages collected from different Web sites can be considered under a coherent model improving the extraction quality. This also leads to another characteristic that various information such as the hot item feature information can be easily integrated in the graphical model structure. We have conducted extensive experiments on several real-world auction Web sites to demonstrate the effectiveness of our framework.

We have reported a preliminary investigation on the problem of extracting and summarizing hot item features across different auction sites in our previous work [ 30 ]. The work described in this paper is substantially extended from our previous work.
The remaining paper is organized as follows. Section 2 presents some existing works related to hot item features extraction and summarization. Next, an overview of our framework is described in Sect. 3 .Section 4 describes the modeling and the learning algorithm employed in our framework. Section 5 describes our text fragment iden-tification method which segments a HTML document into a set of text fragments. Section 6 presents the experimental results on hot item feature extraction and sum-marization. Finally, we draw the conclusions and present several directions of future work in Sect. 7 . 2 Related work Ghani and Simmons propose a closely related work on end-price prediction from auction Web sites [ 13 , 14 ]. They predict the price of items at the end of the bidding period using four different kinds of features. The first kind of features is related to sellers such as the seller rating. The second kind of features is related to auction such as the first bid price of the item. The third kind of features is related to the item. This kind of features consists of the indicators of the occurrence of certain phrases such as  X  X ike new X  in the title. The last kind of features is called temporal features which are obtained from the recent history of the same item. They compared different machine learning techniques such as neural network and decision tree for the end-price predic-tion based on these features. Our proposed framework is different from their work in several aspects. First, the objective of their approach is to predict end-price whereas our framework is to extract and summarize hot item features. Second, their approach assumes that each item placed for bidding is independent. However, as mentioned in Sect. 1 , items actually have a substantial level of mutual influences.

Hui and Liu [ 15 ] investigate the task of summarizing customer reviews posted on the Web sites. Their work is similar to sentiment classification [ 32 ]. Their objective is to classify sentences with subjective orientation. They make use of opinion terms such as  X  X refect X ,  X  X ood X  as clues and extract frequent features of the product from reviews. Popescu and Etzioi [ 26 ] conduct research on this problem. They first make use of the extraction system called KnowItAll [ 10 ] to extract explicit features of the product. Next the extracted explicit features are utilized to identify the opinion or orientation from reviews. Both of these two methods apply linguistic techniques and focus on sentences which are largely grammatical. In contrast, the proposed work in this paper is to discover product features of hot items from descriptions provided by sellers in auction Web sites. Such descriptions can vary largely in layout format ranging from rigid tables to free texts. Our work is also different from the research work on text summarization [ 22 ], whose objective is to produce text summary from text documents, and the work of summarizing databases [ 27 ], which is basically to mine the frequent item set in a transaction database.

For semi-structured documents such as Web pages, different information extrac-tion techniques have been proposed [ 3 , 11 , 18 , 25 ]. Wrapper is a popular information extraction method and it usually consists of a set of extraction rules which can identify attributes of interest from Web documents. Several machine learning methods have been developed for automatic wrapper generation by learning extraction models from methods suffer from one common shortcoming that the learned wrapper can only extract the attributes specified in training examples. For example, if we just annotate the start time, end time, location, and speaker in the set of training examples in the seminar announcement domain, the learned wrapper can only extract these four attri-butes. Some other useful information such as the seminar title will not be extracted. In our previous work, we extend the traditional information extraction technique for discovering new attributes in Web pages [ 29 ]. It should be noted that our objective of hot item feature extraction and summarization is different from the objective of ordinary information extraction since our goal is not only to extract product features, but also to generate the summary for hot items across different auction Web sites. Some techniques have been developed for fully automatic information extraction from Web pages without using any training examples. IEPAD [ 5 ]andMDR[ 21 ] make use of the repeated patterns in a Web page for extraction. A recent method proposed by Li et al. [ 20 ] represents a Web document as a DOM structure and dis-cover the data schema by detecting the largest common subtrees. These method can only handle Web pages containing multiple records. However, a Web page normally consists of one item for bidding in auction sites. Roadrunner does not require Web pages contain multiple records [ 9 ]. However, Web pages are required to have similar layout format and this is rare in auction Web sites.

Recently, various techniques have been proposed for collectively conducting infor-mation extraction and data mining [ 23 ]. For example, Wellner et al. propose an ap-proach for extracting different fields in citation and solving the citation matching problem using conditional random fields [ 28 ]. McCallum and Wellner also propose an approach to extracting proper nouns and linking the extracted proper nouns using a single model [ 24 ]. Bunescu and Mooney study the use of relational Markov networks to collectively extract information from documents [ 4 ]. One major difference between these methods and our approach is that our approach considers Web pages collected from different auction Web sites rather than documents from a single information source. 3 Overview of our framework We develop a unified framework which can extract product features and discover hot item features from multiple auction Web sites collectively. Our model is designed based on the undirected graphical model known as Conditional Random Fields (CRF) [ 19 ] which is a discriminative probabilistic model. A strength of CRF is that it can model the inter-dependence between different entities without know-ing the actual casualty between them. Unlike naive Bayesian approach, another advantage of CRF is that it allows the use of dependent and overlapping features. Moreover, many recent works have been conducted and show that discriminative approaches generally achieve a better performance compared with generative approaches.

A Web page can be considered as a set of text fragments which correspond to sentences, rows in a table, or items in a list, etc. Each of these text fragments can be regarded as a sequence of tokens. In information extraction, tokens are labeled with different tags. For example, we can design tag labels such as  X  X roduct feature X , and  X  X ormal text X , etc to denote different semantic meaning of tokens. The problem is then reduced to assigning the appropriate tag for each token. An undirected graph can be automatically constructed representing the conditional dependence among the observation and the tag labels of tokens. Next, the labeling can be accomplished by conducting inference on the graph structure. Table 1 shows a sample of text fragment from a Web page of an auction site. In this example, the first row is the original text fragment extracted from the Web page and the second row is the tag labels of tokens where f and n represent the product feature and the normal text respectively. To extract the product features, the goal is to identify the tags of tokens.
One challenging issue of this problem is that descriptions provided by different sellers are organized in different formats as shown in Figs. 1 and 2 . Therefore, it poses a difficulty for extracting accurate text fragments related to product features. Another characteristic is that product features to be extracted are not specified in advance. For example, the product feature  X  X ovie mode X  in the example depicted in Table 1 may be a previously unseen product feature and only be found in very few items listed for bidding. We tackle this problem by considering the clues embodied in the layout format of the individual Web page. For example, the Web page shown in Fig. 1 consists of some product features arranged in a quite regular format. This regularity provides a very useful information for solving the extraction task. However, as mentioned before, since the format of descriptions provided by sellers varies greatly, such clue cannot be obtained from other Web pages or obtained through training. We solve this problem by designing an Expectation-Maximization (EM) based training algorithm to discover new product features. The idea of this EM based training algorithm is that we treat tokens to be labeled in the testing set as unlabeled data characterizing by their content and context characteristics. For instance, the word  X  X ovie X  and  X  X ode X  are the content characteristics. The context characteristics include layout format such as boldness, font size, or capitalization of tokens in Web pages. For example, the text fragment  X 4.0 megapixel CCD ...  X  X sformattedinalistinFig. 1 . Our EM based train-ing algorithm can make use of both content and context characteristics to discover previously unseen product features from different Web pages.

Another property of our undirected graphical model is that besides modeling the conditional dependence between the observation and the tag labels of tokens, it can also be easily integrated with various information such as the hot item feature infor-mation in the auction Web sites. This is achieved by representing such information with nodes in the graph. A single graph can be automatically constructed. The product feature extraction and the hot item feature mining can then be carried out under this unified model by collaboratively conducting inference on this automatically generated graphical structure. The advantage of using a unified model for the two tasks is that it allows tight interaction between the two tasks removing the unnecessary boundary between them. A global solution can then be obtained by optimizing the quality of both tasks and at the same time eliminating conflicts.

Figure 3 shows the system overview of our framework which consists of two major components. Our framework takes a set of Web pages, such as the ones in Figs. 1 and 2 , collected from different auction sites as the input. Each of these Web pages is firstly segmented into a set of text fragments by the first component, namely, the text fragment identification. One sample of the text fragment identified is shown in Table 1 . Each text fragment corresponds to a line, a row in a table, an item in a list, etc in Web pages. This component utilizes the Document Object Model 2 (DOM) structure representation of a HTML document to identify such text fragments from Web pages.

After obtaining the text fragments from Web pages, the second component, namely, the hot item feature extraction and summarization, is invoked. The objective of this component is to extract product features from Web pages and produce a summary of hot item features. Among the text fragments identified from Web pages by the first component, some of them may contain tokens about product features, whereas some of them may contain uninformative or unrelated texts. We employ the graphical model and technique described above to identify hot item features. A summary is then generated for assisting users in decision making. Table 2 shows samples of text fragments contained in the summary produced by our framework in the digital camera domain. 4 Graphical model for hot item feature mining 4.1 Model formulation In CRF, each node in the graph represents a variable and each edge represents the inter-dependence between the connected variables. Suppose we collect a set of Web pages from auction Web sites and we wish to discover hot item features. Figure 4 shows a simplified CRF model automatically constructed for the hot item feature mining task. The size of the graph is much larger when dealing with real data. There are two kinds of nodes. Shaded nodes represent observable variables while unshaded nodes represent unobservable variables. Suppose we have a collection of Web pages P . As mentioned above, a Web page, M  X  P , can be regarded as a set of text frag-ments denoted by S M and each text fragment is considered as a sequence of tokens. For a particular sequence A  X  S M , each token is actually composed of two kinds of information. The first kind of information is the observation of tokens such as their content characteristics and context characteristics. This information can be observed and is represented by the observable variable X A . The second kind of information is the labeling information of the tokens. In product feature extraction, each token is labeled with either product feature or normal text . This information is hidden and is represented by the unobservable variable Y A .Noticethat X A and Y A actually repre-sent a sequence of variables X A i and Y A i respectively where 0 &lt; i &lt; L and L denotes the number of tokens in the sequence A . A node denoted by W A represents the X
A ,and W A asshowninFig. 4 since the tag label of each token is inter-dependent with the tag labels of neighbouring tokens, the observation of the sequence, and product features. There is another unobservable node called Z A which refers to the hot item features found in the sequence. An observable variable denoted by  X  M in Fig. 4 rep-resents the number of bids for the item listed in page M . In page M , Z A is connected with W A and X A because a hot item feature is related to the observation and the product feature found in the sequence. Z A is also connected to  X  M because a hot item feature is inter-dependent with the number of bids of the item listed in page M .For example, it is likely that the product feature is a hot item feature if the item receives a high number of bids from potential buyers. In Fig. 4 , the sequence B , C  X  S N are collected from the same page N  X  P and N = M . As mentioned in Sect. 1 , a hot item is not only related to its number of bids, but also related to other items listed for bidding. Therefore, X B and Z B ,aswellas X C and Z C in the page N are also connected to Z A in the page M .

Once the undirected graph is constructed, the conditional probability of a partic-ular configuration of hidden variables, given the values of all observed variables can be written as follows: where x and y are the set of observable variables and the set of unobservable variables respectively, C ( x , y ) refers to the set of cliques of the graph. A clique is defined as the maximal complete subgraph. ( C ( x , y )) refers to the clique potential for C ( x , y ) . Z is called the partition function and defined as:
We define the clique potential as a linear exponential function as follows: where f i ( x , y ) and  X  i are the i th binary feature and the associated weight respectively. For example, f i ( x , y ) equals to one if the underlying token is  X  X esolution X  and the tag label is product feature and equals to zero otherwise in the digital camera domain. Hence, Equation 1 can be written as follows: Given the set of  X  i , one can find the optimal labeling of unobserved variables of the graph via conducting inference. The graph typically consists of a large number of combination for the labels of all unobservable variables. Hence, direct computation of the probability of a particular labeling of unobservable variables is infeasible. The inference can be carried out by the message passing algorithm, also known as the sum-product algorithm, by transforming the graph into junction tree or factor graph [ 16 ]. By finding the configuration of hidden variables achieving the highest conditional probability stated in Eq. 1 , hot item features can then be discovered from these Web pages. 4.2 Adaptive training of CRF Learning in CRF refers to estimating the value of the weight  X  i associated with each f in Eq. 4 . Suppose we have a set of training examples denoted by Tra for which the actual labels of variables are known. We define the log likelihood function as follows: ing example respectively. Maximum likelihood approach aims at finding the set of  X  i which maximizes Eq. 5 . It can be shown that Eq. 5 is convex and achieves maximum when the following condition holds:
Therefore, one can obtain the set of  X  i achieving the maximum of Eq. 5 by using iterative methods such as conjugate gradient methods or the voted perceptron algo-rithm [ 7 ]. In particular, Fig. 5 shows the outline of the voted perceptron algorithm for learning the parameters. In essence, the voted perceptron algorithm estimates the weight by iteratively minimizing the following expression: where  X  y ( j ) is the predicted labeling using the current weighting.

However, recall that one objective of our framework is to extract previously unseen product features contained in Web pages. To achieve this, we exploit the clue embodied in the context characteristic such as the layout format of the extracted data. How-ever, the extracted data cannot be directly used because they involve uncertainty. To tackle this problem, we treat the extracted data as unlabeled data and develop an expectation-maximization (EM)-based voted perceptron algorithm as shown in Fig. 6 . In the E-step of our algorithm, we estimate the probability of the labeling of unobservable variables. In the M-step, we employ the voted perceptron algorithm augmented with the following weight updating function:
Compared with the algorithm stated in Fig. 5 , our EM based voted perceptron algorithm estimates the weight by iteratively diminishing the following expression: the data set is sufficiently large. 5 Text fragment identification As mentioned in Sect. 3 , the objective of the text fragment identification component is to segment a HTML document into a set of text fragments. Each text fragment corre-sponds to a line, a row in a table, an item in a list, etc. To achieve this, we first make use of the DOM structure representation of the HTML document to identify a set of text fragment candidates. However, some of the text fragment candidates identified may contain information related to item features, while some of them may contain uninfor-mative texts such as advertisement, navigation menu, footer, or copyright statement. We develop a method for filtering those uninformative text fragment candidates and the remaining ones become the text fragments for conducting inference described in Sect. 4 .

Suppose there are two Web pages originated from the same auction site, but coming from two different domains. We observe that uninformative texts normally appear in both pages, while those text fragments about item features appear in only one page. For instance, a text fragment coming from a Web page in the digital camera domain may contain tokens about the resolution of a particular digital camera. It is not likely that such tokens also appear in Web pages coming from the MP3 player domain. This provides a very useful clue for filtering those uninformative text fragments in Web pages.

Figure 7 shows a portion of the DOM structure representation for the Web page depicted in Fig. 1 . A DOM structure is an ordered tree consisting of two kinds of nodes. The first kind of nodes, namely, element nodes, contains the HTML tag information of the Web page. These nodes are labeled with HTML tag names such as  X  &lt; table &gt;  X  and  X  &lt; li &gt;  X . The second kind of nodes is called text nodes which are labeled with the texts displayed in browsers. For example, Fig. 7 contains text nodes labeled with  X 4.0-megapixel CCD ...  X  and  X 1.8-inch color LCD monitor X . We identify a set of HTML tags which are useful for segmenting the HTML document into text fragments. Table 3 shows the HTML tags used and their functions. For example, the HTML tag  X  &lt; BR &gt;  X  denotes a new line in an HTML document and can be used to signal the beginning of a new text fragment. Our approach for identifying text fragment candidates first traverses the DOM structure of the HTML document in a depth-first manner. If the visited node belongs to one of the tags in Table 3 ,itsignalsthestartofanewtext fragment. After the traversal, we can obtain a set of text fragment candidates rep-resenting the document. For example,  X  X asic Features X ,  X 4.0-megapixel CCD ...  X ,  X 1.8-inch color LCD monitor X ,  X  X opyright  X  1995 X 2006 eBay Inc. X  are samples of text fragments identified from the Web page shown in Fig. 1 .

Each text fragment candidate can be represented by a set of distinct tokens. Let t p ) denote the i th text fragment candidate in the Web page p . We define the similarity between the i th text fragment from page p and the j th text fragment from page q as follows: where | t | denotes the number of elements in the set t . The outline of text fragment identification algorithm is depicted in Fig. 8 . Our method first collects Web pages from two different domains in the same auction site. This can be easily achieved by making use of the search engines provided by auction sites and querying with different key-words such as  X  X igital Camera X  and  X  X P3 Player X . Let p and q be the two Web pages collected in different domains. These Web pages can then be represented by DOM structures. Next, sets of text fragment candidates representing p and q respectively can be obtained as described above. Recall that normally uninformative texts repeat in both pages, while those text fragments about item features only appear in one page. Therefore, we compute the similarity between all candidates obtained from p and q . If the similarity is less than a predefined threshold  X  , they will be removed from the sets of text fragment candidates. Finally, the remaining candidates are those dissimilar text fragments and they are selected for discovering hot item features as described in Sect. 4 . 6 Experimental results We conducted extensive experiments on three real-world auction Web sites in two domains, namely, the digital camera domain and the MP3 player domain, to demon-strate the effectiveness of our framework. The three auction Web sites are www.ebay. com , auctions.yahoo.com ,and www.ubid.com . In each domain, we collected 50 Web pages from each of the auction sites for evaluation. Each Web page contains an item listed for bidding and the remaining bidding period is less than an hour. We conducted two sets of experiments to evaluate our approach to product feature extraction and hot item feature summarization.

We manually annotated the product features in the Web pages. These annotated product features were served as the gold standard in our evaluation. In each domain, we randomly chose 5 pages whose items received at least one bid from potential buyers in each of the Web sites (a total of 15 Web pages) to produce the set of train-ing examples to train our model as described in Sect. 4.2 . The trained model is then applied to the remaining Web pages to extract product features of items. Recall(R) , precision(P) ,and F-measure(F) are adopted as evaluation metrics. Recall is defined as the number of items for which the system correctly identified divided by the total number of actual items. Precision is defined as the number of items for which the sys-tem correctly identified divided by the total number of items it extracts. F-measure is defined as 2 PR /( P + R ) .Table 4 depicts the extraction performance of our approach. Our approach achieves about 81 and 75% for average precision and recall respectively in the digital camera domain and achieves 76 and 74% for average precision and recall respectively in the MP3 player domain. This shows that our approach can effectively leverage the content and context characteristics to extract product features.
Next, we employ our framework to generate the summary of hot item features in the digital camera and MP3 player domains. To increase comprehensibility, we gener-ate the summary by outputting text fragments containing hot item features instead of individual token. Table 5 shows some text fragments extracted. We manually investi-gate items listed for bidding in auction Web sites and find that over 70% of the items receiving at least one bid from potential buyers contain at least three of the reported product features mentioned in the summary. This demonstrates that the summary generated is very helpful for the auction Web site participants. 7 Conclusions and future work We have developed a unified framework which is able to extract and summarize hot item features across different auction Web sites. Our system can assist sellers and potential buyers in decision making. One challenge of this problem is to extract infor-mation from product descriptions whose layout format vary greatly among different sellers. We formulate the problem as a single graph labeling problem employing conditional random fields. The solution is then obtained by conducting inference on the graph. One characteristic of our framework is to extract previously unseen product features by making use of the clues embodied in the layout format of text fragments. We have designed an EM based voted perceptron algorithm to conduct training. Extensive experiments from several real-world auction Web sites have been conducted to demonstrate the effectiveness of our framework.

We intend to extend our framework in several directions. One possible extension is to incorporate the prior knowledge of users into our framework. Very often, users may have some prior knowledge about the domain in advance. For example, they may know that the resolution and optical zoom are some common features of digital cameras. Such domain knowledge may be represented in the form of an ontology, and incorporated in the training and inference in our framework. We intend to develop a mechanism which allows users to incorporate domain knowledge easily. Another possible direction is to apply our framework to other data mining problems. The undirected graphical model described in this paper is a general model capturing the dependence between different variables. It can be applied to some other data mining problems such as important product feature mining. Normally, in an online vendor Web site, each product has a list of features. Some of these features can be regarded as important features because they are some special and characteristics of the particular product. Important feature mining aims at extracting product features from different online vendor Web sites and identifying those important features.
 References
