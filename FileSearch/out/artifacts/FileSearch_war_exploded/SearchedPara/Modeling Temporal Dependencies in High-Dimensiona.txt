 Nicolas Boulanger-Lewandowski boulanni@iro.umontreal.ca Yoshua Bengio bengioy@iro.umontreal.ca Pascal Vincent vincentp@iro.umontreal.ca Modeling sequences is an important area of machine learning since many naturally occurring phenomena such as music, speech, or human motion are inher-ently sequential. Complex sequences are non-local in that the impact of a factor localized in time can be delayed by an arbitrarily long time-lag. For example, musical patterns or themes appearing at the beginning of a piece are often repeated towards the end. Recur-rent neural networks (RNN) (Rumelhart et al., 1986) incorporate an internal memory that can, in princi-ple, summarize the entire sequence history. This prop-erty makes them well suited to represent long-term de-pendencies, but it is nevertheless a challenge to train them efficiently by gradient-based optimization (Ben-gio et al., 1994). It was recently shown that training RNNs via Hessian-free (HF) optimization could help reduce these difficulties (Martens &amp; Sutskever, 2011). Many sequences of interest are over high-dimensional objects, such as images in video, short-term spectra in audio music, tuples of notes in musical scores, or words in text. In these cases, simply predicting the expected value at the next time step given the observed values of the previous time steps is not satisfying. With such high-dimensional objects at each time step, the condi-tional distribution is very often multi-modal, and we would strongly prefer our models of such sequences to predict the conditional distribution of the next time step given previous time steps. For the case of poly-phonic music, it is obvious that the occurrence of a par-ticular note at a particular time modifies considerably the probability with which other notes may occur at the same time. In other words, notes appear together in correlated patterns, or simultaneities , that cannot be conveniently described by a typical RNN architec-ture designed for the multiclass classification task, for example, because enumerating all configurations of the variable to predict would be very expensive. This dif-ficulty motivates energy-based models which allow us to express the negative log-likelihood of a given config-uration by an arbitrary energy function, among which the restricted Boltzmann machine (RBM) (Smolensky, 1986) has become notorious.
 In this context, we wish to exploit the ability of RBMs to represent a complicated distribution for each time step, with parameters that depend on the previous ones, an idea first put forward with the so-called tem-poral RBM (Taylor et al., 2007; Sutskever &amp; Hin-ton, 2007) which is trained via a heuristic procedure. Combining the desirable characteristics of RNNs and RBMs has proven to be non-trivial. The recurrent temporal RBM (RTRBM) (Sutskever et al., 2008) is a similar model that allows for exact inference and efficient training by contrastive divergence (CD). De-spite its simplicity, this model successfully accounts for several interesting sequences. A similar architec-ture based on the echo state network was also recently developed (Schrauwen &amp; Buesing, 2009). In this work, we demonstrate that the RTRBM outperforms many traditional models of polyphonic music, and we intro-duce a generalization of the RTRBM, called the RNN-RBM, that allows more freedom to describe the tem-poral dependencies involved.
 More precisely, we will consider sequences of symbolic music, i.e. represented by the explicit timing, pitch, velocity and instrumental information typically con-tained in a score or a MIDI file rather than more complex, acoustically rich audio signals. Musical mod-els mostly focus on the basic components of western music, harmony and rhythm, and are trained to pre-dict the pattern of notes (simultaneities) to be played together in the next time interval, given the previ-ous ones. Two elements characterize the qualitative performance of a model: temporal dependencies and chord conditional distributions. While most existing models output only monophonic notes along with pre-defined chords or other reduced-dimensionality repre-sentation (e.g. Mozer, 1994; Eck &amp; Schmidhuber, 2002; Paiement et al., 2009), we aim to model unconstrained polyphonic music in the piano-roll representation, i.e. as a binary matrix specifying precisely which notes oc-cur at each time step. Despite ignoring dynamics and other score annotations, this task represents a well-defined framework to improve machine learning algo-rithms and is directly applicable to polyphonic tran-scription.
 The objective of polyphonic transcription is to deter-mine the underlying notes of a polyphonic audio signal without access to its score. Human experts approach this difficult problem by giving importance to what they expect to hear rather than exclusively to what is present in the actual signal. Most existing transcrip-tion algorithms are frame-based and rely exclusively on the audio signal, even though some approaches employ rudimentary musicological constraints (e.g. Li &amp; Wang, 2007). It has long been known that, in the same way that natural language models tremen-dously improve the performance of speech recogni-tion systems, musical language models can improve purely auditive approaches to music information re-trieval (Cemgil, 2004). However, combining these two sources of information is not trivial, with the result that temporal smoothing with an HMM is often the only post-processing involved in state-of-the-art tran-scription (Nam et al., 2011). We will show how to en-rich an arbitrary transcription algorithm (under basic assumptions) to include the advice of an expert trained on symbolic sequences. Using our hybrid approach, we can improve transcription accuracy (Bay et al., 2009) much more than the popular HMM approach.
 The remainder of the paper is organized as follows. In Sections 2, 3 and 4 we introduce the RBM, the RTRBM and the RNN-RBM architectures. In Sec-tion 5 we validate our model on benchmark datasets. In Section 6 we present our results on musical se-quences, and we detail our hybrid transcription ap-proach in Section 7. An RBM is an energy-based model where the joint probability of a given configuration of the visible vec-tor v (inputs) and the hidden vector h is: where b v , b h and W are the model parameters and Z is the usually intractable partition function. When the vector v is given, the hidden units h i are conditionally independent of one another, and vice versa: where  X  ( x )  X  (1 + e  X  x )  X  1 is the element-wise logistic sigmoid function. The marginalized probability of v is related to the free-energy F ( v ) by P ( v )  X  e  X  F ( v ) Inference in RBMs consists of sampling the h i given v (or the v j given h ) according to their conditional Bernoulli distribution (eq. 2). Sampling v from the RBM can be performed efficiently by block Gibbs sam-pling, i.e. by performing k alternating steps of sam-pling h | v and v | h . The gradient of the negative log-likelihood of an input vector v ( l ) involves two opposing terms, called the positive and negative phase: where  X   X  { b v ,b h ,W } . The second term can be esti-mated by a single sample v ( l )  X  obtained from a k -step Gibbs chain starting at v ( l ) : resulting in the well-known contrastive divergence (CD k ) algorithm (Hinton, 2002).
 The neural autoregressive distribution estimator (NADE) (Larochelle &amp; Murray, 2011) is a tractable model inspired by the RBM and specializing (with ty-ing constraints) an earlier model for the joint distribu-tion of high-dimensional variables (Bengio &amp; Bengio, 2000). NADE is similar to a fully visible sigmoid be-lief network in that the conditional probability distri-bution of a visible unit v j is expressed as a nonlinear function of v k ,  X  k &lt; j . In the following discussion, one can substitute RBMs with NADEs by replacing equa-tion (6) with the exact gradient defined in (Larochelle &amp; Murray, 2011) where the biases are set to b = v ( t ) c = v ( t ) h . The advantages of a tractable distribution estimator will become obvious when used as part of sequential models.
 Figure 1 presents mean-field samples P ( v j = 1 | h where h  X   X  P ( h ), drawn from RBMs trained on a di-verse collection of classical piano music (top) and on the four-part chorales by J. S. Bach (bottom), along with chord labels where the analysis is unambiguous. It is obvious that for the diverse collection, each sample has some room for additional melody notes with prob-abilities depending on the harmonic context (grey), whereas for JSB chorales, the simultaneities are taken from a more restricted pool and the samples are more clear-cut. This mechanism makes sense musically and the fact that RBMs can adapt to various styles will be useful for the following. The RTRBM (Sutskever et al., 2008) is a sequence of conditional RBMs (one at each time step) whose parameters b ( t ) v ,b ( t ) h ,W ( t ) are time-dependent and de-pend on the sequence history at time t , denoted A ( t )  X  h ( t ) . Its graphical structure is depicted in Figure 2(a). The RTRBM is formally defined by its joint probabil-ity distribution: of the t th RBM whose parameters are defined below (eq. 8 and 9).
 While all the parameters of the RBMs can depend on the previous time steps, we will consider the case where only the biases depend on  X  h ( t  X  1) : which gives the RTRBM six parameters: W,b v ,b h ,W 0 ,W 00 ,  X  h (0) . The general case is derived in a similar manner.
 While the hidden units h ( t ) are binary during inference and sampling, it is the mean-field value  X  h ( t ) that is transmitted to its successors (see eq. 10). This impor-tant distinction makes exact inference of the  X  h ( t ) very easy and improves the efficiency of training (Sutskever et al., 2008):  X  is obtained directly from equations (2) and (8). Note that equation (10) is exactly the defining equation of a single-layer RNN with hidden units  X  h ( t ) . The RTRBM can be understood as a sequence of con-ditional RBMs whose parameters are the output of a deterministic RNN, with the constraint that the hid-den units must describe the conditional distributions and convey temporal information. This constraint can be lifted by combining a full RNN with distinct hidden units  X  h ( t ) with the RTRBM graphical model as shown in Figure 2(b). We call this model the RNN-RBM. The joint probability distribution of the RNN-RBM is also given by equation (7), but with  X  h ( t ) defined arbitrarily, here as per equation (11).
 For simplicity, we consider the RBM parameters to be single-layer RNN (bottom portion of Fig. 2(b)) whose hidden units  X  h ( t ) are only connected to their direct predecessor  X  h ( t  X  1) and to v ( t ) by the relation: The RBM portion of the RNN-RBM (upper portion of Fig. 2(b)) is otherwise exactly the same as its RTRBM counterpart. This gives the single-layer RNN-RBM nine parameters: W,b v ,b h ,W 0 ,W 00 ,  X  h (0) ,W 2 ,W 3 The training algorithm is slightly different than for the RTRBM since the mean-field values of the h ( t ) are now distinct from  X  h ( t ) . An iteration of training is based on the following general scheme: 1. Propagate the current values of the hidden units 2. Calculate the RBM parameters that depend on the 3. Use CD k to estimate the log-likelihood gradient 4. Propagate the estimated gradient with respect to This procedure can be adapted to any RNN architec-ture and conditional distribution estimator assuming the RNN provides the estimator X  X  parameters (step 2) and can be trained based on a stochastic gradi-ent signal on those parameters (obtained in step 3). The RNN-NADE, obtained by substituting NADEs for RBMs, allows for exact gradient computation.
 Note that the single-layer RNN-RBM is a generaliza-tion of the RTRBM and reduces to this simpler model by setting W 2 = W , W 3 = W 0 and b  X  h = b h in equa-tions (10) and (11). The RTRBM was not gaining computationally from sharing these connections, hence untying them does not make it slower. In practice, the ability to distinguish between the number of hidden units h and  X  h allows to scale RBMs to several hundred hidden units while keeping the RNNs to their (typi-cally smaller) optimal size, improving performance. 4.1 Initialization strategies Initialization strategies based on unsupervised pre-training of each layer have been shown to be important both for supervised and unsupervised training of deep architectures (Bengio, 2009). A recurrent network cor-responds to a very deep architecture when unfolded in time, and indeed we find that pretraining can clearly affect the overall performance of both the RTRBM and the RNN-RBM. To ensure the quality of the learned weight matrices, we found that initializing the W , b v and b h parameters from a trained RBM yields less noisy filters. The hidden-to-bias weights W 0 ,W 00 can then be initialized to small random values, such that the sequential model will initially behave like indepen-dent RBMs, eventually departing from that state. In order to capture better temporal dependencies, we initialize the W 2 ,W 3 ,b  X  h ,W 00 ,b v ,  X  h (0) parameters of the RNN-RBM from an RNN trained with the cross-entropy cost: where y ( t ) =  X  ( b ( t ) v ) and equations (9) and (11) hold. This deterministic objective allows the use of a second-order optimization method for pretraining of the RNN. Note that the RTRBM could use this strategy to ini-tialize W,W 0 ,b v ,b h ,W 00 ,  X  h (0) , but in practice we have found the initialization from an RBM more important. 4.2 Details of the BPTT algorithm Suppose we want to minimize the negative log-likelihood cost C  X   X  log P ( { v ( t ) } ). The gradient of C with respect to the parameters of the conditional RBMs can be estimated by CD using equations (4) and (6):  X  X   X  X  The gradient then back-propagates through the hidden-to-bias parameters (eq. 8 and 9): For the single-layer RNN-RBM, the BPTT recurrence relation follows from (11): for 0  X  t &lt; T (  X  h (0) being a parameter of the model) and  X  X / X   X  h ( T ) = 0. Formulas for the remaining RNN-RBM parameters are: In this section, we compare the performance of the RTRBM with the RNN-RBM on two baseline datasets: bouncing balls videos and motion capture data (Sutskever et al., 2008). We use the mean frame-level squared prediction error as a basis of compari-son. The prediction of the t th conditional RBM is per-formed by 50 steps of block Gibbs sampling starting at v ( t  X  1) and hoping to reconstruct v ( t ) optimally. The bouncing ball videos dataset 1 is based on a simu-lation of balls bouncing in a box (Sutskever &amp; Hinton, 2007). The generated videos are of length T = 128 and of resolution 15  X  15 pixels in the [0 , 1] interval, which makes binary RBMs (eq. 1) well suited for this task. With up to 300 hidden units and an initial learn-ing rate of 0.01, we obtain a squared prediction error of 2.11 for the RTRBM and 0.96 for the RNN-RBM, i.e. less than half the error . The receptive fields (weights) of the first 48 hidden units h ( t ) (RNN-RBM) are plotted in Figure 3. Localized edge detectors are apparent in nearly all the learned filters.
 The human motion capture dataset 2 is represented by a sequence of joint angles, translations and rotations of the base of the spine in an exponential-map parame-terization (Hsu et al., 2005; Taylor et al., 2007). Since the data consists of 49 real values per time step, we use the Gaussian RBM variant (Welling et al., 2005) for this task. We use up to 450 hidden units and an initial learning rate of 0.001. The mean squared pre-diction test error is 20.1 for the RTRBM and reduced substantially to 16.2 for the RNN-RBM . In this section, we show results with main applica-tion of interest for this paper: probabilistic modeling of sequences of polyphonic music. We report our ex-periments on four datasets of varying complexity con-verted to our input format.
 Piano-midi.de is a classical piano MIDI archive that Nottingham is a collection of 1200 folk tunes 3 with MuseData is an electronic library of orchestral and JSB chorales refers to the entire corpus of 382 four-Each dataset contains at least 7 hours of polyphonic music and the total duration is approximately 67 hours. The polyphony (number of simultaneous notes) varies from 0 to 15 and the average polyphony is 3.9. We use an input of 88 binary visible units that span the whole range of piano from A0 to C8 and tempo-rally aligned on an integer fraction of the beat (quarter note). Consequently, pieces with different time signa-tures will not have their measures start at the same interval. Although it is not strictly necessary, learning is facilitated if the sequences are transposed in a com-mon tonality (e.g. C major/minor) as preprocessing. In addition to the models previously described, we evaluate the following commonly used methods:  X  The simplest baseline model consists in outputting a Gaussian density centered on the previous frame  X  = v ( t  X  1) and learned covariance  X .  X  N-grams simulate the evolution of note simultane-ities as an ( N  X  1) th -order Markov chain. We use add-p or Gaussian smoothing and back-off.  X  Note N-grams model each note independently by a binary N-gram, possibly with shared parameters (IID).  X  An interesting model for chorales harmonisation (Allan &amp; Williams, 2005) has been adapted to serve as a generative model. It can only be evaluated on the JSB chorales dataset.  X  The  X  X andom fields X  approach of Lavrenko &amp; Pick-ens (2003) is a type of fully visible sigmoid belief network with learned connectivity.  X  Other common methods include Gaussian mixture models (GMM), hidden Markov models (HMM) us-ing GMM indices as their state, and multilayer per-ceptrons (MLP) with the last n time steps as input. The log-likelihood (LL) and expected frame-level accu-racy (ACC) (Bay et al., 2009) of the symbolic models are presented in Table 1. We estimate the partition function of each conditional RBM by 100 runs of an-nealed importance sampling (Salakhutdinov &amp; Mur-ray, 2008). We make a few key observations:  X  The complexity of the dataset, such as the simplistic chord accompaniment of Nottingham and the redun-dant style of four-part chorales by a single composer, in comparison with diverse piano and orchestral mu-sic, is clearly reflected in the obtained log-likelihoods and accuracies.  X  N-gram models (optimal N  X  = 2) perform reason-ably well for simple datasets but fail in more realistic settings due to the increased data sparsity. In this case, note N-grams ( N  X   X  [8 , 14]) are a better alter-native albeit ignoring harmonic dependencies. This inherent trade-off in traditional polyphonic music models can be addressed robustly by the RNN-based models, that perform better on a range of datasets.  X  The harmonisation model of Allan &amp; Williams (2005), tailored to the specific style of four-part chorales, requires annotated harmonic symbols and yet performs relatively poorly compared to our best performer. Similarly to the GMM + HMM, this model is penalized by the limited history of the
HMM and by the difficulty to generalize to new chord voicings in a principled manner.  X  In accordance with earlier results (Martens &amp;
Sutskever, 2011), the use of HF significantly helps the density estimation and prediction performance of RNNs (eq. 12) which would otherwise perform worse than simpler MLPs. This motivates our strat-egy of pretraining the RNN layer of an RNN-RBM via HF.  X  In addition to the distinct recurrent hidden units  X  h ( t ) that convey temporal information more freely, and the fact that suitable learning rates can be spec-ified differently for the RNN and the RBM parts, pretraining the W 2 , W 3 and b  X  h parameters can have the most impact on the RNN-RBM prediction per-formance. Figure 4 clearly demonstrates the impor-tance of pretraining and finetuning the RNN and the additional advantage of using HF.  X  Although frame-level NADEs are slightly less pow-erful than RBMs, their desirable properties make the combined RNN-NADE model the most robust distribution estimator. We believe this is due to their tractable distribution, for two reasons. First,
CD may not be ideally suited for conditional RBMs with slowly-mixing Gibbs chains (Mnih et al., 2011), a non-issue for exact-gradient models. Secondly, the joint sequential model, and not only the RNN por-tion, can benefit from second-order optimization as can be seen from the last two rows of Table 1. We evaluate our models qualitatively by generating sample sequences, provided on the authors X  website 5 , and discussed here. While note correlations are ob-viously neglected in the simpler models (sequence 2), RBM-based models learned basic harmony rules (se-quence 3), melody lines (sequences 4, 8) and local temporal coherence (sequence 5). However, long-term structure and musical meter remain elusive. Multiple fundamental frequency ( f 0 ) estimation, or polyphonic transcription, consists in estimating the audible note pitches in the signal at 10 ms intervals without tracking note contours. We combine our poly-phonic sequence models with the acoustic model of Nam et al. (2011) in order to demonstrate a practi-cal application of the sequence models. Their model was adapted for multiple instruments, and it can be generalized to any method that can score hypothetical combinations of f 0 for a given time frame.
 At each time frame, the Nam et al. (2011) algorithm outputs independent probabilities that each note is present and reports every note with probability p  X  0 . 5. To incorporate our symbolic model prediction P ( v ( t ) |A ( t ) ), we consider the k most promising f candidates ( k = 7) from the acoustic model P a ( v ( t ) and jointly evaluate all combinations of M candidates  X  M  X  k by the following cost function: structed from the f 0 estimated so far in at least half the audio frames corresponding to each past symbolic time step 6 . This corresponds to a product of experts where the hyperparameter  X  is the confidence coeffi-cient of our symbolic predictor. If our algorithm is run on audio signals without preprocessing, tempo track-ing must be performed first. Since the symbolic models describe only fixed tonality pieces, a first audio-only pass is needed to transpose the estimated f 0 in the correct tonality. Once the optimal f 0 estimates have been determined, HMM smoothing can still filter out spurious results and enhance onset accuracies. Digital audio has been generated for the four datasets and we report in Figure 5 the frame-level transcrip-tion accuracy of the Nam et al. (2011) algorithm, ei-ther alone, after HMM smoothing, or using our best performing model as a symbolic prior. We observe an improvement in absolute accuracy between 1.3% and 10% over the HMM approach. It can be seen easily that an HMM with emission probabilities P a ( v ( t ) equivalent to equation (23) with a note 2-gram sym-bolic model, one time step per audio frame and  X  = 1. It is therefore unsurprising that the advantage of our search algorithm decreases when the note N-gram al-ready performs well, e.g. for Piano-midi.de (Table 1). However, the HMM allows for a global search of the most likely f 0 (the Viterbi path), whereas our algo-rithm requires a greedy chronological search, a limita-tion we are currently working to address.
 We presented an RNN-based model that can learn harmonic and rhythmic probabilistic rules from poly-phonic music scores of varying complexity, substan-tially better than popular methods in music informa-tion retrieval. We showed that different strategies re-lated to the description of temporal dependencies can improve prediction accuracy of such models. While longer-term musical structure remains elusive in our unconstrained representation, our model can immedi-ately serve as a symbolic prior for polyphonic tran-scription, clearly improving the state of the art in this area.
 The authors would like to thank NSERC, CIFAR and the Canada Research Chairs for funding, and Compute Canada/Calcul Qu  X ebec for computing resources.
