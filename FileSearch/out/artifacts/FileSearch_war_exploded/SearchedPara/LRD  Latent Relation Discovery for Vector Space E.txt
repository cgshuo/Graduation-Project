 Textual corpora, such as web pages on a departmental website and blogs of a group of people, often mention named entities which are related to each other, and their relat-edness is often shown by their co-occurrence in the same documents and their occur-ring close to each other in these documents, e.g., one document mentions Thomas works on project X in one sentence, and another document mentions Jack works on X in one paragraph. Given an entity, we can use either standard statistical measures such as mutual information [12] or our own CORDER method [11] to find related entities nally occurring in the document, however, entities which are related to these original work on X but one document only mentions Thomas works on X. 
Therefore, we propose to enhance the content description of a document with enti-document. By doing so, we can enrich what is missing but in fact very relevant to the document, e.g., since Thomas and Jack both work on X, we add Jack to one document which only mentions Thomas works on X. 
In terms of information retrieval (IR), vector space models are traditionally used to index a document with terms and words occurring in the document for term-based querying and document clustering. Thus, we propose to enhance the vector of a docu-ment with entities and terms (CORDER and statistical methods are applied to terms in the same manner as entities) which are not in the document but are closely related to existing entities and terms in the document. Since humans X  term-based queries are often an approximation of the kind of information they are looking for, these enhanced vectors can often lead to improved quality of returned documents, e.g., one document, which has Thomas and Jack as original dimensions and project X as an enhanced di-mension, will match the query  X  X  X , and the user may find this document useful since it provides detailed information about Thomas and Jack, two members of X. 
In this paper, we propose a text mining method called LRD (latent relation discov-ery) which can automatically process a textual corpus for unearthing relationships among entities and terms, and use these relationships to enhance traditional vector space model for IR and document clustering. 
We propose a relevance measure for a pair of co-occurring entities by taking into account both their co-occurrence and distance. The relevance measure measures the strengths to the target entity. 
LRD is based on our own CORDER algorithm [11]. LRD can be viewed as an un-supervised machine learning method, i.e., the method does not need either richly annotated corpora required by supervised learning methods or instances of relations as initial seeds for weakly supervised learning methods. occurrence and distance with other entities in a textual corpus. Given a document, document, are used to enhance the vector representation of the document. The en-hanced vector space model has led to improved IR on these documents and document another document B in the traditional vector space, ho wever can be judged as relevant to the query Q or document B in the enhanced vector space. We have evaluated LRD in terms of F measure, a combination of precision and recall, in IR and compared with five other standard methods, and LRD has significantly outperformed all of them and the original vector space model. The rest of the paper is organized as follows. We present related work in Section 2. Section 4. Finally, we conclude the paper and discuss future work in Section 5. Co-occurrence based methods have been widely applied, for instance, in the identifi-cation of collocations and information retrieval. Such methods aim to correlate textual Semantic Indexing (LSI) [3], which automatically discovers latent relationships among documents and terms through Singular Vector Decomposition (SVD). LSI has been applied mainly in the information retrieval area, and also used to discover highly related terms [4]. Furthermore, LSI can reduce the dimensionality without undermin-ing precision in information retrieval systems. However the method is time-consuming when applied to a large corpus [7]. 
Other related methods which can be applied in this context are t test, chi-squared, z score, and mutual information (MI) [12]. Criticisms of these methods are that prob-abilities are assumed to be approximately normally distributed in t test, Z score is only frequency events, and mutual information does not deal properly with data sparseness. corpus since LRD treats each document as an atomic unit and any change requires only unitary reprocessing (the details of LRD is presented in Section 3). 
In the line of document clustering, Hotho and Stumme [6] have made use of For-mal Concept Analysis using background knowledge by mapping words to some con-proposed a model called COSA (Concept Selection and Aggregation) which uses ontologies for restricting the set of document features through aggregations. Another approach is based on analogy, aiming to produce, through alignment, pairs of defini-tions that share the same headword term, and promotes replacements in pairs without extracts relevant terms from researchers X  curricula vitae integrated with ontology ontology dependency. Our LRD method analyzes co-occurrences through textual proves IR and document clustering tasks. 
In essence, our proposed LRD method is similar to those co-occurrence based ap-proaches which aim to enhance context representation. However, by combining rela-space model, our LRD method has achieved better results. 3.1 Overview Our LRD model maps entities and their relationships extracted from documents. Enti-ties are named entities extracted from documents using a Named Entity Recognition (NER) tool called ESpotter [10] and terms in the document. We calculate the relation strength between every pair of entities 1 by taking into account the pair X  X  co-occurrences in these documents. We represent each document as a vector of entities, and construct an entity-by-document matrix. Given a document and its vector, the document vector. We use these expanded vectors for query based information re-trieval and document clustering. 3.2 Entity Extraction Named Entity 2 Recognition (NER) is a well studied area [2]. We have used ESpotter [10], a NER system based on standard NER techniques and adapted to various do-mains on the Web by taking into account domain knowledge. ESpotter recognizes Named Entities (NEs) of various types. Users can configure ESpotter to recognize new types of entities using new lexicon entries and patterns. Domain knowledge, taken from sources such as ontologies, is represented as lexicon entries (e.g., the pro-ject names in an organization). 3.3 Relation Strength Given a target entity ( E1 ) which occurs in various documents, there are a number of entities which co-occur with it in these documents. We propose a latent relation dis-covery algorithm which ranks co-occurring NEs based on relation strength. Thus, NEs into account three aspects as follows: Co-occurrence: Two entities are considered to co-occur if they appear in the same text fragment, which can be a document or a text window. For simplicity, in this sec-tion, we use document as the unit to count entity co-occurrence. The effect of differ-one entity is closely related to another entity, they tend to co-occur often. To normal-ize the relatedness between two entities, E1 and E2 , the relative frequency [8] of co-occurrence is defined as follows. where Num ( E 1, E 2) is the number of co-occurring documents for E1 and E2 , and N is the total number of documents in a corpus. Distance. Two NEs which are closely related tend to occur close to each other. If two NEs, E1 and E2 , occur only once in a document, the distance between E1 and E2 is the difference between the word offsets of E1 and E2 . When E1 or E2 occur multiple times in the document, given E1 as the target, the mean distance between E1 and E2 in the i th document, m i (E1, E2) is defined as follows. m (E1, E2) is not equal to m i (E2, E1) . Relation Strength: Given an entity, E1 , the relation strength between two entities E1 and E2 takes into account their co-occurrence, mean distance, and frequency in co-In this paper, named entities are proper names consisting of words or collocations extracted from documents and labeled as a particular class, i.e., person or organization. 126 A. Gon X alves et al. asymmetric depending on whether E1 or E2 is the target. the i th document, N is the number of documents in the corpus, and df j is the number of documents that contain the entity j . 3.4 Vector Expansion In vector composition, we intend to enhance the vector space by co-occurred entities. ties using Equation 3. For example, in Table 1, an entity-by-document is constructed from 3 documents (D1, D2, and D3) and 7 entities. 
We create a table consisting of pairs of related entities. Each row in the table con-sists of a document ID, a source and a target entity co-occurring in the document with their frequencies, the frequency of their co-occurrences, and their intra-document distance. As an example, Table 2 shows pairs of related entities in document 1, their frequencies and the distance between them calculated using Equation 2 and the intra-document relation strength calculated using the second part of Equation 3 (i.e., entities in document 2 and 3. Given a pair of entities, we can calculate their relation strength shown in Table 3. For example, the relation strength between target entity (TE) E1 and source entity (SE) E 3,is computed using Equation 3 as R ( E1 , E3 )=2/3*(0.4938+0.5850)=0.7192. Relation strength is used to recompose the vector space. For example, in Table 1, the vector of document 1 does not contain E 5 and E 6. However, judging by relation strength, the most relevant entity to E3 not in the vector of document 1 is E5 and to respectively. Since E3 and E2 are dimensions in the vector of document 1, E5 and E6 originally in a document vector as the target, we add each of the top n entities related to the target and not in the document vector (ranked by their relation strengths), E new , to the document vector. The weight of E new , w(E new ) , is defined as follows. most relevant entities in terms of relation strength. 
In Table 4, we set n =1. We add E5 (No. 1 entity not in document vector (N1NDV) SE: E1, E4) and E7 (N1NDV of SE: E3) to document two, and E3 (N1NDV of SE: 0.5850*0.2590+0.4387*0.3155+0.1462*0.1847 = 0.3169. 3.5 Query-Based Information Retrieval and Document Clustering We calculate a cosine coefficient between the expanded vector of each document and with respect to the query. We setup a threshold on the cosine coefficient to trade pre-cision against recall in retrieving these documents. 
We apply a clustering algorithm to generate patterns for in-depth analysis of how documents and entities are inter-connected. Unlike the traditional k -means algorithm which is based on the parameter k (number of clusters), we use an approach based on a radius parameter ( r ) to control the cluster formation. 
The algorithm starts with selecting a vector (either randomly or the one most sepa-vector is selected and compared with the first cluster by applying the cosine measure defined as follows. 
If the similarity between a vector and a cluster centroid subtracted from 1 is greater than the r parameter, the vector forms a new cluster. Otherwise, it is assigned to the cluster and we recalculate the centroids of the clusters. Experiments using a range of r = 0.3. During the next iterations, if the vector moves from one cluster to another, the centroid updating is carried out in both the new cluster to which the vector has been added and the old cluster from which the vector has been removed. 
The clustering process stops when it reaches convergence, which is determined by the total average difference between the current and previous epoch. Our experiments on different datasets have shown that epochs between 2 and 10 are required. After the clustering, we get clusters consisting of vectors and cluster centroid average. We have evaluated our proposed relation strength model (LRD) in term of F measure, which combines precision and recall, by comparing with five standard statistical methods (LSI and four other methods based on a relation strength model for vector Glasgow Information Retrieval benchmark dataset called CISI 3 containing 1,460 documents and 112 queries has been used. Terms in the documents and entities ex-tracted from documents using ESpotter are used during the correlation and vector expansion processes. 4.1 Relation Strength Models We have compared LRD with four standard statistical methods in relation strength calculation. These relation strengths are used for vector expansion. The four methods, i.e., mutual information (MI), improved MI, phi-squared, and Z score are presented as follows. other linguistic unit, such as named entities, appearing together against the probability relevance between two entities. MI is defined as follows. occur individually. 
We have also applied Vechtomova et al. X  X  improved MI (VMI) method [13]. The P ( x , y ) = P ( y , x ). Unlike traditional MI, VMI is asymmetrical. An average window size currence of y in the windows around x . VMI is defined as follows. cies of independent occurrence of x and y in the corpus, v x is the average window size around x in the corpus, and N is the corpus size. 
Phi-squared ( 2  X  ) makes use of a contingency table as follows: times neither entity occurs, that is, , N dabc and S the size of the text window. 2  X  measure between w 1 and w 2 is defined as: where 2 01  X   X  X  X  . Unlike MI which typically favors entities with low frequency, 2  X  can be used as an alternative, since it tends to favor high frequency ones. 
Z score has been used by Vechtomova et al. [13] for query expansion. Z score is defined as follows. cies of independent occurrence of x and y in the corpus, v x is the average window size around x in the corpus and N is the corpus size. 4.2 Experimental Results of Vector Space Model for Information Retrieval By expanding document vectors and applying different relation strength methods we intend to establish a way to automatically evaluate our proposed method. In this sense we have compared LRD, Phi-squared, MI, VMI and Z score in order to find out enti-ties and terms closely related to the original entities and terms in the vector. For each method, the original vector is expanded using the method by taking into account dif-ferent text windows and expansion factors. 
Given a document vector, it is expanded using the method presented in Section 3 with different text windows and n factors (the n most related entities to each original are added to the vector). We have used the text window of 20, 50, 100 and 200, and the whole document (i.e., two entities are considered as co-occurring as long as they occur in a same document). For the n factor, values of 1, 5, 10, 20, 30, 40 and 50 most relevant entities are used to expand the vector space. The same vector expansion proc-ess using each of the relation strength methods is applied to the corpus using different text window and n factor. 
We have applied each relation strength method to the 1,460 documents in the CISI dataset. The constructed vector space by each method using different text window and 112 queries in CISI. Given a query, we calculate a cosine coefficient between the vector of each document and the vector of the query to rank these documents against the query. 
Given a query, we set a threshold on the cosine coefficient and only documents having cosine coefficient with the query above the threshold are taken into account in our precision and recall calculation. Given a query, the precision ( P ) of our answer is the number of relevant documents returned divided by the total number of returned the total number of relevant documents as the gold standard in CISI. We define the F measure as 2 PR F 0.54 which maximizes F measure on most queries. Given a relation strength method with different window size and n factor, we average the F measure for each of the 20 answers to get the total F measure and the results are shown in Table 5. 
The average F measure for the original vector space model, i.e., without vector ex-pansion and text window, is 9.2% and provides a baseline for our comparison. As shown in Table 5, LSI, which is only evaluated based on the use of whole documents rather than text windows, is also included. For LSI and the other five methods which work on different window settings, LRD consistently performs the best. The highest F measure is 19.3% using LRD with no window and n =30. The second best performing method is Z score with highest F measure 17.4% with window size 100 and n =40. MI and VMI have similar performance. In terms of the influence of n factor on these methods, when n factor increases, the F measure of LSI keeps roughly the same. For a given window setting, when n factor small even some small decreases), the F measure of phi-squared, MI and VMI de-crease. Since the baseline is 9.2%, we can see that vector expansion with LRD, LSI, other methods have a negative effect on information retrieval. LRD has achieved the best performance for all window sizes when n = 30, and larger n values will not bring benefit to the performance and on the contrary bring computational cost. 
In terms of the effect of window size on average F measure, LRD and Z score are the other methods. Phi-squared, MI, and VMI achieve better F measures with smaller window sizes than those with larger window sizes. We present a co-occurrence based approach, namely LRD (Latent relation Discov-ery), which associates entities using relation strengths among them. We propose to documents in order to provide additional meaning and improve query based informa-tion retrieval on these documents. Our initial experiments using the CISI dataset have shown that LRD can dramatically improve the F measure of information retrieval over the traditional vector space model, and significantly outperformed five standard methods for vector expansion. Our experiments on the CISI dataset show that LRD X  X  running time increases linearly with the size of documents and the number of docu-relations by taking into account new documents. Thus, LRD can scale well to a large corpus. 
Our future work is five-fold. First, we are working on refining the LRD model in and improve the clustering method. Second, we propose clustering documents based on the enhanced vector space models produced by our LRD method, however, the evaluation and interpretation of these clusters is neither an easy nor an intuitive task. We are carrying out work on using various techniques to evaluate these clusters. Our work underway is the visualization of these clusters in order to show complex patterns of inter-connected entities in clustered documents for easy comparison between these clusters produced by different vector space models. Third, we are evaluating our en-hanced vector space models for information retrieval and clustering on large scale TREC collections such as TIPSTER. Fourth, dimensionality reduction is another direction and needs to be studied in order to improve the performance of our method. Finally, entities and their relations constitute a social network of communities of prac-tice. We are working on using the social network to help analyze and understand the behavior of these communities. 
