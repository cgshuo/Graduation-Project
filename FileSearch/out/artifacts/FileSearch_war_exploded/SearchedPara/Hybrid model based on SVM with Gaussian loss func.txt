 1. Introduction
As an implementation of the structural risk minimization (SRM) principle where the generalization error is bounded by the sum of the training error and a confidence interval term depending on the VC dimension, support vector machines (SVM) (Vapnik, 1995 ) have recently attracted a lot of researchers from the machine learning and pattern classification community for its fascinating properties such as high generalization performance and globally optimal solution ( Mohammadi and Gharehpetian, 2008 ). In SVM, original input space is mapped into a higher dimensional feature space in which an optimal separating hyper-plane is constructed on the basis of SRM to maximize the margin between two classes, i.e., to maximize the generalization ability.
SVM was initially designed to solve pattern recognition problems (Mohammadi and Gharehpetian, 2009 ;  X  Ubeyli, 2008 ; Yang et al., 2003 ; Liu and Chen, 2007 ; Hao, 2008 ). Recently, with the introduction of Vapnik X  X  e -insensitive loss function, SVM has been extended to function approximation and regression estimation problems ( Tao et al., 2005 ; Goel and Pal, 2009 ; Osowski and Garanty, 2007 ; Colliez et al., 2006 ; Vong et al., Bao et al., 2005 ; Sun and Sun, 2003 ; Wu, 2009 ; Lute et al., 2009 ; Wu et al., 2008a, b ).

In SVR approach, the parameter e controls the sparseness of the solution in an indirect way. However, it is difficult to come up with a reasonable value of e without the prior information about the accuracy of output values. Sch  X  olkopf et al. (2000) modify the original e -SVM and introduce n -SVM, where a new parameter v controls the number of support vectors and the points that lie is traded off between model complexity and slack variables via the constant v .

However, some SVMs encounter some difficulty in real improved SVMs have been put forward to solve the pattern recognition problems ( Liu and Chen, 2007 ; Hao, 2008 ) and regression estimation problems ( Huang et al., 2005 ; Wang et al., 2005 ). The standard SVM adopting e -insensitive loss function has good generalization capability in some applications ( Mohammadi and Gharehpetian, 2009 ;  X  Ubeyli, 2008 ; Yang et al., 2009 ; Frias-and Chen, 2007 ; Hao, 2008 ; Tao et al., 2005 ; Goel and Pal, 2009 ; Bergeron et al., 2005 ; Huang et al., 2005 ; Wang et al., 2005 ; Bao et al., 2005 ). But it is difficult to handle the normal distribution noise parts of data set ( Vapnik, 2000 ; Cao and Wang, 2007 ).
Therefore, the main contribution of this paper focuses on the modeling of a new v -SVM that can penalize the Gaussian noises from input series.

In this paper, a new v -SVM with Gaussian loss function, which is called g -SVM, is proposed to penalize white noises from series effectively, while some SVMs with e -insensitive loss function are unsuitable to do it. Many SVM literatures focus on slack variables with one power, while our support vector machine with square slack variables on the conditions of immovable constraints. Our new SVM is also different from least square SVM in Vong et al., 2006 .In Vong et al., 2006 , least square SVM has also square slack variables, but its constraint conditions have been modified into equality form.
 The g -SVM is described in Section 2. Section 3 provides a new
PSO, which is called PSO with adaptive and normal Gaussian mutation (ANPSO), to obtain the optimal parameters of g -SVM.
Two applications based on g -SVM and ANPSO are given in Section 4, and then g -SVM is also compared with the standard v -SVM and autoregressive moving average (ARMA Wu et al., 2008a ). Section 5 draws the conclusions. 2. g -SVM 2.1. g-SVM model
Suppose training sample set T ={( x 1 , y 1 ), y ,( x i described as follows: c  X  x ; y i ; f  X  x i  X  X  X j y i f  X  x i  X j e  X  1  X 
The standard v -SVM with e -insensitive loss function can be described as follows: min where w is a column vector with d dimension, C 4 0 is a penalty regularization parameter, e is also an adjustable tube X  magnitude parameter.
 Definition 1. White noise Noises with normal distribution are called white noises.
Gaussian noises are a special case of white noises. Generally, the standard Gaussian density model N (0,1) is commonly used to describe noise.

However, for e -insensitive loss, it is difficult to handle the white noises from inputting series ( Cao and Wang, 2007 ). To solve the shortage of e -insensitive loss, Gaussian function is selected as the loss function of v -SVM. Then Gaussian loss function can be defined as follows: c  X  x ; y i ; f  X  x i  X  X  X j y i f  X  x i  X j e  X  3  X  e -tube.

By the integration of Gaussian loss function and SVM theory, a new v -SVM called g -SVM is proposed. g -SVM can penalize the normal Gaussian noises from sample data, which is described as follows: min
It is obvious that slack variables x i (*) of g -SVM whose structure is illuminated in Fig. 1 appear in square, ones of v -SVM do in one power. Since g -SVM shown in Fig. 1 has the same structure with standard v -SVM, g -SVM can be also solved by quadratic programming (QP). Obviously, the constraint conditions of g -
SVM are the same with that of v -SVM. Compared with least square support vector machine (LS-SVM) ( Vong et al., 2006 ), the constraint conditions of g -SVM appear in the form of inequation, while that of LS-SVM do in the form of equation.

Problem (4) is a quadratic programming (QP) problem. The steps of its solution are described as follows:
Step 1: Suppose the training sample set T ={( x 1 , y ( x , y ), y ,( x l , y l )}, where x i A R d , y i A R .

Step 2: Select the kernel function K , regularization parameter v and the penalty factor C . Construct the QP problem (4) of the g -SVM.

Step 3: By introducing Lagrangian multipliers, a dual problem can be defined as follows:
The Lagrangian multipliers a i (*) can be determined by solving the problem (5).

Step 4: For a new input x , construct the following regression function f  X  x  X  X 
K ( x 1 , x ) K ( x 2 , x ) K ( x i , x ) K ( x n , x )
Select the two scalars a j ( a j A (0, l / C )) and a * k parameter b can be computed by b  X  1 2  X  y j  X  y k
Parameter e can be obtained by either of the following equations:  X  or  X  y k
Weight vector w of the problem (4) has a unique solution, while Lagrangian multiplier a (*) have many solutions. Theorem 1 which explains the relationship between Lagrangian multiplier a (*) and weight vector w is given as follows: problem (5), the unique solution of weight vector w can be represented as follows: w  X 
The proof of Theorem 1 is arranged in Appendix A. 3. Particle swarm optimization with adaptive and normal Gaussian mutation
It is difficult to confirm the optimal parameters of the SVM model. There exist experiential errors in crossover validation method used traditionally to seek these optimal parameters of
SVM such as penalty coefficient, controlling vector and kernel parameter. Now, evolutional algorithms such as genetic algorithm (Samanta et al., 2003 ) and particle swarm optimization ( Kenedy and Eberheart, 1995 ) are used for parameters selection of SVM universally. Nevertheless, these standard evolutional algorithms encounter premature convergence and cannot obtain the global optimum ( Samanta et al., 2003 ; Wu et al., 2008b ). To overcome the shortage, a new PSO with adaptive and normal Gaussian mutation operators is proposed, namely adaptive and normal
Gaussian particle swarm optimization (ANPSO), as is utilized to optimize the parameters of g -SVM. 3.1. Standard particle swarm optimization
Similar to evolutionary computation technique ( Samanta et al., 2003 ), PSO uses a set of particles, representing potential solutions to the problem under consideration. The swarm consists of n particles. Each particle has a position X i =( x i 1 , x i 2 and moves through an m -dimensional search space. According to the global variant of the PSO, each particle moves towards its best previous position and towards the best particle g in the swarm. Let us denote the best previously visited position of the i th particle that gives the best fitness value as P i =( p i 1 , p i 2 the best previously visited position of the swarm that gives the best fitness as pg =( pg 1 , pg 2 , y , pg j , y , pg m ).
The change of position of each particle from one iteration to another can be computed according to the distance between the current position and its previous best position, and the distance between the current position and the best position of swarm.
Then the updating of particle velocity and position can be obtained by the following equations: v x where w is called inertia weight and is employed to control the impact of the previous history of velocities on the current one. Accordingly, the parameter w regulates the trade-off between the global and local exploration abilities of the swarm. A large inertia weight facilitates global exploration, while a small one tends to usually provides the balance between global and local exploration abilities, and consequently results in a reduction of the number of iterations required to locate the optimum solution. k denotes the iteration number, c 1 is the cognition learning factor, c learning factor, r 1 and r 2 are random numbers uniformly distrib-uted in the range [0, 1].

Thus, the particle flies through potential solutions towards P and pg k in a navigated way while still exploring new areas by the stochastic mechanism to escape from local optima. Since there was no actual mechanism for controlling the velocity of a particle, it was necessary to impose a maximum value V max on it. If the velocity exceeds the threshold, it is set equal to V max , which controls the maximum travel distance at each iteration to avoid this particle flying past good solutions. The PSO is terminated with a maximal number of generations or the best particle position when the entire swarm cannot be improved further after a sufficiently large number of generations. The PSOs have shown its robustness and efficacy in solving function value optimiza-tion problems in real number spaces ( Wu, 2009 ; Wu et al., 2008b ; Zhao and Yin, 2009 ; Fei et al., 2009 ; Krohling and Coelho, 2006 ). 3.2. The PSO with adaptive and normal Gaussian mutation operators (ANPSO)
One of the major drawbacks of the standard PSO is its premature convergence, especially while handling problems with more local optima. Some improved PSOs have been published to solve many real problems such as parameters identification ( Cao and Wang, 2007 ), constrained optimization problems ( Zhao and Yin, 2009 ). Aim at these drawbacks of the standard PSO, the adaptive mutation operator is proposed to regulate the inertia weight of velocity by means of the fitness value of object function and iterative variable. The normal Gaussian mutation operator (Zhao and Yin, 2009 ) is also considered to correct the direction of particle velocity at the same time. The adaptive mutation is highly efficient operator on the conditions of real number code. The quality of the solution is related tightly with the mutation operator. The aforementioned problem is addressed by incorpor-ating adaptive mutation and normal Gaussian mutation for the previous velocity of the particle. Thus, the PSO with adaptive and normal Gaussian mutation operators (ANPSO) can update the particle velocity and position by the following equations: v id  X  X  1 l  X  w x w id  X  b 1  X  f  X  x k i  X  = f  X  x k m  X  X   X  X  1 b  X  w 0 id exp  X  a k i  X  s k i exp  X  N i  X  0 ; D s  X  X   X  16  X  where D s is standard error of normal Gaussian distribution, b is the adaptive coefficient, l is the increment coefficient, a is the attenuation coefficient of controlling particle velocity.

The first item of Eq. (13) denotes the adaptive mutation of velocity inertia weight mutation based on iterative variable ( k ) fitness mutate in a smaller scope, while the ones with the smaller fitness mutate in a bigger scope by Eq. (15). The second item of Eq. (13) represents the normal Gaussian mutation based on the iterative variable ( k ). The particles mutate in big scope on the smaller iterative variable ( k ) and search the local optimal value, while the particles mutate in small scope on the bigger iterative variable, search the optimal value in small space and gradually reach the global optimal value. The operator of normal Gaussian mutation correct the change of particle velocity is represented in
Eqs. (13) and (16). In the strategy of normal Gaussian mutation, the proposed velocity vector v k +1 =( v 1 k +1 , v 2 k +1 of last generation velocity vector v k =( v 1 k , v 2 k , tion vector mutates itself by Eq. (16) on the each iterative process as a controlling vector of velocity vector.

The adaptive and normal Gaussian mutation operators can restore the diversity loss of the population and improve the capability of the global searching performance of the proposed algorithm. 3.3. The parameters selection of g-SVM based on ANPSO
To analyze the regression estimation performance of g -SVM, two applications are studied in this paper. Then, we combine the forecasts with g -SVM, a hybrid model shown in Fig. 2 can be described as follows: ANPSO optimizes the parameters of g -SVM. The obtained optimal parameters are input into g -SVM, then g -SVM gives the forecasting results.

Many actual applications ( Wu et al., 2008a, b ) suggest that radial basis functions tend to perform well under general smoothness assumptions, so that they should be considered especially if no additional knowledge of the data is available. In this paper Gaussian radial basis function is used as the kernel function of g -SVM. 4. Applications
In many real applications, the observed input data cannot be measured precisely and usually described in linguistic levels or ambiguous metrics. However, traditional support vector regres-sion (SVR) method cannot cope with qualitative information. It is well known that fuzzy logic is a powerful tool to deal with fuzzy and uncertain data. All linguistic information of influencing factors is dealt with by fuzzy comprehensive evaluation and forms numerical information. Suppose the number of variables is of fuzzy linguistic variables and crisp numerical variables. The linguistic variables are evaluated in several description levels, and a real number between 0 and 1 can be assigned to each description level. Distinct numerical variables have different dimensions and should be normalized firstly. The following normalization is adopted: x  X  value and the normalized value, respectively. In fact, all the numerical variables from (1) through (16) are the normalized values although they are not marked by bars.

The g -SVM has been implemented in Matlab 7.1 programming language. The following two experiments are made on a 1.80GHz
Core (TM) 2 CPU personal computer (PC) with 1.0G memory under Microsoft Windows xp professional. The proposed ANPSO algorithm has been implemented in Matlab 7.1 programming language. For standard PSO, the inertia weight w 0 varies in (0,1], biggish inertia weight w 0 can facilitate global exploration, while a lesser one tends to facilitate local exploration. Therefore, the initial parameters of inertia weight and positive acceleration constants in ANPSO are given as follows: w 0 =0.9 and c 1 experiment confirmation. The adaptive coefficient ( b A (0,1]) is equal to 0.8. The Gaussian perturbation adjusts (corrects) the direction of particle velocity by increment coefficient ( l The fitness accuracy of the normalized samples is equal to 0.0002.
The attenuation coefficient of controlling particle velocity ( a 4 1) is equal to 2. To evaluate forecasting capacity of g -SVM, some evaluation indexes, such as mean absolute error (MAE), mean absolute percentage error (MAPE) and mean square error (MSE), are utilized to handle the forecasting results of g -SVM. The computational formulations of these selected indexes are shown in Table 1 .
 Example 1. The g -SVM model is applied in car sale forecasts. In this experiment, car sale series are selected from past sale record in a typical company. The detailed characteristic data and sale series of these cars compose the corresponding training and testing sample sets. During the process of the car sale series forecasting, six influencing factors shown in Table 2 , viz., brand famous degree (BF), performance parameter (PP), form beauty (FB), sales experience (SE), oil price (OP) and dweller deposit (DD) are taken into account.
 The optimal combinational parameters are obtained by
Algorithm ANPSO, viz., C =715.11, v =0.96 and s =0.01. Fig. 3 illuminates the forecasting results provided by ANPSO g -SVM.
To analyze the forecasting capability of the proposed hybrid model (ANPSO g -SVM) based on ANPSO and g -SVM, the models (autoregressive moving average (ARMA), hybrid model (PSO v -
SVM) based on PSO and standard v -SVM, and hybrid model (ANPSO v -SVM) based on ANPSO and standard v -SVM) are Forecasting Results selected to handle the above car sale series. Their forecasting results are shown in Table 3 .

The indexes MAE, MAPE and MSE are used to evaluate the forecasting capability of four models shown in Table 4 . The sample data of the latest 12 months are used for testing set. The corresponding forecasting results are used to analyze the forecasting performance of the above models. It is obvious that the forecasting accuracy provided by support vector machines excel the one by autoregressive moving average (ARMA). For the same v -SVM, the indexes (MAE, MAPE and MSE) provided by ANPSO are better than ones of PSO. For the same ANPSO, the indexes (MAE, MAPE and MSE) provided by ANPSO g -SVM with Gaussian loss function are better than ones of ANPSO v -SVM with e -insensitive loss function. Considering the non-linear influence from multidimensional series, it is found that g -SVM can handle the normal Gaussian noise from input series effectively. Example 2. The g -SVM model is applied in the estimation of product design time.

As global competition increases and product life cycle shortens, companies try to employ effective management to accelerate product development. However, product development projects are often suffered with schedule overruns. In most cases, problems of overruns were due to poor estimations. That is coincident with the saying  X  X  X ou can X  X  control what you do not measure X  X  ( DeMarco, 1998 ). In the whole product development process (PDP), product design is an important phase. The control and decision of product development is based on the pre-estimation of product design time (PDT). Nevertheless, PDP always means the brand-new or modified product design. Thus the cycle time of design process cannot be measured directly. Much attention has been focused on reducing the time/cost in product design ( Griffin, 1997a, b ; Bashir and Thomson, 2001 ; Seo et al., 2002 ), but little systematic research has been conducted into the time estimation. Traditionally, approximate design time is determined empirically by designers in companies. With the increase of market competition and product complexity, compa-nies require more accurate and creditable solutions.

To illustrate the time estimation method, the design of plastic injection mold is studied. The injection mold is a kind of single-piece-designed product and the design process is usually driven by customer orders. Some time factors with large influencing weights are gathered to develop a factor list, as shown in Table 5 . The first three factors are expressed as linguistic information and the last three factors are expressed as numerical data.
In this experiment, 72 sets of molds with corresponding design time are selected from past projects in a typical company. The detailed characteristic data and design time of these molds compose the corresponding patterns, as shown in Table 6 .We train the g -SVM with 60 patterns, and the others are used for testing. The simulation environment of Example 2 is the same as that of Example 1.

To analyze the estimating capability of ANPSO g -SVM, the models (ARMA, PSO v -SVM and ANPSO v -SVM) are also selected to handle the above series. Their estimating results are shown in Table 7 .

The indexes (MAE, MAPE and MSE) are also used to evaluate the estimating capability of three models shown in Table 8 .
The sample data of the latest 12 molds are used for testing set. The corresponding estimating results are used to analyze the forecasting performance of the above models. The conclusion can be obtained as follows: For the same v -SVM, the indexes (MAE, MAPE and MSE) provided by ANPSO are better than ones of
PSO. For the same ANPSO, the indexes (MAE, MAPE and MSE) provided by ANPSO g -SVM with Gaussian loss function are better than ones of ANPSO v -SVM with e -insensitive loss function.
Considering the non-linear influence from multidimensional series, it is obvious that g -SVM can handle the normal Gaussian noise from input series effectively. ANPSO is also available for the g -SVM to seek the optimal parameters.
 It is shown in Tables 2 and 3 of Example 1 and Tables 7 and 8 of
Example 2 that the selection of loss function is important for production sale forecasting. The optimal loss function in regres-2000 ). For the normal additive noise, the Gaussian loss function is the best choice. For the noise with symmetric density, Huber X  X  least modulus function performs best ( Cao and Wang, 2007 ).
Considering the enterprise environment, some errors exist inevitably in the process of data gather and estimation. Thus, the above forecasting (estimating) results are satisfying. The results of applications indicate that the forecasting (estimating) method based on g -SVM is feasible and effective. 5. Conclusion
In this paper, a new version of SVM, named g -SVM, is proposed to handle white noises from inputting data by the integration of
Gaussian loss function and n -SVM. The performance of the g -SVM is evaluated by the above two examples, and the results of the simulation demonstrate that the g -SVM is effective in dealing with white noises of uncertain data and finite samples. Moreover, it is shown that the parameter-choosing algorithm (ANPSO) presented here is available for the g -SVM to seek the optimal parameters.

Compared to ARMA, the g -SVM has some other attractive properties, such as the strong learning capability for these cases with a small quantity of data, the good generalization perfor-mance, the insensitivity to noise or outliers ( Cao and Wang, 2007 ) and the steerable approximation parameters. Compared to v -SVM, the g -SVM can penalize the white noise of data effectively.
In two experiments, the fixed adaptive coefficients ( b , l ), the second step control parameter D s of normal mutation and the attenuation parameter a of the control velocity are adopted.
However, how to choose an appropriate combinatorial coefficients are not explored deeply in this paper. The research on the velocity changes is a meaningful problem for future research when different above-mentioned parameters are adopted.
 Acknowledgements This work is supported by the National Natural Science Foundation of China under grant 60904043, China Postdoctoral Science Foundation (20090451152), Jiangsu Planned Projects for Postdoctoral Research Funds (0901023C) and Shanghai Education
Development Foundation Chenguang Project (2008CG55). We thank the editor-in-chief and three reviewers for their helpful comments that greatly improved the article.
 Appendix A. The proof of Theorem 1
Proof. Suppose H =( y  X  i , y  X  j ( x  X  i x  X  j )) l l y =( y 1 , y , y l ) T .If a i (*) =( a 1 , a * 1 , y , a dual problem (5), in the light of Karush X  X uhn X  X ucker (KKT) formulations: H  X  a a  X  y  X  b e  X  e e s  X  n  X  0  X  18  X 
H  X  a a  X  X  y b e  X  e e s  X  n  X  0  X  19  X  s  X  X  Z 0 ; x  X  X  Z 0 ; e Z 0  X  20  X  n The following inequality can be obtained by Eqs. (18) X (21): H  X  a a  X  y  X  b e Z e e n  X  22  X 
H  X  a a  X  X  y b e Z e e n  X  23  X  x  X  X  Z 0 ; e Z 0  X  24  X  (22) X (24), respectively.  X  w x i  X  b  X  y i r e  X  x i  X  25  X  y  X  w x i  X  b  X  r e  X  x i  X  26  X  n  X  X  Z 0 ; e Z 0  X  27  X 
According to Eq. (21), the original problem (4) can be described as follows: 1 2
J w J 2  X  C v e  X  1 2
Eq. (28) is equal to Eq. (5), namely, the original problem Eq. (4) is equal to the objective function of dual problem. According to Theorem 1. &amp; References
