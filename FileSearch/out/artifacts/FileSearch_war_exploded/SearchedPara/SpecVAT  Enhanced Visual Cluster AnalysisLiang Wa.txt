
Given a pairwise dissimilarity matrix D of a set of ob-jects, visual methods such as the VAT algorithm (for visual analysis of cluster tendency) represent D as an image I(  X  D where the objects are reordered to highlight cluster struc-ture as dark blocks along the diagonal of the image. A major limitation of such visual methods is their inability to highlight cluster structure in I(  X  D ) when D contains clus-ters with highly complex structure. In this paper, we address this limitation by proposing a Spectral VAT (SpecVAT) algo-rithm, where D is mapped to D in an embedding space by spectral decomposition of the Laplacian matrix, and then reordered to  X  D using the VAT algorithm. We also pro-pose a strategy to automatically determine the number of clusters in I(  X  D ) , as well as a method for cluster formation from I(  X  D ) based on the difference between diagonal blocks and off-diagonal blocks. We demonstrate the effectiveness of our algorithms on several synthetic and real-world data sets that are not amenable to analysis via traditional VAT.
A general question in the data mining community is how to organize observed data into meaningful structures or tax-onomies. As an exploratory data analysis tool, cluster anal-ysis aims at grouping objects of a similar kind into their respective categories. Given a data set O comprising n ob-clustering partitions the data into c groups C 1 ,C 2 ,  X  X  X  so that C i  X  C j = X  , if i = j and C 1  X  C 2  X  X  X  X  X  C c =
There have been a large number of clustering algorithms reported in the recent literature [24]. In general, clustering of unlabeled data poses three major problems: (1) assessing cluster tendency, i.e. , how many clusters to seek or what is the value of c ? (2) partitioning the data into c groups; and (3) validating the c clusters discovered. Given  X  X nly X  a pair-wise dissimilarity matrix D  X  X  n  X  n representing a data set of n objects, this paper addresses the first two problems in cluster analysis, i.e , determining the number of clusters c prior to clustering and partitioning the data into c clusters.
Most clustering algorithms require the number of clus-ters c as an input parameter, so the quality of the resulting clusters is largely dependent on the estimation of c . A num-ber of attempts have been made to estimate c [24, 13]. How-ever, most methods are post-clustering measures of cluster validity. In contrast, tendency assessment attempts to esti-mate c before clustering occurs. Visual methods for clus-ter tendency assessment for various data analysis problems have been extensively studied [7]. For data that can be pro-jected onto a 2D or 3D Euclidean space (which are com-monly depicted with a scatter plot), direct observation can provide a good insight on the value of c . However, the com-plexity of most real-world high-dimensional data, or the availability of only pairwise relational data, restricts the ef-fectiveness of this strategy.

The representation of data structures in an image format has a long and continuous history, e.g. , [11, 21, 2, 19]. Usu-ally, pairwise dissimilarity information about a set of ob-jects { o 1 ,o 2 ,  X  X  X  ,o n } is depicted as an n  X  n image, where the objects are reordered so that the resulting image is able to highlight potential cluster structure in the data. The in-tensity of each pixel in the image corresponds to the dis-similarity between the pair of objects addressed by the row and column of the pixel. A  X  X seful X  reordered dissimilarity image highlights potential clusters as a set of  X  X ark blocks X  along the diagonal of the image, and can be viewed as a visual aid to tendency assessment.

This paper focuses on one method for generating re-ordered dissimilarity images (RDIs), namely VAT (Visual Assessment of cluster Tendency) of Bezdek and Hathaway [2], although our approach can be applied to any method that generates RDIs. Several algorithms extend VAT for re-lated assessment problems, e.g. , bigVAT [10] offers a way to approximate the VAT reordered dissimilarity image for very large data sets, and coVAT [3] extends the VAT idea to rect-angular dissimilarity data to enable tendency assessment for co-clustering. However, while RDIs have been widely used for data analysis, they are usually only effective at high-lighting cluster tendency in data sets that contain compact well-separated clusters. Many practical applications involve data sets with highly complex structure, which invalidate the assumption of compact, well-separated clusters. We propose a new approach to generating RDIs that combines VAT with spectral analysis of pairwise data. The resulting Spectral VAT (SpecVAT) images can clearly show the num-ber of clusters and their approximate size for data sets with highly irregular cluster structures. We also propose an ef-fective strategy to measure the  X  X oodness X  of spectral VAT images for automated determination of the number of clus-ters c . In addition, we derive a visual clustering algorithm based on the spectral VAT image to partition the data into c groups. By integrating cluster tendency assessment and cluster formation using an RDI, we provide a natural envi-ronment for visual cluster validation and interpretation.
The remainder of the paper is organized as follows: Sec-tion 2 briefly reviews the VAT algorithm, and Section 3 il-lustrates our spectral VAT algorithm. Section 4 presents two applications of SpecVAT in cluster analysis, i.e. , determin-ing the number of clusters c and finding the c clusters from the SpecVAT image. The results of several examples are discussed in Section 5, prior to a summary in Section 6.
The VAT algorithm [2] works on a pairwise dissimilar-ity matrix. Let O = { o 1 ,o 2 ,  X  X  X  ,o n } denote n objects in the data and D a pairwise matrix of dissimilarities between objects, each element of which d ij = d ( o i ,o j ) is the dis-similarity between objects o i and o j , and generally, satisfies 1  X  d
The VAT algorithm displays a reordered dissimilarity matrix of D as a gray-scale image, each element of which is a scaled dissimilarity value d ij between objects o i and o .Let  X  () be a permutation of { 1 , 2 ,  X  X  X  ,n } such that  X  ( i ) is the new index for o i . The reordered list is thus { o p ij =1 if j =  X  ( i ) and 0 otherwise, then the matrix the reordered list is a similarity transform of D by P , Input :An n  X  n scaled matrix of pairwise dissimilarities
D =[ d 1  X  i, j  X  n (1): Set I = X  , J = { 1 , 2 ,  X  X  X  ,n } and  X  =(0 , 0 ,  X  X  X  (2): Repeat for t =2 , 3 ,  X  X  X  ,n (3): Form the reordered matrix  X  D =[  X  d ij ]=[ d  X  ( for 1  X  i, j  X  n .

Output : A scaled gray-scale image I (  X  D ) , in which max {  X  d ij } corresponds to white and min {  X  d ij } to black . The reordering idea is to find P so that  X  D is as close to a block diagonal form as possible. The VAT algorithm reorders the row and columns of D with a modified ver-sion of Prim X  X  minimal spanning tree algorithm. If an ob-ject is a member of a cluster, then it should be part of a sub-matrix with low dissimilarity values (corresponding to within-cluster distances), which appears to be one of the dark blocks along the diagonal of the VAT image I(  X  D ) , each of which corresponds to one cluster. To make the paper self-contained, we summarize the VAT algorithm in Table 1.
An example of VAT is shown in Figure 1. Figure 1( left ) is a scatter plot of n = 3000 points in R 2 , which is gener-ated from a mixture of c =5 bi-variate normal distributions. These data points were converted to a 3000  X  3000 dissim-ilarity matrix D by computing the Euclidean distance be-tween each pair of points. The 5 visually apparent clusters in Figure 1( left ) are reflected by the 5 distinct dark blocks along the main diagonal in Figure 1( right ), which is the VAT image of the data. Given the image of D in the original input order in Figure 1( middle ), reordering is necessary to reveal the underlying cluster structure of the data.
Figure 1. VAT: ( left ) scatter plot of a 2D data set, ( middle ) unordered image I( D ) ,and( right ) reordered VAT image I(  X  D )
Two points about VAT are noted here: 1) only a pair-wise dissimilarity matrix D is required as the input. When vectorial forms of objects are available, it is easy to con-vert them into D using any dissimilarity measure. Even when vectorial data are unavailable, it is still feasible to use certain flexible metrics to compute a pairwise dissimilarity matrix, e.g. , using DTW (Dynamic Time Warping) to match sequences of different lengths. 2) Although the VAT image suggests both the number of and approximate members of object clusters, matrix reordering produces neither a parti-tion nor a hierarchy of clusters. It merely reorders the data to reveal its hidden structure, which can be viewed as an illustrative data visualization for estimating the number of clusters prior to clustering. However, hierarchical structure can be detected from the reordered matrix by the presence of diagonal blocks within larger diagonal blocks.
At a glance, a viewer can estimate the number of clus-ters c from a VAT image by counting the number of dark blocks along the diagonal if these dark blocks possess vi-sual clarity. However, this is not always possible. Note that a dark block appears in the VAT image only when a tight (or ellipsoidal) group exists in the data. For complex-shaped data sets where the boundaries between clusters be-come less distinct due to either significant overlap or irregu-lar geometries between different clusters, the resulting VAT images will degrade, e.g. , non-distinct boundaries and an unclear diagonal form. See Figures 5(a) and 6(a) for exam-ples. Accordingly, viewers may deduce different numbers of clusters from such poor-quality images, or even cannot estimate c at all. This raises a problem of whether we can transform D into a new form D so that the VAT image of
D can become clearer and more informative about the cluster structure. In this paper, we address this problem by combining VAT with spectral graph analysis.

Recently a number of researchers have used spectral analysis of a graph in applications such as graph embed-ding for dimensionality reduction [1, 26], image segmenta-tion [23, 20] and data clustering [22]. These spectral meth-ods generally use the eigenvectors of a graph X  X  adjacency (or Laplacian matrix) to construct a geometric representa-tion of the graph. Different methods are strongly connected, e.g. , Laplacian Eigenmaps [1] is very similar to the map-ping procedure used in a spectral clustering algorithm de-scribed in [15]. Let G ( V ,E, W ) be a weighted undirected graph, where V is a set of n vertices ( e.g. , corresponding to n objects { o 1 ,o 2 ,  X  X  X  ,o n } ), E =[ e ij ] is the edge set with e ij =1 showing that there is a link between vertices i and j and 0 otherwise, and W =[ w ij ] ,a n  X  n affinity matrix, includes the edge weights, with w ij representing the rela-tion of the edge connecting vertices i and j . Most spectral methods differ in terms of the following aspects: 1) Differ-ent graphs , reflected in E , e.g. ,the  X  -neighborhood graph (connect all vertices whose pairwise distances are smaller than  X  ); the K -nearest neighbor graph (connect vertices i and j if o j is among the K nearest neighbors of o i and/or o
Input : D =[ d ij ] :an n  X  n scaled matrix of pairwise dissimilarities, with 1  X  d ij  X  0; d ij = d ji ; d ii =0 ,for 1  X  i, j  X  n mension of the embedding subspace). (1): Compute a local scale  X  i for each object o i using where o K is the K -th nearest neighbor of o i . a (2): Construct the weighting matrix W  X  X  n  X  n by defining w ij =exp(  X  d ij d ji / (  X  i  X  j )) for i = j , and (3): Let M to be a diagonal matrix with m ii = n j =1 w ij ( i.e. ,the ( i, i ) element of M is the sum of W  X  X  i -th row), and construct the matrix which is a normalized version of the Laplacian matrix. b (4): Choose v 1 , v 2 ,  X  X  X  , v k ,the k largest eigenvectors of
L to form the matrix V =[ v 1 ,  X  X  X  , v stacking the eigenvectors in columns. (5): Normalize the rows of V with unit Euclidean norm to generate V  X  X  n  X  k , i.e. , v ij = v ij / v i : . (6): For i =1 , 2 ,  X  X  X  ,n ,let u i  X  X  k be the vector cor-responding to the i -th row of V and treat it as a new instance in the k -dimensional embedding space (corre-sponding to original o i ). c Then construct a new pair-wise dissimilarity matrix D between objects by defining d (7): Apply the VAT algorithm to D to obtain I(  X  D ) .
Output : Spectrally-mapped and reordered dissimilarity matrix  X  D and its corresponding scaled gray-scale image
I(  X  D ) their corresponding gray-scale histograms is among the K nearest neighbors of o j ), and the fully con-nected graph (simply connect all vertices with each other); 2) Different weighting functions , reflected in W , e.g. ,sim-ple 0-1 weighting or the commonly used Gaussian similar-ity function s ( f i , f j ) = exp(  X  f i  X  f j with a global scaling parameter  X  ; and 3) Different graph Laplacians , e.g. , the unnormalized Laplacian matrix L = M  X  W and the normalized version  X  L = M  X  1 / 2 LM  X  1 / where M is a diagonal matrix whose elements are the de-grees of the nodes of G , i.e. , m ii = n j =1 w ij .
The spectral decomposition of the Laplacian matrix pro-vides useful information about the properties of the graph [6]. It has been shown experimentally that natural groups in the original data space may not correspond to convex re-gions, but once they are mapped to a spectral space spanned by the eigenvectors of the Laplacian matrix, they are more likely to be transformed into tight clusters [22]. Based on this observation, we wish to embed D in a k -dimensional spectral space, where k is the number of eigenvectors used, such that each original data point is implicitly replaced with a new vector instance in this new space. Such an embed-ding for the data comes from approximations to a natural map that is defined on the entire data manifold [1]. After a comprehensive study of recent spectral methods, we adopt a combination of adjacency graph, weighting function and graph Laplacian for obtaining better graph embedding (and thus better SpecVAT images, see Figures 5(b) and 6(b)). We summarize our spectral VAT algorithm in Table 2.
Clustering in unlabeled data O is the assignment of la-bels to the objects in O , where two necessary ingredients are respectively the number of groups to seek, c , and a par-titioning method to discover the c clusters. We explore the use of spectral VAT for these two problems. That is, we wish to answer the following two questions: (Q1) Can we automatically determine the number of clusters c , as suggested by I(  X  D ) , in an objective manner, without viewing the visual display? This enables us to capi-talize on the information possessed by the SpecVAT image. (Q2) Can we automatically extract a crisp c -partition of O , which is suggested by the visual evidence in I(  X  D ) ?If so, how well does it perform?
Figure 2(a) shows an example of the original VAT im-age and SpecVAT images with different numbers of eigen-vectors (corresponding to synthetic data S-1 in Figure 4). We can see that the SpecVAT images are generally clearer than the original VAT image in revealing real data struc-ture. See Figures 5 and 6 for more examples. To en-able automatic determination of the number of clusters, we need to find a  X  X est X  SpecVAT image in terms of  X  X larity X  and  X  X lock structure X . The corresponding gray-scale his-tograms in Figure 2(b) suggest that a good SpecVAT image should, ideally, include two explicit modalities in the distri-butions, ideally with a narrow distribution of each modality and a large distance between the two modalities. It is eas-ily understood that the two modalities in the histogram im-plicitly correspond to  X  X ithin-cluster distances X  (diagonal dark-block regions) and  X  X etween-cluster distances X  (off-diagonal non-dark-block regions). A narrow distribution for any one modality means that values in either  X  X ithin-cluster distances X  or  X  X etween-cluster distances X  are close, whereas a big distance between two modalities means that these two modalities are easily distinguished.

A nonparametric thresholding method for image bina-rization is proposed in [17], where only the gray-level his-togram suffices without other prior knowledge. We bor-row its idea of deriving an optimal threshold to establish an Figure 3.  X  X oodness X  measures of original VAT and SpecVAT images with different k
Input : D =[ d ij ] :an n  X  n scaled matrix of pairwise dissimilarities, with 1  X  d ij  X  0; d ij = d ji ; d ii =0 ,for 1  X  i, j  X  n of the eigenvectors used in spectral VAT
For k =1 to k max can maximize  X  2 B for the image I(  X  D k ) , i.e. , T sure GM ( k )=  X  2 B ( T  X  k ) .
 End
Output : The number of clusters c = arg max k GM ( k ) . appropriate criterion for evaluating the  X  X oodness X  of the SpecVAT images from a more general standpoint. Let the pixels of an image be represented in L gray levels. The number of pixels at level l is denoted by m l and the to-tal number of pixels by N = L l =1 m l . Such a gray-level histogram may be normalized and regarded as a probability distribution, i.e. , p l = n l /N , p l &gt; 0 , L l =1 p
Suppose that the image pixels can be divided into two classes C 1 and C 2 ( e.g. , implicitly corresponding to  X  X ithin-cluster blocks X  and  X  X etween-cluster blocks X  in the VAT or SpecVAT image) by a threshold at level T . C 1 de-notes pixels with levels [1 ,  X  X  X  ,T ] and C 2 denotes pixels with levels [ T +1 ,  X  X  X  ,L ] . Then the probabilities of class occurrence are respectively  X  1 = P ( C 1 )= T l =1 p l and  X  2 = P ( C 2 )= L l = T +1 p l . The class mean levels are  X  where  X  ( T )= T l =1 p l and  X  ( T )= T l =1 lp l are the zeroth-and the first-order cumulative moments of the his-togram up to the T -th level, respectively, and  X  l =1 lp l is the total mean level of the original image. Note that  X  1  X  1 +  X  2  X  2 =  X  L and  X  1 +  X  2 =1 . The class vari-ances are then given by  X 
Based on the discriminant criteria, Otsu used the follow-ing measures for evaluating the class separability [17] where are the within-class variance, the between-class variance, and the total variance of levels, respectively. A good choice of threshold is to solve the optimization problem by maxi-mizing  X  ,  X  or  X  (these discriminant criteria are equivalent to one another). Note that  X  2 W and  X  2 B are functions of T ,but  X 
T is independent of T . In particular,  X  second-order statistics (class variances), while  X  2 B is based on the first-order statistics (class means). Thus  X  is the sim-plest measure to obtain an optimal threshold T  X  .
Naturally, the maximum value  X  ( T  X  ) can be used as a measure to evaluate the separability of classes (or ease of thresholding) for the image or the bi-modality of the his-togram [17]. Such a measure is semantically consistent with our knowledge of the field in question. For each SpecVAT image with respect to different k ( e.g. , k =1  X  k max ), we can find an optimal threshold T  X  that maximizes  X  (or equivalently maximizes  X  2 B ). We denote the value of  X 
B ( T cordingly, we select the best SpecVAT image as the one with the maximum goodness value, and determine the number of clusters as We give two examples of  X  X oodness X  values in Figure 3, showing the effectiveness of such a measure in determining a relatively good SpecVAT image ( e.g. , k =3 for S-1 and k =4 for S-5). Table 3 summarizes the algorithm for Au-tomatically Determining the Number of Clusters (ADNC) from D .
Input : I(  X  D ) : image generated from n object samples; (1): Set the genome of each individual x i ( i =1 ,...,b ) as a binary string of length n  X  1 , corresponding to the indices of the first n  X  1 samples. (2): Create the initial population: randomly set c  X  1 ele-ments in each x i to  X 1 X  and others to  X 0 X . (3): Set the fitness function as taking the input x i , cal-culating the candidate partition U from x i , and returning the result of Eq. (18). (4): Apply the Genetic Algorithm to start the evolution until there is no improvement within g =10 generations to find the optimum genome x  X  . (5): Transform x  X  into cluster partition U  X  . The position p 1 of the first  X 1 X  in x  X  means the first cluster partition is from sample 1 to p 1 . The position p j ( j =2 ,...,c  X  1) of the j -th  X 1 X  means a the j -th cluster partition is from sam-ple ( p j  X  1 +1) to p j . The last (the c -th) cluster partition is from sample ( p c  X  1 +1) to n .

Output : The sizes of each cluster { n 1 ,  X  X  X  ,n c gether with the permutation index  X  () obtained during re-ordering, real object indices in each cluster C i be retrieved, i.e. , C 1 = { o  X  (1) ,  X  X  X  ,o  X  ( n 1 ) } { We now consider how to find data clusters from a given RDI, in which the proximity matrix has been reordered as close as possible to a block diagonal form. The c -partitions of
O are generally sets of c  X  n values u niently arrayed as a c  X  n matrix U =[ u ik ] .Thesetofall non-degenerate c -partition matrices for O is Element u ik of U is the membership of object k in cluster i . In the case of  X  X risp X  partition (not fuzzy or probabilistic), u ik =1 if o k is labeled i and 0 otherwise.

The important property of I(  X  D ) is that it has, beginning in the upper left corner, dark blocks along its main diagonal. Accordingly, we can constrain our search through H hcn for agiven c under consideration to those partitions that mimic the block structure in I(  X  D ) [9]. U in H hcn is called an aligned c -partition of O when its entries form c contiguous blocks of 1 X  X  in U , ordered to begin from the upper left corner, and proceeding down and to the right, i.e. , with the properties of u 1 k =1 , 1  X  k  X  n 1 ; u ik = 1 ,n i  X  1 &lt;k  X  n i , 2  X  i  X  c . The special nature of aligned partitions enables us to specify them in an alternative form. Every member of H  X  hcn is isomorphic to the unique set of c distinct integers, i.e. , the cardinalities of the c clusters in U , that satisfy { n i | 1  X  n i ;1  X  i  X  c ; c i =1 n i = n aligned partitions can be specified by { n 1 :  X  X  X  : n c } U =[11000;00110;00001]= { 2:2:1 } .

The important characteristics of I(  X  D ) that can be ex-ploited for finding a good U are the contrast difference be-tween the dark blocks along the main diagonal and the pix-els adjacent to them. The proposed algorithm aims to gener-ate candidate partitions in H  X  hcn by testing their fitness to the clusters suggested by the aligned dark blocks in I(  X  D ) .To-wards this end, an objective function is defined to implicitly account for two components of block structures:  X  X quare-ness X  and  X  X dginess X . An intuitively appealing measure is the difference of the mean dissimilarity values between ap-parent clusters ( i.e. , dissimilarities in non-dark blocks off-diagonal) and those within apparent clusters ( i.e. , dissimi-larities in dark blocks along the diagonal).

Let U be a candidate partition H  X  hcn , { C i , 1  X  i  X  c the crisp c -partition of O corresponding to U , | C i | = n and we abbreviate the membership o s  X  C i as s  X  i . Mean dissimilarity between dark and non-dark regions in I(  X  D ( i.e. , between-cluster distances) E b and mean dissimilarity within dark regions in I(  X  D ) ( i.e. , within-cluster distances) E w are respectively represented by The objective function is defined as A good U should maximize this objective function, i.e ,
In principle, a number of optimization algorithms might be used. We use a Genetic Algorithm (GA) [8]. As a partic-ular class of evolutionary algorithms, a genetic algorithm is implemented as a computer simulation in which a popula-tion of abstract representations (called a genome) of candi-date solutions (called individuals) to an optimization prob-lem evolves toward better solutions. The evolution usually starts from a population of randomly generated individuals and happens in generations. In each generation, the fitness of every individual in the population is evaluated, multi-ple individuals are stochastically selected from the current population and modified to form a new population that is then used in the next iteration. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population, or there is no further improve-ment within a number of generations. Our visual clustering procedure is summarized in Table 4, leading to a new clus-tering algorithm in which the final randomly initialized K -means stage (as used in spectral clustering) is eliminated.
Figure 4. Scatter plots of 9 synthetic data sets. From left to right and from top to bot-tom: S-1  X  S-9
In order to test our algorithms, we carried out a number of experiments on 9 artificially generated data sets, as well as 6 real-world data sets. Unless otherwise mentioned, in the following experiments the (Euclidean) distance matrix D is computed in the original attribute space (if the object vectorial representation is available).
Synthetic data: Nine synthetic data sets with different data structures are used in our experiments. The scatter plots of these synthetic data sets are shown in Figure 4, in which each color represents a visually meaningful group. The first 6 data sets are from [27] 2 , and the last 3 are gen-erated by ourselves. Except for S-7, which is a mixture of 3 Gaussian shapes, all other data sets involve more irregu-lar data structures, in which an obvious cluster centroid for Data c # attri. n [ n 1 ,  X  X  X  ,n c p ] S-1 3 2 299 [61 99 139] S-2 3 2 303 [95 106 102] S-3 3 2 266 [73 118 75] S-4 5 2 622 [150 111 136 109 116] S-5 4 2 512 [150 122 123 117] S-6 3 2 238 [100 56 82] S-7 3 2 1000 [291 519 190] S-8 2 2 2000 [1000 1000] S-9 3 2 2000 [500 1000 500] R-1 2 9 683 [444 239] R-2 3 1200 1755 [585 585 585] R-3 3 -194 [21 87 86] R-4 3 4 150 [50 50 50] R-5 2 16 435 [267 168]
R-6 3 13 178 [59 71 48] each group is not necessarily available. In addition, some of them include different scales between clusters, or some clusters are hidden in a cluttered background.

Real data: Six real-world data sets were also consid-ered to evaluate our algorithms, four of which are from the UCI Machine Learning Repository 3 . These data sets are summarized as follows: a) R-1: Breast-cancer database in-cludes 699 instances, each of which has 9 attributes and be-longs to one of 2 classes. Since there are 16 instances that contain a single missing attribute value, we removed them and used the remaining 683 instances for our experiment. b) R-2: This data set was used in [4]. It contains single-light-source Face images of 3 different individuals, each seen under 585 viewing conditions. Each original image was down-sampled to 30  X  40 pixels, thus providing in total 1755 images with 1200 dimensions ( i.e. , 30  X  40 ). c) R-3: Genetic data set is originally from the work in [18], which is a 194  X  194 matrix consisting of pairwise dissimilarities from a set of 194 human gene products that were clustered into three protein families. d) R-4: Iris data set contains 3 physical classes, 50 instances each, where each class refers to a type of iris plant and the attributes of each instance in-clude 4 numeric values. e) R-5: Voting data set consists of 435 US House of Representatives members X  votes on 16 key votes (267 democrats and 168 republicans). Votes were numerically encoded as 0.5 for  X  X ea X , -0.5 for  X  X ay X  and 0 for  X  X nknown disposition X , so that the voting record of each congressman is represented as a ternary-valued vec-tor in R 16 .f)R-6: Wine data set contains the results of a chemical analysis of wines grown in the same region, but derived from 3 different cultivars. The analysis determines the quantities of 13 constituents found in each of three types of wines. The total number of instances is 178 . 5.2. Determining c
The characteristics of the synthetic and real data sets are clearly summarized in Table 5. For each of them (except for R-5), we computed a pairwise dissimilarity matrix D in the original attribute space. The VAT images are shown in Figure 5(a) for synthetic data and Figure 6(a) for real data. It can be seen that the cluster structure of the data in these VAT images is not necessarily clearly highlighted. Accordingly, viewers have difficulties in giving a sound re-sult about the number of clusters in these data sets, and dif-ferent viewers may deduce different estimations of c . Fur-ther, we performed the SpecVAT algorithm, and showed SpecVAT images in Figure 5(b) for synthetic data and Fig-ure 6(b) for real data. In contrast to the original VAT im-ages, the SpecVAT images have generally clearer displays in terms of block structure, thus better highlighting the hid-den cluster structure. Table 6 summarizes the number of clusters determined from SpecVAT images automatically, along with the results estimated from the VAT/SpecVAT im-ages using manual inspection by the authors for compari-son. For iris, our method gives c =2 . This is due to that in this data set, one class is linearly separable from the other two classes, while the latter two are not linearly separable from each other. The results of cluster number estimation from the SpecVAT images for other 14 data sets are accurate in terms of the number of real physical classes, whether it was estimated automatically by our ADNC algorithm or by manual inspection. The results again highlight the benefits of converting D to D by graph embedding for obtaining Table 7. Clustering algorithm comparison (%) more accurate estimation of c .
We evaluate our visual clustering algorithm X  X  perfor-mance by comparing the cluster labels of the objects given by our algorithm with the ground-truth labels. An accu-racy ( AC ) metric has been widely used for clustering perfor-mance evaluation [5, 16, 25]. Suppose that z c i is the cluster-ing label of an object o i and z g i is the ground truth label, AC is defined as max map n i =1  X  ( z g i ,map ( z c i )) /n , where n is the total number of objects in the data,  X  ( z 1 ,z 2 ) is the delta function that equals 1 if and only if z 1 = z 2 and 0 otherwise, and map is the mapping function that permutes clustering labels to match equivalent labels given by the ground truth. The Kuhn-Munkres algorithm is usually used to obtain the best mapping [12]. The clustering accuracy of our VC al-gorithm on the original VAT image ( V ov ) and the SpecVAT image ( V sv ) is summarized in Table 7, from which we can see that no doubt V sv obtains better results than V ov .
We also implemented several typical clustering algo-rithms for comparison. These algorithms are K -means, and Ward X  X  hierarchal clustering algorithm [14], spectral clus-tering with a global scale  X  [15], and spectral clustering with local scale  X  i [27]. For the K -means algorithm, we reported the average accuracy of the result over 100 tri-als. The clustering accuracies of these algorithms are listed in Table 7, in which the best results for each data set are bolded. From Table 7, we can see that the overall accuracy of our clustering algorithm on the SpecVAT image is bet-ter than that of K -means, Ward X  X  algorithm and standard spectral clustering with a global scale, and is comparable to that of spectral clustering with local scaling. Moreover, our visual methods give intuitive observations on the num-ber of clusters, cluster structure and partition results, as well as eliminating the randomly initialized K -means stage (as usually used in spectral clustering).
There are strong relations between the SpecVAT algo-rithm (and thus VC) and other works: both the SpecVAT algorithm and the spectral clustering algorithm described in [15] use spectral decomposition of the normalized Lapla-cian matrix that is essentially the graph embedding proce-dure of [1]. A prominent property of the graph embedding framework is the complete preservation of the cluster struc-ture in the embedding space. For new representations in the embedding space, spectral clustering in [15] uses K -means to cluster them; while for our visual clustering algo-rithm, we first convert them to a new reordered image (cor-responding to a new pairwise dissimilarity matrix), and then use the GA to partition its block structures. A local scal-ing scheme is suggested in [27] to replace the global scale  X  in [15], leading to better clustering, especially when the data includes multiple scales or when the clusters are placed within a cluttered background. These connections naturally suggest that our VC algorithm can perform competitively with spectral clustering [27, 15]. The slight difference in accuracy between VC and the algorithm of [27] could be due to the difference between different objective functions and optimization strategies in the partitioning stage.
Our algorithms will probably reach their useful limit when the image formed by any reordering of D is not from a well-structured dissimilarity matrix. While our ADNC algorithm may return a slightly over-estimated or underes-timated value of c , it provides an initial estimate of the clus-ter number, thus avoiding running a clustering algorithm multiple times over a wide range of c in an attempt to find valid clusters. In this way, our method compares favorably to post-clustering validation methods in computational effi-ciency. Note that our method does not eliminate the need for cluster validity ( i.e. , the third problem in cluster analysis), but it improves the probability of success.

There are other methods besides VAT that produce RDIs, e.g. , [11, 21, 19]. We are thus not restricted to using VAT, and can apply our algorithms to the output of any method for finding reordered dissimilarity images.
This paper has presented a new visual technique for auto-matically determining the number of clusters and partition-ing either object or pairwise relational data. Our contribu-tions are summarized as follows: 1) The VAT algorithm is enhanced by using spectral analysis of the proximity matrix of the data. The new SpecVAT algorithm can better reveal the hidden cluster structure, especially for complex-shaped data sets. 2) Based on spectral VAT, the cluster structure in the data can be reliably estimated by visual inspection. As well, we propose a  X  X oodness X  measure of SpecVAT im-ages for automatically determining the number of clusters. 3) We derive a visual clustering algorithm based on Spec-VAT images and its unique block-structured property. 4) We perform a series of primary and comparative experiments on 9 synthetic data sets and 6 real-world data sets, and obtain encouraging results in terms of the first two major problems in cluster analysis.
