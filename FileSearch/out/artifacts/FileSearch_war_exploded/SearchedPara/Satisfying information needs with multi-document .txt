 1. Introduction
Multi-document summarization systems have traditionally focused on distilling the most globally relevant pieces of information contained within a collection of documents into a short passage that can be read  X  and digested  X  quickly by an end-user. While summaries can provide users with valuable information about the range of information contained in a corpus, multi-document summaries are often of limited value in an infor-mation-gathering environment where users seek particular types of information in order to perform a specific task or to achieve a research goal. Even though the content of a summary could potentially address some of the information needs of a user, many relevant information  X  X  X uggets X  X  may not be sufficiently relevant to war-rant inclusion in a  X  X  X lobal X  X  summary, despite having been mentioned in a collection of documents. In order to address the shortcomings of  X  X  X ndirected X  X  multi-document summaries, the recent 2005 and 2006
Document Understanding Conferences (DUC) have required participants to provide summary answers in response to research scenarios consisting of one (or more) complex questions. These types of summaries  X  which we will refer to as question-directed summaries (QDS)  X  require systems to generate coherent, passage-length answers that are both relevant to the topic of a collection of documents and respond the infor-mation needs of users as well.
In this paper, we introduce a new framework for question-directed summarization which combines the question decomposition and answer retrieval techniques pioneered for complex question-answering systems with a novel textual inference-based method for estimating the relevance of content included in a summary.
First, we show that by leveraging different combinations of question representation, passage retrieval, and relevance ranking techniques, we can generate sets of candidate summaries which address different dimensions of the user X  X  information needs. Once these candidate summaries have been created, we then use textual infer-ence in order to create models of the semantic content shared by these candidates in order to identify the most responsive candidate summary generated in response to a question.

Complex questions  X  such as those  X  X  X sked X  X  by the DUC organizers  X  cannot be answered with the same techniques that have so successfully been applied to answering  X  X  X actoid X  X  questions ( Harabagiu et al., 2001;
Harabagiu et al., 2003; Moldovan et al., 2002 ). Unlike informationally-simple factoid questions, complex questions seek multiple types of information simultaneously and do not presuppose that one single answer it can be safely assumed that the submitter of the question is looking for a country (or an organization) which is associated with the Falkland Islands. However, with a complex question like  X  X  How have relations between have been established since the conclusion of the 1982 war. In order to answer complex questions precisely, we believe that question-answering  X  as well as question-directed summarization  X  systems need to employ ques-tion decomposition strategies capable of understanding the range of information needs presupposed by a user X  X  question. The question-directed summarization framework we introduce in this paper tackles the processing of complex questions with three question decomposition strategies that approximate the information sought by a complex question; these three question decomposition strategies are then used with two different passage retrieval engines, resulting in a total of six different sets of candidate passages which can be assembled into summary-length answers.

Unlike complex question-answering systems, question-directed summarization systems must do more than retrieve information relevant to a user X  X  information needs. In order to meet the structural and length con-straints imposed on summaries, QDS systems also need to include methods for estimating the inherent respon-siveness of each sentence included in the summary returned to a user. Our QDS framework relies on a form of textual inference  X  known as textual entailment  X  in order to automatically identify the content that is most responsive to a user X  X  information needs. In this work, we use a state-of-the-art system for recognizing textual ment with greater than 75% precision. 1 In this paper, we show that by considering the entailment relationships that exist between sentences originating in different summaries, we can construct hierarchical representations of relevance, similar to the hierarchical content models (or  X  X  X yramids X  X ) first proposed in Nenkova and Pas-sonneau (2004) . Once constructed, we use these hierarchical representations in order to model the semantic content of an ideal summary response to a question and to select the most responsive summary  X  without the need for expensive and time-consuming human annotations.

The remainder of this paper is organized as follows. Section 2 presents an overview of GISTexter, our QDS system. Section 3 describes techniques for performing the decomposition of complex questions. Section 4 pro-vides details of our system for recognizing textual entailment and describes how we used these types of infer-ential relationships to create content models that can be used in a summarization context. Section 5 details the results from evaluations performed using this framework, while Section 6 summarizes our conclusions. 2. The GISTexter system
In this section, we describe GISTexter, our question-directed summarization (QDS) system. (The architec-ture of GISTexter is provided in Fig. 1 .)
GISTexter begins the process of QDS by submitting complex questions to a Question Processing Module , which employs three different types of question decomposition techniques in order to represent the types of information sought by a question. First, keywords are extracted heuristically from the question. Second, ques-tions are decomposed syntactically in order to extract each overtly-mentioned query from the text of the com-plex question. Third (and finally), questions are decomposed semantically in order to identify the complete set of queries that are presupposed by the meaning of the complex question itself.

Once question processing is complete, the sets of queries (or sub-questions) generated by each of the three question decomposition strategies are then simultaneously sent to two different summarization engines: (1) a question-focused summarization (QFS) system and (2) a multi-document summarization system . While GISTex-ter X  X  question-focused summarization system employs techniques inspired by the work first pioneered for tex-tual question-answering (Q/A), its multi-document summarization system, known as Lite-GISTexter, uses a battery of lightweight natural language processing techniques for relevance estimation first developed for the Document Understanding Conference (DUC) summarization evaluations. We believe that by combining these two summarization system within the same architecture, GISTexter can retrieve more relevant sentences than frameworks that utilize only one type of retrieval engine.

The three different query representations produced by the question processing modules are then sent to each of these two summarization engines in order to retrieve a total of six different sets of relevant sentences.
Each of these six sets of sentences are then sent to a Summary Generation module to be compiled into a fixed-length summary answer. ( Table 1 lists the six different summarization strategies currently employed in GISTexter.)
GISTexter uses a framework inspired by recent approaches to evaluating multi-document summaries ( Nenkova &amp; Passonneau, 2004 ) in order to select the most responsive summary from the set of candidate sum-maries it generates. Recent work ( Nenkova &amp; Passonneau, 2004 ) has argued that multiple summaries created for the same topic  X  or in response to the same question  X  can be used in order to create a hierarchical model (known as a  X  X  X yramid X  X ) of the content that an  X  X  X deal X  X  summary should contain. Under this model, the text of a multi-document summary or summary answer is assumed to encode a set of summary content units (SCUs) which represent the set of propositions that the author of the summary believes to be the most relevant content contained in a set of documents. Although different authors will ultimately use different models of relevance when creating a summary, this model assumes that content that is common to multiple summaries will gen-erally prove to be more relevant than content that is contained in one (or only a few) summaries. Given these assumptions, this approach predicts the best summaries will contain the greatest concentration of SCUs that appear multiple times in a  X  X  X old standard X  X  set of summaries.

While Nenkova and Passonneau X  X  (2004) summarization evaluation methodology depends on the use of human annotators in order to identify  X  and track the distribution of  X  SCUs across a set of human-created summaries, we believe that recent advances in the recognition of a form of textual inference  X  known as textual entailment  X  could be leveraged in order to (1) create  X  X  X yramid X  X -like content models from a group of  X  X  X eer X  X  summaries and to (2) score summaries based on the number and type of SCUs they contain. Introduced by
Dagan, Glickman, and Magnini (2005) , the task of recognizing of textual entailment (TE) requires systems to determine whether the meaning of a sentence (referred to as a hypothesis ) can be reasonably inferred from the meaning of another sentence (referred to as a text ). While TE was not intended a measure of semantic equivalence, we believe that systems for TE can prove useful in identifying content that is common across a group of summaries.

Once GISTexter generates a set of summaries, the textual entailment system (TES) described in Hickl et al. (2006) is used in order to create a  X  X  X yramid X  X  model of the content common to the candidate summaries. This model is then used in conjunction with the TES to score each of the six candidate summaries using the Mod-ified Pyramid scoring algorithm described in Passonneau, Nenkova, McKeown, and Sigelman (2005) ; the indi-vidual summary that receives the highest Pyramid score is then returned to the user. 2.1. The question-focused summarization system Most current textual question-answering (Q/A) systems ( Harabagiu et al., 2001; Harabagiu et al., 2003; language question. First, questions undergo Question Processing in order to (1) extract keywords that can be used to retrieve candidate answers and (2) identify the expected answer type of the question. Second, keywords extracted from the question are submitted to a Document Processing module in order to retrieve a set of relevant (paragraph-length) text passages. (At this point, text passages that do not contain any entity of the same semantic class as the expected answer type of the question are generally filtered.) Third (and finally), passages are sent to an Answer Processing module responsible for pinpointing exact answers from the set of retrieved text passages. Lists of candidate answers are ultimately presented to a user, ranked in order of their expected relevance to the user X  X  question.

While question-answering systems have proven successful in identifying answers to questions from individ-ual documents ( Harabagiu, Moldovan et al., 2005 ), we believe that this traditional Q/A architecture (illus-trated in Fig. 2 a) needs to be modified in order to produce the types of summary-length answers sought in question-directed summarization task. In this paper, we refer to this type of Q/A-inspired architecture for QDS as a question-focused summarization (QFS) system. (The architecture of the QFS system integrated into GISTexter architecture is presented in Fig. 2 b.)
As with a traditional Q/A system, questions in a question-focused summarization system are initially sent to a Question Processing module. Queries formed during Question Processing are sent to a Sentence Retrieval module which retrieves (and ranks) a relevant set of sentences, based on (1) the number and proximity of ques-tion keyword terms found in each sentence and (2) the presence of entities of the same semantic class as the expected answer type. Retrieved sentences are sent to a Summary Generation module, which compiles the top-ranked sentences into a coherent, fixed-length summary. 2.2. Multi-document summarization with Lite-GISTexter Lite-GISTexter is a stand-alone multi-document summarization (MDS) system that was evaluated in the
DUC 2004 and DUC 2005 evaluations ( Lacatusu, Hickl, Harabagiu, &amp; Nezda, 2004; Lacatusu, Hickl, Aars-eth, &amp; Taylor, 2005 ). (The architecture of LiteGISTexter is provided in Fig. 3 .)
With LiteGISTexter, keywords extracted from a question are initially used to retrieve a set of documents relevant to the question. These documents are then used to compute two different types of topic representa-
Lin and Hovy (2000) , we assumed that the topic of a collection of documents can be represented by a topic signature TS 1  X h topic ;  X  t 1 ; w 1  X  ; ... ;  X  t n ; w addition to the term-based TS 1 representation, we also followed Harabagiu (2004) in computing enhanced topic signatures for each retrieved set of documents. With enhanced topic signatures, topics are characterized by representative relations that exist between TS 1 terms: TS ary relation between two topic concepts. The weights associated with TS with keywords extracted from the question in order to compute a composite topic score for each sentence in the document collection. Sentences are then ranked according to topic score; the top-ranked sentences are then sent to a Summary Generation module to be compiled into a fixed-length summary. 2.3. Summary generation
In our QFS-based and MDS-based systems, summaries are generated by selecting the top-ranked sentences from a list of sentences returned during Sentence Retrieval and merging them into a single paragraph of pre-determined length. 2 Two types of optimizations were performed in order to enhance the overall linguistic quality of summaries.
First, in order to reduce the likelihood that redundant information would be included in a summary, sentences selected for a candidate summary were clustered using k -Nearest Neighbor clustering based on cosine similar-ity. Following clustering, only the top-ranked sentence from each cluster was included in the summary. (An example of a cluster can be found in Table 2 .)
Second, we sought to enhance the referential clarity of summaries by developing a set of heuristics that would allow a system to automatically predict whether the antecedent of a pronoun could be (1) found in the current sentence, (2) found in the preceding sentence, or (3) not found without the use of a pronoun re-solution system. While we are still committed to integrating a state-of-the-art coreference resolution system into the architecture of GISTexter, we believe that by being able to predict which pronominal mentions could be included in a summary without adversely impacting referential clarity, we can enhance both the legibility and coverage of multi-document summaries without significant increasing the overhead required by a summa-rization system.

In a pilot study using a decision-tree-based classifier, we found that we could predict both the form and location of the antecedent of pronouns occurring in subject position with approximately 74% precision. We trained two classifiers using newspaper texts annotated with coreference information. For each instance of a pronoun, the first classifier learned whether an antecedent could be found in (1) the current sentence, (2) the preceding sentence, or (3) not in either the current or immediately preceding sentence. When antecedents were classified as occurring in either the current or preceding sentences, a second classifier was used to deter-mine whether the candidate antecedent was (1) a full NP or (2) another pronoun.

We transformed the decision-tree rules into a set of heuristics for examining all the pronouns in a given sentence: for each pronoun contained in a summary sentence S be (1) kept in the summary, (2) added along with the immediately previous sentence found in the original doc-ument, or (3) dropped altogether from the summary. 3. Question processing and decomposition
We believe that the quality of question-directed summaries depends (in part) on systems X  ability to interpret the information needs expressed by complex questions. In order to provide summary answers that are respon-sive to a user X  X  particular information needs, we hypothesize that complex questions need to be decomposed into the set of simpler queries  X  or sub-questions  X  they either (1) mention overtly or (2) presuppose semanti-cally. In this section, we describe two types of question decomposition that we believe must be conducted prior to beginning the process of question-directed summarization: (1) syntactic question decomposition and (2) semantic question decomposition. 3.1. Syntactic question decomposition
Complex questions often include multiple requests for information in the same sentence. In an analysis of 125 complex questions taken from the 2004 AQUAINT Relationship Q/A Pilot, the 2005 TREC Q/A Track
Relationship Task and the 2005 DUC question-focused summarization task, we found that 49 questions (39%) included more than one overt, simple question.

We refer to complex questions that include the mention more than one overt question as syntactically com-plex questions . We have identified three types of syntactically complex questions: (1) questions that feature coordination (of question stems, predicates, arguments, or whole sentences), (2) questions that feature lists of arguments or clauses, and (3) questions that feature embedded or indirect questions. Examples of each of these three types are provided in Table 3 .

In GISTexter, questions featuring coordination and lists were decomposed syntactically using sets of heu-ristics. With instances of coordination, questions were first split on the conjunction; each conjunct was then reconstructed into a new, grammatical question using information extracted from the other conjunct. For example, a coordinated question like:  X  X  When and where did Fidel Castro meet the Pope?  X  X  is decomposed into: stem and re-supplying the verb phrase found in the original question. Likewise, questions involving lists of arguments were decomposed by creating sets of new sub-questions that replaced the list in the original ques-tion with each member of the list. For example, in a question like:  X  X  What international aid organization oper-ates in Afghanistan , Iraq , Somalia , and more than 100 other countries?  X  X , four sub-questions were generated, zation operates in more than 100 other countries?  X  X .

Syntactic question decomposition often requires the resolution of anaphora. We begin by identifying the antecedents of all referential expressions found in a complex question. For example, the complex question the Congolese government }. 3 Once coreference is established, we process questions by using a set of syntactic patterns to extract embedded questions and to split questions that featured conjoined phrases or lists of terms into individual questions. For example, the complex question illustrated in Fig. 4 is decomposed syntactically in Table 4 . 3.2. Semantic question decomposition
Even after syntactic question decomposition is performed, most complex questions still need to be decom-posed semantically before they can be submitted to a traditional Q/A system.

We believe that the relations that exist between a question and its decompositions may be of semantic
Furthermore, unlike discourse relations introduced by various coherence theories, the relations between ques-tions have an argument. This argument may take any of the values: (1) P (4) A TTRIBUTE , or (5) H YPERNYMY /H YPONYMY . The first four values refer to a predicate, event, argument or attribute detected in the mother-question, which will also be referred by the daughter-question. The last value (Hypernymy/Hyponymy) indicates that there is such a semantic relation (defined in WordNet) between a pair of concepts, one for the mother-question, one for the daughter-question. For example, in Fig. 5 we illustrate several relations between questions that are labeled E FFECT expected answer type (EAT) of the decomposed question is a class of concepts that are caused by some event (here, the 1982 Falklands War between Argentina and Great Britain); and (2) the event is explicit in both ques-tions. We have considered eight different classes of relations between questions that can be used to perform semantic question decompositions. The relations are illustrated in Appendix A of the paper.
 3.2.1. Top-down question decomposition
When decomposing complex questions in a top-down manner, systems need to have access to four forms of information: Semantic Dependencies (in the form of predicate-argument structures) found in the question; The Expected Answer Type (EAT) of the question; Sets of topical terms and relations derived from a collection of documents relevant to the question; Pragmatic associations between the question and potential decompositions;
Predicate-argument structures are provided by shallow semantic parsers trained on PropBank
Bank. 5 The EAT of the question is discovered with the technique reported in Pasca and Harabagiu (2001) . The most relevant relations from the question topic are identified by the enhanced representations of topic signa-tures reported in Harabagiu (2004) .

For each EAT, we have created a large set of 4-tuples  X  e association vector consists of (1) e 1 , the EAT of the mother-question; (2) r , the relation between the pair of questions; (3) e 2 , the EAT of the daughter-question; and (4) A , the set of lexically-aligned tokens found in between two questions. (In our current work, we use the lexical alignment system developed by Hickl et al. (2006) in order to identify pairs of tokens in each pair of questions which are likely to contain corresponding semantic information.) Fig. 6 illustrates the association vectors for two questions from Fig. 5 .
When generating a decomposition, we use the association information to build association rules similarly to the method introduced in Nahm and Mooney (2000) . The association information is akin to the fillers of a template. Therefore, by representing it as binary features that are provided to a decision-tree classifier (C5.0, Quinlan, 1998 ), we generate automatically association rules from the decision rules of the classifier.
In order to find the new information, that specializes the decomposed question, we select the topic relation that (a) fits best the predicate-argument structure of the decomposed question, and (b) produces similar lexical alignment while preserving grammaticality. These last two conditions must be met by the question surface realization function. The Top-Down Question Decomposition Procedure is illustrated in Fig. 7 . 3.2.2. Bottom-up question decomposition
In contrast to the top-down question decomposition described in Section 2.2 , complex questions can also be semantically decomposed in a bottom-up fashion by identifying the potential decomposition relations that may exist between sets of factoid questions related to the same topic.

In previous work ( Harabagiu, Hickl, Lehmann, &amp; Moldovan, 2005 ) we have described an approach that used syntactic patterns  X  in conjunction with semantic dependency and named entity information  X  to generate a bottom-up decomposition produced by the procedure from Fig. 9 .In Fig. 8 , all dashed arrows correspond to further produced decompositions that are not distancing themselves semantically from the complex question (Step 10 in Fig. 9 ).
 4. Textual entailment between summaries
In this section, we describe how we leveraged a state-of-the-art system for textual entailment system (TES) in order to select amongst each of the six candidate summaries generated by GISTexter. In Section 4.1 ,we provide an overview of the TES we first developed for the 2006 Second PASCAL Recognizing Textual Entail-ment Challenge (RTE-2) ( Hickl et al., 2006 ). In Section 4.2 , we describe how we used TES output was used to automatically generate content models from the content of the candidate summaries generated by GISTexter.
Finally, in Section 4.3 , we describe how we again employed TES output in order to score each candidate sum-mary against a content model in order to identify the candidate summary which best meets the user X  X  infor-mation needs. 4.1. The textual entailment system
In this section, we provide a brief overview of the system for recognizing textual entailment first described in Hickl et al. (2006) . The architecture of this TES is presented in Fig. 10 .

Following Dagan et al. (2005) and Bar-Haim et al. (2006) , we consider that a sentence S referred to as a text ) textually entails a second sentence S be reasonably inferred from the meaning of S 1 . For example, in the positive example of TE in Table 5 , the hypothesis is considered to be textually entailed by the text, since under most normal readings, the meaning of a statement such as  X  X  planning to give the starting job to  X  X  someone is seen as compatible with a statement of being  X  X  ready to hand the reins over to  X  X  (the same) someone.

In order to acquire the necessary linguistic information needed to recognize textual entailment, text-hypoth-esis sentence pairs are first sent to a Text Preprocessing module. Here, sentences are syntactically parsed, semantic dependencies are identified using a semantic parser trained on predicate-argument annotations derived from PropBank, entities are associated with named entity information from LCC X  X  CiceroLite Named
Entity Recognition system, time and space expressions are normalized (using a method first described in Leh-icates are annotated with semantic features such as polarity and modality.

Once preprocessing is complete, sentence pairs are sent to a Lexical Alignment module which uses a Max-imum Entropy-based classifier in order to determine the likelihood that single tokens (or phrases) selected from each text and hypothesis corresponding or otherwise equivalent information. (An example of the align-ments computed for the TE pair in Table 5 is depicted in Fig. 11 .)
Information from Lexical Alignment was then used in conjunction with a Paraphrase Acquisition module in order to identify sets of phrase-level alternations  X  or paraphrases  X  which could be used to relate the semantic content of the two sentences being compared. Since paraphrases generally predicate about the same sets of individuals, we assume that pairs of entities which receive high-confidence lexical alignment scores can be used to construct queries to retrieve sets of sentences which may be paraphrases  X  or near paraphrases  X  of the semantic content encoded in either text or the hypothesis. For example, the given the high-confidence aligned tokens from the hypothesis in Table 5 , X  X  The Bills  X  X  and  X  X  J.P. Losman  X  X , the eight sentences in Table 6 were retrieved from a standard Google WWW search. 6
In our system, we use the two highest-confidence aligned tokens from each text-hypothesis pair to construct a search query; sentences containing both terms within a context window are returned from the top 500 doc-uments returned retrieved for this query. Since not all retrieved sentences will be synonymous with either the paraphrases into sets that are presumed to convey the same content.

Semantic information  X  including features derived from Text Processing, Lexical Alignment, and Para-phrase Generation  X  is then combined into an Entailment Classifier which determines the likelihood that a tex-alignment features , which compare properties of aligned constituents, (2) dependency features , which compare mine whether passages extracted from the two sentences match acquired paraphrases, and (4) semantic fea-tures , which contrast semantic values assigned to predicates in each example sentence. Based on these features, the Entailment Classifier outputs both an entailment classification (either YES or NO ) and a confi-dence value. 4.2. Automatic pyramid generation Once a complete set of six candidate summaries have been generated, we used our TE system described in
Section 4.1 in order to select the candidate summary that best met the expected information need of the com-plex question.

In order to create a model Pyramid from the candidate summaries, each sentence from each of the six sum-maries were paired with every other sentence taken from the remaining summaries. Sentence pairs (e.g. h S 1 ; S 2 i ) were then submitted to the TE system, which returned a judgment  X  either yes or no  X  depending on whether the semantic content of S 1 could be considered to entail the content of S output for each sentence pair were then used to group sentences into clusters that, when taken together, were expected to represent the content of a potential semantic content unit (or SCU).
When a sentence S 1 was judged to entail a sentence S 2 , S entailing sentence S 1 and the index associated with the cluster (assumed to be equal to the SCU weight) was incremented by 1. If entailment was judged to be bidirectional  X  that is, S
S 1  X  the two sentences were considered to convey roughly the same semantic content, and all sentence pairs containing S 2 were dropped from further consideration. When entailment could only be established in one direction  X  i.e. S 1 entailed S 2 but S 2 did not entail S not strictly found in S 1 and was permitted to create a cluster corresponding to a separate SCU. Finally, sentences that did not exhibit any entailment relationship with any other sentence were assigned a weight of 1. Table 7 provides a synopsis of the rules used to construct Pyramids from entailment judgments.
When the identification of TE is complete, sentence clusters were assembled into a model Pyramid based on their SCU weights. An example of the top levels of an automatically-generated Pyramid is presented in Table 8 . In Table 8 , the original sentence used to construct each Pyramid cluster is presented along with its weight.
While each node in an automatically-constructed Pyramid may contain multiple SCUs, every sentence added to a Pyramid cluster is expected to be textually entailed by the original sentence. While this may lead to sit-uations where a sentence added to a cluster may only be entailed by a portion of the original sentence, we expect that, when taken together, each cluster will approximate a content unit that should be included in a summary answer. 4.3. Pyramid scoring of summaries
Each of the six candidate summaries was assigned a Pyramid score using the Modified Pyramid scoring algorithm described in Passonneau et al. (2005) . First, each sentence in each candidate summary was assigned an  X  X  X CU score X  X  based on the SCU weight assigned to its cluster. Sentences associated with an SCU cluster of weight w &gt; 1 received a score equal to their weight; sentences associated with an SCU cluster of weight w =1 received a zero SCU score. Next, in order to ensure that sentences assigned to large SCU clusters represented entailed by the sentence from the cluster that was assigned the highest passage retrieval score by either the question-focused summarization or the multi-document summarization sentence retrieval engines. Sentences that passed textual entailment retained their original SCU score; sentences that were not textually entailed received a zero score. Finally, a composite Modified Pyramid score was computed for the summary, and the top-scoring summary was returned to the user. 5. Experimental methods
In this section, we present results from an evaluation of the performance of our overall approach to ques-tion-directed summarization. Section 5.1 describes the test data that was used to create QDS along with the data that was used to train and to test our system for recognizing textual entailment. Section 5.2 provides quantitative results from three different evaluations of QDS. Section 5.3 details methods for evaluating the output of a semantic question decomposition module, while Section 5.4 describes how we evaluated each of the components of our system for recognizing textual entailment. Finally, in Section 5.5 , we discuss the per-formance of our Pyramid-based summary selection module for selecting the most responsive summary from among a set of candidate summaries. 5.1. Test data
Over the past three years, the answering of complex questions has been the focus of much attention in both the automatic question-answering (Q/A) and the multi-document summarization (MDS) communities. While most current complex Q/A evaluations (including the 2004 AQUAINT Relationship Q/A Pilot, the 2005 Text
Retrieval Conference (TREC) Relationship Q/A Task, and the 2006 GALE Distillation Effort) require sys-tems to return unstructured lists of candidate answers in response to a complex question, recent MDS eval-uations, including the 2005 and 2006 Document Understanding Conference (DUC) have tasked systems with returning paragraph-length answers to complex questions that are responsive, relevant, and coherent.
We conducted the evaluations of the GISTexter QDS system on a total of 100 different topics taken from the Document Understanding Conference (DUC) question-directed summarization shared task from 2005 and 2006. In this task, systems were presented with a complex question (generally known as a  X  X  X UC topic X  X ) and a set of approximately 25 newswire documents that were assumed to contain all of the relevant information needed to construct a perfectly responsive summary answer to the complex question. Summaries were required to be less than 250 words in both DUC 2005 and DUC 2006.

Human assessors from the National Institute of Standards and Technology (NIST) were used to create the  X  X  X opics X  X   X  and to assemble the document sets  X  used in the DUC evaluations. Once a set of topics had been agreed upon for an evaluation, assessors were then tasked with hand-creating a set of  X  X  X odel X  X  summaries that could be used in evaluating machine-generated summaries. In DUC 2005 and DUC 2006, another set of anno-tators followed the techniques outlined in Nenkova and Passonneau (2004) in order to convert the model sum-maries created by the NIST assessors into  X  X  X odel pyramids X  X  that could be used for the manual Pyramid scoring of automatically-generated summaries.

We conducted the evaluation of our textual entailment system using the set of 3200 entailment sentence pairs compiled for the 2005 and 2006 PASCAL Recognizing Textual Entailment Challenges ( Bar-Haim et al., 2006; Dagan et al., 2005 ). This set consisted of pairs of sentences derived from the output of a number of natural language processing applications (including automatic question-answering systems, multi-docu-ment summarization systems, information retrieval systems, and information extraction systems); approxi-mately 50% of these examples were deemed by human annotators to be positive instances of textual entailment, while another 50% were deemed by annotators to be negative instances of textual entailment. 5.2. Evaluation of question-directed summarization In this section, we present results from three different types of evaluations of question-directed summaries.
In Section 5.2.1 , we discuss two types of subjective measures designed to evaluate the responsiveness of sum-mary answers. In Section 5.2.2 , we present results from two different automatic summary scoring algorithms:
ROUGE ( Lin, 2004 ) and Basic Elements (BE) ( Hovy, Lin, &amp; Zhou, 2005 ). Finally, since overall readability and coherence are an important part of any summary generation task, we discuss results from 5 different lin-guistic quality metrics that have been used to evaluate question-directed summaries. 5.2.1. Evaluating the responsiveness of summaries
Evaluating the responsiveness of summary-length answers to complex questions has traditionally repre-sented more of a challenge than evaluating answers to the types of  X  X  X actoid X  X  style questions typically asked in the annual Text Retrieval Conference (TREC) Question-Answering Evaluations. Unlike informationally-simple  X  X  X actoid X  X  questions, complex questions often seek multiple different types of information simulta-neously and do not presuppose that one single entity or proposition could meet all of the information needs autism?  X  X , it can be safely assumed that the submitter of the question is looking for an age range which is con-ventionally associated with a first diagnosis of autism. However, with complex questions like  X  X  What is thought to be the cause of autism?  X  X , the broader focus of this question may suggest that the submitter may not have a single or well-defined information need and therefore may be amenable to receiving additional information that is relevant to an (as yet) undefined information goal.

In the DUC 2006 Question-Directed Summarization Evaluations, human assessors were tasked with pro-viding two different subjective estimates of how well they felt a question-directed summary responded to the information need of a complex question. In the first measure, known as content responsiveness , annotators indicated the degree to which the information contained in the QDS satisfied the information need of the sum-mary. In the second measure, known as overall responsiveness , annotators scored summaries based on both their information content and their overall readability. Annotators scored both measures on a 5-point scale, with 1 representing the lowest level of satisfaction and 5 representing the highest.

In order to evaluate the relative responsiveness of the different question-directed summarization strategies described in this paper, we tasked a team of 5 human annotators to evaluate each of the 6 candidate summaries generated for each of the 50 DUC 2006 QDS topics for both content responsiveness and overall responsiveness .
Results from these evaluations are presented along with GISTexter X  X  official DUC 2006 results in Table 9 . 5.2.2. Automatic evaluations (Rouge+BE)
While subjective measures of responsiveness are an effective way to gauge the quality of a multi-document or a question-directed summary, gathering human judgments can often be too time-consuming and/or expen-sive to be a useful source of feedback for developers of summarization systems. In addition to evaluating
GISTexter X  X  candidate summaries in terms of content responsiveness and overall responsiveness , we evaluated each of the summaries generated for the 50 DUC 2006 topics using two different automatic summarization scoring systems: ROUGE and Basic Elements (BE).

Introduced in Lin and Hovy (2003) , ROUGE (Recall-Oriented Understudy for Gisting Evaluation) auto-matically compares machine-generated candidate summaries against a human-created model by counting the number of n -grams, word sequences, or word pairs that the two summaries have in common. Features com-mon to both summaries are then used to compute a score which approximates the degree of correspondence between the machine-generated summary and the human-created model summary. While ROUGE has been one of the  X  X  X fficial scores X  X  used by the DUC organizers to evaluate system submissions since DUC 2004,
ROUGE X  X  dependence on token overlap often limits in its effectiveness, as summaries can receive ROUGE scores that say more about the number of words or phrases in common between the two summaries than the amount of shared semantic content they contain. In order to account for some of the shortcomings of
ROUGE X  X  term-based approach, Hovy et al. (2005) suggested that automatic summary scoring should be per-formed using text fragments (known as basic elements (BEs)) which express syntactic or semantic dependen-cies. Unlike ROUGE-based scoring metrics, which treated individual summaries as bags-of-words, Hovy et al. argued that a BE-based scoring metric could be used identify correspondences between summaries based on phrase-level constituents that were derivable automatically from the output of syntactic or semantic parsers.
Average results from two different ROUGE metrics  X  ROUGE-2 and ROUGE SU-4  X  and from BE are provided in Table 10 for each strategy. (Results from GISTexter X  X  DUC 2006 submission are provided as well.) 5.2.3. Linguistic quality of summaries
In addition to content-based evaluation, machine-generated summaries in DUC have traditionally also been evaluated with regards to a number of linguistic factors designed to gauge the overall clarity, coherence, and readability of multi-document summary or question-directed summary answer.
 We used a team of 5 human annotators to evaluate each of the candidate summaries generated for the 50
DUC 2006 topics along 5 different sets of criteria: (1) grammaticality , a rough measure of the well-formedness (in terms of grammar and orthographic case) of sentences in the summary; (2) non-redundancy , corresponding to the amount of repetition or redundant information contained in the summary text; (3) referential clarity ,a measure of how often referring expressions (such as pronouns, names, or definite NPs) could be associated with their antecedents; (4) focus , corresponding to the degree to which a summary discussed a single topic or theme; and (5), structure and coherence , a measure corresponding to the level of organization and structure found in the summary. As with responsiveness, annotators were asked to rate each summary on a 5-point scale, with 1 representing a poorly-formed summary and 5 representing an well-formed summary. Table 11 presents results for these five linguistic criteria from our internal evaluations as well as the official DUC 2006 results for GISTexter.

Since GISTexter X  X  approach to QDS treats the acquisition of relevant content separately from summary generation, it is not surprising that there are not significant differences between types of candidate summaries in terms of grammaticality, non-redundancy, and referential clarity, and coherence. Only focus  X  a measure of how well the information in a summary pertains to the same topic  X  appears to provide any meaningful var-iation, as summaries constructed from semantically decomposed questions were judged to have a slightly higher focus score than summaries constructed from either syntactically-decomposed questions or sets of keywords. 5.3. Evaluation results of question decompositions
In this section, we present results from experiments targeting both (1) the evaluation of the decomposed questions and (2) the evaluation of the impact of the decomposed questions on the quality of answer summaries. 5.3.1. Intrinsic evaluations
The evaluation of the decomposed questions was performed in two ways. First, the decomposed questions were evaluated against decompositions created by humans. Second, question decompositions were evaluated against questions generated from the answer summaries. The second evaluation was also compared against an evaluation involving only human-generated questions, both from the complex question and from the answer summaries. The evaluation was performed against 8 complex questions that were asked as part of the DUC 2005 question-directed summarization task. The questions correspond to the topics listed in Table 12 .
We had 4 human annotators perform manual question decomposition based solely on the complex ques-tions themselves. Annotators were asked to decompose each complex question into the set of sub-questions they felt needed to be answered in order to assemble a satisfactory answer to the question. (For ease of ref-erence, we will refer to this set of question decompositions as QD annotators were then compiled into a  X  X  X yramid X  X  structure similar to the ones proposed in Nenkova and Pas-sonneau (2004) . In order to create pyramids, humans first identified sub-questions that sought the same infor-mation (or were reasonable paraphrases of each other) and then assigned each unique question a score equal to the number of times it appeared in the question decompositions produced by all annotators.
Next, we used the top-down question decomposition model described in Section 3.2.1 in order to generate a second set of question decompositions ( QD top -down ). Finally, we used the bottom-up question decomposition algorithm presented in Section 3.2.2 in order to generate a third set of question decompositions( QD
As with QD human , the sub-questions generated for QD top -down structures by human annotators.

Each of these three sets of question decompositions were then compared against a set of  X  X  X old standard X  X  decompositions created by another team of 4 human annotators from the 4  X  X  X odel summaries X  X  prepared by
NIST annotators as  X  X  X old standard X  X  answers to the 8 complex questions. Each of the three question decom-positions described above (i.e. QD human , QD top -down , and QD ing  X  X  X odel X  X  question decomposition pyramid using the technique outlined in Nenkova and Passonneau (2004) . Table 12 illustrates the Pyramid coverage for QD that although the QD human captured 45% of the questions contained in the  X  X  X odel X  X  pyramids, the high aver-age Pyramid score (0.5000) suggests that human question decompositions typically included questions that corresponded to the most vital information identified by the authors of the  X  X  X odel X  X  summaries. 5.3.2. Impact of questions decompositions on QDS
We evaluated the impact of question decompositions by using the responsiveness score. As in Section 5.2.1 , the responsiveness score was assessed by a team of five annotators who selected an integer value between 1 and 5 to assess their satisfaction with the information contained in the summary as an answer to the question. In order to better evaluate the impact of question decomposition on the quality of QDS, we separated the  X  X  X emantic QD X  X  strategy used in Strategies 3 and 6 into 3 separate strategies: (1) a top-down QD strategy, (2) a bottom-up QD strategy, and (3) the  X  X  X ormal X  X  hybrid QD strategy which contains decomposed questions from both semantic QD strategies.

To be able to evaluate the impact of question decomposition on multi-document summarization, we created eight different summaries for each of the 50 DUC 2006 topics and had human annotators evaluate them in terms of overall responsiveness. Results from these experiments are listed in Table 13 .

When we wanted to measure the impact of question decomposition on multi-document summarization, we compared the results of the experiments listed in Table 13 against the two baseline strategies (Strategies 1 and 4) in which no question decomposition is available. By computing the difference in responsiveness score between the results obtained in the experiments listed in Table 13 , and the baseline experiments, we have found that the largest impact of question decomposition for MDS was obtained in experiment E least impact was obtained in experiment E 5 . 5.4. Evaluating textual entailment
In this section, we discuss how we used data taken from the PASCAL Second Recognizing Textual Entail-ment Challenge ( Bar-Haim et al., 2006 ) in order to evaluate the performance of our textual entailment system. 5.4.1. Textual entailment evaluation The system for recognizing textual entailment was evaluated as part of the Second PASCAL Recognizing
Textual Entailment (RTE) Challenge ( Bar-Haim et al., 2006 ). Systems were evaluated in two ways. First, systems received an accuracy score equal to the number of examples from the PASCAL RTE-2 test set where the existence (or non-existence) of entailment was identified correctly. Systems also received an average preci-sion score which evaluated systems X  ability to rank sentence pairs in order of confidence. Following Voorhees and Harman (1999) , average precision was computed as the average of the system X  X  precision values at all points in the ranked list in which recall increases. Results from the 2006 RTE-2 Challenge are provided in Table 14 .

As noted in Hickl et al. (2006) , access to additional sources of training data significantly ( p &lt; 0.05) enhanced the performance of our system to correctly identify instances of textual entailment. When the system was trained on an additional 201,000 entailment pairs (101,000 positive instances, 100,000 negative instances), performance increased by over 10% overall. 5.4.2. Lexical alignment
The performance of the textual entailment system X  X  Lexical Alignment module was evaluated on a set of 1000 phrase pairs extracted from the 800 textual entailment sentence pairs assembled for the 2006 PASCAL RTE-2 Training Set. 8
Three versions of the alignment module were evaluated. In the first version, a set of 10,000 human-anno-tated alignment token pairs were used to train a hill-climber-based alignment classifier; the second version used the same set of annotated data to train a Maximum Entropy-based classifier. In the third version, the hill-climber classifier was used to annotate a set of 450,000 alignment pairs extracted from more than 200,000 additional entailment sentence pairs compiled by Hickl et al. (2006) ; these machine-generated anno-tations were then used to train a new version of the Maximum Entropy-based alignment classifier. Perfor-mance of each of these classifiers is presented below in Table 15 .

While alignment classifiers trained on human-annotated data performed admirably (with F -scores over 0.8), access to a large amount of machine-annotated training data resulted in substantial boosts in both precision and recall. 5.4.3. Evaluations of the quality of paraphrases
We evaluated the paraphrases generated for each sentence in an entailment pair in two ways. First, we had two human annotators judge whether an automatic-generated paraphrase approximated the semantic content of the original passage. If the paraphrase was deemed to be acceptable by both judges, another pair of anno-tators were asked to subjectively grade the quality of the paraphrase on a 5-point scale, with a score of 5 being equal to a  X  X  X erfect X  X  paraphrase, and 1 being equal to a  X  X  X arginally acceptable X  X  paraphrase of the original text. No additional instruction was given to annotators on how to score paraphrases. All paraphrases which received annotator scores that differed by more than 3 points were excluded from our results.
In order to perform this subjective evaluation of paraphrase quality, we used our Paraphrase Generation module to generate (as many as) the top 250 paraphrases for 350 (randomly-selected) sentences taken from the PASCAL RTE-2 Test Set. (As in Hickl et al. (2006) , paraphrases were generated for the text span that occurred between the top pair of aligned tokens identified by the Lexical Alignment module for each sentence.)
Paraphrases were generated using two different corpora: (1) a large, 1 million-document newswire corpus and (2) the top 500 documents retrieved from the World Wide Web from the top 10 newswire-and WWW-based paraphrases generated for each of the 350 original sentences; these text-paraphrase pairs were then pseudo-randomized and submitted to annotators for judgment.
Table 16 presents results from this evaluation. Annotators found 49.3% of the text-paraphrase pairs gen-erated from WWW documents to be at least  X  X  X arginally acceptable X  X ; this number dipped to 22.4% when paraphrases generated from newswire documents were considered. Despite the lower overall accuracy of the newswire-based paraphrases, annotators generally considered these paraphrases to be of higher quality: newswire-based paraphrases received an average 4.05  X  X  X araphrase quality X  X  score, while WWW-based para-phrases received only a 3.03 average score.

In addition to measuring the accuracy and quality of generated paraphrases, we also evaluated the impact that access to paraphrases had on the performance of our system on the 2006 PASCAL RTE-2 Test Set. In order to perform this evaluation, we used our Paraphrase Generation module in order to generate (as many as) the top 250 candidate paraphrases for the 1600 sentences used in the PASCAL RTE-2 Test Set. (A total of 340,402 candidate paraphrases were generated from these 1600 sentences.) Access to this collection of boosted the overall accuracy of the system by 4.13%, increasing from 71.25% accuracy to 75.38% overall. 5.5. Evaluations of the summary selections
In this section, we evaluate the performance of our Pyramid-based automatic summarization selection sys-tem using data from the 2006 DUC Question-Directed Summarization Task.

Table 17 presents the output of the Pyramid-based summary selection module for the 50 topics featured in the 2006 DUC evaluations.

Although summaries based on semantic question decomposition received the highest automatically-com-puted Pyramid score for 32 of the 50 topics (64%), the average overall responsiveness (as determined by NIST assessors) did not degrade significantly ( p &lt; 0.05) when other types of summaries were selected. In addition, even though slightly more summaries were created using sentences derived from our QDS system X  X  ques-tion-answering based strategies (56%) than its traditional summarization-based strategies (44%), the average overall responsiveness remained relatively constant for both types of summaries: Q/A-based summaries received an average overall responsiveness score of 2.875, while MDS-based summaries scored 2.828.
We used the content responsiveness scores compiled by our own annotators (as described in Section 5.2.1 ) in order to determine how frequently the Pyramid-based summary selection module selected the most respon-sive of the 6 candidate summaries generated for each topic. Table 18 details the accuracy of the Pyramid-based summary selection module for each of the 6 strategies.

When judged over the 50 topics in DUC 2006, our summary selection module selected the most responsive summary (as judged by our human annotators) 86% of the time (43/50). This a particularly encouraging result, as it suggests that Pyramid creation using TE is sufficiently discriminative to identify differences even among sets of highly responsive and similar summaries. 6. Conclusions
In this paper we have described GISTexter, a summarization system that was designed to satisfy the user information needs expressed by a complex question. GISTexter processes complex questions by performing syntactic and semantic decompositions. Additionally, two approaches for generating summaries are used, which enable six different summarization strategies. To combine the summarization strategies, we have used textual entailment in two ways: (1) for selecting information; and (2) for scoring the summaries based on pyr-amid-based measures. We have shown that these methods have allowed us to automatically generate question-directed summaries that are not only coherent and readable but that are highly responsive to the information needs of users.
 Acknowledgement
This material is based upon work funded in whole or in part by the U.S. Government, and any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not nec-essarily reflect the views of the US Government.
 Appendix A. Relations between decomposed questions Relation Example D D EFINITION (predicate) What are the procedures for generating new drugs? D EFINITION (argument) What is a drug? E E LABORATION (Hyponymy) Which new drugs are being produced? E LABORATION (Number) How many new drugs are being produced? E LABORATION (Time) When are new drugs being produced? E LABORATION (Location) Where are new drugs being produced? E LABORATION (Manner) How do pharma companies produce new drugs? E LABORATION (Quantity) How many new drugs did pharma companies produce? E LABORATION (Rate) What was the greatest number of new drugs that pharma companies produced? E LABORATION (Duration) How long will pharma companies produce new drugs? E
LABORATION (Trend) How much has pharma companies X  production of new drugs increased/ E LABORATION (Inchoative) When did pharma companies begin producing new drugs? E LABORATION (Terminative) When did pharma companies stop producing new drugs? E LABORATION (Subjective) How beneficial/detrimental was pharma companies X  production of new drugs? E C AUSE (Event) What steps did pharma companies take to produce new drugs? I NTENTION (Event) Why did pharma companies produce new drugs? E FFECT (Event) What happened because pharma companies produced new drugs? R ESULT (Event) What advantages resulted pharma companies producing new drugs? O UTCOME (Event) What profits did pharma companies take from producing new drugs? T EMPORAL (Event) What happened after/before pharma companies produced new drugs? R
ELATIONSHIP (Event) What is the connection between pharma companies producing new drugs and the G S PECIALIZATION (Predicate) What kind of activities are involved in the creation of new drugs? G (Predicate) C N EGATION (Predicate) What pharma companies don X  X  produce new drugs? N EGATION (Argument) What pharma companies have produced no (new) drugs? N EGATION (Attribute) What pharma companies produce only existing drugs? E XCEPTIVE (Argument) What pharma companies are producing new drugs other than MAOI inhibitors? C
ONTRARY (Fact) Despite the FDA X  X  ban on new drug development, which pharma companies are A R ESTRICT (Location) What pharma companies are producing new drugs in the U.S.? R ESTRICT (Temporal) What pharma companies are producing new drugs in 2006? R ESTRICT (Attribute) What up-and-coming pharma companies are producing new drugs? E E
PISTEMIC (Event) Are pharma companies producing new drugs? References Appendix A ( continued ) Relation Example E PISTEMIC-CONDITIONAL (Event) Is it known if pharma companies are producing new drugs? E PISTEMIC-EVIDENTIAL (Event) Is there evidence that pharma companies are producing new drugs? E PISTEMIC-REPORTED (Event) Does anyone believe that pharma companies are producing new drugs? E PISTEMIC-ALTERNATIVE (Event) Do U.S. pharma companies produce new drugs or research new drugs? E PISTEMIC-ELABORATION (Event) Do pharma companies produce new drugs [with the help of foreign labs]? P P ARALLEL (Predicate) What pharma companies work with infectious agents? P ARALLEL (Predicate) What pharma companies research new drugs? P ARALLEL (Argument) What pharma companies produce vaccines? P ARALLEL (Attribute) What pharma companies produce affordable drugs?
