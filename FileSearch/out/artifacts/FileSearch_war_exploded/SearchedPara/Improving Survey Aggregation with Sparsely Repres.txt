 In this paper, we develop a new aggregation technique to reduce the cost of surveying. Our method aims to jointly estimate a vector of target quantities such as public opin-ion or voter intent across time and maintain good estimates when using only a fraction of the data. Inspired by the James-Stein estimator, we resolve this challenge by shrink-ing the estimates to a global mean which is assumed to have a sparse representation in some known basis. This assump-tion has lead to two different methods for estimating the global mean: orthogonal matching pursuit and deep learn-ing. Both of which significantly reduce the number of sam-ples needed to achieve good estimates of the true means of the data and, in the case of presidential elections, can esti-mate the outcome of the 2012 United States elections while saving hundreds of thousands of samples and maintaining accuracy.
 Survey aggregation; Presidential elections; James Stein es-timator; Compressive Sensing; Multi-task learning; Deep learning
Surveys are a common way of inferring information about an entire population. In social polling for example, peo-ple study how different groups of the population respond to some binary questions, such as  X  X o you use Facebook? X ,  X  X o you think abortion should be legal? X  [3] [2], or  X  X ho will  X 
Tianlin Shi and Forest Agostinelli contributed equally to this paper.
 be the new president? X  These polls are frequently used to make predictions (e.g., predicting presidential elections), for market analysis (e.g. to answer questions such as which seg-ments of the population prefers Xbox over PlayStation), or to obtain feedback. Survey -based companies such as Nielsen are represented in more than 100 countries and boast rev-enues of billions of dollars per year. At the same time, con-ducting a survey is expensive. For example, for conducting a survey based on a short phone-call interview, survey compa-nies typically charge customers on the order of $20-$30 per call; and even small-scale surveys can easily cost up to mul-tiple tens of thousands of dollars [30]. Nielsen, for example, employs approximately 40,000 people worldwide, reflecting the high-labor cost involved in conducting surveys. In ad-dition, research shows that people are becoming more resis-tant to answering surveys [19], which may force companies to make good predictions with only a fraction of the data. This drives the search for advanced surveying and sampling strategies that could reduce the number of samples needed in social surveying.

The underlying fundamental problem in aggregating so-cial surveying data is averaging . From the sample responses collected, the survey designer applies averaging to estimate the true mean response of each group of interest. At first thought, this problem looks easy: why not just average by  X  X ample means X ? That is, we just average all the samples collected within each group. In fact, if there is only one group, then  X  X ample means X  is admissible: no other averag-ing methods could dominate it. However, if there are more groups, then it is known that  X  X ample means X  is not nec-essarily the best due to the so-called Stein X  X  paradox [13]. Stein [27] constructs the James-Stein (JS) estimator that re-covers every single mean using information from all groups, and shows that it outperforms  X  X ample means X  no matter what the true means are. Efron and Morris [12] cast JS into an empirical Bayes framework and show that JS estimates are the result of a prior regularization and shrinkage of the sample averages.

In this work, we aim to explore the potential of regularization-based averaging that achieves good estimation while remain-ing flexible. An inevitable characteristic of the regulariz-ers would be a sparsity structure , i.e., a limited number of non-zero parameters. Otherwise, there is little chance that those parameters could be estimated from data, and help the reconstruction of the true means. In fact, spar-sity has already been deployed as a killer feature in sensing other types of data such as images, sounds, sensor read-ings, or network conditions. The well-known compressive sensing framework [10] allows sampling such data at a sig-nificantly lower rate beyond Shannon-Nyquist limit. This gain owes much to the fact that natural signals can be spar-sified  X  sparsely represented under some known basis such as wavelets for images and speech. It would be great if we could similarly apply compressive sensing techniques to so-cial surveying data. Alas, direct application of compressive sensing to social surveying is difficult, because social data (the true means) are too noisy to be sparsified.
 We resolve this challenge by leveraging inspirations from James-Stein estimators. Instead of using sample averages of all groups as in JS, we consider shrinking the sample aver-ages towards some unknown global vector , and use compres-sive sensing to recover this vector from the samples. There-fore, instead of sparsifying the true means, we propose to sparsify this unknown global vector. This idea marries the idea of compressive sensing and averaging estimators in a coherent framework, which we name compressive averaging . In this paper, we derive efficient algorithms based on this novel framework for estimating the true means based on survey samples. We also provide theoretical insight into the framework and understanding of when and how it works. Our evaluations based on real-world surveys indicate that compressive averaging can yield substantial improvements over the commonly used surveying methods.

This paper is organized as follows: In section 2, we in-troduce the problem setting, in section 3 we present com-pressive averaging, section 4 provides theoretical analysis of compressive averaging, section 5 shows empirical results, section 6 discusses related work, section 7 discusses future work, and section 8 is the conclusion.
A typical economical or sociological survey aims to gather information about characteristics of a population, such as incomes, attitudes, or interests. Formally, we assume that there are T groups in total, and every group is asked the same question. The quantity of interest is the average re-sponse per group, which we denote as  X  [ t ] for group t . No-tice that for binary questions,  X  [ t ] is also the probability of a positive response in group t .

To estimate the quantity  X  [ t ], the surveying methodol-ogy uses two basic steps: sampling and aggregation . For sampling, researchers send out questionnaires to individ-ual entities in each group, and obtain samples z j 1 , 2 ,...,n [ t ]) for every cell [ t ]. Depending on the response rate and the design of the survey, different number of sam-ples are collected for each group, denoted as n [ t ]. A reason-able model for z j [ t ] would be number of samples n [ t ] are the sufficient statistics for esti-mating  X  [ t ]. For convenience, we treat these two cases in a unified setting: For the continuous case, this approximation is exact; and for the binary case, the Central Limit Theorem implies that this approximation is good given a sufficient number of samples.
In the aggregation step, the survey designer performs sta-tistical inference to estimate the quantity of interest  X  [ t ] from samples z j [ t ]. We would like the estimate to be as ac-curate as possible. Usually the expenses of data collection dominates the cost of aggregation. Therefore, to reduce the overall cost of surveying, we seek to reduce the total number of samples needed to incur a bearable amount of error.
Finally, note that for convenience we interchangeably use  X  as a function of t , or as a vector. The same thing goes for y , n , and  X  2 .
The straightforward estimator would just use the  X  X ample means X  y [ t ] of each group, namely, Sample averaging is also the maximum likelihood estimator, and is optimal when T = 1.
The James-Stein estimator (JS) [27] shrinks the sample averages y towards the global mean f = T  X  1 P T t =1 y [ t ], the unweighted average of all y [ t ]. Precisely, where (  X  ) + = max(0 ,  X  ), and the shrinkage factor  X  is Where  X  2 is the true variance of the data. The smaller the distance between y and f , the more aggressive the shrinkage is. So the optimal amount of shrinkage is adapted to the data. By doing so, the James-Stein estimator makes a better tradeoff between variance and bias, and achieves a better estimation performance.
To see how sparsity might help improve the estimates, we first look at an example as shown in Figure 1, where James-Stein estimators do not work well. Suppose we have the true means and 10 samples are collected from each category with noise variance  X  2 [ t ] = 50 for all t . It is well known that JS per-forms worse when such outliers exist [11] [13]. If we shrink the sample means towards the global mean, f = 26 . 95, the shrinkage parameter becomes very small,  X  = 0 . 002. That is, JS almost becomes sample means. Figure 1: Intuitive example where James-Stein estimator (JS) does not work well. As the sample averages y (MLE) deviate a lot from global mean f , the shrinkage effect that JS exploits becomes negligible. So JS reduces to MLE.
A natural idea to improve JS would be to prevent the cases t  X  9 and t &lt; 9 from sharing the same global mean. For example, we could construct a global mean f [ t ], then we could shrink the estimates towards f [ t ] so that the overall risk is much smaller. The problem with this idea is that when carrying out a survey, such global mean f [ t ] is not known a priori. Therefore, it must be estimated from the data.
At the conceptual level, all we need is a function f [ t ] that fits the data well and comes from a restricted model fam-ily. For the outlier example in Figure 1, we might assume that f [ t ] is a sparse combination of various components: a constant global mean f 0 [ t ] := 1, and singleton functions f [ t ] := I [ t = i ]. The sparse solution f = f 0 + f is exactly the function we wish for in Equation (6) and is quite stable due to the occurrence of f 0 .

In general, we consider a model family F and a function f [ t ]  X  X  . Then we explain data through a generative model: For some constant A and with known noise level  X  2 [ t ]. The marginal distribution of y given f is The function f encodes our flexible prior about the latent quantities  X  , and A controls how strong that prior ought to be. We jointly estimate both of them from data by mini-mizing the negative log likelihood of equation 9 which leads to the following optimization problem: In the case of binary data, we find that modeling the u [ t ] and y [ t ] as being generated from a Beta distribution is in-tractable. Since the maximum variance for a Bernoulli ran-dom variable is 0.25, we put a prior on A to add a penalty for improbable A and only search in the range of possible values for A.

Once f and A are computed from the data, the means  X   X  can be estimated using the posterior mode, which is The core challenge in using Eq.(11) is to devise the family F . If we allow f [ t ] to be arbitrary the optimal solution would be to set f = y which reduces to sample averaging.

Real social data can be noisy and therefore there does not generally exist a sparse representation for the true means  X  . The idea of sparsity-based aggregation is that we instead exploit the sparse representation of regularizers f , and then use f to improve the estimates  X   X  of the true means.
To formalize this intuition in the example, we first con-struct a basis  X  = {  X  1 , X  2 ,..., X  k } with  X  i  X  R T such that where  X   X  R K is a sparse coefficient vector with ||  X  || k K .
Now that we know how to model f , we need a method to determine what f should be. In the next two sections we present two different ways to do this: orthogonal matching pursuit and deep learning.
Based on our framework, the first step is to recover  X  (and hence f ) as well as A empirically from the data. We solve the optimization problem Eq.(10) by iterating between A and  X  . This problem resembles the sparse recovery formulations in compressive sensing [10]. Therefore, we solve it via similar techniques. We present a greedy algorithm (Algorithm 1) based on Orthogonal Matching Pursuit (OMP) [32] with modifications to handle the heterogenous noise levels.
Due to the connection to compressive sensing, we call this approach  X  X ompressive averaging X  (or  X  X ompressive survey-ing X ) for aggregating survey data.
Deep learning [20] has been shown to excel on a wide vari-ety of tasks such as computer vision [28], speech recognition [17], and high energy physics [7]. We propose using deep learning to learn f . The input to the network is the y ,  X  and a scaled n . The target output is the global mean f .
Using the assumption that the global mean is sparsely rep-resented in some given basis,  X , we can generate examples Algorithm 1 Modified Orthogonal Matching Pursuit for solving  X  . 1: Input. Sample average y , basis  X  = (  X  1 ,..., X  K ). 2: Let  X  = diag( n [1] / X  2 [1] ,...,n [ T ] / X  2 [ T ]). 3: Compute y 0 =  X  1 2 y ,  X  0 =  X  1 2  X . 4: Let residual r 0 = y , support set S 0 =  X  . 5: for i = 1  X  k do 7: S i = S i  X  1  X  X  j } . 10: Update residual r i = y  X   X  0 S i  X  S i . 11: end for 12: Output. Support set S k and coefficients  X  S k . of a global mean by first randomly generating sparse coeffi-cients  X  such that ||  X  || 0  X  k K and then, using equation 12, produce a global mean f . Given the global mean f and variance A (where A is randomly selected from a given range of numbers), we can generate true means  X  using equation 7. Given the true means  X  [ t ], randomly generated  X  2 [ t ], and randomly generated n [ t ], we can then get y [ t ] using equation 8. For the binary case,  X  2 [ t ] is determined by  X  [ t ]. Figure 2 gives a visualization of the process of generating data to train the neural network. The benefit to this approach is we have, for all intents and purposes, infinite training data since we can always generate new examples. We take advantage of this by generating new data every 10 epochs.

The deep neural network (DNN) we used has 3 layers with 1000 hidden units in each layer. We use the adaptive piece-wise linear (APL) activation units [5]. For every hidden layer we initialize the weight matrices with the  X  X avier X  filler [16]. The loss function is half the mean squared error between the output of the DNN and the global means:
Where  X  f is the output of the DNN and M is the batch size. We use M = 100 for our experiments. We train the DNN using backpropagation and momentum [23] for 60,000 iterations. There learning rate starts at 0.1 and decays ac-cording to 0 . 1 1 . 0001 i where i is the iteration. The momentum starts at 0.5 and goes to 0.9 over 5000 iterations. No weight regularization is used.

After training the DNN, we optimize equation 10 by first obtaining f by doing a feedforward pass through the DNN. Then we can obtain A by fixing f to be the output of the DNN and optimizing equation 10.

As a note, we tried using a DNN to directly predict the true means  X  , however, this proved too hard for the DNN to learn and performance was not any better than sample averaging. It seems that it is much better to first train a DNN to predict f and then shrink y to this prediction.
A wide range of social surveys are conducted periodically, such as the General Social Survey [1]. For example, to study how presidential approval rates evolve, investigation insti-tutes send out surveys monthly or yearly. So each category t corresponds to a month/year. Borrowing techniques from compressive sensing literature, we treat y [ t ] as a discrete signal in time t , and use wavelets as the basis.

The basis we use in all the experiments is the Daubechies least-asymmetric wavelet packet ( X  X psym X  in MATLAB). The wavelet can be constructed at multiple scales, and we use the first four coarse scales (equivalent to 8 lowest fre-quency components). The sparsity level k is set to be 3.
The reason we select this basis is because the components of the basis can easily be used to make smooth curves. A basis such as the Haar wavelets would be more difficult since each component is discontinuous. k = 3 was chosen because it will lead to sparse solutions. We use the 8 lowest frequency components instead of all the frequency components of the basis because the higher frequency components will fit high variations in the data, which, in the case of noisy data, will lead to poor solutions.

However, in section 5.3 our experiments show that other smooth basis functions such as the Discrete cosine transform-II basis and the polynomial basis both give good results, while the discontinuous Haar wavelets sometimes perform poorly. Our experiments show that good results can be ob-tained with varying levels of k . In addition, our experiments show that using the low frequency components in the basis is important for good performance.
In this section, we provide theoretical insights into the performance of the orthogonal matching pursuit (compres-sive averaging) algorithm for estimating the global mean. For simplicity, we assume all noise levels are the same and equal to 1, i.e.  X  2 [ t ] /n [ t ] = 1.
 Lemma 4.1. Suppose the support of  X  is S , and let  X  denote the sub-dictionary with columns restricted to index S . If  X  S is full-rank in column, then
Proof. Notice when we assume  X  2 [ t ] /n [ t ] = 1, the opti-mization problem Eq.(13) for solving  X  does not involve A at all. Given the support of  X  , the problem has a quadratic form, and the optimal solution is just Eq.(15).
 Therefore, given S , the sensitivity matrix is also the projection operator that f = Gy .

Lemma 4.2. Every optimal solution ( A,f ) of Eq.(10) sat-isfies the following equation, Proof. Given any solution of f , the optimal solution for A must satisfy Eq.(17).

Theorem 4.3. Define  X  as and Notice  X ,k are also random variables dependent on y . We have Proof. The total squared error can be decomposed as Plug in the expression of  X   X  from Eq.(11) , we would have Tr( G ) = k . Then we can prove the theorem.

This theorem sheds light into when compressive averaging is useful. First of all, the following corollary shows it is safe to use compressive averaging whenever sampling averaging is applicable.

Corollary 4.4. Compressive averaging dominates sam-ple averaging for T  X  2 , i.e. for all  X  ,
Next, compared to James-Stein estimator, the following corollary shows that compressive averaging could introduce an overhead compared to JS. And the overhead is small when the representation of f [ t ] is sparse.

Corollary 4.5. The relationship of compressive averag-ing and James-Stein estimator can be established as
Notice that  X   X  [  X  max , X  min ] where  X  max and  X  min are the largest and smallest eigenvalue of G . This implies k  X   X  . Therefore, the right-hand side is always non-positive.
So when do we expect this approach to work? This ap-proach is useful because it address a fundamental problem in James-Stein estimators: global means could be an inap-propriate prior and lead to small shrinkage. This could have significant advantage when sparsity structure does exist in the problem. But we also must be aware that compres-sive averaging makes a tradeoff by reducing the deviation || f  X  y || 2 at the cost of an additional possible penalty term that is related to the inherent sparsity of the regularizing global function. As baselines, we include sample averaging ( Avg ) and James-Stein estimator ( JS ). We also include multi-task averag-ing [15] ( MTAvg ), which uses a regularization that enforces all estimates to be close to one another. We use constant multi-task averaging since experiments from [15] show that this often gives the best performance. When referring to compressive averaging in our plots, using orthogonal match-ing pursuit to predict f is labeled as OMP , whereas the using a deep neural network (DNN) to predict f is labeled as DNN .
Sections 5.1 and 5.2 show the results of estimating the means of the GSS and Xbox datasets (the datasets are ex-plained in their respective sections). The evaluation metrics used in section 5.1 and 5.2.2 is the mean absolute difference between each true mean (the mean when 100% of the data is used) and each estimated mean. The estimated and true means are first converted to percentages and then the mean absolute difference between the two percentages is obtained. For example, for the presidential election, a mean of 1.8 means that 80% of the people voted for Obama. The eval-uation metrics in section 5.2.3 are slightly different because there is only one true mean: the exit poll data on election day. So, the mean absolute difference is taken between the estimated mean at the last day of the survey and the exit poll data.

Section 5.3 investigates how sensitive compressive averag-ing is to changes in parameters and basis functions. Section 5.4 shows how each method performs when the data is par-ticularly noisy.
We take columns from the yearly General Social Survey (GSS) [1] data: a question regarding gun law ( GUNLAW ) and a question regarding women working ( FEWORK ). GUNLAW asks Figure 3: Scatter plot of the true means on two datasets extracted from the General Social Survey. As we see, the true means exhibit patterns on the macroscopic scale, but contains inherent noise in individual groups. people X  X  opinions towards whether a police permit should be required before a person can buy a gun. We put 1 if per-son answers  X  X AVOR X  and 2 if a person answers  X  X PPOSE. X  Other answers such as  X  X on X  X  Know X  are ignored. The data for this survey is available from 1972 to 2012, with a total of 36,921 samples. We would like to estimate the average ap-proval rate of carrying a gun without a permit per year. Sim-ilarly, FEWORK is a question about people X  X  opinion regarding married women earning money in business or industry given their husband is capable of supporting her. Label  X 1 X  means APPROVE and label  X 2 X  means DISAPPROVE. It contains a total of 24,401 samples. Figure 3 shows the scatter plot of true means of both datasets. The plots show that the true means of both datasets exhibit macroscopic patterns, but for each category, there exists inherent noise. We vary the sampling percentage from 1% to 30%, run each estimator for 30 runs, and compute the mean absolute difference between the estimated means and the true means (the means when 100% of the data is used). The mean absolute difference is shown in terms of percentage to provide the readers with an intuitive interpretation of the results. For example, if the estimated mean is 1.6 and the true mean is 1.7, this means that it was estimated that 60% percent disapprove but the truth is that 70% disapprove, leading to a mean absolute difference of 10%.

The results are shown in figure 4. One can see that, when taking small percentages of the total data, the DNN method outperforms all the other methods and the OMP method al-most always outperforms all the other methods. For exam-ple, when 3% of the data is given the DNN method reduces error rates up to 30% over JS and 45%-61% over Avg .
We would like to apply compressive averaging to the data in [33]. The dataset in this paper contains non-representative polls taken from Xbox users about who they would vote for in the 2012 presidential elections. The surveys were taken in the 45 days leading up to the presidential elections and focuses on the two party (only Obama vs Romney) outcome. We collect a 1 for  X  X itt Romney X  and 2 for  X  X arack Obama. X 
Since the Xbox data was not representative of the entire population, post-stratification [22] was used to correct for the differences between the population that was sampled from the Xbox and the true population. To do this, [33] first splits the samples into cells based on the demographic information of each sample. Figure 4: Performance of different averaging methods on estimating the true mean of the dataset.

Given the cell values we can apply post-stratification using equation 21. Where y j is the estimate for cell j , J is the number of cells, and N j is the true size of the population in the j th cell. In addition, the post-stratification estimate for a subpopulation (i.e. how all college graduates will vote), can be computed by equation 22. Where J s is the indexes of all the cells that contain the subpopulation s .
Since the dataset is not representative, the means of the sampled data will not, by themselves, give useful informa-tion. However, we can still verify compressive averaging does a good job at adjusting means by adjusting the means of the raw samples as shown in figure 5.
To determine how well the algorithm works, the 2012 exit poll data can be used to estimate the true value for how subpopulations voted in the 2012 elections. Each subgroup in the demographics of sex, race, age, education, party ID, and ideology was evaluated for accuracy. An example of how the subgroups of demographics vary across time is shown in Figure 5: Performance of predicting the means of the raw samples of the Xbox dataset. figure 6. In addition, analysis was performed on the largest 30 two-dimensional demographic subgroups (i.e conservative males, females between 45 and 64 years old, etc.). For each subgroup or two-dimensional subgroup that is to be ana-lyzed, we post-stratify to that subgroup for each timepoint (i.e. get post-stratified estimate for college graduates at each timepoint) and use the post-stratified estimates as y , obtain the estimated means  X   X  , and then get the mean absolute dif-ference between the estimated mean at the last timepoint and the 2012 exit poll data. After we have done this for all the subgroups we obtain the mean and median absolute differences.

Figure 7 shows the results. We post-stratify on the age, sex, race, education, party, and ideology demographics. There were 246,683 respondents surveyed in total. Using only 10% of the data saves 221,967 samples.

The results show that compressive averaging has around the same accuracy when using 10% of the data as it does when using 100% of the data. In addition, when compressive averaging uses 10% of the data it performs comparable to, and often even better than, when the Avg method uses 100% of the data (This is shown in figure Figure 8). The mean or median absolute difference of the OMP method at 10% is, at most , greater than the Avg method at 100% by 0.1.

One can notice that in figure 7b that for all the meth-ods but Avg , there are percentages smaller than 100% at which they perform better. One reason this might be is that perhaps the value n is not precise because of post-stratification. It could be the value of n should be smaller, meaning we should trust y less, because of noise induced by post-stratification. Of course, n is smaller for smaller percentages, which may mean using 10% of the data gives a better estimate of n . However, we are not absolutely certain this is the correct explanation.

In addition, figure 7 is the only case in which the OMP method performs better than the DNN method. This could be due to the different evaluation methods. In the previous sections, we were evaluating how close the estimated means were to the true means. For this task, we are evaluating how the estimated mean at the very last day compares to the 2012 exit poll data.
We test how sensitive compressive averaging is to the spar-sity parameter k and the choice of the basis. The results for how the performance varies for choices of k between 1 and Figure 6: The average of who the categories in the age and party demographics said they would vote for across time. The dashed line represents 1.5 (or 50% voting for Obama). 6 is shown in figure 9. The results show that any choice of k greater than or equal to 3 does not lead to any significant decrease in performance.

Figure 10 shows how changing the amount of L lowest fre-quency components affects performance. Results show that it is necessary to use a small L . Higher L leads to worse performance. This is due to the higher frequency compo-nents fitting high variations in the data; which is especially problematic for noisy data. Using the 4 lowest frequency components actually leads to better performance than using the 8 lowest frequency like we did in our experiments.
Figure 11 shows how the performance varies when using a different basis. One can see that using the Harr wavelets results in a significant decrease in performance for the GUN-LAW dataset. This was expected because Haar wavelets are discontinuous. The discrete cosine transform-II performs slightly worse than the basis used in our experiments (wmp-sym) but performs better than sample averaging, multi-task averaging, and James Stein in most cases. The polynomial basis actually outperforms wmpsym.
We would like to investigate how the estimated means across time actually look under high noise, we take small fraction of the samples (as low as only one sample per time-point) for the GUNLAW dataset and visualize the estimated means along with the sample means ( y ) and true means (  X  ). The plots are shown in figure 12. Of course, most methods perform very poorly when there is only one sam-ple per timepoint, however, the DNN method is the closest to the true means in this case. As more samples are added the estimated means become closer to the true means, but even with 1146 total samples, the James Stein and multi-Figure 7: The mean and median absolute differences of all the demographic subgroups as a function of sample ratio. No adjustments or imputations were made to the cell estimates. We run 30 different trials at each percentage. task averaging methods deviate significantly more from the true mean than the OMP and DNN methods.
An explanation of Stein X  X  phenomenon is that the esti-mator shrinks unbiased maximum likelihood y [ t ]. Efron and Morris formalize this intuition through an Empirical Bayes argument [12]. The key assumption is a hierarchical Bayesian model, where the means to be estimated are drawn from a normal distribution centered around zero. This as-sumption implies that the maximum posterior estimate of the true means are shrinking the sample averages.

Several extensions of James-Stein estimator have been in-troduced. Bock [9] considers dependencies between the true means and uses a covariance matrix to model the sample av-erages. Senda and Taniguchi [24] develop a type of James-Stein estimator for time series regression models. It has been shown in [21] that the James-Stein estimator itself is not admissible, and is dominated by the positive part of James-Stein estimator.

The idea of using information from all groups to improve the estimate of the quantity of a single group falls into the big realm of Multi-Task Learning (MTL) in the ma-chine learning and data mining community. Early work in Figure 8: This plot shows the results from figures 7 of the av-erage method with 100% of the samples vs the performance OMP with only 10% of the samples. Points to the right of the dashed line means that our method performs better than the Avg method while using 200,000 fewer samples.
 MTL includes Thrun X  X   X  X earning to learn X  [31], and Baxter X  X   X  X earning internal representation X  [8]. One approach to mod-ern multi-task learning is to build a hierarchical Bayesian model that infers characteristics shared by all groups [29]. Another line of work relies on a regularizer that penalizes the estimates for different groups. The types of regulariza-tion include distance to means [14], trace norm [4], pairwise distance [18] constraints, etc. These multi-task learning al-gorithms are usually tailored for regression [26], classifica-tion [34], feature learning [6], etc. For the survey aggregation problem, recently Sergey et al. [15] proposed a Multi-Task Averaging (MTA) approach that adds a penalty term in-vented in MTL literature to improve the estimates.
Results from [33] show that one can use an adjustment model to impute missing cell data to get even better accu-racy when doing post-stratification. This introduces a new problem of adjusting n [ t ] (the number of samples at each timepoint t ) to account for the artificially added informa-tion which is, to the best of our knowledge, an open research problem. Preliminary results incorporating this method look promising.

We have shown how assuming the global mean of a tem-poral signal is sparsely represented in some basis can im-prove the estimation of the means. However, spatial data can be represented as a graph, which [25] showed can also be sparsely represented in some basis. Using this knowl-edge, it is possible that spatial data can also benefit from compressive averaging.

We would also like to extend this method to multi-way survey data, which can also be applied to the Xbox data. By dividing the population into demographic cells we could utilize low-rank matrix factorization techniques to further reduce the sampling rate required for these types of survey data.
In this paper we exploit the temporal relationships be-tween data by shrinking to an unknown global mean that is assumed to be sparsely represented in a given basis. We present two ways of estimating this global mean: orthogonal matching pursuit and deep learning. In our experiments, we were able to increase accuracy over sample averaging, James our experiments.
 Stein estimators, and multi-task averaging. In addition, we were able to estimate the results of the 2012 presidential election using 10% of the samples (saving 221,967 samples) and still getting accuracy similar to or better than sample averaging with 100% of the samples.
We would like to thank David Rothschild for providing the Xbox data for the 2012 presidential elections. Forest Agostinelli was supported by the NSF Graduate Research Fellowship Program (GRFP). Matthew Staib was supported by the Air Force Office of Scientific Research, National De-fense Science and Engineering Graduate (NDSEG) Fellow-ship, 32 CFR 168a. [1] General Social Survey (GSS). In P. Lavrakas (Ed.), [2] Coming and Going on Facebook. Pew Research [3] Widening Regional Divide over Abortion Laws. Pew [4] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A [5] F. Agostinelli, M. Hoffman, P. Sadowski, and P. Baldi. [6] A. Argyriou and T. Evgeniou. Multi-task feature [7] P. Baldi, P. Sadowski, and D. Whiteson. Enhanced [8] J. Baxter. Learning internal representations. In [9] M. Bock. Minimax estimators of the mean of a [10] E. J. Cand`es and M. B. Wakin. An introduction to [11] B. Efron. Large-scale inference: empirical Bayes [12] B. Efron and C. Morris. Limiting the risk of Bayes [13] B. Efron and C. Morris. Stein X  X  paradox in statistics. [14] T. Evgeniou and M. Pontil. Regularized multi X  X ask [15] S. Feldman, M. Gupta, and B. Frigyik. Multi-task The wpsym basis is the basis used in our experiments. Figure 12: Visualization of the estimated means from GUN-LAW survey taken between 1972 and 2012. The plots go from the extreme case of only one sample per timepoint (26 total samples because the survey was not conducted every year) to having all of the samples for each timepoint (36,921 sam-ples). [16] X. Glorot and Y. Bengio. Understanding the difficulty [17] A. Hannun, C. Case, J. Casper, B. Catanzaro, [18] J. Honorio and D. Samaras. Multi-task learning of [19] A. Kohut, C. Doherty, S. Keeter, et al. Polls face [20] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. [21] E. L. Lehmann and G. Casella. Theory of point [22] R. J. Little. Post-stratification: a modeler X  X  [23] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. [24] M. Senda and M. Taniguchi. James-stein estimators [25] T. Shi, D. Tang, L. Xu, and T. Moscibroda.
 [26] M. Solnon, S. Arlot, and F. Bach. Multi-task [27] C. Stein et al. Inadmissibility of the usual estimator [28] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, [29] J. B. Tenenbaum, C. Kemp, T. L. Griffiths, and N. D. [30] The Wallace Foundation. Workbook F: Telephone [31] S. Thrun. Learning to learn: Introduction. In In [32] J. A. Tropp and A. C. Gilbert. Signal recovery from [33] W. Wang, D. Rothschild, S. Goel, and A. Gelman. [34] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram.
