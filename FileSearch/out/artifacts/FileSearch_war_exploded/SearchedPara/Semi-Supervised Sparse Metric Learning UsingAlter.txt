 In plenty of scenarios, data can be represented as vectors and then mathematically abstracted as points in a Euclidean space. Because a great number of machine learning and data mining applications need proximity measures over data, a simple and universal distance metric is desirable, and met-ric learning methods have been explored to produce sensible distance measures consistent with data relationship. How-ever, most existing methods suffer from limited labeled data and expensive training. In this paper, we address these two issues through employing abundant unlabeled data and pur-suing sparsity of metrics, resulting in a novel metric learn ing approach called semi-supervised sparse metric learning . Two important contributions of our approach are: 1) it propa-gates scarce prior affinities between data to the global scope and incorporates the full affinities into the metric learning ; and 2) it uses an efficient alternating linearization method to directly optimize the sparse metric. Compared with con-ventional methods, ours can effectively take advantage of semi-supervision and automatically discover the sparse met -ric structure underlying input data patterns. We demon-strate the efficacy of the proposed approach with extensive experiments carried out on six datasets, obtaining clear per -formance gains over the state-of-the-arts.
 H.2.8 [ Database Management ]: Database Applications-Data Mining; H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms Metric learning, semi-supervised sparse metric learning, sparse inverse covariance estimation, alternating linearization
Vectored data frequently occur in a variety of fields, which are easy to handle since they can be mathematically ab-stracted as points residing in a Euclidean space. An appro-priate distance metric in this space spanned by input data vectors is quite demanding for a great number of applications including classification, clustering and retrieval. Two most commonly used distance metrics are Euclidean distance and Mahalanobis distance, of which the former is independent of the input data while the latter is related to second-order statistics of the input data. In practice, we need to seek distance metrics suitable for the requirements of different tasks.

In the context of classification, distance metrics are fre-quently applied in concert with kNN classifiers. As such, the goal of metric learning towards kNN classification is to keep the distances among nearby points as small as possible and push the differently labeled neighbors out of the neighbor-hood of any of these points. Neighbourhood Components Analysis (NCA) [11] and its seminal work Maximally Col-lapsing Metric Learning (MCML) [10] emphasize that the target metric should support tight neighborhoods and even reach zero distances within all neighborhoods. Like the no-tion of SVMs, Large Margin Nearest Neighbor classification (LMNN) [28] not only narrows the distance gap within all neighborhoods, but also maximizes the soft margin of dis-tances over each neighborhood.

As for clustering, metric learning usually cooperates with constrained clustering, namely, semi-supervised clustering [26][3][15] where some background knowledge about data proximities is given beforehand. Specifically, two kinds of pairwise links, i.e., must-links and cannot-links , are given. The must-links indicate that two data points must be in the same cluster, while the cannot-links require that two data points not be grouped into the same cluster. Therefore, the purpose of metric learning applied to semi-supervised clus-tering is to minimize the distances associated with must-links and simultaneously maximize the distances associated with cannot-links. There have been some works [29][4][8][25] which are engaged in learning metrics towards better clus-terings.

In the field of content-based image retrieval (CBIR), choos-ing appropriate distance metrics plays a key role in establis h-ing effective systems. Regular CBIR systems usually adopt the Euclidean distance measure for images represented in a vector form. Unfortunately, Euclidean distance is gener-ally not effective enough in retrieving relevant images. A main reason stems from the well-known semantic gap be-tween low-level visual features and high-level semantic con -cepts [24]. The commonly used relevance feedback scheme [23] may remedy the semantic gap issue, which produces, aided by users, a set of pairwise constraints about relevanc e (similarity) or irrelevance (dissimilarity) between two im-ages. These constraints along with involved image examples are called log data . Then the key to CBIR is to find an effective way of utilizing the log data in relevance feedback so that the semantic gap can be successfully reduced. A lot of ways have been studied to utilize the log data to boost the performance of CBIR. In particular, one can use a met-ric learning technique devoted to semi-supervised cluster-ing for tackling CBIR since these relevance constraints are essentially must-links and cannot-links. The recent works [2][14][18] have recommended learning proper distance met -rics for image retrieval tasks.

In this paper, we pose metric learning under the semi-supervised setting where only a few pairwise constraints in-cluding similar and dissimilar exist and most data instances are not involved in such constraints. We propose a novel metric learning technique called semi-supervised sparse met-ric learning (S 3 ML). The major features of S 3 ML include: 1) it is capable of propagating scarce pairwise constraints to all data pairs; 2) it generates a sparse metric matrix which coincides with the sparse spirit of feature correlations in the high-dimensional feature space; and 3) it is quite efficient by using the alternating linearization method in contrast to existing metric learning approaches using expensive opti-mizations such as semidefinite programming. The proposed S
ML technique has widespread applicability without be-ing limited to particular backgrounds. Quantitative exper-iments are performed for classification and retrieval tasks, uncovering the effectiveness and efficiency of S 3 ML.
The remainder of this paper is arranged as follows. Section 2 reviews the related work on recent metric learning. Section 3 describes and addresses the semi-supervised metric learn-ing problem. Section 4 presents the S 3 ML algorithm by us-ing the alternating linearization optimization method. Se c-tion 5 validates the efficacy of the proposed S 3 ML through extensive experiments. Section 6 includes conclusions.
In recent years, there are some emerging research inter-ests in learning data representations in some intrinsic low -dimensional space embedded in the ambient high-dimensiona l space such that regular Euclidean distance is more meaning-ful in the low-dimensional space. The early efforts are learn -ing linear representations by Principal Component Analy-sis (PCA) and learning nonlinear representations by mani-fold learning. However, these methods are unsupervised and loosely related to the distance outcome. This paper inves-tigates distance metric learning which is vital to a lot of machine learning and data mining applications. The recent metric learning research can be classified into three main categories as follows.
The first category is supervised metric learning approaches for classification where distance metrics are usually learn ed from training data associated with explicit class labels. Th e representative techniques include Neighbourhood Compo-nents Analysis (NCA) [11], Maximally Collapsing Metric Learning (MCML) [10], and metric learning for Large Mar-gin Nearest Neighbor classification (LMNN) [28]. Neverthe-less, the performance of these supervised approaches rests highly on the amount of labeled data that are often prac-tically difficult and expensive to gather. Moreover, all of them request nontrivial optimizations such as semidefinite programming [5], which is inefficient for real-world dataset s.
Our work is closer to the second category of weakly su-pervised metric learning which learns distance metrics from pairwise constraints present in input data, or known as side information [29]. It is manifest that such side informa-tion is weaker than exact label information. In detail, each constraint indicates whether two data points are relevant (similar) or irrelevant (dissimilar) in a particular learn ing task. A well-known metric learning method with these con-straints was proposed by Xing et al. [29], which casts the learning task into a convex optimization problem and ap-plies the generated solution to data clustering. Following this work, there are several emerging metric learning tech-niques in the  X  X eakly supervised X  direction. For instance, Relevance Component Analysis (RCA) learns a global lin-ear transformation by exploiting only the equivalent (rele-vant) constraints [2]. Recently, Information-Theoretic M et-ric Learning (ITML) [8][7] expresses the weakly supervised metric learning problem as a Bregman optimization prob-lem where the pairwise constraints are treated as inequality constraints.
In [21], to speedup the training time of metric learning,  X  regularization is incorporated into the original non-spar se metric learning objective, resulting in a much faster learn ing procedure: Sparse Distance Metric Learning (SDML). Be-sides, the sparsity of desirable metric matrices makes sens e since the Mahalanobis matrix is nearly sparse under the high-dimensional data space. This sparsity spirit stems fr om the weak correlations among different feature dimensions in the high-dimensional feature space because most distinc t features are measured by distinct mechanisms and relativel y independent of each other. In another perspective, [22][30 ] attempt to learn a low-rank sparse metric matrix by induc-ing sparsity to a low-rank linear mapping whose outer prod-uct constitutes the sparse metric. Nonetheless, learning l ow-rank sparse metrics often implicates complex optimization which has a large computational cost and is sensitive to the choice of the low rank as well.
In this section, we investigate the semi-supervised met-ric learning problem [14] which takes scarce pairwise con-straints as input and, importantly, exploits abundant unla-beled data that are not involved in these constraints. Con-sequently, semi-supervised metric learning supplements the aforementioned weakly supervised metric learning due to the merit of accessing unlabeled data. Different from standard semi-supervised learning [31], semi-supervised metric learn-ing does not need any class labels, so it can be applied to a broad spectrum of applications such as semi-supervised classification, semi-supervised clustering, relevance fee dback based information retrieval, etc.
Assume that we are given a set of n data points X = { x these data points: S = { ( i, j ) | x i and x j are judged to be similar }
D = { ( i, j ) | x i and x j are judged to be dissimilar } , (1) where S is the set of similar pairwise constraints, and D is the set of dissimilar pairwise constraints. Each pairwis e constraint ( i, j ) indicates if two data points x i and x relevant or irrelevant judged by users under some applicati on context. Note that it is not necessary for all the points in X to be involved in S or D .

For any pair of points x i and x j , let d ( x i , x j ) denote the distance between them. To compute the distance, let M  X  R m  X  m be a symmetric metric matrix. We can then express the distance measure as follows: d
M ( x i , x j ) = k x i  X  x j k M = p ( x i  X  x j )  X  M ( x In practice, the symmetric matrix M is a valid metric if and only if it satisfies the non-negativity and the triangle inequality conditions. In other words, M must be positive semidefinite, i.e., M 0. Generally, the matrix M parame-terizes a family of Mahalanobis distances on the vector spac e R m . As an extreme case, when setting M to be identity ma-trix I  X  R m  X  m , the distance in eq. (2) becomes the common Euclidean distance.

Definition 1. The goal of semi-supervised metric learn-ing is to learn an optimal symmetric matrix M  X  R m  X  m from a collection of data points X on a vector space R m to-gether with a set of similar pairwise constraints S and a set of dissimilar pairwise constraints D , which can be formulated as the following optimization prototype: where M is maintained to be positive semidefinite and g ( ) is a proper objective function defined over the given data and constraints.

Given the above definition, the strategy to attack metric learning is to first design an appropriate objective functio n g and then seek an efficient algorithm to minimize it. In the following, we discuss some principles for formulating reas on-able optimization models. Importantly, we emphasize that it is critical for solving real-world metric learning proble ms to avoid overfitting.

Min-Max Principle. One common principle for metric learning is to minimize the distances among the data points with similar constraints and meanwhile to maximize the dis-tances among the data points with dissimilar constraints. We refer to it as a min-max principle. Many existing met-ric learning works such as [29][28][14] can be interpreted via this min-max principle. Immediately, we can define g simply based on this principle: where  X  &gt; 0 is the trade-off parameter. Although the above objective function has been shown effective for some cluster -ing tasks, it may be unsuitable for the applications in which the pairwise constraints are scarce and most data instances are not involved in any constraint. As a matter of fact, optimizing the above g is likely to overfit the two limited constraint sets S and D . Hence, we should incorporate all data in X into the design of g .

To facilitate the following derivations, we assume that there exits a linear mapping U  X  : R m  X  R r ( U = [ u 1 , . . . , u  X  R m  X  r ) such that M = U U  X  . We require that u 1 , . . . , u be linearly independent so that r is the rank of the metric matrix M . Then the distance under M between two inputs can be computed as: Actually, the target metric M is usually low-rank in high-dimensional data spaces [7]. Thus, we may pursue the sub-space U equivalently.

Affinity-Preserving Principle. To remedy overfitting, we aim at taking full advantage of unlabeled data that are demonstrated to be quite beneficial to the semi-supervised learning problem. Due to this consideration, we define g based on the notion of affinity preserving [18]. Given the collection of n data points X including the data involved in the given constraints and the unlabeled data, we need an affinity matrix W  X  R n  X  n on X such that each entry W ij measures the strength of affinity between data pair x i and x . The larger W ij , the closer x i and x j . Through absorbing all data information X = [ x 1 , , x n ] guided by the affinity matrix W , we formulate g as follows: = 1 = = tr U  X  XLX  X  U = tr XLX  X  U U  X  = tr ( XLX  X  M ) , where tr ( ) stands for the trace operator, and D is a diagonal matrix whose diagonal elements equal the sums of the row entries of W , i.e., D ii = P n j =1 W ij . The matrix L = D  X  W is known as the graph Laplacian [31]. Laplacian Regularized Metric Learning (LRML) [14] and SDML [21] follow the same affinity-preserving principle. Un-der supervised settings, it is quite simple to obtain the affin-ity matrix W by setting W ij = 1 if ( i, j )  X  S and W ij if ( i, j )  X  D . However, under semi-supervised settings, there is no intuitive way to acquire W since S and D often con-tain very few data instances. As a weak measure of affinities, LRML defines W in an unsupervised fashion, i.e., W ij = 1 if x i is among k nearest neighbors of x j or vice versa. Such a definition for W fails to absorb the pairwise constraints and thus does not take full advantage of semi-supervision.
We aim at designing better affinity matrices for semi-supervised settings through integrating the min-max princi -ple and the affinity-preserving principle. Based on the weak affinities revealed by k -NN search, we intend to propagate the strong (definitely correct) affinities revealed by the giv en pairwise constraints to the global scope of the data. Let us define a neighborhood indicator matrix P  X  R n  X  n on X : where N i denotes the index list composed of k nearest neigh-bors of data point x i using the Euclidean distance. Note that P is asymmetric and provides weak affinities.
To enable metric learning techniques to work for practi-cal applications, we should shrink the distances between as many similar pairs as possible and enlarge the distances be-tween as many dissimilar pairs as possible. Although the affinity-preserving function g has incorporated all unlabeled data, it does not emphasize the distances between X  X eal X  X im -ilar or dissimilar data pairs. Since the two real constraint sets S and D are available, we desire to propagate the lim-ited real constraints to all data pairs via the found neigh-borhoods P using the Euclidean distance. Specifically, we learn an affinity matrix W such that W ij reflects the extent of propagated affinity between data pair ( x i , x j ).
Let us begin with an initial affinity matrix W 0  X  R n  X  n where we set W 0 ii = 1 for any i , W 0 ij = 1 for any ( i, j )  X  S , W ij =  X  1 for any ( i, j )  X  D , and W 0 ij = 0 otherwise. Clearly, W 0 represents the strong affinities. If we consider  X  1-entries in W 0 as signed energies, our intent is to propagate the ener-gies in W 0 to its 0-entries. The propagation path coincides with the neighborhood structure at each data point, so we pose the affinity propagation criterion as the locally linear energy mixture, i.e., where W ( t ) i. denotes the i th row of W ( t ) and t = 0 , 1 , 2 , is the time stamp. We write the matrix form of eq. (7) as where 0 &lt;  X  &lt; 1 is the trade-off parameter and P is like the transition probability matrix widely used in Markov random walk models. Because 0 &lt;  X  &lt; 1 and the eigenvalues of P are in [  X  1 , 1], the limit W  X  = lim t  X  X  X  W ( t ) exists. It suffices to solve the limit as Then, we build a new reliable affinity matrix by symmetriz-ing the converged affinity matrix W  X  and removing unreli-able affinities, that is in which the operator  X  S  X  || X   X  zeros out the entries of S whose absolute values are smaller than 0 &lt;  X  &lt; 1.
In summary, we are capable of learning a better affinity matrix provided with strong affinities, i.e., real constraint s. The effect of learning affinity matrices subject to the known pairwise constraints is enhancing the generalization and r o-bustness capabilities of the affinity-preserving function g for-mulated in eq. (5). The learned affinities between all data pairs lead to better generalization characteristics of g than using only strong affinities or only weak affinities. Compared with our parallel work [18] which employs similarity (posi-tive affinity) propagation to learn non-sparse metrics, the proposed affinity propagation in this paper discloses more hidden relevances and irrelevances among data.
So far, we can give a general formula for semi-supervised metric learning as the minimization of the log-determinant Bregman divergence D  X  d ( M, M 0 ) = tr ( M M  X  1 0 )  X  log det( M M 0 )  X  m [8] between a given metric matrix M 0  X  R m and the desirable metric matrix M regularized by the affinity-preserving function g ( M, X , S , D ) = tr ( XLX  X  M ) prescribed in subsection 3.1, that is where  X  &gt; 0 is the regularization parameter and the graph Laplacian L is computed based on the learned affinity matrix in eq. (10). This optimization problem is convex, but it is not easy to solve efficiently. In the next section, we propose a fast algorithm to solve it with  X  1 regularization. We adopt the following notations throughout this section. We use S m + to denote the set of positive semidefinite matrices in dimensions m  X  m . For matrix M , k M k 0 represents the number of components which are nonzeros in M ; k M k 1 is the sum of absolute values of all components in M ; and the matrix inner product operator is h M, A i = tr ( M  X  A ).
Let us start with the hot statistical problem Sparse In-verse Covariance Estimation (SICE) [1][9]. To estimate the sparse inverse covariance matrix, one common approach is to penalize its maximum likelihood function by a cardinality term. Thus the maximum likelihood estimate of the sparse inverse covariance matrix reduces to the following optimiz a-tion problem: where  X  is the empirical covariance matrix, k M k 0 is a penalty term to enforce the solution to be sparse, and  X  &gt; 0 is the trade-off parameter to balance the maximum likelihood and the sparsity. This problem is combinatorial in essence and thus numerically intractable. A common approach to over-come this difficulty is to replace the cardinality norm k M k by its tightest convex relaxation,  X  1 norm k M k 1 (see [13]), which results in the following convex optimization problem : or equivalently, the following minimization problem: If we regarded  X  = M  X  1 0 +  X XLX  X  , the semi-supervised metric learning problem formulated in eq. (11) could be converted to the SICE problem by introducing the  X  1 norm. Therefore, we call the process of optimizing eq. (13) as semi-supervised sparse metric learning (S 3 ML).

Note that eq. (13) can be rewritten as Thus the dual problem of eq. (13) is given by exchanging the order of max and min in eq. (14), i.e., which is equivalent to We will see in the next subsection that our algorithm easily constructs a feasible solution to the primal problem eq. (13) and a feasible solution to the dual problem eq. (15). Thus we can easily compute the duality gap and use it to determine when to terminate the algorithm and claim optimality.
If we define and h ( M ) :=  X  k M k 1 , then eq. (13) can be viewed as mini-mizing the sum of two convex functions f and h : We can leverage the alternating linearization method (ALM) proposed in [12] to solve eq. (17). ALM requires the two functions f and h to be both in the class C 1 , 1 ( C 1 , 1 tains differentiable functions whose gradients are Lipschi tz continuous), which means their gradients are Lipschitz con -tinuous. So we need to smooth the  X  1 term h ( M ) first. One way to smooth the  X  1 function h ( M ) is to apply Nes-terov  X  X  smoothing technique [20]. We use h  X  ( M ) to denote a smoothed approximation to h ( M ) with a smoothness pa-rameter  X  . According to the Nesterov X  X  technique, h  X  ( M ) is given by in which k . k F denotes the Frobenius norm. It is easy to check that U  X  := min {  X , max { M/ X ,  X   X  }} is the optimal solution to eq. (18). According to Theorem 1 in [20], the gradient of h ( M ) is given by  X  h  X  ( M ) := U  X  and is Lipschitz continu-ous with constant L ( h  X  ) = 1 / X  . ALM which can solve the smoothed SICE problem Algorithm 1 ALM for SICE
Input: M 0 = Y 0 , i = 0. repeat until M i converges
Output: M = M i . is described in Algorithm 1 where := 1 / max { L ( f ) , L ( h It is easy to see that in the i -th iteration of Algorithm 1,  X  h  X  ( M i ) +  X  is a feasible solution to the dual problem eq. (15), so we can conveniently compute the duality gap between this dual feasible solution and the primal feasible solution M i . We terminate Algorithm 1 once the duality gap achieves the desired accuracy.

Remark 2. There are four advantages of our ALM algo-rithm over the block coordinate descent algorithm in [1] and [9]. First, we consider the primal problem eq. (17) where the  X  1 term is involved, so ALM will preserve the sparsity of the desired metric matrix M . However, the block coordinate descent algorithm in [1][9] was proposed for the dual problem eq. (15) where the desired matrix M is obtained by inverting the optimal dual solution. Such a matrix inversion usually results in a dense matrix due to the floating-point errors en-countered in inverting a matrix. Second, the matrix M in Algorithm 1 is obtained by solving the subproblem where the log det( M ) term is involved, so M is always positive definite throughout Algorithm 1. Third, both subproblems in Algo-rithm 1 have closed-form solutions and thereby can be solved substantially efficiently. Finally, there are no iteration com-plexity results about the block coordinate descent algorithm in [1][9]. In contrast, we can obtain an iteration complexity bound for ALM as we will show below.

When we apply ALM to the smoothed SICE problem eq. (19), we do get an iteration complexity bound. However, the complexity results in [12] do not apply to Algorithm 1 right away. To apply the complexity results in [12], we need to modify Algorithm 1 a little bit. First, although the func-tion h  X  is in the class C 1 , 1 , the function f is not in the class C 1 , 1 since log det M is not in C 1 , 1 on S m + . However, re-stricted on the convex set { M : M  X   X I } with  X   X  &gt; 0, f ( M ) is in C 1 , 1 , i.e., its gradient  X  f ( M ) =  X  M  X  1 + X  is Lipschitz continuous with Lipschitz constant L ( f ) =  X   X   X  2 . According to Proposition 3.1 in [19], the optimal solution M  X  to eq. (13) satisfies M  X   X I where  X  = 1 / ( k  X  k + m X  ) and k  X  k denotes the largest eigenvalue of matrix  X . We denote the domain in which f ( M ) is in C 1 , 1 by C , i.e., C := { M  X  S m : M  X I } . Thus we can impose constraint M  X  C to the subproblem with respect to M in Algorithm 1 to guarantee that f is in the class C 1 , 1 . Second, Y i might be not positive definite and f ( Y i ) is thus not well-defined. To this end, we need to also impose the constraint Y  X  C to the subproblem with respect to Y in Algorithm 1 to guarantee that f ( Y i ) is well-defined.
Before addressing our main complexity result, we need to define the terminology  X  -optimal solution first.
Definition 3. Suppose x  X  is an optimal solution to the problem where C  X  R n is a convex set. x  X  C is an  X  -optimal solution to eq. (20) if f ( x )  X  f ( x  X  )  X   X  holds.

We have the following result about the relation between an approximate solution to eq. (19) and an approximate solution to eq. (17).

Theorem 4. For any  X  &gt; 0 , we let  X  := M  X  is an  X / 2 -optimal solution to eq. (19), then M  X  is an  X  -optimal solution to eq. (17).

Proof. It is easy to verify that (see equation (2.7) in [20]): where D h := max { 1 2 k M k : k M k  X   X   X  } = 1 2 m 2  X  2 M  X  is an optimal solution to eq. (17) and M  X   X  is an opti-mal solution to eq. (19), then we exploit the inequalities in eq. (21) to derive where the third inequality is due to the fact that M  X  is an  X / 2-optimal solution to eq. (19) and the last equality is due to  X  =  X 
Now we are ready to give the iteration complexity bound for ALM.

Theorem 5. By imposing constraints M  X  C and Y  X  C in the two subproblems in Algorithm 1, Algorithm 1 returns an  X  -optimal solution to eq. (17) in O (1 / X  2 ) iterations.
Proof. From Theorem 4.2 and discussions in section 5.3 in [12], after imposing constraints M  X  C and Y  X  C in the two subproblems in Algorithm 1, Algorithm 1 returns an  X / 2-optimal solution to eq. (19) in O ( 1 we choose  X  =  X  conclude that Algorithm 1 returns an  X  -optimal solution to eq. (17) in O ( 1 Now we show how to optimize the two subproblems in Algorithm 1. The first-order optimality condition for the M -subproblem in Algorithm 1 is Since  X  f ( M ) =  X  M  X  1 + X , it is easy to verify that M V diag(  X  ) V  X  satisfies eq. (22) and is thus optimal to the M -subproblem in Algorithm 1, where V diag( d ) V  X  is the eigenvalue decomposition of Y i  X  ( X  +  X  h  X  ( Y i )) and Table 1: Dataset Information: the numbers of fea-tures, samples and classes.
 Note that  X  j is always strictly positive since &gt; 0 and matrix M i is hence always positive definite. If the constraint M  X  C is imposed in the M -subproblem in Algorithm 1, the only change in the optimal solution is changing eq. (23) to The first-order optimality condition for the Y -subproblem is Since  X  h  X  ( Y ) = min {  X , max { Y / X ,  X   X  }} , it is easy to verify that satisfies eq. (24) and is thus optimal to the Y -subproblem in Algorithm 1. In summary, the M -subproblem is correspond-ing to an eigenvalue decomposition and the Y -subproblem is a trivial projection operation. Consequently, solving M -subproblems dominates the computational complexity since solving Y -subproblems is much cheaper compared with the computational effort on solving M -subproblems.
Given a set of n data points X = { x i } n i =1  X  R m with a similar constraint set S and a dissimilar constraint set D , an 0 and  X  &gt; 0, and input metric matrix M 0 (identity matrix or inverse covariance matrix), we summarize the proposed semi-supervised sparse metric learning (S 3 ML) algorithm in Algorithm 2. Interestingly, the proposed S 3 ML algorithm can also work under supervised settings when simply setting W = W 0 , so S 3 ML is appropriate for various metric learning problems.
In this section, we compare the proposed semi-supervised sparse metric learning (S 3 ML) algorithm with several exist-ing state-of-the-art metric learning algorithms on six dat asets including four benchmark UCI datasets and two real-world image datasets. Table 1 describes fundamental information about these datasets. The compared methods include: Euclidean : the baseline denoted as  X  X U X  in short.
Mahalanobis : a standard Mahalanobis metric denoted as  X  X ah X  in short. Specifically, the metric matrix A = Cov  X  1 where Cov is the sample covariance matrix. Algorithm 2 S 3 ML
Input: Three sets X , S , D , an integer k , four real-valued parameters 0 &lt;  X  &lt; 1,  X  &gt; 0,  X  &gt; 0 and  X  &gt; 0, and input metric matrix M 0 . 1. k -NN Search: Construct a neighborhood indicator matrix P  X  R n  X  n upon all n samples in X = [ x 1 , , x in which N i denotes the set consisting of the indexes of k nearest neighbors of x i . 2. Affinity Propagation: Set an initial affinity matrix
W 0  X  R n  X  n by W 0 ii = 1 for  X  i  X  { 1 , , n } , W 0 for  X  ( i, j )  X  S , W 0 ij =  X  1 for  X  ( i, j )  X  D , and W otherwise. Calculate W  X  = (1  X   X  )( I  X   X P )  X  1 W (0) and the final affinity matrix is Calculate the matrix T = XLX  X  where L = D  X  W and
D = diag( W 1 ). 3. Alternating Linearization Method: Set  X  = M  X  1 0 +  X T . Solve the following optimization problem using Output: The sparse metric matrix M .

LMNN [28]: Large Margin Nearest Neighbor which works under supervised settings where each sample has an exact class label.

ITML [8]: Information-Theoretic Metric Learning which works under pairwise relevance constraints but does not ex-plicitly engage the unlabeled data.

SDML [21]: Sparse Distance Metric Learning which works under pairwise relevance constraints to produce sparse met -rics but does not explicitly engage the unlabeled data. S 2 ML : the supervised counterpart of S 3 ML with W = W 0 , which works under pairwise relevance constraints to produce sparse metrics and does not engage the unlabeled data.

LRML [14]: Laplacian Regularized Metric Learning which works under pairwise relevance constraints and explicitly en -gages the unlabeled data.

S 3 ML : our semi-supervised sparse metric learning method which works under pairwise relevance constraints to produce sparse metrics and explicitly engages the unlabeled data.
To sum up, the compared distance metrics include two standard unsupervised metrics, four (weakly) supervised met-rics LMNN, ITML, SDML and S 2 ML, as well as two semi-supervised metrics LRML and S 3 ML. For the implementa-tion of affinity propagation concerned in S 3 ML, we obtain the full affinity matrix in eq. (10) by setting k = 6,  X  = 0 . 5, and  X  = 0 . 01 for all datasets. To run ALM for both of S and S 3 ML, we fix the smoothing parameter  X  = 10  X  6 for all datasets. We tune two regularization parameters  X  (for affinity-preserving) and  X  (for sparsity) to the best values on each dataset.
 Table 2: Comparisons of classification error rates (%) on UCI datasets.

We apply eight distance metrics listed above on four UCI datasets: Iris , Wine , Breast Cancer and Car , where we randomly choose 5% examples from each class as labeled data and treat the other examples as unlabeled data. We evaluate kNN (k=1) classification performance in terms of error rates on unlabeled data. For each dataset, we repeat the evaluation process with the 8 algorithms 50 times, and take the average error rates for comparison. Table 2 reports the average error rates for all compared methods. In this group of experiments, we let the inverse covariance matrix be the initial metric matrix that is fed to ITML, SDML, S ML and S 3 ML.
 In contrast to EU, Mah, LMNN, ITML, SDML, S 2 ML and LRML, the proposed S 3 ML achieves the lowest average error rates across all these datasets. The significant improvemen t of S 3 ML over S 2 ML demonstrates that the X  X emi-supervised X  scenario makes sense and that the proposed affinity propaga-tion trick used in S 3 ML effectively utilizes the information of unlabeled data. While the computational efficiency of S
ML is comparable to the recently proposed sparse metric learning method SDML that applies the block coordinate descent algorithm to solve the related SICE, S 2 ML achieves better average classification performance on three dataset s, and more importantly, SDML cannot guarantee to produce truly sparse metrics in bounded iterations.
We conduct the experiments for image retrieval tasks on two image datasets. One is the MSRA-MM dataset [27] which consists of 10,000 images and labeled with 50 con-cepts. The other is the NUS-WIDE dataset [6] which con-sists of 269,648 images with 81 concepts labeled.

In this group of experiments, we set the identity matrix to be the initial metric matrix used for ITML, SDML and S
ML. Since the supervised metric learning approach LMNN requires explicit class labels, it is unsuitable for image re-trieval. Therefore, we only compare six methods (excluding LMNN and S 2 ML). We construct a subset for each image concept by selecting 2500 images in which 500 are labeled as relevant and 2000 as irrelevant from the entire datasets. Then, we randomly select 20% samples to form the similar and dissimilar pair sets. We perform 20 random trials and show the averaged performance. The visual features we used in this experiment is the 225-dimentional block-wise color moments extracted over 5  X  5 fixed grid partitions with each block described by 9-dimensional feature. The performance rate.
 Table 3: Comparisons of Mean APs (MAPs) (%) on image datasets.
 is measured by the widely used non-interpolated Average Precision (AP) which averages the precision values obtaine d when each relevant image occurs. We average the APs over all concepts in each dataset to get the Mean AP (MAP) for overall performance measurement.

Table 3 lists MAPs of six methods in comparison. We also show APs for ten concepts in each dataset in Fig. 1 and Fig. 2, respectively. We can observe that S 3 ML consistently achieves the highest accuracy for the most of the concepts. Both MAPs and APs results show that the proposed S 3 ML is very promising for handling image retrieval tasks.
This paper proposes a semi-supervised sparse metric learn-ing (S 3 ML) algorithm which works under a few of the pair-wise similar and dissimilar constraints and produces favor -able sparse metrics. In contrast to previous metric learning techniques, the proposed S 3 ML can employ unlabeled data in a principled way, i.e., affinity propagation, that assigns reliable affinities to all data pairs through propagating pri or strong affinities. Observing that the existing sparse metric learning methods did not optimize the sparse metrics explic-itly, we apply the alternating linearization method to opti -mize the sparse metrics directly. This alternating lineari za-tion method has appealing computational and theoretical properties. Extensive experiments have been conducted to evaluate classification and retrieval performance using th e sparse metrics offered by S 3 ML. The promising results show that S 3 ML is superior to the state-of-the-arts.

Suppose data do not have a vector form and are merely measured via some kernel function, then nonlinear metrics are needed. Our latest work [16] actually learned a nonlinear metric for semi-supervised classification. Another concern is that if the data dimension is substantially large any metric learning method will be computationally expensive and even prohibitive. Motivated by our earlier work [17], it is possi ble to embed subspace learning into sparse metric learning so that S 3 ML is able to work on very high-dimensional data such as microarray data and text data.
This work was supported by NTU NAP Grant with project number M58020010 and the Open Project Program of the State Key Lab of CAD&amp;CG (Grant No. A1006), Zhejiang University. This work was also supported by grants from Natural Science Foundation of China (No. 60975029) and Shenzhen Bureau of Science Technology&amp;Information, China (No. JC200903180635A).
 rate.
