 Automatic image annotation plays an important role in mod-ern keyword-based image retrieval systems. Recently, many neighbor-based methods have been proposed and achieved good performance for image annotation. However, existing work mainly focused on exploring a distance metric learning algorithm to determine the neighbors of an image, and ne-glected the subsequent keyword propagation process. They usually used some simple heuristic propagation rules, and propagated each keyword independently without consider-ing the inherent semantic coherence among keywords. In this paper, we propose a novel learning-based keyword prop-agation strategy and incorporate it into the neighbor-based method framework. In particular, we employ the structural SVM to learn a scoring function which can evaluate differ-ent candidate keyword sets for a test image. Moreover, we explicitly enforce the semantic coherence constraint for the propagated keywords in our approach. The annotation of the test image is propagated as a whole rather than sepa-rate keywords. Experiments on two benchmark data sets demonstrate the effectiveness of our approach for image an-notation and ranked retrieval.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Performance, Experimentation image annotation, semantic coherence, structural learning
In recent decades, the number of digital images has been growing rapidly and there is an increasingly urgent demand for indexing and retrieving these images effectively. Users often prefer searching images with a textual query, which can be achieved by first annotating images manually, and then searching over the annotations using the query. However, manual image annotation is a laborious and time-consuming process. Therefore, researchers have attempted to develop automatic image annotation techniques for image retrieval.
Automatic image annotation aims to assign relevant key-words to a new image from an annotation vocabulary. Re-cently, the neighbor-based methods [4, 6, 7] have become more attractive because of their superior performance and straightforward implementation scheme.

Given an unlabeled image, the neighbor-based methods first find its visually similar neighbors, and then propagate the keywords associated with these neighbors to it. Exist-ing work mainly focused on the first step, where they tried to explore a distance metric learning algorithm to deter-mine the neighbors of an image. However, the subsequent keyword propagation process was not well investigated. In most cases, they used some heuristic propagation rules, e.g., simply transferred the most frequent keywords in the near-est neighbors to the given image. But there is no guarantee that the keywords selected by these simple heuristic rules are always the most suitable candidate annotations. Be-sides, most of the neighbor-based methods propagate each keyword independently without considering the correlation-s among keywords. In fact, the keywords associated with an image do not appear in isolation, instead they appear correlatively and interact coherently with each other at the semantic level. Therefore, it is difficult for these methods to propagate semantically coherent keywords to the given image.

In light of the above problems, we propose a novel learning-based keyword propagation strategy and incorporate it into the neighbor-based method framework. The image annota-tion task is formulated as a structured prediction problem, where the goal is to learn a mapping from an image to an as-sociated subset of the keywords. To this end, we utilize the structural SVM [5] as the backbone of our learning approach. Specifically, based on the annotated training examples, the structural SVM seeks to learn a scoring function which can ev aluate different candidate keyword sets for a test image. The quality of a keyword set is assessed based on its rele-vance to the neighbors of the test image in different aspects. Finally the keyword set maximizing the scoring function is propagated to the test image. In addition, the semantic co-herence constraint for the propagated keywords is explicitly enforced as part of optimization objective in our approach. Therefore, the annotation of the test image is predicted as a semantically coherent whole instead of separate keywords.
Let X = { x 1 ,x 2 ,...,x N } denote an image collection, and all unique keywords appearing in this collection are W = { w 1 ,w 2 ,...,w M } . The goal of the annotation task is to learn a hypothesis h : X  X  Y , where Y denotes the space of all possible keyword subsets. Given an image x  X  X , we use h to predict an associated keyword subset y  X  X  for x .
In the supervised learning scenario, we are given a set of annotated training images, S = { ( x ( i ) ,y ( i ) )  X  X  X Y 1 ,...,T } , where y ( i ) is the ground-truth annotation of the image x ( i ) . We hope the learned hypothesis h can minimize the empirical risk, notation h ( x ( i ) ) compared to the ground-truth annotation y ( i ) . For the annotation task, a loss function similar to F measure is defined as follows: where y and y  X  are two annotations, | y | denotes the num-ber of keywords in y , and | y  X  y  X  | is the number of common keywords they share.

In this paper, we adopt the structural SVM [5] as the back-bone of our learning strategy to tackle the above problem. The idea behind the structural SVM is to discriminatively learn a scoring function F ( x,y ) : X  X Y  X  R , which mea-sures how well the candidate annotation y fits for a given image x . We represent the image/annotation pair ( x,y ) by a feature vector  X ( x,y ). In analogy to the linear SVM, the scoring function F ( x,y ) is assumed to be linear in terms of  X ( x,y ): where w denotes the weight vector. Intuitively, the feature representation  X  must be able to provide significant discrim-inative power between high quality and low quality candi-date annotations. We will discuss the exact form of  X  in the next section.

Once the scoring function F ( x,y ) is learned, the hypoth-esis h can predict the annotation y  X  for an image x by max-imizing F ( x,y ) over all possible y  X  X  : Following previous work [4, 6, 7], we assign L ( L = 5) key-words to each test image, i.e. | y  X  | = L .
In this section, we discuss the feature representation  X  in equation (3). For training examples,  X  should represen-t a set of discriminating features that can differentiate the ground-truth annotation of an image from other alternative annotations. Intuitively, visually similar images often re-flect similar themes and thus are typically annotated with similar keywords. Starting from this intuition, given an im-age/annotation pair ( x,y ), we first find the visual neighbors of x , and then formulate  X ( x,y ) based on the relations be-tween y and these neighbors. The form of  X ( x,y ) is given as where NN 1 ,...,NN K are the K nearest neighbors of x , and S
NN 1 ,...,S NN K denote their similarities to x .  X  ( w,NN is a feature vector encoding the relation between the key-word w and the i th neighbor NN i . From the above defini-tions, we can see that  X ( x,y ) actually represents the vector composition of K relation components.

In this paper, we simply determine the visual neighbors by the average of several distances computed from different features as [4] did. The impact degree of NN i on x is as-sumed to be positively correlated with their similarity S and we define S NN i as follows: Here, d ( x,NN i ) is the normalized visual distance between x and NN i .

In equation (5), the feature vector  X  ( w,NN i ) encodes the relation between the keyword w and the i th neighbor NN i It is formulated from the aspects of frequency, co-occurrence and semantic similarity of w given NN i .

According to the frequency of w , we can estimate the prob-ability of annotating NN i with w using a multiple Bernoulli model: Here,  X  is a smoothing parameter estimated using cross val-idation.  X  w;NN i = 1 if w occurs in the annotation of NN and zero otherwise. T w denotes the number of training im-ages that contain w in their annotations, and T is the total number of training images.

To further explore the relation between the keyword w and the neighbor image NN i , we consider two other kinds of key-word correlations, i.e. co-occurrence and WordNet semantic similarity. The co-occurrence S co between two keywords is defined as: where w 1 and w 2 are two keywords, tf ( w 2 ) denotes the total frequency of w 2 in training examples, and tf ( w 1 ,w 2 ) is the number of images containing both w 1 and w 2 . Moreover, we employ Lin X  X  similarity measure [3] to estimate the WordNet sem antic similarity S wn between two keywords. According to S co and S wn , the co-occurrence and WordNet semantic similarity between w and the annotation of NN i are sub-sequently computed respectively by looking for the  X  X losest X  keyword in NN i with respect to w :
Following equation (7)(9)(10), the exact form of  X  ( w,NN is expressed as a three-dimensional vector: Therefore, the total dimension of the feature representation  X ( x,y ) is 3 K when we consider K nearest neighbors of x .
In above study, we individually utilize the relevance of each keyword in a candidate annotation without consider-ing the correlations between them. However, as we discussed in Section 1, the keywords associated with an image are de-pendent on each other at the semantic level, and they to-gether constitute the annotation as a whole for that image. To guarantee the semantic coherence of the annotated key-words for an image, we append a constraint term to the scoring function in equation (3). As a result, the objective hypothesis becomes: where  X ( y ) is defined to be the average semantic similarity between each pair of keywords in y : Here, L is the size of y . The weighting parameter  X  tunes the relative importance between co-occurrence and WordNet semantic similarity, and is determined through cross valida-tion.
In this section, we employ the structural SVM to train a robust model for image annotation. Given a set of training SVM learn the weight vector w in equation (12) through the following quadratic programming problem [5]:
Optimization Problem 1. (Structural SVM) subjected to: In the optimization formulation, the constraint condition (15) requires that for each training image, each incorrect Al gorithm 1 Cutting plane algorithm Output: w 1: Initialize W i  X  X  X  for all i = 1 ,...,T 2: repeat 3: for i = 1 ,...,T do 4: H ( y ; w )  X   X ( y ( i ) ,y ) + w T  X ( x ( i ) ,y ) 5: compute  X  y = arg max y  X  X  H ( y ; w ) 6: compute  X  i = max { 0 , max y  X  X  i H ( y ; w ) } 7: if H ( X  y ; w ) &gt;  X  i +  X  then 8: W i  X  X  i  X  X   X  y } 9: optimize (14) over W = 10: end if 11: end for 12: until no W i has changed during iteration. 13: return w Al gorithm 2 Greedy keyword subset selection Output:  X  y 1: Initialize  X  y  X  X  X  2: V ( x,y,y  X  )  X   X ( y,y  X  ) + w T  X ( x,y  X  ) +  X ( y  X  ) 3: for k = 1 ,...,L do 5:  X  y  X   X  y  X  X  t } 6: end for 7: return  X  y an notation is associated with a constraint. Therefore, there are an exponential number of constraints with respect to the total number of unique keywords to be considered. In our study, we employ the cutting plane algorithm [2] (Algo-rithm 1) to solve the optimization problem. The algorithm aims at finding a subset of constraints so that the solution for this subset can also fulfill all constraints at a precision of  X  . It iteratively finds  X  y which generates the most vio-corresponding constraint is violated by more than  X  , the al-gorithm adds  X  y into the working set W i , and then re-solves (14) using the constraints in the updated working sets (line 7-9).

In Algorithm 1, we need to find the most violated con-straint for each iteration via solving the maximization prob-lem in line 5. However, since we add the semantic coherence constraint in the objective hypothesis (see equation (12)), the maximization problem becomes: In this paper, we propose a greedy strategy (Algorithm 2) which is simple but effective in solving the problem. The al-gorithm repeatedly selects the keyword  X  t which can bring the highest gain for the current keyword set  X  y and stops when the size of  X  y reaches L . Although here we use an approx-imate constraint generation algorithm, the learned model still achieves good performance in our experiments.
Given the learned weight vector w , we predict the anno-tation for a new image x by solving equation (12), and the greedy strategy in Algorithm 2 can also be applied for this purpose. T able 1: Performance comparison in terms of P % ,R % and N + between our method and previous published work Our Method 31 36 151 33 33 258 T able 2: Ranked retrieval performance in terms of MAP% with different types of queries
Ou r Method 40 28 47 25
We evaluate our method on two publicly available data sets: Corel 5K and IAPR TC12. The two data sets have been widely used in previous work so we can directly com-pare the experiment results. Each image is represented with the same features described in [4].

The quality of predicted annotations is assessed by retriev-ing test images using the keywords in annotation vocabulary. We use the average precision ( P ) and recall ( R ) over all key-words as two evaluation measures. In addition, the number of keywords with non-zero recall ( N +) is also considered.
The number of neighbors K in equation (5), is a parameter to be determined. We experiment with several values for K and find that the best performance is achieved with K = 100 for Corel 5K and K = 400 for IAPR TC12.
In order to evaluate our method, we compare it with some previous neighbor-based approaches. Besides, we design a simplified version of our method, Str-SVM, which does not enforce the semantic coherence constraint in the objective hypothesis (12). Table 1 shows the annotation results of different algorithms. On Corel 5K, our method outperforms Str-SVM by 3% in P with almost no loss in R and N +. This emphasizes the importance of the requirement for semantic coherence. Compared with JEC method which adopts the same features and visual distance measurement, our method achieves an improvement of 4%, 4% and 12 in terms of P , R and N + respectively. This shows the benefits brought by our proposed learning-based keyword propagation strategy. In addition, although we use a simple technique to find visual neighbors, the performance of our method is still superior to that of the methods involving complicated distance metric learning, such as MSC [6], Lasso [4], and GS [7]. On IAPR TC12, our method outperforms other algorithms as well.
We also examine the performance of our method for the problem of ranked retrieval of images. In our model, we can directly utilize the scoring function (3) to evaluate the com-patibility between an image and a query. Given a textual query q , the confidence of an image I relevant to q can be estimated as: We evaluate the performance in terms of mean average pre-cision (MAP) on Corel 5K. In order to facilitate direct com-parison, we adopt the same experimental setting as that in [1]. There are four types of queries, including single-word, multiple-words,  X  X asy X  and  X  X ifficult X . Table 2 shows that our results improve those of PAMIR in all types of queries, which was found outperforming a number of alternative approach-es in [1]. This demonstrates the retrieval ranking provided by our method is preferable.
In this paper, we have introduced a novel image anno-tation approach, which adapts the neighbor-based methods with a learning-based keyword propagation strategy. We utilize the structural SVM to learn a scoring function for e-valuating the candidate keyword sets, and the annotation of a test image is predicted as a semantically coherent whole. Experiments demonstrate the effectiveness of our approach for image annotation and ranked retrieval. For future s-tudy, we plan to investigate the scalability of our approach and experiment on realistic large-scale web image data sets.
This work is supported by the Natural Science Founda-tion of China (60970047,61103151,61173068,61272240), the Humanity and Social Science Foundation of Ministry of Ed-ucation of China (12YJC630211), the Doctoral Fund of Min-istry of Education of China (20110131110028), the Natural Science Foundation of Shandong Province (ZR2012FM037, BS2012DX012) and the Graduate Independent Innovation Foundation of Shandong University (YZC12084). [1] D. Grangier and S. Bengio. A discriminative [2] T. Joachims, T. Finley, and C.-N. Yu. Cutting-plane [3] D. Lin. An information-theoretic definition of [4] A. Makadia, V. Pavlovic, and S. Kumar. A new baseline [5] I. Tsochantaridis, T. Joachims, T. Hofmann, and [6] C. Wang, S. Yan, L. Zhang, and H.-J. Zhang.
 [7] S. Zhang, J. Huang, Y. Huang, Y. Yu, H. Li, and
