 Information extraction is usually approached as an annota-tion task: Input texts run through several analysis steps of an extraction process in which different semantic concepts are annotated and matched against the slots of templates. We argue that such an approach lacks an efficient control of the input of the analysis steps. In this paper, we hence propose and evaluate a model and a formal approach that consistently put the filtering view in the focus: Before spend-ing annotation effort, filter those portions of the input texts that may contain relevant information for filling a template and discard the others. We model all dependencies between the semantic concepts sought for with a truth maintenance system, which then efficiently infers the portions of text to be annotated in each analysis step. The filtering view en-ables an information extraction system (1) to annotate only relevant portions of input texts and (2) to easily trade its run-time efficiency for its recall. We provide our approach as an open-source extension of Apache UIMA and we show the potential of our approach in a number of experiments. I.2.7 [ Artificial Intelligence ]: Natural Language Pro-cessing X  Text analysis ; F.2.3 [ Analysis of Algorithms and Problem Complexity ]: Tradeoffs among Complexity Measures; I.2.3 [ Artificial Intelligence ]: Deduction and Theorem Proving X  Nonmonotonic reasoning and belief revi-sion ; H.3.3 [ Information Storage and Retrieval ]: Infor-mation Search and Retrieval X  Information filtering ; B.2.1 [ Arithmetic and Logic Structures ]: Design Styles X  Pipeline information extraction; filtering; run-time efficiency; truth maintenance; relevance Figure 1: A sample text that contains information matching the slots of templates of a financial event (left) and of a foundation relation (right). The vari-ous shown semantic concepts can be abstracted into the two types Entity and Relation .
Information extraction aims at identifying various seman-tic concepts in natural language texts, ranging from named entities, numeric expressions, and attributes over references to relations, events, and their anchors [23]. From an ex-traction viewpoint, these concepts can be unified into two abstract types of information, as illustrated in Figure 1: An entity type , whose instances are represented by spans of text, and a relation type , whose instances are expressed in a text, indicating relations between two or more entities. This uni-fication reflects the view of information extraction as filling the (entity) slots of (relation) templates.

Usually, information extraction is approached as an anno-tation task where an input text is annotated in a scheduled process of extraction steps. Given an extraction problem, such an approach first performs certain lexical analyses (e.g. tokenization or part-of-speech tagging) followed by an anno-tation of all relevant entity types in the whole text. Next, it conducts syntactic analyses (e.g. dependency parsing) that are needed to extract relations between the annotated enti-ties as well as to resolve coreferences [1].

In times of big data, the need for efficiency in information extraction is constantly growing because extraction prob-lems of increasing complexity have to be tackled under strict time constraints on increasing numbers of input texts. While existing information extraction systems control the process of creating all output sought for, they often lack an efficient control of the processed input. In particular, much effort is spent for annotating portions of input texts that are not rel-evant for a given extraction problem at all. Assume, for in-stance, that the financial event template in Figure 1 requires an appropriate time expression (which is missing). Then the effort of filling its other slots is wasted. In order to improve the run-time efficiency of extraction, it hence makes sense to discard irrelevant portions of text as early as possible, thereby filtering only the relevant portions.

The idea of filtering for improving efficiency is well-known in information extraction [8] as well as in areas that combine retrieval and extraction techniques such as question answer-ing [9]. Until today, however, many information extraction systems either do not incorporate filtering, or they filter rel-evant portions of text only based on vague statistical models or hand-coded heuristics (cf. Section 2 for details).
In this paper, we propose to consistently address informa-tion extraction as a filtering task , which means to infer, an-notate and filter only relevant portions of text in each extrac-tion step. In case of the text in Figure 1, an approach that performs filtering on the sentence level might e.g. annotate time expressions first. Only the second sentence remains rel-evant then and, thus, needs to be filtered. Depending on the schedule of the extraction steps, that sentence sooner or later turns out not to contain a financial event, and so on. 1
To enable filtering, we model the dependencies between all entity and relation types that are relevant for the extraction problem at hand (Section 3). In addition, we manually spec-ify a degree of filtering for each relation type, i.e., the unit of text (e.g. a sentence or a paragraph) all dependent types of information must lie within. Small units favor extraction ef-ficiency over recall, whereas efficiency can also be optimized without losing effectiveness by specifying degrees that match the actual analyses (e.g. most binary relation extractors an-alyze only entity pairs within sentence boundaries).
As outlined above, the relevance of a portion of text is non-monotonic within a process of extraction steps. To handle non-monotonicity, we efficiently analyze the modeled depen-dencies with a truth maintenance system [35] that formally determines in advance whether possible output annotations of an extraction step within a portion of text may contribute to fulfilling the extraction problem at hand. The actually created output annotations are then used to filter the rele-vant portions of text after each step (Section 4).

We implemented our approach as an open-source software framework on top of Apache UIMA 2 (Section 5). While the efficiency potential of filtering naturally depends on the amount of relevant information in the processed input texts, we evaluate the main parameters of filtering as well as the ef-ficiency of our approach for different extraction problems on text corpora of different languages (Section 6). Our results demonstrate the main contributions of our research: 1. The efficient input control based on truth maintenance 2. The proposed filtering view of information extraction 3. The realized software framework allows researchers to
The example indicates that the schedule of the extraction steps affects efficiency, as studied in related work (see Sec-tion 2). However, scheduling is not the focus of this paper.
Apache UIMA, http://uima.apache.org
The common view of information extraction originates in the Message Understanding Conferences , which focused on the effective extraction of entities and relations from newswire texts in order to fill complex event templates [6]. While current approaches address the same problems of ex-tracting relations [28] and events [27], information extraction is now on the verge of being exploited at web-scale [26] and gradually finds its way into search engines [32]. Still, many approaches fail computational efficiency.

In the last decade, the efficiency of information extrac-tion is getting increasing attention in research and indus-try [13]. Applied techniques range from parallelization [19], specialized search indexes [4], and relation-oriented search queries [2] over the use of cheap algorithms for preprocess-ing [3] and extraction [31] to the efficient processing of input texts, e.g. based on string hashing [18] or filtering [1]. We focus on filtering, but we point out that our approach does not render any of the other approaches impossible.

Filtering is very common in tasks where information needs have to be addressed in real-time, such as question answer-ing, which usually includes so called passage retrieval to filter relevant portions of input texts [25]. The authors of [5] com-pare the benefit of statistical and linguistic knowledge for fil-tering candidate answers, and a fuzzy matching of questions and possibly relevant portions of text is proposed in [9]. As in these examples filtering in question answering usually re-lies on heuristics and vague statistical models, whereas we approach filtering formally in information extraction.
Information extraction has adopted filtering techniques since the early times [8], mostly to improve efficiency. How-ever, [33] observed that classifying the relevant portions of text before extraction can also improve effectiveness. [22] ex-ploits this for template filling, and [46] filters trustworthy candidate relations in unsupervised relation extraction. Fil-tering approaches for efficiency purposes often focus on com-plete texts, e.g. using fast text categorization [40]. In [37], the author complains that techniques for efficiently finding the relevant portions of texts are still restricted to hand-coded heuristics. [29] stresses the importance of filtering for all kinds of extraction problems where relevant information is sparse. There, machine learning is used to predict rele-vant sentences. In contrast, we infer the relevant portions of text from the currently available information. Moreover, a restriction to sentences limits effectiveness [41], whereas we provide a means to specify the sizes of filtered portions, thereby trading efficiency for effectiveness.

In [45], we approach the creation of efficient information extraction systems without losing effectiveness, partly by in-cluding steps to filter relevant portions of text. Optimal effi-ciency then results from an optimal schedule of the filtering steps, as shown in [44]. Schedules are also optimized by Sys-temT [7], which follows the paradigms of declarative infor-mation extraction : user queries define extraction steps with logical constraints, while the system manages the work-flow [12]. SystemT restricts some analyses to scopes of a text based on location conditions in a query [39]. We also use queries and scopes in Section 3, but only to determine rele-vance, which significantly reduces query complexity. While SystemT is limited to rule-based extraction, we do not place constraints on the kinds of analyses to be performed.
The filtering view targets at the classic and dominant form of information extraction system, i.e., a pipeline [11]. A pipe-line sequentially applies a set of extraction algorithms to its input. The main frameworks for natural language process-ing, UIMA [15] and GATE [10], focus on pipelines. In [43], we present a system that builds filtering pipelines automat-ically for given extraction problems. Without an input con-trol, however, the system cannot tackle different problems at the same time. Because of filtering, we do not consider pipe-lines that employ more than one algorithm to annotate the same type of information [47], though filtering could be de-layed in such cases. Accordingly, iterative pipelines [17] and probabilistic pipelines [21] are not covered in this paper.
Our approach profits from dividing an extraction process into small steps. This contradicts the idea of joint informa-tion extraction [34]. Still, joint extraction algorithms can be used in pipelines, as is e.g. common in entity recognition [16]. Particularly, our approach benefits complex extraction prob-lems, such as those of the BioNLP task [24]. Nevertheless, it applies to all extraction processes with dependencies be-tween the relevant types of information. To the best of our knowledge, we are the first to consider these dependencies for filtering. Moreover, since our approach performs filtering during the extraction process, it can be integrated with all existing approaches where filtering precedes extraction.
In this section, we propose a filtering view of information extraction, which makes it possible to address every extrac-tion problem as a filtering task , i.e, to analyze and filter only the relevant portions of each input text.
In order to infer the relevance of a portion of text u , we need a clear specification of the output sought for in an ex-traction problem. As motivated in Section 1, we distinguish two types of information: An entity type , denoted as E , can be regarded as atomic in that its instances are used to fill slots of templates. In contrast, a relation type R expresses a conjunction of types, i.e., a template. Extraction problems may target at different templates concurrently. We represent such a disjunction of conjunctions in the form of a query : Query A query q specifies the relevant types of information in an extraction problem. Its abstract syntax is defined by the following grammar:
A portion of text u is relevant at some point of an extrac-tion process, if it may still contain all information needed to fulfill the associated query. As an example, consider the query q 1 that addresses a simple binary relation type:
Before extracting relations, only those portions of text are relevant that contain instances of both entity types. If orga-nization entities are e.g. annotated first, then time entities need to be annotated only in portions of texts with organi-zation entities, and vice versa. Hence, we can filter the rele-vant portions of a text based on the output of an extraction step (and discard the others). Figure 2 shows the portions of a sample text to be filtered for q 1 and opposes the slot-filling view to the filtering view of information extraction. Figure 2: The slot-filling view of information extrac-tion (bottom) and the proposed filtering view (top). For the query q 1 = Founded(Organization, Time) , the relevant portion of text is p 2 on the paragraph level and s 2 on the sentence level, respectively.

In general, it is reasonable to filter different portions of text for the different relation types in a query. The following two queries illustrate this. While the former targets at arbi-trary forecasts (i.e., statements about the future) with time information, the latter asks for financial relations, which re-late such forecasts to money entities:
Assume that forecasts are extracted from single sentences while financial relations may span a whole paragraph. Then, a sentence without time entities is irrelevant for q 2 , but since it might contain a money entity, its enclosing paragraph re-mains relevant for q 3 as a whole. In case of disjunctive que-ries, relevance is even independent for each conjunction:
For instance, a portion of text without financial relations can, of course, still contain foundation relations. Generally, every relation type in a query may entail a different set of relevant portions of text at each extraction step. For a given input text, we call such a set a scope of that text: Scope A scope S R = ( u 1 ,...,u n ) is the ordered set of n  X  0 portions of a text where instances of a relation type R ( r 1 ,...,r k ) may occur.
The concept of scopes reveals that a query q alone does not suffice to perform filtering: While q enables us to automati-cally infer the relevance of a portion of text u , it does not de-fine the size of the portions of text to be filtered, when given the output of an extraction step. We hence manually assign a degree of filtering to each relation type in q that binds in-stances of the relation type to units of the text: Degree of Filtering A degree of filtering U is a type of lexical or syntactic text unit that defines the size of a portion of text, all information of an instance of a relation type R ( r 1 ,...,r k ) must lie within, denoted as U [ R ( r 1
The definition accounts for the fact that all extraction al-gorithms operate on a certain text unit level. E.g., most binary relation extractors process one sentence at a time, taking as input only candidate entity pairs within that sen-tence. In contrast, coreference resolution algorithms rather analyze paragraphs or even the entire text . Figure 3: The dependency graph of the scoped query q = q  X  1  X  q  X  3 (left) and a mapping from the degrees of
Degrees of filtering provide a means to influence the trade-off between the efficiency and the effectiveness of extraction: small degrees allow for filtering small portions of text, which positively affects run-time efficiency. Larger degrees provide less room for filtering, but they allow for higher recall if rela-tions exceed the boundaries of smaller text units. When the degrees match the text unit levels of the employed extrac-tion algorithms, efficiency will be improved without losing recall. 3 Hence, the slot-filling view and filtering view of in-formation extraction can be integrated without loss. We call a query with defined degrees of filtering a scoped query : Scoped Query A scoped query q  X  is a query q that has assigned a degree of filtering to each contained relation type R ( r 1 ,...,r k ).
Within an extraction process, the degrees of filtering in a scoped query q  X  are associated to respective scopes of the current input text. These scopes may depend on each other, as the following scoped version of the query q 4 shows:
According to this query, paragraphs without time entities will never span sentences with forecasts and, thus, will not yield financial relations. Similarly, if a paragraph contains no money entities, then there is no need for extracting fore-casts from the paragraph X  X  sentences. So, filtering one of the scopes of Forecast and Financial affects the other one. As in this example, a query implies hierarchical dependen-cies between the relevant types of information that can be represented as a dependency graph .
 Dependency Graph The dependency graph of a scoped query q  X  = q 1  X  ...  X  q m is a set of directed trees with one tree for each conjunction q i  X  X  q 1 ,...,q m } . An inner node of q corresponds to a degree of filtering and a leaf to an entity type E or a relation type R . An edge from an inner node to a leaf means that the respective degree is assigned to the respective type, and edges between inner nodes imply that the associated scopes are dependent. The degree of filtering of q i itself defines the root of the tree.
While there is no clear connection between degrees of filter-ing and precision , a higher precision seems easier to achieve if extraction is performed only on small portions of text.
For complex relation types like coreference , the degree of filtering of an inner relation type possibly exceeds the degree of filtering of an outer relation type. In such a case, filtering with respect to the outer relation type affects the entities to be resolved, but not the entities to be used for resolution.
Figure 3 visualizes the dependency graph of q  X  4 and the associated scopes of a sample text. Such a graph can be ex-ploited to automatically maintain relevant portions of text.
We now show how to automatically determine and filter scopes of an input text in each step of an extraction process. This enables the employed extraction algorithms to analyze only portions of text their output is relevant for.
An extraction process can be regarded as non-monotonic in that knowledge about input texts (i.e., annotated entities and relations) changes in each step. In the beginning, usu-ally no knowledge is given and, hence, each portion u of an input text must be assumed relevant. If u lacks any required knowledge in some step, it becomes irrelevant and can be ex-cluded from further analyses. In artificial intelligence, such non-monotonicity is well-studied and it is tackled with an assumption-based truth maintenance system (ATMS) , which justifies and retracts assumptions expressed as propositional formulas based on a set of believed assumptions [35].
We adapt the ATMS concept to filter the scopes of input texts. For this purpose, we interpret all queries, entity types and relation types as propositional symbols. Given a scoped query q  X  , we then model relevant portions of text as follows. For each degree of filtering U that is a root in the dependency graph of q  X  , the relevance q  X  ( u ) of each portion of text u in the scope associated with U corresponds to the truth value of the relation type R ( r 1 ,...,r k ), to which U is assigned: For all child nodes r ( u ) i of a root node U of the form U [ R i ( r i 1 ,...,r il )], we additionally model the relevance of a portion of text u 0 of the scope associated to U i as:
This modeling step is repeated recursively until each child node r ( u 0 ) ij in a new formula  X  ( u 0 ) represents either an entity type or a relation type. Now, the set of assumptions about an input text is given by the set of all formulas  X   X  ( u ) of that text. In case of the example in Figure 3, four assumptions are initially believed for the paragraph p 2 : Pseudocode 1 Determine Scope (Output types O ) 1: for each Output type O in O do 3: return the whole input text 4: Scopes S 5: for each Output type O in O do 7: S .addAll( S O ) 8: Scope S  X  9: for each Scope S in S do 10: for each Portion of text u in S do 13: return S  X 
However, the relevance of a portion of text u at a partic-ular extraction step depends on the set of currently believed assumptions. This set follows from the output of all extrac-tion algorithms applied so far. Instead of maintaining all assumptions, we filter the scopes of an input text according to the output of the last algorithm and maintain assump-tions for the scopes only. For instance, if time entities are found only in the sentences s 2 and s 4 in Figure 3, then  X  becomes false, whereas the other assumptions of p 2 are up-dated as follows:
An algorithm employed in an extraction step then has to analyze all current scopes, its output types are relevant for: Output Type An output type O of an extraction algorithm is an entity or relation type annotated by that algorithm.
Below, we describe how to determine and filter the scopes of an input text based on the set of output types O of an ex-traction algorithm. Initially, all scopes span the whole text. These scopes must be generated by segmentation algorithms , i.e., extraction algorithms whose output types include a de-gree of filtering (e.g. a sentence splitter). However, we do not explicitly distinguish algorithm types but simply create scopes when the according output is annotated.
Given a scoped query q  X  , an employed extraction algo-rithm must be applied to each portion of text u , for which an assumption  X  ( u ) or  X  ( u ) exists that depends on an out-put type of the algorithm. E.g., the assumptions  X  ( p 2 ) and  X  ( s 4 ) imply that an algorithm with output type Forecast needs to analyze the sentences s 2 and s 4 . In general, an al-gorithm with a set of output types O must be applied to the union S  X  of the set S of all scopes S that meet one of two conditions: (1) S is associated to a degree of filtering that is assigned to any O  X  O in q  X  . (2) S is associated to a degree of filtering assigned to an entity or relation type in q  X  needs instances of any O  X  O as input. E.g., part-of-speech tags are not specified in q  X  4 from Section 3, but they might be necessary for extracting the type Organization .
The determination of S  X  based on a set of output types O is sketched in Pseudocode 1. Lines 1 X 3 identify segmentation algorithms, whereas the scopes to be unified are collected in Pseudocode 2 Filter (Scopes S , Output types O ) 1: for each Scope S in S do 3: for each Portion of text u in S do 5: S .remove( u ) 8: for each Portion of text u in S 0 do lines 4 X 7. The union S  X  is then obtained by taking all of the scopes X  non-overlapping portions of text and by merging overlapping portions (lines 8 X 12). 5
For each analyzed portion of text u , the believed assump-tions  X  ( u ) and  X  ( u ) containing a type O  X  O of the applied ex-traction algorithm can be updated based on the algorithm X  X  output. This in turn leads to a recursive update of assump-tions that contain the consequent of some  X  ( u ) . In case all forecasts and their anchors have been detected in the sample text in Figure 3,  X  ( s 2 ) and  X  ( s 4 ) turn out to be true. Hence, q 2 is true and  X 
Thus, the output of an extraction algorithm is not only used to filter the scopes in the above-mentioned set S but also their dependent scopes. The set of dependent scopes of a scope S consists of the scope associated to the root of the node of S in the dependency graph of q  X  as well as of all scopes of the root X  X  descendant nodes. This, of course, includes all ancestor scopes of S .

Pseudocode 2 shows how to perform filtering. For each scope S  X  S , a portion of text u is maintained only if it con-tains an instance of one of the output types O 0  X  O that are relevant for S (lines 1 X 5). Afterwards, lines 6 X 9 remove all portions of text from the root scope S 0 of S that do not inter-sect with any portion of text in S . 6 Accordingly, only those portions of text in the set of descendant scopes S 0 of S retained that intersect with a portion in S 0 (lines 10 X 13).
We realized all concepts that are needed to address extrac-tion problems as filtering tasks in an efficient Java software framework as an extension of Apache UIMA. The source code of this filtering framework has been designed with a focus on easy integration and minimal additional effort. It can be freely accessed at http://www.arguana.com , together with usage instructions and some sample applications.
For a concise presentation, Pseudocode 1 contains nested loops. Actually, the unification can be realized in time linear in the number of the portions of text of all scopes by stepwise comparing portions according to their ordering in the text.
Similar to a unification, an intersection can be achieved in time linear in the number of text units of all scopes.
Apache UIMA is a software framework that allows devel-opers to easily compose natural language processing applica-tions while managing the applications X  control flow and data flow [15]. For this purpose, algorithms are accompanied by descriptor files with metadata.

Throughout this section, we provide a simplified concep-tual view of Apache UIMA. Its architecture is defined by the white classes and their associations on the left of the UML class diagram [30] in Figure 4. Applications input texts and analyze them with aggregate analysis engines (say, informa-tion extraction systems). An aggregate analysis engine con-trols a composition of primitive analysis engines (say, extrac-tion algorithms), which access common analysis structures to process and to create annotations . Each annotation spec-ifies a span of a text, thereby representing an entity. Besides, it may have features that store values or references to other annotations. Hence, also relations can be realized as annota-tions. In an application, subtypes of the Apache UIMA type Annotation specify the concrete types of information. They are defined in an application-specific type system.
The implemented extension of the Apache UIMA frame-work by the filtering framework is illustrated on the right of Figure 4. It consists of four main classes: Filtering Analysis Engine Extraction algorithms that analyze and filter only relevant portions of their input texts are represented by filtering analysis engines , which inherit from primitive analysis engines and, hence, can be composed in an aggregate analysis engine. Prior to analysis, a filtering analysis engine automatically determines the scope its out-put annotation types and features O are relevant for. After analysis, it triggers the generation or the filtering of scopes based on O and its created output. 7 Scoped Query Each scoped query to be addressed by an aggregate analysis engine is defined from an application and then automatically parsed to derive the query X  X  dependency graph (cf. Section 3.3) as well as the degrees of filtering of all associated scopes.
 Scope We have realized a scope as a set of generic anno-tations in order not to require explicit scope types. In an application, a scope of an input text may have a text unit type assigned, e.g. Sentence . According to the derived de-pendency graph, a scope can have at most one root scope and an arbitrary number of descendant scopes.
 Scope TMS To avoid modifications of the Apache UIMA framework, we maintain all scopes using a blackboard archi-tecture [20]. 8 In particular, filtering analysis engines deter-mine and filter scopes via a globally accessible truth main-tenance system. This scope TMS maintains the dependency graph of each scoped query, a mapping from the degrees of filtering to the respective scopes, and a mapping from the entity and relation types in a scoped query to their scopes. Dependencies between the output types of analysis engines
In Apache UIMA, the set O can be inferred from the result specification of an analysis engine, which in turn is automat-ically derived from the analysis engine X  X  descriptor file. Future versions of Apache UIMA could integrate the scope TMS in the common analysis structure to allow for an opti-mized integration of extraction and truth maintenance. Figure 4: Architecture of the filtering framework ex-tension of Apache UIMA. A filtering analysis engine is a primitive analysis engine, which analyzes only a scope of a text determined by the scope TMS. are derived from their metadata. Given the output types of an analysis engine, the scope TMS determines scopes ac-cording to Pseudocode 1. If a type O is a degree of filtering, the scope TMS generates each associated scope S by adding all annotations of type O to S . Otherwise, it filters all con-cerned scopes as shown in Pseudocode 2. 9
Maintaining the scopes of an input text imposes compu-tational costs linear in the number of portions of text of the scopes. In Section 6, we see that these additional costs only marginally affect the efficiency of an application.
We now present experimental results on the efficiency and effectiveness of filtering. A comprehensive evaluation of fil-tering in information extraction seems infeasible since its po-tential depends on the amount of information in the given input texts that is relevant for the given extraction problem. We hence provide an appropriate proof-of-concept instead, based on the example queries from Section 3. Our goal is to reveal the impact of the main parameters intrinsic to filtering (i.e., the complexity of a query and the degree of filtering), as well as to demonstrate the efficiency of our approach. All experiments were conducted on an 2 GHz Intel Core 2 Duo MacBook with 4 GB memory. The Java source code of this evaluation is attached to the filtering framework given at http://www.arguana.com .
 Text Corpora In the evaluation, we analyze filtering on two text corpora of different languages, which have been used for information extraction purposes in the last years: First, the complete English corpus of the CoNLL X 03 Shared Task [36] with 1393 news stories. And second, the complete German Revenue corpus that we introduced in [42] and that consists of 1128 online business news articles.
Notice that the Scope TMS never removes any annotations from the UIMA indexes, but it only deletes references of the annotations in order to exclude them from further analyses. positives (P) as well as the resulting precision of pipeline  X  1 for the query q 1 = Founded(Organization, Time) Algorithms Our experiments refer to the example queries from Section 3, for which we employed eleven extraction al-gorithms that can be parameterized to work for both English and German. All algorithms have more or less comparable run-time that scales linear with the text length. 10
Concretely, we relied on self-implemented rule-based al-gorithms for tokenization ( tok ), paragraph splitting ( par and sentence splitting ( sen ), while we used the TreeTag-ger [38] wrapper tt4j 11 for part-of-speech tagging ( pos chunking ( chu ). Organization entities ( org ) were extracted with Stanford NER [16] using the model from [14] for Ger-man texts. We employed the regex-based recognizers for time entities ( tim ) and money entities ( mon ) as well as the SVM-based forecast event detector ( for ) presented in [45]. Finally, we developed lexicon-based relation extractors for Founded ( fou ) and Financial ( fin ) that qualify for arbi-trary degrees of filtering. These relation extractors look for indicator words of the relation type in question and relate a pair of entities if it is close enough to such a word. Approaches Each information extraction system that we evaluate is a pipeline  X , which sequentially applies a subset of the eleven extraction algorithms to its input. The con-crete pipelines are given below; some perform filtering, some do not. We provide no comparison to existing filtering ap-proaches (cf. Section 2), as these approaches do not compete with our approach, but rather can be integrated with it. Measures We determined the filter ratio of each pipeline  X , which we define as the quotient of the number of characters processed by  X  and the number of characters processed by a respective non-filtering pipeline. Similarly, we measured the run-time of each  X  in seconds (averaged over ten runs) to compute the time ratio as the quotient of the run-time of  X  and the run-time of a non-filtering pipeline.

In terms of effectiveness, we counted the positives , i.e., the number of extracted relations, in order to roughly compare the recall of pipelines. An exact evaluation of recall is hardly feasible on the given corpora for lack of appropriate manual event and relation annotations. In Section 6.2, we show the precision of extracting foundation relations under different degrees of filtering. To this end, we decided for each found positive manually whether it is true or false. In particular, a relation was considered a true positive if and only if its
We explicitly avoided to include computationally expensive algorithms (e.g. a dependency parser). While such algo-rithms significantly increase the efficiency potential of filter-ing, they would make it difficult to distinguish between the effects of filtering and of the order of algorithm application. tt4j wrapper, http://code.google.com/p/tt4j Figure 5: Interpolated curves of the filter ratios of the algorithms in pipeline  X  1 under three degrees of filtering for query q 1 = Founded(Organization, Time) on the CoNLL X 03 corpus. The more computation-ally expensive the algorithms later in a pipeline are, the higher the efficiency impact of filtering is. anchor was brought into relation with the correct time entity while spanning the correct organization entity. 12
In order to analyze the effects of filtering on the efficiency and the effectiveness of extraction, we considered the query q = Founded(Organization, Time) from Section 3.1 under different degrees of filtering on both corpora. In particular, we separately assigned the degrees Paragraph and Sentence to q 1 . Then, we ran the pipeline to compare filtering for the according scoped queries to the application of  X  1 without filtering.

Figure 5 visualizes the filter ratios of all algorithms in  X  on the CoNLL X 03 corpus depending on the degree of filter-ing. The first algorithm that discards significant portions of irrelevant text is tim , which filters 73.6% of its input on the paragraph level and 28.9% on the sentence level. These percentages further decrease after org to 60.3% and 10.8%, respectively. While the efficiency and effectiveness impact of filtering depends on the employed algorithms, the overall values of  X  1 on both corpora are listed in Table 1.
In case of the CoNLL X 03 corpus, 81.5% of the 12.70 million characters that are processed without filtering are analyzed when performing filtering on the paragraph level. Thereby, about a third of the run-time of 75.4 seconds is saved. For
The evaluation of precision is only fairly representative, as in practice many extractors do not take into account cross-sentence or even cross-paragraph relations at all. In such cases, precision remains unaffected by the degree of filtering. Table 2: The filter ratio, the time ratio, and the number of positives (in terms of extracted relations) of pipeline  X  2 for q 2 = Forecast(Anchor, Time) under different degrees of filtering on the Revenue corpus. both these degrees of filtering, the same eight relations were extracted with a precision of 87.5%. This implies that no re-lation was found, which exceeds paragraph boundaries. Fil-tering on the sentence level lowered the filter ratio to 40.6% and the time ratio to 32.9%, but also reduced the number of positives to 5. The fact that all found in-sentence relations are true positives might be coincidence, but it also indicates a tendency to achieve better precision, when the size of the filtered portions of texts remains small.

On the Revenue corpus, the filter and time ratios are hig-her than on the CoNLL X 03 corpus due to a larger amount of time entities (which are extracted first in  X  1 ). Still, under the degree Sentence ,  X  1 needs only 47.5% of the time of a non-filtering pipeline. Hence, the benefit of filtering is obvi-ous even for simple binary relation types like Founded and even though we did not employ expensive algorithms like a dependency parser. Moreover, the numbers of positives in Table 1 (52 in total, 38 within paragraphs, 19 within sen-tences) suggest that degrees of filtering provide an intuitive means to adjust the trade-off between a pipeline X  X  efficiency and its recall, whereas precision remains rather stable.
As discussed in Section 3.1, filtering can also be exploited to optimize the efficiency of a pipeline without compromising effectiveness. To demonstrate this, we assigned the same degrees of filtering as above to q 2 = Forecast(Anchor, Time) while knowing that the forecast event detector for operates only on the sentence level. For all three degrees, we then addressed q 2 on the Revenue corpus with the pipeline  X  2
Table 2 underlines the implied efficiency optimization po-tential of filtering: Irrespective of the degree of filtering, the same 3622 sentences were recognized as forecast relations. At the same time, filtering reduces the fraction of analyzed characters to less than two third (64.8%), though more than every tenth sentence is classified relevant (3 622 of 33 364 sentences in the Revenue corpus). Such filtering forms the basis of our and other pipeline scheduling approaches [39, 44, 45]. Here, it improves the run-time of  X  2 by almost factor 2, as expressed by a time ratio of 53.9%. Figure 6: Interpolated curve of the filter ratios of the eleven algorithms in pipeline  X  4 for the disjunctive scoped query q  X  4 = q  X  1  X  q  X  3 on the Revenue corpus.
In a last experiment, we analyzed the benefit and compu-tational effort of filtering under increasing complexity of the addressed query on the Revenue corpus. For this purpose, we considered the following scoped versions of the queries q , q 3 , and q 4 from Section 3:
We applied pipeline  X  1 for q  X  1 again and we used the fol-lowing pipelines for q  X  3 and q  X  4 , respectively:
Table 3 lists the results. While the time ratios slightly increase from q  X  1 to q  X  4 , they reveal that all three pipelines took less than half of the run-time of their respective non-filtering pipelines. Not surprisingly, the longest pipeline  X  processed the largest number of characters (24.40 millions). However, the filter ratio of  X  4 (57.9%) seems more like a  X  X eighted average X  of the filter ratios of  X  1 and  X  3 .
The reason behind can be inferred from Figure 6, which illustrates the filter ratios of all algorithms in  X  4 . The values of the interpolated curve do not decline monotonously along the pipeline, but they depend on what portions of the input texts are relevant for which conjunction in the disjunctive scoped query q  X  4 . E.g., the algorithm for analyzed only text units relevant for q  X  3 , i.e., sentences that contain a time entity in paragraphs that contain a money entity. In contrast, chu processed the 42% of the characters that belong to sentences with time entities. The same holds for pos , although the output of pos was needed for both q  X  3 and q  X  4 .
Table 3 also shows the efficiency of our approach by com-paring the analysis times of each of the three pipelines (i.e., the time taken by their employed extraction algorithms) to the time required by the filtering framework. Only 1.0% (0.7 of 74.9 seconds) was spent for the generation, determination, and filtering of the scopes of q  X  1 . This percentage grows only marginally under increasing query complexity, as the val-ues for q  X  3 (1.1%) and q  X  4 (1.2%) suggest. We hence conclude that the filtering view of information extraction can be oper-ationalized efficiently, though our implementation certainly leaves much room for optimization.
The need for run-time efficiency in information extraction is pressing in times of big data where extraction problems are tackled at large scale. To improve extraction efficiency, we propose to view and to consistently address information extraction as the task to filter the relevant portions of input texts. For this purpose, we introduce an input control that maintains the dependencies between all relevant types of entities and relations in order to analyze and filter only rel-evant portions of text in each step of an extraction process. Thereby, the efficiency of an extraction process can be opti-mized without losing effectiveness, and we can easily trade efficiency for effectiveness, in particular for recall. Still, other approaches to improve efficiency remain applicable.
We have implemented and evaluated the filtering view in an easy-to-use and open-source software framework on top of the Apache UIMA framework. While the exact effi-ciency potential of filtering naturally depends on the amount of information in the given input texts that is relevant for the extraction problem at hand, the results emphasize that our proposed approach significantly improves extraction ef-ficiency without requiring notable additional time. This work has been partly funded by the research project ArguAna of the German Federal Ministry of Education and Research (BMBF) under contract number 01IS11016A. [1] E. Agichtein. Scaling Information Extraction to Large [2] E. Agichtein and L. Gravano. Querying Text [3] R. Al-Rfou X  and S. Skiena. SpeedRead: A Fast Named [4] M. J. Cafarella, D. Downey, S. Soderland, and [5] C. Cardie, V. Ng, D. Pierce, and C. Buckley.
 [6] N. Chinchor, D. D. Lewis, and L. Hirschman.
 [7] L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan, [8] J. Cowie and W. Lehnert. Information Extraction. [9] H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua. [10] H. Cunningham, D. Maynard, K. Bontcheva, [11] A. Das Sarma, A. Jain, and P. Bohannon. Building a [12] A. Doan, J. F. Naughton, R. Ramakrishnan, A. Baid, [13] A. Doan, R. Ramakrishnan, and S. Vaithyanathan. [14] M. Faruqui and S. Pad  X o. Training and Evaluating a [15] D. Ferrucci and A. Lally. UIMA: An Architectural [16] J. R. Finkel, T. Grenager, and C. D. Manning. [17] J. R. Finkel, C. D. Manning, and A. Y. Ng. Solving [18] G. Forman and E. Kirshenbaum. Extremely Fast Text [19] D. Gruhl, L. Chavet, D. Gibson, J. Meyer, [20] B. Hayes-Roth. A Blackboard Architecture for [21] K. Hollingshead and B. Roark. Pipeline Iteration. In [22] L. Jean-Louis, R. Besan  X con, and O. Ferret. Text [23] D. Jurafsky and J. H. Martin. Speech and Language [24] J.-D. Kim, S. Pyysalo, T. Ohta, R. Bossy, N. Nguyen, [25] E. Krikon, D. Carmel, and O. Kurland. Predicting the [26] G. Limaye, S. Sarawagi, and S. Chakrabarti.
 [27] W. Lu and D. Roth. Automatic Event Extraction with [28] Mausam, M. Schmitz, R. Bart, S. Soderland, and [29] C. Nedellec, M. O. A. Vetah, and P. Bessi`eres. [30] OMG. Unified Modeling Language (OMG UML) [31] P. Pantel, D. Ravichandran, and E. Hovy. Towards [32] M. Pasca. Web-based Open-Domain Information [33] S. Patwardhan and E. Riloff. Effective Information [34] H. Poon and P. Domingos. Joint Inference in [35] S. J. Russell and P. Norvig. Artificial Intelligence: A [36] E. F. T. K. Sang and F. D. Meulder. Introduction to [37] S. Sarawagi. Information Extraction. Foundations and [38] H. Schmid. Improvements in Part-of-Speech Tagging [39] W. Shen, A. Doan, J. F. Naughton, and [40] B. Stein, S. M. zu Eissen, G. Gr  X  afe, and F. Wissbrock. [41] M. Stevenson. Fact Distribution in Information [42] H. Wachsmuth, P. Prettenhofer, and B. Stein.
 [43] H. Wachsmuth, M. Rose, and G. Engels. Automatic [44] H. Wachsmuth and B. Stein. Optimal Scheduling of [45] H. Wachsmuth, B. Stein, and G. Engels. Constructing [46] W. Wang, R. Besan  X con, O. Ferret, and B. Grau. [47] C. Whitelaw, A. Kehlenbeck, N. Petrovic, and
