 H.3.3 [ Information Storage and retrieval ]: Information Search and Retrieval X  Relevance Feedback Algorithms, Experimentation, Measurement BlindRelevanceFeedback,AutomaticQueryExpansion,Com-parison
QueryexpansionbasedonBlindRelevanceFeedback(BRF) has been demonstrated to be an effective technique for im-proving retrieval results. There are two types of BRF-based query expansion. BRF Type 1 (BRFT1) is the original ver-sion of BRF, where query expansion is performed on the BRF information extracted from top N documents selected from an initial search on the same collection that the tar-get documents are in [1]. This collection is called  X  X arget collection X  in this paper. BRF Type 2 (BRFT2) has been explored as an alternative to BRFT1. The query expansion isperformedbasedontheBRFinformationofthetopNdoc-uments selected from the initial search on a DIFFERENT collection. Such a collection is called  X  X xpansion collection X  in this paper. The expanded query is then used to search on the target collection to find the relevant documents.
The effectiveness of BRF depends on two key factors: 1) the documents selected from the initial search for BRF shouldcontainreasonablenumberoftopicallyrelevantdocu-ments to the query; and 2) those selected documents should sharethesimilargenrewiththetargetrelevantdocumentsso that there is high chance that the important content terms used in these two sets of documents are the same[2]. Both BRFT1 and BRFT2 may encounter situations that at least one of the two conditions cannot be satisfied. For example, there are not enough truly relevant documents in the target collection for many topics in Robust track of TREC evalua-tion, which makes it difficult to utilize BRFT1 based query expansion techniques to improve the search results.
However, with the amount of electronic resources avail-able, it is often possible that both BRFT1 and BRFT2 can be performed as means to improve retrieval results. How-ever, there has been no study to examine the relationships between the two BRF approaches, from which we can an-swer questions like: when both BRF approaches are feasible to perform, should BRFT1 bepreferred over BRFT2, or the other way round? Does it make sense to combining both BRF approaches? Will they select similar or different set of terms for expansion? Any relationship among the terms obtained from these two approaches?
In this paper, we will present our initial study about the relationship between BRFT1 and BRFT2 in the context of retrieving news articles on TREC evaluation platform. The research questions that we want to examine are: 1. IsthereaperformancedifferencebetweenusingBRFT1 2. What are the relationships between the query expan-3. Does it make sense to combine BRFT1 and BRFT2?
AQUAINT corpus was used for this study. It contains documents from three news agencies: 239,576 articles from the Associate Press (APW),314,452 articles from NewYork Times(NYT),and479,433articlesfromXinhuaNewsAgency (XIE). For our experiment, we separated AQUAINT into a target collection, including all documents from APW and XIE, and an expansion collection, which contains all arti-cles from NYT. Because there are relevant documents in both collections for almost all topics, and all the documents are news articles, the experiment setting can be seen as fa-vorable for both BRFT1 and BRFT2.
 Thesearchtopicswere50HARD2005topics. Weadopted Indri2.0asourretrievalsystem 1 . WemodifiedIndri X  X build-in BRF module so that it can perform both BRFT1 and BRFT2usingthesamedefaultBRFmodel. Theparameters for BRF were defined as selecting top 20 terms from top 20 documents. The relative weights were set as 0.9 for the original query terms and 0.1 for the expanded terms in the case of BRFT1 (i.e. BRFT1@0.9), and 0.8 for the original query terms and 0.2 for the expanded terms in the case of BRFT2 (BRFT2@0.8). These parameters were obtained by testing on those 50 HARD05 topics. We use the same set of parameters for the run that combines BRFT1 and BRFT2. http://www.lemurproject.org/indri/ Figure 1: The stages of performing retrievals with BRFT1 and BRFT2 in our studies Four sets of retrieval results were collected in our studies. The first one (marked as [I] in both Figures 1 and 2) is the baseline run that employed no relevance feedback at all. The run corresponding to BRFT1 is marked as [II], and that toBRFT2 is marked as [III]. The one marked with [IV] is the run that contains query expansion based on BRFT2 first, then followed by query expansion based on BRFT1. Run [IV] is an initial exploration to the question whether it makes sense to combine BRFT1 and BRFT2.
 Figure 2: The Mean Average Precision (MAP) of the runs. The color difference on bars indicates sig-nificant difference between the two results.

AsshowninFigure2,bothBRFapproaches([II]and[III]) achieved significant improvement over non-expansion base-line([I])(usingthepairedtTest, andbothp=0). However, there was no significant difference between the two BRF re-sults. In fact, further analysis of their performance at indi-vidual topics demonstrated that these two BRF approaches achieved comparable results from individual topics down to individual returned relevant documents: Table 1: The results of the four retrieval runs. Ret-Rel-Num means the total number of returned rele-vant documents across 50 topics.

Our results show some potential for combining BRFT1 and BRFT2. The run [IV], which applied BRFT1 after per-forming BRFT2 on the expanded collection, achieved sig-nificantimprovement overthe two runs using either BRFT1 or BRFT2 alone (paired t-test and both p = 0.01). The improvements is also reflected by the higher number of re-turned relevant documents in [IV] (see Table 1). However, no definite claim can be made before further experiments on 1) the difference between multiple iteration of BRFT1 or BRFT2 and the combination of these two, and 2) different combination settings.
By examining query expansion based on BRF using the target collection only (BRFT1) or that using the expan-sion collection first then the target collection (BRFT2), we demonstrated that the two BRF approaches are similar in improving retrieval effectiveness. They have succeeded and failed on the similar topics, and have retrieved similar set of relevant documents even though the terms that they picked up are different in majority. Future work includes studies on the effectiveness of combining these two approaches. ThisworkhasbeensupportedinpartbyDARPAcontract HR0011-06-2-0001. [1] D. Evans and R. Lefferts. Design and evaluation of the [2] D.K. Harman. Relevance feedback revisited. In
