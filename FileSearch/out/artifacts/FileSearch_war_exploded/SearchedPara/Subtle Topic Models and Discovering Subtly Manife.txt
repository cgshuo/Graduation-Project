 Mrinal Kanti Das  X  mrinal@csa.iisc.ernet.in Suparna Bhattacharya bsuparna@in.ibm.com IBM Research-India Chiranjib Bhattacharyya  X  chiru@csa.iisc.ernet.in K. Gopinath  X  gopi@csa.iisc.ernet.in  X  Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India Hierarchical Dirichlet process (HDP) ( Teh et al. , 2007 ) is one of the most widely used topic models. Re-call that, HDP places a Dirichlet process (DP) prior over potentially infinite number of topics at corpus level. Subsequently, it uses a DP prior over the top-ics for each document, and each document level DP is distributed as the corpus level DP. Though HDP is extremely successful in discovering topics in gen-eral, it fails to discover topics which occur in very few documents, often referred as rare topics. This inabil-ity stems from the fact that HDP inherently assumes that a frequent topic will on average occur frequently within each document, leading to a positive correla-tion between proportion of a topic in an article and prevalence of a topic in the entire corpus.
 This important problem has partially been addressed in ( Williamson et al. , 2010 ). By using Indian Buf-fet process (IBP), ( Williamson et al. , 2010 ) defined a compound DP namely ICD to decorrelate document wise prevalence and corpus wide proportion. ICD was applied on focused topic models (FTM) to detect rare topics which are prominently placed in very few doc-uments.
 Consider the corpus, proceedings of NIPS, 2005 1 . Be-cause of limited number of papers on supervised classi-fication, an HDP based approach fails to identify top-ics related to supervised classification but FTM de-tects this easily(see supplementary material for more details).
 However, there are some topics which are not only rare across the documents but also rarely appear within a document. Under these situations FTM will fail to discover them. A case in point is, a topic re-lated to neuromorphic engineering about cochlear mod-elling . The topic has been discussed only in one paper( Wen &amp; Boahen , 2005 ). In addition to that, the main theme (cochlear) is rarely explicitly mentioned (5% of sentences) in that paper. Therefore, the topic is assigned an extremely low probability making it ex-tremely difficult for even FTM to detect. This phe-nomena is not specific only to scientific corpora, but is also observed in other text corpora too. We studied the speeches of Barack Obama from July 27, 2004 till October 30, 2012 2 , a span of eight years. We observe that, in this corpus there are two speeches on Carnegie Mellon University (CMU). Those speeches were given when he visited CMU on 2 June, 2010 and 24 June 2011. It is a difficult task for FTM to detect this topic as  X  X arnegie Mellon X  is contained not only in two documents(which is rare), but also present in less than 10% of sentences in those two documents. These examples show that, discovering topics which rarely occur in individual documents still remain an unsolved problem. To this end, we propose to study the discov-ery of subtle 3 topics , which rarely occur in the corpus as well as in individual documents.
 An immediate motivation for studying subtle topics is the automatic discovery of cross cutting concerns in software codes. Latent Dirichlet Allocation(LDA) ( Blei et al. , 2003 ) has been applied in software analysis to automatically discover topics that represent soft-ware concerns( Baldi et al. , 2008 ). The use of topic models for this problem is attractive because unlike most other state-of-the-art concern identification tech-niques it is neither limited by apriori assumptions about the nature of concerns to look for nor by the need for human input and other sources of informa-tion beyond the source code. However, in framework based softwares, important program concerns can have such a subtle presence in the code that existing topic models fail to detect them. In Berkeley-DB, a widely used software code base, the cross-cutting concern involved in the updation of various book-keeping counts is not easy to de-tect as the counters are named as nWaits , nRequests , nINsCleanedThisRun ... etc which do not contain the word  X  X ount X . Such concerns that are expressed sub-tly in the source code cannot be ignored as they may be sources of high resource usage or support a crit-ical program functionality. For example, Verify is a useful cross-cutting concern in Berkeley-DB, yet it is too subtle in the code for any of the existing models, including FTM, to recognize it.
 Contributions: In this paper we propose the sub-tle topic models (STM) which has the ability to detect topics those occur very rarely in individual documents. HDP and FTM use a single distribution over topics for a document and we have observed that makes it dif-ficult for them to detect subtle topics. In order to give importance to subtle topics within a document, we propose to split the co-occurrence domain inside a document by using multiple distributions over topics for each document. It is non trivial to select a proper prior over these topic vectors. We use generalized stick breaking process (GSBP) ( Ishwaran &amp; James , 2001 ) to address this issue. Using GSBP, STM allows the topic vectors to be shared across the document and the proportions over the topic vectors to be independent of each other which is essential in modeling subtle top-ics as explained in detail later. The inference problem due to GSBP is not standard. We propose to solve this by utilizing the relationship between GSBP and gen-eralized Dirichlet distribution ( GD ) and subsequently conjugacy between GD and the multinomial distribu-tion. We believe that this process and the associated inference procedure is novel and is of independent in-terest to the Bayesian non-parametric community. The most significant contribution in terms of the util-ity of STM lies in its ability to detect subtly manifested concerns in software programs, a known hard problem in software engineering. The results obtained here thus mark a breakthrough in this area. In addition, STM finds subtle topics from proceedings of NIPS, 2005 and speeches of Barack Obama since 2004, that shows its ability to do well on general text corpora.
 Structure of the paper: The paper is organized as follows. Section 2 discusses the application of topic models in discovering concerns in software code bases. In section 3, we present the proposed model, while sec-tion 4 describes the inference procedure. Experimental study has been covered in section 5.
 Software concerns are features, design idioms or other conceptual considerations that impact the implemen-tation of a program. A concern can be characterized in terms of its intent and extent( Marin et al. , 2007 ). A concern X  X  intent is defined as its conceptual objective (or topic). A concern X  X  extent is its concrete repre-sentation in software code, i.e. the source code mod-ules and statements where the concern is implemented. Program concerns may be modular , i.e. implemented by a single source file or module, or cross-cutting , i.e. dispersed across several code modules and interspersed with other concerns.
 Identifying and locating concerns in existing pro-grams is an important and heavily researched prob-lem in software (re)engineering ( Robillard , 2008 ; Savage et al. , 2010a ; Marin et al. , 2007 ; Eaddy et al. , 2008 ; Revelle et al. , 2010 ). Yet, it remains hard to automate completely in a satisfactory fashion. Typi-cal concern location and aspect mining techniques are semi-automatic: some use manual query patterns and some generate seeds automatically based on structural information ( Marin et al. , 2007 ). Both approaches have the restriction that they tend to be driven by some prior expectation or search clues about the con-cern(s) of interest either in terms of the concerns X  in-tent (e.g seed word patterns, test cases), or about the concerns X  extent (e.g. fan-in analysis).
 Recently it was shown that LDA can automatically detect prominent cross-cutting concerns ( Baldi et al. , 2008 ; Savage et al. , 2010b ) quite successfully without these restrictions.
 Although the LDA approach works well for surfacing concerns (including CCCs) that have a statistically significant manifestation in the source code (a large extent), it can miss interesting CCCs (e.g. concerns that are executed heavily and thus impact runtime re-source usage) just because they may not have a promi-nent presence in source code. This is especially likely in framework based code where all underlying module sources may not be available, and a concern X  X  extent may include a small percentage of statements in source code files to be analyzed. Or even when a concern X  X  extent is not all that small, it can elude detection be-cause of the subtle presence of representative words that reflect its intent.
 Consider the example of Verify , an important CCC in Berkeley-DB. According to a published manual anal-ysis that includes a fine grained mapping of Berkeley-DB code concerns (available at ( Apel et al. , 2009 ; Kastner et al. , 2007 )), this CCC occurs individually as a main concern and also has 7 derivative concerns (combination of multiple concerns). See supplemen-tary material for more details.
 However, this concern is surprisingly hard to detect not just by LDA/HDP but even by FTM and MG-LDA( Titov &amp; McDonald , 2008 ). The concern X  X  extent is not particularly small but its statements are spread across files and contain the internals of operations per-formed to verify different structures. Thus the word  X  X erify X  occurs in only a small fraction of these state-ments. It is very challenging to surface such subtle traces of the concern X  X  intent automatically without relying on any apriori information. Despite FTM X  X  strength in detecting rare topics, FTM fails at this task as well because even in the file with the strongest presence of the verify concern, the word is reflected in less than 10% of the statements in that file. Thus, detecting subtly manifested concerns remain to be a challenging open task. In this section we present subtle topic models (STM), designed to detect subtle topics which are rarely present across the corpus as well as within documents. We will briefly discuss generalized stick breaking pro-cess before describing STM. 3.1. Generalized stick breaking process Under the generalized stick breaking process frame-work, any P is a stick breaking random measure if it is of the following form.
 where v j  X  Beta (  X  j ,  X  j ) 4 , and  X  j s are independently chosen from a distribution H .  X   X  crete measure concentrated at  X  j . By construction, 0  X   X  j  X  1, and J can be finite or infinite. When  X  j = 1 ,  X  j and  X  =  X ,  X  j , and J  X   X  , then it reduces to DP (  X H ). The two parameter Poisson-DP (Pitman-Yor process) corresponds to the case when J  X  X  X  ,  X  j = 1  X   X  , and  X  =  X  + j X  with 0  X   X  &lt; 1 and  X  &gt;  X   X  . For more discussion see ( Ishwaran &amp; James , 2001 ). Here we are interested in the situation when J &lt;  X  , for which to ensure v
J = 1. We will utilize one interesting property of this finite dimensional stick-breaking process  X  random weights  X  j s defined in this manner are also generalized Dirichlet ( GD ) distributed. 3.2. Subtle topic models where D is the number of documents in the corpus, S d is the number of sentences in document d , N di be-ing the number of words in sentence i of document d . In addition, let us denote the number of words in a document by N d .
 In STM, for each document d , we propose to have J d  X  1 number of distributions over topics. Topics denoted by  X  k s are shared across the corpus. We as-sume a distribution over these J d topic vectors at sen-tence level, denoted by  X  di for sentence i in document d . Note that, a distribution over the topic vectors at document level will lead to the problem of having high probability for those topic vectors which are popular in the document. 3.2.1. Selecting prior over topic vectors There are various options in choosing J d ,  X  di and a prior distribution over  X  di . The simplest possibility is that: J d = J,  X  d , and  X  di  X  Dirichlet (  X  ),  X  be-ing a J dimensional vector. The problem with a fixed J is that it can not model the fact that, the docu-ments with higher S d (or N d ) in general have higher probability of being more incoherent than those with smaller S d . In order to avoid this issue, one can use J d  X  P oisson ( S d ). However, this will make expected value of J d large for documents with large S d . That in turn increases the chance of documents with large S d to be incoherent in most of the cases which is undesir-able. However, due to the rich getting richer property HDP is not suitable in this case. On the other hand, using ICD in this case will make it difficult to learn as the content of a sentence is too small.
 Noting that, J d can be at most the number of sen-tences S d in document d (when  X  di has 1 in one com-ponent and zero else where and each  X  di is different for different i ), we set J d = S d . Then, we use GSBP as described in section 3.1 to construct  X  dij s as follows. For document d , i = 1 , . . . , S d and j = 1 , . . . , S GSBP S vectors of parameters. Note that, 1  X  struction, S d is the upper limit and not the exact num-ber of distributions over topics per document. There-fore, although there is a possibility of higher value of J for a larger document but selecting higher indexed topic vectors are discouraged.
 As discussed earlier, with proper parameter setting, finite GSBP can be treated as truncated-DP or truncated-PYP (Pitman-Yor process). DP or PYP can also be used alternatively which does not affect rest of the model. However, GSBP is a more flexible distribution and is better suited for small sentences encountered in software datasets. 3.2.2. Construction of topic vectors We denote the distributions over topics in document d as {  X  dj } for j = 1 , 2 , . . . , S d . HDP is not a suitable prior for  X  dj as we need the distribution over topics to be uncorrelated to the document level topic propor-tions and with each other as much as possible. There-fore, ICD seems to be a more appropriate choice here. We use two parameter IBP( Griffiths et al. , 2007 ) to sample binary random vectors  X  dj and then we sam-ple  X  dj s from ICD as follows. For, j = 1 , . . . , S d , and k = 1 , . . . , K  X  djk  X  Bernoulli (  X  k ) ,  X  djk  X  Dirichlet (  X  1 K . X  dj where  X  k  X  Beta (  X  K ,  X  ). 1 K denotes a K -dimensional vector with all one and  X . X  denotes component wise (Hadamard) product.  X  is a repulsion parameter, with same expected number of topics the variability among  X  dj s across j increases when  X  increases, and when  X  = 1 it reduces to standard IBP. K is the trunca-tion level and ( Doshi-Velez et al. , 2009 ) shows that the probability of  X  djk to be 1 for any j is very low if K is sufficiently high.
 Using the above two constructions we get the base dis-tribution corresponding to a sentence, as follows This forms a dependent Dirichlet process where the  X  s are shared across all the sentences in all the docu-ments and the  X  dj s are shared across all the sentences within a document. STM assumes the generative pro-cess as described in Algorithm 1 .
 Algorithm 1 Generative process of STM for k = 1 , 2 , . . . do end for for documents d = 1 to D do end for We use the Gibbs sampling approach to sample the latent variables using the posterior conditional distri-bution. The main challenges in the inference proce-dure are due to the binary random vectors  X  s, and the generalized stick breaking process(GSBP) vari-ables v s. We sample the binary random vectors con-sidering truncated-IBP. A discussion on the effect of truncation can be found in ( Doshi-Velez et al. , 2009 ). Inference procedure due to GSBP is relatively unex-plored area and not straightforward. We utilize the fact that GSBP is equivalent of generalized Dirichlet distribution ( GD )( Wong , 1998 ). The benefit we draw from this relationship is that, like Dirichlet distribu-tion GD is also conjugate to the multinomial distribu-tion. This approach makes the inference very simple as we describe next.
 We collapse the conditional distributions by integrat-ing out the topic distributions (  X  ), distributions over topics (  X  ), Bernoulli parameters (  X  k ) and distribu-tion over topic vectors for each sentences (  X  da ). We however, sample topic assignment variables z s, and b s along with binary vector  X  s.
 We will use notation for counts as follows. d is the document index, a is the sentence index and i is the word position index. n represents the counting vari-able and indices are put in the subscript, where  X  .  X  represents marginalization. Super-script denotes that in all counts the current word is excluded (we do not repeat this in the text follows). Thus, n  X  dai ...kw number of times word type w dai is associated with topic k . n  X  dai ...k. represents the number of times topic k is used in the whole corpus. n  X  dai d.b of times topic k and b dai are used. n  X  dai d.b the number of times b dai is used. n  X  dai daj.. is number of times topic vector indexed by j is used, and n  X  dai da... the number of words in the sentence. K is the trun-cation level for topics. For the sake of brevity, in the following text we do not put all the variables in the conditional hoping that is easy to track following the generative process (Algorithm 1 ).
 Sampling z and  X  : The conditional probability of topic assignment of word i at sentence a in document d can be expressed as: Notice that, we need to infer only  X  , and b in order to assign topics. Note that,  X  contain binary selection values. Therefore, if n  X  dai d.jk. &gt; 0, then a.s. posterior probability of  X  djk to be one is 1. Otherwise: Sampling b : From the relation between GSBP and GD we get that, if  X  di s are constructed as Eq. 2 , then they are equivalently distributed as GD and the den-sity of  X  di is: for j = 1 , 2 , . . . , S d  X  2 and  X  S that,  X  diS  X  j  X  1 =  X  j +  X  j , 2  X  j &lt; S d , GD reduces to standard Dirichlet distribution.
 Now using the conjugacy between GD and multi-nomial we integrate out  X  s and v s. If  X  da  X  sampled from mult (  X  da ), then the posterior distri-bution of  X  da given b daj s is again a GD with den-sity GD S  X  conditional p ( b dai = j | b  X  dai ,  X ,  X  ), for j &lt; S and p ( b dai = S d | b  X  dai ,  X ,  X  ) = 1  X  l | b  X  dai ,  X ,  X  ). Notice that, the stick breaking property of GSBP is clearly visible here. The posterior prob-ability of selecting a topic vector for a word can be found to be as below: Equations 3 , 4 , 6 together form the inference procedure of STM.
 Discussion: Note that, when J = 1, we get a model equivalent to FTM, and that way we get an alter-native inference procedure for FTM. Recall that, in ( Williamson et al. , 2010 ) the binary vectors are inte-grated out using approximation for computing condi-tional for topic assignment variables z s, and the binary vectors are sampled to compute  X  k s. We have observed that, both these alternatives are equally well, however sampling the binary vectors makes the inference sim-pler with the cost of marginally slower convergence rate (matches up in likelihood in about 100 iterations) in case of truncated IBP. In this section we empirically study the proposed model STM on a special task of finding out subtle cross-cutting concerns from software repositories. In addition we apply STM on two text datasets which are apparently rich of subtle topics. This section is organized as follows. First we explain challenges re-lated to empirical evaluation and our approach under the limited scope. Then we describe the baselines fol-lowed by the datasets used in the evaluation. Next, we discuss our results in two subsections followed by a short discussion on the empirical findings 5 . 5.1. Evaluation approach We evaluate STM on two aspects, (1) modeling abili-ties by using perplexity and topic coherence. (2) abil-ity to discover subtle topics. For the first case, we will use standard metrics. However, it is not easy to evalu-ate on the second aspect. Unlike semantic coherence, it is difficult for a human to judge subtlety of a topic by looking at few top words. Moreover, subtlety is rel-ative to the dataset i.e. a topic may be subtle with re-spect to a dataset, but that may be prominent in some other dataset. As an alternative to human-judgment, we can check how good a model can find out some known or pre-defined subtle topics. But, it is difficult to find a dataset with a set of pre-defined subtle topics as gold standard and then to compare against that. In the given condition of unavailability, we manually cre-ate our gold-standard as explained later and provide the complete list in the supplementary. 5.2. Baselines For evaluation, we compare with HDP, FTM 6 and MG-LDA( Titov &amp; McDonald , 2008 ). MG-LDA has the ability to discover local topics which might be missed by HDP or FTM. Although, subtle topics may not localize properly inside a document, it is useful to benchmark STM against MG-LDA. 5.3. Dataset &amp; pre-processing NIPS-05: This is a corpus of 207 accepted papers from the proceedings of Neural Information Processing Sys-tems, 2005 7 ( Globerson et al. , 2007 ).
 Obama-speech: Collection of public speeches by Barack Obama since July 27, 2004 till October 30, 2012 8 that comprises 142 articles which are transcribed directly from audio.
 BerkeleyDB: We have selected Berkeley DB Java Edi-tion as our software dataset( Apel et al. , 2009 ). As of 2012, Berkeley DB is the most widely used database toolkit in the world 9 , and it is known to have a wide range of cross-cutting concerns.
 JHotDraw: JHotDraw is a well known open source GUI framework for drawing technical and structured graphics 10 . We have selected JHotDraw as LDA is observed to find a good set of concerns.
 For software datasets, only the textual content (with-Dataset HDP MG-LDA FTM STM BerkeleyDB 182 127 80 60 JHotDraw 131 156 93 81 NIPS-05 941 2107 413 402 Obama-speech 3591 4721 901 582 Average 1211 1778 372 281 Dataset HDP MG-LDA FTM STM BerkeleyDB -58.6 -49.6 -20.3 -27.9 JHotDraw -80.9 -94.2 -37.9 -28.2 NIPS-05 -78.1 -43.7 -45.1 -37.7 Obama-speech -72.4 -53.2 -67.2 -52.5
Average -72.5 -59.9 -42.6 -36.6 out programming syntax) of the  X .java X  files(no doc-umentation etc) used as input. Each statement has been treated as a sentence and Java key-words 11 are removed but common java library names are retained. Tokens like StringCopy have been split into two words String and Copy based on the position of a capital face inside a token. For all datasets, we have removed stan-dard English stop words, digits, sentences smaller than 20 characters and words smaller than 3 characters. We converted capital faces to small faces. In case of NIPS dataset, we have used most frequent 5000 words and in other cases we have used full vocabulary. We have used the parameters  X ,  X , ,  X  j ,  X  j as 1 and  X  as 100. We have run all the models for 2000 iterations(we found it sufficient for all models to converge in terms of log-likelihood), and used the truncation parameter K as 100(adequate considering the size of our datasets). 5.4. Evaluation on perplexity &amp; coherence We have randomly picked one-third of the datasets as held-out datasets and used the standard definition of perplexity as can be found in ( Blei et al. , 2003 ). Lower value in perplexity means that the model fits the dataset better. By approximating the user ex-perience of topic quality on W top words of a topic topic coherence (TC) can be measured as: T C ( W ) = P frequency of any word w , and D ( w i , w j ) is the docu-ment frequency of w i and w j together( Mimno et al. , 2011 ).  X  is a small constant to avoid log zero. Val-ues closer to zero indicates better coherence. We have BerkeleyDB Coherence -48.48 -42.89 -40.11 -21.99 Recall 0.42 0.59 0.68 0.94 JHotDraw Coherence -36.64 -52.13 -43.26 -46.17 Recall 0.31 0.16 0.42 0.97 NIPS-05 Coherence -40.23 -38.55 -37.38 -34.84 Recall 0.41 0.49 0.56 0.79 Obama-speech Coherence -32.65 -22.03 -53.15 -40.6 Recall 0.26 0.27 0.58 0.95 BerkeleyDB 0 0.25 0.38 1.0 JHotDraw 0 0 0.29 0.86 NIPS-05 0 0 0.07 0.69
Obama-speech 0.14 0.07 0.28 0.93 used top 5 words to compute coherence of a topic. Table 1 contains results on perplexity and average topic coherence for all the datasets. We observe that STM is a better model than all others in terms held-out data perplexity and coherence(in most of the cases). Note that, the ability of STM to detect subtle topics lies in splitting the co-occurrence domain, how-ever this brings in mild difficulty for normal topics to be learnt. Hence, coherence may suffer little bit which is observed in Table 1 too. 5.5. Evaluation in detecting subtle topics Measure of subtlety: We define degree of subtlety of topic k as DoS ( k ) = p describing topic k . I [ w  X  S di ] is 1 if word w is present in sentence i of document d . Note that, 0  X  DoS  X  1. The value of DoS increases if a rare word is included into K k and it decreases if a frequent word is inserted. Gold standard: In order to compare performance on subtle topics, we hand-picked some topics from each corpus so that their DoS is greater than 0 . 2 which we
BerkeleyDB JHotDraw NIPS Obama speech recovery architecture, circuit internet consider as a reasonably challenging degree of sub-tlety. For comparison we however considered only those hand-picked topics for which a least 75% of the keywords are retrieved among top 5 words by at least one method in comparison. We compute recall of each topic considering top five words with K of each gold standard topic. A topic is said to be a match for the gold-standard topic which has the highest recall, if re-call is less than 0 . 75, then we say the topic is not de-tected by the model. The reason of keeping threshold of recall high is that some keywords may be popular and part of normal topics. Hence, detecting such a keyword alone does not signify detecting the subtle topic in consideration.
 Following the above approach, we hand picked 11, 10, 21, and 16 topics respectively from BerkeleyDB, JHot-Draw, NIPS and Obama-speech datasets. For each gold standard topic we consider the recall and coher-ence of the best matched topic for each model and then we average over all the gold standard topics cor-responding to each dataset and report at Table 2 . Ta-ble 3 contains the result on fraction of subtle topics detected by all the models. A complete list can be found in the supplementary material. In Table 4 , we provide five example subtle topics from each dataset which are detected by STM, but other models hardly detect them. 5.6. Discussion Subtle topics in many cases consist of rare words. For example, the  X  cochlea  X  topic is subtle due to rareness of its keywords and is detected only by STM. In cer-tain cases, some keywords may not be rare but it is the combination of the words that makes the topic subtle to detect. For example, in Berkeley-DB the topic  X  trace, level, info, config  X  consists of four words of which trace is not a rare word. The topic as a whole signifies the ability to configure trace levels, and mani-fests in a very localized fashion in the Tracer class and hence can be detected by FTM, but not by HDP or MG-LDA. On the other hand the cross-cutting concern  X  checksum, errors, validate  X  appears subtly in individ-ual files but is diffused widely across the corpus and hence it can be detected by MG-LDA, but HDP and FTM fail. Not only STM detects all of these topics, but it also succeeds in detecting the more interesting cross-cutting concern,  X  verify  X , which manifests sub-tly in every file and therefore eludes HDP, FTM and MG-LDA. Another example of an important yet sub-tle topic detected only by STM is  X  X yber security X  on the internet. This is an important topic that indicates policies or priorities of president Obama. The utility of a topic is not necessarily linked to its prevalence in a corpus. When topic models are used for automatically discovering concerns, the inability to discover important or interesting concerns just be-cause they are subtly manifested in the source code can be a critical drawback. In this paper we propose a novel model, namely STM to address this problem for the first time in the literature. STM, by using multiple distributions over topics per document is ob-served to effectively discover subtle topics where state of art models fail. This is a promising result for ad-vancing the state-of-the-art in the difficult problem of automatic concern discovery in software engineering. On empirical evaluation, we find STM to out perform state of art models not only in case of subtle topics but also in general with low perplexity on unseen data, and good topic coherence.
 We are thankful to all the reviewers for their valuable comments. The authors MKD and CB were partially supported by DST grant(DST/ECA/CB/1101).
 Apel, Sven, Kastner, Christian, and Lengauer, Chris-tian. FEATUREHOUSE: Language-Independent,
Automated Software Composition. In Proceedings of the 31st International Conference on Software En-gineering(ICSE) , pp. 221 X 231, 2009.
 Baldi, Pierre F., Lopes, Cristina V., Linstead, Erik J., and Bajracharya, Sushil K. A theory of aspects as latent topics. In Proceedings of the 23rd ACM SIG-
PLAN conference on Object-oriented programming systems languages and applications(OOPSLA) , pp. 543 X 562, 2008.
 Blei, David M., Ng, Andrew Y., and Jordan, Michael I. Latent Dirichlet Allocation. The Journal of Machine Learning Research(JMLR) , 3:993 X 1022, 2003.
 Doshi-Velez, Finale, Miller, Kurt T., Gael, Jur-gen Van, and Teh, Yee Whye. Variational Infer-ence for the Indian Buffet Process. In Proceedings of the Intl. Conf. on Artificial Intelligence and Statis-tics(AISTATS) , pp. 137 X 144, 2009.
 Eaddy, M., Aho, A. V., Antoniol, G., and Gueheneuc, Y. G. CERBERUS: Tracing Requirements to Source
Code Using Information Retrieval, Dynamic Analy-sis, and Program Analysis. In International Confer-ence on Program Comprehension In Program Com-prehension(ICPC) , pp. 53 X 62, 2008.
 Globerson, A., Chechik, G., Pereira, F., and Tishby, N. Euclidean Embedding of Co-occurrence Data. The
Journal of Machine Learning Research(JMLR) , 8: 2265 X 2295, 2007.
 Griffiths, T. L., Ghahramani, Z., and Sollich, Peter. Bayesian Nonparametric Latent Feature Models. In Bayesian Statistics , pp. 201 X 225, 2007.
 Ishwaran, H. and James, L. F. Gibbs Sampling Meth-ods for Stick-Breaking Priors. Journal of the Amer-ican Statistical Association , 96:161 X 173, 2001. Kastner, Christian, Apel, Sven, and Batory, Don. A Case Study Implementing Features Using AspectJ. In Proceedings of the 11th International Software Product Line Conference(SPLC) , pp. 223 X 232, 2007. Marin, Marius, Deursen, Arie Van, and Moonen, Leon.
Identifying crosscutting concerns using fan-in anal-ysis. ACM Transactions on Software Engineering and Methodology(TOSEM) , 17, 2007.
 Mimno, David, Wallach, Hanna, Talley, Edmund,
Leenders, Miriam, and McCallum, Andrew. Op-timizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing(EMNLP) , pp. 262 X 272, 2011.
 Revelle, Meghan, Dit, Bogdan, and Poshyvanyk,
Denys. Using data fusion and web mining to support feature location in software. In Proceedings of the 2010 IEEE 18th International Conference on Pro-gram Comprehension(ICPC) , pp. 14 X 23, 2010.
 Robillard, M. P. Topology Analysis of Software De-pendencies. ACM Transactions on Software Engi-neering and Methodology(TOSEM) , (4), 2008.
 Savage, T., Revelle, M., and Poshyvanyk, D. FLAT 3 :
Feature Location and Textual Tracing Tool. In Pro-ceedings of the 32nd ACM/IEEE International Con-ference on Software Engineering(ICSE) -Volume 2 , pp. 255 X 258, 2010a.
 Savage, Trevor, Dit, Bogdan, Gethers, Malcom, and
Poshyvanyk, Denys. TopicXP: Exploring topics in source code using Latent Dirichlet Allocation. In
Proceedings of the IEEE International Conference on Software Maintenance(ICSM) , pp. 1 X 6, 2010b. Teh, Y., Jordan, M. I., and Beal, M. Hierarchical Dirichlet processes. Journal of American Statistical Association , pp. 1566 X 1581, 2007.
 Titov, Ivan and McDonald, Ryan. Modeling Online
Reviews with Multi-grain Topic Models. In Proceed-ings of the 17th International Conference on World Wide Web(WWW) , pp. 111 X 120, 2008.
 Wen, Bo and Boahen, Kwabena. Active Bidirectional Coupling in a Cochlear Chip. In Advances in Neural Information Processing Systems(NIPS) , 2005. Williamson, Sinead, Wang, Chong, Heller, Kather-ine A., and Blei, David M. The IBP Compound Dirichlet Process and its Application to Focused
Topic Modeling. In Proceedings of the 27th Inter-national Conference on Machine Learning(ICML) , 2010.
 Wong, T. T. Generalized Dirichlet distribution in
Bayesian analysis. Applied Mathematics and Com-
