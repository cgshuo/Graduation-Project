 1. Introduction
In surveillance systems, the suc cess of the down-stream stages such as image acquisition ( Jakubek and Strasser, 2004 ), segmentation prior efficient filtering. In literature many approaches have been proposed for surveillance systems and these systems have been used for practical applications. Image analysis based surveillance systems were developed for quality monitoring ( Chatterjee and Bhattacherjee, 2012 ), mobile robot control ( Posadas et al., 2008 ), control behavioral patterns ( Boussemart and Cummings, 2011 ). and Learning of traffic signals ( Bazzan et al., 2010 ). An efficient multi-object recognition scheme was proposed for surveillance based on interest points of objects ( Kim et al., in press ). The intelligent approaches based surveillance systems have also been proposed and are becoming popular for diverse applications ( Vallejo et al., 2011 ). In intelligent surveillance systems, high-level in terpretation of events within the scene requires low-level vision computing of the image and the moving objects. In real applicatio ns, these systems are developed for motion detection ( Delgado et al., 2010 ; Huang and Cheng, in press ), fault detection ( Jakubek and Strasser, 2004 ; Tan et al., 2007 ; Byttner et al., 2011 ), road detection ( Chen and Tai, 2010 ), heterogeneous network ( Leuetal.,inpress ), smart home environment ( Kang et al., in press ), scenario-based video-surveillance ( S -aykol et al., 2010 )andreal time skin color detection based surveillance system ( Chen et al., in press ).
 is essential in surveillance systems ( Vallejo et al., 2011 ). Fig. 1 shows the basic components of surveillance system. In this regards, the performance of conventional imaging devices with long focal lengths suffer from the limited depth of field. In the captured frames, some parts of the scene may be well-focused while other parts de-focused ( Mahmood et al., 2011 ). Further, during the image capturing process, due to the relative motion of objects or camera lens, blurring may introduced in the acquired frames. This effect is formulated through the point spread function (PSF). The amount of blurring introduced can be adjusted through this function. The objective of image restoration is to obtain an estimate of true image is obtained due to the convolution of the actual image with
Gaussian function h ( i , j ). The degraded image is expressed as g  X  i , j  X  X  f  X  i , j  X  * h  X  i , j  X  X  n  X  i , j  X  ,  X  1  X  where n denotes 2-D linear convolution operator and n ( i , j )isthe additive noise.
 complete knowledge of blur function and noise statistics. How-ever, in practice, it is not easy to compute all the necessary parameters in advance. To restore the noisy blurred image, the blind image deconvolution approaches involve the complete knowledge of the PSF ( Li et al., 2007 ). These approaches are useful when the original object is not available. However, both linear and nonlinear blind image restoration approaches face ill-posed problem ( Sezan and Tekalp, 1988 ; Chong and Tanaka, 2010 ). The ill-conditioned problem is addressed in regularization or bayesian theory based methods by incorporating additional constraints. The proposed algorithms restore the original image by minimizing the cost function. It is believed that nonlinear probabilistic methods are superior to conventional linear meth-ods. However, there is no guarantee about the uniqueness con-vergence of the linear and nonlinear algorithms. At high noise levels, these algorithms may diverge.

We compare the performance of the proposed approach scheme with Richardson X  X ucy (LR) deconvolution algorithm and Wiener filtering approaches. Due to their effectiveness, both
Richardson X  X ucy (LR) deconvolution algorithm ( Richardson, 1972 ) and Wiener filtering ( Grimble, 1984 ) has gained consider-able attention. This maximum likelihood estimate (MLE) based the LR algorithm is effective when complete information of the spread function and little information about the additive noise is known. Maximum a posteriori estimate (MAPE) is another prob-abilistic based approach, which is closely related to the MLE method. However, this probabilistic method incorporates a prior distribution over the original scene to iteratively restores the degraded image ( Xu and Lam, 2009 ). The major limitation of these nonlinear iterative techniques is their slow convergence because they offer computationally intensive solution. The MLE based methods begin to amplify the noise after a certain number of iterations. Wiener filtering is commonly used for the digital image reconstruction, deblurring and de-noising. This filter reduces the difference between the filtered noisy and the true image in the least squares sense. Wiener filter is completely deterministic and the user has to tune its parameters according to the situation. The performance of the filter depends on the spatial-invariant blur and stationary Gaussian noise.

Under such situation, machine learning based methods can offer an alternate approach for effective blind image deconvolu-tion ( Li et al., 2007 ). In practices, the overall characteristics of original or degraded images are different from each another.
However, there exit some common features in all the images degraded with diverse types of blur. For example, all types of blur average the neighboring pixels. This common property found in most frequently exist Gaussian noise. The problem is how to select this local useful information and then optimally combine to develop a generalized mapping function that perform equally well under diverse type of noisy environment. To address this problem, adaptive neuro-fuzzy (Anfis), support vector machine (SVM) and artificial neural network (ANN) intelligent approaches are proposed to learn the common features for image restoration ( Da  X  vila and Hunt, 2000 ; Jakubek and Strasser, 2004 ; Li et al., 2007 ; Tan et al., 2007 ). An adaptive neuro-fuzzy (Anfis) impulsive noise filter was developed for distorted images ( Bes -dok, 2004 ). Recently, a multiscale dictionary learning approach is introduced for the removal of image noise. In this approach, first images are represented by a translation invariant dictionary and coefficients are then denoised using learned multiscale dictionaries ( Yang et al., in press ). Both SVR and ANN approaches optimize the model parameters. However, due to the poor generalization of ANN, an alternative SVR approach develops a mapping by selecting sup-port vectors from the input examples. The performance of SVR approach is dependent on the characteristics of the input exam-ples. Therefore, these approaches are lack of developing an example-independent mapping function. Developing a reliable and generalized technique is still a major challenge.

Under this scenario, we introduce the Genetic Programming (GP) based blind image deconvolution approach that automati-cally extracts and combines the useful features to develop an optimal function under a fitness criterion. GP based optimization technique has widely used in the applications of pattern recogni-tion ( Khan et al., 2008 ), information systems ( Mahmood et al., 2011 ), and computer vision ( Petrovic and Crnojevic, 2008 ; Majid et al., 2010 ). GP searches solution in the defined problem space. GP is effectively in developing mathematical models by combin-ing focus measures for depth estimation ( Mahmood et al., 2011 ). These models are very useful for 3D shape recovery of micro-scopic images ( Majid et al., 2010 ). GP based function was develop for impulse noise detector ( Petrovic and Crnojevic, 2008 ). How-ever, in ( Majid et al., 2011 ), convolution kernels were used in the noise detection stage and filtering is carried out by developing noise-free pixels based filter. During GP based training phase, an improved performance estimator function is developed by taking advantages of local information of degraded images. In the proposed approach, local pixels information of degraded images is used as input features and the corresponding pixels information of the true images are used as targets. During GP evolution, a numerical mapping establishes a common high performance framework that can handle various types of blurs and other noisy information. In this method, the step of blur identification is automatically incorporated. The experimental results are evalu-ated under various metrics. Our comparative analysis demon-strates the effectiveness of the GP based proposed scheme. The organization of the rest of the paper is as follows. In Section 1 , a brief introduction to surveillance systems and its possible application areas is given. In Section 2 , GP based proposed scheme is explained in detail. Experimental results and discussion are presented in Section 3 . Finally, Section 4 concludes this study. 2. Proposed scheme
GP technique is based on the princ iples of natural selection and recombination to search the space of all possible solutions. Through GP evolution cycle, the most optimal solution in the form of a numerical function is developed. The proposed scheme is divided into three modules; (1) Preprocessing module ,(2) GP module ,and(3) Estimation module .In Preprocessing module ,trainingdataisformedby computing the grey scale features o f the object. During GP process, optimal estimation function is developed using the training data.
Estimation module is used to estimate the pixel value. The block diagram of GP based proposed approach is shown in Fig. 2 . 2.1. Preprocessing module
The first step in the proposed approach is the formation of feature vector. For each central pixel of a degraded image g ( i , j ), we picked local pixel information by employing a small window W .
Let t be the corresponding target pixel intensity selected from the un-degraded image f ( i , j ) correspond to the feature vector x  X  x 1 , , x d , where d shows the number of pixels within window W . It is notable, that the length of feature vectors can be changed by modifying the neighborhood window size or collecting the resultants pixel values after applying various operators. We prefer to use small window size 3 3.We found that the larger window was not productive. First, correlation between pixels decreases as the pixels are separated further apart, secondly, larger neighborhood also increase the dimension-ality of the feature vector, which results in the increase of training and testing time.
 In order to construct training data, standard LENA and CAM-
ERAMAN images are used. First, these images are degraded by adding blur and noise. From the degraded images, for each pixel, a 9-dimnesional feature vector x  X  x 1 , , x 9 fg is obtained by arran-ging 3 3 neighborhood. Thus, a training dataset S of N input X  represents the n th feature vector correspond to n th target t . where, N is representing randomly selected data points out of total data points for estimator training. 2.2. GP module
The objective of GP module is to develop an improved-performance estimator function y  X  F h  X  x  X  , where x A R h represents appropriate set of GP tree structural parameters. In order to represent adequately the structure of the target function, according to the defined fitness function, we provided suitable set of functions, variables, and constants. In this way, there are more chances that GP simulation can exploiting and exploring the search space in finding optimal candidate solution.

First step is to represent the candidate solutions h h ( x ) in a tree-like data structure. The GP trees are constituent of terminals and non-terminal set. Table 1 highlights all the necessary parameters used as terminals and non-terminal. Terminal set consist of set of feature vectors and random constants generated from uniform distribution are provided. During GP evolution process, the most informative parametric values of arithmetic, trigonometric func-tions, variable features, and random constants are selected.
After representing GP tree, initial population of size 100 indivi-duals are generated using ramped half-and-half method ( Majid et al., 2006 ). Second step is to assess the fitness score of each individual candidate in the population. Fitness function plays a vital role to develop the useful solution within a large search space. We used mean square error (MSE) as a fitness criterion, which is defined as: h s  X  1 N where h h ( x ( n ) ) gives the estimate of GP individual for the training pattern x ( n ) . The minimum fitness score h s indicates how effec-tively a GP individual moves towards the optimal solution ( Majid, 2006 ). In the third step, the best candidates are to be selected from the current population. The fitness probability of individual candidates, within the population of size P , is computed as
Pr  X  h p s  X  X  h where, P P p  X  1 h p s represents the total population fitness. The higher probability values give more the chances for the individual to take part for the production of offspring.
 principal, three selection strategies are commonly used. (1)
Fitness proportional selection technique is highly biased towards the more fit individuals. This gives little chance for the less-fit individuals to be selected for the next step. However, in natural evolutionary process, there are chances that the less-fit indivi-duals in the previous generation may improve in the coming cycle. To release this selection pressure, relatively more advance technique, (2) Roulette Wheel selection is used. This technique relatively gives chances to the less fit individual to be selected.
This technique consumed more computational steps as compared to (3) Tournament based selection . This technique is based on the competition within a small subset of the population. In this technique, a small number of individuals called the tournament size, are randomly selected. This selection works by selecting trees at random from the current generation. Two trees with the highest fitness values are exchanged sub-trees resulting in two new possible solutions. Based on the fitness values, individuals are selected from the selective competition. In this way, to obtain good generalized function, we used the tournament based selection method ( Koza et al., 2008 ).

In the fourth step, crossover, mutation, and replication opera-tors are applied on the selected individuals to generate new population. Crossover operator creates offspring by exchanging genetic material between two individual parents. Crossover/ mutation ratio play important role in converging the candidate solution towards optimal solution in the search space. Crossover helps in converging to optimal/near-optimal solution. However, in mutation process, a small part of individual often brings diversity in the solution space, which help to avoid the local minima/ premature convergence. We used variable crossover/ mutation ratio, which automatically adjust ratio to converge global minima. During simulation, each new generation has either a higher average fitness score or remain constant. In this way, the solution space is refined and converges to the optimal/near optimal solution. The simulation is stopped if either the numbers of generations reach the maximum set limit (300) or the fitness score h s approaches the minimum set value 0.01. Finally, the best individual in the population is selected for the estimation pur-pose, i.e. h h  X  x  X  -F h  X  x  X  .

The selected candidates are used for the creation of next generation. Crossover, mutation, and replication operators are applied on the selected individuals to generate new population.
Crossover operator creates offspring by exchanging genetic mate-rial between two individual parents. To obtain good results through crossover, we used tournament selection method ( Koza et al., 2008 ). This selection works by selecting trees at random from the current generation. Two trees with the highest fitness values are exchanged sub-trees resulting in two new possible solutions. Crossover helps in converging to optimal/near-optimal solution. However, in mutation process, a small part of individual often brings diversity in the solution space. For GP simulation, a ratio of crossover/mutation is automatically adapted. During simulation, each new generation has a slightly higher average fitness score. In this way, the solution space is refined and converges to the optimal/near optimal solution. The simulation is stopped if either the numbers of generations reach the maximum limit or the fitness value (MSE) approaches the mini-mum set value. In order to represent a possible solution in the form of an optimal function, a summary of all the necessary parameters including arithmetic functions, variables, and con-stants are provided in the Table 1 . After adjusting parameters, several GP simulations are carried out using GPLAB toolbox in MATLAB 7.0 environment ( Silva and Almeida, 2003 ). 2.3. Estimation module
Once the estimation function F h  X  x  X  by the GP module is developed, it is straightforward to restore the degraded image. It is important to note that, for the estimation function develop-ment, we prepare training data from standard images with known pixels values. Moreover, randomly selected data points are used that not only reduce the training time but also the improve generalization capability. During the development of the function, we found that the order of the feature vectors is insignificant. In order to obtain a complete deblurred and denoised image, we need to provide input feature vector for each pixel to the estimator. Thus, feature vectors for the estimation are very different from the features used in training.

In order to obtain a complete restored image, we need to provide input feature vector for each pixel to the developed GP-based estimator. Consider again, an test image sequence g that is acquired using CCD camera. We are interested in estimat-image, first, feature vector x  X  x 1 , , x 9 fg is computed. Then, using the developed function F h  X  x  X  , the intensity of the restored image is estimated as; ^ f  X  i , j  X  X  F h  X  x 1 , , x 9  X  X  4  X  3. Results and discussion 3.1. Experimental setup
We carried out several experiments to report the performance of the proposed GP based approach using LENA, CAMERAMAN, and LIVINGROOM images as shown in Fig. 3 . For training purpose, LENA and CAMERAMAN images are blurred by convolving with 3 3 average filter. These blurred images are further degraded by adding Gaussian random noise with zero mean and 0.01variance. For each pixel, a feature vector x  X  x 1 , , x 9 fg is obtained by arranging 3 3 neighborhood, where x 5 represents the central pixel. In this way, (2 512 512  X  524,288) total numbers of feature vectors are formed. In order to improve the generalization capability of F h  X  x  X  , we picked randomly a small portion (70,000) of training data samples for LENA and CAMERAMAN images. This results in small train/test ratio ( o 1:5). In this way, approximately 4/5 image data was kept out during the mapping function development phase.

Fig. 4 (a) shows the improvement of the best fit individual up to 200 generations. This figure highlights that there is appreciable fitness improvement of the best GP individual in the initial generations as compared to the later generations. After 50 generations, there occur smooth fitness transition and best fit individual converges to the near optimal point. The complexity is expressed as a function of tree depth level and the number of nodes. During GP evolution, constructive blocks are created that try to minimize the destruction of useful building blocks ( Majid et al., 2006 ). As a result, in several regions of Fig. 4 (b), the size of
GP individual grows exponentially without appreciable improve-ment in performance curve of the best individual. This is due to the bloating phenomenon occurring during GP cycle. Many branches do not contribute in improving its performance. There-fore, the best genome X  X  total number of nodes increases and its average tree depth becomes very large. Therefore, with the increase of complexity, performance curve of the best individual approaches towards the optimal solution. During the training phase, several GP runs were carried out and the best filter function is reported. Each GP simulation took considerable computational time that depends on several input parameters i.e., input data size, population size, and number of generations.
The numerical function developed, at the end of GP evolution cycle, in prefix form is given below:
F  X  x 1 , , x 9  X  X  X  X  X  X  X  X  X  X  X  x 5 , sin  X  X  X  X  x 1 , x 2  X  ,  X  X  X  X 
While, developing F h  X  x  X  function, the most informative values of the arithmetic and trigonometric functions are selected. Some constants are also picked randomly. The improved performance of this function is dependent on the useful combination of local information of the degraded image.
 function, which may not be understood by human being ( Langdon, 2000 ). However, this function can easily be computed by providing the values of input variables x 1 , ... , and x structure of filter function is shown in the Fig. 5 . This graphical representation demonstrated the functional dependency of this empirical expression on the input features x 1 , ... , and x with some selected arithmetic and trigonometric functions. mance is reported in terms of three qualitative measures; root mean square error (RMSE), peak signal-to-noise ratio (PSNR), and improvement in signal-to-noise ratio (ISNR). RMSE quantifies the differing amount between a restored and original image and defined as
RMSE  X  tively. The smaller the value of RMSE, the better the result will be.
PSNR measure is the ratio between the maximum possible signal power and the noise power. It usually expressed in terms of the logarithmic decibel scale:
PSNR  X  10 log 10 MAX where MAX 1 is the maximum gray value of the image. A higher value of PSNR indicates good quality of restored image. The ISNR of the restored image is computed by using the same formula as provided by Li et al., (2007) i.e., ISNR  X  10log 10 where g ( i , j ) represents the degraded image.
 3.2. Quantitative analysis
In this section, we will try to investigate the accuracy, robust-ness, and generalization capability of the F h  X  x  X  function. The experimental results of the proposed F h  X  x  X  function are compared with LR algorithm based F LRA method and Wiener filter based F method.

The proposed function is estimated by using various degraded images. LENA and CAMERAMAN images were blurred by convol-ving with Gaussian mask of size 3 3. The resultant blurred image is then corrupted with Gaussian random noise with zero mean and 0.02 variance to obtain degraded images. These degraded images are restored using the F WF , F LRA , and F methods. The experimental results given in Table 2 and Table 3 highlight the RMSE, PNSR and ISNR performance measures for
LENA and CAMERAMAN images, respectively. From Table 2 ,itis observed that RMSE values for F WF and F LRA methods to be 0.452 and 0.132, respectively. On the other hand, RMSE value 0.0583 for
F  X  x  X  method is considerably lower than F WF and F LRA methods.
Similarly, using F h  X  x  X  method, we obtained improved PSNR (12.164) and ISNR (5.984) values as compared to PSNR and ISNR values obtained using F WF and F LRA . Similarly, from the experi-mental results in the Table 3 , we observed higher performance of
PNSR and ISNR values, which are also better than the existing approaches. The improved experimental results highlight the generalization capability in the proposed function. 3.3. Qualitative analysis
In this section, comparative analysis qualitatively using differ-ent standard images and real videos. Various degraded and blurred standard images are restored using developed GP-based estimator and conventional filters. Fig. 6 shows the restored images by using different deconvolution methods. It can be seen from this figure that the restored LENA and CAMERAMAN images, using the F h method, are of superior quality as compared to conventional methods F WF and F LRA . Further, the blurring features used for the restoration of degraded images are different than that used in the training phase. Moreover, the LIVINGROOM image is degraded by using motion blur of size 9 15 (length 9 and angle 15 1 investigate the robustness, the blurred image is degraded by introducing Gaussian noise with zero mean non-zero variance.
The proposed method was also tested using image sequence of real world scene. Detecting moving objects in video sequence is one of the most fundamental tasks in real-time machine vision applications. In surveillance systems, object regions extracted from stationary backgrounds usually become the input data to a higher level process such as recognition, tracking and behavior analysis. Figs. 7 X 9 show the results obtained using computer vision toolkit from MATLAB, version 7.12, release 2011a, with video named  X  X iptraffic.avi X  and by modifying the demo program for object tracking based on optical flow. Fig. 7 shows intermedi-ate stages for cars detection results using proposed filter F different frames containing different number of cars. From the figure, it can be observed that the objects are detected success-fully in different frames. In order to investigate the robustness of the proposed estimator, we applied it on degraded video by adding different noises and applying different blur kernels. First, video is degraded by adding Gaussian noise with zero mean and 0.05 variance. Fig. 8 shows that without applying any filter, the results are not accurate. The tracker failed to detect one of the two objects accurately. However, by applying proposed filter, the tracker has been able to detect objects in frame 38 of the video.
Similarly, the video is degraded by adding blur and salt-and-pepper noise. Fig. 9 shows the results by applying median filter and the proposed GP based estimator. It can be observed that median filter has smooth the video frames however, tracker is unable to detect the third object in the frame 73. Whereas, the after applying proposed estimator, the tracker has detected the all three cars in the frame successfully. It can be observed that the proposed GP-based estimator is effective in deconvolution and deblurring of the images. 4. Conclusion
In this work, we have developed GP based numerical function, which is used to restore the image from its degraded version.
The developed function combines the useful information of neighboring pixels. During GP process, the most important func-tional parameters, arithmetic functions and random constants are automatically selected and combine under a fitness criterion. The improved performance of the proposed estimator function has been evaluated using various types of degraded images. The experimental results have demonstrated that the proposed scheme is more accurate, generalized, and robustness as com-pared to the LR algorithm and the Wiener filter. It is anticipated that the performance of the proposed scheme can further be enhanced by including the knowledge of PSF and information extracted from existing deconvolution approaches. Then, GP can effectively combine the advantages of conventional deblurring approaches by suppressing their weaknesses.
 Acknowledgment This research is made possible through the support given by
Korea University of Technology and Education 2010 Research support for new Professors.
 References
