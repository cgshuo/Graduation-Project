 Learning a good classier requires a suf cient number of labeled training instances. In man y cir -cumstances, unlabeled instances are easy to obtain, while labeling is expensi ve or time consuming. For example, it is easy to download a lar ge number of webpages, howe ver, it typically requires man-ual effort to produce classication labels for these pages. Randomly selecting unlabeled instances for labeling is inef cient in man y situations, since non-informati ve or redundant instances might be selected. Hence, acti ve learning (i.e., selecti ve sampling) methods have been adopted to control the labeling process in man y areas of machine learning, with the goal of reducing the overall labeling effort.
 Given a lar ge pool of unlabeled instances, acti ve learning pro vides a way to iterati vely select the most informati ve unlabeled instances X the queries X to label. This is the typical setting of pool-based acti ve learning. Most acti ve learning approaches, howe ver, have focused on selecting only one unlabeled instance at one time, while retraining the classier on each iteration. When the training process is hard or time consuming, this repeated retraining is inef cient. Furthermore, if a parallel labeling system is available, a single instance selection system can mak e wasteful use of the re-source. Thus, a batch mode acti ve learning strate gy that selects multiple instances each time is more appropriate under these circumstances. Note that simply using a single instance selection strate gy to information overlap between the multiple instances into account. Principles for batch mode acti ve learning need to be developed to address the multi-instance selection specically . In fact, a few batch mode acti ve learning approaches have been proposed recently [2, 8, 9, 17, 19]. Ho we ver, most extend existing single instance selection strate gies into multi-instance selection simply by using a heuristic score or greedy procedure to ensure both the instance diversity and informati veness. In this paper , we propose a new discriminati ve batch mode acti ve learning strate gy that exploits information from an unlabeled set to attempt to learn a good classier directly . We dene a good classier to be one that obtains high lik elihood on the labeled training instances and low uncertainty on labels of the unlabeled instances. We therefore formulate the instance selection problem as an optimization problem with respect to auxiliary instance selection variables, taking a combination of discriminati ve classication performance and label uncertainty as the objecti ve function. Un-fortunately , this optimization problem is NP-hard, thus seeking the optimal solution is intractable. Ho we ver, we can approximate it locally using a second order Taylor expansion and obtain a subop-timal solution using a quasi-Ne wton local optimization technique.
 The instance selection variables we introduce can be interpreted as indicating self-supervised, op-timistic guesses for the labels of the selected unlabeled instances. A concern about the instance selection process, therefore, is that some information in the unlabeled data that is inconsistent with the true classication partition might mislead instance selection. Fortunately , the acti ve learning method can immediately tell whether it has been misled, by comparing the true labels with its opti-mized guesses. Therefore, one can then adjust the acti ve selection strate gy to avoid such over-tting in the next iteration, whene ver a mismatch between the labeled and unlabeled data has been detected. An empirical study on UCI datasets sho ws that the proposed batch mode acti ve learning method is more effecti ve than some current state-of-the-art batch mode acti ve learning algorithms. Man y researchers have addressed the acti ve learning problem in a variety of ways. Most have focused on selecting a single most informati ve unlabeled instance to label at a time. Man y such approaches therefore mak e myopic decisions based solely on the current learned classier , and select the unlabeled instance for which there is the greatest uncertainty . [10] chooses the unlabeled instance with conditional probability closest to 0.5 as the most uncertain instance. [5] tak es the instance on which a committee of classiers disagree the most. [3, 18] suggest choosing the instance closest to the classication boundary , where [18] analyzes this acti ve learning strate gy as a version space reduction process. Approaches that exploit unlabeled data to pro vide complementary information for acti ve learning have also been proposed. [4, 20] exploit unlabeled data by using the prior density over the unlabeled data. [11 ] uses an EM approach to inte grate information from unlabeled data. [13, 22] consider combining acti ve learning with semi-supervised learning. [14 ] presents a mathematical model that explicitly combines clustering and acti ve learning. [7] presents a discriminati ve approach that implicitly exploits the clustering information contained in the unlabeled data by considering optimistic labelings.
 Since single instance selection strate gies require tedious retraining with each instance labeled (and, moreo ver, since the y cannot tak e adv antage of parallel labeling systems), man y batch mode acti ve learning methods have recently been proposed. [2, 17, 19] extend single instance selection strate gies that use support vector machines. [2] tak es the diversity of the selected instances into account, in the cluster centers of the instances lying within the mar gin of a support vector machine. [8, 9] choose multiple instances that efciently reduce the Fisher information. Ov erall, these approaches use a variety of heuristics to guide the instance selection process, where the selected batch should be informati ve about the classication model while being diverse enough so that their information overlap is minimized.
 Instead of using heuristic measures, in this paper , we formulate batch mode acti ve learning as an optimization problem that aims to learn a good classier directly . Our optimization selects the best set of unlabeled instances and their labels to produce a classier that attains maximum lik elihood on labels of the labeled instances while attaining minimum uncertainty on labels of the unlabeled instances. It is intractable to conduct an exhausti ve search for the optimal solution; our optimization problem is NP-hard. Ne vertheless we can exploit a second-order Taylor approximation and use a quasi-Ne wton optimization method to quickly reach a local solution. Our proposed approach pro vides an example of exploiting optimization techniques in batch model acti ve learning research, much lik e other areas of machine learning where optimization techniques have been widely applied [1]. In this paper , we use binary logistic regression as the base classication algorithm. Logistic re-gression is a well-kno wn and mature statistical model for probabilistic classication that has been acti vely studied and applied in machine learning. Given a test instance x , binary logistic regression models the conditional probability of the class label y 2 f +1 ; 1 g by where w is the model parameter . Here the bias term is omitted for simplicity of notation. The model parameters can be trained by maximizing the lik elihood of the labeled training data, i.e., minimizing the logloss of the training instances where L inde xes the training instances, and over-tting problems. Logistic regression is a rob ust classier that can be trained efciently using various con vex optimization techniques [12]. Although it is a linear classier , it is easy to obtain nonlinear classications by simply introducing kernels [21]. For acti ve learning, one typically encounters a small number of labeled instances and a lar ge number of unlabeled instances. Instance selection strate gies based only on the labeled data therefore ignore potentially useful information embodied in the unlabeled instances. In this section, we present a new discriminati ve batch mode acti ve learning algorithm for binary classication that exploits information in the unlabeled instances. The proposed approach is discriminative in the sense that (1) it selects a batch of instances by optimizing a discriminati ve classication model; and (2) it selects instances by considering the best discriminati ve conguration of their labels leading to the best classier . Unlik e other batch mode acti ve learning methods, which identify the most informati ve batch of instances using heuristic measures, our approach aims to identify the batch of instances that directly optimizes classication performance. 4.1 Optimization Pr oblem classier . We assume the learner selects a set of a x ed size m , which is chosen as a parameter . Su-pervised learning methods typically maximize the lik elihood of training instances. With unlabeled data being available, semi-supervised learning methods have been proposed that train by simultane-ously maximizing the lik elihood of labeled instances and minimizing the uncertainty of the labels for unlabeled instances [6]. That is, to achie ve a classier with better generalization performance, one can maximizing the expected log lik elihood of the labeled data and minimize the entrop y of the missing labels on the unlabeled data, according to where is a tradeof f parameter used to adjust the relati ve inuence of the labeled and unlabeled data, w species the conditional model, L inde xes the labeled instances, and U inde xes the unlabeled instances.
 The new acti ve learning approach we propose is moti vated by this semi-supervised learning princi-ple. We propose to select a batch of m unlabeled instances, S , to label in each iteration from the total unlabeled set U , with the goal of maximizing the objecti ve (2). Specically , we dene the score function for a set of selected instances S in iteration t + 1 as follo ws where w t +1 is the parameter set for the conditional classication model trained on the new la-beled set L t +1 = L t [ S , and H ( y j x P ( y j x j ; w t +1 ) , such that The proposed acti ve learning strate gy is to select the batch of instances that has the highest score. labels for instances S are not kno wn when the selection is conducted. One typical solution for this problem is to use the expected f ( S ) score computed under the current conditional model specied by w t Ho we ver, using P ( y ready exists in the current classication model w t , since it has been trained on a very small labeled beled instances S can achie ve over all possible label congurations. This optimistic scoring function can be written as Thus the problem becomes how to select a set of instances S that achie ves the best optimistic f ( S ) score dened in (4). Although this problem can be solv ed using an exhausti ve search on all size m subsets, S , of the unlabeled set U , it is intractable to do so in practice since the search space is exponentially lar ge. Explicit heuristic search approaches seeking a local optima do not exist either , since it is hard to dene an efcient set of operators that can transfer from one position to another one within the search space while guaranteeing impro vements to the optimistic score.
 Instead, in this paper we propose to approach the problem by formulating optimistic batch mode acti ve learning as an explicit mathematical optimization. Given the labeled set L t and unlabeled set U best score dened in (4). To do so, we rst introduce a set of f 0 ; 1 g -valued instance selection vari-ables . In particular , is a j U t j 2 sized indicator matrix, where each row vector for iteration t + 1 can be formulated as the follo wing optimization problem column vector with all 1s; 1 is a j U t j -entry column vector with all 1s; E is a U t 2 sized matrix with all 1s; is matrix inner product; is a user -pro vided parameter that controls class balance during instance selection; and is a parameter that we will use later to adjust our belief in the guessed labels. Note that, the selection variables not only choose instances from U t , but also select labels for the selected instances. Solving this optimization yields the optimal for instance selection in iteration t + 1 .
 The optimization problem (5) is an inte ger programming problem that produces equi valent results to using exhausti ve search to optimize (4), except that we have additional class balance constraints ( 9 ). Inte ger programming is an NP-hard problem. Thus, the rst step toward solving this problem in practice is to relax it into a continuous optimization by replacing the inte ger constraints (6) with continuous constraints 0 1 , yielding the relax ed formulation If we can solv e this continuous optimization problem, a greedy strate gy can then be used to reco ver the inte ger solution by iterati vely setting the lar gest non-inte ger value to 1 with respect to the (10) is not a conca ve function of . 1 Ne vertheless, standard continuous optimization techniques can be used to solv e for a local maxima. 4.2 Quasi-Newton Method To deri ve a local optimization technique, consider the objecti ve function (10) as a function of the instance selection variables As noted, this function is non-conca ve, therefore con venient con vex optimization techniques that achie ve global optimal solutions cannot be applied. Ne vertheless, a local optimization approach exploiting quasi-Ne wton methods can quickly determine a local optimal solution . Such a local optimization approach iterati vely updates to impro ve the objecti ve (15), and stops when a local maximum is reached. At each iteration, it mak es a local mo ve that allo ws it to achie ve the lar gest impro vement in the objecti ve function along the direction decided by cumulati ve information ob-tained from the sequence of local gradients. Suppose rst deri ve a second-order Taylor approximation ~ f ( ) for the objecti ve function f ( ) at where vec ( ) is a function that transforms a matrix into a column vector , and r f H original optimization function f ( ) is smooth, the quadratic function ~ f ( ) can reasonably approx-imate it in a small neighborhood of a quadratic programming with the objecti ve ( 16 ) and linear constraints (11), (12), (13) and (14). Suppose the optimal solution for this quadratic program is ~ d , w t +1 has to be retrained on L t [ S to evaluate the new objecti ve value, since S is determined by . In order to reduce the computational cost, we approximate the training of w t +1 in our empirical study , by limiting it to a few Ne wton-steps with a starting point given by w t trained only on L t . The remaining issue is to compute the local gradient r f ( assume w t +1 remains constant with small local updates on . Thus the local gradient can be ap-proximated as and therefore r f ( (Bro yden-Fletcher -Goldf arb-Shanno) to compute the Hessian matrix, which starts as an identity matrix for the rst iteration, and is updated in each iteration as follo ws [15 ] where y from the sequences of local gradients to help determine better update directions. 4.3 Adjustment Strategy In the discriminati ve optimization problem formulated in Section 4.1, the variables are used to optimistically select both instances and their labels, with the goal of achie ving the best classication partition (clustering) information contained in the lar ge unlabeled set is inconsistent with the true classication, the labels optimistically guessed for the selected instances through might not match the underlying true labels. When this occurs, the instance selected will not be very useful for iden-tifying the true classication model. Furthermore, the unlabeled data might continue to mislead the next instance selection iteration.
 Fortunately , we can immediately identify when the process has been misled once the true labels for the selected instances have been obtained. If the true labels are dif ferent from the labels guessed by the optimization, we need to mak e an adjustment for the next instance selection iteration. We have that the being-misled problem is caused by the unlabeled data, which affects the tar get classication model through the term P the parameter . Specically , at the end of each iteration t , we obtain the true labels y selected instances S , and compare them with our guessed labels ^ y consistent, we will set = 1 , which means we trust the partition information from the unlabeled data as same as the label information in the labeled data for building the classication model. If y
S 6 = ^ y S data for the next selection iteration t + 1 . We use a simple heuristic procedure to determine the value in this case. Starting from = 1 , we then multiplicati vely reduce its value by a small factor , 0 : 5 , until a better objecti ve value for (15) can be obtained when replacing the guessed indicator variables with the true label indicators. Note that, if we reduce to zero, our optimization problem will be exactly equi valent to picking the most uncertain instance (when m = 1 ). To investig ate the empirical performance of the proposed discriminati ve batch mode acti ve learning algorithm ( Discriminative ), we conducted a set of experiments on nine two-class UCI datasets, com-paring with a baseline random instance selection algorithm ( Random ), a non-batch myopic acti ve learning method that selects the most uncertain instance each time ( MostUncertain ), and two batch mode acti ve learning methods proposed in the literature: svmD , an approach that incorporates diver-sity in acti ve learning with SVM [2]; and Fisher , an approach that uses Fisher information matrix for instance selection [9]. The UCI datasets we used include (we sho w the name, follo wed by the num-ber of instances and the number of attrib utes): Australian(690;14), Cle ve(303;13), Corral(128;6), Crx(690;15), Flare(1066;10), Glass2(163;9), Heart(270;13), Hepatitis(155;20) and Vote(435;15). We consider a hard case of acti ve learning, where only a few labeled instances are given at the start. In each experiment, we start with four randomly selected labeled instances, two in each class. We then randomly select 2/3 of the remaining instances as the unlabeled set, using the remaining instances for testing. All the algorithms start with the same initial labeled set, unlabeled set and testing set. For a x ed batch size m , each algorithm repeatedly select m instances to label each time. In this section, we report the experimental results with m = 5 , averaged over 20 times repetitions. Figure 1 sho ws the comparison results on the nine UCI datasets. These results suggest that although the baseline random sampling method, Random , works surprisingly well in our experiments, the proposed algorithm, Discriminative , always performs better or at least achie ves a comparable per -formance. Moreo ver, Discriminative also apparently outperforms the other two batch mode algo-rithms, svmD and Fisher , on ve datasets X Australian, Cle ve, Flare, Heart and Hepatitis, and reaches a tie on two datasets X Crx and Vote. The myopic most uncertain selection method, MostUncertain , sho ws an overall inferior performance to Discriminative on Australian, Cle ve, Crx, Heart and Hep-atitis, and achie ves a tie on Flare and Vote. Ho we ver, Discriminative demonstrates weak perfor -mance on two datasets X Corral and Glass2, where the evaluation lines for most algorithms in the gures are strangely very bump y. The reason behind this remains to be investig ated.
 These empirical results suggest that selecting unlabeled instances through optimizing the classi-cation model directly would obtain more rele vant and informati ve instances, comparing with using heuristic scores to guide the selection. Although the original optimization problem formulated is NP-hard, a relax ed local optimization method that leads to a local optimal solution still works effec-tively . In this paper , we proposed a discriminati ve batch mode acti ve learning approach that exploits in-formation in unlabeled data and selects a batch of instances by optimizing the tar get classication model. Although the proposed technique could be overly optimistic about the information presented by the unlabeled set, and consequently be misled, this problem can be identied immediately after obtaining the true labels. A simple adjustment strate gy can then be used to rectify the problem in the follo wing iteration. Experimental results on UCI datasets sho w that this approach is generally more effecti ve comparing with other batch mode acti ve learning methods, a random sampling method, and a myopic non-batch mode acti ve learning method. Our current work is focused on 2-class clas-sication problems, howe ver, it is easy to be extended to multiclass classication problems. [1] K. Bennett and E. Parrado-Hernandez. The interplay of optimization and machine learning [2] K. Brink er. Incorporating diversity in acti ve learning with support vector machines. In Pro-[3] C. Campbell, N. Cristianini, and A. Smola. Query learning with lar ge mar gin classiers. In [4] D. Cohn, Z. Ghahramani, and M. Jordan. Acti ve learning with statistical models. Journal of [5] Y. Freund, H. S. Seung, E. Shamir , and N. Tishby . Selecti ve sampling using the query by [6] Y. Grandv alet and Y. Bengio. Semi-supervised learning by entrop y minimization. In Advances [7] Y. Guo and R. Greiner . Optimistic acti ve learning using mutual information. In Proceedings [8] S. Hoi, R. Jin, and M. Lyu. Lar ge-scale text cate gorization by batch mode acti ve learning. In [9] S. Hoi, R. Jin, J. Zhu, and M. Lyu. Batch mode acti ve learning and its application to med-[10] D. Le wis and W. Gale. A sequential algorithm for training text classiers. In Proceedings of the [11] A. McCallum and K. Nig am. Emplo ying EM in pool-based acti ve learning for text classica-[12] T. Minka. A comparison of numerical optimizers for logistic regression. Technical report, [13] I. Muslea, S. Minton, and C. Knoblock. Acti ve + semi-supervised learning = rob ust multi-vie w [14] H. Nguyen and A. Smeulders. Acti ve learning using pre-clustering. In Proceedings of the 21st [15] J. Nocedal and S.J. Wright. Numerical Optimization . Springer , Ne w York, 1999. [16] N. Ro y and A. McCallum. Toward optimal acti ve learning through sampling estimation of [17] G. Schohn and D. Cohn. Less is more: Acti ve learning with support vector machines. In [18] S. Tong and D. Koller . Support vector machine acti ve learning with applications to text classi-[19] Z. Xu, K. Yu, V. Tresp, X. Xu, and J. Wang. Representati ve sampling for text classication us-[20] C. Zhang and T. Chen. An acti ve learning frame work for content-based information retrie val. [21] J. Zhu and T. Hastie. Kernel logistic regression and the import vector machine. Journal of [22] X. Zhu, J. Laf ferty , and Z. Ghahramani. Combining acti ve learning and semi-supervised learn-
