 c if c% of transactions in T that support X also support Y. The rule has supports in T if s% of the transactions in T contains X u Y. mining association rules is to discover all association rules that have support and confidence greater than the user-specified minimum support (called minsup) and minimum confidence (called minconj). I. generate all large itemsets that satisfy minsup. 2. generate all association rules that satisfy minconf using the An itemset is simply a set of items. A large itemset is an itemset that has transaction support above minsup. studies, however, has always been the same, i.e., finding all rules that satisfy user-specified minimum support and minimum confidence constraints. is the minsup. It is used to prune the search space and to limit the number of rules generated. However, using only a single minsup implicitly assumes that all items in the data are of the same nature (to be explained below) and/or have similar frequencies in the database. This is often not the case in real-life applications. In many applications, some items appear very frequently in the data, while others rarely appear. If the frequencies of items vary a great deal, we will encounter two problems: 1. If minsup is set too high, we will not find those rules that 2. In order to find rules that involve both frequent and rare items, This dilemma is called the rure item problem [9]. When confronted with this problem in applications, researchers either split the data into a few blocks according to the frequencies of the items and then mine association rules in each block with a different minsup [6], or group a number of related rare items together into an abstract item so that this abstract item is more As mentioned, existing algorithms for mining association rules typically consists of two steps: (1) finding all large itemsets; and (2) generating association rules using the large itemsets. focused on the first step since it is computationally more expensive. Also, the second step does not lend itself as well to smart algorithms as confidence does not possess closure property. Support, on the other hand, is downward closed. If a set of items satisfies the minsup, then all its subsets also satisfy the minsup. Downward closure property holds the key to pruning in all existing mining algorithms. level-wise search [3]. Let k-itemset denote an itemset with k items. At level 1, all large I-itemsets are generated. At level 2, all large 2-itemsets are generated and so on. If an itemset is not large at level k-l, it is discarded as any addition of items to the set cannot be large (downward closure property). All the potentially large itemsets at level k are generated from large itemsets at level k-l. algorithm to find all large itemsets, the downward closure property no longer holds. Example 3: Consider four items 1, 2, 3 and 4 in a database. Their minimum item supports are: 
If we find that itemset { 1, 2) has 9% of support at level 2, then it does not satisfy either MIS(l) or MIS(2). Using an existing algorithm, this itemset is discarded since it is not large. Then, the potentially large itemsets (1, 2, 3) and (1, 2, 4) will not be generated for level 3. Clearly, itemsets [ 1, 2, 3) and ( 1, 2, 4) may be large because MIS(3) is only 5% and MIS(4) is 6%. It is thus wrong to discard ( 1, 2). But if we do not discard ( 1, 2}, the downward closure property is lost. Below, we propose an algorithm to generate large itemsets that satisfy the sorted closure proper-v (see Section 3.3), which solves the problem. The essential idea is to sort the items according to their MIS values in ascending order to avoid the problem. The proposed algorithm generalizes the Apriori algorithm for finding large itemsets given in [3]. We call the new algorithm, MSapriori. When there is only one MIS value (for all items), it reduces to the Apriori algorithm. wise search. It generates all large itemsets by making multiple passes over the data. In the first pass, it counts the supports of individual items and determines whether they are large. In each subsequent pass, it starts with the seed set of itemsets found to be large in the previous pass. It uses this seed set to generate new possibly large itemsets, called candidate itemsets. The actual supports for these candidate itemsets are computed during the pass over the data. At the end of the pass, it determines which of the candidate itemsets are actually large. However, there is an important exception in the second pass as we will see later. items in I in ascending order of their MIS values. This ordering is used in all subsequent operations of the algorithm. The items in each itemset also follow this order. For example, in Example 3 of the four items 1, 2, 3 and 4, and their given MIS values, the items are sorted as follows: 3, 4, 1, 2. This ordering helps to solve the problem identified in Section 3.1. 
Example 5: Let us continue with Example 4. We obtain, &lt;1,2&gt; is not a candidate 2-itemset because the support count of item 1 is only 9 (or 9%), which is less than MIS(l) (= 10%). Hence, &lt;1,2&gt; cannot be large. 
Note that we must use F rather than L, because L, does not contain those items that may satisfy the MIS of an earlier item (in the sorted order) but not the MIS of itself (see the difference between F and L, in Example 4). Using F, the problem discussed in Section 3.1 is solved for Ca. Correctness of level2-candidate-gen: see [7]. 
Let us now present the candidate-gen function. It performs a similar task as apriori-gen in Apriori algorithm [3]. candidate-gen takes as argument Lk., (k &gt; 2) the set of all large (k-l)-itemsets, and returns a superset of the set of all large k-itemsets. It has two steps, the join step and the prune step. The join step is the same as that in the apriori-gen function. The prune step is, however, different. The join step is given below. It joins Lk., with Lt.,: insert into C, select pitem,, pitem*, . . ., pitem,.,, q.itemk., from 41 P, &amp;I 9 where p.itemi = qitemi , . . . . p.itemkMz = q.itemk.a, 
Basically, it joins any two itemsets in Lk., whose first k-2 items are the same, but the last items are different. 
After the join step, there may still be candidate itemsets in C, that are impossible to be large. The prune step removes these itemsets. This step is given below: 1 for each itemset c E C, do 2 for each (k-l)-subset s of c do 3 if (c[l] E s) or (MIS(c[2]) = MIS(c[ 11)) then 4 if (s G-Z Lkel) then delete c from Ck; 
It checks each itemset c in C, (line 1) to see whether it can be deleted by finding its (k-1)-subsets in Lk.,. For each (k-l)-subset s in c, if s is not in Lk.,, c can be deleted. However, there is an exception, which is when s does not include c[l] (there is only one such s). This means that the first item of c, which has the lowest MIS value, is not in s. Then, even if s is not in Lk.,, we cannot delete c because we cannot be sure that s does not satisfy 
MIS(c[l]), although we know that it does not satisfy MIS(c[2]), unless MIS(c[2]) = MIS(c[ 11) (line 3). sorted order. After the join step, C, is The prune step deletes the itemset ~1, 4, 5, 6&gt; because the 5&gt; is not in L3 because the minimum support for ~3, 4, 5&gt; is MIS(3), which may be higher than MIS( 1). Although ~3, 4, 5&gt; does not satisfy MIS(3), we cannot be sure that it does not satisfy MIS(l) either. However, if we know MIS(3) = MIS(l), then &lt;1,3,4,5&gt; can also be deleted. Correctness of candidate-gen: See [7]. 
The problem discussed in Section 3.1 is solved for C, (k &gt; 2) because due to the sorting we do not need to extend a large (k-l)-itemset with any item that has a lower MIS value, but only an item with a higher (or equal) MIS value. Such itemsets are said to have satisfied the sorted closure property. 
The subset function checks to see which itemsets in C, are in transaction t. Itemsets in C, are stored in a tree similar to that in experiment. Again the three thick lines give the number of candidate itemsets using the existing approach of a single minsup at 0. l%, 0.2% and 0.3% respectively. significantly reduced by our method when CL is not too large. When a becomes larger, the number of large itemsets found by our method gets closer to that found by the single minsup method. The reason is because when a becomes larger more and more items X  MIS values reach LS. From our experiences, the user is usually satisfied with the large itemsets found at a = 4. At (II = 4 and LS = 0.2%, for example, the number of large itemsets found by our method is less than 61% of that found by the single minsup method. From Figure 2, we see that the corresponding numbers of candidate itemsets are also much less. The execution times are roughly the same (hence are not shown here) because database scan dominates the computation in this experiment. Below, we will see that for our real-life data set, the reductions in both the number of large itemsets found and the number of candidate itemsets used are much more remarkable because the item frequencies in our real-life data set vary a great deal. The execution times also drop drastically because the data set is small and the computation time is dominated by the itemsets generation. We tested the algorithm using a number of real-life data sets. Here, we only use one application data set. The results with the others are similar. details of the application. Here, we only give the characteristics of the data. The data set has 5.5 items and 700 transactions. Each rules can have one minsup, and at the level of milk, cheese, pork and beeJ there can be a different minsup. This model is essentially the same as the original model in [2] because each level has its own association rules involving items of that level. 
Our proposed model is more flexible as we can assign a MIS value for each item. [13] presents a generalized multiple-level association rule mining technique, where an association rule can involve items at any level of the hierarchy. However, the model still uses only one minsup. 
It is easy to see that our algorithm MSapriori is a generalization of the Apriori algorithm [3] for single minsup mining. That is, when all MIS values are the same as LS, it reduces to the Apriori algorithm. A key idea of our algorithm 
MSapriori is the sorting of items in I according to their MIS values in order to achieve the closure property. Although we still use level-wise search, each step of our algorithm is different from that of algorithm Apriori, from initialization, candidate itemsets generation to pruning of candidate itemsets. 
This paper argues that a single minsup is insufficient for association rule mining since it cannot reflect the natures and frequency differences of the items in the database. In real-life applications, such differences can be very large. It is neither satisfactory to set the minsup too high, nor is it satisfactory to set it too low. This paper proposes a more flexible and powerful model. It allows the user to specify multiple minimum item supports. This model enables us to found rare item rules yet without producing a huge number of meaningless rules with frequent items. The effectiveness of the new model is shown experimentally and practically. [l] Aggarwal, C., and Yu, P.  X  X nline generation of association [2] Agrawal, R., Imielinski, T., Swami, A.  X  X ining association [3] Agrawal, R. and Srikant, R.  X  X ast algorithms for mining [4] Brin, S. Motwani, R. Ullman, J. and Tsur, S.  X  X ynamic [S] Han, J. and Fu, Y.  X  X iscovery of multiple-level association [6] Lee, W., Stolfo, S. J., and Mok, K. W.  X  X ining audit data [7] Liu, B., Hsu, W. and Ma, Y. Mining association rules with [8] Liu, B., Hsu, W. and Ma, Y.  X  X runing and Summarizing the [9] Mannila, H.  X  X atabase methods for data mining. X  KDD-98 [lo] Ng. R. T. Lakshmanan, L. Han, J.  X  X xploratory mining and [I I] Park, J. S. Chen, M. S. and Yu, P. S.  X  X n effective hash [12] Rastogi, R. and Shim, K.  X  X ining optimized association [13] Srikant, R. and Agrawal, R.  X  X ining generalized [14] Srikant, R., Vu, Q. and Agrawal, R.  X  X ining association 
