 Finding topic experts on microblogging sites with millions of users, such as Twitter, is a hard and challenging problem. In this paper, we propose and investigate a new methodol-ogy for discovering topic experts in the popular Twitter so-cial network. Our methodology relies on the wisdom of the Twitter crowds  X  it leverages Twitter Lists , which are often carefully curated by individual users to include experts on topics that interest them and whose meta-data (List names and descriptions) provides valuable semantic cues to the ex-perts X  domain of expertise. We mined List information to build Cognos , a system for finding topic experts in Twitter. Detailed experimental evaluation based on a real-world de-ployment shows that: (a) Cognos infers a user X  X  expertise more accurately and comprehensively than state-of-the-art systems that rely on the user X  X  bio or tweet content, (b) Cog-nos scales well due to built-in mechanisms to efficiently up-date its experts X  database with new users, and (c) Despite relying only on a single feature, namely crowdsourced Lists, Cognos yields results comparable to, if not better than, those given by the official Twitter experts search engine for a wide range of queries in user tests. Our study highlights Lists as a potentially valuable source of information for future content or expert search systems in Twitter.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: selection process; H.3.5 [On-line Information Services]: Web-based services General Terms: Algorithms, Design, Experimentation. Keywords: Twitter, topic experts, Lists, crowdsourcing, hubs, authorities. Microblogging sites, out of which Twitter is the most popu-lar, have emerged as an important platform for exchanging real-time information on the Web. Recent estimates sug-gest that 200 million active Twitter users post 150 million tweets (messages) daily [1]. These messages contain a wide variety of information, varying from conversational tweets to highly relevant information on niche topics. The users posting these messages range from globally popular news organizations and celebrities to locally popular community organizers or activists and from domain experts in fields like computer science and astrophysics to spammers that fake the identities of well-known users.

As a result, the quality of information posted in Twitter is highly variable and finding the users that are recognized sources of relevant and trustworthy information on specific topics (i.e., topic experts) is an important challenge. Iden-tifying topic experts is also the first step towards finding authoritative information on the topic. Recognizing this, Twitter itself has created a topical expert search system (known as the Twitter Who To Follow (WTF) service [16]). However, as we show later in this paper, the results from this service leave a lot of scope for improvement.
In this paper, we present Cognos , a system for finding topic experts in Twitter. Cognos is based on a new method-ology for inferring users X  expertise. Traditional approaches to identify topical experts in Twitter rely either on the in-formation provided by the user herself (e.g., in the  X  X io X  or short autobiography of the Twitter account) [17] or on an-alyzing the network characteristics and tweeting activity of users [10, 19]. Cognos takes an entirely different approach to identify topical experts in Twitter utilizing crowdsourced topical annotation of experts. Specifically, Cognos exploits the Lists feature in Twitter, using which any user can group Twitter accounts that tweet on a topic that is of interest to her, and follow their collective tweets. We observe that many users carefully create Lists to include other Twitter users who they consider as experts on a given topic. Fur-thermore, they generate meta-data, such as List names and descriptions, that provide valuable semantic cues to the top-ical expertise of the users included in the List. Our key idea is to analyze the meta-data of the Lists containing a user to infer the user X  X  topics of expertise, which in turn enables us to identify topical experts.

To build Cognos, we address three key challenges: (1) How to accurately and comprehensively infer an individual user X  X  topics of expertise from Lists? (2) How to rank the relative expertise of different users identified as experts on a given topic?, and (3) How to crawl the Lists meta-data for hun-dreds of millions of Twitter users efficiently and scalably? The main contributions of this paper lie in the methodolo-g ies we propose to tackle the above challenges.

We present an extensive evaluation of Cognos based on user feedback obtained using a real-world deployment, which can be accessed at http://twitter-app.mpi-sws. org/whom-to-follow/ . We summarize here a few highlights from our evaluation: We find that Cognos performs as well as or better than the official Twitter WTF service for more than 52% of the queries. Cognos yields particularly better search results in those cases in which experts do not pro-vide account bios, or whose bios do not contain information about the user X  X  topic of expertise. Moreover, Cognos rarely produces entirely irrelevant results, unlike the Twitter WTF service whose top results, at times, include a few users who are not at all related to the given query, but whose names or bios contain the terms in the query. Furthermore, as Cognos is based on the use of a single and simple feature (Twitter Lists), it is more scalable than prior approaches, which use computationally intensive machine learning algorithms over graph and content-based metrics [10,19]. As the number of users and information shared in Twitter grows exponentially, information retrieval techniques, such as search and recommender systems, are increasingly being used to find trending topics, interesting users, and valuable content [14,16]. A critical component of such systems con-sists of identifying users who are important sources of infor-mation on specific topics (topical experts).

There have been several attempts to measure the influence of Twitter users and thereby identify influential users or ex-perts [3, 4, 8, 12]. However, none of these efforts attempts to identify experts in any specific topic . To our knowl-edge, there have been only two notable efforts that have approached the problem of identifying experts in specific topics [10,19]. Weng et. al. [19] proposed a Page-Rank like algorithm TwitterRank that uses both the Twitter graph and processed information from tweets to identify experts in particular topics. On the other hand, Pal et. al. [10] used clustering and ranking on more than 15 features extracted from the Twitter graph and the tweets posted by users.
Apart from the above research studies, there also exist some services for identifying topical experts in Twitter. Rec-ognizing the importance of searching for experts on spe-cific topics, Twitter itself provides an official  X  X ho to fol-low X  (WTF) service [16] where one can search for experts on a given topic (query). Though the exact details of im-plementation of the service are not publicly known, it is reported [17] that Twitter WTF uses several factors such as the profile information (e.g., name and bio) of users, their social links, their level of engagement in Twitter, and so on to identify topical experts.

All the above approaches primarily rely on the information provided by the user herself (e.g., her account name and bio, the tweets posted by her) and her social graph to infer the topics in which she is an expert. In contrast, the present work relies on the  X  X isdom of the Twitter crowd X  (i.e., how others describe this user), collected through crowdsourced Lists. Further, all of the above mentioned research studies use fixed Twitter datasets collected at a certain point in time. To the best of our knowledge, this study is the first to address the practical challenge of keeping an OSN-based search / recommender system up-to-date, a challenge that has become essential given the phenomenal growth rate of user populations in today X  X  OSNs [2].

Finally, a few prior studies have used Twitter Lists for different purposes, such as, to identify popular users in a few specific topics (e.g., celebrities, bloggers) and study how they interact with the rest of the Twitter population [20], or to serve as seed nodes for algorithms like topic-sensitive Pagerank [18], or to contextualize individual users [11]. The present study uses Lists for a fundamentally different pur-pose, which is to find topic experts. In a prior workshop paper [13], we described how we can use Lists to infer topics of expertise for individual Twitter users. In this paper, we not only extend the work to build a fully functional search system, but also present an in-depth evaluation of the qual-ity of List-based expertise inference and ranking. In this section, we first propose our methodology for finding topic experts using Twitter Lists . Later we identify the key design challenges in designing a search system based on the methodology. Twitter introduced Lists in late 2009, to help users organize their followings (i.e., the people whom a user follows) and the information they post [7]. By creating a List, a user can group other Twitter users, and view the aggregated tweets posted by all the listed users in the List timeline. When creating a List, a user typically provides a List name (free text, limited to 25 characters) and optionally adds a List description. For instance, a user can create a List named  X  X elebrities X  and add celebrities to this List. Then, the user can view tweets posted by these celebrities in the List time-line.

Table 1 presents illustrative examples of Lists, extracted from our dataset (to be detailed in Section 5.2). The key observation here is that the List names and descriptions pro-vide valuable semantic cues to the topics of expertise of the members of the Lists. For example, using List meta-data, we can associate BarackObama with politics and politicians , Eminem with music and musicians , and Daniel Tunkelang with SIGIR . Thus, Lists provide a way to annotate Twitter users with their topics of expertise. Interestingly, these an-notations are generated by arbitrary Twitter users and so they reflect the collective wisdom of the crowds.
Our methodology relies on extracting the information con-tained in the crowdsourced Lists to build an expert search system . Specifically, it has three parts: (i) gather crowd-created Lists for all Twitter users, (ii) mine List meta-data to infer the topical expertise of individual Twitter users, and (iii) for a given query topic, rank the relative expertise of the users whose topical expertise matches the query. Our proposed methodology for building a search system for experts in Twitter raises a number of important questions and key design challenges, which we enumerate below: 1. How to infer users X  topics of expertise from Lists? Do 2. How to rank the relative expertise of different users 3. How to crawl the Lists meta-data for tens of millions of We address the above research challenges in each of the sub-sequent sections. In Section 4, we describe how we use Lists to infer the topics of expertise of individual Twitter users. Section 5 presents Cognos, a topical expert search system for Twitter that leverages the topical expertise inferred us-ing Lists to identify experts on a given topic and rank them. In Section 6, we propose efficient strategies that minimize the number of Lists that we need to crawl to keep the Cog-nos system up-to-date. We conduct an extensive evaluation of our proposals by comparing their performance with two systems: (i) the state-of-the-art research system for iden-tifying topical experts in Twitter [10], and (ii) the official Twitter Who-To-Follow service [16]. In this section, we first describe our methodology of inferring the expertise of individual Twitter users and then evaluate the accuracy and expressiveness of the inferred expertise. As described in our prior workshop paper [13], our strat-egy consists of extracting frequently occurring topics (words) from the List meta-data (names and description) and associ-ating these topics with the listed users. The intuition behind our strategy is that a user listed by many other users un-der a certain topic is very likely to be an expert on that topic. Previous efforts that analyzed Twitter Lists showed that nouns and adjectives in List names and descriptions are particularly useful for this purpose [11]. So our strategy to extract topics from List meta-data consists of the following steps: 1. Since List names cannot exceed 25 characters, users often combine multiple words using CamelCase (e.g., TennisPlay-ers). We separate these into individual words. 2. We apply common language processing techniques such as case-folding, stemming, and removal of stop-words. In ad-dition to the common stop-words, a set of domain-specific stop-words are also filtered out, e.g., Twitter, List, and For-mulist (a tool frequently used to automatically create Lists). 3. Then, we identify nouns and adjectives using a part-of-speech tagger. 4. As a significant fraction of List names and descrip-tions are in languages other than English, we group together words that are very similar to each other based on edit-distance among words, e.g., politics and politica , journalist and jornalistas , etc. It can be noted that these words are not unified even by a standard stemmer. 5. As List names and descriptions are typically short, we consider only unigrams and bigrams as topics.
 The above strategy produces a set of topics for each user, as well as the frequency with which a topic appears in the names and descriptions of the Lists containing the user. When evaluating the quality of inferred topics of expertise, we check for two metrics: (i) accuracy  X  is the user really an expert in the inferred topics? (ii) expressiveness  X  do Lists comprehensively capture all the different topics in which a user has expertise?
For our evaluation, we need to gather ground truth in-formation about some Twitter users X  expertise. Since such ground truth is difficult to obtain for a random set of Twitter users, we consider the following strategies: First, we evalu-ate for a select set of popular users whose true topics of expertise are generally well-known or easily verifiable. Sec-ond, for a given set of topics, we collect the top experts identified by the state-of-the-art research system for iden-tifying topical authorities [10], and by the official Twitter WTF service [16]. We then check if our methodology iden-tifies these users as experts in the given topics. The results not only demonstrate the high quality of our expertise in-ference, but they also uncover drawbacks of the competing state-of-the-art methods. Table 2 shows the top 10 topics (inferred using our List-based method) for a few Twitter users whose expertise is well-known. Our inference is accurate and comprehensive not only for users with millions of followers, but also for users with hundreds or thousands of followers. For instance, for Mark Sanderson (Program Committee Chair at SIGIR 2012), even though his Twitter account is included in only 12 Lists , the inferred topics identify that he is a researcher in computer science ( X  X s X ), specializing in information retrieval, machine learning ( X  X l X ), search and so on. Again, for US senators (two examples shown in Table 2  X  Chuck Grassley and Claire McCaskill), our methodology could accurately identify a variety of topics, such as, their political party (Re-publicans / Democrats), their state, their gender ( X  X omen X  in case of Claire McCaskill), their political ideology (con-servative / progressive) and even a number of the senate committees of which each senator is a member ( X  X ealth X  in case of Chuck Grassley). We verified the accuracy of our inference using the Wikipedia pages for these people, and found them to be almost always accurate (see [13] for de-tails). Thus, List meta-data is often sufficiently rich to yield high quality expertise inference for a variety of users. Next we compare the extent to which the experts identified by a state-of-the-art research system built by Pal et. al. [10] can be recalled by our methodology. Pal et. al. use more than 15 features extracted from the Twitter social graph and the content of the tweets posted by users to identify topical experts. Though an implementation of this system is not publicly available, their paper lists the top 10 experts identified for three specific topics  X   X  X phone X ,  X  X il spill X  and  X  X orld cup X . We test whether the topics inferred by our methodology for these experts match with the topic reported by Pal et. al.

We find that for a majority of the top 10 experts  X  8 out of 10 for  X  X phone X , 7 out of 10 for  X  X orld cup X , and 6 out of 10 for  X  X il spill X   X  in each of the three topics, the set of topics inferred by us includes the topic for which they are reported by Pal et. al. Table 3 shows some of these experts, along with their bio.
 However, for the rest of the cases, the topics inferred using Lists do not contain the topic reported by Pal et. al. . Ta-ble 4 lists these users along with their bios. Examining their bio, it is evident that these users are, in fact, not specifi-cally related to the topic of the corresponding query. For example, a social media entrepreneur and technology blog-ger teedubya was identified as an expert on  X  X phone X , even though he is not a specialist on Apple products. Similarly, Reuters , CBSNews and channel4news are popular news me-dia that report on a variety of topics, including those that are not related to  X  X il spill X  or  X  X orld cup X . It is likely that the algorithm used by Pal et. al. identified these users as experts because a number of their tweets were related to the topic in question during the period when the evaluation was done.

It is worth noting that Pal et. al. had explicitly set out to discover topical experts that are not just overtly general and highly followed authorities like popular news media ac-counts. They highlight the discovery of dedicated specialists that mostly post tweets related to their specialization. Inter-estingly, our methodology has successfully recalled all such experts (i.e., 100% recall) even though it relies only on one feature, namely Lists. In comparison, Pal et. al. rely on 15 features, which indicates the relative advantages of using crowdsourced Lists to identify users X  expertise.
 The official Twitter Who-To-Follow (WTF) service [16] helps to search for topical experts for a given topic (query), and is reported to use several factors, such as, the pro-file information (e.g., name and bio) of users, their social links, and their level of engagement in Twitter [17]. As part of a user survey to evaluate our system (detailed in Sec-tion 5.3.3), we obtained the top 20 experts returned by the Twitter WTF service for a few hundred queries generated by users. We investigated the extent to which our methodology could recall these experts.

Out of the 3,495 distinct users returned by Twitter as top 20 results for some given query, the topics inferred us-ing Lists include the corresponding topic (word in the given query) for 83.4% (2,916) of the users. However, the topics inferred by the List-based methodology for the other 16.6% (579) users did not contain the topic (word) in the query. To understand these missing experts better, we manually verified 50 randomly selected users out of the 579 users.
We found 9 out of these 50 users (i.e., 18%) to be relevant experts on the query topics. Our List-based methodology in-fers topics very similar to the query, but none matching the exact query-word. Table 5 shows two such examples. For the Twitter account of the dineLA restaurant, the inferred topics include  X  X ood X  and  X  X estaurant X  but not the query-word  X  X ining X  (for which it was returned by Twitter WTF). Sim-ilarly, for the Twitter user HubbleHugger77 who is a space explorer and has directed the film  X  X aving Hubble X , we iden-tify  X  X pace X ,  X  X osmology X  and  X  X asa X  but not the query-word  X  X ubble X . This would appear to suggest that a user X  X  name and bio contain clues to the user X  X  expertise.

However, in 29 out of the 50 cases (i.e., 58%), we found that the official Twitter WTF service returns wrong results, i.e., the returned user is not at all related to the topic of the query for which she is returned. We conjecture that this is because the query-word appears in the name or bio of the user. For instance, the well-known comedian Jimmy Fallon has (mockingly) described himself as an astrophysicist in his bio, as a result of which he shows up in the top 20 Twitter WTF results for the query  X  X strophysicist X . Table 5 shows other examples of users who are wrongly included within the top 20 results returned by Twitter WTF. We were not able to infer the relevance of the expert to the query in the remaining 12 out of the 50 (24%) manually verified user accounts, as we found the query to be ambiguous.

Thus, not only does our methodology recall a vast major-ity (83.4%) of the experts identified by Twitter WTF, but also a majority of the missing experts were incorrectly iden-tified by Twitter. Our List-based methodology fails to recall only a small fraction of experts who are actually related to the given query, and even in those cases, we identify topics that are semantically quite similar to the query word. Our evaluation demonstrates that our proposed methodol-ogy of utilizing crowdsourced List meta-data provides an ac-curate and comprehensive inference of topics of expertise of individual Twitter users. We also show that in many cases, the List-based methodology is more accurate compared to the existing techniques of inferring topics of a user from her profile or her tweets. In the next section, we describe how we utilize the topics inferred using Lists, to build a search system for topical experts in Twitter. In this section, we leverage our previously discussed method-ology to infer users X  expertise to build Cognos 1 , a search system for topical experts in Twitter. Cognos uses crowd-sourced Lists as the only source of information and so its performance illustrates the potential uses of Lists in finding experts. We first describe how we rank experts in Cognos and then present an extensive evaluation of the Cognos sys-tem. Ranking of users related to a given topic is a well-studied problem, and over the years, several ranking algorithms have been proposed for the Web [6], online topical communi-ties [21], and even for topical experts in Twitter [10,19]. The expert ranking schemes in Twitter take into account several metrics extracted from the social graph and the content of the tweets posted by users. In contrast, we decided to evalu-ate a ranking scheme that is based solely on the Lists feature, since one of our objectives is to evaluate crowdsourced Lists 1 T he name is derived from the word cognoscenti , i.e., people who are considered to be especially well informed about a particular topic. as the only source of information for topical experts  X  we have already shown that Lists can be used to accurately in-fer topics of expertise, now we investigate whether Lists are also an effective metric to rank topical experts.
The method described in Section 4.1 gives for each indi-vidual user, a topic vector { ( t i , f i ) } , where t i topics inferred for the user, and f i is the frequency of occur-rence of topic t i in the names and descriptions of the Lists containing the user. Given a query, we compute a topical similarity score between this topic vector for a user and the given query vector, using the algorithm in [5] which com-putes the cover density ranking between the vectors. We chose this similarity score (which is suited to queries con-taining one to three terms) since queries to expert search systems are almost always short, hence using cosine simi-larity on tf-idf based representations may not be very effec-tive [9,10]. Finally, we multiply the topical similarity score for a user with the logarithm of the number of Lists contain-ing the user  X  the intuition behind this is that a user who is included in more number of Lists (by other users) is likely to be more popular in Twitter.

Thus, given a query (topic), Cognos identifies the set of experts related to the topic using the List-based methodol-ogy discussed in Section 4.1, and then ranks them using the algorithm described above. In the remainder of this section, we evaluate this List-based methodology of identifying and ranking topical experts in Twitter. To populate the Cognos expertise database, we started to crawl all the Lists containing all Twitter users. We quickly realized that a brute-force crawl of all Lists for all users would be prohibitively expensive and would not scale. So we only crawled the Lists containing all the 54 million Twitter users in a complete snapshot of the Twitter social network taken in August 2009 [4]. This is only a small fraction of the estimated 465 million Twitter users as of January 2012 [2]. We address the challenge of crawling Lists efficiently and scalably to include experts that joined after 2009, in Sec-tion 6.

Of the 54 million Twitter users, we found that 6,843,466 users have been listed at least once. In order to reliably infer topical expertise of a user from Lists, it is important that a user has been listed at least a few times. So we considered only those users who were listed at least 10 times. There were 1,333,126 users listed more than 10 times. Some of these users were listed more than a hundred thousand times. Due to rate-limitations in accessing the Twitter API, we limited our data collection to at most 2000 Lists per user. Overall, for the 1.3 million users, we gathered a total of 88,471,234 Lists. Out of these, 30,660,140 (34.6 %) Lists had a description, while the others only had the List name. Judgments on the quality of the results returned by a search system are to an extent subjective. So we chose to evaluate Cognos through an user study where a set of human eval-uators judged the relevance of the results returned by Cog-nos, using a web-based feedback service (available at http: //twitter-app.mpi-sws.org/whom-to-follow/ ). We also gathered another set of user evaluations where the results returned by Cognos were directly compared with those re-turned by the official Twitter WTF service [16]. We also compared the top experts returned by Cognos with those returned by the state-of-the-art research system [10].
The above URL was publicly advertised to a few hun-dred people at each of the three home institutions of the authors, which are located across three different continents. We preferred such an in-the-wild evaluation (instead of a controlled evaluation with a fixed set of evaluators and few selected queries, as used by [10]) as it resembles a real-world deployment of the search system. In this evaluation, an evaluator issues a query, for which she is shown the top 10 results returned by Cognos. Then the evaluator gives a binary judgment on each of the top 10 results as to whether it is relevant to the given query. The queries used for the evaluations could be selected from a given set of 55 sample queries spread over the 10 categories shown in Table 6. In the rest of this section, we use the term  X  X valuation X  to indicate a relevant / non-relevant judgment for an individual result (topical expert) given by Cognos for a particular query.
 top 10 results for the 55 sample queries, out of which 1,680 (i.e., 78.7%) judged the result to be relevant to the query. all 10 results for a particular query.
 We found that the fraction of evaluations that judged a re-sult as relevant, for each individual rank out of the top 10 (i.e., considering the results shown at a certain rank for any of the 55 queries) to be largely invariant  X  the top 4 results were judged to be relevant in more than 80% of the evalua-tions, while the results ranked 5 X 10 were judged relevant in more than 75% of the evaluations.

Next we examined the Cognos results that received the 456 (i.e., 21.3%)  X  X on-relevant X  judgments. We found a large amount of subjectivity in these judgments driven by whether a particular evaluator recognizes a Twitter user as a top expert on a given topic. We found a number of cases where the same result for the same query was judged relevant by some evaluators and non-relevant by others. For example, for the query  X  X loud computing X , Werner Vogels, who is one of the principal architects of Amazon X  X  approach to cloud computing, was rated as relevant in 4 evaluations, and as non-relevant in 6 evaluations, possibly because his name was not known to these evaluators.

To understand the subjectivity in judgments, we consider for each particular query, (i) what fraction of evaluations judged a result for this query as relevant, (ii) what fraction of the top 10 results were judged relevant at least once, and (iii) what fraction of the top 10 results were judged relevant in the majority of evaluations. Fig. 1(a) shows the distri-bution of these fractions for all queries (where queries are ranked by the fraction of evaluations that judged a result as relevant). It can be seen that for 37 out of the 55 queries, every result was judged relevant by at least one evaluation, and for 30 out of the 55 queries, every result was judged rel-evant by the majority of the evaluations for that particular result.

The effects of subjectivity can be seen even more clearly in Fig. 1(b) where we plot the above three fractions for each query, considering only those results that were evaluated at least twice. Note that there are 13 queries (out of the 55) for which no individual result was evaluated twice, hence Fig. 1(b) shows the other 42 queries. For as many as 40 out of these 42 queries, every result (that was evaluated at least twice) was judged relevant by at least one evaluation, and for 33 out of these 42 queries, every result (that was evaluated at least twice) was judged relevant by the majority of the evaluations for that result.

Finally, we computed the average precision of the top 10 results returned by Cognos. Fig. 1(c) shows the average pre-cision for all 55 queries, considering two cases  X  (i) a result is said to be relevant if it is judged relevant in the majority of evaluations, and (ii) a result is said to be relevant if it is judged relevant in at least one evaluation. For 30 out of the 55 queries, all the top 10 results were voted relevant (i.e., av-erage precision of 1) in the majority of the evaluations. Also, for 45 out of the 55 queries, the average precision was above 0.8 in both cases. The Mean Average Precision (MAP) for the top 10 Cognos results, considering all 55 queries, was 0.905 in case (i) and 0.939 in case (ii).

The above statistics show that a vast majority of the re-sults returned by Cognos were judged topically relevant to the given query (topic) by at least some evaluators. Thus, Cognos can successfully identify relevant experts over a wide variety of topics. As discussed in Section 4, Pal et. al. [10] give the top 10 ex-perts identified by their algorithm for three specific queries: oil spill , iphone , and world cup . For these queries, Table 7 compares the top 5 results from Cognos and the top 5 re-sults reported by Pal et. al. , along with the bio and number of followers of each result. Note that while the top results reported by Pal et. al. contain some general news media sites (as also discussed in Section 4.2.2), the top Cognos re-sults are much more topic-specific, even if they are not as popularly followed as the news media sites. Interestingly, in their paper, Pal et. al. explicitly set out to discover such specialized topic-specific experts, even if they are not as highly visible as globally popular celebrities or media ac-counts. Cognos achieves this goal better than the state-of-the-art system. In this evaluation, when an evaluator issues a query, she is simultaneously shown the top 10 results returned by Cog-nos as well as the top 10 results returned by the official Twitter WTF service for the same query. The results are anonymized, i.e., the evaluator is not told which result-set is from which service, in order to prevent bias in judgment. Then the evaluator indicates which set of results is better for the given query, or whether both result-sets are equally crawled in 2009 (see Section 5.2), we filtered out from the Twitter WTF results those user-accounts that were created after 2009, in order to make the comparison fair. 4 In order to test the performance of Cognos  X  X n-the-wild X , we allowed the evaluators to issue any query of their choice.
We obtained relevance judgments for a total of 325 queries, out of which 259 are distinct. These queries are evaluator-chosen and they cover a wide variety of topics. Given the high subjectivity observed in user relevance judg-ments in the previous section, we choose to focus our evalu-ation on the 27 distinct queries that were asked at least two times. In total, these 27 queries were asked 93 times. Table 8 shows the 27 queries that were asked at least twice. For each query, we consider the verdict  X  Cognos better / Twitter WTF better / tie  X  based on majority voting. The queries for which there was a unanimous verdict (i.e., all evaluations for this query agreed that one or the other was better) are italicized in Table 8. Cognos was judged to be better for 12 out of the 27 queries, while Twitter WTF was judged better for 11, and there was a tie for 4 queries. The fact that Cognos was judged to be better than the official Twitter WTF service for 44% of the queries, clearly indi-cates the potential of crowdsourced Lists (the only feature used in Cognos) in identifying topical experts in Twitter. It can be noted that a significant fraction of the cases where Twitter WTF was unanimously judged better are names of individuals (celebrities) or organizations. Since such names vealed to the evaluators after the evaluation is done. the profile information. appear very rarely in the List names / descriptions, Cognos does not handle these queries well.

Table 8 also shows that the top 10 Cognos results have very low overlap with top 10 Twitter WTF results across all queries. This is in spite of the fact that 83.4% out of the Twitter WTF top 20 results for some query (topic) were inferred by our List-based methodology to be related to the same topic (as was reported in Section 4.2.3). This implies that the low overlap between the top Cognos results and top Twitter WTF results is primarily due to the List-based ranking used in Cognos.

In general, we observe that the top Twitter WTF results mostly include organizations / business accounts while the top Cognos results mostly include personal accounts. This is possibly because the Twitter WTF considers the name and bio of users [17], and organizational / business accounts are more likely (compared to personal accounts) to have names or bios containing terms related to their topics of exper-tise. We present some examples in Table 9 for the queries  X  X usic X  (for which the majority voted Twitter WTF better), and  X  X indows phone X  (for which the majority voted Cognos better). As such, these examples again bring out the sub-jective nature of human judgment, where some evaluators preferred the personal accounts while others preferred the organizational accounts. Our evaluation of the Cognos search system shows that a vast majority of its results are relevant for a wide variety of topics. In fact, Cognos rarely produces entirely irrelevant results for queries. Comparing Cognos with state-of-the-art research system by Pal et. al. and the official Twitter WTF service highlights the advantages of relying on crowdsourced Lists to identify experts. Cognos yields particularly better search results in the cases when the bio or tweets posted by a user does not correspond to or contain information about the user X  X  topic of expertise. In fact, Cognos performs as good as or better than the official Twitter WTF service for more than 52% of the queries, even though it is based on a single and simple feature (Lists). In this section, we address the practical challenge of keeping our Cognos system up-to-date, even as hundreds of thou-sands of new Twitter accounts and new Lists are created every day. We begin by analyzing the scalability of a simple updation strategy that relies on periodically crawling all the Twit-ter users and the Lists that contain them. Recent reports indicate that 200 million new users joined Twitter in the last 9 months [2], which roughly amounts to 740,000 new users joining per day. For each user, we would need to make at least one extra request to crawl her Lists. Thus, just to keep the system up-to-date by crawling the newly joining users everyday, a lower-bound rate limit would be of at least 1,480,000 requests per day. Twitter normally rate-limits the number of API requests from a single machine (IP address) to 150 per hour [15], i.e., to 3600 user profile crawls per day. Fortunately, three of our machines were white-listed by Twitter, which allows each of them to crawl at a significantly higher rate of 20,000 API requests per hour. Thus, we can make at most 1,440,000 (20,000  X  3  X  24) API requests per day from all three of our white-listed machines. Note that our maximum crawl rate is still lower than the lower-bound rate we would need to gather the profiles and Lists of all new users joining Twitter every day. Given that we would need to periodically crawl the new Lists for the already existing 465 million Twitter users [2], it becomes quite evident that the strategy of crawling all users X  Lists would not scale.
Next, we estimated the number of highly listed users a mongst the 465 million Twitter accounts as of January 2012. Since Twitter assigns userids in an integer sequence starting from 1, we took a random sample of 300,000 integers in the range 1 to 465 million, and attempted to crawl the profiles of Twitter userids in the sample. The distribution of experts within this large random sample can be expected to be similar to the distribution of experts among all Twitter users. For instance, only 363 out of the 300,000 sampled users (i.e., 0.12%) were listed 100 or more times; hence we expect the total number of Twitter users who are listed 100 or more times to be 0.12% of the entire Twitter population. Thus, only a small fraction of all Twitter users are highly listed experts and once they are identified, it would be pos-sible to crawl the Lists containing them periodically. The key challenge, however, lies in efficiently identifying these experts from the large Twitter user population. Our discussion above shows that we cannot exhaustively crawl Lists for all Twitter users. However, we can crawl Lists for the small fraction of expert users, if we somehow identified them from the Twitter user population. We now propose and evaluate a strategy to efficiently identify expert users.

We begin by observing that the Twitter social network consists of a number of hubs , users who follow a large number of popular experts and include them in Lists. Our strategy is to first identify popular hubs in an older snapshot of the network (when the network was considerably smaller) and then leverage the Lists created by the top hubs in order to find new authorities. It can be noted that this strategy also relies on crowdsourcing  X  we expect the Twitter crowd (in particular, the top hubs) to discover experts who newly join Twitter, and we can utilize their discovery by periodically crawling the Lists created by the top hubs.

We used the well-known HITS algorithm to identify the top hubs in the snapshot of the Twitter network gathered in 2009 [4] (introduced in Section 5.2), when the network had only 54 million users. We then crawled the Lists cre-ated by the top 1 million hubs in the network to efficiently discover experts. In all, the top 1 million hubs had created 479,129 Lists, which taken together contained 4,100,367 dis-tinct users. Out of these, 2,064,373 (i.e., 50.3%) have been included in 10 or more Lists. In comparison, only 1.13% of all the users in our large random sample of Twitter users are listed 10 or more times. The difference clearly indicates that our strategy is effective in focusing our crawls on experts in Twitter. The crawl for the Lists created by the top 1 million hubs took about 3 weeks (January 20  X  February 8, 2012) using three machines white-listed by Twitter, and hence, it can be repeated every month to discover new experts. In this section, we estimate the fraction of experts covered by our strategy to crawl Lists created by top hubs. We measure the fraction of most listed users in Twitter that is covered by our methodology as follows. First, we estimate the number of Twitter users listed at least K times by com-puting the number of such users in our random sample of 300,000 users, and then scaling it to the total Twitter popu-lation of 465 million users. Next we calculate the fraction of the estimated number of users listed at least K times, that is actually discovered by crawling the Lists created by the top hubs.

Figure 2 plots the fraction of experts discovered against the number of top hubs crawled. By crawling the Lists cre-ated by the top 1 million hubs, we discover 25,887 experts who are listed 1000 or more times, which is 70.6% of our es-timated total number of experts listed at least 1000 times in Twitter. Further, we find that crawling the Lists created by only the top 100,000 hubs is sufficient to discover 53.3% of the estimated number of experts listed 1000 or more times. Thus, the hub-based updation methodology can be used to efficiently discover a large fraction of experts in Twitter. Our expert discovery strategy is also effective in discovering newly joined experts. Even though our top hubs were se-lected using a 2009 snapshot of the Twitter network, more than 42.3% of the 4,100,367 users in the Lists created by these hubs have joined Twitter after 2009. Further, we show some examples of very recently created Twitter ac-counts that our hub-based crawl could discover, in Table 10. Our crawl of Lists created by the top 1 million hubs, which ended on February 8, 2012, discovered some experts who joined Twitter as recently as Feb 6 or Feb 4 (i.e., while the crawl was going on). This not only validates our hypoth-esis that the top hub nodes quickly discover newly joined experts and add them to their Lists, but it also shows the effectiveness of the hub-based updation strategy. We evaluate whether our updation methodology can dis-cover topical experts returned by the Pal et. al. research system and Twitter WTF service. Out of the 30 topical ex-perts stated by Pal et. al. (for the three topics  X  X il spill X ,  X  X orld cup X  and  X  X Phone X ), 29 are included in the crawls of Lists created by the top 1 million hubs (the 1 remaining ac-count no longer exists in Twitter). Next, we consider the top 20 experts returned by Twitter WTF for each of the 259 queries obtained by our user-survey (see Section 5.3.3) and calculate what fraction of these experts are covered by our hub-based crawls. Figure 3 plots the distribution of the fraction of Twitter WTF top 20 results included in our hub-based crawls, across all queries. It can be seen that our crawls include all Twitter WTF top 20 results for more than 50% of the queries and at least 15 out of the Twitter WTF top 20 results for close to 80% of the queries.

The above results indicate that our hub-based strategy  X  periodically discovering experts through the Lists created by top hubs  X  can be used to efficiently discover newly joined experts, and keep our expert search system up-to-date in the face of rapid increase in the Twitter population. As Twitter emerges as a popular platform for finding real-time information on the Web, an important research chal-lenge lies in identifying experts in specific topics. In this pa-per, we show that an effective solution to this hard problem lies in exploiting the wisdom of the Twitter crowds. Specifi-cally, we observe that individual Twitter users, for their own convenience, annotate and classify experts in various topics using the Lists feature. By aggregating the List information for individual Twitter users, we can obtain an extremely rich characterization of their topical expertise as perceived by the Twitter crowds. We leverage this insight to build and deploy Cognos, a topical expert search system. Through extensive evaluation, we demonstrate that even though Cognos is built utilizing only the Lists feature, it can compete with the com-mercial who-to-follow system deployed by Twitter itself. We believe that crowdsourced Lists provide a valuable founda-tion for building future content search / recommendation / discovery services in Twitter.

Our current List-based methodology is vulnerable to List spamming, where malicious users can create fake Lists in-cluding a target user to manipulate the inferred attributes for the user. To date, we did not find much evidence of such attacks. However, such attacks can easily be launched in the future. Defending against such attacks would require the List-based methodology to consider the reputation of the users who create the Lists  X  a crucial challenge that we plan to address in future work.
 Acknowledgment The authors thank the anonymous reviewers whose sugges-tions helped to improve the paper. This research was sup-ported in part by a grant from the Indo-German Max Planck Centre for Computer Science (IMPECS).
