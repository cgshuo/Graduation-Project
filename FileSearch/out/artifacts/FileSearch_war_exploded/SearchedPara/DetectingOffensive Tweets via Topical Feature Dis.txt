 In this paper, we propose a novel semi-supervised approach for detecting profanity-related offensive content in Twitter. Our ap-proach exploits linguistic regularities in profane language via statis-tical topic modeling on a huge Twitter corpus, and detects offensive tweets using these automatically generated features. Our approach performs competitively with a variety of machine learning (ML) algorithms. For instance, our approach achieves a true positive rate (TP) of 75 . 1% over 4029 testing tweets using Logistic Regression, significantly outperforming the popular keyword matching base-line, which has a TP of 69 . 7% , while keeping the false positive rate (FP) at the same level as the baseline at about 3 . 77% . Our ap-proach provides an alternative to large scale hand annotation efforts required by fully supervised learning approaches.
 I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  Text analysis ; I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier design and evaluation Algorithms, Languages, Human Factors Twitter, Hadoop, topic modeling, machine learning
Social media sites represent some of the most popular sites on the Internet today. Offensive conent included in the user-generated content on many of these sites makes users X  online experience un-pleasant and may also be something that certain users want to filter (e.g. parents). Thus, an effective approach to detecting inappropri-ate online content is of great practical importance. In this paper, we opted to focus on profane language first, given that this is the most common category of inappropriate language in social media.
In our work we treat vulgarity as a type of linguistic style [11] that is expressed within a sentence with a certain rhythm or peri-odicity, which explains the tendency for a piece of vulgar text to contain more than one vulgar word. Another key component of our approach is leveraging a large text corpus with a high enough den-sity of interesting patterns. The availability of a large enough cor-pus is more critical than the specifics of the algorithmic approach, especially in connection with data sets at the scale that is available over the web on social media sites [12]. In this work, we focus on Twitter, a popular microblogging service that provides a handy public platform for users to follow each other and share messages called tweets, which are text-based posts of up to 140 characters.
Our earlier results indicated that bag-of-words, part-of-speech (POS) and other pattern-based methods including belief propaga-tion did not work well for profane tweet detection due to the signif-icant noise in tweets. In this paper, we propose a hierarchical ap-proach that exploits the co-occurrence of vulgar language via statis-tical topic modeling techniques and detects profane language with automatically generated features using a machine learning frame-work. In particular, we explore the predictive value of highly ex-pressive topical features as well as reliable lexical features, and combine them in a single compact feature space.

In building the composite topical features, we first use a boot-strapping strategy to automatically collect a set of tweets from a number of offense-pro twitterers (i.e., twitterers who have a high prevalence of seed profane words) and law-abiding twitterers (i.e., twitterers who rarely use seed offensive words) over a large tweet corpus using a list of pre-defined offensive seed words; we then learn topic models from this tweet set via Latent Dirichlet Alloca-tion (LDA) [3], a well-known generative topic modeling algorithm.
The major contribution of our proposed method is two-fold. 1. To the best of our knowledge, our work in this paper is the 2. Our approach presented in this paper is the first to demon-
Offensive content detection is not a brand new area. Neverthe-less, available techniques described in the literature are few and mainly utilize a pre-defined dictionary of hostile words or patterns. In general, these approaches are rigid and crucially depend upon the coverage of the seed word or pattern list.

In the seminal work called Smokey [16], Spertus hand designed 47 features based on the syntax and semantics of the training sen-tences, catching 64% of the abusive messages in a testing set of 460 messages with an FP of 2% . Many of the features in Smokey suffer either from low coverage or a high false positive rate. Other work utilizing seed words or rules include [18, 10, 15].
In recent years, a variety of manual and automatic feature engi-neering techniques have been developed to construct feature spaces that are adept at capturing interesting language variation. Among them, two pattern-based approaches [6, 17] are similar to our own. However, given the extremely noisy and flexible nature of the Twit-ter messages, machine learning algorithms could be easily over-whelmed by a large set of trivial patterns.
Twitter has drawn significant attention in recent years, and much work related to sentiment analysis has been published, such as [5, 4, 9]. Compared with offensiveness detection, the definition of a pos-itive or negative attitude in sentiment analysis is relatively straight-forward. For our task, the notion of vulgarity is rather subjective and the degree of offensiveness varies considerably among people, rendering the labeling process in our task even harder.
Bootstrapping is a widely-used technique that provides labels given a large amount of unlabeled data and a small amount of seed information. Research that has applied bootstrapping to NLP prob-lems has existed for more than a decade [8]. In this paper, we lever-age bootstrapping to aid in vulgar tweet detection, and to the best of our knowledge, our work is the first in doing so. Furthermore, our target of offensive language is more subtle than many other tasks in the bootstrapping literature such as place name extraction [8].
The architecture of our system is given in Figure 1. Our tweet corpus contains the textual messages along with other meta data such as twitterer ID, posting time, etc. The raw tweets in our ex-periment were collected by querying the Twitter API as well as archiving the  X  X ardenhose X  real-time stream [13]. Our raw corpus for training and testing purposes has more than 680 million and 16 million tweets respectively.

Tweets are expressed in an extremely colloquial fashion, with substantial noise and linguistic variation. For example, tweets con-tain a high volume of novel words, interjections, repetitions, ortho-graphical errors such as word shortening (acronyms, words with characters removed, words shortened by phonetic spellings like nite for night ), etc. Moveover, dropping spaces between words is also common, such as howareyou , which increases the scale of the tweet vocabulary significantly and imposes a huge burden for text analy-sis tasks.
We designed a word cleaning algorithm, applying a series of fil-ters in the following order to process the raw tweet corpus prior to topic induction and feature extraction. 1. We removed non-English tweets using LingPipe [1] with Hadoop. 2. To reduce the bias from heavy twitterers and increase diver-3. We intentionally dropped retweets (indicated by  X  X t X ) from 4. We removed the shortened URLs in tweets. 5. Twitterers often use mentions in the body of their tweets to 6. The # symbol, called a hashtag, is used in Twitter to mark 7. To tackle the problem of intentional repetitions, we designed 8. For sequences of 2 repetitive letters, we counted how many 9. We removed all stopwords. 10. We defined a word to be a sequence of letters, -or  X , and
The idea is to treat each tweet as a finite mixture over an un-derlying set of topics, each of which is in turn characterized by a distribution over words, and then examine tweets via such topic distributions. Intuitively, offensive topics may be associated with higher probabilities for offensive words.

To learn a model that can infer topic distributions from tweets, we need a set of labeled training tweets with both offensive and non-offensive content, and to that end, we designed a bootstrapping algorithm to extract training tweets from a large tweet corpus using the map-reduce framework in Hadoop. The details are shown in algorithm 1, 2, and 3.

Our bootstrapping technique does not assume every word from the constant offenders to be offensive. Neither does it require a curse-free tweet set from benign twitterers. We expect topic mod-eling to pick up lexical collocation patterns in the profane content and produce meaningful topics for our task.

One merit of bootstrapping between twitterers and tweets is that with a limited list of seed words, we are able to capture a lot more novel offensive patterns automatically, thus tremendously reducing the effort in manual annotation.
 With a set of training tweets as obtained in Algorithm 1, we adopt Latent Dirichlet Allocation (LDA) [3], a renowned generative prob-abilistic model for topic discovery, to build the composite topical features. We chose the LDA implementation by Phan et al.[14].
W e used a threshold of 3 in this work. Algorithm 1 BuildTrainingTweetsViaBootstrapping Requir e: raw tweets T , threshold t , seed words S Ensure: tweet set TS for topic learning 1: ot , gt  X  ClassifyTwitterers( T , t , S ) 2: TS  X  ExtractTweets( T , ot , gt ) 3: return TS Algorithm 2 ClassifyTwitterers Requir e: raw tweets T , threshold t , seed words S Ensure: offensive twitterers ot , good twitterers gt 1: preprocess T with the English detector and word cleaner by 2: compute the percent p of offensive tweets for each twitterer 3: ot  X  twitterers with p &gt; = t 4: gt  X  twitterers with p = 0 5: return ot , gt Algorithm 3 ExtractTweets Requir e: raw tweets T , offensive twitterers ot , good twitterers gt Ensure: tweet set TS for topic learning 1: preprocess T with the English detector and word cleaner by 2: OT  X  get all tweets from each twitterer in ot by Hadoop 3: GT  X  randomly sample | OT | tweets from gt by hadoop 4: TS  X  OT + GT 5: return TS
The keyword matching technique, though narrow in coverage, can catch common vulgar language (with false positives sometimes depending on the context) and we exploit this property to introduce a lexicon feature into our framework, which is a binary indicator that there is at least one word from our offensive lexicon in the tweet. With the two types of features, our approach builds machine learning models to classify tweets, as shown in Algorithm 4. Algorithm 4 ClassifyTweets Requir e: tweet set TS for topic learning, testing tweet set TT , Ensure: classification result set r 1: m  X  learn topic models with LDA on TS 2: F  X   X  3: for each tweet t in TT do 4: f t  X  infer the topic distributions with m 5: f b  X  check whether t has an offensive word in S 6: f  X  build a feature vector concatenating f t , f b 7: F  X  F  X  f 8: end for 9: r  X  do 10-fold cross validation on F with respect to L 10: return r
We compiled a dictionary of 338 most common offensive words based on [2], and manually removed entries used often yet not very offensive such as  X  X ell X . Moreover, we used the tweets crawled from May 25, 2009 to October 17, 2010 as the raw training corpus for topic model learning, and took tweets from October 18, 2010 to October 27, 2010 as the raw testing corpus from which we selected testing tweets for the final evaluation. Table 1 gives some basic statistics about these two raw corpus.

For evaluation, we randomly sampled a subset of 4029 tweets from the raw testing corpus. To guarantee enough offensive pat-terns in this testing set, we first computed the percentage of offen-sive tweets p from each twitterer in the raw testing corpus based on our seed lexicon, and then randomly chose tweets from twitterers with p &gt; = 40% . We then recruited three participants on campus with different backgrounds to label the tweets in this testing set. Table 1: Statistics of our raw training and testing corpus. 4 . 58% tweets in the raw training tweet corpus contain at least one offensive word in our seed list.

To fully evaluate our approach, we adopted 4 popular machine learning algorithms, including J48 decision tree learning, Support Vector Machines (SVM), logistic regression (LR) and random for-est (RF). We found that these 4 algorithms performed competi-tively, and due to the limitation of space, we only report the result of LR, which slightly outperformed the others. Moreover, though the volume of the raw training corpus is huge (Table 1), the tweet set produced by our bootstrapping technique (Algorithm 1), which was actually used to learn the topic model, has only 860 , 071 tweets, a tiny number compared with the raw training corpus. Therefore, we chose to learn a small number of topics (from 10 to 50 ) via LDA.
Table 2 shows the F1 values of LR under 10-fold CV using a threshold of 0 . 5 on the predicted probabilities. The F1 values im-proved as the number of learned topics increased, which is reason-able because we can catch more fine-grained patterns and thus bet-ter separate offensive and non-offensive content with more topics. Our approach using the full feature set outperformed the keyword matching baseline under all configurations with all machine learn-ing algorithms, suggesting the robustness of our approach across various learning schemes.

In addition, TP and FP are two important metrics in evaluating binary classification tasks. We chose a threshold on the predicted probabilities in LR such that the resultant FP of our approach is on the same level as the keyword matching baseline, and plot the TP of our approach using LR in Figure 2. The graph shows that under such configurations, our approach using the full feature space im-proves the TP over the baseline by up to 5 . 4% . The ROC curves in Figure 2 also indicate that our approach with the full feature space has superior performance, dominating the algorithm with topic fea-tures only. Even the keyword matching baseline works reasonably well, and existing techniques rely to a great extent on it in detecting vulgarity. The experiment results here suggest that our approach is able to detect up to 5 . 4% more profane patterns without sacri-ficing the FP, which is a statistically significant improvement and is of great practical importance. Moreover, we can always tune the threshold on the predicted probabilities and other parameters to achieve a desirable detection rate, depending on the specific needs to go more aggressively or conservatively against offensive lan-guage.
To further understand our technique, we conducted an error anal-ysis after the experiment using 10 topics.

The keyword matching baseline utilizes a lexicon of offensive words compiled by us, however, 101 testing tweets with at least one word appearing in our seed lexicon were assigned a label of  X  X ot-offensive X  by the human labelers we recruited. This led to all the 3 . 77% false positives by the baseline algorithm, which indirectly propagated to the lexicon feature of our proposed approach. This again confirms that the degree of offensiveness is rather subjective, and the task of offensiveness detection is difficult.

Moreover, we found that two features, i.e., the lexicon feature and topic 6 , are responsible for many false positives. This comes as no surprise in that topic 6 contains the most offensive terms, either known ones in the seed lexicon or novel ones. On the other hand, no topics had a significant impact on the false negatives, and we conjecture that false negatives were caused by an additive effect of the topical features except for topic 4 , 6 , 9 .
Our approach can be further improved as follows. First, we uti-lized the topical features only via the word level distributions to de-tect frequently co-occurring patterns, while in the future we could consider the benefits of more complex features within the topic rep-resentation. This is consistent with our finding that a great many unigrams in topic 4 learned by LDA over the training tweets are not offensive by themselves, but are clearly indicative of sex-related offense when combined with other words, such as  X  X et X ,  X  X irty X , etc. Second, we currently simply used a binary lexicon feature to capture the appearance of profane words for each tweet, and alter-natively, we could adopt a complex weighting mechanism like TF-IDF. Third, the current tweet set for training the topic model has 860 , 071 tweets only. With more data, we can learn more topics via model tuning.
In this paper, we propose an approach that exploits the lexical collocation of profane language via statistical topic modeling tech-niques and detects offensive tweets using highly expressive topical features as well as the reliable lexicon feature in a single machine learning framework. The keyword matching technique has been shown to perform very well in the literature and achieved a TP of 69 . 7% with an FP of 3 . 77% in our experiment. While keep-ing the FP on the same level as the baseline, our approach had a TP of 75 . 1% over 4029 testing tweets using Logistic Regression, a significant 5 . 4% improvement over the baseline. In addition, our approach also provides an alternative to large scale hand annotation efforts required by supervised learning approaches. [1] Alias-i. Lingpipe 4.0.1. 2008. [2] AllSlang. List of swear words. 2010. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet technique in classifying swearing tweets. [4] J. Bollen, H. Mao, and A. Pepe. Modeling public mood and [5] D. Davidov, O. Tsur, and A. Rappoport. Enhanced sentiment [6] P. Gianfortoni, D. Adamson, and C. Rose. Modeling of [7] A. Go, R. Bhayani, and L. Huang. Twitter sentiment [8] R. Jones, A. Mccallum, K. Nigam, and E. Riloff.
 [9] E. Kouloumpis, T. Wilson, and J. Moore. Twitter sentiment [10] A. Mahmud, K. Z. Ahmed, and M. Khan. Detecting flames [11] J. R. Martin and P. R. White. The Language of Evaluation: [12] P. Norvig. Statistical learning as the ultimate agile [13] B. O X  X onnor, R. Balasubramanyan, B. R. Routledge, and [14] X.-H. Phan and C.-T. Nguyen, 2007. GibbsLDA++: A [15] A. H. Razavi, D. Inkpen, S. Uritsky, and S. Matwin. [16] E. Spertus. Smokey: Automatic recognition of hostile [17] O. Tsur, D. Davidov, and A. Rappoport. Icwsm -a great [18] Z. Xu and S. Zhu. Filtering offensive language in online
