 Decision trees constitute one of the mo st popular classification techniques in data mining and have been the subject of a large body of investigation. The typical construction algorithm for a d ecision tree starts with a training set of objects that is split recursively. The s uccessive splits form a tree where the sets assigned to the leaves consist of objects that belong almost entirely to a single class. This allows new objects that belong to a test set to be classified into a specific class based on the path induced by the object in the decision tree which joins the root of the tree to a leaf.

Decision trees are useful classification algorithms, even though they may present problems related to overfitting and excessive data fragmentation that results in rather complex classification schemes.

A central problem in the construction of decision trees is the choice of the splitting attribute at each non-leaf node. We show that the usual splitting cri-terion (the information gain ratio, or the similar measure derived from the Gini index) are special cases of a more general approach. Furthermore, we propose a geometric criterion for choosing the splitting attributes that has the advantage of being adaptable to various data sets and user needs.
 The betweenness relation defined by the metric space ( S, d ) is a ternary relation the fact that ( s, u, t )  X  R by [ sut ] and we say that u is between s and t .
We explore a natural link that exists between random variables and parti-tions of sets that allows the transfer of certain probabilistic and information-theoretical notions to partitions of sets.

Let PART ( S )bethesetofpartitionsofaset S . The class of all partitions of finite sets is denoted by PART . The one-block partition of S is denoted by  X  S . The partition {{ s }| s  X  S } is denoted by  X  S .If  X ,  X   X  PART ( S ), then  X   X   X  if every block of  X  is included in a block of  X  . Clearly, for every  X   X  PART ( S ) we have  X  S  X   X   X   X  S .  X  covers  X  if  X   X   X  and there is no partition  X   X  PART ( S ) such that  X &lt; X &lt; X  .Thisfactisdenotedby  X   X   X  .Itisknown[1]that  X   X   X  if and only if  X  is obtained from  X  by fusing two blocks of this partition into a new block.

For every two partitions  X ,  X  both inf {  X ,  X  } and sup {  X ,  X  } in the partial or-dered set ( PART ( S ) ,  X  ) exist and are denoted by  X   X   X  and  X   X   X  , respectively. It is well known that ( PART ( S ) ,  X  ) is an upper semimodular lattice.
If S, T are two disjoint and nonempty sets,  X   X  PART ( S ),  X   X  PART ( T ), where  X  = { A 1 ,...,A m } ,  X  = { B 1 ,...,B n } , then the partition  X  +  X  is the partition of S  X  T given by  X  +  X  = { A 1 ,...,A m ,B 1 ,...,B n } .
 Whenever the  X + X  operation is defined, th en it is easily seen to be associative. In other words, if S, U, V are pairwise disjoint and nonempty sets, and  X   X  PART ( S ),  X   X  PART ( U ),  X   X  PART ( V ), then  X  +(  X  +  X  )=(  X  +  X  )+  X  .Observe that if S, U are disjoint, then  X  S +  X  U =  X  S  X  U .Also,  X  S +  X  U is the partition { S, U } of the set S  X  U .
 If  X  = { B 1 ,...,B m } ,  X  = { C 1 ,...,C n } are partitions of two arbitrary sets S, U , respectively, then we denote the partition { B i  X  C j | 1  X  i  X  m, 1  X  j  X  n } of S  X  U by  X   X   X  .Notethat  X  S  X   X  U =  X  S  X  U and  X  S  X   X  U =  X  S  X  U .
Let  X   X  PART ( S )andlet C  X  S .Denoteby  X  C the  X  X race X  of  X  on C given by  X  C = { B  X  C | B  X   X  such that B  X  C =  X  X  . Clearly,  X  C  X  PART ( C ); also, if C is a block of  X  ,then  X  C =  X  C .

A subset T of S is pure relative to a partition  X   X  PART ( S )if  X  T =  X  T .In other words, T is pure relative to a partition  X  if T is included in some block of  X  .
In [2] the notion of  X  -entropy of a probability distribution p =( p 1 ,...,p n ) was defined as: where p 1 +  X  X  X  + p n =1and p i  X  0for1  X  i  X  n . In the same reference it was observed that Shannon X  X  entropy H ( p ) can be obtained as lim  X   X  1 H  X  (  X  ).
In [3] we offered a new interpretation of the notion of entropy for finite distribu-tions as entropies of partitions of finite sets. Our approach took advantage of the properties of the partial order of the lattice of partitions of a finite set and makes use of operations defined on partitions. We defined the H  X  entropy for  X   X  R ,  X &gt; 0 as a function H  X  : PART ( S )  X  X  X  R  X  0 that satisfies certain conditions. Un-der these conditions, we have shown in [3] that if  X  = { B 1 ,...,B n } X  PART ( S ), then In the special case, when  X   X  1wehave: Note that if | S | =1,then PART ( S ) consists of a unique partition (  X  S =  X  S )and H  X  (  X  S ) = 0. Moreover, for an arbitrary finite set S we have H  X  (  X  )=0ifand only if  X  =  X  S .

These facts suggest that for a subset T of S the number H  X  (  X  T )canbeused as a measure of the purity of the set T with respect to the partition  X  .If T is  X  -pure, then  X  T =  X  T and, therefore, H  X  (  X  T ) = 0. Thus, the smaller H  X  (  X  T ), the more pure the set T is. The  X  -entropy defines naturally a conditional entropy of partitions. We note that the definition introduced here is an improvement over our previous definition given in [3]. Starting from conditional entropies we will be able to define a family of metrics on the set of partitions of a finite set and study the geometry of these finite metric spaces.
 Definition 1. Let  X ,  X   X  PART ( S ) and let  X  = { C entropy is the function H  X  : PART ( S ) 2  X  X  X  R  X  0 defined by: for  X ,  X   X  PART ( S ) .
 partition  X   X  PART ( S ). Also, we can write:
H  X  (  X  S |  X  )= where  X  = { C 1 ,...,C n } . The conditional entropy can be written explicitly as: where  X  = { B 1 ,...,B m } . Theorem 1. Let  X ,  X  be two partitions of a finite set S . We have H  X  (  X  |  X  )=0 if and only if  X   X   X  .
 It is possible to prove that for every  X ,  X   X  PART ( S )wehave: which generalizes a well-known p roperty of Shannon X  X  entropy.

The next result shows that the  X  -conditional entropy is dually monotonic with respect to its first argument and is monotonic with res pect to its second argument.
 Theorem 2. Let  X ,  X ,  X   X  PART ( S ) ,where S is a finite set. If  X   X   X  ,then H  X  (  X  |  X  )  X  H  X  (  X  |  X  ) and H  X  (  X  |  X  )  X  H  X  (  X  |  X  ) .
 Since H  X  (  X  )= H  X  (  X  |  X  S ) it follows that if  X ,  X   X  PART ( S ), then H  X  (  X  )  X  H  X  (  X  |  X  ).

The next statement that follows from the previous theorem is useful in Sec-tion 5.
 Corollary 1. Let  X ,  X ,  X  be three partitions of a finite set S .If  X   X   X  ,then The behavior of  X  -conditional entropies with respect to the  X  X ddition X  of parti-tions is discussed in the next statement.
 Theorem 3. Let S be a finite set,  X ,  X  be two partitions of S ,where  X  = { D 1 ,...,D h } .If  X  i  X  PART ( D i ) for 1  X  i  X  h ,then If  X  = { F 1 ,...,F k } ,  X  = { C 1 ,...,C n } be two partitions of S ,andlet  X  i  X  PART ( F i ) for 1  X  i  X  k .Then, In [4] L. de M  X  antaras proved that Shannon X  X  entropy generates a metric d : We extend his result to a class of metrics that can be defined by  X  -entropies, thereby improving our earlier results [5].
 Our central result follows.
 Theorem 4. The mapping d  X  : PART ( S ) 2  X  X  X  R  X  H  X  (  X  |  X  )+ H  X  (  X  |  X  ) for  X ,  X   X  PART ( S ) is a metric on PART ( S ) . It is clear that d  X  (  X ,  X  S )= H  X  (  X  )and d  X  (  X ,  X  S )= H (  X  S |  X  ).
The behavior of the distance d  X  with respect to partition addition is discussed in the next statement.
 Theorem 5. Let S be a finite set,  X ,  X  be two partitions of S ,where  X  = {
D 1 ,...,D h } .If  X  i  X  PART ( D i ) for 1  X  i  X  h ,then The distance between two partitions can be expressed using distances relative to the total partition or to the identity partition. Indeed, we have the following result: Theorem 6. Let  X ,  X   X  PART ( S ) be two partitions. We have: or d  X  (  X  S , X  )= d  X  (  X  S , X  ), then  X  =  X  for every  X ,  X   X  PART ( S ). Theorem 7. Let  X ,  X   X  PART ( S ) . The following statements are equivalent: 1.  X   X   X  ; 2. we have [  X ,  X ,  X  S ] inthemetricspace ( PART ( S ) ,d  X  ) ; 3. we have [  X  S , X , X  ] inthemetricspace ( PART ( S ) ,d  X  ) .
 Metrics generated by  X  -conditional entropies are closely related to lower valua-tions of the upper semi-modular lattices of partitions of finite sets. This connec-tion was established in [6] and studied in [7, 8, 9].

A lower valuation on a lattice ( L,  X  ,  X  ) is a mapping v : L  X  X  X  R such that is referred to as an upper valuation .

If v  X  L is both a lower and upper valuation, that is, if v (  X   X   X  )+ v (  X   X   X  )= v (  X  )+ v (  X  ) for every  X ,  X   X  L ,then v is a valuation on L . It is known [6] that if there exists a positive valuation v on L ,then L must be a modular lattice. Since the partition lattice of a set is an upper-semimodular lattice that is not modular ([6]) it is clear that positive valuations do not exist on partition lattices. However, lower and upper valuations do exist, as shown next: Theorem 8. Let S be a finite set. Define the mappings v  X  : PART ( S )  X  X  X  R respectively, for  X   X  PART ( S ) .Then, v  X  is a lower valuation and w  X  is an upper valuation on the lattice ( PART ( S ) ,  X  ,  X  ) . We begin by defining the notion of object system as a triple S =( S, H, C ), where of mappings of the form A i : S  X  X  X  D i called the features of S for 1  X  i  X  n ,and C : S  X  X  X  D is the classification function .Thesets D 1 ,...,D n are supposed to contain at least two elements and they are referred as the domains of the attributes A 1 ,...,A n .
 A set of attributes X , X  X  H generates a mapping  X  X : S  X  X  X  { D i | A i  X  X } , defined by  X  X ( t )= { ( A ( t ) ,A ) | A  X  X } for every t  X  S ,where denotes the disjoint union of a family of sets; we refer to  X  X as the projection on X of S . Projections define partitions on the set of objects in a natural manner; namely if X is a set of attributes, a block B v of the partition  X  X is a non-empty set of the form { t  X  S |  X  X ( t )= v } ,where v is an element of the range of  X  X .
To introduce formally the notion of decision tree we start from the notion of tree domain. A tree domain is a non-empty set of sequences D ,overthesetof natural numbers N such that every prefix of a sequence s  X  D also belongs to D ,andforevery m  X  1, if ( p 1 ,...,p m  X  1 ,p m )  X  D ,then( p 1 ,...,p m  X  1 ,q )  X  D for every q  X  p m . The elements of D are called the vertices of D . The notions of descendant and ancestor of a vertex have their usual definitions.
 Let S be a finite set and let D be a tree domain. An S -tree is a function T : D  X  X  X  P ( S ) such that T (  X  )= S ,andif u 1 ,...,um are the descendants of a vertex u , then the sets T ( u 1) ,..., T ( um ) form a partition of the set T ( u ).
A decision tree for an object system S =( S, H, C )isan S -tree T , such that if the vertex v has the descendants v 0 ,...,vm , then there exists an attribute A  X  H (called the splitting attribute in v ) such that { T ( vi ) | 1  X  i  X  m } is the partition  X  A T ( v ) .
 Thus, each descendant vi of a vertex v corresponds to a value a of the attribute A that was used as a splitting attribute in v .If  X  = v 1 ,v 2 ,...,v k = u is the path to v 2 ,...,v k , respectively, then we say that u is reached by the selection:
It is desirable that the leaves of a decision tree contain C -pure or almost C -pure sets of objects. In other words, the objects assigned to a leaf of the tree should, with few exceptions, have the the same value for the class attribute C . This amounts to asking that for each leaf w of T we must have H  X  (  X  C S w )as close to 0 as possible. To take into acco unt the size of the leaves note that the collection of sets of objects assi gned to the leafs is a partition  X  of S and that we need to minimize: which is the conditional entropy H (  X  C |  X  ). By Theorem 1 we have H (  X  C |  X  )=0 if and only if  X   X   X  C , which happens when the sets of objects assigned to the leafs are C -pure.

The construction o f a decision tree T  X  ( S ) for an object system S =( S, H, C ) evolves in a top-down manner according to the following high-level description of a general algorithm [10]. The algorithm starts with an object system S = ( S, H, C ), a value of  X  and with an impurity threshold and it consists of the following steps: 1. If H  X  (  X  C S )  X  , then return T as an one-vertex tree; otherwise go to 2. 2. Assign the set S to a vertex v , choose an attribute A as a splitting attribute
Note that if is sufficiently small and if H  X  (  X  C S )  X  ,where S = T ( u )isthe setofobjectsatanode u , then there is a block Q k of the partition  X  C S that is dominant in the set S . We refer to Q k as the dominant class of u .

Once a decision tree T is built it can be used to determine the class of a new object t  X  S such that the attributes of the set H are applicable. If A i 1 ( t )= a ,...,A i k  X  1 ( t )= a k  X  1 ,aleaf u was reached through the path v 1 ,...,v k = u , then t is classified in the class Q k ,where Q k is the dominant class at leaf u .
The description of the algorithm shows that the construction of a decision tree depends essentially on the method for choosing the splitting attribute. We focus next on this issue.

Classical decision tree algorithms make use of the information gain criterion or the gain ratio to choose splitting attribute. These criteria are formulated using Shannon X  X  entropy, as their designations indicate.

In our terms, the analogue of the information gain for a vertex w and an realizes the highest value of this quantity. When  X   X  1 we obtain the information gain linked to Shannon entropy. When  X  = 2 one obtains the selection criteria for the Gini index using the CART algorithm [11].

The monotonicity property of conditional entropy shows that if A, B are two attributes such that  X  A  X   X  B (which indicates that the domain of A has more for A is larger than the gain for B . This highlights a well-known problem of choosing attributes based on information gain and related criteria: these criteria favor attributes with large domains, which in turn, generate bushy trees. To alleviate this problem information gain was replaced with the information gain ratio defined as: which introduces the compensating divisor H  X  (  X  A S w ).
We propose replacing the information gain and the gain ratio criteria by choos-ing as splitting attribute for a node w an attribute that minimizes the distance d L. de M  X  antaras in [4] for the metric d 1 induced by Shannon X  X  entropy. Since one could obtain better classifiers for various data sets and user needs using values of  X  that are different from one, our approach is an improvement of previous results.

Besides being geometrically intuitive, the minimal distance criterion has the The first limitation insures that the choice of the splitting attribute will provide a high information gain; the second limitation insures that attributes with large domains are not favored over attributes with smaller domains.
 Suppose that in the process of building a decision tree for an object system S =( S, H, C ) we constructed a stump of the tree T that has n leaves and that the sets of objects that correspond to these leaves are S 1 ,...,S n . This means that we created the partition  X  = { S 1 ,...,S n } X  PART ( S ), so  X  =  X  S 1 +  X  X  X  +  X  S n . We choose to split the node v i using as splitting attribute the attribute A that minimizes the distance d  X  (  X  C S i , X  A S i ). The new partition  X  that replaces  X  is Note that  X   X   X  . Therefore, we have: This shows that as the construction of the tree advances the current partition  X  gets closer to the partition  X  C  X   X  . More significantly, as the stump of the tree grows,  X  gets closer to the class partition  X  C . Indeed, by Theorem 5 we can write: where  X  = { S 1 ,...,S n } . Similarly, we can write: d These equalities imply: If the choices of the node and the splitting attribute are made such that H  X  of the tree stump will decrease. Since the distance between  X  C  X   X  and  X  decreases in any case when the tree is exp anded it follows that the  X  X riangle X  determined by  X  C ,  X  C  X   X  ,and  X  will shrink during the construction of the decision tree. We tested our approach on a number of data sets from [12]. Due to space limi-tations we included only the results shown in Figure 1 which are fairly typical. Decision trees were constructed using metrics d  X  ,where  X  varied between 0.25 and 2.50. Note that for  X  = 1 the metric algorithm coincides with the approach of de M  X  antaras. We also built standard decision trees using the J48 technique of the well-known WEKA package [13].In a ll cases, accurracy was assessed through 10-fold cross-validation. The experimental evidence shows that  X  can be adapted such that accuracy is comparable, or better than the standard algorithm. The size of the trees and the number of leaves show that the proposed approach to decision trees results consistentl y in smaller trees with fewer leaves. We introduced a family of metrics on the set of partitions of a finite set that can be used for a new splitting criterion f or building decision trees. In addition to being more intuitive than the classic approach, this criterion results in decision trees that have smaller sizes and fewer le aves than the trees built with standard methods, and have comparable or better accuracy.

The value of  X  that results in the smallest trees seems to depend on the relative distribution of the class attribute and the values of the feature attributes of the objects. We believe that further investigations should develop numerical characteristics of data sets that allow predicting  X  X ptimal X  values for  X  ,thatis, values that result in the smalle st decision trees for data sets.

Another future direction is related to clustering algorithms. Since clusterings of objects can be regarded as partitions, metrics developed for partitions present an interest for the study of the dynamics of clusters, as clusters are formed during incremental algorithms [14], or as data sets evolve.

