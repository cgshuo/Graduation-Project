 Clustering documents is an important dimension of data mining and information re-trieval [Berkhin 2001; Xu and Wunsch 2005]. Clustering algorithms group a set of documents into subsets or clusters of similar documents. The aim of the algorithms is to create clusters that maximize similarities across documents in the same clus-ter (making the cluster internally cohesive) and minimize the similarities between documents in different clusters.

Document classification is a supervised learning methodology that uses training sets to simplify the algorithms and improve their performance [Salton 1989]. Unlike classification, clustering algorithms are unsupervised learning algorithms and do not require training sets [Jain 2010]. Though clustering is more complex, it is an attractive technique in several applications, as the topics and the associated training sets may not be known or easily generated.
 Several techniques have been developed for clustering, and a description appears in Steinbach et al. [2000], which introduces the basic approach I follow. These techniques may be grouped into two categories: hierarchical and partitional.
 Hierarchical algorithms can be arranged into two types: agglomerative and divisive. Agglomerative clustering starts with each document as a cluster of one and iteratively merges the most similar clusters until the algorithm reaches the desired number of clusters. Divisive clustering starts with all documents in one cluster and at each step splits each cluster into two clusters until the designated number of clusters is reached.
In the second category, the partitional (K-means) technique is a one-level partition of the documents into the nominated number of clusters. The first step of the algorithm is to choose a random centroid to each cluster. Following this, the algorithm iteratively creates a partition by assigning each document to the cluster with maximum similarity and then recalculates the centroids. The algorithm halts when the partition converges to a local minimum (each document remains in its assigned cluster). Section 3.2 pro-vides more details for implementing the K-means algorithm.

The hierarchical clustering approach is likely to provide better results but is ham-pered by quadratic time complexity in proportion to the number of documents [Kishida 2010]. In contrast, the K-means algorithms are more efficient as their running time is linear in the number of documents, but they produce clusters of inferior quality. A few hybrid algorithms that combine both approaches have been developed to increase the quality and reduce the execution time [Steinbach et al. 2000].

The bisect K-means [Steinbach et al. 2000] is a hybrid of the partitional and the divisive hierarchical techniques. It starts with one cluster and at each step splits the cluster into two using the standard K-means algorithm. It is usual to select the cluster with the maximum number of documents for bisection, but other criteria can be incorporated or substituted. The process of bisecting a cluster is repeated several times (ITER times), and the split that produces a higher similarity is selected (see Section 3.2 and Section 5.6). In this article, I use SKM to denote the standard K-means and BKM to denote the bisect K-means algorithms.

Similarity functions play a central role in document clustering [Huang 2008; Fu et al. 2004]. The functions impact the running time of the associated algorithms and the ac-curacy of the results. There are five commonly used functions: Pearson correlation coef-ficient, cosine, Jaccard coefficient, Euclidean distance, and averaged Kullback-Leibler divergence.

Purity and entropy are two commonly used measurements to evaluate the quality of clustering [Steinbach et al. 2000; Manning et al. 2008]. Purity captures the intracluster similarity, while entropy measures the intercluster similarity. The aim of clustering algorithms is to achieve high purity and low entropy. The F-measure is another quality measure used in information retrieval that combines precision and recall [Steinbach et al. 2000]. Dang et al. [2008] introduced a new measure for clustering effectiveness that assigns more than one cluster for each document.
 In recent years, there has been rapid growth in Arabic documentation on the web. Arabic is a morphologically rich and highly inflected language; consequently, many algorithms that were developed for the English language perform poorly when applied to Arabic [Newsri 2008]. In particular, Arabic stemmers tend to produce high stem-ming error ratios [Al-Shammari and Lin 2008]. While root-based stemmers produce overstemming errors, light stemmers [Larkey et al. 2007] suffer from understemming errors. Recent research suggests that lemmatization improves the accuracy of Arabic information retrieval [El-Shishtawy and El-Ghannam 2012].

In contrast to Arabic document classification, very few papers have been published regarding Arabic document clustering, which are briefed in Section 2. The motiva-tion of this research is derived from the survey of clustering techniques presented by Steinbach et al. [2000]. The results confirm that BKM marginally outperforms SKM in the analysis of Arabic documents. The rest of this article is organized as follows: Section 2 provides an overview of related work; Section 3 includes details of the clus-tering techniques; Section 4 presents details of the dataset and the implementation of the clustering algorithms; Section 5 discusses the experiments and the results; and Section 6 highlights the main results and outlines directions for future work. As mentioned in the introduction, the SKM technique is popular as it is simple, is efficient, and converges to a local minimum after a small number of iterations [Fahim et al. 2006]. Selecting the initial values for the centroids has a significant impact on the quality of results, and several strategies have been developed to improve the overall quality of clustering [Pe  X  na et al. 1999]. A large volume of research has been published investigating several applications with different optimizations of SKM [Jain 2010; Gang and Fei 2012].

Several papers investigated the use of parallel computing to reduce the lengthy running time of hierarchical algorithms [Cathey et al. 2007]. The authors suggested a parallel agglomerative clustering algorithm that can be used as a subroutine in the buckshot algorithm to cluster the complete TREC collection.

As discussed in the introduction, BKM is a hybrid of hierarchical divisive and SKM methods. Murugesan and Zhang [2011] presented a hybrid clustering algorithm that combines divisive and agglomerative clustering techniques and that outperforms the standard BKM. Archetti et al. [2006] introduced an induced BKM that outperformed the standard implementation of BKM. Kashef and Kamel [2009] enhanced BKM using intermediate cooperation. Tarczynski [2011] examined several concepts, metrics, and algorithms including BKM.

Although there is considerable published work on Arabic document classification, little research has been published related to clustering Arabic documents. This para-graph summarizes the research using SKM: Bsoul and Mohd [2011] investigated the impact of using an Arabic root-based stemmer (ISRI), where the five common similarity and distance functions have been used (Pearson correlation coefficient, cosine, Jaccard coefficient, Euclidian distance, and averaged Kullback-Leibler divergence). The paper suggests that stemming using the ISRI stemmer improves the quality of clustering. The authors used an in-house dataset with four categories and 1,680 documents. Sim-ilar to this article, Ghanem and Ashour [2012] compared light stemming, root-based stemming, and no stemming. They suggest that light stemming outperforms both root-based stemming and no stemming using precision and recall evaluation measures. Froud et al. [2013a] used text summarization to improve Arabic document clustering. In another paper, Froud et al. [2013b] used a new algorithm to extract key phrases in Arabic text, which resulted in improving the performance.

One paper discussed applying agglomerative technique to Arabic [Froud and Lachkar 2013]. The paper discussed the performance of agglomerative methodology using sev-eral similarity measures and different stemmers. The authors reported that stemming has a negative impact on the results and that cosine, Jaccard, and Pearson produce comparable results.
 To the best of my knowledge, no paper has been published that discusses applying BKM to cluster Arabic documents. Document representation and similarity (distance) functions are essential elements of clustering algorithms. In addition, to measure the quality of the algorithm, the output is compared to human judgment. This section provides details of document representation, clustering algorithms, the five similarity and distance functions, and the quality measures used in this study. The tfidf representation, term frequency inverse document frequency, is widely used to represent documents in most data mining and information retrieval applications [Salton and Buckley 1988]. In this representation, the frequencies of the terms in the documents are considered, while the positions of the terms within the documents are ignored.

Let D be the set of all documents, and let tf ( a , t ) be the frequency of the term t in document a  X  D , normalized by the size of the document. The inverse document the number of documents containing the term t . The aim of idf ( t )istomitigatethe negative impact of common terms that appear in most documents and are not beneficial to information retrieval. A document a  X  D is represented by the tfidf values of the terms it contains, which for each term t is calculated as
Let T ={ t 1 ,..., t m } be the set of all terms in the collection D, and let w t tfidf ( a , t i ) , 1  X  i  X  m , be the tfidf value of the term t i in the document a . For each document in the dataset, Algorithm 1 creates an associated text file that contains the list of terms and their corresponding tfidf values.
 The implementation of BKM and SKM follows from the description that appears in Steinbach et al. [2000]. Algorithms 2 and 3 outline the basic steps for both algorithms. Similarity and distance functions have a crucial impact on information retrieval al-gorithms [Huang et al. 2012]. The Pearson correlation coefficient, cosine, Jaccard co-efficient, Euclidean distance, and average Kullback-Leibler divergence functions are widely used in document clustering algorithms [Huang 2008]. For the Euclidean dis-tance, the similarity is calculated by subtracting the distance from 1 (see the equation later). Each centroid of a cluster is represented by the sum of all the tfidf values of all the documents assigned to the cluster normalized by the number of documents. Con-sequently, calculating the similarity between a document and a centroid is the same as calculating the similarity between two documents. In the rest of the article, Pearson denotes the Pearson correlation coefficient, Jaccard denotes the Jaccard coefficient, and KLD denotes the average Kullback-Leibler divergence.

The previous five functions calculate the similarity between documents a and b, as follows: 4. Euclidean(a , b) = 1  X  ( m i = 1 | w t 5. KLD(a , b) = m i = 1 (  X  1 XD ( w t As mentioned earlier, purity and entropy are commonly used to evaluate the quality of clustering documents. Let the output of the clustering algorithm be the set of clusters ={ w (ground truth) assigned these documents to the different classes. 3.4.1. Purity. The purity of clustering, P ( , C ) , is calculated as defined in Manning et al. [2008], which is equal to 1 N k max j | w k  X  C j | , where N is the total number of value of purity is 1 when the sets and C are identical. Consider the set of documents in cluster w i . Within this set, we group the documents according to their membership of the classes in C . To calculate the purity, we take the size of the maximum (dominant) group. The higher the size of the dominant group, the better the purity is. 3.4.2. Entropy. The entropy for a given cluster evaluates the distribution of categories within the cluster. Lower values indicate better clustering, where 0 indicates perfect clustering and higher values could be bigger than 1. The calculation of entropy follows the standard formula [Steinbach et al. 2000].

Let p i , j be the  X  X robability X  that a member of cluster j belongs to class i . The entropy is the sum of the entropies of all clusters weighted by the corresponding sizes: number of documents.
 The experiments in this study use the dataset that appears in Abuaiadah et al. [2014]. The dataset is publicly available for download from http://diab.edublogs.org/dataset-for-arabic-document-classification/. It has nine categories: Art, Economy, Health, Law, Literature, Politics, Religion, Sport, and Technology. Each category contains 300 docu-ments.

The documents were collected manually from well-known Arabic websites and con-verted to text files. The dataset has five versions. Version 1 (V1) represents the docu-ments without preprocessing, and Version 2 represents the documents after removing stop words but without stemming. The list of stop words was generated manually. The documents in Version 2 are used to create documents for Version 3, Version 4, and Version 5 (V2, V3, V4, and V5). V3 represents the results of applying the Light10 stemmer [Larkey et al. 2007], which strips off a predefined set of prefixes and suffixes. V4 represents the results of applying Chen X  X  stemmer [Chen and Gey 2002], which expands the predefined set of prefixes and suffixes of the Light10 stemmer. V5 is the result of engaging the Khoja root-based stemmer [Khoja and Garside 1999]. Table I contains the basic details of the datasets.

The use of different stemmers results in different term frequencies of the associated documents that have an impact on the performance and the running time of information retrieval algorithms. As mentioned previously, the dataset has five versions that correspond to removing stop words and to three leading stemmers. Consequently, the implementation of removing the stop words and the three stemmers is not required.

The first phase of the implementation is to create the tfidf files as described in Sec-tion 3.1. A straightforward in-house Java program creates these files for each version of the dataset. The program scans all the files and uses the hash table data structure to calculate the idf value of each term. Following this, for each document, another hash table is used to calculate the frequency of each term. Finally, to calculate the tfidf val-ues, the frequency of each term is normalized with respect to the size of the document and multiplied by the corresponding idf value. The set of all the terms in the document and their associated tfidf values are written to a text file. This text file, the tfidf file, represents the document and is used as an input for the clustering algorithms.
All tfidf files are stored in one directory, and integers (converted to strings) are used to name these files. The integer value of the name of a file reveals the class to which the file belongs (ground truth). This naming technique facilitates the calculation of the purity and entropy values.

Each centroid of a cluster is initialized by selecting a random tfidf file where the terms and the associated tfidf values are read from the tfidf file and stored in a hash table. There are two main steps in SKM: assigning a document to a cluster and recalculating the centroids. For the first step, the associated tfidf file is read from the text file to a hash table in memory. The similarities between a document and the centroids (using the associated hash tables) are calculated, and the document is assigned to the cluster that has the maximum similarity. Then the hash table associated with the document is discarded from memory. For the second step, the tfidf file is also read from the text file to a hash table in memory and added to the hash table associated with the centroid. Then, the hash table associated with the file is discarded from memory. This process repeats itself in each iteration of SKM. This manipulation removes the memory limitation, but the running time might not be efficient. Since bisecting a cluster is done using SKM, this manipulation is also applied to BKM.
 For BKM, several published papers set the value of ITER to 5 [Archetti et al. 2006; Steinbach et al. 2000; Tarczynski 2011]. In this article, BKM implies that ITER = 5. For comparison purposes, two variations were added: BKM_1 and SKM_5. BKM_1 is BKM with ITER = 1, and SKM_5 is a variation of SKM with respect to step 3 in BKM, namely, repeat the selection of the random centroids five times and choose the selection that produces the maximum overall similarity between all documents and their corresponding centroids.

An in-house Java program implements the four clustering techniques and calculates the purity and entropy values.

The correctness of the implementation was validated and tested by clustering the well-known 20news dataset (http://people.csail.mit.edu/jrennie/20Newsgroups/). The achieved results (purity and entropy) are comparable to what has been published in Huang [2008]. The values of purity and entropy are used to evaluate the quality of clustering. The experiments include all combinations of the five similarity functions, the five versions of the dataset, and the four clustering techniques. The experiments repeat each run 10 times and calculate the average. Since consecutive runs could produce marginally different purity and entropy values, the standard deviation of purity (Std-Purity) is also presented. The experiments show that any improvement for purity results in an improvement for the entropy value. Therefore, the standard deviation for entropy is not presented.

The results for the Euclidean distance are significantly inferior to all other func-tions for the five versions of the dataset, and the standard deviation of these runs is remarkably high. The following triplets (version of the dataset, purity for BKM, purity for SKM) summarize the results: (V1, 0.37, 0.11), (V2, 0.45, 0.11), (V3, 0.55, 0.25), (V4, 0.63, 0.3), and (V5, 0.7, 0.43). It is noticeable that the increased stemming presents significant improvements but the results remain significantly inferior to other similar-ity functions. For testing purposes, the experiments included applying the Euclidean distance to the 20news dataset, and for SKM, the results were comparable to what was reported in Huang [2008].

For the KLD function, the purities are (V1, 0.82, 0.72), (V2, 0.85, 0.74), (V3, 0.86, 0.80), (V4, 0.87, 0.76), and (V5, 0.76, 0.71). The results are better than that of the Euclidean distance but after removing stop words and stemming the results are inferior to the other three functions. It is worth noting that for BKM and V1 of the dataset, this function outperforms the other functions, which could highlight its usefulness where preprocessing documents is a challenge (e.g., Arabic dialect or slang).

Since the results of these two functions are noticeably inferior and to improve the readability of this section, a detailed analysis is limited to Pearson, cosine, and Jaccard functions.

The first set of results is grouped around the five versions of the dataset (Sections 5.1 X  5.5). Tables II through VI contain the results for the four techniques, three measures, and three functions.

The second set of experiments draws the values of purity and entropy as a function of ITER values for BKM when using the cosine function and V5 of the dataset. Table II shows the results for the three functions. Without preprocessing the dataset, the results are inferior to other versions of the dataset but are useful for comparing BKM to BKM_1, SKM, and SKM_5.

It is noticeable that for the cosine and Jaccard functions, the purity of BKM (ITER = 5) is better than that of BKM_1 (ITER = 1) by 14%. This shows that repeating the split of a cluster and choosing the one that produces maximum similarity is crucial to the performance of BKM. For the Pearson function, BKM outperforms BKM_1 by more than 7%.
 For BKM_1, the Pearson function significantly outperforms the cosine and the Jaccard functions. Removing stop words appears to have a decisive factor on the performance as it pro-duces variable improvements to the purity and entropy in all settings (combinations of the four techniques and the three functions), as shown in Table III. There are dramatic improvements for the purity and entropy of BKM and BKM_1 but less significant im-provements for SKM and SKM_5. BKM marginally outperforms BKM_1, SKM, and SKM_5 (BKM_1 has inferior results for V1 of the dataset, and the dramatic improve-ment for V2 did not improve the overall performance).

The purity for BKM reached 0.897 when using the Pearson function and 0.882 when using the cosine function (more than 10% increase compared to version V1 of the dataset for both functions). Removing stop words has less impact on the results when using the Jaccard function compared to that of the cosine and Pearson functions.
For BKM, there is a noticeable reduction for the values of standard deviation for all functions. For SKM and SKM_5, the standard deviation is high. This indicates that removing stop words does not contribute to stabilizing the results for SKM and it continues to be sensitive to the initial random selections of centroids.

The purity and entropy values are comparable for BKM_1, SKM, and SKM_5. Since the standard deviation is high, another set of experiments could produce slightly dif-ferent results for these three techniques. Table IV shows the results for V3 of the dataset that correspond to the Light10 stemmer. The Light10 stemmer provides additional improvements to the purity and the entropy values for all settings. BKM outperforms SKM by more than 8% when using the Pearson and cosine functions.

When using the Jaccard function, the Light10 stemmer has a greater impact on SKM compared to that of BKM. The purity of SKM reached 0.884, which is the best purity achieved for SKM in all settings but is still less than that of BKM.

For BKM, the values of the standard deviations are significantly lower than the corresponding values of SKM and SKM_5. This indicates that the Light10 stemmer is beneficial for stabilizing the results for BKM but not for SKM. This also indicates that there is a potential to optimize SKM.

It is worth noticing that for the Jaccard function, the entropy of SKM and SKM_5 are better than that of BKM. Table V shows the results for V4 of the dataset. For the Pearson function together with BKM, there is a minor improvement in the purity of BKM compared to that of V3. It reached 0.927, which is the best purity for all settings. In addition, there is a reduction in the standard deviation, which indicates that more stemming provides more stability in BKM. For BKM_1, SKM, and SKM_5, the purity and entropy values are slightly worse. For these three techniques, the standard deviation is relatively high. Using the purity value, BKM outperforms SKM by more than 10%.

For the cosine function, for BKM, the purity and entropy values are comparable to that of V3 and the standard deviation is noticeably low. For the other techniques, the standard deviations remained high, which leads to the conclusion that purity and entropy values are comparable to that of V3, although there are minor deteriorations.
For the Jaccard function, Chen X  X  stemmer (V4 of the dataset) deteriorates the pu-rity value for SKM compared to that of V3 (0.884  X  0.844). For BKM, the purity is comparable to that of V3. The standard deviation is slightly higher compared to that of the cosine and Pearson functions. Table VI shows the results for the Khoja root-based stemmer (V5 of the dataset). Compared to V3 of the dataset, for the Pearson function, the purity of BKM is lower by nearly 3% (0.927  X  0.896) and there is slight deterioration in the entropy as well, but there is a minor improvement for BKM_1 (0.850  X  0.878). There are no noticeable changes in the results of SKM and SKM_5.

When using the cosine function, the results are comparable to that of V3 and V4 (the root-based stemmer is competitive with the two light stemmers). BKM has a more than 8% purity advantage over SKM. SKM_5 has slightly lower purity than SKM, but the standard deviation is noticeably higher. This suggests that repeating the random selection of the initial centroids and choosing the selection with the maximum overall similarity produce a slight deterioration in the purity and the stability of the results.
When using the Jaccard function, BKM achieved comparable results to that of the cosine function and to that of V4. The combination of the root-based stemmer and the Jaccard function slightly improves the results for SKM compared to that of V4. The results in previous sections indicate that the results for BKM are sensitive to the values of ITER. Though changing the ITER values does not change the theoretical asymptotic running time, it does significantly slow down the application in practice. Figure 1 shows the values of purity and entropy for ITER values from 1 to 15 when using Khoja X  X  stemmer (version V5 of the dataset) and the cosine function. The experiments show a significant improvement in purity and entropy when the value of ITER is increased from 1 to 2 but no improvement when ITER is more than 4. For the Pearson and cosine similarity functions, the experiments showed that BKM remarkably outperformed SKM. Furthermore, it was shown that while removing stop words provided significant increases in BKM performance, the three stemmers pro-vided modest comparable improvements. Moreover, removing stop words and stem-ming significantly improved the stability of the results for BKM but had little to no impact on SKM.

For the Jaccard similarity function and after stemming, the results for BKM were comparable to that produced by the cosine and Pearson functions. In most settings, the Jaccard function produced results that are less stable than that of the cosine and Pearson functions. Removing stop words and stemming have a greater positive impact on the results of SKM compared to that of the cosine and Pearson functions. Nevertheless, BKM outperformed SKM.

Khoja X  X  root-based stemmer significantly reduces the size of the dataset (the number of different terms and the length of each term), which results in a significant practical improvement in the running time of the associated applications. For BKM, it was shown that Khoja X  X  stemmer achieved 0.907 purity for both functions and is competitive with the two light stemmers when evaluating the quality of the results. Consequently, in practical applications, Khoja X  X  stemmer is likely to significantly speed up BKM without any deterioration in the quality of the results. However, if the running time of clustering is crucial to the application, the combination of SKM, the Jaccard function, and the root-based stemmer attained 0.865 purity and could be a competitive candidate.
For applications where the practical running time is important, for BKM, the value of ITER is critical. In this case, comparing BKM_1 to SKM depends on the employed function. For the Pearson function, BKM_1 still outperforms SKM. For the Jaccard function, SKM outperforms BKM_1. For the cosine function, there is no clear advantage to BKM_1 or SKM.

The standard deviations for purity of SKM are high in all settings and in particular for V3 of the dataset. As published in many papers, this confirms that the results are sensitive to the random selections of the centroids. In addition, this implies that SKM could be improved to compete with or outperform BKM.
 The results achieved when using the KLD function are worth further investigation. Though this function is significantly inferior to the Pearson, cosine, and Jaccard func-tions after removing stop words and stemming, it actually outperforms these three functions before preprocessing the dataset. For Arabic, which is spoken over a wide region with multiple dialects each with their own slang, this could be valuable as preprocessing is a challenge for such documents.

For clustering, little work has been done to date on Arabic documents. The present article is the only published work the author is aware of on the subject of bisect K-means. Further study is required to investigate the different clustering algorithms, particularly the hierarchical approach.

