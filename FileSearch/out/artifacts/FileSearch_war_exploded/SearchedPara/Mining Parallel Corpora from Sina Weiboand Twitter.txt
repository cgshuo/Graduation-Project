 Google DeepMind Feedzai Research Carnegie Mellon University Carnegie Mellon University Instituto Superior T  X  ecnico University of Lisbon INESC-ID  X  X elf-translated X  messages targeting audiences who speak different languages, either by writing the same message in multiple languages or by retweeting translations of their original posts in a second language. We introduce a method for finding and extracting this naturally occurring parallel data. Identifying the parallel content requires solving an alignment problem, and we extract nearly 3M Chinese X  X nglish parallel segments from Sina Weibo using a targeted crawl of
Weibo users who post in multiple languages. Additionally, from a random sample of Twitter, we obtain substantial amounts of parallel data in multiple language pairs. Evaluation is performed by assessing the accuracy of our extraction approach relative to a manual annotation as well as in terms of utility as training data for a Chinese X  X nglish machine translation system. Relative translation quality improvements in translating microblog text and modest improvements in translating edited news content. 1. Introduction In the span of about two decades, the Web has evolved from a collection of mostly static
Web pages created, to dynamic, interactive content, and to content created by users themselves. The advent of microblogs, such as Facebook and Twitter, has particularly transformed the kind of information that is published, since traditional barriers to publication (e.g., expertise in Web development) have been virtually eliminated by making publication easy for anyone with Web access. Microblog posts are created in the service of a variety of communicative and identity-building goals (Marwick and print media, user-generated content on social media can be informal, colloquial, and is in particular marked by innovative and varied use of orthography. For example, we can readily find tweets like ( R U still with me or what? ) and nonstandard abbreviations ( idk! smh ).
 translation) face particular difficulty with this new kind of content. On one hand, these have been developed with the conventions of more edited genres in mind. For example, they often make strong assumptions about orthographic and lexical uniformity (e.g., completely unrelated lexical items). While modeling innovations are helping to relax these assumptions (Han and Baldwin 2011; Ritter et al. 2012; Owoputi et al. 2013; Ling that our tools are learned from are drawn from edited genres, and poor generalization from edited to user-generated genres is a major source of errors (Gimpel et al. 2011; Kong et al. 2014).
 social media sites that is suitable for training machine translation (MT) systems. In MT, the domain mismatch problem is quite acute because most existing sources of parallel data are governmental, religious, or commercial, which are quite different from user-generated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Addition-ally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014).
 able number of microblog users tweet  X  X n parallel X  in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English X  X hinese parallel messages, for example, watup Kenny
Mayne!! -Kenny Mayne !!, where an English message and its Chinese 308 their publicists) who tweet in multiple languages; we see this with casual users as well. For example, on Twitter, we found | I don X  X  like this Ehomaki!!: http://t.co/9FTXgV0w , which contains a Japanese to English translation, separated by | and followed by an image link, which is displayed as an image when viewed on the Twitter Web site. Other examples of parallel posts are shown in Figure 1. We note that these parallel posts contain information that is valuable, since they contain elements that are rare in standard edited genres. In the examples we have given here, the terms watup and Ehomaki are not correctly or not translated by online MT systems.
 although later in the article we will provide an analysis of the content that provides further insight into the question. One class of parallel posts are found in public celebri-ties X  profiles (e.g., Snoop Dogg X  X  Sina Weibo posts), done to enhance their popularity in non-English markets. Another class is posted by people who live or have lived abroad and wish to communicate with people who speak different languages. In fact, our first contact with such posts was with the parallel messages that Chinese students at
Carnegie Mellon University were posting on Facebook so their friends both in China and in Pittsburgh could read them.
 and MT methods. As part of the elaboration of this work, we made the following contributions to the field of NLP and MT.
 parallel segments from each tweet using a dynamic programming algorithm. Third, a classifier determines if the proposed parallel segment is correct. If so, the material is extracted, otherwise it is discarded.
 in which we obtained over 1 million Chinese X  X nglish parallel segments from Sina
Weibo, using only their public application program interface (API). This automatically extracted parallel data yielded substantial translation quality improvements in trans-lating microblog text and modest improvements in translating edited news. Following this work, we developed a method for crowdsourcing judgments about parallel seg-language pairs and for the Twitter domain. This article extends these two papers in several ways: 310 data extraction. Section 3 presents our model to align segments within one single docu-ment and the metrics used to evaluate the quality of the segments. Section 4 describes the extraction pipeline used to extract the parallel data from Sina Weibo and Twitter.
Section 5 describes the method used to obtain gold standards for translation in different languages. We then present, in Section 7, the experiments showing that our harvested data not only substantially improve translations of microblog text with existing (and arguably inappropriate) translation models, but that they improve the translation of more traditional MT genres, such as newswire. We conclude in Section 8. 2. Related Work
The automatic identification and retrieval of translated text ( bitext ) is a well-studied problem in natural language processing. In particular, there has been a long tradi-tion of exploiting the multilingual Web as a source of bitext (Resnik and Smith 2003;
Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012). In general, existing approaches extract bitext in two steps.
 retrieved from a set of documents using an efficient retrieval mechanism. This step is necessary to narrow down the potential candidates for parallel Web sites, since consid-ering all possible pairs in a given set of Web pages is intractable (a direct comparison of O ( n 2 ) documents would be impractical for any modestly sized collection). Resnik and
Smith (2003) used features from URLs to build this set of pairs, by checking for Web sites with patterns that may indicate the presence of a parallel Web site. For instance, the pattern lang=en generally indicates that by changing en to another language, such as pt , we may find another Web site, which is the translation of the original one. However, this method has a low recall, because it ignores the content within the Web document.
Uszkoreit et al. (2010) proposed a content-based approach to improve retrieval that executes this first step by translating all documents into English using a baseline trans-lation system, and then finds rare (i.e., informative) n -grams that are common across
Web documents with different original languages using an inverted indexing approach. translation pair by examining the content of the documents. Resnik and Smith (2003) proposed structural filtering, a language independent approach, which compares the
HTML structures of the pair of documents, as well as a content-based approach. One possible implementation is to word-align (Brown et al. 1993) the documents using an existing lexical translation model, and then compute the following score: score is higher than the threshold.
 because our method will also depend on it. Given a source sentence x =  X  x in x and y , IBM Model 1 defines the joint probability of a lexical alignment a and translation y given input x as where a  X  [0, n ] m gives the alignment of each word in y to a word in x (or a null word, indicating it is not a translation), and t ( y i | x a that word y i is the translation of x a to all alignment configurations. Although this is an obviously flawed assumption, the posterior alignment probability under Model 1 (i.e., P M1 mative. More robust models make less-naive prior assumptions and generally produce higher-quality alignments, but the uniform prior probability assumption simplifies the complexity of performing inference. Despite its simplicity, Model 1 has shown particu-larly good performance as a component in sentence alignment systems (Xu, Zens, and Ney 2005; Braune and Fraser 2010).
 segments from comparable corpora (Smith, Quirk, and Toutanova 2010; Munteanu,
Fraser, and Marcu 2004). Smith et al. (2010) uses conditional random fields to identify parallel segments from comparable Wikipedia documents (since Wikipedia documents in multiple languages are not generally translations of each other, although they are about the same topics). The work of Jehl, Hieber, and Riezler (2012) uses cross-lingual information retrieval techniques to extract candidate English X  X rabic translations from
Twitter. These candidates are then refined using a more expressive model to identify translations (Xu, Weischedel, and Nguyen 2001).
 example, the work on mining parenthetical translations (Lin et al. 2008), which attempts to find translations within the same document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quality control. Thus, in order to find good translations, subsequent post-editing and/or ranking is generally necessary. 3. The Intra-Document Alignment (IDA) Model
As discussed above, content-based filtering is a method for parallel data detection that relies on lexical translation/alignment models to provide evidence that a candidate pair is actually a translation. However, in previous work, the candidate pairs have been independent documents. In contrast, in our problem, translated material is embedded in a single document, with different regions in different languages. In this section, we describe a model for identifying translated material in these documents.
  X  x to find the location of the parallel segments, the languages of those segments, and their word alignments (for simplicity, we assume that there are at most two continuous spans of the left and right segments. To avoid degenerate solutions due to overlapping content, we require that p  X  q &lt; u  X  v . We use l and r to identify the language of the left and right segments, respectively. Finally, a represents the word alignment between the words in the left and the right segments. An example analysis is shown in Figure 2.
In the given tweet, the left and right segment spans [1, 5] and [11, 13] are identified in (A), the languages for these spans zh and en in (B), and the alignments (2, 12), (3, 13), indicating that words in indexes 2 and 3 are aligned to words at indexes 12 and 13, are shown in (C).
 a dynamic programming search algorithm for finding the best analysis of a tweet under the model (Section 3.2). The basic version of the algorithm is tuned to detect parallel data in a particular pair of languages, and in Section 3.3, we discuss an extension of algorithm to deal with more than a single language pair. Finally, we will describe the metrics used to evaluate the quality of the extracted parallel segments in Section 3.4. 3.1 Model Components
The core of our parallel span detection model is IBM Model 1 (whose parameters are learned from seed parallel data). This model assigns a conditional probabilities
Although this model will assign higher probabilities to translated content, there are spans, since longer spans must consist of more (conditionally independent) generation events whose probabilities multiply. To understand the problem, consider the example in Figure 2, and assume that we know the language pair l , r and the word alignments a .
Suppose that the Chinese characters and are aligned to the words fighting and
The desired translation from to We fighting together ; (2) the segment of all aligned words translating from to fighting together ; (3) and the segment with a word-to-word translation from to together . According to Equation (2), the translation probabilities would be calculated as: 314 words. This can be seen by comparing the first and second cases, where it is evident that the first case X  X  translation probability is bounded by the second one regardless of the word translation probabilities, as the normalization factor to the number of words in the segments. A more aggravating issue is the fact that the model will always pick word-to-word translation segments. This can be seen by comparing case (2) and (3), where we can observe that the second case is bounded by the third one, as the second case is a product of more translation probabilities. obtains a lower score than (2) and (3), because it contains unaligned words. the spans of the parallel segments, their languages, and word alignments. This model is defined as follows: score ([ u , v ], r , [ p , q ], l , a | x ) =
Each of the component scores (  X  S ,  X  L , and  X  T ) returns a score in the interval [0, 1]. We now describe the role each score plays in the model. ously, we relied on IBM Model 1 probabilities for this score:
This equation is a reformulation of Model 1 presented in Equation 2 for a single docu-ment with the segment p , q as source, and u , v as target. The lexical tables t various language pairs are trained a priori using available parallel corpora. The null translation probability is set to , which is set to a negligible probability, so that these are only used if x i cannot be aligned to any word. Note that the translation score by itself also allows the identification of the language pair l , r , as using a lexical table from an incorrect language pair is likely to yield lower scores.
 ities for pairs of words in some language pairs tend to be higher, as there are fewer equivalent translations for the same word. This would bias the model to pick those lan-guage pairs more often, which is not desirable. Thus, we redefine this score by adapting the content-based matching metric, which assumes lexical translation uniformity: performed under IBM Model 1.
 ferent set of alignments, we also compute the alignments from r to l . These alignments are generated for both directions, which are denoted as a is defined as: spans [ p , q ], [ u , v ] is defined as:  X  ( l , r | [ p , q ],[ u , v ] | x ) = segmentations that cover more words in the document. It assigns the highest probability to segmentations that cover all the words in the document, which encourages the model to favor larger segments.
 whether all the segment boundaries are valid according to the list  X  of violations. The only exception occurs when there are no valid boundaries, in which case  X  returns 1.
This violation list allows the definition of hard constraints regarding a segment X  X  valid-ity based on prior knowledge.
 unicode range tend to belong to the same sentence. For instance, in the example in
Figure 2, segments fighting together or fighting would not be valid, because they do not include the whole sequence of Latin words We fighting together . However, the segment tag: We fighting together or any other segment that contains the whole sequence of Latin words is acceptable. The same applies to the Chinese segment .
 contain the corresponding parenthesis ender. For instance, for the tweet Yoona taking the  X   X  (be healthy)  X   X  , the segment (be healthy would not be valid, because it does not contain the parenthesis closer ) . However, both (be healthy) and be healthy are acceptable.
The exception to this rule occurs when either the parenthesis starter or ender is missing in the tweet, in which case this violation is ignored. We consider the following universal starter and ender pairs: () , [] , and {} ; as well as the Chinese and Japanese pairs: (), , [], and dc .
 in Figure 2, as the span score will give preference to longer segments and the list of violations will force the model to use complete sentences. Consider the French X  X nglish example in Figure 1, and assume that the alignment model is able to align est to is , le to the and the left question mark ? is aligned to the right one. In this case, the current model would be able to find the correct segments since the list of violations would enforce that the left segment contains Qui est le v  X eritable avare and the right segment contains Who is the real miser? . Furthermore, as the question marks are aligned, the model is capable 316 of correctly extracting the translation from Qui est le v  X eritable avare? to Who is the real miser? .
 based on structural features like parentheses, such features are not always present. For instance, consider that the previous example was tweeted as Who is the real miser Qui est le v  X eritable avare . We can see that correctly translated segments from Who is the real to the span score, because they cover all words in the tweet. This is because the model does not know whether miser belongs to the English or French segment. One solution to solve this would be to augment the lexical table so that the alignment model can align the word miser to avare , so that the translation score is higher in the first case. However, this is an unreasonable requirement as it would require large amounts of parallel data. An easier solution would include a language detector in it will be placed in the French segment. This information is encoded in the language score. language labels l , r are appropriate to the document contents: where P L ( x | l ) is a language detection function that yields the probability that word x is in language l . In our previous work (Ling et al. 2013), this was a 0-1 function based on unicode ranges, which assumed that all Latin words were English, all Han Characters were Chinese, and all Perso-Arabic letters were Arabic. This was not a problem, because we were only extracting Arabic X  X nglish and Chinese X  X nglish pairs. To cope with any language pair X  X ncluding pairs where both languages have the same unicode range X  we estimate P L ( x | l ) by training a character-based language detector the posterior probability of a language l given a word x .
 tioned above, so that we can address tweets with languages in the same unicode using the alignment model. The reason for this is the fact that some languages, such as Portuguese and Spanish or Chinese and Japanese, contain overlapping words. For instance, in the tweet Have a nice weekend! #ohayo , the characters mean weekend in both Chinese and Japanese. Thus, if these are the only characters that are aligned during the computation of the translation score, both languages are equally likely to be correct. However, the Hiragana characters , , and that surround it only exist in Japanese, so the entire sequence is much more likely to be
Japanese than Chinese. These details are captured by the character-based language score. 3.2 Searching for the Best Analysis
In our search problem, we need to efficiently search over all pairs of spans, languages, and word alignments. We will show that (assuming a Model 1 alignment model), a dynamic programming search algorithm is available. Our search problem is formalized as follows: with variables defined as in the previous section. A high model score indicates that the predicted bispan [ p , q ], [ u , v ] is likely to correspond to a valid parallel span.  X  the span score  X  S for all combinations of u , v , p , q values only requires a sum of boundary positions. The set of violations  X  only needs to be computed once for each sample.
 only need to be computed once. The detector can compute the language probabilities thermore, the number of character n -grams that need to be considered is relatively small, as we are applying it on words rather than documents. Once P puted for all possible values of l and x , calculating the language score can be trivially computed.
 alignments in  X  T over all possible segmentations, which requires O ( | x | using a naive approach [ O ( | x | 4 ) segmentation candidates, and O ( | x | document with 40 tokens would require approximately 7 million operations in the worst case for  X  T to be computed for all segmentations. To process millions of documents, this process needs to be optimized.
 without resorting to approximations. This is done by showing that, under Model 1,
Viterbi alignments do not need to be recomputed every time the segment spans are changed, and can be obtained by updating previously computed ones. Thus, we pro-pose an iterative approach to compute the Viterbi word alignments for IBM Model 1 using dynamic programming.
 expensive, and we will show that we can limit the number of pairs without approxima-tions by using the language identification approach described in Section 3.3.
Dynamic programming search. We leverage the fact that the Viterbi word alignment of a bispan (or pair of spans) under Model 1 can be reused to calculate the Viterbi word alignments of larger bispans. The algorithm considers a four-dimensional chart of bispans and computes the Viterbi alignment for the minimal valid span (i.e., [0, 0], [1, 1]).
Then, it progressively builds larger spans from smaller ones. Let A chart can be manipulated using  X  recursions, which guarantees that the new alignment 318 will be maximized according to IBM Model 1. The following update functions are defined: value of the best cell. The most important aspect to consider when updating existing a b -A
B used defines the performance of the system. Most spans are reachable using any of the four update functions. For instance, the span A 2,3,4,5 can be reached using  X   X  instance, the state A 2,2,3,4 cannot be reached using  X  exist, since it breaks the condition p  X  q &lt; u  X  v . In this situation, incrementally more expensive updates must be considered, such as  X  + v or  X  + q of complexity. Finally, we want to minimize the use of  X  recomputation of the Viterbi alignments. Formally, we define the following recursive formulation that guarantees a most likely outcome:
This transition function applies the cheapest possible update to reach state A reach the conclusion that this algorithm runs at O ( n the worst update  X  + q , we observe that this is only needed in the following cases update is quadratic in the worst case, the complexity of these operations is O ( n denotes an arbitrary number within the span constraints but not present in previous twice, this update also takes O ( n 3 ) operations in total. The update  X  and a linear update, it runs in O ( n 4 ) time. Finally, update  X  but is needed for all remaining cases, so it also requires O ( n summing the executions of all updates, we observe that the order of magnitude of our exact inference process is O ( n 4 ). Note that for exact inference, a lower order would be unfeasible, because simply iterating all possible bispans once requires O ( n does require more complexity when computing the most probability alignment for a fixed sentence pair, thus it would increase the complexity of our search algorithm. Recall that  X  + u is the preferred update to be used whenever possible during inference, as it only takes one operation. However, this is only true for Model 1. In Model 2, an absolute distortion model must be considered that takes into account the distance between the positions of the right segment and the aligned word in the left segment. However, if we remove the first word in the right segment, we would be shifting the word positions of all words in the right segment, which would require the recomputation of the absolute distortion values of all remaining words in the right segment. This would raise the complexity of this operation to O ( n ). Thus, with the new set of update functions, the optimal order of updates would require O ( n 5 ) operations. 3.3 Language Pair Filtering
The algorithm just proposed takes the pair of languages as inputs. If we are looking for a particular language pair, this is fine, but when we search for translations in a large 320 number of language pairs, this is impractical. However, in most cases, many language pairs can be trivially ruled out. For instance, if there are no Han characters, any pair involving Japanese or Chinese can be excluded. Thus, one can efficiently filter out unlikely language pairs without losses.
 cal translation table in each operation, which is computationally expensive. However, this operation is only expensive if we compute the translation score, as the span and language scores can be more efficiently computed.
 score, in the [0, 1] interval. This means that if we know that the highest score value for Chinese X  X nglish is 0.4, and wish to check if we can obtain a better score for Arabic X 
English, we can first compute the S inc score for Arabic X  X nglish, and check if it is higher language pair, as it will not be higher than 0.4. Thus, if we find that the highest S score among all possible segmentations of p , q , u , v score for a given language pair l is lower than score for any other language pair, it follows that l highest scoring language pair. More formally, we can discard a language pair l the following condition applies:
Then, starting from the highest scoring language pairs l , r , we compute their real IDA scores, while keeping track of the highest IDA score. The algorithm can stop once it reaches a language pair whose incomplete IDA score is lower than the highest real IDA score, as we know that they will never achieve the highest real IDA score. 3.4 Evaluation Metrics
The goal of the IDA model is to find the most likely parallel segments [ p , q ][ u , v ], their languages l , r , and the alignments a . We also define an evaluation metric that compares the predictions of our model with those obtained by manual annotation. That is, we test whether the predicted values of [ p h , q h ], l h , [ u [ p evaluate the predictions made by our model.
 and [ a , b ]  X  [ a 0 , b 0 ]. The intersection between two segments [ a , b ]  X  [ a [ a , b 0 ]) computes the number of tokens within the union of the intervals, given by [min( a , a 0 ), max( b , b 0 )]. One important aspect to consider is that the segments can span half words, which can happen if there is a missing space between a sentence pair bound-ary, such as uneasyBom , which contains the English word uneasy and the Portuguese word Bom . To cope with this, we add the fractional count as the ratio between the number of characters that is included in the interval and total number of characters in the token. Thus, the segment corresponding to uneasy in uneasyBom would correspond to two-thirds of a single word.
 [ a , b h ] with language l r as: ence, as well as each token in the reference that is not in the hypothesis. Furthermore, segments that differ in language will have a zero score. Unlike our previous work, we decided not to evaluate the language pair detection as a separate task, as only a negligible number of spurious parallel sentences (less than 0.1%) are caused by a incorrect detection of the language pair.
 of the parallel segments: it emphasizes that both parallel segments must be accurate to obtain a high score. This is because parallel segments are only useful when both sides are accurate on the span and language. 4. Parallel Data Extraction
The previous section describes a method for finding the most likely segments p , q , u , v and the language pair l , r and the alignments a , according to the IDA model score, for any document x . However, extracting parallel sentences using this model requires addressing other issues, such as identifying the tweets that contain translations from those that do not. This section will describe how the parallel data extraction process is performed.
 which we denote as T . The process is divided into three steps. First, we filter the set T in order to remove all monolingual tweets, which will result in a set T solely of multilingual tweets, which substantially reduces the number of tweets that need to be processed by the following steps. Second, assuming that all tweets in T contain parallel data, we extract the parallel segments using the IDA model, and these are placed in the second candidate set D s , t . The fact that we are applying the IDA model to tweets that may not contain translations means that many instances in D parallel. Thus, as the last step, we filter these instances, using a classifier that is trained 322 to predict whether each sample in D s , t is actually parallel. These steps will be described in detail in Sections 4.1, 4.2, and 4.3. 4.1 Filtering
The first step is to filter the set of tweets T so that only multilingual tweets are kept. A tweet is multilingual if it includes more than one language. Thus, tweets passing this filter can contain not only translated material, but also code switching and references to foreign words or names.
 only tweets that simultaneously contained a trigram of Chinese characters and a trigram of Latin words. However, this approach is only effective on finding multilingual tweets with languages that do not share the same unicode range. Thus, to allow the extraction of parallel data for language pairs such as English X  X panish, a more general method is needed. 4.1.1 Multilingual Document Detection. Although language identification is largely re-garded as a solved problem, traditional language detectors do not identify multiple languages within a document, but instead make the assumption that documents are primarily in one language. Some recent work has begun to address this (Lui, Lau, and Baldwin 2014), and our application imposes an additional efficiency requirement. language detection approach. The approach is based on estimating the probability that two tokens a and b are in different languages. The probability of a pair of tokens a and b and a pair of respective languages ` a and ` b is given by the product of the probabilities P ( ` a | a )  X  P L ( ` b | b ), where P `  X  L P L ( ` | a ) = 1 and P of the probabilities of all pairs of language pairs P ` equal to 1, and we can efficiently compute the probability that the pair of tokens are in different languages as: where P L ( ` | x ) is once again the probability that token x is in language ` , according to a character-based model, and L is the set of all languages considered. Thus, given a tweet, our model attempts to find a pair of words where P ( ` a 6 = ` instance, in the tweet eu quero ver este cartoon , where the message is mainly in Portuguese except for the word cartoon , which is in English, the model can use the high probability are not interested in the accuracy provided by considering contextual information in language detectors for the following reasons. Firstly, the language ambiguity is not a problem, because we are attempting to detect whether a pair is multilingual rather than of the word ver by itself is ambiguous, since both Portuguese and Spanish contain this word. However, the model is only interested in knowing if the language of ver is different from the language of cartoon . Thus, as long as the probability that the ver is low for English, the multilingual language detector will successfully identify this pair as multilingual. Secondly, even if a pair fails to be detected as multilingual, the model will still test all remaining pairs on whether these are parallel. 4.1.2 Word Pair Indexing. Traversing the whole Twitter corpora and computing lions of tweets to process. To cope with this, we use an approach based on the work a list of documents containing that word pair. Next, we traverse the index of word pairs and perform the detection using Equation (7), tagging the list of documents with that word pair as multilingual. Using this approach, we can avoid recomputing the same word pair more than once, if they occur in multiple tweets. Furthermore, we traverse the index so that word pairs that occur in more documents are processed first, and skip word pairs whose associated documents have already been classified as multilingual. 4.2 Location
The set of multilingual tweets T mult is then processed using the IDA model in order segments and their languages. Then, each parallel sentence is extracted and placed in the set D s , t = { ( s 1 , t 1 ), ... , ( s k , t k languages of the left and right segments, respectively, in the set D sentences that contain the language pair s , t , regardless of their order. Thus, we define the following insertion function: where x b a denotes the segment corresponding to the indexes from a to b in the original tweet x . This function simply checks if the s and t correspond to the left l or right r segments in the detected parallel data, and places the appropriately aligned parallel segments.
 multilingual messages in T mult are not guaranteed to contain translated material. Fur-thermore, we must also consider errors from misalignments of the IDA model and mis-classifications of the multilingual message detector. Thus, in order to identify messages that are actually parallel, a final identification step is necessary. 4.3 Identification
Given a candidate sentence pair ( s , t ), many existing methods for detecting parallel data can be applied (Resnik and Smith 2003; Munteanu and Marcu 2005), as this problem becomes a regular unstructured bitext identification problem. In our initial work (Ling et al. 2013), we simply defined a threshold  X  on the IDA model score, which was fier for each language pair, similar to that presented in Munteanu and Marcu (2005), which detects whether two segments are parallel in a given language pair by looking at features of the candidate pair. Training is performed to maximize the classification decisions on annotated candidate pairs. 324 annotated parallel data D gold ( s , t ). The method used to obtain the necessary annotations is described in Section 5.

Intrinsic evaluation. The quality of the classifier can be determined in terms of precision and recall. We count one as a true positive ( tp ) if we correctly identify a parallel tweet, and as a false positive ( fp ) if we spuriously detect a parallel tweet. Finally, a true nega-tive ( tn ) occurs when we correctly detect a non-parallel tweet, and a false negative ( fn ) if we miss a parallel tweet. Then, we set precision as tp tp + fp
F-measure is used to test the overall accuracy of the system in terms of precision and recall. may be more important than precision or vice-versa. For instance, as training data for
MT models, recall tends to matter more, as phrase-based MT models are robust to errors in the training data. 5. Crowdsourcing Parallel Segment Identification
Section 4.3 presented a supervised method to train a classifier that discriminates parallel and non-parallel data. However, training requires annotated instances where parallel and non-parallel segments are identified. Furthermore, to evaluate the quality of both the location and identification tasks, we also need annotated data. The same can be said about evaluating the quality of MT, which typically uses a gold standard of translated corpora.
 the microblog domain. Using expert annotators is expensive and it may be hard to find bilingual speakers for many language pairs. We will rely on crowdsourcing to produce annotated data sets, which has been successfully used to generate parallel data (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Post, Callison-Burch, and Osborne 2012; Ambati, Vogel, and Carbonell 2012). However, these methods have been focused on using workers to translate documents. Our classifier is ideally trained with tweets labeled with whether they contain parallel data. 5.1 Labeling Parallel Tweets
To define a set of tweets to annotate for each language pair, we use the IDA model to find the highest scoring language pair of that tweet (assuming it contains translated material). This allows us to set up tasks for specific language pairs, so that only workers who understand both languages will take them.
 whether a tweet contains translated material in the given language pair. Additionally, another four reference questions (whose answers are known) are added to the set to evaluate the worker X  X  quality. Jobs are accepted if the worker can answer at least three of the four reference questions correctly. Finally, each task is performed until five workers are accepted and the final result for each tweet is given by a weighted average of the answers of all five workers. More formally, we compute the weighted average given by of the worker. The weight w ( i ) is defined as the ratio of correct answers from job i in the reference set R , given by c R . Finally, if the weighted ratio is higher than 0.5, we label the tweet as being parallel, otherwise it is labeled as negative. 5.2 Obtaining High-Quality Bitext
In the previous task, we obtain judgments as to whether a particular tweet contains parallel data or not. In order to assess the quality of the identified spans, we add an additional task, where workers retrieve the bitext within the tweets that were previously identified to contain such data.
 segments. Once again, workers are accepted based on their performance on the four 326 reference questions. More specifically, we use the evaluation metric defined in Sec-tion 3.4 to measure how the character indexes defined by the worker perform against the reference annotation. To set a minimum score for each reference, we degrade the reference so that the smallest non-parallel segment is placed within the closest parallel segment. For instance, in the Korean X  X nglish example in Figure 1, the segment Hahah would be merged with the English parallel segment. If the worker X  X  scores are higher
Finally, each task is performed until two workers are accepted, and the annotations of the worker that scored highest against the reference are chosen. 6. Sina Weibo Data Crawling
In the previous sections, we described a method to obtain parallel data from microblog crawling method we used to crawl 3M parallel Chinese X  X nglish sentence pairs from Sina Weibo within 6 months.
 over 1.6 billion tweets from Twitter only yields approximately 300K English X  X hinese sentence pairs. However, because of the rate limiting 3 established by Sina Weibo X  X  API, we are only able to send 150 requests per hour. Each request can fetch up to 100 posts from a user, and subsequent sets of 100 posts request additional API calls. This means that crawling 1.6 billion tweets is beyond our capabilities. In fact, we were only able to crawl approximately 90 million tweets in 6 months.
 this by observing that some users post many messages in parallel, whereas others post none. Thus, we use one request to obtain the most recent 100 messages a user has posted.
We run the IDA model on that sample of messages to determine if they contain any parallel data. 4 If the number of automatically identified parallel messages within those 100 tweets is higher than 10, that user becomes a crawl target. We obtain all messages from crawl targets, and we periodically check if new messages have been posted. 1. Pick a random user and crawl 100 posts. 2. For all crawl targets that have not been updated within the last week, 3. Repeat from 1.
 we run the following operations: 1. Run the IDA model for users with unprocessed tweets. 2. Set the user as a crawl target if more than 10% of their tweets are parallel. unspent requests are lost after an hour. 7. Experiments
In this section, we describe the experiments performed to show the effectiveness of our proposed algorithm and the value of the data obtained. There are three sets of experi-ments that are performed. We evaluate the parallel data extraction process intrinsically by testing each of the three steps described in Section 4, and extrinsically by testing its utility when used as training data for MT systems. 7.1 Set-up We consider the following languages: Arabic, German, English, French, Japanese,
Korean, Chinese, Portuguese, Russian, and Spanish. On Sina Weibo, we focus on ex-tracting sentence pairs involving Chinese as one of the elements in the pair, as Chinese is the main language in Sina Weibo. On Twitter, we focus on finding sentence pairs involving English as one of the languages. 7.1.1 Tokenization. Tokenization converts a tweet into a sequence of tokens. Our tokenizer must consider a variety of languages and some common artifacts in the microblog domain. General properties of the tokenizer we used include: of the tweet from which each token was extracted. This is done so that after finding the parallel segments in the non-tokenized tweet. 7.1.2 Language Detection. A character-based language detector is required for the calcu-lation of the language score in Section 3.1.3 and for the multilingual tweet detection extracted from Wikipedia. Although we do not extract parallel sentences for all the 328 112 languages, more information regarding existing languages allows the detector to estimate the language probabilities more accurately. As we are using a character trigram model, a large amount of data is not required to saturate the model probabilities. Thus, for each language, we extract all data from Wikipedia up to a limit of 100K lines in order to keep the model compact. 7.1.3 Translation Lexicons. The IDA model uses translation lexicons to determine the translation score, as described in Section 3.1.1, which are estimated using parallel corpora. More specifically, we use the aligner described in Dyer, Chahuneau, and Smith (2013) to obtain the bidirectional alignments from the parallel sentences. Afterwards, we intersect the bidirectional alignments to obtain sure alignment points. Finally, we prune the lexicon using significance pruning (Johnson et al. 2007) with the threshold  X  + (as defined in that work). The intersection and the pruning are performed to reduce the size of the lexicon to make the look-up faster. The breakdown of the different lexicons built is shown in Table 1. 7.2 Building Gold Standards
To train and test the classifier described in Section 4.3, and perform MT experiments, a corpus of annotated tweets is needed for different language pairs. Table 2 summarizes the annotated corpora for the two domains (column Source ) and the different language pairs (column Language Pair ). We also report the method used to obtain the annotations (column Method ), where Expert denotes the manual annotation from native speak-ers, and exampCrowdsourcing denotes data that were crowdsourced using Amazon
Mechanical Turk. The number of tweets that were annotated and the number of parallel sentences that were found are reported in columns #Annotatated Tweets and #Parallel , sentences in the parallel data after tokenization in columns Average Size (English) and smaller than those in Weibo, which can be explained by the fact that posts in Twitter are limited to 140 characters, whereas in Sina Weibo, this restriction is not enforced. the Twitter data as a whole, as we are filtering monolingual tweets prior to annotation. Thus, we cannot draw conclusions about the ratio between parallel and non-parallel the case in a uniformly extracted data set. 6 However, performing the annotation in a uniformly extracted data set is problematic for two reasons. Firstly, a huge annotation effort would be required to find a significant number of tweets containing translations, since this is only the case for a minority of tweets. Secondly, training the classifier on such an unbalanced data set would bias the classifier towards the negative case, as the majority of the samples belong in this category, which is not desired. 7.3 Parallel Data Extraction Experiments
We report on the experiments performed in each of the stages of the parallel data extraction process described in Section 4. 7.3.1 Filtering. The filtering step (described in Section 4.1) attempts to filter out monolin-gual tweets, because these are sure to not contain parallel data. Ideally, we would uni-formly sample tweets and annotate them on whether they are multilingual. However, this would require an extraordinary amount of effort to obtain a sample with a large number of multilingual tweets, as most tweets are monolingual. Thus, we use a pre-existing annotated data set from Twitter, where each word in the tweet was annotated with its language. In this data set, there are 773 annotated samples from Twitter. We filter these so that all tweets contain words in at least two different languages, resulting in 554 multilingual tweets. From this data set, we define two splits. The first one contains only the language pairs that we are extracting parallel data from in this work, which allows us to estimate the degree our algorithm is spuriously removing multilingual tweets in the filtering step. In all, 291 tweets from the 554 were obtained according to this criteria.
The second subset is restricted to languages that use the Latin alphabet more difficult to label correctly. This subset contains 222 tweets. Finally, to build a data set of monolingual tweets, we sample tweets from Twitter uniformly until we find 2,000 tweets that are monolingual. 330 of tweets that were labelled correctly from the different sets, using different thresholds for Equation (7).
 labelled ( y -axis) for different values for different thresholds ( x -axis). Plot lines Multi-lingual (Used Languages) and Multilingual (Latin) denote that percentage of correctly labelled multilingual tweets from the subsets created by restricting the 554 tweets to the used languages in this work for parallel data extraction and the set consisting of Latin languages, respectively. Finally, the plot line Monolingual represents the set of monolingual tweets that were correctly labelled. We can observe that by simply removing the top 90% of word pairs, we can remove 67.8% of the monolingual tweets at the cost of losing 10 X 15% of the multilingual tweets at threshold 0. When we start increasing the threshold, we observe a substancial improvement for the detection of monolingual tweets, at the cost of mislabelling multilingual tweets. As expected, the task is slightly hard for Latin languages, because of the orthographic similarities between these languages.
 lingual tweets that are removed. Although this produces worse results for the detection of multilingual tweets, it substantially reduces the number of tweets that need to be processed. Furthermore, a large portion of the misclassifications for multilingual tweets are the result of tweets that only contain one or two words in a different language, such as a person X  X  name, and these tweets are not likely to contain parallel data. compare the automatic segment boundaries with the human annotations using the S metric defined in Section 3.4, which measures the overlap between the proposed and the reference boundaries. Results for different language pairs and domains can be found in Table 3, where the average overlap score S seg for the English and Foreign segments are shown in columns English Overlap and Foreign Overlap , respectively. The S obtained as a harmonic mean between the previous scores is shown in the S results are significantly higher than those in Twitter. One explanation for this is the fact that parallel segments found in Weibo are longer. This allows the alignment model to find more word alignments, which can be used to find better boundaries for the parallel spans.
 parallel data were used to train the lexical translation table. For instance, in the English X 
Korean and English X  X apanese language pairs, where the parallel corpora used consisted of only 60K and 150K sentence pairs, the results are evidently worse compared to the results obtained for the English X  X rabic and English X  X ussian language pairs, where approximately 1 million sentence pairs were used. 7.3.3 Identification Results. The aim of the identification task is to determine whether a given tweet contains parallel data. We used the data sets described in Section 7.2, and these were evenly divided into training and test sets. The max-entropy classifier was 332 trained using Weka (Hall et al. 2009), which maximizes the weighted F-measure of the positive and negative samples. We calculate the precision, recall (on positive labels), and accuracy at increasing intervals of 10% of the top scoring samples.
 domains using the full set of features are presented in Figure 5. We can observe that
Weibo contains a larger number of parallel tweets as the precision when all the data is considered parallel is only 46% for Twitter, compared with 61% in Weibo. This is because the Twitter data set we used to extract the parallel data from was extracted uniformly, whereas the Sina Weibo tweets were crawled using the algorithm described in Section 6, which attempts to maximize the amount of tweets containing translations while crawl-ing. We can also observe that results are significantly better for Weibo, as the precision curve for the Weibo data set is consistently higher than that for the Twitter data set. results for this task, but we wish to show that additional features can be used to improve the quality of the classifier, some even containing Twitter or Weibo specific attributes, such as the meta-information regarding the user that posted each tweet. To do this, we trained the max-entropy classifier using an increasingly larger set of features and present the weighted average of the F-measure for positive and negative labels. that results can be improved by adding richer features, similar to previous work (Resnik and Smith 2003; Munteanu and Marcu 2005). The microblog-specific User features seems to work best in Sina Weibo, as the crawling methodology we used would obtain a large number of posts from the same user. On the other hand, the Twitter data set was obtained from a uniform crawl where fewer tweets from the same user can be found, which is why less significant improvements were observed. We can also observe incremental improvements from widely used features for this task (Length, Repetition, and Language).
 automatically detected parallel segments. For instance, we can observe better results for Arabic X  X nglish, which also obtained a high S IDA in Table 3. Furthermore, similar language pairs, such as English X  X panish and English X  X rench, tend to be better aligned, and therefore obtain higher scores. In these cases, additional features are less beneficial for the classifier. 7.3.4 Data Representation. To provide an interpretable view of the contents of the parallel data that was crawled, we look at the distribution over topics of the parallel data set inferred using latent Dirichlet allocation (LDA) (Blei, Ng, and Jordan 2003). Thus, we grouped the Weibo filtered tweets by user, and ran LDA over the predicted English segments, using 12 topics. The seven most interpretable topics are shown in Table 5. We see that the data contain a variety of topics, both formal (Chinese news, religion) and informal (entertainment, music). 7.4 Machine Translation Experiments
In order to measure the impact of the extracted corpora, we perform an extrinsic experiment where we use the extracted data as training parallel sentences for existing
MT systems. We first perform an extensive test on the English X  X hinese language pair, where we show that the extracted corpus contributes to improve the state-of-the-art 334 results in Twitter, Sina Weibo, and also more formal domains. Then, we show that these results generalize for other language pairs.
 be conducted using fixed development and test sets for each domain as well as the same system set-up but with different training sets. 7.4.1 Data Sets. We report on machine translation experiments using our harvested data in three domains: edited news, Twitter, and Sina Weibo. 7.4.2 Training Data. We report results on these test sets using different training data. First, we use the FBIS data set, which contains 300K high quality sentence pairs, mostly in the broadcast news domain. Second, we use the full 2012 NIST Chinese X  X nglish data set (approximately 8M sentence pairs, including FBIS). Finally, we use our crawled data from Sina Weibo (referred to as Weibo) and those extracted from Twitter (referred to as
Twitter) by themselves but also combined with the two previous training sets. The max-entropy classifier for detecting parallel data was tuned for 50% precision, where 113K
Twitter parallel sentences from Twitter and 800K sentence pairs from Sina Weibo were extracted. 7.4.3 Set-up. We use the Moses phrase-based MT system with standard features (Koehn, Och, and Marcu 2003). As the language model, we use a 5-gram model with Kneser-
Ney smoothing. The weights were tuned using MERT (Och 2003). Results are presented with BLEU-4 (Papineni et al. 2002). 7.4.4 Results. The BLEU scores for the different parallel corpora are shown in Table 6 and the top 10 out-of-vocabulary words for each data set are shown in Table 7. Results for the microblog test sets (Weibo and Twitter) suggest that considerable improvements can be obtained by using the crawled corpora for this domain.
 on the Weibo test set with the NIST and Weibo training corpora, where improvements ranging from 250% to 300% can be observed. Although this is a promising result, it is important to consider the fact that we are drawing training and test samples from the same domain, which naturally leads to better results than using training corpora that were drawn from other domains. In some cases, these improvements can also observe a BLEU improvement from 9.55, using the NIST corpus, to 23.57, using the
Weibo corpus. This shows that the extracted corpus contains many translated elements that are representative of the microblogging domain that are not found in publicly 336 available corpora, such as NIST. Examples include translations for newer terms, such as iTunes . Table 7 illustrates the list of terms in each of the test sets ( Syndicate , Weibo , and Twitter ) that could not be found in the respective training data ( FBIS , NIST , correctly, it is a promising result that, using the Weibo corpus, we can find translations for all words that occur more than once in the Twitter data set. We can observe that the NIST and FBIS data sets do not possess frequent terms, such as wanna , lol , and omg , and also newer terms like kpop 10 in their translation inventory. An aspect that does not reflect in Table 7 are words with the same orthography, such as u and 4 , which are found in the NIST data set, but can be used with different meanings in microblogs. We also found examples of slang terms that are not addressed by the NIST data set, and are learned from the Weibo training corpus. First, we observe the character ji X ong , which generally means embarrassed in informal speech. The term ( f X en s  X  X  ) is a phonetic translation of the English term fans . This is translated incorrectly using the FBIS data is very common in Chinese microblogs. There is no direct translation for this term, but it is generally translated based on context into loser or sucker in our extracted parallel sentence pairs from Weibo.
 that improvements are much more reduced. In fact, for the Weibo test set, the obtained results are lower than using the NIST data set. This can be explained by the lexical gaps caused by the small size of the Twitter training set (117K sentence pairs). However, we can see that a considerable improvement can be obtained by combining these data sets (NIST + Twitter row), which suggests that a meaningful translation inventory can be obtained from this training set for the translation of microblog data.
 domain. Yet, we can observe that training with the Weibo corpus still obtains a similar result compared to the NIST and FBIS data sets, due to the fact that many news events are posted on Weibo and thus contain the formal language present in NIST and
FBIS. Furthermore, combining data sets leads to improvements in BLEU. Error analysis indicates that one major factor is that names from current events, such as Romney and Wikileaks , do not occur in the older NIST and FBIS data sets, but are represented in the
Weibo data set. 7.4.5 Experiments for Other Language Pairs. To further validate the generalizability of our algorithm, we perform the same test on the other test data we obtained from Twitter, as the Weibo data set mainly included English X  X hinese sentence pairs (because the crawler was parameterized to find users who post in this language pair). The Twitter data set was crawled uniformly, which lowered the amount of training data that can be found within this data set. We do not perform experiments for language pairs where the number of extracted training sentences is particularly small (3K for Korean X  X nglish).
For other language pairs, we test whether we can improve translation results by adding the extracted corpora into formal data sets.
 and test sets for MT. The in-domain training corpus for Twitter is extracted automati-cally by optimizing the threshold for 50% precision. The size of these corpora varies from 10K to 150K. As out-of-domain training data, we use the data from which the lexicons (Table 1) were built. As monolingual data for English, we use the same 10M tweets from Twitter as in the previous experiment, and we always translate from the foreign language into English.
 338 combining both the in-domain and out-of-domain data improves the results further, similarly to the Chinese X  X nglish experiments.
 quently used variations for fundamental function words in each language, which do not occur in formal domains. For instance, in the Portuguese X  X nglish language pair, the Portuguese terms muito and para , which mean very and for in English, are generally written as mto and pra in Twitter. This can be seen on the Portuguese X  X nglish example in Figure 1. Because the in-domain corpus that is used is relatively small, adding a large amount of out-of-domain bitext yields large improvements in BLEU. 7.4.6 Translation of Online Terms. In order to provide insight of the distintive translations of online terms that are learned in our extracted data, we manually selected some examples of parallel sentences containing terms that are generally not found in other media. These are shown in Figure 6.
 buzzword among Chinese communities and is used to describe a class of under-guages, and these must be addressed using context. In this case, the term is translated into loser , which is an acceptable translation considering the lack of better English terms. used. In this example, the Chinese translation is translated formally without the stylistic features present in the English sentence. However, in some parallel sentences, these properties are preserved during translation. For instance, the term toooooo in the third row corresponds to the word too with extra o  X  X  to further emphasize the following adjective thick . Likewise, in the Chinese sentence, it is translated into XX , which is composed by the character , an equivalent for the word too in English, and the string XX , which adds a similar connotation to the extra o  X  X  in the English sentence. where new English words are constructed from Chinese terms. In the fourth row, the term , which is an adjective to describe that something is great, is translated row, where other similar terms (niubility, zhuangbility, shability, and erbility) are also translated. In this case, we can also observe examples of abbreviations in the Chinese translations, where the character is replaced by the letter B , which is the first letter in the Pinyin representation of the character.

Translate 11 and Bing Translator 12 fail to translate these examples. For instance, the term diaosi in the first row is translated character-by-character, which leads to a translation with a very different meaning. Both abbreviations in English (e.g., imma ) and in Chinese (e.g., B ) also tend to be mistranslated. These are either incorrectly translated phonet-ically for English words, or character-by-character for Chinese terms. Finally, we also tried translating  X  X hinglish X  terms using online systems and found that these are either not translated or translated incorrectly.
 need for the methods to find parallel data where the translations for those terms can be learned. As such, we believe that the method we propose in this work has the potential to substantially improve the state-of-the-art MT systems in these domains. 8. Conclusions
This work presented a method for finding and extracting translated content present 340 segments within the post. We applied this to extract content from Twitter and Sina Weibo.
 filter the initial set of tweets. We then present an intra-document alignment model, which finds the most likely parallel segments, language pair, and alignments for each tweet. Finally, each set of parallel segments is classified as parallel or non-parallel by training a max-entropy classifier. To test and show the validity of the various steps of the extraction process, we use a crowdsourcing approach where non-experts can be used to generate annotated data.
 be obtained from Twitter and Sina Weibo. Applied to machine translation, it is demon-strated empirically that the extracted corpora have the potential to substantially increase the quality of the translations in informal genres.
 http://www.cs.cmu.edu/ ~ lingwang/microtopia/ .
 Acknowledgments References 342
