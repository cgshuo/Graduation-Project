 communities for model fitting and solving inverse problems. It is also important in neuroscience as it is thought to underlie the neural representations used by the brain. The operation to compute the sparse representation of a signal x  X  R n with respect to a dictionary of basis functions  X   X  R n  X  m can be implemented via an ` 1 -penalized least-square problem commonly referred to as Basis Pursuit Denoising (BPDN) [2] or Lasso [3] where  X  is a regularization parameter that controls the tradeoff between the quality of the reconstruc-tion and the sparsity. This approach has been applied to problems such as image coding, compressive sensing [4], or classification [5]. The ` 1 penalty leads to solutions where typically a large number of coefficients are exactly zero, which is a desirable property to achieve model selection or data compression, or for obtaining interpretable results. The cost function of BPDN is convex, and many efficient algorithms have been recently developed to solve this problem [6, 7, 8, 9].
 Minimizing the cost function of BPDN corresponds to MAP inference in a probabilistic model signal model assumed by BPDN is linear, generative , and the basis function coefficients are inde-pendent . In the context of analysis -based models of natural images (for a review on analysis -based and synthesis -based or generative models see [10]), it has been shown that the linear responses of natural images to Gabor-like filters have kurtotic histograms, and that there can be strong dependen-cies among these responses in the form of common amplitude fluctuations [11, 12, 13, 14]. It has also been observed in the context of generative image models that the inferred sparse coefficients exhibit pronounced statistical dependencies [15, 16], and therefore the independence assumption is violated. It has been proposed in block-` 1 methods to account for dependencies among the coeffi-cients by dividing them into subspaces such that dependencies within the subspaces are allowed, but not across the subspaces [17] . This approach can produce blocking artifacts and has recently been generalized to overlapping subspaces in [18]. Another approach is to only allow certain configura-tions of active coefficients [19].
 We propose in this paper a new class of prior on the basis function coefficients that makes it possible to model their statistical dependencies in a probabilistic generative model, whose inferred represen-tations are more sparse than those obtained with the factorial Laplacian prior, and for which we have efficient inference algorithms. Our approach consists of introducing for each coefficient a hyperprior on the inverse scale parameter  X  i of the Laplacian distribution. The coefficient prior is thus a mixture of Laplacian distributions which we denote  X  X aplacian Scale Mixture X  (LSM), which is an analogy to the Gaussian scale mixture (GSM) [12]. Higher-order dependencies of feedforward responses of wavelet coefficients [12] or basis functions learned using independent component analysis [14] have been captured using GSMs, and we extend this approach to a generative sparse coding model using LSMs.
 We define the Laplacian scale mixture in Section 2, and we describe the inference algorithms in the resulting sparse coding models with an LSM prior on the coefficients in Section 3. We present an example of a factorial LSM model in Section 4, and of a non-factorial LSM model in Section 5 that is particularly well suited to signals having the  X  X roup sparsity X  property. We show that the non-factorial LSM results in a divisive normalization rule for inferring the coefficients. When the groups are organized topographically and the basis is trained on natural images, the resulting model resem-bles the neighborhood divisive normalization that has been hypothesized to occur in visual cortex. We also demonstrate that the proposed LSM inference algorithm provides superior performance in image coding and compressive sensing recovery. A random variable s i is a Laplacian scale mixture if it can be written s i =  X   X  1 i u i , where u i has positive random variable with probability p (  X  i ) . We also suppose that  X  i and u i are independent. distributions with different inverse scales, and it can be computed by integrating out  X  i Note that for most choices of p (  X  i ) we do not have an analytical expression for p ( s i ) . We denote Mixture (GSM) [12] as the Laplacian distribution can be written as a GSM. We propose the linear generative model where x  X  R n ,  X  = [  X  1 ,..., X  m ]  X  R n  X  m is an overcomplete transform or basis set, and the columns  X  i are its basis functions.  X   X  N (0 , X  2 I n ) is small Gaussian noise. The coefficients are endowed with LSM distributions. They can be used to reconstruct x and are called the synthesis coefficients. Given a signal x , we wish to infer its sparse representation s in the dictionary  X  . We consider in this section the computation of the maximum a posteriori (MAP) estimate of the coefficients s given the  X  s is given by In general it is difficult to compute the MAP estimate with an LSM prior on s since we do not necessarily have an analytical expression for the log-likelihood log p ( s ) . However, we can compute the complete log-likelihood log p ( s, X  ) analytically Hence, if we also observed the latent variable  X  , we would have an objective function that can be maximized with respect to s . The standard approach in machine learning when confronted with such a problem is the Expectation-Maximization (EM) algorithm, and we derive in this Section an EM algorithm for the MAP estimation of the coefficients. We use Jensen X  X  inequality and obtain the following upper bound on the posterior likelihood which is true for any probability distribution q (  X  ) . Performing coordinate descent in the auxiliary function L ( q,s ) leads to the following updates that are usually called the E step and the M step. Let &lt; . &gt; q denote the expectation with respect to q (  X  ) . The M Step (6) simplifies to which is a least-square problem regularized by a weighted sum of the absolute values of the coeffi-cients. It is a quadratic program very similar to BPDN, and we can therefore use efficient algorithms developed for BPDN that take advantage of the sparsity of the solution. This presents a significant computational advantage over the GSM prior where the inferred coefficients are not exactly sparse. that in the M step we only need to compute the expectation of  X  i with respect to the maximizing distribution in the E step. Hence we only need to compute the sufficient statistics Note that the posterior of the multiplier given the coefficient p (  X  | s ) might be hard to compute. We will see in Section 4.1 that it is tractable if the prior on  X  is factorial and each  X  i has a Gamma dis-tribution, as the Laplacian distribution and the Gamma distribution are conjugate. We can apply the efficient algorithms developed for BPDN to solve (7). Furthermore, warm-start capable algorithms expect the solution to change much after a few iterations of EM. We propose in this Section a sparse coding model where the distribution of the multipliers is facto-the shape parameter and  X  is the inverse scale parameter. With this particular choice of a prior on the multiplier, we can compute the probability distribution of s i analytically: This distribution has heavier tails than the Laplacian distribution. The graphical model correspond-ing to this generative model is shown in Figure 1. 4.1 Conjugacy The Gamma distribution and Laplacian distribution are conjugate , i.e. the posterior probability of  X  i given s i is also a Gamma distribution when the prior over  X  i is a Gamma distribution and the conditional probability of s i given  X  i is a Laplace distribution with inverse scale  X  i . Hence, the posterior of  X  i given s i is a Gamma distribution with parameters  X  + 1 and  X  + | s i | . The conjugacy is a key property that we can use in our EM algorithm proposed in Section 3. We saw with parameters  X  + 1 and  X  + | s ( t ) i | , and the sufficient statistics (8) are given by A coefficient that has a small value after t iterations but is not exactly zero will have in the next iteration a large reweighting factor  X  ( t +1) i , which increases the chance that it will be set to zero in the next iteration, resulting in a sparser representation. On the other hand, a coefficient having a large value after t iterations corresponds to a feature that is very salient in the signal x . It is can account for as much information as possible.
 We saw that with the Gamma prior we can compute the distribution of s i analytically, and therefore we can compute the gradient of log p ( s | x ) with respect to s . Hence another inference algorithm is to descend the cost function in (3) directly using a method such as conjugate gradient, or the method proposed in [20] where the authors also exploit the conjugacy of the Laplacian and Gamma priors. We argue here that the EM algorithm is in fact more efficient. The solution of (7) indeed has typically few elements that are non-zero, and the computational complexity scales with the number of non-zero coefficients [6, 7]. On the other hand, a gradient-based method will have a harder time identifying the support of the solution, and therefore the required computations will involve all the coefficients, which is computationally expensive.
 The update formula (9) is coincidentally equivalent to the reweighted L1 minimization scheme pro-posed by Cand ` es et al. [21]. They solve the following sequence of problems that the solutions achieved by their algorithm are more sparse than the solution where  X  i = 1 for all i . Whereas they derive this rule from mathematical intuitions regarding the L1 ball, we show that this update rule follows from from Bayesian inference assuming a Gamma prior over  X  . It was also shown that evidence maximization in a sparse coding model with an automatic relevance determination prior can also be solved via a sequence of reweighted ` 1 optimization problems [22]. 4.2 Application to image coding It has been shown that the convex relaxation consisting of replacing the ` 0 norm with the ` 1 norm is able to identify the sparsest solution under some conditions on the dictionary of basis functions [23]. However, these conditions are typically not verified for the dictionaries learned from the statistics representations with a prior over the coefficients that is a mixture of a delta function at zero and a Gaussian distribution than with the Laplacian prior. We show that our proposed inference algorithm also leads to representations that are more sparse, as the LSM prior with Gamma hyperprior has heavier tails than the Laplacian distribution. We selected 1000 16  X  16 image patches at random, and computed their sparse representations in a dictionary with 256 basis functions using both the conventional Laplacian prior and our LSM prior. The dictionary is learned from the statistics of natural images [24] using a Laplacian prior over the coefficients. To ensure that the reconstruction error is the same in both cases, we solve the constrained version of the problem as in [21], where we require that the signal to noise ratio of the reconstruction is equal to 10 . We choose  X  = 0 . 01 and 5 EM iterations. We can see in Figure 2 that the representations using the LSM prior are indeed more sparse by approximately a factor of 2 . Note that the computational complexity to compute these sparse representations is much lower than that of [16]. Figure 1: Graphical model representation of our proposed generative model where the multipli-ers distribution is factorial. higher-order, sparse structure in which active coefficients occur in groups corresponding to basis functions having similar properties (position, orientation, or frequency tuning) [25, 1]. We focus in this Section on a class of signals that has a particular type of higher-order structure where the active coefficients occur in groups. We show here that the LSM prior can be used to capture this group structure in natural images, and we propose an efficient inference algorithm for this case. 5.1 Group sparsity We consider a dictionary  X  such that the basis functions can be divided in a set of disjoint groups or neighborhoods indexed by N k , i.e. { 1 ,...,m } = S k  X   X  N k , and N i  X  X  j =  X  if i 6 = j . A signal having the group sparsity property is such that the sparse coefficients occur in groups, i.e. the indices of the nonzero coefficients are given by S k  X   X  N k , where  X  is a subset of  X  . The group sparsity structure can be captured with the LSM prior by having all the coefficients in a group share the same inverse scale parameter, i.e. for all i  X  N k ,  X  i =  X  ( k ) . The corresponding graphical model is shown in Figure 3. This addresses the case where dependencies are allowed within groups, but not across groups as in the block-` 1 method [17]. Note that for some types of dictionaries it is more natural to consider overlapping groups to avoid blocking artifacts. We propose in Section 5.2 inference algorithms for both overlapping and non-overlapping cases. Figure 3: The two groups N ( k ) = { i  X  2 ,i  X  1 ,i } and N ( l ) = { i + 1 ,i + 2 ,i + 3 } are non-overlapping. 5.2 Inference In the EM algorithm we proposed in Section 3, the sufficient statistics that are computed in the E step are  X   X  i  X  p (  X  parameters  X  and  X  . Using the structure of the dependencies in the probabilistic model shown in Figure 3, we have in the group. Using the conjugacy of the Laplacian and Gamma distributions, the distribution of  X  ( k ) given all the coefficients in the neighborhood is a Gamma distribution with parameters  X  + |N k | and follows The resulting update rule is a form of divisive normalization. We saw in Section 2 that we can write s k =  X  play an important role in the visual system. [25] Now let us consider the case where coefficient neighborhoods are allowed to overlap. Let N ( i ) denote the indices of the neighborhood that is centered around s i (see Figure 4 for an example). We propose to estimate the scale parameter  X  i by only considering the coefficients in N ( i ) , and suppose that they all share the same multiplier  X  i . In this case the EM update is given by Note that we have not derived this rule from a proper probabilistic model. A coefficient is indeed a member of many neighborhoods as shown in Figure 4, and the structure of the dependencies implies (13) gives good performance. A similar approximation is used in the GSM analysis -based model [26]. Note that the noise shaping algorithm, which bears similarities with the iterative thresholding algorithm developed for BPDN [7], is modified in [27] using an update that is essentially inversely proportional to ours. The authors show improved coding efficiency in the context of natural images. 5.3 Compressive sensing recovery In compressed sensing, we observe a number n of random projections of a signal s 0  X  R m , and it is in principle impossible to recover s 0 if n &lt; m . However, if s 0 has p non-zero coefficients, it has been shown in [28] that it is sufficient to use n  X  p log m such measurements. We denote by W  X  R n  X  m the measurement matrix and let y = Ws 0 be the observations. A standard method to obtain the reconstruction is to use the solution of the Basis Pursuit (BP) problem Note that the solution of BP is the solution of BPDN as  X  converges to zero in (1), or  X  = 0 in (10). If the signal has structure beyond sparsity, one can in principle recover the signal with even fewer measurements using an algorithm that exploits this structure [19, 29]. We therefore compare the performance of BP with the performance of our proposed LSM inference algorithms We denote by RWBP the algorithm with the factorial update (9), and RW 3 BP (resp. RW 5 BP) the algorithm with our proposed divisive normalization update (13) with group size 3 (resp. 5 ). We consider 50 -dimensional signals that are sparse in the canonical basis and where the neighborhood size is 3 . To sample such a signal s  X  R 50 , we draw a number d of  X  X entroids X  i , and we sample three values for s i  X  1 , s i and s i +1 using a normal distribution of variance 1 . The groups are thus allowed to overlap. A compressive sensing recovery problem is parameterized by ( m,n,d ) . To explore the problem space we display the results using phase plots as in [30], which plots performance as a function of different parameter settings. We fix m = 50 and parameterize the phase plots using the indeterminacy of the system indexed by  X  = n/m , and the approximate sparsity of the system indexed by  X  = 3 d/m . We vary  X  and  X  in the range [ . 1 ,. 9] using a 30 by 30 grid. For a given value (  X , X  ) on the grid, we sample 10 sparse signals using the corresponding ( m,n,d ) parameters. The underlying sparse signal is recovered using the three algorithms and we average the recovery error k  X  s  X  s 0 k 2 / k s 0 k 2 for each of them. We show in Figure 5 that RW 3 BP clearly outperforms RWBP. There is a slight improvement by going from BP to RWBP (see supplementary material), but this improvement is rather small as compared with going from RWBP to RW 3 BP and RW 5 BP. This illustrates the importance of using the higher-order structure of the signals in the inference algorithm. The performance of RW 3 BP and RW 5 BP is comparable (see supplementary material), which shows that our algorithm is not very sensitive to the choice of the neighborhood size. Figure 5: Compressive sensing recovery results using synthetic data. Shown are the phase plots for a sequence of BP problems with the factorial update (RWBP), and a sequence of BP problems with the divisive normalization update with neighborhood size 3 (RW 3 BP). On the x-axis is the sparsity of the system indexed by  X  = 3 d/m , and on the y-axis is the indeterminacy of the system indexed by  X  = n/m . At each point (  X , X  ) in the phase plot we display the average recovery error. 5.4 Application to natural images It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefficients results in a set of dictionary elements whose spatial prop-erties match those of V1 (primary visual cortex) receptive fields [24]. However, the basis functions are learned under a probabilistic model where the probability density over the basis functions coef-ficients is factorial, whereas the sparse coefficients exhibit statistical dependencies [15, 16]. Hence, a generative model with factorial LSM is not rich enough to capture the complex statistics of natural images. We propose here to model these dependencies using a non-factorial LSM model. We fix a topography where the basis functions coefficients are arranged on a 2D grid, and with overlap-ping neighborhoods of fixed size 3  X  3 . The corresponding inference algorithm uses the divisive normalization update (13).
 We learn the optimal dictionary of basis functions  X  using the learning rule  X  X  =  X  ( x  X   X  X  s ) X  s T as in [24], where  X  is the learning rate,  X  s are the basis functions coefficients inferred under the model (13), and the average is taken over a batch of size 100 . We fix n = m = 256 , and sample 16  X  16 image patches from a set of whitened images, using a total of 100000 batches. The learned basis functions are shown in Figure 6. We see here that the neighborhoods of size 3  X  3 group basis functions at a similar position, scale and orientation. The topography is similar to how neurons are arranged in the visual cortex, and is reminiscent of the results obtained in topographic ICA [13] and topographic mixture of experts models [31]. An important difference is that our model is based on a generative sparse coding model in which both inference and learning can be implemented via local network interactions [7]. Because of the topographic organization, we also obtain a neighborhood-based divisive normalization rule.
 Does the proposed non-factorial model represent image structure more efficiently than those with factorial priors? To answer this question we measured the models X  ability to recover sparse struc-ture in the compressed sensing setting. We note that the basis functions are learned such that they represent the sparse structure in images, as opposed to representing the images exactly (there is a noise term in the generative model (2)). Hence, we design our experiment such that we measure the recovery of this sparse structure. Using the basis functions shown in Figure 6, we first infer the sparse coefficients s 0 of an image patch x such that k x  X   X  s 0 k 2 &lt;  X  using the inference algorithm corresponding to the model. We fix  X  such that the SNR is 10 , and thus the three sparse approxi-mations for the three models contain the same amount of signal power. We then compute random projections y =  X  W  X  s 0 where  X  W is the random measurements matrix. We attempt to recover the sparse coefficients as in Section 5.3 by substituting W :=  X   X  W , and y :=  X  s 0 . We compare the recovery performance k  X  X  s  X   X  s 0 k 2 / k  X  s 0 k 0 for 100 16  X  16 image patches selected at random, and we use 110 random projections. We can see in Figure 7 that the model with non-factorial LSM prior outperforms the other models as it is able to capture the group sparsity structure in natural images. Figure 6: Basis functions learned in a non-factorial LSM model with overlapping groups of size 3  X  3 We introduced a new class of probability densities that can be used as a prior for the coefficients in a generative sparse coding model of images. By exploiting the conjugacy of the Gamma and Laplacian prior, we were able to derive an efficient inference algorithm that consists of solving a sequence of reweighted ` 1 least-square problems, thus leveraging the multitude of algorithms already developed for BPDN. Our framework also makes it possible to capture higher-order dependencies through group sparsity. When applied to natural images, the learned basis functions of the model may be topographically organized according to the specified group structure. We also showed that exploiting the group sparsity results in performance gains for compressive sensing recovery on natural images. An open question is the learning of group structure, which is a topic of ongoing work.
 We wish to acknowledge support from NSF grant IIS-0705939.

