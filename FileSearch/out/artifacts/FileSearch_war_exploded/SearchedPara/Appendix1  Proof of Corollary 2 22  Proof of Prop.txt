 Xi Chen xichen@cs.cmu.edu Qihang Lin qihangl@andrew.cmu.edu Dengyong Zhou dengyong.zhou@microsoft.com Recall that where B ( a,b ) is the beta function.
 I ( a,b ). We re-write 1  X  I ( a,b ) as follows where the second equality is obtained by setting t := 1  X  t . Then we have: Since t &gt; 0 . 5, t 1  X  t &gt; 1. When a &gt; b , t 1  X  t We use the proof technique in (Xie &amp; Frazier, 2012) to prove Proposition 2.3. By Proposition 2.1, we first obtain the value function: To decompose the final accuracy P K i =1 h ( P T i ) in-to the intermediate reward at each stage, we de-P tion can now be re-written as follows:
V ( S 0 ) = G 0 ( S 0 ) + sup Here, the first inequality is true because G 0 is deter-minant and independent of  X  , the second inequality is due to the tower property of conditional expecta-tion and the third inequality holds because P t +1 i and P , and thus, G t +1 depend on F t only through S t and i . We define intermediate expected reward gained by labeling the i t -th instance at the state S t as follows:
R ( S t ,i t ) = E ( G t +1 | S t ,i t ) The last equation is due to the fact that only P t i changed if the i t -th instance is labeled next. With the expected reward function in place, our value function takes the following form: To prove the failure of deterministic KG, we first show a key property for the expected reward function: Lemma 3.1. When a,b are positive integers, if a = b , To prove lemma 3.1, we first present several basic prop-erties for B ( a,b ) and I ( a,b ), which will be used in all the following theorems and proofs. 1. Properties for B ( a,b ): 2. Properties for B ( a,b ): Proof. When a = b , by Corollary 2.2, we have I ( a + fore, the expected reward (5) takes the following form: When a &gt; b , since a,b are integers, we have a  X  b + 1 0 . 5 according to Corollary 2.2. The expected reward (5) now becomes: When a  X  b  X  1, we can prove R ( a,b ) = 0 in a similar way.
 With Lemma 3.1 in place, the proof for Proposition 3.1 is straightforward. Recall that the deterministic KG policy chooses the next instance according to and breaks the tie by selecting the one with the small-t i the label y i 0 , either a 0 i a t another instance i 1  X  E with the  X  X urrent X  largest expected reward and the expected reward for i 1 after obtaining the label y i 1 will then become zero. As a consequence, the KG policy will label each instance in E for the first |E| stages and R ( a |E| i ,b |E| i all i  X  { 1 ,...,K } . Then the deterministic policy will break the tie selecting the first instance to label. From for other instances are all zero, the policy will still la-bel the first instance. On the other hand, if a t 1 = b t tive expected reward and the policy will label it. Thus Proposition 3.1 is proved.
 Remark. For randomized KG, after getting one label pected reward for each instance has become zero. Then randomized KG will uniformly select one instance to label. At any stage t  X |E| , if there exists one instance randomly select an instance to label. To prove the consistency of the optimistic KG pol-max( R 1 ( a,b ) ,R 2 ( a,b )). 1. When a  X  b + 1: 2. When a = b : 3. When b  X  1  X  a : For better visualization, we plot values of R + ( a,b ) for different a,b in Figure 1.
 As we can see R + ( a,b ) &gt; 0 for any positive integers the following Lemma.
 Lemma 4.1. Properties for R + ( a,b ) : 1. R ( a,b ) is symmetric, i.e., R + ( a,b ) = R + ( b,a ) . 2. lim a  X  X  X  R + ( a,a ) = 0 . 3. For any fixed a  X  1 , R + ( a + k,a  X  k ) = R + ( a  X  4. When a  X  b , for any fixed b , R + ( a,b ) is mono-By the above four properties, we have Proof. We first prove these four properties.  X  Property 1: By the fact that B ( a,b ) = B ( b,a ),  X  Property 2: For a &gt; 1, R + ( a,a )  X  Property 3: For any k  X  0,  X  Property 4: When a  X  b , for any fixed b : According to the third property, when a + b is an even to the fourth property, when a + b is an odd number R and a  X  b  X  1, we have R + ( a,b ) &lt; R + ( a,b  X  1) &lt; R According to the second property such that lim a  X  X  X  R + ( a,a ) = 0, we conclude that Using Lemma 4.1, we first show that, in any sample path, the optimistic KG will label each instance in-finitely many times as T goes to infinity. Let  X  i ( T ) be a random variable representing the number of times that the i -th instance has been labeled until the stage T using optimistic KG. Given a sample path  X  , let stances that has been labeled only finite number of times as T goes to infinity in this sample path. We need to prove that I (  X  ) is an empty set for any  X  . We prove it by contradiction. Assuming that I (  X  ) is not empty, then after a certain stage b T , instances in I (  X  ) will never be labeled. By Lemma 4.1, for any j  X  I c , exist  X  T &gt; b T such that: max Then according to the optimistic KG policy, the next instance to be labeled must be in I (  X  ), which leads to the contradiction. Therefore, I (  X  ) will be an empty set for any sample path  X  .
 Let Y s i be the random variable which takes the val-s = 1 , 2 ,... are i.i.d. random variables. By the fact that lim T  X  X  X   X  T ( i ) =  X  in all sample paths and us-ing the strong law of large number, we conclude that, conditioning on  X  i , i = 1 ,...,K , the conditional prob-ability of lim for all i = 1 ,...,K , is one. According to Propo-sition 2.1., we have H T = { i : a T i  X  b T i } and H  X  = { i :  X  i  X  0 . 5 } . The accuracy is Acc( T ) = ( | H T  X  H  X  | + | H c T  X  ( H  X  ) c | ) . We have: whenever  X  i 6 = 0 . 5 for all i . The last inequali-ty is due to the fact that, as long as  X  i is not 0 . 5 in any i , any sample path that gives the event ther implies lim T  X  X  X  ( | H T  X  H  X  | + | H c T  X  ( H  X  ) Finally, we have: where the second equality is because {  X  i :  X  i, X  i = 0 . 5 } is a zero measure set. We assume there are K instances with the soft-label  X  i  X  Beta( a 0 i ,b 0 i ) and M workers with the reliability  X  th instance by the j -th worker, we have the probability of the outcome Z ij :
Pr( Z ij =  X  1 |  X  i , X  j ) = (1  X   X  i )  X  j +  X  i (1  X   X  We approximate the posterior so that at any stage for all i,j ,  X  i and  X  j will follow Beta distributions. In par-ticular, assuming at the current state  X  i  X  Beta( a i ,b and  X  j  X  Beta( c j ,d j ), the posterior distribution con-ditioned on Z ij takes the following form: defined in (12) and (13) respectively and
Pr( Z ij = 1) = E (Pr( Z ij = 1 |  X  i , X  j ))
Pr( Z ij =  X  1) = E (Pr( Z ij =  X  1 |  X  i , X  j )) takes the form of the product of Beta distributions on  X  and p j . Therefore, we use variational approximation by first assuming the conditional independence of  X  and  X  j : p (  X  i , X  j | Z ij = z )  X  p (  X  i | Z ij = z ) p (  X  j | Z In particular, we have the exact form for the marginal distributions: To approximate the marginal distribution as Beta dis-tribution, we use the moment matching technique. In particular, we approximate such that first and second order moment of Beta(  X  a i ( z ) , To make (14) and (15) hold, we have: Similarly, we approximate such that first and second order moment of Beta(  X  c j ( z ) , To make (14) and (15) hold, we have: Furthermore, we can compute the exact values for e E (  X  i ), e E z (  X  2 i ), e E z (  X  j ) and e E z (  X  2 j Algorithm 2 Optimistic Knowledge Gradient with Workers X  Reliability
Input: Parameters of prior distributions for in-stances { a 0 i ,b 0 i } K i =1 and for workers { c 0 j ,d total budget T . for t = 0 ,...,T  X  1 do end for
Output: The positive set H T = { i : a T i  X  b T i } . Assuming at a certain stage,  X  i for the i -th instance has the Beta posterior Beta( a i ,b i ) and  X  j for the j -th worker has the Beta posterior Beta( c j ,d j ). The reward of getting label 1 for the i -th instance from the j -th worker and getting label -1 are: and (21), which further depend on c j and d j . With the reward in place, we present the optimistic knowl-edge gradient algorithm for budget allocation with the modeling of workers X  reliability in Algorithm 2. 6.1. Incorporating Feature Information When each instance is associated with a p -dimensional feature vector x i  X  R p , we incorporate the feature in-formation in our budget allocation problem by assum-ing: where  X  ( x ) = 1 1+exp { X  x } is the sigmoid function and w is assumed to be drawn from a Gaussian prior N (  X  0 ,  X  0 ). At the t -th stage with the state S (  X  i -th instance according to a certain policy (e.g., KG) and observes the label y i t  X  { X  1 , 1 } . The posteri-following log-likelihood: ln p ( w | y i t ,S t ) = ln p ( y i t | w ) + ln p ( w | S t ) + const  X  where  X  t = (  X  t )  X  1 is the precision matrix. To ap-N (  X  t +1 ,  X  t +1 ), we use the Laplace method (see Chap-ter 4.4 in (Bishop, 2007)). In particular, we define the mean of the posterior Gaussian using the MAP (max-imum a posteriori) estimator of w : And  X  t +1 can be solved by Newton X  X  method. and the precision matrix:  X  By Sherman-Morrison formula, the covariance matrix We also calculate the transition probability of y i t = 1 and y i t =  X  1 as follows: where  X  ( s 2 i ) = (1 +  X s 2 i / 8)  X  1 / 2 and  X  i =  X   X  s To calculate the reward function, in addition to the transition probability, we also need to compute: i = Pr(  X  i  X  0 . 5 |F t ) where  X  (  X  ) is the Dirac delta function.Let Since the marginal of a Gaussian distribution is still a Gaussian, p ( c ) is a univariate-Gaussian distribution with the mean and variance:  X  i = E ( c ) =  X  E ( w ) , x i  X  =  X   X  t , x i  X  s 2 i = Var( c ) = ( x i ) 0 Cov( w , w ) x i = ( x i ) Therefore, we have: where  X (  X  ) is the Gaussian CDF.
 With P t i and transition probability in place, the ex-pected reward in value function takes the following form : R ( S t ,i t ) = E We note that since w will affect all P t i , the summation from 1 to K in (27) can not be omitted and hence (27) cannot be written as E h ( P t +1 i In this problem, the myopic KG or optimistic KG need to solve O (2 TK ) optimization problems to compute the mean of the posterior as in (25), which could be computationally quite expensive. One possibility to address this problem is to use the variational Bayesian logistic regression (Jaakkola &amp; Jordan, 2000), which could lead to a faster optimization procedure. 6.2. Multi-Class Setting In multi-class setting with C different classes, we as-sume that the i -th instance is associated with a proba-bility that the i -th instance belongs to the class c and P i =1  X  ic = 1. We assume that  X  i has a Dirichlet prior  X  i  X  Dir(  X  0 i ) and our initial state S 0 is a K  X  C matrix with  X  0 i as its i -th row. At each stage t with the cur-rent state S t , we determine an instance i t to label and collect its label y i t  X  X  1 ,...,C } , which follows the cat-egorical distribution: p ( y i t ) = Q C c =1  X  I ( y i the Dirichlet is the conjugate prior of the categorical distribution, the next state induced by the posterior distribution is: S t +1 i all i 6 = i t . Here  X  c is a row vector with one at the c -th entry and zeros at all other entries. The transition probability: In multi-class problem, at the final stage T when all budget is used up, we construct the set H T c for each class c to maximize the conditional expected classifi-cation accuracy: instances that belong to class c . Therefore, { H T c } C should form a partition of all instances { 1 ,...,K } . Let To maximize RHS of (28), we have If there is i belongs to more than one H T c , we only as-sign it to the one with the smallest index c . The max-imum conditional expected accuracy takes the form: Then the value function can be defined as: where P T i = ( P T i 1 ,...,P T iC ) and Following Proposition 2.2, let P t ic = Pr( i  X  H  X  c |F t P i = ( P function at each stage: The value function can be re-written as: tion only depends on S t i the reward function in a more explicit way by defining: Here  X  c be a row vector of length C with one at the c -th entry and zeros at all other entries; and I (  X  ) = ( I 1 (  X  ) ,...,I C (  X  )) where Therefore, we have R ( S t ,i t ) = R (  X  t i To evaluate the reward R (  X  ), the major bottleneck the C -dimensional integration on the region {  X  c  X   X  ,  X   X  c 6 = c } X   X  C will be computationally very expen-sive, where  X  C denotes the C -dimensional simplex. Therefore, we propose a method to convert the com-putation of I c (  X  ) into a one-dimensional integration. It is known that to generate  X   X  Dir(  X  ), it is equiv-alent to generate { X c } C c =1 with X c  X  Gamma(  X  c , 1) low Dir(  X  ). Therefore, we have: I (  X  ) = Pr( X c  X  X  X  c ,  X   X  c 6 = c | X c  X  Gamma(  X  c It is easy to see that Gamma distribution with the parameter (  X  c , 1) and F
Gamma ( x c ;  X   X  c , 1) is the CDF of Gamma distribution at x c with the parameter (  X   X  c , 1). In many softwares, F without an explicit integration. Therefore, we can e-valuate I c (  X  ) by performing only a one-dimensional numerical integration as in (35). We could also use Monte-Carlo approximation to further accelerate the computation in (35).
 Bishop, C. M. Pattern Recognition and Machine Learning . Springer, 2007.
 Jaakkola, T. and Jordan, M. I. Bayesian parameter estimation via variational methods. Statistics and Computing , 10:25 X 37, 2000.
 Xie, J. and Frazier, P. I. Sequential bayes-optimal poli-cies for multiple comparions with a control. Techni-
