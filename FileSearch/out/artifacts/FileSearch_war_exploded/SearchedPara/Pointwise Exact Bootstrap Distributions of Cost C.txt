 Charles Dugas dugas@dms.umontreal.ca David Gadoury gadoury@dms.umontreal.ca A cost curve (Drummond &amp; Holte, 2000; Drummond &amp; Holte, 2006) is a plot of a classifier X  X  expected cost as a function of operating conditions, i.e. misclassifi-cation costs and class probabilities. Performance as-sessment in terms of expected cost is paramount but cannot be visualized through ROC analysis although knowledge of the distribution of a classifier X  X  total mis-classification error cost is often among the enduser X  X  interests.
 Cost curve analysis can be enhanced if dispersion mea-sures of the curve are provided along with the curve itself, thereby allowing the enduser to assess the re-liability of the estimated performance of the classi-fier considered for implementation. In order to obtain confidence intervals from a single test set, resampling methods such as the bootstrap (Efron &amp; Tibshirani, 1993) technique can be used: from the test set, a cer-tain number of samples are drawn with remplacement and from these samples, a distribution of the cost can be obtained. In certain cases, the bootstrap technique lends itself to analytic derivations for the limit case where the number of samples tends to infinity. Distri-butions thus obtained are referred to as exact bootstrap distributions. The purpose of this paper is to derive exact bootstrap distributions for a classifier X  X  total cost of misclassification errors as well as the difference be-tween two classifiers X  total costs, for varying operating conditions.
 Except for Drummond &amp; Holte (2006), little attention has been given to developing and evaluating the per-formance of confidence intervals for cost curves. ROC curves have received much more attention. Arguably, the recency of cost curves explains in part this situa-tion. Recent literature on the derivations of confidence intervals for ROC curves can be segmented in three categories: parametric, semi-parametric or empirical. Semi-parametric methods mainly refer to kernel-based methods (Hall &amp; Hyndman, 2003; Hall et al., 2004; Lloyds, 1998; Lloyds &amp; Wong, 1999). Bootstrap re-sampling has been used for ROC curves as an empiri-cal method but to date, exact bootstrap distributions for the ROC curve have not been presented.
 A technical difficulty arises from the fact that, when sampling from the entire test set, a procedure we shall refer to as full sampling, relative proportions of classes will vary from one sample to another. Mathemati-cal derivations of exact bootstrap distributions, in the context of full sampling, are thus more complicated. In this paper, we first use a procedure referred to as stratified sampling according to which proportions of positive and negative instances of each bootstrap sam-ple are fixed as equal to those of the original test set. Here, an instance is an element of the test set. In-stances of the class for which the event has (not) taken place are called positive ( negative ). For example, for a credit card fraud detection application, fraudulous transactions would be labelled as positive whereas le-gitimate transactions would be labelled as negative. Within the stratified sampling framework, each sample is obtained from the combination of two independent bootstrap samples: one drawn from the set of positive instances and the other drawn from the set of neg-ative instances. This procedure has previously been used in the context of ROC (Bandos, 2005) as well as cost curves (Drummond &amp; Holte, 2006). After ob-taining results under this simplified stratified sampling approach, we derive exact bootstrap distributions for the full sampling approach.
 From the user X  X  perspective, the two sampling pro-cedures, stratified and full, provide different informa-tion so that the difference between the two approaches reaches beyond mere mathematical derivations. Ac-cording to stratified sampling, the user is provided with a cost distribution conditional on the operat-ing conditions that will eventually prevail once the model is implemented. We refer to these as the de-ployment conditions. This corresponds to the view of Drummond &amp; Holte (2006) who argue in favor of plotting cost curves in terms of all possible values of the unknown future deployment conditions. Within the stratified sampling approach, cost dispersion mea-sures obtained for a specific value of the deployment conditions make no provision for uncertainty around expected deployment conditions. On the other hand, according to the full sampling approach, class pro-portions are implicitly assumed to be binomially dis-tributed around those of the test set so that cost dis-persion measures incorporate uncertainty around class proportions. Since the two approaches provide differ-ent information that may both be of interest, both are treated in this paper.
 The rest of the paper is as follows: in section 2, we briefly review the main aspects of ROC and cost curves. Then, mathematical derivations are presented in section 3 for stratified sampling and in section 4 for full sampling. In section 5, we perform simulations and measure coverage accuracies of the confidence in-tervals. Limitations of the proposed approach are dis-cussed in section 6. Finally, we conclude in section 7. An ROC curve is a plot of the probability of correctly identifying a positive instance (a true positive) against the probability of mistakenly identifying a negative in-stance as positive (a false positive), for various thresh-old values. Fawcett (2004) provides an excellent in-troduction to ROC curves along with descriptions of the essential elements of ROC graph analysis. Classi-fier performance assessment in terms of expected total error cost cannot be done using ROC curves and for this reason (and others (Drummond &amp; Holte, 2006)), cost curves have been introduced as an alternative (or a complement) to ROC curves.
 The main objective of cost curves is to visualize clas-sifier performance in terms of expected cost rather than through a tradeoff between misclassification error probabilities. Expected cost is plotted against operat-ing conditions where, as mentionned above, operating conditions include two factors: class probabilities and misclassification costs. Once these values are fixed, all possibly attainable true and false positive rates pairs are considered. Given class probabilities, misclassifi-cation costs, and true and false positive rates, a cost is obtained. The pair that minimizes the cost is se-lected. It is assumed that given certain operating con-ditions, the enduser would select the cost minimizing pair and set the classifier X  X  threshold accordingly. In order to obtain a cost curve, this optimization process is repeated for all possible operating conditions values. As shown below, a set of operating conditions can be summarized through a single normalized scalar value ranging between 0 and 1. Figure 1 illustrates this pro-cess.
 Cost curves are obtained assuming the enduser selects the threshold that minimizes expected cost, given op-erating conditions, based on the test set . One approach to obtain cost distributions is to draw bootstrap sam-ples from the test set, obtain a cost curve for each of the samples and derive a distribution for the cost from these cost curves. Now consider a specific set of values for the operating conditions. Each of the samples will lead a possibly different optimal threshold for this set of operating conditions. This can be viewed in Figure 1 by comparing the left-and right-hand columns. Thus, averaging cost curves (fixed operating condi-tions but varying thresholds) in order to obtain an estimate of the expected cost would correspond to the enduser being able to select the optimal thresholds, de-pending on the actually observed sample of instances. In other words, the enduser would be required to have knowledge of the test set before deciding on a threshold value, something that can X  X  be done in practice. Ob-viously, thresholds must somehow be selected prior to test set cost measurements. This can be done through the standard machine learning process of splitting the data in three sets: training, validation and test. In our simulations, we assume the user chooses the opti-mal theoretical thresholds for all operating conditions, thus implicitely assuming an infinite sized validation set. The impact of this assumption is discussed later, in section 6. Our approach can therefore be consid-ered as a form of threshold averaging of the costs. But since both operating conditions (abscissa values) and thresholds are fixed for each computed distribution, then the approach could be considered as vertical av-eraging as well. We now turn to more formal deriva-tions of the cost curves and associated exact bootstrap distributions. Consider a test set consisting of n instances from which stratified bootstrap samples are drawn. In this paper, we shall assume bootstrap samples are of the same size as the test set itself, a common procedure. Let n + and n  X  be the numbers of positive and negative instances in the test set. According to the stratified bootstrap procedure and since we assume sample size equals test set size, the numbers of sampled positive and negative instances are fixed for all samples and also equal to n + and n  X  , respectively. Let n + of instances, among the n + positive instances of the test set, with score greater or equal to the threshold t = t ( w ) associated to operating conditions w , where w will be defined shortly. The corresponding value for a set of sampled positive instances is noted N + t and follows binomial distribution with parameters n + t /n + and n + which we note as N + t  X  Bin( n + t /n + ,n + ). The random variable for the true positive rate, at threshold t , is denoted TP + t = N + t /n + . Similarly for negative instances, n  X  t refers to the number of instances with score greater or equal to t among the n  X  negative in-stances of the test set, N  X  t is the random variable for the corresponding number of sampled instances and FP  X  t = N  X  t /n  X  is the false positive rate, at threshold t , with N  X  t  X  Bin( n  X  t /n  X  ,n  X  ). Note that, according to the stratified sampling procedure, samples from pos-itive and negative instances are drawn independently so that TP + t and FP  X  t are independent as well. Let us now formalize the above mentionned operat-ing conditions and define w . Let p + = n + /n and p  X  = n  X  /n represent class probabilities for positive and negative instances, respectively. Misclassification costs are noted c + /  X  and c  X  / + for false positive and false negative errors, respectively. Total cost is there-fore given by the following: Drummond &amp; Holte (2006) divide the total cost by its maximum possible value, in order to obtain a normal-ized cost with maximum value of one. This maximum total cost value is reached when 1  X  TP + t = FP  X  t = 1 and the total cost is then equal to p + c  X  / + + p  X  c + /  X  Defining w as the normalized cost is given by with w  X  [0 , 1]. As mentionned above, true and false positive rates are independent when stratified sam-pling is used. Thus, the expected value and variance of C N t follow as: We use these expectation and variance of the distribu-tion of C N t to fit a gaussian distribution from which confidence intervals are easily obtained.
 Now, in order to assess the statistical significance of the difference in performance of two classifiers, we need to obtain the distribution of the difference in their nor-malized costs: where we use subscripts 1 and 2 to differentiate values obtained for the two classifiers. The values of C N t C ble that the scores assigned by two different classifiers are correlated: for example, obvious fraudulous trans-actions will likely obtain high scores on all classifiers. Also note that only instances that are falsely labelled by one and only one of the two classifiers will affect the difference in costs. Errors made by both classifiers will offset each other when computing cost differences. Let n labelled as positive by the first classifier and negative by the second classifier, given operating conditions w . Similarly, let n + t set instances labelled as positive by classifier 2 and neg-ative by classifier 1. Note that thresholds t 1 = t 1 ( w ) and t 2 = t 2 ( w ) associated to operating conditions w may differ from one classifier to the other since score distributions and scales may vary from one classifier to the other. Values n  X  t for negative instances, given the same operating con-ditions value w .
 Let N + t variables for the number of instances in a bootstrap sample. Values N + t mial distribution. This also applies to N  X  t Accordingly, moments of  X  C N t Let us now evaluate the computational time required to obtain confidence intervals for the performance of a single classifier and for the difference between the per-formances of two classifiers. Here, we assume the num-ber of different operating conditions considered, i.e. the number of different values for w is proportional to n . Also, as explained above, we assume the thresholds associated to each of these operating conditions have previously been determined through a validation pro-cess. For the case of a single classifier performance, we first need to sort instances with respect to their score, which requires time O ( n ln n ). Then, values of n + t and n t are easily obtained in linear time. There remains to compute expectations and variances, using equations (4) and (5), and derive confidence intervals using these values. This is realized in constant time for each value of w , thus overall linear time. Globally, the entire pro-cess is therefore dominated by the sorting phase and total computational time is O ( n ln n ). Confidence in-tervals for the difference in performance between two classifiers can be obtained in O ( n ln n ) computational time as well, although less trivially. Naive solutions lead to quadratic time but, given careful sorting pre-processing, values n + t in linear time. Then, moments and confidence inter-vals for  X  C N t values of w ) using equations (7) and (8). Within the framework of full sampling, the proportions of positive and negative instances vary from one sam-ple to another. Whereas with stratified sampling, the number of positive and negative instances in each sam-ple, n + and n  X  , were set as equal to those of the test set, we now consider these numbers as random vari-ables, and accordingly use capital notation N + and N  X  . Here again, these values follow binomial distri-butions: N +  X  Bin( n + /n,n ). Thus, full sampling implicitly assumes a binomial distribution for the ob-served class proportions P + = N + /n and P  X  = N  X  /n but this distribution could easily be replaced. Equation (1) still holds in the case of full sampling, but with the difference that P + and P  X  are now treated as random variables. In the previous section the nor-malized version of the total cost was obtained by dividing the total cost by the largest possible cost: p c  X  / + + p  X  c + /  X  , a weighted average between misclas-sification costs c  X  / + and c + /  X  . Since P + and P  X  are no longer fixed, we must consider the largest possible weighted average which simply is the maximum of the two misclassification costs, c max = max[ c  X  / + ,c + /  X  The case where C T t = c  X  / + is obtained when N + = n and TP + t = 0. Similarly, we have C T t = c + /  X  if N  X  = n and FP  X  normalized cost can be written as Then, expected normalized cost and normalized cost variance are obtained through iterated expectations: where Here again, equations (9) and (10) can be used to ob-tain a fitted gaussian distribution from which confi-dence intervals are easily derived.
 Let us now turn to the difference in performance be-tween two classifiers. In the case of full sampling, this difference is Again, expected normalized cost and normalized cost variance are obtained through iterated expectations:
E [ X  C N t
V [ X  C N t where  X   X   X  This completes mathematical derivations. A total of four distributions have been obtained. For all four distributions, computation of confidence intervals is dominated by the need to sort instances so that com-putational time is O ( n ln n ) in all cases. Note that such time efficiency is obtained because we rely on the gaussian fitting of the variables X  distributions. Com-puting true exact bootstrap distributions would lead to higher computational time orders. But as we show in the next section, results obtained with gaussian fit-ting are already excellent. In this section, we conduct a series of experiments in order to assess the performance of the confidence in-tervals derived in sections 3 and 4. Performance is measured in terms of coverage accuracy of confidence intervals.
 The first experiment is based on the framework used by Macskassy et al. (2005) in which four methods for obtaining pointwise confidence intervals for ROC curves are compared: threshold averaging, vertical averaging, kernel smoothing (Hall et al., 2004) and Working-Hotelling bounds. Positive and negative in-stance scores follow normal distributions but with var-ious parameter values. We set the scale parameter to 3.00 for both positive and negative instances scores. The location parameter  X  for positive instances varies rameter for negative instances is set equal to  X   X  . Sam-ple size is set to 1,000, i.e. a set of 1,000 instances is drawn from the positive instances distribution and an-other set of 1,000 negative instances is drawn from the negative instances distribution. The sampling proce-dure is repeated 1,000 times, i.e. 1,000 simulations are performed for each value of  X  . We shall refer to this experiment as the spread experiment. Confidence intervals are obtained for a significance level of 10%. Figure 2 provides simulation results which clearly show that better results are obtained when score distribu-tions of positive and negative instances have few over-lap, i.e. for high values of  X  . Breaks in coverage ac-curacy appear as w is close to 0 or 1. This recurring pattern is discussed in section 6.
 As a second experiment, we consider the effect of sam-ple size on coverage accuracy. This experiment is ev-erywhere similar to the previous one except for two modifications: (1) the location parameter not longer varies: it is set to  X  = 3 . 0 and (2) the sample size takes values in { 25; 250; 2 , 500; 10 , 000 } instead of being fixed at 1,000. We shall refer to this experiment as the size experiment. Simulation results appear in Figure 3. As the sample size increases, the range of operating con-dition values with good coverage accuracy widens. For sample sizes of 25, only a very narrow range of oper-ating condition values lead to a coverage rate that is on target.
 Our third experiment addresses the modeling of the difference in performance between two classifiers. The experiment design is similar to the ones used for the previous two experiments, i.e. the spread and size ex-periments. Scores are distributed according to a binor-mal distribution with scale of 3.00. Confidence inter-vals are obtained for a significance level of  X  = 10%. The location parameters are set as follows: for pos-itive instances of the first classifier, we consider two values:  X   X  X  1 . 0 , 3 . 0 } . For negative instances of both classifiers the parameter is set equal to  X   X  . Finally, for positive instances of the second classifier we consider three values:  X  ,  X  + 2 . 0 and  X  + 4 . 0. The difference between the location parameters of the two classifiers X  positive instances distributions, either 0.0, 2.0 or 4.0, is referred to as the shift parameter. In order to in-clude some form of dependency between the scores of the two classifiers, three values of a correlation factor are considered:  X   X  X  0 . 3 , 0 . 6 , 0 . 9 } . We shall refer to this experiment as the difference experiment. Results appear in Figure 4. As in the previous two experiments, coverage accuracy breaks for very low or high total positive rates. Comparing curves on the left of Figure 4 with those on the right, we see the spread parameter  X  has some impact: higher values of  X  cause the range of total positive rate values with good coverage accuracy to widen. With  X  = 1 . 0, higher shift parameter values lead to better coverage accuracy whereas with  X  = 3 . 0, the shift parameter has the op-posite, but less pronounced, effect. The correlation coefficient seems to have very little effect on cover-age accuracy which is a welcome property: the perfor-mances of the confidence intervals seem independent of the level of correlation between the scores of two models.
 Figure 5 and 6 repeat the spread (first) and diffrence (third) experiments described above, but with the use of full sampling. Looking at Figure 5, it it clear that full sampling leads to better coverage accuracy than stratified sampling for low values of the spread param-eter (  X  = 0 . 75). In fact, the effect of the spread pa-rameter seems to have reversed although performance at  X  = 5 . 00 is better than with  X  = 3 . 0. Finally, Figure 6 indicates that both stratified and full sampling per-form equally well for modeling the difference between two classifiers X  performances. A first consideration is whether actually performing a certain number of bootstrap resamplings of the test set instances would allow us to reach coverage ac-curacy similar to that obtained in the previous ex-periments, using exact bootstrap distributions. Let b be the number of empirical bootstrap samples drawn. Computational time is dominated by the need to sort instances, as a preprocessing, for each sample and is thus within O ( bn ln n ). Obtaining confidence inter-vals through empirical bootstrap is therefore both an order of magnitude slower and less precise than using the exact bootstrap approach. Obviously, coverage ac-curacies similar to those presented here could be ob-tained with a large number of resamples but at high computational cost.
 Another issue is the presence of breaks in coverage accuracy for extreme values of operating conditions. When considering operating conditions close to 0 or 1, optimal thresholds are likely to lie outside the range of observed scores of the simulated test sets. For such thresholds and simulations, variances are either zero (equations 5,8, and 13) in which case coverage is impossible or very close to zero (equation 10) in which case coverage is very unlikely. Coverage accu-racy breaks appear as the probability that the optimal threshold is outside the range of observed score values rises. Also, as is apparent from Figure 1, the expected value of the cost (thus the cost difference as well) drops to zero as operating conditions reach extreme values. Finally, we may wonder how the assumption of opti-mal threshold selection impacts the results presented in this paper. Instead of assuming optimal threshold selection, consider selecting the thresholds, for each simulation of the previous experiments, based on a randomly generated finite-sized validation set which leading to suboptimal thresholds. Of course, expected costs are, by definition, higher for suboptimal thresh-olds than for optimal thresholds but what is of interest here is whether we can develop reliable confidence in-tervals for the cost, at the chosen thresholds, whether optimal or not. Given certain operating conditions, the selected suboptimal threshold follows a distribu-tion centered around the optimal value so that cover-age accuracy, given these operating conditions, is the expected coverage accuracy where the expectation is taken over the distribution of the suboptimal thresh-old. This results in a smoothing of the coverage accu-racy breaks observed in the experiments above. In this paper, we have derived exact bootstrap dis-tributions for the (normalized) cost of the misclassifi-cation errors of a classifier X  X  decisions. We have also derived exact bootstrap distributions for the difference between the costs of two classifiers. The first and sec-ond moments of these distributions have been used to fit gaussian distributions and thus approximate the true exact bootstrap distributions. From these approx-imated distributions, we were able to obtain confidence intervals for the variables of interest. Table 1 sum-marizes these results. All confidence intervals can be derived in O ( n ln n ) time.
 Results obtained in this paper are excellent but limited to a few simulations. In a few cases, severe breaks in coverage accuracy appear when operating conditions values close to 0 or 1. These breaks can be avoided if cost distribution computations are limited to thresh-olds within the range of sampled scores. Another pos-sibility is to extrapolate score distributions beyond ob-Sampling Variable Equations Figures
Stratified
Full served values, an area for future work.
 Bandos, A. (2005). Nonparametric methods in compar-ing two correlated ROC curves . Doctoral disserta-tion, Graduate School of Public Health, University of Pittsburgh.
 Drummond, C., &amp; Holte, R. (2000). Explicitly repre-senting expected cost: an alternative to ROC repre-sentation. KDD  X 00: Proceedings of the sixth ACM
SIGKDD international conference on Knowledge discovery and data mining (pp. 198 X 207). ACM. Drummond, C., &amp; Holte, R. (2006). Cost curves: an improved method for visualizing classifier per-formance. Machine Learning , 65 , 95 X 130.
 Efron, B., &amp; Tibshirani, R. (1993). An introduction to the bootstrap . No. 57 in Monographs on Statistics and Probability. Chapman &amp; Hall.
 Fawcett, T. (2004). ROC graphs: Notes and prac-tical considerations for researchers (Technical Re-port). HP Laboratories.
 Hall, P., &amp; Hyndman, R. (2003). Improved meth-ods for bandwidth selection when estimating ROC curves. Statistics &amp; Probability Letters , 64 , 181 X 189. Hall, P., Hyndman, R., &amp; Fan, Y. (2004). Nonpara-metric confidence intervals for receiver operating characteristic curves. Biometrika , 91 , 743 X 750. Lloyds, C. (1998). The use of smoothed ROC curves to summarise and compare diagnostic systems. Journal of the American Statistical Association , 93 , 1356 X  1364.
 Lloyds, C., &amp; Wong, Z. (1999). Kernel estimators of the ROC curve are better than empirical. Statistics &amp; Probability Letters , 44 , 221 X 228.
 Macskassy, S., Provost, F., &amp; Rosset, S. (2005). Point-wise ROC confidence bounds: An empirical evalu-ation. Proceedings of the ICML 2005 workshop on
ROC Analysis in Machine Learning . Bonn, Ger-
