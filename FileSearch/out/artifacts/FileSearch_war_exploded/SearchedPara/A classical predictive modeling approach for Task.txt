 This paper describes one possible way to solve task  X  X ho rated what? X  of the KDD CUP 2007. The proposed solution i s a history-based model that predicts whether a user wi ll vote a given movie. Key points to our approach are (1) the estim ation of the model baseline, (2) the definition of the explanato ry variables and (3) the mathematical model form. Given the binary o utcome of the problem, the estimation of the true baseline (r atio of 1 X  X  in the test data) is critical in order to correctly make p redictions. In parallel, to improve the model predictive power, we have developed a careful construction of the input varia bles. These explanatory variables can be grouped as: user votin g behaviour variables, the movie characteristics and user-movie interactions. Finally, the mathematical model form (linear logist ic regression) has been chosen among various model form competitor s. I.5.1 [ Pattern Recognition ]: Models  X  statistical Predictive modeling, data mining. Task 1 at the KDD CUP 2007 is based on the competit ion organized by Netflix (http://www.netflixprize.com) which provides a historic database of more than 100 milli on movie ratings [1]. Netflix training data lasts up to Dece mber 2005 and the Netflix Competition goal is to build a model wh ich predicts the rating given by a user to a movie. In order to accurately estimate the mean prediction error for each propose d model, Netflix uses a test dataset with 2 million user rat ings. Task 1 at KDD Cup X 07 is based on the Netflix data; but the goal is slightly different: Here we are asked to predict whether a user has rated a given movie during 2006. Therefore the model must have a binary outcome. The first difficulty in this task is to accurately determine the rate of positive events (baseline) on the provided data. In fact, having a look to the final results of the task 1 KDD Cup X 0 7, one can see that just five teams manages to perform better than a benchmark model constructed by assigning to each pair in the scoring data the baseline probability. Our modeling approach consists of the classical two steps: The paper is organized as follows: first, we descri be how we solved the estimation of the baseline for the year 2006 and how it was used to build the training table. Then the inpu t variables are described and finally the relevance of such variabl es is discussed. In order to estimate the baseline we must pay atten tion to the KDD Cup X 07 FAQ X  X . The FAQ document states that the 100.000 score pairs were selected by randomly picking up pa irs (user, movie) with probability proportional to the number of times each component appears in the 2006 ratings; Furthermore the user and the movie are chosen independently. We consider that correct estimation of the baseline is important in order to attain a good solution to the problem pose d.. For baseline estimation we shall proceed to replicate the proced ure used to create the scoring data, in order to produce a trai ning dataset with similar characteristics. The sampling algorithm is as follows: The direct application of this algorithm to the tim e range of 2005 provides a rating of 1 X  X  of around 20%. But a close r look to the data shows that most of those ratings belong to eit her new users (those whose first rating was at the end of 2004) o r new movies (those who were first rated at the end of 2004). Th erefore these two issues must be taken into account: proportion o f new users and movies that is present in the data prior to the given time frame. In fact the proportion of new users and movi es at the end of 2005 is much lower than the one at the end of 20 04, Therefore a lower baseline is expected for the 2006 period. This result shows that the model under construction is time-dependent (on the specific year), since there are p ractically no ratings from new users and new movies in 2005, by c ontrast to 2004 and preceding years, where many ratings from n ew users and new movies could be found. This fact makes it n ecessary to search for a procedure to make model results indepe ndent from the year of application, so as to correct the previ ous baseline. This solution is simpler than to build a time dependent model. For this reason we decided to clean the data used t o estimate the baseline and therefore we avoid the possible bias p roduced for those users and movies that appeared at the end of 2004. The following percentages of movies and users by first appearance month were eliminated. These percentages are obtain ed by comparing the average monthly percentage of new use rs in 2002, 2003 y 2004 with the percentage of new users in 200 5, and dividing for each month. . Table 1. Percentage of users and movies left out by month. Table 1 shows the percents of movies and users elim inated from the data used to estimate the baseline. Applying the algorithm described in the previous se ction to the cleaned data we obtain a baseline between 4.5% and 5% for 2005. Note that these values are more realistic than the initial 20%. The algorithm was applied to the years 2003, 2004 a nd 2005. Then a linear model was used in order to forecast t hose estimated baseline using information from the previous years. The final model is quite simple and predicts the baseline for the next year as a linear combination of the percent of new movie s in the current year and the percent of new users in the cu rrent year. The application of this model provides an estimated bas eline for 2006 of 3.8%. Note that the real rating obtained once th e scoring data was released was 7.8%, but a wrong baseline of 20% could have been used if the baseline estimation is not perform ed. Our modeling approach tries to reproduce the scorin g task, that is, to predict the rating events during 2006 based on i nformation recorded up to 2005. To do so, the training data was divided in two piec es. The first part (ratings with date prior to January 1, 2005) w as used to create the input variables. The second part (ratings with date posterior to January 1, 2005) was used to build the target. In order to reproduce the sampling scheme used to c reate the scoring data, the sampling algorithm introduced in the previous section was used. A training dataset of 500,000 sam ples was created by iterating the sampling algorithm five ti mes. Therefore, the target variables in the training data consist o n 500.000 binary events (1 if the user has rated the given movie dur ing 2005 and 0 otherwise). Additionally, in order to correct for the issue des cribed in the previous section, the training data was cleaned by eliminating those users and movies that first appeared at the e nd of 2004; using the percentages shown in table 1. This step i s necessary; if omitted, definitions of input variables built for t he training dataset in 2004 would differ from those for the scoring dat aset in 2005, as they would be built on two scoring histories that w ould be quite different in nature  X  Influx of new users and new movies is quite different in either case. Once training dates were cleaned, 250.000 out of th e 500.000 observations were left out for test purposes and th e remaining observations were used to build the predictive mode l. The model used is a logistic model built over  X  intelligent variables  X . Each intelligent variable was created by first summarizing the raw input information and then transforming it into a d iscrete variable using supervised decision trees. The relevance of t hese discrete variables is critical, they add nonlinearities, and saturation effects to the model and are constructed in order to maximi ze its predictive power. Once cleaned, the training data was used to build t hree types of explicative variables: These variables are described in detail below. The selected mathematical model form is a logistic regression [2]. In parallel three other types of model formulations were analyzed but they all yielded poorer results on the test dat a: En la siguiente tabla se compara el rendimiento seg  X n el RMSE de de los modelos anteriores y de la baseline esti mada y real sobre los datos de score proporcionados para la tar ea. Table 2. Most important variables under the final m odel. Boosting algorithms over decision trees each of the m 
Neural Network Multilayer Perceptron with a six Figure 1 shows the expected captured response compu ted using the test data. Given this figure, we were expecting a 10% improvement over the estimated baseline which was 0 .038. The scoring data showed a 6% improvement over our basel ine. Here we described the most significant variables co nsidered. These variables can be classified into 3 categories : These variables intend to add cross-section informa tion; that is to include information on user and/or movies that beha ve similarly. In order to build these variables we proceed to ide ntify groups of users which might have similar rating behaviours an d also groups of movies that were rated in similar ways. The algorithm for the identification of these group s is as follows: A user-movie pair variable is created utilizing the defined user groups. The variable is defined as the ratio betwee n the percentage of users inside the user X  X  group that ra ted the movie and the overall percentage of users that rated the movie. In a similar way the movie groups are used to creat e another user-movie pair variable, defined as the ratio between t he percentage of movies in the movie group rated by the user and the overall percentage of movies rated by the user. This last v ariable has been proven to have a great predictive power. Table 2 lists the most important variables in desce nding order, according to the Gini index. Note that the used var iables are discrete transformations of the described variables . Table 3. Most important variables under the final m odel. 
Expected number of ratings for next year, estimated
Likelihood of rating similar movies more than the 
Percentage of user ratings corresponding to movies 
Likelihood of similar users rating the movie more 
Percent of ratings received by the movie in the las t Percent of 5-star ratings received by the movie. 0 .2320 
Percentage of ratings received by the movie during the last 3 months over ratings received during the Number of months since the movie was first rated 0. 1548 Percentage of user ratings in the last three months 0.1263 We believe that a great amount of our success relie s on the work developed to correctly estimate the baseline model. Although the absolute error of our estimated baseline was over 4 %, (3.8% against 7.8%) we must remember that the raw data ha s a 20% for the year 2005. On the other hand the variables included are also c ritical. We emphasize three ideas about the variables: First note that the model and variable selection is done using the training data previous to 2005, but the final scori ng must include the 2005 information. The key point is to correct t he raw data so the information up to December 2004 is similar to t he whole training data. This is done via the cleaning and sa mpling methods described in sections 2 and 3. The second key point is the fact that the most impo rtant variable in the model is the output of task 2. The brilliant qualification as first runner up obtained on task 2 has allowed us t o use a quality variable available for task 1. Third, the use of user-movie pair variables. These variables are very important and can be used to solve both task 1 and the original Netflix problem. Finally we would like to state that we are not curr ently working on the Netflix problem. Although we have performed the experiment of applying an adapted version of the pr ocedures described in this paper to the Netflix problem. Bas ically a model with the same inputs have been built but with a mul tinomial target (ratings ranging from 1 to 5). The result on the  X  X  robe X  data gave as an error of 0.958; Still above baseline provided by Netflix and very far from the leading positions at the leaderbo ard. We are very grateful to Neo Metrics for the support they have given us since the beginning of this project. In pa rticular we are in debt with David Arias, Tania C X mara, Jesus Figueres , Pen X lope Garz X n and Paz Gil-Delgado for the effort put to so lve this task and Juan-Carlos Iba X ez and Fausto Morales for his h elp in writing this paper. We would also like to express our thank s to the KDD CUP organizers for the work they have carried out. [1] J. Bennet and S Lanning. The Netflix prize . KDD Cup and [2] P. A. Devijver and J. Kittler. Pattern Recognition: A [3] Abdi, H. Singular Value Decomposition (SVD) and [4] R. O. Duda, P. Hart, and D. G. Stork. Pattern classification. [5] R. Salakhutdinov, A. Mnih and G. Hinton Restricted [6] Schapire, R. E., and Singer, Y. Improved boosting [7] T. Hastie, R. Tibshirani, and J. H. Friedman. The elements of 
