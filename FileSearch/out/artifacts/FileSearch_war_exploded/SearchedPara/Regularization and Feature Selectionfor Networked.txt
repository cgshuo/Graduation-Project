 In the standard formalization of supervised learning prob-lems, a datum is represented as a vector of features without prior knowledge about relationships among features. How-ever, for many real world problems, we have such prior knowledge about structure relationships among features. For instance, in Microarray analysis where the genes are fea-tures, the genes form biological pathways. Such prior knowl-edge should be incorporated to build a more accurate and interpretable model, especially in applications with high di-mensionality and low sample sizes. Towards an efficient in-corporation of the structure relationships, we have designed a classification model where we use an undirected graph to capture the relationship of features. In our method, we com-bine both L 1 norm and Laplacian based L 2 norm regular-ization with logistic regression. In this approach, we enforce model sparsity and smoothness among features to identify a small subset of grouped features. We have derived effi-cient optimization algorithms based on coordinate decent for the new formulation. Using comprehensive experimental study, we have demonstrated the effectiveness of the pro-posed learning methods.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms, Experimentation Regularization, Feature Selection, Logistic Regression
Data whose features have intrinsic structure relationships is becoming abundant in many application domains such as bioinformatics, text mining, and computer vision, among others. For instance, in microarray classification, the genes form a biological network (the pathway graph) [7, 8]. In text mining where key words are features, we have additional information about synonyms or antonyms of the features. Such information is usually captured with a word net [1]. Exploring the intrinsic structure of features and utilizing such structure for deriving low dimensional representations of the original data is a critical step for accurate modeling and better model interpretation.

Feature selection for data with  X  X tructured features X , usu-ally high-dimensional data whose features have a known and fixed structure, has recently attracted research intensive in-terest in the machine learning and data mining communities [11, 12, 13]. For example Tibshirani [11] studied feature se-lection for data whose features form a linear chain and pro-posed a method called fused lasso. Yuan and Lin explored the situation where features may be naturally partitioned into groups and devised a technique called grouped Lasso [12]. In [13], both group structure and hierarchical relation of features have been studied in a unified framework.
Adopting existing techniques to aforementioned applica-tions where features adopts a graph structure, rather than groups, chains, or trees for classification, is non-trivial. The challenge originates from the optimization. For linear re-gression, it is relatively easier to manipulate least square loss function than hinge loss, logit loss and exponential loss that are widely used in classification. Furthermore, there is no natural way to incorporate the additional feature graph information into the classification framework. As a prelim-inary study, Li et al. utilized the pathway information of Microarray and proposed a gene selection method by com-bining L 1 norm and L 2 norm regularized pathway graph Laplacian [7]. However, their algorithm is constrained in linear regression for genomic data analysis. In [10], a net-worked feature regularization for classification was proposed, but it only imposed smoothness by L 2 norm graph Lapla-cian on the features and no feature selection was performed. Hence the method may not provide the optimal performance and we will demonstrate that in our experiment study.
In this paper, we devised a more general framework to perform graph structured feature selection for classification. We extended the coordinate descent algorithm presented in [2] for the Elastic Net to derive efficient optimization algo-rithms for a penalized logistic regression model with both L 1 norm and Laplacian based L 2 norm penalty. The advan-tage of the penalization framework is that it combines both the L 1 norm and the graph Laplacian penalties to enforce model sparsity and smoothness in the feature space. Using comprehensive experimental study, we have demonstrated the effectiveness of the proposed learning method.
The rest of the article is organized as follows. Section 2 discusses related work. Section 3 presents background in-formation and detailed discussion of our algorithms. Sec-tion 4 presents the experimental study of our algorithms as compared to competing methods. Finally we give a short conclusion.
To handle structure prior among features, methods that are closely related to ours are fused lasso [11], network con-strained regression [7] and logistic regression with with net-works of features [10]. Fused lasso [11] can be interpreted as a regularization method with an L 1 norm penalty on the graph Laplacian, where the graph is a chain. However, the L 1 penalty in fused lasso forces the weights of neighboring features to be identical rather than similar, which is favor-able in only a few applications. Li and Li [7] regularize fea-ture weights by penalizing the normalized graph Laplacian for biomedical prediction tasks and their model only works for linear regression with least square loss function. San-dler et. al [10] proposed regularized learning on networks of features for logistic regression, in which the feature net-work is built from prior knowledge of whether the pairwise features are similar or dissimilar. However, they only intro-duce the L 2 norm penalty, therefore their model does not enjoy sparsity and cannot perform feature selection.
Our work is different from existing network regularized regression work in that we use a general graph to capture relationships between features for logistic regression. By in-corporating both an L 1 penalty and a L 2 Laplacian penalty, we enforce model sparsity and smooth variation over the known graph, effectively selecting features that are grouped according to the known graph structure (see section 3 for details). Hence the key insight is that the lasso penalty in-troduces sparsity to the model and L 2 Laplacian penalty pe-nalizes parameter values that diverge more from their neigh-boring parameters X  values and obtains smoothness to achieve grouping effect. Though we exclusively work on binary clas-sification in this paper, logistic regression naturally extends to multi-class classification and hence we do not expect any problems in applying our method to multi-class classifica-tion.
We consider a supervised learning problem with incor-poration of structure prior among features for logistic re-gression. Given a set of n training samples T = f (  X  X  i , y  X  X  i 2X R p , y i 2Y = f 0 , 1 g , i 2 [1 , n ], the task of lo-gistic regression is to learn model coefficients  X   X  so that the log-likelihood function (1) and I ( . ) is the indicator function.

In our model, we capture the structure relationship among features as an undirected graph G , whose nodes correspond to the p features. Edges in the graph G are weighted, where the weight w i;j &gt; 0 indicates the  X  X elationship X  be-tween the two features. We call this graph  X  X eature graph X . We incorporate the a priori domain knowledge by adding a Tikhonov regularization factor 1 2 convex fitness function loglik ( . ) to enforce smoothness for neighboring features. The Tikhonov regularization factor could be conveniently written in matrix format  X   X  T L  X   X  where L is the Laplacian of G given by: L = D W . W is the p by p edge weight matrix W = ( w i;j ) p i;j =1 , and D is the density matrix of W , defined as D = ( d i;j ) p i;j =1 d ture  X  X ominate X  the penalization function, we use the nor-malized Laplacian L = D  X  1 2 LD  X  1 2 to normalize the weight of each feature. Tikhonov regularization does not lead to the sparsity of the model. In other words, Tikhonov reg-ularization cannot perform feature selection. To obtain a sparse model, we add the L 1 norm of  X   X  to the negative log-likelihood function. Specifically in our learning, we seek to identify a vector  X   X  that minimizes the following loss func-tion:  X  ( X,  X  X  ;  X   X  ) = loglik (  X   X  ) + 1 2  X  2  X   X  T L  X   X  binary classification for simplicity. Logistic regression can be easily generalized to multi-class classification. A similar formalization was proposed in [7] for linear regression, how-ever, their work focuses on linear regression for genomics study. We are investigating a more general classification problem rather than linear regression across a wide range of application domains.

Using L 2 norm regularized graph laplacian, our method provides two insights: 1) smoothness: the coefficients of neighboring features are close to each other due to the L norm regularized feature graph Laplacian regularization. 2) Grouping effect: Once a feature is selected, its neighboring features will be more likely selected. Thus our algorithm can select groups of neighboring features in the feature graph.
We designed a coordinate descent [2, 3] algorithm to solve the logistic regression problem with the mixture penalty of L 1 and Laplacian regularized L 2 norm. In designing our optimization algorithm, we followed the general framework proposed in [3] for Elastic Net where both L 2 norm and L norm are used for regularized linear regression. Adopting the solution for Elastic Net for our current problem is non-trivial. First, we use logistic regression, rather than linear regression. Second we use a Laplacian weighted L 2 norm (  X   X  T L  X   X  ), rather than the regular L 2 norm (  X   X  T  X   X  ). Recently Friedman et al. [3] have proposed a quadratic approximation scheme for extending coordinate descent algorithm for logis-tic regression with L 1 penalty. Their strategy demonstrated computational efficiency and hence we adopt the same strat-egy to handle the first issue.

Below, we give a detailed derivation of a modified coor-dinate descent algorithm extending the work presented in Elastic Net for handling the combination of Laplacian regu-larized L 2 norm and L 1 norm to handle the second issue. In our derivation, we use the same least squares fitness func-tion as Elastic Net and we discuss about the extension of replacing the least squares fitness function with the nega-tive log-likelihood function later.

Lemma 3.1. Suppose that the data set contains n obser-vations and p predictors, with the response vector Y = ( y y ) T and the data matrix X = (  X  X  1 , . . . ,  X  X  n ) T . We also as-sume that the predictors are standardized and the response is centered so that for all j , and tion is L (  X  1 ,  X  2 ,  X   X  ) = 1 2 ( Y X  X   X  ) T ( Y X  X   X  jj  X   X  jj 1 . The coordinate-wise update has the form (for each  X  ):  X   X  j = S (  X  excluding the contribution from x ij and S ( z,  X  ) = sgn ( z )(  X  ) + is the soft thresholding operator where: The derivation of Lemma 3.1 is based on coordinate descent for elastic net in [3]. Due to space constrain, we do not provide the details.

With Lemma 3.1, we have an effective solver for least square fitting with the mixture penalty of the L 1 norm and the regularized Laplacian L 2 norm. We followed the general framework of coordinated decent algorithm recently proposed by Friedman et al. [3] for L 1 norm regularized logistic regression. Their approach relies on the connection between the Newton X  X  method for optimizing logistic regres-sion and the least square formulation. The Newton X  X  method amounts to using Taylor expansion, up to a quadratic func-tion, to approximate the negative log-likelihood function. In this way, applying Newton X  X  method can be viewed as solving a series of least squares problem (also called itera-tive reweighted least squares tting [3]). Applying Taylor X  X  expansion at current estimate  X   X   X  to negative log-likelihood function (1), we have the reweighted least square problem l (  X   X  ) =  X  p (  X  X  i )) / (  X  p (  X  X  i )(1  X  p (  X  X  i ))), w i =  X  p (  X  X  a constant. The Newton update is obtained by minimizing the following regularized reweighed least squares problem: It is obvious L Q (  X   X  ) can be minimized by coordinate descent with the update rules proposed in Lemma 3.1.

We summarize what is briefly discussed previously in the algorithm called LogLapElasnet. Given the training data X and  X  X  and regularization parameters  X  1 and  X  2 , our al-gorithm iteratively solves logistic regression with L 1 norm and L 2 norm penalty on normalized Laplacian of the feature graph.
 Algorithm 1 LogLapElasnet(  X  1 ,  X  2 , M axIteration,  X  ) 1: Initialize  X   X   X  (0) =  X  0; 2: for i=1 to MaxIteration do 3: Compute the quadratic approximation for (1); 4: Use the coordinate descent method in lemma 3.1 to 6: Break; 7: end if 8: end for 9: return  X   X   X  =  X   X   X  ( i ) ;
We have performed a rigorous evaluation of our regular-ized learning algorithm in terms of modeling accuracy and feature selection performance using two real-world data sets. For comparison, we implemented our own solver for Logistic regression with the Lasso ( L 1 ) penalty (LogLasso), elastic net ( L 2 plus L 1 ) penalty (LogElasnet), and the FNR [10] method with networked L 2 Laplacian penalty. We did not get a chance to compare with [5] but will study it in our future work.
We used the following two data sets for our real-world data study:
Diabetes Data. The data set is obtained from [9] and contains the gene expression values of 22,280 genes for 44 different subjects, 17 with type 2 diabetes (DM2), 17 with normal glucose tolerance (NGT) and 10 with impaired glu-cose tolerance (IGT). As in [8], we use only the 34 samples of subjects with type 2 diabetes and those with normal glu-cose tolerance. We collected all pathway information from the Kyoto Encyclopedia of Genes and Genomes (KEGG) database [6] and use the global test method [4] to obtain re-lated pathways to the diabetes outcome with a significance p-value of less than 0.1. We keep those 13 non-overlapping pathways and merge them to generate a graph. 20 Newsgroup Data. The data is available at http: //people.csail.mit.edu/jrennie/20Newsgroups/ and we use the second one (by date). We merge the original train-ing (60%) and test (40%) sets to form a whole data set. To perform classification, we single out two classes that are very correlated to each other (comp.os.ms-windows.misc and comp.sys.ibm.pc.hardware). The feature set was con-strained to be 610 key words which occur at least 25 times in these 1942 documents excluding stop words. Following the same procedure in [10], we build the graph on features. Refer to [10] for more information.
Below we present our approaches for model construction and model comparison. Model Construction. We par-tition the data set into 10-folds to perform 10-fold cross-validation (CV). We use another 10-fold CV on the training data set to select the regularization parameters  X  1 and  X  using a simple grid search. We then generate a single model from the entire training set with the selected parameters and apply the model to the testing data set for prediction.
Model Comparison. For model comparison, we collect the sensitivity (TP/(TP+FN)), specificity (TN/(TP+FP)) and accuracy ((TP+TN)/ S ) of the trained model, where TP stands for true positive, FP stands for false positive, TN stands for true negative, FN stands for false negative, and S stands for the total number of samples. All the values (specificity, sensitivity, accuracy) reported are collected from the testing data set only and are averaged across 10-fold CV with 3 replicates in a total of 30 experiments.

To compare different feature selection strategies, we also collect the selected features in each cross validation and re-port the average # features selected during the experiments. To demonstrate the group feature selection effect, for Mi-croarray data where the feature graph is sparse, we collect the number of selected feature clusters or pathways. For News group data, the feature graph is dense and there is no natural way to partition the graph into  X  X omponents X . We define the average feature separation  X  d as the average short-est path length of pairs of selected features:  X  d = est path length between feature i and j in the original feature graph.
In Table 1, we report the average test sensitivity, speci-ficity, accuracy, number of selected features, number of se-lected pathways for the Microarray data, and the average feature separation for the NewsGroup data with 200 sam-ples (randomly sampled 100 documents from the two classes) in the 3 replicates of 10-fold cross validation. As shown in Table 1, LogLasso selects the least number of features and LogElasnet selects the most. LogLapElasnet usually  X  109 1159 0.92  X  0.66 0.74 0.61 610 8949 0.34 0.80 0.65 builds a relatively sparser model belonging to less number of pathways with comparable or even better classification performance and with a good balance between sensitivity and specificity of the model.

To further study the X  X rouping effect X  X or feature selection of our method for the Microarray data, we have singled out all genes that are selected in all 30 experiments and inves-tigated the pathways that these selected genes belong to. We observe that genes selected by LogLapElasnet belongs to 9 pathways, while those selected by LogLasso belong to 12 pathways and 13 pathways for LogElasnet. Figure 1: Average accuracy in 30 experiments for data
We hypothesize that the regularization framework works best with small number of samples. To test the hypothe-sis, we randomly sampled equal number of instances from the two classes of newsgroup data and form 5 data sets with 100, 400, 600, 800 and 1000 samples. In Figure 1, we present the average accuracy for 10-fold CV with 3 replicates. First, there is a clear trend that the performance of all the meth-ods increases as the number of samples is increasing. Sec-ond, the performance for the methods with feature selection is superior to that of FNR for all the cases. Finally, our method LogLapElasnet outperforms the others significantly when the sample size is low, and comparable to others when the sample size is high. Such facts confirm the effectiveness of our method for the problem of n &lt;&lt; p when exploring the structure information among features.
We present a learning framework of regularization and feature selection on networked features to incorporate prior knowledge of structure information for logistic regression. By introducing normalized graph Laplacian to regulariza-tion term, we combine L 1 and L 2 norm regularization to achieve both sparsity and smoothness with respect to the coefficients of features. Additionally, our method can se-lect clustered features that are related to the outcomes. We have demonstrated the performance of our method on two real-world data sets.
 This work has been supported by an Office of Naval Re-search award (N00014-07-1-1042) and the National Science Foundation under Grant No. 0845951.
