 Given its importance, the problem of predicting rare classes in large-scale multi-labeled data sets has attracted great at-tentions in the literature. However, the rare-class problem remains a critical challenge, because there is no natural way developed for handling imbalanced class distributions. This paper thus fills this crucial void by developing a method for Classification using lOcal clusterinG (COG). Specifically, for a data set with an imbalanced class distribution, we perform clustering within each large class and produce sub-classes with relatively balanced sizes. Then, we apply traditional supervised learning algorithms, such as Support Vector Ma-chines (SVMs), for classification. Indeed, our experimen-tal results on various real-world data sets show that our method produces significantly higher prediction accuracies on rare classes than state-of-the-art methods. Furthermore, we show that COG can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.5.2 [ Pattern Recognition ]: Design Method-ology X  Classifier Design and Evaluation Algorithms, Experimentation Rare Class Analysis, K-means Clustering, Support Vector Machines, Local Clustering
Classification provides insight into the data by assigning objects to one of several predefined categories. An emerg-Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. ing critical challenge for classification is to address so-called  X  X mbalanced classes X  in the data. Specifically, people are interested in predicting rare classes in the data sets with imbalanced class distributions. For example, in the domain of network intrusion detection, the number of malicious net-work activities is usually very small compared to the number of normal network connections. It is crucial and challenging to build a learning model which has the prediction power to capture future network attacks with low false positive rates. Indeed, rare class analysis is often of great value and is de-manded in many real-world applications, such as the detec-tion of oil spills in satellite radar images [20], the prediction of financial distress in enterprises [33], and the diagnoses of rare medical conditions [25].

To meet the above challenge, considerable research efforts have been focused on the algorithm-level improvement of the existing classifiers for rare class analysis. Two promis-ing research directions are the use of re-sampling techniques and cost-sensitive learning [29]. These two methods indeed show encouraging performances in some cases by directly or indirectly adjusting the class sizes to a relatively bal-anced level. Nevertheless, in this paper, we reveal that the class imbalance problem is strongly related to the presence of complex concepts (inherent complex structures) in the data. For imbalanced data sets with complex concepts, it is often not sufficient to simply manipulate the class sizes. In fact, our experimental results show that adjusting the class sizes alone usually can improve the predictive accuracy of the rare classes slightly, but at the cost of seriously decreas-ing the accuracy of the large classes. As a result, we need to develop a classification method which follows two criteria. 1. The ability to divide imbalanced classes into relatively 2. The ability to decompose complex concepts within a
Indeed, this paper fills this crucial void by designing a method for classification using local clustering (COG). Specif-ically, for a data set with an imbalanced class distribution, we perform clustering within each large class and produce sub-classes with relatively balanced sizes. Then, we apply traditional supervised learning algorithms, such as SVMs, for classification. Since the clustering is conducted indepen-dently within each class but not across the entire data set, we call it local clustering , which is the essential part of our COG method. By exploiting local clustering within large classes, we can decompose the complex concepts, e.g., non-linear-separable concepts for linear classifiers, into relatively simple ones, e.g., linearly separable concepts. Another effect of local clustering is to produce subclasses with relatively uniform sizes. In addition, for data sets with highly skewed class distributions, we further integrate the over-sampling technique into the COG scheme and propose the COG with over-sampling technique (COG-OS).

The merit of COG lies in three aspects. First, COG has the ability to divide imbalanced classes into relatively bal-anced and small sub-classes, and thus provide the opportu-nities in exploiting traditional classification algorithms for better predicting rare classes. Second, similar to the re-sampling schemes, COG is not a  X  X ottom-level X  algorithm but provides a general framework which can incorporate var-ious existing classifiers. Finally, COG is especially effective on improving the performance of linear classifiers. This is noteworthy, since linear classifiers have shown their unique advantages, such as simplicity and understandability, higher executive efficiency, less parameters, less generalization er-rors [12, 28], in many cases.

We have conducted extensive experiments on a number of real-world data sets. Our experimental results show that, for data sets with imbalanced classes, COG and COG-OS show much better performances in predicting rare classes than two popular re-sampling schemes as well as two state-of-the-art rule induction classifiers without compromising the predic-tion accuracies of large classes. In addition, for data sets with balanced classes, we show that our COG method can also improve the performance of traditional linear classifiers, such as SVMs, by decomposing the non-linear-separable con-cepts into linearly separable ones.
In this section, we first describe our methods: COG (Clas-sification using lOcal clusterinG) and COG-OS (COG with Over-Sampling). Then, we present the details about how to perform COG by a simple example.
In a nutshell, COG provides a general framework which can incorporate various linear classifiers and improve their classification performance on data sets with non-linear-separable concepts as well as imbalanced class distributions .Asto COG-OS, it is an extended version of COG, which integrates the over-sampling technique into the COG scheme for the purpose of better predicting rare classes in data sets with extremely imbalanced class distributions.

Figure 1 shows the pseudo-code of COG containing four phases. In Phase I, we employ K-means clustering on class i ( i =1 , 2 ,  X  X  X  ,C ) according to the user preset cluster number K ( i ), and change the instance labels of class i with the sub-class labels provided by the K-means clustering, thus form a multi-class data set with clustering inside every class (if necessary) but not across the entire data set, we call it local clustering . Phase II is ded-icated to COG-OS. In this phase, we replicate R ( j )times the instances of class j ( j =1 , 2 ,  X  X  X  ,C ), to form a more balanced data set. Phase III is straightforward; that is, we build the model on the modified data set using a user speci-fied linear classifier. Phase IV is simply for testing; however, each instance from the test set will be assigned with a label 1. for class i=1 to C //C represents #classes 2. clusterLabel(i)=Clustering(TR(i), K (i)); 3. TR(i)*=changeLabel(TR(i), clusterLabel(i)); 4. end for 5. for class i=1 to C 6. TR(i)**=replicate(TR(i)*, R (i)); 7. end for 9. CM=train(TR**,LCF); 10. clusterLabel=predict(TE, CM); 11. predictLabel=convertLabel(clusterLabel); 12. CR=compareLabel(predictLabel, trueLabel(TE)); of a subclass which must be converted into the label of the corresponding parent-class.
 There are some points needed to be further addressed. Typically, we do local clustering and over-sampling (if nec-essary) on different classes; that is, in Phase I and II, the cluster number K ( i ) is larger than 1 for the relatively large class i , while the replication ratio R ( j ) is larger than 1 for the small or rare class j . In practice, we first assign K ( i )witha small number, e.g., four, on the large class i ( i =1 ,  X  X  X  then use R ( j ) on the small class j ( j =1 ,  X  X  X  ,C ) to adjust the data set to a relatively balanced situation.

Also, we must emphasize that COG and COG-OS have more impact on the performances of linear classifiers. We know that linear classifiers have various merits such as sim-plicity and understandability, higher executive efficiency, less parameters, less generalization errors, and so on [12, 28]. In contrast, although non-linear classifiers such as SVMs with RBF kernel can find sophisticated boundaries, their param-eters are typically hard to specify, and the use of non-linear kernels can easily lead to overfitting [12]. Therefore, in this paper, we use SVMs with a linear kernel. Furthermore, non-linear-separable concepts in the data is a long-standing chal-lenge for the use of linear classifiers. To this end, COG and COG-OS can strengthen the use of linear classifiers by de-composing the non-linear-separable complex concepts into linear-separable ones. By contrast, as shown in our exper-imental results (Section 4), COG shows no consistent im-provements on non-linear classifiers such as decision trees and rule based learning algorithms.

While we use k-means as the clustering scheme in COG, the choices of clustering algorithms in COG is not limited to k-means. Any other clustering algorithm which can produce clusters with relatively balanced sizes, such as the EM algo-rithm, can also be used in COG. Finally, COG is efficient in terms of the computational performance. First, if K-means is used for the local clustering, the time required in the clus-tering phase is modest  X  basically linear in the number of data points [29]. Then in the training phase, as the num-ber of classes increases, the time required is where T is the time required for the training without local clustering. Since the number of relatively large classes in a data set is often very small, and in practice each K ( i )isusu-ally assigned with a small number, e.g., four, we can expect to keep the computational cost of COG in the same level as the original classifier.
Here, we use a synthetic data set with three classes and the SVMs classifier to illustrate the process of COG. The sizes of the three classes are 133, 60 and 165 respectively, and please note that the small class is non-linear-separable from the other two large classes, as shown in the  X  X riginal Data X  subplot of Figure 2. The SVMs tool we used here is LIBSVM [5] with the linear kernel.
 First, we build the classification model by simply applying SVMs on the original data set, and the results are shown in the pure SVM subplot. In this subplot, the solid line represents the maximal margin hyperplane (MMH) learned by SVMs algorithm. One interesting observation is that the instances of the small class, i.e., class 2, have totally  X  X isappeared X ; that is, they are all assigned to either class 1 or class 3 according to the only one MMH. This is due to the non-linear-separable concepts in the data.

Instead, we employ COG. First, we apply local clustering on class 1 and 3 respectively, given the cluster number is two. The clustering results can be seen in subplot  X  X OG: Phase I X . That is, class 1 and class 3 are divided into two sub-classes respectively by K-means. Thus we obtain a modified data set with five relatively balanced and linear-separable classes. Next, we apply SVMs on this five-classes data set and get results shown in subplot  X  X OG: Phase III X . As can be seen, more MMHs appear in the model, which enables the model to identify the instances of class 2. Finally, for each instance, we convert its predictive label of some subclass into the label of the parent-class. This is equal to delete the MMHs separating the subclasses derived from a same parent-class, as indicated by the  X  X OG: Phase IV X  subplot. Therefore, by applying COG, we build up a more accurate model which can identify the instances from the small class among non-linear-separable concepts.
In this section, we illustrate why COG is especially ef-fective on predicting rare classes using an example. First, we generate synthetic data sets for three different scenarios: data with simple concepts, data with non-linear-separable concepts, and data with complex concepts. In this example, we again use LIBSVM [5] with the linear kernel as the clas-sifier in COG and COG-OS. Also, the synthetic data sets are two-class data sets with two dimensions, and the sizes of the rare and normal classes are 14 and 136, respectively.
First of all, let X  X  give an informal definition. We call that a concept is complex in data if the distributions of the in-stances from the two classes are too close to be separable by the linear classifiers. Therefore, in this scenario, we have two well-separated classes which represent a rather simple concept in the data, as shown in  X  X cenario I X  of Figure 3. As can be seen, for this simple concept, pure SVM sepa-rates the rare and normal classes easily and precisely (the solid line represents the maximal margin hyperplane learned by the SVMs algorithm). This implies that the rare class problem will be inapparent in the case of simple concepts.
In Scenario II, we consider the case that data sets contain non-linear-separable concepts, which can seriously hinder the performance of linear classifiers. In the COG scheme, we exploit local clustering to divide non-linear-separable con-cepts into smaller linear-separable concepts. In this way, traditional linear classifiers can still work well in this sce-nario. In Figure 3, two subplots of  X  X cenario II X  show the process of COG on handling non-linear-separable concepts. As can be seen in subplot II-I, pure SVM cannot effectively identify the rare class, since the instances of rare class and normal class are very close. After applying COG, however, we can divide the large class into two subclasses and form a data set with three linear-separable sub-classes, which can be easily learned by SVMs, as shown in subplot II-II.
In this subsection, we consider a more complicated case that data sets contain complex concepts. In other words, the instances of the rare class are adjacent to the instances of the normal class in the data, as shown by subplots of  X  X cenario III X  in Figure 3.

In this scenario, COG alone seems not very helpful for rare class analysis. As can be seen in subplot III-I of Fig-ure 3, COG cannot separate the rare class from one of the subclasses of the normal class. Also, in subplot III-II, we can see that the traditional over-sampling technique per-forms poorly for predicting rare class, since many instances of the normal class have been assigned to the rare class (the replicative time is set to be 8 on the rare class to obtain a relatively balanced distribution). However, COG with the over-sampling technique (COG-OS) can help SVMs success-fully isolate the over-sampled instances of the rare class from the two sub-classes partitioned by K-means clustering on the normal class, as shown in the Subplot III-III of Figure 3.
Discussion. In summary, the rare class problem is strongly related to the presence of the complex concepts (inherent complex structures) in the data. The more complex the concepts are, the more significant the rare class problem is. By applying local clustering, COG can handle the rare class problem in the presence of the non-linear-separable con-cepts; by further incorporating the over-sampling scheme, COG-OS can handle the rare class problem in the case of more complex concepts.
In this section, we present experimental results to vali-date the performance of the COG and COG-OS methods on balanced and imbalanced classification problems. Experimental Tools . We used four types of classifiers: sup-port vector machines, Bayesian logistic regression, decision trees, and rule-based classifiers. Their corresponding imple-mentations are LIBSVM [5], BMR [1], C4.5 [2], and RIP-PER [7]. In all the experiments, default settings were used except that the kernel type of LIBSVM was set to be linear. Thus we have two linear classifiers, i.e., LIBSVM and BMR, and two non-linear classifiers, i.e., C4.5 and RIPPER.
Also, we applied K-means, a widely used clustering scheme which tends to produce clusters with relatively uniform sizes, as the clustering method in our COG method. During the K-means clustering, for data sets with relatively small number of dimensionality, squared Euclidean distance was used as the proximity measure; and for data sets with high dimen-sionality, however, the cosine similarity was used instead. This is due to the fact that Euclidean notion of proximity is not very meaningful for high-dimensional data sets, such as document data sets. Note that for each data set, K-means ran ten times and returned the best partitioning result.
Finally, some notations are given in Table 1. The default classifier used in these schemes is SVMs. If other classifiers are used instead, e.g, BMR in the COG scheme, we will explicitly denote it by  X  X OG(BMR) X .
 Experimental Data Sets . For our experiments, we used a number of benchmark data sets that were obtained from different application domains. Some characteristics of these data sets are shown in Table 2. In the table, CV  X  Coef-ficient of Variation [9] X  shows the dispersion of the class distribution for each data set. In general the larger the CV value is , the greater the variability is in the data.
UCI Data Sets. In our experiments, we used eight well-known benchmark data sets from UCI Repository [26]. Among them two data sets, breast-w and pima-diabetes , are binary data sets from the medical domain. The breast-w data set contains two types of results from real-world breast cancer diagnosis, and the pima-diabetes data set is about the information of whether the patient shows signs of dia-betes according to the WHO criteria. The rest six data sets are frequently used by the pattern recognition community. letter , optdigits and pendigits are data sets containing the information of handwritings; that is, letter has the let-ter information from A to Z , and the other two have the number information from 0 to 9. The satimage data set contains the multi-spectral values of pixels in 3  X  3neigh-borhoods in a satellite image. The page-blocks data set contains the information of five types of blocks from a docu-ment page layout. And the last data set vowel was designed for the task of speaker independent recognition of the eleven steady state vowels of British English.

Document Data Sets. We also used high-dimensional document data sets in our experiments. The data set k1b was from the WebACE project [15]. Each document cor-responds to a web page listed in the subject hierarchy of Yahoo!. The la12 data set was obtained from articles of the Los Angeles Times that was used in TREC-5 [30]. The cat-egories correspond to the desk of the paper that each article appeared and include documents from the entertainment, financial, foreign, metro, national, and sports desks. For these two document data sets, we used a stop-list to remove common words, and the words were stemmed using Porter X  X  suffix-stripping algorithm [27].

LIBSVM Data Sets. Finally, we applied four binary data sets: fourclass , german.numer , splice and SVMguide1 from the LIBSVM repository [5].

Please note that for any data set without an appointed test set, we did random, stratified sampling on it and had 70% samples as the training set and the rest as the test set.
In this subsection, we show how COG can improve the performance of linear classifiers on imbalanced data sets. As discussed in Section 3, the problem of imbalanced classes is related to the complex concepts in the data  X  such as non-linear-separable concepts for linear classifiers. The COG method is a natural solution to this problem: handling the non-linear-separable concepts as well as making the class sizes be relatively balanced. For data sets with highly imbal-anced class sizes, such as binary data sets with rare classes, we apply COG with the over-sampling scheme (COG-OS). Specifically, for the large class of any binary data set, we did K-means clustering on it, and set the cluster number consis-tently to be 4; and for the rare class, we did over-sampling on it, and made the size be approximate to the average size of the partitioned large class. In this way, we can have much more balanced data sets. Also, the non-linear-separable con-cepts can be transformed into linearly separable concepts.
Also, we prepared the imbalanced data sets with rare classes via sampling on various binary data sets. Specif-ically, for each data set with two classes, we did random sampling on the small class to turn it into a rare class, then combined it with the original large class to form a sample data set. Detailed information of the samples can be found in Table 3. We did sampling ten times for each data set and returned the average classification results for it, as shown in Table 4. Finally, since SVMs shows best classification performance in many cases [8], we used it as the benchmark classifier for all the experiments in this subsection. Results by COG-OS on Two-class Data Sets.

Table 4 shows the performance of COG-OS(SVMs) and pure SVM on six two-class data sets. As can be seen, pure SVM assigned all the instances to the large class of data sets pima-diabetes , fourclass and german.numer . This indi-cates that pure SVM has no prediction power on rare classes for these three data sets. In contrast, COG-OS can success-fully identify more than 25 percent instances of the rare classes. Indeed, the F-measure values of the rare classes by COG-OS(SVMs) are consistently higher than the F-measure values produced by pure SVM for all six data sets, as indi-cated in Table 4. For instance, for data sets SVMguide1 and fourclass , COG-OS(SVMs) results in the increases of the F-measure values by more than 0 . 4.
 Figure 4: The Effect of the Size of the Rare Class.
Another observation is that, for COG-OS, the increase of the F-measure value of the rare class is NOT at the high cost of the prediction accuracy of the large class. For example, for data sets breast-w and fourclass , the F-measure values of both large and rare classes by COG-OS are higher than the F-measure values produced by pure SVM. Also, for the rest four data sets, the F-measure values of the large class by COG-OS are just slightly smaller. This is acceptable since the rare class is usually the major concern in many real-world applications.

In addition, we also investigate how the F-measure value changes as the increase of the sampling ratio on the small class. As an example on the SVMguide1 data set, Table 5 Table 7: COG-OS vs. the Resampling Scheme.
 shows the information of various samples as the increase of the size of the rare class. Figure 4 shows the classification results on these samples. Please note that for each sam-pling ratio, we did sampling ten times and therefore had ten samples for each ratio. Then we applied pure SVM and COG-OS on the ten samples respectively, and finally got ten results for each sampling ratio, as indicated by the box plots in Figure 4. As can be seen, the F-measure values by COG-OS are consistently higher than the ones produced by pure SVM, no matter what the sampling ratio is. Moreover, COG-OS performs much better than pure SVM when the rare class size is relatively small. However, as the increase of the size of the rare class, the performance difference is decreased.

Results by COG on Multi-classes Data Sets. In ad-dition to the experiments on the two-class data sets, we also employed some multi-classes data sets with imbalanced classes to validate the COG method. For these multi-classes data sets, we simply used the COG scheme. Since the Eu-clidean distance is not very meaningful for the sparse doc-ument data sets k1b and la12 , for the COG method, we used the CLUTO [19] implementation of K-means on these two data sets with cosine similarity as the proximity mea-sure. Table 6 shows the classification results by pure SVM and COG. As can be seen, for data set k1b , the F-measure value for every class using COG is higher than that produced by pure SVM. Meanwhile, the results on the page-blocks and la12 data sets show a similar trend as k1b .Insum-mary, COG indeed can improve the prediction performance on rare classes, and this improvement is achieved without a big loss of the prediction performance on large classes.
In previous sections, we mentioned that resampling is a widely used technique to improve the classification perfor-mance on imbalanced data sets. Here, we compare the per-formances of COG-OS with two resampling strategies  X  under-sampling and over-sampling. Details of these two re-sampling methods can be found in various books [29, 23].
In this experiment, we also employed the six sampled data sets as shown in Table 3. We set the sampling ratio for under-sampling or over-sampling to make the modified size of the rare class be approximate to the one of the large class, and the classifier we used here is SVMs. Table 7 shows the results. One observation is that, for all data sets in Table 7, COG-OS performs the best for the rare class, except for one data set: splice , on which COG-OS and over-sampling show comparable results. Another observation is even more encouraging; that is, while obtaining excellent performances on the rare classes, COG-OS also provides much higher pre-dictive accuracies on the large classes, which has long been the  X  X hoke point X  of the resampling schemes. This is not sur-prising since the non-linear-separable concept in the data is usually the bottle-neck of the class imbalance problem. By integrating the over-sampling scheme, COG can further im-prove its ability to identify the instances from the rare class.
In summary, compared to two widely used resampling strategies, COG-OS shows appealing performances on han-dling non-linear-separable data with rare classes, yet keeps a much better performance on large classes.
Here, we demonstrate an application of COG-OS for net-work intrusion detection. For this experiment, we used a real-world network intrusion data set, which is provided as part of the KDD-CUP-99 classifier learning contest [3], and now is a benchmark data set in the UCI KDD Archive [4].
The KDD CUP Data Set. The data set was collected by monitoring a real-life military computer network that was intentionally peppered with various attacks that hack-ers would use to break in. Original training set has close to 5 million records belonging to 22 subclasses and 4 classes of attacks, i.e., dos, probe, r2l and u2r, and still one nor-mal class. In this experiment, we applied 10% sample of this original set which is also supplied as part of the con-test. We present results for two rare classes: probe and r2l , whose populations in the 10% sample training set are 0 . 83% and 0 . 23%, respectively. The test set provided with the 10% training set, however, has some new subclasses that are not present in the training data. So for the evaluation concern we deleted these new subclasses, and the resultant populations of probe and r2l in the test set are 0 . 81% and 2 . 05%, respectively. Table 8 shows the detailed information of these data sets. Note that we obtained the probe binary data set by making the probe class as the rare class, and the rest four classes as one large class. The other data set, i.e., r2l binary , was prepared in a similar fashion. Table 10: Classification Accuracies by COG with Different Classifiers.

The Benchmark Classifiers. In this experiment, we apply four classifiers: COG-OS(SVMs), pure SVM, RIP-PER [7], and PNrule [17]. For COG-OS, the cluster number for the large class is 4 for each data set, and the replicative times of over-sampling on the rare classes for probe binary and r2l binary are 30 and 120, respectively. For SVMs, we set the parameters as: -t 0. Ripper and PNrule are two rule induction classifiers. RIPPER builds rules first for the smallest class and will not build rules for the largest class. Hence, one might expect that RIPPER can provide a good performance on the rare class. As to PNrule, it con-sists of positive rules (P-rules) that predict presence of the class, and negative rules (N-rules) that predict absence of the class. It is right the existence of N-rules that can ease the two problems induced by the rare class: splintered false positives and error-prone small disjuncts. These two clas-sifiers have shown the appealing performance on classifying the modified binary data sets in Table 8, and the PNrule classifier even shows superior performance [17]. To our best knowledge, we used the same source data as [17] and the pre-process procedure for the modified data sets is also very similar to [17]. Therefore, we simply adopted the results of PNrule in [17] for our paper. Figure 5: Ratio of the Classes with Accuracy Im-provements by COG.

The Results. Table 9 shows the classification results by various methods on the probe binary data set. As can be seen, COG-OS performs much better than pure SVM and RIPPER on predicting the rare class as well as the large class, while PNrule shows slightly higher F-measure on the rare class. For data set r2l binary , however, COG-OS shows overwhelming advantages among all classifiers. As indicated in Table 9, the F-measure value of the rare class by COG-OS is 0 . 496, far more higher than the ones produced by the rest classifiers. Meanwhile, the predictive accuracy of the large class by COG-OS is also higher than that of pure SVM and RIPPER. This real-world application nicely illus-trates the effectiveness of COG-OS  X  the combination of local clustering and over-sampling schemes. We believe that COG-OS is a prospective solution to the difficult classifica-tion problem induced by the non-linear-separable concepts and imbalanced class distributions. In the previous subsections, we have shown that COG and COG-OS indeed can improve the prediction accuracies of the rare classes for imbalanced data sets. In this subsection, however, we would like to show COG is also applicable to data sets with balanced class distributions.
 In this experiment, we used five balanced data sets with CV &lt; 0 . 5, i.e., letter , optdigits , pendigits , satimage and vowel . Among them, four data sets have been split into training and test sets by UCI repository except for the letter data set. Four classifiers including SVMs, BMR, C4.5 and RIPPER were used for the purpose of comparison. The clustering method in the COG scheme is K-means with Euclidean notion proximity, and the cluster number for each class in a data set is exactly the same, ranging from 2 to 8.
Results by COG with Linear Classifiers. Table 10 shows the experimental results on these balanced data sets. As can be seen, for linear classifiers SVMs and BMR, COG indeed can improve the classification accuracies no matter what the cluster number is. For instance, for the data set letter , the accuracies achieved by pure SVM and BMR are merely 0 . 851 and 0 . 750 respectively (as indicated by the italic numbers). In contrast, COG with SVMs and BMR can increase the prediction accuracies of the rare classes steadily as the increase of the cluster number, and finally up to 0 . 952 and 0 . 850 respectively when the cluster number is 8. Indeed, the resultant prediction accuracies are 10% higher than the ones obtained by pure SVM and BMR.

Next, we take a closer look at the performance of COG in the class-wise level. Table 11 shows the classification accu-racies on the data set optdigits by pure SVM and COG. Figure 6: Comparison of the Maximal Accuracy Gains by COG with SVMs and BMR.
 In the table, we can observe that COG can simultaneously improve the classification accuracies for nearly all the classes of optdigits . In addition, Figure 5 shows the improvement ratio of classes in the five balanced data sets by COG with SVMs and BMR ( X #clusters X =8). A very similar improve-ment trend can be observed for all five data sets. Indeed, COG can transform the non-linear-separable concepts in the data into the linear-separable concepts so as to simultane-ously improve the classification performance of linear clas-sifiers for most of the classes in the data.

Another interesting observation is that, the accuracy im-provements gained by COG with SVMs and BMR are quite close. To illustrate this, we compute the maximal accuracy gain for each data set; that is, we first select the highest ac-curacy among four values achieved in different  X #clusters X  levels, then subtract it by the accuracy obtained by the pure classifier. Figure 6 shows the results. As can be seen, the maximal accuracy gains of all data sets by COG with SVMs and BMR are quite close except for pendigits .Thisim-plies that the non-linear-separable concept in the data is the bottle-neck that hinders the analysis of linear classifiers.
Results by COG with Non-linear Classifiers. Ta-ble 10 also shows the results of COG with non-linear classi-fiers such as RIPPER and C4.5 on five balanced data sets. In the table, we can see that COG(RIPPER) has worse per-formance than pure RIPPER on all five data sets. This is due to the fact that the rule learning algorithm aims to build up a rule set in a greedy fashion by employing the standard divide-and-conquer strategy. Meanwhile, COG partitions instances of the same class into different sub-classes. This can increase the number of negative examples for some tar-get rules, and ultimately result in missing such rules.
Finally, for another widely used non-linear classifier C4.5, the performance of COG with C4.5 is not consistent on five balanced data sets as shown in Table 10. For instance, COG can improve the classification accuracy of optdigits , but lead to worse performances on letter , vowel and satimage . This is due to the fact that COG can increase the number of sub-classes so as to make the branch-splitting decision even harder to make. In other words, the splitting attributes of the tree can be better or worse selected in such  X  X ncertain X  scenarios, which results in the inconsistent performances. Figure 7: Comparison of the Classification Accura-cies by COG and Random Partitioning.

COG versus Random Partitioning. In this experi-ment, we compare the effect of clustering in the COG scheme with that of simple random partitioning. To this end, we take the data set letter and the classifier SVMs to illus-trate this. First, we randomly split the letter data set into two parts, 70% of which as the training set and the rest as the test set (the class size distribution holds). Then we per-formed training and testing in a very similar fashion to the procedure of COG except that we use random partitioning instead of clustering on each class. Figure 7 shows all the results. Please note that for random partitioning and COG,  X #clusters X = 4. As indicated by Figure 7, while the perfor-mance of random partitioning with SVMs, i.e., RP(SVMs), are slightly better than the results of pure SVM, they are much worse than the results by COG(SVMs). This indi-cates that the clustering phase in COG is very important. In fact, the local clustering process can transform the non-linear-separable concepts of the data into linear or  X  X uasi X  linear concepts, and so as to improve the classification ac-curacy of linear classifiers.

In summary, COG is of great use on improving the classi-fication accuracy of linear classifiers by eliminating or miti-gating the negative impact of the non-linear-separable con-cepts in the data. But for the non-linear classifiers, such as C4.5 and RIPPER, COG shows no competitive results.
In the literature, there are a number of methods address-ing the class imbalance problem. For instance, the sam-pling based methods are one of the simplest yet effective ones. The over-sampling scheme replicates the small classes to match the sizes of large classes [22, 29]; under-sampling, however, cuts down the large class sizes to achieve a simi-lar effect [21, 29]. Drummond and Holte [11] provided de-tailed comparisons on these two resampling schemes. An-other popular method is the cost-sensitive learning scheme which takes the cost matrix into consideration during model building and generates a model that has the lowest cost. The properties of a cost matrix had been studied by Elkan [13]. Margineantu and Dietterich [24] examined various methods for incorporating cost information into the C4.5 learning algorithm. Other cost-sensitive learning methods that are algorithm-independent include AdaCost [14], MetaCost [10], and Costing [32]. In addition, Joshi et al. [18] discussed the limitations of boosting algorithms for rare class modeling and proposed PNrule, a two-phase rule induction algorithm, to handle the rare class purposefully [17]. Other algorithms developed for mining rare classes include SMOTE [6], RIP-PER [7] etc. A survey paper is given by Weiss [31].
Finally, in her inspiring paper, Japkowicz [16] shows the idea of  X  X upervised learning with unsupervised output sep-aration X . This work shares some common grounds with our COG method in terms of combining supervised and unsu-pervised learning techniques. However, in this paper, we have a novel perspective on rare class analysis. We develop the foundation of classification using local clustering (COG) for enhancing linear classifies on handling both balanced and imbalanced classification problems.
In this paper, we propose a method for classification using local clustering (COG). The key idea is to perform cluster-ing within each class and produce linearly separable sub-classes with relatively balanced sizes. For data sets with imbalanced class distributions, the COG method can im-prove the performance of traditional supervised learning al-gorithms, such as Support Vector Machines (SVMs), on rare class analysis. In addition, the COG method has the capa-bility in enhancing linear classifiers on data sets contain-ing non-linear-separable classes. Finally, as demonstrated by our experimental results on various real-world data sets, COG with over-sampling can have much better prediction performance on rare classes than state-of-the-art methods. This research was partially supported by the National Science Foundation of China (NSFC) Research Fund Nos. 70621061 and 70518002. Also, this research was supported in part by a Faculty Research Grant from Rutgers Business School-Newark and New Brunswick. [1] Bmr. In http://www.stat.rutgers.edu/ madigan/BMR/ . [2] C4.5. In http://www.rulequest.com/Personal/ . [3] Kddcup. In [4] Kddcup99data. In [5] Libsvm. In www.csie.ntu.edu.tw/ cjlin/libsvm/ . [6] N. Chawla, K. Bowyer, L. Hall, and W. Kegelmeyer. [7] W. Cohen. Fast effective rule induction. In ICML , [8] N. Cristianini and J. Shawe-Taylor. An Introduction to [9] M. DeGroot and M. Schervish. Probability and [10] P. Domingos. Metacost: a general method for making [11] C. Drummond and R. Holte. C4.5, class imbalance, [12] R. Duda, P. Hart, and D. Stork. Pattern classification . [13] C. Elkan. The foundations of cost-sensitive learning. [14] W. Fan, S. Stolfo, J. Zhang, and P. Chan. Adacost: [15] E.-H. Han and et al. Webace: A web agent for [16] N. Japkowicz. Supervised learning with unsupervised [17] M. Joshi, R. Agarwal, and V. Kumar. Mining needle [18] M. Joshi, R. Agarwal, and V. Kumar. Predicting rare [19] G. Karypis. Cluto  X  software for clustering [20] M. Kubat, R. Holte, and S. Matwin. Machine learning [21] M. Kubat and S. Matwin. Addressing the curse of [22] C. Ling and C. Li. Data mining for direct marketing: [23] O. Maimon and L. Rokach, editors. The Data Mining [24] D. Margineantu and T. Dietterich. Learning decision [25] P. Murphy and D. Aha. In UCI Repository of Machine [26] D. Newman, S. Hettich, C. Blake, and C. Merz. Uci [27] M. F. Porter. An algorithm for suffix stripping. [28] S. Raudys and A. Jain. Small sample size effects in [29] P.-N. Tan, M. Steinbach, and V. Kumar. Introduction [30] TREC. In http://trec.nist.gov . [31] G. Weiss. Mining with rarity: a unifying framework. [32] B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive [33] J. Zurada, B. Foster, and T. Ward. Investigation of
