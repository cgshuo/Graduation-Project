 between mobile devices in order to obtain a further improvement of the pattern database. The last two methods to handle the tion accuracy. Also, the algorithms should not have a complex implementation or be time consuming in order to be used in these devices.

Prototype reduction techniques have been divided in categories the outcoming prototypes. For example, Fayed and Atiya (2009) accomplished by removing prototypes that generate misclassi fi ed data, for example, removing  X  outlier  X  patterns or removing pat-terns that are surrounded mostly by others belonging to different second approach is aimed at obtaining a small template that is a subset of the training set without a substantial change of the decision boundary are crucial to the KNN decision, but those far away from the class boundary do not affect the decision.
Therefore, a systematic removal of the ineffective patterns helps reducing the computation time. Additionally, Chih-Feng et al. (2011) have considered the existence of two other categories, the instance-fi ltering and the instance-abstraction based methods. described using a single parameter. One of the most simply the volume grouping these patterns, or for high dimensional data a hyper-sphere. In order to propose a radius for the hyper-sphere that contains the patterns x i , we will use the following theorem: patterns y 1 ,y 2 ,..,y j , ... .y m. Then: 1 2  X  x n y i  X  r  X  y i x i  X   X  2  X 
Proof. The triangular condition:  X  x y i  X  r  X  x n x i  X   X   X  y i x i  X   X  3  X  replacing  X  x n x i  X  for  X  y i x i  X  in (3) we have:  X  x y i  X  r 2  X  y i x i  X   X  4  X  which is equivalent to expression (2) .

Using expressions (1) and (2) , a more restricted condition for the patterns x i could be de fi ned as:  X  x x i  X  r 1 2  X  x n y i  X  r  X  y i x i  X  or  X  x x i  X  r 1 2  X  x n y i  X   X  5  X 
Expression (5) is a particular case of expression (1) . Thus, the number of patterns that ful fi ll condition (1) .
 condition:  X  x x i  X  r 1 2 min  X  x n y i  X   X  6  X  also holds.
 The radius
R  X  1 2 min  X  x n y i  X   X  7  X  is called PG Radius.
 Furthermore, it is possible to de fi ne the Spherical Prototype Generator (SPG) as:
De fi sional Euclidian Space, then the position of a SPG in the feature of patterns x i A A that ful fi ll:  X  x x i  X  r R  X  8  X  j  X  0 ... m .

According to De fi nition 2.2 , all the pattern data inside the sphere with radius R could be hypothetically generated by apply-space. Therefore, the SPG could be selected using the following algorithm: , and x 22 .

The SPG approximates the pattern data by a set of hyper-spheres
It can be seen in Fig. 3 that the number of SPGs in Fig. 3 (b) is the nn parameter was used.

SPG spheres which forms what we call the core of the classes as shown in Fig. 4 .
 order to obtain this boundary a second algorithm is proposed and called here after Prototype  X  s Front Propagation (PFP-2). 1.1.2. Prototype ' s Front Propagation algorithm
The PFP-2 algorithm increases the prototype radius, until the boundary (front) of the hyper-spheres (formed by the prototypes) of one class intercepts with the front of the hyper-spheres corresponding to the other class. This means that the algorithm f ; f j  X  arg min  X  x n i x n j  X  R i R j  X  9  X  Algorithm. Prototypes Front Propagation (PFP-2) Inputs: Outputs: Method: While (true) For of each pair of prototypes generator ( p i , q j )in  X  End For End While End
Fig. 5 shows a schematic representation of the PFP to obtain the
The radius of the pair of closest SPG of two different classes is increased up to the point where the boundaries intercept. This point corresponds to the position of a new support vector and the
The next step is to update the radius of the others SPGs to the same value  X  R and repeat the previous procedure as long as there are no more SPG pairs.

The resulting support vectors form a hyper-surface that divides the classes. This hyper-surface is shown in Fig. 6 .Theobtained normal to this hyper-surface.

Since this boundary hyper-surface divides two classes, it can be used for the classi fi cation of the input data. However, before vector of the input vectors.

Fig. 7 shows a schematic representation of the data classi fi ca-support vector P
P given by: x
DE 4 0 8 x !
A detailed description of the PFP 1 and PFP 2 algorithms is presented in Appendix B . 2. Numerical experiments
The numerical experiments concerning the proposed algorithm datasets: (a) Gaussian-like pattern, (b) Chessboard-like pattern, algorithm and the dependency of the classi fi cation and conden-sing rates on the data dispersion (  X  ) in comparison with other condensing algorithms.

The second part of the numerical experiments uses real datasets from UCI databases. The purpose of this part is to compare the accuracy of the proposed algorithm with the accuracy of general
Table 1 shows the information of the real datasets used in the numerical experiments.

The general purpose algorithms used for comparison with the proposed algorithm with respect to the accuracy performance were applied using the RapidMiner Studio 6.0 Starter edition software. They are summarized in Table 2 .
 not include the test data. 3. Analysis of the complexity of the PFP algorithm
In order to study the complexity of the proposed algorithm, it (PFP-1) and (ii) the Front Propagation Algorithm (PFP-2). expressed as:
C  X  OdN 1 N 2  X  d  X  N 2 1  X  N 2 2  X  13  X  where N 1 and N 2 are the number of training patterns of class 1 a coef fi cient.

Eq. (13) is deduced as follows: belonging to class 1 and class 2 and between the elements of each class need to be computed. The complexity of these computations are ON 1 N 2  X  X  , ON 2 1 , and ON 2 2 ,respectively. (2) In the second step of PFP-1 is necessary to fi nd the minimum
ON 1 and for class 2, in order to fi nd the prototype x whole process is:
C  X  OdN 1 N 2  X  d  X  L or
C  X  OdN 1 N 2  X  d  X  L where
N are the number of resulting patterns in the classes 1 and 2, respectively, after each loop cycle is performed.
 the loop number is given by: i  X  X  X   X  i  X  X  X  C exp i = T  X  16  X 
The values of C and T are independent of N 1 and N 2 . The values
The fact that C and T are constant means that L should increase reduction of L and consequently the increase of the speed of this part of the algorithm is mainly a consequence of the simultaneous detection of prototypes, i.e. for the same loop cycle several ful fi lled by several prototypes not sharing patterns inside their hyper-spheres, then all these prototypes are selected simulta-neously reducing the computational time and the number of patterns that should be selected in the next loop cycle.
Following the complexity analysis, substituting (16) in (15) gives:
C s 1  X  OdN 1 N 2  X  d  X 
For the PFP-2 algorithm, the complexity is determined by the distances sorting between the prototypes of the classes 1 and 2. Thus, the complexity is given by
C  X  OP 1 P 2 log P 1 P 2  X  X  X  19  X  where P 1 and P 2 are the number of prototypes of the classes 1 and 2, respectively. The complexity depends on the number of number of prototypes depends on the class topology and its dispersion. Fig. 9 shows the consumed CPU time as a function of the number of patterns corresponding to all algorithms: ENN,
DROP3, ICF, HMNC, HMNE, HMNEI when the three arti fi cial data-sets: Gaussian-like, Chessboard, and Simple Non-Convex are used, with a data dispersion parameter value of  X   X  0.35.

Fig. 9 shows that the CPU time taken by the PFP algorithm is smaller in comparison with the algorithms ENN, DROP3, ICF,
HMNC, HMNE, and HMNEI. Additionally, Fig. 9 shows that PFP-2 consumes most of the CPU time of the PFP algorithm for the Gaussian-Like and Chessboard-like data-sets. However, in the
Simple Non-Convex data-set, the contribution of PFP-2 is negli-the fact that the number of prototypes necessary to describe the training data using the proposed PFP algorithm depends on the topology of the used data-sets. On the other hand, Fig. 10 shows the dependence of the consumed CPU time of PFP on the number of patterns of the training data for a real data-set from the UCI database:  X  Skin segmentation  X  .

PFP consumes less CPU time than the other methods. 4. Results and discussions 4.1. Application of the PFP algorithm to arti fi cial data-sets. tion methods.
 of the data dispersion parameter (  X  ) for the algorithms ENN,
DROP3, ICF, HMN-C, HMN-E, HMN-EI, 1-NN, in comparison to the proposed PFP. The best value of the nn parameter for the PFP was selected as nn  X  2 for all data sets.
 classes boundary formed by these support vectors do no change signi fi cantly for different values of the data dispersion parameter.
Fig. 15 shows the resulting prototypes and support vectors corre-sponding to the Chessboard-like and simple non-convex data-sets 4.2. Application of the PFP algorithm to real data-sets.
Tables 3 and 4 summarized the highest accuracy values obtained uction algorithms (ENN, DROP3, ICF, HMNC, HMNE, HMNEI) and the in Table 5 .

Tables 3 and 4 show that the mean accuracy of the Logistic regression (multiquadratic) and the Perceptron Neural Network are lower than the mean accuracy of the PFP and Table 5 reveals that that the mean accuracy of k-NN and Support Vector Machine (ANOVA) algorithms seems to be higher that the accuracy of the hypothesis could not be rejected).

Table 6 shows a comparison of the condensing rate between the proposed algorithm and others condensing algorithms for real datasets with the largest number of instances.

It can be deduced from Table 6 that, in general, PFP gives the
Tables 3 and 4 than the other analyzed condensing algorithms. 5. Conclusions
This work presents a new method for condensing data. The method is based on the selection of a set of prototypes and the mine the boundary between classes. The outcomes reveal that the proposed method presents high-average classi fi cation rates, low shown in Tables 3 and 4 reveal that the k-NN and some SVMs algorithms present higher accuracy than the PFP. However, the use of these algorithms in devices with low memory and processing capabilities could be unpractical. In particular, the k-NN needs high memory requirements for classi fi cation ( Yingquan Wu and complexity and extensive memory requirements of the quadratic memory and processing capabilities.

Additionally, although it has not been directly treated in this work, it can be easily deduced from the description of the PFP algorithm that the:  X 
PFP is intrinsically a multiclass method unlike SVMs.  X 
PFP could be easily parallelized for fasted training and testing.  X 
PFP is easy to implement.  X 
PFP do not require transforming the data in the feature space to obtain the hyper-surface dividing the classes
Furthermore, the propose method could be improved by:  X 
Using another method than the force brute method for the prototypes' selection.  X 
Improving the boundary hyper-surface by performing post-training.

The presented method shows some advantages for data classi fi ca-tion but mostly for data condensing that indicates its potential fi nancially supported by Instituto Polit X cnico
