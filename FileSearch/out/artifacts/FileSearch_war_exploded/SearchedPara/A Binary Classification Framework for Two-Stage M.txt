 Abhishek Kumar abhishek@cs.umd.edu Alexandru Niculescu-Mizil alex@nec-labs.com Koray Kavukcoglu koray@nec-labs.com NEC Laboratories America, Princeton, NJ 08536, USA Hal Daum  X e hal@umiacs.umd.edu Kernel methods such as support vector machines (SVM) (Cortes &amp; Vapnik, 1995), kernel ridge regres-sion, or kernel PCA (Smola &amp; Muller, 1999), use a pos-itive semi-definite (PSD) kernel to implicitly map the instances from the original instance space to a feature space where the standard linear algorithm is applied. The main drawback of kernel methods is that they re-quire the user to specify a single suitable kernel in the first place, which is often critical to the method X  X  suc-cess, but is usually a hard task even when the user has a good familiarity with the problem domain. To ease this burden, significant attention has been given the problem of automatically learning the kernel. The majority of the previous work in this area has focused on the Multiple Kernel Learning (MKL) setting, where the user is only tasked with specifying a set of base ker-nels, and the learning algorithm is in charge of finding a combination of these base kernels that is appropriate for the problem at hand.
 There have been two main lines of work in this direc-tion. The first one learns both the the weights of the kernel combination and the parameters of the classifier by solving a single joint optimization problem. This one-stage approach was first proposed by (Lanckriet et al., 2004) and has since received significant atten-tion (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien &amp; Ong, 2007; Cortes et al., 2009; Sindhwani &amp; Lozano, 2011).
 The second line of work in kernel learning follows a two-stage approach: first learn a  X  X ood X  combination of base kernels using the training data, then use the learned kernel with a standard kernel method such as SVM or kernel ridge regression to obtain a clas-sifier/regressor. This approach has been initially pro-posed in (Cristianini et al., 2001) and (Kandola et al., 2002), and recently revisited by (Cortes et al., 2010b). The two-stage leaning approaches so far have been based on the notion of target alignment . Intuitively, target alignment, is a measure of similarity (agree-ment) between a kernel and the target kernel , which is derived from the training labels, and represents the optimal kernel for the training sample.
 In this paper we introduce TS-MKL, a general ap-proach to Two-Stage Multiple Kernel Learning that encompasses the previous work based on target align-ment as special cases. We formulate the kernel learn-ing problem as a standard linear classification problem in a new instance space. In this space, any linear clas-sifier with weights  X  directly corresponds to a linear combination of base kernels with weights  X  . To avoid confusions, we will denote this new instance space as the K-space , and a classifier in the K-space as a K-classifier throughout the paper. Thus the problem of finding a  X  X ood X  kernel combination reduces to find-ing a  X  X ood X  linear classifier in the K-space, a very familiar problem. One big advantage of this approach is that one can easily adapt techniques from binary classification to solve the MKL problem. For instance, one can use familiar and well understood max-margin methods to obtain better performing MKL algorithms, or take advantage of the recent advances in large scale learning to scale up and/or parallelize the MKL im-plementations. For the results presented in this paper we learn K-classifiers (and hence kernels) by training L 2 regularized linear SVMs with positive weights using the stochastic projected sub-gradient descent method from Pegasos (Shalev-Shwartz et al., 2007).
 On the theoretical side, we prove a finite sample gen-eralization bound for the original classification task in terms of the expected hinge loss and the margin of a K-classifier in the K-space. This justifies our approach of training a K-classifier that has low hinge loss and high margin in the K-space in order to learn a good kernel for the original classification problem. To the best of our knowledge, this result represent the first finite sample bound for two-stage kernel learning, im-proving on previous bounds that were only asymptotic. We also give a concentration bound for the expected hinge loss of a K-classifier.
 On the empirical side, we run a comprehensive eval-uation on two object recognition datasets (Caltech 101 and 256), three bioinformatics datasets (Psort+, Psort-, Plant) and four UCI datasets. On all these datasets our method performs better than, or the same as target alignment, showing that choosing a better K-classifier is beneficial. Our method also fares well against one-stage multiple kernel learning approaches significantly outperforming them on Caltech-256 and being essentially tied on the others.
 We consider a classification problem where instances ( x,y ) are drawn from a distribution P over X  X  Y , with Y a finite discrete set of labels. We assume that we have access to p positive semi-definite (PSD) base kernel functions K 1 ,  X  X  X  ,K p with K i : X  X X  X  R . Our goal is to learn a combination of these kernel functions that is itself a positive semi-definite func-tion and is a  X  X ood X  kernel for the classification task at hand. To achieve this, we define a new bi-nary classification problem over a new instance space { ( z xx 0 ,t yy 0 ) | (( x,y ) , ( x 0 ,y 0 ))  X  P  X  P }  X  where z We will call this space the K-space , and call z xx 0 a K-example or K-instance and t yy 0 a K-label . Any function h : R p  X  R in this space induces a similarity function  X  K h between instances in the original space:
K h ( x,x 0 ) = h ( z xx 0 ) = h ( K 1 ( x,x 0 ) ,  X  X  X  ,K If  X  K h is also positive semi-definite, hence a valid kernel, we say that h is a K-classifier . For exam-ple, all linear functions with positive coefficients (i.e. h ( z xx 0 ) =  X   X  z xx 0 with  X   X  0) are K-classifiers with the induced kernels  X  K  X  being linear combinations of the p base kernels. Figure 1 shows a toy example for the case of two base kernels. Each point in the figure is a labeled K-example ( z xx 0 ,t yy 0 ) corresponding to a pair ( x,y ) , ( x 0 ,y 0 ) of original instances. Note that the figure is drawn in K-space, not in input space. For a linear K-classifier h  X  , the value of its induced ker-nel for a parir of original instances,  X  K  X  ( x,x 0 ), is the projection of the corresponding K-example z xx 0 on the vector  X  (represented by the green line). The left and center sub-figures show the cases where  X  is (0 , 1) and (1 , 0) respecively. In both cases the induced kernel combination is suboptimal. The linear combination in the right sub-figure corresponds to  X  = (1 , 1) and is a good combination because the kernel values of pairs of instances in the same class are separated from the kernel values of pairs of instances in different classes. The key insight behind our method is that, if a K-classifier h is a good classifier in the K-space, then positive when x and x 0 belong to the same class and negative otherwise. This makes  X  K h a good kernel for the original classification task. This intuition is made more precise in Section 3 where we provide a general-ization bound that shows that a K-classifier that sep-arates the positive and negative K-examples with high margin will indeed induce a kernel that allows learn-ing a good classifier for the original task. Note that having a good K-classifier is a sufficient condition, not a necessary one. There can very well exist combina-tions of base kernels that do not correspond to a good K-classifier, but are good kernels nevertheless. Unlike one-stage kernel learning approaches, our method will not be able to find such combinations and it might miss on some good kernels. The results in Section 4, however, show that this does not seem to be the case in practice, as we consistently matched or exceeded the performance of one-stage MKL.
 Thus the problem of learning a good kernel can be re-duced to the problem of learning a good K-classifier in the newly defined K-space: given a training sam-ple ( x i ,y i ) n i =1 for the original classification task, con-struct a K-training set ( z ij ,t ij ) 1  X  i  X  j  X  n and learn a K-classifier h from this sample. Any learning algorithm can be used for learning h provided that the induced kernel can be guaranteed to be a valid PSD kernel 1 . In line with the majority of the MKL work, in this pa-per we focus on learning linear K-classifiers, and hence linear combinations of base kernels. The results in Sec-tion 3 suggest that it is desirable to have a maximum margin K-classifier, thus we use L 2 regularized linear SVM to learn the K-classifier, and ensure that the in-duced kernel is PSD by constraining the weights to be positive. One could, however, use a sparsity promoting regularizer (e.g., L 1 penalty) if a sparse combination of kernels is desired.
 The optimization problem for learning the kernel weights  X  is thus given by min where [1  X  s ] + = max { 0 , 1  X  s } is the hinge loss. To optimize this objective we use the stochastic pro-jected sub-gradient descent implemented in Pega-sos (Shalev-Shwartz et al., 2007), with an additional projection to the non-negative constraint set after every gradient step. Using a stochastic optimiza-tion method allows us to scale very well despite the quadratic number of K-examples: computation time is not directly dependent on the number of instances, linear in the number of base kernels, and independent of the number of classes. If needed, memory usage can be reduced through streaming techniques or on the fly construction of the K-examples. 2.1. Connection to Target Alignment Previous two-stage kernel learning approaches (Cris-tianini et al., 2001; Cortes et al., 2010b) learn a non-negative linear combination of base kernels that maximizes the alignment with the target ker-achieved by solving the optimization problem where A is the Gram matrix of kernel A on the training set,  X  A , B  X  = tr ( AB T ) and || A || 2 F = tr ( AA T ). The above optimization problem can be re-written in our terminology of K-examples as follows: max When the base kernels are centered, as proposed in (Cortes et al., 2010b), the denominator represents the overall standard deviation of the projections of the K-examples on the vector  X  . Hence target alignment attempts to find a projection direction  X  that max-imize the difference between the sums of the projec-tions of the positive and negative K-examples, while minimizing the overall variance of the projected K-examples. This is very similar to using Fisher-LDA in the K-space, with non-negativity constraints on  X  . In fact, viewing target alignment from this perspective, makes it clear that it implicitly makes the assump-tion that the data is homoscedastic (the positive and negative K-examples have the same covariance), which might not be appropriate in real applications. 2.2. Connection to Learning with The approach proposed in this paper can also be cast in the framework of learning with hyperkernels (Ong et al., 2005) which provides a general recipe for kernel learning and includes Multiple Kernel Learning as a special case. It introduces the notions of kernel quality functional , a measure of  X  X oodness X  of a kernel that depends on the training data, and Hyper Reproducing Kernel Hilbert Space , an RKHS over kernel functions that defines the class of kernels that can be learned. Once the desired Hyper-RKHS and quality functional are specified, one has to solve a semi-definite program (SDP) to optimize the quality functional regularized by the norm induced by the Hyper-RKHS.
 When using an SVM as the K-classifier, TS-MKL can be put in the learning with hyperkernels framework by defining the Hyper-RKHS to be the set of non-negative linear combinations of base kernels, and the quality functional to be the hinge loss in K-space. Consider-ing this specific setting has significant advantages: it enables the use of simple and well understood binary classification techniques to learn the kernel, it enables a theoretical analysis, and it allows a significantly more scalable implementation. Equally important, all these advantages do not seem to come at the cost of reduced performance, as we are still performing on par with or better than competing MKL techniques. In this section we make the connection between the performance a K-classifier in the K-space and the per-formance on the original problem precise. This jus-tifies the approach taken in this paper not only intu-itively, but also from a theoretical standpoint. Specif-ically, we bound the generalization error of an SVM that uses the kernel induced by a K-classifier in terms of the expected hinge loss and the margin of the K-classifier in the K-space: Theorem 3.1 Let P be a distribution on X  X { X  1 } , z xx 0 and t yy 0 be as in Equation 1, h be a K-classifier, and R be a constant s.t. h ( z xx )  X  R 2  X  x  X  X  . Let be the expected K-space hinge loss relative to margin  X  of the K-classifier h . Then, with probability 1  X   X  , a classifier  X  f with generalization error can be learned efficiently from a training sample of n instances drawn IID from P . The theorem follows from the two lemmas stated be-low. The first lemma shows that a K-classifier that has low expected hinge loss in the K-space will induce a  X  X ood X  kernel. The second lemma shows that a good kernel allows for a classifier with low generalization error to be efficiently learned from a finite training sample. The following definition states formally what we mean by a good kernel (Srebro, 2007). 3 Definition A kernel K is an ( , X  ) good kernel in hinge loss with respect to a distribution P on X X { X  1 } if there exist a classifier w  X  X  K with k w k H K = 1 s.t. where H K is the Hilbert space and  X  (  X  ) is the feature mapping corresponding to K .
 Lemma 3.2 Let P , h , HL h, X  , R be as in Theo-rem 3.1. Then the  X  K h is a ( HL h, X  ,  X  R ) good kernel in hinge loss with respect to P .
 Lemma 3.3 Let K be an ( , X  ) good kernel in hinge loss, with K ( x,x )  X  R 2  X  x  X  X  . Let ( x i ,y i ) n i =1 IID training sample, and  X  f ( x ) =  X  w  X   X  ( x ) with be a kernel classifier that minimizes the average hinge loss relative to  X  on the training sample. Then, with probability at least 1  X   X  , we have: Lemma 3.3 follows directly from Theorem 21 in (Bartlett &amp; Mendelson, 2002).
 Note that, unlike in the one-stage kernel learning case, the generalization bound in Theorem 3.1 is in terms of the expected hinge loss of the K-classifier not the train-ing hinge loss. While we are hopeful a generalization bound for the classification problem in the K-space can be obtained, as of now it remains an open problem. We can, however, prove a concentration bound for the expected hinge loss of a K-classifier. This is the analog of the concentration bounds for target alignment in (Cortes et al., 2010b; Cristianini et al., 2001). 4 Theorem 3.4 Let P , h , HL h, X  , R be as in Theo-rem 3.1. Let ( x i ,y i ) n i =1 be an IID sample distributed according to P . Then the following inequality holds with probability at least 1  X   X  We evaluate the proposed method on two object recog-nition datasets (Caltech-101 and Caltech-256), three bioinformatics datasets (Psort+, Psort-and Plant), and four UCI datasets (Sonar, Pima, Vertebral and Ionosphere). We compare our method with several baselines: best kernel, uniform combination of base kernels (Average), target alignment, and the one-stage MKL algorithms SILP (Sonnenburg et al., 2006), SimpleMKL (Rakotomamonjy et al., 2007), L 2 -Norm MLK (Kloft et al., 2011), and UFO-MKL (Orabona &amp; Jie, 2011). For two-stage methods we use LIB-SVM (Chang &amp; Lin, 2011) to train the data classifier and select the regularization parameter C via 4-fold cross-validation for all datasets except Caltech where it is fixed at 1000. On multi-class problems, we use a one-vs-rest SVM. For one-stage approaches other than UFO-MKL, we selected C as above and use a one-vs-rest scheme for multi-class problems. For UFO-MKL we use the joint multi-class formulation and search over  X  and C using a bi-dimensional grid. Following (Orabona &amp; Jie, 2011), we run the optimization for 20 epochs on UCI datasets, 30 epochs on Caltech-101 and 100 epochs on Caltech-256. All kernels used in the ex-periments are centered and standardized to have zero mean and unit variance in feature space. 4.1. Methodology for TS-MKL To learn kernel combination weights  X  with TS-MKL we optimize the objective in Eq. 2 using Pe-gasos (Shalev-Shwartz et al., 2007) with an additional projection to the non-negative constraint set after each sub-gradient step. We use a batch size of 100 for each sub-gradient computation and run 10 3 sub-gradient steps for UCI datasets and 10 5 for all others. Fig-ure 2, plots the test data accuracy versus the number of gradient iterations on Caltech-101, showing that af-ter 10 5 iterations the change in accuracy is minimal. For the bigger Caltech-256 there is also essentially no change after 10 5 iterations. We use subsampling to balance the positive and negative K-examples. To select the parameter  X  , we use a single 80%-20% random split of the Pegasos training set and search for the  X  with the lowest validation hinge loss 5 . The search grid for  X  is taken to be in the range of 100 to 10  X  8 dividing in each step by 4. A big advantage of this selection scheme for  X  is that it is completely in-dependent from the data classifier that will ultimately use the learned kernel. This keeps the setup simple and avoids intricate multi-level multi-dimensional val-idation schemes across the parameters of the data clas-sifier and the K-classifier. Fig. 2, shows the hinge-loss in K-space, the accuracy of the K-classifier, and the accuracy of the data classifier that uses the learned kernel, as a function of  X  . The plot shows a clear correlation between hinge loss in K-space and data ac-curacy with the learned kernel. The data accuracy in-creases when the hinge loss in K-space decreases and vice versa. This experiment provides further empri-cal evidence for our theoretical results that show that a good K-classifier (having low hinge loss in K-space) corresponds to a good learned kernel.
 After  X  is selected, Pegasos is retrained on the full training set of K-examples. The obtained weight vec-tor  X  is then used to linearly combine the base ker-nels, and the SVM data classifier is trained using this learned kernel with C selected as described above. 4.2. Caltech-101 and Caltech-256 Both these datasets contain pictures of objects and the task is to recognize the object category. Caltech-101 has 102 classes and Caltech-256 has 256 classes. Caltech-101 is perceived as an easier dataset than Caltech-256 in which images are not left-right aligned and there are more categories. We follow the exper-imental setup used in (Gehler &amp; Nowozin, 2009) and use the same 39 base kernels and train test splits. We report results using all 102 classes for Caltech-101 averaged over five splits. For Caltech-256, the results are for 256 classes (excluding the clutter cat-egory), for a single split. The performance measure used is mean prediction rate per class. The num-ber of training images per class is varied in the range 5 , 10 , 15 , 20 , 25 , 30 for Caltech-101, and in the range 5 , 10 , 15 , 20 , 25 , 30 , 40 , 50 for Caltech-256. The num-ber of test images used is up to 50 images per class for Caltech-101 and 25 images per class for Caltech-256. The regularization parameter for the data SVM, C, is fixed to 1000 for all methods 6 .
 The results for Caltech-101 and Caltech-256 are shown in Fig. 3. 7 On Caltech-101 our approach yields a mean accuracy of 0.512, 0.630, 0.691, 0.725, 0.752, 0.772 for 5, 10, 15, 20, 25, 30 samples per class respectively. Comparing to UFO-MKL, our performance is higher for 5 samples per class, and very similar for all other sample sizes. One-stage MKL methods using the one-vs-all multi-class scheme perform significantly worse and do not even outperform the average kernel un-til the training set has 25 samples per class. This is probably because data is too scarce to allow learning a separate kernel for each class. Target alignment per-forms a little better than the average kernel, but is still significantly worse than TS-MKL. We also show the performance of LP- X  (Gehler &amp; Nowozin, 2009), which, to the best of our knowledge, is the state of the art method on this data set 8 . The performance of TS-MKL and UFO-MKL is almost on par with LP- X  , especially for larger sample sizes. While LP- X  is simi-lar in spirit to multiple-kernel learning, it is not a true kernel learning algorithm as it does not produce a ker-nel, but rather learns an ensemble of SVM classifiers, each of which is trained on an individual kernel. On Caltech-256 dataset, our approach performs bet-ter than all competing kernel learning baselines. We achieve 0.245, 0.320, 0.370, 0.426, 0.448, 0.475, 0.494 mean accuracy for 5, 10, 15, 20, 25, 30, 40, 50 training samples per class. This performance is significantly higher than the best results reported in the litera-ture for 5, 10, and 15 training samples, after which we again perform on par with LP- X  . On this dataset, UFO-MKL performance 9 is similar to that of the aver-age kernel, while the rest of the one-stage MKL tech-niques perform worse. Exact target alignment is worst among all other approaches, however approximate tar-get alignment is able to at least match the performance of the average kernel. 4.3. Bioinformatics datasets We evaluate our method on a problem relevant to cell-biology predicting: the subcellular localization of proteins, which is crucial in making inference about protein function and protein interactions. We follow the experimental setup of (Zien &amp; Ong, 2007) and use the same 69 kernels. The kernels used are: 2 kernels on phylogenetic trees, 3 kernels from BLAST E-values and 64 sequence motif kernels.
 We experiment with three datasets. The first two datasets are for the problem of bacterial protein lo-cations (Gardy et al., 2004). The Psort+ dataset has 541 data points with 4 classes and Psort X  dataset has 1444 data points with 5 classes. We report average F1 score over all classes over 10 random splits for both these datasets as done in (Zien &amp; Ong, 2007). The third dataset used is the original plant dataset of Tar-getP (Emanuelsson et al., 2000), and has 940 exam-ples with 4 classes. We use the performance measure of Matthew X  X  Correlation Coefficient (MCC) following the evaluation in (Zien &amp; Ong, 2007). Again, average MCC score over all 4 classes is reported.
 The results are shown in Table 1. The papers that have used the Psort datasets in the past (Gardy et al., 2004; Zien &amp; Ong, 2007), reported results after filtering out the most unsure predictions in the test set. For Psort+ and Psort-, about 15% and 13 . 3% of the test examples were filtered out respectively and the performance is reported only for the remaining predictions. We follow the same procedure to be able to compare with these methods. We also report performance for full test set. On these datasets, all the kernel learning methods have similar performance, and are better than the best ker-nel and average kernel baselines. Multi-class multiple kernel learning (MC-MKL) of (Zien &amp; Ong, 2007) is also close to our method and other baselines. 4.4. UCI datasets We use four UCI datasets: Sonar, Ionosphere, Pima and Vertebral (the three class version). For each of these datasets, we perform two types of MKL exper-iments. In first setting, we construct a total of 13 kernels on the full feature vectors: 9 Gaussian kernels ( e nomial kernels of degree 2,3 and 4, and a linear kernel. In the second setting, we augment these 13 kernels with another set of Gaussian, polynomial and linear kernels constructed on individual features of the data. The range of parameter  X  for Gaussian and degree pa-rameter for polynomial kernel is kept same as before. If the data has d features, we have total 13 d + 13 ker-nels in the second setting. We report average accuracy accuracy over 10 random 80%  X  20% train-test splits. The results are shown in Table 2. On all these datasets, no kernel learning approach seems to im-prove performance over the straightforward baselines of best kernel and average kernel. Although further study is needed to reach a definite conclusion, these results seem to indicate that blindly using a kitchen sink of standard kernels is not beneficial if the goal is to combine these kernels using an MKL approach. This highlights the importance of evaluating MKL tech-niques using datasets like Caltech and PSORT, where the kernels have been carefully designed using domain knowledge to capture different, potentially useful, no-tions of similarity in the data. 4.5. Computational Efficiency Since the number of K-examples is quadratic in the number of training instances, one might worry about the scalability of the TS-MKL method. In this sec-tion we compare the running time of TS-MKL with Target Alignment, and UFO-MKL (Ultra-Fast Opti-mization MKL) which, to the best of our knowledge, is the fastest one-stage MKL technique to date. Table 3 shows the running times for the Sonar, Pima and Caltech 101 datasets. The running time is for a single run using the best setting of parameters (i.e. it does not include the time for parameter selection). For TS-MKL and Target Alingment we also show in paranthesis the time taken by the kernel learning stage alone, without the final data SVM, on Caltech-101. For Sonar, which has only 166 training samples, the running time of UFO-MKL and TS-MKL is compara-ble. However, on Pima, which has 614 samples, and on Caltech, which has 3060 samples and 102 classes, TS-MKL is more than an order of magnitude faster than UFO-MKL. This shows that, by taking advan-tage of the advances in large scale stochastic optimiza-tion, TS-MKL is not only able to gracefully handle the quadratic increase in the number of K-examples, but it is actually the fastest MKL method to date. Framing kernel learning as a standard classification problem in a properly defined instance space allows us to easilly adapt well understood classification tech-niques to obtain a scalable and high performing two-stage multiple kernel learning algorithm. Our ap-proach is backed up by formal theoretical guarantees, and by empirical evaluation that shows it always out-performs or is on par with leading one-stage and two-stage kernel learning methods. This is a remarkable feat for a method that is quite simple and intuitive. This new perspective on multiple kernel learning opens the door to a number of interesting questions to be addressed in subsequent research. Examples are: ex-ploring the use of non-linear K-classifiers in conjunc-tion with the learning with similarity functions frame-work; improving performance in scarce data condi-tions through semi-supervised and multi-task mul-tiple kernel learning by using such techniques to learn the K-classifier; or applying TS-MKL to semi-supervised clustering and dimensionality reduction problems where the supervised signal is usually given in terms of pairwise must-link and can-not-link con-straints rather than labels.

