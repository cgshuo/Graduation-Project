 1. Introduction
Explicit and often manually curated knowledge is frequently being added to documents for a variety of reasons, e.g., to a meta-language and can be either formal (e.g., in the form of a thesaurus or ontology) or more informal (e.g., in the form of user-generated tags). Annotations of this kind may be found in a broad range of domains and a variety of document types.
News articles, for example, can be annotated with concepts from the NewsCodes taxonomies provided by the IPTC. Another example is the annotation of bibliographic records with indexing terms from a controlled vocabulary. In the biomedical do-main citations in the MEDLINE database are manually indexed with concepts from the Medical Subject Headings (MeSH) thesaurus. We will refer to this broad range of meta-languages as concept languages and to their vocabulary terms as con-cepts . Tables 1 and 2 show two examples of document X  X oncept annotations from the two test collections we describe later.
The introduction of concept languages was initially driven by a need to facilitate search and navigation of the collection (Roberts, 1984; Joyce &amp; Needham, 1958 ). Concepts were defined to unambiguously and precisely represent the content of documents. Today, most of these early retrieval systems have been replaced by full-text search systems which have been terms (which typically comprise of all the terms used in the documents in a given collection) can be more ambiguous or less expressive than concepts. Not surprisingly then, information retrieval (IR) researchers continue to study ways of incorporat-ing information from concept languages to address problems in textual query representations. For example, a textual query may be mapped to one or more concepts in a thesaurus and expanded with their synonymous terms ( Voorhees, 1994 ). Re-sults of such approaches, however, have been mixed at best.

In this paper we show that a concept language can be effectively used to improve full-text retrieval. In a two step process that extends on relevance feedback and uses a conceptual representation as a pivot language we improve the query model representing the information need of the user.

In the first step, the textual information need is translated into a conceptual representation. In a process we call concep-tual query modeling , feedback documents from an initial retrieval run are used for obtaining a conceptual query model. This model represents the user X  X  information need at a different, higher conceptual level than the original query. The intuition behind this step is that this conceptual representation gives an unambiguous representation of the information need. In con-trast to traditional textual relevance feedback, where the query refinement is biased towards terms occurring in the initial query, this intermediate conceptual representation is less dependent on the original query words. On its own, this explicit however, we translate the conceptual query model back into a contribution to the textual query model. We hypothesize that, since the textual representation of documents is more detailed than its conceptual representation, with a textual query representation translated from a conceptual form, results in better retrieval performance than strictly matching with only concepts. Essential to these two translation steps is the estimation of a query model, both for terms and for concepts. The textual query should be captured by a small set of specific concepts and the conceptual query model should be translated to specific textual terms. To achieve this, we employ an expectation maximization algorithm inspired by par-simonious language models ( Hiemstra, Robertson, &amp; Zaragoza, 2004 ).

The paper is organized around a number of research questions that aim to investigate the effectiveness of our proposed conceptual language models and place it in the context of state-of-the-art full-text retrieval systems. These questions are defined as follows: 1. To estimate a conceptual query model we propose a method that looks at the top-ranked documents in an initially retrieved set (Section 4.1). In order to assess the effectiveness of this step, we compare the results of using these concepts with a standard language modeling approach. Moreover, since this method relies on pseudo-relevant documents from an initial retrieval run, we also compare the results of our conceptual query models to another, established pseudo-relevance feedback algorithm based on relevance models. We ask: What is the relative retrieval effectiveness of this method with respect to the standard language modeling and conventional pseudo-relevance feedback approach? 2. For the estimation of both the conceptual query model and generative concept model we apply an iterative EM algorithm which emphasizes more informative terms. We ask: What is the impact of applying this algorithm compared to conven-tional estimates in terms of retrieval effectiveness? 3. The proposed method based on conceptual language models is dependent on a number of parameters. We ask: What is the sensitivity of the method to its parameter settings? How robust are the results across different collections and test sets? 4. By definition, curated knowledge is domain specific. So we ask the question: How portable is our conceptual language model? What are the results of the model across multiple test collections? Can we say anything about which evaluation measures are helped most using our model? Is it mainly a recall or precision-enhancing device? We make the following contributions in this article:
We propose a method for determining the concepts that are most likely to be associated with a given query, which allows effective conceptual (blind) relevance feedback. Moreover, this explicit conceptual query representation may be used as a means of suggesting query-related concepts to the user.

We propose generative concept models, that are used to generate terms for concepts related to the query. Besides this particular application, they may also be employed to determine semantic relatedness.
 Finally, we provide an empirical comparison of our proposed method to existing relevance feedback models.
The remainder of this paper is organized as follows: We discuss related work in Section 2. We then describe our retrieval framework and our conceptual language models are introduced next. We describe our experimental setup in Section 5 and report on the outcomes of our experimental evaluation and discuss our findings in Section 6. We end with a concluding section. 2. Related work
Work related to our proposed conceptual language models may be found in overlapping areas, viz. query expansion, con-ceptual retrieval, and cluster-based retrieval. These will be discussed in this section.

Query expansion aims at bridging the vocabulary gap between queries and documents by adding and reweighing terms in the original query ( Voorhees, 1994 ). Query expansion approaches can be local or global (Xu &amp; Croft, 1996 ). Local query expansion methods try to take into account the context of a query; one might, for example, consider a user X  X  history or pro-file, in order to automatically enrich queries (Korfhage, 1984 ). Much later, similar notions were adopted in a language mod-documents to locate additional query terms.

Relevance feedback is a form of local query expansion that relies on the analysis of documents from an initial retrieval run. The retrieved documents serve as examples to select additional query terms (Rocchio, 1971 ). Pseudo-relevance feedback methods assume the top-ranked documents to be relevant, but explicit or implicit relevance judgements from users may also
The recent interest of the semantic web community regarding models and methods related to ontologies have also sparked a renewed interest in using ontological information for relevance feedback (Bhogal, Macfarlane, &amp; Smith, 2007; Rocha, Schw-abe, &amp; Aragao, 2004). In a language modeling setting, local query expansion has been applied to estimate query language of looking at the terms in these documents, however, we consider the concepts associated with the documents.
Global query expansion uses global collection statistics or  X  X  X xternal X  knowledge sources such as concept languages to en-hance the query. For example, concepts and lexical-syntactic relations as defined in a thesaurus have been used for query 2007; Roberts et al., 1984; Voorhees, 1994).

Our method can be viewed as a combination of a local and global expansion method; a local expansion method is used to obtain a conceptual representation of a query, whereas a global method is used to translate the conceptual representation to a textual query contribution.

Using a conceptual representation obtained from pseudo-relevance feedback has been investigated by different research-ers in the biomedical domain. Srinivasan (1996) proposes adding concepts directly to an initial query and reports the largest improvement in retrieval effectiveness when another round of blind relevance feedback on vocabulary terms is applied after-wards. This method is similar to ours, although there are distinct differences in her approach and evaluation. For one,
Srinivasan (1996) creates a separate  X  X  X oncept index X  in which tokenized concept labels are used as terms. In this way, searching using a concept labeled  X  X  X tomach cancer X  also matches the related, but clearly different concept  X  X  X reast cancer X  because they share the word  X  X  X ancer X . In our opinion, this obfuscates the added value of using clearly defined concepts; searching with a textual representation containing the word  X  X  X ancer X  will already result in matching related concepts.
Therefore, we decide to use unique concept identifiers in our conceptual representation. Srinivasan (1996) concludes that concepts are beneficial for retrieval, but remarks that the OHSUMED collection used for evaluation was quite small. Our re-search uses the larger TREC Genomics test collections and, additionally, investigates the use of document level annotations in another domain using the CLEF domain specific test collections. Finally, we remark that our proposed model is an extension of the language modeling retrieval framework, whereas Srinivasan (1996) extends a vector space retrieval model. Camous,
Blott, and Smeaton (2006) also use the annotations of the top-5 retrieved documents to obtain a conceptual query represen-tation, but incorporate them in a different fashion. The authors use them to create a new ranked list of documents, which is subsequently combined with the initially retrieved documents. In contrast, we explicitly update the original query model.
All of the methods based on concept languages need a way of mapping between the concepts and their textual represen-tation. Where the described approaches look for exact occurrences of the concepts in the text, we use the vocabulary terms associated with concepts to make this connection, as detailed in Section 4.

Taking a step back from query expansion, many different ways of directly improving text-based retrieval by incorporating concepts or a concept language have been proposed. For example, the entries from a concept language may be used to define the indexing terms employed by the retrieval system. In the absence of a concept language, similar information might be rence analysis of the entire collection might be applied to estimate dependencies between vocabulary terms (Bai et al., 2005; Chung, 2004). Alternatively, term dependencies may be determined on a query-dependent subset of the collection, dependencies may then be employed to locate terms related to the initial query.

One of the first attempts at automatically relating concepts with text was introduced in the 1980s. Giger (1988) incorpo-rated a mapping between concepts from a thesaurus and words as they appear in the collection. The main motivation was to move beyond text-based retrieval and bridge the semantic gap between the user and the information retrieval system, a motivation closely related to ours. His algorithm first defines atomic concepts , which are string-based concept to term map-pings. Then, documents are placed in disjoint groups based on so-called elementary logical conjuncts, which are defined through the atomic concepts. At retrieval time, the query is parsed and the sets of documents with the lowest distance to the requested concepts are returned. His ideas relate to recent work done by Zhou, Hu, Zhang, Lin, and Song (2006) and Zhou,
Hu, and Zhang (2007), who use so-called topic signatures to index and retrieve documents. These signatures are comprised of recognizing named entities within each document and query; when named entities are not available, term pairs are used.
Their named entity recognition step is automated and might not be completely accurate; we suspect that the errors in this concept detection process do not strongly affect retrieval performance because pairs of concepts (topic signatures) are used for retrieval. In our method, we rely on manually curated concept annotations, making the topic signatures superfluous.
Trieschnigg, Kraaij, and Schuemie (2007) also use named entity recognition to obtain a conceptual representation of que-ries and documents. They conclude that searching only with an automatically obtained conceptual representation seriously degrades retrieval when searching for short documents (citations). Interestingly, the same approach performs on par with text-only search when larger documents (full-text articles) are retrieved.

Instead of using named entity recognition, Gabrilovich and Markovitch (2007) employ document-level annotations, in the form of Wikipedia categories. They represent the categories as term vectors, where the individual term weights are deter-mined using TF.IDF scores from the documents that are labeled with the concept at hand. In this way, the strength between vocabulary terms and concepts can be quantified, which can subsequently be used to generate vectors of concepts for a piece of text X  X ither a document or query. This approach is similar to the topic modeling approach described by Wei (2007) , which uses Open Directory Project (ODP) concepts in conjunction with generative language models. Instead of using concept X  X oc-ument associations, however, she uses an ad hoc approach based on the descriptions of the concepts in the concept language (in this case, ODP categories). Our conceptual language models are related to these approaches in that they also bridge be-tween concepts and terms. We, however, use an iterative EM algorithm in tandem with a statistical translation model to establish the association between terms and concepts. Interestingly, all of these approaches open up the door to providing conceptual relevance feedback to users. Instead of suggesting vocabulary terms that are related to the query, we can now their system keeps track of a user X  X  history by classifying visited web pages onto the concepts from the ODP. Further examples of mapping queries to conceptual representations can be found in the area of web query classification.
Broder et al. (2007) use a pseudo-relevance feedback technique to classify rare queries into a commercial taxonomy of web queries, with the goal to improve web advertisements. A classifier is used to classify the highest ranked results, and these classifications are subsequently used to classify the query by means of voting. We use a similar method to obtain the con-ceptual representation of our query described in Section 4.1, with the important difference that all our documents have been manually classified. Mishne et al. (2006) classify queries into taxonomies using category-based web services. Shen, Sun,
Yang, and Chen (2006) improve web query classification by mapping the query to concepts in an intermediate taxonomy which in turn are linked to concepts in the target taxonomy. In our work, we use a single concept taxonomy which is used as a pivot language to improve the textual query model. Chen, Xue, and Yu (2008) use a taxonomy to suggest keywords. After mapping the seed keywords to a concept hierarchy, content phrases related to the found concepts are suggested. In our ap-proach the concepts are used to update the query model, i.e., to update the probabilities of terms based on the found con-cepts rather than the addition of related discrete terms or phrases.

Concepts can be recognized at different levels of granularity, either at the term level, by recognizing concepts in the text, or at the document level, by using document-level annotations or categories. While the former can be described as a form of
Work done on cluster-based retrieval can be viewed as a variation on the same theme; in our case the clusters are defined by the concepts that are associated with the documents in the collection. Kurland et al. (2004) , for example, determine over-lapping clusters of documents in a collection, which are considered facets of the collection. They use a language modeling framework in which their aspect-x algorithm smoothes documents based on the information from the clusters and the strength of the connection between each document and cluster. Liu and Croft (2004) evaluate both the direct retrieval of clusters and cluster-based smoothing. Their CBDM model is a mixture between a document model, a collection model, and the cluster each document belongs to, which is able to significantly outperform a standard query-likelihood baseline.
Instead of smoothing documents, Minker, Wilson, and Zimmerman (1972) use cluster-based information for query expan-sion. The authors evaluate their algorithm on several small test collections, without achieving any improvements over the unexpanded queries. More recently, Lee, Croft, and Allan (2008) have shown that detecting clusters in a set of (pseudo-)rel-evant documents is helpful for identifying dominant documents for a query and, thus, for subsequent query expansion, a finding which was corroborated on different test collections by Kurland (2008) . These approaches all exploit the notion that  X  X  X ssociations between documents convey information about the relevance of documents to requests X  ( Jardine &amp; van Rijsber-documents labeled with this concept have a higher prior probability of being relevant to the query. This is the main moti-vating idea for our current work. 3. The KL-divergence retrieval framework
The success of generative language models in statistical machine translation and automatic speech recognition inspired several IR researchers to re-cast IR in a generative probabilistic framework, by representing documents as generative prob-abilistic models. Such models can be used to compute the probability of observing a sequence of terms, by computing the product of the probabilities of observing the individual terms. The first published application of generative models for IR was based on the multiple Bernoulli distribution (Ponte &amp; Croft, 1998 ), but the simpler multinomial unigram model became the mainstream model ( Hiemstra, 1998; Miller, Leek, &amp; Schwartz, 2000 ). Recent work has addressed some of the shortcom-ings of the multinomial model for modeling text and considers the Dirichlet compound multinomial distribution instead ( Xu improvements over the standard multinomial model. Whether it is a better candidate for representing text in our current context remains a subject for future work.
 In the multinomial unigram model, each document D is represented as a multinomial probability distribution P  X  t j h all the terms t in the vocabulary. At retrieval time, each document is ranked according to the likelihood of having generated the query, i.e., the probability that the query terms  X  t 2 Q  X  are sampled independently and identically from the document language model (Hiemstra, 1998 ): need can also be modeled by a language model. In this way, a more general and flexible retrieval model can be obtained by using a comparison of two language models as the basis for ranking. Several authors proposed the use of the Kullback X  X ei-bler (KL)-divergence for ranking, since it is a well established measure for the comparison of probability distributions with some intuitive properties X  X t always has a non-negative value and equal distributions receive a zero divergence value (Laff-erty et al., 2001; Ng, 2001; Xu &amp; Croft, 1999). Using KL-divergence, documents are scored by measuring the divergence be-tween a query model h Q and each document model h D . Since we want to assign a high score for high similarity and a low score for low similarity, the KL-divergence is negated for ranking purposes. More formally, the score for each query X  X ocu-ment pair using the KL-divergence retrieval model is: where V denotes the set of all terms used in all documents in the collection. KL-divergence is also known as the relative entropy, which is defined as the cross-entropy of the observed distribution (in this case the query) as if it was generated by a reference distribution (in this case the document) minus the entropy of the observed distribution. KL-divergence can also be measured in the reverse direction (also known as document likelihood), but this leads to poorer results for ad-hoc search tasks ( Lavrenko, 2004 ). The entropy of the query, ignored for ranking purposes. In fact, one could argue that ranking on just the cross-entropy term provides a more concise ranking formula and is a suitable distance measure for comparing probability distributions in its own right ( Kraaij, 2004 ). When the query model is generated using the empirical, maximum-likelihood estimate (MLE) on the original query, i.e., where j Q j indicates the length of the query, it can be shown that documents are ranked in the same order as using the query query model is therefore an estimate of the model for the underlying information need, sometimes called a relevance model relevance feedback techniques as described in Section 2. Next, we describe our baseline query modeling (Section 3.1) and document modeling (Section 3.2) approaches. In Section 4 we define our conceptual language models on top of these base-line approaches. 3.1. Query models
Relevance models (Lavrenko &amp; Croft, 2001 ) are one of the baselines we employ. Here, it is assumed that for every infor-mation need there exists an underlying relevance model and that the query and relevant documents are random samples from this model. The query model, parametrized by h Q , may be viewed as an approximation of this model. However, in a typical retrieval setting improving the estimation of h Q
The authors present two methods for estimating relevance models without training data by constructing models from the queries and a set of pseudo-relevant documents, using different independence assumptions. They determine the probability of observing t after having observed Q as: where q 1 ; ... ; q k are the individual query terms. Under their method 2, the query terms are independent of each other, but keep their dependence on t : where D Q is a set of pseudo-relevant documents and
Then, in order to obtain a query model that is a better estimate of the information need, the initial query P  X  t j tial sample:
In the next section, we will describe how we extend this work by leveraging conceptual knowledge in the form of document annotations to improve the estimation of P  X  t j ^ h Q  X  . We discuss the issue of setting the smoothing parameter k 3.2. Document models
It is an essential condition for retrieval models that are based on measuring the probability or cross-entropy of observed data given a reference generative model, that the reference model is adequately smoothed. Smoothing is applied both to avoid zero-frequency problems occurring with a MLE approach and to account for general and document-specific language use. We adopt Jelinek X  X ercer smoothing by considering each document to be a mixture of a document-specific model and a more general background model. Thus, each document model is estimated as the MLE of each term in the document P  X  t j D  X  , sufficiently large corpus, such as the document collection (Jelinek &amp; Mercer, 1980; Zhai &amp; Lafferty, 2004):
We address the parameter setting procedure for k D in Section 5.5. Now that we have described the main components of our framework, we will zoom in on our proposed methods.
 4. Conceptual language models
Our goal is to utilize the knowledge that is encapsulated in a concept language to enhance the estimation of the query the method proposed by Berger and Lafferty (1999) . Specifically, we utilize the concepts that are associated with a query to find terms related to these concepts in order to estimate the expanded part of the query model, P  X  t j graphical representation of the dependencies of this process.

Put differently, first we translate the query into a set of relevant concepts. Next, the vocabulary terms associated with the concepts are considered as possible terms to include in the query model. More formally, for a query Q and concepts c 2 C : where we assume that the probability of selecting a term is only dependent on the concept once we have selected that con-cept for the query.
 refer as a conceptual query model . As to the former, we will need to associate terms with concepts in the concept language.
While the concepts may be directly usable for retrieving documents (Hersh, Hickam, Haynes, &amp; McKibbon, 1994; Srinivasan, 1996; Trieschnigg et al., 2009), we associate each concept with a weighed set of most characteristic terms using a multino-mial unigram model. To this end we consider the documents that are annotated using c as bridges between the concept and further in Section 4.2 below.

The second component X  X he conceptual query model P  X  c j Q  X   X  X s a distribution over concepts specific to the query. In some settings, concepts are provided with a query or as part of a query, see, e.g., the PubMed search interface (Herskovic, Tanaka, leverage the document annotations to approximate this step: this is what we do in the next section. 4.1. Conceptual query modeling
We now turn to defining P  X  c j Q  X  , the conceptual query model. Contrary to the alternatives mentioned at the end of the previous section, concepts are not provided with a query in a typical IR setting and need to be inferred, estimated, or recog-language modeling manner, by determining which concepts are most likely given documents relevant to the query. Alter-natively, we could involve the end user and ask which documents, associated concepts, or terms are relevant. Since we do not have access to such assessments, however, we resort to using pseudo-relevance methods. In recent work we studied different approaches of estimating a conceptual query model and concluded that using feedback documents is far more effective than using, e.g., string matching methods that try to recognize concepts in the query ( Trieschnigg et al., 2009 ).
Like Lavrenko and Croft (2001) , we view the process of obtaining a conceptual query model as a sampling process from a number of representative sources. The user has a notion of documents satisfying her information need, randomly selects one of these, and samples a concept from its representation. Hence, the conceptual query model is defined as follows: cept language model of the document, the estimation of which is discussed in the next paragraph. Note that we assume that the probability of observing a concept is independent of the query once we have selected a document given the query, i.e., P  X  c j D ; Q  X  X  P  X  c j D  X  . The term P  X  D j Q  X  denotes the probability that document D is chosen from D using the retrieval scores.
 We assume that pseudo-relevant documents are a good source from which we can sample the conceptual query model. Indeed, manual inspection shows that they are annotated with many relevant concepts, but also that they contain a lot of noise: some concepts are very frequent for all documents and, despite being related to the query, not very informative. Sam-pling from the maximum likelihood estimate for these documents would thus result in very general conceptual query models. Therefore, to re-estimate the probability mass of the concepts in the sampling process, we use a parsimonious lan-guage model. Table 3 illustrates the difference between a maximum likelihood estimation and a parsimonious estimation. It shows the concepts (in this case MeSH terms) with the highest probability for topic 186 from the TREC Genomics 2006 test collection. The conceptual query model based on the parsimonious document models contains more specific X  X nd thus more useful X  X oncepts, such as  X  X  X resenilin-1 X  and  X  X  X resenilin-2 X . The model based on maximum likelihood estimates includes more general concepts such as  X  X  X umans X , which are relevant but too general to be useful for searching. In the next section we detail how re-estimation is performed. 4.2. Generative concept models language used for annotating the documents. We determine the level of association between a term and a concept by looking at the way trained annotators have labeled the documents. In the end, this method defines the parameters of a generative language model for each concept: a generative concept model . We determine the strength of association between a concept c and a term t by determining the probability of observing t given c :
Concepts that are used to annotate documents may have different characteristics from other parts of a document, such as title and content. Annotations are selected by human indexers from a concept language while the remaining content consists of free text. Since the terms that make up the document are  X  X  X enerated X  using a different process than the concepts, we may assume that t and c are independent and identical samples given a document D in (or with) which they occur. So, the prob-ability of observing both t and c is where D C denotes the set of documents annotated with concept c . When we assume each document in this set to have a uniform prior probability of being selected, we obtain
Hence, it remains to define three terms: P  X  c  X  ; P  X  t j D  X  , and P  X  c j D  X  . First, the term P  X  c  X  occurring and thus relatively non-informative concepts. We estimate this term using MLE on the document collection: where n  X  c ; D  X  is the number of times document D is labeled with concept c .
 quently occurring concepts. Moreover, as exemplified above, not all of the observed events (where events are either terms or concepts) are equally informative. Some may be common, whilst others may describe the general domain of the document.
Earlier, we have assumed that each document is a mixture of document-specific and more general terms (Section 3.2, Eq. (8)); we now generalize this statement to also include concepts. Further, given this assumption, we may update each doc-ument model by reducing the amount and probability mass of non-specific events. We do so by iteratively adjusting the indi-vidual probabilities in each document, based on a comparison with a large reference corpus such as the collection. More formally, we maximize the posterior probability of D after observing x : Note that k x may be set differently for D (Eq. (8)) and C . For these estimations, we fix k nificantly anymore:
This updating mechanism enables more specific events, i.e., events that are not well-explained by the background model, to receive more probability mass, making the resulting document model more specific. After the EM algorithm has converged, we remove those events with a probability lower than a certain threshold d . Thus, the resulting document model for terms,
P  X  t j ^ h D  X  , to be used in Eq. (13) is given by: where Z D t is a document-specific normalization factor: Z applying this algorithm on a document from the CLEF document collection (that will be introduced in Section 5). Similarly, the resulting document model for concepts, P  X  c j ^ h D  X  , to be used for P  X  c j D  X  in Eq. (13), is given by: where Z D c is a document-specific normalization factor: Z applying this algorithm on a topic from the TREC document collection (that will be introduced in Section 5). For the exper-iments in this paper we fix d t  X  d c  X  0 : 01. 5. Experimental setup
To answer the research questions specified in the introduction, we set up a number of experiments in which we compare our conceptual language models with other retrieval approaches. Below, we first describe our test collections, the baseline approaches that we use for comparison, our experimental environment, estimation methods, and the method we use for sig-nificance testing. In Section 6, we turn to the results of our experiments. 5.1. Test collections
The test collections we employ were selected for several reasons. First, our retrieval model requires collections in which the documents have been manually annotated with an appropriate concept language. The TREC and CLEF test collections that we describe below both satisfy this requirement. Moreover, they have been used for evaluating well-defined IR tasks and have relevance assessments based on a sufficiently large pool. Tables 5 and 6 list key characteristics of the test collections we use. All documents (in all test collections) are stemmed using a Porter Stemmer and we do not remove stopwords. 5.1.1. CLEF domain specific 2007 X 2008
The CLEF domain-specific track evaluates retrieval on structured scientific documents, using bibliographic databases from the social sciences domain as document collections (Petras, Baerisch, &amp; Stempfhuber, 2007; Petras &amp; Baerisch, 2008). The track emphasizes leveraging the structure of data in collections (defined by concept languages) to improve retrieval performance. The 2007 (CLEF-DS-07) and 2008 (CLEF-DS-08) tracks use the combined German Indexing and
Retrieval Testdatabase (GIRT) and Cambridge Scientific Abstracts (CSA) databases as their document collection. The GIRT database contains extracts from two databases maintained by the German Social Science Information Centre from the years 1990 X 2000. The English GIRT collection is a pseudo-parallel corpus to the German GIRT collection, providing trans-lated versions of the German documents (17% of these documents contain an abstract). For the 2007 domain-specific track, an extract from CSA X  X  Sociological abstracts was added, covering the years 1994, 1995, and 1996. Besides the title and ab-stract, each CSA record also contains subject-describing keywords from the CSA Thesaurus of Sociological Indexing Terms abstract.

We only use the English mono-lingual topics and relevance assessments, which amounts to a total of 50 test topics. The documents in the collection contain three separate fields with concepts,
EN and CONTROLLED-TERM-EN-MINOR ; we only use the CLASSIFICATION-TEXT-EN 5.1.2. TREC Genomics 2004 X 2005
The document collection for the TREC 2004 and 2005 Genomics ad-hoc search task (TREC-GEN-04 and TREC-GEN-05) consists of a subset of the MEDLINE database (Hersh et al., 2005, 2006 ). MEDLINE is the bibliographic database maintained by the US National Library of Medicine (NLM). It currently contains over 16 million biomedical citations from around 5200 journals and several hundred thousand records are added each year. Despite the growing availability of full-text articles on the Web, MEDLINE remains a central access point for biomedical literature. Each Medline record contains free text fields (such as title and abstract), a number of fields containing other metadata (such as publication date and journal), and, most important for our current work, terms from the Medical Subject Headings (MeSH) thesaurus. We only use the main descrip-tors, without qualifiers. MeSH terms are manually assigned to citations by trained annotators from the NLM. The over 20,000 biomedical concepts in the MeSH thesaurus are organized hierarchically. Relationships between concepts in the MeSH the-saurus are primarily of the  X  X  X roader/narrower than X  type. The  X  X  X arrower than X  relationship is close to expressing hypern-ymy (is a), but can also include meronymy (part of) relations. One concept is narrower than another if the documents it is assigned to are contained in the set of documents assigned to the broader term. Each MEDLINE record is annotated with 10 X 12 MeSH terms on average.

It should be noted that the Medical Subject Headings thesaurus is not the most appropriate for Genomics information retrieval, since it covers general biomedical concepts rather than the specific genomics terminology used in the TREC topics tiveness, as we will show later.

The document collection for TREC Genomics 2004 and 2005 contains 10 years of citations covering 1993 X 2004, which amounts to a total of 4,591,008 documents. All documents have a title, 75.8% contain an abstract and 99% are annotated with for 2005 (one of which has no relevant documents) follow pre-defined templates, so-called Generic Topic Types. An example terms. The topics in our experiments are derived from the original topic by only selecting the instantiated terms and discard-ing the remainder of the template. 5.1.3. TREC genomics 2006
The TREC 2006 Genomics track introduced a full-text document collection, replacing the bibliographical abstracts from entific journal papers. The files themselves are provided as HTML, including all the journal-specific formatting. Most of the documents (99%) have a valid Pubmed identifier, through which the accompanying MEDLINE record can be retrieved. We use the MeSH terms assigned to the corresponding citation as the annotations of the full-text document.

The 2006 test topics are again based on topic templates and instantiated with specific genes, diseases or biological pro-cesses. Thus, we preprocess them in a similar fashion as the topics for the TREC Genomics 2005 track, by removing all the template-specific terms. This test collection has 28 topics, of which two do not have any relevant documents in the collec-tion. The task put forward for this test collection is to first identify relevant documents and then extract the most relevant passage(s) from each document; relevance is measured at the document, passage, and aspect level. We do not perform any passage extraction and only use the judgments at the document level. 5.2. Evaluation measures and significance testing
We report on the following evaluation measures, which are obtained with the trec_eval tables. 5.3. Parameter estimation
Given the models introduced in the previous sections, we have a number of parameters to estimate. Table 7 summarizes the parameters that we need to set.

There are various approaches that may be used to estimate these parameters. We choose to optimize the parameter val-ues by determining the mean average precision for each set of parameters and show the results of the best performing set-with increments of 1. We determine the MAP scores on the same topics that we present results for, similar to Liu and Croft (2004), Metzler and Croft (2005), Mitra et al. (1998), Lafferty et al. (2001) and Zhai and Lafferty (2004). While computation-ally expensive (exponential in the number of parameters), it does provide us with an upper bound on the retrieval perfor-mance that one might achieve using the described models. 5.4. Complexity and implementation
For all our experiments we use the Lemur Toolkit. 4 As to the complexity of our methods, we need to calculate two terms conceptual query model (online). The former is most time-consuming, with a maximum complexity per concept proportional to the number of terms in the vocabulary, the number of documents annotated with the concept, and the number of EM iterations.
The advantage of this step, however, is that it can be performed offline. Determining a conceptual query model is, in terms of efficiency, comparable to standard pseudo-relevance feedback approaches except for the addition of the number of EM itera-collections. 5.5. Baselines
We use two baseline retrieval approaches for comparison purposes, viz. query likelihood and relevance models, which are described next. Table 8 shows an example of the generated query models for these baseline approaches and the CLEF 2008 query  X  X  X hrinking cities X . As our first baseline, we employ a run based on the KL-divergence retrieval method and set k (cf. Section 3, Eq. (7)). This uses only the information from the initial, textual query and amounts to performing retrieval using query likelihood.
 It has been shown that making the document interpolation parameter k yields superior performance (Zhai &amp; Lafferty, 2004 ). Thus, for our baseline experiments we set k results in Bayesian smoothing using a Dirichlet prior (Chen &amp; Goodman, 1996 ). All the results on which we report use this baseline as their initially retrieved document set.

Since our concept language models also rely on pseudo-relevance feedback (PRF), we use the text-based PRF method introduced by Lavrenko and Croft (2001) ( X  X  X odel 2 X ) which was described in Section 3, Eq. (5) as another baseline. The func-tional form of our conceptual query model is reminiscent of Lavrenko and Croft X  X  (2001)  X  X  X odel 1 X  and we also evaluated  X  X  X odel 1 X  as a text-based pseudo-relevance feedback baseline. We found that its performance was inferior to  X  X  X odel 2 X  og, 2008 ). Consequently, we use  X  X  X odel 2 X  in our experiments and refrain from mentioning the results of  X  X  X odel 1 X . 6. Results and discussion
Now that we have detailed our conceptual language modeling approach (Section 4) and laid out the experimental envi-ronment (Section 5), we present the results of the experiments aimed at answering the research questions listed in the intro-duction. First, we look at the performance of the query likelihood model, which we use as our baseline. We emphasize that all the other models that we evaluate use the initial ranking from the query likelihood model as a set of pseudo-relevant documents. Whether improving upon this baseline will also improve the estimations based on it is a question for future work. We then look at the results of applying an established pseudo-relevance feedback algorithm based on relevance mod-els. Next, we evaluate the results of using the conceptual language models as described in Section 4, using the conceptual query models and the generative concept models in conjunction.

We then perform an ablation study, by zooming in on the results after removing each component in the conceptual lan-guage models. First, we consider the generative concept models that we use to translate the conceptual query model to free-text terms. We look at the results of using MLE, i.e., without applying the EM algorithm described in Section 4.2. Second, since each document in our collections has associated concepts, we may use the conceptual query model in conjunction with individual parameter settings and zoom out in order to see whether we can relate collection-specific properties with the re-ported results. 6.1. Baselines
Table 9 shows the results of the query likelihood model as well as the relevance model X  X hich were introduced in Section 3.1 X  X n the five test collections. 6.1.1. Query likelihood
This model (abbreviated by QL) uses MLE on the initial query to build a query model, by distributing the probability mass 2005, 2006 ) and CLEF domain specific tracks (Petras et al., 2007; Petras &amp; Baerisch, 2008 ). As to the TREC Genomics test collections, we do not perform any of the elaborate and knowledge-intensive preprocessing of the queries and/or documents knowledge, our baseline outperforms many systems that do. 6.1.2. Relevance models
The runs based on relevance models (abbreviated by RM) use the retrieved documents from the query likelihood run to construct an improved query model which is subsequently used for retrieval. The optimal parameter settings for the rele-vance model, with which we obtain these results are determined in the same fashion as for our conceptual language models, performance in terms of MAP.
 Table 9 shows the results of the baseline QL model and the RM model. We observe that, on the CLEF collections, the
RM runs show improvements over the baseline in terms of mean average precision (+6% and +1% for the 2007 and 2008 collection, respectively), average recall (+6% and +0.3%) and early precision (P@5: +6%, +8%). None of these differences is significant, however. Results on the individual CLEF-DS-07 topics show that three of the topics substantially increase average precision (a difference of more than 0.05), whereas only one topic decreases. The number of CLEF-DS-08 topics which improve in terms of average precision is about the same as the number which are hurt, causing the modest improvement.

The RM runs on the TREC Genomics collections do show significant differences compared to the QL baseline. For the 2004 query set, average precision (+17%), recall (+9%) and early precision (P@10: +12%) increase significantly. TREC-GEN-06 shows a larger significant improvement on mean average precision (10%). Recall and precision show improvements although they are not significant. Similar to the CLEF collections, TREC-GEN-05 shows a positive difference on average but, besides recall, none of the changes are significant. The increase in mean average precision on the TREC 2005 topics can be mainly attributed to a single topic which strongly benefits from using relevance models.

These findings regarding pseudo-relevance feedback using relevance models, i.e., where some topics are helped and some topics are hurt, are often found when applying pseudo-relevance feedback. 6.2. Conceptual language models
We now turn to the results of the conceptual language model presented in Section 4. Recall that this model consists of three steps. First, each query is mapped onto a conceptual query model, i.e., a distribution over concepts relevant to the query using Eq. (10). The concepts found are then translated back to terms using Eq. (13) in conjunction with the EM algo-rithm from Eq. (16).
 In the first subsection, we discuss the results of applying all the steps in our conceptual language model (GC; Section 4).
Then, in the following sections, we will perform an ablation study and discuss the results of not applying the EM algorithm (MLGC; Section 6.2.2) and not translating the found concepts using generative concept models (EC; Section 6.2.3). Example query models for GC and EC can be found in Table 8 for the CLEF topic  X  X  X hrinking cities X .
 6.2.1. Results
In this section we present the results of using every step of the conceptual language model (abbreviated GC) we detailed
GC model can result in a significant improvement in recall over the query likelihood approach: 13% and 9% more relevant documents are returned for CLEF-DS-07 and CLEF-DS-08, respectively. Fig. 3 shows the precision X  X ecall graphs for our con-ceptual language model, versus the query-likelihood baseline and relevance models. The precision X  X ecall curve of the CLEF-
DS-07 query set shows improved precision over almost the whole recall range. The CLEF-DS-08 runs shows improved pre-improvements in mean average precision (19% and 6%, respectively), but only the results on CLEF-DS-07 are significantly dif-ferent. We note that the RM approach was unable to achieve a significant difference against the query-likelihood baseline on these test collections and measures.
 The three TREC Genomics test collections show a less consistent behavior. In terms of mean average precision, the TREC-
GEN-04 and TREC-GEN-06 collections show significant improvements in favor of the GC model (+6.6% and +15.4% respec-tively). The TREC-GEN-05 topics also show substantial improvements between the query likelihood and GC model, although these changes are not significant. Fig. 2 shows a per-topic analysis of the difference of the GC model with respect to the QL baseline; a positive value in these graphs indicates the GC model outperformed the QL baseline. For TREC-GEN-05, it shows ence to be non-significant. The overall increase in average precision measured over al the topics, however, is larger than its loss.

From a further look on the per-topic plots, we can observe that, in terms of MAP, more topics are helped than hurt for all the other test collections. The early precision plots show a less clear picture. The ratio between the number of topics that improve precision@5 (P5) versus topics that worsen is about 1.5, averaged over all test collections. The average number of topics which precision@10 (P10) scores increase is about the same as the number of topics for which it decreases.
A more in-depth analysis of the terms that are introduced provides more insight into when and where the GC model im-proves or hurts retrieval. We observe that when the initial textual query is not specific, the resulting set of feedback docu-ments is unfocused. Hence, fairly general and uninformative words are added to the query model and it fails to achieve higher retrieval performance. Another reason for poor performance is that particular aspects in the original query are over-emphasized in the updated query model, resulting in query drift. For example, the CLEF-DS-08 topic 210 entitled  X  X  X stablish-ment of new businesses after the reunification X  results in expansion terms related to the aspect  X  X  X stablishment of new
When the updated query model is a balanced expansion of the original query, i.e., when it does include expansion terms for all aspects of the query, the GC model show improved results.

Overall, we see that our conceptual language model mainly has a recall enhancing effect, indicated by the significant in-creases in MAP for the CLEF-DS-07 and TREC-GEN-06 test collections and the significant increases in recall on both CLEF to-pic sets.

Table 11 shows a comparison between the GC and the RM model. When comparing these results, we find significant improvements in terms of recall on the CLEF test collections. On the TREC-GEN-04 and TREC-GEN-06 topic set we find a significant improvement in terms of MAP. The results on the TREC Genomics 2004 and 2005 topic sets indicate that the GC model performs comparably (TREC-GEN-05) or slightly worse (TREC-GEN-04). We believe the latter result is caused by the fixed setting of d t in Eq. (18) in conjunction with the rather small average document length and the large number of docu-ments in this particular document collection.
 Unlike the relevance model, the GC model provides a weighted set of concepts in the form of a conceptual query model. conceptual language models after a user has selected the concepts most relevant to his query would improve retrieval effec-for future work.

In the following sections, we look at the results of not using the EM algorithm in the generative concept models and di-rectly using the conceptual query models for retrieval. 6.2.2. Maximum likelihood-based generative concept models
In this section, we investigate the added value of using the EM algorithm described in 4.2, by comparing a maximum like-lihood based GC model (named MLGC ) to the GC model shown in the previous section. Table 12 shows the results of this method. We observe that applying the EM algorithm improves overall retrieval effectiveness compared to the MLGC model, although not significantly, and only in terms of recall and MAP. Only the number of relevant retrieved documents for the CLEF-DS-08 significantly improves when using the EM algorithm.
 The topics that are helped most by the application of the EM algorithm X  X n terms of an absolute gain in MAP X  X nclude
TREC-GEN-05 topic 146:  X  X  X rovide information about Mutations of presenilin-1 gene and its/their biological impact in Alz-heimer X  X  disease X  (increased MAP by 0.51) and TREC-GEN-06 topic 160  X  X  X hat is the role of PrnP in mad cow disease? X  (in-model introduces the term  X  X  X RP X , which is a synonym for  X  X  X rnP X . The second topic shows that the GC model introduces three new terms which do not seem directly relevant to the query, but are able to boost MAP substantially.

Besides having the potential of improving certain topics automatically we believe that, similar to our observation with regard to the GC model, the biggest improvements may be realized when a user selects the most relevant concepts. Future work should indicate if this is a valid assumption. Moreover, when one considers presenting the found concepts and/or terms to the user, the EM algorithm does provide a transparent function that helps filtering non-content-bearing terms and concepts. 6.2.3. Explicit conceptual query models
In Section 4.1 we introduced a method for acquiring a weighted set of concepts for a query, by translating a textual query to a conceptual representation. In this section, we evaluate the results of using the conceptual query model (abbreviated EC ) all the documents in our current test collections have two representations (terms and concepts), we can use both disjunc-each individual component as follows 5 :
Here, the first term is the regular query-likelihood score. The second term is the score obtained from matching the concep-tual query model with the conceptual representation of each document: cepts as regular indexing terms.

Thus, the EC model uses an explicit conceptual representation in combination with the textual representation for search-ing documents and, similar to the approaches described in the previous sections, the EC approach uses the same feedback documents for improving the query. However, instead of sampling terms from these documents, we now use their associated concepts.

When we look at the results as compared to the GC model as depicted in Table 13 , we find marginal differences. Only recall on the CLEF-DS-08 topic set is significantly different from the run based on conceptual language models. In comparison to the query-likelihood baseline (cf. Tables 9 and 13 ), the EC model shows similar improvements as the relevance models.
The runs on the CLEF collections show small improvements in mean average precision, recall and initial precision. When tested, these differences are not statistically significant The EC model, when applied to the TREC Genomics collections, shows significant improvements for the 2004 and 2006 collection with respect to the QL baseline.

Before turning to the answers to our research questions based on the results in this section, we present a brief analysis of the parameter sensitivity of our conceptual language model. 6.3. Parameter sensitivity analysis
Both our conceptual language model and the relevance model have a number of parameters that need to be set, as intro-Similar to related work ( Eguchi &amp; Croft, 2006; Zhai &amp; Lafferty, 2001 ), we did not evaluate j D restriction, the obtained results are clear improvements and further improvements may be obtained with a larger set of terms or documents.
 Table 14 lists the optimal parameter settings for the relevance model per test collection. We observe that the setting of k for this model is roughly dependent on the document collection. Table 15 lists the optimal parameter values for the concep-tual language model. Again we observe that the optimal value for k on the sensitivity of the results of the conceptual language model towards the setting of k collection and for both measures, with both maxima lying around k which both use the TREC 2004 document collection X  X ollow a less similar pattern, although their maximum MAP scores have a similar corresponding k Q value. The TREC-GEN-06 and the CLEF-DS-2007 topics show the largest relative improvement (both nearly 20% improvement over the query likelihood in terms of MAP, i.e., when k the best value for k Q based on the highest MAP scores does not necessarily lead to the highest score in terms of early pre-cision. Interestingly, the TREC-GEN-06 topics reach roughly the same precision@5 scores for the query likelihood model as when we would only use the terms suggested by the conceptual language model. 7. Conclusion
We have proposed and investigated conceptual language models for domain-specific document retrieval. The goal of con-ceptual language models is to leverage document-level concept annotations for improving full-text retrieval. In our method, query model is used to update the original, textual query model. The motivation behind this dual translation is that an ex-plicit conceptual representation of the information need can be used to derive related terms which are less dependent on the original query text. In both translation steps we have applied an EM algorithm to improve model estimation. Using an exten-sive set of experiments on five test collections from two domains, we have shown that conceptual language models can im-prove text-based retrieval, both with and without conventional pseudo-relevance feedback.

We now turn to answering the research questions posed in the introduction. First, we compared conceptual language models to a query-likelihood baseline and a model incorporating pseudo-relevance feedback. When evaluated on five test collections from two domains, we find that the conceptual language models yield significant improvements over a query-likelihood baseline on all the evaluated measures. In particular we have observed a significant improvement in terms of recall on all collections, which is in line with results obtained from relevance feedback methods in general. On the TREC col-lections, however, we have also observed a significant increase in early precision. As such, our method is both a recall and a precision-enhancing device.

When compared to relevance models and using the same pseudo-relevant documents, conceptual language models show collections. On the remaining measures, it gives similar improvements as relevance models. However, conceptual language
MAP models have the added advantage of offering query and browsing suggestions in the form of clearly understandable con-cepts. It should be noted that while each step in applying conceptual language models is not significantly different from each other or the steps combined, the full model is able to significantly outperform both a standard language modeling and a rel-evance modeling approach.

Our second research question concerns the use of an iterative EM algorithm to re-estimate textual and conceptual doc-ument models. These models are used in the process of determining a conceptual query model based on pseudo-relevant documents and for determining the translation probabilities from concepts to text. We have shown that this  X  X  X arsimonisa-tion X  step is an essential component in order to achieve good performance, since it makes sure that the language models only generate content-bearing terms. Moreover, since the resulting terms and concepts are more specific (than without EM-based re-estimation), we believe they are more useful in case these were to be presented to a user. Third, we looked into the parameter sensitivity of the proposed approach. Similar to conventional pseudo-relevance feedback, the optimal parameter settings have to be determined on a per collection basis.

Our fourth and final research question concerned the portability of our models. The usefulness of the proposed approach has been evaluated in two domains, the social science and genomics domain, each with different types of documents and their own concept vocabularies. Despite these large differences, the concept-based feedback shows consistent improve-be used to improve retrieval effectiveness. The MeSH thesaurus can, be used to improve genomics information retrieval de-spite its general biomedical coverage. The annotations of the CLEF collections seems to fit the information needs better, resulting in even better retrieval performance in the social science domain.

As to future work, we envisage several directions. First, in this paper we have relied on manually curated concept anno-tations of documents. Future work should look into the robustness of the approach when working with automatic conceptual representations of documents, such as obtained through document classification.

Second, we want to look into the relationship between conceptual and (traditional) term-based relevance feedback. In our current work we have used relatively simple baseline results for the estimation of our models. We hypothesize that combin-ing our generative concept models with well-performing methods such as relevance models may improve results even fur-ther and we will investigate this in future research. Further, for the results that we have presented we have utilized blind jrelevance feedback, i.e., we have assumed that the top-ranked documents and concepts were relevant. With the test collec-tions currently available we are unable to confirm or refute whether and how explicit relevance assessments would influ-ence the results. The same could be posited not only for documents, but also for the associated concepts. As such, we leave verifying whether user interaction influences the end results for future work. As an added value of this approach we noted that we obtain an explicit conceptual representation of the query. In future work we will look whether this con-ceptual representation is appreciated by and useful to an end user. Finally, although we have obtained significant improve-ments, we concede that the number of terms and documents employed in the estimations is of distinct influence on the end results. Whether increasing these numbers positively affect retrieval effectiveness remains a topic for future work. Acknowledgments
We thank the anonymous reviewers for their helpful comments and remarks. We also thank Wouter Weerkamp for his insightful suggestions. This research was supported by the BioRange programme of the Netherlands Bioinformatics Centre (supported by a BSIK grant through the Netherlands Genomics Initiative), by the DuOMAn project carried out within the STE-VIN programme which is funded by the Dutch and Flemish Governments (http://www.stevin-tst.org ) under project number
STE-09-12, and by the Netherlands Organisation for Scientific Research (NWO) under Project Numbers 017.001.190, 640.001.501, 640.002.501, 612.066.512, 612.061.814, 612.061.815, 640.004.802, and by the Virtual Laboratory for e-Science
Project ( http://www.vl-e.nl), which is supported by a BSIK grant from the Dutch Ministry of Education, Culture and Science and is part of the ICT innovation program of the Ministry of Economic Affairs.
 References
