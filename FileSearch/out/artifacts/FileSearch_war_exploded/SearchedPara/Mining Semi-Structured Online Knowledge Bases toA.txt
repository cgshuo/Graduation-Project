 Over the past few years, community QA websites (e.g. Ya-hoo! Answers) have become a useful platform for users to post questions and obtain answers. However, not all ques-tions posted there receive informative answers or are an-swered in a timely manner. In this paper, we show that the answers to some of these questions are available in online domain-specific knowledge bases and propose an approach to automatically discover those answers. In the proposed ap-proach, we would first mine appropriate SQL query patterns by leveraging an existing collection of QA pairs, and then use the learned query patterns to answer previously unseen questions by returning relevant entities from the knowledge base. Evaluation on a collection of health domain questions from Yahoo! Answers shows that the proposed method is effective in discovering potential answers to user questions from an online medical knowledge base.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models
Community QA (cQA) websites such as Yahoo! Answers, are highly popular. However, many questions are either left unanswered, partially answered or are answered with a sig-nificant delay. Past research has tackled this problem by finding questions from the archive, similar to the one posed by the user [7]. In many cases however, similar questions may not already exist or may be hard to find.

In this paper we propose a novel and complementary solu-tion. We believe that many of the questions may be answer-able via online knowledge-base websites such as wikipedia http://www.wikipedia.org or eMedicinehealth 2 . The following example shows that a question about lowering blood pressure can be poten-tially automatically answered by mining an answer para-graph from eMedicineHealth: Question: What is a good way to lower blood pressure? I would like to go off of my meds if possible. I would like to know the right types of foods and drinks and self care i can do. Thanks in advance.
 Answer value from eMedicineHealth: Lifestyle changes are important to help control high blood pressure. Even if your doctor has prescribed medicine for you, you can still take many steps at home to lower your blood pressure and reduce your risk. Some people can even take less medicine after making these changes. Make these lifestyle changes to help lower your blood pressure:Lose extra weight,Eat healthy foods....

Such knowledge-base websites tend to follow a standard structure. Each webpage discusses different aspects of a particular entity. For example an article on Bronchitis 3 emedicinehealth, discusses the various aspects of the disease entity  X  X ronchitis X , like  X  X auses X ,  X  X ymptoms X ,  X  X reatment X ,  X  X rognosis X  etc. Moreover, each aspect generally appears as a section heading (eg.  X  X ronchitis Treatment X ) followed by its text description.

One can view this data as being organized in a relational database with schema represented by the section headings. For example a relation with attributes  X (Disease,Symptoms) X  will store tuples of the form  X (Bronchitis, &lt; Text describ-ing symptoms of Bronchitis &gt; ) X . Note that while the val-ues stored in the database are predominantly verbose text descriptions, there are no restrictions on the kind of in-formation that can be stored. For example one could de-fine more complex relations that store numeric values eg.  X (Disease,Drug,Dosage) X , where the attributes  X  X isease X  and  X  X rug X  store text values while  X  X osage X  is numeric.
Our goal in this paper is to answer a new question by mining the most suitable text value from the database. We will subsequently refer to such a domain specific relational database as a knowledge-base to distinguish it from a reg-ular database. We will also refer to this novel problem of answering questions by mining text values from it, as http://www.emedicinehealth.com http://www.emedicinehealth.com/bronchitis/article em.htm knowledge-based Question Answering (kbQA). We will also frequently use the terms  X  X ocument X ,  X  X alue X  and  X  X atabase value X  interchangeably to refer to a specific atomic value in the knowledge-base.

The kbQA is a novel text mining problem which has clear difference from the existing information retrieval and ques-tion answering tasks. First, it differs from regular text re-trieval in that rather than retrieving documents based only on keyword/semantic similarity, the presence of relations be-tween text values also offers us the opportunity to perform limited  X  X easoning X  via sql queries. Thus the challenge in kbQA is to identify relevant sql queries that can help re-trieve the answer to the question. In this regard our problem is closer to research in Natural Language Interfaces (NLI), since both attempt to return values from a database [12, 11, 15]. However the nature of questions posed, databases used and the answers expected are all quite different from those in our case. In NLI applications, the natural language input from a user are often short and precise commands or re-quests making them relatively easy to parse. The databases also generally contain precise values which are easy to match with the keywords in the input command. Consequently, the methods used in NLI involve parsing the question text and directly trying to infer an underlying well-formed relational database query. In contrast, questions posted on community QA websites are uniformly real questions, and tend to be long, noisy and verbose, with a lot of background informa-tion, which makes NLI parsing methods unsuitable. More-over our goal is not to find a unique sql query. Infact since the knowledge-base values are also verbose, the question will partially match many different values and there will likely be multiple sql queries capable of retrieving the answer. More details differentiating our work from other related areas such as question answering are provided in section 2.

To solve this novel text mining problem, we propose a gen-eral probabilistic framework that would generate the answer in two steps. First we generate candidate sql queries along with a confidence score on their likelihood of generating the answer. We then execute the high scoring queries to generate candidate answer values. The final score of each candidate value is based on the confidence scores of all queries that generated it.

We further use the framework to derive a specialized model for answering questions on cQA websites, that takes into ac-count the noisy descriptive questions and verbose values. In particular, we show how the set of relevant sql queries may be mined, and the parameters of the model estimated, by automatically generating a training set from questions al-ready available on cQA websites.

We evaluate our model by building a kbQA system for answering healthcare questions on Yahoo! answers, using wikipedia as the knowledge-base. Results show that it is indeed feasible to answer cQA questions using our method.
To summarize, this paper makes the following major con-tributions: 1. We introduce and study the novel knowledge-based 2. We propose a general probabilistic framework for solv-3. We show how relevant queries may be mined and model 4. We evaluate the proposed framework in healthcare do-
Prior work in automatically answering cQA questions has centered around finding similar questions from within a QA archive. These approaches tend to treat a question thread as a structured document with question and answer fields and focus on identifying the semantic similarity between ques-tions. For example, Jeon et. al. [7] propose a retrieval model which exploits similarity between answers of exist-ing questions to learn translation probabilities, which allows them to match semantically similar questions despite lexical mismatch. A subsequent work by Xue et. al. [16] com-bines a translation-based language model for the question field with a query likelihood model for the answer. Other related approaches include finding answers from frequently asked questions on the web [14, 8]. Our work differs from these approaches in that our answers come from a knowl-edge base. Matched database values in our case only serve as constraints to query with and are not themselves suffi-cient to uniquely identify an answer. We must in addition also identify an appropriate target to retrieve.

Another related area is that of structured document re-trieval, where the focus is on developing retrieval models for documents with well defined fields. Many field based mod-els have been developed in the past including BM25F [13, 9, 10]. The principle idea behind them is that each query keyword may have been intended to match the content of a specific field. Hence they tend to assign different weights to document fields while estimating relevance.

Extensive research has also been done in developing ques-tion answering systems. Over the past few years, both open [1] and closed domain systems [2] have been proposed. How-ever most of these approaches are designed to answer only a restricted set of questions such as short and precise fac-toid [3] or definitional [5]. In addition they tend to require deep natural language processing steps such as generation of parse trees to analyze the syntax and semantics of the question, which are unlikely to work well with noisy cQA data.
In this section we provide a formal definition of the kbQA task and the notations that will be used in subsequent sec-tions.
 Problem: Given a knowledge database D and a question q , our goal is to return a database value v a as the answer. Input : There are two input variables q and D . q may be (semi-)structured or unstructured. While our pro-posed framework does not restrict the nature of q , questions encountered in community QA websites tend to be predom-inantly unstructured.
The database D comprises a set of relations R = { r 1 ,..,r The schema of each r i is represented via the set of attributes A i = { a i 1 ,a i 2 ,...,a iK i } . Each attribute a ij represents the j th attribute in the i th relation and is considered unique. Finally we define A D =  X  n i =1 A i as the set of all database attributes and use a  X  A D to represent some attribute in D . For example in a knowledgebase with two relations R = { Dis Treat,Dis Symp } with attributes ( Disease,Treatment ) and ( Disease,Symptoms ) we get
Note that even though the attributes Dis Treat.Disease and Dis Symp.Disease have the same semantic interpreta-tion, we treat them as two different and unique attributes. Consequently the total attributes in a database is just the sum of number of attributes in individual relations i.e. P
A database value v ijk represents the atomic value appear-ing in the k th tuple of the i th relation under the j attribute. We define Attr ( v )  X  A D to represent the attribute of the value v i.e. for v = v ijk , Attr ( v ijk ) = a ij . Finally we use V D to represent the set of all knowledge-base values. Output : The expected output is a value v a  X  V D such that v a forms a plausible answer to q . We assume the question is answerable through a single value in the database.
We view the core computation problem as that of ranking knowledge-base values based on their probability of being the answer and return the one that scores the highest. Al-ternatively depending upon the application needs and prob-ability scores, we can also return more than one or no values as well.
Intuitively, our approach involves the following steps: 1. First we identify values in the knowledge-base that are 2. Next we  X  X eason X  with these values by incorporating 3. Finally we rank the values returned by the sql queries
Consider an example in the healthcare domain. For sim-plicity we assume, our knowledge-base only contains a sin-gle relation Rel with three attributes (Disease, Symptoms, Treatment), and stores values for only two diseases i.e. two tuples { ( dis 1, symp 1, treat 1), ( dis 2, symp 2, treat 2) } . Now let a user asks a question describing a set of symptoms and expects a treatment description in response. In the first step we match the question to all values stored in the knowledge-base. Each of the 6 values in the knowledge-base will have some similarity score, with the symptom descriptions symp 1 and symp 2 likely having the highest. Consequently, a subset of queries we can execute will be (here we refer to symp 1 and symp 2 as constraints) select Symptoms from Rel where Symptoms = symp 1 select Symptoms from Rel where Symptoms = symp 2 select Treatment from Rel where Symptoms = symp 1 select Treatment from Rel where Symptoms = symp 2
The first two queries return the matched symptom value itself. This is equivalent to the behavior of a traditional re-trieval method. The next two queries will return the treat-ment text value corresponding to the matched symptom. These queries are more likely to retrieve the answer the user expected. Our confidence in the query to be executed will depend on a) how relevant the query is to the question and b) how well its constraint matches the question.

In a more general case, we may have to consider 100s of matched knowledge-base values as constraints and po-tentially 1000s of candidate queries of arbitrary complex-ity. This significantly complicates the problem of finding the most relevant answer value. In subsequent discussion we present a principled way of approaching the problem through a general probabilistic framework for modeling the uncertainties.

We begin with the conditional probability P ( V = v | Q = q ), which is the probability that a value v in the knowledge-base is the answer to the question q . The best answer v would then be given by maximizing over the set of all pos-sible values in the database V D :
We now decompose this expression into two conditional probabilities  X  X ridged X  by a sql query that can potentially capture the semantics of the user X  X  question q . Formally, let S
D be the set of legitimate queries that one is allowed to execute on the database D , then the probability of a value v may be obtained by marginalizing over it: P ( V = v | Q = q ) = X This decomposition has a quite meaningful interpretation: P ( S = s | Q = q ) captures the uncertainty in inferring what query to execute for question q . Since a query could poten-tially return multiple knowledge-base values, P ( V = v | S = s,Q = q ) captures the uncertainty in deciding the answer among the returned values. In cases where a query only re-turns a single value P ( V = v | S = s,Q = q ) trivially becomes 1. On the other hand if v is not among the values generated by s , it is 0. To keep the notation simple, in subsequent text, unless otherwise necessary we will drop the random variables and simply write for example P ( S = s | Q = q ) as P ( s/q ).

In theory the set S D could encompass all possible queries executable on the knowledge-base. However we want to re-strict it to only those queries which we feel are relevant in answering questions. To this end, we can make additional simplifying assumptions. We will restrict S D to only queries that have a single target attribute and use a single value as constraint. The first assumption is natural since we are trying to retrieve a single value. As we will see later, the second assumption is also not particularly restrictive.
A sql query is a complex object which can be encoded us-ing many different features such as  X  X onstraint used X , X  X arget attribute X ,  X  X umber of relations touched X  etc. We will en-code a query using two features -its constraint and the tar-get attribute i.e.
 Where Cons ( s )  X  V D is the constraint used in query s , Att ( s )  X  A D is its target attribute and S v  X  S D is the set of queries that generate the value v . Assuming that the infer-ence of target attribute and the constraint given a question are independent, we can further simplify the equation as:
P ( v | q ) = X Finally since any candidate answer value appears only under a single attribute in the knowledge-base, assuming Att ( v ) to be the attribute of v , we can write the final ranking function as logP ( v | q ) = log ( P ( Att ( v ) | q ))
Note from the equation that the distribution over the tar-get attribute and the constraint appear as separate compo-nents in the log. This is a major benefit of the model. It means that when maximizing the log likelihood over some training data, we can estimate the parameters of these two distributions independently.

Also note that the distribution over constraints, appears in a summation. This means that while calculating P ( v | q ) we sum over all constraints that can be used in queries to generate v . Thus restricting S D to only single constraint queries, still allows v to receive contributions from all rele-vant constraints.

Finally the equation suggests that in order to build a kbQA system, one needs to instantiate the following com-ponents
Legitimate Query Set : S D defines the set of legal queries which the system is allowed to execute to retrieve an answer. For most large knowledge-bases, this will have to be auto-matically inferred by mining queries from some collection of known QA pairs.

Constraint Prediction Model : P ( Cons ( s ) | q ) captures our uncertainty on whether some knowledge-base value can be used as a constraint for the question. Note that Cons ( s )  X  V
D . Hence it is a distribution over all knowledge-base values, conditioned on the question.

Attribute Prediction Model : P ( Att ( v ) | q ) captures our un-certainty on the attribute of the value expected in the an-swer. It is a distribution over all possible attributes Att ( s )  X  A
D , given a question. Its effect is similar in spirit to the question type detection that generally forms an initial step in most QA systems [6].

Value Prediction Model : P ( v | s,q ) allows us to favor some values over others based on question features, among all val-ues generated by query s . For example, we can use it to ensure some degree of textual similarity between the answer and the question. When no such preferences exist, the dis-tribution can simply be made uniform. For this work we will assume the distribution is uniform i.e.
 where V al ( s ) is the set of all values generated on executing the query s .

In the next section we describe how the different compo-nents may be instantiated to build a QA system for commu-nity QA websites.
In this section we discuss a possible instantiation of gen-eral kbQA suitable for web communities and show how its parameters may be estimated. We discuss all components except query mining which is discussed later in section 6.2 after describing the knowledgebase.
Since both questions and the database values tend to be verbose, one cannot ascertain the existence of a database value merely by looking at whether a question contains it. The constraint model instead needs to incorporate an intelli-gent similarity function between the question and a database value. In addition, questions tend to contain a number of discourse related background keywords such as  X  X hanks in advance X  etc., which we need to filter out. We define the probability of a knowledge-base value v  X  V D of being a constraint for a question q to be
Where for some v , Sim ( v,q ) denotes textual similarity between v and q calculated by an intelligent similarity func-tion and  X  is a parameter. Sim ( v,q ) is defined by treating the task of finding a matching database value as a retrieval problem, with the question q as a query and a value v as the document. We use the KL-Divergence retrieval model [17] for scoring relevance of values.
 where  X  q and  X  v are multinomial word distributions for question and value respectively.
 The value model  X  v characterized as the word distribution P ( w |  X  v ) is estimated using Dirichlet prior smoothing over the collection [17]: where c ( w,D ) is the count of word w in the document D , P ( w | V D ) is the probability of w in the entire collection. Optimal value of  X  is generally set at 4800.

To remove the background keywords while estimating the question model  X  q , we treat the question as being gener-ated from a two component mixture model, with a fixed background model  X  C Q which represents the probability of a word w in a large collection of questions C Q . Probability of a word in a question is the given by The model  X  q can then be estimated using Expecation Maximization as in [18]. In effect, the estimated P ( w |  X  would favor discriminative words and give them high prob-abilities.
Attribute prediction can be viewed as a multi-class classi-fication task over question features. We therefore model the distribution P ( Att ( v ) | q ) with Att ( v )  X  A D being the target attribute, using a maximum entropy model.
 where w a is the weight vector for attribute a and q F is the vector of question features.

Question features q F are defined over n -grams (for n = 1 to 5) and information gain is used to select top 25 K most in-formative features. Parameters for the attribute prediction model are learnt using the L-BFGS [4] method using the Mallet 4 toolkit. A default gaussian prior with interpolation parameter set to 1 is used for regularization.

Based on our definition of unique attributes in section 3, we treat any two attributes appearing in different rela-tions as different classes for the attribute distribution. For some schema, it may be the case that attributes across two relations are semantically the same. While we do not en-counter this case in our knowledge base, in such scenarios one can group together all such attributes into a single at-tribute class. Alternatively if we know for sure that certain attributes are unlikely to contain an answer value, we can simply ignore them.
Once the set of queries S D and all component models are estimated, the final ranking algorithm for a new question q works as follows: 1. Constraint Selection: Rank all knowledge-base values 2. Attribute Selection: Generate features for q and use 3. Query Selection: Find all queries in S D containing one http://mallet.cs.umass.edu/ 4. Answer Selection: Score each candidate value v by 5. Return the highest scoring value as the answer.
In order to mine the queries and estimate the parame-ters of the attribute and constraint distributions, we need to construct a training set containing questions and their suit-able answers from the knowledge-base. This is achieved by leveraging a collection of 80 K existing healthcare questions threads from Yahoo! Answers website. For any question q , let { a 1 ,a 2 ...a n } be the answers provided for it by the users. We want to find a knowledge-base value v a  X  V D which is most similar to the user provided answers and treat that as the answer.

More specifically, we use the KL-divergence retrieval model, the same function used for constraint selection, to generate a ranking of values for each answer a i . Just like the ques-tions, answers also tend to be noisy and may at times be completely irrelevant. Hence we learn the answer model  X  in the same manner as we learnt  X  q in constraint prediction, using a background answer model  X  C A over a large collection of answers. We ignore all answers with less than 30 words to ensure quality.

Once the rankings for each answer are generated, we now need to aggregate them. For each retrieved value, we pick the the K best ranks assigned to it by the answers and average them to generate the final score. The value with the lowest score is labeled as the answer. In order to ensure that we only generate training examples for which we are confident, we also reject an instance if: 1. A question has less than K answers available. 2. The lowest scoring value is not ranked at the top by 3. Either two or more values end up with the same lowest
The parameter K was tuned using a small fully judged validation set discussed in section 8.2.
In this section we discuss how we used wikipedia to con-struct our healthcare knowledgebase and subsequently mined queries to define our legitimate query set S D .
We used wikipedia to build our knowledge base. We chose wikipedia because it covers a wide range of healthcare re-lated information, in a language that tends to be at the same level of sophistication as a naive user asking questions on a cQA website. In addition wikipedia is comprehensive enough to contain answers to many questions raised by users. Finally the strategy used for building the healthcare knowl-edge base from wikipedia could be used to also build knowl-edge bases for other domains.

However since there is no fixed set of well defined at-tributes known beforehand. As a result some effort needs to be put in, a ) Identifying relevant domain specific wikipedia pages and b ) Converting the section headings into attributes. In this section we detail the process we followed in converting the raw wikipedia pages into a semi-structured knowledge base.
Most wikipedia articles are assigned one or more high level category tags which loosely identify the high level topic of the page. The category keywords are further organized into a hierarchy based on the generality and  X  X s-a X  relationships. For example the category  X  X ealth X  covers 33 subcategories such as  X  X iseases and Disorders X ,  X  X ealth Law X  etc. each of which themselves contain multiple sub-categories. More details regarding wikipedia categories are available here CatScan 6 is a tool that allows users to browse through the wikipedia category trees.

In all we identified categories covering a total of 29 K wikipedia pages on healthcare related entities. These spanned pages related to Diseases, treatments, medical techniques, drugs etc.

In order to extract these pages we downloaded the en-tire english wikipedia dump 7 and processed it using the WikipediaExtractor python script available at 8 . The script allowed us to parse the wiki notation and ultimately obtain plain text with some xml markup identifying section head-ings.
Once the domain relevant pages were identified, our next goal was to identify relations and attributes from the semi-structured wikipages. The section headings in wiki pages are good for this. For example when looking at a wikipage on  X  X alaria X  observe that the section headings consist of  X  X ntroduction X ,  X  X reatment X  etc. Other disease related pages tend to have similar section headings. Thus the text under  X  X reatment X  is related to the  X  X ntity X  malaria by the  X  X reat-ment X  relation. In other words we can define a treatment relation with two attributes  X  X ntity X  and  X  X reatment Text X  which will contain tuples of the form  X (Malaria, X  &lt; Text De-scription covering malaria treatment &gt;  X ) X .

It is clear that from the standpoint of answering user ques-tions, the attributes that appear frequently i.e. appear in more entities are more likely to be useful. The query tem-plates learnt over these attributes will be more applicable in answering questions across entities.

In order to identify the most useful attributes we rank all section headings based on the frequency of entity pages they appear in. We pick nearly 300 top headings (each appearing in atleast 25 pages) and manually analyzed them. Some ir-relevant headings such as  X  X ee Also X ,  X  X eferences X  etc. were eliminated. Of the remaining, many similar headings were merged into the same attribute. For example  X  X igns and http://en.wikipedia.org/wiki/Wikipedia:Categorization http://meta.wikimedia.org/wiki/CatScan http://en.wikipedia.org/wiki/Wikipedia:Database download http://medialab.di.unipi.it/wiki/Wikipedia Extractor Symptoms X ,  X  X ymptoms X ,  X  X ymptoms and Signs X  etc. were all grouped into the same attribute  X  X igns and Symptoms X . Similarly  X  X dverse effects X ,  X  X ide effects X  X  X ide-effects X  X  X ide Effects X  X  X ealth effects X  etc. were all merged under the same attribute  X  X ide Effects X . Ultimately after pruning and merg-ing we ended up with 59 attributes covering a total of 68 K text values over the 29 K entities.

The nature of these relationships is such that each text value is related to only a single entity.
In step two we were able to define relationships between entities and various text descriptions. However the enti-ties themselves are also related to each other. For exam-ple the drug  X  X hloroquine X  is a  X  X edication X  of the disease  X  X alaria X . Incorporating such relationships into the knowl-edgebase is critical to answering user questions.

A good way to identify such entity-entity relations is to simply define them based on the attribute in whose text value an entity appeared in. For example  X  X hloroquine X  appears in the medication description of  X  X alaria X , which is a text value related to  X  X alaria X  via the  X  X edication Text X  relation.

To this end we further define 59 additional relations of the form  X  X ntity, &lt; Attribute &gt; Entity X  for example the relation medication entity has the attributes  X (Entity,Medication En-tity) X  and a sample tuple  X (Malaria, Chloroquine) X 
As a result after defining the extended relations our fi-nal knowledge base comprised of 118 relations covering 29 K entities and 68 K text values. We believe there was a sig-nificant overlap between the knowledge-base and the cQA collection.
Recall a sample sql query we discussed earlier s 1 : select Treatment from Rel where Symptoms = symp1
We refer to  X  X ymp1 X  as a constraint and the remaining query as the template i.e. t 1 : select Treatment from Rel where Symptoms = &lt;some symptom value&gt;
Note that while query s 1 is only useful in answering ques-tions that match the symptom value symp 1, the template t 1 allows us to generalize across questions and is useful for an-swering any question that provides a symptom description and expects a treatment in the answer. t 1 intuitively cap-tures a kind of general  X  X easoning X  that may be performed to answer some of the questions that provide symptom descrip-tions. Hence we can treat any sql query that is generated by adding a constraint with template t 1 as legitimate. More generally, our main intuition behind defining S D is to iden-tify a set T of such templates, and assume that any query they generate is in S D .

Now assuming we have a training set of questions and their answers from the knowledge base available, we can use it to first discover single constraint queries, and then con-vert them into templates by simply dropping the constraint. More general templates will naturally be found in multiple training question-answer pairs. Figure 1: Knowledgebase in Table 1 viewed as a graph
Thus the main challenge in mining such templates is to identify a sql query given a question, its answer value and the knowledgebase. Our approach to solving this problem is to view our entire wikipedia knowledgebase as a graph. Each value in the knowledgebase (either verbose text, or entity name) is treated as a node in the graph. Any two values that appear in the same tuple are connected via an edge. Naturally any two neighboring nodes are bound to have some relationship between them. This relationship is assigned to the edge as a label.

Consider the three relations in Table 1 which are similar to those in our wikipedia knowledgebase. The corresponding knowledge base graph is shown in Figure 1.
 Table 1: A Sample Knowledgebase to be mined Each edge in the graph is assigned a label of the form Attribute 1 Attribute 2 which represents the relationship be-tween the nodes. Now assuming that our question matched the constraint S 1 and our answer contained the value A 1, we first obtain the shortest path between the two nodes. Which in this case is S 1  X  D 1  X  M 1  X  A 1. Once the path is found, we traverse from the constraint node ( S 1), one step at time towards the answer node ( A 1). In each step a new sql construct of the following form is added in a nested fashion. select Attribute 2 from &lt;Relation containing Attribute 1 _ Attribute 2 &gt; where Attribute 2 =&lt;current node value&gt;
More specifically in the first step we generate select Entity from Entity_SymptomText where Symptom-Text = S 1
The eventual query template generated is select AdverseEffectsText from Entity_AdverseEffectsText where Entity = (select Med-icationEntity from Entity_MedicationEntity where En-tity = (select Entity from Entity_SymptomText where SymptomText = &lt;symptom text value&gt;)
Note that the queries that get generated in this fashion may at times end up returning multiple values along with the answer value. Especially because multiple entity val-ues may result from one of the nested queries. However our model does take that into account through the value predic-tion model P ( v | s,q ) and will favor queries generating fewer values.

To summarize, for each question in the training set, we used the top 10 knowledge base values found by the con-straint prediction component as our constraints i.e. start nodes, and the answer value as the end node. Shortest paths were then found in each case and converted into query tem-plates. All templates found in more than 2 questions were considered as legitimate.
In order to validate our training set generation method and tune its parameter K , we needed to create a small fully judged collection containing questions and their an-swers from a knowledge base. For this we used 21 ques-tions from Yahoo! answers cQA website and manually ex-haustively labeled all their answers in a secondary database constructed from eMedicinehealth. We preferred eMedicine-Health, since it is much smaller than wikipedia making it possible to easily create a fully judged collection. The in-formation is mainly centered around diseases and their re-lated properties. The schema comprises 13 attributes such as Disease , Treatment , Symptoms etc. In all it contained information on 2285 diseases with a total of 8825 values. The judgement set contained an average of 3 . 5 relevant val-ues per question (out of a total 8825). All judgements were reviewed by a medical expert.
We processed 80 K healthcare questions from Yahoo! An-swers website using our automated training set generation approach to generate an automated evaluation set. Many questions were filtered out by one or more filtering crite-ria (see section 5.4). The resulting dataset contained 5 . 7 K questions, for each of which a value from our wikipedia knowledge-base was identified as the answer. This dataset served as a large automated evaluation set for training and evaluation of our approaches.
 For automatic evaluation we used 5-fold cross validation. In each fold 4 . 6 K instances were used for finding the tem-plate set S D , learning the attribute prediction model and tuning the parameters of the constraint prediction model. The learnt model was then evaluated on the remaining 1 . 1 K questions.
We also created a smaller manual evaluation set consisting of 60 manually judged questions. The judgements for this dataset were created by employing a pooling strategy often used in information retrieval evaluation. For each question, we pooled together top 5 values returned by different ranking methods 1. The Sim () function used in constraint prediction. This 2. Our kbQA method trained on the automatically gen-3. Training Set Generator
Each of these values was then judged by a medical expert as being a relevant or irrelevant answer to the question. The resulting relevant values then became our final evaluation set. In all 116 relevant answers were found.

For manual evaluation, we trained on all 5 . 7 K training questions excluding the 60 manually judged questions which were used for evaluation. We used Success at 1 ( S @1), Success at 5 ( S @5) and Mean Reciprocal Rank (MRR) for evaluation. Each criterion al-lows us to analyze a different aspect of the performance and guess utility for a specific application. S @1 gives us the percentage of questions answered correctly at rank 1 and is relevant if for example we want to automatically post an answer response on some cQA website. On the other hand MRR and S @5 are more useful if we assume the user may be able to look at a short ranked list of answers.

Note that since the automatic trainingset generator is only confident about the best answer, for each question in the automatic evaluation set, we only have one relevant answer.
The main goal behind our experiments was to check if it was feasible to answer questions through text mining a knowledge base and whether our kbQA approach would out-perform a state of the art baseline. The baseline we pri-marily want to compare against is the state of the art KL-Divergence retrieval method ( Sim ()) which we use for con-straint prediction. Out performing it would ascertain that the kbQA model indeed adds value over baseline retrieval. Other questions critical to success of kbQA, that we are interested in are a) To check if automated trainingset gen-eration was feasible and b) if it was common to find query templates useful in answering multiple questions.
Figure 2 compares the 5-fold cross validation performance of our method with the constraint prediction baseline. In each fold, we use 4 . 6 K questions for mining templates and estimating parameters. The rest 1 . 1 K questions are used for prediction. We observe that kbQA outperforms the state of the art KL-Divergence retrieval method by 39 . 44% in terms of S @1 and 21 . 17% in terms of MRR. Both improvements were found to be statistically significant using the Wilcoxon signed rank test with level a = 0 . 05. We also notice that the improvement is greater for S @1 than S @5 suggesting that kbQA is better at pushing the most relevant answer to the top. Overall kbQA succeeded in answering nearly 17% of the answers correctly. A sample answer generated by kbQA in response to a question regarding cramps is shown below. It clearly shows that useful answers for cQA questions do exist in online knowledgebases and our proposed approach can help retrieve them, thereby helping the users.
 Question: How to relieve cramps in feet? I often get cramps in my feet and i have no idea as to why. Does anybody know what could cause them and any way to relieve them when they occur?
Answer by kbQA: Skeletal muscles that cramp the most often are the calves, thighs, and arches of the foot......this kind of cramp is associated with strenuous activity and can be intensely painful.......though skeletal cramps can occur while relaxing..... It may take up to seven days for the muscle to return to a pain-free state...... Nocturnal leg cramps are in-voluntary muscle contractions that occur in the calves, soles of the feet, or other muscles in the body during the night or (less commonly) while resting.......Potential contributing factors include dehydration, low levels of certain minerals (magnesium, potassium, calcium, and sodium), and reduced blood flow through muscles attendant in prolonged sitting or lying down....Gentle stretching and massage, putting some pressure on the affected leg by walking or standing, or taking a warm bath or shower may help to end the cramp.
 We next analyze the results on the manually judged dataset. The results are shown in Figure 3. We observe that the per-formance of kbQA is higher than the KL-Divergence method, but lower than the training set generator (TsetGen) which represents a kind of upper bound performance. The kbQA method answers nearly 35% of the questions correctly, sug-gesting that the automatically generated trainingset was in-deed useful for the task.

Finallly Figure 4 compares the automatic evaluation per-formance of kbQA and KL-Divergence on the 60 questions used for manual evaluation. We assume the results of our training set generator to be the gold standard. kbQA con-tinues to outperform the baseline. We also observe that per-Figure 3: Performance on manually judged set Figure 4: Automatic evaluation on manually judged set formance is generally lower than the one obtained through manual evaluation because in the automatically generated evaluation set we are unable to generate more than one rel-evant result per question.
We validate the performance of our automated training set generator on the 21 query validation set. For each ques-tion in the validation set, we use our training set generator to find the best answer from the eMedicinehealth knowledge-base and compare it to the manually judged gold standard. Note that many of the questions will get filtered out due to the filtering criteria (see section 5.4). The performance is evaluated only on the questions that are retained. Our goal is to find a parameter setting for K such that we retain a sufficiently high accuracy in terms of S @1 and MRR, with-out filtering out too many questions. The results are shown in Figure 5.

The figure shows how three different metrics S @1, MRR and Percentage of training questions returned vary with the parameter K . We observe that while K = 2 generates the most number of instances (61 . 9% or 13 out of 21), its accu-racy is quite low. On the other hand for values 3 through 5, we retain slightly over 50% of the 21 queries as our training questions. Among these, the accuracy is highest for K = 3 which is the value we select for K .
Our next goal is to analyze whether it is possible to learn query templates that are reusable across questions. We ex-Figure 5: Analysis of training set generator perfor-mance on validation set. We choose K = 3 ecuted our template mining step on all 5 . 7 K training ques-tions and plotted two graphs. The first is shown in Figure 6. It plots the question frequency of a query template on the x-axis and the number of query templates that were found with that frequency on the y-axis. The question frequency of a template is the number of questions this template was useful in finding answers to. For example the first data point means that there were nearly 4000 query templates that were useful in answering 5 questions. Naturally, as we increase the question frequency, the number of templates drops. The last data point shows that there were 340 templates that were useful in answering 20 questions. The plot clearly sug-gests that many templates are useful across questions.
We next need to ensure that the high frequency templates are not concentrated only among a selected few questions. To analyze this, we plot the number of questions that will become unanswerable if we used only query templates above a certain question frequency (see Figure 7). For example the plot shows that if we restrict to query templates with a ques-tion frequency of 5 or more, we will be left with no templates capable of answering 142 questions. In general we observe that for most questions there tends to be atleast 1 high ques-tion frequency template. Which is why even with threshold of 20, only 10% of the questions become unanswerable.
We introduced and studied a novel text mining problem, called knowledge-based question answering, where the com-putational problem is to mine an online semi-structured knowledge base to discover potential answers to a natural Figure 7: Showing the number of unanswerable questions against question frequency language question on cQA sites. We proposed a general novel probabilistic framework which generates a set of rele-vant sql queries and executes them to obtain answers. We presented in detail an instantiation of the general framework for answering questions on cQA sites by leveraging the ex-isting questions and answers as training data. Evaluation has shown that the proposed probabilistic mining approach outperforms a state of the art retrieval method.

Although we only evaluated the our work on healthcare domain, the framework and algorithms are general and can be potentially applicable to other domains as well. Our main future work is to extend our work to additional domains and to refine the different framework components.
This material is based upon work supported by the Na-tional Science Foundation under Grant Number CNS-1027965. [1] Trec question answering track. [2] S. J. Athenikos and H. Han. Biomedical question [3] M. W. Bilotti, J. Elsas, J. Carbonell, and E. Nyberg. [4] R. H. Byrd, J. Nocedal, and R. B. Schnabel.
 [5] H. Cui, M.-Y. Kan, and T.-S. Chua. Generic soft [6] Z. Huang, M. Thint, and A. Celikyilmaz. Investigation [7] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [8] V. Jijkoun and M. de Rijke. Retrieving answers from [9] J. Kim, X. Xue, and W. B. Croft. A probabilistic [10] J. Y. Kim and W. B. Croft. A field relevance model [11] M. Minock. C-phrase: A system for building robust [12] A.-M. Popescu, A. Armanasu, O. Etzioni, D. Ko, and [13] S. Robertson, H. Zaragoza, and M. Taylor. Simple [14] R. Soricut and E. Brill. Automatic question answering [15] V. Tablan, D. Damljanovic, and K. Bontcheva. A [16] X. Xue, J. Jeon, and W. B. Croft. Retrieval models [17] C. Zhai. Statistical language models for information [18] C. Zhai and J. Lafferty. Model-based feedback in the
