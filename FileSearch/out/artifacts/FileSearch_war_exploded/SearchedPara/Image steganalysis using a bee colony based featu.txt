 1. Introduction
One of the most important recent discussions in information forensics is related to the improvement of steganalysis performance and protection of concealed information. On the other hand, it is important to transfer the most important information through a safe way somehow; it cannot be attacked, detected and accessed using steganography techniques. Several methods have been pre-sented to improve the steganalysis performance by increasing feature space to be blind steganalysis that take a long time to steganalysis ( Holub et al., 2013 ; Kodovsky and Fridrich, 2012 ).
In this paper, a novel feature selection algorithm (IFAB) is proposed to improve the steganalysis performance, decrease the time, and obtain the highest accuracy of steganalysis through selecting the relevant and most important features to improve the accuracy. Steganography is a dynamic domain with an extensive history.
Steganographyhavehadthecompetencetoadjustnewstagesto information technology. Steganography is the action of covering private or sensitive data within covers that keeps signi fi each pixel without changing the color noticeably. Except the sender and the receiver, nobody knows the presence of the embedded message. Therefore, it is necessary to maintain the information from unauthorized or unexpected viewing ( Geetha and Kamaraj, 2010 ).
Steganography has been noticed secure approach if the stego-images (2) Feature selection might also be employed to obtain the highest classi fi cation accuracy of a classi fi er. (3) Another bene fi t of using feature selec tion is training a classi that the chosen features can assist to draw attention to the features which are sensitive to an arranged steganographic pattern. (4) The last bene fi t of employing feature selection is the depletion at computational complexity for both training the classi fi and extracting the features. If we choose a series of K features out of N , the training time will be distributed by N / K .
In recent years, swarm intelligence has become an interesting research to scientists of related fi elds. The swarm intelligence is considered as any effort to redesign algorithms or extensive problem-solving devices taken by the collective behavior of social insect colonies and other animal societies by Bonabeau et al. (1999) concentrated on social insects lonely in their researches such as bees, termites, wasps in addition to varied ant species. The word swarm is used in a general manner to refer to any controlled collection of cooperating agents. The classical instance of a swarm is bees swarming around their hive. However, this sample can easily be expanded to other systems with an identical structure ( Karaboga and Basturk, 2008 ). This paper proposes a new image steganalysis approach (IFAB) using arti fi cial bee colony to feature selection.
The goal of IFAB is to choose a subset of presenting features from the whole feature set. While using the most complete feature set, we should reduce the total number of used features as much as possible. As a result, the total computation time would be decreased extensively. The selected features set in fl uences numer-ous aspects of image classi fi cation such as the time required to train a classi fi cation algorithm like a support vector machine (SVM), the predictive accuracy of the trained classi fi cation algo-rithm, the time-space cost related to the features, and the number of instances required for training the classi fi cation algorithm.
The rest of the paper is organized as follows: In Section 2 ,we have presented a summary review of feature selection (FS) algo-rithm. Section 3 presents the main arti fi cial bee colony. Section 4 introduces our arti fi cial bee colony-based feature selection algo-rithm. The following section will discuss the experimental obtained results. In the last section of the paper some conclusions have been drawn. 2. Related works
In recent years, a considerable number of research studies have adopted dimension reduction as a pre-analysis processing to separate the irrelevant and unimportant features from the relevant and important features. This process can be classi fi ed into feature selec-tion methods and feature extraction methods. The former are techniques of selecting a possible features set from the whole set of candidate features. The latter are techniques that have been used to extract many features from the original data (Image) in order to generate dataset. Futhermore, The feature selection techniques are a subset of the more general fi eld of feature extraction. This section contains two subsections that summarize the concepts of feature extraction, feature selection methods and presents previous works related to per method. This section reviews the earlier studies have been performed in this fi eld. Section 2.1 gives the signi feature extractor and the kind of the common suggested techniques.
Section 2.2 belongs to feature selection and the techniques have been presented so far. 2.1. Feature extractor
Based on whether an image includes hidden message or not, images can be categorized into the image with no hidden message called cover-image, and the image with a message hidden called can provide the least error rates and outperform PCA and other methods ( Hinton and Salakhutdinov, 2006 ). This work also sets evidence that NN was very effective in non-linear dimension decrease ( Hinton and Salakhutdinov, 2006 ).

There are many studies presented fl exible and robust heuristic feature selection approaches based on swarm intelligence (SI) algorithms. SI is a kind of arti fi cial intelligence that simulates the collective behavior of social animals. Those animals cooperate through self-organizing procedures to solve complex tasks and obtain their aims. Several SI algorithms can create outstanding results in the feature selection procedure. For example, Aghdam et al. (2009) focused on the feature selection process based on Ant
Colony Optimization (ACO), which is an algorithm inspired by the foraging behavior of ants.

The result indicates that the ACO has very low computational complexity when compared with GA and other stochastic algo-rithms ( Aghdam et al., 2009 ). Yang et al. (2006) used particle swarm optimization (PSO) which is an algorithm based on the social behavior of bird fl ock and fi sh schools, in combination with GA in feature selection procedure.

On the other hand, algorithms for feature selection can be classi into two categories: fi lter methods and wrapper methods ( Liu and Yu, 2005 ). Filter methods choose a features set as a pre-analysis proces-sing step which is independent of the learning algorithm. The best feature subset is chosen through evaluating some prede fi without using any learning algorithms. Thus, this concept frequently considers a faster speed. Furthermore, the fi lter method is computa-tionally cheap and more common. Wrapper methods use the pre-dictive accuracy of the classi fi er to evaluate the worth of feature subsets. The wrapper method can obtain the high classi fi accuracy for a particular classi fi er at the cost of high computational complexity and less generalization of the selected features on other classi fi ers. However, the wrapper methods generally do better than the fi lter methods in the aspect of the accuracy of the learning process.
So, a number of researchers try to speed up the convergence of the wrapper algorithm by means of kinds of techniques. A lot of researchers have proposed the combination of fi lter and wrapper methods ( Est X vez et al., 2009 ; Huang et al., 2007 ), that use the obtained knowledge through fi lter methods to learn the classi
In Foithong et al. (2012) a new feature selection method was proposed based on the hybrid model ( fi lter  X  wrapper) which use the mutual information criterion without requiring a pre-de fi ned parameter for the selection of the available subset features.

In Peng and Xu (2013) , a new feature selection algorithm was presented to data regression with a lot of irrelevant features. The proposed method is one of wrapper methods based on well-established machine learning technique without underlying data distribution. In Sharma et al. (2012) ,another wrapper method that proposed a feature selection algorithm in gene expression data analysis of sample classi fi cations. Yang and Ong (2011) presented two wrapper-based feature-selection methods for SVR using its probabilistic predictions under the assumptions that the noise are generated from Gaussian and Laplacian distributions.

In the framework of handwriting recognition, Most of the approaches have been proposed in the context of handwriting recognition use wrapper methods ( Chung and Yoon, 1997 ; Kim et al., 2000 ; Oliveira et al., 2003 ). Their main purpose is to reduce the number of features, keeping the recognition rate unchanged, or at most slightly worse. De Stefano et al. (2013) proposed a novel
GA-based feature selection algorithm which feature subsets are evaluated by using a speci fi cally devised separability index. 3. Arti fi cial bee colony (ABC) algorithm
In ABC algorithm, the colony of arti fi cial bees consists of three kinds of bees: employed, onlookers and scouts bees. A half of the give some deeper explanation. Section 4.1 explains the schematic approach in details to choose the best subset features. The structure of the proposed arti fi cial bee colony based feature selection is well explained in Section 4.2 .

The general structure of the IFAB is given in Fig. 2 that contains three important steps. These steps are the stages of the IFAB according to feature extractor, ABC feature selection, support vector machine (SVM). Fig. 2 illustrates how to use IFAB, and fi need to provide two datasets with extracting features from a lot of images. For this issue, we have employed two feature extractors to solve it. We will discuss them in the following section. After fi nishing feature extractor step, we need an approach to select relevant and the most important features using ABC, applied SVM to evaluate the selected features set, and the result feeds back to
ABC in order to validate the result, then if condition is met, the process will fi nish. 4.1. Structure of proposed feature selection approach
In general, a speci fi c feature selection algorithm contains four factors: a subset generation or examination process, an assessment function, a stopping condition in ABC and a validation process. This overall process of feature selection is evaluated by ABC which is explained in Fig. 3 . An important subject for the feature selection process is how to search the huge space of feature subsets. 4.2. A novel arti fi cial bee colony algorithm for feature selection (IFAB)
In the proposed ABC based feature selection approach, ABC algorithm improves the procedure of feature selection and pro-duces the ideal feature subset that increases the performance of the classi fi er. Fig. 3 displays the schematic fl ow chart of imple-menting feature selection using ABC. ABC is employed as a feature selection approach and produces the feature subsets. A classi employed to evaluate every feature subset created by the onloo-kers, therefore, the suggested method is a kind of wrapper based methods. The steps of the proposed ABC based feature selection algorithm are given in Fig. 4 that is well-explained as follows: 4.2.1. Initial population In this paper, the IFAB is used to explore the novel search space.
Initial swarm is sometimes created randomly. Do to generate an classi fi er. The predictive accuracy of each feature is evaluated by SVM called F r .

Fitness  X  R  X  X  4.2.4. Onlooker bees process
An onlooker bee selects a topmost food source based on its winning probability value, which is similar to the roulette wheel selection. After sharing information with onlooker bees is per-formed by employed bees, the onlooker bees get information from the employed bees and choose a food source to visit according to the probability of each food source using Eq. (2) and make the process of updating possible solution based on Eq. (3) .
P  X 
R  X  X  fitness
After that, the onlooker calculates the novel solution V i the predictive accuracies of the feature the employed bee is pointing to and the feature the onlooker bee has chosen. The novel solution V i is computed by using Eq. (3) . j  X  rand  X  1 ; N
V  X  f i  X   X   X  f i f j  X  X  3  X  where, i  X  {0,1,2 ... , N ) and j  X  {0,1,2, ... , N } and N of features and f i stands for the predictive accuracy of the feature assigned to the employed bee and f j stands for the predictive accuracy of the feature the onlooker has chosen and  X  stands for a real random number in the range [ 1,1]. According to Eq. (3) , V supposed to be the accuracy of a new solution that has been all feature dimensions calculated by using Eq. (4) . This process is performed to prevent choosing the sub-optimal solution. 4.2.6. Termination process
The process of employed bees, onlooker bees and scout bees will be continual till the number of runs obtains the pre-determined maximum number, or satis fi ed the criteria. The procedure of using ABC method and fi nding best feature among features are given in
Fig. 5 . 5. Experimental results
To reveal the performance of the proposed method and compar-ing with other well-known feature selection approaches and also with a novel Markov blanket embedded GA (MBEGA) approach ( Zhu et al., 2007 ; Geetha and Kamaraj, 2010 )thathasbeenusedfor steganalysis problem. MBEGA employed Markov blanket to fi tune the search by adding the relevant and important features in the
GA solutions. This memetic process takes advantage of both Markov blanket and GA wrapper feature selection with the aim to improve the classi fi cation accuracy and accelerate the search to keep relevant features. Three sets of experiments were carried out in steganalysis.
In experimental studies, the breaking out steganography system (BOSS) ( Bas et al., 2011 ) version 1.01 grey scale image databases is employed that the rate of hidden text embedding is 0.4 per pixel.
This database contains 10,000 cover images and 10,000 stego images. Likewise, the number of classes in our experiments is two. 5.1. The spam features
In this paper, both the subtractive pixel adjacency model (SPAM) method ( Pevny et al., 2010 ) and CC-PEV are employed to extract the features for steganalysis. SPAM has 686 features and one class feature, and CC-PEV ( Liu, 2011 ; Kodovsk  X  and Fridrich, 2009 ) 548 feature and one class feature. 5.2. Performance evaluation of proposed method
In this section, the performance of the proposed method for different parameters setting is investigated. The effect upon the subset features threshold is presented in Figs. 6 and 11 . It is clear the optimum value for this parameter in the SPAM is 80, and in the
CC-PEV is 250. These numbers indicate that we should apply the subset features for the maximum possible predictive accuracy.
According to these numbers we can prove that our proposed method is capable of improving the performance of SPAM and CC-PEV algorithms.

Fig. 7 demonstrates the effect on different food source values in proposed method. It is obvious that by increasing the food sources, it increases the predictive accuracy signi fi cantly.
 5.3. Comparison with other approaches 5.3.1. Applying SPAM
The results of those steps are given in Table 2 that reveal the best average detection accuracy of each approach. It is obvious that the high accuracy of image steganalysis is related to the proposed method (IFAB).

Fig. 9 reveals that IFAB outperform other well-known and related classic methods. On the other hand, ABC-NB does the worse accuracy in comparison with others. The ABC-NB is a kind of feature selection methods that is based on bee colony through using Na X ve
Bayes (NB) to evaluate the feature subsets. As mentioned, the predictive accuracy after feature selection has increased by using several feature selection methods such as MBEGA. Releif-J48, SMO ( Geetha and Kamaraj, 2010 ) and kinds of ABC based method.
Furthermore, we have used 2 classic methods such as K nearest neighbour ( Aha et al., 1991 ) and Spegasos ( Shalev-Shwartz et al., 2011 ) and also the SPAM with total features. In Fig. 10 we have already tried to illustrate depletion at feature dimension in spite of the original data set. It shows that too many depletion is not fair enough, and we should respect the accuracy as well. As discussed, we would keep at least the primitive accuracy and increase it by decreasing the feature dimension. 5.3.2. Applying CC-Pev In this section, we are going to explain in details about applying
CC-Pev and show that how the parameter (number of features) have chosen and used. Fig. 11 declares 250 is an ideal number for selecting the important features when CC-Pev is employed.
By employing CC-Pev, the results in Table 3 show that the predictive accuracy totally has increased with fewer features in comparison with SPAM, but the most important features in this extracted dataset are more than in SPAM extracted dataset.
Fig. 12 reveals that the proposed method outperform other well-known and related methods on CC-Pev. On the other hand,
ABC-NB does the worse accuracy in comparison with other methods. As we mention, the predictive accuracy after feature selection has increased by using several feature selection methods such as MBEGA. Releif-J48, SMO and kinds of ABC based method.
Fig.13 shows depletion at feature dimension beside the original data set. It shows that too much depletion is not fair enough, and we should respect the accuracy as well. As we debated, we would keep increase the predictive accuracy by decreasing the feature dimension. 5.3.3. SPAM versus CC-PEV
Applying both SPAM and CC-Pev on the similar image data set shows a signi fi cant difference between these two feature extrac-tors. Tables 2 and 3 declare that the number of decreasing features in SPAM are more than in CC-PEV. In other words, the most important and relevant features in SPAM are less than in CC-PEV in comparison with the predictive accuracy, so we can conclude that
SPAM is more suf fi cient to feature selection more than CC-Pev. On the other hand, the classi fi cation accuracy of CC-Pev is more than
SPAM. As Fig. 14 illustrates after applying the SPAM for feature 6. Conclusion
In this paper, a novel Image steganalysis feature selection algorithm is proposed based on arti fi cial bee colony (IFAB). To show the usefulness of the proposed algorithm, and compare with
ABC-based and other well-known feature selection methods, three sets of experiments were carried out using both SPAM and CC-PEV data set of features in Image steganalysis. The behaviour of IFAB under different control parameter values has also been studied, and the performance of the proposed method (IFAB) has been compared with that of differential well known algorithm. Experi-mental results show that IFAB outperforms the mentioned algo-rithms and can be expeditiously employed to solve the multi-model engineering problems with high dimensionality. However, the main advantage of this paper is the strong specialized capability in the aspects of the number of selected features and performance in classi fi cation accuracy.
 References
