 Preference based methods for collecting relevance data for information retrieval (IR) evaluation have been shown to lead to better inter-assessor agreement than the traditional method of judging individual documents. However, little is known as to why preference judging reduces assessor dis-agreement and whether better agreement among assessors also means better agreement with user satisfaction, as sig-naled by user clicks. In this paper, we examine the relation-ship between assessor disagreement and various click based measures, such as click preference strength and user intent similarity, for judgments collected from editorial judges and crowd workers using single absolute, pairwise absolute and pairwise preference based judging methods. We find that trained judges are significantly more likely to agree with each other and with users than crowd workers, but inter-assessor agreement does not mean agreement with users. Switching to a pairwise judging mode improves crowdsourcing quality close to that of trained judges. We also find a relation-ship between intent similarity and assessor-user agreement, where the nature of the relationship changes across judging modes. Overall, our findings suggest that the awareness of different possible intents, enabled by pairwise judging, is a key reason of the improved agreement, and a crucial require-ment when crowdsourcing relevance data.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  performance evaluation (efficiency and effec-tiveness) Click based intent similarity measure; Crowdsourcing vs. trained judges; Single vs pairwise relevance judging
A standard practice of evaluating IR effectiveness is based and similar IR evaluation forums. It involves purposely cre-ated test collections that comprise document corpora, search topics and corresponding relevance judgments [24]. While relevance judgments play a central role, it is well-known that they are subjective, leading to assessor disagreements. For instance, the reported agreement between two TREC-5 assessors on binary judgments was 70-80% on average, but this varied greatly per topic [24]. Depending upon which as-sessor is then used for computing performance metrics, the reliability of an evaluation may also vary [19].

Recently, preference judging has been proposed as an al-ternative to the traditional method of judging individual documents. It has been shown to be a more natural user task and one that leads to higher inter-assessor agreement levels, as well as increased measurement sensitivity [4]. These prop-erties are especially desirable with the increasing adoption of crowdsourcing methods for collecting relevance judgments [2, 18]. In crowdsourcing, since crowd workers are rarely trained as relevance judges and have diverse backgrounds and motivations to engage in crowdsourcing tasks, there is an increased chance of noise and assessor disagreement.
Despite the various reports on assessor disagreement, for trained or crowd judges, little work has been done on char-acterizing disagreement. In particular, very little is known about why preference judging reduces assessor disagreement and whether better agreement among assessors also means better agreement with user satisfaction, as signaled by user clicks. Even less is known about what role user intent may play in judging behavior. For example, judges may agree better when rating pairs of documents that satisfy more sim-ilar or more diverse intents.

In this paper, we examine the relationship between as-sessor disagreement and various click based measures, such as click preference strength and user intent similarity, for judgments collected from editorial judges and crowd work-ers using absolute and preference based methods in order to answer the following research questions: To facilitate our investigation, we create a data set consisting o f pairs of web search results with different click characteris-tics and gather absolute and preference relevance judgments using a single and a pairwise judging interface, both from editorial judges and crowd workers. We analyze the col-lected relevance labels and calculate both inter-assessor and assessor-user (click) agreement statistics over bins of click preference and intent similarity scores.

We find that editorial judges are significantly more likely to agree with clicks, indicating a problem in crowd judging. Editorial judges also show high inter-assessor agreement, perhaps due to their shared training. Crowd judges have the lowest inter-assessor agreement and click-agreement in the single judging interface, but can achieve similar levels of click-agreement to that of editorial judges X  when using the pairwise judging interface. Our pairwise judging inter-face gathered one relative preference and two absolute judg-ments for each pair of documents, with both relative and ab-solute judgments being of similar quality according to our analysis. Focusing on user intents, we calculate three dif-ferent types of click-based intent scores and correlate these with the agreement statistics. Our intent analysis shows changing relationships between the intent similarity mea-sures and click-agreement for both judge groups across the single and pairwise judging methods. This suggest that the awareness of different possible intents, enabled by a pairwise judging method, is a key reason of the improved assessor agreement, and a crucial requirement when crowdsourcing relevance data.

The paper is organized as follows. The next section re-views related work. We detail our experiment setup in sec-tion 3 and the results of our analysis in section 4. We close with conclusions and thoughts on future work.
Studies showed that judgments obtained from relevance assessors in the form of preferences tend to have a higher cross-judge agreement rate compared to absolute judgments [4, 25]. Looking at the reasons for why preference judgments lead to reduced assessor disagreement levels, the study in [25] found a strong connection between the rank at which a document is returned by a set of retrieval systems and the likelihood that a second assessor will disagree with the rele-vance assessment of the initial assessor. Similarly, we study assessor disagreement, but focusing on click based signals that characterize the queries and search results.
Despite the benefits of preference judging, most IR met-rics require absolute judgments as they are not defined over preferences. Hence, a significant amount of work focused on using click data to estimate relevance and to infer absolute labels for documents [9, 1]. We show that pairwise absolute labels have similar agreement levels to preference judgments.
The relationship between clicks and judgments has at-tracted a lot of attention in the literature due to the high cost of obtaining relevance judgments. Although click data is cheap and plentiful, it is inherently noisy and biased [6]. However, research showed that, while the interpretation of clicks as absolute relevance judgments is difficult, the rela-tive preferences derived from clicks are reasonably accurate on average [14]. Extending this work, Hofmann et al. [11] proposed a probabilistic method to infer preferences from clicks. We follow their approach and use click data to in-fer preferences between two documents. We then compare clicks and judgments over pairs of search results using the obtained relative preference relations.

Apart from inferring the relevance of documents, click data has also been used to indicate whether two documents are duplicates of each other from the user X  X  perspective [21]. Two URLs are likely substitutable if, when presented with both, users click whichever they see first. Whereas if two documents satisfy different intents, then users tend to skip to the document that corresponds to their intent. Here we extend the approach to also consider the case where users tend to click both URLs, which is another indication of sim-ilar intent. An alternate indicator of intent similarity is ac-cording to the bipartite query-click graph, for example, as used for intent clustering in [23]. In our case, we consider URLs u and v to satisfy similar intents if the adjacent query set of u intersects with the adjacent query set of v , according to Jaccard similarity.

Judgments are traditionally obtained from trained edito-rial judges. Recent years have, however, seen a rapid adop-tion of crowdsourcing methods [12, 20, 7] in IR evaluation [3, 10, 16, 2, 18]. With the appearance of crowdsourc-crowdsourcing has become broadly accessible. Employers can easily post specific jobs in the form of Human Intelli-gence Tasks (HITs) and engage a large population of workers within a short period of time at a relatively low cost. How-ever, crowd workers are rarely trained as relevance judges and have diverse backgrounds and motivations to engage in crowdsourcing tasks. Consequently, the resulting labels may be of varied quality. Moreover, recent research has also re-ported on random behavior by dishonest workers, cheating, and adversarial conduct [26, 17, 8, 15, 16].

Our work builds on the above works and complements them by studying why relevance judgments -both by ed-itorial judges and crowd workers -may disagree with user clicks, paying special attention to click characteristics and user intent.
Our goal is to investigate inter-assessor and assessor-user (click) disagreement when judging web search results with different click and intent characteristics. By click character-istics, we mean click-based evidence that one search result is preferred by users to some degree over the other, given a query and a pair of results extracted from impressions shown to real web users. The user intent dimension, also based on web users X  click behavior, reflects whether one search result satisfies a similar or different intent from the other.
We study judgments by different groups of judges and two different judging methods: absolute and preference based. For our experiments, we sample a balanced set of queries and their search results that exhibit different click characteristics from the query logs of Bing. We then collect relevance labels for these from professional judges and crowd workers, using three different HIT designs: 1) a single judging UI where judges provide an absolute rating for a single search result for a given query, 2) a pairwise judging UI where a pair of results are judged simultaneously, collecting both absolute and preference judgments, and 3) a serialized pairwise UI, which collects absolute judgments for a pair of search results that are shown to the assessor one after the other. The pur-pose of the latter test is to ensure that possible differences b etween the single and pairwise experiments are not due to the fact that the same judge is assessing both pages in the pairwise condition while different judges may be assigned to the same pair in the single judging case. We analyze the col-lected relevance labels by comparing them to click evidence over bins of click preference and intent similarity scores. We detail each of these aspects of the experimental setup below.
We measure the degree to which one search result (URL for short) is preferred by web users over another, and define our click preference strength metric as the probability of one search result being clicked over the other. In addition, we consider click volume as a second dimension, reflecting the strength of evidence in the click preference score.
Inspired by previous work [13, 14, 22, 21], our click prefer-ence strength measure is defined as the proportion of times web users prefer one search result, URL u , over another, URL v , for a given query, by solely clicking on u even though both results are observed. In order to identify the cases where both u and v are observed by the user, we focus only on the cases where u and v are presented in consecutive rank positions (regardless of the order in which they are presented) and where at least one result ( u or v ) is clicked by the user. This way, we can reasonably guarantee that both search results are seen by the user [14]. We then com-pute the click preference strength of u over v , ( P uv ), as the proportion of times only u is clicked by the user minus the proportion of times only v is clicked. We use c  X  uv to denote the number of times when the two results were shown with u immediately above v (e.g., u at rank 2 and v at rank 3), where u was clicked and v was not clicked. Similarly, let c be the number of times v was clicked and u was not, and c The same logic applies to when v is ranked above u . Then, P uv can be computed as: Note that P uv is a value that ranges between  X  1 and 1, with 1 implying a perfect user preference for u over v , while  X  1 a perfect preference for v over u . In general, the closer P is to 1 (or  X  1), the stronger the user preference between the two results, with 0 implying no preference. It can be seen from the definition of the statistic that P uv = 1  X  P vu in order to keep the preference strength value in the { 0 , 1 } range, we can simply swap the two URLs when P uv &lt; 0.
For our experiments, we create 11 bins of possible P uv value of P uv = 0, while the remaining 10 bins cover prob-ability values in the ( bin i includes P uv values of (0 . 2 , 0 . 3].
From Equation 1, we can see that the same value of click preference strength can be obtained for a pair of frequently clicked results of popular head queries as well as for a pair of less frequently clicked URLs of rare tail queries. In order to differentiate between such cases, we include click volume as a control parameter in our experiments and incorporate it into our sampling strategy, see Section 3.3.1. Similarly to the bins we created for the click preference strength, we bucket the click volume data into four bins: 50,500,5k, 50k for click volumes of [1 , 50), [50 , 500), [500 , 5 000) and over 5,000 number of clicks, respectively.
Click data can also be used to compute whether two doc-uments are redundant (or duplicates of each other) [21] or whether they are about the same intent [23]. In order to compute the redundancy score between two documents, we adopt the approach proposed by Radlinski et al. [21]. Us-ing the notation introduced in Section 3.1.1, and given two search results, u and v , that are presented in adjacent rank positions for the same query q , the redundancy (dupe) score is computed as:
In the work by Radlinski et al. [21], a redundancy score above 0.8 was shown to indicate that the two web pages are very likely to be dupes of each other.
In order to compute our intent similarity score between two web pages u and v , i.e., the score that the two pages are about the same intent, we use the fact that two URLs will be clicked together (co-clicked) for the same query if they are about the same intent of the query. Note that another reason for two URLs to be co-clicked could be due to one URL being irrelevant. To avoid this issue, we focus on the cases where u and v are shown to users in both orders ( u before v and v before u ) and both URLs always get clicked. Based on this, we define the intent similarity score I uv between two URLs u and v by modifying the dupe score measure [21] as follows:
Another click-based indication of the relationship between two URLs is based on a broader set of queries for which we have observed each URL being clicked. If the two sets of queries are largely overlapping, this can be seen as an indication that the two URLs cover similar topics (e.g., both URLs have been clicked for the queries rental car and hire cars ). If the two URLs are clicked for very different sets of queries, that is some evidence that the URLs cover different topics (one URL was clicked for the query jaguar car , the other for jaguar cat ). We measure this relationship via the Jaccard similarity between the two sets of queries associated with the pairs of URLs, which is the size of the intersection divided by the size of the union.
We use the click logs of the Bing search engine and employ a stratified sampling method in order to compile a suitable data set that covers the full range of click characteristics
W e report Fleiss kappa values for inter-assessor agreement over the individual relevance labels assigned by different judges to a given query-URL pair (l), the explicit preference judgments that judges provided for query-URL-URL tuples (e), and the implicit preference labels (i) derived based on the procedure described in section 3.6.

In all cases, we observe significantly higher inter-assessor agreement levels for trained judges than for crowd workers, e.g., 0.43 vs 0.12 in the single UI. One possible reason for the differences between the judge groups is that crowd workers are not trained and thus their judgments are not calibrated, while trained judges are more likely to assign the same label. We will return to this question in section 4.2.3.
Looking at the agreement levels over the implicit pref-erence judgments (i) in the single vs pairwise judging UIs, we see improved agreement when using the latter approach:  X  xSC = 0 . 24 &lt;  X  xP C = 0 . 29 for crowd workers and  X  0 . 51 &lt;  X  xP E = 0 . 57 for trained judges. We note that the agreements over the implicit preference judgments in the Cartesian sets (xSC, xPC, etc.) are somewhat optimistic (e.g., the true SPC (i) is 0.17 while xSPC (i) overestimates this as 0.20) due to the duplicate query-URL pairs. However, their comparative evaluation remains reasonable. Kappa values are lowest in the SPC case, when the same judge rates both URLs of a query-URL-URL tuple, but has to rate them one after the other. This confirms that the dif-ferences between the single and pairwise judging methods is not a result of the same or different judges being assigned to one or both the URLs in a given tuple.
In all experiments, the editorial judges agree significantly more with user click preferences than crowd workers do: 58% (xSE) vs 45% (xSC) in the single UI, and 62% (PE) vs 55% (PC) in the pairwise experiments, respectively, with differences being statistically significant between the edito-rial judgments using the pairwise UI (PE, xPE) and the crowd judgments using the single (xSC) or the serialized pairwise UI (SPC, xSPC) ( p &lt; 0 . 05, chi-square test).
The observed differences across the two judging UIs in-dicate that both groups agree better with user clicks when using a pairwise judging method: click-agreement of 58% (xSE) vs 62% (PE) and 66% (xPE) for trained judges and 45% (xSC) vs 55% (PC) and 56% (xPC) for crowd workers. One hypothesis is that by showing two results in a single HIT, judges may be able to self-calibrate. This is however not supported by the inter-assessor agreement scores over the relevance ratings, which indicate no real improvement across the two UIs:  X  SC = 0 . 12 (l) vs  X  xP C = 0 . 13 (l) for crowd and  X  SE = 0 . 43 (l) vs  X  xP E = 0 . 46 (l) for editorial judges. An alternative hypothesis is that the presence of two web pages allows judges to contextualize and weigh up the two results in relation to each other and thus label them more in agreement with click preferences. We will further explore these two hypotheses in sections 4.2.3 and 4.3. As with the kappa values, the serialized pairwise UI has the worst click-agreement rate.
In this section, we examine the relationship between the click preference strength and click volume indicators and Table 2: Inter-assessor agreement (Fleiss kappa) and click-agreement statistics (as %) for the single (S), pairwise (P) and serialized pairwise (SP) experi-ments with crowd workers (C) or editorial judges (E), over relevance labels (l), implicit preference judgments (i) and explicit preference judgments (e) ID # Kappa Agree Undet X  Disagree
SC 5,601 0.12 (l) ---xSC 9,172 0.24 (i) 45 27 28
SE 5,652 0.43 (l) ---xSE 9,310 0.51 (i) 58 21 21
PC 3,118 0.13 (l) 55 21 24 xPC 19,697 0.29 (i) 56 24 20
PE 3,064 0.46 (l) 62 18 20 xPE 19,456 0.57 (i) 66 18 16
SPC 3,015 0.09 (l) 42 20 38 xSPC 19,221 0.20 (i) 47 28 25 inter-assessor and assessor-user agreement statistics with the a im to answer our second research question whether asses-sors agree more with each other and with users when click based evidence suggests stronger preference for one search result over another. For the sake of comparability, we focus on the Cartesian sets and agreement values based on the implicit preference judgments. We omit the plots for the se-rialized pairwise judging case as they look almost identical to the xSC plots.
Figure 3 shows the Fleiss kappa inter-assessor agreement statistic trend lines over the click preference strength bins (x axis) across the single and pairwise experiments, bro-ken down by click volume bins (series). A striking finding is the difference between the agreement trends for crowd and trained judges. While we observe no relationship be-tween inter-assessor agreement and click preference strength or click volume for crowd workers (shallow or flat lines in Figure 3a,c), there is a clear connection for trained judges (Figure 3b,d): agreement among trained assessors increases with higher P uv probability. This means that trained judges are more likely to assign the same relative labels to pairs of search results that are characterized by bigger differences in web users X  click behavior. Looking at the trends observed for the different click volume bins for trained judges (xSE and xPE), we see that judges are not able to better agree with each other despite web users X  increasing click preference when the volume of clicks is in the 50 to 5k range. This is interesting as the majority of the queries in these bins are navigational, e.g.,  X  X ommercebank X  or  X  X he weather chan-nel X , with most having a valid informational interpretation, e.g., the Wikipedia page for Commerce Bank.

Comparing the single and pairwise judging methods, we observe a slight shift, with the crowd doing better on low click volume URL pairs in the pairwise UI than in the single UI. Editorial judges improve for all click volume bins, with different shifts in the angles of the trend lines for the differ-ent bins. This means that the relationship between trained judges X  inter-judge agreement levels and the probability of one URL being preferred by web users changes depending on the volume of user clicks across the two judging UIs. C ertainly, a pairwise UI can help to expose differences be-tween two web pages, and from the trend lines for xPE, it is also clear that such differences are much easier to spot by the judges with higher P uv and click volume values. The nature of the relationship between P uv , click volume and inter-assessor agreement for explicit preference judgments is very similar to when using pairwise judging with pairs of absolute labels (xPC/E), thus we omit the plots. F igure 3: Inter-assessor agreement trends over click preference strength ( P uv ) bins (x) and click volume bins (series) for the single (S) and pairwise (P) judg-ing with crowd (C) and editorial judges (E)
Figure 4 shows the assessor-user (click) agreement statis-tic trends over the click preference strength bins (x axis) across the 4 experiments. We can clearly see that in all cases agreement increases and disagreement drops with the strength of the click preference increasing. The relation-ship is the weakest for xSC (and xPSC, not shown). The increasing agreement trends suggest that judges are better at assessing the relevance of web pages that are different from each other in terms of user click behavior. For exam-ple, these pages may satisfy different user intents or reflect different interpretations of an ambiguous query. We will at-tempt to gain insights into this in section 4.3. The main finding, however, is that crowd judges X  agreement with web users is brought to a similar level as that of the editorial judges when using a pairwise judging UI, despite the lack of improvement we saw in inter-assessor agreement levels between xSC and xPC.

Figure 5 shows the click-agreement trends (for clarity, only agreement is shown), broken down by the click volume bins (series). I.e., the single  X  X gree X  (blue) series in Figure 4a is the aggregate of the four click volume based series in Figure 5a. We observe similar trends over all sets, but to varying degrees. The differences in the angles of the trend F igure 4: Click-agreement trends over click prefer-ence strength ( P uv ) bins (x) for the single (S) and pairwise (P) experiments with crowd (C) and edito-rial judges (E) F igure 5: Click-agreement trends over click prefer-ence strength ( P uv ) bins (x) and click volume bins (series) for the single (S) and pairwise (P) experi-ments with crowd (C) and editorial judges (E) lines for different click volume bins indicate that both judge groups agree better with user clicks when click volume is higher. The flatter lines obtained for low click volume bins indicate that the chances of the judges agreeing with users in these cases is independent of the strength of the click pref-erence. This is somewhat expected since low click volumes workers X  inter-judge agreement trend for the pairwise UI is t he opposite: they agree less with each other as the pairs of pages become more dupe-like.
Figure 8 shows the click-agreement scores obtained for the different intent measures. Similar to the observed inter-assessor trends, we see that judges, in general, are more likely to disagree with web users X  click signals when the pairs of web pages have more similar topics (query overlap). One exception is the pairwise judging condition when edito-rial judges maintain the same high levels of click-agreement, highlighting the benefits of pairwise judging. On the other hand, the relationship between our intent score and click-agreement shows a different story from that observed for inter-assessor agreement. The intent score does not mirror the query overlap measure in this case, but instead shows better click-agreement as the URLs become more similar in the intent they satisfy (except for xSE). The dupe score highlights another difference between the single and pair-wise judging methods: in xSC, the more interchangeable the pairs of web pages are, the more likely that crowd work-ers disagree with user clicks, while this trend flips for the pairwise UI. This is interesting as now crowd workers dis-play the same behavior as editorial judges in terms of their affinity to label dupes. F igure 8: Click-agreement statistic trends over in-tent bins (x axis) for the different intent scores (se-ries) for the single (S) and pairwise (P) experiments with crowd (C) and editorial judges (E)
Furthermore, comparing the different intent trends be-tween the single and pairwise settings, we observe a strong dependency between the dupe and query overlap scores and the click-agreement statistics (stat. sig. for all, except in xSC). This suggests that the pairwise UI exposes proper-ties of the web pages that can then improve judging quality when faced with more interchangeable documents, leading to better agreement with web users. Overall, the differences between intent and agreement score trends across the single and pairwise cases suggest that it is in fact the contextual-ization of one web result with respect to another that helps judges in labeling search results such a way that their judg-ments better agree with user clicks.
In this paper, we studied how assessor agreement by dif-ferent groups of judges in different judging modes compare to users X  opinions expressed via clicks. We collected abso-lute and preference judgments both from crowd workers and editorial judges, using a single and a pairwise judging UI for a set of query-URL-URL tuples with different click profiles.
We found that crowd judges have lower inter-assessor agree-ment and lower click-agreement levels, compared to editorial judges. The two judge groups differ not only in their employ-ment status, but also in their access to judging guidelines and their level of training and experience. It is plausible that guidelines and training helped the editorial judges not only with consistency but with understanding of user needs.
When using a pairwise judging interface, the greatest im-provement was with click-agreement for crowd judges, with significant differences between the single and pairwise inter-faces. The observed improvement in the lower click prefer-ence strength and click volume bins is especially encouraging as these lower values dominate the click-characteristic dis-tribution in the full click log data. This gain was, however, not accompanied by a corresponding improvement in inter-assessor agreement. So the crowd didn X  X  achieve a calibrated consistency like editorial judges, but seeing two documents at once may have helped the crowd to better judge their relative value to real users.

Our pairwise judging interface yields one relative prefer-ence and two absolute judgments per document pair. Our results throughout are consistent between the relative judg-ment and pairwise absolute judgment. For many IR applica-tions, the absolute judgments are more useful, for example to create a reusable test collection.

In cases where user clicks had a strong preference, all types of judges were more likely to agree with the clicks. Strong preferences were easier for judges to understand, and the ef-fect is stronger for pairs with more click observations. Inter-assessor agreement also improved with strong click prefer-ence for trained judges, but not for crowd judges.
Our intent analysis uncovered several differences between the behavior of judges across the two judging interfaces. While crowd workers agreement with user clicks showed no relation with the intent scores in the single judging UI, the relationship changed to closely match that of the editorial judges when switching to the pairwise UI. This suggests that it is the context and reference point that another search re-sult can provide that can help judges in their assessing task and not so much the training itself. This leads to the rec-ommendation to change the current practices of relevance data gathering, especially for crowdsourcing, moving to a pairwise interface.

This current work focused on the relevance judgments, as ground truth for the evaluation of search engines, and their relationship with user clicks. What we have not discussed is the reliability of the click data in the first place. Although we only made use of click preference information, which has been shown to be more reliable [4], cases of disagreements between judges and users remain a question for the future. [ 1] R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra, [2] O. Alonso and R. A. Baeza-Yates. Design and [3] O. Alonso and S. Mizzaro. Can we get rid of TREC [4] B. Carterette, P. N. Bennett, D. M. Chickering, and [5] C. W. Cleverdon. The Cranfield tests on index [6] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [7] A. Doan, R. Ramakrishnan, and A. Y. Halevy.
 [8] J. S. Downs, M. B. Holbrook, S. Sheng, and L. F. [9] G. Dupret and C. Liao. A model to estimate intrinsic [10] C. Grady and M. Lease. Crowdsourcing document [11] K. Hofmann, S. Whiteson, and M. de Rijke. A [12] J. Howe. Crowdsourcing: Why the Power of the Crowd [13] T. Joachims. Optimizing search engines using [14] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [15] A. Kapelner and D. Chandler. Preventing satisficing [16] G. Kazai, J. Kamps, M. Koolen, and [17] J. Le, A. Edmonds, V. Hester, and L. Biewald. [18] M. Lease and G. Kazai. Overview of the TREC 2011 [19] D. W. Oard, B. Hedin, S. Tomlinson, and J. R. Baron. [20] A. J. Quinn and B. B. Bederson. Human computation: [21] F. Radlinski, P. N. Bennett, and E. Yilmaz. Detecting [22] F. Radlinski and T. Joachims. Minimally invasive [23] F. Radlinski, M. Szummer, and N. Craswell. Inferring [24] E. M. Voorhees and D. K. Harman, editors. TREC: [25] W. Webber, P. Chandar, and B. Carterette.
 [26] D. Zhu and B. Carterette. An analysis of assessor
