 Q-learning is a popular reinforcement learning algorithm t hat was proposed by Watkins [1] and can be used to optimally solve Markov Decision Processes (MDPs) [2]. We show that Q-learning X  X  performance can be poor in stochastic MDPs because of large o verestimations of the action val-ues. We discuss why this occurs and propose an algorithm call ed Double Q-learning to avoid this overestimation. The update of Q-learning is The next state s t +1 is determined by a fixed state transition distribution P : S  X  A  X  S  X  [0 , 1] , rewards and transitions in order to converge in the limit to t he optimal action value function. This optimal value function is the solution to the following set o f equations [3]: lem that is to be solved, weighing immediate rewards more hea vily than later rewards. Second, in defined. It has been proven that Q-learning reaches the optim al value function Q  X  with probability one in the limit under some mild conditions on the learning ra tes and exploration policy [4 X 6]. Q-learning has been used to find solutions on many problems [7  X 9] and was an inspiration to similar algorithms, such as Delayed Q-learning [10], Phased Q-lear ning [11] and Fitted Q-iteration [12], to name some. These variations have mostly been proposed in o rder to speed up convergence rates compared to the original algorithm. The convergence rate of Q-learning can be exponential in the number of experiences [13], although this is dependent on th e learning rates and with a proper choice of learning rates convergence in polynomial time can be obta ined [14]. The variants named above can also claim polynomial time convergence.
 Contributions An important aspect of the Q-learning algorithm has been ove rlooked in previous work: the use of the max operator to determine the value of the next state can cause large over-estimations of the action values. We show that Q-learning ca n suffer a large performance penalty because of a positive bias that results from using the maximu m value as approximation for the max-imum expected value. We propose an alternative double estim ator method to find an estimate for the maximum value of a set of stochastic values and we show tha t this sometimes underestimates rather than overestimates the maximum expected value. We us e this to construct the new Double Q-learning algorithm.
 The paper is organized as follows. In the second section, we a nalyze two methods to approximate the maximum expected value of a set of random variables. In Se ction 3 we present the Double Q-learning algorithm that extends our analysis in Section 2 and avoids overestimations. The new algorithm is proven to converge to the optimal solution in th e limit. In Section 4 we show the results on some experiments to compare these algorithms. Some gener al discussion is presented in Section 5 and Section 6 concludes the paper with some pointers to futu re work. In this section, we analyze two methods to find an approximati on for the maximum expected value of a set of random variables. The single estimator method use s the maximum of a set of estimators as an approximation. This approach to approximate the value of the maximum expected value is positively biased, as discussed in previous work in economi cs [15] and decision making [16]. It is a bias related to the Winner X  X  Curse in auctions [17, 18] and it can be shown to follow from Jensen X  X  inequality [19]. The double estimator method uses two estim ates for each variable and uncouples the selection of an estimator and its value. We are unaware of previous work that discusses it. We analyze this method and show that it can have a negative bias.
 Consider a set of M random variables X = { X 1 , . . . , X M } . In many problems, one is interested in the maximum expected value of the variables in such a set: Without knowledge of the functional form and parameters of t he underlying distributions of the variables in X , it is impossible to determine (3) exactly. Most often, this value is approximated by constructing approximations for E { X i } for all i . Let S = S M i =1 S i denote a set of samples, where S i is the subset containing samples for the variable X i . We assume that the samples in S i are independent and identically distributed (iid). Unbias ed estimates for the expected values can be obtained by computing the sample average for each variabl e: E { X i } = E { i }  X  i ( S ) def = sample s  X  S i is an unbiased estimate for the value of E { X i } . The error in the approximation thus consists solely of the variance in the estimator and decreas es when we obtain more samples. We use the following notations: f i denotes the probability density function (PDF) of the i th variable X i and F i ( x ) = R the PDF and CDF of the i th estimator are denoted f  X  i and F  X  i . The maximum expected value can be expressed in terms of the underlying PDFs as max i E { X i } = max i R  X   X  X  X  x f i ( x ) dx . 2.1 The Single Estimator An obvious way to approximate the value in (3) is to use the val ue of the maximal estimator: Because we contrast this method later with a method that uses two estimators for each variable, we call this method the single estimator . Q-learning uses this method to approximate the value of the next state by maximizing over the estimated action values in that state. The maximal estimator max i i is distributed according to some PDF f  X  max that is dependent on the ability that the maximum estimate is lower or equal to x . This probability is equal to the probability Q which can thus be given by However, in (3) the order of the max operator and the expectat ion operator is the other way around. This makes the maximal estimator max i i ( S ) a biased estimate for max i E { X i } . This result has been proven in previous work [16]. A generalization of this p roof is included in the supplementary material accompanying this paper. 2.2 The Double Estimator The overestimation that results from the single estimator a pproach can have a large negative impact on algorithms that use this method, such as Q-learning. Ther efore, we look at an alternative method Both sets of estimators are updated with a subset of the sampl es we draw, such that S = S A  X  S B and S A  X  S B =  X  and A i ( S ) = 1 , both A i and B i are unbiased if we assume that the samples are split in a prope r manner, for be the set of maximal estimates in A ( S ) . Since B is an independent, unbiased set of estimators, pick one at random. Then we can use B a  X  as an estimate for max i E { B i } and therefore also for max i E { X i } and we obtain the approximation As we gain more samples the variance of the estimators decrea ses. In the limit, A i ( S ) = B i ( S ) = E { X i } for all i and the approximation in (6) converges to the correct result .
 Assume that the underlying PDFs are continuous. The probabi lity P ( j = a  X  ) for any j is then equal to the probability that all i 6 = j give lower estimates. Thus A j ( S ) = x is maximal for some of A i . The expected value of the approximation by the double estim ator can thus be given by For discrete PDFs the probability that two or more estimator s are equal should be taken into account and the integrals should be replaced with sums. These change s are straightforward.
 Comparing (7) to (5), we see the difference is that the double estimator uses E { B j } in place of x . The single estimator overestimates, because x is within integral and therefore correlates with the monotonically increasing product Q i 6 = j F  X  i ( x ) . The double estimator underestimates because the probabilities P ( j = a  X  ) sum to one and therefore the approximation is a weighted esti mate of unbiased expected values, which must be lower or equal to the maximum expected value. In the following lemma, which holds in both the discrete and the con tinuous case, we prove in general that the estimate E { B a  X  } is not an unbiased estimate of max i E { X i } . values. Let a  X  be an element that maximizes A : A a  X  = max i A i . Then E { B a  X  } = E { X a  X  }  X  max i E { X i } . Furthermore, the inequality is strict if and only if P ( a  X  /  X  X  ) &gt; 0 . Proof. Assume a  X   X  M . Then E { B a  X  } = E { X a  X  } def = max i E { X i } . Now assume a  X  /  X  M and mutually exclusive, so the combined expectation can be expr essed as
E { B a  X  } = P ( a  X   X  X  ) E { B a  X  | a  X   X  X } + P ( a  X  /  X  X  ) E { B a  X  | a  X  /  X  X } different expected values, but their distributions overla p. In contrast with the single estimator, the double estimator is unbiased when the variables are iid, sin ce then all expected values are equal and P ( a  X   X  X  ) = 1 . We can interpret Q-learning as using the single estimator to estimate the value of the next of the same experiment and not X  X s it is often used in a reinforc ement learning context X  X s the expectation over the next state, which we will encounter in the next subsection as E {| P t } . Therefore, max a Q t ( s t +1 , a ) is an unbiased sample, drawn from an iid distribution with me an E { max a Q t ( s t +1 , a ) } . In the next section we show empirically that because of this Q-learning can indeed suffer from large overestimations. In this secti on we present an algorithm to avoid these overestimation issues. The algorithm is called Double Q-le arning and is shown in Algorithm 1. Double Q-learning stores two Q functions: Q A and Q B . Each Q function is updated with a value from the other Q function for the next state. The action a  X  in line 6 is the maximal valued action updated on the same problem, but with a different set of exper ience samples, this can be considered It is important that both Q functions learn from separate set s of experiences, but to select an action to perform one can use both value functions. Therefore, this algorithm is not less data-efficient than Q-learning. In our experiments, we calculated the average o f the two Q values for each action and then performed  X  -greedy exploration with the resulting average Q values.
 Double Q-learning is not a full solution to the problem of find ing the maximum of the expected values of the actions. Similar to the double estimator in Sec tion 2, action a  X  may not be the ac-max a E { Q A ( s  X  , a  X  ) } , and underestimations of the action values can occur. 3.1 Convergence in the Limit In this subsection we show that in the limit Double Q-learnin g converges to the optimal policy. Intuitively, this is what one would expect: Q-learning is ba sed on the single estimator and Double Q-learning is based on the double estimator and in Section 2 w e argued that the estimates by the single and double estimator both converge to the same answer in the limit. However, this argument does not transfer immediately to bootstrapping action valu es, so we prove this result making use of the following lemma which was also used to prove convergence of Sarsa [20]. Algorithm 1 Double Q-learning 1: Initialize Q A , Q B , s 2: repeat 3: Choose a , based on Q A ( s, ) and Q B ( s, ) , observe r , s  X  4: Choose (e.g. random) either UPDATE(A) or UPDATE(B) 5: if UPDATE(A) then 6: Define a  X  = arg max a Q A ( s  X  , a ) 7: Q A ( s, a )  X  Q A ( s, a ) +  X  ( s, a ) r +  X Q B ( s  X  , a  X  )  X  Q A ( s, a ) 8: else if UPDATE(B) then 9: Define b  X  = arg max a Q B ( s  X  , a ) 10: Q B ( s, a )  X  Q B ( s, a ) +  X  ( s, a )( r +  X Q A ( s  X  , b  X  )  X  Q B ( s, a )) 11: end if 12: s  X  s  X  13: until end equations:  X  0 are P 0 -measurable and  X  t ,  X  t and F t  X  1 are P t -measurable, t = 1 , 2 , . . . . Assume that the maximum norm. Then  X  t converges to zero with probability one.
 We use this lemma to prove convergence of Double Q-learning u nder similar conditions as Q-learning. Our theorem is as follows: Theorem 1. Assume the conditions below are fulfilled. Then, in a given er godic MDP, both Q A and Q B as updated by Double Q-learning as described in Algorithm 1 w ill converge to the optimal value function Q  X  as given in the Bellman optimality equation (2) with probabi lity one if an infinite number of experiences in the form of rewards and state transitions f or each state action pair are given by a proper learning policy. The additional conditions are: 1) The MDP is finite, i.e. | S  X  A | &lt;  X  . 2)  X   X  [0 , 1) . 3) The Q values are stored in a lookup table. 4) Both Q A and Q B receive an  X  ( s, a ) 6 = ( s t , a t ) :  X  t ( s, a ) = 0 . 6)  X  s, a, s  X  : Var { R s  X  sa } &lt;  X  . A  X  X roper X  learning policy ensures that each state action pa ir is visited an infinite number of times. For instance, in a communicating MDP proper policies includ e a random policy.
 Sketch of the proof. We sketch how to apply Lemma 2 to prove Theorem 1 without going into full technical detail. Because of the symmetry in the updates on t he functions Q A and Q B it suffices r Q t ( s t , a t ) , where a of the lemma hold. The fourth condition of the lemma holds as a consequence of the boundedness condition on the variance of the rewards in the theorem.
 This leaves to show that the third condition on the expected c ontraction of F t holds. We can write c zero. Depending on whether Q B or Q A is updated, the update of  X  BA t at time t is either t ( s t , a t ) . We define  X  BA t = that the selection whether to update Q A or Q B is independent on the sample (e.g. random). Clearly, one of the two assumptions must hold at each time ste p and in both cases we obtain the zero, which in turn ensures that the original process also co nverges in the limit. practical comparison with Double Q-learning. The settings are simple to allow an easy interpretation of what is happening. Double Q-learning scales to larger pro blems and continuous spaces in the same way as Q-learning, so our focus here is explicitly on the bias of the algorithms. The settings are the gambling game of roulette and a small gri d world. There is considerable ran-domness in the rewards, and as a result we will see that indeed Q-learning performs poorly. The discount factor was 0 . 95 in all experiments. We conducted two experiments on each pro blem. The where n A t and n B t store the number of updates for each action for the correspon ding value function. The polynomial learning rate was shown in previous work to be better in theory and in practice [14]. 4.1 Roulette In roulette, a player chooses between 170 betting actions, i ncluding betting on a number, on either of the colors black or red, and so on. The payoff for each of the se bets is chosen such that almost all bets have an expected payout of 1 38 $36 = $0.947 per dollar, resulting in an expected loss of -$0. 053 per play if we assume the player bets $1 every time. 1 We assume all betting actions transition back to the same state and there is one action that stops playing, y ielding $0. We ignore the available funds of the player as a factor and assume he bets $1 each turn.
 Figure 1 shows the mean action values over all actions, as fou nd by Q-learning and Double Q-learning. Each trial consisted of a synchronous update of al l 171 actions. After 100,000 trials, Q-learning with a linear learning rate values all betting ac tions at more than $20 and there is little progress. With polynomial learning rates the performance i mproves, but Double Q-learning con-verges much more quickly. The average estimates of Q-learni ng are not poor because of a few poorly estimated outliers. After 100,000 trials Q-learnin g valued all non-terminating actions be-tween $22.63 and $22.67 for linear learning rates and betwee n $9.58 to $9.64 for polynomial rates. In this setting Double Q-learning does not suffer from signi ficant underestimations. Figure 1: The average action values according to Q-learning and Double Q-learning when playing roulette. The  X  X alk-away X  action is worth $0. Averaged over 10 experiments.
 Figure 2: Results in the grid world for Q-learning and Double Q-learning. The first row shows average rewards per time step. The second row shows the maxim al action value in the starting state S. Averaged over 10,000 experiments. 4.2 Grid World Consider the small grid world MDP as show in Figure 2. Each sta te has 4 actions, corresponding to the directions the agent can go. The starting state is in the l ower left position and the goal state is in the upper right. Each time the agent selects an action that walks off the grid, the agent stays in the same state. Each non-terminating step, the agent receiv es a random reward of  X  12 or +10 with equal probability. In the goal state every action yields +5 and ends an episode. The optimal policy ends an episode after five actions, so the optimal average rew ard per step is +0 . 2 . The exploration assuring infinite exploration in the limit which is a theoret ical requirement for the convergence of both Q-learning and Double Q-learning. Such an  X  -greedy setting is beneficial for Q-learning, since this implies that actions with large overestimations are se lected more often than realistically valued actions. This can reduce the overestimation.
 Figure 2 shows the average rewards in the first row and the maxi mum action value in the starting state in the second row. Double Q-learning performs much bet ter in terms of its average rewards, but this does not imply that the estimations of the action val ues are accurate. The optimal value of the second row of Figure 2 with a horizontal dotted line. We se e Double Q-learning does not get much closer to this value in 10 , 000 learning steps than Q-learning. However, even if the error o f the action values is comparable, the policies found by Double Q-learning are clearly much better. We note an important difference between the well known heuri stic exploration technique of opti-mism in the face of uncertainty [21, 22] and the overestimati on bias. Optimism about uncertain events can be beneficial, but Q-learning can overestimate ac tions that have been tried often and the estimations can be higher than any realistic optimistic est imate. For instance, in roulette our initial action value estimate of $0 can be considered optimistic, si nce no action has an actual expected value higher than this. However, even after trying 100,000 action s Q-learning on average estimated each gambling action to be worth almost $10. In contrast, althoug h Double Q-learning can underestimate the values of some actions, it is easy to set the initial actio n values high enough to ensure optimism for actions that have experienced limited updates. Therefo re, the use of the technique of optimism in the face of uncertainty can be thought of as an orthogonal con cept to the over-and underestimation that is the topic of this paper.
 The analysis in this paper is not only applicable to Q-learni ng. For instance, in a recent paper on multi-armed bandit problems, methods were proposed to ex ploit structure in the form of the presence of clusters of correlated arms in order to speed up c onvergence and reduce total regret [23]. The value of such a cluster in itself is an estimation ta sk and the proposed methods included taking the mean value, which would result in an underestimat ion of the actual value, and taking the maximum value, which is a case of the single estimator and res ults in an overestimation. It would be interesting to see how the double estimator approach fare s in such a setting.
 Although the settings in our experiments used stochastic re wards, our analysis is not limited to MDPs with stochastic reward functions. When the rewards are deter ministic but the state transitions are stochastic, the same pattern of overestimations due to this noise can occur and the same conclusions continue to hold. We have presented a new algorithm called Double Q-learning t hat uses a double estimator approach to determine the value of the next state. To our knowledge, th is is the first off-policy value based reinforcement learning algorithm that does not have a posit ive bias in estimating the action values in stochastic environments. According to our analysis, Doubl e Q-learning sometimes underestimates the action values, but does not suffer from the overestimati on bias that Q-learning does. In a roulette game and a maze problem, Double Q-learning was shown to reach good performance levels much more quickly.
 Future work Interesting future work would include research to obtain mo re insight into the merits of the Double Q-learning algorithm. For instance, some prel iminary experiments in the grid world showed that Q-learning performs even worse with higher disc ount factors, but Double Q-learning is virtually unaffected. Additionally, the fact that we can construct positively biased and negatively biased off-policy algorithms raises the question whether i t is also possible to construct an unbi-ased off-policy reinforcement-learning algorithm, witho ut the high variance of unbiased on-policy Monte-Carlo methods [24]. Possibly, this can be done by esti mating the size of the overestimation and deducting this from the estimate. Unfortunately, the si ze of the overestimation is dependent on the number of actions and the unknown distributions of the re wards and transitions, making this a non-trivial extension.
 More analysis on the performance of Q-learning and related a lgorithms such as Fitted Q-iteration [12] and Delayed Q-learning [10] is desirable. For instance , Delayed Q-learning can suffer from similar overestimations, although it does have polynomial convergence guarantees. This is simi-lar to the polynomial learning rates: although performance is improved from an exponential to a polynomial rate [14], the algorithm still suffers from the i nherent overestimation bias due to the sin-gle estimator approach. Furthermore, it would be interesti ng to see how Fitted Double Q-iteration, Delayed Double Q-learning and other extensions of Q-learni ng perform in practice when they are applied to Double Q-learning.
 Acknowledgments The authors wish to thank Marco Wiering and Gerard Vreeswijk for helpful comments. This re-search was made possible thanks to grant 612.066.514 of the d utch organization for scientific re-search (Nederlandse Organisatie voor Wetenschappelijk On derzoek, NWO).
 [1] C. J. C. H. Watkins. Learning from Delayed Rewards . PhD thesis, King X  X  College, Cambridge, [2] C. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning , 8:279 X 292, 1992. [3] R. Bellman. Dynamic Programming . Princeton University Press, 1957. [4] T. Jaakkola, M. I. Jordan, and S. P. Singh. On the converge nce of stochastic iterative dynamic [5] J. N. Tsitsiklis. Asynchronous stochastic approximati on and Q-learning. Machine Learning , [6] M. L. Littman and C. Szepesv  X  ari. A generalized reinforcement-learning model: Converg ence [7] R. H. Crites and A. G. Barto. Improving elevator performa nce using reinforcement learning. In [8] W. D. Smart and L. P. Kaelbling. Effective reinforcement learning for mobile robots. In [9] M. A. Wiering and H. P. van Hasselt. Ensemble algorithms i n reinforcement learning. IEEE [10] A. L. Strehl, L. Li, E. Wiewiora, J. Langford, and M. L. Li ttman. PAC model-free reinforce-[11] M. J. Kearns and S. P. Singh. Finite-sample convergence rates for Q-learning and indirect [12] D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch m ode reinforcement learning. Journal [13] C. Szepesv  X  ari. The asymptotic convergence-rate of Q-learning. In NIPS  X 97: Proceedings of [14] E. Even-Dar and Y. Mansour. Learning rates for Q-learni ng. Journal of Machine Learning [15] E. Van den Steen. Rational overoptimism (and other bias es). American Economic Review , [16] J. E. Smith and R. L. Winkler. The optimizer X  X  curse: Ske pticism and postdecision surprise in [17] E. Capen, R. Clapp, and T. Campbell. Bidding in high risk situations. Journal of Petroleum [18] R. H. Thaler. Anomalies: The winner X  X  curse. Journal of Economic Perspectives , 2(1):191 X  [20] S. P. Singh, T. Jaakkola, M. L. Littman, and C. Szepesv  X  ari. Convergence results for single-step [21] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforc ement learning: A survey. Journal [22] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction . The MIT press, [23] S. Pandey, D. Chakrabarti, and D. Agarwal. Multi-armed bandit problems with dependent [24] W. K. Hastings. Monte Carlo sampling methods using Mark ov chains and their applications.
