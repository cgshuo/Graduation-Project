 1. Introduction
The use of artificial neural network (ANN) as an excellent function approximation technique in tool condition monitoring (TCM) applications has seen a remarkable growth in the last decade or so, with the burgeoning interest of researchers in neural network models beyond the standard multi-layer perceptron (MLP) network trained through the backpropagation algorithm. One of such alternative network models is the RBFN, which has found widespread usage in a number of applications because of the simplicity of its network architecture, reduced computational times, the local data approximation provided by it, and all this without compromising too much with the accuracy in the network performance ( Garg et al., 2007 ). Sonar et al. (2005) applied the RBFN for predicting the surface roughness in a turning process for dry and wet turning of mild steel using HSS and carbide tools, and compared the performance of the studied network with the predetermined performance of a multi-layer perceptron neural network. Moody and Darken (1989) proposed a hybrid learning process for training RBFN with Gaussian basis function, which employs a supervised scheme for updating the output weights, i.e. the weights that connect the RBFs with the output units, and an unsupervised clustering algorithm for determining the centers of the radial basis functions. Amoudi and Zhang (2000) provided an RBFN approach to solar-array modeling and maximum power point prediction, and demanded that their model does not have a local minima problem associated with the conventional models. Ghodsi and Schuurmans (2003) provided a criterion that defines the optimum number of basis functions for the RBF networks. In their work, they used the
Stein X  X  unbiased risk estimator to derive an analytical criterion for assigning the appropriate number of basis functions. Kuo and
Cohen (1999) proposed an on-line tool wear estimation system using the integration of RBFN and fuzzy neural network techniques. They developed an RBF model for recognizing those features extracted using signal processing that corresponded to different amounts of tool wear.

Most of the works employing RBFN have utilized the k -means clustering algorithm ( Moody and Darken, 1989 ) to set up the network architecture, predefined in terms of the number of nodes to be used, in the multidimensional space around the input data.
However RBFN trained by such conventional algorithms may not be as accurate as they require fixing the number of radial basis functions a priori. If the selected number is too small, the function approximation performance of the network may not be satisfac-tory. Also, large network architecture may cause over-fitting of the input data, hence upsetting the global generalization performance. Besides, this translates into a time consuming procedure as it requires the examination of many different network architectures using trial and error. Another drawback of the standard RBF network is that the clustering approach that determines the position of the cluster centers is totally separated from the actual objective, which is to minimize the prediction error, and the cluster centers invariably end up getting deter-mined based on the  X  X ocal X  information only. It is therefore desirable to combine the structure identification with parameter estimation as a single optimization problem. However, combining the two objectives results in a fairly complex problem for standard optimization methods such as steepest gradient descent and other linear programming techniques, which cannot exten-sively search a  X  X lobal X  solution space.
 research area offer a significant breakthrough in this field by providing a number of innovative optimization strategies. Today,
GA is one of the most interesting optimization strategies. This stochastic method is based on the principles of natural selection and evolution. A key advantage of using the GA as a neural-network learning method is that it is capable of achieving optimal or near-optimal network topology and weight settings under given training conditions. The GA has been used in a number of ways for the optimization of important parameters in RBFN. Chen et al. (1991) presented a two-level learning method for RBF networks. A regularized orthogonal least squares algorithm is employed at the lower level to construct RBF networks, while the two key learning parameters, the regularization parameter and the RBF width, are optimized using a GA at the upper level.
Whitehead and Choate (1994) proposed an evolutionary training algorithm for RBFN in which the centers of the radial basis functions are governed by space filling curves whose parameters evolve genetically. Billings and Zheng (1995) showed that in case of RBF network design, global search techniques like GAs could offer remarkable improvements over methods like orthogonal least squares (OLS) and other smaller models. Sheta and Jong (2001) designed an effective time-series for forecasting purposes by utilizing the GA to simultaneously optimize the three main RBF parameters, namely the center vectors, basis widths and hidden-output layer weights. Suresh et al. (2002) developed a surface roughness prediction model and used GA to optimize the objective function of the model. Their GA program gave the minimum and maximum values of the surface roughness and their respective optimal machining conditions. Gonzalez et al. (2003) presented a multiobjective evolutionary algorithm to optimize size, shape and position parameters of the RBFN in order to approach target functions from a set of input X  X utput pairs. They developed some global mutation operators and confirmed that these operators yielded an improved procedure to adjust the parameters of the RBFN. Sarimveis et al. (2004) proposed a new methodology that produces dynamical RBF neural network models based on a specially designed GA, which is used to auto-configure the structure of the networks and obtain the model parameters. Their method has the advantage of providing a single run of the algorithm and prevents the optimum architec-ture searching process from getting trapped in local minima.
Alexandridis et al. (2005) provided a two-stage multi-objective optimization approach for variable selection in the development of RBF neural network models: the first stage using a specially designed GA to minimize the prediction error over a monitoring data-set and the second stage using a simulated annealing technique that aims at the reduction of the number of explanatory variables. Neruda and Kudova (2005) presented the learning methods for RBFN. They introduced a gradient-based learning, a three-step algorithm with unsupervised part, and evolutionary algorithms like GA and compared their performance on bench-mark problems. Han et al. (2005) modeled the plasma etching process using RBFN and used the GA to search for an optimized set of training factors. They found that compared to conventional RBFN models, GA-RBFN models exhibited improved predictions of more than 20% for a number of etch responses like aluminium (Al) etch rate, Al sensitivity, silica sidewall roughness, etc. Manrique et al. (2006) devised a GA-RBFN model based on two cooperating algorithms: the first using a new binary coding, called the basic architecture coding, to get the neural architecture that best solves the problem, and the second using real coding to train the architecture output by the binary GA.

This paper serves to highlight the advantages of using a unified global optimization scheme involving genetic algorithms in optimizing a RBFN compared to the conventional k -means clustering algorithm used to search for the optimal RBFN architecture by a trial and error process. The GA-based optimiza-tion approach utilizes a self growing procedure ( Zhang and Bai, 2005 ) for finding the optimum number of cluster centers and dynamically optimizes their Euclidean distance (and hence their weight factors) relative to the input patterns to an optimum value. The proposed methodology is validated by comparing the performance of the RBFN trained through the standard k -means clustering algorithm with that of the GA-RBFN in prediction of drill flank wear. The work highlights the effectiveness of the self-growing GA-optimized RBFN by the fact that the need for any subsequent error reduction by utilization of linear supervised techniques such as the gradient-based learning of the weight factors becomes redundant. 2. Radial basis function network: a structural overview
The architecture of a typical RBF network for drill wear prediction is shown schematically in Fig. 1 and it consists of the following layers:
Input layer : the input pattern enters this layer and is subjected to a direct transfer function i.e. the output of the node equals its input.

Hidden layer : the nodes of the hidden layer called cluster centers are radially symmetric and these nodes involve the following important parameters: (1) Center vector c k : this vector, stored typically as the weight (2) Euclidean distance : this is a distance measure which deter-(3) Basis function : a transfer function, such as the Gaussian
Output layer : the output from this layer is obtained through the weighted sum of the output from the hidden layer.
 3. The k -means clustering approach The conventional k -means clustering algorithm ( Moody and Darken, 1989 ) is discussed here to compare and highlight the advantages (in terms of computational time and model performance) of using a unified global optimization approach for merging the RBFN structure identification problem with parameter estimation, as discussed in this work. Fig. 2 shows a flowchart representation of the k -means clustering algorithm for RBFN training, and the steps shown are discussed below:
The method proceeds by assuming an initial set of L cluster centers, i.e. center vectors c k which correspond to the L nodes in the hidden layer. It is also assumed that there are T training samples available to the input layer with N nodes, represented by T training vectors x with elements x i . The adaptive k -means clustering algorithm then iteratively finds a desirable set of L center vectors c k that will minimize the sum of the squares of the distance between T training vectors x and their L nearest centers. This involves the following steps: (1) Initialization step : Assume a randomly selected set of cluster (2) Iterative steps : 4. Genetic algorithm: an overview
The generalized GA proceeds through the following steps: (1) Coding of the strings : the desired variable that is to be optimized (cluster distance factor, e , in this work) is first coded in some string structures (represented as s i ) called chromosomes. Binary coded strings having 1 X  X  and 0 X  X  have been used. The length of the string l i is determined according to the desired solution accuracy. Accordingly, the current work utilizes 5 binary digits for the chromosome length. The number of these chromosomes (8 in this work) in the initial population depends on the expanse and dimensionality of the search space. (2) Evaluation (decoding) of the strings : the linear mapping generally used for decoding the strings is given by Eq. (4). x where the variable x i is the decoded value of the string s length l i , while x  X  L  X  i and x  X  U  X  i represent the lower and upper bounds of the variable x i . s i simply represents the numeric equivalent of the binary representation on the string (3) Fitness function : the fitness of the i th chromosome is evaluated by applying a non-linear fitness function defined by Eq. (5). f  X  where y n k are the target values of the network output when the network is presented with input vector X n . %MSE represents the percentage mean square prediction error, N is the total number of testing samples and M is the total number of nodes in the output layer.
  X   X   X  (4) Reproduction operator : reproduction selects good strings in a (5) Crossover operator : this operator creates new strings by (6) Mutation operator : the mutation operator changes from 1 to 0 The values of p c and p m were taken as 0.8 and 0.03, respectively. This completes one set of GA iteration called one generation. The new set of substrings produced at the end of one generation is again evaluated (Step 2), and the process continued from there on, until the terminating criteria are met. 5. GA-based RBFN scheme 5.1. Selecting the cluster distance factor
The parameter that is subjected to GA optimization in this work is called the cluster distance factor, e , which is defined as the maximum allowable distance between an input sample to a prototype RBF center. This estimated e value serves to classify the input data space into L units each of which defines an RBF prototype, where L varies depending upon the e value. The approach to selecting the optimal e value is to search through a class of RBFNs trained by using different e values for that network structure that gives the best performance in terms of the specified criteria. This is done with the help of GA which can search different areas of the parameter space and direct the search to those regions where the probability of finding the optimal solution is high. 5.2. The self growing RBFN algorithm
The method of self growing RBFN ( Zhang and Bai, 2005 ) involves determining the first RBF center m 1 by randomly selecting one data sample x 1 from the input training data-set and evaluating the Euclidean norm between m 1 and the next input data x 2 , namely 99 m 1 x 2 99 . The result is compared with e .Ifitis smaller m 1 is established as the center of an RBF prototype and its elements updated using Eq. (7).  X  new  X  X  m 1i  X  old  X  X  a  X  x 2i m 1i  X  old  X  X  7  X  where i  X  1to T , and T is the total number of training samples. The value of the updating ratio, a , chosen here is 0.5. If the Euclidean norm is greater than e , a new prototype m 2 located at x Continuing to the next sample x 3 , the Euclidean norm values for and x 3 plus m 2 and x 3 are calculated, respectively. The smaller of the norm is selected and its value compared with e , if it is larger, a new prototype m 3  X  x 3 is generated; otherwise the components of the smaller of the two norms are rescaled according to Eq. (7). Continuing this way, to include the whole of input data-set, the number of clusters grows and their centers self-adjust continuously until all the samples are processed. Thus the number and location of RBF in the hidden layer is determined. 5.3. Updating hidden-output layer weights
The %MSE used in the evaluation of the fitness function provides the prediction error following the establishment of RBF structure in the input-space. At that stage, the hidden-output layer weights were not trained by any of the linear supervised techniques. The idea is to allow the GA to provide a global search for the network architecture that best minimizes the prediction error in the first stage of RBFN training itself. Any further error reduction, if required, can be achieved by local gradient descent learning of the hidden-output layer weights.

Fig. 3 shows a flowchart representation of the GA based self growing RBFN algorithm ( Zhang and Bai, 2005 ) as detailed in the foregoing discussion. 6. Experimental set-up 6.1. Data acquisition apparatus
Fig. 4 shows a schematic representation of the experimental set up used for performing the drilling operation as well as for online data acquisition during the process. A column type drilling machine [Hindustan Machine Tools (HMT) make] was used for this purpose. For each cutting condition, the drilling process was carried out in mild-steel workpieces up to 15 mm depth of cut and the torque and thrust force signals were measured through a strain-gauge type dynamometer, which is based on the principle of Wheatstone bridge balance. The dynamometer utilizes a combination of 16 strain-gauges of which 8 pickup torque through bending of arms in the horizontal plane while the remaining 8 form the bridge circuit for measuring axial thrust through bending in the vertical plane. The signals are stored in an IBM PC through PCI-DAS 4020/12 data acquisition card. An offline analysis of this instantaneous data stored in the computer was done to calculate both the average and the maximum values of the torque and the thrust force for each set of cutting conditions, as both are important parameters. The wear is noted after examining the flank faces of the drill under an inverted metallurgical microscope (Olympus make) and the average flank wear thus calculated. 6.2. Cutting conditions and normalization of data-set
The experimental data has been obtained for the drilling process with mild steel as the workpiece. Two d ifferent diameter twist drills 8 and 10 mm are used in the drilling operations for these workpieces. The drills are made of high speed steel (HSS). They are used in conjunction with a wide range of cutting conditions to obtain an exhaustive set of experimental data for which the torque, thrust force and flank wear have been measured. Three different values of spindle speed were employed: 630, 900 and 1250 rpm. The feed-rate was also incremented g radually to include a set of three values: 0.1, 0.16 and 0.25 mm/rev. These three process parameters are taken in 67 combinations as shown in Table 1 . But the table also finding the ||) 1 ) ( % + y MSE shows that there are multiple entries for the same set of combination of the three parameters. This is because although the drill diameter value is same for the same set, but the state of the drill (the number of holes already drilled using a drill) is not the same and progressive wear is definitely known to depend on the state of the drill.

The data-set thus obtained was normalized in the range 0.1 X 0.9 for each given and measured parameter. The normalization of the parameters was done using Eq. (8). y  X  0 : 1  X  0 : 8 x x min x where x max and x min are the maximum and minimum values of the parameter respectively, x is the actual value and y is the normal-ized value of x . 7. RBFN prediction
Generalized computer codes using C++have been developed for the RBFN based on the k -means clustering algorithm and also for the GA-optimized self growing RBF algorithm.

For network prediction, the data-set containing 67 patterns was segregated randomly into 50 training patterns and the remaining 17 are used for testing purposes. Seven process parameters were used as inputs for the RBF network: drill diameter, spindle-speed, feed-rate and average and maximum values of torque and thrust force. The corresponding average flank wear was used as the network output.

While training RBFN using the clustering algorithm, the number of iterations during the clustering phase was kept around 1000. This was found by trial and error to give better prediction results than very few or very large iteration values. The exact value, however, is not significant. During the gradient-based hidden-output layer weight training, the learning rate and momentum coefficient were decided through a series of trial runs and kept fixed throughout. The optimum momentum coefficient was found to be 0.25, and the learning rate to be 0.01. Again, it may be noted that there is no definite criterion for deciding the required values of these parameters. Depending upon the performance of the network during the trial runs when these parameters are varied, these values were found to give faster yet stable prediction behaviour.

While training the RBFN using GA optimization, the initial number of chromosomes was kept to be 8 and the bit-length to be 5. The chromosome number need not be very large because the optimized variable is a single parameter e , and thus the search space is uni-dimensional. Consequently not a very large initial population is required to converge to the optimal solution and this also reduces the extent of computations in each generation. The bit-length determines the degree of accuracy to which the solution will be known. Since the optimal network architecture will not vary with a small change in the optimized e value, we do not require a large bit-length. The lower and upper bounds for the e value were decided through extensive trial runs to be 0 and 4. The crossover probability p c was taken to be 0.8 and the mutation probability p m was kept at 0.03.

The network performance is throughout expressed as percen-tage mean square error (%MSE), the definition of which is incorporated in Eq. (5). 8. Results and discussion 8.1. Prediction results for RBFN trained through k-means scheme
As Table 2 shows, the variation of mean square testing error with the changing number of hidden units shows that the error reduces as the number of nodes in the hidden layer increases but tends to increase if the network architecture is extended beyond an optimum level. This is typical of RBFN prediction results ( Garg et al., 2007 ). The best architecture that minimizes the testing error is obtained from a series of trial runs by varying the number of hidden units. The optimum number of hidden nodes is found to be 10 and the corresponding testing error to be 2.0236%. 8.2. Prediction results for GA-RBFN The prediction results for GA trained RBFN are summarized in
Table 3 . It can be seen that the self-growing GA-RBFN algorithm predicts the optimum network architecture to contain 10 nodes in the hidden layer, a result that was alternatively achieved through time consuming trial and error procedure for k -means scheme.
This highlights the importance of GA based optimization of RBFN structure. The number of generations taken for the application of
GA operators on the initial population is found to be 10. Fig. 5 shows that during this training phase, the average fitness of the population increases from 0.233 to 0.381. At the same time, the
MSE drops from a value of 0.054 at the start of the training phase to a value as low as 0.009 at the end of 10 generations. This fact is also succinctly represented through Fig. 6 . Table 3 also shows that the optimized e value is found to be 3.87097. The convergence of the e value within the assumed bounds of 0 and 4 is represented by Fig. 7 , which shows the variation in e value corresponding to each chromosome (which is in fact the binary representation of the e variable). As the GA operators search the input space for the optimum e value with increasing number of generations, the value can be seen to converge finally to a distinct real number at the end of 10 generations. This was also precisely selected as the termination point of the algorithm, because all chromosomal values converge to the same solution at this stage. Continued application of GA will enable the mutation operator to change the coding of the strings and drive the solution out of the convergence point.
 8.3. Comparison of RBFN trained through k-means and GA based self growing algorithms
Table 4 lists a number of common comparison parameters for both the training algorithms. As discussed earlier, a distinct advantage of the GA-RBFN is that it itself searches for the optimum network architecture and thus saves a lot of effort that is spent in making trial runs as with the k -means trained RBFN, after the number of hidden units has been pre-determined. This fact is also important when we note the computational times for both the training algorithms. The faster prediction results for GA-RBFN are mitigated by the fact that the training time for RBFN trained through the k -means scheme is just the computational time for training the best architecture. It does not take into consideration the time involved in the trial runs leading up to the best architecture. Another important observation as seen from the table is that the self growing RBFN algorithm optimizes the selection and positioning of the cluster centers in the multidimensional space around the input patterns so well that there is no need for subsequent error reduction by utilization of linear supervised techniques such as the gradient-based learning of the hidden-output layer weights. Thus it serves to further optimize the computational expense in RBFN training by making the need for the second stage training process to be redundant. Fig. 8 precisely depicts this fact. The figure shows the second stage training process for both the RBFN algorithms. It can be clearly seen that the self growing GA based algorithm shows little variation in its prediction performance as was at the end of the first stage, thus nullifying the requirement for carrying out this % MSE k-means clustering stage. This further simplifies the training procedure. Besides, the prediction performance of the GA-RBFN (1.577%) is also found to be better than the RBFN trained by the k -means scheme (2.0236%). These results are consistent with the works of other researchers ( Zhang and Bai, 2005 ). 9. Conclusion
This paper focuses on the importance of using evolutionary algorithms like the GA for optimizing the structure of RBFN. This is done by comparing it with the performance of RBFN trained through the conventional k -means scheme. The GA was combined with a self growing RBFN architecture that not only optimized the number and position of nodes in the input space but also simultaneously trained the network to provide good prediction results that did not require the use of subsequent hidden-output weight training to improve the network performance. Besides the prediction results for GA-RBFN were found to be better than its conventional counterpart. The results of this study indicate that the use of this GA-based technique for RBF structure and performance optimization can be effectively used to characterize and model the wear phenomenon in the drilling process, which is an important application in the field of tool condition monitoring. References
