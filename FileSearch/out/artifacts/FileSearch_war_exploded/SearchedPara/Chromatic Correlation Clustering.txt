 We study a novel clustering problem in which the pairwise relations between objects are categorical . This problem can be viewed as clustering the vertices of a graph whose edges are of different types ( colors ). We introduce an objective function that aims at partitioning the graph such that the edges within each cluster have, as much as possible, the same color. We show that the problem is NP -hard and propose a randomized algorithm with approximation guarantee pro-portional to the maximum degree of the input graph. The algorithm iteratively picks a random edge as pivot, builds a cluster around it, and removes the cluster from the graph. Although being fast, easy-to-implement, and parameter free, this algorithm tends to produce a relatively large number of clusters. To overcome this issue we introduce a variant algo-rithm, which modifies how the pivot is chosen and and how the cluster is built around the pivot. Finally, to address the case where a fixed number of output clusters is required, we devise a third algorithm that directly optimizes the objective function via a strategy based on the alternating minimiza-tion paradigm.

We test our algorithms on synthetic and real data from the domains of protein-interaction networks, social media, and bibliometrics. Experimental evidence show that our al-gorithms outperform a baseline algorithm both in the task of reconstructing a ground-truth clustering and in terms of objective function value.
 Categories and Subject Descriptors: H.2.8 [ Database Management ]: Database Applications -Data Mining Keywords: Clustering, Edge-labeled graphs.
Clustering is one of the most well-studied problems in data mining. The goal of clustering is to partition a set of objects in different clusters, so that objects in the same cluster are more similar to each other than to objects in other clusters. A common trait underlying most clustering paradigms is the existence of a function sim( x,y ) representing the similarity between pairs of objects x and y . The similarity function Figure 1: An example of chromatic clustering: (a) input graph, (b) the optimal solution for chromatic-correlation-clustering (Problem 2). is either provided explicitly as input, or it can be computed implicitly from the representation of the objects.

In this paper, we consider a different clustering setting where the relationship among objects is represented by a re-lation type, such as a label ` ( x,y ) from a finite set of possible labels L . In other words, the range of the similarity func-tion sim( x,y ) can be viewed as being categorical , instead of numerical . Moreover, we model the case where two objects x and y do not have any relation with a special label l 0 Our framework has a natural graph interpretation: the in-put can be viewed as an edge-labeled graph G = ( V,E,L,` ), where the set of vertices V is the set of objects to be clus-tered, the set of edges E  X  V  X  V is implicitly defined as E = { ( x,y )  X  V  X  V | ` ( x,y ) 6 = l 0 } , and each edge has a label in L or, as we like to think about it, a color .
The key objective in our framework is to find a partition of the vertices of the graph such that the edges in each clus-ter have, as much as possible, the same color (an example is shown in Figure 1). Intuitively, a red edge ( x,y ) pro-vides positive evidence that the vertices x and y should be clustered in such a way that the edges in the subgraph in-duced by that cluster are mostly red . Furthermore, in the case that most edges of a cluster are red , it is reasonable to label the whole cluster with the red color. Note that a clustering algorithm for this problem should also deal with inconsistent evidence, as a red edge ( x,y ) provides evidence for the vertex x to participate in a cluster with red edges, while a green edge ( x,z ) provides contradicting evidence for the vertex x to participate in a cluster with green edges. Aggregating such inconsistent information is resolved by op-timizing a properly-defined objective function.
 Applications. The study of edge-labeled graphs is moti-vated by many real-world applications and is receiving in-creasing attention in the data-mining literature [8, 10, 16]. As an example, biologists study protein-protein interaction networks, where vertices represent proteins and edges repre-sent interactions occurring when two or more proteins bind together to carry out their biological function. Those inter-actions can be of different types, e.g., physical association, direct interaction, co-localization, etc. In these networks, for instance, a cluster containing mainly edges labeled as co-localization, might represent a protein complex , i.e., a group of proteins that interact with each other at the same time and place, forming a single multi-molecular machine [11].
As a further example, social networks are commonly rep-resented as graphs, where the vertices represent individuals and the edges capture relationships among these individu-als. Again, these relationships can be of various types, e.g., colleagues, neighbors, schoolmates, football-mates.
In bibliographic data, co-authorship networks represent collaborations among authors: in this case the topic of the collaboration can be seen as an edge label, and a clus-ter of vertices represents a topic-coherent community of re-searchers. In our experiments in Section 5 we show how our framework can be applied in all the above domains.
 Contributions. In this paper we address the problem of clustering data with categorical similarity, achieving the fol-lowing contributions:  X  We define chromatic-correlation-clustering , a  X  We introduce a randomized algorithm, named Chromatic  X  Though of theoretical interest, Chromatic Balls has some  X  We empirically assess our algorithms both on synthetic
The rest of the paper is organized as follows. In the next section we recall the traditional correlation clustering prob-lem and introduce our new formulation. In Section 3 we introduce the Chromatic Balls algorithm and we prove its approximation guarantees. In Section 4 we present the two more practical algorithms, namely Lazy Chromatic Balls and Alternating Minimization . In Section 5 we report our experi-mental analysis. In Section 6 we discuss related work.
Given a set of objects V , a clustering problem asks to par-tition the set V into clusters of similar objects. Assuming that cluster identifiers are represented by natural numbers, a clustering C can be seen as a function C : V  X  N . Typi-cally, the goal is to find a clustering C that optimizes an ob-jective function that measures the quality of the clustering Numerous formulations and objective functions have been considered in the literature. One of these, considered both in the area of theoretical computer science and data min-ing, is that at the basis of the correlation-clustering problem [3].
 Problem 1 ( correlation-clustering )
Given a set of objects V and a pairwise similarity func-tion sim : V  X  V  X  [0 , 1] , find a clustering C : V  X  N that minimizes the cost cost( C ) = X
The intuition underlying the above problem is that the cost of assigning two objects x and y to the same cluster should be equal to the dissimilarity 1  X  sim( x,y ), while the cost of assigning the objects in different clusters should cor-respond to their similarity sim( x,y ). A common case is when the similarity is binary, that is, sim : V  X  V  X  { 0 , 1 } . In this case, Equation (1) reduces to counting the number of pairs of objects that have similarity 0 and are put in the same cluster plus the number of pairs of objects that have similarity 1 and belong to different clusters. Or equivalently, in a graph-based terminology, the objective function counts the number of  X  X ositive X  edges that are cut plus the number of  X  X egative X  (i.e., non-existing) edges that are not cut.
In chromatic-correlation-clustering , which we for-mally define below, we still have negative edges (i.e., l edges), but the positive edges may have different colors, rep-resenting different kinds of relations among the objects. Problem 2 ( chromatic-correlation-clustering ) Given a set V of objects, a set L of labels, a special label l , and a pairwise labeling function ` : V  X  V  X  L  X  X  l find a clustering C : V  X  N and a cluster labeling function c` : C [ V ]  X  L so to minimize the cost cost( C ,c` ) = X
Equation (2) is composed by two terms, representing intra-and inter-cluster costs, respectively. In particular, ac-cording to the intra-cluster cost term, any pair of objects ( x,y ) assigned to the same cluster should pay a cost if and only if their relation type ` ( x,y ) is other than the predom-inant relation type of the cluster indicated by the function c` . For the inter-cluster cost, the objective function does not penalize a pair of objects ( x,y ) only if they do not have any relation, i.e., ` ( x,y ) = l 0 . If ` ( x,y ) 6 = l function incurs a cost, regardless of the label ` ( x,y ). Example 1 For the problem instance in Figure 1(a), the solution in Figure 1(b) has a cost of 5: there is no intra-cluster cost, because the two clusters are cliques and their edges are monochromatic, while we have an inter-cluster cost of 5 as equal to the number of edges that are cut.

It is trivial to observe that, when | L | = 1, the matic-correlation-clustering problem corresponds to the binary version of correlation-clustering . Thus, our problem is a generalization of the standard problem. Since correlation-clustering is NP -hard, we can easily con-clude that chromatic-correlation-clustering is NP -hard too.
The previous observation motivates us to consider whether applying standard correlation-clustering algo-rithms, just ignoring the different colors, is a good solution to the problem. As we show in the following example, such an approach does not guarantee to produce good solutions. Example 2 For the problem instance in Figure 1(a), the optimal solution for the standard correlation-cluster-ing which does not consider the different colors, would be composed by a single cluster containing all the six vertices, as, according to Equation (1), this solution has a (min-imum) cost of 4 corresponding to the number of missing edges within the cluster. Conversely, this solution has a non-optimal cost 12 when evaluated according to the chro-matic-correlation-clustering formulation, i.e., accord-ing to Equation (2). Instead, the optimum in this case would correspond to the cost 5 solution depicted in Figure 1(b).
Although the example shows that for the chromatic ver-sion of the problem we cannot directly apply algorithms de-veloped for the correlation-clustering problem, we can use such algorithms at least as a starting point, as shown in the next section.
We present next a randomized approximation algo-rithm for the chromatic-correlation-clustering prob-lem. This algorithm, called Chromatic Balls , is motivated by the Balls algorithm [1], which is an approximation algorithm for standard correlation-clustering .
 For completeness, we briefly review the Balls algorithm. The algorithm works in iterations. Initially all objects are considered uncovered . In each iteration the algorithm pro-duces a cluster, and the objects participating in the cluster are considered covered . In particular, the algorithm picks as pivot a random object currently uncovered, and forms a cluster consisting of the pivot itself along with all currently uncovered objects that are connected to the pivot.

The outline of our Chromatic Balls is summarized in Algo-rithm 1. The main difference with the Balls algorithm is that the edge labels are taken into account in order to build clus-ters around the pivots. To this end, the pivot chosen at each iteration of Chromatic Balls is an edge, thus a pair of objects, rather than a single object. The Chromatic Balls algorithm employs a set V 0 to keep all the objects that have not been assigned to any cluster yet; hence, initially, V 0 = V . At each iteration, a random edge ( u,v ) such that both objects u and v are currently in the set V 0 is selected as pivot (line 3). Given the pivot ( u,v ), a cluster C is formed around it. Beyond the objects u and v , the cluster C additionally con-tains all other objects x  X  V 0 for which the triangle ( u,v,x ) is monochromatic, that is, ` ( u,x ) = ` ( v,x ) = ` ( u,v ) (lines 4 and 5). Since the label ` ( u,v ) forms the basis for creating the cluster C , the cluster is labeled with this label (line 6). All objects added in C are removed from V 0 (line 7), and the algorithm terminates when V 0 does not contain any pair of objects that share an edge, i.e., that is labeled with a label other than l 0 (line 2). All objects remaining in the set V if any, are eventually made singleton clusters (lines 8-11). Computational complexity. The complexity of the Chromatic Balls algorithm is determined by two steps: ( i ) picking the pivots (line 3), and ( ii ) building the clusters (line 4). Choosing the pivots requires O ( m log n ) time, where n = | V | and m = | E | , as selecting random edges can be im-plemented by building a priority queue of edges with random priorities, and subsequently removing edges; each edge is re-moved once from the priority queue, whether it is selected as Algorithm 1 Chromatic Balls Input: Edge-labeled graph G = ( V,E,L,` ) Output: Clustering C : V  X  N ; cluster labeling function 1: V 0  X  V ; i  X  1 2: while there exist u,v  X  V 0 such that ( u,v )  X  E do 3: randomly pick u,v  X  V 0 such that ( u,v )  X  E 4: C  X  X  u,v } X  X  x  X  V 0 | ` ( u,x ) = ` ( v,x ) = ` ( u,v ) } 5: for all x  X  C do C ( x )  X  i 6: c` ( i ) = ` ( u,v ) 7: V 0  X  V 0 \ C ; i  X  i + 1 8: for all x  X  V 0 do 9: C ( x )  X  i 10: c` ( i )  X  a random label from L 11: i  X  i + 1 pivot or not. Building a single cluster C , instead, requires to access all neighbors of the pivot edge ( u,v ). As the current cluster is removed from the set of uncovered objects at the end of each iteration, the neighbors of any pivot are not con-sidered again in the remainder of the algorithm. Thus, the step of selecting the objects to be included into the current clusters requires visiting each edge at most once; therefore, the process takes O ( m ) time. In conclusion, we can state that the computational complexity of the Chromatic Balls algorithm is O ( m log n ). We analyze next the quality of the solutions obtained by Chromatic Balls . Our main result, given in Theorem 1, shows that the approximation guarantee of the algorithm depends on the number of bad triplets incident to a pair of objects in the input dataset. The notion of bad triplet is defined below; however, here we note that this result gives a constant-factor guarantee for bounded-degree graphs.

Even though the Chromatic Balls algorithm is similar to the Balls algorithm, which can be shown to provide a constant-factor approximation guarantee for general graphs too, the theoretical analysis of Chromatic Balls is much more complicated and requires several additional and nontrivial arguments. Due to the limited space of this paper, we re-port next only an outline of our analysis. Further details, including complete proofs, can be found in an extended tech-nical report. 1
We begin our analysis by defining special types of triplets and quadruples among the vertices of the graph.
 Definition 1 (SC-triplet) We say that { x,y,z } is a same-color triplet (SC-triplet) if the induced triangle is monochromatic, i.e., ` ( x,y ) = ` ( x,z ) = ` ( y,z ) 6 = l Definition 2 (B-triplet) We say that { x,y,z } is a bad triplet (B-triplet) if the induced triangle is non-monochromatic and it has at most one pair labeled with l 0 Definition 3 (B-quadruple) A Bad-quadruple is a set { x,y,z,w }  X  V that contains at least one SC-triplet and at least one B-triplet.

Note that, according to the cost function of our problem as defined in Equation (2), there is no way to partition a B-triplet without paying any cost. Next we define the no-tions of hitting and d -hitting .
 Definition 4 (hitting) Consider a pair of objects ( x,y ) and a triplet t , which can be either SC-triplet or B-triplet. We say that t hits ( x,y ) if x  X  t and y  X  t . Additionally, if http://francescobonchi.com/CCC.pdf q is a B-quadruple, we say that q hits ( x,y ) if x  X  q , y  X  q , and there exists z  X  q such that { x,y,z } is a B-triplet. Definition 5 ( d -hitting) Given any pair of objects ( x,y ) and any B-quadruple q = { x,y,z,w } , we say that q deeply hits ( d-hits ) ( x,y ) if q hits ( x,y ) and either { x,z,w } or { y,z,w } is an SC-triplet.

In reference to the above notions, we hereinafter denote by S , T , and Q the sets of all SC-triplets, B-triplets, and B-quadruples for an instance of our problem. Moreover, given a pair ( x,y )  X  V  X  V we define the following sets: T xy  X  X  denotes the set of all B-triplets in T that hit ( x,y ); Q xy  X  Q denotes the set of all B-quadruples in Q that hit ( x,y ); Q d xy  X  Q xy  X  Q denotes the set of all B-quadruples in Q that d -hit ( x,y ).

Let us now consider some events that may arise during the execution of the Chromatic Balls algorithm. Given an object x  X  V , P ( i ) x denotes the event  X  x is chosen as pivot in the i -th iteration . X  Given a set { x 1 ,...,x n }  X  V , with n  X  2, A 1  X  X  X  x n denotes the event  X  all objects x 1 ,...,x n enter the i -th iteration of the algorithm, while two of them are chosen as pivot in the same iteration . X  reference to a pair ( x,y ). Given a B-triplet { x,y,z }  X  T T z | xy denotes the event  X  A not chosen both as pivots in the i -th iteration . X  Given a B-quadruple { x,y,z,w }  X  Q d xy , Q ( i ) zw | xy denotes the event  X  A xyzw occurs while neither x nor y are chosen as pivots in i -th iteration . X  we also consider their counterparts that assert that the events occur at some iteration i . For instance, A x 1  X  X  X  x T z | xy and Q zw | xy are defined analogously. Formally:
As reported in the next two lemmas, the probabilities of the events T z | xy and Q zw | xy can be expressed in terms of the probabilities of the events A xyz and A xyzw .
 Lemma 1 Given a pair ( x,y )  X  V  X  V and a B-triplet { x,y,z }  X  T xy , it holds that 1 2 Pr [ A xyz ]  X  Pr[ T Pr [ A xyz ] .
 Lemma 2 Given a pair ( x,y )  X  V  X  V and a B-quadruple { x,y,z,w } X  X  d xy , it holds that 1 6 Pr [ A xyzw ]  X  Pr[ Q Analyzing carefully the probabilities of events T z | xy Q zw | xy is crucial for deriving the desired approximation fac-tor, as shown next.

We consider an instance G = ( V,E,L,` ) of our problem and rewrite the cost function in Equation (2) as sum of the costs paid by any single pair ( x,y ). To this end, in order to simplify the notation, we hereinafter write the cost by omitting C and c` while keeping G only: where c xy ( G ) denotes the aforementioned contribution of the pair ( x,y ) to the total cost. Moreover, let E[ c ( G )] denote the expected cost of Chromatic Balls over the random choices made by the algorithm. By the linearity of expectation, the expected cost E[ c ( G )] can be expressed as Finally, let c  X  ( G ) be the cost of the optimal solution on G .
To derive an approximation factor r ( G ) on the perfor-mance of the Chromatic Balls algorithm, we look for an upper bound Ub ( G ) on the expected cost E[ c ( G )] of the algorithm, and a lower bound Lb ( G ) on the cost c  X  ( G ) of the optimal solution, so that We next show how to derive such upper and lower bounds. Deriving the upper bound Ub ( G ) . For a pair ( x,y ) we define the collection of events  X  xy = { T z | xy | { x,y,z }  X  T xy } X  X  Q zw | xy | { x,y,z,w }  X  Q d xy } . As the following two lemmas show, if pair ( x,y ) contributes to the cost paid by the algorithm, then exactly one of the events in  X  xy occurs. Lemma 3 If c xy ( C ,c`,G ) &gt; 0 then at least one of the events in  X  xy occurs.
 Lemma 4 The events within the collection  X  xy are disjoint. Combining Lemmas 3 and 4 with the expressions of the probabilities of the events T z | xy (Lemma 1) and Q (Lemma 2) we can derive an upper bound on the expected contribution E[ c xy ( G )] of a pair ( x,y ) to the total cost. Lemma 5 For a pair ( x,y )  X  V  X  V the following bound holds.

E[ c xy ( G )]  X  X The bound in Lemma 5 together with Equation (7) can be used to give the desired (upper) bound on the overall ex-pected cost E[ c ( G )].
 Lemma 6 The expected cost E[ c ( G )] of the Chromatic Balls algorithm can be bounded as follows E[ c ( G )]  X  Ub ( G ) = X where: Finally,  X  xyzw denotes the number of B-triplets contained in any B-quadruple { x,y,z,w } . Deriving the lower bound Lb ( G ) . Recalling that a B-triplet incurs a non-zero cost in any solution, a lower bound on the cost of the optimal solution c  X  ( G ) can be ob-tained by counting the number of disjoint B-triplets in the input. Considering the set T of B-triplets we can restate the following result of Ailon et al. [1] that provides a lower bound on the optimal by  X  X ractionally assigning X  all pairs of objects in V  X  V to the triplets in T .
 Lemma 7 (Ailon et al. [1]) Let {  X  xyz |{ x,y,z } X  X } be any assignment of nonnegative weights to the B-triplets in
We can then obtain a lower bound on the optimal solution by finding a suitable set of weights  X  xyz that satisfies the conditions of the previous lemma. We derive such a set of weights in the following further lemma.
 Lemma 8 For any pair ( x,y )  X  V  X  V the following condi-tion holds.
 Thus, combining Lemmas 7 and 8, we can obtain the desired lower bound Lb ( G ) as follows.
 Lemma 9 The cost c  X  ( G ) of the optimal solution on any input instance G is lower bounded as follows c ( G )  X  Lb ( G ) = = X of B-triplets that hit a pair of objects.
 The approximation ratio r ( G ) . The upper and lower bounds obtained in Lemmas 6 and 9 are at the basis if the final form of the approximation ratio of Chromatic Balls . Theorem 1 The approximation ratio of the Chromatic Balls algorithm on any input instance G is of B-triplets that hit a pair of objects.
 Theorem 1 shows that the approximation factor of the Chromatic Balls algorithm is bounded by the maximum num-ber t max of B-triplets that hit a pair of objects. The result is meaningful as it quantifies the quality of the performance of the algorithm as a property of the input graph. For exam-ple, as the following corollary shows, the algorithm provides a constant-factor approximation for bounded-degree graphs. Corollary 1 The approximation ratio of the Chromatic Balls algorithm on any input instance G is where D max = max x  X  V |{ y | y  X  V  X  ` ( x,y ) 6 = l 0 maximum degree in the problem instance.
In this section we present two additional algorithms for the chromatic-correlation-clustering problem. The first one is a variant of the Chromatic Balls algorithm that attempts to overcome some weaknesses of Chromatic Balls by employing two heuristics, one for pivot selection and one for cluster selection. The second one is an alternating min-imization method that is designed to optimize directly the objective function.
The algorithm we present next is motivated by the follow-ing example, in which we discuss what may go wrong during the execution of the Chromatic Balls algorithm.
 Example 3 Consider the graph in Figure 2: it has a fairly evident green cluster formed by vertices { U,V,R,X,Y,W,Z } . Figure 2: An example of an edge-labeled graph.

However, as all the edges have the same probability of be-ing selected as pivots, Chromatic Balls might miss this green cluster, depending on which edge is selected first. For in-stance, suppose that the first pivot chosen is (Y,S) . Chromatic Balls forms the red cluster { Y,S,T } and removes it from the graph. Removing vertex Y makes the edge (X,Y) missing, which would have been a good pivot to build a green clus-ter. At this point, even if the second selected pivot edge is a green one, say (X,Z) , Chromatic Balls would form only a small green cluster { X,W,Z } .
 Motivated by the previous example we introduce the Lazy Chromatic Balls heuristic, which tries to minimize the risk of bad choices. Given a vertex x  X  V , and a label l  X  L , let d ( x,l ) be the number of edges incident to x having label l . Also, we denote by  X ( x ) = max l  X  L d ( x,l ), and  X  ( x ) = arg max l  X  L d ( x,l ). Lazy Chromatic Balls differs from Chromatic Balls in two ways: Pivot random selection. At each iteration Lazy Chro-matic Balls selects a pivot edge in two steps. First, a vertex u is picked up with probability directly proportional to  X ( u ). Then, a second vertex v is selected among the neighbors of u with probability proportional to d ( v, X  ( u )).
 Ball formation. Given the pivot ( u,v ), Chromatic Balls forms a cluster by adding all vertices x such that  X  u,v,x  X  is a monochromatic triangle. Lazy Chromatic Balls instead, iteratively adds vertices x in the cluster as long as they form a triangle  X  X,Z,w  X  of color ` ( u,v ), where X is either u or v , and Z can be any other vertex already belonging to the current cluster.
 Example 4 Consider again the example in Figure 2. Ver-tices X and Y have the maximum number of edges of one color: they both have 5 green edges. Hence, one of them is chosen as first pivot vertex u by Lazy Chromatic Balls with higher probability than the remaining vertices. Suppose that Algorithm 2 Alternating Minimization (AM) Input: Edge-labeled graph G = ( V,E,L,` ); Output: Clustering C : V  X  N ; cluster labeling function 1: initialize A = [ a 1 ,..., a N ] and C = [ c 1 ,..., c 2: repeat 3: for all x  X  V compute optimal a x according to Propo-4: for all k  X  [1 ..K ] compute optimal c k according to 5: until neither A nor C changed X is picked up, i.e., u = X . Given this choice, the second pivot v is chosen among the neighbors of X with probabil-ity proportional to d ( v, X  ( u )) , i.e., the higher the number of green edges of the neighbor, the higher the probability for it to be chosen. In this case, hence, Lazy Chromatic Balls would likely choose Y as a second pivot vertex v , thus making (X,Y) the selected pivot edge. Afterwards, Lazy Chromatic Balls adds to the being formed cluster the vertices { U,V,Z } because each of them forms a green triangle with the pivot edge. Then, R enters the cluster too, because it forms a green triangle with Y and V , which is already in the cluster. Similarly, W enters the cluster thanks to Z .
 Computational complexity. Like Chromatic Balls , the running time of the Lazy Chromatic Balls algorithm is deter-mined by picking the pivots and building the various clus-ters. Picking the first pivot u can be implemented with a priority queue with priorities  X   X  rnd , where rnd is a ran-dom number. This requires computing  X  for all objects, which takes O ( nh + m ) (where h = | L | ). Managing the pri-ority queue itself requires instead O ( n log n ), as each object is put into/removed from the queue only once during the execution of the algorithm. Given u , the second pivot v is selected by probing all (non-chosen) neighbors of u . This takes O ( m ) time, as for each pivot u , its neighbors are ac-cessed only once throughout the execution of the algorithm. Finally, building the current cluster takes O ( m ) time, as it requires a visit of the graph, where each edge is accessed O (1) times. In conclusion, the computational complexity of Lazy Chromatic Balls is O ( n (log n + h ) + m ), which, for small h , is better than the complexity of Chromatic Balls .
A nice feature of the previous algorithms is that they are parameter-free: they produce clusterings by using informa-tion that is local to the pivot edges, without forcing the num-ber of output clusters in any way. However, in some cases, it could be desired having an output clustering composed by a pre-specified number K of clusters. To this purpose, we present here an algorithm based on the alternating mini-mization paradigm [7], that receives in input the number K of output clusters and attempts to minimize Equation (2) directly. The pseudocode of the proposed algorithm, called Alternating Minimization , is given in Algorithm 2.
In a nutshell, AM tries to produce a solution by alternat-ing between two optimization steps. In the first step the algorithm finds the best cluster assignment for every x  X  V given the assignments of every other y  X  V and the current cluster labels. In the second step, it finds the best label for every cluster given the current assignment of objects to clus-ters. Below we show that both steps can be solved optimally. As a consequence the value of Equation (2) is guaranteed to decrease in every step, until convergence. Finding the global minimum is obviously hard, but the algorithm is guaranteed to converge to a local optimum.
 Definitions. For presentation sake, we adopt matrix no-tation. We denote matrices by uppercase boldface romans and vectors by lowercase boldface romans. We write X for the ( i,j ) coordinate of matrix X , and x ( i ) for the i -th coordinate of vector x .

The parameter space of Problem 2 consists of a cluster as-signment for every object x  X  V , given by the binary matrix A , and a label assignment for every cluster k  X  { 1 ,...,K } , given by the binary matrix C . We have A kx = 1 when object x is assigned to cluster k , and A kx = 0 otherwise. Similarly, we set C lk = 1 when label l is assigned to cluster k , and C lk = 0 otherwise. Since every object must belong to one and only one cluster, and every cluster must have one and only one label assigned, both A and C are constrained to consist of all zeros with a single 1 on every column. Denote by a x the column of A that corresponds to object x .
The input is represented by a set of binary matrices, with a matrix Z x for every x  X  V . These matrices encode the labeling function ` as follows. Let z xy denote the column of Z x that corresponds to the object y  X  V . We have z xy ( l ) = 1 if and only if ` ( x,y ) = l , otherwise z xy ( l ) = 0. Every Z consists thus of zeros, with exactly one 1 on every column. Finally, denote by b a special binary vector where b ( l ) = 1 when l = l 0 and b ( l ) = 0 otherwise. We have then z T if and only if ` ( x,y ) = l 0 .

The above formulation of the problem assumes that the input is represented by many large matrices. Note however that this representation is only conceptual. In the actual im-plementation we do not have to materialize these matrices and we can represent the input with the minimal amount of space required, as shown next. The benefit of our formula-tion is that it allows to write our objective function and our optimization process using linear-algebra operations, and ar-gue about the optimality of the local optimization steps. Optimal cluster assignment. Denote by N  X  xk the num-ber of objects y  X  V in cluster k that have ` ( x,y ) = l Since ` ( x,y ) = l 0  X  z xy b = 1, we have N  X  xk = ( AZ Similarly, let N + xk denote the number of objects y  X  V in cluster k that have ` ( x,y ) = c` ( k ). Since y  X  k , we have ` ( x,y ) = c` ( k )  X  z xy Ca y = 1 and can write N xk = ( Aw x )( k ), where w x = [ z Proposition 1 The optimal cluster assignment for x  X  V given A and C is k  X  = arg min k N  X  xk  X  N + xk .
 Proof. We can rewrite Equation (2) as follows: where w x is defined as above, and 1 denotes the | V | -dimensional vector of all 1s. Terms that correspond to a fixed x  X  V further simplify to where the constant d x = 1 T 1  X  1 T Z T x b is the  X  X egree X  of object x , the number of objects y  X  V where ` ( x,y ) 6 = l Since we must assign exactly one cluster for x , the above expression is minimized simply by assigning x to the cluster k that minimizes ( AZ T x b )( k )  X  ( Aw x )( k ) = N  X  xk The result is quite intuitive. The best cluster for x is the one having the least  X  X ush X  in terms of l 0 connections, and the most  X  X ull X  given by connections having the appropriate label. However, evaluating N  X  xk in practice is very slow, as it involves checking all l 0 connections of x . Ideally the update rule should only require access to edges having some label other than l 0 . This is easy to achieve, however.
Let N 0 xk denote the remaining objects in cluster k , that is, those with ` ( x,y ) 6 = c` ( k ) 6 = l 0 . Also, let S k of cluster k . Clearly we have S k = N + xk + N 0 xk + N  X  x  X  V . Using this we obtain N  X  xk  X  N + xk = S k  X  2 N which is much faster to evaluate.
 Optimal label assignment. The update rule for the clus-ter label is intuitive as well. Denote by E k the number of ordered ( x,y ) pairs so that both x and y belong to cluster k , and ` ( x,y ) = c` ( k ).
 Proposition 2 The optimal label assignment for cluster k given A is l  X  = arg min l S 2 k  X  E k .

Proof. We can partition the cost in Equation (9) as a sum over clusters. That is, for a fixed cluster k we sum only over those x and y that belong both to k . Also, the second term in Equation (9) does not depend on C and can therefore be omitted. This leaves us with the sum where we can replace Ca y with the binary vector c k indicates the label assigned to cluster k . Clearly we have P counts all ( x,y ) pairs having the same label that is currently assigned to k , which is by definition equal to E k . This means that the optimal label for cluster k is simply the label shared by the majority of the pairs in k .
 Computational complexity. The running time of Alternating Minimization depends on the (optimal) cluster and label assignment steps. Cluster assignment requires two sub-steps: evaluating S k  X  2 N + xk  X  N 0 xk for each vertex and cluster, which can be performed in O ( m ) by a simple visit of the input graph, and looking at all clusters to choose the best one for each vertex, which clearly takes O ( Kn ). Label assignment requires to compute the number of intra-cluster edge labels for each cluster k and label l . This takes O ( m ), as it can be performed, again, by visiting the input graph. Then, the assignment of labels to clusters by evalu-ating S 2 k  X  E k can be performed in O ( Kh ). In conclusion, as usually h = | L | n , the computational complexity of Alternating Minimization is O ( s ( Kn + m )), where s is the number of iterations to convergence.
In this section, we report our empirical assessments. We experiment with all three proposed algorithms, Chromatic Balls , Lazy Chromatic Balls , and Alternating Minimization , to which we refer by CB , LCB , and AM , respectively. We also evaluate the performance of the baseline described in the Introduction, namely the  X  X tandard X  Balls algorithm [1] that ignores colors. We refer to this baseline as B . All measure-ments reported are averaged over 50 runs.
 Algorithm 3 Synthetic data generator Input: number of vertices n , number of clusters K , number Output: edge labeled graph G = ( V,E,L,` ) 1: V  X  [1 ,n ], E  X  X  X  , L  X  X  l 1 ,...,l h } 2: assign each vertex x  X  V to a randomly selected cluster 3: assign to each cluster a randomly selected label from L 4: for all pairs ( x,y )  X  V  X  V do 5: pick 3 random numbers r 1 ,r 2 ,r 3 ranging within [0 , 1] 6: if C ( x ) = C ( y ) then 7: if r 1 &lt; p then 8: if r 2 &lt; w then 9: E  X  E  X  ( x,y ) 10: ` ( x,y )  X  a random label from L \{ c` ( C ( x )) } 11: else 12: E  X  E  X  ( x,y ) 13: ` ( x,y )  X  c` ( C ( x )); 14: else if r 3 &lt; q then 15: E  X  E  X  ( x,y ) 16: ` ( x,y )  X  a random label from L
We evaluate our algorithms on synthetic datasets gener-ated by the process outlined in Algorithm 3. In a nutshell, the generator initially assigns vertices and labels to clusters uniformly at random, and then adds noise according to the probability parameters p , q , and w . Given the assignment of vertices to clusters, intra-cluster edges are sampled with probability p , and they are given the correct label (the label of the cluster they are assigned to) with probability 1  X  w , while, inter-cluster edges are sampled with probability q .
The initial assignment of objects and labels to clusters can be interpreted as a ground truth underlying the correspond-ing synthetic dataset. We compare the resulting clusterings with the ground-truth clustering using the well-known F -measure external cluster-validity criterion. Given a ground-truth clustering  X  C and a clustering solution C having  X  K clusters, respectively, F -measure is defined in terms of precision and recall as follows: where F  X  kk = (2 P  X  kk R  X  kk ) / ( P  X  kk + R  X  kk ) such that P S ber of common objects between the  X  k -th cluster of e C and the k -th cluster of C , and S  X  k and S  X  k are the sizes of clusters and k , respectively. It easy to see that F  X  [0 , 1].
We generate datasets with a fixed number of objects ( n = 1000), and we vary ( i ) the noise level (controlled by playing the number of clusters K in the ground truth. Even though we perform tests by varying all parameters p , q , and w , due to space limitations we only report results obtained for varying q and keeping p and w equal to 0 . 5.

For the number of clusters required as input for the AM algorithm, we consider two options: the average number of clusters produced by the CB algorithm, and the number of clusters in the ground truth. We refer to these two settings by AM and AM  X  , respectively. In Figure 3 we report the performance of our algorithms in terms of F -measure, as well as solution cost (Equation (2)). F igure 3: Accuracy on synthetic datasets in terms of F -measure (left) and solution cost (right), by varying level of noise (1st row), number of labels (2nd row), and number of ground-truth clusters (3rd row).
 All trends observed by varying the parameters q , h , and K are intuitive. Indeed, for all methods, the performance decreases as the noise level q increases (Figure 3, 1st row). On the other hand, all methods give better solutions, in terms of cost, as the number of ground truth clusters K increases (Figure 3, 3rd row right). The reason is that since CB and LCB tend to produce a large number of clusters, by setting a larger K the difference tends to disappear.
All proposed methods generally achieve both F -measure and solution cost results evidently better than the baseline. Particularly, in terms of solution cost, CB , LCB , and AM perform very close to each other and generally better than AM  X  . In terms of F -measure, instead, LCB is recognized as the best method in most cases. We experiment with three real datasets (Table 1).
 String. A protein-protein interaction (PPI) network ob-tained from string-db.org , i.e., a database of known pro-tein interactions for a large number of organisms. The dataset is an undirected graph where vertices represent pro-teins and edges represent protein interactions. Edges are labeled with 4 types of interactions. The PPI datasets are usually very sparse, therefore, we keep only the 30-core of the entire network, i.e., we recursively remove the vertices with degree less than 30 until a fix-point has been reached. Youtube. This dataset represents a network of associations in the youtube site. The vertices of the network represent users, videos, and channels. Entities in the network have five different types of associations: contact , co-contact , co-subscription , co-subscribed , and favorite ; these are the edge Table 1: Characteristics of real data. n : number of vertices; m : number of edges; d : average degree; | L | : number of labels; c : clustering coefficient. l abels considered by our algorithms. For edges with multiple labels we picked one label at random from the available ones. The dataset has been compiled by Tang et al. [14] and it is available at http://www.public.asu.edu/~ltang9/ .
 DBLP. We obtain a recent snapshot of the DBLP co-authorship network ( http://dblp.uni-trier.de/xml/ ). For each co-authorship edge, we consider the bag of words obtained by merging the titles of all papers coauthored by the two authors. Words are stemmed and stop-words are removed. We then apply Latent Dirichlet Allocation (LDA) [5] to automatically identify 100 topics on each edge. After LDA topic-modeling, for each edge, we assign its most prominent topic discovered as edge label.
 Results. Table 2 summarizes the results obtained on real data. Like in synthetic data, all proposed algorithms clearly outperform the baseline B . CB is the best method on Youtube and DBLP , achieving up to 27.74% of improvement with respect to the baseline in terms of solution cost. Instead, CB is slightly outperformed by LCB and AM on String , while LCB outperforms AM on String and DBLP .

As far as the runtime, we observe that the baseline is faster than the proposed methods, as expected. This is mainly due to a smaller complexity in choosing vertex pivots com-pared to choosing edge pivots. However, all proposed meth-ods remain very efficient, as they take a few seconds ( CB and LCB ) or minutes ( AM ) on large and dense graphs like Youtube and DBLP . All runtimes comply with the computa-tional complexity analysis reported previously. Indeed, AM is the slowest method, mostly due to the typically high num-ber of iterations needed to convergence, while LCB is faster than CB , especially on dense datasets like Youtube . Finally, Figure 4 shows an example cluster from the DBLP co-authorship network recognized by the LCB al-gorithm, containing 23 authors (vertices). Among the 71 intra-cluster edges, 58 have the same label, i.e., Topic 18 , whose most representative (stemmed) keywords are: queri, effici, spatial, tempor, search, index, similar, data, dimension, aggreg . Other topics (edge colors) that appears are  X  X ensor networks X ,  X  X requent pattern mining X ,  X  X lgorithms on graphs and trees X ,  X  X upport vector machines X ,  X  X lassifiers and Bayesian learning X . Edge-labeled graphs and multidimensional net-works. Graphs in which edges are labeled with a type of re-lation occurring among the connected vertices are receiving increasingly attention. To the best of our knowledge no pre-vious work has investigated the problem of clustering in such graphs. The problems studied so far on this kind of graphs are mainly on label-constrained reachability queries [8, 10, 12, 16], whose main goal is to answer whether a vertex u can reach vertex v trough a path whose edge labels belong to a given set. Clustering has been studied, instead, in so called multidimensional networks , i.e., networks defined as a col-lection of multiple networks over the same set of actors. In our jargon these are simply graphs where each edge can have more than one color [4, 13, 15]. Although the input of that problem might seem close to ours, the objective is seman-tically far away. In clustering multidimensional networks, the objective is to find a partitioning of vertices which is meaningful and relevant in all dimensions at the same time . Taking again the colors metaphor, in that setting is a clus-tering is considered as good if it makes sense in the green network and as well as the red network, and so on. In our work, we are rather interested in finding groups of objects that induce color-coherent clusters while looking at all the colors together.
 Correlation Clustering. The problem of correlation-clustering was first defined by Bansal et al. [3] in its bi-nary version. Ailon et al. [1] proposed the Balls algorithm that achieves expected approximation factor 5 if the weights obey the probability condition. If the weights X ij obey also the triangle inequality, then the algorithm achieves expected approximation factor 2. Giotis and Guruswami [9] consider correlation clustering when the number of clusters is given, while Ailon and Liberty [2] study a variant of correlation clustering where the goal is to minimize the number of dis-agreements between the produced clustering and a given ground truth clustering. We recently extended correlation clustering to allow overlaps, i.e., objects belonging to more than one cluster [6].
In this paper, we introduce a variant of the correlation-clustering problem, in which the pairwise relations between objects are categorical. The problem has interesting appli-cations, such as clustering social networks where individuals are connected with different types of relations, or clustering protein networks, where proteins are associated with differ-ent types of interactions. We propose three algorithms that we evaluate on synthetic and real datasets.

Our problem is a novel clustering formulation well-suited for mining multi-labeled and heterogeneous datasets that are becoming increasingly common. We believe that there are many interesting extensions and fruitful future research directions. For example, we would like to extend the prob-lem formulation in order to capture overlapping clusters as well as multiple-labeled edges.

