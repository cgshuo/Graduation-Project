 The goal of traditional information extraction is to accurately extract as many fields or records as possible from a collect ion of unstructured or semi-structured text documents. In many scenarios, however, we already have a partial database and we need only fill in its holes. This paper proposes methods for finding such information in a large collection of external documents, and doing so efficiently with limited computational r esources. For instance, this small piece of informa-tion may be a missing record, or a missing field in a database that would be acquired by searching a very large collection of documents, such as the Web. Using traditional information extraction models for this task is wasteful, and in most cases computationally intractable. A more feasible approach for obtaining the required information is to automatically issue appropriate queries to the ex-ternal source, select a subset of the retri eved documents for processing and then extract the specified field in a focussed a nd efficient manner. We can further en-hance our system X  X  efficiency by exploiting the inherent relational nature of the database. We call this process of searc hing and extractin g for specific pieces of information, on demand, Resource-bounded Information Extraction (RBIE). In this paper, we present the design of a general framework for Resource-bounded Information Extraction, discuss various design choices involved and present ex-perimental results.
 1.1 Example Consider a database of scientific publication citations, such as Rexa, Citeseer or Google Scholar. The database is created by crawling the web, downloading papers, extracting citations from the bibliographies and then processing them by tagging and normalizing. In addition, the information from the paper header is also extracted. In order to make these citations and papers useful to the users, it is important to have the year of publication information available. Even after integrating the citation information with other publicly available databases, such as DBLP, a large fraction of the papers do not have a year of publication associated with them. This is because, of ten, the headers or the full text of the papers do not contain the date and venue of publication (especially for preprints available on the web). Approximately one third of the papers in Rexa are missing the year of publication field. Our goal is to fill in the missing years by extracting them from the web.

We chose this particular example task, since it demonstrates the relational aspect of RBIE. Along with extracting headers, the bibliographic information is often extracted, creating a citation network. This network information can be further exploited by noting that in almost all cases, a paper is published before (or the same year) as other papers that cite it. Using these temporal con-straints, we obtain 87.7% of the original F1 by using only 13.2% of computational resources such as queries and documents. 1.2 Motivation The knowledge discovery and data mining community has long struggled with the problem of missing information. Most real-world databases come with holes in the form of missing records or missing fe ature values. In some cases, the values exist, but there is considerable uncertain ty about their correctness. Incomplete-ness in the database provides incomplete responses to user queries, as well as leads to less accurate data mining models and decision support systems. In order to make the best use of the existing information, it is desirable to acquire missing information from an external source in an efficient manner. The external source can be another database that is purchas ed, or a large collection of free docu-ments, such as the web. In the latter case, we may run information extraction to obtain the missing values in our database. However, the traditional models of information extraction can not be directly applied in this  X  X n demand X  setting.
Note that, in the setting described above, we are often not interested in obtain-ing the complete records in the database, but just filling in the missing values. Also, the corpus of documents, such as th e web, is extremely large. Moreover, in most real scenarios, we must work under pre-specified resource constraints. The resource constraints may be computational, such as processors, network band-width, or related to time and money. Any method that aims to extract required information in the described setting must be designed to work under the given resource constraints.

Many of these databases are relational in nature, e.g. obtaining the value of one field may provide useful information about the remaining fields. Similarly, if the records are part of a network structure with uncertain or missing values, as in the case of our example task, then information obtained for one node can reduce uncertainty in the entire network. We show that exploiting these kinds of dependencies can significantly reduce the amount of resources required to complete the task.

In this paper, we propose a general framework for resource-bounded informa-tion extraction, along with the design of a prototype system used to address the task of finding missing years of publication in citation records. We also present first results on this task. Resource-bounded Information Extraction encompasses several different types of problems. It deals with extracting information from a large corpus, such as the web; it actively acquires this information under resource constraints; and it exploits the interdependency within the data for best performance. Here we discuss various related tasks and how RBIE is uniquely positioned between them. 2.1 Traditional Information Extraction In the traditional information extraction settings, we are given a database schema, and a set of unstructured or semi-structured documents. The goal is to automat-ically extract records from these documents, and fill in the values in the given database. These databases are then used for search, decision support and data mining. In recent years, there has been mu ch work in developing sophisticated methods for performing information ext raction over a closed collection of doc-uments, e.g. [3]. Several approaches hav e been proposed for different phases of information extraction task, such as segmentation, classification, association and coreference. Most of these proposed appr oaches make extensive use of statistical machine learning algorithms, which have improved significantly over the years. However, only some of these methods remain computationally tractable as the size of the document corpus grows. In fa ct, very few systems are designed to scale over a corpus as large as, say, the Web [2,15]. 2.2 Information Extraction from the Web There are some large scale systems that extract information from the web. Among these are KnowItAll [2], InfoSleuth [11] and Kylin [14]. The goal of the KnowItAll system is a related, but different task called,  X  X pen Information Extraction X . In Open IE, the relations of interest are not known in advance, and the emphasis is on discovering new relations and new records through extensive web access. In contrast, in our task, what we are looking for is very specific and the corresponding schema is known. The emphasis is mostly on filling the missing fields in known records, using resource-bounded web querying. Hence, KnowItAll and RBIE frameworks have very different application domains. In-foSleuth focuses on gathering informat ion from given sources, and Kylin focuses only on Wikipedia articles. These systems also do not aim to exploit the inherent dependency within the database for maximum utilization of resources.

The Information Retrieval community is rich with work in document relevance (TREC). However, traditional information retrieval solutions can not directly be used, since we first need to automate the query formulation for our task. Also, most search engine APIs return full do cuments or text snippets, rather than specific feature values.

A family of methods closely related to RBIE, is question answering systems [8]. These systems do retrieve a subset of relevant documents from the web, along with extracting a specific piece of informat ion. However, they target a single piece of information requested by the user, whereas we target multiple, interdependent fields of a relational database. They formulate queries by interpreting a natural language question, whereas we formulate and rank them based on the utility of information within the database. They do not address the problem of selecting and prioritizing instances or a subset of fields to query. This is why, even though some of the components in our system may appear similar to that of QA systems, their functionalities differ. The semantic web community has been working on similar problems, but their focus is not targeted information extraction. 2.3 Active Information Acquisition Learning and acquiring information under resource constraints has been studied in various forms. Consider these different scenarios at training time: active learn-ing selects the best instances to label f rom a set of unlabeled instances; active feature acquisition [10] explores the problem of learning models from incomplete instances by acquiring additional features; budgeted learning [9] identifies the best set of acquisitions, given a fixed co st for acquisitions. More recent work [12] deals with learning models using noisy labels. At test time, the two common scenarios are selecting a subset of featur es to acquire [13,1,7], and selecting the subset of instances for which to acquire features [6,5].

The interdependency within the data set is often conveniently modeled using graphs, but it poses interesting question s about selection of instances to query and propagating uncertainty through the graph [4]. In [5], the test instances are not independent of each other, and the impact of acquisition in the context of graph partitioning is studied. The general RBIE framework described in this pa-per aims to leverage these methods for both train and test time for optimization of query and instance selection.

In summary, RBIE requires a comprehens ive architecture for efficiently inte-grating multiple functionalities, such as instance and query selection, automatic query formulation, and targeted information extraction by exploiting inherent data dependency under limited resources. This leads us to the new framework presented in this paper.
 As described in the previous section, we need a new framework for performing information extract ion to automatically acquire s pecific pieces of information from a very large corpus of unstructured documents. Fig. 1 shows a top-level architecture of our proposed framework. In this section, we discuss the general ideas for designing a resource-bounded information extraction system. Each of these modules may be adapted to suit the needs of a specific application, as we shall see for our example task.
 3.1 Overview of the Architecture We start with a database containing missing values. In general, the missing information can either be a c omplete record, or values of a subset of the features for all records, or a subset of the records. We may also have uncertainty over the existing feature values that can be reduced by integrating external information. We assume that the external corpus prov ides a search interface that can be accessed automatically, such as a search engine API.
 The information already available in the database is used as an input to the Query Engine . The basic function of the query engine is to automatically formu-late queries, prioritize them optimally, and issue them to a search interface. The documents returned by the search interface are then passed on to the Document Filter . Document Filter removes documents that are not relevant to the original database and ranks the remaining documents according to the usefulness of each document in extracting the required information.

A machine learning based information extraction system extracts relevant features from the documents obtained from the Document Filter, and combines them with the features obtained from the original database. Hence, information from the original database and the external source is now merged, to build a new model that predicts the values of missing fields. In general, we may have resource constraints at both training and test times. In the training phase, the learned model is passed to the Confidence Evaluation System , which evaluates the effectiveness of the model learned so far and recommends obtaining more documents through Document Filter, or issuing more queries through the Query Engine in order to improve the model. In the test phase, the prediction made by the learned model is tested by the Confidence Evaluation System. If the model X  X  confidence in the predicted value crosses a threshold, then it is used to fill (or to replace a less certain value) in the original database. Otherwise, the Confidence Evaluation System requests a new do cument or a new query to improve the current prediction. This loop is continued until either all the required information is satisfactorily obtained, or we run out of a required resource. Additionally, feedback loops can be designed to help improve performance of Query Engine and Document Filter.

This gives a general overview of the proposed architecture. We now turn to a more detailed description for each module, along with the many design choices involved while designing a system for our specific task. 3.2 Task Example: Finding Paper X  X  Year of Publication We present a concrete resource-bounded information extraction task and a prob-abilistic approach to instantiate the framework described above: We are given a set of citations with fields, such as, paper title, author names, contact informa-tion available, but missing year of publication. The goal is to search the web and extract this information from web documents to fill in the missing year values. We evaluate the performance of our sys tem by measuring the precision, recall and F1 values at different confidence levels. The following sections describe the architecture of our prototype system, along with possible future extensions. 3.3 Query Engine The basic function of query engine is to automatically formulate queries, pri-oritize them optimally, and issue them to a search interface. There are three modules of query engine. The available r esources may allow us to acquire the values for only a subset of the fields, for a subset of the records. Input selection module decides which feature values should be acquired from the external source to optimize the overall utility of the database. The query formulation module combines input values selected from the database with some domain knowledge, and automatically formulates queries. For instance, a subset of the available fields in the record, combined with a few keywords provided by the user, can form useful queries. Out of these querie s, some queries are more successful than others in obtaining the required information. Query ranking module ranks the queries in an optimal order, requiring fewer queries to obtain the missing values. In the future, we would like to explore sophisticated query ranking methods, based on the feedback from other components of the system.

In our system, we use existing fields of the citation, such as paper title and names of author, and combine them with keywords such as  X  X v X ,  X  X ublication list X , etc. to formulate the queries. We experiment with the order in which we select citations to query. In one method, the nodes with most incoming and outgoing citation links are queried first. We issue these queries to a search API and the top n hits (where n depends on the available resources) are obtained. 3.4 Document Filter The primary function of the document filter is to remove irrelevant documents and prioritize the remaining documents for processing. Following are the two main components of the Document Filter . Even though queries are formed using the fields in the database, some documents may be irrelevant. This may be due to the ambiguities in the data (e.g. pers on name coreference), or simply imper-fections in retrieval engine. Initial filter removes documents which are irrelevant to the original database. The remaining documents are then ranked by docu-ment ranker, based on their relevance to the original database. Remember that the relevance used by the search interface is with respect to the queries, which may not necessarily be the same as the relevance with respect to the original database. In the future, we would like to learn a ranking model, based on the feedback from the information extraction module (via Confidence Evaluation System ) about how useful the document was in making the actual prediction.
In our system, many of the returned documents are not relevant to the orig-inal citation record. For example, a query with an author name and keyword  X  X esume X  may return resumes of differe nt people sharing a name with the paper author. Hence, even though these docum ents are relevant to an otherwise use-ful query, they are irrelevant to the original citation. Sometimes, the returned document does not contain any year information. The document filter recognizes these cases by looking for year information and soft matching the title with body of the document. 3.5 Information Extraction The design of this module differs from traditional information extraction, posing interesting challenges. We need a good integration scheme to merge features from the original database with the features obtained from the external source. As new information (documents) arrives, the parameters of the model need to be updated incrementally (at train time), and the confidence in the prediction made by the system must be updated efficiently (at test time).
 Probabilistic Prediction Model. In our task, the field with missing values can take one of a finite number of possible values (i.e. a given range of years). Hence, we can view this extraction task as a multi-class classification problem. Features from the original citation and web documents are combined to make the prediction using a maximum entropy classifer.

Let c i be a citation ( i =1 ,...,n ), q ij be a query formed using input from citation c i and d ijk be a document obtained as a result of q ij . Assuming that we use all the queries, we drop the index j .Let y i be a random variable that assigns a label to the citation c i . We also define a variable y ik to assign a label to the document d ik .If Y is the set of all years in the given range, then y i ,y ik  X  Y . For each c i , we define a set of m feature functions f m ( c i ,y i ). For each d ik ,we f m ( c i ,y i ) is empty. This is because the information from the citation by itself is not useful in predicting the year of publication. In the future, we would like to design a more general model that takes these features into account. We can now construct a model given by where Z d = y exp(  X  l f l ( c i ,d ik ,y ik )) Combining Evidence in Fea ture Space vs. Output Space. The above model outputs y ik instead of the required y i . We have two options to model form a single feature function. This is equivalent to combining all the evidence for a single citation in the feature space. Alternatively, we can combine the evidence from different d ik  X  X  in the output space. Following are two possible schemes for combining the evidence in the output space. In the first scheme, we take a majority vote , i.e., the class with the highest number of y ik is predicted as the winning class and assigned to y i . In the second scheme, highest confidence scheme, we take the most confident vote, i.e., y i = argmax y 3.6 Uncertainty Propagation in Citation Graph The inherent dependency within the given data set can be exploited for better resource utilization. In our case, the citation link structure can be used for infer-ring temporal constraints. For example, if paper A cites paper B, then assuming that papers from future can X  X  be cited, we infer that B must have been published in the same or earlier year than A. Initially, we have no information about the publication year for a citation. As information from the web arrives, this uncer-tainty is reduced. If we propagate this reduction in uncertainty (or belief) for one of the nodes through the entire graph, we may need fewer documents (or fewer queries) to predict the publicatio n year of the remaining nodes. Selecting the citations to query in an effective order may further improve efficiency. Notation. Let c  X  C be the citation which is currently being queried. Let a  X  b denote that citation a cites citation b .Let C { c of c ; P c ( X = x ) be the probability that it takes one of finite values in the given range, and P ( X = x ) be the posterior probability from the Document Classifier . Propagation Methods. The method Best Index passes the uncertainty mes-sage to the neighbors of c as follows: Where y = argmax y P c ( X = y ). P ( X = x | x  X  y )and P ( X = x | x&lt;y )aregiven by one of the update methods described below. The method Weighted Average takes a weighted average over all possible y s : Update Methods. If we know that the given paper was published after a certain year, then we can set the probability mass from before the corresponding index to zero and redistribute it to the years after the index. We only show update in one direction here for brev ity. The first update method, Uniform Update , simply redistributes the probability mass, P ( x  X  y ) uniformly to the remaining years. The second update method, Scale Update , uses conditional probability. Combination Methods. Along with passing a message to its neighbors, the node updates itself by combining information from the Document Classifier and the graph structure.
 The following options can be used for computing P c ( X = x ). Basic , P ( X = x | x = y ) Product P c ( X = x )  X  P c ( X = x )and Sum P c ( X = x )+ P c ( X = x ) 3.7 Confidence Evaluation System At train time, after adding each new training document, the Confidence Eval-uation System can measure the  X  X oodness X  of the model by evaluating it on a validation set. At test time, confidence in the prediction improves as more in-formation is obtained. It sets a threshold on the confidence, to either return the required information to the database, or to request more information from external source. It also makes the choice between obtaining a new document or to issue a new query at each iteration, by taking into account the cost and util-ity factors. Finally, it keeps track of th e effectiveness of queries and documents in making a correct prediction. This information is useful for learning better ranking models for Query Eng ine and Document Filter.

In our system, we train our model using all available resources, and focus on evaluating test time confidence. For merging evidence in the output space, we employ two schemes. In max votes , we make a prediction if the percentage of documents in the winning class crosses a threshold. In highest confidence ,we make a prediction if P ( y ik | c i ,d ik ) value of the document with the highest P in the winning class passes a threshold. These schemes help determine if we have completed the task satisfactorily. For combining evidence in feature space, we use the Entropy Method , in which we compute the value H =  X  i p i log p i of the current distribution, and compa re it against the confidence threshold. 4.1 Dataset and Setup Our data set consists of five citation graphs (462 citations) , with years of pub-lication ranging from 1989 to 2008. The sampling process is parameterized by size of the network (20-100 citations per graph) and density (min in-degree = 3 and min out-degree = 6). We use five-fold cross validation on these data sets for all our experiments. We use the Mallet infrastructure for training and testing, and the Google search API to issue queries. The queries formed using the in-formation from input citations include the raw title, title in quotes, and author names combined with keywords like  X  X ublication list X ,  X  X esume X ,  X  X v X  ,  X  X ear X  and  X  X ear of publication X . We issue queries in a random order, and obtain top 10 hits from google. We use around 7K queries and obtain around 15K documents after filtering. The documents are tokenized and tokens are tagged to be possi-ble years using a regular expression. The document filter discards a document if there is no year information found on the webpage. It also uses a soft match between the title and all n-grams in the body of the page, where n equals the title length. The selected document s are passed on in a random order to the MaxEnt model, which uses the following features for classification: Occurrence of a year on the webpage; the number of unique years on the webpage; years on the webpage found in any particular order; the years that immediately follow or precede the title matches; the distance between a  X  X urrounding X  year and its corresponding title match and occurrence o f the same  X  X ollowi ng X  and  X  X receding X  year for a title match. 4.2 Results and Discussion We first run our RBIE system without exploiting the citation network informa-tion. We first present the results for com bining evidence in the feature space. We measure Precision, Recall and F1 based on using a confidence threshold, where F1 is the harmonic mean of precision and r ecall. As seen in table 1, as we increase the entropy threshold, precision drops, as expected. F1 peaks at threshold 0.7. Note that the number of documents is proportional to the number of queries, because in our experiments, we stop obta ining more documents or issuing queries when the threshold is reached.

Next, we present the results of exploiting citation network information for better resource utilization. Fig. 2 shows F1 as well as the fraction of the total documents used for the baseline method, and for one of the graph based method ( Weighted Avg propagation, Scaling update, and Basic combination). The F1 values are smaller compared to the base line because we use far fewer resources, and the uncertainty propagation methods are not perfect. Using this method, we are able to achieve 87.7% of the baseline F1, by using only 13.2% of the documents. This demonstrates the effectiv eness of exploiting relational nature of the data. Table 2 shows the results of different uncertainty propagation methods at entropy threshold 0.7.

We also experiment with combining evidence in the output space using the two schemes described in section 3.5, and the confidence evaluation schemes described in section 3.7. Fig. 3 shows t he four precision-recall curves. We see that for High Confidence Confidence evaluation scheme (fig. 3(a),(c)), we obtain high values of precision and recall for reasonable values of confidence. That is, in the confidence region below 0.9, we obtain a good F1 value. Especially, the Majority Vote -High Confidence scheme (fig. 3(c)) performs exceptionally well in making predictions. However, in the confidence region between 0.9 to 1.0, the Max Vote scheme (fig. 3(b),(d)) gives a better degradation performance. We propose a new framework for targeted information extraction under resource constraints to fill missing values in a d atabase. We present first results on an example task of extracting missing year of publication of scientific papers, along with exploiting the underlying citation network for better resource utilization. The overall framework is flexible, and can be applied to a variety of problem do-mains and individual system components can be adapted to the task. The specific methods recommended here can also be generalized in many different relational domains, especially when the dataset has an underlying network structure. In future, we would like to explore more sophisticated uncertainty propagation methods, such as belief-propagation. We would also like to develop individual components like Query Engine and Document Filter , by using good ranking procedures. Finally, it would be interesting to see how these methods extend to extracting multiple interdependent fields.
 We thank Andrew McGregor for useful discussions. This work was supported in part by the Center for Intelligent Information Retrieval and in part by The CIA, the NSA and NSF under NSF grant #II S-0326249. Any o pinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor.

