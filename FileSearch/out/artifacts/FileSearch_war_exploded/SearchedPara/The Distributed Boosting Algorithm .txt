 In this paper, we propose a general framework for distributed boosting intended for efficient integrating specialized classifiers learned over very large and distributed homogeneous databases that cannot be merged at a single location. Our distributed boost-ing algorithm can also be used as a parallel classification tech-nique, where a massive database that cannot fit into main com-puter memory is partitioned into disjoint subsets for a more effi-cient analysis. In the proposed method, at each boosting round the classifiers are first learned from disjoint datasets and then ex-changed amongst the sites. Finally the classifiers are combined into a weighted voting ensemble on each disjoint data set. The ensemble that is applied to an unseen test set represents an en-semble of ensembles built on all distributed sites. In experiments performed on four large data sets the proposed distributed boost-ing method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requir-ing less memory and less computational time. In addition, the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large-scale databases. Boosting, distributed learning, classifier ensembles. The number and the size of databases are rapidly growing in vari-ous business and scientific fields thus resulting in an exceptional opportunity to develop automated data mining techniques for extracting useful knowledge from massive data sets. This problem may be further complicated by the fact that in many cases, the databases are located at multiple distributed sites. Data may be distributed across a set of sites or computers for several reasons. For example, several data sets concerning business information (e.g. telephone or credit card fraud) might be owned by separate organizations that have competitive reasons for keeping the data private. In addition, these data may be physically dispersed over many different geographic locations. However, business organiza-tions may be interested in enhancing their own models by ex-changing useful information about the data. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the t~rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA Copyright ACM 2001 1-58113-391-x/01/08...$5.00 nique for distributed learning. The only exception was boosting for scalable and distributed learning [5], where each classifier was trained using a small fraction of the training set. In this distributed version, the classifiers were trained either from random samples (r-sampling) or from disjoint partitions of the data set (d-sampling). In r-sampling, a fixed number of examples were ran-domly picked from the weighted training set (without replace-ment), where all examples had equal chance of being selected. In d-sampling, the weighted training set was partitioned into a num-ber of disjoint subsets, where the data from each site was taken as a d-sample. At each round, a different d-sample was given to the weak learner. Both methods can be used for learning over very. large data sets, but d-sampling is more appropriate for distributed learning, where data at multiple sites cannot be pulled together to a single site. The reported experimental results indicated that their distributed boosting is either comparable to or better than learning single classifiers over the complete training set, but only in some cases comparable to boosting over the complete data set. The modifications of the boosting algorithm that we propose here are variants of the AdaBoost.M2 procedure [6], which proceeds in a series of Trounds. In every round t, a weak learning algorithm is called and presented with a different distribution Dt that is altered by emphasizing particular training examples. The distribution is updated to give wrong classifications higher weights than correct classifications. The entire weighted training set is given to the weak learner to compute the weak hypothesis h t. At the end, all weak hypotheses are combined into a final hypothesis hf,. The boosting algorithm may be appropriate for distributed learn-ing for several reasons: it can be applied to a wide variety of al-gorithms, it is superior to other combining methods and its weighted voting ensemble can easily scale the magnitudes of clas-sifters giving a large weight to a strong hypothesis thus correcting wrong classifications of weaker hypotheses. In addition, a natural way of learning in a distributed environment is by combining classification predictors. Our aim, hence, is to exploit all of these advantages in order to apply boosting to distributed learning. As classifiers, we trained multilayer (2-layered) feedforward neural network models with the number of hidden neurons equal to the number of input attributes, and with the number of output nodes equal to the number of classes, where the predicted class is from the output with the largest response. We used two learning algo-rithms: resilient propagation [11] and Levenberg-Marquardt [8]. The objective of our distributed boosting algorithm is to effi-ciently construct a prediction model using data at multiple sites such that the prediction accuracy is similar to boosting when all the data are centralized at a single site. Towards such an objective, we propose several modifications of the boosting algorithm within the general framework presented at Figure 1. All distributed sites perform the learning procedure at the same time. Assume there are k distributed sites, where site j contains set Sj with mj examples, j = 1 .... k. Data sets Sj contain the same attrib-utes and do not necessarily have the same size. During the boost-ing rounds, site j maintains a local distribution Aj, t and local weights wja that directly reflect the prediction accuracy on that site. vector wt, such that the q-th interval of indices [ ~mp +1, ~mp ] in the weight vector wt corresponds to the weight vector wq.t from the q-th site. The weight vector wt is used to update the global distribution D t as in step 5 at Figure 1. How-ever, merging all the weight vectors wj.t requires a huge amount of time for broadcasting, since they directly depend on the size of the distributed data sets. In order to reduce this transfer time, instead of the entire weight vectors wj, t, only the sums Vj.t of all their ele-ments are broadcast (step 9 in Figure 1). Since data sitej samples only from set Sj, there is no need to know exact values of the ele-ments in the weight vectors wq.t (q ej, q =1 .... k) from other dis-tributed sites. Instead, it is sufficient to know only how many data examples need to be sampled from the site q. Therefore, each sitej creates a weight vector Uj, t (step 10, Figure 1), where its j-th interval [ ~-~mt, +l,~-~mp ] represents the weight vector wi, t, while all the other intervals that correspond to the weight vectors from other distributed sites may be set arbitrar-ily such that the values inside the q-th interval of indices (q e j) sum to the value Vq, t. The simplest method is to set all values in the q-th interval to the value Vq,/mq. Using this method, expensive broadcasting of the huge weight vectors is avoided, while still pre-serving the information which site is more difficult to learn and where more examples need to be sampled. As a result, each site at round t maintains its version D u of the global distribution Dr, and its local distribution Aj, t. At each site j, the samples in boosting rounds are drawn according to the distri-bution Dj, t, but the sampled training set Qj, t for site j is created only from those data points that match the indices drawn from the j-th interval in the distribution D u (step 1, Figure 1). The classifi-ers Li, t are constructed on each of the samples Q.u and then ex-changed among the distributed sites at each boosting round t. Since all sites contain a set of classifiers L u, j = 1...k, the next steps involve creating an ensemble Ezt by combining these classi-fiers and computing a composite hypothesis h.i,t. The local weight vectors wi, t are updated at each sitej in order to give wrong classi-fications higher weights than correct classifications (step 8, Figure 1) and then their sums Vj, t are broadcast to all distributed sites. Each sitej updates its local version Dj, t according to the created weight vector U~;t. At the end, the composite hypotheses h.u from different sites and different boosting iterations are combined into a final hypothesis hf,. We explore several variants of the proposed distributed boosting algorithm from Figure 1. The algorithms differ in (a) the method for combining the classifiers into an ensemble Ej, t (step 4), (b) computing a representative hypothesis hi, t (step 5) and (c) updat-ing the weights w~,t (step 8). In the first distributed learning algorithm, denoted as Competing Classifiers from Distributed Sites, the learned classifiers Lj, t from all distributed sites are combined such that each data instance on a local site is assigned to the classifier with the highest prediction confidence on that data pattern. As a result, the composite hy-pothesis hj, t uses a different classifier Lj, t for each data example. split into five sets of 10000 examples each, where four of them were used for distributed learning, and the fifth data set was used as a test set. The LED data set was generated with 10000 exarn-pies and 10 classes, where four sets with 1500 examples were used for training in a distributed environment, and the set with 4000 examples was used for testing. The Covertype data set, cur-rendy one of the largest databases in the UCI Database Reposi-tory, contains 581012 examples with 54 attributes and 7 target classes representing the forest cover type for 30 x 30 meter cells obtained from US Forest Service (USFS) Region 2 Resource In-formation System [1]. In Covertype data set, 40 attributes are binary columns representing soil type, 4 attributes are binary col-umns representing wilderness area, and the remaining I0 are con-tinuous topographical attributes. Since the training of neural net-work classifier would be very slow if using all 40 attributes repre-senting a soil type variable, we transformed them into 7 new or-dered attributes. These 7 attributes were determined by computing the relative frequencies of each of 7 classes in each of 40 soil types. Therefore, we used a 7-dimensional vector with values that could be considered continuous and therefore more appropriate for use with neural networks. This resulted in the transformed data set with 21 attributes. The 149982 data instances separated into 8 disjoint data sets were used for distributed learning, while the 431032 data examples were used for out of sample testing. The major advantage of the proposed distributed boosting algo-rithm is that it requires significantly less computational time per each boosting round since the classifiers are learned on smaller data sets. Figure 3 shows how the time required for training neural networks (NN) depends on the number of examples in the training set for all four reported data sets when measured on a Pentium HI processor with 768 MB of main memory. Analyzing the Figure 3a, it is evident that the time needed for constructing a NN classifier on the three times reduced synthetic spatial training set resulted in more than three times faster computing time, while for LED and Waveform data sets, four times smaller data set caused more thma four times faster learning (Figure 3b, 3c). Finally, for Covertype data set, time needed for training a NN on an eight times smaller data set was more than eight times smaller than time required for training a NN when using the entire training set (Figure 3d). Figure 3. The time needed for learning neural network (NN) classifiers for different sizes of four different data sets showed similar prediction accuracy for all proposed variants of boosting algorithm probably due to high homogeneity of data (Figure 5c, 5d). Results from experiments performed on the synthetic spatial data sets indicate that the methods of voting the classifiers constructed on multiple sites achieved approximately the same classification accuracies as the standard boosting algorithm on merged data (Figure 5a), while the method of competing classifiers was always significantly less accurate than standard boosting (Figure 5a). This was probably due to the fact that none of the classifiers con-structed on the multiple sites were sufficiently competent for pre-diction on the unseen test set, and the prediction results were comparable or even slightly worse than when making predictions from a single distributed site (Figure 5a). ~t~-~ X ' --:-:-:-= .... ~;;~ii',~---~---: .... = ....... .................. Figure 5. Out of sample averaged classification accuracies of different boosting algorithms distributed over (a) 3 synthetic spatial data sets; (b) 8 Covertype data sets (c) 4 LED data sets; (d) 4 Waveform data sets (---Standard Boosting, ... Simple Majority, ~ Weighted Majority, ooo Competing among classi-fiers, &gt;o&lt;x Boosting from a single site) When performing the experiments on the Covertype data set (Fig-ure 5b), the voting algorithms for distributed learning were consistently comparable in prediction accuracy to standard boosting on the centralized data. The method of competing classifiers was almost as accurate as standard boosting for large data sets, but slightly worse than standard boosting for smaller data sets (Figure 5-b3). It is also noticeable that for achieving the maximal prediction accuracy the larger number of boosting Finally, we also performed experiments on 3 synthetic spatial data sets using the confidence-based method of combining classifiers with all three modifications for dividing the weights wj. t by the factor acc p (p=0,1,2) (Figure 6b). The graphs in Figure 6b show that the confidence-based combining classifiers slightly outper-formed standard boosting applied on centralized data as well as the other methods considered for distributed boosting. The im-provement in prediction accuracy was more significant when learning from smaller data sets, but instability was also more evi-dent for smaller data sets (Fig. 6-b3). The increase in prediction accuracy with decreasing the data sets was probably due to the fact that the data sets were homogeneous and more data points were needed in order to improve the generalizability of our mod-els. When the number of data instances decreased, there were not enough examples to learn data distribution on a single site, but the variety of data instances from multiple sites still helped in achiev-ing diversity of built classifiers. Due to homogeneous distributions, the experiments performed on LED, Waveform and Covertype data sets again demonstrated the small observable difference in accuracy between the standard boosting and all variants of confidence-based distributed boosting algorithms when p = 0, 1 and 2 (Table 1). A framework for distributed boosting is proposed. It is intended to efficiently learn stable non-linear classifiers over large and dis-tributed homogeneous databases that cannot fit into the computer main memory. Experimental results on several data sets indicate that the proposed boosting techniques can effectively achieve the same or even slightly better level of prediction accuracy than standard boosting when applied to centralized data, while the cost of learning and memory requirements are considerably lower. This paper raised several interesting issues that recently have gained a lot of attention. First, successful learning from very large and potentially distributed databases imposes major performance challenges for data mining, since learning a monolithic classifier can be prohibitively slow due to the requirement that all the data need to be held in the main memory. Second, many distributed data sets cannot be merged together due to a variety of practical constraints including data dispersed over many geographic loca-tions, security services and competitive interests. Third, the pre-diction accuracy of employed data mining algorithms is of funda-mental impact for their successful application. Finally, the compu-tational time required for constructing a prediction model is be-coming more important as the amount of available data is con-stantly growing. Our experiments performed on several data sets indicate that the proposed boosting techniques successfully over-come these concerns, thus offering a fairly general method for effective and efficient learning in distributed environment. Although performed experiments have provided evidence that the proposed methods can be successful for distributed learning, fu-ture work is needed to fully characterize them especially in dis-tributed environment with heterogeneous databases, where new algorithms for selectively combining classifiers from multiple sites with different distributions are worth considering. It would also be interesting to examine the influence of the larger number of distributed sites and their sizes to the achieved prediction accu-racy, speedup and scale up and to establish a satisfactory trade off. 
