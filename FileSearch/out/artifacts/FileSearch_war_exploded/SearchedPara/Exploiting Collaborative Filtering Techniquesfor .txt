 As the Internet technology develops at a staggering rate, an increasing num-ber of tests such as TOEFL-IBT are taken online instead of in the traditional paper-based pattern. Compared with paper-based test(PBT), computer-based test (CBT) and Internet-based test(IBT) can save much labor of human raters and resources. Although computers are capable of assessing answers of students to some types of questions like multiple choice questions, human raters are still indispensable at present because it is a challenge for computers to accurately assess free-text responses of students to some questions such as English writing tasks. Also, current e-learning systems have limited capability for giving stu-dents feedback and providing automatic assessment since there is no established technology for assessing natural language responses to questions. Therefore, au-tomatic assessment of student answers is worthy of investigation.
 2013 [1] tries to deal with the challenge of automatic assessment of student responses. The goal of the task is to grade student answers for enabling well-targeted and flexible feedback in a tutorial dialogue setting.
 used LSA [2], classifiers based on  X  X ag-of-words X  features [3] to determine if a student answer corresponds to one of the expected correct or incorrect answers anticipated by a system designer. More recently, [4, 5] formulated the problem of assessing student input in terms of recognizing textual entailment. response based on only information about its corresponding question such as ref-erence answers and grading level of responses to this question. In other words, they do not take into consideration the grading information about responses to other questions. Unlike these conventional approaches, this paper exploits the idea of collaborative filtering to analyze the student responses, which predicts the grading level of a student response based on both the grading records of its corresponding question and the global information of gradings across the dataset. It is not difficult to understand the fact that the global grading in-formation is useful for accurately assessing the responses to a specific question because this information can tell us which grading level the most responses get and what kind of responses tend to get high or poor grades. For instance, a response whose text is  X  X  don X  X  know X  is always a poor response to whatever questions and this fact can help accurately predict such responses to a unseen question in the test set. Furthermore, in a recommendation perspective, if two users have similar shopping records, then they may have the similar preferences for items; likewise, some questions may have similar  X  X reference X  for semantic information of student responses. Assuming that good responses to given two questions always share many features, if a response to one of the questions is similar to a good response to the other question, then it is likely to be a good response to its corresponding question. Based on this intuition, we consider this task as a rating prediction problem where we try to predict the  X  X ating X  of questions(users) to student responses(items) by using a popular collaborative filtering model  X  feature-based matrix factorization model. Since it is a new task in both natural language processing and educational tech-nology community, we briefly describe the task of automatic assessment of stu-dent responses. The goal of this task is to assess student answers to exercise ques-tions that can be useful in tutorial dialogue and/or e-learning systems. Specif-ically, given a question, a known correct  X  X eference answer X , a set of student answers with manually annotated grading levels and a 1-or 2-sentence student answer, the goal is to determine the student X  X  answer accuracy.
 we mainly address the 5-way task, where the system is required to classify the student answer according to one of the following judgments:  X  Correct , if the student answer is a complete and correct paraphrase of the  X  Partially correct incomplete , if the student answer is a partially correct  X  Contradictory , if the student answer explicitly contradicts the reference an- X  Irrelevant , if the student answer is  X  X rrelevant X , talking about domain con- X  Non domain , if the student answer expresses a request for help, frustration which is a set of transcripts of students interacting with an intelligent tutorial dialogue system for teaching conceptual knowledge in the basic electricity and electronics domain. Fig. 1 shows a snippet of the Beetle dataset.
 In this section, we discuss how to use collaborative filtering models to deal with the challenge. We first present the intuition of collaborative filtering in this task. Then, a popular collaborative filtering model  X  feature-based matrix factoriza-tion model is to be discussed in detail in Section 3.2. Finally, Section 3.3 presents a re-ranking method for re-ranking the marginal predictions. 3.1 Why collaborative filtering Collaborative filtering is one of the most promising technologies for recommender systems. In the newer, narrower sense, collaborative filtering is a method of mak-ing automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underly-ing assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B X  X  opinion on a different issue x than to have the opinion on x of a person chosen randomly. This intuition is very popular in the online shopping recommendation.
 addressed using the idea of collaborative filtering. For a given question, different student responses may be graded with different grading levels just as different items might be rated with different ratings by a given user under online shopping scenario.
 spectively. Question of q1 is  X  X hy are wires made of copper? X  and question of q2 is  X  X an silver conduct? If so, why can we hardly see wires made of silver? X . By analyzing the student responses, we can find responses involving the issue of conductivity and price to both q1 and q2 are graded with a high grade such as Correct . In contrast, responses which do not involve these two aspects are graded with a poor grade like Irrelevant for the two questions. Based on the facts mentioned above, assuming that r1 and r2 are a response to q1 and q2 respectively and they are very similar (e.g. similar unigram features), if r1 is a good response to q1, then a response r2 which is similar to r1 is very likely to be a good response to q2 and vice-versa because q1 and q2 have the similar  X  X ref-erences X  for the semantic information concerning  X  X onductivity X  and  X  X rice X , which is exactly the idea of collaborative filtering.
 graded with a poor grade for whatever questions. For a collaborative filtering model, a negative bias will be assigned to such responses. As a result, even for a new question without any prior information or reference answers, such responses will be graded with a poor grading level by the collaborative filtering model, which can hardly be handled by other previous models.
 tive filtering model  X  feature-based matrix factorization model to address the challenge. Unlike the previous models which predict the grading level of a given response based on only information about its corresponding question such as reference answers and grading records of responses to this question, the feature-based matrix factorization model predicts the grade of a response based on both the grading records of its corresponding question and the global information of gradings across the dataset. Furthermore, this model can capture key seman-tic information with a latent factor space, which also contributes to a better performance than baseline models. 3.2 Feature-based Matrix Factorization Model for Automatic One of collaborative filtering models is latent factor models. Latent factor models transform both items and users to the same latent factor space which tries to ex-plain ratings by characterizing both products and users on factors automatically inferred from user feedback.
 is first proposed by Simon Funk. Funk-SVD maps both users and items to a joint latent factor space of dimensionality f , such that user-item interactions are modeled as inner products in that space, as shown in Fig. 2.
 elements of q i measure the extent to which the item possesses those factors. For a given user u , the elements of p u measures the extent of interest the user has in items. The dot product, p T u q i , captures the interaction between user u and item i , i.e. the overall interest of the user in characteristics of the item. descent approach (SGD) by minimizing loss function such as root mean squared error (RMSE).
 recommendation problems well. However, it is not difficult to find that the task of predicting rating a user rates to an item is very similar to the task of assessing student responses given a question.
 we regard questions as users, responses as well as reference answers as items and grading levels as ratings. For the 5-way task, the grading levels Correct , Partially correct incomplete , Contradictory , Irrelevant and Non domain are mapped to the rating space R which is defined as follows: each column represents a response and each element of the matrix is the grade level of a response(column) to a question(row). For the factor matrices P and Q , the dimension f i is a latent semantic dimension. For the example mentioned by Section 3.1, f 1 might represent the semantic information about conductivity and f 2 might represent the information about price. If the value of f 1 of a question is large, then it means that the desirable answer to this question should contain sufficient information about conductivity. On the other hand, if the value of f 1 of a response is large, then it means that the response provides much information about conductivity. In this way, some complex information can be captured by the latent factor spaces and also  X  X reference X  of a question for a response is modeled.
 because this model does not use any features about questions and responses ex-cept their IDs. Since each response corresponds to only one question, features are extremely sparse and responses to different questions do not have any con-nection even if their text is very similar. For example, assuming that there are two responses to two different questions and text of these two responses is the same, e.g. their text is  X  X  don X  X  know X , but the model cannot be aware of that the responses are the similar because their features i.e. their IDs are totally different and they are never  X  X ated X  by the same questions. Likewise, similar questions also cannot be identified by this model. As a result, this model cannot work at all.
 tures for profiling questions and answers in order to make features less sparse. For leveraging more features of questions and responses, feature-based matrix factorization which was proposed by [8] is used. This model generalizes the Funk-SVD model and is an abstract of many variants of matrix factorization models, and new types of information can be utilized by simply defining new features. The framework of feature-based matrix factorization is shown by Equation 2 where  X  is a constant indicating the global mean value of rating, b ( g ) , b ( u ) and b ( i ) are biases of global features, user features and item features respectively, p and q represent factors of features of users and items respectively and  X  ,  X  and  X  are weights of user features, item features and global features respectively. as the feature of questions and bag-of-words of responses as the response fea-tures. The reason why we do not select text of questions as features is that the question text is very confusing. For example, text of many questions is a word such as  X  X hy X . The specific feature-based matrix factorization model for our task is shown in Equation 3, in which S ( i ) is the set of features of the response i . 3.3 Re-ranking the marginal predictions Feature-based matrix factorization is naturally a regression model so its predic-tion of each test example is a numeric instead of a nominal class. Although we can use rounding-off method to remap the numeric to class label, it is not effec-tive for some cases. For a test example predicted with a marginal score e.g. 4.5, it is difficult to tell whether it should be graded as Correct or Partially correct for the matrix factorization model. Therefore, we used a maximum entropy classifier to re-rank such marginal predictions.
 example t  X  T , MaxEnt classifier serves re-predicting their classes. T is defined as follows and pred ( t ) is the predicting score by the matrix factorization model. In this section, we first introduce the experimental settings in detail. Then we discuss the results and give an analysis. 4.1 Experimental Setting Dataset The dataset we used for evaluation is the Beetle Dataset, which con-tains 47 questions. Each question is associated with 1 to 10 different reference answers provided by experienced tutors and dozens of student responses. Pre-processing In the pre-processing step, we filter out stop words and perform lemmatization to each token. Also, we normalize some abbreviations by using a mapping rule.
 Evaluation We first perform cross validation to evaluate the performance of our model. To simulate the scenario of unseen answers [1], we randomly divided the student responses into 20 groups and perform 20-fold cross validation. The test set in each fold contains 197 or 198 test examples of which there are on average 4.2 student responses to each question. Then, we train our model on the whole training set and predict the grading level of responses in the official test set. [9] and empirically set the number of factors f = 50, the global constant  X  = 4, the learning rate  X  = 0 . 005 and regularization parameter  X  = 0 . 004. 4.2 Experimental Results We set the following models which only use lexical features as baselines for evaluations: Baseline1: Majority Class The majority class baseline is to assign Correct (the most frequent class) to every test instance. Baseline2: Lexical Similarity The lexical similarity baseline is a simple de-cision tree classifier with features such as lexical similarity and overlap by using an implementation toolkit  X  Weka [10].
 Baseline3: MaxEnt Classifier This baseline is a maximum entropy classifier using bag-of-word features. The classifier also serves re-ranking the marginal predictions, as discussed in Section 3.3.
 task. We compare the performance of following models with these baselines: Model1: Feature-based matrix factorization model which predicts grading levels of student responses. We used the rounding-off method to map the numeric prediction to one of the five given classes.
 Model2: Feature-based matrix factorization model with re-ranking. The classi-fier for re-ranking is the model described by Baseline3.
 dation. For saving space, we use integers to represent the grade levels in Table 1 in which  X  X verall X  represents the overall accuracy of models.
 ously at the expense of serious errors. For instance, such a system would tell the students that they are correct even if they are saying something contradictory. This is reflected in a much lower macro-averaged F score. Compared with the majority baseline, Baseline2 and Baseline3 can assess student responses more ac-curately and achieve the overall accuracy 54.9% and 60.3% respectively but their abilities to capture semantic of responses are not so good as the feature-based matrix factorization model which tries to represent the semantic information in a latent factor space and achieved a performance of 60.9% overall accuracy. performance of feature-based matrix factorization models for the reason that errors due to marginal predictions made by the matrix factorization model are corrected. The matrix factorization model with re-ranking can achieve 63.6% overall accuracy.
 be identified most easily by all models except the majority baseline since the responses of these two grades have more distinct features than ones of other grades. In contrast, it seems quite difficult for models to identify irrelevant re-sponses since such responses may contain some important words mentioned in either reference answers or good responses. As a result, they are likely to be graded with a high grade. Therefore, identifying such responses requires deeper semantic analysis. On the other hand, the matrix factorization model seems good at handling high graded responses while MaxEnt classifier based on bag-of-word features (B3) seems better at dealing with low graded responses than the matrix factorization models. Therefore, when the matrix factorization model is com-bined with MaxEnt classifier, its weakness in handling the low-graded responses can be addressed to some extent and that is probably one of reasons why com-bining a MaxEnt classifier helps improve the performance of the factorization model.
 Table 2. The performance of the models is similar to that on the 20-fold cross validation and our matrix factorization model with re-ranking phase still per-forms best out of these models and achieves a performance of 62.6% overall accuracy, which is a competitive performance for a model which only exploits the lexical features without too much pre-processing such as spelling correction for the task. This performance can be ranked in the 3rd place in the 5way task on the beetle dataset in semeval-2013 task7. This paper addresses the task of automatic assessment of student responses in a novel perspective. We model the problem as a rating prediction problem and used a feature-based matrix factorization model to predict the grading levels of responses to their corresponding questions based on the idea of collabora-tive filtering. The experiments show that the feature-based matrix factorization model can deal with the assessment task better than classifiers based on lexi-cal similarity and bag-of-word features. Furthermore, the feature-based matrix factorization model combined with a re-ranking phase achieves a higher perfor-mance on the test set  X  62.6% overall accuracy  X  a competitive performance for this task, which proves the effectiveness of our model. Additionally, our model is so flexible that we can also exploit more promising features such as POS tag, category of questions, syntactic features and textual entailment in this model for better performance, which is to be explored in future work.
 This paper is supported by NSFC Project 61075067 and National Key Technol-ogy R&amp;D Program (No: 2011BAH10B04-03).

