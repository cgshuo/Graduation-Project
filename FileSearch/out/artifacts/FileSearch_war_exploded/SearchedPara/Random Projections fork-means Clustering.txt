 many practical applications [9].
 Let A be an n  X  d matrix containing n d -dimensional points ( A the n -point set formed by the rows of A . We say that an embedding f : A  X  R  X  d with f ( A Singular Value Decomposition (SVD), where one finds an embed ding with image  X  A = U of worst case analysis of the form we describe in this work. describe the approximability framework adopted in the k -means clustering literature and fix the notation. clusters, find the n  X  k indicator matrix X Here X denotes the set of all n  X  k indicator matrices X . The functional F ( A, X ) = A  X  XX  X  A 2 k and only if X where I 2.1 Approximation Algorithms for k -means clustering Finding X algorithms for k -means clustering. The following definition captures the fr amework of such efforts. returns an indicator matrix X In the above,  X  2.2 Notation Given an n  X  d matrix A and an integer k with k &lt; min { n, d } , let U of the top k left (resp. right) singular vectors of A , and let  X  k singular values of A in non-increasing order. If we let  X  be the rank of A , then A A that A  X  any two matrices C and T of appropriate dimensions, k CT k F  X  k C k F k T k 2.3 Random Projections with high probability over the randomness of R , as an added bonus, savings on the computation of the projecti on.  X  -approximation k -means algorithm. It returns an indicator matrix X
Input: n  X  d matrix A ( n points, d features), number of clusters k , error parameter  X   X  (0 , 1 / 3) , and  X  -approximation k -means algorithm.

Output: Indicator matrix X 3.1 Running time analysis with the matrix R described in Algorithm 1 and employ MatLab X  X  matrix-matrix BLAS implementation to proceed A , k ,  X  , and the  X  -approximation algorithm in order to construct an indicato r matrix X least 0 . 97  X   X  Proof of Theorem 1 Lemma 2. Assume that the matrix R is constructed by using Algorithm 1 with inputs A , k and  X  . novel results of general interest. Lemma 3. Under the same assumptions as in Lemma 2 and w.p. at least 0 . 99 , Proof. Let  X  = V  X  matrices, and V since V diagonal matrix. Assuming that, for all i  X  [ k ] ,  X  i -th diagonal element of  X  , respectively, it is Since  X  is a diagonal matrix, proof.
 Lemma 4. Under the same assumptions as in Lemma 2 and for any n  X  d matrix C w.p. at least 0 . 99 , the third statement of Lemma 2, the fact that k  X  1 , and Chebyshev X  X  inequality we get proof.
 Lemma 5. Under the same assumptions as in Lemma 2 and w.p. at least 0 . 97 , where E is an n  X  d matrix with k E k F  X  4  X  k A  X  A Proof. Since ( AR )( V  X  A The first statement of Lemma 2 implies that rank ( V  X  matrix. Replacing A To bound the second term above, we drop V  X  inequality and submultiplicativity: U second statement of Lemma 2 with S = A application of Markov X  X  inequality on the random variable A The second two terms can be bounded using Lemma 3 and Lemma 4 on C = A on Lemma 3, Lemma 4 and Inq. (8), we get that w.p. at least 0 . 97 , The last inequality holds thanks to our choice of  X   X  (0 , 1 / 3) . notice that X 4.1 The proof of Eqn. (4) of Theorem 1 We start by manipulating the term A  X  X Pythagorean theorem (the subspaces spanned by the componen ts A perpendicular) we get We first bound the second term of Eqn. (10). Since I  X  X increasing a unitarily invariant norm. Now Proposition 6 im plies that We now bound the first term of Eqn. (10): In Eqn. (12) we used Lemma 5, the triangle inequality, and the fact that I  X   X  X the fact that V  X  factor  X   X  appeared in the first term. To better understand this step, no tice that X satisfies In Eqn. (15) we used Lemma 4 with C = ( I  X  X Mac machine with a dual core 2.26 Ghz processor and 4 GB of RAM. Our empirical findings are very promising large-scale data. 5.1 An application of Algorithm 1 on a face images collection version of F , i.e.  X  F = F/ || A || 2 with the approach of running the Lloyd X  X  heuristic on the ori ginal high dimensional data. of of which corresponds to an almost 90% speedup of the overall procedure. reduction step) are T ( t = 100 ) approximation accuracy results. 5.2 A note on the mailman algorithm for matrix-matrix and mat rix-vector multiplication Based on these findings, we chose to use (MM1) for our experime ntal evaluations. Acknowledgments: Christos Boutsidis was supported by NSF CCF 0916415 and a Ger ondelis Foundation Fellow-ship; Petros Drineas was partially supported by an NSF CAREE R Award and NSF CCF 0916415. [20] X. Wu et al. Top 10 algorithms in data mining. Knowledge and Information Systems , 14(1):1 X 37, 2008. [21] http://www.cs.uiuc.edu/  X  dengcai2/Data/FaceData.html [22] http://www.cs.uiuc.edu/  X  dengcai2/Data/data.html [23] http://www.cs.nyu.edu/  X  roweis/lle/
