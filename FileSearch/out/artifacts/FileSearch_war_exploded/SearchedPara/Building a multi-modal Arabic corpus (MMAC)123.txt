 ORIGINAL PAPER Ashraf AbdelRaouf  X  Colin A. Higgins  X  Tony Pridmore  X  Mahmoud Khalil Abstract Traditionally, a corpus is a large structured set of text, electronically stored and processed. Corpora have become very important in the study of languages. They have opened new areas of linguistic research, which were unknown until recently. Corpora are also key to the devel-opment of optical character recognition (OCR) applica-tions. Access to a corpus of both language and images is essential during OCR development, particularly while train-ing and testing a recognition application. Excellent corpora have been developed for Latin-based languages, but few relate to the Arabic language. This limits the penetration of both corpus linguistics and OCR in Arabic-speaking coun-tries. This paper describes the construction and provides a comprehensive study and analysis of a multi-modal Arabic corpus (MMAC) that is suitable for use in both OCR develop-ment and linguistics. MMAC currently contains six million Arabic words and, unlike previous corpora, also includes connected segments or pieces of Arabic words (PAWs) as well as naked pieces of Arabic words (NPAWs) and naked words (NWords); PAWs and Words without diacritical marks. Multi-modal data is generated from both text, gathered from a wide variety of sources, and images of existing documents. Text-based data is complemented by a set of artificially gen-erated images showing each of the Words, NWords, PAWs and NPAWs involved. Applications are provided to gener-ate a natural-looking degradation to the generated images. A ground truth annotation is offered for each such image, while natural images showing small paragraphs and full pages are augmented with representations of the text they depict. A statistical analysis and verification of the dataset has been carried out and is presented. MMAC was also tested using commercial OCR software and is publicly and freely available.
 Keywords Corpora  X  Arabic  X  Linguistics  X  Pattern recognition  X  OCR 1 Introduction Traditionally, a corpus is a large, structured collection of text covering a huge number of words from different domains of a given language. The first modern corpus was the Brown Corpus. It was collected and compiled in 1967 [ 1 ] and con-tains almost 1 million English words from different dis-ciplines. Currently, the three best-known English corpora are as follows: The Corpus of Contemporary American English, which contains 410+ million words, is available via the internet and was created during the period (1990 X 2008). It is updated once or twice a year [ 2 ]; The British National Corpus, which contains 100 million words, is also avail-able via the internet, was created during the period (1980 X  1993) and still being updated [ 3 ]; and the Time Archive (1923 X  X o present) from Time magazine. The Time Archive was originally created to archive the magazine X  X  articles and contains more than 275,000 articles with around 100 million words [ 4 ].

Corpora are increasingly valuable in computational linguistics, as well as supporting statistical analysis of lan-guages. They are central to the development of computa-tional techniques associated with language; such as optical character recognition (OCR), speech recognition and natu-ral language processing. Corpora intended for use in these areas may include data beyond simple text, such as audio (speech) or images. These multi-modal corpora are also gain-ing increased attention in mainstream corpus linguistics [ 5 ].
Multi-modal corpora are of particular value during devel-opment of optical character recognition (OCR) applications. OCR methods must be trained with and evaluated against datasets that capture the structure of the target language, but emphasize its appearance in image data.

The Arabic language is widely spoken and has been used since the fifth century when written forms were stimulated by Islam. It is the native language of more than 340 million speakers and is one of the six official languages of the United Nations (along with Chinese, English, French, Russian and Spanish) [ 6 ]. It has been estimated to be one of the ten most used languages on the Internet [ 7 ] and is the official language of 22 countries located in the geographical area of the Middle East [ 8 ]. As there is no central organization for the Arabic language, no standard linguistic or image corpus exists [ 9 ].
This paper describes the generation of a corpus contain-ing six million Arabic words and associated image data. Compared to earlier Arabic corpora, our corpus has a num-ber of advantages that make it useful in a wider range of application areas. The lists and words it contains are in a variety of formats, allowing access and retrieval in diverse ways. It allows access via pieces of Arabic words (PAWs) [ 10 , 11 ] as well as naked pieces of Arabic words (NPAWs) and Naked Words (NWords); PAWs and words without dia-critical marks. A set of artificially generated images showing each of the tokens found in the corpus is provided, with each token shown in three key Arabic fonts. A set of applications is provided that add noise, generating degraded images that simulate real images of Arabic text. Each image is accom-panied by annotations in XML format, creating a valuable ground truth resource. The original images and degraded copies are beneficial when testing document image anal-ysis applications and make MMAC a real benchmark for Arabic OCR projects [ 12 ]. The corpus also includes real images of the Arabic documents used in its production, a nec-essary addition for OCR usage. These documents are in full-page and single-paragraph formats. The three different image formats X  X ingle token, paragraph and full page X  X ncrease the usability of MMAC. The corpus X  value as a benchmark dataset is evaluated using Readiris Pro 10 [ 13 ], a well-known commercial software package useful as an exemplar of Arabic OCR applications. Finally, statistical analysis of the corpus X  contents is also provided. The corpus is freely available from http://www.ashrafraouf.com/mmac .

OCR methods often rely upon knowledge of the struc-ture of the target language, and it can be beneficial to inte-grate a corpus into a developing OCR method or system. A corpus may, for example, be used in the recognition phase to eliminate erroneous hypotheses. We would stress, how-ever, that is not the intention here. Though it may be used in this way at a later date, MMAC is primarily conceived as a benchmark dataset to be used during training and testing and not as a system component. Other languages also use the Arabic alphabet, for example Pashto, Persian, Sindhi and Urdu. Though study of these languages may benefit from access to high-quality, multi-modal Arabic corpora, they again are not explicitly considered here.
 The remainder of the paper is organized as follows. Section 2 briefly reviews existing Arabic corpora, while Sect. 3 describes key features of the written Arabic lan-guage and the specific recognition difficulties they present. Section 4 describes the textual data sources used, and the processes by which the data was collected and processed to produce a combined text and image-based corpus. Further multi-modal data was obtained from real images, by scanning a number of documents and generating associated text data. This process and the resulting data are described in Sect. 5 . Section 6 considers the use of MMAC for testing OCR applications describes a real experiment. Section 7 presents a statistical analysis of the corpus. Section 8 explains how the corpus was validated and tested. Section 9 provides a descrip-tion of the freely available MMAC corpus and defines the transliteration encoding used with the Arabic tokens. Finally, Sect. 10 describes the planned future development and usage of the corpus. 2 Related works Considerable previous research has been directed toward the creation of corpora describing printed Arabic. The emphasis, however, has been on text-based corpora. 2.1 Collected word corpora The Linguistic Data Consortium (LDC) at the University of Pennsylvania has produced the  X  X rabic Gigaword Fourth Edition X  [ 14 ]. This is a database of 850 million Arabic words collected over several years from news agencies. Though important, it has a number of drawbacks for our pur-poses. First, the corpus is collected only from news agencies, limiting the linguistic style of the material captured. Sec-ondly, most of the files come from Lebanese news agencies. The lack of samples from other Arab countries limits the scope of any OCR application that relies on them. Finally, the corpus format is based upon paragraphs and not single words. This makes it less useful for testing and training of OCR methods.
 The Institute of Ancient Near Eastern Studies, Charles University Prague (Czech Republic) has compiled the CLARA Corpus. This is an electronic corpus of Modern Stan-dard Arabic. It contains 37 million words. Though the project started in 1997 [ 15 ], the corpus lacks variety in the disciplines and geographical areas sampled. This lack of variety reduces its usefulness to the developers of OCR applications. The University of Essex, in collaboration with the Open University, has developed a corpus of Al-Hayat newspaper articles. It contains 18.5 million words from 42,591 articles, but covers only 7 main subjects [ 16 ]. The An-Nahar newspa-per has also produced its own an An-Nahar text corpus. This contains 24 million words from 45,000 articles. It was devel-oped between 1995 and 2000 [ 17 ]. This type of corpus has some drawbacks that limit its value to OCR and, to a certain extent, corpus linguistics in general. First, they represent the home country of the newspaper, and not all Arab countries. Secondly, they sample only a small time period. Finally, they cover only the subjects discussed in a particular newspaper. Lack of variety in a corpus obviously reduces its usefulness to the developers of OCR applications.

The DIVA Group from the University of Fribourg (Switzerland) in collaboration with REGIM Group from the University of Sfax (Tunisia) and the Software Engineering Unit of the Business Information System Institute (HES-SO //Wallis X  X witzerland) recently generated the APTI Arabic Printed Text Image Database. This is a large-scale benchmark of open-vocabulary, multi-font, multi-size and multi-style text recognition systems in Arabic. Using 113,284 words with 10 different Arabic fonts, 10 Arabic font sizes and 4 font styles, APTI generated a total of 45,313,600 words. Its variety of fonts, text sizes and styles makes APTI a valu-able resource when testing Arabic OCR applications, but its sources of data lack variety, and its image generation process relies upon down-sampling, which is not a good simulation of the real data. APTI can be used with OCR applications that recognize words only, and offers no support to applications working at the PAWs level [ 18 ]. 2.2 Morphologically created word corpora DIINAR.1 is an Arabic lexical corpus produced by the Euro-Mediterranean project. It comprises 119,693 lemmas distributed between nouns, verbs and adverbs. It uses 6,546 roots [ 19 ].

The Xerox Arabic Morphological Analyzer/Generator was developed by Xerox in 2001. This contains 90,000 Arabic stems, which can create a derived corpus of 72 million words [ 20 ]. This type of corpora partially solves the problem of not having a definitive Arabic corpus, but it does not include many words used in everyday life. 2.3 Requirements and limitations To adequately represent a given language, a corpus must have sufficient capacity [ 9 ] and include words from a wide variety of sources [ 9 , 21 , 22 ]. We believe that a good corpus should include the following [ 23 ]:  X  Language from different disciplines, e.g. engineering,  X  Popular language from everyday life.  X  Different morphological features of the language. These  X  Examples created over a long period of time. The Arabic  X  Examples from varying geographical areas. All the differ- X  Images of the contents of the corpus with different image
These features are also required of corpora intended to provide good training/testing sets for OCR.

Corpora should provide flexible manipulation and retrieval of data. This is eased by the use of lower level representa-tions; language stored as words, for example, can be accessed in more ways than that held only in paragraphs or full pages. Again, similar criteria apply to multi-modal corpora created to support research in OCR.

The corpora discussed earlier each fail to meet one or more of these criteria. Moreover, previous Arabic corpora are purely linguistic. A substantial literature review by the authors has failed to locate any freely available corpora of images of printed Arabic. Note also that existing corpora operate exclusively at the word level. Though words are clearly important, the structure of the Arabic language and the challenges it presents to OCR suggest that this is not suf-ficient basis upon which to build a truly effect multi-modal Arabic corpus. 3 The written Arabic language In this section, we provide an overview of the written Arabic languages and discuss the problems these create for the developer of an OCR application. These factors impact upon the design of successful corpora. The Unicode naming convention has been adopted here, though other schemes are in use [ 25 , 26 ]. 3.1 Key features of written Arabic Written Arabic is both rich and complex, features of note are as follows:  X  The Arabic language consists of 28 Arabic letters [ 26 ]  X  The widths of letters in Arabic are variable (for example  X  The connecting letter known as Tatweel or Kashida is  X  Arabic alphabets depend on dots to differentiate between  X  Arabic letters have four different shapes according to their  X  Three letters only ( ) have four different glyphs  X  Fifteen Arabic letters have dots: 12 letters have dots above  X  The Arabic language incorporates ligatures such as (Lam  X  Arabic script can use diacritical marking above and below  X  An Arabic word may consist of one or more sub-words. 3.2 Recognizing written Arabic The Arabic language is not an easy language for automatic recognition. Some of the particular difficulties that must be faced by the developers of OCR applications are as follows:  X  Characters are cursive and not separated, as in the case of  X  Characters change shape depending on their position in  X  Sometimes Arabic writers neglect to include whitespace  X  Repeated characters are sometimes used, even if this  X  There are two ending letters ( ) that sometimes indi- X  There is often misuse of the letter ALEF ( ) in its different  X  The individual letter ( ) that means  X  X nd X  in English  X  The Arabic language contains a number of similar letters  X  Arabic font files exist that define character shapes similar  X  It is common to find transliterations of English-based  X  The Arabic language is not based on the Latin alphabet 3.3 PAWs and NPAWs Arabic is an intricate language. It is therefore crucial that an equally rich and complex corpus is available to support development of Arabic OCR applications. In particular, the importance of PAWS and NPAWS is such that they must be included in any realistic text/image corpus. The addition of PAWs and naked PAWs is a novel extension of the previ-ous word-based approach to Arabic corpora that specifically facilitates Arabic OCR. We include PAWs and NPAWs in our multi-modal Arabic corpus (MMAC) for the following reasons:  X  PAWs are the smallest token in the Arabic language.  X  Each PAW is one glyph, making written Arabic a discon- X  Naked Words and naked PAWs reduce the process of  X  The total number of PAWs representing the language in  X  PAWs consisting of one and two digits can be extracted 4 Building a multi-modal corpus from textual data 4.1 Coverage and sources The multi-modal Arabic corpus contains 6 million Arabic words selected from various sources covering old Arabic, religious texts, traditional language, modern language, dif-ferent specialisations and very modern material from online  X  X hat rooms. X  The sources used are as follows: Topical Arabic websites These were obtained via an Arabic search engine. The search engine specifies the web-pages according to topic. Pages allocated to different topics including literature, women, medicine, business, children, religion, sports, programming, design and entertainment were downloaded and incorporated into MMAC.
 Arabic news websites A set of the most widely used Arabic news websites were identified and downloaded. These included the websites of Al Jazeera, Al Ahram, Al Hayat, Al Khaleej, Asharq Al Awsat, Al Arabiya and Al Akhbar. Professional news websites include language that is qualitatively different from that used in the first set of websites, which were created by the general public. They also come from a variety of Arabic countries.
 Arabic chatrooms Some Arabic chatrooms were used for areas such as sports, cultural and social.
 Arabic-Arabic dictionaries These dictionaries contain all the most common Arabic words and word roots, along with their definition. Old Arabic books These were generally written centuries ago. They include religious books and books using traditional language. Arabic research A PhD thesis in Law was included to sample the research literature.
 The Holy Quran The wording of the Holy Quran was also used. 4.2 Text data collection The following steps were taken to overcome the difficulties discussed in Sect. 3.2 , which are mainly due to the use of different code page encodings and words often containing repeated letters or being formed from connected words. The steps are as follws: 1. An Arabic search engine [ 34 ] was used to search for 2. The code page used for each part of the file was identified. 3. A program was developed to removes Latin letters, num-4. Some common typographical errors were corrected. For 5. A text file was created containing one Arabic word per 6. A program was written to correct the problem of con-7. Words containing repeated letters were checked auto-8. A program was written and applied that replaces the final 4.3 Creating NWords, PAWs and NPAWs The words obtained by the process outlined earlier allow generation of NWords, PAWs and NPAWs data files. This is achieved as follows: 1. A program to create NWord data files was developed. 2. A program to create PAW data files was developed 3. The program used to convert Words to NWords was used 4.4 Creating images from corpus text data The methods described in Sects. 4.2 and 4.3 produce a text-based Arabic corpus. To support the development of Arabic OCR applications, MMAC augments its textual data with images [ 37 ] and ground truth information for these images [ 18 ]. Each token (Word, NWord, PAW or NPAW) is shown in the three most common Arabic fonts; Simplified Arabic, Arabic Transparent, and Traditional Arabic [ 38 ]. Token images are stored in greyscale Windows bmp file format, with a resolution of 300DPI [ 39 ]. Figure 1 gives an example of a word represented as images showing its appearance in three common Arabic fonts.

The data collection process produced a data file con-taining the unique Word, NWord, PAW and NPAW tokens, sorted alphabetically. To produce the image files, we transfer each token from this text file to a Windows bmp image file. A program was developed to achieve this as follows: 1. The program opens the text file containing the list of 2. The program saves the bitmap memory allocation to 3. The program generates a ground truth XML file for each
The final token image dataset consists of files containing the images of the token. Each token has three font files, Sim-plified Arabic, Traditional Arabic and Arabic Transparent. To simplify handling of the huge number of files, the pro-gram saves every 500 tokens files in a separate sub-folder. Table 4 shows the total number of images of the different tokens that are generated from the list of unique tokens.
Table 5 shows the average letters per token, average width and average height of the Words, NWords, PAWs and NPAWs. The information about the token images shows that the average width of the words and NWords are the same, while the height is different due to the existence of dots. This is also recorded in the case of the PAWs and NPAWs. The ratio between the average widths of the Words and NWords on one side and PAWs and NPAWs on the other is almost the same as in the case of the number of letters per token. The average height of words and NWords is greater than that of PAWs and NPAWs. 4.5 Degrading the computer-generated token images The process of degrading the computer-generated token images is very important if they are to simulate the charac-teristics of real document images during OCR development. The degradation stage is composed of two parts: in the first, the image is skewed to simulate the distortion that may occur during document scanning; in the second, artificial noise is added to simulate scanned image data. 4.5.1 Skewing the image Rotated images are created from the computer-generated image to simulate the rotation that often occurs while scan-ning documents. The application provided can generate any number of rotated samples of the original image rotated in both sides. It uses the Box X  X uller transform algorithm to convert the uniform distribution random number to normal distribution random number [ 40 ]. This algorithm is used to simulate the real-life rotation angles of scanned documents. Nearest neighbor interpolation completes the rotated image [ 41 ]. Figure 2 b shows the skewed token image. 4.5.2 Adding noise Artificial noise is added to the image to simulate that found in real document images. Three effects are applied; blurring, filter and additive noise [ 42 ]. A Gaussian blurring effect is first added to simulate the blur that may be introduced dur-ing scanning. A high-pass filter is then applied to simulate change in the paper color over time. Gaussian noise is finally added to simulate image acquisition noise. Figure 2 cshowsa token image after applying the Gaussian blurring and noise. 4.6 Ground truth In document image analysis and recognition, ground truth refers to various attributes associated with the text on the image such as the size of tokens, characters, font type, font size, etc. Ground truth data is crucial to the systematic train-ing and testing of document image analysis applications [ 43 ].
Each token image in the MMAC contains ground truth information [ 18 ]. Figure 3 shows the ground truth XML file for the word  X   X . MMAC X  X  ground truth XML files contain the following attributes: 1. Content: this contains the token name in Arabic, translit-2. Internal PAWs: this shows the number of PAWs of the 3. Font: this contains the name of the font, style and font 4. Image: this contains the image file name, the file format 5. Diacritics: this contains the number of letters in the token 5 Building a multi-modal corpus: real images The procedures described in Sect. 4 produce a multi-modal Arabic corpus from text data, with images being created arti-ficially from that data. The result is a set of files containing descriptions of key sections of Arabic text, with correspond-ing image files showing its appearance in a number of com-mon fonts. This dataset provides both a representation of the Arabic language and data needed during development of OCR applications X  X ample images of text with associated XML ground truth information.

The images generated by the method described earlier are, however, idealized. As Fig. 1 showed, the token images are so clear that they can appear as if they were typed. Though the applications described in Sect. 4.5 allow common distortions and noise processes to be simulated, a complete evaluation should include real images containing the errors, noise and distortions that arise in real life.

To provide the data needed to support OCR, two different datasets are added to MMAC. The first contains images of small paragraphs of Arabic documents with text files pro-viding ground truth. The second dataset is created from real scanned images. These datasets can be used to check the validity of the output of a developing OCR application. They are also valuable during development of preprocesses such as skew detection and correction, noise removal, binarization, thinning and layout analysis. 5.1 Paragraph image dataset MMAC X  X  paragraph document dataset contains images of 552 paragraphs. It is generated from 15 full-page docu-ments. There are three different categories: real scanned images, computer-generated images and computer-generated images with artificial noise. Each category contains 5 full pages. Each single paragraph image includes around two lines containing around 10 words. The number of images are 223 real, 141 computer generated and 188 computer gen-erated with noise. Figure 4 shows a sample paragraph image from each category. The computer-generated and computer-generated with noise documents include different Arabic font type, sizes and styles (regular, italic and bold). This dataset includes around 8,000 words.

The need for this dataset in the OCR development pro-cess is to test the application with a small sample of data enabling us to trace any errors. Interestingly during the test-ing of MMAC with commercial software, the recognition accuracy of a paragraph image is sometimes different from that of the full-page image.
 5.2 Full-page real image dataset MMAC X  X  scanned document dataset contains images of 19 different documents. These comprise books, work docu-ments, medical scripts, faxes, magazines and newspapers. The documents were scanned at 300DPI, in 24bit RGB color mode and stored in tiff file format. Document quality var-ies significantly over the dataset, which includes documents at different orientations and some skewed examples. It con-tains different sizes of documents. Some of the documents are watermarked, others have large page borders. Figure 5 shows two examples. Though the scanned document data set is currently small compared to the text-based data, the goal is to include examples of most (commonly) occurring real documents.

To create associated text files, the contents of each doc-ument were manually typed on Microsoft Word 2003 soft-ware. The resulting text files were proof read by two different groups of people, with the writing group different from each of the two proof-reading groups. The data set created from real images currently includes around 11,000 Arabic words. 6 Using the MMAC corpus in Arabic OCR development MMAC was originally generated to support research in Arabic OCR. The primary motivation was to create a bench-mark against which Arabic OCR applications could be eval-uated. Its inclusion of images of different tokens makes MMAC suitable for training and testing OCR applications that operate in different layers. Some OCR applications deal with the whole word, while others deal with PAWs [ 44 , 45 ]. In the same manner, some applications use diacritics to search for particular words, while others neglect them [ 46 ]. We believe that MMAC can be used to evaluate any Arabic OCR application using data at four different levels that start from accurate computer-generated images and move to real images of full-page scanned documents. These four different levels represent different degrees of difficulty for an OCR applica-tion. As the application passes each level successfully and moves to the next, we gain more confidence in the quality of the application.

MMAC was tested with commercial software to check its usefulness in a real world application. The four different levels were tested using the well-known commercial soft-ware Readiris pro 10 [ 13 ]. Recognition results from Readiris were compared to the ground truth documents using the Levenshtein Distance algorithm [ 47 ]. The Levenshtein Distance algorithm is an approximate string matching algo-rithm that gives a tentative accuracy of recognition. This accuracy is measured based on the percentage of correct char-acters within the volume of characters covered, defined by the majority of the OCR application vendors [ 48 ]. Figure 6 explains the process applied to test MMAC with the com-mercial software. This process can be used to test any OCR engine with MMAC. 6.1 Computer-generated token images In the first step, MMAC X  X  computer-generated images are used to test recognition accuracy (Fig. 2 a). The challenge of MMAC in this step is to maximize the number of token images that are covered by the developing application. In addition to the commonly used PAWs and Words, MMAC can be used to evaluate OCR applications that deal with NWords or NPAWs. As the token images are very clear, it is expected that many applications will be capable of high lev-els of performance. The importance of this step is to evaluate OCR application with a wide variety of tokens that repre-sents most of the language. This step gives credibility to the OCR application. In the experiment reported here, the 1,274 most commonly used words were selected. These words rep-resent approximately half of our corpus words. A recognition accuracy of 94% was achieved. 6.2 Computer-generated token images with artificial noise In the second step, degraded images are generated from the clean computer-generated originals (Sect. 4.5 ). When an OCR application achieves high accuracy at this stage, it shows that the OCR application not only can detect most of the tokens in the language but also can detect them in the pres-ence of realistic noise. The ability to control the noise added allows detailed analysis of the effect of different image con-ditions on any proposed method. Extreme values can also be used to test a given technique  X  X o destruction X .

In the exemplar experiment, three different levels of noise were used. These represent a good document, a medium document and a bad document. Table 6 gives the exact param-eters used. The same 1,274 most common words from the pre-vious step were examined. Recognition accuracy was 91% for good documents, 89% for medium documents and 77% for bad documents. 6.3 Paragraph documents In the third step, MMAC X  X  small images of real scanned, computer-generated and computer-generated with artificial noise documents are used to test the OCR application. Figure 4 shows examples of the available images. We believe this step represents a higher level of testing than the previ-ous two. It employs bigger images and different types of image. The recognition accuracy for this step gives a better indication of the likely quality of the OCR application given real-world input.

Sixty-two documents were selected from the three dif-ferent types of documents that represent the majority cases. The recognition accuracy of the real documents ranged from 100% down to 47%. The recognition accuracy of the com-puter-generated documents ranged from 93% down to 33%, while that for computer generated with noise documents ranged from 95% down to 54%. The variety of accuracy values are based on the status of the document images and font properties. We found that the Italic font style gives bad accuracy results. We also found that Traditional Arabic font gives comparatively poor accuracy results. MMAC X  X  breadth allows the effect of a wide range of text and image features to be assessed. 6.4 Real scanned documents In the final step, MMAC X  X  images of real scanned documents are used to test the OCR application. The real scanned images are ground trusted by the documents that were scanned. These images show a variety of document types from a wide range of sources. Figure 5 shows examples of the available images. Passing this step with reasonable recognition accuracy gives the final evaluation of the OCR application. This step tests not only the recognition accuracy but also the overall perfor-mance of the OCR application. It probes the effect of page skew, page layout analysis, page orientation, etc. We con-sider this step as the final step because the objective of any OCR application is to recognize full-page documents with different aspects.

During the testing of this step, we selected 9 differ-ent documents representing different clarity of images. The recognition accuracy achieved ranged from 95% for good documents down to 46% for very poor documents. This wide range of recognition accuracy indicates the usefulness of the large variety of images, documents and added noise provided by MMAC in testing OCR applications. 7 Statistical analysis of the MMAC corpus Corpus analysis is concerned with the statistical properties of words and other lexical tokens. In this section, we describe an investigation into the frequency of these entities in Arabic. Previous studies of Arabic corpora only analyzed words. We emphasize the analysis of other tokens, namely NWords, PAWs and NPAWS. This is achieved by studying the most and least frequently occurring tokens. We compared our analysis of words with previous research to verify our results. 7.1 Frequency list A frequency list gives the number of occurrences of each element (usually word) in a corpus and is an important part of any corpus [ 49 ]. It is useful for information retrieval, text categorization and numerous other purposes [ 50 ]. Frequency lists are very useful to the OCR application developer as they can be used to validate the accuracy of the recognition phase. Comparison of the frequency of each word in the corpus and in the system output is an important step when assessing the accuracy of a recognition process. The frequency lists in MMAC are not limited to words, but also include NWords, PAWs and NPAWs. The variety in the types of token used here gives flexibility to the developers of OCR applications, who can use the MMAC to support a wide range of appli-cation evaluations. The file format is a text file; each line contains a word and the number of occurrence of this word sorted alphabetically. We have 4 separate files in this format: for words, NWords, PAWs and NPAWs. 7.2 Analysis of words and PAWs Tables 7 and 8 show the detailed analysis of the words and PAWs in MMAC. The most common Words are preposi-tions, while the most common PAWs are those that consist of a single letter. There are 25% fewer unique NWords than unique Words. The number of NWords that occur once or are repeated twice is less than in the case of Words, while NWords that are repeated heavily are more common than in the case of unique Words. The number of unique PAWs is very limited relative to the total number of PAWs. More PAWs are heavily repeated than Words. There are 50% fewer unique NPAWs than unique PAWs. The number of NPAWs that have few repetitions is less than in the case of PAWs, although the average number of repeated NPAWs is greater than that of PAWs.

The total number of one-letter PAWs is 6,275,167 that represents 44.7% of the total number of PAWs; while there are 3,652,709 two-letter PAWs, 26% of the total. The total number of NPAWs in the corpus is 14,025,044, while the total number of unique NPAWs is 32,804 with an average repetition of 427.54 for each NPAW. 7.3 Discussion It is important to measure the frequency of occurrence of the Words, NWords, PAWs and NPAWs making up the Arabic language. Frequency data aids the development of OCR applications by encouraging the OCR developer to rec-ognize more common elements first. For example, the PAW  X   X  represents more than 20% of the written Arabic language. As Fig. 7 shows, this percentage increases dramatically when measured in NPAWs instead of words. The 25 most repeated NPAWs account for more than half the words used in prac-tice.
 Statistical analysis shows that the number of unique NPAWs in the MMAC corpus is very limited. The number of unique NWords is almost six times that of NPAWs. More-over, this increases rapidly, with no evident asymptote, as new texts are added to the corpus as shown in Fig. 8 .The number of unique NWords is about 80% of the number of unique Words, and this ratio remains fairly stable once a rea-sonably sized corpus has been established.

Analysis of MMAC also shows the least repeated Words and PAWs in the language. Table 9 shows the relationship between the total number of Words, NWords, PAWs and NPAWs that occur only once; as percentages of the total number of distinct words and the total number of words, NWords, PAWs and NPAWs in the corpus. The twenty-five most repeated Words, NWords, PAWs and NPAWs and the percentage of each of them in the corpus are shown in Table 10 . The analysis of words in Table 10 gives almost the same results as that found by others [ 49 , 51 , 52 ], which gives credibility to our analysis. The analysis of NWords, PAWs and NPAWs in Table 10 is very important to the OCR developer in the sense of giving more emphasis not only to the most repeated words but also the most repeated NWords, PAWs and NPAWs. Tokens that are repeated most should be recognized before those that appear only rarely. According to the previous statistics, we can advise the OCR developers to consider recognizing the one-letter PAWS in their isolated shape first.

The other important advice to the OCR developer is to deal with the two-letters PAW as if they are isolated letters or new glyphs. These new glyphs present just 612 different shapes. If the OCR developers recognize these isolated letters and two-letter glyphs, they can process 70% of the language. 8 Testing the corpus X  validity Testing the validity and accuracy of the data is a very impor-tant issue in creating any corpus [ 53 ]. Our testing process started by collecting a testing dataset from unusual sources consisting of scanned images (as mentioned earlier in Sect. 5 ) from faxes, documents, books, medicine scripts and well-known Arabic news websites. This was done one year after the original MMAC data collection was completed. The test-ing dataset was collected from sources that intersected mini-mally with those of the corpus. The total number of Words in the testing dataset is 69,158 and the total number of PAWs is 165,501. Table 11 shows the total and the number of distinct Words, NWords, PAWs and NPAWs of the testing dataset. 8.1 Testing words and PAWs A program was developed to search in the corpus binary data files using a binary search algorithm. The percentage of unique words in the testing dataset found in the corpus data file is 89.8%. Table 12 gives the percentage of tokens found from the testing dataset in the corpus data files. This analysis deals with the unique data only. On the other hand, the total testing dataset words missing (that are not unique) in the corpus data files are 2,457 Words with a percentage of 3.5%. Table 13 shows this ratio in the corpus tokens. 8.2 Checking corpus accuracy The testing dataset was also used to check the accuracy of the statistics obtained from the corpus. By extrapolation, we believe that if the curve between the number of words and the unique number of words (Fig. 8 ) is extended according to the number of words that exists in the testing dataset, we will find that the increase in the unique number of words is almost equal to the missing words in the corpus.

The shape of the curve between the total number of words and the unique number of words is a nonlinear regression model. We used the CurveExpert 1.3 shareware program [ 54 ] and applied its curve finder process. We found that the best regression model is the Morgan X  X ercer X  X lodin (MMF) model, from the Sigmoidal Family [ 55 ]. The best-fit curve equation is y = ab where: a = X  42 . 558444 b = 40335 . 007 c = 753420 . 21 d = 0 . 64692321 StandardError : CorrelationCoefficient : 156 . 0703663 0 . 9999980
Upon applying Eq. 1 , we found that the increase in the number of words to 6,069,158 will increase the number of distinct words by 1,676, while the number of missing words is 1,799. This result supports our previous suggestion, with an accuracy of 93%. 9 The online MMAC corpus In this section, we describe the contents of the MMAC corpus available via the project website. It also includes the method applied to transliterate the tokens text to English tokens name. 9.1 MMAC corpus contents The contents of the online version of MMAC are as follows: 1. Original Data File: This contains the original sequence 2. Original Data File After Replacing Alef: This file is 3. Sample Raw Data Collected: This contains samples of 4. Data Files: This folder contains four sub-folders for the 5. MMAC tokens Images: This folder contains four sub-6. Paragraph documents Dataset: This folder contains 7. Real Scanned Dataset: This folder contains two sub-8. Corpus testing data: This folder contains four sub-9. Frequency List: This folder contains the frequency lists 10. Applications: This folder contains all the applications 11. All Statistics.xls: This is an Excel spreadsheet file con-9.2 Transliteration encoding For flexibility and ease of use, we need to use Roman character-based letters for the Arabic image text. We used the Arabic English transliteration encoding to name the Arabic text in English. We explain here the different types of Ara-bic English transliteration encoding available. Transliteration is a mapping between two different languages, in our case Arabic and English [ 56 ]. There are many types of transliter-ation between Arabic and English which can be summarized into two main categories: Arabic Transliteration [ 57 ] and Arabic Chat Alphabets [ 58 ].

We believe that using Arabic transliteration encoding is much better in our case than Arabic Chat Alphabets X  although  X  X hat X  is more readable to Arabic natives for the following reasons: 1. The Arabic Chat Alphabets neglect letters with diacritics 2. The Arabic Chat Alphabets sometimes use two charac-3. The Arabic Chat Alphabets sometimes use special char-
The most famous Arabic Transliteration encoding is the Buckwalter Transliteration as shown in Table 14 [ 59 ]. We decided to use this encoding (37 characters) with modi-fications for the following reasons: 1. Buckwalter uses a single character for transliteration. 2. Buckwalter includes all written Arabic characters and 3. Buckwalter uses fewer capital letters (it only uses A, H, 9.3 Buckwalter modified transliteration encoding The purpose of transliteration in our case is different from most other transliteration. We use transliteration encoding for many purposes, including file names, so we have to deal with all the restrictions of the use and also file name restrictions in the most common operating systems (Windows, Macintosh and UNIX). These restrictions are as follows: 1. The file name may be case insensitive . Hence, we 2. White space is not allowed . This restriction is not appli-3. Some special characters like ( | , &gt;, *) are not allowed. So
The Modified Buckwalter Transliteration is almost the same as the original but with 13 of the original 37 characters modified. Table 15 shows the characters and their proposed transliteration characters. 10 Future works and conclusion Creating a corpus is a large and complex task. While cur-rently extremely useful, the MMAC corpus would benefit from additional work. First, saving the data files in a hyper-text mark-up formatting language and adding other informa-tion for each word, such as the morphological information explained in Sect. 2.3 . Secondly, adding statistical analysis regarding the words X  frequency pair lists. Thirdly, a power-ful stemming/lexical algorithm must be applied to the corpus to enhance data retrieval for improved accuracy. Fourthly, the process of adding new words should continue. Fifthly, it would be advantageous if MMAC could be extended to include other languages that use the Arabic alphabet. Finally, it must be capable of including words with diacritics.
After considering the extensions proposed earlier, we con-clude that an organization dealing with Arabic corpora is urgently required. Also, using Unicode as a standard code page is very important because unification among various languages is a significant problem in automating the pro-cessing of non-Latin languages. Statistical analysis shows the importance of PAWs and NPAWs in the Arabic language; these should be included in future corpora.

To conclude, the MMAC corpus was designed to meet the needs of users of traditional linguistic corpora, but at the same time to be beneficial to OCR applications developers. It can be used in testing and training each phase of an Arabic OCR application. References
