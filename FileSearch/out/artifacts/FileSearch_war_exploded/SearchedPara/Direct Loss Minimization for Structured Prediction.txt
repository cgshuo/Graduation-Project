 Many modern software systems compute a result as the solution, or approximate solution, to an op-timization problem. For example, modern machine translation systems convert an input word string into an output word string in a different language by approximately optimizing a score defined on the input-output pair. Optimization underlies the leading approaches in a wide variety of computational problems including problems in computational linguistics, computer vision, genome annotation, ad-vertisement placement, and speech recognition. In many optimization-based software systems one must design the objective function as well as the optimization algorithm. Here we consider a param-eterized objective function and the problem of setting the parameters of the objective in such a way that the resulting optimization-driven software system performs well.
 We can formulate an abstract problem by letting X be an abstract set of possible inputs and Y an abstract set of possible outputs. We assume an objective function s w : X  X Y  X  R parameterized by a vector w  X  R d such that for x  X  X  and y  X  X  we have a score s w ( x,y ) . The parameter setting w determines a mapping from input x to output y w ( x ) is defined as follows: Our goal is to set the parameters w of the scoring function such that the mapping from input to output defined by (1) performs well. More formally, we assume that there exists some unknown probability distribution  X  over pairs ( x,y ) where y is the desired output (or reference output) for input x . We assume a loss function L , such as the BLEU score, which gives a cost L ( y,  X  y )  X  0 for producing output  X  y when the desired output (reference output) is y . We then want to set w so as to minimize the expected loss. In (2) the expectation is taken over a random draw of the pair ( x,y ) form the source data distribution  X  . Throughout this paper all expectations will be over a random draw of a fresh pair ( x,y ) . In machine learning terminology we refer to (1) as inference and (2) as training. Unfortunately the training objective function (2) is typically non-convex and we are not aware of any polynomial algorithms (in time and sample complexity) with reasonable approximation guarantees to (2) for typical loss functions, say 0-1 loss, and an arbitrary distribution  X  . In spite of the lack of approximation guarantees, it is common to replace the objective in (2) with a convex relaxation such as structural hinge loss [8, 10]. It should be noted that replacing the objective in (2) with structural hinge loss leads to inconsistency  X  the optimum of the relaxation is different from the optimum of (2).
 An alternative to a convex relaxation is to perform gradient descent directly on the objective in (2). In some applications it seems possible that the local minima problem of non-convex optimization is less serious than the inconsistencies introduced by a convex relaxation.
 Unfortunately, direct gradient descent on (2) is conceptually puzzling in the case where the output space Y is discrete. In this case the output y w ( x ) is not a differentiable function of w . As one smoothly changes w the output y w ( x ) jumps discontinuously between discrete output values. So one cannot write  X  w E [ L ( y,y w ( x ))] as E [  X  w L ( y,y w ( x ))] . However, when the input space X is continuous the gradient  X  w E [ L ( y,y w ( x ))] can exist even when the output space Y is discrete. The main results of this paper is a perceptron-like method of performing direct gradient descent on (2) in the case where the output space is discrete but the input space is continuous.
 After formulating our method we discovered that closely related methods have recently become popular for training machine translation systems [7, 2]. Although machine translation has discrete inputs as well as discrete outputs, the training method we propose can still be used, although without theoretical guarantees. We also present empirical results on the use of this method in phoneme alignment on the TIMIT corpus, where it achieves the best known results on this problem. Perceptron-like training methods are generally formulated for the case where the scoring function is linear in w . In other words, we assume that the scoring function can be written as follows where  X  : X  X Y  X  R d is called a feature map . Because the feature map  X  can itself be nonlinear, and the feature vector  X  ( x,y ) can be very high dimensional, objective functions of the this form are highly expressive.
 Here we will formulate perceptron-like training in the data-rich regime where we have access to distribution  X  . In the basic structured prediction perceptron algorithm [3] one constructs a sequence of parameter settings w 0 , w 1 , w 2 , ... where w 0 = 0 and w t +1 is defined as follows. is  X  -separable, i.e., there exists a weight vector w with the property that y w ( x ) = y with probability 1 and y w ( x ) is always  X  -separated from all distractors, then the perceptron update rule will eventually lead to a parameter setting with zero loss. Note, however, that the basic perceptron update does not involve the loss function L . Hence it cannot be expected to optimize the training objective (2) in cases where zero loss is unachievable.
 A loss-sensitive perceptron-like algorithm can be derived from the structured hinge loss of a margin-scaled structural SVM [10]. The optimization problem for margin-scaled structured hinge loss can be defined as follows. It can be shown that this is a convex relaxation of (2). We can optimize this convex relaxation with stochastic sub-gradient descent. To do this we compute a sub-gradient of the objective by first computing the value of  X  y which achieves the maximum. This yields the following perceptron-like update rule where the update direction is the negative of the sub-gradient of the loss and  X  t is a learning rate. Equation (4) is often referred to as loss-adjusted inference . The use of loss-adjusted inference causes the rule update (5) to be at least influenced by the loss function.
 Here we consider the following perceptron-like update rule where  X  t is a time-varying learning rate and t is a time-varying loss-adjustment weight. In the update (6) we view y t direct as being worse than y w t ( x t ) . The update direction moves away from feature vectors of larger-loss labels. Note that the reference label y t in (5) has been replaced by the inferred label y w t ( x ) in (6). The main result of this paper is that under mild conditions the expected update direction of (6) approaches the negative direction of  X  w E [ L ( y,y w ( x ))] in the limit as the update weight t goes to zero. In practice we use a different version of the update rule which moves toward better labels rather than away from worse labels. The toward-better version is given in Section 5. Our main theorem applies equally to the toward-better and away-from-worse versions of the rule. The main result of this paper is the following theorem.
 Theorem 1. For a finite set Y of possible output values, and for w in general position as defined below, we have the following where y direct is a function of w , x , y and . where We prove this theorem in the case of only two labels where we have y  X  { X  1 , 1 } . Although the maintain the clarity of the presentation. We assume an input set X and a probability distribution or a measure  X  on X  X { X  1 , 1 } and a loss function L ( y,y 0 ) for y,y 0  X  { X  1 , 1 } . Typically the loss loss of a false negative, L (1 ,  X  1) .
 By definition the gradient of expected loss satisfies the following condition for any vector  X  w  X  R d . Using this observation, the direct loss theorem is equivalent to the following For the binary case we define  X   X  ( x ) =  X  ( x, 1)  X   X  ( x,  X  1) . Under this convention we have y w ( x ) = are the same then the quantity inside the expectation is zero. We now define the following two sets which correspond to the set of inputs x for which these two labels are different.
 Figure 1: Geometrical interpretation of the loss gradient. In (a) we illustrate the integration of the constant value  X  L ( y ) over the set S + and the constant value  X   X  L ( y ) over the set S  X  (the green show the integration of  X  L ( y )( X  w ) &gt;  X   X  ( x ) over the sets U + = { x : w t  X   X  ( x )  X  [0 , ] } and U general conditions these integrals are asymptotically equivalent in the limit as goes to zero. and We define  X  L ( y ) = L ( y, 1)  X  L ( y,  X  1) and then write the left hand side of (8) as follows. These expectations are shown as integrals in Figure 1 (a) where the lines in the figure represent the decision boundaries defined by w and w +  X  w .
 To analyze this further we use the following lemma.
 Lemma 1. Let Z ( z ) , U ( u ) and V ( v ) be three real-valued random variables whose joint measure  X  can be expressed as a measure  X  on U and V and a bounded continuous conditional density function f ( z | u,v ) . More rigorously, we require that for any  X  -measurable set S  X  R 3 we have following. Proof. First we note the following where V + denotes max(0 ,V ) . Similarly we have the following where V  X  denotes min(0 ,V ) . Subtracting these two expressions gives the following.
 Applying Lemma 1 to (9) with Z being the random variable w T  X   X  ( x ) , U being the random variable  X   X  L ( y ) and V being  X  ( X  w ) T  X   X  ( x ) yields the following. Of course we need to check that the conditions of Lemma 1 hold. This is where we need a general position assumption for w . We discuss this issue briefly in Section 3.1.
 Next we consider the right hand side of (8). If the two labels y direct and y w ( x ) are the same then the quantity inside the expectation is zero. We note that we can write y direct as follows. We now define the following two sets which correspond to the set of pairs ( x,y ) for which y w ( x ) and y direct are different.
 We now have the following.
 These expectations are shown as integrals in Figure 1 (b). Applying Lemma 1 to (11) with Z set to w &gt;  X   X  ( x ) , U set to  X  ( X  w ) &gt;  X   X  ( x ) and V set to  X   X  L ( y ) gives the following. Theorem 1 now follows from (10) and (12). 3.1 The General Position Assumption The general position assumption is needed to ensure that Lemma 1 can be applied in the proof of Theorem 1. As a general position assumption, it is sufficient, but not necessary, that w 6 = 0 and  X  ( x,y ) has a bounded density on R d for each fixed value of y . It is also sufficient that the range of the feature map is a submanifold of R d and  X  ( x,y ) has a bounded density relative to the surface of that submanifold, for each fixed value of y . More complex distributions and feature maps are also possible. In many applications the inference problem (1) is intractable. Most commonly we have some form of graphical model. In this case the score w &gt;  X  ( x,y ) is defined as the negative energy of a Markov random field (MRF) where x and y are assignments of values to nodes of the field. Finding a lowest energy value for y in (1) in a general graphical model is NP-hard.
 A common approach to an intractable optimization problem is to define a convex relaxation of the objective function. In the case of graphical models this can be done by defining a relaxation of a marginal polytope [11]. The details of the relaxation are not important here. At a very abstract level the resulting approximate inference problem can be defined as follows where the set R is a relaxation of the set Y , and corresponds to the extreme points of the relaxed polytope. We assume that for y  X  Y and r  X  R we can assign a loss L ( y,r ) . In the case of a relaxation of the marginal polytope of a graphical model we can take L ( y,r ) to be the expectation over a random rounding of r to  X  y of L ( y,  X  y ) . For many loss functions, such as weighted Hamming loss, one can compute L ( y,r ) efficiently. The training problem is then defined by the following equation. Note that (14) directly optimizes the performance of the approximate inference algorithm. The pa-rameter setting optimizing approximate inference might be significantly different from the parameter setting optimizing the loss under exact inference.
 The proof of Theorem 1 generalizes to (14) provided that R is a finite set, such as the set of vertices of a relaxation of the marginal polytope. So we immediately get the following generalization of Theorem 1. where Another possible extension involves hidden structure. In many applications it is useful to introduce hidden information into the inference optimization problem. For example, in machine translation we might want to construct parse trees for the both the input and output sentence. In this case the inference equation can be written as follows where h is the hidden information. In this case we can take the training problem to again be defined by (2) but where y w ( x ) is defined by (15).
 Latent information can be handled by the equations of approximate inference but where R is reinter-which we can take to be equal to L ( y,y 0 ) . In this section we present empirical results on the task of phoneme-to-speech alignment. Phoneme-to-speech alignment is used as a tool in developing speech recognition and text-to-speech systems. In the phoneme alignment problem each input x represents a speech utterance, and consists of a pair sequence of phonemes p = ( p 1 ,...,p K ) , where p k  X  X  , 1  X  k  X  K is a phoneme symbol and P is a finite set of phoneme symbols. The lengths K and T can be different for different inputs although typically we have T significantly larger than K . The goal is to generate an alignment between the two sequences in the input. Sometimes this task is called forced-alignment because one is forced Table 1: Percentage of correctly positioned phoneme boundaries, given a predefined tolerance on the TIMIT corpus. Results are reported on the whole TIMIT test-set (1344 utterances). to interpret the given acoustic signal as the given phoneme sequence. The output y is a sequence ( y 1 ,...,y K ) , where 1  X  y k  X  T is an integer giving the start frame in the acoustic sequence of the k -th phoneme in the phoneme sequence. Hence the k -th phoneme starts at frame y k and ends at frame y k +1  X  1 .
 Two types of loss functions are used to quantitatively assess alignments. The first loss is called the  X  -alignment loss and it is defined as In words, this loss measures the average number of times the absolute difference between the pre-dicted alignment sequence and the manual alignment sequence is greater than  X  . This loss with different values of  X  was used to measure the performance of the learned alignment function in [1, 9, 4]. The second loss, called  X  -insensitive loss was proposed in [5] as is defined as follows. This loss measures the average disagreement between all the boundaries of the desired alignment sequence and the boundaries of predicted alignment sequence where a disagreement of less than  X  is ignored. Note that  X  -insensitive loss is continuous and convex while  X  -alignment is discontinuous and non-convex. Rather than use the  X  X way-from-worse X  update given by (6) we use the  X  X oward-better X  update defined as follows. Both updates give the gradient direction in the limit of small but the toward-better version seems to perform better for finite . Our experiments are on the TIMIT speech corpus for which there are published benchmark results [1, 5, 4]. The corpus contains aligned utterances each of which is a pair ( x, y ) where x is a pair of a phonetic sequence and an acoustic sequence and y is a desired alignment. We divided the training portion of TIMIT (excluding the SA1 and SA2 utterances) into three disjoint parts containing 1500, 1796, and 100 utterances, respectively. The first part of the training set was used to train a phoneme frame-based classifier, which given a speech frame and a phoneme, outputs the confident that the phoneme was uttered in that frame. The phoneme frame-based classifier is then used as part of a train the phoneme classifier consisted of the Mel-Frequency Cepstral Coefficient (MFCC) and the log-energy along with their first and second derivatives (  X  +  X  X  X  ) as described in [5]. The classifier used a Gaussian kernel with  X  2 = 19 and a trade-off parameter C = 5 . 0 . The complete set of 61 TIMIT phoneme symbols were mapped into 39 phoneme symbols as proposed by [6], and was used throughout the training process.
 The seven dimensional weight vector w was trained on the second set of 1796 aligned utterances. We trained twice, once for  X  -alignment loss and once for  X  -insensitive loss, with  X  = 10 ms in both cases. Training was done by first setting w 0 = 0 and then repeatedly selecting one of the 1796 training pairs at random and performing the update (6) with  X  t = 1 and t set to a fixed value . It should be noted that if w 0 = 0 and t and  X  t are both held constant at and  X  respectively, then the direction of w t is independent of the choice of  X  . These updates are repeated until the performance of w t on the third data set (the hold-out set) begins to degrade. This gives a form of regularization known as early stopping. This was repeated for various values of and a value of was selected based on the resulting performance on the 100 hold-out pairs. We selected = 1 . 1 for both loss functions.
 We scored the performance of our system on the whole TIMIT test set of 1344 utterances using  X  -alignment accuracy (one minus the loss) with  X  set to each of 10, 20, 30 and 40 ms and with  X  -insensitive loss with  X  set to 10 ms. As should be expected, for  X  equal to 10 ms the best performance is achieved when the loss used in training matches the loss used in test. Larger values of  X  correspond to a loss function that was not used in training. The results are given in Table 1. We compared our results with [4], which is an HMM/ANN-based system, and with [5], which is based on structural SVM training for  X  -insensitive loss. Both systems are considered to be state-of-the-art results on this corpus. As can be seen, our algorithm outperforms the current state-of-the-art results in every tolerance value. Also, as might be expected, the  X  -insensitive loss seems more robust to the use of a  X  value at test time that is larger than the  X  value used in training. The main result of this paper is the loss gradient theorem of Section 3. This theorem provides a theoretical foundation for perceptron-like training methods with updates computed as a difference between the feature vectors of two different inferred outputs where at least one of those outputs is inferred with loss-adjusted inference. Perceptron-like training methods using feature differences between two inferred outputs have already been shown to be successful for machine translation but theoretical justification has been lacking. We also show the value of these training methods in a phonetic alignment problem.
 Although we did not give an asymptotic convergence results it should be straightforward to show that under the update given by (6) we have that w t converges to a local optimum of the objective provided that both  X  t and t go to zero while P t  X  t t goes to infinity. For example one could take  X  = t = 1 / An open problem is how to properly incorporate regularization in the case where only a finite cor-pus of training data is available. In our phoneme alignment experiments we trained only a seven dimensional weight vector and early stopping was used as regularization. It should be noted that naive regularization with a norm of w , such as regularizing with  X  || w || 2 , is nonsensical as the loss E [ L ( y,y w ( x ))] is insensitive to the norm of w . Regularization is typically done with a surrogate loss function such as hinge loss. Regularization remains an open theoretical issue for direct gradi-ent descent on a desired loss function on a finite training sample. Early stopping may be a viable approach in practice.
 Many practical computational problems in areas such as computational linguistics, computer vision, speech recognition, robotics, genomics, and marketing seem best handled by some form of score op-timization. In all such applications we have two optimization problems. Inference is an optimization problem (approximately) solved during the operation of the fielded software system. Training in-volves optimizing the parameters of the scoring function to achieve good performance of the fielded system. We have provided a theoretical foundation for a certain perceptron-like training algorithm by showing that it can be viewed as direct stochastic gradient descent on the loss of the inference system. The main point of this training method is to incorporate domain-specific loss functions, such as the BLEU score in machine translation, directly into the training process with a clear theoretical foundation. Hopefully the theoretical framework provided here will prove helpful in the continued development of improved training methods.
 [1] F. Brugnara, D. Falavigna, and M. Omologo. Automatic segmentation and labeling of speech [2] D. Chiang, K. Knight, and W. Wang. 11,001 new features for statistical machine translation. [3] M. Collins. Discriminative training methods for hidden markov models: Theory and experi-[4] J.-P. Hosom. Speaker-independent phoneme alignment using transition-dependent states. [5] J. Keshet, S. Shalev-Shwartz, Y. Singer, and D. Chazan. A large margin algorithm for speech [6] K.-F. Lee and H.-W. Hon. Speaker independent phone recognition using hidden markov mod-[7] P. Liang, A. Bouchard-Ct, D. Klein, and B. Taskar. An end-to-end discriminative approach to [8] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In Advances in Neural [9] D.T. Toledano, L.A.H. Gomez, and L.V. Grande. Automatic phoneme segmentation. IEEE [10] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured [11] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational
