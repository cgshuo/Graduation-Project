 ronnyluss@gmail.com Assume we have a set of n data observations ( x where the partial order here will usually be the standard Euclidean one, i.e., x  X  j . Given these definitions, isotonic regression solves problems.
 An equivalent formulation of L by solving in practice as will be shown) has resulted in overlooking the results that follow [5, 6]. 1.1 Contribution detailed below, even larger problems can be solved.
 with future directions.
 Notation The weight of a set of points A is defined as y of entire space. For two blocks A and B , we denote A B if  X  x  X  A, y  X  B such that x y and  X  ( minorant ) of X  X  A where A =  X  k We first describe the structure of the classic L globally optimal isotonic regression solution. 2.1 Structure following Karush-Kuhn-Tucker (KKT) conditions:  X  ij &gt; 0  X   X  y i =  X  y j have the same fitted value. For observations in different blo cks,  X  as one of finding a division into isotonic blocks. 2.2 Partitioning total number of iterations in the worst case.
 optimal and thus  X  y  X  outflow of a group V as to the KKT conditions. The partition here looks for two such g roups. Denote by C problem where V  X  optimal block. The optimal cut problem (3) can also be writte n as the binary program the optimal cut can be determined by solving the linear progr am where z optimal global (isotonic) solution to the isotonic regress ion problem (2). Algorithm 1 Paritioning Algorithm Require: Observations y Require: V = {{ 1 , . . . , n }} , C = { (0 , { 1 , . . . , n } , {} ) } , W = {} . 1: while V 6 = {} do 2: Let ( val, w  X  , w + )  X  C be the potential cut with largest val . 3: Update V = ( V \ ( w  X   X  w + ))  X  { w  X  , w + } , C = C \ ( val, w  X  , w + ) . 4: for all v  X  { w  X  , w + } do 5: Set z i = y i  X  y 6: Solve LP (5) with input z and get x  X  . 7: if x  X  8: Update V = V \ v and W = W  X  v . 9: else 10: Let v  X  = { i : x  X  11: Update C = C  X  { ( z T x  X  , v  X  , v + ) } 12: end if 13: end for 14: end while 15: return W the optimal groups 2.3 Convergence holes) and is the union of optimal blocks.
 a cut made by solving (5) does not cut through any block in the g lobal optimal solution. V that get broken by the cut. Define M define M L the algorithm makes partitions, the following two conseque nces can be proven: (1) y optimality (i.e. according to KKT conditions) and isotonic ity and (2) y This is proven by showing that y lower side of the cut, resulting in M y leads to the contradiction y then a direct consequence of repeatedly applying Theorem 1 i n Algorithm 1: having to rejoin observations that are divided at a previous iteration). be solved efficiently. 3.1 Network flow problems network algorithms.
 where again z t where z an overview of network simplex methods). 3.2 Large-scale decompositions I ( z i  X  0 I
J  X  I divides the nodes in J into a lower and upper group, denoted J the same problem solved on the remaining nodes in V \ J nodes. This is formalized in Proposition 3.
 to Problem (5) on the reduced set J and full set V of nodes, respectively. If w  X  instead w  X  Proof. Denote W the set of nodes such that w  X  written in the following form with separable objective: Start with an initial solution x groups.
 C with k nodes is then constructed as the upper left (lower right) k  X  k sub-matrix of C. Algorithm 2 Iterative algorithm for linear program (5) Require: Observations y Require: M AXSIZE of problem to be solved by general LP solver Require: V = { 1 , . . . , n } , L = U = {} . 1: while |V|  X  M AXSIZE do 2: ELIMINATE A MINORANT SET OF NODES: 3: Build a minorant subtree T . 4: Solve linear program (5) on T and get solution  X  y  X  { X  1 , +1 } |T | . 5: L = L  X  { v  X  T :  X  y v =  X  1 } , V = V \ { v  X  T :  X  y v =  X  1 } . 6: ELIMINATE A MAJORANT SET OF NODES: 7: Build majorant subtree T . 8: Solve linear program (5) on T and get solution  X  y  X  { X  1 , +1 } |T | . 9: U = U  X  { v  X  T :  X  y v = +1 } , V = V \ { v  X  T :  X  y v = +1 } . 10: end while 11: Solve linear program (5) on V and get solution  X  y  X  { X  1 , +1 } |V| . 12: L = L  X  { v  X  T :  X  y v =  X  1 } , U = U  X  { v  X  T :  X  y v = +1 } . Computational performance of this reduction is demonstrat ed in Section 5. Lemma 4 Algorithm 2 optimally solves Problem (5).
 not yet been optimally solved for. levels, and at each level k , LP (5) is solved 2 k times on instances of size n complexity of on large simulated data sets. 5.1 Large-Scale Computations to worse, the two algorithms scale comparably in practice.
 (right axis) which goes up to more than 10 7 constraints. 5.2 Predictive Performance Data is simulated as above and responses are constructed as y while higher dimensions increase overfitting which, in turn , decreases performance. for isotonic regression with different loss functions.
