 Long named entities are frequently abbreviated in oral Chinese language for efficiency and simplic-ity. Therefore, abbreviation modeling is an impor-tant building component for many systems that ac-cept spoken input, such as directory assistance and voice search systems.

While English abbreviations are usually formed as acronyms, Chinese abbreviations are much more complex, as shown in Figure 1. Most of the Chi-nese abbreviations are formed by selecting several characters from full-names, which are not necessar-ily the first character of each word. Usually the orig-inal character order in the full-name is preserved in the abbreviation. However, re-ordering of charac-ters as shown in the third example in Figure 1 where characters  X  n  X  and  X   X   X  are swapped in the abbre-viation, also happens.

There has been a considerable amount of research on extracting full-name and abbreviation pairs in the same document for obtaining abbreviations (Li and Yarowsky, 2008; Sun et al., 2006; Fu et al., 2006). However, generation of abbreviations given a full-name is still a non-trivial problem. Chang and Lai (Chang and Lai, 2004) have proposed using a hidden Markov model to generate abbreviations from full-names. However, their method assumes that there is no word-to-null mapping, which means that every word in the full-name has to contribute at least one character to the abbreviation. This assump-tion does not hold for organizations X  names which have many word skips in the abbreviation genera-tion.

The CRF was first introduced to natural language processing (NLP) by (Lafferty et al., 2001) and has been widely used in word segmentation, part-of-speech (POS) tagging, and some other NLP tasks. In this paper, we convert the Chinese abbreviation generation process to a CRF tagging problem. The key problem here is how to find a group of discrim-inant and robust features. After using the CRF, we get a list of abbreviation candidates with associate probability scores. We also use the prior condi-tional probability of the length of the abbreviations given the length of the full-names to complement the CRF probability scores. Such global information is hard to include in the CRF model. In addition, we apply the full-name and abbreviation candidate co-occurrence statistics obtained on the web to increase the correctness of the abbreviation candidates. Chinese abbreviations are generated by three meth-ods (Lee, 2005): reduction, elimination, and gener-alization.

Both in the reduction and elimination methods, characters are selected from the full-name, and the order of the characters is sometime changed. Note that this paper does not cover the case when the or-der is changed. The elimination means that one or more words in the full-name are ignored completely, while the reduction requires that at least one char-acter is selected from each word. All the three ex-amples in Figure 1 are produced by the elimination, where at least one word is skipped.

Generalization, which is used to abbreviate a list of similar terms, is usually composed of the number of terms and a shared character across the terms. A example is  X  n  X  (three forces) for  X   X   X   X  (land force, sea force, air force). This is the most difficult scenario for the abbreviations and is not considered in this paper. 3.1 CRF model A CRF is an undirected graphical model and assigns the following probability to a label sequence L = l l
P ( L | C ) =
Here, f ture,  X  of the k -th feature in the model, and Z ( C ) is the nor-malization term that makes the summation of the probability of all label sequences to 1. CRF training is usually performed through the typical L-BFGS al-gorithm (Wallach, 2002) and decoding is performed by Viterbi algorithm (Viterbi, 1967). In this paper, we use an open source toolkit  X  X rf++ X . 3.2 Abbreviation modeling as a tagging In order to use the CRF method in abbreviation gen-eration, the abbreviation generation problem was converted to a tagging problem. The character is used as a tagging unit and each character in a full-name is tagged by a binary variable with the values of either Y or N: Y stands for a character used in the abbreviation and N means not. An example is given in Figure 2.
 3.3 Feature selection for the CRF In the CRF method, feature function describes a co-occurrence relation, and it is defined as f ( l tion, and takes the value 1 when both observation c and transition l breviation generation model, we use the following features: 1. Current character The character itself is the most important feature for abbreviation as it will be either retained or discarded. For example,  X   X   X  (bu-reau) and  X   X   X  (institue), indicating a government department, are very common characters used in ab-breviations. When they appear in full-names, they are likely to be kept in abbreviations. 2. Current word In the full name of  X   X  I  X   X   X   X   X  (China Agricultural university), the word  X   X  I  X  (China) is usually ignored in the abbreviation, but the word  X   X   X   X  (agriculture) is usually kept. The length (the number of characters) is also an im-portant feature of the current word. 3. Position of the current character in the cur-rent word Previous work (Chang and Lai, 2004) showed that the first character of a word has high possibility to form part of the abbreviation and this is also true for the last character of a three-character word. 4. Combination of feature 2. and 3. above Combination of the features 2 and 3 is expected to improve the performance, since the position infor-mation affects the abbreviation along with the cur-rent word. For example, ending character in  X   X   X   X  (university) and that in  X   X   X   X  (research institute) have very different possibilities to be selected for ab-breviations.

Besides the features above, we have examined context information (previous word, previous char-acter, next character, etc.) and other local features like the length of the word, but these features did not improve the performance. The reason may be due to the sparseness of the training data. 4.1 Length model There is a strong correlation between the length of organizations X  full-names and their abbreviations. We use the length modeling based on discrete prob-ability of P ( M | L ) , in which the variables M and L are lengths of abbreviations and full-names, re-spectively. Since it is difficult to incorporate length information into the CRF model explicitly, we use P ( M | L ) to rescore the output of the CRF.

In order to use the length information, we model the abbreviation process with two steps:  X  1st step: evaluate the length in abbreviation ac- X  2nd step: choose the abbreviation, given the We assume the following approximation: in which variable A is the abbreviation and F is the full-name; P ( M | L ) is the length model, and the sec-ond probability can be calculated according to the Bayesian rule:
It is obvious that P ( A,M | F ) = P ( A | F ) (as A contains the information M implicitly) and P ( A | F ) can be obtained from the output of the CRF. 4.2 Web search engine Co-occurrence of a full-name and an abbreviation candidate can be a clue of the correctness of the ab-breviation. We use the  X  X bbreviation candidate X +  X  X ull-name X  as queries and input them to the most popular Chinese search engine (www.baidu.com), and then we use the number of hits as the metric to perform re-ranking. The hits is theoretically re-lated to the number of pages which contain both the full-name and abbreviation. The bigger the value of hits, the higher probability that the abbreviation is correct.

We then simply multiply the previous probability score, obtained from Eq. 2, by the number of hits and re-rank the top-30 candidates accordingly.
There are some other ways to use information re-trieval methods (Mandala et al., 2000). Our method has an advantage that the access load to the web search engine is relatively small. 5.1 Data introduction The corpus we use in this paper comes from two sources: one is the book  X  X odern Chinese abbre-viation dictionary X  (Yuan and Ruan, 2002) and the other is the wikipedia. Altogether we collected 1945 pairs of organization full-names and their abbrevia-tions.

The data is randomly divided into two parts, a training set with 1298 pairs and a test set with 647 pairs. Table 1 shows the length mapping statistics of the training set. It can be seen that the average length of full-names is about 7.29. We know that for a full-name with length N , the number of abbrevia-tion candidates is about 2 N  X  2  X  N (exclude length of 0, 1, and N ) and we can conclude that the average number of candidates for organization names in this corpus is more than 100. 5.2 Results The abbreviation method described is part of a project to develop a voice-based search application. For our name abbreviation system we plan to add 10 abbreviation candidates for each organization name into the vocabulary of our voice search application, hence here we consider top-10 coverage.
Figure 3 shows the result for various combina-tions of features introduced in Section 3.3.
Figure 4 displays the coverage results obtained using the CRF method and the improvements gained from the inclusion of the length feature and the web search hits. As we can see the CRF gives a coverage 79.9%. Both length model and web search engine show significant improvement over the CRF base-line and the coverage increases to 88.3%. The CRF works well in generating abbreviations for organization names, while both length model and web search engine further improve the performance.
We are going to perform word clustering or char-acter clustering to alleviate the data sparseness prob-lem. Also we notice that multiple abbreviations for single full-name is very common, such as  X   X  I  X  &gt;  X   X  (China central television) with abbrevi-ations  X   X   X  and  X   X   X . We plan to collect multiple abbreviations for reference. After that we are going to combine the abbreviation modeling in the voice search system to alleviate the weakness of speech recognition for unknown abbreviation words, which are unlikely to be correctly recognized due to the out of vocabulary problem.

