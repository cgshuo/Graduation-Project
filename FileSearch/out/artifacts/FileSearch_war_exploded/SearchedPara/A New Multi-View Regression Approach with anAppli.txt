 Motivated by the problem of customer wallet estimation, we propose a new setting for multi-view regression, where we learn a completely unobserved target (in our case, cus-tomer wallet) by modeling it as a  X  X entral link X  in a directed graphical model, connecting multiple sets of observed vari-ables. The resulting conditional independence allows us to reduce the maximum discriminative likelihood estimation problem to a convex optimization problem for exponential linear models. We show that under certain modeling as-sumptions, in particular, when there exist two conditionally independent views and the noise is Gaussian, this problem can be reduced to a single least squares regression. Thus, for this specific, but widely applicable setting, the  X  X nsu-pervised X  multi-view problem can be solved via a simple supervised learning approach. This reduction also allows us to test the statistical independence assumptions under-lying the graphical model and perform variable selection. We demonstrate the effectiveness of our approach on our motivating problem of customer wallet estimation and on simulation data.
 H.4.8 [ Database Management ]: Database Applications  X  Data Mining; I.2.6 [ Artificial Intelligence ]: Machine Learning Algorithms Multi-view learning, Bayesian networks, Regression
In standard predictive modeling methodology, an observed  X  X arget X  variable of interest is modeled as a function of a set of predictors. The ultimate goal is predicting the target Co pyright 2006 ACM 1 -59593-339-5/06/0008 ... $ 5.00. variable in future cases when we only observe the predic-tors. In this paper, we are interested in an  X  X nsupervised X  situation where a specific target variable exists, but is never observed, and we still desire to build a prediction model for it. The only information available is in the form of domain knowledge that indicates the existence of multiple views, which provide  X  X ndependent X  information about the unob-served target. This domain knowledge allows us to obtain a directed graphical model by formalizing the independence relations and provides a framework for inference about the target.

One example of such an application is the problem of cus-tomer wallet estimation, which is of great practical interest to us at IBM. One definition of a customer X  X  wallet for a specific product category (for example, Information Tech-nology (IT)) is the customer X  X  total budget for purchases in this product category across various vendors. As an IT vendor, IBM observes the amount its customers (which are almost invariably companies) spend with it, but does not typically have access to the customers X  budget allocation de-cisions, their spending with competitors, etc. Information about the customers X  wallet, as an indicator of their poten-tial for growth, is considered extremely valuable for market-ing, resource planning and other tasks. For a detailed survey of the motivation, problem definition, and some alternative solution approaches, see [13]. For our purpose, the impor-tant aspect of this problem is that the desired target, i.e., the customer wallet, is completely unobserved, but we have access to two sources of related information: IBM X  X  inter-nal databases, which tell us about IBM X  X  relationship with the customer, including the current and past sales by prod-uct; and publicly available firmographics about the customer company, including its revenue, industry, location, etc. Let us now take a closer look at the IT purchase process. One can reasonably argue that this involves two stages: the first where the customer company X  X  executives decide on the company X  X  IT wallet W based on the company X  X  situation and needs, which are captured by firmographics X ,andthe second, where the IT department decides on the portion of the wallet that is spent on IBM products S depending on their relationship with IBM captured by Y . The causal re-lations emerging from this purchase model can be readily represented in the form of a Bayesian network as shown in Figure 1 where X and ( S, Y )are conditionally independent of each other given W . A similar picture can be argued to apply for other business and scientific problems, e.g., esti-mating an online advertiser X  X  share of customers X  clicks. Figure 1: Causal relations between customer wallet and observed predictors
In this paper, we consider unsupervised learning problems that follow the special structure described above and pro-pose an efficient approach for solving them. In Section 2, we develop a formal description of the  X  X ulti-view X  prob-lem with conditional independence in terms of likelihood maximization, which can be readily solved using the EM methodology [10]. Further, we show that when the con-ditional distributions in the graphical model correspond to exponential linear models, the likelihood maximization re-duces to a convex optimization problem so that the EM algorithm converges to a global optimum. In Section 3, we concentrate on the special case of two views and linear mod-els with Gaussian noise, and show that it can be reduced to a supervised learning problem that involves fitting the sur-rogate response on the observed predictors. In addition to being computationally favorable, this also allows us to har-ness the inferential power of linear modeling, including vari-able selection and ANOVA-based hypothesis testing, which can be used to test the validity of our conditional indepen-dence assumptions. Section 4 presents experimental results on simulation data, which indicate that the predictive per-formance of the multi-view learning approach with no la-beled data (i.e., target is unobserved) is comparable to that of the standard supervised learning approach (i.e., when we do observe the target, but do not make use of conditional independence) using significant amount of training data. In Section 5, we apply our learning approach to our motivat-ing problem of wallet estimation and present several pieces of indirect evidence on the success of our models. Section 6 is devoted to a survey of related work and Section 7 presents our concluding remarks.

Notation. Sets such as { a 1 ,  X  X  X  ,a m } are enumerated as { a i } m i =1 and an index i running over the set { 1 ,  X  X  X  ,m denoted by [ i ] m 1 .
We first describe our multi-view learning setting and the associated directed graphical model. Then, we provide a formal definition of the unsupervised learning problem in terms of maximizing the observed discriminative likelihood.
Our modeling approach is based on grouping the predic-tor variables in the learning problem into three classes: (a) Direct predictors, which directly influence the target, or Figure 2: Bayesian network corresponding to our multi-view (here, three-view) learning setting (b) Surrogate responses, which include the variables directly (c) Indirect predictors, which influence a ce rtain surrogate
In Bayesian network terms, these three groups correspond to the parents of the target, children of the target and other parents of the children of the target, respectively.
We are interested in the case that all relevant variables in our graphical model, i.e., predictors in the Markov blan-ket [8] of the target can be disjointly partitioned into these three categories with no dependencies other than the ones specified above. We focus on this class of configurations as these result in multiple views that are conditionally inde-pendent of each other given the target, which enables us to obtain a more  X  X earnable X  parametric form for the joint distribution, i.e., one with a fewer degrees of freedom. Using the same notation as in our wallet example, let W denote the (unobserved) target and X denote all the direct predictors or the Bayesian parents of W .Further, let { S k } N c k =1 be the surrogate responses or the children of W and { Y k } N c k =1 the corresponding indirect predictors or Bayesian parents where N c is the number of children of W . Figure 2 shows the resulting Bayesian network. The Markov blanket of the target W is given by the set M = { pendent of all other factors given the predictors in M ,we focus only on this set. First, we observe that since the three sets of predictor (direct, surrogate and indirect) are dis-joint, the views corresponding to the sets { X } , { S 1 ,Y { S
N c ,Y N c } are all conditionally independent of each other given W .Hence, W forms a central link connecting all these multiple views as in Figure 2 and has to be reconstructed so as to be consistent with all the N c + 1 views. In addition to the conditional independence encoded by the Bayesian network, the parametric forms of conditional distributions of each node given its antecedents or Bayesian parents also need to be specified in order to determine the parametric form of the joint distribution of all the variables.
We now consider the problem of predicting the unobserved target W given the predictors. When W is observed, one can use the Bayesian network to estimate the parameters  X  that maximize the discriminative likelihood p ( D,  X ) ( W | M )where M is the Markov blanket of W and D denotes the observed data. In the absence of training data on W , it is not possible to compute p ( D,  X ) ( W | M ). Thebestonecandoistopre-dict the target using the parameter estimates that are most  X  X onsistent X  with the observed data as well as the Bayesian network assumptions. A natural way to quantify this con-sistency is in terms of the incomplete data likelihood, i.e., likelihood of the observed predictors. Since the goal is to estimate only the unobserved target, it is sufficient to con-sider the incomplete discriminative likelihood corresponding to the surrogate responses. Hence, our unsupervised learn-ing problem can be stated as: The above likelihood maximization problem can be readily addressed using the standard EM methodology [10, 9], which essentially involves a repeated invocation of the following two steps: (i) M-step. Estimate the parameters that correspond to (ii) E-step. Estimate the conditional distribution p ( W |
For certain special cases, the likelihood maximization prob-lem 1 can be further simplified.
 Theorem 1 1 When the conditional distributions p ( W | X ) and p ( S k | W, Y k ) correspond to generalized linear models with matching link functions, the incomplete discriminative log-likelihood corresponding to the Bayesian network in Section 2.1, i.e., L D ( X )  X  log p D,  X  ( S 1 ,  X  X  X  ,S N c | X, Y a concave function of the parameters  X  .
 As a consequence of the above theorem, the likelihood maxi-mization problem (1) reduces to a (not always strict) convex optimization problem with a unique local optimum so that the EM algorithm is guaranteed to converge to the globally optimal solution.
In this section, we present a detailed analysis of the case where the conditional distributions in the Bayesian network
Formal statement of the theorems and proofs have been omitted for brevity. Please see [9] for details. described in Section 2.1 correspond to the widely used Gaus-sian linear models. For simplicity 2 , we restrict our analy-sis to the case where there is a single surrogate response ( N c = 1) and demonstrate that the unsupervised prediction problem (1) for this case can be reduced to a single linear least squares regression.

Let the dataset D consist of n tuples of the form ( x i , y where the unobserved target w i and surrogate s i are real-valued while the direct predictor x i  X  m 1 and indirect pre-dictor y i  X  m 2 . Further, let the target W be distributed according to a Gaussian linear model based on X , i.e., where and  X  w are the model parameters that need to be estimated. Similarly, let the surrogate response S be distributed according to a Gaussian linear model based on the target W and the indirect predictor Y , such that the coefficient of W equals 1, i.e., Putting together (2) and (3), we can now compute the in-complete likelihood L D ( X ) as a function of the model pa-rameters  X  = ( , , X  w , X  s ) and optimize it using the EM-based approach [9] so as to converge to a global optimizer  X   X 
We now consider the linear least squares regression prob-lem obtained by eliminating the unobserved response W from (2) and (3). Let Z =[ X , Y ]and t =[ t , t ]. Then, we have where the error ws is the sum of the two independent errors Theorem 2 Let X and Y denote the matrices [ x 1 ,  X  X  X  x n mates for the linear regression model in (4) and let (  X  MLE be the maximum likelihood estimates for (2) and (3). Then, the estimates (  X  LS ,  X  LS ) are identical to (  X  MLE ,  X  when Z =[ X , Y ] is a full column rank matrix. Further, when [ X , Y ] is not a full column rank matrix, the optimal parameter estimates for (4) are not unique, but the set of op-timizers is identical that of the maximum likelihood problem (1) determined by the coupled models (2) and (3). The above equivalence provides a straightforward means to estimate the unobserved target via a supervised learning ap-proach on the surrogate target. Theorem 2 also ensures that the  X  MLE and  X  MLE are unbiased and consistent estimates of the true parameters while corresponding wallet predic-tions are unbiased with respect to the true wallet values. This reduction to least squares regression is also very ben-eficial from a computational perspective, as it allows us to harness the full power of linear regression methodology [14]. In particular, it is now possible to apply variable selection methodologies, such as forward and backward selection, and analysis of variance (ANOVA) for testing goodness of fit for nested models.

The use of ANOVA is particularly interesting, since it allows us, to some extent, to test the conditional indepen-dence implied by our graphical model. Equation (4) defines
A similar analysis is possible for multiple surrogate re-sponses [9].
 Figure 3: Target prediction error using multi-view approach and regular least squares regression with varying number of samples. Num. Attributes =3 and variances  X  s =  X  w =0 . 5 . the predictor matrix Z as a concatenation of the columns of X and Y . What if we wanted to extend the predictor matrix as  X 
Z =[ X 2 , Y 2 ], where X 2 denotes a matrix of size n  X  m containing all interactions between variables in X ,andsim-ilarly for Y 2 ? Such a model would be completely consistent with both our linear model assumption and the graphical model in Figure 2, it would just be a more elaborate model, and an ANOVA can determine whether it is supported by the data. But what if we also wanted to add interactions between variables in X and variables in Y ?Thatwouldbe a violation of the conditional independence assumption in-herent in Figure 2, since it defies the additive representation in (2, 3). Thus, if an ANOVA would tell us that a model with interactions between variables in X and Y is superior, that would cast a severe doubt on our independence assump-tions and/or our parametric assumptions. In Section 5, we show an example of such an ANOVA on our customer wal-let prediction problem, and demonstrate that the additivity hypothesis  X  and hence, our conditional independence and parametric assumptions  X  cannot be rejected.
We now present results on simulation data to demonstrate the effectiveness of our learning methodology. We consider two learning tasks: (i) linear least-squares regression suit-able for a continuous real-valued target, and (ii) logistic re-gression tailored for a binary-valued target. In both cases, we show that even without any training data on the tar-get, one can obtain good prediction accuracy by exploiting the conditional independence relations between the various predictors.
The first task involves predicting a continuous real-valued target W using predictors S , X and Y of similar type. We assume a generative model similar to (2) and (3) and cre-ated synthetic datasets using random choices of the attribute values X , Y and model parameters , . On this data, we compared the performance of our multi-view approach de-scribed in Section 3, with standard least squares regression that requires training data on W and directly builds a lin-Figure 4: Misclassification error using multi-view approach and regular logistic regression with vary-ing number of samples. Num. Attributes =10. ear model on all the predictors. The quality of prediction in each case was measured in terms of mean squared error of the target on a hold out set of 100 samples and averaged over 100 runs. Figure 3 shows the prediction accuracy of our unsupervised multi-view approach and that of regular least squares regression approach with varying number of sam-ples. We observe that the multi-view method with about 200 samples provides an accuracy similar to the supervised approach with 70 labeled samples clearly indicating that the coupled model constraints along with the data on the pre-dictors provide information that compensate for the lack of training data on the target.
The second task corresponds to a classification scenario where the goal is to predict a binary-valued target W using predictors S , X and Y where S is binary-valued while X and Y are set of continuous real-valued attributes. For this case, we assume the following logistic generative model: and generate data by randomly choosing the attribute values and the model parameters. Since the parameter estimation problems corresponding to the above coupled models can-not be reduced to a simpler form as in case of Gaussian linear models, we follow the EM-based approach outlined in Section 2 to estimate the unobserved target W .Asinthe previous case, we compare our multi-view approach with that of a regular logistic regression model based on all pre-dictors, i.e., S, X and Y . Since this is a classification task, the quality of prediction was measured in terms of the mis-classification error on a hold out set of 100 samples. Figure 5 shows the misclassification error for the unsupervised multi-view approach and that of regular logistic regression with increasing number of samples. As in the previous case, we find that the multi-view approach can provide fairly good accuracy without using the target information or class la-bels.

In this section, we provide a more detailed description of our motivating customer wallet estimation problem. We apply the linear regression methodology of Section 3 to a dataset of IBM customers, use an ANOVA test to validate the conditional independence assumption under these mod-eling assumptions, and discuss some of the practical issues involved in obtaining useful predictions. For a more detailed discussion of the business motivation and alternative mod-eling approaches, see [13].

Recall our setting from Section 1: we have a set of firmo-graphic variables X , an unobserved wallet W ,customer X  X  actual spending with IBM S , and a set of IBM relationship variables Y .Thevariablesin X are publicly available data from Dun &amp; Bradstreet ( http://www.dnb.com ) containing information about a company X  X  industry, size, etc. The cus-tomer X  X  actual spending with IBM S and the IBM relation-ship variables Y come from IBM X  X  internal data warehouse.
We first assume the causal and conditional independence relationships described in Figure 1. As in the setup of Section 3, we additionally assume that the discriminative models for the target p ( W | X ) and the surrogate response p ( S | W, Y )arelinearinanappropriat erepresentationofthe variables and have Gaussian noise. Throughout our anal-ysis, we transform all monetary variables  X  in particular, S =IBM SALES, used as response  X  to the log scale, due to the fact that these numbers have very long tailed dis-tributions (cf. the oft-cited  X  X areto rule X ). Thus, our two fundamental modeling equations are: where c w ,c sw are constants and the second equation is con-ditional on W ,and 1 , ..., n and  X  1 , ...,  X  n are i.i.d. Gaussian random variables denoting the noise (as we showed in Sec-tion 3, our setup inevitably leads to assuming equal noise variance). When equations (5) and (6) are added together, we obtain the equivalent linear regression, as discussed in Section 3: log( s i )= f  X  ( x i )+ g  X  ( y i )+( c w + c sw )+( i +  X  From Theorem 2, we observe that a maximum likelihood solution  X   X ,  X   X  of (7) obtained through linear least squares regression, is also a maximum likelihood solution for (5, 6).
We consider three possible forms for f  X  and g  X  : (A) Simple linear model with no interactions, i.e., f  X  ( x  X  x i and similarly for g  X  . (B) Linear model that includes interactions only within the variables in X and Y , but not between variables in both groups. Thus, we assume f  X  ( x ) only consists of factors of the type x k  X  x l , and similarly for g  X  . (C) Finally, we also consider a model with all possible inter-actions between X and Y . This model is not consistent with the representation in 7, and we use this model to validate the conditional independence assumption via an analysis of variance (ANOVA).

Table 1 shows an ANOVA table resulting from fitting these three nested models to our data. As we can see, the within-view interaction model B is clearly a better fit than the no interaction model A (F-statistic p value of about 10  X  4 ). On the other hand, the all-interaction model C does not significantly improve our fit over model B ( p value of 0.08). If the improvement were significant, it could be taken No interaction (A) 10736.7 21 Within-group (B) 10033.5 75 1.74 0.0001 All interaction (C) 9382.5 100 1.21 0.081 as evidence against the conditional independence we assume (and the validity of the graphical model), since one possible reason for the model C being better would be if the errors in (5) were not independent of the variables in Y or the errors in (6) were not independent of the variables in X .Asit is, the results can be taken as validation (although not as proof) of conditional independence.

We next want to recover wallet predictions from our model, and investigate their quality, which we have limited ability to do since the actual wallet is never observed. First, we have to address the existence of intercept (often referred to as  X  X ias X  in machine learning) in our basic models (5, 6). Recall that our main result in Theorem 2 requires that the predictor matrix be of full column rank. Consequently, we cannot estimate separate intercepts for (5, 6) through the linear regression in (7), but rather can only estimate the sum of the two intercepts ( c w + c sw ). To allow an intercept in both of (5, 6), we need to use additional, external infor-mation to estimate an adjustment  X  c w , leading us to predict the case of wallet estimation, the additional piece of infor-mation we typically use is based on our expectation that every customer company X  X  IT wallet should be smaller than their revenue (which is included in X  X  let X  X  denote it by R ) and larger than the customer X  X  IT spending with IBM S . Thus, if we denote every customer X  X  revenue by r i ,and given estimates  X  f  X  ,  X  g  X  ,  X  c from (7), we can estimate  X  c value which minimizes the number of such order violations: In Figure 5, we show the results of applying this full es-timation methodology to our wallet data. The plot shows wallet estimates on 500 hold out samples compared to the observed customer revenue R and IBM sales S . The sanity check of preserving R  X   X  W  X  S holds very well, as only five predictions give S&gt;  X  W and only one gives  X  W&gt;R .
Our current work is primarily related to three main ar-eas  X  (i) statistical market analysis, (ii) Bayesian network inference, and (iii) multi-view learning.
 Statistical Market Analysis. Most of the classical mar-ket analysis approaches such as life time value modeling [12] focus on sales history. Recent studies [7] show that the share-of-wallet is in fact a better indicator of the customer growth potential. However, there has been little work on designing principled statistical methods for estimating the share-of-wallet or equivalently, the wallet itself. Our recent work [13] presents novel predictive techniques for estimat-ing wallet using quantile regression and k -nearest neighbor approaches. The current work is also an effort in the same direction, but it differs from [13] in the definition of the wallet and the modeling assumptions.
 Figure 5: Evaluating wallet predictions on hold out data.
 Bayesian Network Inference. The core idea in our learn-ing methodology is to transform our  X  X nsupervised X  predic-tion problem into a Bayesian network inference problem [8] by exploiting the causal relations between the target and predictors. Our approach is similar in spirit to other unsu-pervised learning techniques based on latent variable mod-eling such as mixture learning [6], factor analysis [1] and probabilistic PCA [3] with the key difference being in the structure of the network. Due to the special structure of the Bayesian networks that we consider, one can obtain a significant simplification for a large class of parametric dis-tributions based on exponential linear models. In particular, for coupled Gaussian linear models, the unsupervised learn-ing problem reduces to a single least squares regression. Multi-view Learning. Recently, there has been much in-terest in the area of multi-view learning, which deals with learning from multiple sets of features that provide  X  X nde-pendent X  information about the desired target. Most of the existing techniques in this area such as co-training [4, 5], co-EM [11, 2] assume conditional independence of views and focus on the classification task, usually in semi-supervised setting. These techniques also involve additional compatibil-ity assumptions and learn the desired target while explicitly minimizing the disagreement rate between the predictions arising from the multiple views. Our methodology also as-sumes conditional independence relations among multiple views, but differs from co-training and co-EM as we do not make any additional compatibility assumptions or minimize an explicit disagreement term. Instead, we learn the tar-get by maximizing the consistency of the views directly in terms of the observed discriminative likelihood of a Bayesian network that conforms to the causal relations among those views.
We proposed a multi-view regression methodology for un-supervised settings that involves exploiting the causal rela-tions between an unobserved target and different subsets of the available predictors to obtain a Bayesian network with a special structure, which can then be learned using an EM-based approach. For parametric distributions based on ex-ponential distributions that satisfy certain structural con-straints, the EM algorithm was shown to converge to the global optimum. We also presented a detailed analysis of the widely applicable case involving two views and Gaus-sian linear models and showed how that it can be reduced to a single least squares regression problem. This reduction is practically significant as it allows us to perform variable selection and test the independence assumptions underlying the Bayesian network. Experimental results on the customer wallet estimation problem as well as on simulation data in-dicate the effectiveness and flexibility of our approach. We believe the proposed methodology and the reduction to least squares regression are likely to be useful for other real-life business and scientific applications, beyond our wallet es-timation problem, that have the conditional independence and causality structure supported by our formulation. [1] D. Bartholomew and M.Knott. Latent Variable Models [2] S. Bickel and T. Scheffer. Estimation of mixture [3] C. M. Bishop. Latent variable models. In Learning in [4] A. Blum and T. Mitchell. Combining labeled and [5] S. Dasgupta, M. Littman, and D. McAllester. PAC [6] A. P. Dempster, N. M. Laird, and D. B. Rubin. [7] R. Garland. Share of wallet X  X  role in customer [8] D. Heckerman. A tutorial on learning with Bayesian [9] S. Merugu, S. Rosset, and C. Perlich. A new [10] R. Neal and G. Hinton. A view of the EM algorithm [11] K. Nigam and R. Ghani. Analyzing the effectiveness [12] S. Rosset, E. Neumann, U. Eick, N. Vatnik, and [13] S. Rosset, C. Perlich, B. Zadrozny, S. Merugu, [14] S. Weisberg. Applied Linear Regression . Wiley, 1985.
