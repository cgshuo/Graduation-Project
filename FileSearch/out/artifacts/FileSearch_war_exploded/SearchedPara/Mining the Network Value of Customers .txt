 and market only to those [19]. Data mining plays a key role in this process, by allowing the construction of models that predict a customer's response given her past buying behavior and any available demographic information [29]. When suc-cessful, this approach can significantly increase profits [34]. One basic limitation of it is that it treats each customer as making a buying decision independently of all other cus-tomers. In reality, a person's decision to buy a product is often strongly influenced by her friends, acquaintances, busi-ness partners, etc. Marketing based on such word-of-mouth networks can be much more cost-effective than the more con-ventional variety, because it leverages the customers them-selves to carry out most of the promotional effort. A classic example of this is the Hotmail free email service, which grew from zero to 12 million users in 18 months on a minuscule advertising budget, thanks to the inclusion of a promotional message with the service's URL in every email sent using it [23]. Competitors using conventional marketing fared far less well. This type of marketing, dubbed viral marketing because of its similarity to the spread of an epidemic, is now used by a growing number of companies, particularly in the Internet sector. More generally, network effects (known in the economics literature as network externalities) are of crit-ical importance in many industries, including notably those associated with information goods (e.g., software, media, telecommunications, etc.) [38]. A technically inferior prod-uct can often prevail in the marketplace if it better leverages the network of users (for example, VHS prevailed over Beta in the VCR market). 
Ignoring network effects when deciding which customers to market to can lead to severely suboptimal decisions. In addition to the intrinsic value that derives from the pur-chases she will make, a customer effectively has a network value that derives from her influence on other customers. A customer whose intrinsic value is lower than the cost of mar-keting may in fact be worth marketing to when her network value is considered. Conversely, marketing to a profitable customer may be redundant if network effects already make her very likely to buy. However, quantifying the network value of a customer is at first sight an extremely difficult un-dertaking, and to our knowledge has never been attempted. A customer's network value depends not only on herself, but potentially on the configuration and state of the entire network. As a result, marketing in the presence of strong network effects is often a hit-and-miss affair. Many startup companies invest heavily in customer acquisition, on the ba-sis that this is necessary to "seed" the network, only to face bankruptcy when the desired network effects fail to materi-This yields 1 The set of variables X u, with joint probability conditioned on X k, Y and M described by Equation 2, is an instance of a Markov random field [2, 25, 7]. Because Equation 2 expresses the probabilities P(Xi IX k, Y, M) as a function of themselves, it can be applied iteratively to find them, start-ing from a suitable initial assignment. This procedure is known as relaxation labeling, and is guaranteed to converge to locally consistent values as long as the initial assignment is sufficiently close to them [33 I. A natural choice for initial-ization is to use the network-less probabilities P(XilY, M). Notice that the number of terms in Equation 2 is expo-nential in the number of unknown neighbors of )/7/. If this number is small (e.g., 5), this should not be a problem; oth-erwise, an approximate solution is necessary. A standard method for this purpose is Gibbs sampling [16]. An alterna-tive based on an efficient k-shortest-path algorithm is pro-posed in Chakrabarti et al. [6]. 
Given N~ and Y, Xi should be independent of the mar-keting actions for other customers. Assuming a naive Bayes model for Xi as a function of Ni, Y1 .... ,Ym and Mi [11], where P(Y, Mi[NI) = P(Y, Mi[Xi = 1)P(Xi = lINi)+ P(Y, Mi[Xi = O)P(gi = O[Ni). The corresponding net-work-less probabilities are P(Xi[Y, M) = P(Xi)P(Mi[Xi) I'I~'n=l P(Y~[Xi)/P(Y, Mi). Given Equation 3, in order to compute Equation 2 we need to know only the following probabilities, since all terms reduce to them: P(Xi[Ni), tion of P(Xi[Ni), all of these are easily obtained in one pass through the data by counting (assuming the Yk are discrete or have been pre-discretized; otherwise a univariate model can be fit for each numeric Yk). The form of P(XI[Ni) de-pends on the mechanism by which customers influence each other, and will vary from application to application. In the next section we focus on the particular case where X is the set of users of a collaborative filtering system. 
For simplicity, assume that M is a Boolean vector (i.e., only one type of marketing action is being considered, such as offering the customer a given discount). Let c be the cost of marketing to a customer (assumed constant), r0 be the revenue from selling the product to the customer if no marketing action is performed, and r, be the revenue if mar-keting is performed, ro and rl will be the same unless the 1 The same result can be obtained by assuming that the Xj are independent given X k, Y and M. amount spent in marketing to her, and that we can estimate how much needs to be spent to produce a given increase in buying probability. The optimal customer acquisition cost for customer i is then the value of c/that maximizes her to-tal value ELP(X k, Y, f~ (M)) -ELP(X k, Y, f X (M)), with IMIc replaced by ~"]~=1 c/ in Equation 5. 
Arguably, a decade ago it would have been difficult to make practical use of a model like Equation 2, because of the lack of data to estimate the influence probabilities" P(Xi[NI). Fortunately, the explosion of the Internet has drastically changed this. People influence each other online (and leave a record of it) through postings and responses to newsgroups, review and knowledge-sharing sites like epin-ions.corn, chat rooms and IRC, online game playing and MUDs, peer-to-peer networks, email, interlinking of Web pages, etc. In general, any form of online community is a potentially rich source of data for mining social networks from. (Of course, mining these sources is subject to the usual privacy concerns; but many sources are public infor-mation.) In this paper we will concentrate on a particularly simple and potentially very effective data source: the col-laborative filtering systems widely used by e-commerce sites (e.g., amazon.corn) to recommend products to consumers. 
In a collaborative filtering system, users rate a set of items (e.g., movies, books, newsgroup postings, Web pages), and these ratings are then used to recommend other items the user might be interested in. The ratings may be implicit (e.g., the user did or did not buy the book) or explicit (e.g., the user gives a rating of zero to five stars to the book, depending on how much she liked it). Many algorithms have been proposed for choosing which items to recommend given the incomplete matrix of ratings (see, for example, Breese et al. [3]). The most widely used method, and the one that we will assume here, is the one proposed in GroupLens, the project that originally introduced quantitative collaborative filtering [35]. The basic idea in this method is to predict a user's rating of an item as a weighted average of the ratings given by similar users, and then recommend items with high predicted ratings. The similarity of a pair of users (i,j) is measured using the Pearson correlation coefficient: where Rak is user i's rating of item k, R~ is the mean of user i's ratings, likewise for j, and the summations and means are computed over the items k that both i and j have rated. Given an item k that user i has not rated, her rating of it is then predicted as where p = 1/~,X~eNi ]W/j[ is a normalization factor, and N/ is the set of n/ users most similar to i according to Equation 6 (her neighbors). In the limit, N/ can be the entire database of users, but for reasons of noise robustness and computational efficiency it is usually much smaller (e.g., 60 0.8 Figure 1: Empirical distribution of/~" and Xi given /~. tion, P(XiI~') was modeled using a piecewise linear func-tion. We measured P(XiI~') for each of nine bins, whose boundaries were -5.0, -2.0, -1.0, -0.5, -0.1, 0.1, 0.5, 1.0, 2.0, and 5.0. Note that while/~ must be between 0 and 5, /~i is a weighted sum of the neighbors' difference from their average, and thus may range from -5 to 5. We also had a zero-width bin located at/~ = 0. Movies were seen with low probability (1-5%), and thus there was a high probability that a movie had not been rated by any of Xi's neighbors. In the absence of a rating, a neighbor's contribution to P~ was zero. 84% of the samples fell into this zero bin. Bin bound-aries were chosen by examination of the distribution of data in the training set, shown in Figure 1. /~' was unlikely to deviate far from 0~ for the reasons given above. We used narrow bins near/~ = 0 to obtain higher accuracy in this area, which contained a majority of the data (96.4% of the data fell between -0.5 and 0.5). To combat data sparseness, both P(XiI~') and the per-bin mean/~, were smoothed for each bin using an m-estimate with m=l and the population average as the prior. 
Initially, we expected P(Xi IRi) to increase monotonically with /~.. The actual shape, shown in Figure 1, shows in-creasing P(Xi[~) as Ri moves significantly away from 0 in either direction. This shape is due to a correlation be-tween [/~i[ and the popularity of a movie: for a popular movie, /~. is more likely to deviate further from zero and Xi is more likely to be 1. Note, however, that P(Xi[~') is indeed monotonically increasing in the [-0.1,0.1] inter-val, where the highest density of ratings is. Furthermore, E[P(X~I~" &gt; 0)] = 0.203 &gt; 0.176 = E[P(Xil~ &lt; 0)]. 
While the EachMovie database is large, it has problems which had to be overcome. The movies in the database which were in theaters before January 1996 were drawn from a long time period, and so tended to be very well known movies. Over 75% (2.2 million) of the ratings were on these movies. In general, the later a movie was released, the fewer ratings and thus the less information we had for it. We divided the database into a training set consisting of all ratings received through September 1, 1996, and a test set consisting of all movies released between September 1, 1996 and December 31, 1996, with the ratings received we used it was always possible to satisfy Equation 9 and this constraint simultaneously. binations only need to be computed once. Further, since in a single search step only one Mi changes, most of the re-sults of one step can be reused in the next, greatly speeding up the search process. With these optimizations, we were able to measure the effect of over 10,000 single changes in 
M per second, on a 1 GHz Pentium III machine. In pre-liminary experiments, we found relaxation labeling carried out this way to be several orders of magnitude faster than Gibbs sampling; we expect that it would also be much faster than the more efficient version of Gibbs sampling proposed in Heckerman etal. [17]. 4 The relaxation labeling process typically converged quite quickly; few nodes ever required more than a few updates. 
To test the accuracy of our model, we computed the esti-mated probability P(X~IX k, Y, M) for each person Xi with M = M0 and X k = $. We measured the correlation between this and the actual value of Xi in the test set, over all movies, over all people. 5 (Note that, since the comparison is with test set values, we did not expect to receive ratings from inactive people, and therefore P(XilY) = 0 for them.) The resulting correlation was 0.18. Although smaller than desir-able, this correlation is remarkably high considering that the only input to the model was the movie's genre. We expect the correlation would increase if a more informative set of movie attributes Y were used. 
For the first movie in the test set ("Space Jam"), we mea-sured the network value for all 9585 active people e in the following scenario (see Equations 4 and 9): r0 = 1, rl = 0.5, c = 0.1, ~ = 1.5, and M = Mo. Figure 2 shows the 500 highest network values (out of 9585) in decreasing order. The unit of value in this graph is the average revenue that would be obtained by marketing to a customer in isolation, without costs or discounts. Thus, a network value of 20 for a given customer implies that by marketing to her we es-sentially get free marketing to an additional 20 customers. The scale of the graph depends on the marketing scenario (e.g., network values increase with ~), but the shape gen-erally remains the same. The figure shows that a few users have very high network value. This is the ideal situation for the type of targeted viral marketing we propose, since we can effectively market to many people while incurring only the expense of marketing to those few. A good customer to market to is one who: (1) is likely to give the product a high rating, (2) has a strong weight in determining the rating prediction for many of her neighbors, (3) has many neighbors who are easily influenced by the rating prediction they receive, (4) will have a high probability of purchasing the product, and thus will be likely to actually submit a rat-ing that will affect her neighbors, and finally (5) has many neighbors with the same four characteristics outlined above, 4In our experiments, one Gibbs cycle of sampling all the nodes in the network took on the order of a fiftieth of a second. The total runtime would be this value multiplied by the number of sampling iterations desired and by the number of search steps. 5Simply measuring the predictive error rate would not be very useful, because a very low error rate could be obtained simply by predicting that no one sees the movie. eInactive people always have a network value of zero. movie offer, the profit from direct marketing could not be positive, since without network effects we were guaranteed to lose money on anyone who saw a movie for free. Figure 3 shows that our method was able to find profitable market-ing opportunities that were missed by direct marketing. For the discounted movie, direct marketing actually resulted in a loss of profit. A customer that looked profitable on her own may actually have had a negative overall value. This situation demonstrates that not only can ignoring network effects cause missed marketing opportunities, but it can also make an unprofitable marketing action look profitable. In the advertising scenario, for small c~ our method increased profits only slightly, while direct marketing again reduced them. Both methods improved with increasing c~, but our method consistently outperformed direct marketing. 
As can be seen in Figure 3, greedy search produced re-sults that were quite close to those of hill climbing. The average difference between greedy and hill-climbing profits (as a percentage of the latter) in the three marketing sce-narios was 9.6%, 4.0%, and 0.0% respectively. However, as seen in Figure 3, the runtimes differed significantly, with hill-climbing time ranging from 4.6 minutes to 42.1 minutes while greedy-search time ranged from 3.8 to 5.5 minutes. The contrast was even more pronounced in the advertising scenario, where the profits found by the two methods were nearly identical, but hill climbing took 14 hours to com-plete, compared to greedy search's 6.7 minutes. Single-pass was the fastest method and was comparable in speed to di-rect marketing, but led to significantly lower profits in the free and discounted movie scenarios. 
The lift in profit was considerably higher if all users were assumed to be active. In the free movie scenario, the lift in profit using greedy search was 4.7 times greater than when the network had inactive nodes. In the discount and adver-tising scenarios the ratio was 4.1 and 1.8, respectively. This was attributable to the fact that the more inactive neighbors a node had, the less responsive it could be to the network. From the point of view of an e-merchant applying our ap-proach, this suggests modifying the collaborative filtering system to only assign active users as neighbors. 
Social networks have been an object of study for some time, but previous work within sociology and statistics has suffered from a lack of data and focused almost exclusively on very small networks, typically in the low tens of indi-viduals [41]. Interestingly, the Google search engine [4] and Kleinberg's (1998) HITS algorithm for finding hubs and au-thorities on the Web are based on social network ideas. The success of these approaches, and the discovery of widespread network topologies with nontrivial properties [42], has led to a flurry of research on modeling the Web as a semi-random graph (e.g., Kumar et al. [28], Barab~si et al. [1]). Some of this work might be applicable in our context. 
In retrospect, the earliest sign of the potential of viral marketing was perhaps the classic paper by Milgram [31] estimating that every person in the world is only six edges away from every other, if an edge between i and j means "i knows j." Schwartz and Wood [37] mined social relation-ships from email logs. The ReferralWeb project mined a so-cial network from a wide variety of publicly-available online information [24], and used it to help individuals find experts who could answer their questions. The COBOT project 
I O 15o 
I 0 lO evant probabilities are the same for all customers, and is only applied to a made-up network with seven nodes. 
Collaborative filtering systems proposed in the literature include GroupLens [35], PHOAKS [40], Siteseer [36], and others. A list of collaborative filtering systems, projects and related resources can be found at www.sims.berkeley.-
The type of data mining proposed here opens up a rich field of directions for future research. In this section we briefly mention some of the main ones. 
Although the network we have mined is large by the staa-daxds of previous research, much larger ones can be en-visioned. Scaling up may be helped by developing search methods specific to the problem, to replace the generic ones we used here. Segmenting a network into more tractable parts with minimal loss of profit may also be important. Flake et al. [13] provide a potential way of doing this. A related approach would be to mine subnetworks with high profit potential embedded in larger ones. Recent work on mining significant Web subgraphs such as bipartite cores, cliques and webrings (e.g., [28]) provides a starting point. More generally, we would llke to develop a characterization of network types with respect to the profit that can be ob-tained in them using an optimal marketing strategy. This between users is richer and stronger there. For example, it may be profitable for a company to offer its products at a loss to influential contributors to such sites. Our method is also potentially applicable beyond marketing, to promot-ing any type of social change for which the relevant network of influence can be mined from available data. The spread of online interaction creates unprecedented opportunities for the study of social information processing; our work is a step towards better exploiting this new wealth of information. 
This paper proposed the application of data mining to vi-ral marketing. Viewing customers as nodes in a social net-work, we modeled their influence on each other as a Markov random field. We developed methods for mining social net-work models from collaborative filtering databases, and for using these models to optimize marketing decisions. An empirical study using the EachMovie collaborative filtering database confirmed the promise of this approach. [1] A. L. Barab~si, R. Albert, and H. Jong. Scale-free [2] J. Besag. Spatial interaction and the statistical [3] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [4] S. Brin and L. Page. The anatomy of a large-scale [5] B. Cestnik. Estimating probabilities: A crucial task in [6] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced [7] R. Chellappa and A. K. Jain, editors. Markov Random [8] D. M. Chickering and D. Heckerman. A decision [9] D. J. Cook and L. B. Holder. Graph-based data [10] G. F. Cooper. A simple constraint-based algorithm for [11] P. Domingos and M. Pazzani. On the optimality of the [28] R. Kumar, P. IL~ghavan, S. Rajagopalan, and [29] C. X. Ling and C. Li. Data mining for direct [30] D. It. Mani, J. Drew, A. Betz, and P. Datta. Statistics [31] S. Milgra.m. The small world problem. Psychology [32] J. Neville and D. Jensen. Iterative classification in [33] L. Pelkowitz. A continuous relaxation labeling [34] G. Piatetsky-Shapiro and B. Masand. Estimating [35] P. Itesnick, N. Iacovou, M. Suchak, P. Bergstrom, and [36] J. Itucker and M. J. Polanco. Siteseer: Personalized [37] M. F. Schwartz and D. C. M. Wood. Discovering [38] C. Shapiro and H. It. Varian. Information Rules: A [39] C. Silverstein, S. Brin, It. Motwani, and J. Ullman. [40] L. Terveen, W. Hill, B. Amento, D. McDonald, and [41] S. Wasserman and K. Faust. Social Network Analysis: [42] D. J. Watts and S. H. Strogatz. Collective dynamics of 
