 P atter n m inin ghasbee n af o cused theme in data m inin g research area wi th a m inin ga n dc l ass i ficat ion [15].
 feature subsets fr o mb in ar y datasets ,w h i ch has bee n pr o p o sed rece n t ly in [15]. T he NIFS s are feature subset wi th h i gher o rder c o rre l at ion, w h i ch i sdefi n ed t o pru n e the search space ,w ef o u n d that there are s o me space t oi mpr ov eth o se b o u n ds b y carefu l re -e x am in at ion o fthemu l t i-in f o rmat ion.
T he c on tr i but ion o f o ur paper ca n be summar i zed as f ollow s :(1)By care -on i t .( 2 )W eusetheseb o u n ds t oi mpr ov e the pru nin gp ow er o fthem inin g the e ff ect iv e n ess o f o ur n e w b o u n ds .

T he rest o fth i spaper i s structured as f ollow s .S ect ion 2d i scusses re l ated T he nw eprese n ts o me b o u n ds o f join te n tr o p y that will be used t o pru nin gthe patter n search in gspace in S ect ion 4 .S ect ion 6 descr i bes o ur m inin ga l g o r i thm o ur wo rk in S ect ion 8 .
 Minin gfreque n t patter n s i samaturedpr o b l em in b in ar y dataset pr o cess in g . pr o p o sed in [ 2 ], there are much wo rk on freque n t patter n m inin g ,in terest mea -ter n m inin gpr o b l em t on umer i c datasets .T he auth o rs used no rma li zed mutua l wo rk a l s o f o cused on the c o rre l at ion o fpa i r o f i tems . m inin gth o se patter n s .
 wo rk i ss o me pru nin gstrateg i es t oi mpr ov etheeffic i e n c yo fthe i re n umerat ion a l g o r i thm .
 o pt i m i zat ion cr i ter i a .
 first search a n d use ser v a l strateg i es t o pru n e the patter n search space ,w h i ch are based on s o me b o u n ds on e n tr o p y. In th i s sect ion w e in tr o duce s o me no tat ion s w e will use .A b in ar y dataset D i sa N  X  m b in ar y matr ix, w here each c ol um ni safeature .W ede no te F = {
X 1 ,  X  X  X  ,X m } as the w h ol e feature subset .Ino ur d i scuss ion, w etreateach v ar i ab l es c o rresp on d in gt o features .
 Definition 1. The entropy of a random variable X , denoted as H ( X ) , is defined as All lo gar i thms are in base 2 , a n d 0lo g 0=0 b y c onv e n t ion. I t i sk nown that 0  X  H ( X )  X  lo g | X | ,wi th H ( X )=lo g | X | only f o rtheu ni f o rm d i str i but ion P ( X = x )=1 / | X | f o ra ll x  X  X .
 Definition 2. The joint entropy of two random variables X and Y , denoted as H ( X, Y ) , is defined as Definition 3. The conditional entropy of a random variable Y given another variable X denoted as H ( Y | X ) , is defined as the o r y, the reader i s referred t o[5,1 4 ].
 Lemma 1. For any two features X and Y , we have: Definition 4. The multi-information of a set of features { X 1 ,  X  X  X  ,X n } is de-fined as W ek now that H ( X 1 ,  X  X  X  ,X n )  X  in f o rmat ion.
 Definition 5. A set of features { X 1 ,X 2 ,  X  X  X  ,X n } is a strongly correlated if C ( X 1 ,X 2 ,  X  X  X  ,X n )  X   X  ,where  X &gt; 0 is a user defined threshold. In this case, { X 1 ,X 2 ,  X  X  X  ,X n } is called a Strong-correlated Feature Subset (SFS).
 Definition 6. A set of features { X 1 ,X 2 ,  X  X  X  ,X n } is a weakly correlated if C ( X 1 , X  X  X  X  ,X Definition 7. A set of features { X 1 ,X 2 ,  X  X  X  ,X n } is a Non-redundant Interacting Feature Subset (NIFS) if the following two criteria are satisfied: 1. { X 1 ,X 2 ,  X  X  X  ,X n } is an SFS; and 2. every proper subset X  X  X  X 1 ,X 2 ,  X  X  X  ,X n } is a WFS.
 Problem Description. G iv e n ab in ar y dataset D , t wo thresh ol ds  X ,  X  &gt; 0, the in teract in g feature subsets fr o m D . B ef o re prese n t in g o ur n e w b o u n ds ,w e in tr o duce a c on c l us ion fr o m [6]. Lemma 2. Let [ n ]= { 1 , 2 ,  X  X  X  ,n } . For any subset A of [ n ] we denote H ( X A ) as the joint entropy H ( X i ,i  X  A ) . For any 1  X  k  X  n ,let Then we have the following inequality: An d W eca nw r i te the in equa li t y H n  X  H 2 e x p li c i t ly as f ollow s : Now w eprese n tas i m il ar the o rem as the pr o pert y3.1 fr o m [11], there the c on c l us ion i sf o r subset c ov erage .
 Theorem 1. Given n  X  3 random variables X 1 ,X 2 ,  X  X  X  ,X n , a upper bound of H ( X 1 ,  X  X  X  ,X n ) is H ( X 1 ,  X  X  X  ,X n )  X  d iff ere n t w a y s .
 S ummar i zat ion the l eft ha n da n dr i ght ha n d o ftheab ov et wo equat ion s respect iv e ly, w eha v e : 2 H ( X 1 ,  X  X  X  ,X n )= H ( X 1 ,  X  X  X  ,X n  X  1 )+ H ( X n | X 1 ,  X  X  X  ,X n  X  1 ) w here , T heref o re , A cc o rd in gt o the ab ov e in equa li t i es ,w eha v e , W h i ch l eads t o the in equa li t y( 2 ).
  X  X  X  ,X = = 2  X  2 W h i ch l eads t o the f ollowin g in equa li t y: C ( X 1 ,  X  X  X  ,X n )  X  w here
By recurs iv e ly us in gtheab ov e in equa li t y, w eca n estab li sh a low er b o u n d o f the mutua lin f o rmat ion o fa n feature subset us in g only pa i rs o ffeature , W h il e [15] estab li shes the f ollowin g low er b o u n d , us in gthe in equa li t y(1): O n the o ther ha n d , due t o the fact that f o ra ny pa i r o f i = j w eha v ethat o f feature subset { X 1 ,  X  X  X  ,X n } as f ollow s [15]: 5.1 Adding an Feature i mpr ov eme n t on the b o u n dg iv e n there .
 Proposition 1. Let X 1 and X 2 be two features in the dataset. If the Hamming distance between X 1 and X 2 is d ,then Proposition 2. Let If the minimum Hamming distance between X n and X i (1  X  i  X  n ) is d ,then U s in gthefactthatf o r 1  X  i  X  n  X  1, H ( X n | X 1 ,  X  X  X  ,X n  X  1 )  X  H ( X n | X i ), w e get the f ollowin g in equa li t y: T heref o re w ek now that the b o u n dg iv e nin the in equa li t y(6)i st i ghter tha n the B ased on th i st i ghter upper b o u n d ,w eha v ethef ollowin gpr o p o s i t ion. Proposition 3. (Adding Proposition) Let We have that, Proof. W eca n re w r i te  X C as f ollow s , D ue t o the in equa li t y(6),w eha v e , H ( X 1 ,  X  X  X  ,X n  X  1 )  X  0. 5.2 Replacing a Feature t wo d iff ere n t w a y s :
H ( X 1 ,X 2 ,  X  X  X  ,X n )= H ( X 1 ,  X  X  X  ,X n  X  1 )+ H ( X n | X 1 ,  X  X  X  ,X n  X  1 )  X  H ( X n | X i ) f o ra ny 1  X  i  X  n  X  1, w eca n get the f ollowin g in equa li t y: A cc o rd in gt o H ( X n | X 1 ,  X  X  X  ,X n  X  1 )  X  0, w eha v e , f ollowin g in equa li t y: Proposition 4. (Replacing Proposition) Let We have, Proof. W eca n re w r i te  X C as f ollow s : U s in gthe in equa li t y(9),w eha v ethat , O n the o ther ha n d , us in gthe in equa li t y (10), w eha v ethat , in NIFS m in er (Fi gure 1), w ed ono t n eed t o ca l cu l ate the H amm in gd i sta n ce pru n e the patter n search space .
 t o get upper a n d low er b o u n ds on the mu l t i-in f o rmat ion o f X .Fi gure 1 a n d2 sh ow o ur NIFS m in er .Fo rm o re deta il, the reader i s referred t o[15].
In the wo rse case ,w here the w h ol efeaturec o mb in at ion ssh o u l dbechecked , the ru nnin gt i me o ftheab ov em inin ga l g o r i thm i se x p on e n t i a lin the n umber o ffeatures .W h il ethee x per i me n ta l resu l ts in S ect ion 7sh ow that the ru nnin g t i me o f NIFS m in er i s quadrat i c in the n umber o ffeatures ,w h i ch dem on strate the e ff ect iv e n ess o f the pru nin gmeth o ds d i scussed in the pre vio us sect ion. pr ov ed NIFS m inin ga l g o r i thm .W h il e o ur wo rk ma inly f o cuses on i mpr ovin g t oi mpr ov ethec l ass i ficat ion accurac y as in [15]. All e x per i me n ts w ere ru non a Win d ow s XP mach in e wi th In te lP e n t i um4 2 . 4G H zC PU a n d2G B R AM. Our a l g o r i thm i s i mp l eme n ted us in g J a v a .

W erep o rt o ur e x per i me n ta l resu l ts on the perf o rma n ce o f o ur NIFS m in er in t o16 ke yvo tes .T he vo tes ca n be  X  X  es  X  X  r  X  X o X  a n darede no ted b y1 a n d 0.
In the first gr o up o fe x per i me n ts ,w etestthesca l ab ili t yon the vo t in gdata set .W eset  X  =3,  X  =3 . 1, a n d n umber o fc ol um ni s 16. Fi gure 3(1) sh ow s the sh ow sthee x ecut ion t i me o f NIFS m in er on the vo t in gdataset .A sca n be see n, the e x ecut ion t i me o ft wo a l g o r i thms in creases wi th the in crease o fthe n umber o fr ow s .W he nw efi x the n umber o fr ow as 4 35, a n dst ill set  X  =3,  X  =3 . 1. W egetthet i me perf o rma n ce in v ar yin g n umber o fc ol um n as Fi gure respect t on umber o fc ol um n. B ut as a n t i c i pated ,o ur appr o ach has the better t i me perf o rma n ce tha n the on e in [15].

In the f ollowin gt wo e x per i me n ts ,w eusethe w h ol e vo t in gdataset .Fi gure 4 sh ow stheresu l ts b yv ar yin g  X  w h il efi x  X  =3 . 3. A sca n be see n, the e x ecut ion the resu l tassh own in Fi gure 4 w here v ar i es the thresh ol d  X  .A sca n be ca n, in [15].
 o fpru nin gstrateg i es based on b o u n ds d i scussed in S ect ion 4aresh own in appr o ach in creases wi th in creases o fthe n umber o fr ow s , us in gtheb o u n ds based on mutua lin f o rmat ion in S ect ion 5. T he b o tt o m o f Fi gure 5 sh ow s the pru nin g e ff ect o f v ar io us b o u n ds w epr o p o sed in pre vio us sect ion s . W ere -e x am in ed the pr o b l em fi n d in g non-redu n da n t in teract in gfeature b o u n ds .

