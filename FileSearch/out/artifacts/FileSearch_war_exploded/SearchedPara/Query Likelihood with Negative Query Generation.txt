 The query likelihood retrieval function has proven to be em-pirically effective for many retrieval tasks. From theoretical perspective, however, the justification of the standard query likelihood retrieval function requires an unrealistic assump-tion that ignores the generation of a  X  X egative query X  X rom a document. This suggests that it is a potentially non-optimal retrieval function.

In this paper, we attempt to improve the query likelihood function by bringing back the negative query generation. We propose an effective approach to estimate the probabil-ities of negative query generation based on the principle of maximum entropy, and derive a more complete query likeli-hood retrieval function that also contains the negative query generation component. The proposed approach not only bridges the theoretical gap in the existing query likelihood retrieval function, but also improves retrieval effectiveness significantly with no additional computational cost. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms Negative query generation, query likelihood, language model, probability ranking principle, principle of maximum entropy
The query likelihood retrieval method [12] has recently enjoyed much success for many different retrieval tasks [18]. The query likelihood retrieval method [12] assumes that a query is a sample drawn from a language model: given a query Q and a document D , we compute the likelihood of  X  X enerating X  query Q with a model estimated based on doc-ument D . We can then rank documents based on the likeli-hood of generating the query.

Although query likelihood has performed well empirically, there was criticism about its theoretical foundation [13, 4]. In particular, Sparck Jones questioned  X  X here is relevance? X  [4]. Responding to this criticism, Lafferty and Zhai [6] showed that under some assumptions the query likelihood retrieval method can be justified based on probability ranking princi-ple [14] which is regarded as t he foundation of probabilistic retrieval models.

However, from theoretical perspective, the justification of using query likelihood as a retrieval function based on the probability ranking principle [6] requires an unrealistic as-sumption about the generation of a  X  X egative query X  from a document, which states that the probability that a user who dislikes a document would use a query does not depend on the particular document. This assumption enables ig-noring the negative query generation in justifying using the standard query likelihood method as a retrieval function. In reality, however, this assumption does not hold because a user who dislikes a document would more likely avoid using words in the document when posing a query. This suggests that the standard query likelihood function is a potentially non-optimal retrieval function.

In order to address this theoretical gap between the stan-dard query likelihood and the probability ranking principle, in this paper, we attempt to bring back the component of negative query generation.

A main challenge in estimating the negative query gen-eration probability is to develop a general method for any retrieval case. Our solution to this problem is to estimate the probability of negative query generation purely based on document D so as to make it possible to incorporate the negative query generation for retrieving any document in re-sponse to any query. Specifically, we exploit document D to infer the queries that a user would use to avoid retrieving D based on the intuition that such queries would not likely have any information overlap with D .Wethenproposean effective approach to estimate probabilities of negative query generation based on the principle of maximum entropy [3], which leads to a negative query generation component that can be computed efficiently. Finally, we derive a more com-plete query likelihood retrieval function that also contains the negative query generation component, which essentially scoresadocumentwithrespecttoaqueryaccordingtothe ratio of the probability that a user who likes the document would pose the query to the probability that a user who dislikes the document would pose the query.

The proposed query likelihood with negative query gener-ation retrieval function not only bridges the theoretical gap in the existing query likelihood function, but also improves retrieval effectiveness significantly in our experiments. In the query likelihood retrieval method [12], given a query Q and a document D , we compute the likelihood of  X  X en-erating X  query Q with a model  X  D estimated based on doc-ument D , and then rank the document based on its query likelihood:
The query generation can be based on any language model [12, 11, 2, 19, 10, 9, 16] . So far, using a multinomial dis-tribution [11, 2, 19] for  X  D has been most popular and most successful, which is also adopted in our paper. With the multinomial distribution, the query likelihood is p ( Q |  X  w p ( w |  X  D ) query Q . According to the maximum likelihood estimator, we have the following estimation of the document language model  X  D for the multinomial model: where c ( w,D ) indicates the frequency of w in document D and | D | is the document length.  X  D needs smoothing to over-come the zero-probability problem, and an effective method is the Dirichlet prior smoothing [19]: Here  X  is the smoothing parameter (Dirichlet prior), and p ( w | C ) is the collection language model which is estimated as of term w in the whole collection C .

The query likelihood scoring function essentially ranks documents using the following formula [19]: where | Q | represents query length.
To understand the retrieval foundation of the query like-lihood method, Lafferty and Zhai [6] provided a relevance-based derivation of the query likelihood method. Formally, let random variables D and Q denote a document and query, respectively. Let R be a binary random variable that in-dicates whether D is relevant to Q or not. Following [5], we will denote by ( X  X ike X ) and  X  ( X  X ot like X ) the value of the relevance variable. The probability ranking princi-ple [14] provides a justification for ranking documents for a query based on the conditional probability of relevance, i.e., p (
R = | D, Q ). This is equivalent to ranking documents based on the odds ratio, which can be further transformed using Bayes X  Rule:
There are two different ways to decompose the joint prob-ability p ( Q, D | R ), corresponding to  X  X ocument generation X  and  X  X uery generation X  respectively. With document gener-ation p ( Q, D | R )= p ( D | Q, R ) p ( Q | R ), we have Most classical probabilistic retrieval models [15, 5, 1] are based on document generation.

Query generation, p ( Q, D | R )= p ( Q | D,R ) p ( D | R focus of this paper. With query generation, we end up with the following ranking formula: in which, the term p ( R | D ) can be interpreted as a prior of relevance on a document. Without any prior knowledge, we may assume that this term is the same across all the documents, and obtain the following simplified formula:
There are two components in this model. p ( Q | D,R = ) can be interpreted as a positive query generation model. It is essentially the basic query likelihood, which suggests that the query generation probability used in all the query likeli-hood scoring methods intuitively means the probability that a user who likes document D would pose query Q . The other component p ( Q | D,R =  X  ) can be interpreted as the genera-tion probability of a  X  X egative query X  from a document, i.e., the probability that a user who dislikes a document D would use a query Q .

However, in order to justify using the basic query likeli-hood alone as the ranking formula, an unrealistic assump-tion has to be made about this negative query generation component, which states that the probability that a user who dislikes a document would use a query does not depend on the particular document [6], formally p ( Q | D,R =  X  p ( Q | R =  X  ).

This assumption enables ignoring the negative query gen-eration in the derivation of the basic query likelihood re-trieval function, leading to the following basic query likeli-hood method: O ( R = | Q, D )  X  p ( Q | D,R = )= p ( Q |  X 
In reality, however, this assumption does not hold because a user who dislikes a document would more likely avoid us-ing words in the document when posing a query, suggesting a theoretical gap between the standard query likelihood and the probability ranking principle. This shows that the stan-dard query likelihood function is a potentially non-optimal retrieval function.

In the following section, we attempt to improve the query likelihood function by estimating, rather than ignoring the component of negative query generation p ( Q | D,R =  X  ).
What would a user like if he/she does not like D ?We assume that there exists a  X  X omplement document X   X  D ,and that if a user does not like D , the user would like  X  D is, when generating query Q , if a user does not like D ,the user would randomly pick a word from  X  D . Formally, The challenge now lies in how to estimate a language model  X  , which we refer to as the  X  X egative document language model X  of D . Note that the negative document language model in our paper is still a  X  X ocument X  model, which is completely different from the relevance model p ( w | R = )[7] and the irrelevance model p ( w | R =  X  ) [17] that capture the probability of observing a word w relevant and non-relevant to a particular information need respectively.

Ideally we should use many actual queries by users who do not want to retrieve document D to estimate the probability p ( w |  X   X  D ). For example, we may assume that if a user sees a document in search results but does not click on it, he/she dislikes the document. Under this assumption, we can use all the queries from the users who  X  X islike X  the document to approximate  X  D . However, in practice, only very few search results will be shown to users and certainly there are always queries that we would not even have seen. Yet, as a general retrieval model, the proposed method must have some way to estimate  X   X  D for any document with respect to any query.
One straightforward way is using the background language most all other documents in t he collection are complemen-ative query generation component will not affect the rank-ing of documents, because the probability of negative query generation will be constant for all documents: it justifies the document independent negative query generation com-ponent in the standard query likelihood method. However, the content of document D is ignored in this estimate.
We are interested in estimating p ( w |  X   X  D )inageneralway based on the content of document D so as to make it pos-sible to incorporate a document dependent negative query generation component when retrieving any document. Our idea is based on the intuition that if a user wants to avoid re-trieving document D , he/she would more likely avoid using words in the document when posing a query. That is, the user would like a document  X  D with little information over-lap with D . Therefore,  X  D should contain a set of words that do not exist in D , because given only document D available, the sole constraint of  X  D is that, if a word w occurs in c ( w, D ) &gt; 0, this word should not occur in  X  D . where  X ? X  means unknown.

How to determine the count of a word in  X  D if it does not occur in D ? As the probability distribution of such a word is unknown, according to the principle of maximum entropy [3], each such a word occurring in  X  D should have the same frequency  X  , which maximizes the information entropy under the only prior data D .Thatis,  X  D contains a set of words that are complementary to D in the universe word space (i.e., the whole word vocabulary V ). Formally,
A ccording to the maximum likelihood estimator, we have the following estimation of the negative document language model  X   X  D for the multinomial model: where |  X  D | is the  X  X ength X  of  X  D , which can be computed by aggregating frequencies of all words occurring in  X  D .Because the number of unique words in D is usually much smaller than the number of unique words in the whole document collection C (i.e., | V | ), the number of unique words in approximately the same as | V | based on Formula 11. Thus
Due to the existence of zero probabilities, p ml ( w |  X   X  D smoothing. Following the estimation of regular document language models, we also choose the Dirichlet prior smooth-ing method due to its effectiveness [19]. Formally, where  X  is the Dirichlet prior. Since the influence of  X  can be absorbed into variable  X  obviously, we thus set it simply to the same Dirichlet prior value as used for smoothing the regular document language model (Equation 3).

Now we can bring back the negative query generation com-ponent to the query generation process:
O ( R = | Q, D ) rank =log p ( Q | D,R = )
The negative query loglikelihood log p ( Q |  X   X  D )canbefur-ther written as The corresponding derivation process has been shown in For-mula 17.

Plugging Equations 4 and 16 into Equation 15, we finally obtain a more complete query likelihood retrieval function that also contains the negative query generation component:
Comparing Formula 18 with the standard query likelihood in Formula 4, we can see that our new retrieval function es-reward the matching of a query term, and it rewards more the matching of a more discriminative query term, which not only intuitively makes sense, but also provides a natu-ral way to incorporate IDF weighting to query likelihood, which has so far only been possible through a second-stage smoothing step [19]. Note that when we set  X  = 0, the pro-posed retrieval function degenerates to the standard query likelihood function.

Note that this new component we introduced is a term-dependent constant , which means that the proposed new re-trieval function only incurs O ( | Q | ) additional computation cost as compared to the standard query likelihood function, which can be certainly ignored. improvement is significant at the 0 . 05 / 0 . 02 / 0 . 01 / 0
More interestingly, the developed query likelihood with negative query generation (Formula 18) leads to the same ranking formula as derived by lower-bounding term frequency normalization in the query likelihood method [8]. However, the formula derived in [8] is based on a heuristic approach, which is inconsistent with the theoretical framework of the query likelihood method. Our method provides a proba-bilistic approach for appropriat ely lower-bounding term fre-quency normalization in the query likelihood method.
We use four TREC collections: WT2G, WT10G, Ter-abyte, and Robust04, which represent different sizes and genre of text collections. We adopt the same preprocess-ing and parameter tuning methods as in our recent study [8]. Our goal is to see if the proposed negative query gener-ation component can work well for improving the standard query likelihood method.

We first compare the effectiveness of the standard query likelihood (labeled as QL ) and the proposed query likelihood with negative query generation (labeled as XQL ). QL has one free parameter  X  , and XQL has two free parameters  X  and  X  . We use cross validation to train both  X  and  X  for XQL and  X  for QL.

We report the comparison results in Table 1. The re-sults demonstrate that XQL outperforms QL consistently in terms of MAP and also achieves better P@10 and recall (#Rel) scores than QL in most cases. The MAP improve-ments of XQL over QL are significant in general. These results show that bringing back the negative query gener-ation component is able to improve retrieval performance, and that the proposed approach works effectively.
Regarding different query types, we observe that XQL usually improves more on verbose queries than on short queries. For example, the MAP improvements on WT2G, WT10G, and Robust04 collections are as high as 5% for verbose queries.

We introduce a parameter  X  to control the negative query generation component. We plot MAP improvements of XQL over QL against different  X  values in Figure 1. It demon-strates that, for verbose queries, when  X  issettoavalue around 0 . 05, XQL works very well across different collec-tions. Therefore,  X  can be safely  X  X liminated X  from XQL for verbose queries by setting it to a default value 0 . 05. Al-though  X  tends to be collection-dependent for short queries, setting it conservatively to a small value, e.g., 0 . 02, can often lead to consistent improvement on all collections.
As XQL and QL share one parameter  X  ,theDirichlet prior, we are also interested in understanding how this pa-rameter affects the retrieval performance of XQL and QL. So we draw the sensitivity curves of QL and XQL to  X  in Figure 2. It shows that XQL is consistently more effective than QL when we vary the value of  X  . Moreover, the curve trends of QL and XQL are very similar to each other. In particular, QL and XQL even often share the same optimal setting of  X  . These are interesting observations, which sug-gest that  X  and  X  do not interact with each other seriously; as a result, we can tune two parameters separately.
In this paper, we show that we can improve the standard query likelihood function by bringing back the component of negative query generation (i.e., the probability that a user who dislikes a document would use a query). Our work not only bridges the theoretical gap between the standard query likelihood method and the probability ranking principle, but also improves retrieval effectiveness over the standard query likelihood with no additional computational cost for various types of queries across different collections. The proposed retrieval function can potentially replace the standard query likelihood retrieval method in all retrieval applications.
This material is based upon work supported by the Na-tional Science F oundation under gra nt CNS-1027965, by U.S. Air Force Office of Scientific Research MURI award FA9550-08-1-0265, and by a Sloan Research Fellowship.
