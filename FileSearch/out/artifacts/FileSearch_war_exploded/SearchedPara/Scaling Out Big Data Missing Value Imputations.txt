 Solving the missing-value (MV) problem with small estima-tion errors in big data environments is a notoriously resource-demanding task. As datasets and their user community con-tinuously grow, the problem can only be exacerbated. As-sume that it is possible to have a single machine ( X  X odzilla X ), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which ac-cesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to ac-cess only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla X  X  imputa-tion estimation errors? In this paper we derive answers to these fundamentals questions and develop principled meth-ods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pil-lars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive ex-perimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.
 Categories and Subject Descriptors: H. Information Systems; I.5.3 Clustering.
 Keywords: Big data; Missing value; Clustering.
Data quality is a major concern in big data processing and knowledge management systems. One relevant problem in data quality is the presence of missing values (MVs). The MV problem should be carefully addressed, otherwise bias might be introduced into the induced knowledge. Common solutions to the MV problem either fill-in the MVs ( impu-tation ) or ignore / exclude them. Imputation entails a MV substitution algorithm (MVA) that replaces MVs in a dataset with some plausible values. Imputed data can be treated as reliable as the observed data, but they are as good estima-tions as the assumptions used to create them.

On the one hand, most computational intelligence and ma-chine learning (ML) techniques (such as neural networks and support vector machines) fail if one or more inputs contains MVs and thus cannot be used for decision-making purposes [1]. Furthermore, the choice of different MVAs affects the performance of ML techniques that are subsequently used with imputed data [2]. On the other hand, the MV problem abounds: it can be found, for instance, in results from medi-cal experimentation and chemical analysis, in datasets from domains such as meteorology and microarray gene monitor-ing technology [4], and in survey databases [5]. MVs can occur e.g., due to wireless sensor faults, not reacting experi-ments, or participants skipping survey questions. Industrial and research databases include MVs [6], e.g., maintenance databases have up to 50% of their entries missing [7]. Pa-tient records in medical databases lack some values; inter-estingly, a database of patients with cystic fibrosis missing more than 60% of its entries was analyzed in [8]. Moreover, gene expression microarray data sets contain MVs, making the need for robust MVAs apparent, since algorithms for gene expression analysis require complete gene array data [9].

Motivations. Given the significance of MVAs, three notes are in order: Firstly, MVAs which can ensure low estimation errors are computationally expensive and typi-cally their performance is largely dependent on dataset sizes. Secondly, nowadays, datasets can be massive. Even worse, existing datasets grow significantly with time; it is not sur-prising that most MVAs in the literature are typically tested over small-to medium sized datasets. Lastly, as if the scala-bility limitations imposed by dataset sizes were not enough, in many applications the user community (e.g., in shared sci-entific datasets in data centers accessed by scientists from all over the world) can be very large and thus the MV im-putation input arrival rates can become high as well. These facts pose a scalability nightmare.

The scalability gospel (as established by the seminal work from Google researchers producing the Map-Reduce (MR) [10] data-access paradigm and systems such as the Google File System [11]) rests on the notion of scaling out : that is, (i) employ a large number of commodity (off-the-shelf and thus inexpensive) machines, each storing a much smaller par-tition of the original dataset, and (ii) access them in parallel.
However, MR is not a panacea, for two reasons. First, not all complex problems are  X  X mbarrassingly parallelizable X  and amenable to MR techniques. In particular, there ex-ist sophisticated MVAs ensuring small errors, which are not MR-able [12]. Second, in the context of MVAs, even if they were  X  X mbarrassingly parallelizable X , not all partitions may be relevant. It may very well be the case that a number of the machines hold data that cannot help (or even hurt) in the MV imputation process. And, obviously, engaging only a fraction of all machines will introduce large benefits: First with respect to performance. MV imputation will be shorter, as these times typically depend on the worst per-forming machine and with increasing machine numbers the probability of a mall-performing machine increases. Fur-ther, overall MV imputation throughput will be higher, as each imputation will be taxing fewer overall system resources (processors, communication bandwidth and disks). Second, with respect to MV estimation errors. In fact, as we shall formally show later, engaging all machines and their dataset partitions may actually introduce large additional MV esti-mation errors.

Goals. In this work, we will consider a stream of MV inputs (or inputs), i.e., multi-dimensional vectors with some MVs in certain dimensions, arriving at a data system. Typi-cally, the system is presented with a batch of data items with MVs, which must be added to the system after MVs have been estimated. There are two system alternatives. The first is based on employing a single machine which stores the whole of the dataset. We affectionately call this ma-chine Godzilla . Godzilla can employ any MVA to perform the MV imputations. As motivated earlier, this approach suffers from several disadvantages. The second alternative employs a (potentially large) number of machines, referred to as cohorts , each storing a partition of Godzilla X  X  dataset. Imputation execution engages cohorts in parallel, whereby each cohort runs an MVA on a much smaller local dataset. This can introduce dramatic performance improvements. As an illustration, assuming, say, 50 cohorts and an MVA op-erating on a dataset of size n with asymptotic complexity O ( n 2 )(or O ( n 3 ); [3], [4]) a scale-out execution is expected to speedup input processing by a factor of 50 2 =2 , 500 (or 50 3 = 125 , 000) as such MVA runs in parallel on a dataset of size 1 50 n . Moreover, this alternative affords the possibility of accessing only a subset of all cohorts for a given input. We will not make any restricting assumptions as to specific characteristics of this system or the method for partitioning the dataset.

The formidable challenges here entail: (i) for data accu-racy (estimation-error) reasons, we should ensure that the subset of cohorts contacted achieve similar, if not smaller es-timation errors, compared to the errors that Godzilla would yield; (ii) swiftly determine the subset of cohorts to engage per imputation, achieving large efficiency/scalability gains.
Contributions. To our knowledge, this is the first study on scaling out MV imputations. We shall derive fundamen-tal knowledge regarding meeting the estimation error and performance goals outlined above. Armed with this knowl-edge, we shall propose a novel, principally derived frame-work, Pythia , which offers large performance speed-ups and better, or comparable, errors to that of Godzilla given a stream of MV inputs. Pythia X  X  salient contribution is that, given an input (of imputation requests), Pythia is able to predict and engage the appropriate subset of cohorts to em-ploy per imputation. Pythia X  X  prediction process relies on (i) the concept of per cohort-dataset signature , which derives from the (local) dataset of a cohort and (ii) novel similarity notions and algorithms which, based on each imputation re-quest and cohort signatures, can determine the best subset of cohorts to engage. Finally, we will provide comprehen-sive experimental evidence substantiating and showcasing Pythia X  X  accuracy and performance, using a variety of met-rics and real and synthetic datasets.

The paper is structured as follows: Section 2 reports on background and discusses related work, while Section 3 in-troduces the problem fundamentals of scaling out the MV imputations. In Section 4 and Section 5 we introduce the Pythia framework and propose two schemes. In Section 6 we evaluate our framework and Section 7 concludes the paper.
Assume a data set X of d -dimensional data points with some MVs on a certain dimension X i . Data on X i are said to be missing completely at random (MCAR) if the proba-bility of MV on X i , q , is unrelated to the value of X i or to the values of any other dimensions. If data are MCAR, a reduced sample of X will be a random sub-sample of X ; MCAR assumes that the distributions of MVs and complete data are the same. Data on X i are said to be missing at random (MAR) if q depends on the observed data, but does not depend on the MV itself. In MAR, the dimension as-sociated with MVs has a relation to other dimensions, i.e., MVs can be estimated by using the complete data of other dimensions. It is impossible to test whether the MAR con-dition is satisfied for X because, since the (actual) values of missing data are not known, we cannot compare the values of those with and without missing data to see if they differ systematically on that X i . Data on X i are missing not at random (MNAR) if q depends on the MVs and, thus, missing data cannot be estimated by using the existing dimensions; MNAR is rarely applicable in practice.
Missing data hinder the application of many statistical analysis and ML techniques available in off-the-shelf soft-ware. To analyze X with MVs, certain MVAs have been proposed [13]. The simplest method is discarding the data points with MVs or removing the corresponding dimensions. Both removals of such points and dimensions result in de-creasing the information content of X and are applicable only when (i) X contains a small amount of MVs, and (ii) the analysis of the remaining complete points will not be biased by the removal. There are many MVAs varying from na  X   X ve methods, e.g., mean imputation, to some more robust methods based on relationships among dimensions. In the dummy variable adjustment , MVs are set to some arbitrary value. The mean / mode imputation replaces MVs of a di-mension by the sample mean / mode of all observed values of that dimension. In hot deck MVA [14], a MV is filled in with a value from an estimated distribution w.r.t. X .In the K-nearest neighbors MVA [15], the MVs of a point are imputed considering the K most similar (observed) points from X . The regression-and likelihood-based MVAs are in-troduced in [16]. In regression-based imputation [17], the MVs of a point are estimated by regression of the dimen-sions corresponding to MVs on the dimensions associated to the observed values of that point. This approach argues that dimensions have relationships among themselves; if no rela-tionships exist among dimensions in X and the dimensions corresponding to MVs, such MVA will not be precise for im-putation. Likelihood-based imputation [16] is based on pa-rameter estimation in the presence of MVs, i.e., X  X  X  param-eters are estimated by maximum likelihood or maximum a posteriori procedures relying on variants of the Expectation-Maximization algorithm. The multiple imputation MVA [18], instead of filling in a single value for each MV, re-places each MV with a set of plausible values that represent the uncertainty about the actual value to impute. These multiply-imputed datasets are then analyzed by using stan-dard procedures for complete data and combining the re-sults from these analyses. In case of MVs in time series, the models in [19] (using dynamic Bayesian networks), [20] (us-ing matrix completion), and [21] (using Gaussian mixtures clustering) recover MVs in motion capture sequences, vital signs, and micro-array gene expression streams, respectively. Furthermore, ML-based MVAs, e.g., decision-trees and rule-based methods, generate a model from X that contain MVs, which is used to perform classification that imputes the MVs (see [2] and the references therein). Finally, the imputation framework [6] applies most existing MVAs (base methods) to improve their accuracy of imputation while preserving the asymptotic computational complexity of the base meth-ods. The interested reader could also refer to [6], [9] and [22] (and the references therein) for a comprehensive survey of the most recent MVAs. Definition 1. Given a set X of d -dimensional data points, X = { x 1 ,..., x |X| } , for each x i we define w i =[ w ik w ik = 0 whenever x i  X  X  k -th dimensional value is missing; otherwise w ik = 1. We express x i as ( z i , z m i ), where z denotes observed values and z m i  X  R ( d  X  n ) denotes MVs, with
Definition 2. Given a finite integer m&gt; 0, X i is a parti-tion of X such that X X  X  X  m i =1 X i and X i = X j ,i = j . S notes the machine ( cohort ), which maintains X i , performs a MVA over X i , and is indexed by i , i =1 ,...,m . S = { S is the set of all cohorts. The (imaginary) Godzilla S 0 assem-bles all X i and is capable of performing a MVA over X .
Definition 3. A single MV input on MVA is i =( x , w ) and output is  X  x expressed by ( z ,  X  z m ).  X  x  X  R d is referred to as estimate containing  X  z m  X  R ( d  X  n ) of imputed MVs by MVA. If x a is the actual vector, the absolute reconstruction error is e =  X  x  X  x a ; x denotes the Euclidean norm. As our contributions are independent of any particular MVA, we overview two popular and representative MVAs as would be used in our framework. To exemplify our frame-work and methods, we employ the weighted K-nearest neigh-bors (KNN) [15] and sequential multivariate regression im-putation method (REG) [17]. These MVAs are widely used for multivariate imputation in many scientific areas.
KNN is widely used [22] since it has many attractive char-acteristics: it is a non-parametric method, which does not require the creation of a predictive model for each dimension with MV and takes into account the correlation structure of the data. KNN is based on the assumption that points close in distance are potentially similar. For given input ( x i with x i =( z i , z m i ), KNN calculates a weighted Euclidean distance D ij between x i and x j  X  X  such that The MV of the k -th dimension of x i (i.e., z m ik of z m estimated by the weighted average of non-MVs of the K most similar x j to x i , i.e.,  X  z m ik = K j =1 D is typically used with K=10,15,20; theses values have been favored in previous studies [22], [23]. (In our experiments we will use K=10).
REG estimates the MVs by fitting a sequence of regression models and drawing values from the corresponding predic-tive distributions. Let Y 1 ,...,Y d  X  n denote d  X  n (dependent) variables with MVs, sorted in ascending order to the num-berofMVsand X =[ X 1 ,...,X n ] denote n (predictor) variables with no MVs. REG consists of c rounds. In round 1, step 1, we regress the variable with the fewest number of MVs, Y 1 ,on X imputing the MVs under the appropri-ate regression model; e.g., if Y 1 is continuous, categorical, or binary then ordinary least squares, generalized logit, or logistic linear regression is applied, respectively. In step 2, after estimating the regression coefficients  X  of the model from step 1, we use the estimated  X   X  to impute the MVs of Y . In step 3, we update X by appending Y 1 and continue to variable, say Y 2 , with the next fewest MVs and repeat the process using updated X as predictors until all the variables have been imputed. That is, Y 1 is regressed on U = X ; Y is regressed on U =( X ,Y 1 ), where Y 1 has imputed MVs; Y 3 is regressed on U =( X ,Y 1 ,Y 2 ), where Y 1 and Y 2 have imputed MVs, and so on. Steps 1 to 3 are then repeated in rounds 2 through c , modifying the predictors set to in-clude all Y s except the one used as the dependent variable. Hence, regress Y 1 on X and Y 2 ,...,Y d  X  n ; regress Y 2 and Y 1 ,Y 3 ,...,Y d  X  n , and so on. Repeated cycles continue for c rounds, or until stable imputed MVs occur.
We consider a discrete time domain t  X  T and at instance t =1 , 2 ,... , we are given input i [ t ]. Assume that Godzilla S exists and is capable of invoking a certain MVA for i [ t ]. At first thought, one could claim that, since Godzilla has global knowledge (i.e., the union of all X i ), the corresponding esti-mate  X  x G [ t ] would be better (in terms of reconstruction error e does not always hold true. It depends on the probability density function (pdf) of {X i } and the (possibly unknown) pdf of z [ t ], z m [ t ], and w [ t ]. Theorem 1. Let e G and e i denote the estimate error of Godzilla S 0 and cohort S i . It is not always true that e e ,  X  S i  X  X  .

Proof. To prove Theorem 1, suppose its converse were true. Then it suffices to show counterexamples. Consider the mean imputation (MEAN) and the KNN. Consider that points in X i are normally distributed, N (  X  i , X  2 i ), with mean  X  and variance  X  2 i and |  X  i  X   X  j | &gt;&gt; 0 ,i = j . Evidently, S data set X =  X  m i =1 X i follows the mixture N (  X ,  X  2 )with  X  = 1. If we were told that all (both observed z and unobserved z m ) inputs followed N (  X  should have engaged only S j thus yielding e j &lt;e G in case of MEAN, and e j = e G in case of KNN (for K&lt;&lt; |X j avoiding engaging all S i .
 Furthermore, consider that all X i follow exactly the same distribution; consequently, S 0  X  X  X follows the same distri-bution. Then, regardless of any knowledge on the pdfs of inputs, we could randomly select one cohort from S ,thus, yielding e i = e G ,  X  S i  X  X  , and avoiding engaging all cohorts.
Example 1: Consider m = 3 cohorts S 1 ,S 2 ,S 3 with 2D datasets X i , corresponding joint pdfs f 1 ,f 2 ,f Godzilla S 0 with X =  X  3 i =1 X i whose joint pdf f G is shown in Fig. 1(a). Assume REG, KNN, and MEAN MVAs. We are given a stream of 10 4 inputs i [1] ,..., i [ t ] and assume that we know the pdf of each i [ t ], i.e., its observed and MVs are known to be produced either by f 1 , f 2 ,or f 3 . For each i [ t ], we invoke a MVA (a) on S 0 and obtain e G [ t ],(b)onlyon the cohort S j with the same pdf f j as that of the input and obtain e j [ t ], and (c) on all cohorts, aggregate their estimates by taking their average and obtain e all [ t ]. Fig. 1(b) shows the root-mean-square error (RMSE) e G ,e j ,and e all for all MVAs. We observe that the knowledge of the pdf of each input results to a significantly lower error e j , because we engage only the cohort S j corresponding to the same pdf as that of the input. Godzilla produces a relatively high e G all MVAs) with high computational cost due to processing high volumes of data. Moreover, the parallel execution of MVAs over all cohorts for each input produces a high e all Unfortunately, the pdf of an incoming input is not known, especially, the pdf of the MVs is unknown since they are never observed. Moreover, we can achieve high parallelism with concurrently engaging all cohorts but, we also obtain high error, because there might be a subset of cohorts that adversely contribute to the aggregated estimate, e.g., due to the fact that the corresponding pdfs of their data sets are different from those of the inputs (see Example 2). Note, however, that in the case of MEAN, e G = e all .
Here we show: (i) that computing the best cohorts subset is computationally hard, (ii) that even if an efficient heuristic can be found, it would not be desirable for our purpose since it would require communication with all cohorts, hence, an-other approach is needed, like our signature-based prediction approach and (iii) that as exemplified using our reference popular MVAs, it is highly beneficial to engage only a good cohort subset per imputation. The above showcases thus the traits and benefits of our approach.

In our framework, we utilize a node called Pythia that at-tempts to predict the best cohorts subset per input. Pythia receives input i [ t ]=( x [ t ] , w [ t ]) with 0 &lt;n [ t ]= d . In the remainder, the time index t is omitted for the sake of readability. Of course, Pythia can, trivially, en-gage all cohorts in parallel. Each cohort S i locally pro-duces an estimate  X  x i (through MVA invocation) and pro-vides it to Pythia. Then, Pythia takes their average value  X  x = 1 m m i =1  X  x i . Let us denote such method as the All Co-horts Method , notated by ACM, so to differentiate it from Pythia X  X  sophisticated methods. ACM implies that all co-horts are equal candidates and available for providing an estimate. It would have been preferable if Pythia could en-gage a subset S  X  X  of cohorts whose average estimate ingly, if Pythia could engage the minimum subset of cohorts whose average estimate is close to  X  x for each input.
Determining the minimum cohorts subset whose aggregate estimate is close to  X  x calls to mind the Subset Sum Problem (SSP) [24]: Consider a pair ( I ,s ), where I is a set of m&gt; 0 positive integers and s is a positive integer. SSP asks for a subset of I whose sum is closest to, but not greater than, s . SSP is NP-hard [24]. Consider now the following problem, referred to as Minimum Subset Average Problem (MSAP).
Problem 1. (MSAP) Given ( I ,s ), find the minimum sub-set I with average s subject to s = s or s = s (C1). Theorem 2. MSAP is NP-hard.

Proof. If there is a polynomial-time algorithm for MSAP, then a polynomial-time algorithm can be developed for SSP. Assume there exists a polynomial algorithm A ( I ,s ) that solves MSAP, i.e., A ( I ,s ) finds in polynomial time the mini-mum subset I subject to constraint C1 in Problem 1. Then, A ( I ,s ) can be used to solve SSP with ( I ,ms ), m = |I| . In general, any solution B ( I ,s ) of SSP with ( I ,s ) can be formulated as Algorithm 1. If the complexity of A ( I ,s ) is a polynomial Q ( m ) then the complexity of B ( I O ( m Q ( m )). But, this implies that there is a polynomial-time algorithm for SSP. Hence, no polynomial-time algo-rithm exists for MSAP.
 ALGORITHM 1: B ( I ,s ) Input : I ,s
Output : I end
Theorem 3. Given input i , the problem of finding the minimum subset S  X  X  of cohorts, whose average estimate  X  x gives the same reconstruction error as  X  x is NP-hard.
Proof. Let e =  X  x  X  x a and e =  X  x  X  x a . In order to show that the problem of finding the minimum subset S with e = e is NP-hard, it suffices to show that finding the minimum subset S  X  X  of cohorts such that  X  x =  X  x subject to C1 is NP-hard. Consider the set I 0 = {  X  x MSAP, which deals with integers is NP-hard from Theorem 2, MSAP with ( I 0 ,  X  x )and( I 1 ,  X  x ) is also NP-hard.

SSP and MSAP are NP-hard, however, one is often satis-fied with an approximate, sub-optimal solution, i.e., in poly-nomial time; see [25] for SSP. Nevertheless, even if Pythia (c) RMSE using KNN were able to use such heuristic to find the minimum set S for given input (let m be small) then this would still not be preferable given our goals. That is because, in order to obtain S for a given input, Pythia would firstly have to en-gage all cohorts and consequently, based on their estimates, produce S . What we want is for Pythia to guess/predict the most appropriate S , which gives the same or, hopefully, smaller reconstruction error than that of S without having to access all cohorts ! For instance, this guess can be in-terpreted as follows: cohort S i  X  X  might consider z (of input i ) as an observation which is deemed unlikely w.r.t. X . Based on the fact that a MVA highly depends on X i , S i will probably provide a bad estimate for i (w.r.t. e i ). Were Pythia capable of predicting the unsuitability of S i provid-ing a good estimate before engaging S i then Pythia could have excluded S i from S .

The task of predicting S per input involves the following issues: (a) the joint pdf of the MVs is evidently unknown since the actual values of z m are not observed; (b) it is not feasible to identify the joint pdf that generates z , since we have only one sample from this at a time; (c) it is not suitable to assume that z is produced by a certain pdf at time t , which remains also the same for subsequent z [  X  ] , X  &gt; t . This is getting more difficult when dealing with non-stationary distributions of z and w , which is not a rare situation.
Example 2: Consider m = 30 cohorts. We are given a stream of 10 4 inputs where the joint pdf of each input is un-known. For each input, we invoke a MVA (KNN and REG) on Godzilla and on all cohorts in parallel, and aggregate their estimates (ACM). For each input, we obtain the order statistics Q 1 = min i { e i } ,...,Q 30 = max i { e i } of the corre-sponding errors of all cohorts and plot their average values in Fig. 1(c-d); the e G is shown for comparison. We can ob-serve that more than 40% of cohorts provide lower error to that of Godzilla for KNN and REG. This indicates that it is of high importance to predict such subset of cohorts for each input while knowing neither the pdfs of the cohorts X  sets nor the pdf of each input. Note that ACM in this case produces a higher average error than even Godzilla. Furthermore, we observe that for each input there is an ideal cohort that gives the minimum error; note that  X  Q 1 is 93% / 95% smaller than e
G for KNN / REG. An ideal Pythia has to predict S hope-fully including the ideal cohort and/or those S i with e i for each input. We now formulate our problems.

Problem 2. Determine what information each S i  X  X  a-priori must convey to Pythia in order to predict whether S is suitable for providing a (local) good estimate  X  x i given an input, i.e., whether S i should be a member of S .This information is referred to as the signature P i of X i .
Problem 3. Determine how signatures { P i } m i =1 are updated for each input.
Pythia aims to solve the above problems. Predicting S for each input, based on per-cohort signatures, avoids the fundamental problems of NP-hardness of exact solutions and of the need to on-the-fly engage all cohorts for approximate heuristic solutions.

Each cohort S i constructs a signature P i from X i . P i reflects the current structure of data points in X i . The idea behind a signature is that S i is engaged for a given i once x can be  X  X xplained X  through P i . S i provides its (locally) created P i to Pythia, which stores all signatures forming P = { P i } m i =1 . Figure 2(a) pictorially depicts the framework X  X  operation. The operation of the framework is as follows: Given i , 1. Pythia predicts S  X  X  w.r.t. P 2. Pythia then engages only the cohorts from S sending 3. Each S i  X  X  4. Pythia constructs the aggregate estimate  X  x that is sent 5. Each S i  X  X  can exploit  X  x for updating its P i . 6. Pythia uses  X  x for updating P .
In this work, P i refers to a clustering structure over X viding a set of representative points (clusters) C i . Each co-hort S i  X  X  employs the Adaptive Resonance Theory (ART) [26], an unsupervised learning model from the competitive learning paradigm, in order to locally construct P i over In ART, whose algorithm is shown as Algorithm 2, each x k  X  X  i is processed by finding the nearest cluster c to x k , i.e., c  X  = arg min c  X  X  i c  X  x k , where C i is the set of clusters. Then, it is allowed x k to modify/update c  X  if c  X  is sufficiently close to x k ( c  X  is said to  X  X esonate X  with x ) i.e., if c  X   X  x k  X   X  i for some vigilance  X  i &gt; 0. In this case, c  X  is updated through the rule c  X   X  c  X  +  X  i ( x where  X  i  X  (0 , 1) is a learning rate, which gradually de-creases. Otherwise, i.e., c  X   X  x k &gt; X  i , a new cluster c is formed handling x k such that c = x k and C i  X  X  i  X  X  c } Definition 4. Cohort S i  X  X  signature P i over X i is the triple ALGORITHM 2: ART algorithm at cohort S i Input : X i , X  i , X  i Output : C i for 1 &lt;k  X |X i | do end
Definition 5. We say that x is a member of P i , notated x  X  P i , iff min c  X  X  The statement  X  x  X  P i  X  denotes that there is at least one c  X  X  i such that x is placed close to c with distance less than  X  i , for instance, the closest cluster c  X  to x . The more clusters c  X  X  i satisfy the criterion c  X  x  X   X  i , the more appropriate C i is for x . In this sense, if x  X  P i then x can be represented by at least one cluster from X i . Based on this intuition, if x  X  P i , cohort S i provides a rather good estimate for some missing parts of x compared to a cohort S j associated with a P j for which it holds true that x  X  P The latter case indicates that no cluster from C j can be a representative point for x .

Since  X  i represents a threshold of similarity between points and clusters, thus, guiding ART in determining when a new cluster should be formed, it should depend on X i . In order to give a physical meaning to  X  i , it is expressed through a set of percentages  X  k  X  (0 , 1) of the ranges between the lowest x k and highest x X , k =1 ,...,d . Let r i =[( x max 1  X  x min 1 ) ,..., ( x and the diagonal d  X  d matrix A with A [ k, k ]=  X  k . Then  X  i = Ar i . High  X  k values result to a low number of clusters and vice versa. Each S i determines a  X  i over X creates P i through Algorithm 2, and sends P i to Pythia.
Note: when dealing with mixed-type data points, e.g., consisting of categorical, binary, and continuous attributes, we can adopt appropriate distance metrics [27] for the dis-tance between x k and x l instead of using the Euclidean distance x k  X  x l ; this does not spoil the generality of signature creation. Figure 2: (a) Inputs with MVs, Pythia and engaged cohorts; (b) Engaged cohorts against m (COE).
Up to this point, we have shown how to use signatures as a guiding light to select appropriate cohorts for MV impu-tations. Now, our concern is twofold: MV imputations must be (i) low cost and (ii) high accuracy. Low cost (once signa-ture processing is performed) refers to the communication cost between Pythia and cohorts and to the cost of run-ning MVAs at cohorts. High accuracy refers to low RMSE. Therefore, we present algorithms with these in mind.
For simplicity we present the top-1 (Best) Cohort (BC) scheme, i.e., K = 1. Pythia is not involved in producing the (final) estimate  X  x , instead, only one cohort (best cohort) is engaged for doing this locally. Pythia communicates only with the best cohort, which runs the MVA, thus, this opti-mizes our cost metric. Given i , Pythia determines the best cohort S  X   X  X  with P  X  = C  X  , X   X  , X   X  such that ( A1 ) c c  X   X  X   X  is the closest cluster to z among all clusters from all signatures, and ( A2 ) z  X  P  X  . Note that z  X  R n with 0 &lt;n = d k =1 w k &lt;d provided that x contains d  X  n MVs. In order to evaluate  X  z  X  P  X   X  Pythia calculates  X   X  ( n associated with the n dimensions of r  X  corresponding to the n non-MVs. Then, it checks if c  X   X  z  X   X   X  ( n ) dealing only with the n dimensions of c  X  . Pythia engages only S which produces the final  X  x . If there is no cohort that satis-fies criteria A1 and A2, BC engages the cohort that satisfies only criterion A1. If K &gt; 1 one can repeat the above criteria for the top K cohorts ranked with the distance between the corresponding c  X  j and z ,1  X  j  X K &lt;m . Inthiscasethe final  X  x is produced by aggregating all  X  x j .
Cohorts Outlier Elimination (COE) trades off additional cost for improving our other metric, accuracy. Given i , Pythia checks whether z  X  P i . This is achieved once Pythia, for each cohort S i , calculates  X  ( n ) i  X   X  i associated with the n dimensions of r i corresponding to the n non-MVs. If c with c  X  i = arg min c  X  X  i c  X  z then S  X  X   X  X  S i } .Once S is determined with = |S |  X  |S| = m , Pythia engages only cohorts from S and obtains their corresponding esti-mates  X  x i ,i =1 ,..., . The aggregate estimate  X  x determined by Pythia is where b i is the weight for estimate  X  x i normalized by the sum of inverse distance from the closest cluster c  X  i to z from cohort S i  X  X  . The set S  X  X  contains cohorts S i  X  X  whose estimates are not considered outliers in E = {  X  x 1 ,...,  X  x } . This is achieved by computing the statistic for each  X  x i  X  E and then considering  X  x i as outlier if u
E exceeds a certain cutoff, usually 2.5 or 3.0 [28]. The median ( E ) and mad ( E ) is the sample median and median absolute deviation about the median of E , respectively. Pythia provides  X  x to each S i  X  X  for updating their signatures; see Section 5.1. If S =  X  , Pythia engages all cohorts; if S = Pythia engages all cohorts from S .
In COE, given i Pythia evaluates  X  z  X  P i  X ,  X  P i  X  X  , i.e., it performs one nearest neighbor (1NN) search for each P i over C i . We adopt a d -dimensional tree structure [31] for each P i over the clusters of C i . Let  X  = 1 m m i =1 |C average number of clusters in signature P i . The correspond-ing time complexity per input i in COE is O ( md log(  X  )). In BC, we also adopt a d -dimensional tree structure over all clusters from all signatures in P . Given i , Pythia performs a 1NN search with O ( d log( m X  )) time since it searches over all clusters from all signatures  X  m i =1 C i . COE and BC require O ( md X  ) space. Pythia requires O ( )and O (1) communica-tion with cohorts from S and the best cohort in COE and BC schemes, respectively. Once Pythia has produced  X  x given an input, it updates P .Only P i  X  X  , which correspond to cohorts S i  X  X  , need to be updated. The update of P i is based on the rule c  X  c  X  only the dimensions of c  X  i are modified, which correspond to the n dimensions of the non-MVs of x . This denotes that no new clusters at P i are formed after the update w.r.t.  X  x , since z  X  P i . The exact update can be locally reflected by S i  X  X  to its signature in order to be secured against a Pythia break-down situation. The magnitude of change in P i w.r.t.  X  x is  X  i =  X  i z  X  c
Let the sum involving the y moments of the reciprocals of binomial coefficients F ( y ) x = x k =0 k y x k  X  1 for non-negative integers x , y . From [29] we obtain that F (0) x = x +1 2 x +1 Theorem 4. The expected magnitude of change in P i , E [  X  i | S i  X  X  ] , in COE is bounded above by  X  max i =  X 
Proof. Consider input i with 1  X  n  X  d  X  1 non-MVs and S i  X  X  . The probability of choosing a subset of n out of d dimensions corresponding to non-MVs is d n  X  1 . The ex-pected magnitude of change of P i is E [  X  i ]= d  X  1 n =1 z  X  c  X  2)  X  i  X  i . The asymptotic expansion of F (0) d  X  2+ 2 Theorem 5. The expected magnitude of change in P , E [  X  ] , in COE is bounded above by  X  max =  X  max  X  max ( F very large m and d , where  X  max = max {  X  i } m i =1 ,  X  {  X 
Proof. The probability that a subset S of cohorts is determined by Pythia is m  X  1 . Hence (from Theorem 4), Since lim m  X  X  X  F (1) m m  X  1 (Theorem 11; [29]) and from Theo-
The best cohort S  X  updates its signature w.r.t.  X  x as de-scribed in Section 5.1, with magnitude of change bounded by  X   X  max (Theorem 4). Note that the change in S  X   X  X  sig-nature is not reflected at Pythia X  X  P and specifically at the corresponding P  X   X  X  .
 Theorem 6. The expected magnitude of change in P , E [  X  ] ,inBCisboundedaboveby ( F (0) d  X  2)  X  max  X  max .
Proof. Each S i  X  X  is equally probable to be selected by Pythia as the best cohort given an input. Hence, from Theorem 5 we obtain E [  X  ]  X  m i =1 1 m ( F (0) d  X  2)  X 
Pythia determines a frequency  X  ( F (0) d  X  1)  X  max  X  max a batch update of P by asking from all (previously engaged as best) cohorts to send their updated signatures changes (referring only to modified clusters), provided that they have not changed from the previous batch update. However, a batch update can be avoided once the best cohort sends the final estimate to Pythia for updating P .
Setup. We conducted an extensive series of experiments to assess the performance of Godzilla, ACM and Pythia X  X  schemes COE and BC on two real datasets (D1 and D2) and a synthetic dataset (DS). Real datasets are adopted from the UCI Machine Learning Repository [32]. D1 con-tains |X| =5  X  10 5 real valued vectors of d = 90 correspond-ing to audio features. D2 contains |X| =5  X  10 4 real val-ued vectors of d = 384 corresponding to features extracted from Computed Tomography images. Each vector of DS is a 20-dimensional point with the first fifteen dimensions randomly sampled from a Gaussian mixture of five compo-nent Gaussian pdfs with equal mixture weights and mean values of each component randomly selected from the uni-form distribution U (0 , 15). The other five dimensions are drawn, independently, from the univariate Gaussian dis-tribution N (0 , 1). The first fifteen dimensions are infor-mative dimensions, while the rest dimensions are random noises artificially added to test Pythia X  X  capability of pre-dicting S . For each dataset, we synthetically produce MVs from each x t for t =1 ,...,T as follows: each dimension k =1 ,...,d from x t is randomly and independently marked as missing with MV probability q . In this case, we expect cases of missing all dimensions or none. We set q =0 . 3, which is a relatively high probability of MVs per dimen-sion, thus, being able to test Pythia X  X  robustness in terms of accuracy. On average, a signature P i contains 0.32% of points of cohort X  X  set X i (this amount refers to the number of clusters stored in Pythia) using ART with initial learning rate  X  =0 . 2, which gradually decreases. Moreover, we set the range percentage  X  k =  X  =0 . 1 for all dimensions in order to construct  X  . We run all experiments 100 times and took their average values for all performance metrics, with a stream of T = 1000 inputs. Pythia X  X  schemes and MVAs (Section 3.2) were written in Matlab. Table 1 summarizes the parameter values used in our experiments.

Parameter Notation Value/Range d dimensions { 20,90,384 }  X  vigilance range pct. 0.1  X  init. learning rate 0.2 q MV probability 0.3 m number of cohorts { 5 , 10 , 20 , 50 , 100 } T number of inputs 1000
Performance metrics. Our metrics include efficiency metrics and accuracy metrics . A scale-out system consist-ing of m cohorts affords two types of parallelism: intra-imputation and inter-imputation parallelism. The former refers to the capability of processing any single imputa-tion using a number of cohorts in parallel, each accessing a dataset partition. The latter refers to the systems X  capa-bility of running in parallel a number of imputations, each of which engages a subset of cohorts. It is crucial to note that Godzilla affords neither of these parallelism types and that ACM affords only intra-imputation parallelism. This latter scenario is particularly important as typically a system is presented with a (large) batch of (vector-) inputs, each with missing values and the goal is to impute all input vectors in the batch as quickly/scalably as possible. Given this, our efficiency metrics embody various efficiency aspects impact-ing scalability. First, we report on imputation latency , de-fined as the time (in seconds) a system (i.e., Godzilla, ACM, Pythia-COE, or Pythia-BC) requires to impute a single in-put (vector) using a MVA. The rate of latency increase as dataset sizes grow is a strong aspect of scalability. In ACM, latency refers to the time a single cohort requires to im-pute a single input on its local dataset partition, assuming m cohorts run in parallel. In Pythia, latency refers to the time for COE / BC to predict best cohort(s) S / S  X  , plus the latency to run MVA in parallel at cohort(s). Imputa-tion speedup is defined as the ratio of Godzilla latency over ACM / COE / BC latency; it indicates how much a system is faster than Godzilla for a single imputation. Imputation throughput is defined as the rate of imputations delivered by a system (number of imputations per second) given a finite stream (batch) of T inputs: with this we capture the inter-imputation parallelism, in addition to the intra-imputation parallelism.

We measure imputation accuracy using the RMSE metric (root-mean squared difference) between x a and  X  x : RM SE = 1
Fig. 3(a-b) shows the imputation speedup against m for all systems using KNN and REG over D2. Similar results are obtained for D1 which are omitted due to space limita-tions. Overall ACM, COE and BC achieve an almost linear speedup using both REG and KNN. The speedup of COE and BC drops slightly as m increases since higher m implies more signatures to be processed at Pyhtia.
 Fig. 3(c-d) shows the latency of Godzilla and Pythia-COE (Pythia-BC curves are very close to Pythia-COE) us-ing REG when m = { 10 , 50 , 100 } and the size of D1 and D2 varies from 5000 to 300,000 points; (similar results exist for KNN, but are omitted for space reasons). Godzilla struggles with increasing dataset sizes: with over 200,000 and 100,000 points, a high latency over 20s and 35s per input for D1 and D2, respectively, is observed. Pythia scales nicely with its latency per input increasing linearly. Moreover, when the number of cohorts increases, we obtain a sublinear increase in latency. Pythia can easily handle large datasets if more cohorts are available to scale to big data missing values.
Fig. 4(a-b) shows the throughput of each system indicat-ing the capability of handling a stream of T inputs. COE engages S for an input (or S  X  in case of BC) thus the other cohorts (  X  S \ S ) are available to be potentially engaged for other inputs in the stream. Now, recall Fig. 2(b) which shows the average number of cohorts engaged by COE per input for all data sets. For m = 100, about 26% of cohorts (average for all data sets) are engaged per input. Obviously, the distribution of the engaged cohorts plays an important role. That is, for a stream of inputs heading for imputation, we achieve very high throughput when (i) | S | is relatively small (in case of COE) and (ii) different imputations en-gage different subsets of cohorts. On the other hand, in ACM, all cohorts are concurrently occupied by the same in-put. The impact of the cohort engagement policy of Pythia X  X  schemes on the throughput is illustrated in Fig. 4(a-b) us-ing REG, where the y -axis is plotted in logarithmic scale for readability. (Similar results exist with KNN). Pythia can handle up to tens of thousands of inputs per second, compared to ACM and Godzilla, which deal with tens of inputs and a few inputs, respectively. As expected, Pythia achieves higher throughput as m increases, as the possibili-ties for intra-imputation parallelism increase. However, note that in Fig. 4(a) as m increases, we do not achieve further significant increase in throughput, because Pythia X  X  process-ing over signatures becomes significant. The latter is higher for higher dimensions. In Fig. 4(b), as m increases, Pythia achieves high throughput. We can observe the impact of the number of dimensions d on throughput. D2 contains points with 326% more dimensions than those in D1. Pythia achieves a throughput over 10 4 (inputs/sec) with m =20in D1, while it achieves the same throughput with m = 100 in D2 (five times more cohorts).

Our results up to now clearly make a strong case for the scale-out advantages of the Pythia framework. Fig. 4(c-d) shows the RMSE against m using KNN and REG on synthetic data. COE and BC, as anticipated based on discussions of Example 1 and 2, obtain significant lower RMSE than Godzilla and ACM. However, this occurs with decreasing benefits as the number of cohorts increases; for m&gt; 50 no further decrease in RMSE is achieved. Specifi-cally, COE predicts a subset of cohorts, out of m cohorts, which achieves quite similar RMSE as that obtained by a subset of cohorts out of m with m &gt;m&gt; 50. In addition, BC engages the best cohort whose estimate is very close to the aggregate estimate of the subset of cohorts engaged by COE. Please note that ACM may yield a higher RMSE de-pending on the MVA used, even compared to Godzilla. For instance, using KNN, Godzilla would provide the global best (c) Latency; (REG) K points, whereas in ACM each cohort, even when storing irrelevant data, will be contributing its best K points. The latter necessarily implies that ACM X  X  imputation involves points which adversely affect imputation errors.

Fig. 5 shows the RMSE against m using KNN and REG on real datasets. Pythia X  X  schemes achieve comparable RMSE with Godzilla, with COE assuming relatively the lowest RMSE for both MVAs and datasets. In addition, the RMSE of COE remains at its lowest value from a certain m value (e.g., m = 50 in D2) thus there is no need to involve more cohorts. BC performs slightly better than Godzilla for both MVAs and datasets. Moreover, BC assumes higher RMSE than COE. This denotes the robustness of COE compared to BC in terms of accuracy due to the aggregate estimate from multiple engaged cohorts. ACM has higher RMSE than Godzilla in both datasets since it aggregates the estimates of all cohorts possibly incorporating estimates that spoil the final result.
The central conclusions of our study are the following:
We have tackled the problem of scaling out MV imputa-tions, a common problem in many big data applications. We studied and developed some of the fundamentals of the prob-lem, based on which we developed Pythia, a framework and algorithms designed for this aim. The Pythia framework is drastically different, as it on the one hand avoids the need to access all cohorts (and all associated costs for communica-tion and for running MVAs at all cohorts), while on the other can achieve better or comparable MV imputation accuracy, compared to centralized solutions. Specifically, our compre-hensive experiments showed that it can provide drastically better efficiency/scalability and accuracy compared to a cen-tralized approach (Godzilla) and a massively parallel, a la Map-Reduce, solution (ACM). Future work plans entail the study of additional cohort prediction schemes, straddling the line between efficiency and accuracy.
This research has been co-financed by the European Union (European Social Fund -ESF) and Greek national funds through the Operational Program  X  X ducation and Lifelong Learning X  of the National Strategic Reference Framework (NSRF) -Research Funding Program: Thales. Investing in knowledge society through the European Social Fund. [1] X. Su, et al ,  X  X sing Classifier-Based Nominal [2] A. Farhangfar, et al ,  X  X mpact of imputation of missing [3] M.T. Asif, et al ,  X  X ow X  X imensional Models for Missing [4] E.C. Chi, et al ,  X  X enotype imputation via matrix [5] I.B. Aydilek, et al ,  X  X  novel hybrid appoach to [6] A. Farhangfar, et al ,  X  X  Novel Framework for [7] K. Lakshminarayan, et al ,  X  X mputation of missing data [8] L. A. Kurgan, et al ,  X  X ining the cystic fibrosis data X , [9] A.W. Liew, et al ,  X  X issing value imputation for gene (c) RMSE; D2 (KNN) and REG. [10] J. Dean, et al ,  X  X apReduce: Simplified Data [11] S. Ghemawat, et al ,  X  X he Google File System X , Proc. [12] C-T. Chu, et al ,  X  X ap-Reduce for Machine Learning [13] C. K. Enders,  X  X pplied Missing Data Analysis X , [14] D. W. Joenssen, et al ,  X  X ot Deck Methods for [15] O. Troyanskaya, et al ,  X  X issing value estimation [16] R.J. Little, et al ,  X  X tatistical Analysis with Missing [17] T.E. Raghunathan, et al ,  X  X  multivariate technique for [18] D.B. Rubin,  X  X ultiple Imputation After 18+ Years X , J. [19] L. Li, et al ,  X  X ynaMMo: mining and summarization of [20] S. Yang, et al ,  X  X nline recovery of missing values in [21] M. Ouyang, et al ,  X  X aussian mixture clustering and [22] T. Aittokallio, et al ,  X  X ealing with missing values in [23] D-W. Kim, et al ,  X  X terative Clustering Analysis for [24] M.R. Garey, et al ,  X  X omputers and Intractability; A [25] B. Przydatek,  X  X  fast approximation algorithm for the [26] G. A. Carpenter, et al ,  X  X he ART of adaptive pattern [27] A. Ahmad, et al , X  X  k  X  X ean clustering algorithm for [28] P. J. Rousseeuw, et al ,  X  X lternatives to the median [29] H. Belbachir, et al ,  X  X ums involving moments of [30] J-H. Yang, et al ,  X  X he asymptotic expansions of [31] J.L. Bentley,  X  X ultidimensional binary search trees [32] K. Bache, et al , UCI Machine Learning Repository
