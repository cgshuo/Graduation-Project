 i i CHENHUI CHU, TOSHIAKI NAKAZAWA, DAISUKE KAWAHARA, called Kanji. Hanzi can be divided into two groups, simplified Chinese (used in main-land China and Singapore) and traditional Chinese (used in Taiwan, Hong Kong, and Macau). The number of strokes needed to write characters has been largely reduced in simplified Chinese, and the shapes may be different from those in traditional Chinese. Because kanji characters originated from ancient China, many common Chinese char-acters exist in hanzi and kanji.
 i i
Since Chinese characters contain a significant amount of semantic information and common Chinese characters share the same meaning, they can be valuable linguis-tic clues in many Chinese-Japanese natural language processing (NLP) tasks. Many used the occurrence of identical common Chinese characters in Chinese and Japanese in an automatic sentence alignment task. Goh et al. [2005] detected common Chinese characters where kanji are identical to traditional Chinese but differ from simplified Chinese. Using a Chinese encoding converter 1 that can convert traditional Chinese into simplified Chinese, they built a Japanese-Simplified Chinese dictionary partly us-ing direct conversion of Japanese into Chinese for Japanese kanji words. Huang et al. [2008] examined and analyzed the semantic relations between Chinese and Japanese at a word level based on a common Chinese character mapping. They used a small list of 125 visual variational pairs of manually matched common Chinese characters.
However, the resources for common Chinese characters used in these previous stud-ies are not complete. In this article, we propose a method for automatically creating a Chinese character mapping table for Japanese, traditional Chinese, and simplified Chinese using freely available resources, with the aim of constructing a more complete resource containing common Chinese characters.

Besides common Chinese characters, there are also many other semantically equivalent Chinese characters in the Chinese and Japanese languages. These Chinese characters would also be valuable in Chinese-Japanese machine translation (MT), es-pecially in word/phrase alignment. However, since there are no available resources for such Chinese characters, we propose a statistical method for detecting these charac-ters, which we call statistically equivalent Chinese characters.

In corpus-based Chinese-Japanese MT, parallel sentences contain equivalent mean-tistically equivalent Chinese characters appear in the sentences. In this article, we point out two main problems in Chinese word segmentation for Chinese-Japanese MT, namely, unknown words and word segmentation granularity, and propose an approach exploiting common Chinese characters to solve these problems. Furthermore, we pro-pose a method for exploiting common Chinese characters and statistically equivalent Chinese characters in phrase alignment. Experimental results show that our proposed approaches improve MT performance significantly. Table I gives some examples of Chinese characters in Japanese, traditional Chinese, and simplified Chinese, from which we can see that the relation between kanji and hanzi is quite complicated.

Because kanji characters originated from ancient China, most kanji have fully cor-responding Chinese characters in hanzi. In fact, despite Japanese having continued to evolve and change since its adoption of Chinese characters, the visual forms of the Chinese characters have retained a certain level of similarity; many kanji are iden-while others are identical to simplified Chinese characters but differ from traditional kanji that have corresponding Chinese characters in hanzi, although the shapes differ from those in hanzi (e.g.,  X  z (begin) X  in Table I). However, there are some kanji that do not have fully corresponding Chinese characters in hanzi. Some kanji only have i i corresponding traditional Chinese characters (e.g.,  X  F (octopus) X  in Table I), because they were not simplified into simplified Chinese. Moreover, there are some Chinese characters that originated in Japan namely, kokuji, which means that these national characters may have no corresponding Chinese characters in hanzi (e.g.,  X  cluded) X  in Table I).

What makes the relation even more complicated is that a single kanji form may cor-respond to multiple hanzi forms. Also, a single simplified Chinese form may correspond to multiple traditional Chinese forms, and vice versa.

Focusing on the relation between kanji and hanzi, we present a method for automat-ically creating a Chinese character mapping table for Japanese, traditional Chinese, and simplified Chinese using freely available resources [Chu et al. 2012b]. Common Chinese characters shared in Chinese and Japanese can be found in the mapping table. Because Chinese characters contain significant semantic information, this mapping table could be very useful in Chinese-Japanese MT. The character set in use for kanji is JIS Kanji code, whereas for hanzi, there are sev-eral, of which we have selected Big5 for traditional Chinese and GB2312 for simplified Chinese, both of which are widely used.
Japanese Industrial Standard, containing 6,879 graphic characters, including 6,355 kanji and 524 non-kanji. The mapping table is for the 6,355 kanji characters, that is, JIS Kanji, in JIS X 0208.  X  Big5 is the most commonly used character set for traditional Chinese in Taiwan,
Hong Kong, and Macau, and was defined by the Institute for Information Industry in Taiwan. There are 13,060 traditional Chinese characters in Big5.  X  GB2312 is the main official character set of the People X  X  Republic of China for sim-plified Chinese characters and is widely used in mainland China and Singapore.
GB2312 contains 6,763 simplified Chinese characters.  X  Unihan database 2 is the repository for the Unicode Consortium X  X  collective knowl-edge regarding the CJK (Chinese-Japanese-Korean) Unified Ideographs contained in the Unicode Standard. 3 The database consists of a number of fields containing data for each Chinese character in the Unicode Standard. These fields are grouped into categories according to their purpose, including mappings, readings, dictionary indices, radical stroke counts, and variants. The mappings and variants categories contain information regarding the relation between kanji and hanzi. i i  X  The Chinese encoding converter 4 is an open-source system that converts traditional
Chinese into simplified Chinese. The hanzi converter standard conversion table, a resource used by the converter, contains 6,740 corresponding traditional Chinese
Table II depicts a portion of the table.  X  Kanconvit 5 is a publicly available tool for kanji-simplified Chinese conversion. It from a Kanji, traditional Chinese, and simplified Chinese mapping table, containing 3,506 one-to-one mappings. Table III depicts a portion of this table. Based on the relation between kanji and hanzi, we define the following seven cate-gories for kanji.  X  Category 1. Identical to hanzi.  X  Category 2. Identical to traditional Chinese but different from simplified Chinese.  X  Category 3. Identical to simplified Chinese but different from traditional Chinese.  X  Category 4. Visual variations.  X  Category 5. With a corresponding traditional Chinese character only.  X  Category 6. No corresponding hanzi.  X  Others . Does not belong to the preceding categories.

We create a Chinese character mapping table for Japanese, traditional Chinese, and simplified Chinese by classifying JIS Kanji into these seven categories and automati-cally finding the corresponding traditional Chinese and simplified Chinese characters using the resources introduced in Section 2.2. The method involves two steps.  X  Step 1. Extraction.  X  Step 2. Categorization and construction.

In Step 1, we extract the JIS Kanji, Big5 Traditional Chinese, and GB2312 Sim-plified Chinese from the Unihan database. These Chinese characters are collected in the mappings category, which contains mappings between Unicode and other encoded character sets for Chinese characters. JIS Kanji are obtained from the kIRG JSource Chinese from the kIRG GSource G0 field.

In Step 2, we categorize the JIS Kanji and construct a mapping table. We automat-Big5 and GB2312, it belongs to Category 1. If the kanji exists only in Big5, we check whether a corresponding simplified Chinese character can be found; if so, it belongs to i i Category 2, otherwise, it belongs to Category 5. If the kanji exists only in GB2312, we check whether a corresponding traditional Chinese character can be found; if so, it be-longs to Category 3. If the kanji exists in neither Big5 nor GB2312, we check whether corresponding hanzi can be found; if a fully corresponding Chinese character exists in hanzi, it belongs to Category 4, else if only a corresponding traditional Chinese char-in hanzi, it belongs to Category 6, otherwise, it belongs to Others.
 To find the corresponding hanzi, we search traditional Chinese and simplified Chinese variants, as well as other variants for all kanji. This search is carried out using the variants category in the Unihan database, in which there are five fields: kTraditionalVariant corresponding to traditional Chinese variants, kSimplifiedVari-ant corresponding to simplified Chinese variants, and kZVariant , kSemanticVariant , and kSpecializedSemanticVariants corresponding to the other variants. In addition, we also use the hanzi converter standard conversion table and Kanconvit mapping table. Note that the resources in the hanzi converter standard conversion table can only be used for the traditional Chinese and simplified Chinese variants search, whereas the Kanconvit mapping table can also be used for the other variants search. The format for kanji in Categories 1, 2, 3, and 4 in the mapping table is as follows.  X  Kanji[TAB]Traditional Chinese[TAB]Simplified Chinese[RET].
 If multiple hanzi forms exist for a single kanji, we separate them with  X , X . Table IV shows some examples of multiple hanzi forms. The formats for kanji in Categories 5 and 6 are as follows.  X  Category 5. Kanji[TAB]Traditional Chinese[TAB]N/A[RET].  X  Category 6. Kanji[TAB]N/A[TAB]N/A[RET].
 Table V gives some statistics of the Chinese character mapping table we created for Japanese, traditional Chinese, and simplified Chinese. Here,  X  X thers X  are the kanji that have a corresponding simplified Chinese character only. There are correspond-ing traditional Chinese characters for these kanji, but they were not collected in Big5 Traditional Chinese. Kanji  X   X  (bastard halibut) X  is one of such example. Compared with using only the Unihan database, incorporating the hanzi converter standard con-version and Kanconvit mapping tables can improve the completeness of the mapping table. Tables VI and VII give some examples of additional Chinese character mappings found using the hanzi converter standard conversion table and Kanconvit mapping table, respectively.
 i i To show the completeness of the mapping table we created, we used a resource from Wiktionary 6 , which is a wiki project aimed at producing a free-content multilingual dictionary. In the Japanese version of Wiktionary, there is a kanji category that pro-nunciation, idioms, kanji in Chinese and Korean, and codes. We are interested in the variants part. Figure 1 gives an example of kanji  X   X   X  from the Japanese Wiktionary, in which the variants part, containing the traditional Chinese variant  X  Chinese variant  X   X   X , and other variant  X  v  X  of kanji  X 
We downloaded the Japanese Wiktionary database dump data 7 and extracted the variants for JIS Kanji. We then constructed a mapping table based on the Wiktionary using the method described in Section 2.3, the only difference being i i the variants extracted from the Japanese Wiktionary.

To evaluate the completeness of the mapping table created using the proposed method, we compared the statistics thereof with those of Wiktionary. Table VIII shows the completeness comparison between the proposed method and Wiktionary. We can see that the proposed method creates a more complete mapping table than Wiktionary. Table IX gives some examples of Chinese character mappings found by the proposed method, but which do not exist in the current version of Wiktionary.

Furthermore, we carried out an experiment by combining the mapping table we created with Wiktionary. The results in Table VIII show that Wiktionary can be used as a supplementary resource to further improve the completeness of the mapping table. Table X gives some examples of Chinese character mappings contained in Wiktionary, but which were not found by the proposed method. We investigated the coverage of shared Chinese characters on a simplified Chinese-Japanese corpus, namely, the scientific paper abstract corpus provided by JST NICT. 9 This corpus was created by the Japanese project Development and Research of Chinese X  X apanese Natural Language Processing Technology. Some statistics of this corpus are given in Table XI.

We measured the coverage in terms of both characters and words under two different experimental conditions.  X  Identical . Only exactly the same Chinese characters.  X  +Common . Perform kanji-to-hanzi conversion for common Chinese characters using the Chinese character mapping table constructed, as described in Section 2.
Table XII presents the coverage results for shared Chinese characters. If we use all the resources available, we can find corresponding hanzi characters for over 76% of the kanji characters. i i Hantology [Chou and Huang 2006] is a character-based Chinese language resource, which has adopted the Suggested Upper Merged Ontology (SUMO) [Niles and Pease 2001] for a systematic and theoretical study of Chinese characters. Hantology repre-as well as variants for different Chinese characters. However, the variants in Hantol-ogy are limited to Chinese hanzi.

Chou et al. [2008] extended the architecture of Hantology to Japanese kanji and in-cluded links between Chinese hanzi and Japanese kanji, thereby providing a platform for systematically analyzing variations in kanji. However, a detailed analysis of vari-ants of kanji has not been presented. Moreover, because the current version of Han-tology only contains 2,100 Chinese characters, whereas our mapping table includes all on Hantology and which is as complete as our proposed method. As there are no explicit word boundary markers in Chinese, word segmentation is con-sidered an important first step in MT. Studies have shown that an MT system with Chinese word segmentation outperforms those treating each Chinese character as a single word, while the quality of Chinese word segmentation affects MT performance [Chang et al. 2008; Xu et al. 2004]. It has been found that besides segmentation accu-racy, segmentation consistency and granularity of Chinese words are also important for MT [Chang et al. 2008]. Moreover, optimal Chinese word segmentation for MT is de-pendent on the other languages, and therefore, a bilingual approach is necessary [Ma and Way 2009].

Most studies have focused on language pairs containing Chinese and another lan-Japanese MT, where segmentation is needed on both sides. Segmentation for Japanese Chinese is still about 95% [Wang et al. 2011]. Therefore, we only do word segmen-tation optimization in Chinese and use the Japanese segmentation results directly.
Similar to previous works, we also consider the following two Chinese word segmentation problems to be important for Chinese-Japanese MT. The first problem re-lates to unknown words, which cause major difficulties for Chinese segmenters and af-fect segmentation accuracy and consistency. Consider, for example,  X  X osaka X  shown in Figure 2, which is a proper noun in Japanese. Because  X  X osaka X  is an unknown word i i Japanese word segmentation result is correct.

The second problem is word segmentation granularity. Most Chinese segmenters adopt the famous Penn Chinese Treebank (CTB) standard [Xia et al. 2000], while most Japanese segmenters adopt a shorter unit standard. Therefore, the segmentation unit in Chinese may be longer than that in Japanese, even for the same concept. This can increase the number of 1-to-n alignments, making the word alignment task more dif-ficult. Taking  X  X ounder X  in Figure 2 as an example, the Chinese segmenter recognizes it as one token, while the Japanese segmenter splits it into two tokens because of the different word segmentation standards.
 tive that exploits common Chinese characters shared between Chinese and Japanese proach, Chinese entries are extracted from a parallel training corpus based on com-mon Chinese characters to augment the system dictionary of a Chinese segmenter. In addition, the granularity of the training data for the Chinese segmenter is adjusted to that of the Japanese one by means of extracted Chinese entries. Chinese entries are extracted from a parallel training corpus through the following steps.  X  Step 1. Segment Japanese sentences in the parallel training corpus.  X  Step 2. Convert Japanese tokens consisting only of kanji using the Chinese character mapping table created in Section 2.  X  Step 3. Extract the converted tokens as Chinese entries if they exist in the corre-sponding Chinese sentence.

For example,  X  B (Kosaka) X ,  X  H (Mr.) X ,  X   X  , (Japan) X ,  X  (anesthesia) X ,  X  f (society) X ,  X   X  (found) X  and  X  (person) X  in Figure 2 would be extracted. Note that although  X  4  X   X   X   X  (clinical) X ,  X  and  X   X   X  u  X  (found) X  are not identical, because  X  4  X   X  (drunk) X  and  X   X  u (create) X  are common Chinese characters,  X  is converted into  X  4  X  (clinical) X ,  X   X  T (anesthesia) X , is converted into  X  (anesthesia), X  and  X  u  X  (found) X  is converted into  X   X  (found) X  in Step 2. Several studies have shown that using a system dictionary is helpful for Chinese word segmentation [Low et al. 2005; Wang et al. 2011]. Therefore, we used a corpus-based Chinese word segmentation and POS tagging tool with a system dictionary and in-corporated the extracted entries into the system dictionary. The extracted entries are not only effective for the unknown word problem, but also useful in solving the word segmentation granularity problem. i i problem, we created a POS tag mapping table between Chinese and Japanese by hand. For Chinese, we used the POS tagset used in CTB, which is also used in our Chinese segmenter. For Japanese, we used the POS tagset defined in the morphological ana-lyzer JUMAN [Kurohashi et al. 1994]. JUMAN uses a POS tagset containing sub-POS tags. For example, the POS tag  X  ^ (noun) X  contains sub-POS tags, such as  X  ^ (common noun) X ,  X   X  ^ (proper noun) X ,  X  B  X  ^ (temporal noun) X , and so on. Table XIII shows a part of the Chinese-Japanese POS tag mapping table we created, with the sub-POS tags of JUMAN given within square brackets.

POS tags for the extracted Chinese entries are assigned by converting the POS tags of Japanese tokens assigned by JUMAN into POS tags of CTB. Note that not all POS tags of JUMAN can be converted into POS tags of CTB, and vice versa. Those that cannot be converted are not incorporated into the system dictionary. posed a short-unit standard for Chinese word segmentation that is more similar to the Japanese word segmentation standard, and which can reduce the number of one-to-n alignments and improve MT performance.

We previously proposed a method for transforming the annotated training data of the Chinese segmenter into the Japanese word segmentation standard using Chinese segmenter [Chu et al. 2012a]. Because the extracted entries are derived from Japanese word segmentation results, they follow the Japanese word segmentation ing data for the Chinese segmenter. If the token is longer than a extracted entry, it is simply split. Figure 3 gives an example of this process, where  X  and  X   X  (element) X  are both extracted entries. Because  X  is longer than  X  H (effective) X , it is split into  X  i i nally annotated one is retained for the split tokens.
 transformation ambiguity. For example, for a long token like  X  ing abroad) X  in the annotated training data, entries  X  ous method randomly chose one entry for transformation. Therefore,  X  studying abroad) X  could be split into  X  Y (stay) X  X nd  X  f (student) X , which is incor-rect. To solve this problem, we improved the transformation method by utilizing both transformation information extracted from the parallel training corpus. Short-unit in-and includes the following steps.  X  Step 1. Segment both Chinese and Japanese sentences in the parallel training cor-pus. using the Chinese character mapping table we created in Section 2. mented Chinese sentence and the corresponding Chinese tokens.
 For example,  X   X  (founder)  X   X  (found) / (person) X  in Figure 2 is extracted as short-unit information.

In the improved transformation method, we modify the tokens in the training data using the following processes in order. (1) If the token itself exists in the extracted entries, keep it. (4) Otherwise, keep it.

Following Chu et al. [2012a], we do not use extracted entries that are composed of only one Chinese character, because these entries may lead to undesirable transfor-mation results. Taking the Chinese character  X  L (song) X  as an example,  X  can be used as a single word, but we can also use  X  L (song) X  to construct other words by combining it with other Chinese characters, such as  X  L  X  and so on. Obviously, splitting  X  L  X  (praise) X  into  X  L (song) X  and  X  splitting  X   X  L (poem) X  into  X   X  (poem) X  and  X  L (song) X  is undesirable. We do not use extracted number entries either, as these, could also lead to undesirable transforma-tion. For example, using  X  A k (18) X  to split  X   X  ~ ] A k  X  A k (18) X  is obviously incorrect. Moreover, there are a few consecutive tokens in the i i training data that could be combined into a single extracted entry; however, we do not consider these patterns.

Figure 4 gives an example of our improved transformation method. In this example, since  X  0 -w (Mediterranean) X  also exists in the extracted entries, it is not changed, even though there is an extracted entry  X  0 -(in earth) X . The long token  X  dent studying abroad) X  can be transferred using short-unit information, so it is trans-ferred into  X  Y f (study abroad) X  and  X  (student) X . Meanwhile, the long token  X   X  (students) X  can be split into  X  f (student) X  and  X   X  (plural for student) X  using the extracted entry  X  f (student) X .

We record the extracted entries and short-unit information used for transformation with the corresponding Japanese tokens and store them as transforming word dictio-nary. This dictionary contains 16K entries and will be helpful for word alignment. We conducted Chinese-Japanese translation experiments on the state-of-the-art phrase-based statistical MT toolkit MOSES [Koehn et al. 2007] and an example-based MT (EBMT) system [Nakazawa and Kurohashi 2011b] to show the effectiveness of exploiting common Chinese characters in Chinese word segmentation optimization. 3.5.1. Experiments on Phrase-Based Statistical MT. The experimental settings for MOSES are given next. The parallel training corpus for this experiment was the same as that used in Section 2.6. We further used CTB 7 (LDC2010T07) segmenter. Training data, containing 31,131 sentences, was created from CTB 7 using the same method described in Wang et al. [2011]. The segmenter used for Chinese was an in-house corpus-based word segmentation and POS tagging tool with a system dictionary. Weights for the entries in the system dictionary were automatically learned from the training data using an averaged structured perceptron [Collins 2002]. For Japanese, we used JUMAN [Kurohashi et al. 1994]. All the default options were used in Moses, except for the distortion limit (6  X  20), which was tuned by MERT using a further 500 development sentence pairs. We trained a word-based 5-gram language model on the target side of the training data using the SRILM toolkit [Stolcke 2002]. We translated five test sets from the same domain as the parallel training corpus. The statistics of the test sets of Chinese and Japanese sentences are given in Tables XIV and XV, respectively. Note that none of the sentences in the test sets are included in the parallel training corpus. i i
We carried out Chinese-Japanese translation experiments, comparing the following three experimental settings.  X  Baseline . Using only entries extracted from the Chinese annotated corpus as the  X  Optimized . Incorporating the Chinese entries extracted in Section 3.2 into the sys- X  +Dictionary . Appending the transforming word dictionary stored in Section 3.4 to
The translations were evaluated using BLEU X 4 [Papineni et al. 2002] calculated on words. For Japanese-to-Chinese translation, we resegmented the translations us-ing the optimized Chinese segmenter. Tables XVI and XVII give the BLEU scores for Chinese-to-Japanese and Japanese-to-Chinese translation, respectively. For compari-short-unit transformation method further improved the Japanese-to-Chinese transla-tion. However, it had no effect on the Chinese-to-Japanese translation. Appending the transforming word dictionary further improved the translation performance. Similar to Chu et al. [2012a], the improvement in Japanese-to-Chinese translation compared with that in Chinese-to-Japanese translation is not that significant. We believe the reason for this is the input sentence. For Chinese-to-Japanese translation, the seg-mentation of input Chinese sentences is optimized, whereas for Japanese-to-Chinese translation, our proposed approach does not change the segmentation results of the input Japanese sentences.
 i i 3.5.2. Experiments on EBMT. We also conducted Chinese-Japanese translation exper-iments on EBMT, comparing the same three experimental settings described in menters were the same as those used in the experiments on MOSES. Since the EBMT system we used is a dependency-tree-based decoder, we further used CNP [Chen et al. 2008] as the Chinese dependency analyzer, while the Japanese dependency analyzer was KNP [Kawahara and Kurohashi 2006].

Tables XVIII and XIX show the BLEU scores for Chinese-to-Japanese and Japanese-to-Chinese translation, respectively. We can see that the Chinese-to-Japanese transla-tion performance on EBMT also improves when exploiting shared Chinese characters in Chinese word segmentation optimization. However, the improvement is not as sig-nificant as that on MOSES. Moreover, it has no effect for Japanese-to-Chinese trans-optimized segmented training data. Therefore, using optimized segmented sentences as input for CNP may affect the parsing accuracy. Compared with the translation per-formance on MOSES, the BLEU scores on EBMT are quite low. The reason for this is that both the alignment model and decoder for EBMT are disadvantaged by the low ac-curacy of the Chinese parser. Although the Chinese parser we used is a state-of-the-art one [Chen et al. 2008], the accuracy is less than 80%. On the other hand, the Japanese parser used in the experiments can analyze sentences with over 90% accuracy. We be-lieve that further improvement of the Chinese parser could improve the translation performance on EBMT.
 i i proach can improve MT performance significantly. We present an example to show the effectiveness of optimized short-unit segmentation results. Figure 5 gives an example of Chinese-to-Japanese translation improvement on MOSES using optimized short-short unit and baseline is whether  X   X  ' (suitability) X  is split in Chinese, whereas the Japanese segmenter always splits it. By splitting it, the short unit improves word alignment and phrase extraction, which eventually affects the decoding process. In de-coding, the short unit treats  X   X   X   X  ' (functional suitability) X  as one phrase, while the baseline separates it, leading to a undesirable translation result. transformation method, there are still some transformation problems. One problem is incorrect transformation. For example, there is a long token  X  an extracted entry  X  } (favor) X , and therefore, the long token is transferred into  X  (not) X ,  X  } (favor) X , and  X  (think) X , which is obviously undesirable. Our current method cannot deal with such cases, making this one of the future works in this study.
Another problem is POS tag assignment for the transformed short-unit tokens. Our proposed method simply keeps the original annotated POS tag of the long token for the transformed short-unit tokens, which works well in most cases. However, there are also some exceptions. For example, there is a long token  X  in the annotated training data, and an entry  X   X   X  (test) X  extracted from the parallel training corpus, so the long token is split into  X   X  (be) X ,  X  As the POS tag for the original long token is NN, the POS tags for the transformed short-unit tokens are all set to NN, which is undesirable for  X  problem. Furthermore, the transformed short-unit tokens may have more than one possible POS tag. All these problems will be dealt with in future work. i i Exploiting lexicons from external resources [Chang et al. 2008; Peng et al. 2004] is one way of dealing with the unknown word problem. However, the external lexicons may not be very efficient for a specific domain. Some studies [Ma and Way 2009; Xu et al. 2004] have used the method of learning a domain-specific dictionary from the character-based alignment results of a parallel training corpus, which separate each Chinese character, and consider consecutive Chinese characters as a lexicon in n-to-one alignment results. Our proposed method differs from these studies in that we obtain a domain-specific dictionary by extracting Chinese lexicons directly from a segmented parallel training corpus, making word alignment unnecessary.

The goal of our proposed short-unit transformation method is to form the segmenta-tion results of Chinese and Japanese into a one-to-one mapping, which can improve learning affix rules from an aligned Chinese-English bilingual terminology bank to ad-just Chinese word segmentation in the parallel corpus directly with the aim of achiev-ing the same goal. Our proposed method does not adjust Chinese word segmentation directly. Instead, we utilize the extracted Chinese lexicons to transform the annotated training data of a Chinese segmenter into a short-unit standard and perform segmen-tation using the retrained Chinese segmenter.

Wang et al. [2010] also proposed a short-unit transformation method. The proposed method is based on transfer rules and a transfer database. The transfer rules are ex-tracted from alignment results of annotated Chinese and segmented Japanese train-manually modified. Our proposed method learns transfer knowledge based on common Chinese characters. Moreover, no external lexicons or manual work is required. Chinese characters contain a significant amount of semantic information, with com-mon Chinese characters having the same meaning. Moreover, parallel sentences contain equivalent meanings in each language, and we can assume that common and Japanese. Figure 6 shows an example of Chinese-Japanese alignment from bi-directional GIZA++, where  X   X   X   X  and  X   X   X   X  (both mean  X  X n fact X ) are not auto-matically aligned. We notice this because these two words share a common Chinese correctly, these two words could be successfully aligned.

Motivated by this phenomenon, we previously proposed a method for exploiting com-mon Chinese characters in a joint phrase alignment model and proved the effective-ness of common Chinese characters in Chinese-Japanese phrase alignment [Chu et al. 2011]. A kanji-to-hanzi conversion method making use of the Unihan database and Chinese encoding converter was proposed in our previous work. In this article, we ex-tend the kanji-to-hanzi conversion method by using the Chinese character mapping table we created in Section 2.

Besides common Chinese characters, there are also several other semantically equiv-alent Chinese characters in Chinese and Japanese. We feel that these Chinese char-acters would also be valuable in MT, especially in word/phrase alignment. However, there are no available resources for these Chinese characters. We proposed a statis-tical method for detecting such Chinese characters, which we call statistically equiv-alent Chinese characters [Chu et al. 2012c]. We improved our previous exploitation i i method to make use of statistically equivalent Chinese characters, together with com-mon Chinese characters in a joint phrase alignment model. We showed that statis-study, we follow the shared Chinese characters exploitation method proposed in Chu et al. [2012c]. Table XX gives some examples of other semantically equivalent Chinese characters in Chinese and Japanese besides the common Chinese characters in Japanese, traditional Chinese, and simplified Chinese. Although these Chinese characters are not common Chinese characters, they have the same meaning. We proposed a statistical method for detecting statistically equivalent Chinese characters.
 ter detection method. The example parallel sentences (both mean  X  X ritical informa-tion is hidden X ) share common Chinese characters (e.g.,  X  as other semantically equivalent Chinese characters (e.g.,  X  der to detect the other semantically equivalent Chinese characters, we first eliminate the kana characters in the Japanese sentence. 12 We treat each Chinese character as a single word and perform character-based alignment using GIZA++ [Och and Ney 2003], which implements a sequential word-based statistical alignment model for IBM models. i i We can see that shared Chinese characters obtain high lexical translation probabili-ties. Although the lexical translation probability of  X   X   X  ' (big) X ), it is still prominent. Furthermore, because  X   X  1  X  of shared Chinese character would be helpful in MT carried out in the same domain. Since we conducted an experiment in the same domain, we kept them in the prelim-inary experiment. However, such shared Chinese characters may be problematic in different domains, because they are not semantically equivalent. We used the Bayesian subtree alignment model on dependency trees proposed by Nakazawa and Kurohashi [2011a]. In this model, the joint probability for a sentence pair is defined as where P (`) is the geometric distribution denoting the number of concepts that gener- X  are omitted here.
 i i pairs as where num zh char and num ja char denote the numbers of Chinese characters in the Chinese and Japanese phrases, respectively, while match zh char and match ja char phrases, respectively. For common Chinese characters, we regard the matching weight ical translation probability for the Chinese character pair estimated in Section 4.2. Taking  X   X  o @  X  and  X   X  1 @  X  (both mean  X  X nformation agency X ) as an example, there is one common Chinese character  X  @ (agency) X  and two statistically equivalent Chinese characters pairs, and thus,
We modified the Bayesian subtree alignment model by incorporating a weight w into the phrase generation distribution and redefined the joint probability for a sentence pair as where weight w is proportional to the shared Chinese character matching ratio where  X  is a variable set by hand.

Note that this exploitation method has the drawback that the joint probability of a sentence pair is no longer a probability. words. Currently, many Japanese words are written in kana even if they have corre-sponding kanji expressions, which are normally used. The Chinese characters in kanji expressions are useful clues for finding shared Chinese characters. We can use kana-kanji conversion techniques to obtain kanji expressions from kana expressions, but here, we simply consult the Japanese dictionary of JUMAN [Kurohashi et al. 1994]. Table XXII gives some examples of kana-kanji conversion results. We only perform kana-kanji conversion of content words, because, as proven in our alignment experi-ments, conversion of function words may lead to incorrect alignment.
 i i 4.6.1. Alignment. We conducted alignment experiments on a Chinese-Japanese corpus to show the effectiveness of exploiting shared Chinese characters in phrase alignment.
The training corpus was the same as that used in Section 2.6. Statistically equiva-lent Chinese characters described in Section 4.2 are also detected using this corpus. As gold-standard data, we used 510 sentence pairs for Chinese-Japanese, which were annotated by hand. Two types of annotations were used: sure (S) alignments and pos-sible (P) alignments [Och and Ney 2003]. The unit of evaluation was the word. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. We set variable showed the best performance in the preliminary experiments for tuning the weights. menter described in Section 3.5.1 and dependency analyzer CNP [Chen et al. 2008]. Japanese sentences were converted into dependency structures using the morpho-logical analyzer JUMAN [Kurohashi et al. 1994] and the dependency analyzer KNP [Kawahara and Kurohashi 2006].

For comparison, we used GIZA++ with its default parameters and conducted word tic [Koehn et al. 2003]. We also applied the BerkelyAligner. with its default settings for unsupervised training. 14 Experimental results are given in Table XXIII. The alignment accuracies of the Bayesian subtree alignment model, the method in Chu et al. [2011], the method exploiting common Chinese characters, and that exploiting both statistically equivalent and common Chinese characters are indicated as  X  X aseline (Nakazawa+ 2011) X ,  X  X hu+ 2011 X ,  X +Common X , and  X +Common probability greater than a manually set threshold (i.e., threshold = 1.0 e  X  07, which preliminary experiments for tuning the threshold. We can see that alignment accu-racy improves when exploiting the shared Chinese characters. Because we extended the kanji-to-hanzi conversion method using the Chinese character mapping table and improved the exploitation method,  X +Common X  yields a slightly improved alignment accuracy compared with Chu et al. [2011]. Alignment accuracy is further improved by exploiting statistically equivalent Chinese characters.

Figure 8 shows an example of alignment improvement using common Chinese char-acters. Because there is a common Chinese character (i.e.,  X  i i  X   X  and  X   X   X   X  (both mean  X  X tandard X ), these two words are successfully aligned using our proposed method, and consequently, the alignment between  X  mean  X  X hat X ) is discovered. Figure 9 shows an example of alignment improvement us-ing statistically equivalent Chinese characters. Because  X  proposed method.

Although in most cases shared Chinese characters achieve correct alignment, there are also some exceptions. Figure 10 shows an example of an exception using common Chinese characters.  X   X   X  and  X  W  X  (both mean  X  X  little X ) are correctly aligned in the baseline alignment model. Because there is a common Chinese character (i.e.,  X   X   X   X   X / X  X ittle X ) in  X   X   X  (means  X  X t least X ) and  X  W  X  (means  X  X  little X ), the proposed method aligns them, resulting in an incorrect alignment. 4.6.2. Translation. Chinese-Japanese translation experiments were carried out on EBMT [Nakazawa and Kurohashi 2011b]. We conducted translation experiments on Section 4.6.1, we also carried out experiments exploiting common Chinese characters i i in both Chinese word segmentation optimization and phrase alignment. We first performed Chinese word segmentation optimization using the method described based on the optimized segmentation results using the exploitation method de-scribed in Section 4. This experimental setting is denoted as  X  X ptimized+Common X . We further conducted experiments after appending the transforming word dic- X  X ptimized+Dictionary+Common X .

Tables XXIV and XXV give the BLEU scores for Chinese-to-Japanese and Japanese-to-Chinese translation, respectively. We can see that translation performance also im-proves after exploiting common Chinese characters in phrase alignment. Exploiting statistically equivalent Chinese characters further slightly improves the translations. However, compared with the baseline system, the improvement by exploiting in phrase ception cases for exploiting shared Chinese characters in phrase alignment mentioned in Section 4.6.1, while the other is that both the alignment model and decoder we used suffer from low accuracy of the Chinese parser, which could affect the effectiveness of exploiting shared Chinese characters.
 i i
Exploiting common Chinese characters in both Chinese word segmentation opti-mization and phrase alignment achieves better translation performance than exploit-ing them separately. We think the reason is due to the alignment improvement by the double effect of both methods. Chinese word segmentation optimization creates a to-ken one-to-one mapping as far as possible between parallel sentences, while exploiting common Chinese characters in phrase alignment enhances the alignment probability of phrase pairs that share Chinese characters. Moreover, translation performance can Chinese word segmentation optimization. Kondrak et al. [2003] incorporated cognate (words or languages with the same origin) information in European languages in the translation models of Brown et al. [1993]. They arbitrarily selected a subset from the Europarl corpus as training data and ex-orthographic similarity. The corpus itself was appended to reinforce the co-occurrence count between cognates. The results of experiments conducted on a variety of bitexts the statistical training algorithm. Common Chinese characters are a kind of cognate, which we exploited together with statistically equivalent Chinese characters in phrase alignment, yielding improved alignment accuracy as well as translation performance. Shared Chinese characters can be very helpful in Chinese-Japanese MT. In this article, we proposed a method for creating a Chinese character mapping table automatically for Japanese, traditional Chinese, and simplified Chinese using freely available re-sources, and constructed a more complete resource of common Chinese characters than the existing ones. We also proposed a statistical method to detect statistically equiv-alent Chinese characters. We exploited shared Chinese characters in Chinese word segmentation optimization and phrase alignment. Experimental results show that our proposed approaches can improve MT performance significantly, thus verifying the ef-fectiveness of using shared Chinese characters in Chinese-Japanese MT.
However, our proposed approaches still have some problems. The proposed method for Chinese word segmentation optimization has problems with incorrect short-unit transformation and POS tag assignment. Our proposed method for exploiting shared Chinese characters in phrase alignment has some drawbacks, and there are also excep-tions that can lead to incorrect alignment results. We plan to solve these problems in future work. Furthermore, in this article we only evaluated our proposed approaches on a parallel corpus from an abstract paper domain, in which Chinese characters are more frequently used than in general Japanese domains. In the future, we intend to evaluate the proposed approaches on parallel corpora for other domains. i i i i
