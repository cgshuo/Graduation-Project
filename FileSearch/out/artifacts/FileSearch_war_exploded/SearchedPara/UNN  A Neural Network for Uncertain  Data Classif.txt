 Data tends to be uncertain in many applications [1], [2], [3], [4], [5]. Uncertainty can originate from diverse sources such as data collection error, measurement precision limitation, data sampling error, obsolete source, network latency and transmission error. The error or uncertainty in data is commonly treated as a random variable with probability distribution. Thus, uncertain attribute value is often represented by an interval with a probability distribution function over the interval [6], [7]. It is impor-data uncertainty is useful information which can be leveraged in order to improve the quality of the underlying results [17]. However, many traditional data mining problems become particularly challenging for the uncertain case. For example, in a classification application, the class which a data point belongs to may be changing as tainty over the whole dataset may blur the boundaries among different classes, which uncertainty models to achieve satisfactory performance. Classification is one of the key processes in machine learning and data mining. class label of data based on the feature vector [8]. An intuitive way of handling uncer-tainty in classification is to represent the uncertain value by its expectation value and applied. However, this approach does not effectively utilize important information distribution as the input. In this paper, we design and develop a new classifier named uncertain neural network (UNN), which employs new activation function in neurons to handle uncertain values. We also propose a new approach to improve the training efficiency of UNN. We prove through experiments that the new algorithm has satis-factory classification performance even when the training data is highly uncertain. Comparing with the traditional algorithm, the classification accuracy of UNN is significantly higher. Furthermore, with the new optimization method, the training efficiency can be largely improved. 
The paper is organized as follows. In section 2, we discuss related work. Section 3 defines the classification problem for uncertain data. In section 4, we first analyze the principle of uncertain perceptron in linear classification, and then construct the multilayer uncertain neural network, and discuss the training approach. Section 5 introduces an optimized activation function to improve the efficiency. The experi-ments results are shown in section 6, and section 7 makes a conclusion for the paper. There has been a growing interest in uncertain data mining. A number of data mining algorithms have been extended to proce ss uncertain dataset. For example, UK-Means [9], uncertain support vector machine [10], and uncertain decision tree [11]. The key idea in [10] is to provide a geometric algorithm which optimizes the probabilistic separation between the two classes on both sides of the boundary [7]. And [11] extends the decision tree to handle interval inputs and takes probability cardinality to simple bounded uncertain model, and in our work, we use Gaussian noise instead to model the uncertainty, which is more common in realistic world. Artificial neural network has been used in model-based clustering with a probability gained from ex-pectation-maximization algorithm for classification-likelihood learning [12]. We adopt the concept to estimate the probabilit y of membership when the uncertain data are covered by multiples classes. However, probability estimation presented here is unprecedented. 
In fuzzy neural network models for classification, either attributes or class labels can be fuzzy and are presented in fuzzy terms [13]. Given a fuzzy attribute of a data tuple, a degree (called membership) is assigned to each possible class, showing the extent to which the tuple belongs to a particular class. Some other fuzzy systems [18] build reasoning mechanisms based on rules, and try to simulate the fuzzy cases inside distribution into account in neuron level. Our work differs from previous work in that we revise the activation functions to comput e the membership based on uncertain data distribution information, instead of using Fuzzy logic for tuning neural network train-ing parameters. Our approach can work on both certain and uncertain data. f ), and a class label c i  X  C. Here, each f i,j is a pdf modeling the uncertain value of widely exists in practice [1], [2], [5], [6], [7]. the test instance has positive probability to be in different classes, then it will be pre-dicted to be in the class which has the highest probability. The work in this paper is to build a neural network when only uncertain training data tuples are available, and the goal is to find the model with the highest accuracy despite of the uncertainty. 4.1 Uncertain Perceptron We start with perceptron, which is a simple type of artificial neural network. Percep-tron is a classical model which constructs linear classifier as: activation function, and y is the perceptron X  X  output. 
For data sets with uncertain attributes, we need revise the functions and develop an uncertain perceptron for linear classification. We will illustrate our approach through a simple 2-dimensional dataset. Assume dataset has two attributes X = ( x 1 , x 2 ) and one and the class type can be +1 or -1, Fig. 1 is a geometric representation of linear classi-represented by an area instead of a single point because each dimension/attribute is an uncertain distribution, not an accurate value. 
The straight line L in Fig 1 represents the equation: where x 1 , x 2 are uncertain attributes. We define a parameter t as these attributes are independent, t will have a distribution as: nitely larger than 0, which means this tuple is in class +1, and locates above the line determined by which class has a high probability. Therefore, we construct an activa-tion function as equation (4.5).  X  the distribution is Gaussian, s can be calculated as: 
Based on the single uncertain neurons, we can develop a multilayer neural network. 4.2 Uncertain Neural Network An uncertain multilayer feed-forward neural network is constructed by adding a hid-den layer which contains the uncertain neurons between input and output layers. We call this algorithm as UNN (for uncertain neural network). Fig. 3 is an instance of the layer structure of neural network. Here, the hidden layer has a transfer function as Where, 
P(t&gt;0) will be computed based on uncertain data distribution function, For exam-ple, if the data follows Gaussian distribution, then 
The output layer can have an activation function as Sigmoid, since the output val-ues fall in the range (0,1), to represent the membership of every class. 4.3 Algorithm Analysis A straight-forward way to deal with the uncertain information is to replace the prob-tion. We call this approach AVG (for Averaging). This approach, as mentioned earlier, does not utilize valuable uncertain information and may result in loss of accu-classifying an uncertain dataset. Line L 1 and L 2 reflect the training result of the hid-den layers of a neural network. Suppose P is a test data instance and we need predict the class type of P. Because the expectation of P locates in area II, it will be assigned consider the distribution of P , it has a larger probability to be in area I than in area II. according to the probability distribution information and predicts it to be in the class which has a larger probability. In this sense, the uncertain neural network can achieve higher classification accuracy. 4.4 Network Training We adopt a Levenberg-Marquardt back propagation algorithm [14], to train this su-pervised feed-forward neural network. It requires all the activation function has a derivative. Suppose Equation (4.7) is the hidden layer activation function of the un-certain neural network, then its derivative is like: And, 
Therefore, by substituting Equation (4.8) (4.9) into Equation (4.10), we can get the activation function X  X  derivatives. When we have the derivatives of these activation functions, it is intuitive to train the network based on traditional method such as gradient decent. After training, we can then use the model for prediction for uncertain data. The hidden layer X  X  activate function, in Equation (4.7), has an output ranging between 0 and 1. When we consider two different data instances that are absolutely in the same class, their function output will both be 1. This may cause the network training to be time consuming in some scenarios. In order to improve the training efficiency, we can design new hidden layer activate functions. For example, when the uncertainty is represent by Gaussian distribution, we devise a new hidden layer activate function, as Equation (5.1) to accelerate the training process. Here F 2 (  X  ,  X  ) is continuous at u t = 0, since F (  X  ,  X  ) also has a derivative: of F 2 . Equation (5.5) then can be used in Levenberg-Marquardt back propagation training algorithm. 6.1 Experiment on Accuracy We have implemented the UNN approach using Matlab6.5[15], and applied them to 5 real data sets taken from the UCI Machine Learning Repository [16]. The results are shown in Table. 2. For the datasets except  X  X apanese Vowel X , the data uncertainty is modeled with a Gaussian distribution with a controllable parameter  X  , which is a we vary the  X  value to be 0.1, 0.3 and 0.5. For  X  X apanese Vowel X  data set, we use the uncertainty given by the original data to estimate its Gaussian distribution. 
In our experiments, we compare UNN with the AVG (Averaging) approach, which Fig 5. From the figure, we can see that UNN outperforms AVG in accuracy almost all the time. For some datasets, for example, Ionosphere and Magic Telescope datasets, UNN improves the classification accuracy by over 6% to 7%. The reason is that UNN utilizes the uncertain data distribution information and computes the probability of data being in all different classes. Theref ore, the classification and prediction process is more sophisticated and comprehensive than AVG, and has the potential to achieve higher accuracy. 6.2 Experiment on Efficiency In section 5, we have discussed an alternative activate function for improving the efficiency of network training process. Here, we present an experiment which com-pares the efficiency of two networks with different hidden layer activate functions. In this experiment, we name the network using the original function (Equation 4.7) as UNN-O, and the network using activate function (5.1) as UNN-M. 
The training time of UNN-O and UNN-M is shown in Fig. 6 (a) and the training epochs of UNN-O and UNN-M is shown in Fig 6. (b). Because of the more complex calculations in handling uncertainty, UNNs generally require more training time and epochs than AVG. However, the figures also indicate that efficiency of UNN-M is highly improved, compared with UNN-O. The training of UNN-M requires much fewer epochs than UNN-O, and is significantly faster. In this paper, we propose a new neural network (UNN) model for classifying and predicting uncertain data. We employ the probability distribution which represent the directly work on uncertain data distributions. Experiments show that UNN has higher classification accuracy than the traditional approach. The usage of probability distri-bution can increases the computational complexity, and we propose new activation function for improved efficiency. We plan to explore more classification approaches for various uncertainty models and find more efficient training algorithms in the future. 
