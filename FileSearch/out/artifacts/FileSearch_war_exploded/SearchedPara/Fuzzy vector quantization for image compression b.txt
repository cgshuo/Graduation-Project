 1. Introduction
Vector quantization (VQ) has been well studied and applied in image processing ( Atsalakis and Papamarkos, 2006 ; Bashyal and basic undertaking is to encode a set of feature vectors X  X f x R p by a set of prototypes V  X f v i 9 c implementation of VQ in grayscal e image compression is based on minimizes the following distortion measure:
D  X  1 n
The image is reconstructed by replacing each feature vector by its closest codeword. Commonly, VQ is carried out in terms objective function in order to partition the set X into a set of (1). The correspondence between the sets V and C is obvious.
In this paper we develop a fuzzy vector quantization (FVQ) tion of fuzzy clustering in the VQ design for image compression.
These problems are: (P1) Fuzzy clustering exhibits high computational cost. one; a major requirement of VQ. However, any hard interpretation may cause serious effects on the quality of the final codebook reconstructed image. of the competition between cluster centers. In general, we are interested in developing VQ algorithms that are able to increase the competition of small and badly delineated clusters.
Our intension is to provide a systematic learning process that will be able to simultaneously address all the above issues. We how they handle the above problems in the following.
A widespread classification of clustering-based VQ methods on the c -means method. The c -means is a fast process that Gary, 1992 ) and the c -means becomes very sensitive on the the LBG algorithm, which starts with the smallest codebook size and by using a splitting procedure finally obtains a codebook of LBG algorithm often finds solutions that are far away from the desired ones. In later publications, many authors managed to improve the behavior of the LBG algorithm ( Fritzke, 1997 ; Lee
When considering the case of fuzzy clustering, the picture is c -means algorithm. Fuzzy c -means offers the advantage that all the learning process with a transition strategy from fuzzy mode, mode where each training vector is clearly assigned to only one known as fuzzy learning vector quantization (FLVQ) ( Karayiannis crisp mode by incorporating special mechanisms to reduce the meter constant ( Karayiannis and Pai, 1995 ; Tsekouras, 2005 ; of the above two procedures ( Tsekouras et al., 2008 ).
Hereby, we propose a novel algorithm to deal with the afore-term. The incorporation of the c -means and the fuzzy c -means provides a trade-off between the speed and the efficiency of the algorithm. Moreover, it transfers a substantial number of train-
In addition, the competitive agglomeration term creates large propose to use a codeword migration strategy according to which, the codewords of small clusters detected above are relocated in step is used to efficient ly address the problem (P3) .
The paper is synthesized as follows: In Section 2 we provide a description of the original competitive agglomeration method. in Section 5 . 2. Competitive agglomeration
The competitive agglomeration (CA) algorithm is a powerful technique that refines good from spurious and badly delineated clusters by minimizing the following objective function ( Frigui and Krishnapuram, 1997 ): J  X  U , V , X  X  X  subject to the constraint matrix. The first component in Eq. (2) is similar to the fuzzy c -means objective function with the fuzziness parameter being equal to m  X  2. The second component is the CA term whose basic contribution is to control the number of clusters. As stated by
Frigui and Krishnapuram (1997) , if the parameter g is chosen distances, while partitioning the available data set into the iteration, the parameter g is calculated as with  X  learning process. Upon the assumption that the membership the cluster centers and the membership degrees that solve the aforementioned optimization problem are given as follows: v  X  u  X  the weighted average cardinality calculated with respect to the training vector x k @ X  C  X  X  ~ @ X  x  X  X 
The competitive agglomeration favors the creation of large given in Frigui and Krishnapuram (1997) . 3. The proposed fuzzy vector quantization algorithm
The general flow sheet of the proposed vector quantizer is two basic design steps: (a) the optimization process and (b) the codeword migration process. In the following subsections we investigation of the resulting computational complexity. 3.1. Optimization process
In Tsekouras et al. (2008) , we have developed an improved fuzzy learning vector quantizer (IFLVQ), which linearly combined shown subsequently: J  X  U , V , X  X  X  y assumption that the membership degrees do not change signifi-cantly between two adjacent iterations, the cluster centers, and the membership degrees are calculated as v  X  u  X  2  X  X  c 2  X  y  X  The analytical derivation of (11) and (12) is presented in
Appendix A . Similarly to Eq. (4), the parameter g is defined as  X  distant to x k may be assigned negative or zero membership affected by x k
I  X f v i A I  X  t 1  X  k : u ik 4 0 g X  14  X  gradually reducing its size by excluding the codewords that are assigned negative membership degrees. Thus, after some itera-mode. On the other hand, if @ ( I k ) 4 1, the vector x k fuzzy mode. For each training vector x k that remains in fuzzy mode, we calculate the fuzzy membership degrees as follows: u  X  2  X  X @ X  I k  X  2  X  y  X  if v
I k , otherwise u ik  X  0. Notice that the membership degrees of x to the codewords that have been removed from the set I k were (3). Therefore, we normalize the membership degrees as ^ u  X 
The cardinalities involved in (15) are given by the following equations: @ X  C  X  X  ~ @ X  x  X  X 
To this end, the Eqs. (11) and (13) are modified to v  X   X 
Based on the above analysis, the resulting partition includes crisp areas, the membership degrees are calculated by the next nearest neighbor condition u  X  8 &lt; :  X  21  X 
Finally, we discuss the effect of the parameter y . If we select a small value for y the learning process is dominated by fuzzy construction of clusters. The latter implies a more aggressive competition between clusters resulting in larger crisp areas. 3.2. Codeword migration strategy 3.2.1. Structure of clusters and neighborhood relations
The algorithm exploits locality in the feature space, according to which the most promising candidate codewords for migration are those with the smallest cardinalities.

Fig. 2 describes the structure of the neighborhood between C  X f x k A X : v i A I k and @ X  I k  X  X  1 g X  22  X  The fuzzy area O i of the i th cluster is O  X f x k A X : v i A I k and @ X  I k  X  4 1 g X  23  X  Thus, the cluster C i (1 r i r c ) is determined as C  X  C i [f X  O i = ^ u ik  X  : v i A I k g X  24  X 
In view of Eq. (21), it is possible for a training sample that belongs to the set C of a specific cluster to be reassigned to we expect the crisp area of a cluster to expand from inside to the k samples that belong to the set O and are closer to the borderline between C and O . However, in this case the parameter k is embedded in the algorithm as a design parameter and since obtain an appropriate value for it, if any. Therefore, we choose another way to increase the cardinalities of small clusters. The of which the v s will be migrated is symbolized as C l and the respective codeword as v l . 3.2.2. Neighborhood rearrangements in the old location of v
We can distinguish two cases: (a) the C s does not contain any can arise when the cluster is not compact, i.e. the training when the cluster is compact.
 to delete this cluster, we just remove the v s from the sets I x codewords are concerned, we leave their status unchanged. This approach is quite obvious, because every training sample that belongs to C s , also belongs to at least one of the neighboring normally. Fig. 3 illustrates the above strategy.
 When considering the second case, where C s a { | } and
O a { | }, we note that for the training samples in the set O simply perform the previously stated procedure. For the other to v s and thus, we have to ensure that their quantization is : x change the status of the samples x k A C s as follows:
I  X f v i : v i A I s 0 , 8 x k A C s g X  26  X  migration of v s . 3.2.3. Neighborhood rearrangements in the new location of v
We turn our discussion to study the steps needed to relocate the movement, the crisp areas of the neighboring clusters to C Crisp areas Fuzzy areas feature vector x l 0 that satisfies the condition : x v s : 2  X  argmax f : x k v l : 2 : x k A O l g X  27  X 
Then the new location of the small cluster is given as v  X 
The second case concerns the situation where the large cluster possesses a considerable crisp area. This situation is described found that a good choice is to select l  X  0.3.Then, we randomly the small cluster to this position. Notice that we can still use second approach is faster. In both cases we change the status of all samples that only belong to C l as follows:
I  X  I k [f v s g8 x k A C l  X  29  X 
The corresponding cardinalities are updated using @ X  I  X  X @ X  I k  X  X  1  X  30  X 
According to the above procedure we do not change the status worst case scenario, the neighboring clusters are forced to
Fig. 5 depicts the graphical representation of the above cases. 3.2.4. Criteria for the migration @  X 
Then, the criterion to select a small cluster is @ X  C
 X  r j @ C  X  32  X  where j is a positive parameter that takes a small value. In we select 0.01 r j r 0.05. On the other hand, the condition for selecting a big cluster is @ X  C  X  4 @ C  X  33  X 
The question that has to be answered is when the migration belong to the old neighborhood of the small cluster can only firmation decision. The clusters near the big cluster are not significantly influenced, because the migration focuses on the tion and by taking into account the need for a fast scheme, we decide to confirm the migration when both the updated cardin-accomplish this we run the algorithm just once using all x constraint is that a small cluster can be migrated only one time throughout the implementation of algorithm, while a big cluster can be taken into account only once within a specific iteration.
Finally, the implementation of the migration process is based on relocating in parallel all small clusters.
 Fuzzy areas 
Crisp areas Fuzzy areas 
Crisp areas 3.3. The proposed algorithm algorithmic format of the proposed fuzzy vector quantizer. ( 9 D fraction becomes smaller than e 2 . A credible selection is e and e 2  X  0.0001. 3.4. Analysis of the computational complexity
To extract helpful and safe conclusions for the computational we also provide an experimental case where we show the computa-number of distance calculations depends only on the number of Let us denote the set of training samples in fuzzy mode as elements of FM is S
Similarly, for the samples that have been transferred in crisp mode S CM  X  X  n n  X  c  X  35  X  In addition, in each iteration, the algorithm performs
S card  X  nc  X  36  X  calculations for the estimation of the cardinalities in (17). By adding up Eqs. (34) X (36) we obtain the total number of distance calculations S  X  S card  X  S CM  X  S FM  X  nc  X  X  n n  X  c  X  Since @ ( I m ( q ) ) o c , Eq. (37) gives S o nc  X  X  n n  X  c  X  Taking into account that v o n , the above condition yields S o 3 nc 2  X  38  X 
That is the computational co mplexity of the algorithm is O ( nc 4. Experimental study
To test the efficiency of the proposed method we compared it with 4 different algorithms namely, the LBG, the fuzzy learning the improved fuzzy learning vector quantization (IFLVQ) devel-titive agglomeration (CA) algorithm ( Frigui and Krishnapuram, 1997 ) is not comparable because it does not use a codebook of fixed size. For the FLVQ the fuzziness parameter was gradually decreasing from m 0  X  2to m f  X  1.1. For the FVQ the fuzziness parameter was set equal to m  X  1.5, while for the IFLVQ, the parameter y was gradually increasing from y 0  X  0.1 to y f
The above values correspond to the best performances reported in old for the migration process was fixed to j  X  0.05. The experi-mental data consisted of 16 well-known grayscale images of size 512 512 pixels, which are shown in Appendix C . For each simulation we run all algorithms using the same initializations for each codebook size and for each image. We used 15 different out the experiments, each image was divided in 4 4 blocks resulting 16,384 training vectors in the 16-dimensional feature space. The performances of the algorithms were evaluated in noise ratio (PSNR)
PSNR  X  10log 10 512 4.1. Statistical analysis and comparison in terms of the distortion measure We used the 16 images to statistically compare the FLVQ, the book size c  X  256. The statistical analysis was performed in and 3rd quartiles) for the achieved distortion measure based on the 15 sample values obtained for each method by image combination by varying the initialization points. Given that the we computed the pair-wise distortion differences of FLVQ, FVQ, and IFLVQ to our proposed method for each simulation, thus
Fig. 7 shows the boxplots of these differences. As seen in this figure our method was superior to the other 3 methods for almost occurred and those correspond to lower distortion achieved by the IFLVQ compared to our proposed method in 3 out of the 15 simulations conducted on the Parthenon image.

In order to better quantify the method and image effects accounting for the random initialization used in our simulations we used a mixed model to perform data analysis. Method, image, and the interaction between method and image were the fixed mation was used for the response.

Thus the statistical model employed for our analysis is as follows: ln  X  D where D ijk denotes the distortion for image i , using method j on simulation k , a i (1 r i r 16) is the image main effect, b effect, and e ijk is a random residual noise term, statistically independent of all p k .

For identifiably purposes the usual ANOVA zero sum con-straints are imposed on both main effects and the interaction effects. The model implies that the mean ln(distortion) achieved by method j on the i th image is m ij  X  m  X  a i  X  b j  X  ( 250 200 150 100 50 0 -50 variance and s 2 e is the residual variance. The model further assumes normality of the random initialization and residual effects. The interaction terms in the model allow differences on the above model was robustified by using the empirical estimator ( X  X  X andwich X  X  estimator) of the estimated parameter covariance matrix ( McCulloch and Searle, 2001 ), which accounts for the misspecification of the variance X  X ovariance structure of observations within each initialization. We first performed an F -test to test the image by method interaction effects in the (estimates and method comparisons) for each image separately. the original scale.

Table 2 shows the estimated ratio of medians of the 3 compet-ing methods to our method, as well as the associated confidence FLVQ)/(median Proposed) for the Gold Hill image is estimated at 1.14415 with a 95% confidence interval of (1.14101, 1.4731). Equivalently we can state with 95% confidence that, as far as medians are concerned, the FLVQ method results in an increased p -values, except one, are very small compared to a significance used for each comparison under the Bonferroni scheme that maintains the overall experiment-wise error rate at 0.05. Thus we can safely conclude that our method is superior to the other three for all images, the only exception occurring when compar-ing our method to IFLVQ on the Parthenon image.
To assess the impact of the parameter y we investigated the sensitivity of the algorithm when choosing different values for the final distortion for each of the above values. We chose to run the 5 algorithms for codebooks of sizes c  X  128 and c  X  512. Tables 3 and 4 depict the results of these simulations.
The distortion mean values achieved by different values of y are all very close to each other in every experimental case. This been reduced. 4.3. Effect of the codebook size on the PSNR In this experiment, we used the Airplane, Girlface, Lena and
Peppers images to generate codebooks of sizes c  X  2 qb ( qb  X  5, 6, y , 10). Since each feature vector represents a block of 16 pixel (bpp).

Tables 5 X 8 summarize the mean PSNR values obtained by the five algorithms. The results reported in these tables are highly convincing, since in all cases the proposed method outperformed the others. 4.4. Image quality evaluation
In this experimental case we used the best PSNR values obtained images. To perform the experiment we employed the Lena and plane image were 30.0637 for the FVQ algorithm, 30.4098 for the
IFLVQ method and 30.6883 for the proposed vector quantizer. 4.5. Computational demands
To perform the simulation, we q uantified the computational algorithm for y  X  0.5. We used the Lena image to generate code-needed by the CPU in seconds per iteration.
 We ran all algorithms using a computer machine with double core algorithm maintain similar computational performances. 4.6. Study of the transition from fuzzy to crisp mode
To study the transition from fuzzy to crisp mode we performed two experiments, using the Lena image. In the first one, we monitored the number of training vectors transferred in crisp this experiment. According to this figure, smaller values of y obtain smaller number of training vectors in crisp mode, meaning that the partition mainly consists of fuzzy areas. Contrary, for
To conduct the second experiment we defined the quantity: @ sample (or samples) affecting the maximum number of code-words. Therefore, @ max measures the maximum number of dis-tance calculations among the training vectors in each iteration.
Fig. 13 illustrates the simulations for this experiment. As expected, for small values of y we get larger value for @ the learning process proceeds and therefore, they strongly sup-port the conclusions obtained in the previous section. 4.7. Literature comparison
In this section we present a comparison between the best performances obtained here and the respective performances of
The comparison is based on the Lena image because it is the most used testing image in the literature.

Table 9 depicts the comparative results. As far as this table 5. Conclusions
We have introduced a novel fuzzy vector quantizer for image compression. There are two basic design facets of the proposed algorithm. The first concerns the minimization of a specialized namely, the c -means, the fuzzy c -means and the competitive agglomeration. The c -means along with the fuzzy c -means act to create clusters that possess crisp and fuzzy areas. This task is accomplished via the utilization of a specialized strategy that performed by the algorithm. On the other hand, the competitive shrinksthesmallonesuntiltheirsizesbecomesmallerthana predefined threshold. The second facet concerns the development of a novel codeword migration technique, according to which the small clusters are not discarded but rather their codewords are migrated in the neighborhoods of large clusters. This migration one. The effect of this procedure determines the whole learning vector quantization scheme that reduces the computational com-experimental cases that concern grayscale image compression. The main results are summarized in the following remarks: (1) The (2) In accordance with the theore tical analysis, the algorithm clustering and therefore, the reconstructed images maintain high This fact is strongly supported by the comparative analysis. Acknowledgements
The authors are grateful to the anonymous reviewers for the helpful comments. 2000 4000 6000 8000 10000 12000 14000
Vectors in crisp mode 50 100 150 200 250
Nmax Appendix A Herein, we derive the Eqs. (11) and (12). We first look at @ J  X  U , V , X  X  @ v get Eq. (11).

To derive Eq. (12), we take into account the constraint (3) and using the Lagrange multipliers l k (1 r k r n ) we arrive at
F  X  J , l  X  X  y
The partial derivative of F ( J , l k ) with respect to u @ F  X  J , l @ u
Setting (A.2) equal to zero and solving for u ik we get u  X  l k = 2 1 y l k = 2 1 y ! which can be rewritten as l 2
Solving the above equation for l k /2 we obtain l 2  X 
If we define ~ @ X  x k  X  X  l 2  X  Substituting (A.4) into (A.3) we get Eq.(12).
 Appendix B
In this appendix we derive the analytical formula to update the membership degrees to take negative or zero values. For a better distance : x k v i : 2 : x
It thus appears that whenever the condition (B.1) is true, the respective membership degree is positive, otherwise negative or meaning, we can safely assume that the codewords not satisfying : x updated according to following formula I k  X f v i A I Appendix C: Testing images See Fig. C1 .
 References
