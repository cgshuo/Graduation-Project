 In applications that require interaction with the physical world, such as location-based services [6] and sensor monitoring [3], data uncertainty is an inherent property due to measurement inaccuracy, sampling discrepancy, outdated data sources, or other er-rors. Although much research effort has been directed towards the management of uncertain data in databases, few researchers have addressed the issue of mining uncer-tain data. We note that with uncertainty, data values are no longer atomic. To apply traditional data mining techniques, uncertain data has to be summarized into atomic values. Unfortunately, discrepancy in the summarized recorded values and the actual values could seriously affect the quality of the mining results. Figure 1 illustrates this problem when a clustering algorithm is applied to moving objects with location un-certainty. If we solely rely on the recorded values, many objects could possibly be put into wrong clusters. Even worse, each member of a cluster would change the cluster centroids, thus resulting in more errors. 
We suggest incorporating uncertainty information, such as the probability density functions (pdf) of uncertain data, into existing data mining methods so that the mining results could resemble closer to the results obtained as if actual data were used in the mining process [2]. In this paper we study how uncertainty can be incorporated in data mining by using data clustering as a motivating example. In particular, we study one of the most popular clustering methods  X  K-means clustering. There is significant research interest in data uncertainty management in recent years. Most work has been devoted to  X  X mprecise queries X , which provide probabilistic guar-antees over correctness of answers. For exam ple, in [4], indexing solutions for range queries over uncertain data have been proposed. The same authors also proposed solutions for aggregate queries such as neares t-neighbor queries in [3]. Notice that all these works have applied the study of uncertain data management to simple database queries, instead of to the more complicated data analysis and mining problems. 
Clusterization has been well studied in data mining research. However, only a few studies on data mining or data clustering for uncertain data have been reported. Ham-dan and Govaert have addressed the problem of fitting mixture densities to uncertain data for clustering using the EM algorithm [5]. However, the model cannot be readily applied to other clustering algorithms and is rather customized for EM. Clustering on interval data also has been studied. However, the pdf of the interval is not taken into account in most of the metrics used. Another related area of research is fuzzy cluster-ing. In fuzzy clustering, a cluster is repres ented by a fuzzy subset of a set of objects. Each object has a  X  X egree of belongingness X  for each cluster. In other words, an ob-ject can belong to more than one cluster, each with a different degree. The fuzzy c -means algorithm was one of the most widely used fuzzy clustering method [1]. representing the attribute values of all the r ecords in the clustering application. Each record o i is associated with a probability density function (pdf), f i ( x ), which is the pdf of o i  X  X  attribute values x at time t . The clustering problem is to find a set C of clusters C algorithms have different objective functions, but the general idea is to minimize the distance between objects in the same cluster while maximizing the distance between objects in different clusters. Minimization of intra-cluster distance can also be viewed the cluster C j that x i is assigned to. To consider data uncertainty in the clustering process, we propose a clustering algo-rithm with the goal of minimizing the expected sum of squared errors E (SSE). Note that a data object x i is specified by an uncertainty region with an uncertainty pdf f ( x i ). Given a set of clusters, C j  X  X  the expected SSE can be calculated as follow: where || . || is a distance metric between a data point x i and a cluster mean c j . 
Cluster means are given by: We propose a new K-means algorithm, called UK-means, for clustering uncertain data: The main difference between UK-mean clustering and the traditional K-means clus-tering lies in the computation of distance and clusters. In particular, UK-means com-pute the expected distance and cluster centroids based on the data uncertainty model. Convergence can be defined based on different criteria. In Step 4, it is often difficult to determine E (|| c j -x i ||) algebraically. In particular, the variety of geometric shapes of uncertainty regions (e.g., line, circle) and different uncertainty pdf imply that numerical integration methods are necessary. We propose to use the squared expected distance E (|| c j -x i || 2 ), which is much easier to obtain. The UK-means algorithm presented in the last section is applicable to any uncertainty region and pdf. In this section, we describe how the proposed algorithm can be ap-plied to uncertainty models specific to moving objects that are moving in a two-dimensional space. According to [4] and [6], there are two types of moving-object uncertainty, namely line-moving uncertainty and free-moving uncertainty. In line-moving uncertainty, an object moves at a velocity vector, which is smaller than V max , along a fixed direction. Line-moving uncertainty can be unidirectional or bidirec-tional. The free-moving uncertainty model assumes that an object cannot move be-yond a certain speed, V max . Given that the current position of the object is ( h , k ) at time t , the object X  X  location is uniformly distributed within a circle of radius V max  X  ( t -t 0 ). 
Suppose we have a centroid c = ( p , q ) and a data object x specified by a line uncer-tainty region with a uniform distribution. Let the end points of the line segment uncer-If f(t) is uniform, then f(t) = 1 , and the above becomes: For free-moving uncertainty, suppose we have a centroid c = ( p , q ) and a data object x specified by a circle uncertainty region with a uniform distribution. Suppose the circle f ( r ,  X  ). Then we have: where A = 2 r ( h -p ), B = 2 r ( k -q ), C = r 2 + ( h -p ) 2 + ( k -q ) 2 
We are thus able to compute the expected squared distance easily for line-moving and free-moving object uncertainty. The use of uniform distribution is only a specific example here. When the pdf X  X  are not uniform (e.g., Gaussian), sampling techniques can be used to estimate E (|| c j -x i ||). In our experiments, we simulate a scenario in which a system that tracks the locations data is stored in a set called recorded . Each object assumes an uncertainty model captured in uncertainty . We compare two clustering approaches: (1) apply K-means to recorded and (2) apply UK-means to recorded + uncertainty . We first generated a set of random data points in a 100 x 100 2D space as recorded . For each data point, we then randomly generated its uncertainty according to a chosen uncertainty model. We also generated actual  X  the actual locations of the objects based on recorded and uncertainty , simulating the scenario that the objects have moved away from their original locations as registered in recorded . We remark that ideally, a system should know actual and apply K-means on the actual locations. Hence, we compute and compare the cluster outputs of the following data sets: (1) recorded (using classicial K-means) (2) recorded + uncertainty (using UK-means) (3) actual (using classical K-means) We use the Adjusted Rand Index (ARI) to measure the similarity between the cluster-ing results [7]. A higher ARI value indicates a higher degree of similarity between two sets of clusters. We compare the ARI between the sets of clusters created in (2) and (3) and the ARI between those created in (1) and (3). Due to limited space, only the results of unidirectional line uncertainty are reported here. 
The number of objects ( n ), number of clusters ( K ), and the maximum distance an object can move ( d ) were varied during the experiment. Table 1 shows the different experiment results by varying d while keeping n = 1000 and K = 20. Under each set of different parameter settings, 500 rounds were run and the results were averaged. In each round, the sets of recorded , uncertainty , and actual were first generated and the same set of data was used for the three clustering processes. The same set of initial centroids were also used in each of the three processes in order to avoid any bias. 
The UK-means algorithm consistently showed a higher ARI than the traditional K-means algorithm applied on the recorded data. Pairwise t -tests were conducted and the results showed that the difference in the ARI values of the two methods was signifi-cant ( p &lt; 0.000001 for all cases). The results demonstrated that the UK-means algo-would be produced if the real-world data were available. In this paper we present the UK-means algorithm, which aims at improving the accu-racy of clustering by considering the uncertainty associated with data. Although in this paper we only present clustering algorithms for uncertain data with uniform dis-tribution, the model can be generalized to other distribution (e.g., by using sampling techniques). We also suggest that our concept of using expected distance could be applied to other clustering approaches (such as nearest neighbor clustering and self-organizing maps) and other data mining techniques (such as data classification). We thank David Cheung (University of Hong Kong), Edward Hung (Hong Kong Polytechnic University) and Kevin Yip (Yale University) for their helpful comments. 
