 Zhilin Zhang, Huaizhong Lin, Pengfei Li, Huazhong Wang, and Dongming Lu With the exponential growth of online documents in the World Wide Web, the task of assigning natural language text documents on the basis of their con-tents to one or more predefined categories or classes have become interesting for the text mining research community. This task is commonly referred to as text classification[18]. However, the major bottleneck of most existing text classifica-tion algorithms is that they require sufficient labeling of documents. Labeling is a task usually done manually, which is expensive, time consuming and error prone. At the same time, there are often abundant unlabeled documents that are much easier to obtain. How to effectively use unl abeled samples in text classification field has become an active research problem. The general problem of exploiting unlabeled data to improve classification a ccuracy leads to semi-supervised text classification.

Previous researches have come up with various methods that aim to reduce efforts for labeling tasks. Some studies proposed models that learn from labeled and unlabeled data, such as transductive support vector machines (TSVM) [13] and Naive Bayes with EM. An alternative strategy that employs clustering meth-ods in text classification is clustering based classification[8],[9],[10],[16]. And it has become an interesting problem for the text mining research community.
However, previous methods archived lim ited improvement because they repre-sented the document as the vector of terms which appear in the document. Some literatures[1],[2],[3],[4],[5],[6],[11] employed external knowledge such as Wikipedia to enrich a document with semantic features. But the authors still adopted the cosine similarity measure when computing the similarity between documents, which ignores the semantic relationships between Wikipedia features. In this pa-per, we address this problem by proposing a new similarity measure considering both the term frequency and wikipedia features.

Our contributions can be summarized as follows. (1) We enrich documents by mining semantic relationships in Wikipedia and present a new document similarity measure . (2) We incorporate the knowledge obtained from Wikipedia and use our proposed similarity measure in semi-supervised text classification to achieve increased accuracy. (3) We conduct comprehensive experiments to validate our approach and study related issues. The rest of the paper is organized as follows: Section 2 reviews related wo rk. Section 3 introduces our proposed method of utilizing Wikipedia semantic features to improve semi-supervised text classification. In Section 4, we present and discuss the experimental results. Finally, we conclude the paper in section 5. 2.1 Semi-supervised Text Classification Recently, there is a growing amount of res earches in semi-supervised text clas-sification and different methods are proposed according to different view of un-labeled data, such as transductive support vector machines (TSVM) [13], Naive Bayes with EM[14],[15] and clustering based classification[8],[9],[10],[16].
TSVM[13] adds a constraint to the SVM optimization function in order to preserve the margin over unknown test labels. It expects to find a low-density area of data and constructs a linear separator in this area so that the margin over both the labeled data and the unlab eled data can be maximized. Nigram et al.[14] combined EM with Naive Bayes to construct a model with the unlabeled data. Their experimental results showed improved performance over supervised classifiers.

Another strategy is using clustering to boost classification [8]. In this method a clustering algorithm is applied to both the labeled and unlabeled data. Then the labels of the labeled data samples are propagated to the unlabeled samples which are closest to cluster centroids. Finally, some classification methods are used to build the classifier with the expanded training dataset. Experiment results demonstrate that this method outperforms existing algorithms such as TSVM and Naive Bayes with EM.
 2.2 Utilization of Wikipedia Nowadays, people focus on enriching doc ument representation with semantic features exploited from Wikipedia. L iteratures focus o n some directions.
One is how to fully mine semantic relation between words. According to the previous researches, we can find out synonyms by redirect links, polysemous words by the disambiguation page, hypernym by hierarchical relations between concepts and categories and association relations by internal page links from Wikipedia.

As for the problem of searching candidate Wikipedia features from text doc-uments, Gabrilovich [11] matched documents with the most relevant articles of Wikipedia by an auxiliary text classifier, and then augmented the bag of words representation with concep ts represented by the relevant Wikipedia articles. Hu [1] presented two approaches, the exact match and the relatedness match, to map text documents to Wikipedia concepts and categories. Wang [3] adopted the forward maximum matching and the window filtering condition method to search candidate concepts. He then made disambiguation by using text similarity and context information.

Semantic features extracted from Wik ipedia can be utilized to enhance text mining tasks, such as text classification [3],[5],[11], document clustering [1], QA [2] and online advertising [6]. However, few works have been reported about semi-supervised text classification. In this section, we first introduce Wikipedia. Then we describe the methods of searching candidate concepts in text documents. Finally, we enrich text docu-ments with concepts and categories.

As the largest encyclopedia in the world, Wikipedia surpasses other knowledge bases due to its coverage of concepts, rich semantic knowledge and up-to-date content. The title of each article is defined as a Wikipedia concept, which is a concise, well-formed phrase and describes a single topic. Many meaningful relation structures can be mined from Wi kipedia, such as synonym, polysemy, associative relation and categories . 3.1 Searching Candidate Concepts To enrich text documents with Wikipedia knowledge, the first issue is to search the candidate concepts from the document and filter out the noise concepts to get the most promising ones. Wikipedia has s uch a large scale of concepts that every document phrase can be mapped to at least one article and most phrases mapped to several. To decrease meaningless mappings and deal with word ambiguities, we build a phrase index which includes the phrases of Wikipedia concepts, their synonym, polysemy and categories in the Wikipedia thesaurus. Based on the generated Wikipedia phrases index, we use the maximum matching algorithm with the window filtering condition to search candidate concepts followed with Pu [5]. The window filtering condition means that every word of a candidate concept must appear in the sequence wi thin a window of certain length, which guarantees all words of a concept appea r in a sequence within certain distance. We adopt two strategies presented in [5] to do word sense disambiguation: the first one is to utilize text similarity for disambiguation; the second one is to disambiguate with context. 3.2 Enrichement of Text Document with Concepts Each Wikipedia article contains a lot of hyperlinks, which express semantic re-lationships between concepts. Hu [4] in troduced two measures to quantify the strength of associative relations between concepts: content-based s tf  X  idf ,out-link category-based s olc . The measure s tf  X  idf reflects the cosine similarity of article pairs in Wikipedia. The measure s olc compares the cosine similarity be-tween the two articles X  out-linked cate gory vectors. The vector is constructed by the categories that out-linked articles of the original article belong to. Combine the two measures linearly to compute t he relatedness between concepts: For synonyms, Wikipedia guarantees that there is only one article for those concepts, and these equivalent concep ts are grouped to the preferred concepts by redirect hyperlink. For instance, the preferred concept of U.S.A, U.S, USA and United States is United St ates. As Hu [1] pointed out that synonymies of a given concept cannot be ranked, and if all its synonymies are added into documents, the recall performance may be sometimes improved, but the precision of clustering will be decreased. Also, if many less rel ated concepts be added, the precision of clustering will be reduced. In our experiment, only the preferred concept and the top 5 most relevant concepts are appended to document for each identified concept and the parameter  X  is set to 0.5. 3.3 Enrichement of Text Document with Categories In Wikipedia, both concepts and categories belong to more than one category, which form a directed acyclic graph G =( V,E ). V = { v 1 ,v 2 , ...v n } is the col-lections of vertexes contained in G , where the vertex v i = { concept | category } graph and each edge concatenates a pair o f vertexes, which indicates that there is hierarchical relationship be tween them. For the two vertexes v i and v j ,the less vertexes along the shortest path be tween the two vertexes, the more close of their semantic relatedness. For exa mple, the distance is only one between concept and its ancestor category. The f ormula[19] that measures the degree of the semantic relatedness between concep t and category or between categories is defined as follows: Where, sl ( v i ,v j ) is the number of vertexes along the shortest path between v i and v j ,and L is the maximum depth of the graph.

A concept belongs to a few levels of categor y. Intuitively, those high level cate-gories have less influence than those low level categories since low level categories are more specific and therefore can depic t the articles more accurately. Lots of categories may bring some noise and reduce the precision of clustering. Accord-ing the experiment results in [1], we app end only the direct ancestor category to documents. In this section, We first describe our improvement in measuring the similar-ity between documents enriched with wikipedia knowledge. Then we improve the performance of clustering based classification using our proposed similarity measure. 4.1 Improvement in Similarity Measure Traditional Similarity Measure. We define D = { D l ,D u } as the training dataset which has been enriched with Wikipedia features including concepts and categories. D l is the labeled dataset, while D u is the unlabeled dataset, T is the set of all different terms occurring in D . Firstly, stopwords are removed from T using a standard stopwords list 1 . Then words in each document are stemmed using the Porter stemmer 2 .Let T = { t 1 ,t 2 , ..., t n } .Eachdocument d  X  D can be represented as a term vector us ing the vector space model, means d which is defined as TF*IDF formula: For d i ,d j  X  D ,their term frequency based similarity measure is defined as: Although d i , d j have been enriched with Wikipedia features, the semantic rele-vance between Wikipedia features is ignored if the cosine similarity measure in formula (4) is used to compute the document similarity. Wikipedia Feature Based Similarity Measure. Define C i as the set of Wikipedia concepts occurring in document d i and Cat i as the set of Wikipedia set of Wikipedia features that have been added to the document d i .Theoverall semantic similarity of Wikipedia features in two documents is defined as follows: Where, rel ( a p ,a q ) represents the semantic relatedness between two Wikipedia features. We define it as follows: Where, if a p = a q , rel ( a p ,a q ) is equal with 1, which represents the highest relatedness. If a p = a q and a p , a q are both concepts in d i and d j ,thenformula (1) is used to calculate the semantic relatedness of the two concepts. Otherwise, either a p or a q is a category in the document. In this case, rel ( a p ,a q )canbe measured by formula (2).

Consequently, the Wikipedia feature based similarity measure of two docu-ments d i ,d j  X  D is defined as: Combined Similarity Measure. Finally, to measure the similarity between documents that have been enriched with Wikipedia features, we use a linear combination of the term frequency based similarity measure and the Wikipedia feature based similarity measure, which is: Where,  X  is used to adjust the weight of two parts. We will demonstrate the influence of parameter changes on the classification performance later in our experiment. 4.2 Clustering Based Classification Using Wikipedia Knowledge Clustering to Expand the Labeled Dataset. Although clustering is an un-supervised task, labeled documents can be used to improve clustering accuracy. K-medoids algorithm aims to partition the observations into k sets, so as to min-imize the within-cluster sum of squares . We use it as the clustering algorithm. Further, we can guide the clustering process with the labeled documents by set-ting the initial parameters. Followed with [8], the k value is set to the number of classes in the labeled data and the initial centroid of each cluster is calculated by  X  i = avg ( measure and run k-medoids on both labeled and unlabeled data. The clustering process is terminated when the result doesn X  X  change anymore, or just before a labeled centroid will be assigned to a wrong cluster.

After the clustering step, we propagate the labels of the labeled data samples to the unlabeled samples which are closest to cluster centroid. To avoid importing too much noise, we define a threshold  X  . If unlabeled document d j satisfies the as labeled data with high confidence. As a result, the scale of labeled data is expanded.
 Classification with Expanded Labeled Dataset. We run TSVM to train a classifier in the expanded labeled data and remaining unlabeled data. To make a comparison, TSVM is also used in the original dataset D as a baseline. 5.1 Dataset The Wikipedia database is downloaded from http://download.wikipedia.org, which releases its database dumps periodically . We got about 832,553 articles and 29000 categories after pre-processing and filtering.

Reuters-21578 (Reuters) is the most widely used text collection for text classi-fication. We use the ModApte 3 split to form the training set and test set, where there are 7,769 training examples and 3,019 test examples. After selecting the biggest ten classes: earn, acq, money-fx, gr ain, crude, trade, interest, ship, wheat, and corn, we get 6,649 training examples and 2,545 test examples. 20 newsgroups data set is collected by Ken Lang 4 . This is a well-balanced data set and consists of 18828 articles spread almost evenly over 20 different categories. The version of the data set used has no duplicates, and most of the headers are removed. 10 categorie s are selected randomly to be used in our experiment. Each category has approximate 1000 articles. 5.2 Performance Measures For evaluating the effectiveness of classification, we use the standard recall, pre-cision and F1 measure. Recall is defined t o be the ratio of correct assignments by the system divided by the total numb er of correct assignments. Precision is the ratio of correct assignments by the system divided by the total number of the system X  X  assignments. The F1 Measu re could be used to combine precision and recall into a unified measure. There are two ways to measure the average F1 of a binary classifier over multi categories, namely, the macro-averaging and micro-averaging. The former way is to first compute F1 for each category and then average them, while the later way i s to first compute precision and recall for all categories and use them to calculate F1. Because the micro-averaging of F1 is in fact a weighted average over all classes and is more plausible for highly unevenly distributed classes, we use this measure in the following experiments. 5.3 Overall Performance We implement three experiments on the 20newsgroup dataset. The results are shown in Fig. 1. We first run the clustering based classification algorithm on the original dataset as the baseline. After enriching the dataset, we choose the traditional similarity measure as shown in formula (3) in the clustering step. Finally, we use our proposed similarity measure as shown in formula (8) in the clustering step. The results are shown in Fig. 1 as  X  X ASE X ,  X  X r W X  and  X  X ew W X  respectively.

As can be seen from Fig. 1, our proposed algorithm performs better on the dataset with Wikipedia knowledge than on the original dataset, so do our pro-posed similarity measure than the traditional measure especially when the num-ber of the labeled data is very small. For example, when the number of the labeled data in all training dataset is 40, our proposed algorithm achieve 7.5% improvement.

Similar results can be found on Reuters-21578 as shown in Fig. 2. By us-ing Wikipedia knowledge, our proposed algorithm still outperforms when the number of the labeled data is very small.
 Thus from Fig. 1 and Fig. 2, it is possible to conclude that when using Wikipedia knowledge, the algorithm preforms better, especially when the num-ber of labeled sample is small. Another conclusion can be made that our proposed similarity measure can help the clustering algorithm to find more appropriate la-bels for the unlabeled documents, then use these to expand the training dataset.
However, with the increasing number of labeled data, the performance of clus-tering based classification grows slowly, and sometimes drops. This is because when the number of labeled samples exceed a certain range, th e original train-ing dataset is sufficient for classification and expanding training dataset with unlabeled samples in the clustering step may introduce noise. 5.4 Clustering Performance We evaluate the clustering performan ce by the purity measure which is based on precision measure in information retrieval field. We run k-medoids algorithm with the traditional similarity as the baseline. Then the combined similarity measure is implemented instead. Table 1 shows the clustering performance on the Reuters-21578 and 20newsgroup.
From Table1, we can see that our improved similarity measure performs better than the traditional similarity measure. The precision of the clustering is very high when the number of labeled documents is relatively less, since the number of the most confident unlabeled documents which are viewed as labeled documents is small. As the number of labeled documents becomes larger, the precision will be reduced. 5.5 Parameter Tuning We then discuss the relationship between the parameter and the classification performance.To observe the significant improvement in performance, we run this experiment when the number of labeled samples is small.

The value of  X  is set to 0.1, 0.2, ..., 1.0 respectively to tuned the similarity measure, and various classifier are build. Then we observe the classification per-formance in the test dataset. Fig. 3 shows the performance with various values of  X  on 20newsgroup collection.

We can see that the best performance appears when  X  is near 0.6. Out of our expectation, when  X  is very small ,for example  X  =0, the performance is worse than the baseline which demonstrates that Wikipedia features cannot completely take place of the term vector to represent the document. This paper presents a method to improve semi-supervised text classification performance by using Wikipedia semantic knowledge. First, we map text doc-uments to Wikipedia concepts, make word sense disambiguation to overlapping ambiguity and polysemous ambiguity, and enrich documents with Wikipedia fea-tures (synonyms, hyponyms and associat ive concepts). Then Wikipedia feature based similarity measure is proposed to compute the semantic similarity of added Wikipedia features in documents. By combining term frequency based similar-ity measure and Wikipedia feature based similarity measure in the clustering step, we can label more appropriate unlabeled documents to expand training dataset. Experiment results show that this new method can boost classifica-tion performance even when the number of labeled samples is small. For future works, more information in Wikipedia can be minded, as Wikipedia contains so much information, and our method only explores part of them. To evaluate the wide suitability and effectiveness of our new proposed similarity measure, more experiments need to be done.
 Acknowledgments. This work was supported in part by the national science and technology support program grant 2012BAH03F02 and 2013BAH62F02, the Public Technology Research Program of Zhejiang Province grant 2010C33151, Science and technology innovation team of Zhejiang Province grant 2010R50040, the cultural relic protection science an d technology project of Zhejiang Province.
