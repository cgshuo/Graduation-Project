 Text categorization is the process of groupi ng texts into one or more predefined cate-gories based on their content. Due to the in creased availability of documents in digital form and the rapid growth of online information, text categorization has become one of the key techniques for handling and organizing text data. 
A major difficulty of text categorization is the high dimensionality of the original seriously projected and carefully investigated. 
In recent years, a growing number of statistical classification methods and machine such as document frequency thresholding, information gain measure, mutual informa-tion measure,  X  2 statistic measure, and term strength measure have been widely used. 
DF thresholding, almost the simplest method with the lowest cost in computation, has shown to behave comparably well when compared to more sophisticated statisti-cal measures [13], it can be reliably used instead of IG or CHI while the computation better performance in Chinese text categorization [1][11] than IG, MI and CHI. In one word, DF, though very simple, is one of the best feature selection methods either for Chinese or English text categorization. ments[7][4][2][6]. However, this method is only based on an empirical assumption that considered to be a very important factor for feature selection[12]. Rough Set theory, knowledge is considered as an ability to partition objects. We then edge quantity. We use the knowledge quantity of the terms to rank them, and then put ments show the improved method has notable improvement in the performances than the original DF. 2.1 Document Frequency Thresholding A term X  X  document frequency is the number of documents in which the term occurs in the whole collection. DF thresholding is computing the document frequency for each frequency are less than some predetermined threshold. That is to say, only the terms complexity approximately linear in the number of training documents. 
At the same time, DF is based on a basic assumption that rare terms are noninfor-formation retrieval (IR), where the terms with less document frequency are the most informative ones [9]. 2.2 Basic Concepts of Rough Set Theory fields, such as machine learning, knowledge acquisition, decision analysis, knowledge discovery from databases, expert systems, pa ttern recognition, etc. In this section, we introduce some basic concepts of rough set theory which used in this paper. 
Given two sets U and A, where U ={x 1 , ..., x n } is a nonempty finite set of objects domain of values of A, V a is the set of values of a, defining an information function f value of attribute a for object x . 
Any subset B  X  A determines a binary relation Ind(B) on U, called indiscemibility relation: The family of all equivalence classes of Ind(B), namely the partition determined by B, B -elementary sets. be defined as: 3.1 The Ability to Discern Objects The important concept in rough set theory is indiscernibility relation. For example, in Table 1, (D 1 , D 2 ) is T 1 -indiscernible, (D 1 , D 3 ) is not T 1 -indiscernible. D D ability of discerning objects as knowledge quantity . 3.2 Knowledge Quantity This section will be discussed on information table (Let decision feature set D =  X  ). Definition 1. The object domain set U is divided into m equivalence classes by the set W
U , P =W  X  n 1  X  n 2  X  ...  X  n m  X  , and it satisfies the following conditions: 1) W(1,1) = 1 2) if m = 1 then W(n 1 )=W(n) = 0 4) W  X  n 1 ,n 2 , ... ,n m  X  = W  X  n 1 ,n 2 + ... +n m  X  + W  X  n 2 , ... ,n m  X  5) W  X  n 1 ,n 2 +n 3  X  = W  X  n 1 ,n 2  X  + W  X  n 1 ,n 3  X  Conclusion 1. If the domain U is divided into m equivalence classes by some feature knowledge quantity of P is: W (n 1 ,n 2 ,...,n m )= 3.3 Interpretation of Document Frequency Thresholding n (the knowledge quantity of T 1 ) is: ments in U, m demotes the number of documents in which term t occurs, the knowl-edge quantity of t is defined to be: m=DF When m  X  n/2, DF t U W ,  X 
After stop words removal, stemming, and converting to lower case, almost all term X  X  DF value is less than n/2. corpus and remove those terms whose knowledge quantity are less than some predetermined threshold, this is our Rough set-based feature selection method which the same as selected by this RS method. This is an interpretation of DF method. there are 3 equivalence classes in our method. (1  X  i  X  k), V={0,1,2}, defining an information function f, U  X  V: An example of such an information system is given in Table 2. more than once in D 5 , the document frequency of term T 1 and T 2 is the same, but n number of documents which t occurs only once, n 3 denotes the number of documents which t occurs at least twice. The knowledge quantity of t is defined as: (2) can be changed to: weight for multiple occurrences of a term. frequency-based document frequency(TFDF). and Na X ve Bayes classifier. We use kNN, which is one of the top-performing classifi-ers, evaluations [14] have shown that it outperforms nearly all the other systems, and inductive learning algorithms for classi fication [16]. According to [15], micro-averaging precision was widely used in Cross-Method comparisons, here we adopt it to evaluate the performance of different feature selection methods. 5.1 Data Collections Two corpora are used in our experiments: Reuters-21578 collection[17] and the OHSUMED collection[19]. 
The Reuters-21578 collection is the original Reuters-22173 with 595 documents which are exact duplicates removed, and has become a new benchmark lately in text categorization evaluations. There are 21578 documents in the full collection, less than half of the documents have human assigned topic labels. In our experiment, we only documents. The training set has 5273 documents, the testing set has 1767 documents. The vocabulary number is 13961 words after stop words removal, stemming, and converting to lower case. 
OHSUMED is a bibliographical document collection. The documents were manually 1800 categories defined in MeSH, and 14321 categories present in the OHSUMED document collection. We used a subset of this document collection. 7445 documents as unique terms in the training set and 10 categories in this document collection. 5.2 Experimental Setting used as our experimental platform. 5.3 Results Figure 1 shows that the Average precision of KNN and Na X ve Bayes varying with the that when c  X  12, as c increases, the Average precision increases accordingly. Reauters-21578 after feature selection DF and TFDF(c=10). We can note from figure 2 reduction, it is notable that TFDF prevalently outperform DF((2-a),(3-a)). Figure 4 and Figure 5 exhibit the performance curves of kNN and Na X ve Bayes on OHSUMED after feature selection DF and TFDF(c=10). We can also note from fig-ure 2 and figure 3 that TFDF outperform DF methods, specially, on extremely aggres-sive reduction, it is notable that TFDF prevalently outperform DF((4-a),(5-a)). almost the simplest method with the lowest cost in computation, has shown to behave well when compared to more sophisticated statistical measures, However, DF method interpretation, and it does not consider Term Frequency (TF) factor, in this paper: we put forward an extended DF method called TFDF which combines the Term Frequency (TF) factor. Experiments on Reuters-21578 collection and OHSUMED collection show that TFDF perform much better than the original DF method, for feature selection. 
Many other feature selection methods such as information gain measure, mutual methods. Acknowledgments. This work is supported by the National Natural Science Funda-mental Research Project of China (60473002, 60603094), the National 973 Project of China (2004CB318109)and the National Natural Science Fundamental Research Project of Beijing (4051004). 
