 With regard to keyword search systems for structured data, research during the past decade has largely focused on per-formance. Researchers have validated their work using ad hoc experiments that may not reflect real-world workloads. We illustrate the wide deviation in existing evaluations and present an evaluation framework designed to validate the next decade of research in this field. Our comparison of 9 state-of-the-art keyword search systems contradicts the re-trieval effectiveness purported by existing evaluations and reinforces the need for standardized evaluation. Our results also suggest that there remains considerable room for im-provement in this field. We found that many techniques cannot scale to even moderately-sized datasets that contain roughly a million tuples. Given that existing databases are considerably larger than this threshold, our results motivate the creation of new algorithms and indexing techniques that scale to meet both current and future workloads.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation ; H.2.4 [ Database Management ]: Systems X  Relational databases Experimentation, Standardization Evaluation, keyword search
Numerous researchers have proposed keyword search strategies for structured data, which includes semi-structured documents (e.g., XML) and information stored in relational databases. This push reflects Internet users X  increasing reliance on keyword search and also reflects a de-sire to hide underlying data representations and to eliminate complex query languages from end users. To the best of our knowledge, none of these proposed systems has reached mainstream use. One potential barrier to deploying these systems is the ad hoc evaluations performed by researchers.
The history of the information retrieval (IR) community illustrates the importance of standardized evaluation. Sing-hal [28] states,  X  X  system for experimentation coupled with good evaluation methodology allowed rapid progress in the field and paved [the] way for many critical developments. X  The Text REtreival Conference (TREC) testifies to the im-pact of standardized evaluation, for search effectiveness dou-bled within six years of its inception [30].

The INitiative for the Evaluation of XML retrieval (INEX) workshop [10] established standardized evaluation proce-dures for XML retrieval. Despite the similarity of key-word search in semi-structured data and relational data, relational keyword search systems have not been evaluated at this venue. Perhaps researchers see evaluation forums such as INEX as too expensive to validate experimental sys-tem designs, but standardized evaluation is essential for real progress. The strategic step of creating a DB&amp;IR evalua-tion forum has yet to occur. Without it, progress will not match that of the larger IR community. In the interim (and as also suggested by Webber [32]), the community should coalesce behind a standardized set of datasets and queries for evaluating search systems X  X e describe such a frame-work in this paper. According to Chen et al. [3],  X  X ontri-butions from the research community are highly demanded for developing comprehensive frameworks for evaluating the retrieval and ranking strategies of keyword search on various structured data models. X  Our evaluation framework enables direct comparison of the effectiveness and performance of different search techniques. In the remainder of this section, we illustrate the difficulties inherent to searching structured data and also present the contributions of this work.
The ubiquitous search textbox has transformed the way people interact with information. Despite the wide-ranging success of Internet search engines in making information accessible, searching structured data remains a challenge. Both semi-structured and relational data introduce chal-lenges not encountered in unstructured IR. For instance, the correct granularity of search results must be reconsid-ered. An XML document might contain a single element that is pertinent to a given query along with many unre-lated elements. The Digital Bibliography &amp; Library Project
Keyword search over data graphs generalizes both. related information as a unified whole. (DBLP) 2 XML dump currently contains more than 1.3 mil-lion publications; searching this repository for a particular paper should return only the information about that paper and not the complete bibliography.

Identifying relevant results is further complicated due to the fact that the data X  X  physical representation often does not match a logical view of the information. For example, relational data is normalized to eliminate redundancy. The schema separates logically connected information, and for-eign keys identify related rows. Whenever search queries cross these relationships, the data must be mapped back to a logical view to provide meaningful search results. As shown in Figure 1, answering the question  X  X ho played Pro-fessor Henry Jones in Indiana Jones and the Last Crusade  X  requires data from all 4 physical relations and is answered by the bottommost logical view (lower right of figure).
Recombining disparate pieces of data into a unified whole makes searching structured data significantly more complex than searching unstructured text. In unstructured IR, each document may be indexed prior to searches. In contrast, indexing all the possible logical (or materialized) views of structured data results in an explosion in the size of the index because the number of possible views is limited only by the data itself. For example, Su and Widom [29] indexed a subset of the logical views over a database and found that the index exceeded the size of the original data between two and eight times.

Searching structured data continues to grow in importance as websites serve increasing amounts of data on-demand in response to user actions and to personalize webpages. This information is normally stored in a relational database, and Bergman [1] estimates that data to be orders of magnitude larger than the static web. The explosive growth of social networking and microblogging websites contributes to the ever increasing amount of data stored relationally. Novel search techniques are required to access this information ef-ficiently and effectively. http://dblp.uni-trier.de/
The major contributions of this paper are threefold. Sec-tion 2 reviews related work, providing a high-level overview of keyword search systems for structured data. We also present details regarding the evaluations of these systems published at top research venues (e.g., VLDB, SIGMOD, ICDE). Our survey exposes the ad hoc techniques currently being used by researchers and a lack of experimental re-peatability given the scanty details provided in the litera-ture. Webber [32] independently considers many of these details and arrives at similar conclusions.

Section 3 describes our evaluation framework. In con-trast to many previous evaluations, our evaluation frame-work covers 3 dramatically different datasets and contains 50 information needs for each dataset. To promote stan-dardized evaluation, we will make our datasets, queries, and relevance assessments available for other researchers to use in their own experiments. Our framework carefully consid-ers the unique requirements of keyword search systems for structured data and follows the traditional definitions of rel-evance developed by the IR community.

We evaluate 9 state-of-the-art systems in Section 4 using our framework. Our results do not support the claimed ef-fectiveness of existing search techniques. In fact, our results indicate that many strategies perform comparably with re-gard to search effectiveness despite contrary claims appear-ing in the literature. Our work is also the first to investigate the correlation among the results produced by each system to determine if faster query processing techniques produce results comparable to more effective ranking schemes. We highlight some of the ranking factors that work particu-larly well for our datasets and query workloads and also present our conclusions regarding state-of-the-art keyword search systems for structured data.
Relational keyword search systems tend to follow a strict dichotomy. One approach uses IR techniques to rank results while the other approach considers the problem a form of proximity search. Proximity search systems minimize the distance between search terms in the data graph. If results are enumerated in increasing order of weight, the problem is an instance of the group Steiner tree problem, which is NP-complete. Hence, a variety of heuristics and specialized data structures have been proposed to make the problem tractable.
DISCOVER [15] proposed the general system architecture that most IR approaches follow. Search results are networks of tuples that collectively contain all search terms. The can-didate network generator enumerates all networks of tuples that are potential results. Because the total number of can-didate networks is limited only by the actual data, efficient enumeration requires that a maximum candidate network size be specified. Hristidis et al. [14] refined DISCOVER by adding support for OR semantics and by including an IR scoring function (pivoted normalization scoring [28]) to rank results. Their monotonic score aggregation function enables efficient execution. Liu et al. [21] propose four ad-ditional normalizations for pivoted normalization scoring to adapt it to a relational context. SPARK [22] returns to a non-monotonic score aggregation function for ranking re-sults; the non-monotonic function precludes the use of exist-ing query processing algorithms. Qin et al. [27] investigate query processing to eliminate the middleware layer between search systems and the underlying relational database.
BANKS [2] introduced the backward expanding search heuristic for enumerating results. Edges in the data graph denote a relationship between the vertices (a foreign key be-tween the relational tuples). For every edge ( u, v ) induced by a foreign key, a backward edge ( v, u ) with a larger weight is also added. These backward edges enable edge direc-tionality to be considered when identifying results. Bidi-rectional search [17] improves the performance of the orig-inal system. Ding et al. [8] use dynamic programming X  the DPBF algorithm X  X o identify the minimal group Steiner tree in time exponential in the number of search terms. BLINKS [13] uses a bidirectional index to improve query performance. EASE [20] proposes a graph index to support efficient searches on unstructured, semi-structured, and re-lational data where query results are subgraphs whose ra-dius does not exceed a maximum size. Golenberg et al. [11] guarantee a polynomial delay when enumerating results but enumerate results by height rather than weight. Dalvi et al. [7] investigate keyword search on external memory graphs using a multi-granular graph representation to reduce I/O costs as compared to data structures managed by virtual memory. STAR [18] approximates the optimal result tree in pseudo-polynomial time, which is shown to outperform several other proximity search heuristics.
In this section, we survey the evaluations of systems pub-lished at leading research venues in this field. In our experi-ence, these evaluations exceed the scope and quality of those published in smaller venues yet clearly illustrate the ad hoc evaluation techniques currently practiced by researchers.
Table 1 summarizes the datasets used in previous evalu-ations. Two of the most popular datasets (DBLP and the Internet Movie Database (IMDb)) lack a canonical schema, which leads to several different schemas appearing in eval-
Legend  X  identical relational schemas  X  different schemas or schemas not provided
This column denotes the presence of additional datasets in the evaluation. None of these datasets are related.
The database content is varied for the experiments.
A random number of citations is added to each paper.
Most papers not cited by or citing another were removed.
Subset includes 15,000 vertices and 150,000 edges; edge weights are assigned randomly.
Subset includes 30,000 vertices and 80,000 edges; edge weights are assigned randomly.
 Table 1: Summary of datasets used by previous re-searchers. In this and future tables and figures, systems are ordered by date of publication, i.e., BANKS is the oldest system. uations. As evidenced by the number of table footnotes, the information contained within each dataset is fluid: re-searchers often use arbitrary subsets in their evaluation, but this practice raises two experimental design questions that have not been addressed. First, do the techniques really scale to the level researchers claim? Approaches that cannot handle the amount of data present in today X  X  data reposito-ries will be unable to cope with tomorrow X  X  infusion of new data. Second, are the properties of the resulting subsets rep-resentative of the original dataset? Arbitrary subsets can mask scalability issues and artificially bolster the reported effectiveness of the system.

Table 2 lists more detailed information about previous system evaluations. As evidenced by the table, many eval-uations reported in the literature focus exclusively on per-formance. Few experiments consider the quality of search results. Both should be considered, for it is trivial to quickly return poor quality results. Given the number of differ-ent ranking schemes and execution heuristics, search quality should not be ignored.

Many experiments use queries that are created using ran-dom keywords that appear in the database. According to Manning et al. [23],  X  X sing random combinations of query terms as an information need is generally not a good idea because typically they will not resemble the actual distri-bution of information needs. X  More recent evaluations use queries constructed by the researchers, but these evalua-The queries are equally partitioned among the number of query terms. tions suffer from an insufficient number of queries. When evaluating search systems, 50 information needs is the tra-ditional minimum [23, 31]. In addition, we have noticed that many queries selected by researchers subtly reflect their proposed ranking scheme. For the evaluation of Effec-tive [21], every query contained a  X  X chema X  term (a search term that matches the name of a database relation or at-tribute). Matching search terms to the relational schema was not considered in previous work so naturally the pro-posed system outperforms competing approaches.

Among the systems that do consider the quality of search results, the definition of relevance is often vague. The devel-opers of EASE [20] state,  X  X nswer relevance is judged from discussions of researchers in our database group X . Such a vague definition does not make the assessment process re-producible by a third-party. SPARK [22] used the following definition: relevant results must 1) contain all query key-words and 2) have the smallest size (of any result satisfying the first criterion). In contrast to this definition, the IR community is clear that relevant results must address the underlying information need and not just contain all search terms [23, 32].

Our survey also revealed that systems often perform ab-normally well with regard to effectiveness metrics, which we attribute to non-standard relevance definitions coupled with very general queries. Bidirectional [17] claims,  X  X he recall was found to be close to 100% for all the cases with an equally high precision at near full recall. X  In the evaluation of EASE [20], the queries admit a large number of relevant answers, e.g.,  X  X ndiana Jones and the Last Crusade person X  where any cast member is relevant. EASE [20] reports a precision of 0.9 for 100 retrieved results, which is consider-ably better than the best scores reported at TREC (roughly 0.25) [32].

In Table 3, we show the systems that compare against previous work. With the exception of STAR [18], these comparisons are limited to 1 X 2 other systems. No evalua-tion appearing at a top-tier conference compares IR ranking schemes with proximity search heuristics. Lack of cross-evaluation makes it difficult to compare the trade-offs be-tween approaches that vary widely with respect to both query processing and ranking results.

In summary, no standardized datasets or query workloads exist for evaluating the performance or effectiveness of ex-isting systems. Even among evaluations that use the same dataset, results are not comparable because researchers cre-ate random subsets of the original database (Table 1). The past decade of research primarily focuses on performance (Table 2). Query workloads vary widely across evaluations from large collections of randomly generated queries (these may not reflect real user queries) to small numbers of more representative queries created by researchers (the number of queries does not meet the accepted minimum for evaluat-ing retrieval systems). Finally, comparison among systems is relatively limited (Table 3). The systems we include in our survey have been published in prestigious proceedings (e.g., VLDB, SIGMOD, ICDE), which indicates the unfortu-nate reality that ad hoc evaluations are an accepted practice rather than aberrations. Table 3: System evaluation comparison matrix. Evaluations that compare against other systems are listed on the left; the systems they compare against appear at the top of the table. Comprehensive eval-uations would compare against all previous work (i.e., the lower left entries would all be  X  ).
Two of our datasets are derived from popular websites (IMDb and Wikipedia). The third ( Mondial ) is an ideal counterpoint due to its smaller size. Table 4 provides de-tailed statistics regarding all three of our datasets. Even though our datasets are relatively small, they are sufficiently challenging for existing search techniques (as shown in Sec-tion 4), and both IMDb and Wikipedia can be scaled up as search techniques improve.

DBLP is one of the more popular datasets included in previous evaluations. We elected not to include it because the content of the DBLP database is similar to IMDb (e.g., names and titles) so results across these two datasets would likely be similar.
The Mondial dataset [24] comprises geographical and de-mographic information from the CIA World Factbook, the International Atlas , the TERRA database, and other web sources. We downloaded the relational version from its web-site. Mondial  X  X  cyclic data graph is much more complex than the others included in our evaluation.
We downloaded IMDb X  X  plain text files and created a rela-tional database using IMDbPY 4.1. Using a third-party tool eliminates any bias in the creation of the schema, which has the potential to significantly impact search effectiveness and performance. The initial database contained 20 relations with more than 44 million tuples. Because many proxim-ity search systems require an in-memory data graph, our dataset is a subset of the original database. We note that our subset potentially overstates the effectiveness of the var-ious search techniques for this dataset. Our final dataset is a selection of articles from Wikipedia. The complete Wikipedia contains more than 3 million ar-ticles, which makes including all of them infeasible. Our selection includes more than 5500 articles chosen for the 2008 X 2009 Wikipedia Schools DVD, a general purpose en-cyclopedia, which contains content roughly equal to a tradi-tional 20 volume encyclopedia. We deemed general content more desirable than a larger number of articles chosen ran-domly from the corpus. We drop all the tables unrelated to articles or users and augment the PageLinks table with an additional foreign key to explicitly indicate referenced pages.
Fifty information needs is the traditional minimum for evaluating retrieval systems [23, 31]. This number of infor-mation needs reflects the fact that performance varies widely across queries for the same document collection. Table 2 shows that other evaluations that use representative queries have not included this number of distinct information needs. Liu et al. [21] repeat a number of information needs in their queries. All our queries reflect distinct information needs.
We do not use real user queries extracted from a search en-gine log for three reasons. First, many queries are inherently ambiguous. Given the query  X  X ndiana Jones, X  it is impossi-ble to determine the underlying information need. Does the user want information about the character Indiana Jones or the films named after that title character? Without knowing the user X  X  intent, it is impossible to judge whether the char-acter or a film is the desired result. In contrast, a synthetic query workload based on overt information needs avoids this problem. Second, we believe a large number of queries will reflect the limitations of existing search engines X  X amely,
Legend primary key , foreign key , ::: full :::: text ::::: Table 4: Characteristics and simplified schema of our three evaluation datasets. The reported size in-cludes database indices. web search engines are not designed to connect disparate pieces of information. Users implicitly adapt to this limita-tion by submitting few (Nandi and Jagadish [25] report less than 2%) queries that reference multiple database entities. Third, the available search logs provide an insufficient num-ber of user queries for many domain-specific datasets (e.g., DBLP and Mondial ).

Ideally, a number of individuals all create candidate infor-mation needs for an evaluation, and a subset from this pool is actually included. This procedure is used by established evaluation forums (e.g., TREC and INEX) but is impractical for this work given the lack of incentive for others to partic-ipate. Consequently, we independently derived a variety of information needs for each dataset.

Table 5 provides the statistics of our query workload and the relevant results for each dataset. Five IMDb queries are outliers because they include an exact quote from a movie. Omitting these queries reduces the maximum number of terms in any query to 7 and the average number of terms per query to 2.91. The statistics for our queries are similar to those reported for web queries [16] and our independent analysis of query lengths from a commercial search engine log [26], which suggests that our queries are representative of real-world user queries. In contrast, the average length of queries used in previous studies (see Table 2) is almost always greater than the average for web queries.
Relevance is assessed relative to the original information need. For all our information needs, we identify relevant re-sults by constructing our information needs around a tem-plate of database relations. We execute a number of SQL queries to identify all possible results satisfying the infor-mation need and judge each of these results for relevance. Thus, careful construction of our information needs allows exhaustive relevance judgments for the collection. As is done at TREC, relevance assessments are carried out by a single individual. While using a single assessment as the gold stan-dard does affect the absolute values of effectiveness metrics, it has not been shown to impact the relative effectiveness of the systems under comparison [23, 31].
 We use binary relevance assessments when judging results. In adherence to the Cranfield paradigm [5], TREC tradition-ally used binary relevance assessments, which also have been used by all the previous evaluations reported in Section 2. In contrast, INEX distinguishes between highly relevant and partially relevant results. We believe this distinction to be good in theory, but it adds considerable complexity to the assessment process and also questions some of the central as-sumptions of the Cranfield paradigm X  X amely, all relevant documents are equally desirable. In practice, the notion of relevance, especially for structured data, is extremely sub-tle, involving novelty and diversity in the results. We refer the reader to Clarke et al. [4] for additional details.
In this paper, we do not consider the efficiency of search techniques and instead focus exclusively on search effective-ness. Obviously, performance plays a key factor when assess-ing system utility. The evaluations reported in the literature already investigate the performance aspect of their systems. Our work complements the evaluations appearing in the lit-erature by comparing systems on the basis of search quality. Omitting a performance comparison also stems from a prag-matic reason: we have not yet had the opportunity to im-plement many of the optimized query processing techniques proposed by the original researchers.

Our experiments target three questions. First, what is the effectiveness of each system, especially in comparison to each other? For previous evaluations that do consider search effectiveness, we hope to corroborate their claims. Second, what impact does the number of retrieved (top-k ) results have when evaluating search quality? Previous experiments at TREC show that retrieving too few results can signifi-cantly impact systems X  precision-recall curves [12], and some previous evaluations of search effectiveness only include the top-10 or top-20 results. Third, are the systems X  results highly correlated with each other? We expect many sys-tems (e.g., BANKS [2] and its successor Bidirectional [17]) to return similar results, which would make performance the only significant difference between these systems.
To measure the effectiveness of search systems, we use four metrics. The number of top-1 relevant results is the number of queries for which the first result is relevant. Reciprocal rank is the reciprocal of the highest ranked relevant result for a given query. Both of these measures tend to be very noisy but indicate the quality of the top-ranked results. Average precision for a query is the average of the precision values calculated after each relevant result is retrieved (and assign-ing a precision of 0.0 to any relevant results not retrieved). Mean average precision (MAP) averages this single value across information needs to derive a single measure of qual-ity across different recall levels and information needs. To summarize the entire precision-recall curve, we use 11-point interpolated average precision. To calculate each metric, we retrieve the top 1000 results for each system.

To measure the correlation between the results returned by the various systems, we use the normalized Kendall dis-tance [19]. The Kendall distance between two permutations is the number of pairwise swaps needed to convert one per-mutation into the other. Because we consider only the top-k results from each system, we use the generalization proposed by Fagin et al. [9]. Our evaluation includes most of the systems described in Section 2.1. Efficient [14], Effective [21], and SPARK [22] all use IR scoring schemes whereas BANKS [2], Bidirec-tional [17], DPBF [8], and BLINKS [13] are proximity search systems. DISCOVER [15] partially bridges these approaches by ranking results by the number of joins (i.e., edges) in the result tree. We also include our own previous work, struc-tured cover density ranking (CD) [6], which is designed to reflect users X  preferences regarding the ranking of search re-sults.Compared to previous evaluations, our work doubles the number of system comparisons (see Table 3). Five sys-tems described in Section 2.1 were not included due to their significant reimplementation effort.

Our reimplementations of systems include a number of en-hancements. 3 We generalized DISCOVER X  X  candidate net-work generation algorithm when we realized it was missing relevant results and modified DPBF to distinguish trees con-taining a single node (which improved its effectiveness for a number of our topics). Due to space limitations, we do not describe query processing and other aspects of these systems but refer readers to the original papers.

For each system, we set all tuning parameters to the values suggested by the authors. None of the systems X  X ncluding our own ranking scheme X  X re tuned using our datasets and queries because such tuning would overstate the effective-ness of the systems [23]. Bidirectional, DPBF, and BLINKS could not handle our IMDb dataset due to excessive ( &gt; 2.7 GB) memory requirements. In these cases, we use any re-sults output before running out of memory. A system X  X  omis-sion from a table or figure means that no query returned even a single result.
Figures 2 and 3 summarize the effectiveness of each sys-tem. The relative rank of each system in comparison to the others is similar in both graphs. We quickly see that effec-tiveness varies considerably across both datasets and differ-ent search techniques. In contrast to previous evaluations, no single system outperforms all the others.

Figure 2 shows the mean reciprocal rank for each system for queries where exactly one database tuple is relevant (20, 20, and 15 topics for the respective datasets). Nandi and Jagadish [25] report that these single entity queries are the most common type of query posed to existing search en-gines. We expected the proximity search systems (BANKS, Bidirectional, DPBF, and BLINKS) to perform poorly on this task because ranking results by edge weight does not allow these systems to distinguish trees containing a single node. Instead, we see that these systems perform very well on the Mondial dataset (the best 3 systems are all proxim-ity search engines), BANKS significantly outperforms the IR approaches (Efficient, Effective, and SPARK) on the IMDb dataset, and both BANKS and Bidirectional tie for second most effective on Wikipedia. These results counter our orig-inal intuition regarding the types of retrieval tasks suited to proximity search techniques. The results for the IR ap-proaches are disappointing in view of the excellent scores of the proximity search systems. Analyzing the results re-turned for each query sheds some light on the underlying reason: the IR-style ranking schemes prefer larger results that contain additional instances of the search terms instead of the smallest result that satisfies the query.
Our reimplementation of Liu et al.  X  X  work [21] does not include phrase-based ranking. Reciprocal Rank Figure 2: Reciprocal rank for the queries targeting exactly one database entity. Higher bars are better. Mean Average Precision (MAP) Figure 3: MAP measured across the various systems and datasets. Higher bars are better.

Figure 3 shows the MAP scores for the systems across all queries and datasets and illustrates three interesting trends. First, the IR approaches (Efficient, Effective, and SPARK) all perform comparably due to their common baseline scor-ing function, pivoted normalization scoring. The different normalizations that each apply to the original scoring func-tion accounts for their minor differences. Even though these three systems are outperformed by proximity search tech-niques, it does not indicate that there is no advantage to IR-style ranking. Cover density ranking (CD) is also based on previous work from the IR community, and it performs much better than the competing IR approaches. In fact, cover density ranking is the second-best system for dial (see also Table 7) and is the most effective system for Wikipedia. Second, scalability remains a significant concern for the proximity search systems. BANKS is the only prox-imity search system that completes any IMDb query, and the overhead of BLINKS X  X  bi-level index prevents it from indexing the Wikipedia dataset. Third, both BANKS and Bidirectional include node prestige when scoring result trees. Their node prestige factor accounts for their good scores on the Wikipedia dataset and is contrasted by the poor score of DPBF, which ranks results solely by edge weight.
Table 6 presents 11-point precision and recall for a sub-set of Wikipedia topics most similar to those encountered at TREC. The query terms are present in many articles, yet most articles containing the search terms are not relevant. Here we see the IR-style scoring functions (particularly Effi-cient) outperforming the proximity search systems because their scoring functions were designed for lengthy unstruc-tured text. Efficient, which least modifies pivoted normal-ization scoring, has the most stable performance across the entire precision-recall curve. In contrast, the effectiveness of the other IR-style scoring functions drops precipitously at higher recall levels. BANKS and Bidirectional both per-form well due to their consideration of node prestige, which interestingly translates to reasonable effectiveness even for our TREC-style topics.

Table 7 summarizes results for the Mondial topics and highlights the differences observed between our evaluation and SPARK X  X  evaluation [22]. 4 The left half of the table indicates that SPARK X  X  scoring function significantly im-proves upon Efficient and Effective. SPARK X  X  purported benefit X  X ore than doubling retrieval effectiveness X  X s not corroborated by our experiments (the right half of the ta-ble), which shows at best 20% improvement over Efficient and Effective. While some variation is natural, the discrep-ancy between 20% improvement and 100% improvement is not, especially given that the only variation is the query workload. When combined with the above-average score re-ported for SPARK (0.986 verses 0.8 for mean reciprocal rank by the best systems at TREC [32]), our results question the validity of the previous evaluation and further underscore the need for standardized evaluation.

In Figure 4, we show interpolated-precision curves for a variety of values of k for the same Wikipedia topics used in Table 6. These topics have the most interesting precision-recall curves due to the number of articles that contain each search term. As shown by the graph, a small value of k (like those used in previous evaluations [21, 22]) sig-Similar differences may be observed with other evaluations. DISCOVER [15] 31 0.671 Bidirectional [17] 34 0.730 Table 7: Mondial results; higher scores are bet-ter. The left two columns of results are copied from SPARK X  X  evaluation [22]. nificantly impact the precision-recall curve, particularly for higher recall levels. In particular, the curves become inaccu-rate above 40% recall. This result mirrors previous findings at TREC [12]. Space prevents us from presenting additional results, but in general, we found that k must be at least double the number of relevant results to ensure accuracy of MAP and interpolated precision.

Table 8 presents the normalized minimizing Kendall dis-tance between each system. Each value is averaged over all the datasets and queries; when comparing the results re-turned by two different systems for a particular query, we use the top-n results where n is the minimum number of results in the two lists. Limiting the comparison to this (variable) number of results follows the precedent of Fagin et al. [9] and is necessary because different systems often return a different number of results even for the same query. The purpose of this analysis is to determine if the only impor-tant difference between the various systems is performance. Obviously if both systems return similar results, we would prefer the faster search technique. Unfortunately, our results suggest that the systems are only moderately correlated at best. Consider Bidirectional, which addresses performance bottlenecks of BANKS. The correlation between the two sys-tems is only 0.391; their results are similar but no more than other approaches that share a baseline scoring function (e.g., Efficient, Effective, and SPARK). Because the results of the various systems are not highly correlated, the effectiveness of each system must be independently validated.
In part, our evaluation was designed to corroborate the claims of search effectiveness previously presented in the lit-erature. Across all our datasets, we found that our mea-surements of search effectiveness are considerably lower than those reported in other evaluations. While it is known that these values cannot be directly compared across different document collections [31], we believe that many previous studies have inflated claims of search quality, perhaps due to unreported methodological problems such as tuning their systems on the evaluation queries. Webber [32] confirms the trend toward reporting above-average effectiveness scores. Precision Figure 4: Top-k interpolated precision curves aver-aged over the systems. The result lists are truncated to contain only k results so the curves for smaller val-ues of k always lie below the curves for larger values of k . 0.0 0.867 0.357 0.925 0.654 1.000 0.492 0.812 0.958 0.1 0.713 0.357 0.870 0.594 0.883 0.377 0.812 0.898 0.2 0.557 0.306 0.834 0.524 0.639 0.296 0.688 0.669 0.3 0.473 0.290 0.739 0.446 0.415 0.267 0.533 0.529 0.4 0.434 0.253 0.696 0.433 0.237 0.234 0.484 0.483 0.5 0.377 0.227 0.523 0.378 0.186 0.179 0.330 0.355 0.6 0.283 0.185 0.475 0.359 0.078 0.157 0.290 0.185 0.7 0.217 0.158 0.363 0.294 0.034 0.081 0.193 0.086 0.8 0.114 0.158 0.227 0.165 0.008 0.060 0.071 0.082 0.9 0.052 0.057 0.116 0.067 0.000 0.002 0.057 0.081 1.0 0.052 0.057 0.112 0.065 0.000 0.002 0.022 0.079 MAP 0.345 0.181 0.518 0.339 0.287 0.179 0.362 0.361 better.
 DISCOVER [15] 0.507 0.000 0.530 0.576 0.583 0.656 0.530 0.482 0.577 Bidirectional [17] 0.391 0.576 0.582 0.000 0.641 0.708 0.605 0.590 0.707 correlations. The table is symmetric about the main diagonal. The scores of retrieval systems evaluated at TREC and INEX are still considerably lower than ours. Perhaps the size of the collections plays a significant role, for effective-ness on the IMDb dataset lags considerably behind both Mondial and Wikipedia.

Beyond this general characterization of the scores, we see that most of the systems score comparably on each dataset. Overall, there is little that distinguishes any one ranking technique although the IR approaches tend to be less ef-fective than proximity search heuristics. A different sys-tem is most effective for each dataset. For the IR-scoring systems, we see no appreciable difference X  X ither in search effectiveness or the set of results retrieved X  X hat would indi-cate the superiority of any one technique. These results sug-gest that computationally cheap ranking schemes should be used instead of more complex scoring functions that require completely new query processing algorithms (e.g., those pro-posed by Luo et al. [22]).

Our evaluation illuminates two important issues that should be considered by future work in this field. First, prestige plays an important factor when ranking results. Due to the inclusion of node weights, BANKS [2] and Bidi-rectional [17] both perform well on the Wikipedia dataset. More recent approaches (as illustrated by DPBF [8]) focus on minimizing the weight of the result tree and perform much more poorly than ranking schemes that incorporate node weights. Given the large number of queries that users make for a specific database entity [25], we consider this approach ill-advised.

Second, our evaluation underscores the scalability issues encountered by systems that require an in-memory data graph to efficiently enumerate results. As stated in Sec-tion 2.2, allowing researchers to define arbitrary subsets of datasets for their evaluations may have masked this issue. With the exception of our reimplementation of BANKS, none of the proximity search systems were able to execute any of the IMDb queries. Given that our IMDb dataset is nearly two orders-of-magnitude smaller than the origi-nal, scalability issues inherent to the approaches cannot be ignored. Kasneci et al. [18] propose storing the complete graph in a database and designing algorithms for this rep-resentation, which corrects the immediate problem at the expense of invalidating previous results regarding the algo-rithms X  performance. Dalvi et al. [7] use a multi-granular graph representation to alleviate this problem and claim that their technique scales to datasets similar in size to our IMDb subset.
In this paper, we evaluate the effectiveness of keyword search systems for relational databases. Previous evalua-tions by researchers have been ad hoc with no standard-ized datasets or query workloads. The effectiveness of the proposed search techniques is often ignored as researchers focus on the performance aspects of systems. Our evalua-tion framework is the first designed for this field and pro-vides common workloads for evaluating current and future systems. Standardized evaluation techniques previously en-abled rapid progress in the IR community; it is past time for DB&amp;IR researchers to adopt this evaluation paradigm. Such a framework is essential for objectively evaluating many aspects of these systems X  X ncluding their performance X  which depend on the query workload. Our datasets, topics, and relevance assessments are available at http://www.cs. virginia.edu/~jmc7tp/projects/search/ .

The evaluation presented in this paper is the first to com-pare a wide variety of IR scoring and proximity search tech-niques. Our results indicate that no existing scheme is best for search effectiveness, which contradicts previous evalua-tions that appear in the literature. We also show that the sets of results retrieved by different systems are not highly correlated, which indicates that performance is not the sole factor that differentiates these systems.

In the future, we will expand our evaluation framework to include additional datasets and query workloads. We welcome collaboration with other researchers so evaluation becomes a community effort as it is at TREC and INEX. We also look to reexamine our design decisions for our eval-uation (e.g., binary relevance assessments) and to include additional metrics (e.g., normalized discounted cumulative gain (nDCG)) for evaluating search effectiveness.
We thank Hristidis et al. for providing us with a refer-ence implementation of their system [14]. In addition, we thank Ding et al. for providing their implementation of DBPF [8] and He et al. for giving us their implementa-tions of bidirectional search [17] and BLINKS [13]. Andrew Jurik, Michelle McDaniel, and the anonymous reviewers all gave helpful comments regarding drafts of this paper.
