 Topic taxonomies present a multi-level view of a document collec-tion, where general topics live towards the top of the taxonomy and more specific topics live towards the bottom. Topic taxonomies al-low users to quickly drill down into their topic of interest to find documents. We show that hierarchies of documents, where doc-uments live at the inner nodes of the hierarchy-tree can also be inferred by combining document text with inter-document links. We present a Bayesian generative model by which an explicit hi-erarchy of documents is created. Experiments on three document-graph data sets shows that the generated document hierarchies are able to fit the observed data, and that the levels in the constructed document hierarchy represent practical groupings.
 I.2.6 [ Artificial Intelligence ]: Learning; G.3 [ Probability and Statis-tics ]: Probabilistic algorithms, Nonparametric statistics Algorithms, Experimentation hierarchical clustering, Bayesian generative models, topic models, model evaluation
As the number of online resources and Web documents continues to increase, the need for better organizational structures that guide readers towards the information they seek increases. Hierarchies and taxonomies are invaluable tools for this purpose. Taxonomies are widely used in libraries via the Library of Congress System or the Dewey Decimal System, and hierarchies were a fixture of the early World Wide Web; perhaps the most famous example be-ing the Yahoo search engine, which is actually an acronym for Yet Another Hierarchical Officious Oracle. These hierarchical systems were developed because their effectiveness at topical organization and their logarithmic depth allowed users to quickly find the rele-vant documents they were searching for.

Unfortunately, taxonomy curation of documents, articles, books, etc. is mostly a manual process, which is only possible when the number of curated documents is relatively small. This process becomes increasingly impractical when the number of documents grows to Web-scale. This has motivated research towards automat-ically inducing document taxonomies from the data [1, 4, 14, 5, 6, 19]. Most of the existing techniques rely on a single type of data  X  usually text. The problem with text-only hierarchy induction is that words often have multiple meanings. For example, the words  X  X orm X  and  X  X ug X  have very different meanings in the contexts of biology and computer science. Therefore without proper context proper taxonomy induction can be difficult.

Most document repositories contain linkages between the docu-ment creating a document-graph . These links provide proper con-text to the terms in each document. Document-graphs are espe-cially common in nonfiction and scientific literature, where cita-tions are viewed as inter-document links. The World Wide Web can be considered to be a single, very large document-graph, where Web pages represent documents and hyperlinks link documents.
Web sites, in particular, are a collection of documents with a very specific and purposeful organizational structure. Web sites are of-ten specifically designed to guide the user from the entry page, i.e., homepage , to progressively more specific Web pages. Similarly, scientific literature can be categorized into a hierarchy of increas-ingly specific scientific topics by their citation links, and encyclo-pedia articles can be categorized into a hierarchy of increasingly specific articles by their cross references. Thus, we assert that most document-graphs contain a hidden document hierarchy.

In this paper we draw a specific distinction between a hierarchy and a taxonomy . We define a taxonomy to be a classification of ob-jects into increasingly finer granularities, where each non-leaf node is a conceptual combination its children. A biological taxonomy is a great example of this definition because a classified species, say homo sapiens ( i.e. , humans), can only be placed at a leaf in the taxonomy; the inner nodes, e.g. , primate, mammal, animal, do not declare new species, rather they are conceptual agglomerations of species. Furthermore, each species is described by its path through the taxonomy. For example, homo sapiens, can be described as primates, as mammals and as animals (among others). A hierar-chy, on the other hand, is an arrangement of objects where some objects are considered to be above , below or at the same level as others. This necessarily means that objects of a hierarchy live at the internal nodes.

Strictly speaking, most existing models infer taxonomies. The goal of this paper is to construct document hierarchies from a docu-ment-graph using document text and inter-document links. For these purposes above , below or at the same level as refers to the topical granularity of the documents. In other words, given a doc-ument graph with an explicitly identified root, such as a Web site homepage, we aim to learn a document-hierarchy which best cap-tures the conceptual hierarchy of the document-graph. This prob-lem poses three technical challenges: 1. Inducing document topic mixtures . We propose learning 2. Selecting document placement . Placement of a document 3. Analysis at Web site-scale . In most document-graph col-
The remainder of this paper is organized as follows. In Sec-tion 2 we review the related literature with specific attention paid to distinctions between similar generative models. In Section 3 we discuss the intuition behind hierarchy induction from document-graphs. Our proposed model is described in Section 4. In Section 5 we perform quantitative experiments on three data sets, as well as a large qualitative exploration based on thousands of human judg-ments. We find that our model constructs document hierarchies that are coherent with respect to their textual topics and conform to the graphical representation of the underlying document-graph.
There has been a substantial amount of previous work on hier-archical clustering of documents. The first approaches were called agglomerative clustering, which used greedy heuristics such as single-link or complete-link [29]. Dendrograms are often the output of such clustering techniques, in which the root node is split into a series of branches that terminate with a single document at each leaf. Ho, et al. , point out that manually-curated hierarchies like the Open Directory Project 1 are typically flatter and contain fewer inner nodes than agglomerative clustering techniques produce [14]. Other hierarchical clustering algorithms include top-down processes which iteratively partition the data [31], incremental methods like C
OB W EB [9], C LASSIT [11], and other algorithms optimized for hierarchical text clustering.

The processes that typically define most hierarchical clustering algorithms can be made to fit in a probabilistic setting that build bottom-up hierarchies based on Bayesian hypothesis testing [13]. On the other hand, most recent work uses Bayesian generative mod-els to find the most likely explanation of observed text and links. The first of these hierarchical generative models was hierarchical latent Dirichlet allocation (hLDA). In hLDA each document sits at a leaf in a tree of fixed depth as illustrated in Figure 1(a). The doc-ument is represented by a mixture of multinomials along the path through the tree from the document to the root. Documents are placed at their respective leaf nodes by the nested Chinese restau-rant process (nCRP).
 NCRP is a recursive version of the standard Chinese Restaurant Process (CRP), which progresses according to the following anal-ogy: An empty Chinese restaurant has an infinite number of tables, and each table has an infinite number of chairs. When the first cus-tomer arrives he sits in the first chair at the first table with probabil-ity of 1. The second customer can then chose to sit at an occupied table with probability of n i  X  + n  X  1 or sit at a new, unoccupied table with probability of  X   X  + n  X  1 ,where n is the current customer, n the number of customers currently sitting at table i ,and  X  is a pa-rameter that defines the affinity to sit at a previously occupied table following a rich get richer scheme.

The nested version of the CRP extends the original analogy as follows: At each table in the Chinese restaurant are cards with the name of another Chinese restaurant. When a customer sits at a given table, he reads the card, gets up and goes to that restaurant, where he is reseated according to the CRP. Each customer visits L restaurants until he is finally seated and is able to each. This process creates a stochastic tree with a width determined by the  X  parameter of a fixed depth L . This process has also been called the Chinese Restaurant Franchise because of this analogy [4].
Adams, et al. proposed a hierarchical topic model called tree structured stick breaking (TSSB), illustrated in Figure 1(c), wherein documents can live at internal nodes, rather than exclusively at leaf nodes. However, this process involves chaining together conjugate priors which makes inference more complicated, and it also does not make use of link data.
 Other work along this line include hierarchical labeled LDA (hL-LDA) by Petinot et al. [22] hLLDA, as well as fixed structure LDA (fsLDA) by Reisinger and Pasca [25] which modify hLDA by fixing the hierarchical structure and learning hierarchical topic distributions. The hierarchical pachinko allocation model(hPAM), shown in Figure 1(b), produces a directed acyclic graph (DAG) of a fixed depth allowing for each internal (non-document) node to be represented a mixture of more abstract, i.e. , higher level, top-ics [19].

In network-only data, community discovery is the process of finding self-similar group, or clusters. The SHRINK algorithm creates hierarchical clusters by identifying tightly-knit communi-ties and by finding disparate clusters by looking for hubs and other heuristics [16]. The focus of this paper is more on probabilistic models to generate hierarchies, rather than heuristic approaches. Clauset, et al , discover dendrograms by Monte Carlo sampling; however, dendrograms poorly represent the manually curated hi-erarchies and taxonomies that we are pursuing [7].

In this paper we merge document text and inter-document links into a single model. This assumes that the words and their latent topics fit within the link structure of the graph, and that the graph structure explains topical relationships between interlinked docu-ments. Topic Modeling with Network Structure (TMN), regular-izes a statistical topic model with a harmonic regularizer based on the graph structure in the data; the result is that topic proportions of linked documents are similar to each other [18]. However, hi-erarchical information is not discovered nor can be easily inferred from this model.

Other work on generative models that combine text and links include: a probabilistic model for document connectivity [8], the Link-PLSA-LDA and Pairwise-Link-LDA methods [21], the La-tent Topic Model for Hypertext (LTHM) method [12], role discov-ery in social networks [17], the author-topic-model [26], and oth-ers. The above models operate by encoding link probability as a discrete random variable or a Bernoulli trial that is parameterized by the topics of the documents. The relational topic model (RTM) builds links between topics, where observed links are given a very high likelihood [6]; although the paper is titled hierarchical rela-tional models for topical networks, the RTM model does not build a topic or document hierarchy. The TopicBlock model combines the non-parametric hLDA and stochastic block models [15] to gen-erate document taxonomies from text and links [14]; however, Top-icBlock does not permit documents to reside at non-leaf nodes of the tree hierarchy.

In contrast to the previous work, our model builds a hierarchy of documents from text and inter-document links. In our model, each node in the hierarchy contains a single document, and the hierar-chy X  X  width and depth is not fixed.
In the previous work, document hierarchies were not actually hierarchies of documents in the literal sense. Instead, leaf nodes of the hierarchy contains the actual, literal documents, and inter-nal nodes contained increasingly more general topics about the an-cestor documents. See Figure 1 for a brief comparison of model outputs. In this paper we require inner nodes, which in previous work are made of word multinomial distributions, to be literal doc-uments. This requires an assertion that some documents are more general than others . This section explores this assertion through examples and a review of similar assertions made in previous re-search.
A Web site G can be viewed as a directed graph with Web pages as vertices V and hyperlinks as directed edges E between Web pages v x  X  v y  X  excluding inter-site hyperlinks. In most cases, designating Web site entry page as the root r allows for a Web site to be viewed as a rooted directed graph. Web site creators and cu-rators purposefully organize the hyperlinks between documents in a topically meaningful manner. As a result, Web documents further away from the root document typically contain more specific topics than Web documents graphically close to the root document. For example, the Web site at the University of Illinois in Urbana-Champaign, shown in Figure 2 contains a root Web document (the entry page), and dozens of children Web documents. Even with a very small subset of documents and edges, the corresponding Web graph is quite complicated and messy. A breadth first traversal of the Web graph starting with the root node is a simple way to distill a document hierarchy from the Web graph. Unfortunately, we shall see that a fixed breadth-first hierarchy cannot account for many of the intricacies of real world Web graphs.

For explanation purposes we define four types of hyperlink edges in a Web site: (1) parent-to-child links, (2) upward links, (3) short-cuts, and (4) cross-topic links. Parent-to-child links direct the user from one Web page to a more topically specific Web page; e.g. ,a hyperlink from ../engineering to cs.illinois.edu is a parent-to-child hyperlink because computer science is topically more spe-cific than engineering. Upward links are hyperlinks that reference a more general document; e.g. , there may exist a hyperlink from cs.illinois.edu to illinois.edu because the computer sci-ence department would like to reference the fact that it belongs to the university. Shortcut links are hyperlinks that skip from very general Web documents to very specific Web documents as a way of featuring some specific topic; e.g. , if a computer science profes-sor wins a prestigious award or grant, his Web page may be linked to from the news section of the root Web page. Cross topic links are hyperlinks that move across topical subtrees; e.g. , the college of media may reference some working relationship with the athletic department by creating a hyperlink between the two Web pages.
Because our goal is to infer the document hierarchy, we are, in a sense, trying to find parent-to-child links. In the event that there is more than one parent-to-child link to a particular Web page, our goal is to find the best topical fit for each Web document in the inferred hierarchy.

Web researchers and practitioners have used the hyperlink struc-ture to organize Web documents for many years. The PageRank and HITS algorithms are two of the most famous examples of in-formation propagation through links. Specifically, PageRank uses the model of a random Web surfer ( i.e. random walker), who ran-domly follows hyperlinks over the Web. A current measure of a Web page X  X  authority corresponds to the probability that a random surfer lands upon that Web page. In our model, we assert that Figure 2: Truncated Web site graph of the University of Illinois Urbana-Champaign. Bold lines represent edges stochastically selected by the RWRH process PageRank X  X  notion of authority corresponds to topical generality. That is, Web pages with a high random surfer probability are likely to be topically more general than others.
Plenty of previous works in the information retrieval domain use document-graph structure to enrich document features for im-proved retrieval performance. We find that some of the intuition behind these previous works are helpful in framing our generative model.

A limitation of the random walker model is that it only looks at the graphical structure of the Web. The word distributions found in each document are clearly an important factor to consider when generating Web hierarchies. Previous work by Song, et al. [27] and Qin, et al. [24] show that a given Web page can be enriched by propagating information from its children. Their relevance propa-gation model modifies the language distribution of a Web page to be a mixture of itself and its children according to the formula: f ( w ; d )=(1+  X  ) f ( w ; d )+ (1 where f ( w ; d ) is the frequency of term w in Web page d before propagation, f ( w ; d ) is frequency of term w in Web page d after propagation, c is a child page of d in the sitemap T ,and  X  is a parameter to control the mixing factor of the children. This propa-gation algorithm assumes that the sitemap, T , is constructed ahead of time using URL features of the Web pages in a particular Web site.
 Note that f ( w ; d ) is a pseudo frequency count that is unsmoothed. The goal of previous works was to perform Web information re-trieval, wherein they used BM25-type functions to normalize and smooth the language distribution. For illustration purposes, lets smooth the term distribution using Dirichlet prior smoothing [30]. The f ( w ; d ) from above is used in place of the usual c ( w ; d ) . where C is the distribution over all terms in V ,  X  is the smoothing parameter, and the length is modified by the propagation algorithm to be | d | =(1+  X  ) | d | .

As a result of the upward propagation p  X  of the root document (Web site entry page) contains all of the words from all of the Web pages in the Web site. The most probable words are those that occur most frequently and most generally across all documents, and are thus propagated the most.
 In traditional topic hierarchy models (hLDA, TopicBlock, etc.), Figure 3: Truncated portion of the Wikipedia category sub-graph rooted at the node COMPUTING. the root topic contains a distribution of all of the most general topic words in the document collection. Table 1: Comparison of most probable words in top document (in p  X  ), and in root topic of hLDA and HDTM
As a small, preliminary example, Table 1 shows the top six most probable words in the top document (via text propagation) and in root topics of hLDA and HDTM of the computer science depart-ment X  X  Web site at the University of Illinois at Urbana-Champaign We see that the most probable words from the sitemap based Web document hierarchy is very similar to the most probable words in the most general topic of hLDA. This small example reinforces our intuition that certain Web sites have a hidden hierarchical topical structure.

In the previous term propagation work, the sitemaps were con-structed ahead of time using URL heuristics. Our goal is to learn the document hierarchy automatically and in conjunction with the topical hierarchy.
Documents from many different collections exist in hidden hier-archies. While technically a Web site, Wikipedia documents and categories form a unique document graph. Wikipedia categories are especially interesting because they provide a type of ontol-ogy wherein categories have more specific sub-categories and more general parent-categories. Most Wikipedia articles are are repre-sented by at least one category description; this allows for users to drill down to relevant articles in a very few number of clicks by browsing the category graph. A partial example of the Wikipedia category graph is shown in Figure 3.
 Bibliographical networks may also be hierarchically structured. In a bibliographic network, papers or authors (wherein each author could be a collection of documents) are represented by nodes and each citation is represented by an edge in the graph.
We treat the problem of inferring the document hierarchy as a learning problem akin to finding the single, best parent for each document-node. Unlike previous algorithms, which discover latent topic taxonomies, the hierarchical document-topic model (HDTM) finds hidden hierarchies by selecting edges from a set of possible edges in the document graph. This section presents a detailed de-scription of the model. A plate diagram of the generative process is shown in Figure 5.
 We begin with a document graph G = { D,E } of documents D and edges E . Each document is a collection of words, where a word is an item in a vocabulary. The basic assumption of HDTM and similar models is that each document can be generated by ran-domly mixing words from among topics. Distributions over topics are represented by z , which is a multinomial variable with an as-sociated set of distributions over words p ( w | z, X  ) ,where  X  is a Dirichlet hyper-parameter. Document-specific mixing proportions are denoted by the vector  X  . Parametric-Bayes topic models also include a K parameter that denote the number of topics, wherein z is one of K possible values and  X  is a K -D vector. HDTM does not require a K parameter as input. Instead, in HDTM there exist topics, one for each graph node, and each document is a mixture of the topics on the path between itself and the root document.
In basic LDA, a single document mixture distribution is p ( w is (1) choose a  X  of topic proportions from a distribution p (  X  where p (  X  |  X  ) is a Dirichlet distribution; (2) sample words from the mixture distribution p ( w |  X  ) for the  X  chosen in step 1.
HLDA is an extension of LDA in which the topics are situated in a hierarchy T of fixed depth L . The hierarchy is generated by the nested Chinese restaurant process (nCRP) which essentially represents  X  as an L -dimensional vector, defining an L -level path through T from root to document. Because of the nCRP process, every document lives at a leaf and the words in each document are a mixture of the topic-words on the path from it to the root.
Because the nCRP process forces documents to the leaves in the hierarchy T , HDTM replaces nCRP with a slightly modified ver-sion of random walk with restart (RWR) called random walk with restart at home (RWRH). In traditional RWR, a walker begins by selecting a random starting point. With probability (1  X   X  ) the walker randomly walks to a new, connected location or chooses to restart his walk at a random location with probability  X  ,where  X  is called the restart probability 3 .
 In HDTM, the root node is fixed, either as the entry page of a Web site, or by some other heuristic. Therefore, for the purposes of hierarchy inference, we force the random walker to start and restart at the root node i.e., at home . Forcing the random walker to restart at the root is similar to, but not the same as, finding the personalized PageRank score [2] between the root node and every document-node in the hierarchy.

Let deg ( u ) be the outdegree of document u in G . Consider a random walker visiting document d at time t . In the next time step, the walker chooses a document v i from among u  X  X  outgoing neigh-bors { v | u  X  T v } in the hierarchy T uniformly at random. In other words, at time t +1 , the surfer lands at node v i  X  X  v | with probability 1 /deg ( u ) . If at any time, there exists an edge k  X  X  v | u  X  G v } , i.e , an edge between the current node u and the target node k in the original graph G , then we record the probabil-ity of that new path possibility for later sampling. Alg. 1 describes this process algorithmically. This procedure allows for new paths from the root r k to be probabilistically generated based on the current hierarchy effectively allowing for documents to migrate up,
Most related works denote the restart probability as  X  ,however, this would be ambiguous with the Dirichlet hyper-parameter  X  .
Algorithm 1: Random Walk with Restart at Home input : Path Probs. P , Current Node u ,Target k , Weight w globals :Graph G , Hierarchy T , Restart Prob.  X  output : P foreach v i  X  T . Ch ( u ) do / * child of u in T * / if u  X  G k then / * Edge u to k exists in G * / down and through the hierarchy during sampling. The bold edges in Figure 2 show an example of edges stochastically selected by the RWRH process.
Because a document hierarchy is a tree, each document-node can only have a one parent. Selecting a path for a document d in the graph G is akin to selecting a parent u = Pa ( d ) (and grandparents, etc.) from { d | u  X  G d } in the document graph G . HDTM creates and samples from a probability distribution over each documents X  parent, where the probability of document u being the parent of d is defined as: where d k is the walkers current position at time k , dep depth of d in T ,and deg T ( d k ) is the outdegree of d k words, the probability of landing at d is the product of the emission probabilities from each document in the path through T from r to d .

The modified random walker function assigns higher probabili-ties to parents that are at a shallower depth than those at deeper po-sitions. This is in line with the intuition that flatter hierarchies are easier for human understanding than deep hierarchies [14]. Simply put, the restart probability  X  controls how much resistance there is to placing a document at successive depths.

Algorithmically, we infer document hierarchies by drawing paths c from the r to the document d . Thus, the documents are drawn from the following generative process:
In this generative process hierarchical nodes represent documents and topics, where internal nodes contain the shared terminology of its descendants.

Like in earlier models, there is statistical pressure in the poste-rior to have more general terms in topics towards the root of the hierarchy. This is because every path in the hierarchy includes the are more likely to be found in topics near the root and vice versa. root node and there are more paths through nodes at higher levels than through nodes at lower levels. As we move down the tree, the topics, and therefore the documents, become more specific.
Hyperparameters also play an important role in the shape and character of the hierarchy. The  X  parameter affects the smoothing on topic distributions, and the  X  parameter affects the smoothing on word distributions. The  X  parameter is perhaps the most important parameter because it affects the depth of the hierarchy. Specifically, if  X  is set to be large ( e.g. ,  X  =0 . 95 ) then resulting hierarchy shal-low. Low values ( e.g. ,  X  =0 . 05 ) may result in deep hierarchies, because there is a smaller probabilistic penalty for each step that the random walker takes.
Exact inference on this model is intractable, so we must use a approximation technique for posterior inference. The Gibbs sam-pling algorithm is ideal in this situation because it simultaneously allows exploration of topic distributions and potential graphical hi-erarchies.

The variables needed by the Gibbs sampler are: w d,n ,the n th word in document d ; z d,n , the assignment of the n th word in doc-ument d ;and c d,z , the topic corresponding to document at the z th level. The  X  and  X  variables are integrated out forming a collapsed Gibbs sampler.

The sampling is performed in two parts: (1) given the current level allocations of each word z d,n we sample the path c given the current state of the hierarchy, we sample z d,n
The first Gibbs sampling step is to draw a path from each docu-ment to the root through the graph. The sampling distribution for a path c d is where w is the count of terms in document d ,and w  X  d are the words without document d .

The second term represents the probability of drawing the path c d,k to document d at depth k from the RWRH process. Recall that each node has an emission probability of 1 / deg T ( d ) ,andarestart probability of  X  . We define the probability recursively: In other words, the probability of reaching d is equal to the proba-bility of a random walker with restart probability  X  being at docu-ment d at time k .

The first term represents the word distribution: p ( w d | c , w  X  d , z , X  ) where max( z d ) is the maximum depth of the current hierarchy state.
Given the current state of all the variables, the sampler must first pick an assignment z for word n in document d . The sampling distribution of z d,n is term is a distribution over word assignments: which is the  X  -smoothed frequency of seeing word w d,n in the topic at level z d,n in the path c d .
The second term is the distribution over levels where we denote #[  X  ] as the number of elements in the vector which satisfy the given condition. We abuse notation in Eq. 6 so that the product from j =1 to k  X  1 combines terms representing nodes at the j th level in the path c down to the parent of d second set of terms represents document d k at level k .The &gt; sym-bol in Eq. 6 refers to terms representing all ancestors of a particular node, and  X  refers to the ancestors of a node including itself.
This section describes the method and results for evaluating our model. We show quantitative and qualitative analysis of the hier-archical document-topic model X  X  ability to learn accurate and in-terpretable hierarchies of document graphs. Our main evaluations explore the empirical likelihood of the data and a very large case study wherein human judges are asked to evaluate the constructed hierarchies.
We evaluate HDTM on three corpora: the Wikipedia category graph, the Computer Science Department Web site at the University of Illinois, and a bibliographic network.
 Table 2: Comparison of most probable words in top document (in p  X  ) and in root topic (in hLDA)
The Wikipedia dataset has been used several times in the past for topic modeling purposes [12, 14]. Gruber et al. , crawled 105 pages starting with the article on the NIPS conference finding 799 links. Ho et al. performed a much larger evaluation of their TopicBlock model using 14,675 document with 152,674 links; however, they truncated each article to only the first 100 terms and limited the vo-cabulary to the 10,000 most popular words. Our Wikipedia dataset is a crawl of the category graph of Wikipedia, beginning at the cat-egory C OMPUTING . In Wikipedia each category has a collection of articles and a set of links to other categories; however, categories don X  X  typically have text associated with them, so we considered the text of each article associated with a particular category as the category X  X  text. For example, the category I NTERNET includes arti-we constructed a graph of 609 categories from 6,745 articles. The category graph is rather sparse with only 2,014 edges between cat-egories, but has vocabulary size of 146,624 with 5,570,868 total tokens. We did not perform any stopword removal or stemming.
We chose a computer science department Web site as the second data set because it a rooted Web graph with familiar topics. By in-ferring the document hierarchy, we aim to find the organizational structure of the computer science department. Our intuition is that Web sites reflect the business organization of the underlying en-tity; thus we expect to find a subtrees consisting of courses, faculty, news, research areas, etc. at high levels, and specific Web pages at lower levels in the hierarchy. We crawled the Web site starting at the entry page and captured 1,078 Web pages and 63,052 hyper-links. In total there were 15,101 unique terms from 771,309 tokens.
The bibliographic network consists of documents and titles from 4,713 articles from the SIGIR and CIKM conferences. There exist 3,908 terms across 43,345 tokens in the document collection. In this collection, links include citations between papers within the CIKM and SIGIR conferences. Citations between documents were provided by the authors of the ArnetMiner project [28], and is not complete. A SIGIR 1998 paper by Ponte and Croft [23] was chosen to be the root document because, in our records, it had the most in-collection citations.
HDTM has some distinct qualities that make apples to apples comparison difficult. Because HDTM is the first model to generate document hierarchies based on graphs, there is nothing to directly compare against. However, some of the models in the related work perform similar tasks, and so we perform comparisons when we are able.

The related works typically perform quantitative evaluation by measuring the log likelihood on held out data or by performing some other task like link prediction, etc. Log likelihood analysis looks at the goodness of fit on held out data. Unfortunately, we are not able to  X  X old out X  any of our documents for testing, because each document, especially a first or second level document, is very important to the resulting hierarchy. Removing certain documents might even cause the graph to separate, which would make hierar-chy inference impossible. For quantitative evaluation, we borrow the setup from [3] by comparing the states of each models X  Gibbs sampler with the highest log complete likelihood.

We perform quantitative experiments on HLDA [3], TopicBlock [14], TSSB [1], and fsLDA [25]. The fixed structure in fsLDA is determined by a breadth first iteration over the document graph be-cause URL heuristics were found to be unreliable. Hyper-parameters are the default unless otherwise specified. The depth of HLDA and TopicBlock is 4.

In all cases, a Gibbs sampler was run for 5,000 iterations; 2000 iterations were discarded as burn-in. Figure 5(a) shows the log complete likelihood for each sample. We ran the the Gibbs sam-pling algorithm on HDTM for various values of  X  , and Figure 6 shows the best cumulative log complete likelihood for each of the tested values of  X  .

Interestingly, Figure 5(b) shows that higher likelihood values are strongly correlated with hierarchies of deeper average depth; Fig-ure 5(c) finds that the same is true for hierarchies of deeper maxi-mum depth.

Figure 6 shows that HDTM with  X  =0 . 05 achieved the best likelihood score, and HDTM with  X  =0 . 95 achieved the worst likelihood score.

Table 3 shows the results of the different algorithms on the three data sets. The TopicBlock and TSSB clearly infer models with the best likelihood. The remaining algorithms, including HDTM, have mixed results.
In order to properly understand the results captured in Table 3, recall that log probability is a metric on the fit of the observa-tions on the configuration of the model. The original work on LDA [4] found that likelihood increases as the number of topics Figure 6: Best cumulative log complete likelihood for each tested  X  value. Lower  X  values result in deeper hierarchies. Table 3: Log complete likelihood results of the best sample from among 5,000 Gibbs iterations. Values are  X  10 6 . Higher values are better. Best results are in bold. increases. Along those lines, Chang, et al. demonstrated that more fine grained topics, which appear in models with a larger number of topics have a lower interpretability, despite having higher like-lihood scores. Simply put, there exists a negative correlation be-tween likelihood scores and human interpretability.

Applying these lessons to our experiments recall that HDTM has as many topics as there are documents, and non-root document top-ics are mixtures of the topics on the path to the root. Also recall that HLDA, TopicBlock and TSSB all generate a large number of latent topics. In HLDA and TopicBlock, there are infinitely many top-ics/tables in the nCRP. Practically speaking, the number of topics in the final model is much larger than the number of documents (conditioned on the  X  parameter). In TSSB, the topic generation is said to be an interleaving of two stick breaking processes; prac-tically, this generates even larger topic hierarchies. The fsLDA algorithm has as many topics as there are in hLDA, however, the fsLDA hierarchy is not redrawn during Gibbs iterations to fit the word distributions resulting in a lower likelihood.

Similarly, Figures 5(b) and 5(c) show that deeper hierarchies have higher likelihood scores. This is because long document-to-root paths, found in deep hierarchies, are able to provide a more fine grained fit for the words in the document resulting in a higher likelihood.
 Therefore, we contend that the better likelihood values of HLDA, TopicBlock and TSSB are due to the larger number of topics that these models infer. A better way to evaluate model accuracy is by some external task or by manually judging the coherence of the topics.
To measure the coherence of the groupings, we modify the word intrusion task developed by Chang et al [6] to create the document intrusion task. In this task, a human subject is presented with a randomly ordered set of eight document titles. The task for the human judge is to find the intruder, that is, which document is out of place or does not belong. If the set of documents with-out the intruder document all make sense together, then the hu-man judge should easily be able to find the intruder. For example, given a set of computer science documents with titles { systems, networking, databases, graphics, Alan Turing }, most people, even non-computer scientists, would pick Alan Turing as the intruder because the remaining words make sense together  X  they are all computer science disciplines.
 For the set { systems, networking, RAM, Minesweeper, Alan Turing }, identifying a single intruder is more difficult. Hu-man judges, when forced to make a choice, will choose an intruder at random, indicating that the grouping has poor coherence.
To construct a set of document titles to present to the human judge, we first select a grouping from the hierarchy at random (dis-cussed in Sec. 5.3.1), and select 7 documents at random from the grouping. If the there are fewer than 7 documents available in the selected grouping, then we select all of the documents avail-able; groupings of size less than 4 are thrown out. In addition to these documents, an intruder document is selected at random from among the entire collection of documents minus the documents in the test group. Titles are then shuffled and presented to the human judges. significance from HDTM  X  =0 . 95 and  X  =0 . 05 respectively. Figure 8: Illustration of the intruder detection task from the Wikipedia collection, wherein human judges are presented with a set of document titles and asked to select the document that does not belong.
In preparation for human judgments, we construct a hierarchy from the mode of sampled hierarchies. Specifically, at every 20th sample ( i.e. Gibbs lag = 20), the parent of each document is recorded. After the Gibbs iterations are complete, each document is endowed by the parent that it saw in the most samples.

Extracting document groupings for evaluation is slightly differ-ent for each model. HDTM and fsLDA store a document at each node in the hierarchy. We select a grouping by first picking a doc-ument at random, and then choosing its siblings. TopicBlock and HLDA store documents at the leaves of the taxonomy, which often include several documents. We select a grouping from these mod-els by first picking a document at random, and then choosing the other documents in the leaf-topic.

The hierarchies that the TSSB model constructed allowed mul-tiple documents to live at inner nodes, We were unsuccessful in our attempts to evaluate groupings on inner nodes with more than 4 documents. We also tried to find nodes with 4 or more siblings, however, the hierarchies that were generated were too sparse to find practical groupings. Thus we were unable to provide human judges with TSSB groupings.

Each document-graph collection had different types of labels presented to the judges. The CompSci web site collection was la-beled by the Web Page title and URL; the Wikipedia collection was labeled by the category title as shown in Figure 8; the bibliography network was labeled by the title of the paper. The intruder detection tasks described above were offered on Amazon Mechanical Turk. No specialized training is expected of the judges. 50 tasks were created for each dataset and model com-bination; each user was presented with 5 tasks at a time at a cost of $0.07 per task. Each task was evaluated by 15 separate judges. In order to measure the trustworthiness of a judge, we selected 5 easy tasks, i.e. , groupings with clear intruders, and created gold answers. Figure 9: Constructed hierarchy of bibliographic network with HDTM  X  = . 95 . Words at the root document represent the most probable words in the root topic. Most probable words for other documents are not shown due to space constraints. Judges who did not answer 80% of the gold answers correctly are thrown out and not paid. In total our solicitation attracted 31,494 judgments, across 14 models of 50 tasks each. Of these, 13,165 judgments were found to be from trustworthy judges.

We measure the model precision based on how well the intruders were detected by the judges. Specifically, if the intruder word w is from model m and task k ,and i m k,j is the intruder selected by the human judge j on task k in model m then where (  X  ) is the indicator function and J is the number of judges. The model precision is basically the fraction of judges agreeing with the model.

Figure 7 shows boxplots of the precision for the four models on three corpora. In most cases, HDTM performs the best. As in [6], the likelihood scores do not necessarily correspond to human judgments. This is probably because the RWRH function essentially constrains the flexibility of the word sampler to operate only over explicit paths in the rooted graph. Paired, two-tailed t-tests of statis-tical significants ( p&lt; 0 . 05 ) performed between HDTM  X  =0 . 95 and  X  =0 . 05 and the other models are represented by  X  and Figure 7 respectively.
 The bibliography network data had relatively low precision scores. This is probably because it was more difficult for the judges, who were probably not computer scientists, to differentiate between the topics in research paper titles. Figure 9 shows a small portion of the document hierarchy for the bibliographic network dataset con-structed with HDTM  X  = . 95 . The root document has 20 children in the hierarchy despite having 145 in-collection links. The remain-ing 120 documents live deeper in the hierarchy because HDTM has determined that they are too specific to warrant a first level position, and have a better fit in one of the subtrees.

Recall that each document is associated with the topics from it-self to the root, where the root is a single, general topic. The seven most probable terms at the root level are also shown adjacent to the root X  X  title in Figure 9. We see that these terms, like in HLDA and TopicBlock, are terms that are general to the entire collection.
We have presented hierarchical document-topic model (HDTM), a Bayesian generative model that creates document and topic hier-archies from rooted document graphs. We hypothesized that docu-ment graphs, such as Web sites, Wikipedia and bibliographic net-works contain a hidden hierarchy, and we show corollaries to this intuition in language model propagation literature. Unlike most previous work, HDTM allows documents to live at non-leaf nodes in the hierarchy, which requires a new path sampling technique we call Random Walk with Restart at Home. An interesting side-effect of the random walker adaptation is that the path sampling step, Eq. 1, is much faster than the nCRP because RWRH only cre-ates a sampling distribution for the parents of a document, whereas the nCRP process creates a sampling distribution over all possible paths in the taxonomy.

We performed several quantitative experiments comparing HDTM with related models. We conclude, as others have before us, that likelihood scores are a poor indicator of hierarchy interpretability, especially when the number of topics are different between com-parison models. We performed a large qualitative case study which showed that the cohesiveness of the document groupings generated by HDTM were statistically better than many of the comparison models despite the poor likelihood scores.
We thank Qirong Ho for his helpful discussion, the developers of the Mallet toolkit, as well as Rob Rutenbar and Abner Guzm X n Rivera for their initial guidance. This work is funded by an NDSEG Fellowship to the first author. The third author is supported by NSF IIS-09-05215, U.S. Air Force Office of Scientific Research MURI award FA9550-08-1-0265, and by the U.S. Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053 (NS-CTA).
