 Many time series prediction methods have focused on single step or short term prediction problems due to the inher-ent difficulty in controlling the propagation of errors from one prediction step to the next step. Yet, there is a broad range of applications such as climate impact assessments and urban growth planning that require long term forecast-ing capabilities for strategic decision making. Training an accurate model that produces reliable long term predictions would require an extensive amount of historical data, which are either unavailable or expensive to acquire. For some of these domains, there are alternative ways to generate po-tential scenarios for the future using computer-driven sim-ulation models, such as global climate and traffic demand models. However, the data generated by these models are currently utilized in a supervised learning setting, where a predictive model trained on past observations is used to es-timate the future values. In this paper, we present a semi-supervised learning framework f or long-term time series fore-casting based on Hidden Markov Model Regression. A co-variance alignment method is also developed to deal with the issue of inconsistencies between historical and model simu-lation data. We evaluated our approach on data sets from a variety of domains, including climate modeling. Our ex-perimental results demonstrate the efficacy of the approach compared to other supervised learning methods for long-term time series forecasting.
 H.m [ Information Systems ]: Miscellaneous Algorithms, Design, Experimentation Time Series Prediction and Semi-supervised Learning
Time series prediction has long been an active area of research with applications in finance [28], weather forecast-ing [14][8], network monitoring [5], transportation planning [18][23], etc. Many of these applications require long-term time series forecasting capabilities for strategic decision mak-ing. For example, scientists are interested in projecting the future climate to assess their potential impacts on the ecosystem and society. Transportation planners are inter-ested in forecasting highway traffic volumes to prevent future congestion and to reduce fuel costs and pollution. One way to perform long term forecasting is by repeatedly invoking a model that makes its prediction one step at a time. How-ever, since the model uses predicted values from the past to infer future values, this approach may lead to an error accu-mulation problem, which is the propagation of errors from one prediction step to the next step [9]. Long-term forecast-ing also requires an extensive amount of historical data in order to train a reliable model.

Instead of predicting the future values of a time series based on its historical values alone, an alternative strategy is to employ multivariate time series prediction methods, where the time series for a set of predictor variables are fit-ted against the time series for the response variable. This strategy is contingent upon the availability of future values of the predictor variables. Fortunately, in some application domains, such values can be obtained from computer-driven simulation models. For example, in climate modeling, out-puts from global climate models (GCMs) [14] are often used as predictor variables to determine the climate of a local re-gion. The GCM models are developed based on the basic principles of physics, chemistry, and fluid mechanics, taking into account the coupling between land, ocean, and atmo-spheric processes. Although the outputs from these models sufficiently capture many of the large-scale (  X  150-300 km spatial resolution) circulation patterns of the observed cli-mate system, they may not accurately model the response variable at the local or regional scales (  X  1-50 km) needed for climate impact assessment studies [27]. As a result, the coarse-scale GCM outputs need to be mapped into finer scale predictions, a process that is known as  X  X ownscaling X  in the Earth Science literature.

Regression is a popular technique for downscaling, where the GCM outputs are used as predictor variables and the re-sponse variable corresponds to the local climate variable of interest (e.g., precipitation or temperature). However, cur-rent approaches utilize the simulation data in a supervised learning setting, where a predictive model trained on past observations is used to estimate the future values. Such an approach fails to take advantage of information about the future data during model building. This paper presents a semi-supervised learning framework for long-term time se-ries forecasting based on Hidden Markov Model Regression (HMMR). Our approach builds an initial HMMR model from past observations and incorporates future data to it-eratively refine the model. Since the initial predictions for some of the future data may not be reliable, we need to ensure that they do not adversely affect the revised model. We developed an approach to overcome this problem by as-signing weights to instances of the future data based on the consistency between their global and local predictions. This approach also helps to ensure smoothness of the target func-tion [29]. We demonstrated the efficacy of our approach using data sets from a variety of applications domains.
One issue that requires particular consideration when ap-plying the semi-supervised HMMR to climate modeling prob-lems is the potential inconsistencies between the training and future data since they often come from different sources. GCM simulation runs are driven by a set of emissions scenar-ios, which may assume greenhouse gas concentrations that are different than those in the training data. Previous work on semi-supervised classification have shown that combining labeled and unlabeled data with different distributions may degrade the performance of a classifier [25]. We encoun-tered a similar problem when applying the semi-supervised HMMR method to climate data. To address this problem, we developed an approach that will transform the data set in a way that aligns their covariance structure while preserving most of the neighborhood information. Experimental results for modeling climate at 40 locations in Canada showed that semi-supervised HMMR with data calibration outperforms conventional (supervised) HMMR method in more than 70% of the locations.

The remainder of this paper is organized as follows. Sec-tion 2 reviews the background materials on time series pre-diction and HMMR. Section 3 describes the value of un-labeled data in regression problems. The proposed semi-supervised HMMR approach with data calibration is given in Section 4. The experimental results are presented in Sec-tion 5. Finally, Section 6 presents the related work on semi-supervised learning and Section 7 conclude our study.
Let L =( X l , Y l ) be a multivariate time series of length of values for the predictor variables and Y l =[ y 1 ,y 2 is the corresponding values for the response variable. The objective of multivariate time series prediction is to learn a target function f that accurately predicts the future val-ues of the response variable, Y u =[ y l +1 ,y l +2 , ..., y given the historical data L and the unlabeled data, X u = [ x using computer-driven simulation models (e.g., GCM for cli-mate modeling or CORSIM for traffic modeling). While the simulation runs can be used to produce coarse-scale prop-erties of a complex system, they do not accurately capture its detailed properties. The outputs from model simulation are therefore used as inputs into regression functions that predict the fine level properties of the system.
There are many multivariate time series prediction tech-niques available, including least square regression[20], recur-rent neural networks[16], Hidden Markov Model Regression [15], and support vector regression [24]. These techniques are currently employed in a supervised learning setting, and thus, may not fully utilize the value of the unlabeled data. This work presents a semi-supervised learning framework that integrates labeled and unlabeled data to improve long-term time series forecasting. The framework is implemented using Hidden Markov Model Regression [15], a stochastic model that has been successfully applied to various domains, including speech recognition [22] and climate modeling [8].
In Hidden Markov Model Regression, a time series is as-sumed to be generated by a doubly stochastic process, where the response variable is conditionally dependent on the val-ues of the predictor variables as well as an underlying un-observed process. The unobserved process is characterized by the state space S = { s 1 ,s 2 ,  X  X  X  ,s N } andisassumedto evolve according to a Markov chain, Q l =[ q 1 ,q 2 , .., q where q i  X  S (as shown in Figure 1). The transition be-tween states is governed by an N  X  N transition probability matrix A =[ a ij ]. Each state s i  X  S is associated with an initial probability distribution  X  i and a regression function f sion parameters, and t  X  N (0 , 1). Given a set of predictor variables x t at time t , the response variable y t is generated based on the following conditional probability:
The likelihood function for the sequence of labeled obser-vations in L =( X l , Y l )isgivenby L = The model parameters  X  =  X  , A ,  X  , C = {  X  i } N i =1 , { a ij } N i,j =1 , {  X  are estimated by maximizing the above likelihood function using the Baum-Welch (BW) algorithm [2]. The model pa-rameters can be estimated iteratively using the following quantities, which are known as the forward (  X  ) and back-ward (  X  ) probabilities: Further details on the derivation of the parameter update formula using these quantities can be found in [15]. Upon convergence of the BW algorithm, the HMMR model can be applied to the unlabeled data X u in the following manner. First, the probability of each hidden state at a given future time step is estimated as follows: p ( Next, the future value of the re sponse variable is estimated using the following equation:
This section provides an example to illustrate the value of unlabeled data for regression analysis. Consider the di-agram shown in Figure 2, where the x -axis corresponds to a predictor variable and the y -axis corresponds to the re-sponse variable. The data set contains 10 training examples (labeled 1  X  10) and 3 unlabeled examples (labeled 11  X  13). The diagram on the left shows the results of applying su-pervised linear regression while the diagram on the right shows the results of applying semi-supervised linear regres-sion. The solid line b indicates the true function from which the data is generated.

The dashed line a in the left diagram corresponds to the target function estimated from training examples. In the right diagram, the response values for the unlabeled data are initially computed using the values of their nearest neigh-bors. The target function, represented by the dashed line is then estimated from the combined training and previously unlabeled examples. Clearly, augmenting unlabeled data in this situation helps to produce a target function that lies closer to the true function. This example also illustrates the importance of using local prediction methods (such as nearest neighbor estimation) to compute the response values of the unlabeled examples. If the unlabeled examples were estimated using the initial regression function instead, then adding unlabeled data will not change the initial model.
It is worth noting that current research in semi-supervised learning suggests unlabeled data are valuable under two sit-uations. First, the model assumptions should match well with the underlying data. Second, the labeled and unla-beled data must be generated from the same distribution. Otherwise, incorporating unlabeled data may actually de-grade the performance of a predictive model [25][12].
This section describes our proposed semi-supervised learn-ing algorithm for HMMR and a data calibration approach to deal with inconsistencies between the distributions of la-beled and unlabeled data.
The basic idea behind our proposed approach is as fol-lows. First, an initial HMMR model is trained from the historical data using the BW algorithm described in Sec-tion 2.2. The model is then used, in conjunction with a local prediction model, to estimate the response values for future observations. The local model is used to ensure that the target function is sufficiently smooth. The estimated future values, weighted by their confidence in estimation, are then combined with the historical data to re-train the model. This procedure is repeated to gradually refine the HMMR model.
 We now describe the details of each step of our algorithm. Let  X  0 be the initial set of model parameters trained from the historical observations L .Weuse  X  0 to compute the initial estimate of each response value in Y u : which is similar to the formula given in Equation (4).
A principled way to incorporate unlabeled data is to re-quire that the resulting target function must be sufficiently smooth with respect to its intrinsic structure [29]. For re-gression problems, this requirement implies that the response values for nearby examples should be close to each other to ensure the smoothness of the target function. We obtain an estimate of the local prediction of the target variable y a future time step t as follows: spond to the k nearest neighbors of the unlabeled observa-observations. Based on their global and local estimations, we compute the weighted average of the response value at each future time step t as follows: The parameter  X  controls the smoothness of the target func-tion; a smaller weight means preference will given to the local estimation.

The newly labeled observations from the future time pe-riod will be augmented to the training set in order to re-build the model. As some of the predicted values  X  y t may deviate quite significantly from either the local or global es-timations (depending on  X  ), incorporating such examples may degrade the performance of semi-supervised HMMR. To overcome this problem, we compute the confidence value for each prediction and use it to weigh the influence of the unlabeled examples during model rebuilding. Let w t denote the weight assigned to the value of y t : of 1 to each historical observation Y l . This is based on the assumption that there is no noise in the historical data. Even if such an assumption is violated, our framework may accommodate noisy observations by applying Equation (6) to both training and future observations. The weight for-mula reduces the influence of future observations whose pre-dictions remain uncertain. After the model has been revised, it is used to re-estimate the response values for the future observations. This procedure is repeated until the changes in the model parameters become insignificant.

We now describe the procedure for estimating the param-eters of the semi-supervised HMMR model. Let  X  Y u denote Algorithm 1 Semi-Supervised Hidden Markov Regression for Time Series Prediction the estimated response values for the unlabeled data ob-tained from Equation 5. The likelihood function for the combined data is: L = where vised learning case, the weights are used to determine the To maximize the likelihood function, observations with large weights will incur higher penalty if their response values disagree with the predictions made by the current HMMR model. Such observations are therefore more influential in rebuilding the HMMR model.

To determine the model parameters that maximize the likelihood function, we introduce the following auxiliary func-tion: = = + + where X =[ X l ; X u ]and It can be shown that maximizing the auxiliary function will produce a sequence of model parameters with increasing like-lihood values. Taking the derivative of O (  X  ,  X  )inEquation (7) with respect to each model parameter in  X  ,weobtain the following update formula: c  X  a  X  where: p We omit the proof for the formula due to space restriction. The overall procedure for our proposed semi-supervised al-gorithm is summarized in Algorithm 4.1. After estimating the model parameters  X  , the response values for future ob-servations are predicted using Equation (4).
The previous section described our proposed algorithm for semi-supervised time series prediction. The underlying assumption behind the algorithm is that the predictor vari-ables for labeled and unlabeled data have the same distribu-tion. This may not be true in real-world applications such as climate modeling or urban growth planning, where the unla-beled data are obtained from a different source (e.g., model simulations) or their distributions may have been perturbed by changes in the modeling domain (e.g., increase of popu-lation growth or greenhouse gas concentration).

Several recent studies on semi-supervised classification have suggested the negative effect of unlabeled data, especially when a classifier assumes an incorrect structure of the data [12] or when the labeled and unlabeled data have differ-ent distributions [25]. None of these studies, however, have been devoted to regression or time series problems. Our ex-perimental results demonstrate that, while in most cases, semi-supervised HMMR indeed outperforms its supervised counterpart, for the climate modeling domain, where the his-torical and future observatio ns come from different sources, semi-supervised learning do not significantly improve the performance of HMMR. To overcome this problem, we pro-pose a data calibration technique to deal with the inconsis-tencies between historical and future data.

A straightforward way to calibrate X l and X u is to stan-dardize the time series of each predictor variable by subtract-ing their means and dividing by their respective standard deviations. The drawback of this approach is that the co-variance structures for X l and X u are not preserved by the standardization procedure. As a result, a model trained on the historical data may still not accurately predict the future data since the relationship between the predictor variables may have changed. In this paper, we propose a new data calibration approach to align the covariance structure of the historical data and future data. Our approach seeks to find a linear transformation matrix  X  that is applicable to the future unlabeled data  X  X u such that the difference between their covariance matrices is minimized.

Let A denote the covariance matrix of X u and B denote the covariance matrix of X l : The covariance matrix after transforming X u to X u  X  is The transformation matrix  X  can be estimated using a least-square approach: The optimization problem can be solved using a gradient descent algorithm: where: and  X &gt; 0 is the learning rate.

Although the preceding data calibration approach helps to align the covariance matrices of the data, our experi-mental results show that the transformation tends to sig-nificantly distort the neighborhood structure of the obser-vations. Since our semi-supervised HMMR framework per-forms local estimation based on the nearest neighbor ap-proach, such a transformation leads to unreliable local pre-dictions and degrades the overall performance of the algo-rithm. An ideal transformation should preserve the neigh-borhood information while aligning the covariance struc-ture. To accomplish this, we create a combined matrix ing the matrix, i.e., After calibration using the gradient descent method described previously, both X l and X u will be transformed as follows: The transformed data X l and X l will serve as the new train-ing and test data for the semi-supervised HMMR algorithm.
We have conducted several experiments to evaluate the performance of our proposed algorithm. All the experiments were performed on a Windows XP machine with 3.0GHz CPU and 1.0GB RAM.
Table 5.1 summarizes characteristics of the time series used in our experiments. The time series are obtained from the UC Riverside time series data repository [21]. We di-vide each time series into 10 disjoint segments. To simulate this as a long-term forecasting problem, for the k -th run, we use segment k as training data and segments k +1 to k + 5 as test data. The future period is therefore five times longer than the training period for each run. The results reported for each data set are based on the average root mean square error (rmse) for 5 different runs. We also con-sider three other competing algorithms for our experiments: (1) univariate autoregressive (AR) model, (2) multiple linear regression (MLR), and (3) supervised HMMR.

There are several parameters that must be determined for our semi-supervised HMMR, such as the number of nearest neighbors k , the number of hidden states N , and smoothness Table 1: Description of the UCR time series data sets parameter  X  . To determine the number of nearest neigh-bors, we perform 10-fold cross validation on the training data using the k nearest neighbor regression method [17]. There are various methods available to determine the num-ber of hidden states N . These methods can be divided into two classes X  X odified cross-validation and penalized likeli-hood methods (such as AIC, BIC, and ICL). In this work, we employ the modified 10-fold cross validation with miss-ing value approach [7] to select the best N .Foreachfold, we randomly select one-tenth of the observations to be re-moved from the training data. The likelihood function will be estimated from the remaining nine-tenth of the training data (while treating the removed data as missing values). The number of states N that produces the lowest root mean square error will be chosen as our parameter. To ensure smoothness in the target function,  X  should be biased more towards the local prediction. Our experience shows that this can be accomplished by setting  X  somewhere between 0 . 1to 0 . 3. We fix the smoothness parameter  X  =0 . 1 throughout our experiments.
Table 5.2 compares the root mean square errors of our proposed framework (semiHMMR) against the univariate AR (UAR), multiple linear regression(MLR), and supervised Hidden Markov Model Regression (HMMR). First, observe that the performance of univariate AR is significantly worse than multivariate MLR and supervised HMMR model. This is consistent with the prevailing consensus that multivariate prediction approaches are often more effective than univari-ate approaches because they may utilize information in the predictor variables to improve its prediction. Second, we ob-serve that HMMR generally performs better than MLR on most of the data sets. This result suggests the importance of learning models that take into consideration the dependen-cies between observations. Finally, the results also show that semi-supervised HMMR is significantly better than HMMR on the majority of the data sets. The improvements achieved using semi-supervised HMMR exceed 10% on data sets such
The purpose of this experiment is to show the value of un-labeled data when there is limited training data. We have used the  X  X team X  time series for this experiment and vary the ratio of labeled to unlabeled data from 0 . 2 to 1 and report its average root mean square. As shown in Figure 3, when the ratio is 1, the rmse for MLR and supervised HMMR are Table 2: Average root mean square error for UAR, MLR, HMMR and SemiHMMR on UCR time series data sets Data Sets UAR MLR HMMR SemiHMMR Greatlake 35.4278 24.5924 23.0445 20.0100 Twopat 1.3111 1.1426 1.2404 1.1402 Logistic 0.8732 0.5354 0.5332 0.5302
Cl2full 0.3336 0.0889 0.0888 0.0575 nearly the same as that for semi-supervised HMMR. How-ever, when the ratio decreases to 0 . 2, the performance of both MLR and supervised HMMR degrades rapidly (from 5 . 8093 to 7 . 9819 for MLR and from 5 . 6074 to 7 . 7537 for HMMR) whereas the rmse for semi-supervised HMMR in-creases only slightly, from 5 . 5370 to 5 . 7442. The results of this experiment show that semi-supervised HMMR can ef-fectively utilize information in the unlabeled data to improve its prediction, especially when labeled data is scarce. Figure 3: The performance of MLR, HMMR and SemiHMMR when varying the ratio of labeled to unlabeled data
This experiment investigates the effect of data calibration on the performance of semi-supervised HMMR. We show that data calibration is useful when the historical and fu-ture observations come from different sources. For this ex-periment, we downloaded climate data from the Canadian Climate Change Scenarios Network web site [1]. The data consists of daily observations for 26 climate predictor vari-ables including sea-level pressure, wind direction, vorticity, humidity, etc. The response variable corresponds to the ob-served mean temperature at a meteorological station. In short, there are three sources of data for this experiment: (1) Mean temperature observations from 1961 to 2001 to be used as response variable, (2) Reanalysis data from NCEP (National Center for Environmental Prediction) reanalysis project from 1961 to 2001 to be used as predictor variables for training data, and (3) Simulation data from HADCM3 global climate model from 1961 to 2099 to be used as pre-dictor variables for future data. Since the mean tempera-ture for the future time period is unavailable, we conducted our experiment using NCEP reanalysis and the observed mean temperature data from 1961 to 1965 for training and HADCM3 simulation data from 1966 to 1991 for testing. Figure 4: Performance comparison between HMMR, SemiHMRR and CSemiHMMR on Cana-dian climate data
To compare the relative performance of supervised HMMR, semi-supervised HMMR (SemiHMMR) and semi-supervised HMMR with data calibration (CSemiHMMR), we applied the algorithms to predict the mean daily temperature for 40 randomly selected meteo rological stations 1 in Canada. The bar chart shown in Figure 4 indicates the fraction of loca-tions in which one algorithm has a lower rmse than the other. Unlike the results reported in Section 5.2, the bar chart seems to suggest that the performance of semi-supervised HMMR is only comparable to supervised HMMR (55% ver-sus 45%). This is actually consistent with the conclusions drawn in [12, 25] for semi-supervised classification in which it was suggested that unlabeled data with different distribu-tion may not improve the performance of a semi-supervised algorithm. However, with the calibration method developed in Section 4.2, CSemiHMMR actually performs better than HMMR on 72% of the data sets. This result confirmed the effectiveness of incorporating the covariance alignment tech-nique to our semi-supervised learning framework. We also illustrate the rmse values for five of the selected stations in Table 3. The latitude and longitude for each station is recorded in the first column of Table 3. Though the perfor-mance of semiHMMR appears to be worse than supervised HMMR at (52 . 2  X  N, 113 . 9  X  W) and (48 . 3  X  N71  X  W), the rmse values for semi-supervised HMMR with data calibration is clearly superior for both locations.

In Section 4.2, we argued that aligning X l with X u (we call this calibration technique 1) may not be as effective as
Another criterion for choosing a station is that the time series must be complete, i.e., it has no missing values. Table 3: Comparing the average rmse values for MLR, HMMR, SemiHMMR, and CSemiHMMR on the Canadian climate data Table 4: Comparing the degree of alignment and loss of neighborhood structure information using cali-bration techniques 1 and 2 calibrating X l with the combined matrix X C =[ X u , X u ] (we call this calibration technique 2). This is because the for-mer may result in significant loss of nearest neighbor infor-mation, thus degrading the performance of semi-supervised HMMR 2 . To measure the degree of alignment and loss of neighborhood information using the calibration techniques, we define the following two measures: RCovDiff and NNLoss. ance matrices constructed from X A and X B before align-after alignment. RCovDiff measures the reduction of the co-variance difference before and after alignment, i.e.: A calibration technique with larger RCovDiff will produce covariance matrices that are better aligned with each other. We also measure the loss of neighborhood structure due to data calibration in the following way. Let M 0 ( X A , X B )de-note a 0/1 matrix computed based on the 1-nearest neighbor after alignment. The NNLoss measure is defined as follows: Unlike RCovDiff, the same equation is applied to both cal-ibration techniques 1 and 2. Table 4 compares the results of both calibration techniques. Although calibration tech-nique 1 produces more well-aligned covariance matrices, it loses more information about its neighborhood structure. This explains our rationale for using calibration technique 2 for semi-supervised HMMR.
There have been extensive studies on the effect of incorpo-rating unlabeled data to supervised classification problems,
Note that the result shown in Figure 4 is based on calibra-tion technique 2. including those based on generative models[13], transduc-tive SVM [19], co-training [4], self-training [31] and graph-based methods [3][32]. Some studies concluded that sig-nificant improvements in classification performance can be achieved when unlabeled examples are used, while others have indicated otherwise [4][10][12][25]. Blum and Mitchell [4] and Cozman et al. [10] suggested that unlabeled data can help to reduce variance of the estimator as long as the modeling assumptions match the ground truth data. Oth-erwise, unlabeled data may either improve or degrade the classification performance, depending on the complexity of the classifier compared to the training set size [12]. Tian et al. [25] showed the ill effects of using different distributions of labeled and unlabeled data on semi-supervised learning.
Recently, there have been growing interests on applying semi-supervised learning to regression problems [30][6][11][33]. Some of these approaches are direct extensions from their semi-supervised classification counterparts. For example, transductive support vector regression is proposed in [11] as an extension to transductive SVM classifier. Zhou and Li developed a co-training approach for semi-supervised re-gression in [30]. Their algorithm employs two KNN regres-sors, each using a different distance metric. Another exten-sion of co-training to regression problems was developed by Brefeld et al. [6]. Graph based semi-supervised algorithms [3][32] utilize a label propagation process to ensure that the smoothness assumption holds for both labeled and unlabeled data. An extension of the algorithm to regression problems were proposed by Wang et al. in [26]. Zhu and Goldberg [33] developed a semi-supervised regression method that in-corporates additional domain knowledge to improve model performance. Since all of the previous approaches ignore the temporal dependencies between observations, they are not well-suited for time series prediction problems.
Long term time series fore casting is an important but challenging problem with many applications. In this pa-per, we develop and evaluate a semi-supervised time series prediction approach for Hidden Markov Model Regression (HMMR). We show that the inconsistency between training and test data may actually hurt the model X  X  performance. To compensate for this problem, we propose a covariance aligning data calibration method that will transform the training and test data into a new space before applying the semi-supervised HMMR algorithm. Experimental results on several real-world data sets cle arly demonstrate the effective-ness of the proposed algorithms.
This work is supported by NSF grant #0712987. The authors would like to thank Dr Eamonn Keogh for providing us with the time series data. The authors would also like to thank Dr Julie Winkler and Dr Sharon Zhong for valuable discussion and comments. [1] http://www.ccsn.ca/, canadian climate change [2] P. M. Baggenstos. A modified Baum-Welch algorithm [3] A. Blum and S. Chawla. Learning from labeled and [4] A. Blum and T. Mitchell. Combining labeled and [5] Y.-A. L. Borgne, S. Santini, and G. Bontempi. [6] U. Brefeld, T. G  X  artner, T. Scheffer, and S. Wrobel. [7] G. Celeux and J. Durand. Selecting hidden markov [8] S. Charles, B. Bates, I. Smith, and J. Hughes. [9] H. Cheng, P.-N. Tan, J. Gao, and J. Scripps.
 [10] I. Cohen, N. Sebe, F. G. Cozman, M. C. Cirelo, and [11] C. Cortes and M. Mohri. On transductive regression. [12] F. Cozman and I. Cohen. Unlabeled data can degrade [13] F. Cozman, I. Cohen, and M. Cirelo. Semi-supervised [14] W. Enke and A. Spekat. Downscaling climate model [15] K. Fujinaga, M. Nakai, H. Shimodaira, and [16] C. Giles, S. Lawrence, and A. Tsoi. Noisy time series [17] T. Hastie and C. Loader. Local regression: Automatic [18] W. Hong, P. Pai, S. Yang, and R. Theng. Highway [19] T. Joachims. Transductive inference for text [20] B. Kedem and K. Fokianos. Regression models for [21] E. Keogh and T. Folias. Uc riverside time series data [22] C. Leggetter and P. Woodland. Maximum likelihood [23] A. Ober-Sundermeier and H. Zackor. Prediction of [24] A. Smola and B. Scholkopf. A tutorial on support [25] Q. Tian, J. Yu, Q. Xue, and N. Sebe. A new analysis [26] M. Wang, X.-S. Hua, Y. Song, L.-R. Dai, and H.-J. [27] R. Wilby, S. Charles, E. Zorita, B. Timbal, [28] C.-C. C. Wong, M.-C. Chan, and C.-C. Lam.
 [29] D. Zhou, O. Bousquet, T. Lal, J. Weston, and [30] Z. Zhou and M. Li. Semi-supervised regression with [31] X. Zhu. Semi-supervised learning literature survey. In [32] X. Zhu, Z. Ghahramani, and J. Lafferty.
 [33] X. Zhu and A. Goldberg. Kernel regression with order
