 1. Introduction
Searching for information about a person on the World Wide Web (WWW) is an increasing requirement in information retrieval. As the population of the WWW is increasing rapidly, the WWW has become the largest document resource ever seen. Search engines are effective tools to help users retrieve documents from such a huge database. Of the queries by users to search engines, a certain portion of queries, from 5% to 10%, includes people X  X  names ( Guha &amp; Garg, 2004 ).

Search results returned from search engines for a personal name query often contain documents relevant to whom they have no interest.

In our research, we endeavor to disambiguate people cited in the result set by reranking documents accord-selecting one document. Then, upon receiving the user X  X  selection, our system will rerank documents in the result set by the relevance order to the selected document.

In previous studies, researchers focused on disambiguation of people in some types of documents, such as 1998 ). However, documents on the web have distinct characteristics that differ from scientific documents or news articles. Documents on the web are  X  X oisy X , cover a broad range of topics, and come in various formats. Therefore, previous approaches are limited when disambiguating people in web documents.
To disambiguate people in web documents, we propose a new method that uses web directories to improve the disambiguating performance. On the WWW, many web directories are created to be available freely for everyone, such as the Dmoz directory, 2 the Google directory, extractable information in web documents. This enrichment enables us to determine the documents X  topics and to extract the common contexts in document pairs more readily. The measure of the common contexts in doc-ument pairs is used to rerank the result documents.

The remainder of this paper is organized as follows. In Section 2 , we summarize previous studies on name name disambiguation system of our proposed method. Experimental results and comparisons with other methods are given in Section 5 . We describe the advantages and limitations of our methods in Section 6 .
Finally, we present our conclusions in Section 7 . 2. Related work
Bagga and Baldwin (1998) solved the problem of personal name disambiguation in news articles. They used ument. Then, they used the vector space model (VSM) ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ) to measure sim-event. Therefore, they usually have the same topic and the traditional VSM method can measure document similarities very well. However, people on the web may have several appearances related to different events.
For example, a computer scientist may have different research interests overtime, so his or her publications though they concern the same general topic of computer science. In such a case, where the topic relationship between documents is not strong, the VSM method may not measure document similarities adequately because there are few co-occurring terms among documents.

Pedersen, Kulkarni, Angheluta, Kozareva, and Solorio (2005) extracted contexts in documents to disam-biguate people. They calculated the context of documents using a method called second order context vectors of terms. Then, they defined the document context vector to be the average vector of context vectors of all terms in the document. Document context vectors were used to cluster documents into groups. In their research, they experimented with famous people, such as the soccer players Ronaldo and David Beckham, and the former Prime Minister of Israel, Shimon Peres. However, this approach may not work well when deal-ing with people who are not famous because only a few documents relate to them, which makes the building of search engines and built key phrases X  contexts using snippets of the resulting documents. However, this method is expensive because it requires many query transactions to build contexts for key phrases.
To deal with ordinary people, who are included in few relevant documents, Bekkerman and McCallum (2005) proposed a method to extract a group of people simultaneously. People in this group are related to one another so their relevant web page set has a greater number of pages; these pages may share the same topic and be connected. The authors proposed two methods of extracting a group of people: one uses link information in web pages and the other uses the Agglomerative Conglomerative Double Clustering (A/ method is limited because, when we search for a person on the web, we may not know his or her social net-work in advance.
 people. Mann and Yarowsky (2003) used the pattern-matching method ( Ravichandran &amp; Hovy, 2001 )to as DBLP 5 and Amazon, 6 to extract authors X  names and research keywords. These methods have some disad-vantages as follows. The method of extracting personal profiles may not work well with web pages other than base. Wan, Gao, Li, and Ding (2005) used natural language-processing techniques to extract named entities in documents. However, because web documents contain much noisy information, the extraction of named enti-ties may not work well.

Previous researchers have targeted several types of documents: articles in newspapers, web documents of famous people, and web documents containing biographic data. However, as the number of documents dis-seminated on the WWW is growing dramatically, there are other types of web documents that have not yet been targeted. In addition to the growing number of web documents, there are numerous variations of doc-ument format and writing style. Therefore, previous approaches are limited when working with such docu-ments. These limitations motivated us to develop a new method that can treat different kinds of web documents existing on the web. The characteristic mark of our approach is that we use a form of complement in previous research, such as the DBLP or the Amazon online bookshop, so they can work with documents exist on the WWW, so the preparation cost is inexpensive. 3. Document similarities via a knowledge base 3.1. A review of the tf-idf weighting scheme
The vector space model ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ) is the conventional method for measuring the similarity of two documents. In the vector space model, a document is represented by a feature vector formed weighting scheme, term weights are calculated using the terms X  occurrences in the document concerned and in document concerned, so its weight should be proportional to its number of occurrences in the document. The itively, if a term appears frequently in many documents its particularity decreases.
 Denote a set of N documents as S doc  X f doc 1 ; doc 2 ; ... ; doc where tf  X  t ; doc  X  is the number of times term t appears in the document doc .

The vector space model based on the tf-idf weighting scheme measures the similarity of two documents by using the inner product of document feature vectors. It works well when the two documents concern the same
Although the tf-idf weighting scheme works well with documents on the same topic, it may not work well with documents relevant to the same person, as they have very few terms in common. There are two reasons for this. First, documents relating to the same person need not to be about the same topic. Rather, they may have slightly different specific topics under the same general topic; therefore, common terms between docu-ments are rare. Second, because documents on the web contain noisy information, only text surrounding a person X  X  name seems to be relevant to that person, not the whole document. This further reduces the number of common terms. 3.2. Measurement of term weights using a knowledge base
The tf-idf weighting scheme is limited when measuring documents relevant to the same person. We propose web directories to improve the measurement of term weights. 3.2.1. A knowledge base
As described in Section 3.1 , text relevant to a person in a web document is short. Therefore, even keywords measure term weights. We call such a collection a knowledge base, because it collects knowledge of several on several topics and we use  X  X  X  directory  X  to refer to a set of documents on the same topic. We name our method  X  X  X imilarity via Knowledge Base (SKB)  X  to separate it from the vector space model based on the scheme as the traditional vector space model, or the VSM for its abbreviation. 3.2.2. Modification of term weight in documents
In a web document, text relevant to a person tends to be short because only a part of the document men-tions the person and the web document may contain noise. Therefore, term weights calculated by Eq. (2) for keyword terms and for other terms differ only slightly.

Assume we have a directory whose topic is close to the document X  X  topic. As the directory has abundant text, keywords related to the topic appear more frequently, so their term weights will be larger than the weights of other terms. It is reasonable to assume that keywords will appear on the web document as fre-we can use the large weights of keyword terms in the directory to amplify the small weights of keyword terms in the document.
 documents in S dir , tf  X  t ; dir i  X  as the number of times term t appears in the directory dir calculate term weights for the feature vector of directory dir
Because we have to compare feature vectors between directories in the next calculation step, we normalize term weights by dividing them by the lengths of the directories to facilitate comparison.
We modify the term weights in documents by taking the mean of the term weights calculated by Eqs. (2) and (4) . We have tested the arithmetic mean and the geometric mean, and the geometric mean gives the better result of the two, because, when taking the arithmetic mean, terms that do not appear in the document have the document. Following on from this experimental result, we use the geometric mean in our research.
The details of term weight modification can be formalized as follows:
Our modification of term weights functions analogously to a signal frequency filter. A document can be regarded as an information source and the set of all terms can be regarded as a range of frequencies. A doc-ument feature vector corresponds to a power spectrum, where a term weight corresponds to the power at a the weights of the other terms. 3.2.3. Modification of term weight in directories
The idf factor in the tf-idf weighting scheme can be explained using the information entropy theory. For example, in Aizawa (2000) , the author explained the idf  X  log N amount gained by t . Without the observation that t appears in a document d , d can be any document from
The explanation by Aizawa (2000) assumed that contexts of documents in the collection were independant to ever, for our directories, documents in the same directory are supposed not to be independant to each other; some documents may have common contexts. If a term t that appears frequently in a certain directory but containing t seem to have common context. Therefore, its gain of information amount should be increased.
We define the normalized document frequency of a term in a directory and in all directories as follows: where df  X  t ; dir i  X  is the number of documents in dir i dir
We assume that when terms have normalized frequencies in a certain directory that are much larger than cerned. We propose the following equations that can appropriately increase idf weights for topic terms: where r is a given threshold that we call the document frequency ratio threshold.

We combine the modification Eq. (9) of term weights in directories with the modification Eq. (5) of term weights in documents and obtain the following equations to measure term weights:
Our idea of using information from directory structure to modify term weights of topic terms has common
Kohonen et al. (2000) . Both our approach and the approach in Kohonen et al. (2000) utilize the term prob-(2000) , training documents and test documents are from the same source and term weights for test documents are calculated using their entropies in training documents. However, in our approach, web directories and name ambiguous documents are from different sources, so we only use term weights in web directories to mod-ify the term weight measurements in name ambiguous documents as above. 3.3. Measurement of document similarities
The measurement of document similarities is performed in two steps. First, we find directories that have topics close to that of the document. Then, we measure the document similarities using these selected direc-tories. The details are as follows: 3.3.1. Find directories close in topic with the document
Because we do not know the documents X  topics in advance, we have to guess their topics. For each docu-ment, we choose k directories in the knowledge base whose similarities to the document are the top k largest values. The similarity between a document d and a directory Dir is measured as follows: where tf -idf SKB  X  t ; doc ; dir  X  is replaced by tf -idf
We call these top k directories of document doc the document X  X  representative directories and denote this set of directories as R  X  doc  X  . 3.3.2. Measure document similarities
Denote a pair of documents as  X  doc 1 ; doc 2  X  . For each directory dir calculate the similarity between documents doc 1 and doc 2 ilarity of the document pair  X  doc 1 ; doc 2  X  4. Name disambiguation system
Fig. 1 is an overview of our name disambiguation system. The system takes a knowledge base and docu-ments of namesake people as input data. Then, it calculates document similarities between documents and helps users to find the desired person by reranking documents so that documents relevant to the person of interest go to the top of the list. The operational details are as follows: (1) Preprocessing documents (2) Calculation of document similarities (3) Discrimination by reranking documents 5. Experiments 5.1. Data sets 5.1.1. Documents of people
We selected 24 names as shown in the right column of Table 1 . For each name, there was a particular per-son with that name who specialized in the research field shown in the left column of Table 1 . We sent each name to the Google search engine and selected the top 100 results. In each result set, there were documents to other people. The documents for all these people were used in our experiments. We removed documents that were not html documents. For each name, the person bearing that name and specializing in the field shown in the left column of Table 1 was associated with from 10 to 50 documents, whereas other people bear-ing that name were associated with between one and 10 documents. Table 2 shows the number of people and the number of relevant documents. The first and third columns show the number of relevant documents, the second and fourth columns show the number of people who had that number of relevant documents. 5.1.2. Creation of pseudo namesake document sets and real namesake document sets
In order to get a number of test data, we created name-ambiguous documents artificially as follows. We selected two result sets corresponding to the names of two people belonging to different research fields and mixed them together. Then, we replaced the personal names in the documents by the name X to create a set of documents of pseudo namesakes. In each mixed data set, there were two people with different profes-sions, each with between 10 and 50 relevant documents. Besides these two people, there were several other people with between one and 10 relevant documents. For example, we mixed together the  X  X  X om M. Mitchell  X  four research fields and six names in each research field, we could create 6 6 names and produce 216 sets of pseudo namesake documents.

Besides experiments on pseudo namesake document sets, we also did carry experiments on real namesake document sets in order to verify the performance of our approach with real problems. The 24 real namesake document sets are result sets of 24 name queries shown in Table 1 . 5.2. Web directory structures
We selected three well-known web directories on the WWW: the Google directory ( http://direc-tory.google.com ), the Yahoo directory ( http://dir.yahoo.com ), and the Dmoz directory ( http://dmoz.org ). are shown in Table 3 . 5.3. Baseline methods
We compared our method with two conventional methods: VSM and named entity recognition (NER). 5.3.1. Vector space model method
In the VSM method, we removed stop words and stem words to their root form by using the Porter stem-ming algorithm. Then, we chose the terms inside the text windows centered at the personal name queries. We used Eq. (2) to calculate the weight of these terms and built the feature vectors of documents. We took the inner products of document feature vectors for the similarities between document pairs. 5.3.2. Named Entity Recognition method
In the NER method, we used the LingPipe software 8 to extract the entity names in the documents. Then, we (1 if a name appears in the document, 0 otherwise). We took the inner products of the document feature vec-tors for the similarities between documents. 5.4. Evaluation metrics
As described in Section 4 , our system disambiguates people by reranking the result documents based on a document selected by the user. Therefore, we assumed that the user may choose any document doc result set, and evaluated the performance of the reranking result based on that document doc the precision values at 11 recall points: 0%, 10%, 20%, ... ,90%, and 100% and denoted these as
P  X  doc i ; 0 %  X  ; P  X  doc i ; 10 %  X  ; P  X  doc i ; 20 %  X  ; ... ; P  X  doc averaged precision values at these 11 recall points for all possible reranking sequences as follows: where k = 0, 10, 20, ... ,90, and 100.
 We also took the averaged value of these 11 averaged precision values 5.5. Experimental results In this section, we compare the experimental results of our SKB methods and those of baseline methods
VSM and NER with the documents of people as described in the Section 5.1 . Furthermore, we also investigate the robustness of our SKB methods over changes in directory structures and varying parameters. We applied six directory structures described in Section 5.1 to our SKB methods and investigated performance. We also varied the window size parameter n , and the number of representative directories parameter k to verify the robustness of SKB methods. We experimented with the document frequency ratio threshold in Eq. (8) , directories parameter, k  X  10 ; 20 ; and 30. 5.5.1. The overall performance for each method
Figs. 2 X 7 show the precision X  X ecall graphs for the SKB methods using different directory structures and their comparisons with the baseline methods. Tables 4 and 5 show the comparison in terms of the averaged precision value P aver between the baseline methods VSM, NER and our proposed methods SKB1 and
SKB2. In this experiment, we set the window size n  X  50 and the number of representative directories k  X  20. We set the frequency document ratio threshold for SKB2 r  X  5. As can be seen from these Tables, our SKB1 and SKB2 methods together with six different directory sets outperform the baseline methods
VSM and NER. 5.5.2. Performance of SKB2 when varying the document frequency ratio threshold
We experimented with SKB2 using different threshold values for document frequency ratio threshold: r  X  1 ; 2 ; 5 ; 10. The directory structures used in these experiments were Dmoz10, Google10, and Yahoo10.
Table 6 shows the experimental results. As can be seen from this table, SKB2 achieves good performances, that a term has, the larger the document frequency ratio it has. 5.5.3. Performance of SKB systems when varying the window size
Tables 7 and 8 show the performance variations with different window size parameters. In these experi-ments, we used the Google20 directory structure with the number of representative directories set to 10. As can be seen from the results in these two tables, the SKB1 and SKB2 methods achieve better performance when the window size increases. We also experimented with the VSM method with different window size parameters. As shown in Table 9 , we noted that the performance values of the VSM method decreased slightly when we increased the window size.

From the performance value decrease of the VSM method, we learn that the further the text is from the personal names, the more noise it contains. On the other hand, from the increased performance values of far from the personal names.
 5.5.4. Performance of SKBs when varying the number of representative directories
Tables 10 and 11 show the different performances with different number of representative directories k  X  10 ; 20 ; 30. In this experiment, we used the Google20 directory structure with the window size fixed at 50. We recognize from the results shown in these two Tables that SKB1 and SKB2 methods achieved improved performance when the numbers of representative directories changed from 30 to 20 and 10. 5.5.5. Performance for each method on real namesake document sets
Table 12 shows the performance comparisons between VSM, NER, and SKB2 in the experiments on real namesake document sets. We use the averaged precision values at 11 recall points for all possible reranking sequences in the comparisons. The results show that our SKB2 performs best in 18 sets, follows by the
NER performs best in 5 sets, and the VSM performs best in 1 sets. On average, our SKB2 also outperforms the baseline methods VSM and NER. 6. Discussion
In this section, we describe how we exploited the web directories. We also note the advantages and the dis-advantages of our method that uses web directories when disambiguating people.

Disambiguation of people in web documents is challenging because web documents are published by resources of different kinds, and useful information is mixed with noise. To improve effectiveness when pro-cessing web documents, we propose a new method that uses web directories to aid the extraction of the doc-uments X  features and the measurement of documents X  similarities. We use information from directories to improve the calculation of the vector space model. The key to our approach is that web directories provide information about the relationship between directories X  documents themselves and the relationship between directories X  documents and other documents; these relationships cannot be found in the conventional vector space model method. We have proposed two approaches to exploit information from web directories. First, by investigating the relationship between documents referring ambiguous personal names and the documents on the web directories, we can improve the measurement of term frequencies in a document. Compared with the vector space model method and the named entity recognition method, we have improved the averaged preci-sions from 3.9% to 9.7%, and from 12.4% to 18.7%, respectively. Furthermore, we can exploit the relationship between the documents in the same web directories. Using this relationship, we can differentiate topic terms from common terms, even if they have the same characteristic in that they appear frequently in some docu-ments. This exploitation can be regarded as an attempt to measure topic frequencies of terms. Although, we cannot count topic frequencies precisely, we can use web directories to modify term frequencies in docu-ments to approach topic frequencies. This second exploitation results in a further improvement of averaged research precision from 6.8% to 12.9%, and from 15.5% to 22.2% over the VSM method and over the NER method, respectively.

We investigated the robustness of our approaches over changes of directory structure and variation of sys-tem parameters. The experimental results with different directory structures and different system parameters support the conclusion that our SKB methods achieve stable performance.

From the practical point of view, our approach has advantages as well as limitations. The greatest advan-cost of computation. The increase is proportional to the number of directories used. 7. Conclusions
Disambiguation of people in web searches is an increasing requirement for the new trends in web search systems. We propose a new method that uses web directories as a knowledge base to improve the disambig-uation performance. Using web directories, we propose two approaches to better measure term weights. We have experimented with our approaches using several existing web directories to disambiguate documents of people on the web. The results showed a significant improvement with our system over the conventional meth-ods: the vector space model method and the named entity recognition method. We also verified the robustness of our methods experimentally with different web directory structures and with different parameter values. References
