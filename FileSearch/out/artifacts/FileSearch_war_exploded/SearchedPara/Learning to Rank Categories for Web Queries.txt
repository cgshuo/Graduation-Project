 In web search, understanding the user intent plays an im-portant role in improving search experience of the end users. Such an intent can be represented by the categories which the user query belongs to. In this work, we propose an in-formation retrieval based approach to query categorization with an emphasis on learning category rankings. To carry out categorization we first represent a category by web doc-uments (from Open Directory Project) that describe the se-mantics of the category. Then, we learn the category rank-ings for the queries using  X  X earning to rank X  techniques. To show that the results obtained are consistent and do not vary across datasets, we evaluate our approach on two datasets including the publicly available KDD Cup dataset. We re-port an overall improvement of 20% on all evaluation metrics ( precision , recall and F  X  measure ) over two baselines: a text categorization baseline and an unsupervised IR base-line.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Algorithms, Experimentation Query categorization, query classification, learning to rank
In this work, we address Query Categorization (QC), which involves classifying a given query into one or more pre-defined categories which the query can belong to; thus effectively providing a means to understand the user intent of the query. Such a topical categorization of queries into pre-defined cat-egories can have many applications. The need for QC origi- X 
Part of the work described herein was conducted while the first author was an intern at Yahoo! Labs India.
 nated from a federated search p aradigm where the task is to search simultaneously various multiple online databases and web resources. In a federated s earch scenario, knowing the categories of a query beforehand can help in determining the sources of evidence for vertical selection [1]. Secondly, QC can also be used in categorical organization of the search results [7]; thus helping the user easily navigate through the results. Advertisements relevant to the user query can be placed on the search results page, thus enriching the users X  search experience [2].

Most of the existing approaches [5, 6] treat QC as a text categorization problem, and solve the problem accordingly. Text categorization [3] has been a well studied problem. Though query categorization can be viewed as a text cat-egorization problem as approached in [5, 6], QC is compli-cated by the presence of very short text (short queries). On the other hand, we propose an information retrieval based approach(Section3)withanattempttolearncategory rankings for a query. Here, we try to learn the rankings which capture query-to-category relationships using an al-ready categorized document corpus such as Open Directory Project 1 (ODP). We evaluate the performance of our system on two different datasets (Section 4) including the publicly available KDD Cup 2005 2 dataset.

This work aims at answering the following research ques-tions relevant to QC: 1) can we solve query categorization by treating it purely as an Information Retrieval (IR) prob-lem?, 2) can we utilize already categorized documents to perform query categorization?, 3) can we learn to rank the categories for a given query? QC is formally defined in Definition 1.
 Definition 1. Suppose there exists a set of L categories C = { c 1 ,c 2 ,  X  X  X  ,c L } . Assume that we are given a set of n queries Q train = { Q 1 ,Q 2 ,  X  X  X  ,Q n } and for each query Q in Q train a set of relevant categories C ( Q )  X  X  is given. Now the goal is to learn a ranking model h (  X  ) such that for any query Q ( i ) , the learnt model h canfindasubset C ( i ) of the set C : such that, rel ( c ( i ) j )  X  rel ( c ( i ) j +1 ) where 1 L ,andwhere rel ( c ( i ) j ) indicates the relevance of a category c j for the given query Q http://www.dmoz.org/ http://www.sigkdd.org/kdd2005/kddcup.html Here, L can vary over time, but it is safe to assume that L will be fixed over a reasonable amount of time. rel (  X  is relevance function that returns relevance score of a cate-gory for a given query. Categories are usually hierarchically organized.
We present an IR based approach to query categorization with an attempt to learn category rankings for a given web query. However at the same time, application of IR tech-niques to solve QC is not a straightforward task since the category names for queries may not contain sufficient con-text which is required to retrieve and rank categories in order to solve QC with an IR approach. To address this problem, we first represent the categories by a set of documents that better describe the semantics of the categories. We follow it up by discussing query representation for this approach which is required to retrieve documents that are topically similar to the query. Then, we present  X  X earning to rank X  for categorizing queries along with a discussion on ranking features used.
Solving QC as an IR problem requires category repre-sentation since category names themselves do not contain enough context. We chose to use the categories of Open Di-rectory Project (ODP) as the set of pre-defined intermediate categories ( C ( I ) ) since it has categories that cover most of the web and for each category, there are web pages grouped under that category. The web pages associated with a cat-egory describe its semantics. At the time of documenting this work, ODP had over 4.5 million web pages, organized into approximately 590K categories. Ignoring non-English web pages and categories, we ended up with 2.4 million web pages, organized into approximately 380K categories. We can safely assume that the documents in a category contain the concepts that describe the semantics of the category and hence the documents represent the category.

Now, we have a set of pre-defined categories and a corpus of documents describing the semantics of those categories. These data are needed to perform QC task in our approach since these data enable us to treat QC as an IR problem. Notations C ( I ) and D 3 now represent the categories in ODP and corpus of web pages in ODP, respectively. For each document d  X  X  , we have a set of categories C d  X  X  ( I ) asso-ciated with the document. Or conversely, for each category c  X  X  ( I ) , there is a set of documents D c  X  X  that define the semantics of the category. Now that we have a document corpus representing the semantics of set of pre-defined cat-egories, we proceed to represent the query so that concepts defining the query are covered.
Usually, users express their information need through a short query, often containing a few words. Hence, we need a way to represent a query such that the newly formed query is able to identify the most relevant documents which are topically identical to the query since our task is to carry out
Henceforth, we refer to the ODP web pages collected for, defining and representing categories, by the corpus D .Un-less explicitly stated, by D , we mean the ODP documents corpus and by C ( I ) , we mean the categories in ODP space. topical categorization of a query. We emphasize here that retrieved documents need not be highly relevant to a query, those documents which are topically similar to the query are good enough to carry out the categorization process. In fact, even a document that is not relevant to the query but topically similar to the query would assist the system in query categorization. For example, assume that the query is  X  X afael nadal X . In this case, the documents that are not related to concept  X  X afael nadal X  but are related to concepts like  X  X ennis X ,  X  X ports X  are good enough to aid QC. That is, even a document talking about  X  X oger federer X  would help in categorizing the query  X  X afael nadal X  since the document is topically similar to the query. Hence, we claim that our system can even benefit from the topically similar irrelevant documents as well.

Also, this step is indeed needed, since our corpus D may not contain the exact query. As an example, assume that the query  X  X erena williams X  does not have an exact match in the corpus. However, when the query is represented with key concepts like  X  X ennis X ,  X  X ta X ,  X  X imbledon X , there is a high probability that the expanded query, now, identifies some related documents from D which are topically similar to the query  X  X erena williams X . Each of these retrieved documents is associated with a category, in turn associating the query with these categories. We want to learn such associations (query-to-category) through a  X  X earning to rank X  framework.
Thus, we claim that even in the absence of an exact query match, the documents that match the query-related top-ics/concepts/terms are good enough to carry out the query categorization. One of the advantages of applying IR to QC over applying IR to document retrieval is that QC does not require that the retrieved documents to be relevant to the query but instead it requires the documents to be categor-ically similar to the query. This fact is leveraged in our information retrieval approach to QC and forms the basis of our work. Once the documents which are topically re-lated to query are retrieved, we attempt to  X  X earn to rank X  these documents for a query effectively resulting in query categorization.

To expand a query for concepts, we submit the query to a web search engine and we collect the title, and a snippet of top N hits. Unlike other systems which work with the whole text content of top hits for a query, we chose to work with only title and snippet of the top hits, since most of the times context of the query is clear through the title and snippet of the top hits.

Once the text in title and snippet of top N hits is obtained through the use of a web search engine, we stem each term in the text and rank the stemmed terms by document fre-quency metric (in the process we filter out stop words since they do not any add useful information to determine query topics. We use a static list of commonly occurring words as stop words.). Document frequency ( df )ischosenwiththe intuition that terms that occur in most of the top N hits, for a query, are most likely concepts that can represent the query. Each stemmed term t in the text of title and snippet of top N hits is weighted by: where D N q represents the set of (title, snippet) pairs of the top N hits, obtained for a query q and 1( p ) indicates truth value of the proposition p and is given by:
We use the top n stemmed terms to represent the query.
Before introducing learning to rank approach, we discuss the category mapper (which maps ODP categories to the target ones) since our target categories are different from that of ODP categories. We map an intermediate category to a target category by matching stemmed keywords be-tween them. An intermediate category c ( I ) gets mapped to one of the target categories as follows: where f ( t, c ( I ) )= 1 1+ level at which the target category t occurs in the interme-diate category c ( I ) which needs to be mapped. level ( t, c returns zero if target category does not occur. In the inter-mediate category  X  X ports/Resources/News and Media X , tar-get category  X  X ports X  is occurring at level 1 and  X  X ews X  is occurring at level 3.

Given a set of intermediate categories C ( q ) for a query, the categories in target space to which most of the categories in C ( q ) are getting mapped to are ranked higher, and top cate-gories in target space are returned as the probable categories of the query.
Having described both category and query representation which enable us to treat query categorization as an IR prob-lem, we now proceed to discuss how an attempt is made at applying  X  X earning to rank X  techniques to solve QC. A de-tailled study of  X  X earning to rank X  can be found in the book [4]. First we represent each category by a set of documents that describe the semantics of the category as described ear-lier. At the end of this representation we have a resulting corpus D where each document belongs to at least one cat-egory as described earlier. Now we construct training data (
X ) from labeled query dataset along with relevance judg-ments so that the resulting data is in a form that can be consumed by a  X  X earning to rank X  algorithm. That is, each (query, document) pair is represented by a set of ranking features (to be discussed in the following section). Then, we apply learning to rank techniques such as Ranking SVM to learn the ranking model L ( T ) from the training data ( X
Now given any query q t ,weusethismodel( L ( T ) )torank the categories. First we retrieve the documents from the corpus D and rank them using the learnt model. Then, categories of these documents are mapped to the subject categories under consideration using the category mapper discussed earlier.
The features used for learning category rankings are sim-ilar to the ones used by  X  X earning to rank X  community. We included features such as relevance scores on different fields of the document:  X  X itle X ,  X  X ody X ,  X  X ategory X  and  X  X hole doc-ument X . We considered IR models such as TF*IDF, Okapi BM25, and language modeling for IR to compute features. We considered three smoothing techniques with Language Modelling, namely, Dirichlet, Jelinek-Mercer and two stage smoothing. In all, we had 20 features describing each (query, document) pair. We used Lemur 4 toolkit to compute these features.
We evaluate our QC system for applications such as fed-erated search. Federated search requires high precision pre-diction of categories for a query since the cost of selecting a wrong vertical is very high. A wrong vertical search re-sult would also affect the search experience of the end users. Hence, in this evaluation, we will weight the precision much higher than recall while measuring the performance of the systems.

We conducted a thorough evaluation of our approach on two datasets: classified 7K US queries sampled from Yahoo! Search and the publicly available KDD Cup 2005 dataset. For all experiments in the section, we set the value of N , numberoftophitsusedtorepresentthequery,to40for performance reasons. To avoid noise, we pick only top 10 stemmed terms to represent the query according to Equation 1. Since we want our application to be precise in predicting categories, we pick top 50 documents after ranking. Again this parameter is application dependent and less interesting in the context of the paper. In both the evaluations, we use our system as a binary categorizer.
We use the evaluation measures defined by KDD Cup 2005 competition. This evaluation is similar to the one used by text categorization community. The evaluation measures X  precision, recall, and F  X   X  X re defined, respectively, as fol-lows: P recision =
Most of the existing works are evaluated either on a smaller dataset or on an older dataset and hence it is not clear about the practical application of such systems. In reality, a web search engine is expected to deal with billions of queries a day. Thus we evaluate our system on a larger and recent dataset. In this section, we brief on the details of our edi-torial dataset and also present the results obtained on this dataset compared against an unsupervised baseline and a supervised text categorization baseline.
This editorial dataset consists of queries sampled from search query logs of Yahoo! Search over 13 weeks from Jan-uary to March 2010. All duplicate queries were removed from this sampled set resulting in about 7k queries. Pro-fessional editors were provided with a taxonomy containing http://www.lemurproject.org/ 21 categories: Autos, Finance, Food, Games, Health, Im-age/Video, Jobs, Local/Regional, Movies, Music, TV, Me-dia, News, People Search, Product/Shopping, Real Estate, Reference, Social Networking, Sports, Technology, Travel; targeting different verticals fo r federated search. They man-ually classified each query into at least one category.
We compare our system against two baselines: a super-vised text categorization based baseline and an unsupervised baseline constructed on an IR approach. Our text catego-rizer baseline consists of a supervised text categorization based categorizer. We chose text categorization method as one of the baselines since most of the existing approaches treat QC as a text categorization problem and solve accord-ingly. We train an SVM using only word features of the queries to categorize queries since SVMs are the best text categorizers known to date [3].

Our second baseline, IR based approach, was constructed as follows: all ODP documents were indexed with stemming. All 7k queries after stemming were searched against this in-dex to retrieve the relevant documents. Once the query relevant documents are retrieved they are mapped to the target taxonomy using the category mapper discussed ear-lier. In this way categories for every query is predicted. We chose to predict at most one category for each query since predicting more than one category for a query would require a thresholding strategy to avoid false positives which might affect applications such as federated search.
 Approach Precision Recall F 0 . 5 Text Categorizer ( B 1 ) 39.56% 30.82% 37.03% IR baseline ( B 2 ) 40.09% 31.65% 38.06% Our System 48.14% 37.92% 45.68% Improvement over B 1 21.68% 23.03% 23.33% Improvement over B 2 20.09% 19.81% 20.02% Table 1: Relevancy measures on 7k classified queries sampled from search query logs of Yahoo! Search for the year 2010.

We report the relevancy metrics precision, recall and F 0 scores of our system and baseline in Table 1. For our system and text categorizer baseline, we report the five fold cross validation results on those 7k queries. The results indicate that our system has an overall improvement of around 20% over both the baselines on all measures. The results obtained for our system have significant improvement over the two baselines, indicating that learning to rank could be the key in improving precision of QC systems. The importance of query categorization was highlighted in KDD Cup 2005. The task was to classify 800K web queries into one or more of 67 pre-defined categories. Each query had to be classified into a ranked list of at most 5 categories. The categories were organized into two levels, with 7 nodes at top level. After the competition, a set of 800 queries, labeled independently by three labelers was also released for evaluation. Each query was labeled into at most 5 categories by each labeler. We omitted categories of the type  X  X thers X  from the evaluation, since finding the mapping was not easy.
As three labelers labeled the evaluation set of 800 queries, the results reported in this section are averaged over the values evaluated on each of them.
 Table 2: Precision and F1 values on evaluation set of KDD Cup 2005 data set.

We use our system as an off-the-shelf query categorizer without tuning the system in anyway for evaluation on KDD Cup dataset. That is we do not retrain the model on KDD dataset, but instead we directly use the  X  X earning to rank X  model learnt from the editorial evaluation (Section 4.2). We compare our system against the existing systems in Table 2. We believe that the results obtained so far complement our goal of improving precision for applications such as federated search.
We attempted to solve QC by applying  X  X earning to rank X  technique to learn rankings of categories for a query. This re-quired us to represent a category by set of documents which better describe the semantics of that category. This setting was required so that we could apply IR techniques and sub-sequently  X  X earning to rank X  algorithm to perform QC even-tually. As future work, it will be interesting to see if cat-egories can be better represented by other semi-structured data such as Wikipedia. Advantage with Wikpedia data is that, it has its own category taxonomy and every Wikipedia article is well structured with well defined topics.
We are thankful to Yahoo! Labs, Bangalore for granting access to the classified query dataset. [1] J. Arguello, F. Diaz, J. Callan, and J.-F. Crespo. [2] A. Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich, [3] T. Joachims. Text categorization with suport vector [4] T.-Y. Liu. Learning to rank for information retrieval. [5] D. Shen, R. Pan, J.-T. Sun, J. J. Pan, K. Wu, J. Yin, [6] D. Shen, J.-T. Sun, Q. Yang, and Z. Chen. Building [7] X. Wang and C. Zhai. Learn from web search logs to
