 1. Introduction
Audio data is an integral part of many modern computer and multimedia applications. A typical multimedia database often contains millions of audio clips, including environmental sounds, machine noise, music, animal sounds, speech sounds, and other non-speech utterances. The effectiveness of their deployment is greatly dependent on the ability to classify and retrieve the audio files in terms of their sound properties or content. The need to automatically recognize to which class an audio sound belongs makes audio classification and categorization an emerging and important research area. However, a raw audio signal data is a featureless collection of bytes with most rudimentary fields attached such as name, file format and sampling rate.

Content-based classification and retrieval of audio sound is essentially a pattern recognition problem in which there are two basic issues: feature selection and classification based on the selected features. In the first step, an audio sound is reduced to a small set of parameters using various feature extraction techni-ques. The terms linear predictive coefficients (LPC), linear pre-dictive cepstral coefficients (LPCC), mel-frequency cepstral coefficients (MFCC) refer to the features extracted from audio
A new approach towards high performance speech/music dis-crimination on realistic tasks related to the automatic transcription of broadcast news is described in Ajmera et al. (2003) ,inwhichan artificial neural network (ANN) and hidden Markov model (HMM) are used. In Kiranyaz et al. (2006) , a generic audio classification and segmentation approach for multimedia indexing and retrieval is described. A method is proposed in Panagiotakis and Tziritas (2005) for speech/music discrimination based on root mean square and zero-crossings. The method proposed in Eronen et al. (2006) investigates the feasibility of an audio-based context recognition system where simplistic low-dimensional feature vectors are eval-uated against more standard spectral features. Using discriminative training, competitive recognition accuracies are achieved with very low-order hidden Markov models ( Ajmera et al., 2003 ).
The classification of continuous general audio data for content-based retrieval was addressed in Li et al. (2001) , where the audio segments were classified based on MFCC and LPC. They also showed that cepstral-based features gave a better classification accuracy. The method described in content based audio classifica-tion and retrieval using joint time X  X requency analysis exploits the non-stationary behavior of music signals and extracts features that characterize their spectral change over time ( Esmaili et al., 2004 ).
The audio signals were decomposed in Umapathy et al. (2005) , using an adaptive time X  X requency decomposition algorithm, and the signal decomposition parameter based on octave (scaling) was used to generate a set of 42 features over three frequency bands within the auditory range. These features were analyzed using linear discriminant functions and classified into six music groups.
An approach given in Jiang et al. (2005) uses support vector machine (SVM) for audio scene classification, which classifies audio clips into one of five classes: pure speech, non-pure speech, music, environment sound, and silence. Radial basis function neural networks (RBFNN) are used in McConaghy et al. (2003) to classify real-life audio radar signals that are collected by a ground surveillance radar mounted on a tank.
 For audio retrieval, a new metric has been proposed in Guo and
Li (2003) , called distance-from-boundary (DFB). When a query audio is given, the system first finds a boundary inside which the querypatternislocated.Then,alltheaudiopatternsinthedatabase are sorted by their distances to this boundary. All boundaries are learned by the SVMs and stored together with the audio database.
In Mubarak et al. (2005) a speech/music discrimination system was proposed based on Mel-frequency cepstral coefficient (MFCC) and
GMM classifier. This system can be used to select the optimum coding scheme for the current frame of an input signal without knowing a priori whether it contains speech-like or music-like characteristics. A hybrid model comprised Gaussian mixtures models (GMMs) and hidden Markov models (HMMs) is used to modelgenericsounds withlargeintraclassperceptualvariationsin
Rajapakse and Wyse (2005) . The number of mixture components in the GMM was derived using the minimum description length (MDL) criterion.

A new pattern classification method called the nearest feature line (NFL) is proposed in Li (2000) , where the NFL explores the information provided by multiple prototypes per class. Audio features like MFCC, ZCR, brightness and bandwidth, spectrum flux were extracted ( Lu et al., 2003 ), and the performance using
SVM, K-nearest neighbor (KNN), and Gaussian mixture model (GMM) were compared. Audio classification techniques for speech recognition and audio segmentation, for unsupervised multispea-ker change detection are proposed in Huang and Hansen (2006) .
Two new extended-time features: variance of the spectrum flux (VSF) and variance of the zero-crossing rate (VZCR) are used to preclassifytheaudioandsupplyweightstotheoutputprobabilities of theGMMnetworks.The classificationisthenimplementedusing weighted GMM networks.
The difference between the actual and the predicted sample value is termed as the prediction error or residual, and is given by e  X  n  X  X  s  X  n  X  ^ s  X  n  X  X  s  X  n  X  X   X 
The LP coefficients f a k g are determined by minimizing the mean squared error over an analysis frame and it is described in Rabiner and Juang (2003) .

Therecursiverelation(4)betweenthepredictorcoefficientsand cepstral coefficients is used to convert the LP coefficients into LP cepstral coefficients f c k g : c  X  ln s 2 c  X  a m  X  c  X  where s 2 isthegain terminthe LPanalysisand D isthe numberofLP cepstral coefficients. The cepstral coefficients are linearly weighted to get the weighted linear prediction cepstral coefficients (WLPCC).
In this work, a 19-dimensional WLPCC is obtained from the 14th order LP analysis for each frame. Linear channel effects are com-pensated to some extent by removing the mean of the trajectory of each cepstral coefficient. The 19-dimensional WLPCC (mean sub-tracted) for each frame is used as an acoustic feature vector. 2.2. Mel-frequency cepstral coefficients
The mel-frequency cepstrum has pr oven to be highly effective in recognizing structure of music signals and in modeling the subjective pitch and frequency content of audio signals. Psychophysical studies have found the phenomena of the mel pitch scale and the critical band, and the frequency scale-warping to the mel scale has led to the cepstrum domain representation. The mel scale is defined as
F  X  where F mel is the logarithmic scale of f normal frequency scale. The which are computed from the fast Fourier transform (FFT) power coefficients. The power coefficients are filtered by a triangular bandpass filter bank. When c in (5) is in the range of 250 X 350, the number of triangular filters that fall in the frequency range 200 X  1200 Hz (i.e.,the frequency range of dominant audio information) is value of c in that range for calculating MFCCs. Denoting the output of the filter bank by S k ( k  X  1,2, y , K ),theMFCCsarecalculatedas c  X  r X K 3. Modeling techniques for audio classification 3.1. Autoassociative neural network models
Autoassociative neural network models ( Palanivel, 2004 ) are feedforward neural networks performing an identity mapping of the input space, and are used to capture the distribution of the determines the shape of the hypersurface obtained by the projec-tion onto the lower dimensional space. Fig. 3 (b) shows the space spanned by the one-dimensional compression layer for the two-dimensional data shown in Fig. 3 (a) for the network structure 2 L 10 N 1 N 10 N 2 L , where L denotes a linear unit and N denotes a nonlinearunit.Theintegervalueindicatesthenumberofunitsused in that layer. The nonlinear output function for each unit is tanh( s ), where s is the activation value of the of the unit. The network is trained using backpropagation algorithm ( Yegnanarayana, 1999;
Haykin, 2001 ). The solid lines shown in Fig. 3 (b) indicate mapping of the given input points due to the one-dimensional compression layer. Thus, one can say that the AANN captures the distribution of the input data depending on the constraints imposed by the structure of the network, just as the number of mixtures and
Gaussian functions do in the case of Gaussian mixture models (GMMs).

In order to visualize the distribution better, one can plot the error for each input data point in the form of some probability that p i is not strictly a probability density function, but we call the resulting surface as probability surface. The plot of the probability surface shows a large amplitude for smaller error E i , indicating better match of the network for that data point. The constraints imposed by the network can be seen by the shape the error surface takes in both the cases. One can use the probability surface to study the characteristics of the distribution of the input data captured by the network. Ideally, one would like to achieve the best probability surface, best defined in terms of some measure corresponding to a low average error. 3.2. Gaussian mixture models
The probability distribution of feature vectors is modeled by parametric or nonparametric methods. Models which assume the shape of probability density function are termed parametric. In nonparametric modeling, minimal or no assumptions are made regarding the probability distribution of feature vectors. In this section, we briefly review Gaussian mixture model (GMM) for audio classification. The basis for using GMM is that the distribu-tion of feature vectors extracted from a class can be modeled by a mixtureofGaussiandensities.Fora D -dimensionalfeaturevector x , the mixture density function for speaker s is defined as p  X  x = l s  X  X 
The mixture density function is a weighted linear combination of M component unimodal Gaussian densities f s i  X  X  . Each Gaussian density function f s i  X  X  is parameterized by the mean vector and the covariance matrix R s i using f  X  x  X  X  covariance matrix R s i , respectively. The mixture weights ( a s , a s 2 , ... , a s M  X  satisfy the constraint P M i  X  1 a s parameters of the model l s are denoted as l s  X f a s i , l s i  X  1,2, y , M . The number of mixture components is chosen empiri-cally for a given data set. The parameters of GMM are estimated using the iterative expectation-maximization algorithm ( Redner and Walker, 1984 ). maleandfemale)indifferentlanguages,100cartoonclips,100clips of movie from different languages, 100 clips of sports and 100 clips of news (both Tamil and English). Audio samples are of different length, ranging from 1 s to about 10 s, with a sampling rate of 8 kHz and 16-bits per sample. The signal duration was slightly increased using the following rationale that the longer the audio signal analyzed, the better the extracted feature which exhibits more accurate audio characteristics. The training data should be suffi-cient to be statistically significant. The training data is segmented into fixed-length and overlapping frames (in our experiments we used 20 ms frames with 10 ms overlapping). When neighboring frames are overlapped, the temporal characteristics of the audio contentcan be taken into consideration in the training process. Due to radiation effects of the sound from lips, high-frequency compo-nents have relatively low amplitude, which will influence the capture of the features at the high end of the spectrum. One simple solution is to augment the energy of the high-frequency spectrum.
This procedure can be implemented via a pre-emphasizing filter that is defined as s u  X  n  X  X  s  X  n  X  0 : 96 s  X  n 1  X  , n  X  1 , ... , N 1  X  9  X  where s ( n ) is the n th sample of the frame s and s u  X  pre-emphasized frame is Hamming-windowed by h  X  n  X  X  0 : 54 0 : 46 cos  X  2 p n = N 1  X  , 0 r n r N 1  X  10  X  5.1. Preprocessing
The aim of preprocessing is to remove silence from a music sequence. Silence is defined as a segment of imperceptible audio, including unnoticeable noise and very short clicks. We use short-time energy to detect silence. The short-time energy function of a music signal is defined as
E  X  1 N where x ( m ) is the discrete time music signal, n is the time index of the short-time energy, and w ( m ) is a rectangular window, i.e., w  X  n  X  X 
If the short-time energy function is continuously lower than a certain set of thresholds (there may be durations in which the energy is higher than the threshold, but the durations should be shortenoughandfarapartfromeachother),thesegmentisindexed as silence. Silence segments will be removed from the audio sequence. The processed audio sequence will be segmented into fixed length and 10 ms overlapping frames. 5.2. Feature selection from non-silent frames
Feature selection is important for audio content analysis. The selected features should reflect the significant characteristics of different kinds of audio signals. The selected features include linear prediction coefficients (LPC)-linear prediction derived cepstrum coefficients (LPCC) and mel-frequency cepstrum coefficients (MFCC). LPC and LPCC are two linear prediction methods and they are highly correlated to each other. LPC-based algorithms ( Abu-El-Quran et al., 2006 ) measure three values from the audio segment to be classified. These values are the change of the energy ofthesignal,speechduration,andthechangeofthepitchvalue.The audio signals are recorded for 60 s at 8000 samples per second and divided into frames of 20 ms, with a shift of 10 ms. A 14th order LP analysis is used to capture the properties of the signal spectrum as described in Section 2.1. The recursive relation (4) between the predictor coefficients and cepstral coefficients is used to convert performance if N c is between 2 and 6, and the performance of the system decreases if it is less than 2 or more than 6. The decrease in the performance for N c o 2 indicates that there may not be a boundary between the components representing the acoustic information. The decrease in the performance for N c 4 6 indicates that the training audio data may not be sufficient for capturing the distribution of feature vectors.

Similarly, the performance is studied by varying the number of units in the second layer (expansion layer) keeping the number of units in the compression layer to 4. The experimental results show that theperformance of thesystem decreases ifthe numberof units in the expansion layer ( N e ) is decreased to 28 but it remains the same when the number of units in the expansion layer ( N e increased to 48. After some trial and error, the network structure 19 L 38 N 4 N 38 N 19 L is obtained. The structure seems to give good performance in terms of computation time and EER. For testing the featurevectorsextractedfromthevariousclassesaregivenasinput to the model and the corresponding class has the maximum confidence score. The performance of audio classification in terms of number of units in the expansion layer is shown in Table 1 . The classification results for the different features are shown in
Fig. 4 . From the results, we observe that the overall classification accuracy is 93% using MFCC as feature. 5.4. Modeling using GMM
The motivation for using Gaussian densities as the representa-tion of audio features is the potential of GMMs to represent an underlying set of acoustic classes by individual Gaussian compo-nents in which the spectral shape of the acoustic class is para-meterized by the mean vector and the covariance matrix. Also,
GMMs have the ability to form a smooth approximation to the arbitrarily shaped observation densities in the absence of other
The performance of the system for MFCC using AANN and GMM for audio classification is given in Table 2 .

A comparison in the performance of audio classification between the various categories for the feature vectors used (LPC, LPCC and MFCC) for ANNN is given in Table 3 .

A comparison in the performance of audio classification between the various categories for the feature vectors used (LPC, LPCC and MFCC) for GMM is given in Table 4 .
 The classification results for the different features are shown in
Fig. 4 . From the results, we observe that the overall classification accuracy is 93% using MFCC as feature. For movie audio and song audio separate experiments are conducted to study the perfor-mance of the audio retrieval and song retrieval algorithms in terms of the performance measures. 5.5. Indexing of movie audio clips using LPCC feature and k-means clustering algorithm
The experiments are conducted using the television broadcast movie audio data. A total dataset of 100 movie clips is used in our study. The audio is sampled at 8 kHz and encoded by 16-bit. Clip consists of a sequence of frames and the clip boundaries may be the result of clip extraction. Alternatively fixed length clips usually
LPCC features were extracted for each movie audio clip of 3 s and each clip of 300 feature vectors. This is because we have chosen a frame size of 20 ms and a frame shift of 10 ms. From these feature vectors, we have randomly chosen one feature vector as the key feature vector. The index was created for two levels with four groupsinthefirstlevelandtwogroupsinthesecondlevel. Afterthe creation of index, the remaining 299 feature vectors were tested. At each level, the feature vectors were assigned to the group having minimum distance of the feature vector to the centre. The clips which were present in the final group which had the minimum distance were all retrieved. If the clip corresponding to the testing feature vector was present in that group we counted it as a success.
Theaboveprocesswasrepeatedforthe remaining299queriesofall the 100 clips and the performance was studied ( Table 5 ).
The performance of movie audio indexing system in terms of accuracy rate of retrieval and average number of clips retrieved for each query was calculated as described in Eqs. (7) and (8). 6. Conclusion
In this paper, we have proposed an automatic audio classifica-tion system using AANN and GMM. Linear prediction cepstrum coefficients (LPC, LPCC) and mel-frequency cepstral coefficients are calculated as features to characterize audio content. The five layer autoassociative neural network model as described in Section 3.1 is used to capture the distribution of the acoustic feature vectors. The structure of the AANN model used in our study is described in
Section 5.3. Experimental results show that the proposed audio classification scheme is very effective and the accuracy rate is 93.1%. The classification performance of AANN shows the results are significant and effective. The distribution capturing ability of the AANN model to capture acoustic features is shown in this work.
The performance was compared to Gaussian mixture model which showed an accuracy of 92.9%. The overall classification perfor-mance of the GMM indicates the potential of GMM in capturing the structure of generic sounds. The classification rate using LPC and
LPCC was slightly lower than the 39-dimensional MFCC feature vectors. Performance of movie audio indexing system was eval-uated for 100 clips, and the method achieves about 88.0% accuracy rate and a rate of average number of clips retrieved for each query to be around 5 clips. These clips are the highly similar clips retrieved. In the future, our audio classification scheme will be improved to discriminate more audio classes. We will also focus on developing an effective scheme to apply audio content analysis to assist music indexing.

