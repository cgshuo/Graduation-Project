
Multivariate time series (MTS) classification is an important topic in time series data mining, and lots of efficient models and techniques have been introduced to cope with it. However, early classification on imbalanced MTS data largely remains an open problem.

To deal with this issue, we adop t a multiple under-sampling and dynamical subspace generation met hod to obtain initial training data, and each training data is used to learn a base learner. Finally, an ensemble classifier is introduced for early classification on imbalanced MTS data. Experimental results show that our proposed methods can achieve effective early prediction on imbalanced MTS data.
 H 2.8 [ Database Management ]: Database Applications  X  Data Mining.
 Multivariate time series, Early classification, Imbalanced data
Recently, early classification of time series has been attracting great interest since it is very useful in some time-sensitive applications [1-5]. It means the class of a time series sample could be discerned by scanning only its prefix subsequence instead of the whole sequence, and we expect to predict its class as early as possible provided that the cla ssification accuracy meets the specified demand. 
Meanwhile, classification on imbalanced time series is another important issue due to abundant imba lanced data in reality, which means that the number of instances in some classes is larger than those in other classes. In imbalanced datasets, the class of interest is generally a small fraction of the total instances, but misclassification of such instances is often expensive. For instance, Clinical data suggest that the number of patients with abnormal EEG is far less than the number of patients with normal EEG. We expect that the classification results could give experts an alarm whether it is time to take some further measures. In order to improve the performance of classi fication on imbalanced time series data, some algorithms we re proposed, such as cost-sensitive learning [6], over-sampling [7-8] and under-sampling [9-10]. For more detailed in formation, please refer to a recent survey [11]. classification on imbalanced MTS data remains untouched. In this paper we introduce a method EPIMTS (Early Prediction on Imbalanced Multivariate Time Series) with under-sampling to handle this issue. For simplicity, in this paper we consider only two-class problem, and this method could extend to a multiple-class problem with little hindrance. To make better use of the given labeled data, we firstly divi de the samples belonging to the dominant class (DC) into multiple subsets. Then the DC samples in each subset, together with all samples belonging to the rare class (RC), form a set of initial tr aining data. Since each subset has the same RC samples, we dynamically generate a subspace of the whole variable space as the variable space of each subset to make each subset of the training data different. To train a base learner, at first we propose a fe ature extraction and selection method to obtain core features of each variable respectively. Then we mine coherent rules from the training data for early classification.

The rest of this paper is organized as follows. In Section 2, we review problem definition and relate d work is discussed in Section 3. Section 4 introduces concrete algorithms to efficiently mine core features. Section 5 discusses a novel under-sampling method to early predict imbalanced MTS data. In Section 6, we perform a comprehensive set of experi ments on various problems of different domains. Finally, we conclude our work and suggest directions for future work in Section 7.
In this section, we define shap elets and the notations used in this paper.

Definition 1. Univariate time series: a univariate time series s=t 1 ,t 2 ,...,t L is an ordered set of L real-valued variables.
Definition 2. Multivariate time series: a multivariate time series is a vector of sequences X=(x 1 ,x 2 ,...,x component x j is a univariate time series, and the lengths of different components may be not equal.

This MTS object X has T variables, and the corresponding component of the i th variable is x i . 
For the sake of simplicity, later we will use the word series series
Definition 3. Subsequence: Given a time series S of length L, s_sub= (s, m, m+n-1), is a s ubsequence of length n&lt;L of contiguous position from S starting at the m th position, that is, s_sub=t m ,...,t m+n-1 for 1 -n+1.

Definition 4. Similarity degree: For two time series b and s (supposing |b| |s|), the similarity degree between b and s is calculated by Sim(b,s)= min{dist(b,s i )}, where s subsequence of a time series s with | s i |=|b|, and dist(b,s Euclidean distance between b and s i .

From this definition we could see that the smaller the Sim (b,s), the higher the similarity degree between b and s.

Definition 5. Shapelet or Feature: A shapelet (feature) is a time series subsequence which is representative of a class. Informally, A shapelet (feature) p = (b, , c), where b is a subsequence, is a threshold, and c is a class label. An unknown time series object s is considered to be matching a shapelet p and is labeled as the class of this shapelet if Sim(b,s) .

For simplicity, later Sim(s, p) represents the similarity degree between the shapelet p and the time series s. Class(s) means the class of a time series s. 
Definition 6. Precision: Given time series data D and a feature p = (b, , c), the precision of p is the ratio of the number of samples which have the class label c and match this feature p and the number of samples which match this feature p in data D.

Definition 7. Recall: Given time series data D and a feature p = (b, , c), the recall of p is the ratio of number of samples which have the class label c and match this feature p and the number of samples which have the class lable c in data D.

Definition 8. Earliness: Given time series data D and a feature p = (b, , c), the minimal identifiable length (MIL for short ) of a time series s D MIL (s, p) is the length of a prefix subsequence from beginning to some point where p could classify this sample s by scanning this subsequence.
 MIL (s,p) = min 1, i ] ,p ) , where sD ={s|Dis ( s, p ) &lt; ,sD} and s [ i Lengt h ( p ) +1,i ] is a subsequence of s starting at the (i Lengt h ( p ) +1 ) th position and ending at i th position. Then the earliness of p is Earliness(p) = 1  X  ( s )
For a time series s in D and a feature set F where could classify s, the minimal identifiable length of the time series s MIL (s)=min{ MIL (s, p)}, . The earliness degree of the sample s is 1-MIL(s) /length(s). If there is no feature could classify the time series s, its earliness degree is 0. 
The earliness degree of data D is the average earliness degree of all samples in D.
Since Diez et al. [12] firstly proposed the problem of early classification, some novel methods have been introduced for early classification on time series. For instance, as we mentioned in Section 1, a method ECTS was proposed [1] to explore the stability of the nearest neighbor re lationship in the full space and in the subspaces formed by prefixes of the training examples. However, ECTS could not ext ract and summarize useful information for users. In order to overcome its drawback, local shapelets [2] was introduced and extracted as features which distinctly manifest the target class locally and early.

Moreover, Ghalwash et al. [5] defined a multivariate shapelet and proposed a method called multivariate shapelets detection (MSD) for early classification on multivariate time series. This kind of shapelets consists of multiple segments, each of which is extracted exactl y from one component. But different starting and ending positions are not allowed in a segment of each component, that is to say, all segments of a multivariate shapelet should be extracted in the same sliding time window at a time. Since shapelets of interest often have different intervals for each variable, multivariate shapelets do not have good interpretability. However, these methods have no t considered the im balanced data, which is universal phe nomenon in real data. imbalanced data because they evaluate the performance on the whole data and pay less attention to rare cases in some classes. For imbalanced classification, lots of efficient models and techniques have been introduced to cope with it. Generally, these methods can be categorized as algorithm level and data level approaches. At algorithm level, special algorithms were designed to consider these rare classes [6,11,13].

The other way is to rebalance the stances in each class, such as different kinds of over-samp ling and under-sampling. For example, Li et al. [10] proposed a novel semi-supervised learning method for imbalanced sentiment classification based on random subspace generation and under-sampling methods. GuoHua [9] proposed a way by combining SVM and under-sampling method for imbalanced time series classification.
In this section our aim is to mine c ore features of each variable respectively.
In the process of feature extracti on, it needs two parameters: minL and maxL . All subsequences of length between maxL are extracted from each com ponent as shapelet candidates respectively. In order to enhance the precision and sensitivity of a shapelet simultaneously, we adopt F-measure method to evaluate the quality of a shapelet f during the process of learning a threshold. And a robust distan ce threshold for the shapelet the highest quality is obtained.
 F-measure( f ) = 2/(1/Precision( f ) + 1/Recall ( f ))
Some feature selection methods have developed and proven to be effective in improving predictiv e accuracy for classification on time series [2,14]. However, few existing feature selection methods touched the issue of sub-concepts in a single class [15], which are popular in many classification problems. Due to the number of samples in each sub-cluster is often diffe rent, shapelets of a non-dominate sub-cluster coul d be easily overlooked. This within-class imbalance is implicit in most cases and the above methods cannot effectiv ely handle this issue. Therefore, selecting distinctive and stable features is critical in the process of building an effective classifier. 
In order to deal with this issu e, we first divide shapelet candidates of each variable belonging to a class label into several clusters with the Silhouette In dex (SI) method [16], which is based on a compactness-separa tion measurement. Then, a core feature is selected to present each cluster by ranking their qualities. F-measure was proposed in [2] for fe ature selection. However, the recall, an important factor of anal yzing the efficiency of a feature and representing the class sensitivity, is often ignored. To further evaluate the qualit y of a feature f by considering the recall, here we propose a generalized extend ed F-measure (GEFM for short) strategy. On the one hand, it extends the properties of F-measure for early classification according to the earliness, precision and recall. On the other hand, this method could improve the sensitivity of a feature f by given different weights for different properties.
 GEFM ( f ) =1/( w 0 /Earliness( f )+ w 1 /Precision( f )+ earliness, precision and recall. to deal with the imbalanced MTS classification. Firstly, we divide DC (dominant class) instances as K subsets. Each subset is under sampling from the DC instances in the original training data, and its size is equal to that of RC (rare class) instances in the original training data. Then the samples of the DC in each subset are combined with all samples of the RC to form a new set of training data. Then each new set of training data are used to train a base learner. Since the samples of the RC in each subset are the same, the performance of all base learners is similar. In order to improve the diversity of base learners, here we dynamically generate a random subspace of the whole variab le space as the variable space of each subset.
 to find coherent rules from the trai ning data for early classification. Each rule is composed of one or m ore core shapelets belonging to a class, but includes one core shapelet of each variable at most. These rules could reflect the relevant connection among the components of MTS data. To improve the efficiency and effectiveness of the classification, we only select a subset of all rules that could cover the training data to build a base learner.
Finally, we build an ensemble classifier with these K base learners to early predict the class of an unlabeled sample base learner is assigned a weight in terms of its accuracy by classifying the original imbalanced training data. And this instance x is labeled positive if the ensemble result is biased towards positive; otherwise it is labeled negative.
 The main framework of our proposed method is as following.
 Input: The imbalanced training data containing N + positive and N negative samples Output: The final classifier C balanced initial training data with k=(int) (N + /N -) initial training data with V i variables obtain its accuracy A i However, this straightforwar d way could lead to low earliness since the earliness degree of di fferent learners being probably different, and the earliness degre e of this object is the minimum of these values. In order to en hance the earliness degree, we introduce a modified prediction strategy as following. At the beginning, all of these base learners are executed on an unlabeled sample x synchronously. This process will be stopped when some base learners ca n classify this sample x and the ensemble result from these recognizable learners is biased towards positive or negative. Finally, the target class of this sample is labeled as positive or negative in terms of the ensemble result.
In this section, we empirically study the proposed methods for early classification of MTS. Al l experiments follow a 10 fold cross validation scheme and each runs 5 times, and we take an average result for each experiment . The parameter settings are the same, MinLen = 5, MaxLen = L/3, where L is the full length of the time series. 
To evaluate the ability of our method, we use synthetic and real-world datasets for early prediction on imbalanced multivariate time series.
 distribution functions, while the second one is generated by union probability distribution functions. For real world dataset of MTS, here we use Wafer and ECG da taset from [17] to verify the performance of our proposed method.

Table 1 shows the summary of four datasets used in the experiments.

In order to select distinctive fe ature for early classification on imbalanced multivariate time series data, we further analyze the effect of weights ( w 0 , w 1 and w 2 ) which control the importance of each property in this evaluation method. For simplicity without negative influence of any propert y, we assume the weights ( and w 1 ) of earliness and precision are equal to 1, and discuss the relationship between precision and recall. We use Wafer data as an example to show their performance on the classification accuracy and recall. The results from the Wafer data with different values of w 2 are shown in Figure 1 and Figure 2. From both figures we could see the accura cy and recall of the EPIMTS method are competitive when w 1 is equal to w 2 .

Figure 1: The results of the 
In order to analyze the effectiveness of this evaluation strategy, here we compare GEFM with F-measure method (FMM) without considering the recall of a feature f as following. 
We do experiments on four datasets to analyze the function of classification by both feature evalua tion methods. To be fair, here we use a classification method similar to EPIMTS except different feature evaluation methods. The classification results of the precision (Prec), recall (Recl) a nd earliness (Eln) on four datasets are shown in Table 2 and Table 3 res pectively. We could see that the precision, recall and earliness of GEFM are generally higher than those of FMM on Syn1 and Wafer data. For Syn2 and ECG data, the precision of GEFM is little lower than that of FMM, while the recall and earliness of GEFM are higher than those of FMM. Since our aim is to early predict abnormal cases as far as possible, GEFM is more efficient than FMM to evaluate a feature in the process of feature sel ection for imbalanced data. There is no work on early classi fication of MTS data except MSD [5] by far. Here we compare our propose method EPIMTS with MSD method on four datasets , and the experimental results are listed in Table 4 and Table 5. From four experimental results we could see that the accuracy (Acc), precision (Prec), recall (Recl) and earliness (Eln) of EPIMTS are much higher than those of MSD method. Since our task is to early classify imbalanced data, the accuracy, precision and r ecall is critical to evaluate the function of a classifier. Therefore , EPIMTS is competitive in comparison with MSD for early classification on imbalanced multivariate time series.

Table 4. The summary results of EPIMTS and MSD on Syn1 EPIMTS 0.98 0.92 1 0.70 0.95 0.77 0.99 0.71 MSD 0.83 0.51 0.9 0.57 0.48 0.25 1 0.64 Table 5. The summary results of EPIMTS and MSD on Wafer EPIMTS 0.96 0.75 0.89 0.69 0.76 0.41 0.92 0.75 MSD 0.93 0.65 0.82 0.43 0.64 0.43 0.84 0.69
In this paper, we address the novel problem of early classification on imbalanced multivariate time series. A method EPIMTS with under-sampling is proposed to handle this issue. Experimental results clearly show that our proposed classification method can achieve effective early classification on imbalanced MTS data. As future work, we plan to do some work on partial labeled data with active learning.
 ACKNOWLEDGMENT This work is supported in part by the Natural Science Foundation of Hubei Province of China (2011CDB462), the National Natural Science Foundation of China (61272275), and the Programme of Introducing Talents of Discipline to Universities (B07037). 1. Zhengzheng Xing, Jian Pei, Ph ilip S Yu. Early prediction on 2. Zhengzheng Xing, Jian Pei, Philip S. Yu, Ke Wang. 3. A Breg X n, M A Sim X n, J J Rodr X guez, C J Alonso, et al.
 4. Z. Xing, J. Pei, and E. Keogh. A brief survey on sequence 5. Mohamed F Ghalwash, Zoran Ob radovic. Early classification 6. Yanmin Sun, Mohamed S Ka mel, Andrew K C Wong, Yang 7. Xiannian Fan, Ke Tang and Thomas Weise. Margin-based 8. Hong Cao, XiaoLi Li, Ye wKwong Woon, SeeKiong Ng. 9. Guohua Liang, Chengqi Zhang. An Efficient and Simple 10. Shoushan Li, Zhongqing Wang, Guodong Zhou, and Sophia 11. Haibo He, Edwardo A Garcia . Learning from imbalanced 12. Griffin MP, O'Shea TM, Bissonette EA, Harrell FE Jr, Lake 13. T Ryan Hoens, Qi Qian, Ni tesh V Chawla, ZHi-Hua Zhou. 14. Lexiang Ye, Eamonn Keogh. Time series shapelets: A new 15. Hyunjin Yoon, Kiyoung Yang, and Cyrus Shahabi. Feature 16. Peter Rousseeuw. Silhouette s: a graphical aid to the 17. Available from: &lt;http: //www.cs.cmu.edu/~bobski/&gt;.
