 1. Introduction
Grinding is a major finishing process used to fabricate engineering objects to the specified dimensions and surface roughness. Grinding operations are carried out with grinding wheels as tools and the tool condition changes from sharp to dull as the grinding operation progresses. A dull wheel is inefficient, draws higher power, and affects the quality of the ground workpiece. Close monitoring of tool condition is very important to grinding as well as other machining operations. In the current industrial practice, tool condition is often monitored by the operator overseeing the operation.

Efforts have been made in the past to develop a sensor-based grinding wheel condition monitoring system to improve the efficiency and integrity of grinding operations. In their keynote paper on grinding process monitoring, Tonshoff et al. (2002) reviewed the sensors used to monitor both the micro-and macro-topography of the active abrasive layer of the grinding wheel, which recognized as the dominant features of the grinding process. A brief review of past publications is given in chron-ological order below. Mokbel and Maksoud (2000) analyzed raw
AE signals using fast Fourier transform and compared the AE spectral amplitudes of different diamond wheel bond types, grit sizes, and their conditions generated by using different grinding wheel/truing speed ratios with the surface roughness ( R a ground mild steel specimens. Hwang et al. (2000) reported that the amplitude of the AE signal, collected in high-speed grinding of silicon nitride using an electroplated single-layered diamond wheel, monotonically increases with wheel wear. To monitor an alumina wheel surface condition in cylindrical grinding of carbon steel, Lezanski (2001) extracted statistical and spectral features from multiple signals (forces, vibration, and AE) and applied a feed-forward backpropagation neural network to select eight features, which are grinding depth of cut, coolant volume rate, standard deviation of vibration, means value of vibration power spectrum, range of vibration power spectrum, mean value of AE RMS, range of AE RMS, and range of RMS power spectrum. The eight selected features were used to train a neuro-fuzzy model for classification. A best performance of 83.3% classification accuracy was reported.

Furutani et al. (2002) introduced an in-process method for measuring the topography change of an alumina grinding wheel in cylindrical grinding. A pressure sensor is set beside a grinding wheel with a small gap. When grinding fluid is dragged into the gap, hydrodynamic pressure, which corresponds to the gap length and the wheel topography, can be measured. The pressure signals were analyzed by fast Fourier transform (FFT). Higher frequency components of the pressure spectra were found to increase with the wheel loading and dulling. Warkentin and Bauer (2003) investigated the relationships between wheel wear and grinding forces for different depths of cut when surface grinding mild steel with an aluminum oxide wheel. For depths of 10, 20, and 30 m m, respectively, the average normal and tangential forces show a trend with a slight positive slope. For depths of 40 and 50 m m, the data shows a negative slope and piecewise negative slope, respectively.

Hosokawa et al. (2004) developed a practical wheel surface condition monitoring system, in which the characteristics of the wheel surface are discriminated on the basis of the dynamic frequency spectrum signals of grinding sound using a neural network technique. The system was successfully tested in the plunge grinding of carbon steel with a vitrified-bonded alumina wheel and in the plunge grinding of hardened die steel with a resinoid-bonded CBN wheel. Kwak and Ha (2004) showed that the grinding force signal in surface plunge grinding of STD11 with an alumina wheel could be better processed by wavelet de-noising than by FFT filtering. To detect the wheel dressing time clearly, the approximation coefficients A4 of Daubechies wavelet transform were used because they were suitable for the detection of a sudden signal change at the time or frequency domain. Liao et al. (2006) extracted features from acoustic emission signals collected at 1 MHz in grinding alumina with a resin-bonded diamond wheel and then applied an adaptive genetic clustering algorithm to the extracted features in order to distinguish different states of grinding wheel condition. Liao et al. (2007) proposed another clustering method, which involves first extracting features from acoustic emission signals based on discrete wavelet decomposi-tion by following a moving window approach, next generating a distance (dissimilarity) matrix using HMM, and then applying a clustering algorithm to obtain clustering results. Table 1 summarizes the previous studies on sensor-based grinding wheel condition monitoring.

Feature extraction and feature selection are two important issues in sensor-based tool health monitoring. Feature extraction is a process that transforms the original sensory signal into a number of potentially discriminant features. The above review indicates that feature extraction is often carried out by different methods; but feature selection is rarely studied except Lezanski (2001) . Feature selection refers to a process that selects an optimal subset of the extracted features based on an evaluation criterion. By removing the irrelevant and redundant not only reduce the dimension but also may increase the classification accuracy. For a dataset with N features, there exist 2 N candidate subsets. Even for a moderate N , the search space is exponentially prohibitive for exhaustive search. The search strategies that have been employed in feature selection can be roughly grouped into three categories: (1) Complete search such as branch and bound that guarantees to (2) Sequential search that adds or removes features one (or some) (3) Random search such as genetic algorithm that starts with a selection algorithm can be distinguished either as a filter approach, a wrapper approach, or a hybrid. The filter approach evaluates and selects feature subset based on the general characteristics of the data without involving any learning model.
On the other hand, the wrapper approach employs a learning model and uses its performance as the evaluation criterion. Compared with the filter approach, the wrapper approach is known to be more accurate but cost higher computationally. The hybrid approach is designed to trade accuracy with computational speed by applying the wrapper approach to only the set of those features selected by the filtering approach first.

This paper investigates the above-mentioned two important issues in acoustic emission sensor-based condition monitoring of a grinding wheel used in grinding operations. For feature extraction, two methods are employed with one based on time-series modeling and one based on discrete wavelet decomposi-tion. For feature selection, three methods are used including the sequential forward floating selecting method and two newly proposed methods based on ant colony optimization (ACO).
More details about the feature extraction and feature selection methods are briefly described in Sections 2 and 3, respectively.
Section 4 describes the data and gives the test results on the identification of grinding wheel condition to show the effective-ness of the proposed methods, followed by the discussion. Section 6 concludes the paper and identifies topics for future study. 2. Grinding experiments, AE signals, and feature extraction
Acoustic emission signals collected at 1 MHz during grinding two ceramic materials (alumina and silicon nitride) with a resin-bonded diamond wheel are used in this study. To avoid redundancy, experimental details as described in Liao et al. (2006, 2007) are omitted here.

Two methods are used to extract features from the AE signals: one using discrete wavelet decomposition and another using autoregressive modeling. For the former, the multilevel 1-D wavelet decomposition function, wavedec , available in Matlab is chosen by specifying db1 Daubechies wavelets (which is the same as the Haar wavelets) with N =12. The energy of each decomposed component is then computed. The energy of a signal is the sum of the squares of its coefficient values. For an N -level decomposition, the number of extracted features is N +1. Therefore, a total of 13 features are obtained for each feature vector.

Autoregressive modeling essentially involves with building a regression model of auto-correlated errors existed in the AE signals. The model has the following form: y  X  c  X  b t  X  v t  X  1  X  v
Eq. (1) can be rewritten as y  X  c  X  b t  X  X  1 f 1 B f 2 B 2 f p B p  X  1 e t  X  2  X  where y t , t =1, 2, y , n , is a AE signal segment and e
One major issue in building a stochastic AR model is to determine the appropriate model order. To this end, we utilize three autocorrelation plots (autocorrelation, partial autocorrela-tion, and inverse autocorrelation) and two well-known criteria: Akaike information criterion (AIC) and Durbin X  X atson statistic.
After analyzing many signal segments for different wheel conditions, the results indicate that 20 is an acceptable AR order.
The Yule X  X alker method is used to estimate the coefficients based on the chosen model order. For an AR model, y t  X  f f y the coefficients of the model, and e t is stochastic shock. It can be shown that the autocorrelation function r ( t ), t =1, related to the autoregressive parameters j i , i =1, y , p , through the
Yule X  X alker equation for the autoregressive process:  X  1  X  X  f 1 r  X  0  X  X  f 2 r  X  1  X  X  f 3 r  X  3  X  X  X  f p r  X  p  X   X  2  X  X  f 1 r  X  1  X  X  f 2 r  X  0  X  X  f 3 r  X  2  X  X  X  f p r  X  p 1  X   X  p  X  X  f 1 r  X  p  X  X  f 2 r  X  p 1  X  X  f 3 r  X  p 2  X  X  X  f p r  X  0  X  X  3  X 
The autocorrelation r ( t ) is estimated by  X  t  X  X  where N is the size of the sample. Plugging 3 r  X  t  X  into Eq. (3), the equation is solved to obtain the estimations of AR model coefficients. The model coefficients are then used as the features for subsequent classification. Only the model coefficients are then used as the features for subsequent classification; the intercept and the slope are not used. 3. Feature selection
Three feature selection methods, including two ACO-based methods and sequential forward floating selection (SFFS), are used to remove the irrelevant and redundant features, potentially exist in the two different feature vectors extracted by the two methods described in the previous section. Details of the ACO-based methods and a brief summary of the SFFS method are given in Sections 3.1 and 3.2, respectively, below. 3.1. Ant colony optimization-based feature selection methods
The first use of ACO for feature selection seems to be reported in ( Yan and Yuan, 2004 ). In that work, Principal Component Analysis (PCA) was used to extract eigenfaces from images at the preprocessing stage; and then ACO was applied to select the optimal subset features using cross-validation with support vector machine as the classifier. Shen et al. (2005) modified ACO to select variables in quantitative structure X  X ctivity relationship (QSAR) modeling and to predict inhibiting action of some diarylimidazole derivatives on cyclooxygenase enzyme. In another work, ant colony optimization was tailored to perform feature selection for the purpose of prostate tissue characterization with support vector machine as the classifier, based on Trans-rectal Ultrasound images ( Mohamed et al., 2005 ). It was shown that ACO out-performed genetic algorithms in obtaining a better feature subset that leads to higher classification accuracy. ACO was modified by substituting the mutual information for random as some problem-specific local heuristics to select feature subsets for modeling of temperature data with support vector machine (Zhang and Hu, 2005 ). A two-step ant colony system for feature selection was developed to split the heuristic search into two stages, in which candidate feature subsets are generated in the first stage and then some of them are randomly selected as the initial state for the ants in the second stage ( Bello et al. (2006) ). A modified discrete binary ant colony optimization algorithm was shown to greatly improve the fault diagnosis performance of support vector machine for the Tennessee Eastman process ( Wang and Yu (2006) ). In a most recent study, the performance of backpropagation feed-forward networks in medical diagnosis was shown to greatly improve when it was used together with ant colony optimization for feature selection ( Sivagaminathan and Ramakrishnan, 2007 ). Most of the ACO-based feature selection methods reviewed above implements the random search strategy with feature subsets generation guided by ant colony optimiza-tion. The only exception is that of Sivagaminathan and Ramak-rishnan, in which a strategy similar to the sequential forward selection is followed. The above review indicates that so far none has used ant colony optimization for feature selection in the domain of condition monitoring.

This section describes the two proposed ACO-based feature selection methods with one following the sequential search strategy and the other the random search strategy. The two methods are designed to include some basic traits commonly seen in nearly all ACO meta-heuristics as well as some unique to the specific task  X  feature selection. These traits are elaborated in
Section 3.1.1. The algorithm of each method is given in Section 3.1.2. The wrapper approaches for performance evaluation are described in Section 3.3. 3.1.1. Traits (1) It employs a colony of artificial ants that work cooperatively to find good solutions.

Each artificial ant is a simple agent that possesses the path-finding behavior of a real ant observed in real ant colonies.
Although each artificial ant can build a feasible solution, high quality solutions are the result of the cooperation among the individuals of the whole colony. In the context of feature selection, each artificial ant is assigned the task to find feasible feature subsets and the whole colony work together to find the optimal feature subsets. The size of colony, or the number of artificial ants, is a parameter needed to be specified, which is similar to the population size, or the numbers of chromosomes, in a genetic algorithm. (2) Each artificial ant builds its solution by applying a stochastic local search policy .

The stochastic local search policy is guided primarily both by ants X  private information, which contains the memory of the ants X  past actions, and by publicly available pheromone trails and a priori problem-specific local information. In the context of feature selection, ants X  private information contains the features that have been selected in building a feature subset.

The stochastic local search policy used in the sequential search strategy follows that used by Sivagaminathan and Ramakrishnan (called the state transition rule in their paper), which is similar to the pseudo-random-proportional rule used in the traveling sales-man problem ( Dorigo et al. (1999) ). Specifically an ant chooses a feature u n from the unselected feature set U at iteration t according to the following probability:
In the above equation, q is a random variable uniformly distributed over [0, 1]; q 0 is a tunable parameter called exploita-tion probability factor, which determines the relative importance of exploitation (in the case that q o q 0 ) versus exploration (in the case that q Z q 0 ); t u is the pheromone level of feature u ; Z represents the inverse of the cost parameter; and b is a parameter that determines the relative importance of pheromone versus heuristic. The cost parameter can be used to account for the feature cost; setting them all equal if there is no distinction in the feature costs. Setting b to zero means that all features are given equal priority irrespective of their costs.

The stochastic local search policy used in the random search strategy is modified from Eq. (3). Specifically an ant builds a feature subset by choosing each feature u from the entire feature set at iteration t according to the following probability: uniformly distributed over [0, 1]. All other notations are identical to those defined in Eq. (3). to real ants via depositing pheromone on the ground while walking . sooner the pheromone is deposited by the real ants; which in turn leads to more ants taking the shorter path. In the meanwhile the pheromone deposited on the longer path receives few or no new deposits and gets evaporated over time. Artificial ants deposit an amount of pheromone that is a function of the quality of the solution found. Unlike real ants, the timing for updating pheromone trails is often problem dependent. Our methods update pheromone trails only after a solution is generated.

Sivagaminathan and Ramakrishnan. For this policy the elitist strategy is adopted; i.e., only the best ant is allowed to update pheromone trails. For any feature, u , in the best feature subset found in iteration t denoted by X 0 ( t ), its pheromone level, t incremented by applying the global updating rule: t u  X  t  X  1  X  X  X  1 e  X  t u  X  t  X  X  e =  X  min E  X  t  X  X  e  X  ; 8 u parameter; min E ( t ) is the classification error associated with the best feature subset found by all ants in iteration t ; and e is a small number added to prevent dividing by zero when min E ( t ) equals to zero ( e =0.1/number of data records in this study). For any feature not in the best feature subset, its pheromone level, t u , is updated by applying the following local updating rule: t u  X  t  X  1  X  X   X  1 e  X  3.1.2. Algorithms
S and ACO-R with S and R denote sequential forward search and random search, respectively. 3.1.2.1. ACO-S. The ACO-S algorithm has the following steps: p  X  t  X  X  or if q 1 Z q 0 and q 3 4 0 : 5 or if q 1 Z q 0 and q 3 r 0 : 5 (4) Write output to the file. 3.1.2.2. ACO-R. The ACO-R algorithm has the following steps:
ACO-R Algorithm (1) Load the data and normalize it by feature to remove the magnitude effect (2) Set parameters including maximum number of runs, max R , maximum size of feature subsets, max S , maximum number of ants, max A , maximum number of iterations, max T , exploita-tion probability factor, q 0 , pheromone evaporation parameter, e , feature cost (=1 for all features in this study), and the relative importance parameter between pheromone and heuristic, b (=0 in this study). (3) For r =1: max R , (4) Write output to the file. 3.2. Sequential forward floating selection
The sequential forward floating selection (SFFS) algorithm was proposed by Pudil et al. (1994) , as an improvement of the sequential forward selection (SFS) method. The SFFS is basically a bottom up search procedure which includes new features by means of applying the SFS procedure starting from the current feature set, followed by a series of successive conditional exclusion of the worst feature in the newly updated set provided that further improvement can be made to the previous sets. The details of the SFFS algorithm are omitted here to save space. 3.3. Wrapper approaches for feature subset evaluation
The wrapper approach is employed to evaluate the goodness of a feature subset found by a feature selection method. To compare their performances, five learning models are alternatively used. The performance is measured in terms of the average test error of stratified 10 -fold cross-validations. The five learning models are the nearest mean (NM), k -nearest neighbor (KNN), fuzzy k -nearest neighbor (FKNN) ( Keller et al. (1985) ), center-based nearest neighbor (CBNN) ( Gao and Wang (2007) ), and k -means-based nearest prototype (KMNP). Most of these classifiers are well-known, except the last two. The CBNN classifier finds the centroid from a given class and defines a center-based line (CL) that connects a training example with its class centroid. In the classification phase, an unseen test datum x is assigned a class label of the training example used to define the CL that is closest to x . For KMNP, the number of prototypes for each class is set equal to the size of the smallest class; k -means clustering is then used on each class to find the prototypes, and finally KNN is applied for classification. More details can be found in the original references. 4. Test results
All three feature selection methods and five classification algorithms were coded in Matlab and executed using a Dell Latitude C640 laptop computer with Mobile Intel 2 GHz Pentium 4-M CPU. A total of 320 records each of both wavelet energy features and AR coefficient features were obtained from the AE signals collected under different grinding conditions and two tool conditions (sharp vs. dull). The test results were obtained with 10 ants and 10 iterations for the ACO-S method and 50 ants and 50 iterations for the ACO-R for both datasets. In the table, they are denoted as ACO-S (10, 10) and ACO-R (50, 50), respectively. Due to its stochastic nature, five runs were made for each ACO-based method by setting both q 0 and e to 0.5 arbitrarily. The effects of other parameter values are discussed in Section 5. Both the classification accuracy and the CPU time were recorded and serve as the two primary measures of performance. The classifier errors are given as  X  X  X verage 7 standard deviation X  X  in the table, with the best performance noted. 4.1. Wavelet energy features
Tables 2 X 4 summarize the classification error rates, CPU times, and locally best feature subsets found for the 13-dimensional wavelet energy features, respectively. In Table 4 , the fraction after a best feature subset indicates the number of runs that it was found over a total of five runs. Based on these results, the following observations can be made: (1) Feature selection improves the classification accuracy, regard-(2) In terms of classification accuracy, the CBNN classifier is the (3) The lowest classification error of 7.81% is achieved by all three (4) The associated best feature subset found by all three feature (5) Comparing the two ACO-based feature selection methods, (6) The three nearest neighbor classifiers attain the same features change with the search process for the best run of each
ACO-based feature selection method when CBNN is used as the classifier with both q 0 and e set at 0.5. Theoretically, the maximal number of updates is determined by both the number of iterations and the number of features for the ACO-S method (130=10 13 in
Fig. 1 ), and by the number of iterations specified for the ACO-R method (50 in Fig. 2 ). Obviously these profiles are determined by the updating rule given in Eqs. (4) and (4 0 ). Note that Fig. 1 has far fewer numbers of updates, which indicate that in many cases no feature subsets were found.
 selected during different stages of the search process. For example, in Fig. 1 the first feature selected to increase its pheromone level in the very beginning is 3 , then 11 , and so on. It is interesting to point out that for ACO-S all features in the best feature subset have the same highest pheromone level, i.e., 12.16, at the end. For ACO-R, however, the same cannot be observed. 4.2. AR coefficient features and locally best feature subsets found for the 20-dimensional AR coefficient features, respectively. Based on these results, the following observations can be made: (1) Feature selection improves the classification accuracy, regard-(2) The lowest classification error is 6.875% for this dataset, which (3) The best feature subsets associated with the lowest classifica-(4) If CPU time is taken into consideration, then the best (5) The ACO-R feature selection method did not produce any best
Figs. 3 and 4 are plotted to show how the pheromone levels of features change with the search process for the best run of ACO-S feature selection method when 1NN and KMNP are used as the classifier, respectively, with both q 0 and e set at 0.5. Theoretically, the maximal number of updates is determined by both the number of iterations and the number of features for the ACO-S method (200=10 20). Obviously these profiles are determined by the updating rule given in Eqs. (4) and (4 0 ). Note that both
Figs. 3 and 4 have fewer numbers of updates, which indicate that in many cases no feature subsets were found, thus no update was done.

From these profiles, it can be known which features are selected during different stages of the search process. For example, in Fig. 3 the first feature selected to increase its pheromone level in the very beginning is 9 ,then 12 , and so on. It is interesting to point out that for both ACO-S with 1NN and ACO-S with KMNP all features in the best feature subset have relatively high pheromone level at the end. However, there are features with higher pheromone level not part of the best feature subset. 5. Discussion
To reaffirm the validity of the test results presented in Section 4, the iris and wine datasets taken from the UCI Repository were also tested with the same test procedure and methods. The reason for choosing the iris dataset is its small size so that the exhaustive method can also be applied to identify the globally optimal feature subset.

Tables 8 X 10 summarize the stratified 10-fold cross-validation classification error rate, the CPU time, and the best feature subsets found for each combination of feature selection method and classifier, respectively. The above results were obtained with two ants and two iterations for the ACO-S method, regardless of the classifier used, and with five ants and five iterations for the ACO-R.
Similar to previous tests, five runs were made for each ACO-based method by setting both q 0 and e to 0.5 arbitrarily.

The results indicate that: (1) Feature selection improves the classification accuracy, regard-less the combination of feature selection method and classifier used. (2) The globally optimal feature subset is {4}, {3, 4}, {1, 2, 3, 4}, {1, 2, 3, 4}, and {3, 4} with the lowest classification error of 4%, 3.33%, 6%, 7.33%, and 3.33% for the NM, 1NN, F1NN, CBNN, and
KMNP classifier, respectively. (3) All three feature selection methods can achieve the best performance, regardless the classifier used. The globally optimal feature subset was found by all feature selection and classifier combinations except one, i.e., ACO-R with NM, which found the best feature subset larger than the globally optimal feature subset. (4) For a dataset with small number of features like the iris
Table 11 summarizes the classification accuracies for the wine dataset. Again, the results indicate that feature selection improves the classification accuracy, regardless the combination of feature selection method and classifier used. The above test results have clearly shown that both the ACO-based feature selection methods and the SFFS method are capable of finding the best feature subsets.

Next, the effects of two more parameters, specifically the exploitation probability factor, q 0 , and the pheromone evaporation parameter, e , are investigated. With all other parameters fixed, these two parameters are varied at three levels each at 0.1, 0.5, and 0.9. Hence, a total of nine combinations are experimented including the one that has been used in the previous runs, i.e., the 0.5 X 0.5 combination. The 1NN classifier is chosen over the other classifiers for this investigation for the following reasons: (1) the CPU time required is not too long; and (2) it is the best classifier for the weld flaw identification dataset.

Table 12 summarizes the effects of the ACO-S parameters on the classification error, CPU time, and best feature subsets found for the dataset of wavelet energy features with max A and max T set at 10. The results indicate that all nine combinations of q e can produce the lowest classification error of 10%. Using a large value of q 0 and e takes less CPU time; however, the number of runs producing the best result also reduces. On the other hand, using a small value of q 0 and e takes longer CPU time; however, the number of runs producing the best result also increases. Table 13 summarizes the results for the dataset of AR coefficients with max A and max T set at 10 as well. Three out of nine combinations did not produce the lowest classification error of 6.875% and they are the (0.1, 0.9), (0.5, 0.9), and (0.9, 0.1) combinations. Generally, to obtain lower classification error lower value of q 0 is better if the e value is not too high, at the cost of slightly longer CPU time.
Lastly, the effectiveness of using db3, instead of db1, for the wavelet-based feature extraction method was also tested. The results are summarized in Table 14 . Comparing with Table 2 , it can be seen that: (1) Without feature selection, the classification accuracies of db3 (2) With feature selection, the best classification accuracy of db3 6. Conclusions
This paper has presented the results obtained in our investiga-tion of two important issues related to sensor-based tool health condition monitoring, i.e., feature extraction and feature selection.
Two feature extraction methods, three feature selection methods, and five classifiers were employed in the study. Specifically, grinding wheel condition was monitored with acoustic emission signals. AE signals were extracted both by discrete wavelet decomposition and autoregressive modeling. The three feature selection methods include the sequential forward floating selec-tion, and two ant colony optimization-based feature selection methods based on different search strategies. The testing method is stratified k -fold cross-validation. The performances were measured in terms of classification error rate and CPU time. drawn: (1) For both datasets of wavelet energy features and AR (2) For the dataset of wavelet energy features, center-based (3) For the dataset of AR coefficients, five combinations of feature selection and classification methods produce the lowest classification error of 6.875%. For this dataset, ACO-R seems to be outperformed by both the ACO-S method and the SFFS method. (4) For the two ACO-based methods, a proper selection of the parameters is necessary in order to produce the best result.
Generally, to obtain lower classification error lower value of q is better if the e value is not too high. This, however, is achieved often at the cost of slightly longer CPU time.
The best combination of feature selection method and classifier seems to be data-dependent. It would be worthwhile to test with more datasets to figure out whether one particular combination works better than others in most datasets. It is also interesting to investigate why the ACO-R method works well for the dataset of wavelet energy features but not the dataset of AR coefficients. The major difference in the two datasets is in the number of features.
But is that the major contributing factor? Lastly, a possible future research topic is to develop more and better ACO-based feature selection methods.
 References
