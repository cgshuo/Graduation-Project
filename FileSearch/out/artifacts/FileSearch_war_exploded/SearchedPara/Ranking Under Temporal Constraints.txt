 This paper introduces the notion of temporally constrained ranked retrieval, which, given a query and a time constraint, produces the best possible ranked list within the specified time limit. Naturally, more time should translate into bet-ter results, but the ranking algorithm should always pro-duce some results. This property is desirable from a number of perspectives: to cope with diverse users and information needs, as well as to better manage system load and variance in query execution times. We propose two temporally con-strained ranking algorithms based on a class of probabilistic prediction models that can naturally incorporate efficiency constraints: one that makes independent feature selection de-cisions, and the other that makes joint feature selection de-cisions. Experiments on three different test collections show that both ranking algorithms are able to satisfy imposed time constraints, although the joint model outperforms the independent model in being able to deliver more effective results, especially under tight time constraints, due to its ability to capture feature dependencies.
 Categories and Subject Descriptors : H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms : Algorithms, Performance Keywords : efficiency, linear models, learning to rank
Most existing ranking functions are rigidly defined and incapable of adapting to diverse retrieval scenarios. In this work, we are interested in developing ranking functions that can robustly adapt to different efficiency constraints that may arise in real-world retrieval applications. As we will show, our proposed temporally constrained ranking functions are capable of producing high quality results given a pre-determined amount of time. Naturally, the quality of re-sults improves as more computation time is allowed, but critically, the ranking function should produce some results given an arbitrary time constraint. This idea is related to anytime algorithms , introduced in the mid-1980s by Dean and Boddy [11] in the context of time-dependent planning. The primary difference, however, is that anytime algorithms provide a solution at any arbitrary point, whereas our tem-porally constrained ranking functions require the time con-strained to be specified in advance .

More formally, we define a temporally constrained rank-ing function as one that solves the following ranking task: given a user query q and a time constraint T ( q ), the goal is to produce a top k ranking of documents from collection C that maximizes a metric of interest such as mean average precision, NDCG, etc. Why is the ability to impose dynamic temporal constraints on a ranking function important? We motivate this problem from three separate angles: users, in-formation needs, and system architectures.

First, users are diverse and have different tolerances to query execution times. Some are impatient and want re-sults as soon as possible, even if the results may be of lower quality. For other users, waiting a bit longer may be accept-able if it means better results. Profiling of users along these lines is possible with log analysis, for example, by noting how frequently users click on the browser stop button.
Second, information needs are diverse. For example, a search for a simple factoid should be handled immediately, and as long as the desired information appears in the top hit or near the top, spending additional time on ranking represents wasted effort. On the other hand, complex infor-mational queries might benefit from additional processing that, for example, takes into account term proximity and complex document features.

Finally, temporally constrained ranking functions are in-teresting from a systems perspective. In real-world settings, high query throughput is usually achieved by service repli-cation and spreading the query load across a large number of servers. System capacity is fixed or relatively static (i.e., there are only so many available servers in a datacenter). Unfortunately, query load is highly variable. In addition to daily cycles, a system might experience sudden spikes in us-age (i.e., the flash crowd effect). How a system behaves un-der load depends on the exact architecture, but temporally constrained ranking functions can help the system adapt. For example, when query load is high, we might tighten the temporal constraints to maintain roughly the same query latency, at the cost of some reduction in quality X  X his might be preferable to forcing users to wait longer for results.
Temporally constrained ranking functions can also help manage variance in query execution times. For real-world services, we are not only interested in the mean, i.e., how long on average a user needs to wait for results, but also out-liers, i.e., how slow the bottom 5% of queries are. Control-ling instances of the latter case is very important, since they may represent dissatisfied users who never come back. Fi-nally, retrieval may comprise only one component in a larger system (e.g., for summarization, interactive browsing, etc.). Successful composition of services often requires some guar-antees (e.g., service level agreements) on the running times of individual components.

It is true that we can address the above issues by devis-ing multiple ranking algorithms that encode specific trade-offs, and then dynamically select the appropriate algorithm at query time. The obvious downside, however, is dupli-cate development effort. Instead, it would be more desir-able to have a single ranking algorithm that comes with an adjustable  X  X nob X  to set the desired query evaluation time.
This work has three primary contributions. First, we in-troduce the notion of temporally constrained ranking func-tions for information retrieval. Second, we propose two methods for automatically constructing and learning tem-porally constrained ranking functions. Finally, we introduce a new metric that accounts for retrieval effectiveness as a function of time, which provides an objective function in our learning-to-rank approach. Experimental results on multi-ple TREC test collections show that our proposed ranking functions are effective across a range of temporal constraints.
The remainder of the paper is organized as follows: We start with a discussion of related work. Section 3 describes temporally constrained linear models and two algorithms for constructing such models. Our methods are evaluated in Section 4, before discussing future work and concluding.
To our knowledge, no existing information retrieval model can satisfy arbitrary time constraints. A particular model, be it probabilistic [25], LM-based [24, 30], or machine learn-ed [19, 4], induces a fixed computational cost that can be quantified in terms of the number of postings traversed, floating-point computations required, etc. It is not possi-ble to force a ranking model to return lower-quality results early, nor is it possible to achieve higher-quality results if given more time. The closest related work is that of Wang et al. [29], who propose learning-to-rank methods that incor-porate both effectiveness and efficiency metrics. However, the learned models merely select a more optimal operating point in the space of effectiveness/efficiency tradeoffs, but do not provide control on an individual query basis, unlike in this work. The work of Collins-Thomspon [10] is also relevant. The proposed approach uses convex programming to choose query expansion terms, incorporating estimates of retrieval cost. However, our work focuses on ad hoc retrieval, as opposed to query expansion.

It is important to distinguish our work from efficient query evaluation approaches. We build ranking functions that sat-isfy each query X  X  time requirement, whereas query evalua-tion strategies, such as early-termination techniques [7, 2, 28], simply improve the speed of existing ranking models, but do not provide an explicit mechanism for controlling the tradeoff between effectiveness and efficiency. The same critique applies to work on index pruning [8, 23], where the goal is to improve query evaluation speed by reducing the size of the index. While Shmueli-Scheuer et al. [27] consider query evaluation under budgetary constraints, but similar to the aforementioned early-termination techniques, the work is concerned with efficient query evaluation, rather than building efficiency-minded ranking functions.

Caching of posting lists or search results provides another way to speed up query evaluation [3], but this is comple-mentary to our proposed approach. The primary purpose of caching is to improve the efficiency of a given ranking func-tion. In contrast, we learn ranking functions according to temporal constraints, where query evaluation and caching strategies are assumed to be given. Thus, our work is or-thogonal to caching and can be integrated with any caching strategy by modifying the cost model.

Finally, our work is also related to machine-learned (lin-ear) ranking functions [21, 13, 5, 19] and feature selection [14, 20]. However, none of the existing approaches address the problem of learning temporally constrained ranking func-tions. In fact, our proposed ranking functions generalize conventional machine-learned linear ranking functions. The two models converge to the same solution as the temporal constraint is relaxed.
This section formalizes the problem of temporally con-strained ranking, where in addition to a query q , we specify a time constraint T ( q ) for the query. We first provide a brief overview of linear ranking functions, and introduce the concept of temporally constrained linear ranking functions. We then present novel formulations based on probabilistic prediction models for constructing temporally constrained functions. Finally, we discuss how to satisfy the time re-quirement and estimate model parameters.
Linear ranking functions are a class of simple, yet effec-tive ranking functions. Many widely used ranking models belong to this family of ranking functions [21, 17, 4, 13, 5]. Our use of this class of ranking models is motivated by their demonstrated effectiveness over publicly benchmarked retrieval tasks [1, 9].

A linear ranking function is characterized by a set of fea-tures F = f 1 ,...,f N and the corresponding model parame-ters  X  =  X  1 ,..., X  N . Each feature f i is a function that maps a query-document pair ( q,d ) to a real value. The relevance of document d with respect to query q is computed as: Hence, each document is scored by a weighted linear com-bination of the feature values computed over the document and query. Similar to previous work [4, 17], we assume that the model parameters  X  i take the following parametric form: where g j  X  X  are meta-features defined over the query for each feature i , and w j  X  X  are the free parameters. Hence,  X  pends on q via g j and w j . Using this formulation, the rank-ing function can be re-written as: ordered window  X  q j q j +1  X  (span= N ) in D (Dirichlet) unordered window  X  q j q j +1  X  (span= N 0 ) in D (Dirichlet) unordered window  X  q j q j +1  X  (span= N 0 ) in D (BM25) Table 1: Features used in our linear ranking functions. Here, tf N scoring function parameters.
 We now describe the features f i and g j that we consider in this work. Both term-based features [24, 30] and term prox-imity features [6, 21] have been widely used in such mod-els, and have been shown to be especially successful when used in combination [20]. We take a similar feature-oriented approach and use both term features and term proximity features as part of our feature pool. Table 1 provides a sum-mary of the query-document features f i considered in this work. Among them, we use two different unigram term fea-tures, each using a different feature scoring function (BM25 or Dirichlet). We use a set of term proximity features defined over bigrams within the query, where each is computed from a BM25 or Dirichlet scoring function, for a specific window type (ordered/unordered) and window length. These are similar to features previously explored [20].

For the query-dependent meta-features g j , we use features similar to those described by Bendersky et al. [4], includ-ing collection-based (collection frequency and document fre-quency), features from external sources (English Wikipedia and a large Web collection), and a constant feature. The meta-features are summarized in Table 2.

Note that as a result of this query-dependent weighting [4, 17], features f i that are defined on the same unigram/bigram (i.e., query concept) in the query will have the same feature weight value  X  i . This property will be used for develop-ing efficient algorithms for creating temporally constrained ranking functions in Section 3.3. Most of the computational cost associated with such ranking functions comes from the cost incurred from evaluating the features f i . Unlike the features f i , we assume that there is negligible cost associ-ated with computing the meta-features g j . Typically, in an operational setting retrieval engines would have access to large-scale, low-latency distributed caches for these types of global statistics.
In a temporally constrained setting, using all features in the linear ranking function may exceed the time require-ment. Instead, for each query, we need to alter the efficiency characteristics of the ranking function by using only a subset g 1 ( q j ,q j +1 ) # times bigram occurs in the collection g 2 ( q j ,q j +1 ) # documents bigram occurs in the collection g 3 ( q j ,q j +1 ) # times bigram occurs in ClueWeb g 4 ( q j ,q j +1 ) # times bigram occurs in a Wikipedia title g 5 ( q j ,q j +1 ) 1 (constant feature) of the features to meet its time requirement. We call the re-sulting linear ranking function a temporally constrained lin-ear ranking function and it has the following general form: where C ( f i ( q,  X  )) denotes the computational cost of evalu-ating feature f i for q over the document collection, T ( q ) is the time requirement for query q , S i is a binary value de-noting whether or not f i ( q,  X  ) is used for q , and  X  i linear feature weights, parameterized by meta-features as described in the previous section.

To instantiate a model, we must address the following: 1) how to best select the subset of query dependent features to construct the corresponding temporally constrained ranking function for each query q ; 2) how to define the cost func-tion C ( f i ( q,  X  )) for the linear features f i ( q,  X  ) such that the response time of the resulting ranking function will not ex-ceed the time requirement; 3) how to determine the free pa-rameters (i.e., meta-feature weights w j ) for the temporally constrained ranking function. We described our proposed solution to these issues in the following sections.
This section introduces two methods for creating tempo-rally constrained ranking functions. Both are based on a class of probabilistic prediction models that can naturally impose time constraints on query execution times. The first method, which we call  X  X ndep X , makes independent decisions when selecting which features f i should be evaluated. The second method, which we call  X  X oint X , goes an extra step by accounting for query-level feature redundancy. We note that in contrast to previously proposed feature selection methods for ranking [14, 20], our problem is unique in that the fea-ture selection is driven by the time constraint for each query, not purely by effectiveness.

In both methods, the decision on what features to use largely depends on the feature weights  X  i ( q ). Features with large weights are more likely to be selected when facing a time constraint because they have more impact on the fi-nal ranking. In particular, in the  X  X ndep X  model, the likeli-hood of a feature f i ( q,  X  ) being selected is directly propor-tional to its query dependent feature weight  X  i ( q ). In the  X  X oint X  model, as we will show, the selection depends both on the feature weight and the feature X  X  redundancy relation-ship with respect to other features.
The independent prediction model is based on a logistic regression model. It aims to directly estimate the likelihood that a given feature f i should be included in the ranking function. Using this model, the probability of selecting fea-ture f i (i.e., S i = 1) is computed as:
P ( S i | q ) = exp(  X  i ( q )) where the g j  X  X  are the meta-features used in our linear rank-ing function and the w j  X  X  are the corresponding model pa-rameters. When defined this way, the likelihood of choos-ing feature i is monotonic with respect to  X  i ( q ), the query-dependent weight from our linear ranking function. As men-tioned earlier, this is intuitively appealing, as it is desirable to choose features with large weights, since they are more likely to have greater impact on the ranking.

Under this independent model, the joint probability of selecting features is simply the product of individual selec-tion likelihoods. Given a time constraint T ( q ) for query q , we need to infer the most likely selections over all features and construct the corresponding constrained ranking func-tion R ( q ), such that the time cost of R ( q ) will be within T ( q ). While many existing methods are available for gen-eral inference for prediction models [15], such methods are unconstrained. Instead, inference under a time constraint T ( q ) for a given query can be shown to be equivalent to the following optimization problem, after doing a few sim-ple logarithmic operations on Equation 1: where S i is a binary variable indicating whether feature f and its corresponding feature weight  X  i will be used in the ranking function or not, C ( f i ( q,  X  )) is the cost of evaluating Algorithm 1 : Independent Ranking
Input : Query execution time requirement T(q); Output : Temporal constrained ranking function R(q)
Compute feature weights:  X  i ( q ) = X
Compute feature profit density: p i =  X  i ( q ) C ( f Queue F : features sorted by decreasing profit density; Initialize R ( q ) = {} ;
Initialize totalCost = 0; while size( F ) &gt; 0 do end return R ( q ); f for q over the collection, and T ( q ) is the time constraint. The goal of the optimization is to find an optimal set of features that maximizes the objective and satisfies the time constraint T ( q ). Similar approaches for casting inference as an optimization task have also been used in the NLP task of semantic role labeling [26]. This formulation has the advan-tage that it enables us to impose arbitrary linear constraints (such as temporal constraints) on the model outputs (i.e., the temporally constrained ranking function).

Solving the stated optimization problem in Equation 2 is done at runtime, so for each query, the complexity can-not be very high. Nonetheless, the Indep model allows for an extremely efficient inference from which the temporally constrained ranking function is constructed. Note that our query cost model excludes the inference cost because, as we will show, the running time complexity for constructing ranking functions is negligible for reasonably sized queries.
The process of inferring the temporal ranking function for the Indep model is presented in Algorithm 1. It should be easy to see that for the Indep prediction model, the opti-mization problem corresponds to the classic knapsack prob-lem . Here, features are  X  X tems X  that we are trying to fit into the knapsack. The value of each corresponds to the fea-ture weight  X  i ( q ), the cost corresponds to C ( f i ( q,  X  )), which will be explained in detail in Section 3.4, and the knapsack capacity is T ( q ). We first compute profit density for each feature f i , which is defined by the ratio between its value this density (largest first), until we can no longer add any more features without overshooting our time constraint, or until when we run out of features to use.

Since the number of features is linear in | q | , the length of the query, the time complexity for this process is O ( | q | log | q | ), which accounts for sorting and adding features into the con-strained ranking function. This is trivial compared to the time taken for query evaluation.
The independent prediction model assumes that feature selection decisions are made independently of each other. While this assumption is reasonable for small feature sets, it may not hold as well for larger feature sets. Therefore, we would like to model relationships between features and eliminate the computation of redundant features that do not add very much to the ranking function in terms of relevance, yet result in increased query execution times.

We define redundancy only for features defined over the same query concept (i.e., unigram or bigram). The intu-ition is that under a time constraint, we would like to use features that provide maximal coverage over all query con-cepts, rather than repeatedly using features for the same query concept. Hence, for a pair of features f i and f j share a common query concept (i.e.,  X  i ( q ) =  X  j redundancy is defined as follows: where  X  is a weight threshold. This definition says that f and f j are redundant if the feature weight does not exceed threshold  X  . This definition allows us to impose redundancy penalty on features defined over less important query con-cepts, as determined by the feature weight value.

We propose using an undirected probabilistic graphical model [12] to capture the feature importance and redun-dancy relationship between features when making the se-lection under a time constraint. An undirected graphical model is a probabilistic model defined over an undirected graph, which encodes the conditional independence assump-tions amongst random variables, corresponding to the nodes in the graph. Here, we make use of the Ising model (a.k.a. Boltzmann machine) [16] to represent individual features and their pairwise redundancy relationships. Under this model, the joint probability over all feature decisions is: where N ( i ) are the features that share an edge with feature i in the graph (i.e., features in N ( i ) are defined over same query concept as f i ), r ( f i ,f i 0 ) is the redundancy feature defined over the pair ( f i ,f i 0 ) and  X  is the model parameter associated with redundancy feature r . S i , w j , and g j defined earlier.

Similar to the Indep model, after performing simple loga-rithmic operations on Equation 3, the probability of select-ing features f 1 ,...,f k under a time constraint can be stated by the following optimization problem: The goal of the optimization is to find a set of features that reach an optimal balance between feature effectiveness and feature redundancy, while satisfying the time constraint T ( q ). Note that when  X  = 0 this problem reduces to the independent case.

The optimization problem is an integer linear program [22], and several practical solutions exist. Most commonly, they Algorithm 2 : Joint Ranking
Input : Query execution time requirement T(q); Output : Temporal constrained ranking function R(q)
Compute feature weights:  X  i ( q ) = X
Compute feature profit density: p i =  X  i ( q ) C ( f Queue F 1 : features sorted by decreasing profit density; Initialize queue F 2 = {} ; Initialize R ( q ) = {} ; Initialize totalCost = 0; For each concept e , let G e = set of features defined on e ; Let  X  e denote the weight of features in G e ;
Let covered [ e ]=false for all e ; while size( F 1 ) &gt; 0 or size( F 2 ) &gt; 0 do end return R ( q ); iteratively make decisions based on a score value for each fea-ture (similar to the knapsack problem). However, the stan-dard solutions require continually re-sorting the features af-ter a feature is added and the values of its neighbor features are re-computed to account for redundancy. Our problem has the nice property that all features defined over the same query concept share a common feature weight value, and their redundancy features are binary valued. This property enables us to derive an efficient solution to the optimiza-tion problem (Algorithm 2) that has the same running time complexity as Algorithm 1. Experiments in Section 4 show that the Joint model is significantly more effective than the Indep model.

Similar to the independent case, we add features according to their profit densities (largest first). When a feature f added, if its feature weight does not exceed  X  , the selection likelihoods of its remaining neighboring features are recom-puted to account for the feature redundancy penalty (if not already done). In this case, feature ordering needs to be adjusted to account for new weights. But because features defined on the same query concept share the same weight value, there is no need to resort them after they receive the same penalty  X  . To make sure they are properly placed with respect to the other features, we maintain two queues: F 1 contains non-penalized features as sorted originally, and F contains the penalized features, which are always sorted as explained earlier. This way, there is no need to repeatedly perform re-ordering operations after new weights are com-puted. We select the feature with the best profit density between F 1 and F 2 as the next feature for the temporally constrained ranking function during each iteration.

The time complexity for this process is O ( | q | log | q | ), ac-counting for the initial sorting of features and for creating the ranking function. In practice, this cost is trivial for rea-sonably sized queries.
In this section, we describe how we define C ( f i ( q,  X  )), the cost for evaluating feature f i ( q,  X  ) for q over the document collection. More specifically, we are interested in character-izing the efficiency of a linear ranking function as a result of using a particular subset of features. Efficiency can be interpreted in two ways: 1) in an absolute sense, in terms of the total amount of time necessary to compute the ranked list, or 2) in a relative sense, where given a baseline ranking model, we measure how much more (or less) efficient our proposed method is compared to the baseline. In our work, we adopt the second interpretation because it factors out differences in hardware.

Thus, we model the query execution time relative to some baseline ranking function. This type of cost model requires us to capture the costs associated with evaluating different features f i within the linear ranking function. The cost for a given linear ranking function R ( q ) is computed as the sum of its individual feature costs: We estimate C ( f i ( q,  X  )) as the sum of document frequencies DF ( t ) of each term t required to compute f i . That is, For features defined over unigrams, this sum only has a sin-gle component, while for features defined over bigrams, it has two components. Intuitively, this analytical model cap-tures the fact that evaluating more features and evaluating each feature over longer postings lists (i.e., large DF values) will both result in greater time complexity.

Given a baseline ranking function, we want to construct a temporally constrained ranking function R ( q ) such that the actual execution time of the model is within a multiple k of the baseline model X  X  query execution time R b ( q ). This can be expressed as the following constraint: where the cost is computed as described above. Our algo-rithm for constructing R ( q ) will enforce this constraint. As will be shown in experiments, this very simple cost model works surprisingly well for ensuring the actual time T ( R ( q )) of the model meets the actual time constraint: where k T ( R b ( q )) denotes the time requirement for q , as a multiple of baseline time T ( R b ( q )).
It should be clear that a new optimization metric must be defined here, because commonly-used effectiveness metrics (MAP, P20, etc.) do not directly account for how the effec-tiveness of models varies across a range of time constraints. To evaluate the performance of a temporally constrained ranking function, the metric must account for the expected effectiveness over different time budgets. More formally, we avg qlen (desc.) 6.08 5.90 5.88 Table 3: Number of docs, topics, and average query lengths of test collections used in our experiments. define the expected effectiveness E q for a query q as: Where e q ( t ) denotes the effectiveness achieved by the con-strained ranking function associated with time constraint t , and P ( t ) represents the likelihood of t being assigned as the time constraint for the query. Any commonly-used effec-tiveness metric (such as average precision, P20, etc.) can be used in the effectiveness measure e q ( t ) at each single time point t . In our experiments section, we report results from using average precision and P20.

For simplicity, we assume all t values are equally likely (i.e., uniform distribution for P ( t )). However, we can assign various other distributions (i.e., Gaussian, exponential, etc.) to model more complex distributions for time constraints.
Taking the mean of E over a set of n queries gives us the mean expected effectiveness: which represents the overall performance of the ranking func-tions across different time constraints and queries.
The set of free parameters we need to learn offline are the meta-feature weights w j and the redundancy feature weight  X  and threshold  X  (Joint model only). We note that the values of these parameters versus the objective metric M
E is not smooth and there are multiple local maxima, so the standard gradient-search based methods cannot be ap-plied. Instead, we employ a simple line search algorithm for optimizing the various model parameters by directly opti-mizing the mean expected effectiveness M E on the training set. This approach has been used by several earlier works for estimating parameters in ranking functions to directly optimize objective metrics [4, 21]. It iteratively optimizes the metric by performing a series of one-dimensional line searches. At each iteration, it searches for an optimal value for a parameter while holding all other parameters fixed. This iterative process continues until the improvement in the objective metric drops below a threshold. More details of this parameter estimation can be found in [22].
We report experimental evaluation of our proposed tem-porally constrained ranking models on three TREC web test collections: Wt10g, Gov2, and Clue (first English segment of ClueWeb09). Details of these test collections are provided in Table 3. The title and description portions of the cor-responding TREC topics were used as queries, split equally into a training and test set. All parameter tuning was per-formed on the training set, and all of the results reported are from applying the learned parameters to the test set. upper-bound is plotted as the upper dotted line.
 Table 5: Avg. query execution time of baseline QL for title and desc. queries of Wt10g, Gov2, Clue.

Our experiments compare average precision (AP), preci-sion at 20 (P20), and mean expected effectiveness M various models, including a baseline query-likelihood (QL) model, a baseline sequential dependence (SD) model [21], the Indep model, and the Joint model. In addition, we con-structed an upper-bound model, called  X  X ll features X , which is a standard learning-to-rank model (i.e., optimized for ef-fectiveness only with no temporal constraints) trained over the entire feature set. To remain consistent with our pro-posed models, the  X  X ll features X  model is constructed in the same manner as the temporally constrained models, except the temporal constraint is set to infinity. The Wilcoxon signed rank test with p &lt; 0 . 05 was used to test for statisti-cally significant differences between the methods.

We implemented our framework on top of Ivory, a newly developed open-source web-scale information retrieval en-gine [18]. All experiments were run on a SunFire X4100, with two Dual Core AMD Opteron Processor 285 at 2.6GHz and 16GB RAM (although all experiments used only a sin-gle thread). Recall that we varied the time constraint for each query q to be a multiple k of the query execution time of baseline QL, denoted by k  X  T QL ( q ). The average query execution times of baseline QL for various data collections and queries are shown in Table 5 for reference.
Figure 1 compares the MAP of our proposed temporally constrained ranking algorithms as a function of time con-straints, from 1  X  T QL ( q ) to 5  X  T QL ( q ) in increments of 0 . 5  X  T
QL ( q ) for each query (title, top row; description, bottom row). In each graph, the effectiveness of the baseline QL model is plotted as the lower solid line, and the  X  X ll fea-tures X  upper-bound is plotted as the upper dotted line.
While in general both the Indep and Joint models produce more effective results than baseline QL when given more time, the Joint model consistently achieves equal or higher MAP than the Indep model across all time constraints for both title and description queries. It also approaches the upper bound more rapidly as the time constraint is relaxed. This shows that the Joint model is superior to the Indep model across a wide range of retrieval scenarios.

Another interesting aspect is that the effectiveness dif-ference between the Indep and Joint models is largest under tight temporal constraints, while the two models tend to per-form similarly when the temporal constraint is relaxed. This can be explained by the fact that eliminating redundant fea-tures is more critical under strict temporal constraints X  X n such a case, using a diverse set of features is preferred. The effect of redundant features diminishes as the time constraint increases, since there is sufficient time to evaluate more fea-tures (redundant or otherwise). Feature redundancy also explains why the Indep model is different with respect to Joint and QL for very strict temporal constraints: it repeat-edly selects features based on profit density alone. Thus, the time constraint will likely be violated before (at least) one feature from each facet in the query can be selected. Note this is not a problem with baseline QL, which only uses one feature for each term so there are no redundant features. In other words, given a rich feature set, under a strict time constraint, effectiveness is more critically dependent on se-lecting both high quality and non-redundant features.
In general, MAP increases as the time constraint is re-laxed, for both the Joint and Indep models. Once again, this makes sense since the models are able to benefit from more features. However, we encounter a point of diminish-ing returns around 4  X  T QL , which suggests that using a very large number of features, as done by previous effectiveness-for title and description queries from Wt10g, Gov2, and Clue. centric ranking models, may not be very desirable in our formulation of temporally constrained ranking, at least on these datasets. In a learning-to-rank scenario that only takes effectiveness into account (which might roughly correspond to the right edges of our graph), we feel that the marginal gains in effectiveness that comes with evaluating more fea-tures (thus taking more time) trades off too much efficiency for a small effectiveness gain. This speaks to the advantage of our temporally constrained ranking functions, which al-lows the tradeoff between effectiveness and efficiency to be explicitly considered.
Recall that our constrained ranking functions use an an-alytical cost model based on the sum of document frequen-cies, which are highly correlated with actual query evalua-tion times. However, whether the time constraints are actu-ally met on the test data is an empirical question, since there are no theoretical guarantees. As described in Section 3.3, the time for constructing ranking functions is negligible com-pared to retrieval time for reasonably sized queries, thus it is not included in the query evaluation times.

Figure 2 illustrates how well, in reality, the temporally constrained ranking functions satisfy query-specific time con-straints on the test data. Each set of bars represents a par-ticular time constraint, defined as before in terms of multi-ples of the baseline QL query evaluation time. The height of each bar shows the fraction of queries that satisfy the re-quired time constraint. For each time constraint t , the tem-porally constrained ranking function is said to have satisfied the constraint if the actual query execution time is less than or equal to t . We see that the  X  X it rates X  (i.e., meeting the specified time constraint) for both ranking algorithms are well above 80% for almost all test collections, while most of the values are above 90%. This suggests our simple sum-of-dfs cost model, which is used by the algorithm to guarantee the time constraints, works reasonably well in practice.
What is not shown on the graphs is that the same queries failed to meet multiple time constraints. For instance, the query  X  X aps of United States X  missed all of the time con-straints for the Clue collection. We found the queries that often missed their targets usually contain many frequent terms and/or are longer than average. This suggests that our analytical model underestimates the cost of these  X  X ut-lier X  queries. As part of future work, we would like to explore ways to improve our analytical model to better handle such queries to minimize the number of queries that miss the specified time constraint.

Finally, a query is classified as a  X  X iss X  if its response time is greater than the required time t . However, if the time exceeds t by only a small amount, it may still be tolerable in practice. For instance, by allowing the response time to exceed t by a small amount (e.g., 5%), we observed that our models achieve 100% hit rates in many cases. Further details are omitted due to space constraints.
To investigate the average effectiveness of our proposed temporally constrained ranking algorithms across different time points, we compared their mean expected effectiveness (according to MAP and P20) in Table 4 for title and descrip-tion queries across all three test collections. Results from the QL baseline are also reported. In terms of averaged effec-tiveness computed using MAP, the Joint model consistently and significantly outperforms both Indep and QL. Specifi-cally, for title queries, the Joint model attains 7 . 5% , 6 . 8%, and 13 . 3% improvements over QL on Wt10g, Gov2, and Clue, respectively. The Joint model achieves 2 . 7% , 7 . 7%, and 13 . 1% gains over the Indep model on the same col-lections, respectively. Similar improvements are observed under averaged effectiveness computed using P20. We also note that the improvements of Joint over QL and Joint over Indep are statistically significant in two out of three collec-tions, under both of the effectiveness metrics. The Indep E (P20) T 98 M E (MAP) M E (P20) T 98 54.76  X   X  3 23.51  X  40.54  X   X  2.5 E (P20) T 98 M E (MAP) M E (P20) T 98 51.25 5 16.93 34.69  X  52.82  X  3 18.18 37.05 4 QL/Indep for Joint. model improves over QL in two out of three collections, un-der both effectiveness metrics, although the improvements are not statistically significant for the title queries.
For the description queries, both the Indep and Joint mod-els achieve significant improvements over QL across all col-lections, while the Joint model also outperforms the Indep model in all three collections. The improvements of the In-dep model over QL in terms of average effectiveness using MAP are 10 . 7% , 7 . 0%, and 1 . 5%, respectively on the three collections. In two of these three cases the improvements of Indep over QL are statistically significant. Similar results are obtained under averaged effectiveness computed from P20. Compared to title queries, this demonstrates that se-lecting features according to query dependent weights un-der time constraints (i.e., largest weights first), as employed by the Indep model, is more beneficial for verbose queries than short title queries. This is in line with previous find-ings [17, 4] for creating effectiveness-centric ranking models (i.e., no time constraint), where query dependent weighting was shown to be more useful for long queries. The Joint model consistently outperforms both QL and Indep by large margins for the description queries, and in most cases the differences are significant, which confirms our earlier obser-vation that by reducing feature redundancy and selecting high quality features, the joint model can achieve high effec-tiveness across a wide range of time constraints. Note that for description queries, the Joint model outperforms the In-dep model even on looser time constraints because there exists more redundancy in longer queries.

In addition to effectiveness, we are also interested in mea-suring how quickly each model converges to the effective-ness of the  X  X ll features X  upper-bound. To approximate this measure, we let T d denote the time (relative to T QL ) taken by each model to achieve d % of the upper-bound effective-ness, where d % is chosen to be 98% in our experiments and effectiveness is measured in MAP. In terms of T 98 , we see that for both title and description queries, the Joint model has quantitatively better convergence rates than the Indep model across all collections. Furthermore, unlike the Indep model, the Joint model is able to attain 98% of upper-bound effectiveness across all collections and query types. We note that the QL baseline is not able to attain the specified effec-tiveness (i.e., 98% of upper-bound), thus it is not meaningful to compute T 98 for it.
How does our temporally constrained ranking models com-pare with a previously proposed effectiveness-centric model? The sequential dependence model (SD) [21] is a widely-used term proximity retrieval model: it adheres to a rigid efficiency-effectiveness tradeoff (i.e., it does not adapt to time constraints) but has demonstrated good performance across various tasks [21, 17, 4]. For fairness of compari-son, we calibrate the time constraints of our temporally con-strained models with respect to the SD model X  X  time T SD , where T SD is approximately equal to 4  X  T QL ( q ) for each query q . Table 6 reports results on title and description queries for the SD, Indep, and Joint models.
 From these results, it is evident that both the Indep and Joint models significantly outperform SD under the same temporal constraint as the SD model. For title queries, the gains in MAP range between 4 . 1% and 5 . 4% for Indep, and range between 5 . 0% and 10 . 7% for the Joint model. Fur-ther, we observe that the gains of Indep over SD are greater for description queries than title queries. For example, the Indep model improves over SD by 11 . 27% and 11 . 7% for Wt10g and Gov2, respectively, more so than for title queries, with the only exception being the Clue collection description queries. However, the Joint model consistently outperforms both the SD and Indep models in all conditions. These re-sults not only suggest that our temporally constrained rank-ing functions can subsume the SD model and the QL baseline (as illustrated previously) as special tradeoff cases between effectiveness and efficiency, but our models can in fact return better results than those commonly-used retrieval models under the same time cost.
In this paper, we introduced the notion of temporally con-strained ranking functions for information retrieval, which, given a query and a time constraint, produces the best pos-sible ranked list within the specified time limit. This new problem was motivated by the fact that although current learning-to-rank approaches for information retrieval are ca-pable of learning highly effective ranking functions, the im-portant issue of efficiency has been largely ignored. Our models can be viewed as a way to control the effective-ness/efficiency tradeoff when learning to rank. We have shown that they subsume commonly-used ranking models as special tradeoff cases that encode fixed points in the ef-fectiveness/efficiency solution space.

We proposed and empirically evaluated two temporally constrained ranking algorithms: one that makes independent feature selection decisions, and the other that makes joint feature selection decisions. Experiments on a number of col-lections show that while both algorithms are effective across different time constraints, the Joint model delivers higher quality results. We also verified that our models can guaran-tee actual query evaluation times in practice. Furthermore, we demonstrated that our models can in fact return more effective results than those commonly-used retrieval models under the same time cost.

There are several specific future directions. First, we would like to develop a better understanding of feature re-dundancy and its impact on ranking functions beyond the current redundancy features examined in this work. Second, due to limited space, we were only able to show a few pos-sibilities for time constraints. We would like to explore var-ious distribution models on time constraints (Gaussian, ex-ponential, etc.) to account for various real-world situations. Finally, we are interested in building and plugging in ad-ditional analytical models of query evaluation time into our general framework that can capture other retrieval strategies (e.g., those that incorporate caching). This work was supported in part by the NSF under awards IIS-0836560 and IIS-0916043; by DARPA contract HR0011-06-02-001 (GALE). Any opinions, findings, conclusions, or recommendations expressed are the authors X  and do not nec-essarily reflect the sponsors X . The third author is grateful to Esther, Kiri, Joshua, and Jacob for their loving support.
