 With millions of tweets posted daily, Twitter en-ables both individuals and organizations to dis-seminate information, from current affairs to breaking news in a timely fashion. In this work, we study the wikification (Disambiguation to Wikipedia) task (Mihalcea and Csomai, 2007) for tweets, which aims to automatically identify each concept mention in a tweet, and link it to a concept referent in a knowledge base (KB) (e.g., Wikipedia). For example, as shown in Figure 1, Hawks is an identified mention, and its correct ref-erent concept in Wikipedia is Atlanta Hawks . An end-to-end wikification system needs to solve two sub-problems: (i) concept mention detection, (ii) concept mention disambiguation.

Wikification is a particularly useful task for short messages such as tweets because it allows a reader to easily grasp the related topics and en-riched information from the KB. From a system-to-system perspective, wikification has demon-strated its usefulness in a variety of applica-tions, including coreference resolution (Ratinov and Roth, 2012) and classification (Vitale et al., 2012).

Sufficient labeled data is crucial for supervised models. However, manual wikification annota-tion for short documents is challenging and time-consuming (Cassidy et al., 2012). The challenges are: (i) unlinkability , a valid concept may not ex-ist in the KB. (ii) ambiguity , it is impossible to determine the correct concept due to the dearth of information within a single tweet or multiple correct answer. For instance, it would be diffi-cult to determine the correct referent concept for  X  Gators  X  in t 1 in Figure 1. Linking  X  UCONN  X  in t 3 to University of Connecticut may also be ac-ceptable since Connecticut Huskies is the athletic team of the university. (iii) prominence , it is chal-lenging to select a set of linkable mentions that are important and relevant. It is not tricky to select  X  Fans  X ,  X  slump  X , and  X  Hawks  X  as linkable men-tions, but other mentions such as  X  stay up  X  and  X  stay positive  X  are not prominent. Therefore, it is challenging to create sufficient high quality la-beled tweets for supervised models and worth con-sidering semi-supervised learning with the explo-ration of unlabeled data.
However, when selecting semi-supervised learning frameworks, we noticed another unique challenge that tweets pose to wikification due to their informal writing style, shortness and noisiness. The context of a single tweet usually cannot provide enough information for prominent mention detection and similarity computing for disambiguation. Therefore, a collective inference model over multiple tweets in the semi-supervised setting is desirable. For instance, the four tweets in Figure 1 are posted by the same author within a short time period. If we perform collective inference over them we can reliably link am-biguous mentions such as  X  Gators  X ,  X  Hawks  X , and  X  Bucks  X  to basketball teams instead of other concepts such as the county Bucks County .

In order to address these unique challenges for wikification for the short tweets, we employ graph-based semi-supervised learning algorithms (Zhu et al., 2003; Smola and Kondor, 2003; Blum et al., 2004; Zhou et al., 2004; Talukdar and Crammer, 2009) for collective inference by ex-ploiting the manifold (cluster) structure in both unlabeled and labeled data. These approaches normally assume label smoothness over a defined graph, where the nodes represent a set of labeled and unlabeled instances, and the weighted edges reflect the closeness of each pair of instances. In order to construct a semantic-rich graph capturing the similarity between mentions and concepts for the model, we introduce three novel fine-grained relations based on a set of local features, social networks and meta paths.

The main contributions of this paper are sum-marized as follows:  X  To the best of our knowledge, this is the first effort to explore graph-based semi-supervised learning algorithms for the wikification task.  X  We propose a novel semi-supervised graph reg-ularization model performing collective infer-ence for joint mention detection and disam-biguation. Our approach takes advantage of three proposed principles to incorporate both lo-cal and global evidence from multiple tweets.  X  We propose a meta path-based unified frame-work to detect both explicitly and implicitly rel-evant mentions. Concept and Concept Mention We define a con-cept c as a Wikipedia article (e.g., Atlanta Hawks ), and a concept mention m as an n-gram from a spe-cific tweet. Each concept has a set of textual repre-sentation fields (Meij et al., 2012), including title (the title of the article), sentence (the first sentence of the article), paragraph (the first paragraph of the article), content (the entire content of the arti-cle), and anchor (the set of all anchor texts with incoming links to the article).

Wikipedia Lexicon Construction We first construct an offline lexicon with each entry as possible referent concepts for the mention m . Following the previous work (Bunescu, 2006; Cucerzan, 2007; Hachey et al., 2013), we extract the possible mentions for a given concept c using the following resources: the title of c ; the aliases appearing in the introduction and infoboxes of c (e.g., The Evergreen State is an alias of Wash-ington state ); the titles of pages redirecting to c (e.g., State of Washington is a redirecting page of Washington (state) ); the titles of the disambigua-tion pages containing c ; and all the anchor texts appearing in at least 5 pages with hyperlinks to c (e.g., WA is a mention for the concept Washing-ton (state) in the text  X  401 5th Ave N [[Seattle]], [[Washington (state) X  X A]] 98109 USA  X . We also propose three heuristic rules to extract mentions (i.e., different combinations of the family name and given name for a person, the headquarters of an organization, and the city name for a sports team).

Concept Mention Extraction Based on the constructed lexicon, we then consider all n-grams of size  X  n (n=7 in this paper) as concept mention candidates if their entries in the lexicon are not empty. We first segment @usernames and #hash-tags into regular tokens (e.g., @amandapalmer is segmented as amanda palmer and #WorldWater-Day is split as World Water Day ) using the ap-proach proposed by (Wang et al., 2011). Segmen-tation assists finding concept candidates for these non-regular mentions. 3.1 Principles A single tweet may not provide enough evidence to identify prominent mentions and infer their cor-rect referent concepts due to the lack of contextual information. To tackle this problem, we propose to incorporate global evidence from multiple tweets and performing collective inference for both men-tion identification and disambiguation. We first in-troduce the following three principles that our ap-proach relies on.

Principle 1 (Local compatibility): Two pairs of  X  m,c  X  with strong local compatibility tend to have similar labels. Mentions and their correct referent concepts usually tend to share a set of characteristics such as string similarity between m and c (e.g.,  X  Chicago , Chicago  X  and  X  Facebook , Facebook  X  ). We define the local compatibility to model such set of characteristics.

Principle 2 (Coreference): Two coreferential mentions should be linked to the same concept. For example, if we know  X  nc  X  and  X  North Car-olina  X  are coreferential, then they should both be linked to North Carolina .

Principle 3 (Semantic Relatedness): Two highly semantically-related mentions are more likely to be linked to two highly semantically-related concepts. For instance, when  X  Sweet 16  X  and  X  Hawks  X  often appear together within rel-evant contexts, they can be reliably linked to two baseketball-related concepts NCAA Men X  X  Di-vision I Basketball Championship and Atlanta Hawks , respectively. 3.2 Approach Overview Given a set of tweets  X  t 1 ,...,t | T |  X  , our system first generates a set of candidate concept mentions, and then extracts a set of candidate concept referents for each mention based on the Wikipedia lexicon. Given a pair of mention and its candidate referent concept  X  m,c  X  , the remaining task of wikification is to assign either a positive label if m should be selected as a prominently linkable mention and c is its correct referent concept, or otherwise a neg-ative label. The label assignment is obtained by our semi-supervised graph regularization frame-work based on a relational graph, which is con-structed from local compatibility , coreference , and semantic relatedness relations. The overview of our approach is as illustrated in Figure 2. We first construct the relational graph G =  X  V,E  X  , where V = { v 1 ,...,v n } is a set of nodes and E = { e 1 ,...,e m } is a set of edges. Each v i =  X  m i ,c i  X  represents a tuple of mention m i and its referent concept candidate c i . An edge is added between two nodes v i and v j if there is a proposed rela-tion based on the three principles described in sec-tion 3.1. 4.1 Local Compatibility We first compute local compatibility (Principle 1) by considering a set of novel local features to cap-ture the importance and relevance of a mention m to a tweet t , as well as the correctness of its link-age to a concept c . We have designed a number of features which are similar to those commonly used in wikification and entity linking work (Meij et al., 2012; Guo et al., 2013; Mihalcea and Cso-mai, 2007).

Mention Features We define the following fea-tures based on information from mentions.  X  IDF f ( m ) = log( | C | number of concepts in Wikipedia and df ( m ) is the total number of concepts in which m occurs, and f indicates the field property, including ti-tle , content , and anchor . how likely m is used as an anchor in Wikipedia, where C a ( m ) is the set of concepts where m appears as an anchor. count ( m,c ) indicates the number of occurrence of m in c .  X  SNIL ( m ) and SNCL ( m ) to count the number of concepts that are equal to or contain a sub-n-gram of m , respectively (Meij et al., 2012).
Concept Features The concept features are solely based on Wikipedia, including the number of incoming and outgoing links for c , and the num-ber of words and characters in c .

Mention + Concept Features This set of fea-tures considers information from both mentions and concepts:  X  prior popularity prior ( m,c ) = sures the frequency of the anchor links from m to c in Wikipedia. tive frequency of m in each field representation f of c , normalized by the length of f . The fields include title , sentence , paragraph , content and anchor .  X  NCT ( m,c ) , TCN ( m,c ) , and TEN ( m,c ) to measure whether m contains the title of c , whether the title of c contains m , and whether m equals to the title of c , respectively.

Context Features This set of features include (i) Context Capitalization features, which indicate whether the current mention, the token before, and the token after are capitalized. (ii) tf-idf based fea-tures, which include the dot product of two word vectors v c and v t , and the average tf-idf value of common items in v c and v t , where v c and v t are the top 100 tf-idf word vectors in c and t .
Local Compatibility Computation For each node v i =  X  m i ,c i  X  , we collect its local features as a feature vector F i =  X  f 1 ,f 2 ,...,f d  X  . To avoid features with large numerical values that domi-nate other features, the value of each feature is re-scaled using feature standardization approach. The cosine similarity is then adopted to compute the local compatibility of two nodes and construct a k nearest neighbor ( k NN) graph, where each node is connected to its k nearest neighboring nodes. We compute the weight matrix that rep-resents the local compatibility relation as: 4.2 Meta Path
In this subsection, we introduce the concept meta path which will be used to detect corefer-ence (section 4.3) and semantic relatedness rela-tions (section 4.4).

A meta-path is a path defined over a network and composed of a sequence of relations between different object types (Sun et al., 2011b). In our experimental setting, we can construct a natu-ral Twitter network summarized by the network schema in Figure 3. The network contains four types of objects: Mention (M), Tweet (T), User (U), and Hashtag (H). Tweets and mentions are connected by links  X  X ontain X  and  X  X ontained by X  tionships can be described similarly.

We then define the following five types of meta paths to connect two mentions as:  X   X  X  -T -M X ,  X   X  X  -T -U -T -M X ,  X   X  X  -T -H -T -M X ,  X   X  X  -T -U -T -M -T -H -T -M X ,  X   X  X  -T -H -T -M -T -U -T -M X . Each meta path represents one particular seman-tic relation. For instance, the first three paths are basic ones expressing the explicit relations that two mentions are from the same tweet, posted by the same user, and share the same #hashtag, re-spectively. The last two paths are concatenated ones which are constructed by concatenating the first three simple paths to express the implicit rela-tions that two mentions co-occur with a third men-tion sharing either the same authorship or #hash-tag. Such complicated paths can be exploited to detect more semantically-related mentions from wider contexts. For example, the relational link between  X  narita airport  X  and  X  Japan  X  would be missed without using the path  X  narita airport -t 1 -u 1 -t 2 -american -t 3 -h 1 -t 4 -Japan  X  since they don X  X  directly share any authorships or #hashtags. 4.3 Coreference A coreference relation (Principle 2) usually occurs across multiple tweets due to the highly redundant information in Twitter. To ensure high precision, we propose a simple yet effective approach utiliz-ing the rich social network relations in Twitter.
We consider two mentions m i and m j corefer-ential if m i and m j share the same surface form or one is an abbreviation of the other, and at least one meta path exists between m i and m j . Then we define the weight matrix representing the corefer-ential relation as: W 4.4 Semantic Relatedness Ensuring topical coherence (Principle 3) has been beneficial for wikification on formal texts (e.g., News) by linking a set of semantically-related mentions to a set of semantically-related concepts simultaneously (Han et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013). However, the short-ness of a single tweet means that it may not pro-vide enough topical clues. Therefore, it is impor-tant to extend this evidence to capture semantic re-latedness information from multiple tweets.

We define the semantic relatedness score be-tween two mentions as SR ( m i ,m j ) = 1 . 0 if at least one meta path exists between m i and m j , otherwise SR ( m i ,m j ) = 0 . In order to compute the semantic relatedness of two concepts c i and c , we adopt the approach proposed by (Milne and Witten, 2008a): SR ( c i ,c j ) = 1  X  where | C | is the total number of concepts in Wikipedia, and C i and C j are the set of concepts that have links to c i and c j , respectively.
Then we compute a weight matrix representing the semantic relatedness relation as: where SR ( N i ,N j ) = SR ( m i ,m j )  X  SR ( c i ,c j ) and  X  = 0 . 3 , which is optimized from a develop-ment set. 4.5 The Combined Relational Graph Figure 4: A example of the relational graph con-structed for the example tweets in Figure 1. Each node represents a pair of  X  m,c  X  , separated by a comma. The edge weight is obtained from the lin-ear combination of the weights of the three pro-posed relations. Not all mentions are included due to the space limitations.
 Based on the above three weight matrices W loc , sponding transition matrices P loc , P coref , and P rel , respectively. The entry P ij of the transition matrix P for a weight matrix W is computed as P obtain the combined graph G with weight matrix  X  , and  X  are three coefficients between 0 and 1 with the constraint that  X  +  X  +  X  = 1 . They con-trol the contributions of these three relations in our semi-supervised graph regularization model. We choose transition matrix to avoid the domination of one relation over others. An example graph of G is shown in Figure 4. Compared to the referent graph which considers each mention or concept as a node in previous graph-based re-ranking ap-proaches (Han et al., 2011; Shen et al., 2013), our novel graph representation has two advantages: (i) It can easily incorporate more features related to both mentions and concepts. (ii) It is more appro-priate for our graph-based semi-supervised model since it is difficult to assign labels to a pair of men-tion and concept in the referent graph. Given the constructed relational graph with the weighted matrix W and the label vector Y of all nodes, we assume the first l nodes are labeled as Y l and the remaining u nodes ( u = n  X  l ) are ini-tialized with labels Y 0 u . Then our goal is to refine u and obtain the final label vector Y u .

Intuitively, if two nodes are strongly connected, they tend to hold the same label. We propose a novel semi-supervised graph regularization frame-work based on the graph-based semi-supervised learning algorithm (Zhu et al., 2003): Q ( Y ) =  X  The first term is a loss function that incorporates the initial labels of unlabeled examples into the model. In our method, we adopt prior popular-ity (section 4.1) to initialize the labels of the un-labeled examples. The second term is a regular-izer that smoothes the refined labels over the con-structed graph.  X  is a regularization parameter that controls the trade-off between initial labels and the consistency of labels on the graph. The goal of the proposed framework is to ensure that the refined labels of unlabeled nodes are consistent with their strongly connected nodes, as well as not too far away from their initial labels.

The above optimization problem can be solved directly since Q ( Y ) is convex (Zhu et al., 2003; Zhou et al., 2004). Let I be an identity matrix and D W be a diagonal matrix with entries D ii = P four blocks as W = an m  X  n matrix. D w is split similarly. We assume that the vector of the labeled examples Y l is fixed, so we only need to infer the refined label vector of the unlabeled examples Y u . In order to minimize Q ( Y ) , we need to find Y  X  u such that  X  X  Therefore, a closed form solution can be derived
However, for practical application to a large-scale data set, an iterative solution would be more efficient to solve the optimization problem. Let u be the refined labels after the t th iteration, the iterative solution can be derived as: Y The iterative solution is more efficient since ( D uu +  X I uu ) is a diagonal matrix and its inverse is very easy to compute. In this section we compare our approach with state-of-the-art methods as shown in Table 1. 6.1 Data and Scoring Metric For our experiments we use a public data set (Meij et al., 2012) including 502 tweets posted by 28 verified users. The data set was annotated by two annotators. We randomly sample 102 tweets for development and the remaining for evaluation. We use a Wikipedia dump on May 3, 2013 as our knowledge base, which includes 30 million pages. For computational efficiency, we also filter some mention candidates by applying the preprocess-ing approach proposed in (Ferragina and Scaiella, 2010), and remove all the concepts with prior pop-ularity less than 2% from an mention X  X  concept set for each mention, similar to (Guo et al., 2013).
A mention and concept pair  X  m,c  X  is judged as correct if and only if m is linkable and c is the correct referent concept for m . To evaluate the performance of a wikification system, we use the standard precision, recall and F1 measures. 6.2 Experimental Results The overall performance of various approaches is shown in Table 2. The results of the super-vised method proposed by (Meij et al., 2012) are obtained from 5 -fold cross validation. For our semi-supervised setting, we experimentally sam-ple 200 tweets for training and use the remain-ing set as unlabeled and testing sets. In our semi-is constructed by a k NN graph ( k = 20 ). The reg-ularization parameter  X  is empirically set to 0 . 1 , and the coefficients  X  ,  X  , and  X  are learnt from the development set by considering all the combina-tions of values from 0 to 1 at 0 . 1 intervals 1 . In order to randomize the experiments and make the comparison fair, we conduct 20 test runs for each method and report the average scores across the 20 trials.

The relatively low performance of the baseline system TagMe demonstrates that only relying on prior popularity and topical information within a single tweet is not enough for an end-to-end wik-ification system for the short tweets. As an exam-ple, it is difficult to obtain topical clues in order to link the mention  X  Clinton  X  to Hillary Rodham Clinton by relying on the single tweet  X  wolfblitzer-cnn: Behind the scenes on Clinton X  X  Mideast trip #cnn  X . Therefore, the system mistakenly links it to the most popular concept Bill Clinton .

In comparision with the supervised baseline proposed by (Meij et al., 2012), our model SSRegu 1 relying on local compatibility already achieves comparable performance with 50% of labeled data. This is because that our model performs collective inference by making use of the manifold (cluster) structure of both labeled and unlabeled data, and that the local compat-( 89 . 4% ). For example, the following three pairs of mentions and concepts  X  pelosi, Nancy Pelosi  X  ,  X  obama, Barack Obama  X  , and  X  gaddafi, Muam-mar Gaddafi  X  have strong local compatibility with each other since they share many similar char-acteristics captured by the local features such as string similarity between the mention and the con-cept. Suppose the first pair is labeled, then its pos-itive label will be propagated to other unlabeled nodes through the local compatibility relation, and correctly predict the labels of other nodes.
Incorporating coreferential or semantic related-ness relation into SSRegu 1 provides further gains, demonstrating the effectiveness of these two re-lations. For instance,  X  wh  X  is correctly linked to White House by incorporating evidence from its coreferential mention  X  white house  X . The corefer-ential relation (Principle 2) is demonstrated to be more beneficial than the semantic relatedness re-lation (Principle 3) because the former is detected with much higher precision ( 99 . 7% ) than the latter ( 65 . 4% ).

Our full model SSRegu 123 achieves significant improvement over the supervised baseline (5% ab-solute F1 gain with 95 . 0% confidence level by the Wilcoxon Matched-Pairs Signed-Ranks Test), showing that incorporating global evidence from multiple tweets with fine-grained relations is ben-eficial. For instance, the supervised baseline fails to link  X  UCONN  X  and  X  Bucks  X  in our examples to Connecticut Huskies and Milwaukee Bucks , re-spectively. Our full model corrects these two wrong links by propagating evidence through the semantic links as shown in Figure 4 to obtain mu-tual ranking improvement. The best performance of our full model also illustrates that the three re-lations complement each other.

We also study the disambiguation performance for the annotated mentions, as shown in Table 3. We can easily see that our proposed approach using 50% labeled data achieves similar perfor-mance with the state-of-the-art supervised model with 100% labeled data. When the mentions are given, the unpervised approach TagMe has already Table 4: The Performance of Systems Without Us-ing Concatenated Meta Paths. achieved reasonable performance. In fact, mention detection actually is the performance bottleneck of a tweet wikification system (Guo et al., 2013). Our system performs better in identifying the promi-nent mention. 6.3 Effect of Concatenated Meta Paths In this work, we propose a unified framework uti-lizing meta path-based semantic relations to ex-plore richer relevant context. Beyond the basic meta paths, we introduce concatenated ones by concatenating the basic ones. The performance of the system without using the concatenated meta paths is shown in Table 4. In comparison with the system based on all defined meta paths, we can clearly see that the systems using concate-nated ones outperform those relying on the sim-ple ones. This is because the concatenated meta paths can incorporate more relevant information with implicit relations into the models by increas-ing 1 . 6% coreference links and 9 . 3% semantic re-latedness links. For example, the mention  X  narita airport  X  is correctly disambiguated to the concept  X  Narita International Airport  X  with higher confi-dence since its semantic relatedness relation with  X  Japan  X  is detected with the concatenated meta path as described in section 4.2. 6.4 Effect of Labeled Data Size
In previous experiments, we experimentally set the number of labeled tweets to be 200 for over-all performance comparision with the baselines. In this subsection, we study the effect of labeled data size on our full model. We randomly sam-ple 100 tweets as testing data, and randomly se-lect 50 , 100 , 150 , 200 , 250 , and 300 tweets as labeled data. 20 test runs are conducted and the average results are reported across the 20 trials, as shown in Figure 5. We find that as the size of the labeled data increases, our proposed model achieves better performance. It is encouraging to see that our approach, with only 31 . 3 % labeled tweets ( 125 out of 400 ), already achieves a perfor-mance that is comparable to the state-of-the-art su-pervised model trained from 100% labeled tweets. 6.5 Parameter Analysis
In previous experiments, we empirically set the parameter  X  = 0 . 1 .  X  is the regularization pa-rameter that controls the trade-off between initial labels and the consistency of labels on the graph. When  X  increases, the model tends to trust more in the initial labels. Figure 6 shows the performance of our models by varying  X  from 0 . 02 to 50 . We can easily see that the system performce is stable when  X  &lt; 0 . 4 . However, when  X   X  0 . 4 , the sys-tem performance dramatically decreases, showing that prior popularity is not enough for an end-to-end wikification system. The task of linking concept mentions to a knowl-edge base has received increased attentions over the past several years, from the linking of concept mentions in a single text (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Milne and Witten, 2008a; Kulkarni et al., 2009; He et al., 2011; Rati-nov et al., 2011; Cassidy et al., 2012; Cheng and Roth, 2013), to the linking of a cluster of corefer-ent named entity mentions spread throughout dif-ferent documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013).

A significant portion of recent work considers the two sub-problems mention detection and men-tion disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Men-tion disambiguation is then formulated as a rank-ing problem, either by resolving one mention at each time (non-collective approaches), or by dis-ambiguating a set of relevant mentions simulta-neously (collective approaches). Non-collective methods usually rely on prior popularity and con-text similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches fur-ther leverage the global coherence between con-cepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferrag-ina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Rati-nov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when apply-ing the collective methods to short messages from social media, evidence from other messages usu-ally needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation with fine-grained relations, (ii) A unified frame-work based on meta paths to explore richer rele-vant context, (iii) Joint identification and linking of mentions under semi-supervised setting.

Two most similar methods to ours were pro-posed by (Meij et al., 2012; Guo et al., 2013) by performing joint detection and disambiguation of mentions. (Meij et al., 2012) studied several supervised machine learning models, but without considering any global evidence either from a sin-gle tweet or other relevant tweets. (Guo et al., 2013) explored second order entity-to-entity rela-tions but did not incorporate evidence from multi-ple tweets.

This work is also related to graph-based semi-supervised learning (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004; Talukdar and Crammer, 2009), which has been success-fully applied in many Natural Language Process-ing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further re-lated to meta path-based heterogeneous informa-tion network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homoge-neous information network analysis without dif-ferentiating object types and relational links. We have introduced a novel semi-supervised graph regularization framework for wikification to si-multaneously tackle the unique challenges of an-notation and information shortage in short tweets. To the best of our knowledge, this is the first work to explore the semi-supervised collective inference model to jointly perform mention detection and disambiguation. By studying three novel fine-grained relations, detecting semantically-related information with semantic meta paths, and ex-ploiting the data manifolds in both unlabeled and labeled data for collective inference, our work can dramatically save annotation cost and achieve bet-ter performance, thus shed light on the challenging wikification task for tweets.
 This work was supported by the U.S. Army Re-search Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA), U.S. NSF CAREER Award under Grant IIS-0953149, U.S.
 DARPA Award No. FA8750-13-2-0041 in the Deep Exploration and Filtering of Text (DEFT) Program, IBM Faculty Award, Google Research Award and RPI faculty start-up grant. The views and conclusions contained in this document are those of the authors and should not be inter-preted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.
