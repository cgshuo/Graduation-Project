 Since its inception in January 2001, Wikipedia has become the largest encyclopedia ever created, containing more than 3 million articles in English alone as of 2010. In the meantime, Wikipedia articles have amazingly evolved, from mostly plain texts at an earlier stage to current ones with substantial structural annotations. It is now the primary knowledge source for many users on a wide variety of entities , including people, institutions, geographical locations, events, etc. For discovering and exploring the entities that fascinate them, users are in need of structured querying facilities, coupled with text retrieval capabilities, that explicitly deal with the entities, their properties, and relationships.

The prevalent manner in which users access Wikipedia is still keyword-based doc-ument search. Although keyword search has been quite effective in finding specific pages matching the keywords, there clearly exists a mismatch between its document-centric view and the aforementioned entity-centric user information needs. Users X  tasks often cannot be clearly expressed with simple keyword queries and processing the query results may require substantial user efforts.

Example 1( Motivating Example ) . Consider a business analyst investigating the de-velopment of Silicon Valley. Particularly, she is interested in this task: Find the list of companies and their founders , where the companies are in Silicon Valley and the founders are Stanford graduates.

There are two major mismatches that make keyword search unsuitable for resolving this task. First, the task focuses on typed entities, PERSON and COMPANY, and, in database terminology, their  X  X oin X  relationships. Second, the task involves synthesiz-ing information scattered across different places, therefore a simple list of pages is not sufficient. For instance, one page may tell the analyst that JERRY YANG is a founder of YAHOO!, but whether YAHOO! is a Silicon Valley company and whether JERRY YANG is a Stanford graduate may have to be found in other pages.

While conceptually simple, with only keyword search, tasks like the given one re-quire substantial user efforts to perform multiple searches and assemble information from a potentially large number of articles. Our analyst may start with a search on  X  X ilicon Valley company X  and scan through the potentially long list of result articles to, hopefully, fetch a list of companies that are likely to be in Silicon Valley. She then sim-ilarly issues another search on  X  X tanford graduate X  to find a list of people graduated from Stanford University. She then manually combine entities in these two lists and, by multiple additional searches, check if a company was founded by a person, for each pair of person and company. Alternatively, she can also go through the list of compa-nies and, for each company, find its founders and check if Stanford is their alma mater by multiple search queries. Both are painful options and require the user to break down the task into a time-consuming, error-prone iterative procedure of searching, reading, and re-searching.

Wikipedia (and the Web) contains various tables and lists, which can be extracted into databases for powerful queries [Cafarella et al. 2008]. For example, a page listing all Silicon Valley companies may exist. However, it is unrealistic to expect such pages exist for arbitrary user tasks. Moreover, it is less common to find such tables/lists for relationships between entities, for instance, who founded which company.

We propose entity-relationship query , 1 a declarative query mechanism for the afore-mentioned task. The results of such queries are tuples of entities that are likely to meet the semantic requirements of the query, instead of articles containing such enti-ties. For Example 1, our analyst can write the following SQL-like query.
 Query 1( Entity-Relationship Query for Example 1 ).
 SELECT x, y FROM PERSON x, COMPANY y WHERE x:["Stanford", "graduate"] // Predicate p 1 AND y:["Silicon Valley"] // Predicate p 2 AND x,y:["found"] // Predicate p 3
We take a DB-IR integration approach in proposing this direction. On the one hand, entity-relationship queries have explicit structured components: typed entity variables (e.g., x , bound to entities of type PERSON, and y , for entities of type COMPANY), selection predicates for selecting entities by their properties (e.g., predicate p 1 ), and relation predicates for specifying relations between entities (e.g., predicate p 3 for  X  x founded y  X ). On the other hand, the individual predicates are specified by keyword-based constraints. The query semantics dictates that entities satisfy a predicate by a simple and intuitive requirement: the entities cooccur with the keywords in some contexts. Such contexts can be sentences, windows of texts, etc. For simplicity, we use sentence as context in this article. For example, predicate p 1 requires every PERSON in the query answer to cooccur with  X  X tanford X  and  X  X raduate X  in at least one sentence. In short, we aim to capture entity properties and relationships through shallow syntax requirements implied by users at query-time, 2 while previous works [Agichtein and Gravano 2000; Brin 1998; Cafarella et al. 2007; Chu et al. 2007; DeRose et al. 2007; Etzioni et al. 2008; Kandogan et al. 2006] explicitly extract and reason about semantic information before query-time. Although such syntax clue is by no means rigorous or error-proof, it becomes robust when we take into account the redundancy in a corpus: true facts are more likely repetitively stated in multiple places. This intuition has been widely used in Web search and mining, for instance, information extraction [Agichtein and Gravano 2000; Brin 1998] and entity search and ranking [Cheng et al. 2007].
Entity-relationship queries can yield many false answers due to abundant acciden-tal cooccurrences (e.g., false evidence such as  X  X  X  X  partner is a Stanford graduate X  for predicate p 1 ). Therefore, how to rank query answers presents a critical challenge. First, the presence of multiple predicates in a query requires us to aggregate the rank-ings of entities for multiple predicates. Second, many true answers have small num-bers of cooccurrence contexts (i.e., low redundancy) in Wikipedia. Using redundancy solely is not sufficient to tell such entities apart from false answers.

We present a ranking framework for general entity-relationship queries. It first evaluates how well an answer satisfies individual predicates and then aggregates mul-tiple predicate scores into an answer score. A Bounded Cumulative Model (BCM) is proposed for scoring predicates. BCM relies on redundant cooccurrence contexts for robust evaluation. To improve ranking accuracy for answers with small numbers of supporting contexts, BCM performs refined assessment on each cooccurrence context based on three positional features: proximity , ordering pattern ,and mutual exclusion . Contexts that are likely true evidence are given higher importance. Existing systems only exploit proximity feature [Chakrabarti et al. 2006; Cheng et al. 2007] and may not leverage redundancy [Chakrabarti et al. 2006]. In certain sensitive cases, BCM tends to yield sketically high scores. To address this issue, we further study two weighting schemes for BCM. The basic idea is to detect and penalize susceptible answers based on the number of supporting contexts.

In summary, we make the following contributions in this article.  X  We propose the concept of entity-relationship queries for structured querying of en-tities directly over Wikipedia text with multiple predicates.  X  We design a ranking framework and a position-based Bounded Cumulative Model for ranking the answers to entity-relationship queries.  X  We develop two weighting schemes, maximal-support weighting and document-frequency weighting, for improving the accuracy of BCM.  X  We conduct comprehensive experiments, and demonstrate the effectiveness of the ranking and weighting methods.
The article is organized as follows. Section 2 reviews related work. Section 3.1 de-fines entity-relationship query and its ranking problem. In Section 3.2, we discuss the three position-based features in our Cumulative Model (Section 3.3) and Bounded Cu-mulative Model (Section 3.4). Section 4 explores the weighting schemes for improving BCM X  X  performance. Section 5 reports empirical results. Section 6 discusses limita-tions and future work. Section 7 concludes the article. Previous studies on structured querying of the Web focus on DB-based approach that explicitly extracts structured information into databases [Agichtein and Gravano 2000; Brin 1998; Cafarella et al. 2007; Chu et al. 2007; DeRose et al. 2007; Etzioni et al. 2008; Kandogan et al. 2006]. This approach lends itself to the rich techniques of database querying. It is constrained by the capability of the information extraction (IE) and natural language processing (NLP) techniques. Particularly, it requires ex-plicit identification of the  X  X ames X  of entity relationships. For example, if a  X  X ound X  relation between JERRY YANG and YAHOO! was not detected during the extraction phase, such information is lost and could not be queried.

Some systems [Auer et al. 2007; Dill et al. 2003; Kasneci et al. 2008; Suchanek et al. 2007] explicitly encode entities and their relations in RDF, the W3C recommen-dation of data model for Semantic Web. They can thus leverage the expressiveness of query languages like SPARQL. 3 Some of them [Auer et al. 2007; Kasneci et al. 2008; Suchanek et al. 2007] only capture structured and semistructured information, for in-stance, infoboxes in Wikipedia, leaving out implicit information in unstructured text. For example, YAGO only supports around 100 relations unified from WordNet and Wikipedia [Suchanek 2009]. Other systems apply IE techniques over Web pages to bootstrap RDF extraction [Dill et al. 2003], thus bearing the same limitation as the aforementioned DB-based approach.

Question answering has been an important task in TREC conferences. 4 The ba-sic factoid QA task expects systems to provide a short answer (typically less than 250 characters) to a factoid question such as Who invented the paper clip? [Voorhees 2003]. The list QA task instead asks for a list of answers to a question. Evolved from the QA tasks, entity ranking tasks of both INEX 5 and TREC have gained significant interest [Demartini et al. 2008; Petkova and Croft 2007; Vercoustre et al. 2008; Zaragoza et al. 2007]. These tasks are highly relevant to our single-predicate queries. However, they do not have the concept of  X  X redicates X  and do not deal with multiple predicates. An-swering complex and multipredicate queries is what distinguishes our work from pre-vious studies. Moreover, an important focus in question answering and entity ranking is to accurately understand the narrative question descriptions, which we do not study. The studies most related to ours are [Chakrabarti et al. 2006; Cheng et al. 2007; Zhou et al. 2010]. Chakrabarti et al. [2006] learns an optimal scoring function on proximity feature. It only scores entities by single context, without integrating information found in multiple documents. EntityRank [Cheng et al. 2007] aggregates scores of locally evaluated cooccurrence contexts into global scores to improve ranking. [Zhou et al. 2010] extends EntityRank with more context matching patterns. All three systems only focus on queries comparable to our single-predicate queries and thus do not study multipredicate queries.

The concept of entity-relationship query and its query result ranking problem were initially studied in Li et al. [2010a]. In this extension, we propose two weighting schemes for improving the BCM-based ranking model (Section 4). We also experimen-tally evaluate the effectiveness of the weighting schemes in Section 5.5. In addition, we have reprocessed our Wikipedia dataset with an updated version of preprocessing module, which fixed some parsing errors in previous version. As a result, Section 5 now reports our findings on the updated data. To facilitate our discussion, Table I summarizes the major notations that we use. We formalize an entity-relationship query as q = V , P . V is a set of entity variables. Each v  X  V is bound to entities of certain type, for instance, PERSON. P is a set of predi-cates. Each p  X  P is a pair V p , C p , where V p  X  V and C p is a set of phrases 6 . Query 1 is thus q 1 = V , P , V = x :PERSON, y :COMPANY ,and P = { p 1 , p 2 , p 3 } . The predicates p = V p selection predicates, and p 3 = V p that although Query 1 only involves binary relationship between two entities, a query in general can have multientity relationships. For instance, a query SELECT p,c,l FROM PERSON p, COMPANY c, PROGRAMMING LANGUAGE l WHERE p,c,l:["design"] can be used to find programming languages designed by scientists at some companies.
An answer to a query q is a tuple of entities, denoted by t . For each v  X  V , there is a corresponding entity e  X  t instantiated from v , for instance, t = JERRY YANG, YAHOO! for Query 1. Given a predicate p = V p , C p ,weuse t p to represent the sub-tuple of t such that each entity e  X  t p is instantiated from a corresponding v  X  V p . Take p 1 in Query 1, for example. t p 1 = JERRY YANG because V p 1 has only one variable x and JERRY YANG is instantiated from x . Similarly, t p
Given a predicate p , if a sentence contains all the phrases in C p and one entity for each variable in V p , it is a (cooccurrence) context for p . These entities in whole are said to satisfy p . Suppose these three sentences are found in the corpus. s : Stanford University graduates JERRY YANG and ... s : ...a senior manager at YAHOO! in Silicon Valley. s : JERRY YANG cofounded YAHOO!.
 JERRY YANG satisfies p 1 by sentence s 1 ; YAHOO! satisfies p 2 by sentence s 2 ;and they together satisfy p 3 by s 3 . Assembling the information together, the entity tuple
JERRY YANG, YAHOO! is an answer to the query since it satisfies all the query predicates. Note that in s 1 ,  X  X tanford University X  is treated as plain text since it is neither a PERSON nor a COMPANY.

A cooccurrence context of answer t for predicate p = V p , C p is a quadruple doc , sent ,  X  V p ,  X  C p . doc and sent refer to the document ID and the sentence number that together identify a unique sentence in the corpus.  X  V p are the positions of entities in the aforementioned subtuple t p and  X  C p are the positions of phrases in C p . Sup-pose the aforementioned s 1 is the 8th sentence of document 9. In this context, JERRY YANG spans from position 3 to 4.  X  X tanford X  and  X  X raduate X  are at positions 0 and 2, respectively. Hence, the context is represented as 9 , 8 , { 3 , 4 } , { 0 , 2 } .
Note that there can be multiple contexts of t p JERRY YANG,  X  X tanford, X  and  X  X raduate. X  We denote all contexts of t p by  X  p ( t ). With-out loss of generality, we use sentence and context interchangeably. Note that in reality contexts with coarser granularity, such as paragraphs and sections, can be applied. rank the answers in A according to  X  = {  X  p | p  X  P } , where  X  p = t  X  A  X  p ( t ).
Since the information that is used for ranking,  X  , is primarily position information (i.e., documents IDs, sentence numbers, entity spans and phrase positions), the prob-lem is called the position-based ranking problem.
 Given a query q = V , P ,our ranking framework consists of three scoring functions F
S , F R and F A , such that for each answer t : (1) its score on a selection predicate p  X  P its final score F A ( t ) aggregates all predicate scores obtained via F S and F R .Inthis framework, the scores of different predicates are computed independently from each other. The intuition can be explained as follows. In Query 1, whether a PERSON is a Stanford graduate ( p 1 ) is assumed independent from whether she founded any COMPANY ( p 3 ) and irrelevant to whether a COMPANY is in Silicon Valley ( p 2 ). This section studies three position-based features that are derivable from co-occurrence contexts. These features are the key components in our ranking models. 3.2.1. Proximity. Intuitively, if the entities in t p and the keywords in C p are close to each other in a context s  X   X  p ( t ), they likely belong to the same grammatical unit of the corresponding sentence (e.g., a phrase like  X  X tanford University graduate JERRY YANG X ) and thus form a piece of true evidence. Given predicate p , we define the proximity of t p in s as the sequence. Note that prox p ( t , s ) is in [0,1] by this definition.

Different representations may be used in various places to refer to the same en-tity and may have different numbers of tokens. For example, the entity IBM may be represented by  X  X BM, X   X  X ig Blue, X  or  X  X nternational Business Machine. X  Hence, | token (IBM , s ) | may be 1, 2, or 3 in different context s .
 capitalized) entities for predicate p 1 in Query 1. Context s 1 is true, supporting a true positive, while s 4 is false, supporting a false positive. s : Stanford University graduates JERRY YANG and ... s : A professor at Stanford University, COLIN MARLOW had a relationship with Cristina Yang before she graduated ...

Predicate p 1 has two phrases,  X  X tanford X  and  X  X raduate, X  each of which has one token. Hence c  X  C tokens,  X  X erry X  and  X  X ang X , hence e  X  t and the two phrases spans 5 tokens, from  X  X tanford X  to  X  X ang X , thus | scope p Therefore, the proximity of JERRY YANG in sentence s 1 is prox p ilarly, the proximity of COLIN MARLOW in s 4 is 2+2 13 =0.31. Based on proximity alone, we say that s 1 is more likely a piece of true evidence. Therefore JERRY YANG is more likely to satisfy p 1 than COLIN MARLOW, given no other context. 3.2.2. Ordering Pattern. An ordering pattern refers to the order of entities and phrases in a cooccurrence context. Consider again predicate p 1 = { x } , {  X  X tanford X ,  X  X raduate. X  } Let c 1 = X  X tanford X  and c 2 = X  X raduate. X  This predicate has six different ordering pat-and punctuations between entities and phrases are irrelevant to the patterns. Hence,  X  X tanford University graduate, JERRY YANG X  and  X  X tanford graduate JERRY YANG X  follow the same pattern, c 1 c 2 x .

We observe that some ordering patterns are better indicators of true evidence than others. For example, to express that somebody is a graduate of Stanford University, true evidence usually follows the pattern c 1 c 2 x (e.g., s 1 ). Contexts following another pattern, c 1 xc 2 , are likely to be false evidence (e.g., s 4 ). To distinguish good patterns (those that tend to indicate true evidence) from others, we may assign a different weight to each pattern, so that entities supported by contexts following good patterns are scored higher. However, it is impossible to predetermine the weights since the goodness of ordering patterns are predicate-dependent. To illustrate, c 1 c 2 x is a good pattern for predicate p 1 in Query 1, but may not be equally good for another predicate p = { x :NOVEL } , {  X  X y X ,  X  X ane Austen X  } , because it is less common to see true evidence such as ... written by Jane Austen, PRIDE AND PREJUDICE ...

In our approach, the weights of ordering patterns for a predicate p are dynamically derived from  X  p , the set of all cooccurrence contexts for p . Denoting  X  p ( o )asthesub-set of contexts following pattern o , we define the weight of o for predicate p as its frequency in  X  p , This definition captures the intuition that good patterns appear more often than bad ones. Although in theory there might be a pattern frequently appearing in false evidence, making a bad pattern more common, we do not observe such case in our experiments. 3.2.3. Mutual Exclusion. Given a predicate p , multiple contexts in  X  p may have the same doc , sent value, that is, coming from the same sentence. They are contexts of different entities and may follow different ordering patterns in that sentence. The co-existence of different patterns in one sentence is called a collision and the patterns are colliding patterns . The mutual exclusion rule dictates that, when collision happens, at most one colliding pattern is effective and the sentence is only considered evidence following that pattern.

Example 3 . The following sentence appears as three contexts for p 1 , one for each highlighted (all capitalized) entity. Ric Weiland follows the pattern o 1 = xc 2 c 1 .PAUL ALLAN and BILL GATES follow o 2 = c 2 c 1 x . Semantically, the former pattern is the effective pattern and the sentence is only a piece of evidence for RIC WEILAND. s : After RIC WEILAND graduated from Stanford University, PAUL ALLEN and
Without understanding the semantics, it is difficult to decide which colliding pattern is absolutely effective. Therefore, we relax the rule with a credit mechanism, where ev-ery colliding pattern is considered partially effective, and patterns with higher credits are more likely to be effective than those with lower credits. We assume each sentence s (that is a context of at least one subtuple t p for predicate p ) has a total credit of 1, meaning that there is only one effective pattern. Given a predicate p , we denote the
To allocate credits to the colliding patterns O p ( s ), we adopt the intuition that pat-terns followed by more prominent entities are more likely to be effective. Specifically, proximity value, that is, T  X  p ( o , s )=arg max t sentatives (and thus the patterns that they follow) by how prominent they are, that is, by their overall numbers of contexts in  X  p . The credit of o in s is the most proximate sub-tuple as the representative of a colliding pattern and allocate credits based on representatives only. The intuition is that the most proximate sub-tuple is most likely to form a grammatical unit with phrases in C p , and hence the most reliable one for allocating credits.

In Example 3, t 1 = T  X  p is RIC WEILAND) since he is the only PERSON in s following pattern o 1 ;and t (0.44), though both follow o 2 . Suppose RIC WEILAND is found in 4 contexts ( |  X  ing patterns in s would be 4 4+2 =0.67 (for o 1 ) and 0.33 (for o 2 ).

Note that the pattern credit here is different from the pattern weight in Sec-tion 3.2.2. The weight of pattern o is a global measure (aggregated over  X  p )ofhow frequent o is. The credit of o , on the contrary, is a local measure particular to each sentence s , indicating how likely o is the effective pattern in s . So far, we have introduced all the position-based features for assessing individual con-texts. Integrating these features together, this section presents the Cumulative Model (CM) for scoring answers for a single predicate. We assume that F S isthesameas F R (i.e., the same function is used for scoring all predicates), hence for brevity, we use F ( t ) instead of F S p ( t )and F R p ( t ).
 Cumulative Model (CM) is credit p ( o , s ) is the credit of o in s .
 contexts in each group follow the same pattern. For each group  X  p ( t , o ), the model computes a group score (the inner summation). The group scores are linearly com-bined using weights f p ( o ) (the outer summation), such that the group scores of better assesses how likely s is a piece of true evidence of t for predicate p . It is monotonic to both the proximity of t p and the credit of t p  X  X  pattern o . Answers supported by contexts having higher proximities and pattern credits will accumulate higher scores and thus be ranked higher.

It is interesting to note that CM can be customized easily by switching its component features on and off, so that we can evaluate the effectiveness of individual features. While detailed evaluations are presented in Section 5, below we list three important customizations.

COUNT is a straightforward method that scores t by the count of its supporting evidence. It can be reduced from CM by turning off all the features, that is, setting and is reduced from CM by setting credit p ( o , s )  X  1and f p ( o )  X  1. MEX only applies the mutual exclusion feature. It is derived from CM by setting prox p ( t , s )  X  1and f ( o )  X  1.
 We extend our single-predicate scoring model to handle multipredicate queries. Given a query answer, CM computes a score on each predicate. However, it remains a task to derive the final score, F A ( t ), from multiple predicate scores.

With CM, predicate scores are unbounded, that is, the more contexts the higher scores. When multiple predicate scores are aggregated, some could be so high that they dominate the aggregate score. To alleviate this predicate dominance problem, we propose the Bounded Cumulative Model (BCM) for better scoring of individual predicates: BCM uses the same three features as CM does, but differs from CM in computing group scores, each of which is computed from a set of contexts  X  p ( t , o ). Basically, BCM bounds all group scores in the range [0,1], and consequently it bounds the predicate scores within [0,1], since o  X  O
Given an answer t to query q = V , P , t  X  X  final score, F A ( t ), is computed as the product of its scores on all predicates, where F p ( t ) can use either BCM or CM. For our problem, product is a more reasonable aggregate function than summation, another common aggregate function, because it favors answers with balanced predicate scores over those with polarized ones. To il-lustrate why balanced scores should be favored, consider Table II. The table shows four answers to Query 1. For each answer, it lists all three predicate scores (by BCM), as well as the final scores using product and summation, respectively. The two aggre-gates agree on the ranking of t 1 and t 4 , which get unanimously (i.e., balanced) high and low predicate scores, but disagree on t 2 and t 3 . The true positive, t 2 , gets modest and balanced scores on all the predicates. It is correctly ranked higher than t 3 ,afalse positive, by using product as the aggregate function, but loses the competition when summation is used. Answer t 3 gains high scores on p 1 and p 2 (both indeed satisfied by t ), but low score on p 3 (It does not satisfy p 3 in the real world.) When summation is the aggregate function, the final score of t 3 is dominated by the high scoring predicates and t 3 is mistakenly ranked above t 2 . We have introduced our multipredicate ranking method based on Bounded Cumulative Model. In this section, we will look further into BCM and explore several weighting schemes to improve its accuracy.

As discussed earlier, BCM is proposed to alleviate the problem of predicate domi-nance. But its multiplicative nature (the product in Formula 1) also makes predicate scoring sensitive to a few good contexts. As an extreme example, if only one ordering pattern is involved (thus, f p ( o )=1), the predicate score F p ( t ) will become 1 (the maxi-supporting contexts. In multipredicate queries, such sensitivity of predicate scores will be further magnified by Formula 2. For instance, a small change in one predicate score from 0.1 to 0.2 will double the answer score. Apparently, ranking judgment made in such sensitive situations is susceptible and should be downplayed.

Our idea for solving the sensitivity problem is to penalize susceptible answers by weighting. The new weighted scoring model for general entity-relationship queries is Formula 3. Compared to Formula 2, an exponential weight W p ( t ) is applied over each F ( t ), the BCM score of answer t for predicate p . In the ideal case, the weight should is computed according to a small number of contexts, it is susceptible (even though the score may be high) and should receive a penalty for that. Briefly speaking, the weight W p ( t ) needs to be larger for more susceptible answers (i.e., answers supported by less contexts). Note that F p ( t ) is bounded between [0 , 1] by BCM, hence larger weight will lower the predicate score.
 The rest of this section discusses two weighting schemes that match our intention: higher weights W p ( t ) for answers with less contexts. With regard to a predicate, let us define the support of a candidate answer t to be the number of supporting contexts support with some  X  X est-possible X  support. In the first weighting scheme, maximal-support weighting, we compare the support of t p with the largest support among all the answers. The larger the difference is, the higher the weight will be. In the second scheme, corpus-frequency weighting, the support of t p is compared with its  X  X orpus frequency, X  which refers to the number of contexts in the corpus that contain t p . The maximal-support weight is defined as The denominator is the logarithm of answer t  X  X  support (plus 1) for predicate p and the numerator is the maximum of such logarithm values among all the answers. The smoothing constant 1 is used to avoid zero-denominator when |  X  p ( t ) | =1. Bythis weighting scheme, answers with less support (i.e., smaller denominators) will receive higher penalties. Suppose there are two answers for predicate p , where t 1 has 3 con-texts (log(3 + 1) = 2) and t 2 has much more contexts, say 127 (log(127 + 1) = 7). Their Thus, t 1 is penalized more with weight 3.5. Note that, although we use base 2 loga-rithm in the calculation for illustration purpose, the weight is actually independent of the choice of base.

For an individual predicate, if two answers have the same support (hence, the same denominator), their maximal-support weights will be the same, and their rel-ative ranking is preserved in spite of the weighting, according to the monotonicity that x &lt; y  X  x w &lt; y w for w&gt; 0.

The numerator of  X  p ( t ) corresponds to the maximum of observed support for a pred-icate. It is a hint on how much support a true answer could get from the corpus. Here, we implicitly assume that the maximally supported answer is true, which is usually the case. We utilize this hint to evaluate the significance of the supports of candi-date answers and weight them accordingly. To illustrate, let us consider an answer t with 3 contexts. We observed two situations in our corpus: (1) the maximal support is much higher than 3 (e.g., 127), and (2) the maximal support is also low (e.g., 7). The former indicates that true answers tend to have many contexts, while the latter indi-cates that it is unlikely to find many contexts for this predicate. Intuitively, answer t should be penalized more in the former situation since it does not look like a true answer in terms of support. In this regard, the definition of  X  p ( t ) tends to penalize poorly-supported answers more severely.

It is important to note that the maximal support (i.e., the numerator), has no effect on ranking single-predicate queries because it is the same for all answers. For multi-predicate queries, each predicate may have a different maximal support. Predicates with higher maximal support tend to yield higher weights, and penalize the answer more severely given the same support value, and thus impact the ranking of candidate answers. In sum, maximal-support weighting applies different treatment of predicates according to their maximal supports, relying on predicates with high maximal support to penalize their poorly-supported answers.

The maximal-support weighting has its own concern, as the maximal support itself is sensitive to outliers. In an undesirable scenario, it is possible that most true answers of a predicate have less than 10 contexts, except for one outlier with 100 contexts. In this case, the maximum support is 100, resulting in high penalty to other true answers. A more robust solution could use the average of the largest k supports as the numerator in  X  p ( t ). We leave this issue for future study. The corpus-frequency weighting takes into consideration the corpus background of an answer for evaluating the significance of its support. Given the same support, the more an answer appears in the corpus, the less significant the support is. This intuition is reflected in our corpus-frequency weighting as because  X  p ( t ) is the set of supporting contexts of t with regard to p ,thatis,those sentences that not only contain t p but also C p , the phrases in p .

For example, for predicate p 2 = { y } ,  X  X ilicon Valley X  on COMPANY, NASDAQ is sup-ported by 3 contexts out of its 845 occurrences in the corpus while MAYFIELD FUND has 3 out of 11. Given its high corpus frequency, NASDAQ is quite susceptible be-cause its cooccurrence with  X  X ilicon Valley X  likely happens just by coincidence. In this sense, we consider the support of MAYFIELD FUND more significant than that of NASDAQ. By applying corpus-frequency weighting, the weight for NASDAQ is log(846) log(4) , while the weight for MAYFIELD FUND is log(12) log(4) . This meets our goal of giving higher weights (penalties), thus lower scores, to more susceptible answers. In contrast, given this example, maximal-support weighting will weight the two answers equally since they have the same support. Hence by looking at the corpus frequency,  X  p ( t ) is able to capture the difference in case of tied support.

Corpus-frequency weighting shares the same intuition as IDF (inverse document frequency) weighting in document retrieval. Both weighting schemes favor specificity over generality. The difference is that IDF weight depends on query terms only and is the same for all hit answers, while our corpus-frequency weight is dependent on both query predicates and the returned answers.

A concern is how to weight between two answers of the same specificity, for in-t and t 2 have equal specificity if we measure specificity by the ratio between support problem and corpus, the latter case should be favored. The intuition is that, since t 2 already has large support, its BCM score is not as susceptible as t 1 and hence the cor-pus frequency becomes less important in determining its significance. Our definition of corpus-frequency weight is capable of capturing this intuition by using logarithm in both numerator and denominator. As a real example from the predicate on Silicon Val-ley companies, APPLE (a true answer) has 19 contexts out of its 2639 occurrences with specificity 19 2639 = 0.72%, while SAP (a false answer) has 2 out of 211 with specificity to SAP X  X  4.9. This section provides empirical evaluation of our prototype system implemented in Apache Lucene. 7 A demo of the system is maintained at http://idir.uta.edu/erq [Li et al. 2010b]. The system consists of several components. An Indexer constructs an entity-centric index over our corpus. It associates each term w with a list of entities (ordered by entity IDs) that cooccur with w somewhere in the corpus. For each entity e in the list, it further records where w and e cooccur. Leveraging this design, the Retriever efficiently retrieves entities and cooccurrence contexts. The results are fed to Ranker for ranking, which is based on the ideas in previous sections. We used the 2008-07-24 snapshot of Wikipedia. 8 After we removed all the irrelevant pages (such as category and administrative pages), there were about 1.8 million arti-cles. This article set is used as the entity catalog. Each article is the description of an entity, by Wikipedia X  X  nature of being an encyclopedia, and the article title corresponds to the entity name. We predefined 10 entity types (Table III) and assigned about 0.63 million entities to these types based on simple handcrafted rules, mainly using their categories in Wikipedia. For example, if an article belongs to a category whose name ends with  X  X ovels X  (e.g., British novels) it is treated as an entity of type NOVEL. This simple method turns out sufficiently accurate for our experiments. These 10 types represent some of the major types of entities available in Wikipedia, for instance, PERSON. They also include the main types (people, places, and organizations) used in the study of named entity recognition (NER). Although a more comprehensive, fine-grained, and accurate categorization of entities can improve the system X  X  query capa-bility, it is not the focus of this study.

The same article set is used as the query corpus. For each article, we removed its section titles, tables, infoboxes, references, etc., retaining only the main content. The main text is segmented into sentences. We removed punctuation marks and stemmed all words using the Porter stemmer. 9 We consider the hyperlinks between Wikipedia articles as occurrences of the link targets (entities). In this way, we collected nearly 12 million occurrences of the 0.63 million typed entities.

Named entity recognition (NER) [Nadeau et al. 2007] and entity disambigua-tion [Dill et al. 2003] are intensively studied problems. Our hyperlink-based anno-tation can be viewed as a rudimentary entity disambiguation method. Recently we have seen advanced entity recognition and disambiguation methods using Wikipedia as entity catalog [Kulkarni et al. 2009; Mihalcea and Csomai 2007; Milne and Witten 2008] to automatically link entities mentioned in plain text to their corresponding Wikipedia articles. One of our ongoing efforts is to use Wikify [Milne and Witten 2008] to annotate entities occurrences that are not hyperlink anchortexts. This method will give us more comprehensive entity occurrences. Furthermore, it can be applied on generic Web pages, enabling entity-relationship queries on Web corpus.

We have previously collected two query sets, INEX17 and OWN28. INEX17 is adapted from the topics in the Entity Ranking track of INEX 2009. From the 60 available topics, we adapted the ones that are on our predefined 10 entity types. We obtained 11 single-predicate queries and 6 multipredicate queries. OWN28 contains our own crafted 28 queries, including 16 single-predicate queries and 12 multipredi-cate queries. Since both query sets are relatively small, we merged them into one set for more robust evaluation. It contains 27 single-predicates (Single-27) and 18 multi-predicate queries (Multi-18).
 collect the ground truth. However, some of the queries return hundreds of answers. Exhaustive checking is prohibitively time consuming. Therefore, for such queries, we adopted the depth-N pooling approach used by INEX. Basically for each query, we only check the top N answers returned by each ranking method. Since different methods overlap greatly in their top N answers, a lot of time is saved. This pooling approach allows us to evaluate ranking accuracy up to rank N . As a typical choice, N is set to 100 in our experiments. In this section we use several sample queries to demonstrate the accuracy of our query evaluation system, based on BCM scoring model. The top-10 answers to each query are displayed. The true answers are marked by bullets.
 Case 1. Find the list of big ten universities.
 Case 2. Our Query 1 X  Silicon Valley companies founded by Stanford graduates. Case 3. Find Academy Award winning films starring Australian actors.
 Case 4. A basketball fan looking for team leaders of NBA champions. Particularly, she In this section, we compare and analyze the multiple ranking methods discussed in Section 3, namely COUNT, MEX, PROX, CM, and BCM. All the methods differ in how they compute predicate scores, that is, F p ( t ). For multipredicate queries, the same Formula (2) is used to aggregate predicate scores into answer score. We compare these ranking methods using three popular measures: MAP , nDCG ,and Precision-at-k (P@ k for short). Since it is extremely time-consuming to obtain the complete ground truth in Wikipedia (which exactly motivates this study), we do not evaluate the absolute recall or F-measure.

The MAP (Mean Average Precision) is the arithmetic mean of average precisions for a set of queries. It is a good indicator of both precision and recall. The top part of Table IV shows MAP for Single-27 and Multi-18. Both MEX and PROX improve over COUNT, by 0.032 and 0.075 on Single-27, respectively, and by 0.03 and 0.109 on Multi-18. PROX is shown to be more effective than MEX. CM and BCM have simi-lar performance on Single-27, but BCM excels over CM by 0.055 on Multi-18. This demonstrates that BCM is a better model for ranking multipredicate queries.
The nDCG (Normalized Discounted Cumulative Gain) discounts the gain of a true answer by its rank in the result list and normalizes the cumulated gain with that of a perfect ranking. The bottom part of Table IV shows the average nDCG on Single-27 and Multi-18. The observation is similar to the case of MAP.

Overall, our conclusion is that, CM and BCM are comparable for ranking single-predicates and BCM has clear advantage on multipredicate queries. We refer inter-ested readers to Li et al. [2010a] for separate evaluation of INEX17 and OWN28 query sets in earlier experiments.

To further analyze how various methods perform at different ranks, we plot precision-at-k curves. Figure 1(a) shows the results for Single-27 with k up to 10 (de-noted as P@10). COUNT has the worst performance. PROX is consistently better than MEX across all ranks, but worse than CM and BCM, agreeing with the conclusion drawn from MAP and nDCG analysis. BCM is the best among all, especially for top 5. We further collected a subset of queries in Single-27, which returned more than 100 answers, and plot the P@100 curve in Figure 1(b). While COUNT and MEX are still the worst, the other methods are fairly close to each other. Our observation on Single-27 indicates that, for single-predicate queries, BCM is good at ranking top an-swers, but does not have advantage over the long range. Figure 1(c) and (d) reports the same experiment on Multi-18. It can be easily observed that BCM has clear advan-tage in ranking multipredicate queries for top answers and beyond. This reinforces our conclusion drawn from Table IV.

Note that, if all true answers are ranked before position K , the precision after K will not change. This is reflected by the flat tails in Figure 1(b) and (d).
In summary, the individual features are effective for ranking entity-relationship queries and they work best in concert when integrated in BCM. BCM rivals CM on single-predicate queries, and excels on multipredicate queries. This is because BCM alleviates the predicate dominance problem as discussed in Section 3.4. As a first study on multipredicate entity-relationship query, we did not find directly comparable systems. By our best effort, we have chosen three state-of-the-art systems proposed for related problems: EntityRank, INEX and INRIA. All the three systems work on the entity ranking problem, though under different task settings. Besides, they are all evaluated using Wikipedia as corpus and entity catalog, though INEX and INRIA worked on different snapshots from ours.

EntityRank (ER) [Cheng et al. 2007] outperforms another closely related sys-tem [Chakrabarti et al. 2006] by a large margin in terms of MRR (Mean Reciprocal Rank), since Chakrabarti et al. [2006] do not focus on aggregating supporting evidence from multiple articles. We reimplemented ER in our system for scoring individual predicates. As ER focuses on single-predicate queries, performance on such queries can be fairly compared. For multipredicate queries, we can also use ER to compute predicate scores and aggregate them by the same Formula 2.

We tested ER with our data set. In Table IV, ER is worse than BCM on Single-27, by 0.082 in MAP and 0.039 in nDCG. It is only marginally better than the base-line COUNT. For multipredicate queries (Multi-18), ER shows relatively better perfor-mance than PROX, however, it is still worse than BCM by a large margin (0.06 in MAP and 0.043 in nDCG).

From Figure 1, it can be seen that ER is good at ranking top 2 answers (with the exception of (a)), rivaling CM and BCM. However, it deteriorates very fast when k &gt; 2. In (c), ER drops below 0.7 around k = 6, while BCM remains above 0.7 at k = 10. It indicates that BCM is more robust for queries with multiple true answers. This is because BCM exploits more features than ER and is thus able to promote the ranking of some hard true answers indistinguishable by ER.

The INEX Entity Ranking track targets on a different problem setting. INEX queries are specified as narrative descriptions on the desired entities. Participating systems can use any techniques to answer the queries, but need to understand the query descriptions, which itself is challenging. Hence their MAPs tend to be low. The MAP achieved by the best system participating in the 2009 track is 0.517. To avoid the overhead of assessing participating systems, INEX used a sampling strategy to estimate their MAPs.

INRIA [Vercoustre et al. 2008] works on the same problem as INEX. Unlike INEX participants, it is not based on cooccurrence of entities and query inputs. Rather, it finds the Wikipedia articles that best match query descriptions, through link analysis and TF-IDF weighting. It achieves MAP of 0.390 on 18 topics adapted from INEX 2006 ad hoc track.

In comparison with INEX and INRIA, the MAP achieved by BCM is 0.852 (averaged over all 45 queries). We acknowledge that this comparison is not strictly fair. First, the results are based on different query sets and snapshots of Wikipedia. Second, they focus on different query styles (structured query vs. narrative description). However, our argument is that the high MAP of BCM at least indicates that the structured entity-relationship queries can be highly effective in reality. In this section we evaluate different weighting schemes introduced in Section 4. We compare plain BCM with BCM- X  (applying maximal-support weight only) and BCM- X  (applying corpus-frequency weight only). We also consider the case of combining the two weights as  X  p ( t )=  X  p ( t )+  X  p ( t ). Table V compares the results using both MAP and nDCG.
 The benefits of the two weighting schemes are not clearly observed on Single-27. One reason is that the accuracy of BCM on single-predicate queries is already quite high, therefore any significant improvement would be challenging. Recall that both weighting schemes are designed to penalize susceptible answers instead of promoting promising ones. Their effects tend to diminish when there are few susceptible answers ranked at top positions, which is the case of Single-27. Nevertheless, the combined weighting, BCM- X  , does show marginal improvement over BCM.

The result on Multi-18 is different. Both BCM- X  and BCM- X  achieve better MAP than BCM, by 0.008 and 0.022, respectively. The corpus-frequency weighting (BCM- X  ) appears to be more effective than maximal-support weighting (BCM- X  ). BCM- X  benefits from both weighting components, achieving the highest MAP (0.788) and nDCG (0.871). There are two main reasons for the improvement. First, the sensitivity of Formula 2 bring more susceptible answers into top ranks in case of multipredicate queries. Second, as discussed in Section 4.1, the maximal support (the numerator) in BCM- X  becomes effective for multipredicate queries. The distinguishing characteristic of our approach is to combine IR-style keyword con-straints with SQL-style structured query constructs. As the initial exploratory study along this line, our work has mainly focused on designing an accurate ranking frame-work and building a prototype system, to demonstrates the effectiveness and promises of the approach. We have thus only supported basic keyword constraints in the queries. To deploy a production query system with good usability in practice, we need to support more advanced query features. Here we briefly enumerate several such features.
Our current definition of entity-relationship query does not support querying en-tities by name (e.g., find entities whose names contain  X  X ichael X ) or querying re-lationships between entities (e.g., find the relationship between BILL GATES and MICROSOFT). It does not distinguish the subject and the object in a binary relation-ship between two entities of the same type. It does not allow disjunctive conditions such as x:["Stanford", "graduate"] OR x:["Berkeley", "graduate"] .

Although the predicates in entity-relationship query are keyword-based, it can be a burden for users to choose the right keywords. We are exploring two directions to address this concern. First, query suggestion can directly help users find the proper keywords. For example, after the user types  X  X tanford X  for p 1 of Query 1, the system could suggest a list of keywords that commonly cooccur with  X  X tanford X  in the corpus or in the query log, such as  X  X raduate, X   X  X rofessor, X  etc. Second, the strict keyword matching can be relaxed by query expansion, for instance, allowing a context for p 1 to contain  X  X lumni, X  if not  X  X raduate. X  Synonym thesaurus and paraphrases mined from the corpus may be used for this purpose. New challenges on ranking shall arise with query expansion.

In addition to position-based features, the assessment of cooccurrence contexts can be improved with more features such as syntactic information (e.g., part-of-speech tags) and lexicographic information. The BCM model may be extended with new fac-tors for these features. Entity-relationship query is a structured facility to query entities over Wikipedia. It distinguishes itself by (1) allowing multiple keyword-based predicates in a query and (2) searching directly in corpus instead of preextracted data stores. We presented a ranking framework for entity-relationship queries and a Bounded Cumulative Model under this framework. Our ranking method exploits three intuitive positional features, which are shown to be effective in the experiments. To address the sensitivity prob-lem in our ranking model, we further studied how to detect and weight susceptible answers. We explored two weighting schemes, both leveraging the intuition that pred-icate scores supported with less contexts are more susceptible and should be penalized accordingly.

