 sensor networks and web applications with the development of storage technology and networking architecture [1]. In these dynamic environments, data streams are massive, temporally ordered, fas t changing and potentially infinite [2] . 
In classification domain for data streams, usually unlabeled data is massive while labeled data is limited. Mostly it will be costly and time consuming to obtain labels of provide sufficient labeled instances, restricting the generalization capability and pr e-dicting accuracy. Therefore, active learning focuses on how to label instances selec-tively to maximize the prediction accuracy as possible with limited label budget. Based on some criteria, active learning method s elects the most representative and informative in stances interactively [3]. Usually the representativeness of an incoming instance can be measured by its uncertainty accor ding to the classifier model trained from the labeled i nstances [4]. Therefore, how to design appropriate metrics to assess the representativeness of an inco ming instance becomes important concern [5]. 
In the scenario of data streams , classifiers need t o make the decision whether to r e-quest the class label for every incoming instance immediately with no re -access as the streams, concept drift, which refers to changing rel ations between the input attributes and the target labels , tend to often emerge over time . Moreover the decision threshold or a region of uncertainty cannot be kept fixed as concept drift negatively impacts the accuracy of the model that learned from the p ast training in stances. 
Usually concept drifts can be divided into sudden, gradual, or recurring drifts. Ty p-ical approaches that deal with concept drift mainly include : sliding window method s, new online means , special detection techniques, and adaptive ensembles [7]. Adaptive ensembles generate base classifiers sequentially from fixed size blocks of training examples called data chunks [8]. However, once the chunk size is too big the ense m-bles may react too slowly for sudden drifts as old classifiers still have remain weights. this may also damage the stability and computational costs performance of the en-semble. Simple incremental learning [9-14] keeps se nsitivity to sudden change s to ting history data and s harp adapt ation to newest status . In order to adapt to both su d-block -based ense mbles and incremental learning approaches.

In this paper, a new online paired ensemble active learni ng framework consisting of a stable classifier and a timely substituted dynamic classifier is proposed to react to different types of concept drift better. Classifiers are built using block based method. Once built, the two classifiers predict and learn fr om incoming instances in an online incremental way. The stable classifier will make suitable reactions to gradual chan g-es; meanwhile the dynamic classifier keeps sensitivity to sudden changes.

The rest of the paper is organized as follows. Section II brief ly reviews the related work. Section III presents the online paired ensemble active learning framework in detail. Experimental results and analysis are discussed in Section IV, and conclusion is given in Se ction V. tiv ely label instances and build a classifier L from them to predict class labels for the future i nstances.

 X liobait X  et al. [15] presents a generic framework for active learni ng incremental ly from drifting data streams. Several active learning strat egies are incorporated into the framework . The strategies are equipped with mechanisms to control and di stribute the The three new proposed effective active lear ning strat egies include Va riable U ncer-diffe rent situations. Usually RanVarU n and Split strat egies work well as they co mbine the u ncertainty strategy and random strat egy so that they will label the instances that some distant instances. However with si ngle classif ier to learn incr ementally, the combin ation strategy may still miss concept drifts occur in short period or even suffer [16] adopts paired lear ning framework to cope with co ncept drift, and incorporates potential changes in data streams. The stable classif ier predicts based on all available labeled instances, while the reactive one pr edicts based on a window of recent i n-stances from random strategy. S o the reactive classif ier is trained with instances un i-stances mainly around the decision boundary. Whe never the reactive classifier has a instances far away from the current window as the reactive classifier pay more atte n-tion to instances labeled in the window. Therefore it weakens the abi lity of the stable replacement al ways happens after drift having been detected. So the framework may be over fit to sudden or local drifts and cannot deal gradual and recu rring drifts well. To make the classifier be sensitive to both sudden and gradual drifts, this paper co n-sider combin ing significant features from paired ensembles and incr emental learning approaches. In this section, the new online paired ensemble active learning framework is described in detail. The main procedure is shown in Fig. 1. A circular array structure buffer win dow with the block size is used to cache incoming stances. When the window gets selected to learn according to the uncertainty strategy and random strategy as instan c-es continue to be read from the stream . The new coming instance will replace the will be built once the window is fulfilled by new instances again. Along with the mentally learn from new instances online . 
When processing instance s in the cache window, uncertainty strategy and random strategy are used in order. If an instance x is chosen to be labeled for its high unce r-tainty, the true class label of the instance x will be requested . If uncertainty strategy Algorithm 1 Onlin e Paired Ensemble Active Learning Framework
Input : S : incoming data streams with unknown label,
Objective : build C d and update C s from S to form a classify instances as accurate as 
Algorithm 2 CreateNewDynamicClassifier() this x should be labeled . The uncertainty strategy is presented in Algorithm 4. It takes the instance x , the ensem ble classifier E , the uncertainty threshold  X  m and the adjus t-ment step for threshold s as input. The algorithm calculates the margin for i nstance x using ensemble classifier E built by the stable classifier C s and the dynamic classifier C Otherwise, instance x should not be labeled according to the uncertainty strategy ( la-siders both the maximum a posteriori probability and the second most probable class label. The margin for instance x is defined as in (1) where  X  Algorithm 3 DealInstance( x )
I nitializ ation : i=0 , i is the index for the instances to be processed and cached 1 instance I x = C [ i ] //new instance comes, get cached instance to from chunk C 2 labeling =uncertaintyStrategy(I x ) //using uncertainty strategy 3 If labeling=true , then 4 label I , then update C s and C d with labeled I x 5 Else 6 labeling = random Strategy( X ) //using random strategy 7 If labeling=true, then 8 label I x , then update C s and C d with labeled I x 9 End If 10 End If 11 C [ i ]= x //the new instance will be cached in position i to overwrite  X  12 i=(i+1)%W 13 If i==0 then // new instances fulfill the chunk again 14 CreateNewDynamicClassifier() 1 5 E nd If
Algorithm 4 uncertaintyStrategy( x ) input : x : inco ming instance, output : boolean variable labeling indicates whether to request the true label of I x . 1 margin( x )=P E ( X  c1 |x ) -P E ( X  c2 | x ) ; 3  X  m =  X  m * (1 -s /numberOfClasses); 4 return labeling= true ; 5 Else // most probability margin is above the thresho ld 6 return labeling= false ; 7 E nd I f and the second most posteriori probability [1 7] [1 8], L is the ensemble classifier used. The margin -based metric is prone to select instances with minimum margin between posterior i probabilities of the two most likely class labels. The random strategy is the same with that used in [16] because of its concision . In this section, the proposed online paired ensemble active learning framework ( On-PEAL ) is empiric ally evaluate d on real streaming classification problems. OnPEAL is compared with the paired ensemble framework for active learning ( PEFAL ) presented in [16] and three representative active learning strategies described in [ 15], including Variable Uncertai nty Strategy ( VarUn ), Uncertainty Strategy with Randomizatio n MOA data stream software suite [ 19]. MOA is an open source software environment for stream data mining, includ ing evaluation measures and a collection of implemen t-ed algorithms. For comparison , the PEFAL algorithm is implemented using MOA those strategies has already been integrated into MOA by authors of [15]. All alg o-duce the maximum a posteriori probability. 4.1 Datasets In the experiments , four real world public datasets referred in [12] are used: Airlines each dataset are listed in T able 1 . Detailed information for the datasets can be referred to [12]. 4.2 Accuracy Evaluation In this subsection , the accuracy on different datasets with labeling budget varying from 0. 1 to 0.5 is evaluated . Fig. 5 plots the accuracy of different methods as a fun c-improvement of the OnPEAL is clear when labeled percentage arises. For Cover Type and Electricity, the accuracy is relatively stable with small improvement trend with the labeled percentage increasing. As for the NSL -KDD dataset, the accuracy of O n-PEAL is stable and the difference between OnPEAL and Split is slight when the l a-beled percentage is higher than 0.2. OnPEAL gets high accuracy on all the datasets when the labeled percentage is low and the accuracy keeps stabl e with improvement trend when the labeled percentage arises. In OnPEAL framework, the stable classifier can follow the long time trend of the instance space and the dynamic classifier keeps sensibility to sudden changes. Therefore the online paired ensembl e framework can make adaptive adjustment to drifts in data streams. 4.3 Sensitivity to Parameters The impact of the window size W over the accuracy and labeled percentage of On-PEA L is experimentally evaluated in this subsection . The initial random selection percentage when building new classifier using block based method is set at 0.3 to control the active labeled percentage in certain region . As shown in Table 2, both the accuracy and labeled percentage are relatively stable for Cover Type and NSL -KDD datasets. Airlines gets stable accuracy and decreasing labeled percentage with window size increasing. However, both the accuracy and labeled percentage reduce when window size improves for Electricity . This may be caused by the s mall amount of Electricity data and its continual drift changes. Cost limitation of labeled instances and the potential concept drift s have posed signi f-icant challenges on stream data classification in practice. Therefore a new online paired ensemble framework for active learning with drifted data streams using comb i-nation labeling strategies is proposed. The ensemble classifier consists of a long stable porated into the framework in order to find out the most informative instances without missing the potential changes happened anywhere in the instance space. Experimental results on real world datasets demonstrated that the novel approach gets good predi c-tion accuracy . For future work we would like to investigate the ensemble framework and active learning strategies in more detail.
