 In real-time display advertising, ad slots are sold per impres-sion via an auction mechanism. For an advertiser, the cam-paign information is incomplete  X  the user responses (e.g, clicks or conversions) and the market price of each ad im-pression are observed only if the advertiser X  X  bid had won the corresponding ad auction. The predictions, such as bid land-scape forecasting, click-through rate (CTR) estimation, and bid optimisation, are all operated in the pre-bid stage with full-volume bid request data. However, the training data is gathered in the post-bid stage with a strong bias towards the winning impressions. A common solution for learning over such censored data is to reweight data instances to correct the discrepancy between training and prediction. However, little study has been done on how to obtain the weights in-dependent of previous bidding strategies and consequently integrate them into the final CTR prediction and bid genera-tion steps. In this paper, we formulate CTR estimation and bid optimisation under such censored auction data. Derived from a survival model, we show that historic bid information is naturally incorporated to produce Bid-aware Gradient De-scents (BGD) which controls both the importance and the direction of the gradient to achieve unbiased learning. The empirical study based on two large-scale real-world datasets demonstrates remarkable performance gains from our solu-tion. The learning framework has been deployed on Yahoo! X  X  real-time bidding platform and provided 2.97% AUC lift for CTR estimation and 9.30% eCPC drop for bid optimisation in an online A/B test.
 Unbiased Learning, Censored Data, Real-Time Bidding, Dis-play Advertising
The rise of real-time bidding (RTB) based display adver-tising and behavioural targeting provides one of the most significant cases for machine learning applied to big data.  X  The major supervised learning tasks range from predicting the market price distribution and volume of a given ad im-pression type [ 4], estimating the click-through rate (CTR) [23] and conversion rate [17 ], to the optimisation of a bid [27 , 38 ]. These data driven prediction and optimisation tech-niques enable ads to be more relevant and targeted to the underlying audience [ 38 ].

A challenging yet largely neglected problem in the afore-mentioned learning tasks is that common supervised learn-ing requires the training and prediction data to follow the same distribution, but in the online display advertising case, the training data is heavily censored by the ad auction se-lection process [ 30 ]. For advertisers, specifically, the above prediction algorithms, e.g., CTR estimation and bid optimi-sation, are operated over the full volume bid request stream in order to evaluate each potential impression and automat-ically generate bid [ 39 ]. However, the auction selects the ad with the highest bid and displays it to the user. Only in this situation the corresponding user feedback, i.e., click and conversion, to this ad impression, along with the second price (or market price [1 ]) for this auction, are received by the advertisers as the labels of this data instance. Thus, as illustrated in Figure 1, the obtaining of a training instance is heavily influenced by its bid value; data instances with high-er bid price (than the expected market price) would generate a higher probability of winning and thus higher chance to be in the training data. A consequence is that the learning will be overly focused on the instances with a high winning probability (high bid), while neglecting the cases where the probability is small. Such a bias is problematic as intuitively conversions or clicks from those low market-valued impres-sions are more crucial than those from high market-valued impressions in order to obtain a more economic solution. Ultimately advertisers not only need to identify the impres-sions that have high chance to be clicks/converted, but also (and equally importantly) require the cost of winning those impressions is relatively small. Thus, we need to have an unbiased learning framework that can take the final optimi-sation objective into account.

Typically, the bias problem is a missing data problem, which has been well-studied in the machine learning litera-ture [8 ]. A direct solution would be to identify or assume the missing process and correct the discrepancy (e.g, [ 25, 22 ]) during the training. However, the data missing in RTB display advertising depends on both the advertiser X  X  previ-ous bidding strategy and the market competition, neither of them are known as a priori. There are some indirect solu-tions of alleviating the data bias such as by adding random ad selection probability in the bidding strategy [9 ], but a better solution would be to decouple the solution with the Figure 1: From an advertiser X  X  perspective, the ad auction selection acts as a dynamic data filter based on bid value, which leads to distribution discrepancy between the post-bid training data (red) and the pre-bid prediction data (blue). The y-axis p (data) means the data p.d.f. while the x-axis is a 1-dimension abstraction of the data feature space. previously employed bidding strategy (when acquiring the training data) and build a link to the final optimisation pro-cess.

In this paper, we consider both CTR estimation [ 23, 17 ] and bid optimisation [27 , 38 ] together and propose a flexible learning framework that eliminates such auction-generated data bias towards a better learning and optimisation per-formance. According to the RTB auction mechanism, the labelled training data instance is observed only when the bid is higher than the market price. Inspired by the cen-sored learning work in [ 1], we explicitly model the auction winning probability with a bid landscape based on a non-parametric survival model, i.e., [ 14], which is then estimat-ed from the advertiser X  X  historic bid. By importance sam-pling with the auction winning probability as propensity s-core [ 12], we naturally incorporate it into gradient derivation to produce a Bid-aware Gradient Descent (BGD) training scheme for both CTR prediction and bid optimisation tasks. Intuitively, our BGD shows that (i) the higher bid price the impression was won with, the lower valued gradient such data should generate; (ii) to generate a bid, historic bids will further adjust the gradient direction and provide a low-er average budget for lower-bidden training instance when learning the bidding function. It is worth noticing that the proposed learning framework is generally applicable to var-ious supervised learning and optimisation tasks mentioned above.

Besides the theoretical derivations, we also conduct em-pirical studies with the tasks of CTR estimation and bid optimisation on two large-scale real-world datasets. The re-sults demonstrate large improvements brought from our so-lution over the start-of-the-art models. Moreover, the learn-ing framework was also deployed on Yahoo! DSP in Sep. 2015 and brought 2.97% AUC lift for CTR estimation and 9.30% eCPC drop for bid optimisation over 9 campaigns in an online A/B test.

The rest of this paper is organised as follows. In Section 2 we discuss related work and compare it with ours. We then formulate the problem and propose our solutions for unbi-ased CTR estimation and bid optimisation under censored auction data in Section 3. Extensive offline empirical study based on two real-world datasets and online A/B test are provided in Section 4. Finally we conclude this paper and discuss future work in Section 5. User Response Prediction. Click-through rate (CTR) estimation and conversion rate (CVR) estimation are criti-cal in data driven targeted advertising as these techniques provide a quantification of the user X  X  interest on a specific displayed ad, which in turn help advertisers better allocate budget across audiences [28 , 33 ]. Essentially, CTR/CVR es-timation is a probability regression problem where the pos-itive instances are extremely sparse [11 ]. Various machine learning models with probability-related loss, such as cross entropy and log-likelihood, are used for user response esti-mation, including linear models such as logistic regression [17], Bayesian probit regression [9 ], FTRL regression [ 23 ], and non-linear ones such as factorisation machines [24 ] and gradient boosting tree models [ 11 ]. Nevertheless, to our best knowledge, none of the existing work considered the bias coming from the ad auction selection for the user response prediction purpose.
 RTB Optimisation. Based on user response prediction, advertisers can estimate the value of a specific ad impression, which is the value of a response (click or conversion) mul-tiplied by the predicted response rate (CTR or CVR) [ 17 ]. According to auction theory [ 7], the truth-telling bidding is the optimal strategy in second price auctions. However, when considering repeated auctions with volume and budget constraints, the optimal bidding strategy is not necessarily truth-telling [ 38, 37 ]. In RTB display advertising, with user response prediction and bid landscape forecasting [ 4], the bidding strategy determines how much to bid on a certain ad inventory. The authors in [27 ] proposed a linear bidding function w.r.t. the predicted CTR and the scaling param-eter is tuned based on the market competition. In [ 3] the authors proposed to set the bid price as the truth-telling bid minus a value, which is dynamically tuned according to the current performance. In [ 38], the authors proposed a func-tional optimisation framework to induce the optimal bidding functions that maximises the target key performance indi-cator (KPI). Recently, a lift-based bidding strategy was pro-posed [ 32 ], where the bid price was set proportional to the user X  X  CVR lift after seeing the ad impression. The authors claimed that such lift-based bidding strategy could substan-tially bring more customers to the advertisers. Again, none of the investigated work discussed the data bias problem which causes the data distribution discrepancy between the training and prediction stages.
 Unbiased Offline Evaluation. As pointed out in [18 ], di-rect online evaluation and optimisation for a new solution are expensive and risky, which is also a dilemma in online advertising [ 2]. However, it is cheap and risk-free if the model can be optimised and evaluated using offline historic data that was previously collected using another (usually un-known) model. The authors in [ 19] proposed to use historic data for unbiased offline evaluation of news article recom-mendation models by replay and rejection sampling. Prereq-uisites of this approach are that the previous model gener-ating the training data (called exploration model) is known, and that the evaluated policy has sufficiently explored all possible actions [15 ]. For cases where historic data is col-lected using a biased or non-stationary policy, the authors in [6 ] suggested an adaptive rejection sampling approach. The authors in [ 36 ] further built a reinforcement learning framework which directly optimised the lower bound of in-verse propensity score based policy value to reduce the train-ing data bias from the historic policy. For cases where the exploration model is unknown, an evaluation scheme with estimated propensity scores and a lower bound of the data observation probability was proposed in [ 29]. In our case, the exploration model is known as we know the historic bid price for each bid request.
 Learning with Missing Data. Handling missing data is a well-studied problem in machine learning [8 ]. A classic application is item recommendation with implicit feedback [25 , 22 ]. The authors in [25 ] proposed uniform sampling of negative items for each user X  X  positive-feedback item. The authors in [22 ] further proposed user response models to learn the missing data distribution instead of regarding it as completely random observations. With the idea that the popular but unrated items were more possible to be the true negative items for a user, the authors in [ 21, 26] proposed to sample the negative items more from the popular items and obtained significant recommendation improvement. More generally, the authors in [ 35] hypothesised that the unrated items with high predicted interest could actually be the neg-ative samples to the user, and proposed dynamic negative item sampling which substantially improved the recommen-dation performance on implicit feedback data.
 In online advertising, our work is closely related to [ 1, 31]. Similar to [1 ], we also employ a survival model [13 ] to esti-mate the market price. However, our purpose and setup are significantly different. The work in [31 ] specifically focused on forecasting and employing censored regression, while we aim at CTR estimation and bid optimisation. The authors in [ 1] considered bidding as a Markov decision process and formulated an online learning algorithm under the censored data. The underlying data bias was not considered in the bid optimisation and another potential drawback of this work is that a large amount of existing historic bidding data would not be utilised. Instead, we consider two distinctive training and prediction stages and develop models that can make use of any existing historic bidding data independent of previous bidding strategies. The main novelty of our work lies in de-riving bid-aware gradient descent that directly incorporates the auction bias into the CTR prediction and bid generation processes to learn unbiased models.
In online RTB display advertising, a bid request can be represented as a high dimensional feature vector [17 ]. Let us denote the vector as x . Without loss of generality, we regard the bid requests as generated from an i.i.d. x  X  p x ( x ) with-in a short period [ 38 ]. Based on the bid request x , the ad agent (or demand-side platform, a.k.a. DSP) will then pro-vide a bid b x following a bidding strategy. If such bid wins the auction, the corresponding labels, i.e., user response y (either click or conversion) and market price z , are observed. Thus, the probability of a data instance ( x ,y,z ) being ob-served relies on whether the bid b x would win or not and we denote it as P (win | x ,b x ). Formally, with the p.d.f. q denoting how the feature vector x is distributed within the observed training data D = { ( x ,y,z ) } , the generative pro-cess of creating the training data is summarised as: where the normaliser of q x ( x ) has been omitted for formula simplicity. Eq. (1 ) indicates the relationship (bias) between the p.d.f. of the pre-bid full-volume bid request data (predic-tion) and the post-bid winning impression data (training); in other words, the predictive models would be trained on D , where x  X  q x ( x ), and be finally operated on prediction data x  X  p x ( x ). In the following sections, we shall focus on the estimation of the winning probability P (win | x ,b and then introduce our solutions of using it for creating bid-aware gradients to solve CTR estimation and bid optimisa-tion problems.
The RTB display advertising uses the second price auc-tion [ 34]. In the auction, the market price z is defined as the second highest bid from the competitors for an auction. In other words, it is the lowest bid value one should have in order to win the auction. Following [1 ], we take a stochas-tic approach rather than game theoretical, and assume the market price z is a random variable generated from a fixed yet unknown p.d.f. p x z ( z ); then the auction winning prob-ability is the probability when the market price z is lower than the bid b x : where to simplify the solution and reduce the sparsity of the estimation, the market price distribution is estimated on a campaign level rather than per impression x [4, 38 ]. Thus for each campaign, there is a p z ( z ) to estimate, resulting the simplified winning function w ( b x ), similar to [1 , 38 ].
If we assume there is no data censorship, i.e., the ad agent wins all the bid requests and observes all the market prices, the winning probability w o ( b x ) can directly come from the observation counting: where z is the historic market price of the bid request x indicator function  X  ( z &lt; b x ) = 1 if z &lt; b x and 0 otherwise. We use it as a baseline of w ( b x ) modelling.

However, the above treatment is rather problematic as it does not take into account that in practice there are always a large portion of the auctions the advertiser loses ( z  X  b in which the market price is not observed in the training data. Thus, the observations of the market price are right-censored : when we lose, we only know that the market price is higher than our bid, but do not know its exact value. In fact, w o ( b x ) is a biased model and over-estimates the win-ning probability. One way to look at this is that it ignores the counts for lost auctions where the historic bid price is higher than b x in the denominator of Eq. (3). In this sit-uation, the market price should have been higher than the historic bid price and thus higher than b x . As we will show in our experiment such estimator consistently over-estimate the actual winning probability.

In this paper, we use survival models [13] to handle the biased auction data. Survival models were originally pro-posed to predict patients X  survival rate for a given time after certain treatment. As some patients might leave the inves-tigation, researchers do not know their exact final survival
In the iPinYou dataset [39 ] we tested, the overall auction winning rate of 9 campaigns is 23.8%, which is already a very high rate in practice. period but only know the period is longer than the investi-gation period. Thus the data is right-censored. The auction scenario is quite similar: the integer market price 2 is regard-ed as the patient X  X  underlying survival period from low to high and the bid price as the investigation period from low to high. If the bid b wins the auction, the market price z is observed, which is analogous to the observation of the pa-tient X  X  death on day z . If the bid b loses the auction, one only knows the market price z is higher than b , which is analogous to the patient X  X  left from investigation on day b . Specifically, we follow [1 ] by leveraging the non-parametric Kaplan-Meier Product-Limit method [14 ] to estimate the market price distribution p z ( z ) based on the observed im-pressions and the lost bid requests.
 Suppose there is a campaign that has participated in N RTB display ad auctions. Its bidding log is a list of N tuples  X  b ,w i ,z i  X  i =1 ...N , where b i is the bid price of this campaign in the auction i , w i is the boolean value of whether this cam-paign won the auction i , and z i is the corresponding market price if w i = 1. The problem is to model the probability of winning an ad auction w ( b x ) with bid price b x .
If we transform our data into the form of  X  b j ,d j ,n j where the bid price b j &lt; b j +1 . d j denotes the number of ad auction winning cases with the market price exactly valued b  X  1 (in analogy to patients die on day b j ). n j is the num-ber of ad auction cases which cannot be won with bid price b  X  1 (in analogy to patients survive to day b j ), i.e., the number of winning cases with the observed market price no lower than b j  X  1 3 plus the number of lost cases when the bid is no lower than b j  X  1. Then with bid price b probability of losing an ad auction is which just corresponds to the probability a patient survives from day 1 to day b x . Thus the winning probability will be Note the calculation is Eq. ( 5) is highly efficient, i.e., O ( N ). Table 1 gives an example of transforming the his-toric  X  b i ,w i ,z i  X  data into the survival model data  X  b and the corresponding winning probabilities calculated by Eqs. ( 5) and ( 3). We see that the Kaplan-Meier Product-Limit model, which is a non-parametric maximum likelihood estimator of the data [ 5], makes use of all winning and lost data to estimate the winning probability of each bid, where-as the observation-only counting model w o ( b x ) does not. As we can see in the table w o ( b x ) is consistently higher than w ( b x ). Later in experiment, we will further demonstrate such comparisons with real-world data in Figure 5.
Generally, given a training dataset D = { ( x ,y,z ) } , where the data instance x follows the training data distribution q ( x ), (the red data distribution in Figure 1), an unbiased supervised learning problem can be formalised into a loss-minimisation problem on prediction data distribution p x ( x )
The mainstream ad exchange auctions require integer bid prices. Without a fractional component, it is reasonable to analogise bid price to survival days.
We assume that if there is tie in the auction, the campaign will not get winning.
 Table 1: An example of data transformation of 8 instances with bid price between 1 and 4. Left: tuples of bid, win and ples  X  b j ,d j ,n j  X  j =1 ... 4 and the calculated winning probabili-ties. Here we also provide a calculation example of n 3 = 4 shown as blue in the right table. The counted cases of n the left table are 2 winning cases with z  X  3  X  1 and the 2 lost cases with b  X  3, shown highlighted in blue color. (the blue data distribution in Figure 1): where f  X  ( x ) is  X  -parametrised prediction model to be learned; L ( y,f  X  ( x )) is the loss function based on the ground truth y and the prediction f  X  ( x );  X (  X  ) is the regularisation term that penalises the model complexity;  X  is the regularisation weight. With Eqs. ( 1) and ( 2), one can use importance sam-pling to reduce the bias of the training data: =
Z = 1 | D | where the last equation is our empirical estimation. Based on this framework, if we obtain the auction winning proba-bility w ( b x ), e.g., Eq. ( 5), we can eliminate the bias for each observed training data instance. Let us look at the case of CTR estimation with logistic regression [28 ]. With the lo-gistic loss between the binary click label { X  1 , +1 } and the predicted probability and L2 regularisation, the framework of Eq. ( 7) is written as where the winning probability w ( b x ) is estimated for each observation instance, which is independent from the CTR estimation parameter  X  ; the update rule of  X  is routine using stochastic gradient descent with the learning rate  X  . The derived Bid-aware Gradient Descent (BGD) of Eq. ( 8) is  X   X  (1  X   X   X   X  )  X  +  X   X  y  X  e Discussion. From the equation above, we observe that with a lower winning bid b x , the probability 1  X  Q b seeing the instance in the training set is lower. However, the corresponding gradient from the data instance is higher and vice versa as it is in the denominator.

This is intuitively correct as when a data instance x is observed with low probability, e.g., 10%, we can infer there are 9 more such kind of data instances missed because of auction losing. Thus the training weight of x should be F igure 2: Winning probability and reweighting term in Eq. (9) against historic bid price. multiplied by 10 in order to recover statistics from the full-volume data. By contrast, if the winning bid is extremely high, which leads 100% auction winning probability, then such data is observed from the true data distribution. Thus there will be no gradient reweighting on this data. Such nonlinear relationship has been well captured in our model in the gradient updates, as illustrated in Figure 2.
Another important problem in online advertising is bid optimisation, i.e. to find the optimal bidding strategy to maximise a campaign KPI, restricted by the campaign bud-get. Essentially, the bidding function is abstracted as a func-tion mapping from the estimated CTR f ( x ) to the bid price b ( f ( x )). 4 According to [ 38 ], with the auction volume T and campaign budget B , it is a functional optimisation problem:
With the auction selection, the observed data distribution is actually q x ( x ). By Eq. ( 1), Eq. ( 10 ) is written as
Note that w ( b x ) is different from w ( b ( f ( x ))), where b the historic bid price for the bid request x while b ( f ( x )) is the bid price we want to optimise.
 The Lagrangian is
L ( b ( f ) , X  ) = Z
A ccording to the derivation of [38 ], the Euler-Lagrangian condition of Eq. (11 ) is  X   X w ( b ( f ( x ))) = h f ( x )  X   X b ( f ( x )) i  X  X  ( b ( f ( x )))
W e drop the CTR estimation parameter  X  here as it is not the parameter to optimise in this task. where we see that the optimal bidding function b ( f ( x )) de-pends on the winning function w ( b ). For example, if where c is a constant, then the corresponding optimal bid-ding function is
For the solution of  X  , the Euler-Lagrangian condition w.r.t.  X  is
The numeric solution of  X  is highly efficient. A feasible solution of Eq. ( 19 ) is
As b ( f ( x ) , X  ) always monotonically decreases w.r.t.  X  and w ( b x ) monotonically increases w.r.t. b ( f ( x ) , X  ), the objec-tive of Eq. (20 ) is convex w.r.t.  X  , which makes the solution of  X  easy to obtain. The BGD to solve  X  is via updating Discussion. Highlighted in Eq. ( 21 ), there are two factors related with the historic bid for updating  X  : (i) the instance reweighting, similar with Eq. ( 9): a small historic bid b would generate a large weight, amplifying the importance of the training instance. (ii) The historic bid of the train-ing instance also has an impact on the gradient direction, evidenced by the second factor of the update in Eq. ( 21 ).
The parameter  X  converges when the second factor be-comes zero. The ratio B/ | D | would ensure the budget to be allocated evenly across the new bids. The ratio between the winning rate of the new bid price w ( b ( f ( x ) , X  )) and that of the historic bid 1  X  Q b crepancy of the probability of seeing the impression in the training and that in the prediction.

To further understand this, Figure 3 illustrates the sec-ond factor (the gradient direction term) in Eq. (21 ) against historic bid price b x on two sample campaigns with two new bids ( b ( f ( x ) , X  ) = 50 and 100). We observe that when the historic bid is small, the gradient direction is more likely to stay positive and leads to higher  X  value (as bidding func-tion gradient term in Eq. ( 21 ) is always negative) in order to decrease the bid. For example, for a data instance that its historic bid b x is low, the probability of observing the data instance is low, which means there are more similar or the same data instances that are missing in the training. If the F igure 3: The gradient direction term in Eq. ( 21 ) against historic bid price b x with two new bids b ( f ( x ) , X  ). new bid price b ( f ( x ) , X  ) is high, then the optimal bid price b ( f ( x ) , X  ) should be lower to avoid budget overspending in full-volume data, which is reflected on the positive value of the gradient direction factor to make  X  higher and b ( f ( x ) , X  ) lower.

Please note that with the pre-calculated reweighting factor 1 /w ( b x ) = 1 / (1  X  Q b calculate the above BGD updating and solve  X  .
Two real-world datasets are used in our repeatable offline empirical study 5 : iPinYou and TukMob. iPinYou runs the largest DSP in China. The publicly avail-TukMob is a major DSP focusing on mobile game and
Each data instance of both datasets can be represented as a triple ( x ,y,z ), where y is the user click binary feed-back, z is the historic winning price of the auction, and x is the bid request and ad features of that auction. The auc-tion features contain the information of the user (e.g. the user interest segments, IP address, browser, operation sys-tem, location), advertiser (e.g. the creative format and size), publisher (e.g. the auction reserve price, ad slot size, page domain and URL).

We mainly report the experimental results on iPinYou dataset for experiment reproducibility while the study on TukMob acts as an auxiliary part particularly for the high-CTR video ad marketplace to make our experiment more comprehensive.

The online A/B testing experiment is conducted based on Yahoo! DSP, a mainstream DSP in United States ad market. The training dataset comes from its ad log in Aug.
Ex periment code link: https://github.com/wnzhang/rtb-unbiased-learning. Dataset link: http://data.computational-advertising.org and Sep. 2015 while the online A/B testing is performed on 9 campaigns during 7 days of Sep. 2015, which involves 117.1M impressions, 95.4K clicks and 68.6K USD expense.
The experiment flow chart is shown in Figure 4. The original impression log data is reasonably assumed as full-volume bid request data in our experiment 7 . A truth-telling bidding strategy [ 17 ] is performed to simulate the historic bidding process and produce the winning (labelled but bi-ased) impression data and lost (unlabelled) bid request data. Based on these two datasets, the bid landscape forecasting module as in Eq. ( 5) estimates the market price distribution which acts as the winning function in Eq. ( 1). Thus the ob-servation bias of each data instance from the impression log is estimated. With Eq. (8 ), the unbiased CTR estimation is performed. Furthermore, with the unbiased CTR estima-tor and the winning function, the unbiased bid optimisation is performed via Eq. ( 11 ) to get the new bidding function, which is in turn operated in the next prediction stage.
CTR estimation and bid optimisation are the two tasks we investigate in this work. For each of these tasks, we compare the following four training schemes:
This assumption is reasonable as this dataset is collected with fixed large bid to reduce the auction-selection bias [20]. F igure 5: Winning probability against bid price (iPinYou).
Before evaluating the practical CTR estimation and bid optimisation tasks, let us first take an analysis of the com-pared models X  performance on winning probability estima-tion, i.e., w ( b x ) in Eq. ( 2).

First, Table 2 demonstrates the statistics of the full-volume data and the winning impression data by the  X  X istoric X  truth-telling bidding strategy as described in Section 4.2. As can be observed, for both datasets the winning impression data which is fed into bias , uomp and kmmp training schemes is much smaller than the full-volume data which is fed into full training scheme.

Figure 5 shows the curves of winning probability w.r.t. the bid price with three compared settings, i.e., uomp , mmp and full , on iPinYou dataset. As expected, all the curves start from 0 given the bid 0 and then increase as the bid price increases and finally converge to 1 when the bid price surpasses a threshold (300 for iPinYou dataset). The truth curve is built from all the market price observa-tions from the full-volume prediction data, regarded as the ground truth here. We observe that full curve is the closest one to truth curve since full makes use of the full-volume training data and is naturally unbiased. The only reason of the slight difference between full and truth is the data Table 2: Winning data statistics: the full-volume data is used in full training scheme, while the winning data is used in bias , uomp and kmmp training schemes (both datasets). distribution shift between the training and prediction peri-od. uomp always over-estimates the winning probability, as pointed out in Section 3.1 . Compared to uomp , kmmp curve is much closer to truth , which shows its advantage of mak-ing use of the lost bid request data to improve the winning probability estimation.

Table 3 presents the detailed Pearson correlation and KL-divergence between each of the three compared settings and truth on iPinYou dataset. We observe that for all investi-gated campaigns, kmmp provides a much better estimation, i.e., higher Pearson correlation and lower KL-divergence, than uomp , and it is even highly comparable with full on Pearson correlation. These results demonstrate the surpris-ingly large improvement that the lost and free bid request data brings to the estimation of winning probability (market price distribution).
With different biased or unbiased settings, we train the logistic regression model and evaluate its performance. Ta-ble 4 presents the detailed AUC and cross entropy perfor-mance of these 4 compared training schemes for each cam-paign in iPinYou dataset. Table 5 presents the AUC per-formance comparison on TukMob dataset. We can observe that (i) the proposed unbiased training schemes uomp and kmmp always outperform the biased but widely adopted bias training scheme on all the test campaigns (except for 3476). Such consistent outperformance shows the effectiveness of our models in eliminating the training data instance bias which makes the prediction model generalise better on pre-diction data. (ii) Comparing the unbiased settings and kmmp and the upper bound oracle setting full , we can see kmmp outperforms uomp for all the campaigns (except for 3476). For some campaigns, e.g., 1458 and 2997, kmmp even slightly outperforms full 8 which again shows the ad-vantages of making use of the lost auction information for better estimating the instance bias.

Figure 6 shows the AUC and cross entropy on predic-tion data of all iPinYou campaigns for each training round. We can observe the unbiased uomp and kmmp models learn stably and consistently outperform bias . full substantial-ly outperforms other compared training schemes, which is not surprising as full obtains much more training data in-stances (as shown in Table 2) and the data distribution is unbiased.

Note that we do not compare calibration techniques [11 ] in our experiment because it is another dimension of reducing the model bias. If the training data is auction-biased, then the calibration based on that is still biased.
For bid optimisation experiment, we mainly focus on the click performance improvement from bidding strategy pa-rameter optimisation via Eqs. (16 ) and ( 19) instead of the difference of CTR estimation. Thus in our training/prediction environment, the logistic regression CTR estimator is trained based on a separate unbiased training data and is shared in all 4 compared training schemes of bid optimisation. For each training scheme, we train the optimal parameter  X  in Eq. ( 19) via the biased or unbiased training data, then apply the corresponding bidding strategy Eq. (16 ) on prediction data to observe its performance.

We follow [38 ] to set the budget proportions to perform offline bid optimisation, where the train/test budget is set as 1/64, 1/32, 1/16, 1/8, 1/4 and 1/2 of the total expense of the train/test dataset. We cannot set the proportion as 1 because in such case one may simply bid infinity to win all the impressions and clicks in the data and just spend all the budget.

Table 6 shows the click performance of the 4 compared training schemes with 1/64 and 1/4 budget settings respec-tively for each iPinYou campaign. Table 7 shows the overall click and eCPC performance comparison against different budget settings on TukMob dataset. We can observe that the unbiased uomp and kmmp consistently outperform the traditional bias which were used in the most of the previ-ous bid optimisation work [16 , 27, 38]. This shows the great potential of our proposed unbiased training schemes in bid optimisation. Furthermore, kmmp outperforms uomp and it
This is mainly caused by the local data distribution, which is not significant.
 F igure 6: CTR performance training convergence (iPinYou).
Table 6: Bid optimisation click performance (iPinYou). T able 7: Bid optimisation click performance (TukMob). i s very close to the theoretic upper bound from full , in 17 out of 20 test cases, suggesting it is generally much better to leverage the winning probability obtained from the cen-sored observations of both winning impressions and lost bid requests.

Figure 7 further provides the click, impression improve-ment percentages and eCPC drop percentage of the unbiased training schemes against bias with different budget settings. The improvements for clicks and impressions are positive for all budget settings and the eCPC drops are negative for all budget settings (except full on 1/2), which show the robustness of the unbiased training schemes. Also we can observe that kmmp dominates uomp and heavily approach the upper bound full .
We deployed the unbiased kmmp training scheme on Ya-hoo! DSP and performed online A/B testing for 9 campaigns during 7 days in Sep. 2015. For each campaign, we created two experiment buckets: control and treatment. Each was allocated 50% of the bid request traffic (based on user ID to avoid attribution conflicts), and 50% of the campaign X  X  bud-get. The control bucket used gradient boosting decision tree Figure 7: Improvement over bias w.r.t. budget proportions. Table 8: Online A/B testing of CTR estimation (Yahoo!). click predictor [ 11 ] trained with bias , while the click model used in the treatment bucket was trained with kmmp . The deployed bidding strategy is the conventional truth-telling bidding [17 ].

In order to perform an unbiased evaluation of the CTR estimation, we deployed a bidding agent performing very high constant bid in Sep. 2015 to collect an ad impressions dataset which can be regarded as full-volume unbiased test data. The training data was still the traditional biased ad impression dataset during Aug. and early Sep. 2015. Ta-ble 8 provides the detailed CTR estimation performance for each campaign and the overall performance. As can be ob-served, kmmp provided a consistent AUC improvement over BIAS across all investigated campaigns. The overall AUC was 73.48% for bias and 76.45% for kmmp , which was a very large improvement for CTR estimation task in practice.
Table 9 further presents the detailed performance of A/B testing of bid optimisation on the 9 campaigns. Figure 8 depicts the relative difference comparing the performance of kmmp againt bias . We found that with the same campaign budget the kmmp -trained model acquired more clicks (most of the time) but fewer impressions than the bias -trained one, which made its CTR much higher than bias . This is because there was less over-prediction on many cheap cases. In the biased training data, the over-predicted CTR on cheap cases were more likely to be sampled because the historic bidding strategy overbid on these cheap cases, vice versa on expen-sive cases. With the kmmp training scheme, the bidding strategy to-some-extent got rid of such bias to avoid over-prediction on cheap cases, which provided fewer impressions but more clicks.
 Table 9: Online A/B testing of bid optimisation (Yahoo!). F igure 8: Relative performance difference between kmmp and bias in Yahoo! online A/B testing: ( kmmp -bias )/
Overall, with the same budget, the bidding strategy trained with kmmp achieved much better eCPC (9.30% drop) and CTR (42.8% rise) than the conventional one trained with bias . The kmmp -trained click model effectively alleviated over-prediction especially in the low-CTR region and thus became more efficient in acquiring clicks. Therefore, with the bidding strategy with unbiased kmmp -trained click mod-el, campaigns could acquire clicks in a more cost-effective way.
In this paper, we studied the data observation bias prob-lem in display advertising generated from the auction se-lection that would hurt the performance of various super-vised learning models. To address this problem, we pro-posed a model-free learning framework that eliminates the model bias generated from censored auction data. The de-rived Bid-aware Gradient Descent (BGD) learning scheme naturally incorporates the historic auction and bid informa-tion, which is the main novelty of this paper. We found that the historic bid for each instance could influence both BGD learning weight and update direction. Comprehen-sive empirical study based on iPinYou and TukMob datasets demonstrated the large improvement of our learning frame-work over strong baselines in both CTR estimation and bid optimisation tasks. With light engineering work, the learn-ing framework was deployed on Yahoo! DSP and brought 2.97% AUC lift in CTR estimation and 9.30% eCPC drop in bid optimisation over 9 campaigns.

It is important to point out that such learning framework is flexible with other supervised learning tasks than the in-vestigated ones in this work, such as budget pacing and fre-quency capping in online advertising as well as other data science problems, such as interactive recommender systems [40], off-policy reinforcement learning [10 ], which are our planned future work.
 Acknowledgement. We sincerely thank Quan Lu from Yahoo! US for his support of the online experiment. Weinan thanks the CSC funding for supporting the research.
