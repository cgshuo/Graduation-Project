 TIMOTHY BALDWIN and SU NAM KIM University of Melbourne FRANCIS BOND Nanyang Technological University SANAE FUJITA Nippon Telegraph and Telephone Corporation DAVID MARTINEZ NICTA Victoria Research Laboratories and University of Melbourne and TAKAAKI TANAKA Nippon Telegraph and Telephone West Corporation 4: 2  X  T. Baldwin et al.
 1. INTRODUCTION The work presented in this article is aimed at developing and extending ma-chine readable dictionary (MRD) based word sense disambiguation (WSD) techniques. The goal of WSD is to link ambiguous occurrences of the words in specific contexts to their meanings, as defined in a predefined sense inven-tory. For instance, given the following input: we aim to identify each content word (namely the first, second, and fourth words: otonash  X  i , inu and ka(u) , 1 respectively) as occurring with the sense cor-responding to the provided English glosses. In the case of the second word (  X  [ inu ]), for example, the dictionary lists two senses for the word, correspond-ing to the English  X  X og X  and  X  X py X , respectively. 2 We need to disambiguate the word to the first sense ( X  X og X ) in the given input. Similarly, we would identify the range of senses for all other content words (namely the first and fourth words), and disambiguate each relative to the context provided.

The relevance of MRDs to the task of WSD is that we build our method solely off information available in this pre-existing resource, namely using the textual definitions of each sense, and potentially ontological links in the dic-tionary. MRD-based WSD methods have the advantage that they are easily adaptable to any given MRD, and don X  X  require any external data or additional sense annotations. That is, they provide the means to perform rapid devel-opment of WSD capabilities for languages or domains where sense-annotated data is not readily available.

There is a variety of reasons why we may wish to build a WSD system. For example, WSD has been shown to enhance parsing accuracy [Fujita et al. 2007; Agirre et al. 2008], so it could be for purely utilitarian reasons. Alternatively, we may require explicit word sense predictions, for example, in a language un-derstanding task [Bond et al. 2004a] or crosslingual word glossing task [Yap and Baldwin 2007], presenting context-sensitive glosses of words in running text. For language understanding, there would be particular emphasis on pre-dicting the most-likely sense for each word with high accuracy, whereas with word glossing, we could instead focus on filtering out implausible senses and possibly retain multiple senses in the case that there is no standout single most-likely sense.

The construction of WSD systems has been the goal of many research initia-tives (see for instance Ide and V  X  eronis [1998] and Agirre and Edmonds [2006]). A broad interest in WSD and derivative tasks, and the need for standardised evaluation platforms, led to the establishment of the Senseval and SemEval initiatives. 3 Two key evaluation tasks (across multiple languages) in each of the four iterations of Senseval/SemEval to date have been an ALL WORDS and LEXICAL SAMPLE task. The ALL WORDS task requires that all (content) words in a sample text be disambiguated, and the LEXICAL SAMPLE task requires that, for a small set of words and selection of sentences containing that word for each, only the word of interest be disambiguated, all relative to a predefined sense inventory.

WSD systems are generally classified according to the type of tasks they are able to perform. The most relevant factors are the coverage of the system (e.g. ALL WORDS vs. LEXICAL SAMPLE , ` a la the Senseval tasks), and the granular-ity of the senses they are able to distinguish (e.g., homographs vs. fine-grained senses). The appropriate coverage and granularity of a system is determined relative to the final application of the WSD and the availability of data. For in-stance, for most applications, such as the cross-lingual glossing example above, we would require full coverage of the text, but the granularity could be reason-ably coarse.

WSD systems can be further classified according to the knowledge sources they use to build their models. A top-level distinction is made between super-vised and unsupervised systems. The former rely on training instances that have been hand-tagged in order to build their classification models, while the latter build their classifiers relying on other types of knowledge, such as lexical databases or untagged corpora. The Senseval/SemEval evaluation tracks have shown that supervised systems perform better when training data is available, but they do not scale well to all words in context. Estimations of the effort required to build training data of reasonable size for all words (and all lan-guages) are pessimistic [Mihalcea and Chklovski 2003]. This is known as the knowledge acquisition bottleneck, and is the main motivation behind research on unsupervised techniques.

While sense-annotated data is hard to come by, in recent years rich lexi-cal resources have become increasingly available for a variety of languages. These knowledge sources are designed to be applied in language technology (LT) applications, and are often reusable for different tasks. In this article, we aim to exploit an existing lexical resource to build an all-words Japanese word-sense disambiguator, that in turn can be applied to applications such as word glossing, or alternatively interfaced with downstream processing such as parse selection. The resource in question is the Hinoki Sensebank [Tanaka et al. 2006; Bond et al. 2006] and consists of the 28,000 most familiar words of Japanese, each of which is annotated with one or more basic senses from the Lexeed dictionary [Kasahara et al. 2004]. The senses take the form of a 4: 4  X  T. Baldwin et al. dictionary definition composed from only the 28,000 words contained in the dictionary, each of which is further manually sense annotated according to the Lexeed sense inventory. Hinoki also has a semi-automatically constructed on-tology [Nichols et al. 2005]. This combination of features makes it an ideal resource for developing and evaluating WSD systems.

Through the Hinoki Sensebank, we investigate a number of areas of gen-eral interest to the WSD community. First, we test extensions to the Lesk al-gorithm [Lesk 1986; Banerjee and Pedersen 2003], focusing specifically on the impact of the scoring metric, the segment representation, and expansion via ontological semantics on WSD performance. These results are significant in porting the extended Lesk method to a language other than English, with the intention of improving the accuracy of unsupervised MRD-based WSD; this is particularly relevant for languages that do not have sense-disambiguated cor-pora. Second, we propose further extensions of the Lesk algorithm that make use of disambiguated definitions. In this, we shed light on the relative benefits we can expect from hand-tagging dictionary definitions, that is, in introduc-ing  X  X emi-supervision X  to the disambiguation task (c.f. Niu et al. [2005] and Pham et al. [2005]). The results are equally of interest for English, as the Ex-tended WordNet [Harabagiu et al. 1999] provides an analogous resource with (semi-automatically) sense-annotated and parsed definitions which could be combined with this algorithm, and the Princeton WordNet Gloss Corpus in-cludes fully sense-annotated definitions. 4
In the remainder of this article, we first summarize related work in unsu-pervised WSD in Section 2. Then we describe the basic method, resources, and characteristics of the Japanese language required to understand our work in Section 3. The extensions to the Lesk method are described in Section 4. Next, we present the evaluation of the different techniques in Section 5, and finally we discuss our conclusions in Sections 6 and 7. 2. RELATED WORK As pointed out above, our work focuses on unsupervised and semi-supervised methods that target all words and parts of speech (POS) in context. We blur the supervised/unsupervised boundary somewhat in combining the basic unsuper-vised methods with hand-tagged definitions from Hinoki, in order to measure the improvement we can expect from sense-tagged dictionary data. Having said this, it is important to recognize that our method differs from conven-tional supervised methods in that it doesn X  X  hinge on the availability of anno-tated training examples for every sense in the sense inventory. Hand-tagged definitions are less costly to produce than sense-annotated open text because: (1) the effects of discourse are limited, (2) the syntax is relatively simple, (3) there is significant semantic priming relative to the word being defined, and (4) there is generally explicit meta-tagging of the domain in technical defini-tions. In our experiments, we will make clear when sense-annotated data is being used.
Unsupervised methods often draw on a range of knowledge sources to build their models. Primarily the following types of lexical resources have been used for WSD: MRDs, lexical ontologies, and untagged corpora (monolingual corpora, second language corpora, and parallel corpora). Although early ap-proaches focused on exploiting a single resource [Lesk 1986], recent trends show the benefits of combining different knowledge sources, such as multiple preprocessors [Stevenson 2003], or hierarchical relations from an ontology and untagged corpora [McCarthy et al. 2007]. In this summary, we will focus on a few representative systems that make use of different resources, noting that this is an area of very active research which we cannot do true justice to within the confines of this article.

The Lesk method [Lesk 1986] is an MRD-based system that relies on count-ing the overlap between the words in the target context and the dictionary definitions of the senses. 5 In spite of its simplicity, it has been shown to be a hard baseline for unsupervised methods in Senseval [Kilgarriff and Rosen-zweig 2000; Mihalcea et al. 2004; Pradhan et al. 2007]. Banerjee and Pedersen [2003] extended the Lesk method for WordNet-based WSD tasks, to include hierarchical data from the WordNet ontology [Fellbaum 1998] via an implicit representation of word context. They observed that the hierarchical relations significantly enhance the basic model. Both these methods will be described extensively in Section 3, as our approach is based on them.

A recent approach that combines ontological relations and untagged cor-pora was presented by McCarthy et al. [2007], who implemented an algorithm to automatically rank word senses in relation to a corpus. They relied on a thesaurus automatically created using the method of Lin [1998], and then overlaid WordNet similarity scores onto this thesaurus in order to create the ranking. The last step was to assign to each test instance the sense of the highest-scoring sense in the ranking.

There are other techniques that exploit untagged data in order to build sense-tagged resources automatically. One such approach was first presented by Leacock et al. [1998], and follows these basic steps: (1) select a set of monosemous words that are related to the different senses of the target word, (2) query the Web to obtain examples for each relative, (3) create a collection of training examples for each sense, and (4) use a machine learning algorithm trained on the acquired collections to tag the test instances. This method has been used to bootstrap large sense-tagged corpora [Mihalcea 2002; Agirre and Martinez 2004].

Bootstrapping techniques consist of algorithms that learn from a few in-stances of labeled data (seeds) and a big set of unlabelled examples. In his widely-cited work, Yarowsky [1995] applied an iterative bootstrapping process to induce a classifier based on decision lists. With a minimal set of seed exam-ples, disambiguation results comparable to supervised methods were obtained, although this was over a limited set of coarse-grained binary sense distinc-tions; this work has not been extended to fine-grained senses. 4: 6  X  T. Baldwin et al.
 Parallel corpora have also been used to avoid the need for hand-tagged data. Following this approach, Chan and Ng [2005] built a classifier from English X  Chinese parallel corpora. They considered English words which correspond to the same Chinese translation to have the same sense, and thus obtained sense-disambiguated data without manual annotation. Others who have used parallel corpora for WSD include Dagan and Itai [1994] and Diab and Resnik [2002]. However, large-scale parallel corpora are expensive to build, tend to have limited coverage, and only exist for a very small number of language pairs. 3. BACKGROUND As background to our work, we first describe the lexical resources we used in our experiments, then outline aspects of Japanese relevant to this work, and finally present the basic and extended Lesk algorithms that are at the core of our approach. 3.1 The Hinoki Sensebank All our experimentation is based on the Hinoki Sensebank [Tanaka et al. 2006]. The Hinoki Sensebank consists of sense annotations based on the Hinoki dic-tionary, which contains 28,000 high-familiarity Japanese words with 46,000 different senses [Kasahara et al. 2004]. The Hinoki dictionary is based on Kindaichi and Ikeda [1988], a standard monolingual dictionary of Japanese which has been repurposed for WSD.

The sense granularity of Hinoki is relatively coarse for most words, with the possible exception of light verbs, making it well suited to open-domain ap-plications. The definition sentences for each sense were rewritten to use only the closed vocabulary of 28,000 words in the Hinoki lexicon (in addition to some function words). Additionally, an example sentence was manually con-structed to exemplify each of the 46,000 senses, once again using the closed vocabulary of the Hinoki dictionary. Both the definition sentences and exam-ple sentences were then manually sense-annotated by five native speakers of Japanese, from which a majority sense was extracted. There were 199,268 ambiguous tokens in the definition sentences. The average annotation rate was 1,799 tokens/day; disambiguating the definition sentences took 111 person days/annotator. Table I details the average polysemy and relative occurrence of monosemous words of different POS type in Hinoki, as well as inter-annotator agreement statistics [Bond et al. 2006].
 We give a (slightly shortened) example of the entry for  X  [ inu ] in Figure 1. The sense ID of each content word is indicated by a subscript, where the sense IDs have been ranked by their frequency in the Hinoki (and ID 1 thus indicates the first sense for that word). In the case of 67 occurrences of sense ID 1, and only 1% (1/67) to sense ID 2. The semantic class links to Goitaikei [Ikehara et al. 1997], and the WordNet ID to version 2.0 of WordNet [Fellbaum 1998]; neither of these resources are used in this research.
Subsequent to the development of the Hinoki Sensebank, an ontology was semi-automatically induced by parsing the first definition sentence for each sense [Bond et al. 2004b; Nichols et al. 2005]. Hypernyms were determined by identifying the highest scoping content predicate (i.e., the genus). Other rela-tion types such as synonymy and domain were also induced based on trigger patterns in the definition sentences. Because each word is sense tagged, the induced ontological relations link senses rather than just words.

For example, for was the highest scoping real predicate in the first sentence of the definition:  X  X  carnivorous animal of the canidae family X .  X  1 [ inu ] itself is the hypernym for many other word senses. We treat hypernymy and hyponymy as transitive equivalents, resulting in those senses which are identified as having as their hypernym (e.g.  X  X  X  1 [ aikeN ]  X  X et dog X  and  X  X  1 [ baNkeN ]  X  X uard dog X ), are equivalently hyponyms of  X  1 . 4: 8  X  T. Baldwin et al.

In  X  2 [ inu ],  X  X  X  X  1 [ supai ]  X  X py X  is the only content predicate in the def-inition, and we therefore take it to be a synonym. This entry has no direct hyponyms.

In the version of the Hinoki Sensebank used in this research, there are 50,562 hypernym links, 765 hyponym links, 1,854 domain links, 682 meronym links, and 14,287 synonym links (including abbreviations and nicknames).
The Hinoki Sensebank also contains annotated newspaper text, as well as syntactic and structural semantic annotations [Tanaka et al. 2006; Bond et al. 2006], although we do not use them in these experiments. 3.2 Peculiarities of Japanese While the basic method we propose is language independent, all our experi-ments are targeted exclusively at Japanese. As such, there are a number of features of Japanese which impinge on our experiments, as outlined below.
First, Japanese is made up of three basic alphabets: hiragana, katakana (both syllabic in nature), and kanji (logographic in nature). 7
Second, Japanese is a non-segmenting language, that is, there is no ex-plicit orthographic representation of word boundaries. The native rendering of (1), for example, is  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  . Fortunately, various packages exist to automatically segment Japanese strings into words and also POS tag and lemmatize the words. All the Hinoki data has been presegmented using ChaSen [Matsumoto et al. 2003], and we make use of this in our experiments. It is also possible, however, to sidestep segmentation altogether and repre-sent the string via its component characters, under the expectation that the predominance of logographic kanji characters will provide a form of semantic smoothing.

Third, Japanese has relatively free word order, or strictly speaking, word order within phrases is largely fixed but the relative ordering of phrases gov-erned by a given predicate is relatively free. For the purposes of WSD, this motivates a representation of context which captures local word order but ab-stracts away from phrase order. 3.3 Basic Lesk The original Lesk algorithm [Lesk 1986] performs WSD by calculating the rel-ative word overlap between the context of usage of a target word, and the dictionary definition 8 of each of its senses in a given MRD. The sense with the highest overlap is then selected as the most plausible hypothesis. This general method can be formalized according to Algorithm 1, which also forms the basis of our proposed method.
 To see the basic Lesk algorithm in action, let us take Example (1) as input. Assuming a POS tagging and lemmatization step, 9 the context of  X   X  X og X  X n Example (1) would be represented as the following bag of lemmatized content words (see Section 4.2 for details). (1 X ) {  X  X  X  X  X  X  X  ,  X  X  X  }
Further assuming the following (multi-sentence) definition for the first sense of  X  (i.e.,  X  X og X ): (2) we would generate the following bag of (content) lemmas: (2 X ) {  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  X  ,  X  X  X  } (3) we would generate the following bag of (content) lemmas: (3 X ) {  X  X  X  ,  X  X  X  X  ,  X  X  X  X  }
In the original Lesk algorithm, the similarity calculation in Algorithm 1 takes the form of a simple set intersection operation and determination of the cardinality of the resulting set. In our example, the algorithm would correctly predict that Example (1) contains an instance of the first sense of inu due to the definition containing one lemma in common with the context (namely  X  X  X   X  X eep X ), as compared to the second sense where there are no overlapping lemmas. 3.4 Extended Lesk An obvious weakness of the original Lesk algorithm is that it requires that the exact words used in the definitions be included in each usage of the target 4: 10  X  T. Baldwin et al. word. To redress this shortcoming, Banerjee and Pedersen [2003] extended the basic algorithm for WordNet-based WSD tasks to include ontological se-mantics, that is, expanding the method to compare not only the definition of a given word sense, but also the definitions of word senses ontologically related to it (e.g., through hypernymy, hyponymy, or meronymy). 10 Further, they mod-ified the original algorithm to compare the definitions of each sense of each word within n words of the target word, rather than comparing the words in the context of the target word directly with the definition of each target word sense. To return to our example from above, given Example (1) and the target word inu , the Banerjee and Pedersen [2003] algorithm would identify otonash  X  i and kau as context words of the target word, and identify all senses of each of the context words in Hinoki. It would further identify directly re-lated senses of each sense of both inu and the context words (i.e., senses which are directly linked by a single edge). Finally, for each each sense of inu , it would take first the expanded set of that sense and all ontologically-linked senses, and second the expanded set of all senses of context words and their ontologically-linked senses, and perform a comparison of the definitions in the pairwise cross-product of the two sets, returning the combined sum of all such comparisons.

Formally, Banerjee and Pedersen [2003] formulate their similarity calcula-tion as follows for a given word sense pairing ( A , B ): where score is calculated as the square of the word length of the longest com-mon substring in the definitions of R i ( A ) and R j ( B ), and RELPAIRS is a set of the method doesn X  X  perform explicit comparison of the target word context with the (extended) word definition, but instead compares the (extended) word de-finition with the (extended) definitions of context words. In this research, we we compare the definitions of the two senses, the definitions of the hypernyms of the two senses (each of which is a set in itself), and the definitions of the hyponyms of the two senses (each of which is, again, a set in itself). 4. PROPOSED ALGORITHM The basis of our algorithm is an amalgam of the original Lesk algorithm and the extended algorithm of Banerjee and Pedersen [2003]: (1) like the original algorithm, we compare the context of the target word directly with definitions, unlike Banerjee and Pedersen who only ever compare definition sentences; and (2) like Banerjee and Pedersen we extend the original algo-rithm by looking beyond the definitions of the individual senses to include the definitions of ontologically-related senses, but our approach for doing this is to take the union of multisets associated with each definition rather than com-bine pairwise comparisons of individual definitions.

Below, we outline the specifics of the proposed algorithm, in terms of the scoring mechanism, tokenization strategy, and different strategies for expand-ing the definitions. 4.1 Scoring Mechanism In Algorithm 1, similarity provides the means to score a given pairing of con-text w and definition d i , j . In the original Lesk algorithm, similarity was sim-ply the number of words in common between the two, which Banerjee and Pedersen [2003] modified by squaring the size of the longest overlapping sub-string. While squaring is well motivated in terms of preferring longer sub-string matches, longer definitions are preferred as they are a priori more likely to generate longer matches; the calculation of the longest substring is also com-putationally expensive. We thus adopt a cheaper scoring mechanism which is normalized relative to the length of w and d i , j , but ignores the original order of the segments. Namely, we use the Dice coefficient, defined for sets A and B as follows: 4.2 Tokenization As mentioned in Section 3.2, Japanese is a nonsegmenting language which makes heavy use of logographic characters. As such, as an alternative to seg-mentation via a word splitter such as ChaSen and using words (or lemmas) as our segment unit, we can simply tokenize strings into their component charac-ters and use characters as our segments. Note that independent of the choice of segment granularity, we require word splitting in order to identify the words in a string to look up senses for in our dictionary; that is, we are not doing away with the need for word segmentation in adopting a character indexing strategy, we are simply modifying the underlying string representation which our simi-larity mechanism is applied to. Character and word tokenization have been compared in the context of Japanese information retrieval [Fujii and Croft 1993] and translation retrieval [Baldwin 2001], and in both of these tasks, characters have been found to be the superior representation in certain con-texts. It is thus interesting to investigate whether these findings carry across to WSD. 4.3 Expanded Definitions The main direction in which Banerjee and Pedersen [2003] successfully ex-tended the Lesk algorithm was in including ontologically linked definitions (e.g., hyponyms and hypernyms). In the method of Banerjee and Pedersen, the expansion takes the form of carrying out multiple comparisons of different pairings of definitions and summing the individual scores together. Our ap-proach is instead to use ontological links to first expand the definition for each word sense, and carry out a single comparison of the expanded definition with 4: 12  X  T. Baldwin et al.
 the context string of the target word. In information retrieval terms, therefore, our method equates to query expansion.

We experiment with a number of approaches to expansion. First, we ex-pand the definition sentences to in turn include the definitions of each word contained in that definition. In the case of the first sense of inu , for example, in Example (2), we expand the definition to include the definitions for each word in the definition which is also defined in the Hinoki Sensebank, namely  X  [ inu ],  X  X  X  [ shokuniku ],  X  X  X  [ d  X obutsu ], and so on. 11 As all of the definitions are sense-tagged, this expansion can occur either in a sense-sensitive manner, expanding the definition to include only the definitions of the sense instanti-ated in the definition, or a sense-insensitive manner, expanding the definition to include all senses of each word.

Second (and orthogonally), we follow Banerjee and Pedersen [2003] in expanding the definitions to include words from the definitions for the synonyms, hypernyms and/or hyponyms of a given sense of the target word. 12 For example, when we combine the first sense of inu (Figure 1) with its hypernym d  X obutsu (Figure 2), we add the set of content words in d  X obutsu 1  X  X  definition {  X  X  X  ,  X  X  X  X  ,  X  X  X  } to those of inu 1 {  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  X  ,  X   X  } to give the set: {  X  ,  X   X  ,  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  X  ,  X   X  } . We then use this set to match against the context of the word being disambiguated. As with the expansion of definitions via the definitions of their component words, this ontology-based expansion can take place in either a sense-sensitive man-ner (i.e., expanding out only the synonyms, hyponyms and hypernyms of a given sense) or sense-insensitive manner (i.e., expanding out all synonyms, hy-ponyms, and hypernyms for all senses of a given word). Note that we use only one level of lexical relations, that is, only direct hyponyms and hypernyms. 4.4 Other Extensions We also experimented with a number of other extensions to the proposed method, but found them to have relatively little impact so we don X  X  present detailed results here. These extensions included:  X  POS-based filtering of the tokens for word-based tokenization (e.g., treating verbal and demonstrative instances of the word  X  X  X  [ aru ] as distinct tokens); this was found to have very little impact on results, due to very few POS-differentiated homographs in Japanese.  X  Stop word filtering, using the same set of stop words as was used in Nichols et al. [2005]; this led to very slight increments in accuracy across the board (of the order of 0.001).  X  Different n -gram orders for both words and characters; this was found to improve results very marginally for character-based tokenization in isolated cases, but generally drove results down.  X  Different scoring metrics (e.g., cosine similarity and simple set overlap), but the Dice coefficient was found to be the best overall. 5. EVALUATION We evaluate our various extensions over two datasets: (1) the example sen-tences in the Hinoki Sensebank, and (2) the Senseval-2 Japanese dictionary task [Shirai 2002].

Each sense in the Hinoki Sensebank has one example sentence, and there-fore we are guaranteed to have at least one token instance of every sense in the sensebank. As stated above, all content words in the example sentences were sense annotated [Tanaka et al. 2006].

The Senseval-2 Japanese dictionary task is a LEXICAL SAMPLE task, which used the Iwanami Kokugo Jiten as its original sense inventory. We include this dataset for calibration purposes, in comparing our method with previously published results on Japanese WSD. As we do not have access to an ontology or sense-annotated definitions for the Iwanami Kokugo Jiten, however, we are not able to run our method over it directly. Instead, we used a re-annotated ver-sion of the Senseval-2 data based on Hinoki senses. Similarly to the example sentences, the Senseval-2 data was five-way manually sense annotated, and sense-arbitrated according to the majority annotation. Naturally, the sense granularity of Hinoki is not identical to that for the Iwanami Kokugo Jiten, and hence the sense distribution of our re-annotated data diverges from that for the original data. Fortunately, all target words in the Senseval-2 task were common enough that Hinoki had full coverage over them.
 All results below are reported in terms of simple accuracy.

For the two datasets, we use two baselines: a random baseline and the first-sense baseline. The former assigns one of the candidate senses randomly, and the latter always picks sense ID 1 for each word (i.e., the word sense with the most token occurrences within the Hinoki definitions). As such, random sense assignment is an unsupervised baseline, while first-sense assignment is a su-pervised baseline. We will use these values as a reference for our algorithm. 4: 14  X  T. Baldwin et al.
 Note that the (supervised) first-sense baseline has been shown to be hard to beat for unsupervised systems [Kilgarriff 2004; McCarthy et al. 2007], for ex-ample for  X  [ inu ], the first sense baseline has an accuracy of 99% over the 67 occurrences of  X  in the sensebank (both definition and example sentences). As a benchmark MRD-based WSD method, we reimplemented the original Banerjee and Pedersen [2003] method to work over the Hinoki data. As men-tioned above, our reimplementation is slightly different to that applied to the English WordNet data, in terms of the selection of ontological relations we use. 5.1 Hinoki Example Sentences The goal of these experiments is to tag all the words that occur in the example sentences in the Hinoki Sensebank.

In our first subexperiment, we experiment with the two tokenization strate-gies (characters [C HAR ] vs. words [W ORD ]) and the optional use of extended definitions (without ontological relations, and at the word rather than sense level). The results are presented in the top portion of Table II.

The first finding is that characters are in all cases superior to words as our segment granularity. This is a somewhat surprising result, given that the median word length is two characters, based on which it would not appear that words would generate excessive data sparseness.

Extended definitions are also shown to be superior to simple definitions, although the relative increment in making use of large amounts of sense an-notations is lesser than that of characters vs. words.

Note that at this point, our best-performing method is at the level of the unsupervised (random) baseline, and well below the supervised (first sense) baseline.

Having found that extended definitions improve results to a small degree, we next experiment with the inclusion of various ontological relations to ex-pand the original definitions. Here, we persevere with the use of word and characters, and experiment with the addition of synonyms, hypernyms and/or hyponyms, with and without the extended definitions (we don X  X  present results or all combinations of lexical relations for reasons of space). We further test the impact of the sense annotations, in rerunning our experiments with the ontol-ogy in a sense-insensitive manner, that is, by adding in the union of word-level hypernyms and/or hyponyms. The results are described in the bottom portion of Table II.

Adding in the ontology makes a considerable difference to our results, in line with the findings of Banerjee and Pedersen [2003]. Hyponyms are better dis-criminators than hypernyms (assuming a given word sense has a hyponym X  the Hinoki ontology is relatively flat), partly because while a given word sense will have (at most) one hypernym, it often has multiple hyponyms (if any at all). Adding in hypernyms or hyponyms, in fact, has a greater impact on re-sults than simple extended definitions (+extdef), especially for the word-based representation. The best overall results are produced for the combination of all ontological relations (i.e., extended definitions, hypernyms, and hyponyms), achieving an accuracy level above both the unsupervised (random) and super-vised (first-sense) baselines and the Banerjee and Pedersen [2003] method.
Looking at the sense-insensitive results, simple hyponyms (without ex-tended definitions) and word-based tokenization produced the best results out of all the variants tried, at an accuracy of 0.655, once again above both base-lines and the original Banerjee and Pedersen [2003] method. This compares with an accuracy of 0.683 achieved for the best of the sense-sensitive meth-ods, indicating that sense information in the definitions enhances WSD perfor-mance. This reinforces our expectation that richly annotated lexical resources improve performance, but also indicates that in the absence of sense annota-tions in the lexical resource, our method is still able to achieve highly compet-itive results. With richer information to work with, character-based methods uniformly give worse results.

In Table III, we present a breakdown of the results over the four main word classes (nouns, verbs, adjectives, and adverbs), with sense-sensitive lexical re-lation expansion. We observe a relatively consistent trend across the four word classes, in terms of words generally outperforming characters when we intro-duce the lexical relations, and the addition of extra lexical relations generally improving accuracy. We also observe a marked imbalance in both the baseline accuracies and performance levels across the different word classes. As com-monly observed for ALL WORDS WSD tasks, verbs are the hardest POS to dis-ambiguate and nouns the easiest. This is in terms of both the baselines and the relative boost in accuracy with our method (based on error rate reduction 13 ). In all cases, the lexical relation which produces the singular greatest increment in accuracy is hyponyms, although the effect is considerably less pronounced for adverbs than the other three word classes. Overall, more lexical relations 4: 16  X  T. Baldwin et al.
 generally lead to higher accuracy, but there are subtle differences across the word classes in terms of exactly what combination of lexical relations produces the best accuracy overall. 5.2 Senseval-2 Japanese Dictionary Task In our second set of experiments we apply our proposed method to the Senseval-2 Japanese dictionary task [Shirai 2002] in order to calibrate our re-sults against previously-published results for Japanese WSD. Recall that this is a LEXICAL SAMPLE task, and that our evaluation is relative to Hinoki re-annotations of the same dataset, although the relative polysemy for the origi-nal data and the re-annotated version is largely the same [Tanaka et al. 2006]. The first sense baselines for the two sets of annotations differ significantly, however, with an accuracy of 0.726 reported for the original task, and 0.577 for the re-annotated Hinoki variant.

In Table IV, we present the results over the Senseval-2 data for the best-performing systems from our earlier experiments. As before, we include results over both words and characters, and with sense-sensitive and sense-insensitive ontology expansion.

Our results largely mirror those of Table II. The best methods surpass the random and first sense baselines, as well as Banerjee and Pedersen [2003]. In this case, the synonyms have the greatest impact out of the three lexical relations, but the combination of all three lexical relations is the best overall performer. There is less difference between word and character tokenization than in our first experiment, and the relative impact of sense annotations was if anything even less pronounced than for the example sentence task.
We achieved our best accuracy with word tokenization, extended definitions, all lexical relations and sense-sensitivity, at an accuracy of 0.776. This rep-resents an error reduction rate of 47.0% over the first-sense baseline, and compares favorably with an error rate reduction of 21.9% for the best of the WSD systems in the original Senseval-2 task, based on  X  X ixed-grain X  senses [Kurohashi and Shirai 2001]. It is particularly impressive given that our method is semi-supervised while the Senseval-2 system is a conventional su-pervised word sense disambiguator. Even when we take away the sense-sensitivity, the error reduction rate drops back only fractionally to 44.4%.
In more recent work, Tanaka et al. [2007] use the Hinoki Sensebank to train a supervised classifier, which uses both the ontological information and seman-tic dependencies from a parser. The split into training and test is different from ours, so the results are not directly comparable. The most comparable results are where Tanaka et al. [2007] train on both definition and example sentences, and test on a held-out set of example sentences. Here, the first sense base-line is 0.704 (compared to 0.633 in our case, training on definition sentences and testing on example sentences), and the supervised system achieved an ac-curacy of 0.788, easily surpassing our best unsupervised results of 0.683 in terms of both raw accuracy and error rate reduction (28.4% vs. 13.6%). 6. DISCUSSION Over the Hinoki example sentence data, the use of the sense-tagged data to link senses in the ontology gave the best result overall at an accuracy of 0.683. 4: 18  X  T. Baldwin et al.
 This was using extended definitions, hypernym and hyponym links, and word tokenization. Without the sense information, the best result was 0.655 using only hyponym links. As can be expected, more precise information leads to a higher precision. The extra information does not, however, come for free: the ontology was automatically extracted, but underwent minor manual post-correction as errors were noticed, and the sense-links were manually anno-tated. If we were interested solely in WSD, the annotation could have been greatly reduced, as only the linked words (e.g., hypernyms, hyponyms, and synonyms) need to be disambiguated, accounting for roughly one third of the total. Alternatively, the annotation could have been performed automatically, as was done for the first version of Extended WordNet [Harabagiu et al. 1999], or just the hypernyms disambiguated, as in Rigau et al. [1997]. In our opinion, the 110 person days (per annotator) to sense tag the definition sentences was a small cost compared to the overall outlay to build the lexicon, and well justi-fied. In the case of WordNet, all links are relative to synsets 14 and hence sense specified, so using WordNet links is equivalent to our sense-sensitive system.
Comparing our method to Banerjee and Pedersen [2003], not only did we achieve higher accuracy over two separate datasets, but our method is compu-tationally cheaper, as we perform a single set intersection calculation rather than multiple calculations of the length of the longest common subsequence between glosses. It is important to point out that we did not attempt to opti-mize the parameterization of the Banerjee and Pedersen [2003] method over either of our datasets, in terms of the combinations of lexical relations or size of the context window. In this sense, more research could be carried out to thoroughly explore the impact of the parameter setting on the accuracy of the method.

Of character and word tokenization, word tokenization was found to be slightly superior when combined with ontological expansion, but in the ab-sence of an ontology, character tokenization consistently outperformed word tokenization. Combining this with the finding that the inclusion of ontological relations boosted accuracy considerably, the overall finding is that it is well worth inducing an ontology (e.g., using the automatic method of Nichols et al. [2005]), and that even in the absence of sense annotation, this will have the single greatest impact on the accuracy of WSD based on our proposed method. 7. CONCLUSION We proposed a new method for MRD-based word sense disambiguation based on definition expansion via an ontology, building on the work of Lesk [1986] and Banerjee and Pedersen [2003]. In this, we experimented with character-and word-based tokenization, definition extension based on the words in the original definition sentences, and a range of lexical relations. With the lexi-cal relations, we experimented with both sense-sensitive and sense-insensitive expansion, interpreting the lexical relations as linking either word senses or words, respectively. In doing so, we measured the contribution of sense-tagged definitions to the overall disambiguation performance. We evaluated our pro-posed method over two Japanese datasets: example sentences from the Hinoki Sensebank, and a retagged version of the Senseval-2 Japanese dictionary task. In making maximal use of the available lexical relations and definition exten-sion, we were able to surpass both unsupervised and supervised baselines for the two datasets, and the method of Banerjee and Pedersen [2003]. We also found that sensitising the lexical relations to word sense consistently improved the accuracy of our method, and that the method performed best over nouns.
One of the strengths of our method is that it can be applied equally to all words in running text, at an accuracy level higher than a first-sense baseline. This full-coverage system opens the way to further enhancements, such as the contribution of extra sense-tagged examples to the expansion, or the combina-tion of different WSD algorithms. In future work, we intend to integrate the WSD tool with other Japanese text processing applications, such as a cross-lingual glossing tool for learners of Japanese text [Yap and Baldwin 2007] and parse selection for Japanese grammars [Fujita et al. 2007].
 We wish to thank Hiromi Nakaiwa and Masaaki Nagata for their support and advice throughout this work.
 4: 20  X  T. Baldwin et al.

