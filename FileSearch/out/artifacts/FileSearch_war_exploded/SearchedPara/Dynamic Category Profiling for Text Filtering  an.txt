 Information is often represented in text form and classified into multiple categories for efficient browsing, retrieval, and dissemination. In such an information space, each category often contains several documents about a specific topic, and hence lots of documents may be entered at any time, but only a small portion of the documents may be classified into some categories. Therefore, text filtering (TF) and text classifi-cation (TC) should be integrated together to autonomously classify suitable docu-ments into suitable categories. 
One of the popular ways to achieve integrated TF and TC was to delegate a classi-fier to each category. The classifier was as sociated with a threshold, and upon receiv-ing a document, it could autonomously make a yes-no decision for the corresponding category. Conceptually, a document was  X  X ccepted X  by the classifier if its degree of acceptance (DOA) with respect to the category (e.g. similarity with the category or probability of belonging to the category) was higher than or equal to the correspond-ing threshold; otherwise it was  X  X ejected. X  With the help of the thresholds, TF was actually achieved in the course of TC. Each document could be classified into zero, one, or several categories. 
Unfortunately, perfect estimation of DOA values could not be expected [1] [7] [15], since no classifiers may be perfectly tuned. Therefore, a document that is be-lieved to be similar to (different from) a category could not always get a higher (lower) DOA value with respect to the category. Obviously, improper DOA estima-tions may heavily deteriorate the performance of both TF and TC. Traditionally, DOA values were often estimated in the space whose dimensions were specified by a set of features (keywords). Therefore, a document that gets a higher DOA value with re-spect to a category under a feature set may get a very low DOA value with respect to the category under another feature set. Feature selection is thus one of the most im-portant issues related to the tackling of improper DOA estimations. 
In this paper, we explore how various classifiers X  performances may be improved by employing more suitable features to distinguish relevant documents from non-relevant documents for each category. This goal differs from many previous related attempts, which aimed at improving the thresholding process (e.g. [7]) and the docu-ment selection process such as boosting [10], adaptive resampling ([4]), and query zoning [11]. The research result of the paper may be used to complement the previous techniques for integrated TF and TC. 
In the next section, we present an observation that provides significant hints to tackle the problem. Accordingly, we develop a novel approach DP4FC (D ynamic each category so that the performance of TC and TF may be improved (ref. Section 3). Empirical evaluation was conducted to evaluate DP4FC under different circum-stances (ref. Section 4). DP4FC was shown to be competent in helping the underlying classifier to achieve both better and stable performances in TF and TC. Feature selection, which is an important issues related to DOA estimations, was often an experimental issue in previous studies [8] [9] [14]. There was no standard guide-line to construct a perfect feature set. Some studies maintained an evolvable feature set covering all features currently seen (e.g. [2]). However, inappropriate features may introduce inefficiency [14] and poor performance [9] in TC. 
Moreover, even a feature set may be perfectly tuned to distinguish among the cate-gories, it is not necessarily suitable to filter out those documents not belonging to all the categories. This is due to the goal of feature selection: selecting those features that may be used to distinguish a category from others. Under such a goal, whether a fea-ture may be selected mainly depends on the content relatedness among the categories, without paying much attention to how the contents of a category c and a document d overlap with each other. If d ( c ) talks too much information not in c ( d ), d should not be classified into c , even though d mentions some content of c . To tackle the problem, features should be dynamically selected in response to each individual input docu-ment (rather than training documents in the categories). This task motivates the re-search in the paper. 
More specially, the observation suggests a dynamic profiling strategy to avoid mis-classifying a document d into a category c : (1) selecting those terms that have occur-rences in c but not in the document, and conversely (2) selecting those terms that have occurrences in the document but not in c . Therefore, each category should have a feature set, which is dynamic in the sense that it is reconstructed once a test document is entered. 
Dynamic profiling may complement the functionality of those classifiers that aim The profile is static in the sense that it is often composed of those terms that are dis-criminative for the categories, and hence does not vary for each input document. Dy-namic profiling complements the classifiers by considering another issue: how d ( c ) talks about those contents not in c ( d ). If d lacks important contents of c or talks much some discriminative contents of c . Based on the above analysis, we develop a dynamic profiling technique DP4FC improve the performances of integrated TF and TC. Figure 1 illustrates the introduc-tion of DP4FC to a classifier. In training, DP4FC joins the thresholding process, while in testing, DP4FC joins the process of making TF and TC decisions. 
Both the underlying classifier and DP4FC estimate each document X  X  DOA with re-spect to each category. The key point is that DOA values estimated by DP4FC are based on dynamic profiling, which aims to measure the extent to which a document X  X  content overlaps that of a category. The al gorithm is depicted in Table 1. Given a category c and a document d , it considers tow kinds of terms: those terms that occurs which is estimated by a modified tf  X  idf (term frequency  X  inverse document fre-quency) technique (ref. Steps 2.1 and 3.1). The term frequency is replaced by the terms in c ], and P( t | d ) is computed by [times t appears in d / total number of terms in d ]. On the other hand, the inverse document frequency (IDF) of t is modified to con-sider d as an additional training document, or more specially, IDF of t is computed by [(total number of training documents + 1) / number of documents (including d and training documents) in which t appears]. Therefore, a smaller DOA value indicates have a lower confidence to classify d into c , no matter whether c is the most suitable category for d or not. 
With the DOA estimation, DP4FC may join the thresholding process to help the underlying classifier to derive proper thresholds for each individual category. The basic idea is that, each category has two thresholds: one for thresholding the DOA values produced by DP4FC, while the other is for thresholding the original DOA values produced by the underlying classifier. The former helps to filter out those ir-relevant documents that would otherwise be noises for the setting the latter. The two thresholds work together in the hope to optimize the category X  X  performance in a predefined criterion (e.g. F 1 = [2PR] / [P+R]). Upon receiving a document to be filtered or classified, its two DOA values (i.e. by DP4FC and the underlying classifier) are produced, and the corresponding thresholds are consulted. The document may be classified into a category only if both DOA values are higher than or equal to their corresponding thresholds. That is, DP4FC and the underlying classifier actually work toge ther to complement each other to make proper TF and TC decisions. Time-complexity of dynamic profiling deserves analysis. The realization of DP4FC requires two main components: thresholding (conducted in training only) and DOA estimation (conducted in both training and testing, ref. Figure 1). As noted above, in thresholding, each document receives two DOA values, which are produced by DP4FC and the underlying classifier, respectively. Therefore, suppose a category has n documents used for thresholding, DP4FC needs to compute n  X  n combinations of DOA values. On the other hand, in DOA estimation, DP4FC needs to check those terms in category c but not in document d (ref. Step 2 in Table 1), and vice versa (ref. enough to realize the idea of dynamic profiling. Experiments were designed to investigate the contributions of DP4FC. To conduct objective and thorough investigation, DP4FC was evaluated under different circum-stances, including (1) different sources of experimental data, (2) different kinds of test data, (3) different settings of training data, and (4) different settings for the classifier. Table 2 summarizes the different circumstan ces, which are to be explained in the following subsections. 4.1 Experimental Data Experimental data came from Reuter-21578, which was a public collection for related studies (http://www.daviddlewis.com/resources/testcollections/reuters21578). There were 135 categories (topics) in the collection. We employed the ModLewis split, which skipped unused documents and separated the documents into two parts based on their time of being written: (1) the test set, which consisted of the documents after April 8, 1987 (inclusive), and (2) the training set, which consisted of the documents before April 7, 1987 (inclusive). The test set was further split into two subsets: (1) the in-space subset, which consisted of 3022 test documents that belong to some of the categories (i.e. fall into the category space), and (2) the out-space subset, which con-sisted of 3168 documents that belong to none of the categories. They helped to inves-tigate the systems X  performances in TC and TF, respectively. An integrated TF and TC system should (1) properly classify in-space documents, and (2) properly filter out out-space documents. 
As suggested by previous studies (e.g. [13]), the training set was randomly split validation) subset. The former was used to build the classifier (to be described later), while the latter was used to tune a threshold for each category. Therefore, to guaran-tee that each category had at least one doc ument for classifier building and one docu-ment for threshold tuning, we removed those categories that had fewer than 2 training documents, and hence 95 categories remained. Among the 95 categories, 12 catego-ries had no test documents. From both theoretical and practical standpoints, these categories deserve investigation [5], although they were excluded by several previous studies (e.g. [13]). After removing those documents to which no categories were as-signed (i.e. not belonging to any of the 95 categories), the training set contained 7780 documents. Moreover, since previous studies did not suggest the way of setting the documents for classifier building and threshold tuning, we will try different settings to conduct more thorough investigation: 50%-50% and 80%-20%, in which 2-fold and 5-fold cross validation were conducted, respectively. That is, 50% (80%) of the data was used for classifier building, and the remaining 50% (20%) of the data was used for threshold tuning, and the process repeated 2 (5) times so that each training docu-ment was used for threshold tuning exactly one time. 
Moreover, to test those out-space documents that are less related to the categories, we randomly sample 370 documents from a text hierarchy extracted from http://www.yahoo.com [6]. The documents were randomly extracted from the catego-ries of science , computers and Internet , and society and culture , and hence were less related to the content of the Reuters categor ies. With the help of the Yahoo out-space documents, we may measure the system X  X  TF performance in processing those out-space documents with different degrees of relatedness to the Reuters categories. 4.2 Evaluation Criteria The classification of in-space test documents and the filtering of out-space test docu-ments require different evaluation criteria. For the former, we employed precision (P) and recall (R). Both P and R were common evaluation criteria in previous studies. P was estimated by [total number of correct classifications / total number of classifica-number of correct classifications that should be made]. To integrate P and R into a single measure, the well-known F-measure was employed: F  X  = [(  X  2 +1)PR] / [  X  2 P+R], where  X  is a parameter governing the relative importance of P and R. As in many studies, we set  X  to 1 (i.e. the F 1 measure), placing the same emphasis on P and R. Note that P, R, and F 1 were  X  X icro-averaged X  rather than  X  X acro-averaged X . Macro-averaged F  X  was the average of the F  X  values for individual categories, where the F  X  value for a category c was computed based on precision and recall for c [13]. It was not employed in the experiment, since we included those categories that had no test documents (for the reasons noted above, ref. Section 4.1), making precision and recall values for these categories incomputable (since the denominators for computing the values could be zero). 
On the other hand, to evaluate the filtering of out-space test documents, we em-ployed two criteria: filtering ratio (FR) and average number of misclassifications for misclassified out-space documents (AM). FR was estimated by [number of out-space documents filtered out / number of out-space documents], while AM was estimated by [total number of misclassifications / number of out-space documents misclassified into the category space]. An integrated TF and TC system should reject more out-space documents (i.e. higher FR) and avoid misclassifying out-space documents into many categories (i.e. lower AM). As P and R, FR and AM complemented each other by focusing on different aspects. For example, suppose there are M out-space docu-ments, and system A misclassifies 1 out-space document into 2 categories, and system B misclassifies 2 out-space documents into 2 categories. Although both systems make 2 misclassifications, system A is better in FR ([M-1]/M vs. [M-2]/M), while system B is better in AM (2/1 vs. 2/2). FR and AM may thus support more in-depth comparison of system performances. 4.3 The Underlying Classifier Each category c was associated with a classifier, which was based on the Rocchio method (RO). Upon receiving a document d , the classifier estimated the similarity between d and c (i.e. DOA of d with respect to c ) in order to make a binary decision RO+DP4FC. By comparing the performances of RO and RO+DP4FC, we may iden-tify the contributions of DP4FC. (e.g. [3]). Some studies even showed that its performances were more promising in several ways (e.g. [6] [7]). RO constructed a vector for each category, and the similar-ity between a document d and a category c was estimated using the cosine similarity between the vector of d and the vector of c . More specially, the vector for a category c was constructed by considering both relevant documents and non-relevant documents documents (i.e. the documents not in c ). We set  X  1 =16 and  X  2 =4, since the setting was shown to be promising in previous studies (e.g. [12]). 
RO required a fixed (predefined) feature set, which was built using the documents for classifier building. The features were selected according to their weights, which were estimated by the  X  2 (chi-square) weighting technique. The technique has been shown to be more promising than others [14]. As noted above, there is no perfect way to determine the size of the feature set. Th erefore, to conduct more thorough investi-gation, we tried 5 feature set sizes, including 1000, 5000, 10000, 15000, and 20000 (there were about 20000 different features in the 2-fold training data). 
To make TF and TC decisions, RO also required a thresholding strategy to set a threshold for each category. As in many previous studies (e.g. [10] [13] [15]), RO tuned a relative threshold for each category by analyzing document-category similari-ties. The threshold tuning documents were used to tune each relative threshold. As suggested by many studies (e.g. [13]), the thresholds were tuned in the hope to opti-mize the system X  X  performance with respect to F 1 . 4.4 Result and Discussion Figure 2 illustrates the performance (in F 1 ) for in-space documents, while Figure 3 and Figure 4 illustrates the performance for out-space documents (FR and AM, re-spectively). The results indicate the following contributions provided by DP4FC: (1) For in-space documents, DP4FC helped RO to achieve better performances. As (2) For out-space documents from Reuters, DP4FC helped RO to achieve both better (3) For out-space documents from Yahoo, DP4FC helped RO to achieve both better (4) In the 2-fold experiment, even under the setting that leads RO to achieve the best (5) In the 5-fold experiment, even under the setting that leads RO to achieve the best Given an information space spanned by a set of categories, lots of documents may be entered at any time, but only a small portion of them may be classified into the infor-mation space. Misclassification of documents into the information space may deterio-rate the management, dissemination, and retrieval of information. We thus present a technique DP4FC to complement and enhance a classifier X  X  capability in mining cate-gory profiles. Instead of distinguishing a category from other categories, DP4FC category c ( d ), since in that case d could not be classified into c , even though d men-tions some discriminative content of c . To achieve that, DP4FC helps the underlying classifier to create dynamic category prof iles with respect to each individual docu-ment. It then works with the classifier to set proper thresholds, and accordingly make proper TF and TC decisions. Empirical results show that DP4FC may help the under-lying classifier to achieve both better and more stable performances. The contribu-tions are of both theoretical and practical si gnificance to the classification of suitable information into suitable categories. This research was supported by the National Science Council of the Republic of Chi-na under the grants NSC 94-2213-E-320 -001. 
