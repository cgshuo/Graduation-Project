 The L 1 Support Vector Machine ( L 1 -SVM or SVM for short) [1, 2, 3] is a powerful tech-nique for learning binary classifiers from examples. Given a training set { ( x i ,y i ) } m i =1 and a positive semi-definite kernel K , the SVM solution is a hypothesis of the form h ( x )= small.
 Our first concern is usually with the accuracy of the classifier. However, in some applications, the size of the support is equally important. Assuming that the kernel operator K can be evaluated in of
S . Therefore, a large support defines a slow classifier. Classification speed is often important and plays an especially critical role in real-time systems. For example, a classifier that drives a phoneme detector in a speech recognition system is evaluated hundreds of times a second. If this classifier does not manage to keep up with the rate at which the speech signal is acquired then its classifications are useless, regardless of their accuracy. The size of the support also naturally determines the amount of memory required to store the classifier. If a classifier is intended to run in a device with a limited memory, such as a mobile telephone, there may be a physical limit on the amount of memory available to store support vectors. The size of S may also effect the time required to train an SVM classifier. Most modern SVM learning algorithms are active set methods , namely, on every step of the training process, only a small set of active training examples are taken into account. Knowing the size of S ahead of time would enable us to optimize the size of the active set and possibly gain a significant speed-up in the training process.
 The SVM mechanism does not give us explicit control over the size of the support. The user-defined parameters of SVM have some influence on the size of S , but we often require more than this. number of support vectors used to define the SVM solution. In this paper, we address this issue and present budget-SVM , a minor modification to the standard L 1 -SVM formulation that allows the user to set a budget parameter. The budget-SVM optimization problem focuses only on the B worst-classified examples in the training set, ignoring all other examples.
 The problem of sparsity becomes even more critical when it comes to L 2 -SVM [3], a variant of the SVM problem that tends to have dense solutions. L 2 -SVM is sometimes preferred over L 1 -SVM tics [4]. We derive the budget-L 2 -SVM formulation by following the same technique used to derive budget-L 1 -SVM.
 The technique used to derive these SVM variants is as follows. We begin by generalizing the L 1 -SVM formulation by replacing the 1 -norm with an arbitrary norm. We obtain a general framework for SVM-type problems, which we nickname Any-Norm-SVM . Next, we turn to the K -method of norm interpolation to obtain the 1  X  X  X  interpolation-norm and the 2  X  X  X  interpolation-norm, and use these norms in the Any-Norm-SVM framework. These norms have the property that they depend only on the absolutely-largest elements of the vector. We rely on this property and show that our SVM variants construct sparse solutions. For each of these norms, we present a simple modification of the SMO algorithm [5], which efficiently solves the respective optimization problem. Related Work The problem of approximating the SVM solution using a reduced set of examples has received much previous attention [6, 7, 8, 9]. This technique takes a two-step approach: begin by training a standard SVM classifier, perhaps obtaining a dense solution. Then, try to find a sparse classifier which minimizes the L 2 distance to the SVM solution. A potential drawback of this ap-proach is that once the SVM solution has been found, the distribution from which the training set was sampled no longer plays a role in the learning process. This ignores the fact that shifting the SVM classifier by a fixed amount in different directions may have dramatically different consequences on classification accuracy. We overcome this problem by taking the approach of [10] and reformulating the SVM optimization problem itself in a way that promotes sparsity. Another technique used to obtain a sparse kernel-machine takes advantage of the inherent sparsity of linear programming solu-tions, and formalizes the kernel-machine learning problem as a linear program [11]. This approach, often called LP-SVM or Sparse-SVM , has been shown to generally construct sparse solutions, but still lacks the ability to introduce an explicit budget parameter. Yet another approach involves ran-domly selecting a subset of the training set to serve as support vectors [12]. The problem of learning a kernel-machine on a budget also appears in the online-learning mistake-bound framework, and it is there where the term  X  X earning on a budget X  was coined [13]. Two recent papers [14, 15] propose online kernel-methods on a budget with an accompanying theoretical mistake-bound.
 This paper is organized as follows. We present the generalized Any-Norm-SVM framework in Sec. 2. We discuss the K -method of norm interpolation in Sec. 3 and put various interpolation norms to use within the Any-Norm-SVM framework in Sec. 4. Then, in Sec. 5, we present some preliminary experiments that demonstrate how the theoretical properties of our approach translate into practice. We conclude with a discussion in Sec. 6. Due to the lack of space, some of the proofs are omitted from this paper. { X  1 , +1 } . Let K : X X X X  R be a positive semi-definite kernel, and let H be its corresponding Reproducing Kernel Hilbert Space (RKHS) [16], with inner product  X  ,  X  H . The L 1 Support Vector Machine is defined as the solution to the following convex optimization problem: where  X  is a vector of m slack variables, and C is a positive constant that controls the tradeoff between the complexity of the learned classifier and how well it fits the training data. The value of  X  i is sometimes referred to as the hinge-loss attained by the SVM classifier on example i . The 1 -norm, defined by  X  1 = m i =1 |  X  i | , is used to combine the individual hinge-loss values into a single number.
 L 2 -SVM is a variant of the optimization problem defined above, defined as follows: This formulation differs from the L 1 formulation in that the 1 -norm is replaced by the squared 2 -norm, defined by  X  2 2 = m i =1  X  2 i . In this section, we take this idea even farther, and allow the 1 -norm of L 1 -SVM to be replaced by any norm. Formally, let  X  be an arbitrary norm defined on R m . Recall that a norm is a real valued operator such that for every v  X  R m and  X   X  R it holds that  X  v = |  X  | v (positive homogeneity), v  X  0 and v =0 if and only if v =0 (positive definiteness), and that satisfies the triangle inequality. Now consider the following optimization problem: L 1 -SVM is recovered by setting  X  to be the 1 -norm. Setting  X  to be the 2-norm induces an not squared. Combining the positive homogeneity property of  X  with the fact that it satisfies the triangle inequality ensures that the objective function of Eq. (2) is convex.
 An important class of norms used extensively in our derivation is the family of p -norms, defined for defined by v  X  = lim p  X  X  X  v p and can be shown to be equivalent to max j | v j | . We also use the notion of norm duality. Every norm on R m has a dual norm which is also defined on R m . The dual norm of  X  is denoted by  X  and given by As its name implies,  X  also satisfies the requirements of a norm. For example, H  X  older X  X  inequal-ity [17] states that the dual of  X  p is the norm  X  q , where q = p/ ( p  X  1) . The dual of the 1 -norm is the  X  -norm and vice versa.
 Using the definition of the dual norm, we now state the dual optimization problem of Eq. (2): As a first sanity check, note that if  X  in Eq. (2) is chosen to be the 1 -norm, then  X  is the  X  -norm, and the constraint  X   X  C reduces to the familiar box-constraint of L 1 -SVM [3]. The proof that Eq. (2) and Eq. (4) are indeed dual optimization problems relies on basic techniques in convex analysis [18], and is omitted due to the lack of space. Moreover, it can be shown that the solution norm used. This allows us to forget about the primal problem in Eq. (2) and to focus on solving the dual problem in Eq. (4). As with L 1 -SVM, the bias term, b , cannot be directly extracted from the solution of the dual. The standard techniques used to find b in L 1 -SVM apply here as well [3]. We note that the Any-Norm-SVM formulation is not fundamentally different from the original L 1 -SVM formulation. Both optimization problems have convex objective functions and linear con-straints. More importantly, the only difference between their respective duals is in the dual-norm constraint. Specifically, the objective function in Eq. (4) is a concave quadratic function for any choice of  X  . These facts enable us to efficiently solve the problem in Eq. (4) for any kernel K and any norm using techniques similar to those used to solve the standard L 1 -SVM problem. In the previous section, we acquired the ability to replace the 1 -norm in the definition of L 1 -SVM with an arbitrary norm. We now use Peetre X  X  K -method of norm interpolation [19] to obtain norms that promote the sparsity of the generated classifier. The K -method is a technique for smoothly interpolating between a pair of norms. Let  X  p 1 : R m  X  R + and  X  p 2 : R m  X  R + be two p -norms, and let  X  q 1 and  X  q 2 be their respective duals. Peetre X  X  K -functional with respect to p and p 2 , and with respect to the constant t&gt; 0 , is defined to be The J -functional is obviously a norm: the properties of a norm all follow immediately from the fact that  X  q 1 and  X  q 2 posses these properties.  X  K ( p 1 ,p 2 ,t ) is also a norm, and moreover, elementary calculus, and this proof is omitted due to the lack of space.
 We use the K -method to interpolate between the 1 -norm and the  X  -norm, and to interpolate be-tween the 2 -norm and the  X  -norm. To gain some intuition on the behavior of these interpolation-m max i | v i | p , and therefore v  X   X  v p  X  m 1 /p v  X  . An immediate consequence of this is other words, the interesting range of t for the 1  X  X  X  interpolation-norm is [1 ,m ] , and for the 2  X  X  X  interpolation-norm is [1 , Next, we prove a theorem which states that interpolating a p -norm with the  X  -norm is approximately the 1  X  X  X  interpolation norm with parameter t (with t chosen to be an integer in [1 ,m ] ) is precisely Theorem 1. Let v be an arbitrary vector in R m and let  X  be a permutation on { 1 ,...,m } such i =1 | v  X  ( i ) | , and for any 1  X  p&lt;  X  ,if t = B 1 /p then it holds that Proof. Beginning with the lower bound, let w and z be such that w + z = v . Then where the first inequality is the triangle inequality for the p -norm. Since the above holds for any w and which defines v K ( p,  X  ,t ) . Therefore, we have that, Turning to the upper bound, let  X  = | v  X  ( B ) | , and define for all 1  X  i  X  m ,  X  w i = for p =1 . Moving on to the case of an arbitrary p , we have that Since the absolute value of each element in  X  w is at most as large as the absolute value of the corresponding element of v , and since  X  w  X  ( r +1) = ... =  X  w  X  ( m ) =0 , we have that  X  w p  X  Our first concrete algorithm is budget-L 1 -SVM, obtained by plugging the 1  X  X  X  interpolation-norm with parameter B into the general Any-Norm-SVM framework. Relying on Thm. 1, we know that this norm takes into account only the B largest values in  X  . Since  X  measures how badly each example is misclassified, the budget-L 1 -SVM problem essentially optimizes the soft-margin with respect to the B worst-classified examples. We now show that this property promotes the sparsity of the budget-L 1 -SVM solution.
 immediately imply that the number of support vectors is less than B . This holds true for every instance of the Any-Norm-SVM framework, and is proven for L 1 -SVM in [3]. Therefore, we focus on the more interesting case, where y i ( f ( x i )+ b ) &lt; 1 for at least B examples. of the primal problem in Eq. (2) and the dual problem in Eq. (4), where  X  is chosen to be the 1  X  X  X  interpolation-norm with parameter B . Define  X  i = y i ( f ( x i )+ b ) and let  X  be a permutation of Proof. We begin the proof by redefining  X  i = max { 1  X   X  i , 0 } for all 1  X  i  X  m and noting that ( f, b,  X  ,  X  ) remains a primal-dual solution to our problem. The benefit of starting with this  X   X   X  also a primal-dual solution to our problem. Moreover, we know that 1  X   X  k &lt; X  k . Using the KKT complementary slackness condition, it follows that  X  k , the Lagrange multiplier corresponding to this constraint, must equal 0 .
 Defining  X  i and  X  as above, a simple corollary of Thm. 2 is that the number of support vectors is upper bounded by B in the case that  X   X  ( B ) =  X   X  ( B +1) .
 From our discussion in Sec. 3, we know that the dual of the 1  X  X  X  interpolation-norm is the func-tion max { u  X  , (1 /B ) u 1 } . Plugging this definition into Eq. (4) gives us the dual optimiza-tion problem of budget-L 1 -SVM. The constraint  X   X  C simplifies to  X  i  X  C for all i and i =1  X  i  X  BC . To numerically solve this optimization problem, we turn to the Sequential Mini-mal Optimization (SMO) [5] technique. We briefly describe the SMO technique, and then discuss its adaptation to our setting. SMO is an iterative process, which on every iteration selects a pair of dual variables,  X  k and  X  l , and optimizes the dual problem with respect to them, leaving all other variables fixed. The choice of the two variables is determined by a heuristic [5], and their optimal values are calculated analytically. Assume that we start with a vector  X  which is a feasible point of the optimization problem in Eq. (4). When restricted to the two active variables,  X  k and  X  l , the can slightly overload our notation and define the linear functions and find the single variable  X  which maximizes our constrained optimization problem. Since the the single variable  X  , takes the form O (  X  )= P X  2 + Q X  + c , where c is a constant, function in Eq. (4) with respect to  X  k and  X  l is equivalent to maximizing O (  X  ) with respect to  X  over an interval. P equals minus the Euclidean distance between the functions K ( x k ,  X  ) and which attains a single (unconstrained) maximum. This maximum can be found analytically by optimum. Otherwise, the constrained optimum falls on one of the two end-points of the interval. Thus, we are left with the task of finding these end-points. To do so, we consider the remaining constraints: The constraints in ( I ) translate to The constraints in ( II ) translate to Constraint ( III ) translates to Finding the end-points of the interval that confines  X  amounts to finding the smallest upper bound and the greatest lower bound in Eqs. (10,11,12). This concludes the analytic derivation of the SMO update for budget-L 1 -SVM.
 L2-SVM on a budget Next, we use the 2  X  X  X  interpolation-norm with parameter t = the Any-Norm-SVM framework, and obtain the budget-L 2 -SVM problem. Thm. 1 hints that setting t = B elements in the vector  X  . The support size of the budget-L 2 -SVM solution is strongly correlated with the parameter B although the exact relation between the two is not as clear as before. Again we begin with the dual formulation defined in Eq. (4), where the constraint  X   X  C becomes max {  X  2 , (1 / a convex and bounded feasible set, and its intersection with the linear equalities in Eq. (8) defines an interval. The objective function in Eq. (4) is the same as before, so the unconstrained maximum is once again given be Eq. (9). To obtain the constrained maximum, we must find the end-points of the interval that confines  X  . The dual-norm constraint can be written more explicitly as of  X  by replacing B with  X  2 +  X  X  +  X   X  0 , where  X  =  X  k y k  X   X  l y l and  X  = 1 explicitly as In addition, we still have the constraint  X   X  0 , which is common to every instance of the Any-Norm-SVM framework. This constraint is given in terms of  X  in Eq. (10). Overall, the end-points of the interval we are searching for are found by taking the smallest upper bound and the greatest lower bound in Eqs. (10,13) and Eq. (12) with B replaced by Figure 1: Average test error of budget-L 1 -SVM (left) and budget-L 2 -SVM (right) for different values of the budget parameter B and the pruning parameter s (all but s weights in  X  are set to zero). The test error in the darkest region is roughly 50%, and in the lightest region is roughly 5%. Many existing solvers for the standard L 1 -SVM problem define a positive threshold value close to zero and replace every weight that falls below this threshold with zero. This heuristic significantly reduces the time required for the algorithm to converge. In our setting, a more natural way to speed up the learning process is to run the iterative SMO optimization algorithm for a fixed number of iterations and then to keep only the B largest weights, setting the m  X  B remaining weights to zero. This pruning heuristic enforces the budget constraint in a brute-force way, and can be equally applied to any kernel-machine. However, the natural question is how much will the pruning heuristic up to its theoretical promise, we expect the pruning heuristic to have little impact on classification accuracy. On the other hand, if we train an L 1 -SVM and it so happens that the number of large weights exceeds B , then applying the pruning heuristic should have a dramatic negative effect on classification accuracy. The goal of our experiments is to demonstrate that this behavior indeed occurs in practice.
 We conducted our experiments using the MNIST dataset, which contains handwritten digits from the 10 digit classes. We randomly generated 50 binary classification problems by first randomly partitioning the 10 classes into two equally sized sets, and then randomly choosing a training set of 1000 examples and a test set of 4000 examples. The results reported below are averaged over these 50 problems. Although MNIST is generally thought to induce easy learning problems, the method described above generates moderately difficult learning tasks.
 For each binary problem, we trained both the L 1 and the L 2 budget SVMs with B = 20 , 40 ,..., 1000 . Note that  X  K (1 ,  X  ,B ) grows roughly linearly with B , and that  X  grows roughly like the square root of B . To compensate for this, we set C =10 /B in the L 1 case and C =10 / of the regularization term with respect to the norm term in Eq. (2), across the various values of B . In all of our experiments, we used a Gaussian kernel with  X  =1 (after scaling the data to have an average unit norm). For each classifier trained, we pruned away all but the s largest weights, with s =20 , 40 ,..., 1000 , and calculated the test error. The average test error for every choice of B (the budget parameter in the optimization problem) and s (the number of non-zero weights kept) is summarized in Fig. 1. In practice, s and B should be equal, however we let s take different values in our experiment to illustrate the characteristics of our approach. Note that the test-error attained by L 1 -SVM (without a budget parameter) and L 2 -SVM are represented by the top-right corners of the respective plots.
 As expected, classification accuracy for any value of B deteriorates as s becomes small. However, the accuracy attained by L 1 -SVM and L 2 -SVM can be equally attained using significantly less support vectors. Using the Any-Norm-SVM framework with interesting norms enabled us to introduce a budget parameter to the SVM formulation. However, the Any-Norm framework can be used for other tasks as well. For example, we can interpolate between L 1 -SVM and L 2 -SVM by using the 1  X  2 interpolation-norm. This gives the user the explicit ability to balance the trade-off between the pros and cons of these two SVM variants. In [20] it is shown that there exists a constant c such that, These bounds give some insight into how such an interpolation would behave. Another possible norm that can be used in our framework is the Mahalanobis norm ( v =( v M v ) 1 / 2 , where M is a positive definite matrix), which would define a loss function that takes into account pair-wise relationships between examples.
 Regarding our experiments, the rule-of-thumb we used to choose the parameter C is not always optimal. It seems preferable to tune C individually for each B using cross-validation. We are currently exploring extensions to our SMO variant that would quickly converge to the sparse solution without the help of the pruning heuristic. We are also considering multiplicative update optimization algorithms as an alternative to SMO.

