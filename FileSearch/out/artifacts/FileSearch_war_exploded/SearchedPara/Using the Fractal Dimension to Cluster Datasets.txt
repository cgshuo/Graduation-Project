 Clustering is a widely used kno wledge disco v ery tec hnique. It helps unco v ering structures in data that w ere not previ-ously kno wn. The clustering of large data sets has receiv ed a lot of atten tion in recen t y ears, ho w ev er, clustering is a still a c hallenging task since man y published algorithms fail to do w ell in scaling with the size of the data set and the n um b er of dimensions that describ e the p oin ts, or in nding arbitrary shap es of clusters, or dealing e ectiv ely with the presence of noise. In this pap er, w e presen t a new cluster-ing algorithm, based in the fractal prop erties of the data sets. The new algorithm whic h w e call F ractal Clustering (F C) places p oin ts incremen tally in the cluster for whic h the c hange in the fractal dimension after adding the p oin t is the least. This is a v ery natural w a y of clustering p oin ts, since p oin ts in the same cluster ha v e a great degree of self-similarit y among them (and m uc h less self-similarit y with resp ect to p oin ts in other clusters). F C requires one scan of the data, is susp endable at will, pro viding the b est an-sw er p ossible at that p oin t, and is incremen tal. W e sho w via exp erimen ts that F C e ectiv ely deals with large data sets, high-dimensionalit y and noise and is capable of recognizing clusters of arbitrary shap e.
 I.5.3 [ Computing Metho dologies ]: P attern Recognition| Clustering F ractals Clustering is one of the most widely used tec hniques in data mining. It is used to rev eal structure in data that can b e extremely useful to the analyst. The problem of clustering is to partition a data set consisting of n p oin ts em b edded in
This w ork has b een supp orted b y NSF gran t I IS-9732113 a d -dimensional space in to k sets or clusters, in suc haw a y that the data p oin ts within a cluster are more similar among them than to data p oin ts in other clusters. A precise de -nition of clusters do es not exist. Rather, a set of functional de nitions ha v e b een adopted. A cluster has b een de ned [1] as a set of en tities whic h are alik e (and di eren t from en-tities in other clusters), an aggregation of p oin ts suc h that the distance bet w een an y p oin t in the cluster is less than the distance to p oin ts in other clusters, and as a connected region with a relativ ely high densit y of p oin ts. Our metho d adopts the rst de nition (lik eness of p oin ts) and uses a fractal prop ert y to de ne similarit ybet w een p oin ts. The area of clustering has receiv ed an enormous atten tion as of late in the database comm unit y . The latest tec hniques try to address pitfalls in the traditional clustering algorithms (for a go o d co v erage of traditional algorithms see [5]). These pitfalls range from the fact that traditional algorithms fa v or clusters with spherical shap es (as in the case of the cluster-ing tec hniques that use cen troid-based approac hes), are v ery sensitiv e to outliers (as in the case of all-p oin ts approac hto clustering, where all the p oin ts within a cluster are used as represen tativ e of the cluster), or are not scalable to large data sets (as is the case with all traditional approac hes). In this pap er w e prop ose a clustering algorithm that pro vides av ery natural w a y of de ning clusters that is not restricted to an y particular cluster shap e. This algorithm is based on the use of the fractal dimension, and clusters p oin ts in suc ha w a y that data p oin ts in the same cluster are more self-ane among themselv es than to p oin ts in other clusters. (Notice that this do es not mean the clusters ha v e to b e fractal data sets themselv es.) Nature is lled with examples of phenomena that exhibit seemingly chaotic b eha vior, suc h as air turbulence, forest res and the lik e. Ho w ev er, under this b eha vior it is al-most alw a ys p ossible to nd self-similarity , i.e. an in v ari-ance with resp ect to the scale used. The structures that app ear as a consequence of self-similarit y are kno wn as frac-tals [7]. F ractals ha v e b een used in n umerous disciplines (for a good co v erage of the topic of fractals and their ap-plications see [10]). F ractal sets are c haracterized b y their fractal dimension. (In truth, there exists an in nite family of fractal dimensions.) By em b edding the data set in an n -dimensional grid whic h cells ha v e sides of size r ,w e can compute the frequency with whic h data p oin ts fall in to the i -th cell, p i , and compute the fractal dimension [3, 4]. The
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 traditional w a y to compute fractal dimensions is b y means of the b o x-coun ting plot. F or a set of N p oin ts, eac hof D dimensions, one divides the space in grid cells of size r (h y-p ercub es of dimension D ). If N ( r ) is the n um ber of cells o ccupied b y p oin ts in the data set, the plot of N ( r )v ersus r in log-log scales is called the b ox-c ounting plot . The negativ e v alue of the slop e of that plot corresp onds to the Hausdor fractal dimension D 0 . Similar pro cedures are follo w ed to compute other dimensions, as describ ed in [6].
 This pap er is organized as follo ws. Section 2 describ es our tec hnique. Section 3 summarizes exp erimen tal results that w e ha v e obtained using our tec hnique. Finally , Section 4 o ers conclusions and guidelines for future w ork. Incremen tal clustering using the fractal dimension (abbrevi-ated as F ractal Clustering for short), is a form of grid-based clustering (where the space is divided in cells b y a grid; other tec hniques that use grid-based clustering are STING [12], W a v eCluster [11] and Hierarc hical Grid Clustering [9]). The main idea b ehind F C is to group p oin ts in a cluster in suc haw a y that none of the p oin ts in the cluster c hanges the cluster's fractal dimension radically . After initializing a set of clusters, our algorithm incremen tally adds p oin ts to that set. In what follo ws, w e describ e the initialization and incremen tal steps. Ob viously , b efore w e can apply the main concept of our tec h-nique, i.e., adding p oin ts incremen tally to existing clusters, based on ho w they a ect the clusters' fractal dimension, some initial clusters are needed. In other w ords, w e need to \b o otstrap" our algorithm via an initialization pro cedure that nds a set of clusters, eac h with sucien t p oin ts so its fractal dimension can b e computed. W e presen t in this sec-tion one initialization algorithm. (Although w eha v e tried sev eral algorithms for this step, space constrain ts limit us to sho wing one of the c hoices here.) The initialization algo-rithm uses a traditional distance-based pro cedure. W e try to cluster the initial sample of p oin ts b y taking p oin ts at ran-dom and nding recursiv ely p oin ts that are close to them. When no more close p oin ts can b e found, a new cluster is initialized, c ho osing another p oin t at random out of the set of p oin ts that ha v e not b een clustered y et. Figure 1 presen ts the pseudo co de for this algorithm.
 Giv en a sample of p oin ts (whic h ts in main memory) and a distance threshold (Line 1), the algorithm pro ceeds to build clusters b y pic king a random y et unclustered p oin t and recursiv ely nding the nearest neigh b or in suc ha w a y that the distance b et w een the p oin t and the neigh b or is less than . The neigh b or is then included in the cluster, and the searc h for nearest neigh b ors con tin ues in depth-rst fashion, un til no more p oin ts can b e added to clusters. Notice that w e do not try to restrict the clusters b y constraining it, as it is customarily done, to ha v ea diameter less than or equal to a certain threshold, since w e w an t to be able to nd arbitrarily shap ed clusters.
 The threshold, is the pro duct of a prede ned, static pa-rameter 0 , and ^ d , the a v erage distance bet w een pairs of p oin ts already in the cluster. Th us, is a dynamic thresh-1.Giv en an initial sample S of p oin ts f p 1 ; ;p M g that t in main memory , and a distance threshold . (Initially = 0 .) 2. Mark all p oin ts as unclustered, and mak e k = 0 3. Randomly c ho ose a p oin t P out of the set of unclustered p oin ts 12. If there are still unclustered p oin ts, mak e k = k + 1 and go to 3.
 NEAR ( P; ) Find the nearest neigh bor of P suc h that dist ( P 0 ;P ) .
 If no suc h P 0 can b e found return NULL Otherwise return P 0 .
 old that gets up dated as p oin ts are included in the clus-ter. In tuitiv ely , w e are trying to restrict the mem b ership to an existing cluster to p oin ts whose minim um distance to a mem ber of the cluster is similar to the a v erage distance bet w een p oin ts already in the cluster. Eac h cluster found b y the initialization step is represen ted b y a set of b o xes (cells in a grid). Eac hbo x in the set records its p opulation of p oin ts. Let k be the n um ber of clusters found in the initialization step, and C = f C 1 ;C 2 ;:::;C where C i is the set of bo xes that represen t cluster i . Let F ( C i ) b e the fractal dimension of cluster i .
 The incremen tal step brings a new set of p oin ts to main memory and pro ceeds to tak e eac h p oin t and add it to eac h cluster, computing its new fractal dimension. The pseudo-co de of this step is sho wn in Figure 2. Line 5 computes the fractal dimension for eac h mo di ed cluster (adding the p oin t to it). Line 6 nds the prop er cluster to place the p oin t b y computing the minimal fr actal imp act , i.e., the minimal c hange in fractal dimension exp erienced b yan y of the cur-ren t clusters when the new p oin t is added. Line 7 is used to discriminate \noise." If a p oin t causes a c hange in the fractal dimension (of its b est c hoice for a cluster) whic his bigger than a threshold , then the p oin t is simply rejected as noise (Line 8). Otherwise, it is included in that cluster. W ec ho ose to use the Hausdor dimension, D 0 , for the frac-1.Giv en a batc h S of p oin ts brough t to main memory: 2.F or eac h p oin t p 2 S : tal dimension computation of Line 5 in the incremen tal step. W ec hose D 0 since it can b e computed faster than the other dimensions and it pro v es robust enough for the task. T o compute the fractal dimension of the clusters ev ery time a new p oin t is added to them, w ek eep the cluster informa-tion using a series of grid represen tations, or la y ers. In eac h la y er, b o xes (i.e., grids) ha v e a size that is smaller than in the previous la y er. The sizes of the b o xes are computed in the follo wing w a y . F or the rst la y er (largest b o xes), w e di-vide the cardinalit y of eac h dimension in the data set b y2, for the next la y er, w e divide the cardinalit y of eac h dimen-D -dimensional b o xes in eac hla y er, where D is the dimen-sionalit y of the data set, and L the maxim um la y er w e will store. Then, the information k ept is not the actual lo cation of p oin ts in the b o xes, but rather, the n um b er of p oin ts in eac h bo x. It is imp ortan t to remark that the n um ber of bo xes in la y er L can gro w considerably , sp ecially for high-dimensionalit y data sets. Ho w ev er, w e need only to sa v e bo xes for whic h there is an y p opulation of p oin ts, i.e., empt y bo xes are not needed. The n um ber of p opulated b o xes at that lev el is, in practical data sets, considerably smaller (that is precisely wh y clusters are formed, in the rst place). Let us denote b y B the n um b er of p opulated b o xes in lev el L . Notice that, B is lik ely to remain v ery stable throughout passes o v er the incremen tal step.
 Ev ery time a p oin t is assigned to a cluster, w e register that fact in a table, adding aro w that maps the cluster mem-b ership to the p oin t iden ti er (ro ws of this table are p eri-o dically sa v ed to disk, eac h cluster in to a le, freeing the space for new ro ws). The arra yof la y ers is used to driv e the computation of the fractal dimension of the cluster, using a bo x-coun ting algorithm. In particular, w ec hose to use FD3 [8], an implemen tation of a bo x coun ting algorithm based on the ideas describ ed in [6].
 It is p ossible that the n um b er and form of the clusters ma y c hange after ha ving pro cessed a set of data p oin ts using the step of Figure 2. This ma y o ccur b ecause the data used in the initialization step do es not accurately re ect the true distribution of the o v erall data set or b ecause w e are clus-tering an incoming stream of data. T o address this issue, w e ha v e devised t w o op erations: splitting a cluster and merging t w o or more clusters in to one. F urther details can b e found in [2]. In this section w e will sho w the results of some exp erimen ts using F C to cluster a series of data sets. (F urther exp eri-men ts can b e found in [2].) Eac h data set aims to test ho w w ell F C doesineac h of the issues w eha v e discussed in the Section 2. F or eac h one of the exp erimen ts w eha v e used a v alue of = 0 : 03 (the threshold used to decide if a p oin t is noise or it really b elongs to a cluster). W e p erformed the exp erimen ts in a Sun Ultra2 with 500 Mb. of RAM, run-ning Solaris 2.5. In eac h of the exp erimen ts, the p oin ts are distributed equally among the clusters (i.e., eac h cluster has the same n um b er of p oin ts). After w e run F C, for eac h clus-ter found, w e coun t the n um b er of p oin ts that w ere placed in that cluster and that also b elonged there. The accuracy of F C is then measured for eac h cluster as the p ercen tage of p oin ts correctly placed there. (W e kno w, for eac h data set, the mem b ership of eac h p oin t; in one of the data sets w e spread the space with outliers: in that case, the outliers are considered as b elonging to an extra \cluster.") In one exp erimen t, w e used data sets whose distribution fol-lo ws the one sho wn in Figure 3. W ev ary the total n um ber of p oin ts in the data set to measure the p erformance of our clustering algorithm. In ev ery case, w e pic k a sample of 600 p oin ts to run the initialization step. The results are sum-marized in Figure 4. The n um b er in the column \time" is the running time. The running time go es from 12 sec. (7 seconds for initialization and 5 for the incremen tal step) for the 30,000 p oin ts data set to 4,987 sec. ( 4,980 sec. for the incremen tal step and 7 seconds for initialization) in the case of 30 million p oin ts. The incremen tal step gro ws linearly from 5 seconds in the 30,000 p oin ts case to 4,980 seconds in the 30 million p oin t data set. The gure sho ws that the memory tak en b y our algorithm is constan t for the en tire range of data sets (64 Kb ytes). Finally , the gure sho ws the comp osition of the clusters found b y the algorithm, indicat-ing the n um b er of p oin ts and their pro cedence: whether they actually b elong to cluster1, cluster2 or cluster3. (Since w e kno w to whic h one of the rings of Figure 3 eac h p oin t really b elongs). Because b oth of cluster 2 and 3 are rings, they ha v e close fractal dimension, and some p oin ts of cluster 3 are misplaced. These gures are a measure of the qualit yof the clusters found b yF C. The qualit y of the clusters found b y using the t w o initialization algorithms is v ery similar, so w e only rep ort it once.
 Some other exp erimen ts, whose results are rep orted in [2], can be brie y describ ed as follo ws. An exp erimen t that used the data set sho wn in Figure 3 augmen ted with 5% of noise (p oin ts that lie outside the clusters), sho ws that F C is extremely resistan t to noise, since it correctly iden ti ed 91.72 % of the noisy p oin ts as outliers. Exp erimen ts with data sets suc h as the one sho wn in Figure 5 demonstrated that F C is capable of iden tifying v ery arbitrarely-shap ed clusters.
 W e also w an ted to compare F C with t w o previously pub-lished algorithms: CURE and BIR CH. W e sho w (for reasons of space) only one of the comparisons w e p erformed. (F ur-
N time memory found to cluster1 cluster2 cluster3 30,000 12s. 64 Kb. 1 10,326 9,972 0 354 99.72 % 300,000 56s. 64 Kb. 1 103,331 99,868 0 3,463 99.86% 3,000,000 485s. 64 Kb. 1 1,033,795 998,632 0 35,163 99.86% 30,000,000 4,987s. 64 Kb. 1 10,335,024 9,986,110 22 348,897 99.86% of p oin ts that w ere correctly put in eac h cluster. Figure 3: Three-cluster data set used for scalabilit y exp erimen ts. ther comparisons can b e found in [2].) W e tried the data set of Figure 5 with 20,000 p oin ts for a comparison exp erimen t whose results are sho wn in Figure 6. CURE declares 6,226 (31.13 %) p oin ts as outliers, placing the rest in the righ t clusters, while F C places all the p oin ts correctly . F or this larger set, F C's running time clearly outp erforms CURE's. In this pap er w e presen ted a new clustering algorithm based on the usage of the fractal dimension. This algorithm clus-ters p oin ts according to the e ect they ha v e on the fractal dimension of the clusters that ha v e b een found so far. The algorithm is, b y design, incremen tal and its complexit yis Algorithm time found cluster cluster1 cluster2
CURE 2,520s 1 4,310 4,310 0 43.10 %
F C 14s. 1 10,000 10,000 0 100 %
BIR CH 3.89 s. 1 9,654 9,654 0 96.5 % O ( N ), where N is the size of the data set (th us, it requires only one pass o v er the data, with the exception of cases in whic h splitting of clusters needs to b e done).
 Our exp erimen ts ha v e pro v en that the algorithm has v ery desirable prop erties. It is resistan t to noise, capable of nd-ing clusters of arbitrary shap e and capable of dealing with p oin ts of high dimensionalit y .
 W e ha v e also sho wn that our F C algorithm compares fa-v orably with other algorithms suc h as CURE and BIR CH, obtaining b etter clustering qualit y . As the data sets get big-ger, F C is able to outp erform BIR CH and CURE in running time as w ell.
 Although not sho wn here, w e ha v e successfully p erformed exp erimen ts with high-dimensional data sets to test F C's abilit y to scale with dimensions [2]. One p oin tw orth men-tioning is that as the dimensionalit y gro ws, the memory de-mands of F C can gro w bey ond the a v ailable memory . T o cop e with this, w eha v e devised t w o memory reduction tec h-niques [2]. The rst one is based on cac hing b o xes in memory (w eha v e disco v ered that the most ecien tw a y to cac he is to k eep the last la y er of bo xes (higher gran ularit y bo xes) in memory and to bring the rest of the la y ers on demand). The second one is based on discarding lo w-p opulated b o xes. Both deal v ery e ectiv ely with the memory gro wth: while they k eep the memory usage at a reasonable lev el, their im-pact on qualit y and p erformance is lo w. (The rst memory reduction tec hnique has no impact on qualit y , but it do es impact the p erfomance of the algorithm, while the second memory tec hique impact is mainly on the qualit y of the clus-ters found.) W e lik e to thank Vipin Kumar and Eui-Hong (Sam) Han for lending us their implemen tation of CURE. W e also lik eto thank Ragh u Ramakrishnan and V enk atesh Gan ti for their BIR CH co de, and for sp ending long hours helping us in the use of BIR CH for our exp erimen ts. [1] E. Bac k er. Computer-Assiste dRe asoning in Cluster [2] D. Barbar a and P . Chen. Using the Fractal Dimension [3] P . Grassb erger. Generalized Dimensions of Strange [4] P . Grassb erger and I. Pro caccia. Characterization of [5] A. Jain and R. C. Dub es. A lgorithms for Clustering [6] L. Lieb o vitc h and T. T oth. A Fast Algorithm to [7] B. Mandelbrot. The Fr actal Ge ometry of Natur e . [8] J. Sarraille and P . DiF alco. FD3. [9] E. Sc hikuta. Grid clustering: An ecien t hierarc hical [10] M. Sc hro eder. F r actals, Chaos, Power Laws: Minutes [11] G. Sheikholeslami, S. Chatterjee, and A. Zhang. [12] W. W ang, J. Y and, and R. Mun tz. STING: A
