 1. Introduction
The increasing demand for access to different types of information available online have led researchers to a renewed summarization, multimedia retrieval, chemical and biological informatics, text structuring, and text mining. The existing document retrieval systems cannot satisfy the end-users X  information need to have more direct access into relevant docu-ments. Question Answering (QA) systems can address this challenge effectively ( Strzalkowski &amp; Harabagiu, 2006 ). The human variant of the QA systems were used effectively over the years. One such system is the popular QA system in Korea, the Korean Naver X  X  Knowledge iN search, 1 which allows users to ask almost any question and get answers from other users ( Chali, Joty, &amp; Hasan, 2009 ). Another widely known QA service is Yahoo! Answers market website launched by Yahoo!. As of December 2009, Yahoo! Answers had 200 million users worldwide and more than  X  1 billion answers. 3 Furthermore, Google launched a QA system This research is a small step towards such an ambitious goal.

QA research can handle different types of questions: fact, list, definition, how, why, etc. Some questions, which we call often require multiple types of information. For example, the question:  X  X  X ow was Japan affected by the earthquake? X  X  sug-Lacatusu, &amp; Hickl, 2006 ).

Multi-document summarization can be used to describe the information of a document collection in a concise manner both the Question Answering (QA) and Multi-document Summarization (MDS) communities ( Carbonell, Harman, Hovy, the 2005 Text Retrieval Conference (TREC) Relationship QA Task, date answers in response to a complex question. The MDS evaluations (including the 2005, 2006 and 2007 Document Under-ful in the domain of document management and search systems. For example, it can provide personalized news services for a single event from different sources in the form of a summary containing multiple perspectives at the same time.
This paper is concerned with automatic answering of complex questions. We define the complex questions as the kind of questions whose answers need to be obtained from pieces of information scattered in different documents. Our experiments and evaluations were mainly influenced by the specific scenario proposed by the DUC (2005 X 2007) tasks. In fact, DUC pro-poses a query-focused summarization task whose features have allowed us to simulate our experiments with complex ques-tion answering. Hence, the considered complex questions are the type of questions that request information such as an about an aspect or term or procedure. We use an extractive plex question answering task.

Effective complex question answering can aid to the improvement of the search systems. When a user searches for some 2007 ). Zaragoza, Cambazoglu, and Baeza-Yates (2010) performed a quantitative analysis about what fraction of the web learning formulation to the complex question answering task so that the system can learn from user interaction to improve its accuracy according to user X  X  information need.

Formally, the complex question answering problem can be mapped to a reinforcement learning framework as follows: given a set of complex questions, a collection of relevant documents the given human-made abstract summary sentences using a reward function. This assumption relies on the intuition that the summary sentences are assigned good reward values. In reinforcement learning, the learner is not aware of which actions &amp; Barto, 1998 ).

Real-time user interaction can help QA systems evolve by improving their policy automatically as time passes. Toward this end, we treat the complex question answering task as an interactive problem. Supervised learning techniques are alone not adequate for learning from interaction ( Sutton &amp; Barto, 1998 ). These techniques require a huge amount of has to act. Instead, a reinforcement learning approach can be used to sense the state of the environment and take suitable defines the quality of the executed actions. In the training stage, the reinforcement learner repeatedly defines action the expected future reward ( Branavan, Chen, Zettlemoyer, &amp; Barzilay, 2009 ).

In this paper, we present a reinforcement learning framework for answering complex questions. As noted before, we sim-plify our formulation by assuming no real time user interaction by considering that the human generated abstract sum-maries are the gold-standard and the users (if they were involved) are satisfied with them. The proposed system tries to produce automatic summaries that are as close as the abstract summaries. The relationship between these two types of sum-maries is learned and the final weights are used to output the machine generated summaries for the unseen data. We employ our model. Experiments on the DUC benchmark datasets demonstrate the effectiveness of the reinforcement learning approach. We also extend this work by proposing a model that incorporates user interaction into the reinforcement learner to guide the candidate summary sentence selection process. Evaluation results indicate that the user interaction component paper includes related work, our reinforcement learning formulation, feature space, user interaction modeling, experiments with results, and finally, conclusion with some future directions. 2. Related work
We perform the complex question answering task using an extractive multi-document summarization approach within a reinforcement learning setting. Extractive summarization is simpler than abstract summarization as the process involves assigning scores to the given document sentences using some method and then picking the top-ranked sentences for the summary. Although this kind of summary may not be necessarily smooth or fluent, extractive summarization is currently been proposed for generic multi-document summarization. In recent years, researchers have become more interested in query-focused (i.e. topic-biased) summarization. The leading systems in the DUC question answering task through multi-document summarization.

Other notable extraction-based summarization systems are as follows. Nastase (2008) expands a query by using the encyclopedic knowledge in Wikipedia and introduce a graph to generate the summary. Daum X  III and Marcu (2006) pre-sent BAYESUM ( X  X  X ayesian summarization X  X ), a sentence extraction model for query-focused summarization. On the other hand, Wan, Yang, and Xiao (2007b) propose a manifold-ranking method to make uniform use of sentence-to-sentence and sentence-to-topic relationships whereas the use of multi-modality manifold-ranking algorithm is shown in Wan et al. (2009) . Other than these, topic-focused multi-document summarization using an approximate oracle score has been proposed in Conroy, Schlesinger, and O X  X eary (2006) based on the probability distribution of unigrams in human summaries. In our proposed approach, we represent each sentence of a document as a vector of feature-values. We incorporate query-related information into our model by measuring the similarity between each sentence and the user query (i.e. the given complex question). We exploit some features such as title match, length, cue word match, named entity match, and sentence position to measure the importance of a sentence. We use a number of features to measure the query-relatedness of a sentence considering n X  X ram overlap, LCS, WLCS, skip-bigram, exact-word, synonym, hyper-nym/hyponym, gloss and Basic Element (BE) overlap, and syntactic information (See details in Section 6 ). These features have been adopted from several related works in the problem domain ( Chali, Joty, et al., 2009; Edmundson, 1969; Litvak, lower the redundancy in the extract summary using the Maximal Marginal Relevance (MMR) model ( Carbonell &amp;
Goldstein, 1998 ). This feature helps the learner to understand which document sentence is less similar to the sentences that are already present in the candidate answer space (i.e. the current summary pool). We use the relevant novelty metric that was previously shown in Goldstein, Mittal, Carbonell, and Kantrowitz (2000) . This metric measures relevance and novelty independently and provides a linear combination of them. A document sentence has a higher chance to be selected if it is both relevant to the given query and useful for a summary, while having minimal similarity to the pre-viously selected sentences.

In the field of natural language processing, reinforcement learning has been extensively applied to the problem of dialogue management where the systems converse with a human user by taking actions that emit natural language
Kearns, Litman, &amp; Walker, 1999 ). The state space defined in these systems encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions through a trial-and-error process of repeated interaction with the user. Branavan et al. (2009) presented a rein-forcement learning approach for mapping natural language instructions to sequences of executable actions. Recently, a related problem of automatic text summarization has been modeled using a reinforcement learning framework by Ryang and Abekawa (2012) . Our approach is significantly different from their approach in a number of ways. Their formulation is only applicable to generic summarization while our system considers a problem that is a kind of query-focused multi-document summarization. Moreover, the reward function of their model is not designed to take user feedback into account whereas the reward function of our reinforcement learning framework is specially designed for considering user feedback as the principle way of improving the search policy in real time. A more recent work has explored alternate reinforcement learning algorithms, reward functions and feature sets to show promising results for the multi-document summarization task ( Rioux, Hasan, &amp; Chali, 2014 ).
 ever, experiments in the complex interactive Question Answering (ciQA) task 2006; Webb &amp; Strzalkowski, 2006 ). An adaptive, open-domain, personalized, interactive QA system called YourQA cess ( Chali et al., 2012 ).

We compare our system with a SVM-based model. In the field of natural language processing, SVMs are applied to text categorization and syntactic dependency structure analysis. These approaches are reported to have achieved higher accu-racy than previous approaches ( Joachims, 1998 ). SVMs were also successfully applied to part X  X f X  X peech tagging ( Gim X nez &amp; M X rquez, 2003 ). Single document summarization systems using SVMs demonstrated good performance for both Japane-Suzuki, Isozaki, and Maeda (2003) showed effectiveness of their multiple document summarization system employing
SVMs for sentence extraction. A fast query-based multi-document summarizer called FastSum used SVM regression rank the summary sentences where the goal was to estimate the score of a sentence based on a given feature set a SVM-based system often makes it harder to use in practice. For this reason, we use an unsupervised summarization model to evaluate our proposed reinforcement system. A k-means clustering algorithm is used to build the unsupervised system ( Chali, Joty, et al., 2009 ). 3. Problem formulation We formulate the complex question answering problem by estimating an action-value function ( Sutton &amp; Barto, 1998 ).
We define the value of taking action a in state s under a policy p (denoted Q taking the action a , and thereafter following policy p :
Here, E p denotes the expected value given that the agent follows policy p ; R function of the reward sequence, r t  X  1 ; r t  X  2 ; ... , where r rewards. We try to find out the optimal policy through policy iteration. Once we get the optimal policy ( p ) the agent chooses the actions using the Maximum Expected Utility Principle ( Russel &amp; Norvig, 2003 ). We show our reinforcement learning framework in Fig. 1 . The figure demonstrates that the agent can choose an action from its action space and per-forms that action at a certain time. This causes the current state of the agent to change. On the other hand, the agent receives an immediate reward for choosing the action which in turn contributes in updating its policy for determining the next action. 3.1. Environment, state &amp; actions
Given a complex question q and a collection of relevant documents D  X  d (extract summary). The state is defined by the current status of the answer pool, which represents the sentences that are pool (i.e. candidate extract summary). 3.2. Reward function
In the training stage of the reinforcement learning framework, for each complex question we are given a relevant docu-tence), we compute the immediate reward, r using the following formula: where relevance  X  a  X  is the textual similarity measure between the selected sentence and the abstract summaries, dancy. By including redundancy in the immediate reward calculation we discourage redundancy in the final extract summary. In our experiments, the value of w is kept to 0.5 to provide equal importance to both relevance and redundancy. We measure the textual similarity using ROUGE (Recall-Oriented Understudy for Gisting Evaluation) ( Lin, 2004 ). 3.3. Function approximation
In many tasks such as the one to which we apply reinforcement learning, most of the states encountered will never have been experienced before. This occurs when the state or action space is very large. As in our case the number of states and vector, ~ h t . Our approximate action-value function is a linear function of the parameter vector, state-action pair  X  s ; a  X  , there is a column vector of features, ~ u nents as ~ h t . The approximate action-value function is given by: 3.4. Markov Decision Process (MDP)
Our environment has the Markov property, that is, given the current state and action we can predict the next state and expected reward. For our problem formulation, given the current state s if we take an action a , the next state will be pool. Given any state and action, s and a , the transition model is defined by: q a action, s and a , together with any next state, s 0 , the expected value of the next reward is: 4. Reinforcement learning
We consider our task as an infinite horizon discounted 18 ~ estimate of h . Policy gradient algorithms optimize a non-convex objective and are only guaranteed to find a local optimum random) to balance between exploration and exploitation during the training phase. We empirically set the value of  X  0 : 1 during our experiments. We note that, 90% of the time our algorithm chooses an action with the best action-value and 10% of the time it chooses an action randomly.

The steps of our reinforcement learning algorithm are shown in Algorithm 1 . Here, u is a vector of feature-values (See details in Section 6 ) that is used to represent each document sentence and for the greedy policy. However, when an exploratory action is selected by the behavior policy, the eligibility traces feature score. The addition of a random jump step avoids the local maximums in our algorithm. The parameter k defines how of 0.99 as learning converges towards the goal.

To the best of our knowledge, the proposed formulation with the modified version of the Watkins X  Q  X  k  X  algorithm is unique in how it represents the complex question answering task in the reinforcement learning framework.
Algorithm 1. Modified Watkins X  Q  X  k  X  algorithm 5. Modeling user interaction
In the basic reinforcement learning model for answering complex questions (discussed in the previous section), we have a iteration, a new sentence is selected based on the learned Q function from the document collection and added to the answer pool that in turn changes the state. We propose an extension to this model and add user interaction in the reinforcement stage, the user is presented with the top five candidate sentences based on the learned Q function. The user can also see the extended reinforcement learning model, the user interaction component enables us to incorporate the human viewpoint and thus, the judgment for the best candidate sentence is supposed to be perfect. Extensive experiments on DUC-2006 benchmark datasets support this claim. In Fig. 2 , we show an example of how exactly the user interaction component works. considered topic is shown next, followed by the complex question, current summary (i.e. answer) and the top five candidate selects the sentences automatically and continues updating the weights accordingly. 6. Feature space
We represent each sentence of a document as a vector of feature-values. We divide the features into two major cate-gories: static and dynamic. Static features include two types of features, where one declares the importance of a sentence in a document and the other measures the similarity between each sentence and the user query. We use one dynamic feature that measures the similarity of already selected candidate with each remaining sentences. The dynamic feature is used to ensure that there is no redundant information present in the final summary. 6.1. Static features: importance 6.1.1. Position of sentences
Sentences that reside at the start and at the end of a document often tend to include the most valuable information. We lify to be considered for this feature. We assign the score 1 to them and 0 to the rest. 6.1.2. Length of sentences
Longer sentences contain more words and have a greater probability of containing valuable information. Therefore, a 0 otherwise. We manually investigated the document collection and set a threshold that a longer sentence should contain at least 11 words. The empirical evidence to support the choice of this threshold is based on the direct observation of our datasets.
 6.1.3. Title match
If we find a match such as exact word overlap, synonym overlap or hyponym overlap between the title and a sentence, we hyponyms. 6.1.4. Named Entity (NE)
The score 1 is given to a sentence that contains a Named Entity class among: PERSON, LOCATION, ORGANIZATION, GPE (Geo-Political Entity), FACILITY, DATE, MONEY, PERCENT, TIME. We believe that the presence of a Named Entity increases the importance of a sentence. We use the OAK System ( Sekine, 2002 ), from New York University for Named Entity recognition. 2004 ). 6.1.5. Cue word match  X  X  X n conclusion X  X ,  X  X  X inally X  X  etc. We use a cue word list words and 0 otherwise. 6.2. Static features: query-related 6.2.1. n X  X ram Overlap n X  X ram overlap measures the overlapping word sequences between the candidate document sentence and the query sen-S and a query Q using the following formula ( Chali, Joty, et al., 2009 ): where n stands for the length of the n-gram  X  n  X  1 ; 2 ; 3 ; 4  X  and Count the query and the candidate sentence. 6.2.2. LCS
Given two sequences S 1 and S 2 , the longest common subsequence (LCS) of S imum length. We use this feature to calculate the longest common subsequence between a candidate sentence and the query. We used the LCS-based F-measure to estimate the similarity between a document sentence S of length m and a query sentence Q of length n as follows ( Lin, 2004 ): tance of precision and recall. We set the value of a to 0.5 to give equal importance to precision and recall. 6.2.3. WLCS
Weighted Longest Common Subsequence (WLCS) improves the basic LCS method by remembering the length of con-secutive matches encountered so far. Given two sentences X and Y, the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated in Lin (2004) . The WLCS-based F-measure between a query and a sen-tence can be calculated similarly as described in Section 6.2.2 . 6.2.4. Skip-bigram
Skip-bigram measures the overlap of skip-bigrams between a candidate sentence and a query sentence. Skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence. The skip bi-gram score between the document sentence S of length m and the query sentence Q of length n can be computed as follows ( Chali,
Joty, et al., 2009 ): nation function. We call the Eq. (11) skip bigram-based F-measure. 6.2.5. Exact-word overlap
This is a measure that counts the number of words matching exactly between the candidate sentence and the query sen-tence. Exact-word overlap can be computed as follows: where WordSet is the set of important 23 words in the sentence and Count and the important query words. 6.2.6. Synonym overlap
This is the overlap between the list of synonyms of the important words extracted from the candidate sentence and the lap can be computed as follows: where SynSet is the synonym set of the important words in the sentence and Count the SynSet and query related words . 6.2.7. Hypernym/hyponym overlap
This is the overlap between the list of hypernyms and hyponyms (up to level 2 in WordNet) of the nouns extracted from the sentence and the query related words. This can be computed as follows: where HypSet is the hyponym/hyponym set of the nouns in the sentence and Count the HypSet and query related words . 6.2.8. Gloss overlap
Our systems extract the glosses for the proper nouns from WordNet. Gloss overlap is the overlap between the list of words. This can be computed as follows: the sentence and Count match is the number of matches between the GlossSet and query related words. 6.2.9. Syntactic feature using a syntactic parser ( Chali, Joty, et al., 2009 ). We use the Charniak parser by an m dimensional vector v  X  T  X  X  v 1  X  T  X  ; v 2  X  T  X  ; ... ; the query and the document sentence based on their syntactic structures. The tree kernel of the two syntactic trees, T actually the inner product of the two m -dimensional vectors, Basili, &amp; Manandhar, 2007; Zhang &amp; Lee, 2003 ): 6.2.10. Basic Element (BE) overlap
Basic Elements are defined as follows ( Hovy, Lin, Zhou, &amp; Fukumoto, 2006 ): the head of a major syntactic constituent (noun, verb, adjective or adverbial phrases), expressed as a single item, or a relation between a head-BE and a single dependent, expressed as a triple: (head j modifier j relation).
We extract BEs for the sentences (or query) by using the BE package distributed by ISI. ranked list contains important BEs at the top which may or may not be relevant to the complex question. We filter out the
BEs that are not related to the query and get the BE overlap score ( Hovy et al., 2005 ). 6.3. Dynamic feature
For each sentence that is selected for the summary pool, we measure its similarity with the remaining non-selected sen-tences using ROUGE. The similarity value is encoded into the feature space of the non-selected sentences as the dynamic ferent from the sentence that is already there. In other words, from the dynamic feature of a sentence we can understand whether the sentence can add any new information into the summary or not. The dynamic feature is updated each time a new sentence is added to the summary. We use the Maximal Marginal Relevance (MMR)
Goldstein, 1998 ) to balance this feature with query relevance. We give equal importance to query relevance and redundancy reduction such that the selected sentence can add some valuable as well as new information to the summary. The concept
Yan, Lu, &amp; Ma, 2006 ). 7. Evaluation framework and results 7.1. Task overview
Over the past few years, complex questions have been the focus of much attention in both the automatic question X  answering and Multi Document Summarization (MDS) communities. Typically, current complex QA evaluations including the 2004 AQUAINT Relationship QA Pilot, the 2005 Text Retrieval Conference (TREC) Relationship QA Task, and the TREC
However, MDS evaluations (including the 2005, 2006 and 2007 Document Understanding Conference (DUC)) have tasked systems with returning paragraph-length answers to complex questions that are responsive, relevant, and coherent. The progress in summarization and enable researchers to participate in large-scale experiments. This paper deals with the query-focused multi-document summarization task as defined in the Document Understanding Conference, DUC-2007.
The task is as follows:  X  X  X iven a complex question (topic description) and a collection of relevant documents, the task is to synthesize a fluent, well-organized 250-word summary of the documents that answers the question(s) in the topic X  X  .

For example, given the topic description (from DUC 2007): &lt; topic &gt; &lt;/ topic &gt; topic description. We consider this task 28 and apply a reinforcement approach to generate topic-oriented 250-word extract summaries. 7.2. Corpus for training and testing
The DUC-2006 and DUC-2007 document sets came from the AQUAINT corpus, which is comprised of newswire from the Associated Press and New York Times (1998 X 2000) and Xinhua News Agency (1996 X 2000). In Table 1 , we present the description of the datasets used in our experiments. We use the DUC-2006 data to learn a weight for each of the features (described in Section 6 ) and then use these weights to produce extract summaries for the document clusters of DUC-2007.
We also use the given abstract summaries for each topic as the training data. In DUC-2006, each topic (including a complex reference summaries were used in the training stage to calculate the numeric rewards. 7.3. Systems for comparisons 7.3.1. Baseline system
We report the evaluation scores of one baseline system (used in DUC-2007) in each of the tables in order to show the level of improvement our system achieved. The baseline system generates summaries by returning all the leading sentences (up to 250 words) in the h TEXT i field of the most recent document(s). 7.3.2. SVM settings
We compare the performance of our reinforcement learning approach with a SVM-based technique to answer complex experiments. We generate a training data set by automatically annotating (using ROUGE similarity measures) 50% of the sen-ment sentences as feature vectors except the dynamic feature. The dynamic feature seemed to be inappropriate for the SVM setting. However, to reduce the redundancy in the system-generated summaries, we use the MMR-approach during the sum-mary generation process.

During the training step, we used the third-order polynomial kernel with the default value of C . ( Joachims, 1999 ) package. We performed the SVM training experiments using WestGrid used the Cortex cluster which is comprised of shared-memory computers for large serial jobs or demanding parallel jobs.
In the multi-document summarization task at DUC-2007, the required summary length was 250 words. In our SVM setup, chose the top N sentences as the candidate summary sentences. Initially, the top-ranked sentence is added to the summary and then we perform an MMR-based computation to select the next sentence that is equally valuable as well as new (bal-ancing the query relevance and redundancy factor). We continue this task until the summary length of 250-words is reached. 7.3.3. K-means clustering
The k-means algorithm follows a simple way to cluster a given data set through a pre-specified number of clusters k . There are several approaches (such as  X  X  X K-Means X  X  by Mirkin (2005) , Hartigan X  X  method ( Hartigan &amp; Wong, 1979 ) etc.) to estimate the number of clusters. These methods may also give incorrect number of clusters. However, in our task, we simply assume that we have two clusters: 1. Query-relevant cluster that contains the sentences which are relevant to the user-questions, and 2. Query-irrelevant cluster that contains the sentences that are not relevant to the user-questions.
 according to a probability model. A Bayesian model is used for this purpose: weights of the clusters as equiprobable (i.e. P  X  q k j H  X  X  1 = K ). We calculated p  X  x j q bution. The gaussian probability density function (pdf) for the d-dimensional random variable x is given by: from the K-means algorithm and calculate the covariance matrix using the unbiased covariance estimation procedure ( Chali,
Joty, et al., 2009 ): 7.4. Evaluation and analysis 7.4.1. Automatic evaluation: ROUGE
Similar to DUC-2006, in DUC-2007, each of the four assessors created a 250-word summary of the document cluster that satisfies the information need expressed in the topic statement. These multiple  X  X  X eference summaries X  X  were used in the evaluation of our system-generated summary 33 content. We considered the widely used evaluation measures Precision (P), the system-generated summaries in common with the reference summaries to the total number of units in the reference sum-mary while precision is the ratio of the number of units of system-generated summaries in common with the reference sum-maries to the total number of units in the system-generated summaries. F-measure combines precision and recall into a single measure to compute the overall performance. We evaluate the system generated summaries using the automatic eval-uation setup. We report the scores of the two official ROUGE metrics of DUC, ROUGE-2 (bigram) and ROUGE-SU (skip bigram). All the ROUGE measures are calculated by running ROUGE-1.5.5 with stemming but no removal of stopwords. ROUGE run-time parameters: ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -a
Tables 2 and 3 show the ROUGE precision and recall scores of the reinforcement system, the supervised SVM system, and the unsupervised k-means system. In Table 4 , we compare the ROUGE-F scores of the baseline system, SVM system, k-means system, and reinforcement system. From here, we find that the reinforcement system improves the ROUGE-2 and ROUGE-
SU scores over the baseline system by 32.9% and 21.1%, respectively. On the other hand, the reinforcement system outper-forms the supervised SVM system demonstrating improvements to the ROUGE-2 and ROUGE-SU scores by 28.4% and 2.7%, respectively besides performing very closely to the unsupervised k-means system.

Statistical significance: An approximate result to identify which differences in the competing systems X  scores are sig-nificant can be achieved by comparing the 95% confidence intervals for each mean. In Table 4 , we show the 95% confidence intervals of all the systems to report significance for doing meaningful comparison. ROUGE uses a randomized method named bootstrap resampling to compute the confidence intervals. Bootstrap resampling has a long tradition in the field of statistics ( Efron &amp; Tibshirani, 1994 ). We use 1000 sampling points in the bootstrap resampling.
Discussion: The performance gain of the reinforcement learning method was achieved from the reinforcement learning such that the machine generated summary has the closest match with the given gold standard summaries. Two systems can the confidence intervals of the SVM , reinforcement and k-means systems do overlap. However, according to the second cri-other hand, there is no significant difference between the reinforcement system and the k-means system if we interpret the ture space contributed in minimizing the redundancy of the automatically generated summaries which also provided a posi-tive impact on the system performance. We claim that the performance of the proposed system would further improve if the reward function could consider syntactic and semantic similarities between a selected summary sentence and the abstract summaries. In our experiments, the use of human interaction with the considered SVM and K-means setup was not seemed generated summaries. Details of this approach are available at Chali and Hasan (2012) . 7.4.2. Manual evaluation It might be possible to get state-of-the-art ROUGE scores although the generated summaries are bad ( Sj X bergh, 2007 ).
Therefore, we conduct an extensive manual evaluation in order to analyze the effectiveness of our approach. Two university graduate students judged the summaries for linguistic quality and overall responsiveness according to the DUC-2007 eval-
They also assigned a content responsiveness score to each of the automatic summaries. The content score is an integer between 1 (very poor) and 5 (very good) and is based on the amount of information in the summary that helps to satisfy the information need expressed in the topic narrative. X  X  34 fact is understandable since our approach did not consider any post-processing and sentence-ordering algorithms to fine-tune the system-generated summaries by ignoring the fluency component of the system task. However, in terms of overall content responsiveness, the reinforcement system outperformed all other systems indicating a better accuracy in meeting the user-requested information need. The differences between the systems were computed to be statistically significant 7.4.3. Most effective features for reinforcement learning
After the training phase, we get the final updated weights corresponding to each feature. The smallest weight value indi-final feature weights (ranked by higher effectiveness) for this problem domain that we find after the training experiment.
The table shows that the Basic Element overlap feature is the most effective feature followed by the syntactic feature and ture has little impact on the reinforcement learner. 7.4.4. Experiments with user interaction
System Description. The major objective of this experiment is to study the impact of the user interaction component in the reinforcement learning framework. To accomplish this purpose, we use the first 30 topics at most (test data).

We follow six different ways of learning the feature weights by varying the amount of user interaction incorporated and
The numbers in the system titles indicate how many user-interaction and non-user-interaction topics each system included SYS _ 30 _ 0 and 30 topics without interaction).

The outcomes of these systems are sets of learned feature weights that are used to generate extract summaries (i.e. framework.

Evaluation. We report the two official ROUGE metrics of DUC-2006 in the results: ROUGE-2 (bigram) and ROUGE-SU (skip bigram). In Table 7 , we compare the ROUGE-F scores of all the systems. In our experiments, the only two systems that were results, we see that the SYS _ 20 _ 0 system improves the ROUGE-2 and ROUGE-SU scores over the SYS _ 0 _ 20 system by 0.57%, and 0.47%, respectively. Again, we see that the SYS _ 20 _ 10 system improves the ROUGE-2 and ROUGE-SU scores over system (both systems had 30 topics with SYS _ 30 _ 0 having more human supervision) by 0.25%, and 0.80%, respectively. The results show a clear trend of improvement when human interaction is incorporated. We can also see that the SYS _ 30 _ 30 system is performing the best since it starts learning from the learned weights that are generated from the outcome of framework that further controls the automatic learning process efficiently (after a certain amount of interaction has been incorporated). In Table 8 , we report the 95% confidence intervals for ROUGE-2 and ROUGE-SU to show the significance of our results.

We also conduct an extensive manual evaluation of the systems. Two university graduate students judged the summaries impact of the user interaction component in the reinforcement learning framework. The improvements in the results are sta-tistically significant ( p &lt; 0 : 05).

Discussion. The main goal of the reinforcement learning phase is to learn the appropriate feature weights that can be used in the testing phase. When the agent is in learning mode, in each iteration the weights get updated depending on the imme-automatic learning continues in the remaining iterations. When the summary length reaches to 250 words, we obtain the weights learned from one topic. These weights become the initial weights for the next topic. The user again interacts with the system for three iterations and the process continues for a certain number of topics. Then, the agent starts automatic learning for the remaining topics.

The effect of the presence or absence of user feedback on the feature weights can be shown using the graphs in Fig. 3 with 20 topics of the DUC-2006 data with user interaction. We draw this graph using the gnuplot overlap, (7) hypernym/hyponym overlap, (8) sentence length, (9) title match, (10) named entity match, (11) cue word match, quickly for the important two features (2-gram overlap and BE overlap) by immediately lowering the values in iteration-2. We can also see that the hypernym/hyponym overlap feature gets an abrupt increase in its value during iteration-8 while shown in Section 7.4.4 also support this claim. 8. Conclusion and future work The main contribution of this paper is a reinforcement learning formulation of the complex question answering problem.
We proposed a modified version of the Watkins X  Q  X  k  X  algorithm to represent the complex question answering task in the reinforcement learning framework. The main motivation of applying a reinforcement approach in this domain was to enhance real-time learning by treating the task as an interactive problem where user feedback can be added as a reward.
Initially, we simplified this assumption by not interacting with the users directly. We employed the human-generated abstract summaries to provide a small amount of supervision using reward scores through textual similarity measurement.
Later, we extended our model by incorporating a user interaction component to guide the candidate sentence selection pro-cess during the reinforcement learning phase.

We compared our reinforcement system with a baseline, a supervised (SVM) system, and an unsupervised (K-means) sys-tem. Extensive evaluations on the DUC benchmark data sets showed the effectiveness of our approach. The performance gain of the reinforcement learning method was achieved from the reinforcement learning algorithm as its major goal was to learn mary has the closest match with the given gold standard summaries. We claim that the performance of the proposed system would further improve if the reward function could consider syntactic and semantic similarities between a selected summa-minimizing the redundancy of the automatically generated summaries which also provided a positive impact on the system performance. Furthermore, our experiments with the user interaction component revealed that the systems trained with answer sentence selection.

The implications of our approach for interactive systems can be observed by the user modeling component as it helps the reinforcement framework to refine the answers based on user feedback. Another scenario of adding user interaction could be as follows: a user submits a question to the QA system and receives an extract summary for a list of relevant documents isfaction which will be encoded as a reward in the reinforcement learning approach. The current answer to the complex imum. This process is definitely time consuming, however, once the system receives a considerable amount of feedback about several complex questions, a reinforcement learning system could learn about the user X  X  interests, choices etc. from this data. The learned model can be used to answer unseen complex questions efficiently. We plan to accomplish this goal in the future.
 amount of exploration and less exploitation while gradually decreasing to reduce exploration as time passes. In this work, we used ROUGE as a reward function to provide feedback to each chosen action. We plan to extend this research by using quence Kernel (ESSK) ( Hirao et al., 2003 ) as the reward functions.
 Acknowledgments supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada  X  discovery grant and the University of Lethbridge. This work was done when the second author was at the University of Lethbridge.
 References
