 This paper describes Koru, a new search interface that offers effective domain-independent knowledge-based information retrieval. Koru exhibits an unde rstanding of the topics of both queries and documents. This a llows it to (a) expand queries automatically and (b) help guide the user as they evolve their queries interactively. Its unders tanding is mined from the vast investment of manual effort and judgment that is Wikipedia. We show how this open, constantly evolving encyclopedia can yield inexpensive knowledge structures th at are specifically tailored to expose the topics, terminology and semantics of individual document collections. We conducted a detailed user study with 12 participants and 10 topics from the 2005 TREC HARD track, and found that Koru and its underlying knowledge base offers significant advantages over traditional keyword search. It was capable of lending assistance to almost every query issued to it; making their entry more efficient, improving the relevance of the documents they return, and narrowing the gap between expert and novice seekers. H.3.3 [ Information Storage and Retrieval ]: Search and Retrieval  X  search process, query formulation. Algorithms, Design, Experimentation. Information Retrieval, Query Expansion, Wikipedia, Data Mining, Thesauri. And how will you enquire, Socrates, into that which you do not know? What will you put forth as the subject of your enquiry? And if you find out what you want, how will you ever know that this is the thing that you did not know? This question, posed by the Greek philosopher Meno some 400 years before Christ X  X  birth, is still relevant in today X  X  internet-savvy age. Whenever we seek out new knowledge X  X henever we turn to the ubiquitous search engines X  X e must grapple with the same fundamental paradox: how can one describe the unknown? That is precisely what must be done to form a query. To make matters worse, search engines are incapable of reasoning with these descriptions as people do. They instead treat a query as nothing more than an excerpt X  X  few words or phrases X  X rom a relevant document. To search e ffectively, one must predict not only the information that relevant documents contain, but also the terms by which this is expressed. In short, one must already know a great deal of what is being sought, in order to find it. What knowledge seekers need X  X  t least those who are not clairvoyant X  X s a bridge between what they know and what they wish to know, between their vague initial query and the concrete topics and terminology availabl e. One possible bridge is a thesaurus: a map of semantic relations between words and phrases. Knowledge seekers who cannot identify the effective terms for their query could benef it from a thesaurus that covers the terminology of both documen ts and potential queries, and describes relations that bridge between them. Those who cannot formulate a specific query at all could use a well-organized thesaurus that exposes the topics available and allows them to be explored. The use of thesauri and similar knowledge structures has the potential to greatly advance the art of information retrieval. In practice, however, thesauri are not widely used to assist with information retrieval. Generic thes auri have shortcomings in any specific technical domain. Domain-specific thesauri are expensive to produce, and their use may require specialist technical knowledge: thus they are only available for a small proportion of document sets, and appeal only to expert users. This research aims to address both issues, by a) automatically producing thesauri that can serve as a bridge b) allowing them to be applied to the searching process This paper focuses on the second goal, the search interface. The next section describes Koru, a search interface that allows a thesaurus, focused to the needs of a particular document collection, to be used intuitively and unobtrusively. This new search system, and its evaluation through a user study, is the main contribution of the paper. Howe ver, it cannot work without a comprehensive thesaurus, and Section 3 sketches our new approach to creating thesauri. S ection 4 gives some examples of how Koru was used in practice by (untrained) experimental subjects. Section 5 describes an evaluation of the system, which to some extent is also an evaluation of the automatically-produced thesaurus. Section 6 presents th e context of research surrounding this work, and Section 7 di scusses its implications. Koru is the M  X  ori word for the newborn, unfurling fern frond; a delicate spiral of expanding fractal shapes. For indigenous New Zealanders it symbolizes growth; re birth; evolution. Likewise, the Koru topic browsing system provides an environment in which users can progressively work towards the information they seek. It exhibits an understanding of th e topics involved in both queries and documents, allowing them to be matched more accurately by evolving queries both automatically and interactively. The interface is illustrated in Figure 1. Implementation is based on the AJAX framework [5], which provides a highly reactive interface couched in nothing more than the standard elements of a webpage. The upper area is a classic search box in which the user has entered the query american airlines security . Below are three panels; query topics, query re sults, and the document tray. What the figure does not convey is that to avoid clutter not all the panels are visible at any given time. There are three possible configurations, which relate to three stages of expected user behavior: 1. Building an appropriate query. This involves adding and removing phrases until the query and corresponding list of query results satisfies the user X  X  info rmation need. At this stage two panels are visible: query topics a nd query results (the leftmost two panels in Figure 1). 2. Browsing the document list. Once a suitable list of documents is returned, the user must determine the most relevant ones and judge whether they warrant further study. At this point the panels in Figure 1 slide across so that only the rightmost two X  X uery results and document tray X  X re visible. http://www.nzdl.org/Koru 3. In-depth reading of a chosen document . Having located a worthy document, the user then devotes time to actually reading the relevant sections. Here only the documents tray is needed. Anything else would be a distraction. The first panel, query topics, pr ovides users with a summary of their query and a base from which to evolve it. It lists each significant topic extracted from the query, and assigns to each a color that is used consisten tly throughout the interface. These topics are identified without requiring any special query syntax: in Figure 1 American Airlines has been identified as a single phrase even though the user did not surround it by quotes. Sophisticated entity extraction is not used: instead the words and consecutive sequences of words in the query are checked against the thesaurus terms. The only  X  X ntelligence X  in the process is embodied in the thesaurus and the t echnique used to generate it. This thesaurus (described in Section 3) is exceptionally comprehensive. It relates specifically to the document collection and is backed up by a resource that excels in describing contemporary concepts, us ing contemporary language. Consequently we anticipate that most queries that are valid for the collection will be recognized, even when non-technical terminology or slang is used. 2 However, in the event that terms are not recognized, interaction does not break down: these terms are still listed as topics and inco rporated into the query. If the query contains overlapping phrases that each match a thesaurus term, the overlapping words are assigned to the topic with the strongest match against the document collection. For the given query this results in the five topics American Airlines , Security , Security (finance) , Airline and Americas . The last is recognized because the th esaurus contains a use-for link from America to the preferred term Americas . Non-preferred synonyms for each term are listed below that term: For example, the topic Airline  X  X  synonyms include air carrier , a irline company, This expectation is borne out by the experimental evaluation described in Section 5. and scheduled air transport . These are used internally to improve queries (see Section 4) and presente d to the user in order to help them understand the sense of the t opic. The user can also learn more about a topic by clicking the adjacent Wikipedia link. Query terms are often ambiguous and relate to multiple entries in the thesaurus. By security , for example, the user could also mean property pledged as collateral for a loan, which appears in Figure 1 as Security (finance) . Each sense is included, and ranked according to the likelihood that it is a relevant, significant topic for the current query. This likelihood, displayed initially as a horizontal bar next to the topic and elaborated on in a tool-tip, is calculated as a function of a topic X  X  statistical and semantic significance within the document collection. The way in which these weights are obtained is explained in Section 3.1.4. Only the top-ranked topics that cover all the query terms are used for retrieval (in the example, American Airlines and the first meaning of security ), as indicated by the checkboxes to the left of the topics. This can be overridde n manually. For example, it is useful for Airlines and Americas to appear separately X  X ven though they are not included in Koru X  X  default interpretation of the query X  X n case the user was interested in all airlines that operated in the U.S. rather than the specific company. Each topic recognized in the query can be investigated in isolation by using it as a starting point for browsing the thesaurus. In Figure 1 the user has chosen to expand topics related to Airline . They have clicked the triangle to the right of that term, which brings up a menu of related topics. They can then investigate further topics of interest such as Singapore Airlines and British Airways . Any of these topics could be incorporated into the query with a simple click of the appropr iate checkbox. As with alternate senses, these topics are ranked according to their expected usefulness, which is elaborated on in tool-tips: the small gray box in Figure 1 shows the tool-tip for British Airways . This is calculated in the same way as before, except that the strength of the relation to the parent topic (in this case, Airline) is also taken into consideration. The second panel in Figure 1, query results, presents the outcome of the query in the form of a se ries of document surrogates. These resemble those found in typical search engines like Google, and document X  X  relationship to the query. Query topics (including synonyms) within both titles and snippets are highlighted for ease of identification. The only unconventional addition is an overview of how topics are distributed throughout the document, which is presented graphically underneath each snippet using tilebars [13] (only one document in Figure 1 has a fully visible set of tilebars). These represent the entire content of the document as a horizontal bar from left (beginning of document) to right (end). Different bars relate to different query topics, in this case American Airlines (upper bar) and Security (lower bar). Points, colored in accordance with the query term, a ppear along the bar to represent the locations in the document of phrases that match the topic. These simple maps can give detaile d insights into the relevance of a document. For example, it is apparent that security is relevant throughout the first document in Figure 1, but American Airline s is mentioned only once. That occurrence is close to a mention of security , so the document likely discusses the security of American Airlines, but only in pa ssing. From this purely spatial information the user can make an informed decision about whether the document is worth opening. The third panel in Figure 1 shows the document tray, which allows the reader to collect mu ltiple documents they wish to peruse. More significantly, its purpose is to facilitate efficient reading by helping users identify relevant sections of a document and navigate between them. Thes e sections are identified using the same information that made the document itself relevant: the query terms used to locate it. Term occurrences are easily seen because they are highlighted accordi ng to the colors defined in the query topics panel. Interesting pa tterns of highlights are likely to indicate sections and paragraphs that should be read. These highlights can easily be missed, however, because most documents are too large to be viewed without scrolling. Consequently tilebars are supplied to provide an overview of how terms are distributed th roughout the document. These tilebars are oriented vertically, and appear on the right-hand side of the standard scrollbar and with a direct mapping to it (they look rather thin in Figure 1). If the scrollbar slider is moved alongside a cluster of points in the tilebar, the highlights that these points represent are visible in the document. Users can jump directly to a particular highlight by clicking the appropriate spot in the tilebar. To work well, Koru relies on a large and comprehensive thesaurus. We took an unconventional approach to obtaining one. Retrieval systems that use thesauri generally use manually-produced ones, either generic (e.g. WordNet [15]) or domain-specific (Agrovoc [7]). Neither are particularly suited to Koru or other information retrieval syst ems. Generic thesauri are too broad and shallow to provide comp rehensive coverage of specific topics, and domain-specific thesauri are expensive to produce and not available in many domains. A nother possible route is to use automatically generated thesau ri obtained through lexical and statistical analysis of the docum ents. Unfortunately such natural language processing is quite impreci se and the results tend to be kept behind the scenes. Koru is very transparent in its use of thesauri, and consequently demands a higher level of accuracy because users can easily see its shortcomings. Manual definition and automatic generation are seemingly exclusive approaches. Our own technique bridges them by automatically extracting thesauri from a huge manually defined information structure. From Wikipe dia, we derive a thesaurus that is specific to each particular document collection. Wikipedia is particularly attractive for this work because it represents a vast domain-independent pool of manua lly defined terms, concepts and relations. By intersecting this with individual document collections, we are able to provide thesauri that are individually tailored to those who seek know ledge from the documents. The intersection operation is necessary because without it an enormous number of links would be presented, most of which would be completely irrelevant to the information retrieval task at hand. The many benefits of such structures, which we call WikiSauri, are covered in [16]. Here we provide an abbreviated sketch of the method by which we derive them. The basic idea is to use Wikipedia X  X  articles as building blocks for the thesaurus, and its skeleton stru cture of hyperlinks to determine which blocks are needed and how they should fit together. Each article describes a single concept; its title is a succinct, well-formed phrase that resembles a term in a conventional thesaurus X  X nd we treat it as such. Concepts are often referred to by multiple terms X  e.g. money might be grouped with cash, currency, and legal tender  X  X nd Wikipedia handles these using  X  X edirects X : pseudo-articles that exist only to connect an alternative title of an article with the preferred one. In earlier work [17] we showed that Wikipedia could provide a viable alternative to Agrovoc [7], a professionally-produced thesaurus for the domain of agriculture, and in particular that Wikipedia re directs match the synonymy encoded in Agrovoc almost perfectly. The danger in using Wikipedia X  X  structure is that because it is so huge (1 million topics, plus a further 1 million synonyms) the Koru user will become swamped w ith irrelevant topics and links. It is essential to identify the c oncepts relevant to a particular document collection, and place these in a structure that allows navigation between related concep ts. This requires a measure of semantic relatedness between Wikipedia articles. Semantic relatedness concerns the strength of the relations between concepts. It can be quan tified: for example, one might say that cash and currency is 100% related, or currency and bank are 85% related. Despite the evident subjectivity, people are capable of fairly consistent j udgments. For example, in [8], 13 participants individually define d relatedness for 350 term pairs and achieved an average correlation of 79% between each individual X  X  judgments and those of the group. The measure that we use quantif ies the strength of the relation between two Wikipedia articles by weighting and comparing the links found within them. Links are weighted by their probability of occurrence; they are less significant for judging the similarity between articles if many other articles also link to the same target. We simply sum the weights of the links that are common to both articles. This yields a correlation of 59% with the above-mentioned manual judgments on the 350 term pairs used in [8]. To identify the concepts relevant to a particular document collection we work through each document in turn, identifying the significant terms and matching th em to individual Wikipedia articles. To lift terms from their surrounding prose, the text is parsed to identify nouns and noun phr ases. Candidate concepts for these terms are found in Wikipedia. The fact that it contains redirects and disambiguation pa ges means this can be done efficiently using only page titles and links. The sheer scale of Wikipedia ma kes disambiguation crucial. For example, the term Jackson covers over 50 different locations and over 100 different people. If all these were included in the thesaurus, it would become bloated and unfocused. We disambiguate each term using the context surrounding it, using our measure of semantic relate dness to choose the senses that relate most strongly to the other t opics in the same sentence. This approach breaks down when the context is insufficient; when there are no unambiguous terms; or if several candidate senses are equally valid. In this case we take a cascading approach: if a sentence contains insufficient info rmation to disambiguate a term the entire surrounding paragraph is used as context; if the paragraph contains insufficient context the entire document is used. It is rare that a term remains ambiguous at the document level, but if so all the equally likely candidate senses are included in the thesaurus. Wikipedia contains many more links than the redirects we use to identify synonymy. It also defines an extensive network of categories that encode hierarchical relations (broader/narrower term, or BT/NT), and millions of hyperlinks between articles which correspond to flat relations (related term RT). These are the links we use to identify related topi cs, such as the various airlines shown in Figure 1. Unfortunately the relations in Wikipedia do not map accurately to those in traditional thesauri: cat egories yield BT/NT relations with only 16% precision, and article hyperlinks are even worse. Consequently we gather all re lations from article and category links, but weight them so that only the strongest are emphasized. Moreover, hierarchical and flat relations are not cleanly separated as the structure would suggest, but are intermingled in both category and article links. This is why the Koru interface simply identifies related topics without attempting to specify the nature of the relationship. Every occurrence of every topic is weighted within the thesaurus. Thus it can be determined whethe r a document is largely about a topic, or merely mentions it in passing. This is calculated as two weights; standard tf-idf (term frequency times inverse document frequency) scores and our own se mantic relatedness measure. The former is based on the assumption that a significant topic for a document should occur many times within it, and be useful in distinguishing the document from others. The second is based on the assumption that a significant topic should relate strongly to other topics in the document: here we use the average semantic relatedness measure between a topic and all the others identified for that document. Some of Koru X  X  functionality depends on these weights. Its ranking of possible senses of query terms (e.g security in Figure 1) is based on the significance of each topic within the document collection. This is calculated by aggregating the statistical and semantic significance of all of their occurrences. Koru X  X  ranking of related topics is based on the same measures, plus the strength of the relation between query topic and related topic. To gain detailed insights into th e performance of Koru for document retrieval, we conducted an expe riment in which participants performed tasks for which the relevant documents had been manually identified. The tasks, doc uments and relevance judgments were obtained from the 2005 TREC HARD track [1], which pits retrieval techniques against each other on the task of high-performance retrieval through user interaction. The tasks were specifically engineered to encourage a high degree of interaction. In order to give a flavor of Koru in action, Table 1 shows three of the TREC tasks, along with information about the initial querying behavior of a few different users for each task. These tasks Table 1: Example retrieval tasks, queries, and topics identified require the user to think carefully about their query terms, and are unlikely to be satisfied by a single query or document. The TREC tasks are paired with the AQUAINT text corpus, a collection of newswire stories fro m the Xinhua News Service, the New York Times News Service, and the Associated Press Worldstream News Service. The thesaurus that was used throughout was generated using th e method described in Section 3; further details are given in Section 5.4. In the first example in Table 1, User 1 types the query black bear humans . Koru the identifies four topics: American Black Bear , Human , Bear , and Black (people) (only the first two are shown in Table 1). The first two cover all terms in the query, and are checked by default in the interface. The query that Koru issues to the back-end search engine contains two clauses AND X  X  together, one for each topic. The first has 4 OR X  X  components and the second 9, corresponding to synonyms of the topics. Koru places each of these 13 components between quotation marks before passing them to the search engine, so that they are treated as phrases. The result is that a fairly sophisticated query, such as a librarian might issue, has been created from the user X  X  simple three-word input X  X ncluding some non-obvious synonyms. User 2 types black bear man , which yields precisely the same results. User 3 types black bear behaviour , which yields a different query. Notice incidentally how Koru caters for spelling variants and plural forms. Many related topics can be obtained by clicking beside each search topic (as for Airline in Figure 1). Examples are Alaska and West Virginia for the topic American Black Bear , Civilization for the topic Human , and Psychology , Brain and Biology for the topic Behaviour . The second example in Table 1 c oncerns email abuse. User 1 simply types these two words as the initial query. Each of these terms is recognized as a topic, and behind the scenes Koru automatically expands them to embrace synonyms and alternate forms. User 2 adds the word employees which is also recognized as a topic in itself, resulting in a lengthy 3-term query. In the third example, which is about the Hubble telescope, User 1 types Hubble telescope achievements . The first two words are identified as the topic Hubble Space Telescope ; the word achievements is not recognized as a topic at all because it does not appear as a term in the thesauru s. Nevertheless it is still added to the query, along with the expansions of the first topic. User 2 introduces universe expansion into the query. Quite fortuitously, the word expansion is related in the thesaurus to Hubble X  X  law because Wikipedia redirects it to that article: no other senses of expansion made it into the thesaurus. This section describes a user study which evaluated Koru and its underlying data structure for their ability to facilitate and improve information retrieval. Of particular interest is whether the topics, terminology and sema ntics extracted from Wikipedia make a conclusive, positive difference in the way users locate information, which we measure by pitting the new knowledge-based topic browsing technique against traditional keyword search. We are also interested in Koru X  X  usability; whether it allows users to apply the knowledge found in Wikipedia to their retrieval process easily, effectiv ely and efficiently. This is assessed by observing participants closely as they interact with the system to perform the realistic tasks provided by TREC. To provide a baseline for comparison we created another version of Koru that provides as much of the same functionality as possible without using a thesau rus, and whose interface is otherwise identical. This allows a clean comparison of the new system with keyword search. The baseline system simply omits the query topics panel in Figure 1. To further reduce interference in the comparison we omitted tilebars from both systems. While they can be of assistance in both topic browsing and keyword searching, they are not a fundamental component of either. We also omitted the Wikipedia links that are placed beside each topic in order to focus the participants on using Koru rather than browsing an external knowledge source. Twelve participants were observed as they interacted with the two systems. All were experienced knowledge seekers; graduate or undergraduate computer scientists with at least 8 years of computing experience, and all use Google and other search engines daily. Sessions typically la sted for one and a half hours, and were conducted in a controlle d environment with video and audio recording, and an observer present. Data was also collected from questionnaires and system logs. Each user was required to perfo rm 10 tasks (of which Table 1 shows three) by gathering the doc uments they felt were relevant. Half the users performed five ta sks using Koru in one session and the remaining five using the traditional search interface in a second session; for the other half the order was reversed to counter the effects of bias and transfer learning. For each task, approximately 750 relevance judgments are made in which a document is identified as strongly relevant, weakly relevant, or irrelevant. The ACQUAINT text corpus that wa s used for the experiments is large X  X bout 3GB uncompressed. It was impractical to create a thesaurus for the entire collection because the process has not been optimized. Instead we used a subset of the corpus: only stories from Associated Press, and only those mentioned in the relevance judgments for the 10 task s. The result is a collection of approximately 1200 documents c oncerning a wide range of topics. This was used throughout the experiments. A thesaurus was created automatically for this document collection, based on a snapshot of Wikipedia released on June 3, 2006. The full content and revision hi story at this point occupy 40 GB of compressed data. We use only the link structure and basic statistics for articles, which consume 500 MB (compressed). Details of the information available in Wikipedia at this time, and of the thesaurus that was produced, are shown in Table 2. While processing the 1200 documents about 18,000 terms were encountered that matched at least one article in Wikipedia. These are candidates for inclusion in our thesaurus. Including multiple matches yields 20,000 distinct topics X  X bout 2% of those available in Wikipedia. The disambiguation techniques de scribed in Section 3 greatly reduce the number of multiple matches but do not eliminate them entirely: 47% of terms are ambiguous according to Wikipedia, but this shrank to 17% in the final th esaurus. This residual ambiguity is understandable. Documents in th e collection used to derive the thesaurus are not restricted to any particular domain, so terms may well have several valid sens es. As an example, the news stories talk of Apple Corporation X  X  business dealings and the theft of Piet Mondrian X  X  painting of an apple tree. The full vocabulary of the thesaurus is almost three times larger than the number of topics, because many topics were referred to by multiple terms. 10% of the concepts are expressed by different terms within the document collection itself: e.g. one document talks of President Bush and also mentions George W . Bush. A further 33% were made so w ith the addition of Wikipedia redirects: e.g. Wikipedia adds the colloquialisms Dubya, Shubya and Baby Bush even though these are never mentioned in the (relatively formal) documents. In this context polysemy is desirable, for it increases the chance of query terms being matched to topics and increases the extent to which these are automatically expanded. The thesaurus was a richly connected structure, with each topic relating to an average of 18 others. As a comparison, Agrovoc [7], a manually-produced and professionally-maintained thesaurus of comparable size, contains just over two relations per topic on average. We compared the two systems, Koru and the traditional interface, on the basis of overall task perfo rmance, detailed query behavior, and questionnaires that users f illed out. In the discussion below we refer to the Koru as  X  X opic browsing X  and the traditional interface as  X  X eyword searching X  because this characterizes the essential difference between the tw o. Koru identifies topics based on the user X  X  query and encourages topic browsing; the traditional interface provides plain keyword searching. The first question is whether the knowledge base provided by the thesaurus is relevant and accurate enough to make a perceptible difference to the retrieval process. The most direct measure of this is whether users perform their assigned tasks better when given access to the knowledge-based system. Examination of the documents encountered during the retrieval experience shows that this is certainly the case. Table 3 records a significant gain in the recall, precision, and F-measure, averaged over all documents encountered using the topic browsi ng system. This means that the new interface returned better documents than the traditional one. The greatest gains are made in recall: the proportion of available relevant documents that the system returned. This can be directly attributed to the automatic expansion of queries to include synonyms. Normally gains made in recall are offset by a drop in precision: the inclusion of more terms causes more irrelevant documents to be returned. This was not the case. Table 3 shows no decrease in precision, which attests to the high quality of the Wikipedia redirects from whic h the additional terms were obtained. Indeed there is even a slight gain, though it is not statistically significant. This can plausibly be attributed to recognition of multi-word terms, which users of traditional interfaces are supposed to encase within quotes . We consistently reminded participants of this syntax when familiarizing themselves with the keyword sear ch interface. Despite this, these expert Googlers did not once us e quotation marks, even though they would have been appropriate in 53% of the queries that were issued. The new system perform s this often overlooked task reliably and automatically. Successful topic browsing depends on query terms being matched to entries in the knowledge base. This is typically a bottleneck when using manually defined structures. It is difficult to obtain an appropriate thesaurus to suit an arbitrary document collection, and any particular thesaurus is unlikely to include all topics that might be searched for. Furthermore, sp ecialist thesauri adopt focused, technical vocabularies, and are unlikely to speak the same language as people who are not experts in the domain X  the very ones who require most assistance when searching. Koru does not seem to suffer the same problems. For 95% of the queries issued it was able to match all terms in the query (the term achievements in Example 3 of Table 1 is a typical exception). We hypothesize that the thesaurus extraction t echnique provides a knowledge base that is well suited to both the document collection, being grown from the documents, and user queries, being grown from a vocabulary that has been created by both experts and novices. Our user study supports this hypothesis. The TREC tasks were specifically selected to encourage user interaction, and participants were invariably forced to issue several queries in order to perform each one. We observed significant differences in query behavior between the two systems. One major difference was the number of queries issued: 338 on the topic browsing system vs. 274 for keyword searching. This did not correlate to an increase in time spent using Koru, despite its unfamiliarity and greater complexity. Participants were always encouraged to spend 5 minutes on each task regardless of the system used. There are two possible reasons for the increase: Koru either encourages more queries by making their entry more efficient, or requires more queries because they are individually less effective. Figure 2 indicates that the additional queries are being issued out of convenience rather than necessity. Queries issued by all participants were divided into two groups, one for each interface. Then each group was sorted by F-measure, and the F-measure was plotted against rank. The figure shows that for both topic browsing and keyword searching th e best queries had the same F-measure X  X n other words, the best queries are equally good on both systems. As rank increases a difference soon emerges, however: the performance of keyword searches degrades much more sharply than topic-ba sed ones. In general, the n th best query issued when topic browsing is appreciably better (on average) than the n th best query issued when keyword searching, for any value of n . This clearly shows that the add itional queries issued using Koru are not compensating for any deficiency in performance X  X or Koru X  X  performance is uniformly better. Instead, it probably reflects the way in which Koru presents the individual topics that make up queries. These are automatically identified and presented to the user, and can be included or excluded from the query with a click of the appropriate checkbox. We observed several participants modifying their search behavior to take advantage of this feature. They initially issued large, overly specific queries and then systematically selected combinations of the individual terms that were identified. To illustrate this, suppose a user issu ed a query similar to that in Figure 1 ( american airlines security ) but with additional terms related to security such as baggage check, terrorism, and x-rays . This is a poor initial query because few documents will satisfy all topics. But it forms a base for several excellent queries (e.g. baggage check and terrorism, or baggage check and x-rays ) which in Koru can be issued with a few mouse clicks. The ability to quickly reformulate queries was greatly appreciated by participants; just under half listed it as one of their favorite features. The only way to emulate this behavior manually in the traditional interface is either by time-consuming re-typing (hence fewer queries issued) or by using Boolean syntax (which even our expert Googlers tended to avoid). Next we investigate whether it is easier for users to arrive at effective queries when assisted by the knowledge-based approach. In assessing queries we take acc ount of the number of users who made them. A good query issued by many participants is a matter of common sense, whereas one issued by a lone individual is likely to be a product of expert knowledge or some nugget of encountered information. Figure 3 plots the average F-measure of queries against the number of participants that issued them. At the left are queries issued by only one participant; at the right are ones issued by five and six participants. For the sake of clarity, we have discarded one of the tasks for which the appropriate query terms were particularly easy to obtain. For topic-based queries, performance climbs as they become more common X  X n other words common queries perform better on average than idiosyncratic ones. This is reversed for keyword searching. Par ticipants were able to arrive at effective queries much more consis tently when Koru lent a hand. The gains we have described ar e almost exclusively due to automatic query expansion and t opic identification. Koru also enables interactive browsing of the topic hierarchy, but we were disappointed to see that participan ts rarely bothered to use this facility X  X nd even more rarely did such browsing yield additional query topics. In part this was due to users being put off by inaccuracy in the relations that were offered. For example, several participants mentioned that they found it bizarre that Koru identified homosexuality as an important topic to investigate if one is interested in art . However, this is an exception; typically users felt that the relations were accurate. A more fundamental problem is that even topics that are closely related to a query topic are often irrelevant to the query as a whole. Consider the second example of Table 1, for which most participants issued the query email abuse. Most of the related topics for email ( browsers, internet, AOL, etc) and abuse ( rape, child abuse, torture, etc) are perfectly valid but completely irrelevant to the task. Each participant completed three separate questionnaires, which solicit their subjective impressions of the two systems. After each session they completed a questionnaire that asked for their impressions of the interface used in that session (Koru or the traditional interface). The third questionnaire was completed at the conclusion of the second session and asked for a direct comparison between the two interfaces, to compare topic browsing and keyword searching directly. Table 4 shows the results of the final questionnaire, which asked questions like which of the two system s was more relevant and useful to your needs? The final question asked participants to name their preferred system overall: two-thirds chose the topic browsing system. Other questions indicate that the main reason for this was relevance and us efulness: in other words the additional functionality that Koru offers is relevant to user needs and produces useful results for their queries. In the words of one participant: This was somewhat offset by Koru X  X  additional complexity; unsurprisingly, participants felt that the simpler, more familiar system was easier to navigate a nd use. Simplicity was the reason cited by all participants who chose keyword searching over topic browsing. Several participants took pains to indicate that the difference was marginal. There was no mention of Koru being cumbersome or confusing, just more complex. The above participant was alluding to Koru X  X  presentation of related topics. As we have alr eady described, this feature was barely used and needs substantia l revision. Many participants found it promising however, and two went so far as to list it as their favourite feature. The remainder of the topic browsing system appeared ergonomic and intuitive for users: there were no other frustrations sited in the surveys and almost all users di scovered Koru X  X  full range of features without instruction. We were particularly pleased with the sliding three panel layout. Participants found this unique layout easy to understand and us eful, despite its uniqueness and unfamiliarity. The central idea of this research is to extract thesauri from Wikipedia and to use them to facilitate query expansion both automatically and interactively. Automatic query expansion is a one step process of adding terms that are synonymous or closely related to those in the query; thus improving recall while hopefully maintaining precision [11]. Interactive query expansion aims to present users with useful terms for exploring new queries and broadening their underlying information needs [18]. Thesaurus-based query expansi on is highly dependent on the quality and relevance of the thesaurus. It has been attempted using the manually defined thesaurus structure WordNet [15], both manually [21] and automatically [12], with mixed results. More success has been achieved with automatically generated similarity thesauri [6], which are less accurate but more closely tied to the document collection in question. However, the best results for query expansion have not been obtained with thesauri at all. The most popular and successful strategy is automatic relevance feedback [3], where terms from the top few documents returned are fed back into the query regardless of any semantic relati on. [14] outlines several reasons why individual thesauri can fail to enhance retrieval (e.g.  X  X eneral-purpose thesauri are not specific enough to offer synonyms for words as used in the corresponding document collection X ). Our own research s uggests that thesauri extracted from Wikipedia do not suffe r the same defects. A general theme in the literature is the use of external sources to make richer connections betw een user queries and document collections. Bhogal et al . [2] provide a recent review of ontology-based approaches. Gabrilovich and Markovitch have used external web sources to enhance performance on text categorization tasks [9,10]. Initially they used the hierarchical relationships available from the Open Directory Project 3 and found that their approach was limited by the Project X  X  unbalanced hierarchies and  X  X oisy X  text in the web pages that it linked to [9]. In [10] they changed their external source to Wikipedia, with the perceived advantages that its  X  X rticles are much cleaner than typical web pages, X  with larger coverage and more cross-links be tween articles. Their empirical evaluation  X  X onfirmed the value of encyclopedic knowledge for text categorization X  and suggested applying similar approaches to other text processing tasks such as information retrieval. Our work follows this theme but di ffers in the use of Wikipedia. The essential difference between Gabrilovich and Markovitch X  X  work and our own is that they focus on Wikipedia as a structured collection of documents, while we focus on it as a network of linked concepts and largely ignore the text it contains. Their perspective lends itself to natura l language processing techniques, while ours lends itself to graph and thesaurus based ones. Although a number of interfaces e nhanced with thesauri have been developed, few have been evaluated to assess their impact on users X  query formulation [20]. In a recent example, Shiri and Revie [19] reported that thesaurus enhancement produced substantially different reactions from university faculty (who commented on narrowing effects) and postgraduate students (who appreciated broadening effects). Their participants also commented on difficulties with AND and OR operators and a dislike of separate term entry. Koru expands queries and permits rapid (re)formulation of queries ba sed on simplified term entry, so we did not encounter responses like this. However, we did experience similar results for topic browsing (Section 5.5.2): where inaccurate additional topic suggestions impeded the ability of participants to develop their queries [19]. It is worth noting that studies such as [19] are often limited by the domain restrictions of the selected thesaurus (in this case agriculture). In principle the Wikipedia-based approach should provide much greater domain covera ge and allow future work to http://www.dmoz.org use authentic user queries rather than those specifically generated for an evaluation exercise. This paper has introduced Koru, a new search engine that harnesses Wikipedia to provide domain-independent knowledge-based retrieval. Our intuition th at Wikipedia could provide a knowledge base that matched both documents and queries has so far been borne out. We have tested it with a varied domain-independent collection of documen ts and retrieval tasks, and it was able to recognize and lend a ssistance to almost all queries issued to it, and significantly improve retrieval performance. Koru X  X  design was also validated, in that it allowed users to apply the knowledge found in Wikipedia to their retrieval process easily, effectively and efficiently. The following quote, given by one participant at the conclusi on of their session, summarizes Koru X  X  performance best: Koru currently provides automatic query expansion that allows users to express their information needs more easily and consistently. This one-step proce ss of improvement can only take queries so far, however. To go further, one must enter into a dialog with the searcher and inte ract with them to hone queries and work progressively towards the information they seek. To invoke the imagery offered by Koru X  X  namesake, such a system would allow initial hazy queries to gradually unfold and open out into complete paths across the information space. Our goal in the future is to improve Koru X  X  interactive query expansion facilities until it provides this ability to unfurl queries, thereby living up to its name. [1] Allan, J. (2005) HARD Track overview in TREC 2005 high [2] Bhogal, J., Macfarlane, A., and Smith, P. (2007) A review of [3] Billerbeck, B. and Zobel, J. (2004) Questioning query [4] Bodner, R. and Song, F. (1996) Knowledge-based [5] Crane, D., Pascarello E. and James, D. (2005) Ajax in Action . [6] Curran, J. R. and Moens, M. (2002) Improvements in [7] FAO (1995) Agrovoc Multilingual Agricultural Thesaurus, [8] Finkelstein, L., Gabrilovich, Y.M., Rivlin, E., Solan, Z., [9] Gabrilovich, E. and Markovitch, S. (2005) Feature [10] Gabrilovich, E. and Markovitch, S. (2006) Overcoming the [11] Greenberg, J. (2001) Automatic query expansion via lexical-[12] Grootjen, F. A. and van der We ide, T. P. (2006) Conceptual [13] Hearst, M.A. (1995) TileBars: visualization of term [14] Mandala, R., Tokunaga, T., and Tanaka, H. (1999) [15] Miller, G. A. (1995) WordNet: a lexical database for [16] Milne, D. and Witten, I.H. ( 2007) Extracting Corpus Specific [17] Milne, D., Medelyan, O. and Witten, I. H. (2006) Mining [18] Ruthven, I. (2003) Human in teraction: Re-examining the [19] Shiri, A. and Revie, C. (2005) Usability and user perceptions [20] Shiri, A. and Revie, C. ( 2006) Query expansion behavior [21] Voorhees, E.M. (1994) Query expansion using lexical-
