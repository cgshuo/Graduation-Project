 Click-through rate (CTR) prediction plays a central role in search advertising. One needs CTR estimates unbiased by positional effect in order for ad ranking, allocation, and pric-ing to be based upon ad relevance or quality in terms of click propensity. However, the observed click-through data has been confounded by positional bias, that is, users tend to click more on ads shown in higher positions than lower ones, regardless of the ad relevance. We describe a probabilistic factor model as a general principled approach to studying these exogenous and often overwhelming phenomena. The model is simple and linear in nature, while empirically jus-tified by the advertising domain. Our experimental results with artificial and real-world sponsored search data show the soundness of the underlying model assumption, which in turn yields superior prediction accuracy.
 I.5.1 [ Pattern Recognition ]: Models X  Statistical Algorithms, Theory, Experimentation Search advertising, factor models, exogeneity
A click-through rate (CTR) prediction system for spon-sored search advertising aims to estimate the CTR given a query-ad pair, typically along with other contextual knowl-edge such as about the user. The CTR prediction is piv-otal for ad ranking, allocation, pricing, and the payoff of users and advertisers as well [7]. The estimated CTR serves as a measure of query-ad relevance, and hence should be made independent of other non-relevance factors. In prac-tice, however, there are factors exogenous to the relevance-based click-through system, and often playing a dominant role in the observed click-through data. One classic example is the ad presentation position. A less cautious treatment of these exogenous factors may lead to a sub-optimal CTR pre-diction, and many real-world systems have been seen with these flaws.

We propose a probabilistic factor model as a general prin-cipled approach to studying these exogenous and often over-whelming phenomena. The model is simple and linear in nature, while empirically justified by the advertising do-main. Extensive research has been undergone for correcting positional bias in algorithmic search, among which repre-sentative works are the examination model [12], the cas-cade model [5], and the dynamic Bayesian network (DBN) model [3], but less so for search advertising. Our approach adopts the same factorization assumption as in the exam-ination model, that is, the probability of clicking on an item (result link for algorithmic search and ad for spon-sored search) is the product of a positional prior probability and a relevance-based probability which is independent of position. Moreover, we specialize the concept of position into the ad domain, by incorporating other advertising spe-cific yet important signals, e.g., the query-ad keyword match type and the total number of ads shown.

Other models originating from algorithmic search typi-cally assume that the estimated CTR of an item is depen-dent on the relevance of items shown above on the search result page, as in the cascade and DBN models. These more sophisticated assumptions are appropriate for algorithmic search results where users have a high probability of clicking on one of the result links. For ads, however, the probabil-ity of clicking on ads generally is extremely low, usually a fraction of a percent. As a consequence, the effect of (not clicking) higher ads is a product of factors which are ex-tremely close to one. In this case for example, the DBN positional prior reduces to a negative exponential function, which is a good fit to the empirical distribution derived from our factor model as described below.
Let i denote a query-ad pair, j be the ad position, c be the number of clicks, and v be the number of impressions. The observed CTR is an empirical conditional probability p (click | i, j ). We make the following simplifying yet classic assumptions in sponsored search advertising [2,6,11]: 1. Clicking an ad is independent of its position, given that 2. Examining an ad is independent of its content or rele-Formally, the position-dependent CTR is factorized as The first factor p (click | exam , i ), simply denoted as p position-normalized CTR that represents the relevance of ad. The second factor p (exam | j ), simply denoted as q j flects the positional bias. With this CTR factorization, we now proceed with two natural stochastic models of clicking behavior, and then arrive at the deployed model smoothed by a spike and slab prior [1,9].
It is natural to assume that the number of clicks follows a binomial distribution Given a training data set D = { ( c ij , v ij ) } , we wish to learn the model parameters  X  = ( p, q ), where p is a vector of relevance CTR p i  X  X  and q is a vector of positional priors q  X  X . The likelihood of the training data is The log likelihood is
We regard one of model parameters as latent variable, e.g., q , and derive an EM algorithm to estimate the MLE of both  X  = ( p, q ). Notice though the objective function is not necessarily concave, even w.r.t. one univariate p i q , hence we only seek local optimal. By taking the partial derivatives w.r.t. p i and q j , respectively, we have Since the model parameters are non-negative, we apply the following multiplicative recurrence [4,10]
If the number of trials n is sufficiently large and the suc-cess probability p is sufficiently small, Binomial( n, p ) Poisson( np ). Since ad is such a domain, we now derive the Poisson model which yields a similar yet more efficient up-date. The generative model is The data likelihood is The log likelihood is By taking the partial derivatives w.r.t. p i and q j we have We then derive the following multiplicative recurrence by leveraging the non-negativity of model parameters
It is clear, by comparing with the binomial recurrence, what the Poisson approximation implies for the EM recur-rence. Moreover, it can be shown that the Poisson EM re-currence is globally convergent w.r.t. a univariate p i or q thus guaranteed to find a unique global maximum. When the E and M-steps are combined, however, the recurrence is still convergent but a global maximum is not guaranteed. Further, the E-step in Eq. (14) reveals the tempting X  X ouble-discounting X  trap found in the na  X   X ve estimator as empirical positional CTR q j = P i c ij / P i v ij .
For both empirical and regularization purposes, we impose a gamma prior on the positional factor in the Poisson model Empirically, the observed CTR is geometrically decreasing as the position lowers down [11], exhibiting a good fit to the gamma signature. In practice, inferior positions (e.g., bottom positions in side bar) might suffer from severe data sparsity, particularly clicks; thus regularizing or smoothing those noisy estimates will yield better generalization. The gamma distribution is a convenient choice, since it is a con-jugate prior of Poisson.

The data likelihood (posterior for q ) with a gamma prior is The log likelihood is  X  (  X  ) = X By taking the partial derivatives w.r.t. p i a nd q j we have The EM multiplicative recurrence is The regularization is on q j in E-step only and the interpre-tation of smoothing is obvious. When  X  = 1 and  X   X   X  , the gamma distribution approaches a uniform distribution, i.e., no prior.
A click model or a CTR prediction model aims to estimate a positional-unbiased CTR for a given query-ad pair, i.e., the relevance CTR p (click | exam , i ) or p i . The positional normalized click model described above does exactly this, as well as produces the positional priors p (exam | j ) or q Another view of the factor model is a k NN model smoothed over ad positions; and when the feature space only contains query-ad pairs, k = 1. It is plausible that for query-ad pairs with sufficient historical clicks, the factor model might perform reasonably well. For a principled treatment of the cold-start problem, a trivial extension would be appending queries and ads unigram features into the feature space, and backing-off CTR estimates when new pairs are encountered in prediction time.

The positional normalized click model can also be ap-plied independently of and in conjunction with other click models that estimate relevance-only CTR [1, 4]. More rig-orously, we make the assumption that the positional factor is independent of other relevance factors. In model train-ing, one shall normalize each ad impression by its posi-tional prior v ij q j . In prediction, the CTR predictor learned from position-normalized training data produces exactly the relevance-only CTR, as we desire.
We first simulated the Gamma-Poisson model on an ar-tificial data set generated by a probabilistic model in the same spirit, i.e., given a sound model assumption. The syn-thetic data largely mimics the real-world search ad data, by carefully designed model parameters. Although the simu-lated data cannot fully reflect a real-world system, it has at least two advantages: (1) allowing for a quick study of the effects of a large number of parameters while abstracted from real-world noises; and (2) exposing the true distribu-tions underlying the data to verify the learned model, which is impossible with real-world data.

The data was generated as follows: 1.  X  position j  X  [1 , . . . , m ], generate a q j  X  Gamma(  X ,  X  ), 2.  X  query-ad pair i  X  [1 , . . . , n ], generate a p i  X  Beta(  X ,  X  ); 3.  X  i , generate a number of impressions s i  X  Poisson(  X  ); 4.  X  i , construct a multinomial distribution over positions 5.  X  i , generate an impression allocation vector over posi-6. Derive an n  X  m matrix of CTRs Z = pq  X  ; 7. Derive an n  X  m matrix of Poisson means Y = V. Z , 8. Generate an n  X  m matrix of clicks C  X  Poisson( Y ), The underlying distributions are  X  = ( p, q ). We used the same true distributions to generate a training set D trn = { C, V } and a testing set D tst = { C  X  , V  X  } . The parameters of the generative model are summarized in Table 1.
Parameter Description Value
After generating the training data D t rn = { C, V } , we ex-amined the empirical positional CTR q emp j = P i c ij / P and the positional prior CTR q pri j = ( p i ) q j . The latter will only realize were ads shown randomly. The coupling of po-sitional bias with ad quality bias is evident from the empiri-cal positional CTR curve (the dashed line) in Figure 1(a), in other words, higher-CTR ads tend to be shown in higher po-sitions. A tempting trap is averaging CTR for each position as the positional prior, referred to as the COEC (clicks over expected clicks) model [8, 13]. This leads to the  X  X ouble-discounting X  problem, as revealed by the COEC learned po-sitional priors (the dashed line) in Figure 1(b). The factor model decouples the positional and relevance factors in a principled way, hence perfectly recovers the true positional priors, as shown in Figure 1(b) (the solid and dotted lines).
Since we know the underlying true distributions  X  = ( p, q ), we then compared the learned distributions  X   X  = ( X  p,  X  q ) with the true ones, as plotted in Figure 2. Both scatter plots for relevance CTR p i = p (click | exam) and positional-biased CTR p ij = p (click | position) show that the true and learned CTRs are well calibrated.

We then evaluated the trained factor model using the test-ing set D tst = { C  X  , V  X  } generated from the same underly-ing distributions, with the only statistical fluctuations from Steps 3, 5, and 8, the Poisson and multinomial processes in the generative model. Figure 3(a) plots the true vs. learned relevance CTR p i = p (click | exam) by both the factor and COEC models. In the higher CTR range ( p i  X  ( p i )), both models calibrate well with the observed. In the lower CTR range ( p i &lt; ( p i )), however, both models tend to overesti-mate, in particular the COEC model performs worse. The first reason is legitimate and applies to both models, that F igure 1: Positional bias coupled with ad relevance bias in empirical data. is, we impose a gamma prior on q j and seek MAP estimate. The second reason exposes the flaw in the COEC model. Impressions with lower-than-average CTRs tend to be al-located to lower positions by the design of the generative process. The COEC model performs poorly because of the  X  X ouble-discounted X  positional effect, that is, q j  X  X  for lower j  X  X  are over-penalized, and hence p i  X  X  allocated to those lower j  X  X  (lower-CTR i  X  X ) are more widely overestimated. Fig-ure 3(b) plots the observed vs. predicted positional biased CTR p ij = p (click | position) for both the factor and COEC models. We observed a similar pattern as the relevance CTR calibration plots.

One primary objective of CTR prediction is to rank ads (RankScore = Bid  X  CTR) 1 , hence we plot the ROC curves
R eal-world sponsored search systems may have other vari-ants such as CTR exponent and relevance terms, but our discussion carries over. of click recall vs. view recall 2 in Figures 4 and 5. The ROC curves for the factor and COEC models across all positions are shown in Figure 4, and the area under the curve (AUC) results are summarized in the X  X UC@all X  X olumn in Table 2. The factor model performs better than the COEC model by a small margin. The gain is small because ranking among different positions is relatively easy, given the strong posi-tional effect.

We further compared the ranking performance between the two models at individual positions, mainline (ML) 1 and 4, and sidebar (SB) 8, as plotted in Figure 5 and numeri-cally reported in Table 2. Mainline refers to the positions above the algorithmic results, and sidebar refers to the po-sitions right to the algorithmic results. The factor model performs better than the COEC model for every individ-
T he CTR term in rank score is estimated to capture the ad relevance in terms of click feedback, hence for evaluating a CTR predictive model, it is sufficient to rank ads by CTR estimates alone. F igure 5: Positional ROC curves Factor and COEC. (a) Positional priors per (position-dude) pair by factor and C OEC models (b) Positional priors per absolute position by factor model fo r different dude states Figure 6: Positional priors per (a) relative positions defined by (position-dude) for factor and COEC models, (b) absolute positions by factor model for different dude states. we initialize the model parameters  X  = ( p, q ) to be empirical expectation. This data-driven initialization leverages data sparsity well since, under the multiplicative recurrence as in Eq. (22), initial zero p i  X  X  (due to no clicks) will remain zero. The positional effect is a function of relative position. More precisely, the positional prior q j depends on at least three factors: (1) the page section, e.g., ML or SB; (2) the relative position or the number of ads above in section, e.g., ML2; and (3) the number of ads below in section, e.g., ML2 with dude = 4 has two ads below in ML. The learned posi-tional priors as shown in Figures 6 and 7 support the above observations.

Figure 6(a) plots the positional priors q j where a position j is defined by an (absolute position, dude) pair. The x -axis indexes (absolute position, dude) pairs in ascending order, (a) Positional priors per (position-dude-matchtype) triplet b y factor and COEC models (b) Positional priors per (position-matchtype) pair by factor m odel for different dude states Figure 7: Positional priors per (a) relative posi-tions defined by (position-dude-match) for factor and COEC models, (b) relative positions defined by (position-match) by factor model for different dude states. e.g., x = 1 for (1 , 1), x = 2 for (1 , 2), and so forth. The global descending trend (by examining local peaks x = 4 , 9 , 14 , . . . ) reflects the positional bias. The discrepancy between the factor and COEC models confirms the  X  X ouble-discounting X  trap. The dude factor (the number of ads in ML) comes into play in two ways. Firstly and intuitively, when a dude value moves an ad to SB, the positional prior drops dras-tically (by examining the points at x = 5 , 6 , 10 , 11 , 12 , . . . ). Secondly and more interestingly, when the number of ads below in section increases, the positional prior rises sub-stantially. For example, the positional priors at x = 1 . . . 4 are all for ML1 but with dude = 1 . . . 4, and the increase is almost two folds from dude = 1 to 4. One plausible ex-planation is that more ML ads prevents a user from moving away from ads to algorithmic results. Another evidence for the effect of the number of ads below can be appreciated by comparing the positional priors at x = 1 (for ML1 with dude = 1) and x = 9 (for ML2 with dude = 4). In this case, ML2 has a higher positional prior than ML1. Figure 6(b) plots the positional priors q j where a position j is defined by an absolute position, and one line per dude state. It is evident that, for each dude line, the positional prior drops most drastically at the dude value, that is, moving from ML to SB.

Figure 7(a), where a position is defined by an (absolute position, dude, match type) triplet, shows the effect of match type. For instance, the positional priors at x = 1 . . . 4 are all for ML1 and dude = 1 but with matchtype = 1 . . . 4. The difference between matchtype = 1 (exact match) and matchtype = 2 (broad match) is greater than two folds. Fig-ure 7(b) separate one line per each dude state, by defining position as a (absolute position, match type) pair. The im-pact of match type given a same position can be appreciated by examining local variations for each position (the points at x = 1 . . . 4 , 5 . . . 8, and so on.

We now evaluate the CTR prediction accuracy of the trained models. Besides the ranking metric AUC, we use the metric of relative information gain (RIG), defined as fol-lows.
 where i indexes impressions in the testing data set, y i  X  { 0 , 1 } indicates click or not, p i is the predicted CTR, n is the number of testing impressions, and H ( p ) is the entropy of the empirical expected CTR, i.e., P i y i /n . RIG is a nor-malized average log likelihood under the binomial assump-tion, or a normalized negative cross entropy. RIG can be interpreted as the relative gain of a model p i ,  X  i over a triv-ial yet foresighted one-parameter model by predicting every impression as the empirical expected CTR, that is, when p = P i  X  y i  X  /n,  X  i , RIG = 0.

We compare models with different definitions of relative position, factor or COEC positional handling, and treating match type as exogenous or endogenous variables, as sum-marized in Table 3. The results are shown in Table 4. (a) ROC of Factor and COEC with (position-dude) prior (b) ROC of Factor and COEC with (position-dude-m atchtype) prior Figure 8: Experimental ROC curves for Factor and COEC with (a) (position-dude) positional prior, and (b) (position-dude-matchtype) positional prior.

The best empirical results were obtained by the factor model using the (position, dude, match) positional prior, with a RIG 0 . 1861 and an AUC 0 . 8750. When only us-ing absolute position to derive positional prior, the predic-tion accuracy is much worse, with a RIG 0 . 1126 and an AUC 0 . 8225. The factor model consistently outperforms the COEC model, yet by a small margin. Adding match type into feature vector does not gain incremental rank-ing performance (from an AUC of 0 . 8716 by Factor(pos-dud) to an AUC of 0 . 8713 by Factor(admat-pos-dud)), and even impairs the testing data log likelihood metric (from a RIG of 0 . 1797 by Factor(pos-dud) to a RIG of 0 . 1430 by Factor(admat-pos-dud)), likely due to overfitting as a con-sequence of increasing the number of model parameters by four folds. The ROC curves of the Factor(pos-dude) and Factor(pos-dud-match) models are shown in Figure 8.
To evaluate the impact of the size of training data on the prediction performance, we varied the number of training days from 1, 7, to 13 preceding a same testing day using the empirically best model (Factor(pos-dud-match)), and the re-sults are shown in Table 5.

It is evident that more training data gives better model g enerality, under the position-smoothed k NN model we are experimenting with. In particular, up to 13-day training, both RIG and AUC metrics monotonically increase. With only one day of training data, although the ranking metric is reasonable (AUC = 0 . 8519), the model likely suffers from overfitting (RIG =  X  0 . 1058). Another tuning parameter influencing the trade-off between model capacity and gener-ality is the feature selection threshold factor c , and its choice should be made empirically or by cross validation. For this evaluation, we intentionally used the data from a smaller non-US market (UK), and the comparison results are shown in Table 6.

Factor(pos-dud-mat) c = 100 0 . 1 755 0 . 8 648 W e have presented a probabilistic factor model to estimate CTR for search advertising, particularly to normalize out the positional bias. Our approach is simple yet principled, and empirically justified by the advertising domain. More-over, we have generalized the concept of position to other important factors specific to ads, including keyword match type and ad presentation layout. Hence our approach shall serve as a general framework to study changes exogenous to the relevance-based CTR model. Finally, we conducted ex-tensive experiments with synthetic and real-world ad data, and established that removing those confounding factors is critical to achieve accurate CTR prediction. [1] D. Agarwal, R. Agrawal, R. Khanna, and N. Kota. [2] S. Athey and D. Nekipelov. A structural model of [3] O. Chapelle and Y. Zhang. A dynamic Bayesian [4] Y. Chen, M. Kapralov, D. Pavlov, and J. F. Canny. [5] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [6] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [7] T. Graepel, J. Q. Candela, T. Borchert, and [8] D. Hillard, E. Manavoglu, H. Raghavan, C. Leggetter, [9] H. Ishwaran and J. Rao. Spike and slab variable [10] D. D. Lee and H. S. Seung. Algorithms for [11] F. Pin and P. Key. Stochastic variability in sponsored [12] M. Richardson, E. Dominowska, and R. Ragno.
 [13] W. V. Zhang and R. Jones. Comparing click logs and
