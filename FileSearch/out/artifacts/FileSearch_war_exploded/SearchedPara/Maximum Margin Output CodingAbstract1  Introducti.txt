 Yi Zhang yizhang1@cs.cmu.edu Machine Learning Department, Carnegie Mellon University Jeff Schneider schneide@cs.cmu.edu The Robotics Institute, Carnegie Mellon University In traditional channel coding (Cover &amp; Thomas, 1991; Costello &amp; Forney, 2007), a message is encoded into an alternative (and usually redundant) representation so that it can be recovered accurately after being trans-mitted through a noisy channel. Error-correcting out-put coding (ECOC) applies the idea of channel coding to multi-class classification (Dietterich &amp; Bakiri, 1995; Allwein et al., 2001) and more recently to multi-label prediction (Hsu et al., 2009; Tai &amp; Lin, 2010; Zhang &amp; Schneider, 2011): we encode the output into a code-word, learn models to predict the codeword, and then recover the correct output from noisy predictions. In this paper, we study output coding for multi-label prediction and focus on two important issues. First, the coding needs to be discriminative : encodings for different outputs should be significantly different from each other, so that the codeword for the correct output will not be confused with incorrect ones, even under noisy predictions. This corresponds to the concept of code distance in coding theory and is related to good error-correcting capabilities (Cover &amp; Thomas, 1991). Second, output codes should be predictable . In out-put coding, codewords need to be predicted from the input (instead of being actually transmitted through a channel), so it is critical that codewords are easy to predict. From the channel coding perspective, hav-ing predictable codewords (and thus low prediction er-rors) corresponds to reducing the channel error. In multi-label prediction, finding predictable codeword-s provides an opportunity to exploit the dependency structure in the label space (Zhang &amp; Schneider, 2011). To design output codes that are both discriminative and predictable, we propose a max-margin formulation defined on the encoding transform. For each sample, the prediction from the input should be close to the encoding of the correct output, and at the same time, the prediction should also be far away from the encod-ing of any incorrect output. This is naturally captured by maximizing the margin between the prediction dis-tance to correct and incorrect encodings.
 We then convert this formulation to a metric learning problem of finding the optimal distance metric in the label space, but with an exponentially large number of constraints as commonly encountered in structured prediction problems. In multi-label prediction, howev-er, the output space does not provide a structure for tractable inference, and we use overgenerating (i.e., re-laxation) techniques combined with the cutting plane method to optimize the metric learning formulation. The encoding and decoding operations can be derived from the optimal distance metric in the label space. We conduct our experiments on multi-label classifica-tion of images, text and music. Empirical results show that the proposed output coding scheme outperforms a variety of recent multi-label prediction methods. In this section, we introduce the general framework for multi-label output coding. Then we review three recently-proposed output codes (Hsu et al., 2009; Tai &amp; Lin, 2010; Zhang &amp; Schneider, 2011), where the en-coding is based on random projections, principal com-ponent analysis and canonical correlation analysis, re-spectively. We also argue that these existing output coding schemes are not designed to optimize both dis-criminability and predictability of the codewords. 2.1. Framework
An output coding scheme usually contains three parts: encoding, prediction and decoding. Consider a set of p input variables x  X  X  X  R p and a set of q output variables y  X  Y  X  R q . In multi-label classification, y will denote the label vector, and thus y  X  X  = { 0 , 1 } q We have a set of n training examples: D = ( X , Y ) = n  X  p and n  X  q , respectively.
 Encoding . Following previous work (Hsu et al., 2009; Tai &amp; Lin, 2010; Zhang &amp; Schneider, 2011), we con-sider linear encoding. In this case, the encoding trans-form can be defined by a set of d linear projections where d is the number of projections, each v k ( k = 1 , 2 ,...,d ) is a q  X  1 vector representing a projection direction in the label space, and V is a q  X  d matrix. Given the projection vectors V , the codeword z for an example ( x , y ) is defined: where z is a d  X  1 vector. Alternatively, we can also include the q original labels y = ( y 1 ,...,y q ) into the codeword z , and in this case we have: where I q is a q  X  q identity matrix, V is a q  X  d matrix, and z is a ( q + d )  X  1 vector.
 Prediction . After defining the encoding projections { v k } d k =1 , we then learn prediction models from train-For a label projection v T k y in the codeword, a regres-sion model is usually considered: and for an original label y j ( j = 1 , 2 ,...,q ), a classifier can be learned from training samples: Given a new sample x , a regression model  X  m k predicts: and a classifier  X  p j predicts: where Decoding . Given a new testing sample x , the de-coding procedure recovers the unknown label vector y from our prediction for the codeword z . The predic-tion contains {  X  m k ( x ) } d k =1 and optionally {  X  p  X  y  X  decoding( x , { v k } d k =1 , {  X  m k ( x ) } d k =1 The decoding is usually achieved by maximizing a probability function or minimizing a loss function de-fined on possible label vector y . Since y  X  X  = { 0 , 1 } this optimization is usually combinatorial in nature and intractable. As a result, certain approximation is required to obtain the solution  X  y , e.g., relaxing y into a continuous domain and then rounding the relaxed solution (Hsu et al., 2009; Tai &amp; Lin, 2010) or using approximate inference (Zhang &amp; Schneider, 2011). 2.2. Coding with compressed sensing Multi-label compressed sensing (Hsu et al., 2009) is one of the earliest works that formally defines a multi-label output code. For encoding, each projection vec-tor v k  X  R q (k = 1, 2, . . . , d) is randomly generat-ed as in compressed sensing (Donoho, 2006; Candes, 2006), e.g., a vector with i.i.d. Gaussian or Bernoul-li entries. Thus, the codeword z = ( v T 1 y ,..., v T d contains random projections of the label vector y . Decoding follows the sparse approximation algorithms in compressed sensing. Two popular classes are convex relaxation such as ` 1 penalized least squares (Tropp, 2006), and iterative greedy algorithms such as CoSaM-P (Needell &amp; Tropp, 2008). For example, an ` 1 penal-ized convex relaxation solves the following problem:  X  y  X  argmin where {  X  m k ( x ) } d k =1 are predictions for the codeword z = ( v T 1 y ,..., v T d y ) T , and the ` 1 penalty P q || y || 1 promotes the sparsity of the solution. Note that this problem is solved in relaxed space y  X  R q . Use of random projections is justified in compressed sensing, e.g., by the restricted isometry property, that if the true signal y is sufficiently sparse, one can recov-er y from only a small number of random projections. However, from the output coding perspective, random projections do not specifically promote either discrim-inative or predictable codewords, and thus may not be the most effective method of output coding. 2.3. Coding with principal component analysis Given the n training examples, principal label space transformation (Tai &amp; Lin, 2010) uses the top d prin-cipal components in the label space as the encoding projections: which is solved by performing SVD on the label ma-trix Y and taking the top d right singular vectors. The codeword z = ( v T 1 y ,..., v T d y ) T contains the top d coordinates of y in the principal component space. Given predicted codeword  X  z = (  X  m 1 ( x ) ,...,  X  m for a test sample x , decoding is performed by project-ing  X  z back to coordinates in the original label space and then rounding them element-wise to 0s and 1s: Note that coding with principal components can po-tentially produce discriminative codewords. The top d principal components provide a coordinate system that keeps as much sample variance as possible by any d -dimensional projections. Therefore, generated code-words for training samples tend to be spread out and far away from each other, although this does not ex-actly maximize the minimum codeword distance.
 However, coding with principal components does not promote code predictability. Indeed, finding encoding projections as in eq. (12) is solely based on the label matrix Y and does not involve the input X . As a result, this may generate codewords with large code distance but difficult to predict from the input. 2.4. Coding with canonical correlation analysis Predictability for multi-label output codes is addressed in recent work (Zhang &amp; Schneider, 2011), where out-put projections are obtained by canonical correlation analysis. CCA tries to find an input projection u  X  R p in the feature space and an output projection v  X  R q in the label space such that the projected variables u
T x and v T y are maximally correlated: This can be solved as a generalized eigenvalue prob-lem, and the top d pairs of eigenvectors { ( u k , v k ) } contain the encoding projections { v k } d k =1 . The codeword in this method is defined as z = ( y 1 ,...,y q , v T 1 y ,..., v T d y ) T . For a new sample x , regression predictions for label projections are {  X  m k ( x ) } d k =1 and classification predictions for original by maximizing a joint probability function (including d Gaussian potentials from regression and q Bernoulli potentials from classifiers), or equivalently minimizing the function (Zhang &amp; Schneider, 2011): where  X   X  2 k is the estimated mean squared error for re-gression model  X  m k . Since the problem is defined on the label space y  X  X  0 , 1 } q , approximate inference such as mean-field approximation is used for optimization. Coding with canonical correlation analysis improves the code predictability by choosing the projection di-rections that are maximally correlated with the input. However, this criterion does not optimize the discrim-inability of the generated codewords. In other words, codewords of different outputs may be close to each other, leading to inadequate error-correcting capabili-ties. Consequently, even a small amount of prediction error can significantly affect the decoding result. In this section we propose a max-margin output coding scheme where the encoding transform promotes both discriminability and predictability of the codewords. 3.1. A Max-Margin Formulation As before, codewords are predicted using regression: where each  X  m k () ( k = 1 ,...,d ) is a univariate regres-sion function for predicting v T k y , which is learned as in (4), and  X  M () is the corresponding multivariate re-gression function for the entire codeword V T y . For each sample i , the codeword V T y ( i ) should be both predictable and discriminative. For predictabil-ity, we want  X  M ( x ( i ) ) to be close to the correct code-word V T y ( i ) . For discriminability, we want the cor-rect codeword V T y ( i ) to have a large distance to any incorrect codeword V T y ,  X  y 6 = y ( i ) . In the contex-t of prediction with output coding, it is even more strightforward and effective if the prediction  X  M ( x ( i ) itself has a large distance to any incorrect codeword V T y ,  X  y 6 = y ( i ) .
 Based on these goals, we propose the following max-margin formulation on output projections V : where || || F is the Frobenius norm, || || 2 is the ` 2 nor-m, C is a regularization parameter, 4 ( y ( i ) , y ) is the hamming distance between binary vectors, and {  X  i } n i =1 are slack variables, each for a training sample. With the help from slack variables, constraint (18) requires that for any sample i , the prediction distance to the correct codeword, denoted by ||  X  M ( x ( i ) )  X  V T y must be smaller than the prediction distance to any codeword ||  X  M ( x ( i ) )  X  V T y || 2 2 by a margin of at least small prediction distance to the correct codeword and large prediction distance to incorrect codewords, and hence promotes predictable and discriminative codes. To simplify this formulation, we assume the regression estimated by least squares . Then given training sam-ples ( X , Y ), we define the p  X  q projection matrix P : A small amount of regularization can be added to the diagonal of X T X for numerical stability. Using P , the regression functions can be written in closed form: and Plugging eq. (21) into problem (17), we have the fol-lowing max-margin formulation that is completely de-fined on projections V and slack variables {  X  i } n i =1 3.2. Metric Learning Formulation Problem (22) is a quadratic program with quadratic constraints, and we first convert it to a metric learning problem. Define q  X  q matrix Q : which is the Mahalanobis distance metric induced by V . Also, define a set of new feature vectors: Now we formulate the metric learning problem as: where Q  X  S + q is positive semidefinite. The objective function and constraints are linear in Q and {  X  i } n i =1 We briefly show the equivalence between problem (22) and (25) as follows. For any feasible solution V to (22), we can define Q = VV T  X  S + q . Also, for any feasible solution Q to (25), since Q is positive semidefinite and thus has no negative eigenvalue, we can define V as: where the q  X  q matrix U contains (as columns) the q eigenvectors of Q , and D is the diagonal matrix of eigenvalues. Given this one-to-one mapping be-tween V and Q , we have trace( Q ) = || V || 2 F and  X  sible (or optimal) solution to (25) gives a feasible (or optimal) solution to (22), and vice versa.
 3.3. Incorporating Original Labels and Their As shown in eq. (3), the codeword can also include q original labels, i.e., z = ( y 1 ,...,y q , v T 1 y ,..., v Classifiers {  X  p j } q j =1 can be learned to predict original labels as in (5), and the decoding algorithm can make use of both regression and classifier outputs, e.g., as in eq. (15). In this case, the encoding projection should also be aware of the original labels ( y 1 ,...,y q ) in the codeword, so that the projection part ( v T 1 y ,..., v T can provide complementary information.
 To adapt our max-margin formulation (25) to this new information, we assume that classifiers {  X  p j } q j =1 already been learned, and thus for each sample x we know the classifier output  X  p j 0 ( x ) = P ( y j = 0 | x ) and  X  p j 1 ( x ) = P ( y j = 1 | x ). We have the new formulation: argmin where is the joint probability of label vector y = ( y 1 ,...,y on sample x ( i ) predicted by classifiers {  X  p j } q j =1 In this new formation (27),  X  T i y Q  X  i y is extended in-distance between the regression prediction on the i th sample and the encoding of the label vector y . We expect that the correct label vector y ( i ) should lead to lower values on this term than other y . Similarly, log P ( y | x ( i ) ) is the log-probability of y predicted by classifiers on sample i , and we expect that y ( i ) should give higher values on this term than other label vec-tors y . As a result, we now use the combined term  X  correct and incorrect outputs. The main outcome of this new formulation is that distance metric Q will fo-cus on the constraints where  X  log P ( y | x ( i ) ) alone is not strong enough to ensure the margin. In other word-s, the output coding concentrates on the cases where classifiers {  X  p j } q j =1 alone tend to misclassify. 3.4. Cutting Plane Method with In this section we consider how to solve problem (27). This problem involves an exponentially large number of constraints due to the combinatorial nature of the label space { 0 , 1 } q . As studied in structured predic-tion (Tsochantaridis et al., 2004; Taskar et al., 2003), problem (27) could be solved efficiently, e.g., by the cutting-plane method, if a computationally tractable separate oracle exists to determine which of the expo-nentially many constraints is most violated (Tsochan-taridis et al., 2004). However, without a specific struc-ture (e.g., a chain or a tree) in the label space to enable efficient inference, the separate oracle for problem (27) is computationally intractable.
 To address this issue, we use overgenerating (i.e., re-laxation) (Finley &amp; Joachims, 2008) with the cutting plane method. To use overgenerating technique, we need to relax  X  y  X  { 0 , 1 } q in the constraint of (27) to a continuous domain, e.g.,  X  y  X  [0 , 1] q . However, y  X  X  0 , 1 } q . To handle this, we redefine 4 ( y ( i ) Then noticing log P ( y | x ( i ) ) = P q j =1 log P ( y j redefine: log P ( y j = 0 | x ( i ) ) and log P ( y j = 1 | x ( i ) Using (29) and (30), the new relaxed problem is: argmin where  X  y  X  X  0 , 1 } q in (27) is relaxed to  X  y  X  [0 , 1] This new problem can be solved by the cutting plane method, because the separate oracle (i.e., finding the most violated constraint for each sample i ) is: argmin  X  i y Q  X  i y is quadratic in y given  X  i y defined as (24). So (32) is a simple box-constrained quadratic program. 3.5. Encoding and Decoding After solving Q in (31), encoding projections are ob-tained as (26), and one can choose d , the number of projections, by keeping only the first d columns of V in (26) for any d  X  q . The codeword as in (3) includes original labels, and decoding is performed as (15). Data . We perform experiments on three real-world data sets 1 : an image data set ( Scene ), a text data set ( Medical ) and a music data set ( Emotions ). Scene is an image collection for outdoor scene recognition. Each image is represented by 294 dimensional color features and labeled as: beach, sunset, fall foliage, field, mountain and urban. Emotions is a music clas-sification problem. Each song is represented by 72 rhythmic and timbre features, and tagged with six e-motions: amazed, happy, relaxed, quiet, sad and an-gry. Medical is a clinical text collection, where each document is represented by 1449 words and labeled with ICD-9-CM codes. Many labels in Medical are rare, so we select the 10 most common labels to study. Methods . We compare the proposed max-margin output coding scheme to several recently proposed multi-label output codes as well as a number of other multi-label classification methods:  X  Binary relevance (BR) . This baseline method learns to classify each label independently. It is also called one-vs-all decomposition.  X  Coding with compressed sensing (CodingCS) (Hsu et al., 2009). As reviewed in Section 2.2, this method uses random projections for encoding and sparse approximation for decoding. Specifically, we use
CoSaMP (Needell &amp; Tropp, 2008) for decoding.  X  Coding with PCA (CodingPCA) (Tai &amp; Lin, 2010).
As reviewed in Section 2.3, this method uses princi-pal components for encoding, and PCA reconstruc-tion and rounding for decoding.  X  Coding with PCA-Redundant (CodingPCA-R) . Cod-ingPCA does not include original labels into the codeword. We also try this option to produce more redundancy as in eq. (3). Decoding follows eq. (15).  X  Coding with CCA (CodingCCA) (Zhang &amp; Schnei-der, 2011). As reviewed in Section 2.4, this method uses CCA for encoding. Decoding follows eq. (15).  X  Calibrated label ranking (CLR) (F  X urnkranz et al., 2008). This method combines both one-vs-one and one-vs-all classifiers for multi-label classification. It can also be considered as an output coding method. Method (#Base Models) Mean Standard Error Method (#Base Models) Mean Standard Error Method (#Base Models) Mean Standard Error  X  Multi-label learning by exploiting label dependency (LEAD) (Zhang &amp; Zhang, 2010). This method learns a Bayes network on labels and use it to cap-ture label dependency in multi-label classification.  X  Max-Margin coding (MaxMargin) . Our max-margin coding formulation where encoding is obtained by solving (31) and (26). Decoding follows eq. (15). Evaluation measures . We consider three evaluation measures for multi-label classification:  X  Subset accuracy: rates of correctly classifying all the labels. It is difficult to achieve high subset accuracy.  X  Macro-averaged F-1 score: calculate the F-1 score for each label and take the average over labels. F-1 Method (#Base Models) Mean Standard Error Method (#Base Models) Mean Standard Error Method (#Base Models) Mean Standard Error score is popular since the distribution of positives and negatives for a label is usually imbalanced.  X  Micro-averaged F-1 score: aggregate true positives, true negatives, false positives and false negatives over labels, and then calculate an overall F-1 score. Experimental settings . On each data set, we per-form 30 random runs and report means and standard errors of each evaluation measure. The number of training samples in each random run is set to 300. For CodingCS , the number of projections d is set to 100 to provide highly redundant codewords. For CodingP-CA , CodingPCA-R , CodingCCA and MaxMargin , the number of output projections is set to the maximum Method (#Base Models) Mean Standard Error Method (#Base Models) Mean Standard Error Method (#Base Models) Mean Standard Error possible number: the number of original labels. For all methods, base regression models are ridge re-gression and base classifiers are ` 2-penalized logis-tic regression, and their regularization parameters are chosen by cross validation. For LEAD, the Bayes net is learned using the score-based searching algorithm in the Bayesian Net Toolbox 2 . For decoding that fol-lows (15),  X  is set to 1, i.e., classifiers and regression models are equally weighted in decoding. The parame-ter C in (31) is set to 10 6 . Most methods need to round their final predictions into 0/1 assignments (e.g., from a probability forecast or a relaxed solution to the la-bel assignment), and in these cases we use 0 . 5 as the threshold without further optimization.
 Empirical Results. Results for the Scene data set are shown in Table 1 -Table 3; results for Medical are shown in Table 4 -Table 6; results for Emotions are shown in Table 7 -Table 9. Each table contains one e-valuation measure. From the results we can see:  X  BR provides a solid baseline with good performance.  X  CodingCS generally underperforms, indicating that encoding with random projections is not effective.  X  CodingPCA-R outperforms CodingPCA because
CodingPCA-R uses more redundant codewords.  X  LEAD X  X  performance is not stable across data sets.
Structure learning for bayes nets is still challenging.  X  CLR performs comparably to BR, despite the fact that it is one of the most redundant methods in terms of the number of base models used.  X  CodingPCA-R, CodingCCA and MaxMargin are most successful. Their codewords include both label projections and original labels, and their decodings combine both regression and classification outputs.  X  MaxMargin outperforms CodingCCA and Coding-
PCA, because max-margin encoding promotes both code discriminability and code predictability.  X  CodingCCA performs better than CodingPCA-R, showing the importance of predictable codewords. Our work follows the direction of multi-label output coding (Hsu et al., 2009) and is motivated by the recent success of coding with PCA (Tai &amp; Lin, 2010) and CCA (Zhang &amp; Schneider, 2011) and their connections to code distance and code predictability. Our max-margin formulation is converted into a metric learning problem, as in (Weinberger et al., 2006), but with a metric defined for the label space and an exponential number of constraints caused by label combinations. The optimization technique developed for structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), more specically the cutting plane method with overgenerating (Finley &amp; Joachims, 2008), is used to solve our metric learning problem. Discriminability and predictability are both impor-tant for output codes. In this paper we propose a max-margin formulation for multi-label output coding, which promotes both discriminative and predictable codes. We convert this formulation into a metric learn-ing problem in the label space, and combine overgener-ating with the cutting plane method for optimization. Our method outperforms many existing methods on multi-label image, text and music data sets.

