 Wei Lee Woon  X  Stuart Madnick Abstract A novel method for automatically constructing taxonomies for specific research domains is presented. The proposed methodology uses term co-occurrence frequencies as an indicator of the semantic closeness between terms. To support the automated creation of taxonomies or subject classifications we present a simple modification to the basic distance measure, and describe a set of procedures by which these measures may be converted into estimates of the desired taxonomy. To demonstrate the viability of this approach, a pilot study on renewable energy technologies is conducted, where the proposed method is used to construct a hierarchy of terms related to alternative energy. These techniques have many and subsequent prediction of future developments in the technology and research. Keywords Taxonomy creation  X  Technology mining  X  Data mining  X  Information distance  X  Asymmetric distance 1 Introduction 1.1 Technology mining The planning and management of research and development activities is a challenging task that is further compounded by the large amounts of information which researchers and deci-sion-makers have at their disposal. Information regarding past and current research is avail-able from a variety of channels, examples of which include publication and patent databases. sifting through these databases is time consuming and subjective, while on the other, they provide a rich source of data which, if effectively utilized, will allow a well-informed and comprehensive research strategy to be formed.

There is already a significant body of research addressing this problem (for a good review, trends and developments [ 8 , 9 , 25 , 26 ].

We also note that taxonomy creation has been addressed before in the literature though the approaches used have been somewhat different. For example, the study described in [ 3 ]uses a distance measure based on the number of shared keywords between clusters of abstracts; a to our approach in that it is based on Google search terms X  X owever, the creation of the taxonomy tree is based on a greedy algorithm while the approach proposed here attempts to take the global structure of the taxonomy into consideration. Certainly, in view of the many in many of these areas.

For researchers and managers new to a field, it is critical to quickly gain a broad under-with potential for growth and which hence need to be prioritized. The work described in this paper targets this important aspect of technology-mining. Specifically, we seek to answer the following research question: given a collection of keywords relevant to a research area of interest, is it possible to automatically organize these keywords into a taxonomy which following issues will also be addressed: 1. Derivation of an asymmetric measure of distance between keywords which indicates the 2. Investigation of methods for converting these distance measurements into an estimate of 3. A pilot study in renewable energy as a demonstration of the proposed approach. 1.2 Pilot study To provide a suitable example on which to conduct our experiments and to anchor our discus-sions, a pilot study was conducted in the field of renewable energy. The incredible diversity of renewable energy research offers a rich and challenging problem domain on which we can test our methods. Besides high-profile topics like solar cells and nuclear energy, renew-able energy related research is also being conducted in fields like molecular genetics and nanotechnology. 2 Keyword distances for taxonomy creation In the following subsections, the methods used for data collection and analysis will be dis-cussed in some detail. The overall process will consist of the following two stages: 2. Use of this indicator to automatically construct a subject area hierarchy or taxonomy 2.1 Keyword distances two areas of research, represented by appropriate keyword pairs. Existing studies have used approaches only utilize information from a limited number of publications at a time, and As such, extending their use to massive collections of hundreds of thousands or millions of documents would be computationally unfeasible.

Instead, we choose to explore an alternative approach which is to define the relationship between research areas in terms of the correlations between occurrences of related keywords number of scientific publications implies a close relationships between the two keywords. keywords, is it possible to infer the overall subject taxonomy of a given domain of research?
In practice, exploiting this intuition is more complicated than might be expected, particu-larly because an appropriate normalization scheme must be devised. It is certainly not clear what the exact form of this distance expression should be; even more importantly, can it be grounded in a rigorous theoretical framework such as probability or information theory? As method utilizes the term co-occurrence frequencies as an indication of the extent to which two terms are related to each other. This is defined as: where NGD stands for the Normalized Google Distance , t 1 and t 2 are the two terms to be compared, n 1 and n 2 are the number of results returned by a Google search for each of the the terms. Finally, N is the size of the sample space for the  X  X oogle distribution X , and can be approximated by the total number of documents indexed by Google or the search engine being used, if this is not Google.

While a detailed discussion of the theoretical underpinnings of this method is beyond the intuitive, and is based on the normalized information distance, given by: where x and y are two strings (or other data objects such as sequences, program source code, The distance is hence a measure of the additional information which would be required to encode both strings x and y given that an encoding of the shorter of the strings is already final value of the distance lies in the interval [0,1].

In the present context, the Kolmogorov complexity is substituted with the prefix code length, which is given by: Substituting ( 3 ),( 4 )  X  ( 2 ) leads to the expression in Eq. ( 1 ).

To adapt the framework above for use in the context of technology mapping and visuali-zation, we introduce the following simple modifications: 1. Instead of a general Web search engine, the prefix code length will be measured using 2. N is set to the number of hits returned in response to a search for  X  X enewable+energy X , 3. We are only interested in term co-occurrences which are within the context of renewable As explained in [ 7 ], the motivation for devising the Google distance was to create an index which quantifies the degree of semantic dissimilarity between objects (words or phrases) which reflects their usage patterns in society at large. By exploiting the same intuition, it would be logical to assume that a similar measure which utilizes term co-occurrence patterns in the academic literature, instead of a general Web search engine, would be able to more appropriately characterize the similarity between technology related keywords in terms of their usage patterns in the scientific and technical community. 2.2 Asymmetric distances for detection of subclassing a given distance function d (, ) : However, there are cases where we expect the relationships between objects being mapped to be asymmetric. Indeed, the present situation is one such example where, for two keywords being studied, it is likely that the information attached to one keyword is a subset of the information associated with the other keyword. This can indicate that the field of research linked to one of the keywords is a subtopic of the other. We postulate that these asymmetries asymmetry. Recall that the numerator of the expression in Eq. ( 2 ) quantifies the amount of information which is needed to produce two objects x , y , given an encoding of the object with the lesser information content. Choosing the object with less information enforces the symmetry condition but also removes the desired directional property.
Thus, a directional version of this distance can easily be obtained as follows: In this equation, the expression To see how this helps us, consider the scenario where object y is a subclass of object x ;in this case, we expect that y would already incorporate most of the information regarding x .
Take the example of a circus elephant, which can be considered a subclass of elephant we could express this as follows: of subclassing. K ( x ) again serves as a helpful normalization term, for example, to guard against the trivial case where K ( x ) = 0  X  K ( x , y ) = K ( y ) .

Finally, as before, we can obtain a form of this equation suitable for use with search tional version of the NGD: we have: Example 1 where, as suggested in [ 7 ], N can be approximated by any suitably large number. As can be
It should be noted, however, that this distance provides a measure of subclassing strictly in the context of term occurrences in academic texts; i.e. if the majority of texts in which term A occurs also contain term B (but not vice versa), this is an indication that term A is frequently researched in the context of term B, and is a  X  X ubclass X  of term B only in this sense. In many cases this would correlate to other notions of subclassing, though there will definitely be exceptions.
To concretize this further, we look at two further examples, one of which is on classes of technology, and the other from biology (document counts in these examples are from Google Scholar): Example 2 Example 3
In Example 2 ( computer science , artificial intelligence ), the  X  X rtificial intelligence X  is a subclass of  X  X omputer science X . However, Example 3 ( mammal , dog ) is an exception to this rule in that the is a much more common topic of research than  X  X ammal X , even though biologically it is a member of the class of mammals. More critically, articles about dogs frequently do not even contain the term  X  X ammal X  since this is already well known, and may not be relevant to the issue being studied. We concede that this is an inherent shortcoming of this measure of distance, and one which would be difficult to overcome without significantly extending the proposed approach, which is beyond the current scope. On the other hand, our method does produce useful results in many cases, as illustrated in Examples 1 and 2 above, as well as in our pilot study, described later.  X  X  X  X 
NGD can now be used to analyze collections of technology related keywords from the perspective of graph theory. Given a collection of keywords V , we can construct a directed and the weighting function w : E  X  R is given by:
In this context, a keyword taxonomy is represented by a subgraph ( V , E  X  ) ,where: 1. E  X   X  E , | E  X  |=| V | X  1. 2. All nodes except one have exactly one incoming edge. 3. ( V , E  X  ) is connected, and there are no cycles.

In graph theory this construct is known as an arborescence , which is basically the directed equivalent of a spanning tree (Fig. 1 ). However, for any digraph there could be a very large number of such arborescences, any one of which could potentially be a valid keyword tax-onomy. To solve this, we choose to follow the principle of parsimony in suggesting that the arborescence with the minimum total edge weight provides the best possible organization of the terms. In graph theory the problem of finding this arborescence is referred to as the minimum arborescence problem.

To demonstrate that this principle works, it is used to automatically infer the taxonomic structure of two small selections of renewable energy related keywords, and these are shown that approximately reflect the inter-dependencies between the terms. 2.3 Weighted cost functions As mentioned above, when searching for the most likely taxonomy of keyword terms, the arborescence.

For example, consider the taxonomy in Fig. 2 a. We see that sugars has been classified under the biomass subtree. However, genomics and model plant have subsequently been placed as subclasses of sugars. However, it would appear that the aspect of genomics research related to sugars may be separate from the subset of research in sugars related to biomass. We can check this by studying the directional distances:  X  X  X  X  NGD ( genomics , sugars ) = 0 . 336, both of which are the smallest values in the respective rows of the distance matrix. However, what greater than the genomics subtree might be better portrayed as a separate branch of research from biomass.
Another example is shown in Fig. 2 b, where the term cell has attracted a large number maps . This is a problem which is frequently encountered, in which very broad terms (such as
In common with many other inverse problems, the two issues stated above can be linked to the fundamentally ill-posed nature of the problem. Not only are we attempting to estimate the underlying taxonomy from indirectly observed and noisy aggregate data but the  X  X ruely optimal X  structure of the taxonomy itself is also difficult to define by human experts.
However, one way in which we can try to improve the situation is by incorporating infor-inconsistencies within the generated taxonomies. As an initial measure, we propose the fol-lowing weighted cost function for evaluating the quality of generated taxonomies: a given node. The co-efficients  X  i are weights which determine the extent to which the score the minimum arborescence).
  X  X  X  X  to the immediate ancestor of a given node, while the influence of subsequent ancestors grad-( a ) (b) ually diminishes. A number of weighting functions were tested and in the following sections we present results generated using three such functions: 1. Uniform weighting  X  1 ... n = 1 2. Linear weighting  X  i = n  X  i 3. Exponential weighting  X  i = 1 2 i As an example, taxonomies containing the same keywords have been generated by opti-mizing the linear weighted cost function, and are shown in Fig. 3 (optimization was done using a genetic algorithm (GA), which is discussed in the following section). As can be seen from these two figures, the use of the weighted cost function produces some noticeable improvements in the resulting taxonomies. In particular, the sub-tree genomics  X  model plant ing from cells is now more structured (in Fig. 2 b, this subtree was mainly a flat hierarchy. Accordingly, the two sense of cells have now been appropriately divided into two separate subtrees, each of which shows a reasonable inheritance structure. 3 Methods and data 3.1 Genetic algorithms for taxonomy optimization While efficient algorithms exist for standard problems such as the minimum spanning tree (Kruskal X  X  algorithm, Prim X  X  algorithm [ 13 ]), as well as Edmond X  X  algorithm for the mini-mum arborescence problem (described in Appendix A), the situation is less clear in cases when the cost function incorporates custom modifications or constraints.
 the number of possible taxonomies grows exponentially with the number of nodes, exhaustive searches quickly become computationally infeasible.
 As such, it was decided to use a GA to optimize the automatically generated taxonomies. While not the only applicable technique, this approach does provide a very flexible frame-work in which a variety of different cost functions can be easily tested without having to devise a new optimization algorithm each time. In addition, GAs have been used in similar applied to problems involving undirected trees.

The basic components of any GA are: 1. A method for encoding a full set of the parameters to be optimized, where each encoded 3. A set of cross-over and mutation operations on the chromosomes. Traditionally, GAs
Once all these components have been specified we are ready to attempt the GA optimiza-tion. Broadly, this proceeds as follows: 1. Initialization of the GA by creating a population of randomly generated individuals. 3.2 Data collection To conduct the pilot study on renewable energy, energy related keywords were extracted using ISI Web of Science X  X  database in the following manner: a search for  X  X enewable+energy X  was top 35 hits were used. In total, 72  X  X uthor Keywords X , i.e. keywords specified by the authors were extracted (the complete lists of keywords are provided in Appendix B of this paper).
Once the keywords were collected, the distances discussed in Sect. 2.1 could be calcu-lated where, as discussed, hit counts obtained from the Google scholar search engine were used. A number of other alternatives were considered including the Web of Science, Inspec, Ingenta, Springer and IEEE databases. However, our preliminary survey of these databases indicated that zero hits were returned for a large number of keyword pairs. There appeared to be two main reasons for this observation: Firstly, most of these search engines simply did not index a large enough collection to provide ample coverage of the more specialized of the keywords that were in the list; furthermore, not all of the search engines allowed full text searches (the Web of Science database, for example, only allows searching by keywords or topics) X  X hile sufficient for literature searches and reviews, keyword searches simply did not provide sufficient data for our purposes.

Even when using Google scholar, there were also a number of keyword pairs for which = 2 . 22  X  10  X  16 ), n 4Results Having described the proposed approach as well as the specific procedures and techniques adopted, we are now in a position to discuss the experimental results and subsequently to evaluate the effectiveness of these methods.
 As discussed in the previous section, 72 keywords were collected using the ISI Web of collection was then randomly divided into two subsets X  X et 1 contains 35 keywords, and Set 2 contained the remaining 37 keywords. In addition, any occurrences of the stop-words described in Sect. 3.2 were also removed before analysis was carried out. In the following subsections the observations obtained which each of the sets are discussed in greater detail. 4.1 Set 1 The proposed methods were first applied to the keywords in Set 1. Taxonomies were gener-ated using Edmond X  X  algorithm and GA optimization using first the uniform weighting then the exponential weighting functions; these are presented in Fig. 7 .
The main observations were: 1. In general, the generated taxonomies appear to capture the high level orderings of the 2. The results obtained using the weighted schemes were almost identical X  X hen  X  i was 3. However, there is a bigger difference between the taxonomy generated using Edmond X  X  4.2 Set 2 Next, the second set of keywords (Set 2) was organized into a taxonomy using the proposed approach. The resulting graphs are shown in Fig. 8 .

Our observations on these graphs are: 1. As before, the taxonomies show a number of significant clusters, which include solar , 2. However, it was observed that there is much less consistency amongst the four taxono-4. The taxonomies created when  X  i was linearly and exponentially decreasing were very 5 Discussions This paper presented a novel approach for automatically organizing selections of keyword into taxonomies. In addition to being an important step in the ontology creation process, these techniques can be hugely useful to researchers seeking a better understanding of the overall research landscape associated with the collection being studied.

On the other hand, the results obtained indicate that there are many technical problems which need to be overcome before this methodology can be used in a fully automated manner. The main issues include: 1. Complexity: as with many other inverse problems, inferring the underlying taxonomy of 2. Inconsistent quality of data; data obtained from publicly available sources are unregu-3. Non-uniform coverage: the number of hits returned for very general or high-profile key-
That said, the methods described in this paper were only intended as an early demonstra-tion of the proposed approach, and in spite of the above-mentioned problems, we believe that the results described here already demonstrate the potential of the approach.
It must also be conceded that while promising, the results were still far from perfect and contained a number of irregularities as described in the paper. These may be viewed from a number of perspectives; on the one hand, they could be manifestations of hitherto unknown relationships or underlying correlations which may only be understood after a more in-depth or  X  X rong X  X  X he which in turn depend on the data available to the algorithm X  X othing more, nothing less; it would appear that these requirements are satisfied for at least a reasonable proportion of understand or to explain, as has also been observed in some of the examples presented here.
Avenues for future research include working more closely with domain experts to improve and validate the results produced using the proposed methodology. We also plan to screen alternative distance measures, which might provide a more robust indication of subclass-ing. An additional issue would be to test a broader range of optimization techniques and genetic operators.
 Appendix A: Edmond X  X  algorithm Edmond X  X  algorithm [ 13 ] provides an efficient means of finding the minimum arborescence. Briefly, this is as follows: Algorithm Edmonds ( V , E ) Input: A digraph consisting of vertices V and edges E Output: Minimum weight arborescence E  X  1. E  X   X  X  X  , V  X   X  V 2. for v  X  V  X  3. do 4. Identify u = argmin u { w [ e ( u ,v) ]: u  X  V , u = v } 5. E  X   X  E  X  + { e ( u ,v) } 6. if no cycles formed, 7. Expand pseudo-nodes (if any), and return E  X  8. else 9. Contract the nodes V  X  V in each cycle into a pseudo-node v 10. V  X   X  V  X   X  V , V  X   X  V  X  + v 11. Replace all incoming edges with: 12. For each outgoing edge, set: 13. Repeat from step 2 until all cycles have been eliminated Appendix B: Renewable energy related keywords Biomass, CDS, CDTE, energy efficiency, gasification, global warming, least-cost energy policies, power generation, populus, qtl, renewable energy, review, sustainable farming and biomass-fired power boilers, carbon nanotubes, chemicals, co-firing, coal, corn stover, elec-tricity, emissions, energy balance, energy conversion, energy economy and management, age, gasification, genome sequence, genomics, high efficiency, hydrolysis, inorganic mate-transesterification.
 References Author biographies
