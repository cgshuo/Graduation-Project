 yen-kuang.chen, christopher.j.hughes @intel.com Traditional decomposition-based solutions to Support Vector Ma-chines (SVMs) suffer from the widely-known scalability problem. For example, given a one-million training set , it takes about six days for SVMLight to run on a Pentium-4 sever with 8 G-byte mem-ory. In this paper, we propose an incremental algorithm, which performs approximate matrix-factorization operations, to speed up SVMs. Two approximate factorization schemes, Kronecker and in-complete Cholesky , are utilized in the primal-dual interior-point method (IPM) to directly solve the quadratic optimization prob-lem in SVMs. We found out that a coarse approximate algorithm enjoys good speedup performance but may suffer from poor train-ing accuracy. Conversely, a fine-grained approximate algorithm en-joys good training quality but may suffer from long training time. We subsequently propose an incremental training algorithm, which uses the approximate IPM solution of a coarse factorization to ini-tialize the IPM of a fine-grained factorization. Extensive empirical studies show that our proposed incremental algorithm with approx-imate factorizations substantially speeds up SVM training while maintaining high training accuracy. In addition, we show that our proposed algorithm is highly parallelizable on an Intel dual-core processor.
 G.1.6 [ Optimization ]: Quadratic programming methods Algorithms Support Vector Machines, Interior-Point Method, Matrix Factoriza-tion
Support Vector Machines (SVMs) are a core machine learning technology. They enjoy strong theoretical foundations and excel-Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. lent empirical successes in many pattern recognition applications such as handwriting recognition [5], i mage retrieval, and text clas-sification [15]. Unfortunately, SVMs do not scale well with respect to the size of training data. Given n training instances, the time to train an SVM model is about O ( n 2 ) in the average case. This excessive training time makes SVMs impractical for large-scale ap-plications. In this paper, we first present the computational issues of SVMs. We then propose algorithms to speed up SVMs.

Let us consider SVMs in a binary classification setting. Given a set of training data X = { ( x i ,y i ) | x i  X  R d } n an observation vector, y i  X  X  X  1 , 1 } is the class label of x n is the size of X , we apply SVMs on X to train a binary clas-sifier. SVMs aim to search a hyperplane in the Reproducing Ker-nel Hilbert Space (RKHS) that maximizes the margin between two classes of data in X with the smallest training error [24]. This prob-lem can be formulated as a quadratic optimization problem using the method of Lagrangian multipliers [6] as follows, where [ Q ] ij = y i y j K ( x i , x j ) ,and  X   X  R n is the Lagrangian multiplier variable (or dual variable). Notice that the dual formula-tion D (  X  ) utilizes the kernel trick [1] by specifying a kernel func-tion to define the inner-product between two mapped instanced in RKHS space. The only requirement on the kernel function K is that it has to be positive semi-definite (psd), which theoretically guar-antees that the RKHS space is mathematically valid [24]. When the given kernel function K is psd, the dual problem D (  X  ) is a con-vex Quadratic Programming (QP) problem with linear constraints, which can be solved via the Interior-Point method (IPM) [19]. The bottleneck of the SVM training is the IPM solver to the dual for-mulation of SVMs in ( 1).
 Currently, the most effective IPM algorithm is the primal-dual IPM [19]. The principal idea of the primal-dual IPM is to remove inequality constraints using a barrier function and then resort to the iterative Newton X  X  method to solve the KKT linear system related with the Hessian matrix Q in D (  X  ) . The computational cost is O ( n 3 ) and the memory usage is O ( n 2 ) . Faster sampling-based and decomposition-based algorithms (discussed in Section 5) have recently been developed to reduce the memory use and the com-putation cost. However, their training time on large datasets is still quite excessive. For example, on a Pentium 2 . 8 G-HZ machine with 8 G-bytes RAM, it takes about six d ays to run a one -million binary-class dataset using SVMLight [14].

In this paper, we explain how to speed up the SVM training via an incremental algorithm with approximate matrix-factorization meth-ods. We use two approximate factorization schemes, Kronecker and incomplete Cholesky , to directly solve the IPM. We further propose an incremental training algorithm, which uses the approx-imate IPM solution of a coarse factorization to initialize the IPM of a fine-grained factorization. Extensive empirical studies show that our proposed incremental algorithm with approximate factor-ization substantially speeds up the SVM training while maintaining high training accuracy. Furthermore, our algorithm is highly paral-lelizable and can take advantage of the Intel many-core processor architecture. Our experiments demonstrated that the six-day train-ing task required by SVMLight took just half a day using a parallel version of our incremental approximate matrix factorization algo-rithm on an Intel dual-core machine.
The factorization scheme aims to directly speed up the QP prob-lem on the whole training data. We propose three approaches in applying factorization: approximate factorization , incremental fac-torization , parallel factorization . In the following, we will first present some preliminaries on matrix factorization for the primal-dual IPM algorithm in Section 2.1, followed by two approximate factorization approaches in Section 2.2 and Section 2.3, respec-tively. The incremental approach and its parallel version will be giveninSection3.
According to the primal-dual IPM algorithm [3, 19], the KKT linear equations of the dual formulation in (1) can be written as 2 6 6 4 =  X  where  X  ,  X  ,and  X  are the dual variables in SVMs for constraints  X   X  C ,  X   X  0 ,and y T  X  =0 , respectively, and vec(  X  i ) means generating a vector with the i -th component as  X  i . The matrix at the left side is a (3 n +1)  X  (3 n +1) sparse matrix. The details on derivation can be referred to [25].

The KKT matrix in (2) is huge when n is large. The linear equations can be solved via matrix factorization on the KKT ma-trix. Some common factorization methods are QR, LU, Cholesky, LUL T , Incomplete Choleksy, or Kronecker factorization. Table 1 shows the computational cost of each factorization method. Table 1: Computational cost of matrix factorizations. The meaning of p in incomplete Cholesky will be explained in Sec-tion 2.3.

When n is large, the factorization cost can be very high. We thus have to exploit the structure of the KKT matrix, such as sparsity, diagonal, or low rank. Take SVMs as an example, only Q nn KKT matrix in (2) is a dense matrix. All other blocks of matrices are either diagonal or in vector format. In addition, although Q is dense, it could be in low-rank or be approximated by a low-rank matrix. All these information can be utilized to help solve the KKT linear equations in (2).

Let us revisit (2) and give a concise expression on the Newton step y pd =[  X  ,  X  ,  X  ,  X  ] T . We yield the following ex-pressions: where  X  and z depend only on [  X  ,  X  ,  X  ,  X  ] from the last iteration as follows: We can see that matrix inverse has to be done on  X  for solving  X  in Eqn. 5 and  X  in Eqn. 6. Next, we show how Kronecker factorization and incomplete Choleksy factorization can be used to compute  X   X  1 and hence the Newton step y pd .
We provide only an overview on Kronecker factorization. For more details please refer to [26]. Given a symmetric matrix A R n  X  n with n = n 1  X  n 2 , we can factorize A into two much smaller matrices, or where B  X  R n 1  X  n 1 and C  X  R n 2  X  n 2 are both positive definite. We call this formulation the Kronecker factorization of a positive definite matrix. Some theoretical properties of the Kronecker prod-ucts [13] are listed below, which provide a foundation to apply the Kronecker factorization to solve the IPM.

T HEOREM 1. If B and C are n 1  X  n 1 and n 2  X  n 2 respectively, then (a) ( B  X  C ) = B  X  C ; (b) tr( B  X  C )=tr( B )tr( C ) ; (c) | B  X  C | = | B | n 2  X | C | n 1 ; and (d) If B and C are nonsingular, then ( B  X  C )  X  1 = B  X  1  X  For an n  X  n matrix A , the computational complexity is O ( n invert it or to perform eigendecomposition on it. The memory us-age is O ( n 2 ) . By means of the Kronecker factorization, the com-putational complexity is reduced to O ( n 3 1 )+ O ( n 3 2 above theorem. The memory usage is reduced to O ( n 2 1 )+ O ( n When we set n 1  X  n 2 , the computational complexity is reduced torization cost. When that cost is added, as shown in [18], the computational complexity is O ( n 2 ) .

The problem of Kronecker factorization is how to estimate B = [ b ij ] and C =[ c ij ] when A is given. We simply use a separable framework proposed in [18] to achieve this goal by minimizing the least-squared (LS) error e A ( B , C )= A  X  B  X  C 2 F . More details can be refered to [26]. When B and C have almost the same size, i.e., n 1  X  n 2 , the LS method has the computational cost of O ( n and memory cost of O ( n ) . The LS method only involves simple matrix-trace operations that are easy to parallelize. (We discuss parallelization in Section 4.3.)
At each iteration of the interior-point method, a linear system must be solved to achieve the Newton step y asshowninequa-tions from (3) to (6). Since the Hessian matrix  X  in (7) must be updated once at each iteration, we have to factorize  X  using the Kronecker product once at each iteration so as to compute the in-verse of  X  . The Hessian hence has to be stored in memory. Fortu-nately, from the algorithms of Kronecker factorization in [26], we can see that at each time, only a block of the Hessian Q with n el-ements is needed to compute b ij or c ij . We can cache such a block ( n elements) in the memory whenever it will be used.
Since at each iteration of IPM, we actually compute the inverse of a dense matrix Q plus a diagonal matrix D , it is beneficial to em-ploy incomplete Cholesky factorization (ICF) to factorize Q into a truncated lower-triangular matrix H ,i.e. Q  X  HH T ,where H  X  R n  X  p and p n .Inotherwords, H is somehow  X  X lose X  to Q  X  X  exact Cholesky factor G . Please note that ICF is especially suitable to factorizing a psd matrix, where Choleksy factorization cannot be applied. In the following, we review the algorithm of factorizing matrix Q . Then, we show how ICF is used for solving SVMs.
We use the same notations as [2] for a clear description. The factorization algorithm depends on a sequence of pivots . Let a vec-tor v to be the diagonal of Q and suppose the current pivots are { i 1 ,...,i k } ,the k -th iteration is
H ( i k ,k )=
H ( J k ,k )= Q ( J k ,i k )  X  where J k denotes the sorted complement of { i 1 ,...,i k gorithm iterates by greedily choosing the column such that the ap-proximation of Q by H k H T k or the size of J k is satisfied with a given threshold. We can also set the maximum size of J k noted as p ) as a stopping criterion. Th e approximation quality is measured by the difference of the sums of the singular values, i.e., tr( Q  X  H k H T k ) . The total computational cost of ICF is O ( p Once H has been calculated, the matrix inversion lemma or Sherman-Morrison-Woodbury formula [10] can be applied to calculate the inverse of Q + D as follows: ( Q + D )  X  1  X  ( D + HH T )  X  1 where E = I + H T D  X  1 H is a p -by-p matrix, not an n -by-n ma-trix. Recall that in SVMs, we have to inverse  X  in equation (7). We can see that  X  is the sum of the predefined dense matrix Q and a diagonal matrix diag(  X  i  X  i +  X  i C  X   X  i ) . By factorizing Q into HH using the ICF algorithm, at each iteration of IPM, we do not need to invert an n -by-n matrix. Instead, we only need to invert a p -by-p matrix E .

The above formula has been widely used in the optimization community for different purposes [2, 3, 7, 8]. In particular, [8] suggests using ICF to train SVMs. However, they do not solve the entire optimization problem directly. Instead, they use ICF in the inner loop of the decomposition-based method. Such implemen-tation is not desirable for two reasons. First, the room for further speedup is limited when | B | is small. Second, the error caused by the approximation nature of ICF can be accumulated from one iteration to the next, and the error can be magnified by the itera-tive process of the decomposition-based method. We confront the QP problem directly. The matrix inversion lemma in [3] suggests an efficient method for solving linear equations ( Q + D ) Considering that D is a diagonal matrix for in the case of SVMs, we can first evaluate z = D  X  1 b = we formulate the matrix E in 2 p 2 n flops. Next, we solve p linear equations as Ew = H T z in 2 3 p 3 +2 pn flops. Finally, we solve  X  = D  X  1 b  X  D  X  1 Hw = z  X  D  X  1 Hw in about 2 pn flops. Please note that D is a diagonal matrix. The total cost is about 2 p
ICF only needs to be performed once before the interior-point method begins Newton iterations. After that, at each Newton it-eration, only the incomplete Cholesky matrix H has to be stored. Therefore, the total computational cost is O ( p 2 + k (2 p O ((1 + 2 k ) p 2 n ) when p n . The memory usage is about O ( pn ) for storing H .
 In summary, the main steps of applying ICF to IPM for solving SVMs are
Table 2 lists the properties of three principal factorization al-gorithms that we have used for experiments in the interior-point method for solving SVMs. In terms of quality, Cholesky factor-ization is the best. In terms of the factorization cost, Kronecker factorization is the best. The attractiveness of incomplete Cholesky factorization is that it does not require factorization to be done and Hessian to be stored at each iteration.
 Table 2: Comparison of three factorization in IPM for solving SVMs.
The implementation of ICF-based IPM for solving SVMs in-cludes the following six steps: factorization , optimality checking , variable updating , matrix formulation , Newton-step computation , line search . The following list explains each procedure and its com-putational cost.

Table 3 compares each procedure in ICF-based IPM for solv-ing SVMs in terms of memory usage, computational cost (mea-sured in floating point operations or flops), and how easily the pro-cedure could be parallelized. As can be seen from the table, the computationally dominant procedure is p -by-p matrix formulation E = I + H T D  X  1 H , which mainly involves a matrix-matrix prod-uct operation and hence is easy to be parallelized.
We propose using an incremental way to perform factorization and training to achieve both good speedup and good training ac-curacy. During our empirical study on approximate factorization algorithms, we observed that an approximate algorithm speeds up SVMs by trading off training accuracy. We further observed that, if an SVM solver can be quickly initialized with an approximate solution, the solver can converge much quickly. Therefore, it is logically to use a quick, approximate solver to jump-start a precise solver to attain both speedup and accuracy.

More specifically, the convergence analysis on Newton X  X  method shows that the running iterations fall into two stages before reach-ing convergence, the damped Newton phase and the quadratically convergent stage [16, 21]. Suppose the objective function f ( in an unconstrained minimization problem has an optimal value f (  X   X  ) , the number of iterations required in the damped Newton phase is about O ( f (  X  (0) )  X  f (  X   X  )) . This is because f ( the damped Newton phase decreases linearly. After reaching the quadratically convergent stage, the objective decreases quadrati-cally and the number of iterations required in this stage is very small, e.g., about six iterations to reach very high training accu-racy [3].

A non-primal-dual IPM algorithm, e.g., the barrier-method [3, 19], requires a strictly feasible starting point  X  , which is usually computed by a preliminary stage, called phase I [11]. Therefore, a good starting point can greatly reduce the number of iterations to converge, especially when it falls into the quadratically conver-gent stage. The primal-dual IPM algorithm is an infeasible start method. The starting point  X  (0) might not necessarily be feasi-ble, but the algorithm can still ta ke advantage of employing a good starting point so that the convergence takes place faster. In the fol-lowing, we discuss three incremental strategies: initializing fine-grained-ICF-based IPM with coarse-ICF-based IPM (Coarse ICF + Fine ICF), initializing fine-grained-ICF-based IPM with coarse-KF-based IPM (Coarse KF + Fine ICF), and initializing decomposition-based SVMs with coarse-grained-ICF-based IPM (Corase ICF + SVMLight). Suppose we have already solved the IPM by factorizing matrix Q into HH T ,where H  X  R n  X  p is an incomplete Cholesky matrix with small p . Denote the solution as  X   X  p when H is used in IPM. We attempt to solve the IPM again by choosing a larger p , denoted as q ( q&gt;p ). We implement an incremental algorithm, which uses p as the starting point to solve the old factorization information in H n  X  p to achieve the new in-complete Cholesky matrix G n  X  q and hence solve the IPM even quicker. We note that G n  X  q can be rewritten in block matrices, where only the second block matrix is what we have to factorize. Therefore, we do not have to compute the q columns of G .In-stead, we only need to compute the last q  X  p columns. The cost is need to run IPM using the coarse ICF until convergence. Instead, we terminate it after running for several iterations. It is partially because the coarse IPM still has a computational cost of O ( p each iteration, and partially because our empirical study shows that the final solution  X   X  of the coarse IPM after convergence might not be an optimal starting point. We found that running the coarse IPM for about 10 iterations empirically serves as a good starting point for the fine IPM. We can thus formulate our incremental approach as follows: 1. Choose a small p to attain an incomplete Cholesky matrix 2. Re-run the IPM algorithm, choosing a large p .Use  X   X  as its
Kronecker factorization usually employs a separable framework to decompose a large matrix into two smaller matrices, as discussed in Section 2.2. It has been shown that the factorization process usu-ally converges very fast with sacrificing a certain degree of accu-racy [18]. Our experiments in Section 4 also demonstrate that using Kronecker factorization in IPM can achieve more speedups over SVMLight than ICF, but result in less accuracy. We thus propose running IPM with Kronecker factorization for several iterations and using achieved  X   X  to initialize IPM with fine-grained ICF. The ap-proach can be formulated as 1. Run the IPM with Kronecker factorization for T iterations 2. Run the IPM algorithm with fine-grained ICF by setting Unlike  X  X oarse ICF + Fine ICF X , such an incremental strategy does not take advantage of the decomposition information in Kronecker to converge. factorization for ICF. However, it utilizes the strengths of both Kro-necker and ICF.
Another incremental strategy is to initialize a decomposition-based solution with a factorization-based IPM solution. For ex-ample, we can run IPM with ICF or KF for several iterations and apply the achieved solution  X   X  to initialize SVMLight. In [25], we demonstrate that decomposition-based methods take a lot of iter-ations to converge. At each iteration, a bag of data is selected as the working set based on the  X  value from the last iteration apply-ing a criterion that the dual objective in (1) decreases. Initializing SVMLight with a good starting point can greatly reduce the num-ber of iterations to convergence. Suppose we use ICF for IPM; the approach can be formulated as 1. Run IPM with for T iterations (e.g., 20 ) and denote the final 2. Run SVMLight with  X   X  as its starting point.
We focus on the parallelization implementation of the interior-point method based on incomplete Cholesky factorization. For the parallelization of Kronecker factor ization, please refer to [26]. Our analysis in Section 2.3.2 shows that the dominant computational cost of IPM is to formulate a p -by-p matrix E while using incom-plete matrix factorization. Such a matrix formulation has to be performed once in each iteration, with the computational cost of O ( p 2 n ) . We use the following strategy to parallelize it. The computational cost of each step is O ( np ) , O ( p 2 1)) , respectively. In addition, we do not increase the memory us-age. The reason to split D and incorporate it in H is that it becomes easy to parallelize the outer-loop of a matrix-matrix product with minimum parallelization overhead. The implementation of paral-lelizing E = H T H can be done using the basic loop-blocking techniques. More complicated but efficient parallelization strate-gies are applicable, but they are not the focus of this paper.
We have conducted extensive empirical studies to examine the performance of our incremental, approximate, and parallel strat-egy for speeding up Support Vector Machines. Specifically, we designed our experiments to answer the following questions:
We used both artificial and real-world datasets to conduct our experiments. Table 4 lists the description of all datasets, including four checkerboard artificial datasets and three real-world datasets. Four checkerboard datasets are in binary-class , generated from a 2 D checkerboard which was evenly divided into 5 -by-5 ( 25 ) quad-rants. Each quadrant is occupied by the data of one class. The ideal boundary is the border of two neighboring quadrants. We generated training data only along the boundaries. In this way, we can assume the data-processing methods by removing non-support vector can-didates have been applied , and can thus research whether or not our factorizati on-based methods can still greatly speed up SVMs in that situation. We generated four such datasets, denoted as cb10 , cb100 , cb400 ,and cb1000 , which have 1 K, 100 K, 400 K, and one million ( 1 M) data instances, respectively. Among three real-world datasets, the w6a is a webpage dataset that was first used in [20]. It has 17 K data instances in two classes. The ijcnn dataset was from IJCNN X 01 neural network c ompetition [ 22]. It has about 127 Kdata instances in a binary class. Each instance is described by 22 fea-tures. The covertype dataset was from UCI KDD archive. It is used to describe the forest cover type for a given observation in a 30 x 30 meter cell. It has about 570 K data instances in seven classes. Since we only research the speedup problem in a binary classification set-ting for this paper, we transformed the dataset to a binary one by differentiating the majority class (class 2) from the others.
All experiments were conducted on an Intel dual-core Linux server in 2.8GHZ with 8 G-byte memory and 2 M-byte L 1 cache. For both decomposition-based and factorization-based SVMs, we chose the hyperparameter C as 100 and the cache size as 4 G bytes. We chose Gaussian RBF kernel for the experiments. The  X  value used for each dataset was empirically chosen to be optimal for SVMLight. For factorization-based SVMs, we chose the maximum iterations of running IPM to be 50 . For all other parameters used for op-timization, such as the primal and dual feasibility thresholds, we followed the default settings in SVMLight.

This experiment compared two approximate matrix factorization methods with the decomposition-based method, i.e., SVMLight, using five large-size datasets.

In this experiment, all three methods, two approximate factor-ization algorithms and SVMLight, were run on a single-threaded process. For all methods, we chose the starting  X  value as a zero vector. We first examined the influence of the rank ( p ) of incom-
Figure 1: Performance of ICF-based IPM using different p . plete Cholesky matrix H on the training performance. The accu-racy of the ICF-based IPM algorithm can be controlled by spec-ifying p . Figure 1 illustrates the performance of ICF-based IPM when different p  X  X  were utilized on a 100 K checkerboard dataset cb100 and a 127 K IJCNN dataset ijcnn . One observation that can be found from Figure 1 is that the test accuracy of SVMs almost remains unchanged when we choose p about 0 . 4% of dataset size n on cb100 , and about 0 . 3% of n on ijcnn . Additional results are reported on Table 5, where we can see that p can be chosen about 0 . 1%  X  0 . 5% of n without severely deteriorating the training per-formance. Therefore, in our experiments, we empirically chose the columns of incomplete Cholesky matrix H ( p ) as about 0 . 5% of the size of dataset (n).
 Table 5: Comparison of Decomposition-based SVMs with Factorization-based SVMs.

Table 5 reports both training time (measured in CPU time in sec-ond), and the corresponding test accuracy of each method on all five large-size datasets, where we denote IPM-ICF and IPM-KF as the IPM solutions using incomplete Cholesky and Kronecker factor-ization, respectively. Both ICF and KF solved SVMs much faster than the decomposition-based method. Moreover, Table 6 reports the number of speedup of IPM-ICF and IPM-CF over SVMLight at the second and third columns, respectively. As can be seen from the table, IPM-ICF achieved an average eight -times speedup over SVMLight, and IPM-KF achieved an average eleven -times speedup on all datasets. We noticed that the speedup degree is different on different datasets, but has no strong relationship to the size of the dataset. We conjectured it might be data-dependent or be caused by non-optimal parameter settings on IPM-ICF and IPM-KF. On the other hand, we also noticed that in Table 5 both IPM-ICF and IPM-KF achieved lower test accuracy than SVMLight on all datasets. The drop in performance is naturally caused by the fact that both factorization methods are approximate, as discussed in Section 2. Compared to SVMLight, IPM-ICF had about 1 . 7 -percentile drops in test accuracy, but IPM-KF had about 4 . 0 -percentile drops, which can be observed from the middle three columns in Table 5.
We examined the performance of incremental factorization by initializing a fine factorization-based IPM using a coarse one. We tried three strategies which we discuss in Section 3, i.e.,  X  X oarse ICF+ Fine ICF X ,  X  X oarse KF + Fine ICF X , and  X  X oarse ICF + SVMLight X . We denote them as  X  X CF + ICF X ,  X  X F + ICF X , and  X  X CF + SVMLight X , respectively. For both coarse IPM-ICF and coarse IPM-KF, we chose the maximum running iteration as 20 . Table 6 reports the amount of speedup over SVMLight for all meth-ods on all datasets. The second and third columns show the amount of speedup while just using the fine IPM solver with ICF and KF, respectively. The next three (from the fourth to the sixth) columns report the amount of speedup while using a coarse IPM solver to initiate IPM-ICF or SVMLight. Three observations can be made from Table 6. First, a fine IPM solver using incremental tech-niques achieved a larger amount of speedup over SVMLight than the one not using the incremental techniques. Second, using coarse IPM-ICF achieved the best performance. Among five datasets,  X  X CF+ICF X  outperformed others in four out of five. Third, the incremental techniques also worked for the decomposition-based method, SVMLight. The sixth column shows that using the result from the coarse IPM-ICF achieved an average of about two-times speedup over SVMLight.
As discussed in Section 2.3.2, matrix factorization is the only hard (but not impossible) part to parallelize while performing IPM-ICF.

We examined the parallelization ability of the incremental feed-ing strategy using two threads for  X  X CF+ICF X  and  X  X F+ICF X . As can be seen from the last two columns in Table 6, compared to  X  X CF+ICF X  and  X  X F+ICF X  using a single thread, their parallel ver-sions using two threads achieved about 1 . 7  X  1 . 8 speedups. For example, the parallelized  X  X CF+ICF X  with two threads was finished with training in about 12 . 5 hours for one -million cb1000 , and about 2 . 5 hours for ha lf-million covertype . While implementing the par-allel version of IPM-ICF, we only used a simple loop-blocking strategy to parallelize the large-size matrix-vector or matrix-matrix products, e.g., matrix formulation in IPM-ICF. We believe that more effective parallelization methods can be utilized to further improve the performance, but that is not the focus of this paper.
Related work in speeding up SVMs can be categorized into two approaches: data-processing and algorithmic .The data processing approach focuses on training-data selection to reduce n .Somerep-resentative work are bagging [4], cascade SVMs [12], intelligent sampling using hierarchical micro-clustering technique [27], and shrinking [14]. The algorithmic approach devises approximate al-gorithms to make the QP solver faster. It can be further divided into two methods: decomposition and factorization . The repre-sentative decomposition -based methods include Sequential Mini-mization Optimization (SMO) [20] and SVMLight [14]. The rep-resentative factorizaton -based methods include variable projection method (VPM) [28], incomplete Cholesky factorization [8], proxi-mal SVMs [9], reduced SVMs [17], a nd core vector machines [23].
Both data-processing and algorithmic methods directly aim to speed up solving the QP problem in (1). These two approaches can be complementary to each other. For more discussions on the related work, especially the difference with our proposed methods, please refer to [25]. We proposed an incremental or hybrid method to speed up SVMs. The proposed strategy uses inexpensive, approximate algorithms to attain an initial solution quickly for IPM, and then switch to solving a problem with more expensive, precise algorithms. We demon-strated that our incremental methods are highly parallelizable. Ac-cording to Amdahl X  X  law, we are optimistic that the training time on the one-million cb1000 dataset can be further reduced (from six days) to about three hours when a large number of processors is employed. [1] M. A. Aizerman, E. M. Braverman, and L. I. Rozonoer. [2] F. R. Bach and M. I. Jordan. Predictive low-rank [3] S. Boyd. Convex Optimization . Cambridge University Press, [4] L. Breiman. Bagging predictors. Machine Learning , [5] C. Cortes and V. Vapnik. Support-vector networks. Machine [6] R. Courant and D. Hilbert. Method of Mathematical Physics , [7] M. C. Ferris and T. S. Munson. Interior-point methods for [8] S. Fine and K. Scheinberg. Efficient svm training using [9] G. Fung and O. L. Mangasarian. Proximal support vector [10] G. H. Golub and C. F. V. Loan. Matrix Computations .The [11] C. C. Gonzaga. Path-following methods for linear [12] H. P. Graf, E. Cosatto, L. Bottou, I. Dourdanovic, and [13] R. A. Horn and C. R. Johnson. Matrix Analysis . Cambridge [14] T. Joachims. Making large-scale svm learning practical. [15] T. Joachims. Transductive inference for text classification [16] L. V. Kantorovich. Functional Analysis and Applied [17] Y.-J. Lee and O. L. Mangasarian. Rsvm: Reduced support [18] C. F. V. Loan and N. Pitslanis. Approximation with [19] S. Mehrotra. On the implementation of a primal-dual interior [20] J. Platt. Sequential minimal optimization: A fast algorithm [21] B. T. Polyak. Introduction to Optimization . Optimization [22] D. Prokhorov. Ijcnn 2001 neur al network competition. slide [23] I. W. Tsang, J. T. Kwok, and P.-M. Cheung. Core vector [24] V. Vapnik. The Nature of Statistical Learning Theory . [25] G. Wu, Y.-K. Chen, C. Hughes, E. Chang, and P. Dubey. [26] G. Wu, Z. Zhang, and E. Y. Chang. Kronecker factorization [27] H. Yu, J. Yang, and J. Han. Classifying large data sets using [28] G. Zanghirati and L. Zanni. A parallel solver for large
