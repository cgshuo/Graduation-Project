 Decision trees is a simple yet widely used method for classification and predictive modeling. A decision tree partitions data into smaller segments called terminal nodes. Each terminal node is assigned a class label. The non-terminal nodes, which include the root and other internal nodes, contain attribute test conditions to separate records that have different characteristics. The partitioning process terminates when the subsets cannot be partitioned any further using predefined criteria. Decision trees are used in many domains. For example, in database marketing, decision trees can be used to segment groups of customers and develop customer profiles to help marketers produce targeted promotions that achieve higher response rates.

This paper studies decision tree based classification methods for uncertain data. In many applications, data contains inherent uncertainty. A number of factors contribute to the uncertainty, such as the random nature of the physical data generation and collection process, measurement and decision errors, unreli-able data transmission and data staling. For example, there are massive amounts of uncertain data in sensor networks, such as temperature, humidity, and pres-sure. Uncertainty can also arise in categorical data. For instance, a tumor is typically classified as benign or malignant in cancer diagnosis and treatment. In practice, it is often very difficult to accurately classify a tumor due to the experiment precision limitation. The lab results inevitably give false positives or false negatives some of the time. Therefore, doctors may often decide tumors to be benign or malignant with certain probability or confidence. [24]
Since data uncertainty is ubiquitous, it is important to develop classification models for uncertain data. In this paper, We focus on the decision tree based classification approach. We choose the decision tree because of its numerous positive features. Decision tree is simple to understand and interpret. It requires little data preparation, while some other techniques often require data normal-ization, dummy variables need to be created and blank values to be removed. Decision tree can handle both numerical and categorical data, while many other techniques are usually specialized in analyzing datasets that have only one type of variable. Decision tree uses a white box model. If a given situation is observ-able in a model the explanation for the condition is easily explained by Boolean logic. Besides, it is possible to validate a decision tree model using statistical tests. Decision tree is also robust and scalable. It performs well with large data in a short period of time.

In this paper, we propose a new decision tree for classifying and predicting both certain and uncertain data (DTU). The main contributions of this paper are: 1. We integrate the uncertainty data model into the design of the decision tree. 2. We develop the DTU based on the widely used C4.5 classification tree so that it can handle both numerical and categorical data with uncertainty. 3. We prove through experiments that DTU has satisfactory performance even when the training data is highly uncertain.

This paper is organized as follows. In the next section, we will discuss related work. Section 3 describes the uncertaint data model. Section 4 shows the mea-sures for identifying the best split for uncertain data. Section 5 illustrates the DTU algorithms in detail. The experimental results are shown in Section 6 and Section 7 concludes the paper. Classification is a well-studied area in data mining. Many classification algo-rithms have been proposed in the literature, such as decision tree classifiers [17], Bayesian classifiers [14], support vector machines (SVM) [20], artificial neural networks [3] and ensemble methods [9]. In spite of the numerous classification algorithms, building classification based on uncertain data has remained a great challenge. There are early work performed on developing decision trees when data contains missing or noisy values [11,15,18]. Various strategies have been developed to predict or fill missing attribute values. However, the problem stud-ied in this paper is different from before -instead of assuming part of the data has missing or noisy values, we allow the whole dataset to be uncertain, and the uncertainty is not shown as missing or erroneous values but represented as uncertain intervals and probability distribution functions. There are also some previous work performed on classifying uncertain data in various applications [4,10,12]. All of the above methods try to solve specific classification tasks in-stead of developing a general algorithm for classifying uncertain data. And Qin et al. [24] propose a rule-based classification algorithm for uncertain data.
Recently, more research has been conducted in uncertain data mining. Most of them focus on clustering uncertain data [8,13,16]. The key idea is that when com-puting the distance between two uncertain objects, the probability distributions of objects are used to compute the expected distance. Xia et al. [22] introduce a new conceptual clustering algorithm for uncertain categorical data. Aggarwal [2] proposes density based transforms for uncertain data mining. There is also some research on identifying frequent itemsets and association mining [7,23] from un-certain datasets. The support of itemsets and confidence of association rules are integrated with the existential probability of transactions and items. Burdicks [5] discuss OLAP computation on uncertain data. None of them address the is-sue of developing a general classification and prediction algorithm for uncertain data. In this section, we will discuss the uncertainty model for both numerical and categorical attributes. Here we focus on the attributes uncertainty and assume the class type is certain.

When the value of a numerical attribute is uncertain, the attribute is called an uncertain numerical attribute (UNA), denoted by A u n i . Further, we use A u n ij to denote the j th instance of A u n i . The concept of UNA has been introduced in [6]. The value of A u n i is represented as a range or interval and the probabil-ity distribution function (PDF) over this range. Note that A u n i is treated as a continuous random variable. The PDF f ( x ) can be related to an attribute if all instances have the same distribution, or related to each instance if each instance has a different distribution.
 [ A ij .l, A A ij , denoted by A that A
A dataset can also have categorical attributes that are allowed to take on uncertain values. We call such attributes uncertain categorical attributes(UCA), denoted by A u c i . Further, we use A u c ij to denote the attribute value of the j th instance of A u c i . The notion of UCA was proposed in [19].

A u c ij takes values from the categorical domain Dom with cardinality | Dom | = n . For a certain dataset, the value of an attribute A is a single value d k in Dom , Pr ( A = d k ) = 1. In the case of an uncertain dataset, we record the information by a probability distribution over Dom instead of a single value. Given a cate-gorical domain Dom = { d 1 ,...,d n } , an uncertain categorical attribute (UCA) A by the probability vector P = { p 1 ,...,p n } such that P ( A u c ij = v k )= p jk and The key issue of a decision tree induction algorithm is to decide the way records be split. Each step of the tree-grow process needs to select an attribute test con-dition to divide the records into smaller subsets. Widely used splitting measures such as information entropy and the Gini index are not applicable to uncer-tain data. In this section, we will define splitting measures for both uncertain numerical data and uncertain categorical data. 4.1 Uncertain Numerical Attributes As described earlier, the value of an uncertain numerical attribute is an interval with associated PDF. Table 1 shows an example of UNA. The data in this table are used to predict whether borrowers will default on loan payments. Among all the attributes, the Annual Income is an UNA, whose precise value is not available. We only know the range of the Annual Income of each person and the PDF f ( x ) over that range. The probability distribution function of the UNA attribute Annual Income is assumed to be uniform distribution.

Each uncertain numerical value has a maximal value and a minimal value, which we call critical points. For each UNA, we can order all critical points of an uncertain numerical attribute in an ascending sort with duplicate elimination. Then the UNA can be partitioned. One partition may overlap with the UNA of many instances. When an instance with UNA overlaps with a partition [a, b), the probability its UNA actually falls in that partition is b a f ( x ) dx . Based on the probability of each individual instance falling in a partition [a, b), we can compute the probabilistic number of instances falling in that partition, which we call probabilistic cardinality.

The probabilistic cardinality of the dataset over a partition Pa = [a, b) is the sum of the probabilities of each instance whose corresponding UNA falls in probabilistic cardinality for class C j of the dataset over a partition Pa=[a, b) is the sum of the probability of each instance T j in C j whose corresponding UNA falls in [a, b). That is, PC ( Pa,C )= n j =1 P ( A u n ij  X  [ a, b )  X  C T C
Refer to the dataset in Table 1, the probabilistic cardinality for the partition [110, 120) on the Annual Income is the sum of the probabilities of instances that have Annual Income falling in [110, 120). Suppose the annual income for each in-stance is uniformly distributed over its uncertain interval; instances 1, 2, 4, 5 and 11 have overlap with [110,120), and the probability for instance 1 with annual in-come in [110, 120) is P ( I 1  X  [110 , 120)) = (120  X  110) / (120  X  110) = 1. Similarly, P ( I 11  X  [110 , 120)) = 0 . 5; therefore, the probabilistic cardinality for this dataset over partition [110, 120) is 3.29. The probabilistic cardinality for class Default-Borrower = NO over the partition [110, 120) on the Annual Income is the sum of the probabilities of instances who are not DefaultBorrowers with Annual Income falling in [110, 120). Among instances 1, 2, 4, 5 and 11 who have overlap with [110,120), only instances 1, 2 and 4 are in class NO; therefore, the probabilistic cardinality for DefaultBorrower = NO over partition [110, 120) is 1.79. Similarly, the probabilistic cardinality for DefaultBorrower = Yes over partition [110, 120) is 1.5.

With the two previous definitions, we can now define the probabilistic entropy for uncertain data as follows: Definition 1. The Probabilistic Entropy for a dataset D is ProbInfo ( D )=
Suppose attribute A is selected as the split attributecan, and it partitions the dataset D into k subsets, { D 1 ,D 2 ,...,D k } . Then the probabilistic entropy, or expected information based on the partitioning is given by ProbInfo A ( D )= partition. The smaller the entropy value, the greater the purity of the subset partitions. The encoding information that would be gained by branching on A is P robGain ( A )= ProbInfo ( D )  X  ProbInfo A ( D ).

Probabilistic Entropy also tends to favor attributes that have a large number of distinct values. The information gained by a test is maximal when there is one case in each subset D j . To overcome this problem, the splitting criterion should be modified to take into account the number of outcomes produced by the attribute test condition. This criterion is defined as P robGain ratio ( A )= and k is the total number of splits. If an attribute produces a large number of splits, its split information will also be large, which in turn reduces its gain ratio. 4.2 Uncertain Categorical Data An uncertain discrete attribute (UCA) A u c i is characterized by probability distri-bution over Dom . As mentioned earlier, it can be represented by the probability vector { p 1 ,...,p n } such that P ( A u c ij = d j )= p j (1  X  j  X  n ).
Table 2 shows an example of UCA [19]. This dataset records vehicle problem information. The problem can be caused by the brake, tire, transmission or other parts. It is derived from the text field in the given tuple using a text classifier/miner. As text miner result tend to be uncertain, the Problem field is aUCA.

Similar to uncertain numerical data, the probabilistic cardinality of the dataset over d j is the sum of the probabilities of each instance whose cor-responding UCA equals to d j .Thatis, PC ( d j )= n j =1 P ( A u c ij = d j ). The probabilistic cardinality for class C of the dataset over d j is the sum of the probabilities of each instance in C j whose corresponding UCA equals to d j . That is, PC ( d j ,C )= j =1 P ( A u c ij = d j  X  C j = C ).
 Refer to the dataset in Table 2, the probabilistic cardinality over Problem = Brake is the sum of the probabilities of each instance whose Problem attribute is Brake, which is 1.8. The probabilistic cardinality for class 0 over  X  X roblem = Brake X  is the overall probabilities of instances in class 0 whose Problem at-tribute is Brake, which is 1.1. Based on the probabilistic cardinality for each class C , we can then compute the probabilistic information entropy and prob-abilistic information gain ratio if the data is split on the categorical attribute  X  X roblem X , following the same process as for uncertain numerical data. If it has the highest probabilistic information gain, then  X  X roblem X  will be chosen as the next splitting attribute. 5.1 Decision Tree Induction Algorithm The algorithm is shown in Algorithm 1. The basic strategy is as follows: Algorithm 1. DTU Induction 1. The tree starts as a single node representing the training samples (step 1). 2. If the samples are all of the same class; then the node becomes a leaf and is labeled with that class (steps 2 and 3). 3. Otherwise, the algorithm uses a probabilistic entropy-based measure, known as the probabilistic information gain ratio, as the criteria for selecting the at-tribute that will best separate the samples into an individual class (step 7). This attribute becomes the  X  X est X  attribute at the node.
 4. If the test attribute is numerical or uncertain numerical, we split for the data at the selected position y (steps 8 and 9). 5. A branch is created for test-attribute  X  y or test-attribute &gt;y respectively. is put into the left branch with the instance X  X  weight R j .w . If an instance X  X  test with the instance X  X  weight R j .w . If an attribute X  X  value [ x 1 ,x 2 ] covers the split point y ( x 1  X  y&lt;x 2 ), it is put into the left branch with weight R j .w  X  y x and the right branch with weight R j .w  X  x 2 y f ( x ) dx . Then the dataset is divided into D l and D r (steps 10-19). 6. If the test attribute is categorical or uncertain categorical, we split the data multiway (steps 21-30). A branch is created for each value of the test attribute, and the samples are partitioned accordingly. For each value a i of the attribute, an instance is put into D i with R j .w weight when the attribute is certain. If the attribute is uncertain, assume the probability of the attribute value a i be R j .a i .p , then the instance is put into the branch a i with the weight R j .a i .p  X  R j .w . 7. The algorithm recursively applies the same process to generate a decision tree for the samples. 8. The recursive partitioning process stops only when either of the following conditions becomes true: 1) All samples for a given node belong to the same class (steps 2 and 3), or 2) There are no remaining attributes on which the samples may be further partitioned (step 4). In this case, the highest weight class is employed (step 5). This involves converting the given node into a leaf and labeling it with the class having the highest weight among samples. Alternatively, the class distribution of the node samples may be stored. 5.2 Prediction with DTU Once a DTU is constructed, it can be used for predicting class types. The pre-diction process starts from the root node, the test condition is applied at each node in DTU, and the appropriate branch is followed based on the outcome of the test. When the test instance R is certain, the process is quite straightforward since the test result will lead to one single branch without ambiguity. When the test is on an uncertain attribute, the prediction algorithm proceeds as follows: 1. If the test condition is on a UNA attribute A and the splitting point is a, suppose R.A is an interval [ x 1 ,x 2 ) with associated pdf R.A.f ( x ):
If a&lt;x 1 , which means the minimal possible value of R.A is larger than a, then P ( R.A &gt; a )= R.w ;weknowforsure R.A &gt; a and R follows the right branch;
If a&gt; = x 2 , which means the maximal possible value of R.A is smaller than a, then P ( R.A &lt; a )= R.w , and it is certain that R.A &lt; a and R follows the left branch;
If ( x 1 &lt;a&lt;x 2 ), then the probability R.A &lt; a is P ( R.A &lt; a )= R.w  X  should be in the left branch with probability R.w  X  a x branch with probability R.w  X  x 2 a f ( x ) dx . 2. If the test condition is on a UCA attribute A and a 1 ,a 2 , ...a k are the values for the categorical attribute A , then suppose R.A is an UCA, that is R should be in the i th branch with probability p i .

For the leaf node of DTU, each class C i has a probability PL ( C i ), which is the probability for an instance to be in class C i if it falls in this leaf node. PL ( C i )is computed as the fraction of the probabilistic cardinality of instances in class C i in a leaf node over the total probabilistic cardinality of instances in that node. Assume path L from the root to a leaf node contains t tests, and the data are classified into one class c i in the end, suppose P ( T i ) is the probability that an instance follow the path at the ith test, then the probability for an instance to be in class c i taking that particular path L is P L c
When predicting the class type for an instance T with uncertain attributes, it is possible that the process takes multiple paths. Suppose there are m paths taken in total, then the probability for T in class c i is P c Finally, the instance will be predicted to be of class c i which has the largest P In this section, we present the experimental results of the proposed decision tree algorithm DTU. We studied the prediction accuracy over multiple datasets.
Based on the J4.8/C4.5 implemented on Weka [21], we implemented the DTU as described in Section 5. The experiments are executed on a PC with an Intel Pentium IV 3.4GHz CPU and 2.0 GB main memory. A collection containing 10 real-world benchmark datasets were assembled from the UCI Repository [1]. We tried to cover the spectrum of properties such as size, attribute numbers and types, number of classes and class distributions. Among these 10 datasets, 5 of them, namely Iris, Sonar, Segment, Diabetes and Glass contain mainly numerical attributes. The remaining 5 datasets, namely Audiology, Bridges, Promoters, Mushroom and voting have mostly categorical attributes.

Due to a lack of real uncertain datasets, we introduce synthetic uncertainty into the datasets. To make numerical attributes uncertain, we convert each nu-merical value to an uncertain interval with uniform probability distribution func-tion. The uncertain interval is randomly generated around the original value. These are uncertainties from random effects without any bias. If the uncertain interval is within 10% of the original data, we call the dataset with uncertainty 10% and denote it by U10. For example, when the original value is 20, then its U10 may be [18.4, 20.4). We make categorical attributes uncertain by converting them into probability vectors. For example, a categorical attribute A i may have k possible values v j , 1  X  j  X  k . For an instance I j , we convert its value A ij introduce 10% uncertainty, this attribute will take the original value with 90% probability, and 10% probability to take any of the other values. Suppose in the original accurate dataset A ij = v 1 , then we will assign p j 1 = 90%, and assign p (2  X  l  X  k ) to ensure k i =2 p jl = 10%. Similarly, we denote this dataset with 10% uncertainty in categorical data by U10. We use U0 to denote accurate or certain datasets.

As prediction accuracy is by far the most important measure of a classifier, we studied the prediction accuracy of DTU classifier first. Figure 1 shows the result for numerical datasets and Figure 2 shows the result for categorical datasets. In both experiments, we use ten-fold cross validation. Data is split into 10 approxi-mately equal partitions; each one is used in turn for testing while the remainder is used for training, that is, 9/10 of data is used for training and 1/10 for test. The whole procedure is repeated 10 times, and the overall accuracy rate is counted as the average of accuracy rates on each partition. When DTU is applied on certain data, it works as a traditional C4.5 classifier.

For numerical data, the uncertainty varies between 0 to 30%. As shown in Fig-ure 1, when the extent of uncertainty increases, the classifier accuracy declines slowly. For most datasets, the performance decrement are within 5%, even when data uncertainty reaches 30%. The worst performance decrement is for the glass identification dataset, the classifier has over 95% accuracy on certain data, re-duces to around 92% when the uncertainty is 10%, to 81% when the uncertainty is 20%, and to 78% when the uncertainty reaches 30% .

The results for categorical datasets are similar, as shown in Figure 2. Overall speaking, the accuracy of DTU classifier remains relatively stable. The overall decrease in classifier accuracy is within 10% even when the uncertainty reaches 40%. Both experiments show DTU is quite robust against data uncertainty. In this paper, we propose a new decision tree algorithm DTU for classifying and predicting uncertain data. We extend the measures used in tradition deci-sion tree, such as information entropy and information gain, for handling data uncertainty. Our experiments demonstrate that DTU can process both uncer-tain numerical data and uncertain categorical data. It can achieve satisfactory classfication and prediction accuracy even when data is highly uncertain.
