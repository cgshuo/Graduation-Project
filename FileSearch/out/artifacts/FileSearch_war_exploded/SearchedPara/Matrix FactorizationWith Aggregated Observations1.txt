 In many real data analysis applications, we often face datasets with missing val-ues due to various reasons such as sensor failures and biased sampling. Since most of the existing data analysis methods are not directly applicable to them, we first need to estimate the missing values before analysis, or we need to develop new methods that can handle data with missing values. With its ubiquitous needs, missing value estimation [9,1,12] has been placed as one of the fundamental tasks in the field of machine learning and data mining, and it has been studied extensively. A typical dataset looks can be represented as a table with missing values (see Table 1). The table shows the numbers of beers of various brands purchased by four customers, where mi ssing values are indicated by  X - X . The table-structured data is ma thematically considered as a matrix; hence, matrix analysis techniques are useful for missing value estimation. Matrix factorization (MF), which decomposes matrices by using the low-rank assumption [3,4], is one of the effective approaches to restore missing values in such matrix-shaped data. Missing value estimation using low-rank matrix factorization does not arise only as a preprocessing step, but also as a primary purpose of the analysis. Typical examples include recommender systems [6] and relational learning [10].
In this study, we consider a more complex situation where some parts of data are not completely missing, but are obs erved at a more abstract category level as aggregated values. Figure 1 shows examples of such cases. In each category (such as  X  X eer X  and  X  X ola X ), several micro-level counts (such as  X  X udweiser X  and  X  X eineken X ) belonging to the category are observed as an aggregated count. To address such situations, we introduce a new variant of the missing value estimation problem, which we call restoration of micro-level observations from aggregated observation , where some parts of data are observed as aggregated val-ues of several features. Since the existin g techniques for missing value estimation including matrix factorization cannot be applied directly to such cases, we extend the existing low-rank matrix factorization formulation for missing value estima-tion to our case. We also devise iterative algorithms for solving the optimization problems, where each step consists of the standard singular value decomposition or closed form updates. Finally, using synthetic and real datasets, we show some experimental results on micro-observation restoration, which demonstrates that the proposed approach performs better than baseline methods.

The remainder of this paper is organized as follows. In Section 2, we intro-duce the restoration of micro-level observations from aggregated observation with a motivating example of purchase data analysis. We formulate matrix factoriza-tion problems with aggregated observations in Section 3, and give an efficient algorithm to solve the optimization problems in Section 4. In Section 5, we demonstrate the advantage of our approach over baseline approaches. Section 6 summarizes the related work, and Section 7 concludes the paper. In this section, we introduce a new problem that we refer to as the restoration of micro observations from aggregated observations using a motivating example of purchase data. We consider two cases that differ according to the assumption we make on categorical structures. 2.1 Motivating Example Let us assume that we have purchase data represented as a matrix X ,where each column corresponds to a customer, each row corresponds to a product (such as a particular brand of beer), and element X ij indicates the number of i -th products the j -th customer purchased. In many cases, the data has many missing example, only five of eight actual purchases are recorded). Restoration of the true purchases is quite important in sales management and analysis, and various missing value imputation methods [5] are employed for the purpose.

Let us now imagine a more complex situation where a part of X ij is not missing, but is observed at a more abstract category level. In each category, sev-eral micro-level counts belonging to the category are observed as an aggregated count. For example, among eight actual purchases of the  X  X udweiser X  brand of beer, only five are observed at the micro l evel (as five purchases of Budweiser), and the other three are observed in the more abstract  X  X eer X  category. The  X  X eer X  category might have ten purchases, including other beer brands such as five  X  X eineken X  bottles and two  X  X arlsberg X  bottles (See Figure 1). Now our goal is to restore the original micro level purch ases (such as ten Budweiser purchases) from the aggregated observations. 2.2 Restoration of Aggregated Observations General Problem Definition. Let us assume that we have two data matrices Z and Y . Z is an I  X  J matrix that represents micro-level observations. In the previous example, I is the number of product brands, and J is the number of customers. Y is an L  X  J matrix which represents category-level observations, where L indicates the number of categories. An example with purchase data is given in Table 2. In addition to Z and Y , we also have a correspondence matrix C as side information about product-category relationships. C is an L  X  I binary matrix, whose ( ,i )-th element is 1 if the i -th product is included in the -th category. Our goal is to restore the hidden micro-level observation matrix X (of size I  X  J )from Y with the help of C and Z .
 Two Different Assumptions on Product-Category Relationships. In our problem setting, we consider two different assumptions on the correspondence matrix C , which results in slightly different formulations of the problem.
The first case is when each dimension of column vectors belongs to only one category (Figure 2 (left)), and the other case is when each dimension can belong to more than one category (Figure 2 (right)). Figure 2 (right) shows that a micro-level product  X  X anqueray X  belongs to two possible categories  X  X in X  and  X  X iqueur X , and another micro-level product  X  X mirnoff X  belongs to both  X  X odka X  and  X  X iqueur X . We denote the former case as Case 1, and the latter as Case 2. The difference between the two case s is reflected by the definition of the correspondence matrix C . In Case 1, each column of C has at most one value as  X 1 X  value and the rest are  X 0 X . On the other hand, in Case 2, each column of C can have multiple values as 1. In this section, we formulate our problem as optimization problems, where we restore micro observations X from aggregated observations Y . Our model is an extension of the matrix factorization approach for missing value estimation. 3.1 Matrix Factorization Approach for Missing Value Estimation We first review the existing matrix factorization approach for missing value estimation, where the observed (micro-level) data matrix Z has missing values, i.e., Z ij are missing for some ( i,j ). Let us assume an observation matrix E ,where E ij =1if Z ij is observed; otherwise, E ij = 0. To impute the missing values of the matrix, the low-rank assumption is often employed. We consider the following optimization problem of rank-k approximation of the observed matrix. where the Frobenius norm of a matrix X  X  R I  X  J is defined as X F = + ments of Z are observed, i.e., E ij =1for  X  ( i,j ), the optimal solution is obtained by singular value decomposition (SVD). H owever, since we have missing elements in
Z , SVD cannot be applied. Furthermore, the optimization problem is not con-vex, hence numerical optimization methods do not guarantee optimal solutions. Recently, instead of using th e rank constraint, the trace-norm constraint is often used, because the tra ce-norm constraint of a matri x is a convex set (whereas the rank constraint is not) [11,2]. Using the trace-norm constraint, we can formulate the low-rank matrix approximation problem as a convex optimization problem as wherethetracenormofamatrix X is defined as X Tr =Tr( XX ). 3.2 Matrix Factorization with Aggregated Observations Now we extend the previous formulation to address our problem setting. Similar to the matrix factorization problem for missing value estimation, we also employ the low-rank assumption that our micro-level observations are of low-rank. We consider two slightly different formulations for the two cases we mentioned in the previous section.
 Case 1. When each row of the micro-observation matrix can belong to at most one category, we need that the linear constraint CX = Y , where each column of C has at most one value as  X 1 X  and the rest are  X 0 X . For example, let us assume that John bought several bottles of beer and cola as in Figure 1, the corresponding column in the constraint CX = Y looks like With the constraint CX = Y , we formulate the optimization problem as follows. Note that we assume that the  X  X rue X  micro-observations X + Z are of low-rank. Case 2. When each row of the micro-observation matrix can belong to more than one category, aggregation from micro-level observations to category-level observations is not unique; therefore, we divide the micro-level observation ma-trix X into a sum of multiple matrices { X ( ) } L =1 so that L =1 X ( ) = X is satisfied.
 In this case, we need that the linear constraints are satisfied. Note that one constraint is made for each of the L categories. If John bought several bottles of alcoholic beverage as in Figure 2 (right), one column in the constraint (2) looks like The optimization problem is defined as follows. Table 3 summarizes the ordinary formulation of matrix factorization, our for-mulation for Case 1, and one for Case 2. Our optimization problems (1) and (3) are minimization problems of convex functions with respect to both A and X . However, the number of variables in-volved is large, and it is time-consuming to minimize the objective functions with respect to them at once. Therefore, we devi se iterative optimization procedures, each of whose step optimizes either of A and X . We elaborate the concrete implementations of the estimation steps for both Case 1 and Case 2 below. 4.1 Case 1 Our proposed optimization procedure for Case 1 starts with initializing X so that the current X satisfies CX = Y . The initialization is discussed in the Experiments section in detail. Then, we iterate the following updates of A and X until convergence.
 When we update A , we need to solve the optimization problem This optimization problem can be solved by applying SVD to X + Z and thresh-olding the singular values. Let the SVD of X + Z be U X V ,where U and V are orthogonal matrices, and  X  is a diagonal matrix with the singular values as its diagonals. We eliminate the singular values less than the threshold  X  ,and denote  X  as the diagonal matrix with diagonal elements greater than or equal to  X  . The optimal solution A NEW of Eq. (4) is obtained as Optimization with respect to X is casted as the minimization problem This is generally a convex quadratic programming problem; however, the optimal solution is given in a simple closed form in this case. Since this problem can be seen as minimization of the Euclidean distance between X and M with the hyper-plane constraint CX = Y , the optimal solution is given as the projection of
M onto the hyper-plane. Assuming that the micro-level feature i belongs to the aggregated category ,theoptimalsolutionof i -th row X NEW i : is obtained as wherewedefined M = A  X  Z . 4.2 Case 2 In Case 2, noting that X = L =1 X ( ) , the update of A is the same as that for Case 1. However, in contrast to Case 1, the update of X cannot be given in a closed form solution anymore in Case 2, and we have to solve the optimization problem Although it is a quadratic programming problem, the number of variables in-volved is rather large; hence, we again resort to iterative optimization, that is, we iterate updates with respect to one of { X ( ) } L =1 at once. The optimization problem with respect to only X ( ) with the other { X ( j ) } j = fixed, the problem (8) is written as This has the same form as that in Case 1; hence, the closed form update becomes 4.3 Non-negativity Constraints Since our original motivation came from the purchase data example, it is some-times more reasonable to make a non-negativity assumption on the micro-level observations.

In Case 1, we make an additional constraint that X is non-negative. The resultant optimization problem for Case 1 with respect to X becomes Accordingly, the previous closed form solution (7) is modified to
In Case 2, we make the assumption that each X ( ) is non-negative. The update (10) is similarly obtained as Note that the modified optimization problems are still convex; therefore, we obtain optimal solutions when converged.
 We show some experimental results using synthetic and real datasets that demon-strate the reasonable perfo rmance of the proposed methods to restore micro-level observations from category-level observations. We compare the restoration er-rors by the proposed methods with those by four baseline methods, and show the advantage of the proposed methods over them. 5.1 Datasets Synthetic Dataset. The first dataset is a set of ra ndomly generat ed matrices. Thesizeofmatrices U and V is 1 , 000  X  5, and each element U ir  X  X  0 , 1 , 2 , 3 } and V jr  X  X  0 , 1 , 2 } is generated uniformly at random over the ranges. The true micro-level observation matrix is generated as A = UV , where 5% of the elements of A are randomly missing.

A 100  X  1 , 000 correspondence matrix C is generated so that the ( ,i )-th element is 1 if and only if i is in { 10  X  (  X  1) + 1 ,..., 10  X  } for Case 1. For Case 2, starting from the C we created for Case 1, we further sample 300 ( ,i ) pairs to make additional  X 1 X  values. To create category-level observations, we employ binomial distributions to divide the true micro-level observations A into the hidden part X and the observed part Z . Namely, each element Z ij is determined by Pr( Z ij = k )= the likeliness of the observation of each micro-observation at its superordinate category. For example, p = 1 corresponds to the perfect observation case with
Once the hidden part X is determined, the corresponding category-level ob-servations Y are created using the correspondence relationship CX = Y for to Y : with C : X ( ) = Y : .
 Purchase Dataset for Internet Stores. Another dataset is a real cross-store purchase dataset collected from 6 interne t stores to include for 494 customers, and 150 product brands belonging to 11 categories (such as electronic devices, undergarments, and magazines). Since the granularities of the input sales logs differ from store to store, not all of them provided detailed product names, and gave only category-level information. One product can belong to more than one category in this dataset; hence, this dataset belongs to Case 2. Since we had no ground truth micro-observations, we simulated category-level observations again from the micro-observed data; we assumed that some stores did not provide micro-level sales, and their sales were given as category-level observations. 5.2 Comparison Methods Although there have not been any existing methods that address the restoration problem to the best of our knowledge, we consider four baseline methods as com-parison methods to evaluate the proposed methods. The first method (that we call  X  X qual X  method) divides each category-level observation into its descendant micro-observation equally. The second method (that we call  X  X rop X  method) divides the category-level observations in proportion to the observed micro-level in Case 2. In addition, we applied the matrix factorization method (SVD) to the matrices obtained using the above methods, which results in two additional baseline methods (which we call  X  X qual+MF X  and  X  X rop+MF X ).

The micro-level estimations obtained by the simple methods are also used for initialization of X in the proposed method. Although our formulations are convex optimization problems and the solutions do not depend on the initial esti-mates, our preliminary experiments suggest that initialization with the  X  X qual X  method shows better numerical stability. 5.3 Results Table 4 and 5 show comparison of errors under different methods with varied p Case 1, Table 5 for that in Case 2. Table 6 shows the results for the purchase dataset (in Case 2) where one, two, or four stores out of six are assumed not to provide micro-level sales. As the eva luation metric, we used the difference between the estimated micro-observations  X  X and the true micro-observations X defined as Error X (  X  X )=  X  X  X  X tion were determined so that the simple MF method (Equal-MF or Prop-MF) performed the best (we reused it for the proposed method); they were 20 for the synthetic dataset (Case 1) and the purchase dataset, and 36 for the syn-thetic dataset (Case 2). The differenc e of the error between each comparison method and proposed method is significant in the Wilcoxon signed-rank test at a0 . 05 significance level. The results show that the proposed matrix factorization method is superior to the baseline methods. Interestingly, the simple application of matrix factorization (Equal-MF and Prop-MF) sometimes made the results worse than those by Equal and Prop. The simple MF methods roughly corre-spond to stopping the iterations of the proposed algorithm at the first iteration, and the results show it improved the performance after several iterations.
Finally, we mention the computational cost of the proposed method; the com-putational cost depends approximately on the number of calls of the SVD rou-tine, which was about five calls to converge. Dealing with incomplete data has been st udied extensively, and widely applied in various fields including machine learning and data mining. Zhu et al. [12] cat-egorized strategies to handle missing data into three categories, that are, case deletion, learning without handling of missing data, and data imputation. Case deletion, which ignores missing values, is the simplest method. These kinds of ap-proaches require robust methods to counter incomplete data [9]. Methods in the second category directly work with missing data. Data imputation approaches estimate unobserved values from the observed ones, and this method includes the matrix factorization approach we employed in this research.

Missing value imputation approaches can be classified into two categories, that are, data-driven approach and model-based approach [7]. Our method is categorized into the latter, and employs the matrix factorization model. There are several studies to impute missing values using matrix factorization techniques such as SVD and non-negative matrix factorization [8]. Missing value estimation is an unavoidable problem in real data analysis. In this study, we introduced an extended matrix factorization for a new missing value estimation problem, that is, restoration of micro-level observations from category-level aggregated observation. Since the existing methods cannot directly be ap-plied to this problem, we formulated an extended low-rank matrix factorization problem, and devised efficient iterative algorithms for solving the optimization problems. The experimental results using synthetic and real datasets showed that our approach performed better than baseline methods.
 Acknowledgment. The authors are grateful to Naonori Ueda, Hiroshi Sawada, and Katsuhiko Ishiguro of NTT Communication Science Laboratories, and Noriko Takaya of NTT Cyber Solution Laboratory.

