 In this paper, we discuss how we can extend probabilistic topic models to analyze the relationship graph of popular social-network data, so that we can  X  X roup X  or  X  X abel X  the edges and nodes in the graph based on their topic similarity. In particular, we first apply the well-known Latent Dirich-let Allocation (LDA) model and its existing variants to the graph-labeling task and argue that the existing models do not handle popular nodes (nodes with many incoming edges) in the graph very well. We then propose possible extensions to this model to deal with popular nodes.

Our experiments show that the proposed extensions are very effective in labeling popular nodes, showing significant improvements over the existing methods. Our proposed methods can be used for providing, for instance, more rele-vant friend recommendations within a social network. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  clustering ; D.2.8 [ Software Engi-neering ]: Metrics X  performance measures Experimentation, Algorithms social-network analysis, topic model, Latent Dirichlet Allo-cation, handling popular nodes
Social network services are gaining popularity. A growing number of users use major social network services, such as Facebook and Twitter, to share their thoughts and where-abouts with their friends and followers. On a social network, a user indicates that she wants to get notified of another user X  X  updates by  X  X ollowing X  (or making friends with) that user. Then when the followed user  X  X pdates her status X  (or shares a new thought), all users who follow her are immedi-ately notified. This  X  X ollow relationship X  among users often looks unorganized and chaotic, because follow relationships are created haphazardly by each individual user and not controlled by a central entity.

In this paper, we explore techniques to provide more struc-ture to this follow relationship (1) by  X  X rouping X  the users based on their topic interests, and (2) by  X  X abeling X  each fol-low relationship with the identified topic group. More for-mally, we consider each user in a social network as a node in a graph and each follow relationship as a directed edge be-tween two nodes. Then our goal is to  X  X roup X  a set of nodes in the graph based on their topics and  X  X abel X  each edge in the graph with a topic group number.

Inferring a structure within the social-network relation-ship graph can be useful for many reasons. For example, a novice user on a social network often encounters the boot-strapping problem: discovering relevant users to connect with. To mitigate this problem, social network services may recommend potentially interesting users to new users if they can group users of similar interests and infer why the new user has decided to follow a certain initial set of other users. Similarly, we can identify a small set of  X  X nfluential X  users on a certain topic (say, for marketing and advertising purposes) if we can identify the users X  topic interests.

Roughly, we can consider our goal as a clustering (or clas-sification) problem, where many popular solutions such as K-means [15] and DBSCAN [6] exist. These existing meth-ods, however, are not appropriate for our task because they either (1) associate each node with a single group (hard clus-tering) or (2) can associate each node with multiple groups (soft clustering), but require a completely separate method to label edges as well as nodes (since a node may be associ-ated with multiple groups). Given the diversity of interest a user may have, it is too restrictive to associate a user with a single topic group. For example, Kevin Rose, one of the most popular users on Twitter, may belong to the entrepreneur group as he is the founder of Digg.com, but may also belong to the Internet commentator group since he runs a popular Internet podcast. Since many users on social networks are active in more than one community, we believe it is too un-realistic to require that every user should belong to just one group.

In this paper, we apply a well-known probabilistic topic model, called Latent Dirichlet Allocation (LDA), to the follow-relationship graph of the social network, in order to label the nodes and the edges in the graph with (possibly) multiple topics. Unfortunately, LDA was developed for labeling doc-uments and words with topics, (not nodes and edges in a graph) so some of the assumptions on which LDA was built are not applicable to social-network graph data. In particu-lar, the direct application of LDA to our task requires that every node in the graph should be of roughly equal popu-larity and that we should remove nodes of high popularity from the dataset. This is particularly problematic because these popular nodes are really the ones that we want to label accurately; many users are particularly interested in identi-fying the topic groups of these popular users. Earlier work on the application of the LDA model to social graph [9, 26] has not addressed the handling of popular nodes.

To address the issues arising from popular nodes in the graph, we first explore existing variations of LDA. We then propose our extensions, two-step labeling and threshold noise filtering , to minimize the labeling noise introduced by pop-ular nodes.

In summary, we make the following contributions in this paper:
The rest of the paper is organized as follows. In Section 2, we briefly review previous work related to our research. In Section 3, we describe LDA and justify why we use LDA to solve this labeling problem. Then we introduce four different approaches to handle the noise generated by popular users in Section 4. In Section 5, we analyze our approaches using the Twitter dataset. We summarize this paper with our conclusion in Section 6.
The problem that we are trying to solve can be viewed as clustering in a social network using a topic model. In this section, we briefly review related work in social-network clus-ter analysis and topic-model-based social-network analysis. In his seminal work for social-network cluster analysis, Kleinberg [13] proposed Hyperlink-Induced Topics Search (HITS). In this work, he modeled the Web as a huge graph and extracted hubs and authorities in order to cluster com-munities on the Web. Recently, Mislove et al. [18] identified communities in Facebook based on the social graph structure and inferred unknown attributes through this community in-formation. The methods they used to cluster communities are based on pruning edges from the whole social graph and adding edges from some seed nodes, both of which are very common and widely used approaches in social-network anal-ysis. However, these approaches produce mutually exclusive groups and cannot support multiple memberships , which is important in our scenario where users have a variety of in-terests.

Topic models, the other class of related work, have also been extended to analyze small social-network data. Though not directly related to social-network analysis, the concept of author/user was initially introduced in the Author-Topic (AT) model [22]. It was used to extract hidden research topics and trends from CiteSeer X  X  abstract corpus. Zhou et al. [27] modified the AT model and proposed the Com-munity User Topic (CUT) model to capture semantic com-munities. McCallum et al. [16] extended the AT model and proposed the Author-Recipient-Topic (ART) model and the Role-Author-Recipient-Topic (RART) model in order to analyze the Enron e-mail corpus and an academic e-mail network. Pathak et al. [19] modified the ART model and suggested the Community-Author-Recipient-Topic (CART) model similar to the RART model. Besides these members of the AT model family, Wang et al. [24] introduced the Group Topic (GT) model and applied it to voting data from US Senate and the General Assembly of the UN. Mei et al. [17] also introduced a regularized topic modeling frame-work incorporating a graph structure in the data. Other LDA extensions and probabilistic topic models were also proposed for annotation data analysis [12], chat data anal-ysis [23], tagging data analysis [8], and pairwise data analy-sis [2]. While most of the LDA approaches introduced above attempted to utilize the authorship information of a given text by adding the author component to the LDA X  X  text gen-erative model, our approach focuses on using only the social part of the data (i.e., the social graph) and is generally ap-plicable to many large social networks.

Perhaps the work by Zhang et al. [26] and by Hender-son et al. [9] is closest to our work. In both works, the authors applied LDA to academic social networks. How-ever, their respective focus was quite different from ours. For example, [26] focused on the issue of how to convert the co-authorship information into a graph (e.g., direct co-authorship or indirect co-authorship, and edge weighting scheme based on collaboration frequency). Henderson et al. [9] addressed the issue of a large number of topic clus-ters generated due to low popularity nodes in the network, while our primary focus is the effective clustering of high popularity nodes.
In this section, we briefly describe LDA. Because LDA evolved from Probabilistic Latent Semantic Indexing (PLSI) [11], we first describe the concept of PLSI and then explain what differentiates LDA from PLSI. We also justify the rea-son why we use LDA for social-graph mining and discuss some different aspects between the standard LDA and our model.
Topic models assume that there are latent (hidden) top-ics behind words in human language. Thus, even though an author uses the word automobile in a document and a searcher uses the word vehicle in a query, topic models as-sume that they might have the same concept (topic) car in mind. Based on this assumption, topic models provide methods to infer those latent topics from visible words.
PLSI introduced a probabilistic generative model to topic models. Equation (1) represents its document generation process based on the probabilistic generative model:
P ( d,w ) is the probability of observing a word w in a doc-ument d and can be decomposed into the multiplication of P ( d ), the probability distribution of documents, and P ( w | d ), the probability distribution of words given a document. This equation describes a word selection for a document, where we first select a document then a word in that document. If we iterate this selection multiple times, we can generate a document and eventually a whole document corpus.

By assuming that there is a latent topic z , we can rewrite the equation above with the multiplication of P ( w | z ), the probability distribution of words given a topic, and P ( z | d ), the probability distribution of topics given a document. This equation describes adding an additional topic selection step between the document selection step and the word selection step. As there are multiple latent topics where a word may come from, we sum the multiplication over a set of all the independent topics Z .

PLSI and other probabilistic topic models support multi-ple memberships using the probabilities P ( w | z ) and P ( z | d ). word vehicle is more closely related to the topic car than the word automobile , though they are all related to the topic car . In this way, we can measure the strength of association between a word w and a topic z by the probability P ( w | z ). Similarly P ( z | d ) measures the strength of association be-tween a topic z and a document d .
 Equation (2) represents the log-likelihood function of PLSI: where D and W denote a set of all d and w respectively, and n ( d,w ) denotes the term frequency in a document (i.e., the number of times w occurred in d ).

By maximizing the log-likelihood function L , we can max-imize the probability to observe the entire corpus and ac-cordingly estimate the P ( w | z ) and P ( z | d ) that most likely satisfy Equation (1).
Though PLSI is equipped with a sound probabilistic gen-erative model and a statistical inference method, it suffers from the overfitting problem and does not cope well with unobserved words. To solve this problem, Blei et al. [4] introduced Dirichlet priors  X  and  X  to PLSI, to constrain P ( z | d ) and P ( w | z ), respectively.  X  is a vector of dimension | Z | , the number of topics, and each element in  X  is a prior for a corresponding element in P ( z | d ). Thus, a higher  X  implies that the topic z i appears more frequently than other topics in a corpus. Similarly,  X  is a vector of dimension | W | , the number of words, and each element in  X  is a prior for a corresponding element in P ( w | z ). Thus, a higher  X  plies that the word w j appears more frequently than other words in the corpus. As a conjugate prior for the multino-mial distribution, the Dirichlet distribution can also simplify (a) Subscription graph rep-resentation of our model Figure 1: Subscription graph representation of mod-els the statistical inference. By placing Dirichlet priors  X  and  X  on the multinomial distributions P ( z | d ) and P ( w | z ), those multinomial distributions are smoothed by the amount of  X  and  X  and become safe from the overfitting problem of PLSI. It is also known that PLSI emerges as a specific instance of LDA under Dirichlet priors [7, 10].
Before justifying our approach, we briefly explain Twitter and a few interesting aspects of its data, which we use in our performance evaluations later in this paper. In contrast to a mutual friendship in other social networks, Twitter X  X  relationships are unidirectional (i.e., a Twitter user does not need an approval from a user with whom she wants to make friends). Thus, we use the term follow when a user adds another user as her friend. Formally, when a user f follows a user g , f generates a follow edge , or simply an edge , e ( f,g ) from a follower f to a followed user g . We also use e 0 to denote an edge from g to f (indicating that g is followed by f ), e ( f ) to denote the set of all outgoing edges from f , and e 0 ( g ) to denote the set of all incoming edges to g . To refer to the set of all followers, the set of all followed users, and the set of all edges in the dataset we use F , G , and E , respectively.

Figure 1(a) depicts this notation using a graph that we refer to as a subscription graph . For example, we observe e ( f 1 ,cnn ) = e 0 ( cnn,f 1 ) = e 1 , e ( f 1 ) = { e 1 { e 2 ,e 3 } in Figure 1(a). Given this subscription graph, our goal is to label each edge with a correct label (interest) and group (label) each followed user based on those labeled edges. For example, since e 2 and e 3 are labeled with broad-cast and sports , respectively, espn is labeled with both. Now we can frame our problem as the graph labeling problem of automatically associating each user g i in G with a set of ac-curate interests z k in Z based on its labeled incoming edges e ( g i ). (We also label f j in F as well.)
We can view the interest here as a topic in a document generative model. As briefly mentioned in Section 3.1, a document topic model assumes that an author has a topic in mind when selecting a word for a document. Likewise, when a user follows another user in Twitter (i.e., when a user generates a follow edge), the follower has an interest in the followed user. This interest may be caused by reasons such as sharing a common interest, having an off-line rela-tionship, being popular, etc. Among these reasons for fol-lowing someone on Twitter, the two most common reasons are sharing a common interest and being popular, since the unidirectional nature of Twitter relationships allows a user to follow another user without requiring that user to follower her in return, as in the case of a blog subscription graph.
Furthermore, we can consider a follower f to be a docu-ment d , a followed user g as a word w , and a list of followed users for the follower as the content of the document. Fig-ure 1 illustrates this equivalence between our edge generative model and the standard LDA and justifies our approach of applying LDA to a relationship graph in a social network without changing LDA X  X  generative model.

We use the same notation with LDA. z denotes a labeling of a followed user with a topic (interest), or simply a topic, P ( z | f ) denotes the multinomial distribution of topics given a follower, and P ( g | z ) denotes the multinomial distribution of followed users given a topic.  X  and  X  are Dirichlet priors constraining P ( z | f ) and P ( g | z ), respectively.
In the previous section, we described the equivalence be-tween our edge generative model and the standard LDA. However, there is a subtle difference between the two gener-ative processes. While words are sampled with replacement in the standard LDA, followed users are sampled without replacement in our model. For example, in a document generative model, a document may contain the word car multiple times. On the other hand, in our edge generative model, a user cannot follow the same user Barack Obama multiple times. As a result, the probabilistic distribution in our model does not follow a multinomial distribution but follows a multivariate hypergeometric distribution. Fortu-nately, the multinomial distribution can also be used for our model because it is known that a multivariate hypergeomet-ric distribution converges to a multinomial distribution as the sample size grows large [1]. In our case, since sampling is done on millions of nodes, the two distributions become practically indistinguishable.

Also, when we represent E in matrix form by putting F in the rows and G in the columns as E  X  B F  X  G , where B = { 0 , 1 } , some differing aspects are noticed: 1. The rows and the columns are from the same entity 2. The matrix is very big and sparse. Because users follow 3. The matrix is a binary matrix, in which 1 indicates 4. As in a matrix formed from a document corpus, the
Among the four major differences described above, the first and the third do not affect our effort to apply LDA to the follow edge dataset. Only appropriate pre-processing is required. The second limits the size of the analysis but can be solved by adding more resources or by dividing the work into multiple machines [20].

However, the last difference has a significant effect on our analysis because it is related to the quality of our analysis results. In a document corpus analysis, the stop words are generally removed before analysis, since an analysis with-out removing these stop words produces a very noisy re-sult where the frequent stop words are labeled with every topic [4]. However, in our analysis, popular users are very important to include in the analysis because most users are keenly interested in following famous and influential users whose topic interests are similar to theirs. Unfortunately, it is not sufficient to simply include the popular users for LDA analysis, because the inclusion of popular users produces the same noisy result seen in the text analysis case: when stop words (or popular users) are included, they get included in every topic group, producing a very noisy result.

In the following section, we explore some LDA extensions to deal with the noise generated by popular users. Note that the earlier work on the application of LDA to an au-thorship graph dataset [9, 26] did not address how we can handle popular users. For example, [9] focused on the is-sue of how to transform co-authorship relations to a graph and [26] focused on how to deal with a large number of topic groups that are produced from the nodes whose connectivity is very low .

Before moving to the next section, we summarize the sym-bols used in this paper in Table 1.
 Table 1: Symbols used throughout this paper and their meanings
As discussed in Section 3.4, popular users generate noise if they are not dealt with carefully. This section describes how to handle this issue efficiently (i.e., how to label popular Figure 2: Topic hierarchy and documents generated in the hierarchy users with correct labels). We first attempt to use the stan-dard LDA with different settings (asymmetric priors). Then we explore the most appropriate LDA approach to this issue (Hierarchical LDA [3]) among a variety of LDA extensions we considered. Finally we propose two new LDA extensions of our own ( two-step labeling and threshold noise filtering ). The two new extensions can be combined together for better labeling quality.
As mentioned in Section 3.2, LDA constrains the distri-butions of topics and words with Dirichlet priors  X  and  X  , respectively. Though each element of vectors  X  and  X  may take different values in principle, in the standard LDA, each element of  X  and  X  is assumed to have the same value (often referred to as the symmetric prior assumption ). Intuitively, this assumption implies that every topic and word in a doc-ument corpus is equally likely.

Though the former sounds agreeable, the latter sounds unrealistic since it is very well known that the probability distribution of words follows a power-law distribution by Zipf X  X  law. It is also the reason why stop words are removed before applying LDA to a document corpus, since stop words correspond to the head of the power-law distribution.
The most intuitive approach to address this issue would be to set a different prior for each followed user. Between the two priors  X  and  X  , we are only interested in  X  , the prior over the distribution of words given a topic, because a followed user corresponds to a word in the standard LDA. As a higher prior value implies a higher likelihood of being observed in the corpus, we set each prior value proportional to the number of incoming edges of each followed user. It is expected to associate popular users with more accurate labels as they are given adequate prior values.

We set  X  g i , the prior for the followed user g i in the vector  X  , as in Equation (3):
Note that we set the lowest value for each element in  X  g as 0 . 01 and the highest value as 0 . 99 to make prior values skewed between 0 and 1.
Hierarchical LDA (HLDA) is also a good candidate for our problem because it generates a topic hierarchy and more fre-quent topics are located at higher levels. Figure 2 shows an example of topic hierarchy and document paths in the topic tree, where z k denotes a topic and d i denotes a document. In HLDA, when a document is generated according to Equa-tion (1), words are chosen from topics in a document path. Since the top level topic is associated with all the docu-ments, common words in every document (i.e., stop words) are expected to be labeled with the top level topic. On the contrary, the bottom level topics are expected to be more specific as they are associated with a small number of docu-ments. For example, if z 2 is a topic about network , z 4 would be a topic about queueing and routing , respectively. As z 1 is usually a topic consisting of the stop words, a doc-ument d 1 from the document tree path of z 1 -z 2 -z 4 consists of words from topic z 1 , z 2 , and z 4 and becomes a document about network queueing . Similarly in our model, z 1 volved in every user X  X  follow edge generation process and is expected to be associated with popular users.

This topic hierarchy is established because HLDA is based on the Nested Chinese Restaurant Process (NCRP), a tree extension to Chinese Restaurant Process, which probabilis-tically generates a partition of a set { 1 , 2 ,...,n } at time n . In NCRP, a document is considered as a Chinese restau-rant traveler who visits L restaurants along a restaurant tree path, where L refers to the level of the tree (i.e., the length of the path).
In the previous sections, we explored existing LDA ap-proaches to handle the popular user issue. Now we propose a new LDA extension: two-step labeling . We decompose the labeling process into two sub processes of establishing topics and labeling users with the established topics. In the first topic establishment step , we run LDA after removing pop-ular users from the dataset similar to how we remove stop words before applying LDA to a document corpus. This step generates clean topics free from the noise generated by popular users. In the second labeling step , we apply LDA only to popular users in the dataset. As we use the collapsed Gibbs sampling algorithm [21], edges to popular users are la-beled according to the pre-established topics as represented in Equation (4): P ( e ( f i ,g j ) = z | X  )  X  where P ( e ( f,g ) = z | X  ) denotes the probability of labeling the edge from a follower f to the followed user g with a topic z given all conditions, N gz denotes the number of times g is labeled with z , and N fz denotes the number of times f is labeled with z .

This equation implies that an edge to a followed user is as-signed to a topic according to how many times that user has been assigned to the topic, as well as how many times that topic has been previously assigned to the follower following that user. Thus, if some assignments are made in the first step, they affect assignments in the second step. For exam-ple, if a user f 1 follows non-popular users g 1 and g 2 and a popular user g 3 , g 1 and g 2 are sampled at the first step and g is sampled at the second step with a higher likelihood to Figure 4: An example of threshold noise filtering process be labeled with the topic that g 1 or g 2 are labeled with at the first step.

This approach is illustrated in Figure 3, where the E 1 part of the dataset is sampled at the first step and the E 2 part is sampled at the second step. Note that two-step labeling does not increase computational complexity because it samples a different part of the dataset at each sampling step. ( O ( | Z | X  | E | ) = O ( | Z | X  ( | E 1 | + | E 2 | ))
There is related research in the literature. Online LDAs introduced in [5] are designed to deal with a growing corpus and sample topics for words in a document as it comes in. Though both two-step labeling and online LDAs use multi-step sampling, their goals are totally different. While online LDAs try to change topics as the corpus grows over time, two-step labeling fixes a set of topics and labels users with this fixed set of topics. From Figure 3, G 1 and G 2 (words) are mutually exclusive in two-step labeling while F 1 and F 2 (documents) are mutually exclusive in online LDAs.
As briefly described in Section 3.1, the association be-tween a user (a word) and a topic has varying association tic topic model. Thus, we can list users labeled with the same topic in descending order of P ( g | z ) and regard the top entries in the list ( topic group ) as more important than the entries at the bottom because they are more strongly associated with the topic. Similarly, we can measure the association strength from the user X  X  viewpoint using P ( z | g ). Though two-step labeling may help label popular users with the right topics, popular users may take top positions even in less-relevant topic groups because even the smallest num-ber of times a popular user is assigned to a topic group could outnumber the largest number of times a non-popular user is assigned to that topic group.

To mitigate this problem, we propose a new approach, threshold noise filtering , which sets a cut-off value to deter-mine whether to label a user with each topic. By ignoring assignments below the cut-off value, we can expect smooth-ing and noise reduction effects as in the anti-aliasing filter . We set N g i z k , the number of times a followed user g i signed to a topic group z k , as 0 if it is below the cut-off value C :
As threshold noise filtering process is done after sampling, it does not increase computational complexity similar to two-step labeling . Figure 4 illustrates this process and shows the top three popular users X  distributions over topic groups. (Other normal users also show similar non-linear distribu-tions.) Alternatively, we may filter out less relevant topics by keeping only the top-K topics for each user, for a rea-sonably small K value. We tested both schemes (threshold noise filtering and top-K filtering), but we couldn X  X  see any practical differences. Due to lack of space, we only report the results with the former scheme. Though threshold noise filtering can be used with any other approaches, we combine it with the two most representative cases, two-step labeling and the standard LDA in our experiments.
In this section, we experimentally compare our proposed extensions to LDA, two-step labeling and threshold noise fil-tering , against existing approaches. We ran experiments on a real-world dataset obtained from Twitter and compared the performance of various approaches under two evaluation metrics, perplexity and quality . As we will see from our re-sults, our proposed extensions show performance equivalent to or significantly better than existing approaches in our experiments.

In Section 5.1, we describe our dataset and the overall experimental setup. In Sections 5.2 and 5.3, we report our results on perplexity and quality , respectively. We also pro-vide a qualitative comparison of different approaches in Sec-tion 5.4.
For our experiments, we use a Twitter dataset we col-lected between October 2009 and January 2010. The origi-nal downloaded dataset contained 273 million follow edges, but we sampled 10 million edges from this dataset to keep our experiment manageable. To ensure that all the follow edges were preserved in our sampled dataset, we first sam-pled followers randomly and included all follow edges from the sampled users, until we obtained 10 million follow edges.
Figure 5 shows the distributions of incoming and outgoing edges in the sampled dataset. The horizontal axis shows the number of edges at a node and the vertical axis shows how many nodes have the given edge count. Both axes are shown in a logarithmic scale. From the graph, it is clear that the number of incoming edges follows a power-law distribution, which is often the case for this type of dataset. Interestingly, we observe that the number of outgoing edges is quite uni-form between edge counts 1 to 100, which is different from the distribution reported in [14]. We do not believe this difference is due to our sampling, because the graph from the complete dataset shows the same flat curve between the edge counts 1 and 100 [25]. It could be an interesting future work to investigate where this difference comes from. In our sampled dataset, barackobama has the most followers, 7410, and zappos follows the most users, 142,669. Table 2 shows some basic statistics of the sampled dataset.

Since we are mainly interested in investigating how differ-ent approaches handle popular users, we categorized the fol-lowed users G into two distinct sub groups according to their incoming-edge counts. One group is the normal user group, where | e 0 ( g ) | X  V , a boundary value, and the other group is the popular user group, where | e 0 ( g ) | &gt; V . We tested three Figure 5: Distributions of incoming and outgoing edges boundary values V = 50, 100, and 500. Since all the results from the three boundary values show consistent patterns, we only report the result from the case when V = 100, where the popular user group consists of only 0 . 3% of all followed users but accounts for 20 . 2% of all follow edges.
Table 3 summarizes the eight representative experimental cases we report on in this section. base is the standard LDA experiment over the whole group. non-popular is also the standard LDA experiment but this experiment is applied only to the normal user group to remove the noise gener-ated by popular users. For beta , we use asymmetric Dirich-let priors for the hyperparameter  X  , where we have tested proportional, inversely-proportional, and ladder-shape prior schemes. hlda-2lv and hlda-3lv are HLDA experiments with two and three levels each. 2step is our two-step labeling ex-periment. For threshold noise filtering , where we have tested multiple threshold settings including C = 0 . 01, 0 . 05, 0 . 10, and K = 1, 2, 3, we generate filter-base and filter-2step by combining threshold noise filtering with base and two-step la-beling each. Due to space limits and to clarify our discussion, we only report the results for a subset of our experimental settings in this paper. In particular, we report the result from proportional  X  priors for beta and C = 0 . 05 for thresh-old noise filtering . The results that are not reported in this paper are very similar to what we report here.

In all of our experiments, we generated 100 topic groups (50 groups for hlda-2lv ) and picked the top ten entries in each topic group according to their probabilities P ( g | z ) in each group.
In this section, we report our results on the perplexity metric, which was also used in earlier related work such as [4, 9,10,26]. Perplexity is a well-known standard metric used in IR. It tries to quantify the accuracy of a model by measuring non-popular LDA over the normal user group dataset filter-base Threshold noise filtering after base filter-2step Threshold noise filtering after 2step how well the trained model deals with an unobserved test data. More precisely, perplexity is defined to be [26]: where E test denotes all the edges in the test dataset. 1 calculated Perplexity values for the 10% held-out dataset after training the models on the remaining 90% dataset. In general, lower perplexity means better generalizability.
Note that the original LDA model is designed to derive an optimal model that minimizes perplexity. Therefore, it is unlikely that extensions to base LDA show lower perplex-ity. Our primariy goal of comparing the results under the perplexity metric is to see whether our proposed extensions significantly degrade perplexity. Figure 6 compares Perplex-ity values of the eight experimental cases. The last set of bars show the Perplexity values on the overall test dataset. The first set of bars show Perplexity only on the edges to nor-mal users and the second set of bars show Perplexity only on the edges to popular users.

From the graphs, we first notice that HLDA ( hlda-3lv and hlda-2lv ) shows significantly worse Perplexity than others. This is due to the fact that the standard LDA is designed to minimize Perplexity while HLDA is not necessarily designed for this task. We also note that the Perplexity value for non-popular is significantly lower than others. This is be-cause we eliminate all edges to popular users in non-popular and do not include these edges in computing the Perplexity value. Therefore, non-popular had an unfair advantage of  X  X gnoring X  all noise from popular users and not being eval-uated on them. Overall, we find that our two-step labeling and threshold noise filtering show similar perplexity values
The Perplexity values of hlda-2lv and hlda-3lv are calcu-lated with the empirical likelihood values provided by Mallet (http://mallet.cs.umass.edu). to the standard LDA model. 2 That is, our extensions to LDA do not introduce noticeable Perplexity degradation to the standard LDA model.
Ultimately, the effectiveness of various approaches should be determined by how users perceive the quality of the iden-tified topic groups from each approach. To measure human-perceived quality, we conducted a survey with a total of 14 participants. The participants in our survey were presented with a random group of ten Twitter users (identified by one of the eight approaches described in Section 5.1) and were asked to indicate if each user in the topic group was relevant to the group or not. Overall, we collected 23 judged topic groups per each approach with a total of 161 judged topic groups.
 Given the survey results, we computed the human-perceived Quality value of the topic groups Z identified by an ap-proach as: where  X  function is defined as:
Note that in the above Quality formula, the factor log( | e is added to assign higher weights to more popular users, be-cause most people are interested in following popular users and pay more attention to the correct topic clustering of those users.

Figure 7 reports the results from this survey, where each bar shows the Quality value of one of the eight approaches. From this graph, we observe the following: 1. Both of our extensions, two-step labeling and thresh-2. As two-step filtering initially forms clean topics free
In fact, the Perplexity values of our two approaches are lower than the standard LDA in our experiments, but we expect that this is simple experimental fluctuation that does not have a significant meaning.
 3. If we apply the standard LDA to popular users as well 4. Using an asymmetric Dirichlet prior for the hyperpa-5. The HLDA model shows only marginal improvements In Figure 8, we also report the false positive rates for popu-lar users, which measures incorrect labeling of popular users. That is, whenever a user in the popular user group is placed in a topic group from our analysis, our survey participants evaluated whether the topic group is a good fit for that user. Figure 8 reports what fraction of them were considered to be a  X  X ad fit X  or an  X  X ncorrect X  association. Different from Figure 7, the HLDA models seemingly perform as well as or even better than our two approaches. This is because HLDA tends to place popular users at top-level groups. When our survey participants looked at such a group, they simply con-sidered it as a  X  X opular user group X  and determined that the popular users in the group were a good fit, even though the group itself did not exhibit any coherent topic. Therefore, even though the false positive rate for HLDA is low, it does not necessarily improve the topic-based recommendation ac-curacy for popular users. We can observe that our 2step and filter-2step show comparably low false positive rates similar to those of the HLDA approaches.
In this section, we examine the topic groups generated from each approach more closely to get a better sense of the quality of produced topic groups.

Figure 9 shows the top ten users in a topic group from base , which we name as the cycle group. Together with their Twitter usernames, we show their follower counts | e 0 ( g ) | , and the bio of each user. By going over the users X  bios, we can Figure 9: Topic group cycle from base showing the popular user issue Figure 10: Topic group mobile gadget blog from non-popular see that many of the users in the group have the same in-terest cycle . However, we also observe that among the three users with the highest number of incoming edges, barack-obama , stephenfry , and lancearmstrong , only lancearmstrong is related to cycle . The other two users, barackobama and stephenfry are included in this group simply because they are famous and followed by many users. In particular, we note that barackobama appears in 15 groups out of 100 topic groups produced by base . Among the 15 groups, only one of them is related to his specialty, politics , which clearly shows that the standard LDA suffers from the noise from popular users if applied directly to our social graph dataset.
Figure 10 shows an example topic group produced by non-popular . Note that in this group, all users have | e 0 ( g ) | values of smaller than 100, because popular users are removed from the dataset. Therefore, none of the popular users will belong to a topic group under this approach. When we go over the users X  bios, we see that all users in this group is somewhat related to mobile gadgets . That is, the topic group produced by non-popular is significantly cleaner (less noisy) than that from base at the expense of not being able to group any popular users.

Figure 11 shows an example topic group from 2step . Note that many users in this group are very popular and they are Figure 11: Topic group from 2step corresponding to Figure 10 Figure 12: Topic group from filter-2step correspond-ing to Figure 10 mainly about the same topic, tech media , indicating that 2step is able to group popular users into the right topic group in general. However, we observe that 2step still suffers from the presence of a few popular, yet less relevant users in the group (in this example, cnnbrk and breakingnews may be considered less relevant to the group than others).
With threshold noise filtering , we achieved less noisy re-sults. Figure 12 shows a result topic group from filter-2step corresponding to the group in Figure 11 from 2step . We observe that cnnbrk and breakingnews , popular users on general media, are now removed from Figure 11 and more technology-centric media such as firefox , youtube , and engad-get are added. Note that firefox and youtube are Twitter ac-counts publishing news related to FireFox and YouTube. As we pick only a few most probable topics for popular users in filter-2step , they have less chance to appear in less-relevant topic groups.
In this paper, we applied LDA to analyze the relationship graph in a large social network. Different from the usual approaches that extract topics from textual data, such as bio and tweets, our approaches rely purely on the social-network graph consisting of follow edges. Even with this relatively limited type of data compared to those of previous approaches, our approaches generated very well-organized results and showed the great potential of applying LDA to these kinds of clustering applications. Our approaches are especially useful when only linkage data is available.
We also dealt with popular user generated noise, which is inevitable in a large social network. Unlike the stop words in the standard LDA applications, popular users have im-portant meanings and should be dealt with carefully. We explored four extensions to the standard LDA and quantita-tively and qualitatively analyzed their results using a Twit-ter dataset. Our proposed approaches, two-step labeling and threshold noise filtering, are very effective in handling this popular user issue, showing 1 . 64 times improvement in the quality of the produced topic groups, compared to the standard LDA model.
We would like to thank Jong Han Park, and Christo-pher Moghbel for their help and feedback throughout this research. [1] Multinomial distribution. http://en.wikipedia.org/ [2] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. [3] D. M. Blei, T. L. Griffiths, M. I. Jordan, and J. B. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] K. R. Canini, L. Shi, and T. L. Griffiths. Online [6] M. Ester, H. peter Kriegel, J. S, and X. Xu. A [7] M. Girolami and A. Kaban. On an equivalence [8] M. Harvey, I. Ruthven, and M. J. Carman. Improving [9] K. Henderson and T. Eliassi-Rad. Applying latent [10] M. D. Hoffman, D. M. Blei, and F. Bach. Online [11] T. Hofmann. Probabilistic latent semantic indexing. In [12] T. Iwata, T. Yamada, and N. Ueda. Modeling social [13] J. M. Kleinberg. Authoritative sources in a [14] H. Kwak, C. Lee, H. Park, and S. Moon. What is [15] S. Lloyd. Least squares quantization in pcm. IEEE [16] A. Mccallum, X. Wang, and A. Corrada-Emmanuel. [17] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic [18] A. Mislove, B. Viswanath, K. P. Gummadi, and [19] N. Pathak, C. DeLong, A. Banerjee, and K. Erickson. [20] A. Smola and S. Narayanamurthy. An architecture for [21] M. Steyvers and T. L. Griffiths. Probabilistic topic [22] M. Steyvers, P. Smyth, M. Rosen-Zvi, and [23] V. Tuulos and H. Tirri. Combining topic models and [24] X. Wang, N. Mohanty, and A. Mccallum. Group and [25] M. J. Welch, U. Schonfeld, D. He, and J. Cho. Topical [26] H. Zhang, B. Qiu, C. L. Giles, H. C. Foley, and [27] D. Zhou, E. Manavoglu, J. Li, C. L. Giles, and
