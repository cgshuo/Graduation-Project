 Principal Component Analysis (PCA) is a well-known data analysis and visu-alization tool. It provides a simple, algebraic way to choose axes in the data space that most fit the data, i.e. that maximize the variance after projection on the subspace spanned by these axes, or alternatively that minimize the projec-tion error. A lower-dimensi onal representation of data is obtained by selecting a restricted number of the principal axes. However, maximal variance and min-imal projection error are quadratic measures: a few outliers may dramatically influence the direction of principal axes, especially in high-dimensional spaces.
Probabilistic PCA [10,13] is a way to formalize the PCA problem as a latent variable model into a probabilistic framework. One of the nice features of the prob-abilistic framework is that non-traditional assumptions can easily be added to the model, the only price to pay being that the optimization of the model may reveal more difficult. For example, the traditional Gaussian noise hypothesis leads to the above detailed quadratic measures of errors and variances; replacing this hypoth-esis by, for instance, a Student-t noise distribution leads to a robust version of PCA [2]. In contrast to other robust approaches to PCA which usually require to optimize several additional parameters, the probabilistic formalism only requires to choose the dimension of the projection space, the other parameters being set automatically by maximum likelihood (ML). Another advantage is that the prob-abilistic model provides likelihood measures, which can be used to compute pos-terior probabilities and eventually to construct a Bayes classifier.
Mixtures of (local) PCA may be used to uncover nonlinear manifolds in data, and are also nicely formalized into a probabilistic framework [12]. The principle is to attribute each observed data to a specific (unknown) local model (or com-ponent), through an indicator variable, and then to mix the local models. An expectation-maximization algorithm can be used to set the parameters of the model, including these indicator variables. An advantage of mixtures of PCA, compared to other mixtures models (a.o. Gaussian mixtures), is that the full-rank, possibly ill-conditioned covariance matrices are approximated by low-rank covariance matrices, without having to ne glect the correlations between the (lo-cal) principal directions to avoid numerical instabilities. The other way to avoid ill-conditioned covariance matrices is to constrain them to be diagonal, leading to suboptimal axis-aligned components [1]. Besides nonlinear manifold uncov-ering, mixtures models can be used in a straighforward way for clustering, and probability density estimation. In both cases the same limitations related to ill-conditioned covariance matrices apply though.

Mixtures of probabilistic PCA [3] can be made robust to atypical observa-tions by using a Student-t noise distribution hypothesis. This paper shows the complete probabilistic learning procedure for this model. It is shown that all parameters (with the exception of the number of components and their dimen-sionality) may be easily optimized by an Expectation-Maximization procedure, without additional complexity with respect to the non-robust version.
The following of this paper is organized as follows. The next section first reminds the Probabilistic PCA model and its robust extension, and then intro-duces the Mixtures of Robust Probabilistic PCA model. Section 3 details how the parameters of the model may be optimized, and Section 4 illustrates the robustness of the model to atypical observations. PCA can be formulated as the search for an optimal linear projection mini-mizing a reconstruction error. The princ ipal components are derived from the observations by projecting them on the principal directions. In the probabilis-tic formulation, the view is inverted in the sense that the observations { y n } N n =1 where y n  X  IR D , are assumed to be generated from a low dimension latent representation { x n } N n =1 ,where x n  X  IR J , J&lt;D .

The principle of probabilistic modeling is to express the uncertainty about (some of) the parameters of the model by prior distributions. Probabilistic PCA (PPCA) was proposed in [10,13]; Gaussi an priors are used in PPCA. Maximising the likelihood of the observations in PPCA leads to principal axes that are equivalent to the principal axes found by the standard PCA, up to a rotation and a scaling [13]; the same subspace is thus spanned.

PCA and PPCA are sensitive to atypical observations and observations not well confined in a low-dimensional subspace, because of their quadratic criterion and Gaussian noise model respectively. The robust probabilistic PCA [2] extends PPCA to make it applicable on datasets containing atypical samples. Instead of the Gaussian noise assumption, the randomness in observations is modeled by a Student-t distribution with an additional parameter  X  (called the number of degrees of freedom ), which regulates the thickness of the distribution X  X  tail. Figure 1(left) shows unit-variance Gaussian and Student-t distributions (  X  =2). Figure 1(right) shows the corresponding negative-log-likelihood which appears in the training criterion of probabilistic models. We see that when  X  is small, the Student-t attributes a much smaller cost than the Gaussian to points lying far from the mean. The sensitivity to atypical observations is therefore reduced.
PPCA makes the assumption that atypical samples might come either from the generation of latent vectors x or from the noise contribution. This is expressed by Student-t distributions on the prior of the latent vectors and on the condi- X  , X   X  1 I D , X  ). Note that in the traditional PPCA model, the Student-t distri-butions are replaced by Gaussian ones. To simplify the parameterization, both distributions are attributed the same degree of freedom  X  . This choice will be commented below. The Student-t distribution can be reformulated as an infinite du,  X  &gt; 0, where G a ( u | X  ,  X  ) is a Gamma distribution over the precision factor u . Making use of this factorization, the generative model can be represented with an additional level in the hierarchy where the latent precision u appears: From this generative formulation, we see that the uncertainty about the obser-vation (i.e. expressed by the variance in (3)) can be amplified by a small latent precision variable u , shared by the x and y conditional distributions. According to intuition, this constraint implies that outliers y in the observation space are also considered as outliers x in the latent space so their contributions to the identification of the latent space are down-weighted.
 For robust PPCA, the marginal distribution of the observations is tractable: P ( y )=  X   X  1 I D . The training procedure consists in maximizing this (marginal) likelihood with respect to  X   X  ( W ,  X  , X , X  ).

In contrast with previous robust approaches to the PCA (see for example [15] and [7], and the references therein), this probabilistic formalism only requires to select the dimension of the projection spa ce (see Section 3), the other parameters being estimated by the maximum likelihood criterion.

Even in its robust and probabilistic versions, PCA is not adequate for repre-senting clusters or nonlinear dependencies in the data. The mixture of PPCA [12] may solve this problem, but is again too sensitive to atypical samples limiting its use on many real world datasets. It is thus natural to look for a robust for-mulation of the mixture of PPCA.
 The probability distribution of a sample generated from a mixture of K robust PPCA is defined as P ( y )= k  X  k P k ( y )where {  X  k } K k =1 is the set of positive mixture proportions, with k  X  k =1;the P k ( y ) are defined as single robust PPCA components P k ( y )= S t ( y |  X  k ,  X  k , X  k )inwhich  X  k  X  W k W k +  X   X  1 k I D . The set of parameters of this model is  X   X { ( W k ,  X  k , X  k , X  k , X  k ) } K k =1 .
Using a latent indicator variable z =[ z 1 ,...,z K ](with z k =1ifthe k th com-ponent generated the observation y ,otherwise z k = 0) simplifies the derivation of an EM algorithm. The factorized mixture of robust PPCA is then where u =[ u 1 ,...,u K ]and  X  = { x 1 ,..., x K } ; the different components could also have different latent dimensionalities { J k } K k =1 .

Increasing the robustness by replacing Gaussian densities with Student-t ones was also proposed for finite mixture models [8,1]. The main advantage of mixtures of PPCA resides in the fact that the full-rank, possibly ill-conditioned covariance matrices are approximated by constrained covariance matrices  X  k , strongly re-ducing the number of free parameters per component. By contrast, constraining the covariance to be diagonal leads to axis-aligned components which does not take the dominant correlations into account [1]. The factorization of the model (4)-(7) allows us to derive an exact Expectation-Maximization (EM) algorithm. Note that this algorithm encompasses the op-timization of the (mixture of) probabilistic PCA: one only needs to add the constraint  X  k =  X  (for all k ) such that the Student-t s are in fact Gaussian distributions.

We seek an optimum of the marginal distribution of the observations to esti-is by deriving an EM algorithm [6] on the factorised distribution (4)-(7). The starting point of the algorithm is to bound the marginal likelihood (making use of the Jensen X  X  inequality): Equation (8) is valid for any distribution Q . The bound is tight when the dis-rior distribution. Fortunately, the posterior distribution of the mixture of robust PPCA model is still tractable. Indeed, applying the Bayes formula, one can show that the posterior is where the factor distributions are and where we have defined C  X  1 k =  X  k W k W k + I J ,  X  k =( D +  X  k ) / 2,and  X  nk =(( y n  X   X  k )  X   X  1 k ( y n  X   X  k )+  X  k ) / 2. Notice that there is only a single observation y n appearing in each of these posterior factor distributions. The EM algorithm then consists in tw o successive and repeated steps. The E-step consists in fixing Q to the distribution given by (9) and developing (8) accordingly. Note that only the first term of (8) (called the log-complete likeli-hood) has to be computed, as the second one does not depend on the values of the parameters. This leads to a somewhat complex expression, not detailed here for simplicity. Its evaluation necessitates to compute the following expectations: The log-complete likelihood of course depends on the model parameters; the M-step then consists in maximizing it with respect to the parameters, leading to a set of update rules for all k (tr { X } is the trace operator):
In these updates rules, the contribution of each data point is weighted ac-cording to  X   X  nk , which accounts for both the effect of the responsibilities  X   X  nk and the expected latent precision variables  X  u nk . The latter ensures robustness as its value is small for y n lying far from  X  k , such that the contribution in the M-step is small. For the non robust formulation (  X  k  X  X  X  )wehave  X  u nk =1forall n and all k . Note also that these updates are coupled: one could cycle through these updates between each E-step un til the M-step has converged.

There is no closed form update for {  X  k } K k =1 . Nevertheless, a solution can be computed by line search at each EM iterati on [2]. Alternatively, a heuristic was proposed by Shoham [11] in the context of mixture modeling.

As the marginal likelihood of mixture models has local optima, it is recom-mended to repeat the optimization with different initializations. A good strategy to initialize the components is to set the centers  X  k with a quantization algorithm and initialize the subspace orientation W k from the first Principal directions in the Voronoi region of  X  k .

Two hyper-parameters still need to be set: the number of components and the dimensionalities of the latent representations. They can be set in a traditional way by cross-validation, or added in a Bayesian way to the probabilistic for-mulation; in the latter case however MCMC sampling techniques [9] or (mean field) variational approximation [14,4] must be used instead of the exact EM algorithm. Finally Automatic Relevance Determination was used in [5] to select the dimensionality of latent subspaces. In this section, the (robust) probabilistic models are applied first on two artifi-cial examples, and then on the USPS high-dimensional real dataset, using the software available from http://www.ucl.ac.be/mlg/.

Figures 2(a)-(b) show an example where samples have been generated along a one-dimensional manifold, with higher density in the right end and higher noise at the other end. The PPCA estimates a global principal direction; the mean of the component lies in an empty region and is thus not representative of typical samples. On the other hand, the robust PPCA discards samples in order to concentrate on the higher density region of the manifold. Using three components in the model (Figures 2(c)-(d)), both the mixture of PPCA and robust PPCA estimate quite well the local principal directions. However one of the components of the mixture of PPCA (Figure 2(c)) tries to account for the noisy samples, forcing its mean to move away from the manifold.

The next example consists in data arranged in three 3-dim. Gaussian clusters (see Figure 3(a)), with diagonal covariance matrices equal to diag { [5 , 1 , 0 . 2] } before rotation around the second coordinate axis. Each component lies on an intrinsic two dimensional space as the variance in the third direction is signifi-cantly smaller. The two outer clusters make an angle of  X  30 degrees with the middle one and are respectively shifted by  X  5 units along the axis of rotation. For the first experiment, 30 data are gene rated for each cluster. The generalisa-tion performances, measured as the log likelihood on a validation set averaged on 50 experiments, are plotted in Figure 3(b) for K  X  X  1 ,..., 12 } components and J  X  X  1 , 2 } latent space dimensions. As expected, the true model with K =3 and J = 2 performs the best. Interestingly, we see that the standard and ro-bust mixture models have comparable performances when the model underfits the data (i.e. K&lt; 2) while the robust mixture has the edge when K increases. Overfitting is thus reduced with the robust formulation.
For the second experiment, K is set to 3 and J to 2 (their optimal values); we look at the sensitivity of the model to the number of outliers. The outliers are generated uniformly in the [  X  10 , 10] 3 box. Again, 30 points are generated from each component; 1 to 60 outliers are a dded. The performances measured on a validation set without outliers, and averaged over 50 repetitions as above, are shown on Figure 3(c). Again, we see the increased robustness of the proposed model, in particular when there are few outliers. When the number of outliers increases to a significant proportion of the learning data the down-weighting of the outliers in the robust model is reduced, and the gap between the perfor-mances decreases. Figure 3(d) shows t he average value of the degree of freedom parameters (  X  k for k =1 ... 3). We note that the down-weighting of the outliers obtained with small value of  X  k , comes mainly from a single component.
The last example illustrates the robustness of the proposed method on high-dimensional data. The USPS handwritten digit dataset consists in 16  X  16 pixels images of digits (0 to 9). Only the (respectively 731 and 658) images of digits 2 and 3 are kept (they form the two dominant clusters), as well as 100 (ran-domly chosen) images of digit 0. We compare the mixtures of PPCAs and of robust PPCAs in their ability to find the two main clusters (thereby identifying the 0 as outliers) and to identify the main variability in these clusters with a one-dimensional latent space. Figure 4 shows sample images close to the one-dimensional subspace. The mixture of robust PPCAs completely ignores the smaller cluster of digits 0. On the other hand, the mixture of PPCAs cannot down-weight the contribution of the digits 0, influencing the two components. This papers introduces the Mixtures of Robust Probabilistic PCA. The method is aimed to represent nonlinear manifolds and possibly identify clusters in data. All parameters of the method, with the exception of the number of clusters and the dimensionality of the latent space, are learned trough the use of a probabilistic latent formulation, and the optimization of the likelihood of the data. Compared to its non-robust parent, the method shows a strongly reduced sensitivity to outliers, even in high-dimensional spaces.

