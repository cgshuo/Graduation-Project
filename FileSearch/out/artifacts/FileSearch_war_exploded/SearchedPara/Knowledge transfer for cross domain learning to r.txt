 Depin Chen  X  Yan Xiong  X  Jun Yan  X  Gui-Rong Xue  X  Gang Wang  X  Zheng Chen Abstract Recently, learning to rank technology is attracting increasing attention from both academia and industry in the areas of machine learning and information retrieval. A number of algorithms have been proposed to rank documents according to the user-given query using a human-labeled training dataset. A basic assumption behind general learning to rank algorithms is that the training and test data are drawn from the same data distri-bution. However, this assumption does not always hold true in real world applications. For example, it can be violated when the labeled training data become outdated or originally come from another domain different from its counterpart of test data. Such situations bring a new problem, which we define as cross domain learning to rank. In this paper, we aim at improving the learning of a ranking model in target domain by leveraging knowledge from the outdated or out-of-domain data (both are referred to as source domain data). We first give a formal definition of the cross domain learning to rank problem. Following this, two novel methods are proposed to conduct knowledge transfer at feature level and instance level, respectively. These two methods both utilize Ranking SVM as the basic learner. In the experiments, we evaluate these two methods using data from benchmark datasets for document retrieval. The results show that the feature-level transfer method performs better with steady improvements over baseline approaches across different datasets, while the instance-level transfer method comes out with varying performance depending on the dataset used.
 Keywords Information retrieval Learning to rank Knowledge transfer Ranking SVM 1 Introduction Ranking is a common issue in many real world problems. For instance, students may be ranked by their GPA scores when applying for the scholarship; soccer teams can be ranked by a scoring system depending on how many matches they win and how many they lose. When considering the ranking problem in IR systems, the number of features becomes large, making the ranking task challenging. Thus, various ranking models were proposed in order to aggregate multiple features into a single ranking score, by which the documents can be properly ranked. Using the ranking model, a subset of documents with highest ranking scores is retrieved from the repository to satisfy users X  information needs expressed in queries. Classical ranking models include the Vector Space Model (Salton et al. 1975 ), the Okapi BM25 probabilistic model (Robertson et al. 1994 ) and language models (Cao et al. 2005 ). Term frequency (tf), inverse document frequency (idf) and document length (dl) are main features which come into play. With the advent of more complicated documents (e.g. web pages), a large quantity of new features are involved such as the link-based PageRank scores (Brin and Page 1998 ) and HITS scores (Kleinberg 1999 ), as well as structure-based features (title-based, metadata-based, anchor text-based etc.). Traditional ranking models often fails in handling these new IR scenarios, resulting in the introduction of machine learning technology for ranking.

The application of machine learning techniques in the ranking problem forms a spe-cialized term  X  X  X earning to rank X  X . Learning to rank employs machine learning techniques to automatically obtain the ranking model using a labeled training dataset. Much contribution has been made in developing advanced rank learning approaches. Existing approaches can be mainly categorized into three types, which are point-wise, pair-wise and list-wise, according to the different loss functions being optimized. The point-wise approaches (Li et al. 2007 ) degrade the ranking problem to classification by treating the document relevance rating as class label. The pair-wise approaches (Herbrich et al. 2000 ; Freund et al. 2003 ) explore the ordinal relationship between two documents and hence convert the ranking 2004a , b ; Burges et al. 2006a ; Xu and Li 2007 ; Cao et al. 2007 ) define a loss function on the entire document ranking list. It is notable that the pair-wise methods such as Ranking SVM (Herbrich et al. 2000 ) are the most prominent ones both in literature and practice.
Among various learning to rank algorithms, most of them belong to the supervised purpose, since human labeler need to judge the relevance level of each query-document pair. As a result, the lack of labeled training data is very common in the practice of learning to rank. Fortunately, in some cases, we may get some additional imprecise labeled data, where imprecise means that they are not drawn from the same problem or domain as the amounts of query-page pairs were previously labeled. After a period of time, these data may become outdated, since the distribution of queries submitted by users is time-varying. Here, these outdated data are treated as the imprecise training data. In another scenario, suppose a ranking model is desired for a newly born vertical search engine while only labeled data from another vertical search engine are available. These out-of-domain data are treated as the imprecise training data. Conclusively, both situations result in the cross domain learning to rank problem.

In this paper, our goal is to define and propose solutions to the cross domain learning to rank problem, i.e., to leverage useful knowledge from the imprecise data in order to enhance the performance of the learned ranking model for current ranking task. For clarity, we call the imprecise data as  X  X  X ource domain data X  X , while a small quantity of labeled data from current problem as  X  X  X arget domain data X  X . Evidently, the source domain data cannot be utilized directly in training a ranking model for the target domain due to the distribution difference. To address this, we theoretically study the cross domain learning to rank problem from two aspects: feature level and instance level. The feature-level transfer learning method assumes that there exists a low-dimensional feature representation shared by both source domain and target domain data, while the instance-level transfer learning method makes use of the source domain data by adapting each instance to the target domain from a probabilistic distribution view. Both the feature-level and instance-level transfer learning method are formalized as optimization problems which can then be solved by using Ranking SVM as the basic learner. In the experiments, we manually construct several cross domain learning to rank cases using benchmark datasets to validate the effectiveness and robustness of our proposed methods. Besides, we further hold a dis-cussion section on our findings.

The paper is organized as follows. In Sect. 2 , we formally define the learning to rank problem and further the cross domain learning to rank problem. A survey on transfer sented in detail. Finally, Sect. 4 gives our experimental details and Sect. 5 concludes our paper. 2 Problem formulation 2.1 Learning to rank Recently, learning to rank is emerging as a new machine learning branch following the traditional classification, regression and clustering research. The difference between learning to rank and these problems is that it aims to construct a model which predicts the actual ranking of documents for a query as accurately as possible. From the machine learning perspective, the loss function in learning to rank takes into account the ordinal relationships between documents rather than the absolute label for a single document. Generally speaking, there are mainly three types of learning to rank approaches, which are point-wise, pair-wise and list-wise, respectively. Among these, the pair-wise approaches have been studied most intensively. Representative pair-wise methods include Ranking SVM (Herbrich et al. 2000 ), RankBoost (Freund et al. 2003 ), RankNet (Burges et al. 2006b ) etc. In the following, we introduce the optimization formation of Ranking SVM which is accepted as one of the state-of-the-art pair-wise methods.
Ranking SVM was first proposed by R. Herbrich to solve the ordinal regression problem on the basis of classification SVM. It introduces the structural risk minimization principle into the learning to rank problem and achieves good performance in various applications. In this paper, we utilize Ranking SVM as the basic learner.

Suppose that there is an input space X 2 R d ; where d is the number of features. Gen-( x i 1 , x i 2 ) indicates that x i 1 x i 2 ; where denotes a preference relationship. The learning process is to find a ranking model f 2 F which predicts the preference relationships as accurately as possible, Further assume that f is a linear function with the regression parameter w , where ; hi stands for the inner product between two vectors. In Ranking SVM, learning to rank is formalized as a classification task on a new vector y i = x i 1  X  x i 2 , by plugging ( 2 ) into ( 1 ), x ; and z i =-1 otherwise. Now, the original learning to rank problem is equivalent to a function for learning to rank, The optimization problem can then be expressed as
Under the assumption that the training and test data share the same data distribution, labeled training data are used to estimate D (d y ,d z ), To avoid overfitting and improve the generalization ability of the learned ranking model, Ranking SVM imports a penalty on model complexity,
This optimization problem can be solved with either gradient descent or quadratic programming. In Joachims X  implementation, 1 the latter one is chosen and we use his tool in the experiments of this paper. 2.2 Cross domain learning to rank In the configuration of cross domain learning to rank problem, two parts of labeled data are available. In other words, this is a supervised transfer learning problem. In this paper, we assume that the source domain and target domain data share a common document feature space X 2 R d ; while the data distributions for them are different.

In learning to rank for information retrieval, training data are often given in the form of individual queries together with its related documents. Suppose we have two collections of labeled data denoted as D s  X  Q  X  i  X  s N s i  X  1 from the source domain and D t  X  Q  X  i  X  t the target domain. Each Q stands for a subset of training data associated with a specific responding query. The pair-wise preferences are constructed within each Q based on the relevance ratings. Note that, the labeled data from source domain are more plentiful than those from target domain, i.e., N t N s : The goal of cross domain learning to rank is to learn a ranking model f : R d ? R for the target domain by exploiting the small-scale D t and the imprecise data D s . 2.3 Related work: transfer learning The cross domain learning to rank idea borrows concept from transfer learning which has been successfully applied in many problems such as document classification. The tech-niques for transfer learning have attracted much attention since its first appearance in the NIPS workshop  X  X  X earning to Learn X  X  2 in 1995. The fundamental motivation of transfer learning is to apply the knowledge gained from one problem to a different but related problem. For example, learning to recognize apples might help to recognize pears; a programmer who has learned C ?? might help to learn Java. Recently, transfer learning techniques have been intensively studied in machine learning and data mining areas. Among various literatures on transfer learning, there are two dominant types of approa-ches. One is instance weighting approaches where each source domain instance is weighted in certain way to adjust its importance during the training process. The other is common feature learning approaches where the useful knowledge in source domain is bridged to help the target domain task via several features.
 Some instance weighting work (Lin et al. 2002 ; Chan and Ng 2005 ) assumes that P s XY j  X  X  X  P
XY j  X  X  while P s ( Y ) = P t ( Y ), called class imbalance. Lin et al. ( 2002 ) succeeded to adapt support vector machines to such nonstandard situations and validate their method via domain adaptation problem in word sense disambiguation (WSD) using naive Bayes classifiers. Similarly, some instance weighting work (Sugiyama and Mu  X  ller 2005 ; Huang Sugiyama and Mu  X  ller ( 2005 ) used non-parametric kernel density estimation to estimate the kernel mean matching problem in a reproducing kernel Hilbert space. The last situation address such a situation. Meanwhile, they also considered the covariate shift in their paper. In the cross domain learning to rank problem, we carefully sample training data to avoid the class imbalance between source and target domain. Then, we adapt the method introduced in Jiang and Zhai ( 2007 ) to this new application scenario.

The common feature learning approach is widely studied in cross domain document text data across domains based on co-clustering. The learned knowledge is passed via common words. Here, single words are treated as features. Do and Ng ( 2006 ) considered to automatically meta-learn a parameter function which may then be applied to novel classification problems. The learned parameter function is thought to be shared knowl-edge across domains. Furthermore, in the multi-task setting, Ando and Zhang ( 2005 ) considered learning predictive structures on hypothesis spaces from multiple learning tasks. Their method works well in the semi-supervised learning setting. Argyriou et al. ( 2007 ) presented a method to learn a low-dimensional representation which is shared across multiple related tasks. In our problem setting, since we have labeled data from both source and target domain, we refer to Argyriou X  X  work and construct a learning problem to find common features which may then be effective for the ranking task in target domain. 3 Optimization for knowledge transfer in learning to rank In our previous work (Chen et al. 2008 ), a pilot method was proposed to conduct knowledge transfer in learning to rank for the first time. It includes two consecutive steps which deal with the source domain and target domain training data, respectively, at instance level and feature level. However, further investigation on transfer learning as well independently. In this section, we introduce two novel promising methods. Different from problem from the machine learning perspective by formalizing it as optimization problems. A similar work by Wu et al. ( 2008 ) concerns the model adaptation problem in learning to rank. They aim to adapt an existing ranking model to various tasks. Differently, we simultaneously use the source and target training data in an integrated learning process. 3.1 Feature level knowledge transfer hyperspace based on which the ranking model is learned. For different datasets, the data distribution varies much. However, during our data analysis, we find that certain features are shared between different datasets. For example, the tfidf feature performs well to rank the documents in both the OHSUMED and WSJ datasets.

In the cross domain learning to rank problem, since learning based only on the small amount of target domain training data D t results in an unreliable ranking model in the imprecise data to strengthen the subset of features which is inherently shared between D s and D t . An improved ranking model can then be obtained based on these shared features for the target domain. During the transfer learning survey, we find that we have a similar assumption with the NIPS work (Argyriou et al. 2007 ). In this subsection, we tailor their work to the learning to rank settings. Furthermore, we prove that the optimization in their method can be converted and solved by Ranking SVM.
 3.1.1 Underlying assumption The underlying assumption of feature-level knowledge transfer is that there exists a low-dimensional feature representation shared by both source and target domain data. For simplicity, we assume these common features to be the linear combinations of the original features, where x is the original feature vector and u i is the regression parameters for the new feature h . We further assume that the vectors u i are orthonormal. Since x is a d dimensional vector, we use U to denote the d 9 d matrix with columns the vectors as u i . Obviously, the matrix U is an orthogonal matrix.

In this meaning, the cross domain learning to rank is a special case of multi-task learning (Caruana 1997 ), more precisely, a two-task learning problem. Multi-task learning has been empirically and theoretically shown to improve learning performance compared to learning each task independently. In this cross domain learning to rank scenario, we aim to learn these common features h i ( x ), i = 1, 2, ... to help the learning of ranking model in target domain. 3.1.2 Optimization formulation As many previous learning to rank algorithms did, we assume the ranking model f to be linear, that is f  X  h  X  X  a ; h hi  X  a T h , where h is the learned common features. As a result, s and t stand for source domain and target domain, respectively, we have W = UA . Intuitively, according to the assumption that the source domain and target domain share a few common features, A should have some rows which are identically equal to zero.
In order to obtain such A and U via a learning process simultaneously, we utilize the (2, 1)-norm as the regularizer. This norm is obtained by first computing the l 2 norm of the rows of a matrix and forming a new vector, and then computing the l 1 norm of this new vector. Formally, where m 1 represents the first row of M , m 2 stands for the second and so on. r is the number of rows of M . Then, the source and target domain ranking tasks can be combined to learn the common features by minimizing A kk 2 2 ; 1 . Using the pair-wise loss function in Eq. 4 , the cross domain learning to rank problem can be formulated as, where s indicates the source domain and t indicates the target domain; a k is the ranking model learned on the new feature representation; c denotes the tradeoff between the training error and the (2, 1)-norm of matrix A . Clearly, this optimization problem is non-convex and we cannot optimize A and U simultaneously.

In their paper, Argyriou et al. ( 2007 ) proved that the above optimization problem can be solved by solving an equivalent convex optimization problem as follows, where S ? d stands for the set of symmetric positive semi-definite matrices; a i denotes the i -th row of A ; D ? is the pseudo-inverse of matrix D . For more details of the proof, please refer to (Argyriou et al. 2007 ). The algorithm solving optimization in Eq. 11 is shown in Fig. 1 . This algorithm is similar to the one proposed in (Argyriou et al. 2007 ), but tailored to the learning to rank problem. Moreover, the column w t of the output W is the desired ranking unseen test data. 3.1.3 Learning with ranking SVM constraints on the target variable w . Also note that the regularizer c w ; D  X  w hi is somewhat following theorem, we prove that this optimization problem can be effectively solved using Ranking SVM after certain conversion on the training data. Please refer to Eq. 7 . Theorem 4.1 The optimization in line 7 is equivalent to the following optimization problem : where P and R are the right singular-vector and diagonal singular-value matrices in the singular-value decomposition (SVD) form of D , particularly D = P T R P . Because D 2 S d  X  is a symmetric positive semi-definite matrix, the left and right matrices after SVD are the transpose of each other.
 Proof Since w 2 range  X  D  X  X  w 2 R d : w  X  Dv for some v 2 R d ; plugging w = Dv into line 7 of Fig. 1 , we can get For the second item, Dv ; D  X  Dv hi  X  v T D T D  X  Dv : Since D is symmetric, we have D T = D .
 Conduct SVD on D , D = P T R P , Let u = R  X  Pv ,
Now, the second term of Eq. 13 becomes c u kk 2 : Look at the first term, that is, the hinge loss. Therefore, we have the following optimization problem,
We can see it is a standard optimization of Ranking SVM after we multiply each training vector y i with R  X  P . After obtaining u , the w can be computed as Combining Eqs. 14 and 15 , this ends the proof of Theorem 4.1. h
We use CLRank feat to denote the cross domain learning to rank at feature level and summarize it in Fig. 2 . For the stop condition mentioned, we can set a threshold on the minimum change of D or simply stop the loop after a preset number of iterations. 3.2 Instance level knowledge transfer 3.2.1 Underlying assumption Besides the feature level knowledge transfer, we further examine a more direct way to make use of the source domain data. The underlying assumption of instance level training data come from different but related distributions, we can adapt each source domain training example to the target domain from a probabilistic distribution X  X  view. Technically, this is implemented via instance weighting (Jiang and Zhai 2007 ). In detail, we analyze the distribution difference in two aspects and carry out the adaptation, while the other deals with the difference between P s ( y i ) and P t ( y i ).
 3.2.2 Optimization formulation According to the basic assumption introduced above, we aim to make full use of the data from both source and target domain. Using D t in the standard supervised learning way, we have Again, l h is the pair-wise loss function for the learning to rank problem. In this way, we use the empirical risk to estimate the theoretical risk in the target domain. As aforementioned, N is with a small scale, and hence it may cause the estimation inaccurate.

On the other hand, we can also utilize the labeled data from the source domain. Dif-ferent from the way of using target domain data, these data cannot be used directly, since words, we cannot use the empirical risk of these data to approximate the theoretical risk. We denote the source and target distributions by P s and P t , respectively. Let d  X  P t  X  z y j  X  = P s  X  z y j  X  and g = P t ( y )/ P s ( y ), we have
If d and g could be computed preliminarily, we can use D s to approximate the theo-retical risk in the target domain, target domain. 3.2.3 Learning with Ranking SVM When using both D t and D s for training, we combine Eqs. 16 and 18 . Therefore, the cross domain learning to rank at instance level, denoted by CLRank ins , is expressed as follows. The last term is the Euclidean norm used to regularize the model complexity. or where w i  X  1 ; for target domain data d
Equation 19 can be solved effectively using Ranking SVM directly. Note that, in this case, the training examples are associated with different weights. We employ some heu-heuristic method. We first train a ranking model from D t and then test it on D s . If a pair of documents in D s is ranked correctly, i.e., the corresponding y i is classified correctly, then in learning to rank, each y i is associated with a specific query, we use the pairwise precision of this query as d i . Formally, distribution. Due to the feature diversity in learning to rank and the absence of parametric pendent problem from learning to rank, in this work, we will not explore further in this direction and simply set g i = 1 for all the source domain training examples. We will study this problem in our future work. 4 Experiments In this section, we systematically evaluate the effectiveness of the proposed knowledge transfer algorithms using three benchmark datasets. Sections 4.1  X  4.3 give the detailed experimental settings. Section 4.4 presents the main experimental results. Finally, Sect. 4.5 holds a discussion about our findings. 4.1 Datasets Three benchmark datasets are used in our experiments: LETOR 3.0 (Liu et al. 2007 ), WSJ and AP. The latter two are datasets from the TREC ad-hoc retrieval track. The former one is a recently published benchmark dataset for learning to rank, including the .gov data from TREC 2003 and 2004 web track, and the OHSUMED data which is a subset of MEDLINE. 3
Since there are no ready datasets suitable for cross domain learning to rank problem, we manually construct several cross domain learning to rank scenarios using the above three datasets. There are three search tasks in the LETOR .gov dataset, namely, topic distillation (td), homepage finding (hp) and named page finding (np). We perform cross domain learning to rank on each task by using TREC 2003 data as the source domain and TREC 2004 data as the target domain. It is a natural instance of using outdated data . We further construct more cases using the OHSUMED (the LETOR version), WSJ and AP datasets. Since they come from different publication source, we treat them as data from different domains in order to simulate the scenario where out-of-domain data are used. The WSJ and AP datasets are not included in LETOR, so we need to manually extract features as same as those defined for OHSUMED in LETOR. WSJ contains 74,520 news articles from Wall Street Journal from 1990 to 1992, and AP contains 158,240 news articles from Associate Press in 1988 and 1990. We selected No. 101 X  X o. 300 TREC topics. Each topic (i.e., query) is associated with a number of documents, labeled  X  X  X elevant X  X  or  X  X  X rrelevant X  X . Following the previous work (Trotman 2005 ), we dropped the queries that have less than ten relevant documents. During feature extraction, for WSJ, we use its \ HL [ field as \ HEAD [ field and the  X  X  X bstract X  X  \ TEXT [ . Moreover, to ensure that each query has nearly equal number of documents with the queries in OHSUMED, we randomly sampled 150 documents for each query in WSJ and AP.

We summarize the usage of datasets in Table 1 , where the symbol T denotes the test data from the target domain. 4.2 Evaluation measures To evaluate the performance of the learned ranking model, we use normalized discounted cumulative gain (NDCG) and mean average precision (MAP) as measures. They are both widely used in the literature of learning to rank. NDCG is the abbreviation of Normalized Discounted Cumulative Gain. NDCG@ n is calculated as where R ( j ) is the relevance level of j -th document in the ranking list. Z n is a normalization factor that guarantees the ideal NDCG@ n equals to 1. MAP is the abbreviation of Mean Average Precision. First, the precision at position n is defined as Then, the average precision of a ranking list is calculated based on P ( n ).
 where N is the total number of ranked documents. pos ( n ) is a binary function, valued 1 when the document at position n is relevant. Finally, MAP is defined as the AP value averaged over all queries. It is worth mentioning that since the OHSUMED dataset has MAP. 4.3 Baselines Since no related work previously proposed, three baseline algorithms are implemented to be compared with our knowledge transfer algorithms. Table 2 gives their descriptions.
LRank std denotes the standard learning to rank using D t , but omitting D s . In LRank mix , we mix D s and D t directly, and use them as training data to obtain a ranking model. LRank mix_w is an improved version of LRank mix . In this method, we weigh the source domain data through tuning on a target domain validation set. All the training examples in source domain will be given a same weight after tuning, and then mixed with the target domain data. All the baseline algorithms above utilize Ranking SVM as the basic learner, with the default parameter setting. For the Ranking SVM tool, we use SVM light . Besides, we represent the proposed knowledge transfer algorithms as CLRank feat and CLRank ins , respectively. 4.4 Experimental results A single round of our experiments consists of the following steps: 1. Randomly split the target domain data into two parts: the target domain training set D t 2. Run the three baseline methods and two knowledge transfer methods using D s and D t . 3. Test the obtained ranking models using T .

Table 3 lists the performance comparison among all the methods when the ratio between target and source domain training data is 0.1. All these values are MAP value averaged over ten random rounds. The c in CLRank feat is tuned to be 100 using a validation set. In our experiments, we find that CLRank feat converges quickly, generally within ten iterations, which indicates that CLRank feat can find a solution with acceptable computa-tional cost.

From Table 3 , we can see that CLRank feat consistently produces the best performance except for Group 4. The percentage in the bracket denotes the relative improvements over LRank std . There are large improvements brought by CLRank feat . All the improvements by CLRank feat have been verified to be statistically significant ( p -value \ 0.05) through t test. This means that the knowledge has been successfully transferred from source domain to target domain and helps obtain a more accurate ranking model. Besides, CLRank ins does work well in some groups while fails in others. This failure phenomenon is known as interesting observation in Table 3 is that LRank mix always lowers the performance of LRank std . In other words, directly using the source domain data does not improve but hurt the performance. It is probably because the data with different distributions confuse and conflict with each other, hence making the learned model unstable. The improved LRank mix_w performs better than other two baselines as expected. For Group 1, 2, 3 and 5 where CLRank ins fails, this simple weighting method keeps getting steady improvements over the other baselines. As in information retrieval, the top-ranked results are much more concerned, we futher give the NDCG@5 values in Table 4 . The results are consistent with those in Table 3 .

We also do experiments to investigate the effect of ratio between target and source domain training data. Figure 3 focuses on Group 1 (source: AP; target: OHSUMED). Similar results are also observed on other groups. We generally increase the ratio from 0.05 to 0.5, by 0.05 each time. As shown in Fig. 3 , CLRank feat can consistently outperform the baseline methods in terms of MAP, while CLRank ins fails as indicated in Tables 3 and 4 . With the ratio getting larger, the improvement by CLRank feat goes slighter. We believe that the source domain dataset D s contains not only useful knowledge, but also noisy data. When there are too few target training examples to train a good ranking model, the useful knowledge in the source domain data is dominant while the noisy data will not affect the learner too much. However, when the amount of target domain data gets larger, nearly enough to train a good ranking model by itself, the source domain data perform much like noisy data. In all, the main contribution of cross domain learning to rank is that it only needs a small amount of target domain labeled data to achieve comparative accuracy. By employing these methods, the tedious labeling work can be lightened.
 Finally, we tune the parameter c on the performance of CLRank feat . We again focus on Group 1 with ratio set to 0.1, and change c from 10 -3 to 10 3 . This experiment also runs ten random rounds. In Fig. 4 , we draw the performance curves for LRank std , LRank mix , LRank mix_w , and CLRank feat . We can see that CLRank feat reaches the peak when c = 100. We also perform this experiment on other groups and get similar results. Therefore, in all our experiments, we fix c to 100. 4.5 Discussions From the above experiments, we can draw the following conclusions: CLRank feat is an effective knowledge transfer method; CLRank ins gains some improvements on the .gov dataset while impairs the performance on the WSJ, AP and OHSUMED. Here are some possible causes of our observations. 1. The estimation methods for d i and g i may not be suitable, especially without a 3. CLRank ins improves the ranking model on the .gov dataset, probably because the 5 Conclusions Motivated by the labeled data shortage in real world learning to rank applications, in this paper, we address a challenging problem, which is defined as cross domain learning to rank. Starting from a heuristic method introduced in our previous work, we theoretically study the knowledge transfer for learning to rank at feature level and instance level, with two promising methods proposed, respectively. Both methods obtain the ranking model by solving an optimization problem using Ranking SVM as the basic learner. Experiments show that the feature-level knowledge transfer method can always gain improvements over baselines, while the instance-level method only works well for certain dataset groups. After further analysis, we draw the conclusion that feature-level knowledge transfer is more suitable for the learning to rank problem.

In our future work, we plan to further study the cross domain learning to rank problem under the scenario where the source and target domain data originally have different feature sets. We may also consider integrating the two methods into a single one bridged by Ranking SVM to seek potential improvement.
 References
