 We propose a method for document ranking that combines a simple document-centric vie w of text, and fast evaluation strate gies that have been developed in connection with the vector space model. The new method defines the importance of a term within a doc-ument qualitati vely rather than quantitati vely , and in doing so re-duces the need for tuning parameters. In addition, the method supports very fast query processing, with most of the computation carried out on small inte gers, and dynamic pruning an effecti ve op-tion. Experiments on a wide range of TREC data sho w that the new method pro vides retrie val effecti veness as good as or better than the Okapi BM25 formulation, and variants of language models. H.3.1 [Information Storage and Retrie val]: Content analysis and inde xing  X  inde xing methods ; H.3.2 [Information Storage and Re-trie val]: Information storage  X  file organization ; H.3.3 [Information Storage and Retrie val]: Information search and retrie val  X  sear ch process ; H.3.4 [Information Storage and Retrie val]: Systems and softw are  X  performance evaluation .
 Efficienc y/scale: architectures, compression, efficient query evalu-ation; Text representation and inde xing.
Two of the most important features of a document ranking mech-anism are its retrie val effectiveness and querying efficiency . The former reflects the ability of the mechanism to retrie ve rele vant documents at the top of the ranking, while the latter represents the desire to control per -query processing time. Other features such as inde x size, inde x building speed, or memory usage during inde xing and querying are also important, but, to some extent, less crucial.
A lar ge body of research work has been devoted to impro ving retrie val effecti veness, and has led to the development of a range of inno vative retrie val models and weighting schemes. The effec-tiveness of ranking mechanisms is demonstrated through empirical Cop yright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00. studies, using resources such as the ones offered by the ad-hoc and web tracks of the annual TREC conference, see trec.nist.gov for details. The figures from these tracks demonstrate the ongoing strength of three mechanisms: the vector space model with pivoted document length normalization [Singhal et al., 1996]; the Okapi BM25 formula (BM25) [Robertson et al., 1998]; and the language modeling approach (LM) [Ponte and Croft, 1998].

Implementations of document ranking systems typically mak e use of an inverted inde x to support efficient computation of the set of similarity scores S d;q between a query q and the N documents d in a collection D . Each distinct term t in D is represented in the inde x by its document frequency f t ; its collection frequency and an inverted list storing pointers of the form h t; f d;t term frequency of t in document d is f d;t . Similarly , to denote the term frequenc y of t in the query q . Each document is also associated with a document length W d , a quantity that is pre-calculated at inde xing time and does not affect efficienc y. For example, one variant of the vector space model calculates S where f a W inde xing time with the slope s = 0 : 7 [Singhal et al., 1996] (see also the work of Cho wdhury et al. [2002]). In operation this scheme is simple: the inverted lists of all terms t 2 q are tra versed, and partial scores for indicated documents are accumulated; then the values
An alternati ve similarity formulation is the Okapi BM25 mecha-nism [Robertson et al., 1998]. One simple implementation (see, for example, Jin et al. [2002]) defines S d;q as This computation can be implemented using the same information as is used in the vector space approach, but in an operational sense is potentially less efficient than the vector space computation, be-cause of the use of the normalized document weight each time a pointer in an inverted list is examined.

In language models, the similarity score S d;q is replaced by the conditional probability P ( q j M d ) of generating the query a model M d of d . Ponte and Croft [1998] calculate this probability as: where the probability of producing and not producing the terms in the query are tak en into account. Ho we ver, the computation of P ( q j M d ) is less efficient than that of S d;q in the BM25 scheme described abo ve. Experiments with language model approaches sho w that it gives good retrie val effecti veness, and the joint state-ment by Allan et al. [2002] is indicati ve of the confidence with which a significant section of the community vie w this approach.
In all three approaches, the computation of S d;q for each docu-ment d is follo wed by a process that selects a number of highly-scoring documents, and presents them or some summary of them to the user .

In this paper we describe another way in which term frequenc y evidence can be used to generate similarity scores. The new method has elements in common with all of these three approaches, but also has a critical distinguishing feature  X  we mak e use of term fre-quenc y ranks in the scoring process, rather than numerically calcu-lated term frequenc y values . The end result of our scoring regime is still a number S q;d  X  there is no alternati ve to that if we wish to score documents  X  but the reduction in the number of tuning fac-tors and transformation functions during the computation means that there is much less room for ambiguity when implementing it.
We have carried out extensi ve experiments using the new ap-proach, using a wide range of TREC data. Despite the simplicity of the scoring regime, it performs well when compared to other similarity computations, and its retrie val effecti veness on a range of TREC tasks is good. The new method also requires less query-time information than other approaches, and has a simple evalu-ation process. The result is good retrie val effecti veness at high query processing rates, an attracti ve blend of attrib utes. The query-processing regime is also amenable to a dynamic pruning process that further enhances query throughput rates.

A brief word on nomenclature is necessary . In this work we concentrate on indi vidual documents d rather than the whole col-lection D , and it is useful to slightly abuse some of the usual no-tation. In particular , a document d is supposed to have n terms t 1 ; t 2 ; : : : ; t n the term list of d . In any formulation of S d;q , the value deri ved solely from a certain term t and the document d is called impact of term t in d , and is denoted by ! d;t . If necessary , a superscript letter is used to indicate precisely which type of impact. Thus, in Formula 1 we have, with respect to t , the cosine document im-pact as ! c the cosine query impact ! c d = ( ! d;t 1 ; ! d;t 2 ; : : : ; ! d;t n d ) as the impact vector for ing mathematical definitions, we say that, in the case of the vector space model and BM25, the similarity score S d;q is the scalar vec-tor product of the document impact vector d and the query impact vector q . The abuse lies on the projection of both the document term list T d and the query term list T q to T d \ T q to allo w the mathematical scalar vector product. That is, we write One classical interpretation of these formulations, for document impacts in particular , is that the y are the product of a term fre-quenc y TF component and an inverse document frequenc y compo-nent IDF , representing respecti vely the importance of the term in the document, and the importance of the term in the collection. We will return to the TF-IDF notion belo w.
Anh and Mof fat [2002] describe an implementation of the vector space model that supports more efficient query evaluation than the standard mechanism. At inde xing time, for each document d term t 2 d , the cosine impact ! c adjusted via a scaling process to give it certain characteristics, and then quantized to an inte ger in the range from 1 to k = 32 this method the impact values are assigned in the conte xt of the whole document collection, since statistics for the whole collection are used in the computations. In particular , the transformation of ! collection: and In this work the transformed inte ger values suggested by Anh and Mof fat are denoted by ! g Value , summarizing its key attrib utes  X  the evaluation is global, and the assignment of impacts is accomplished using a transformation based on numeric values.

Impact transformation impro ves on the standard cosine mecha-nisms in two key areas. First, since the impacts ! g tegers, the y can be stored directly , rather than f d;t values, thereby reducing the amount of computation required as queries are being processed. Second, there is no need to exhausti vely evaluate the standard vector space similarity score. The inverted lists can be impact-sorted , with each inverted list structured as a sequence of equal-impact blocks, with the blocks stored in decreasing impact order . The blocks of the rele vant inverted lists can then be pro-cessed in an interlea ved manner in decreasing order of the product between query impact and block impact. Not all the blocks need to be processed, and effecti ve pruning can be achie ved. Moreo ver, since all of the impacts are small inte gers, the accumulation process can be combined in parallel with the selection and sort process by using a score-inde xed priority queue data structure.

Anh and Mof fat [2002] give experimental evidence that impact transformation can be just as effecti ve as the standard vector space model, especially for content search in web data, where queries are typically just a few words long. The y sho w that for the collec-tion wt10g and TREC queries 451 X 500, the Global-By-V alue im-pact transformation gives excellent retrie val effecti veness.
Here we adopt the main idea of impact transformation: the use of vector space mechanism in conjunction with precomputed inte-gral impacts. The dif ference is that we eliminate the global nor -malization step, and instead use a qualitati ve ranking assignment of impacts based on localized document information, rather than a quantitati ve score-based one built on global collection statistics. We also seek to mak e this process as simple as possible. This lin-eage of ideas ensures that the new ranking method is efficient.
In summary then, our goal in this project was to establish the details of an impact-based similarity scoring mechanism that: The next few sections address these desiderata.
The most direct way of incorporating a document-centric vie w-point into the impact definition is to emplo y the same process of impact transformation described by Anh and Mof fat [2002], but locally within each document.

In this quantitati ve scor e-based approach, the cosine impacts ! formation. The requirement of document locality means that the factor W 0 not incorporate the document length factor which is crucial to the original cosine similarity concept [Singhal et al., 1996]. On the other hand, nor does it necessarily reflect an abandonment of the vector space ranking principle  X  by using each indi vidual docu-ment as the model for transformation, a document length factor is implicitly emplo yed.

In operational terms, for each document d 2 D and term t 2 d ! The population of ! c and Ne xt, each value ! c where is an arbitrarily small positi ve number which ensures that the impacts ! d impact scores, and controls the fidelity of the approximation.
Within each document d , the transformation reduces the gaps between the low and high original impacts in a logarithmic way, and allo ws the collecti ve effort of a number of low impacts to have an increased chance of overcoming a single high impact value. In other words, a document that shares a lar ger number of common terms with the query now has a higher similarity score than pre vi-ously . This transformation scheme is referred to as Local-By-V alue  X  compared to Global-By-V alue , the dif ference is that the initial scoring (prior to the con version to inte ger impacts) is based on at-trib utes local to each document.
 It is interesting to compare the two approaches. With the Global-By-V alue method, the lar gest impact value in a document still be small compared to the lar gest impact in document way of contrast, the Local-By-V alue method forces a re-e valuation process that adjusts the impact values so that, over the set of docu-ments D , the lar gest and smallest impact values of each document are roughly the same. As an intuiti ve justification, consider two documents which have about the same term appearance statistics, but one of which uses man y rare words while the other tends to use more  X  X on ventional X  words. In this case the Global-By-V alue method assigns a greater number of high impacts to the first docu-ment than it does to the second one. On the other hand, the Local-By-V alue method can assign high impact values to the  X  X on ven-tional X  words in the second document. Consequently , when a query with  X  X on ventional X  words is issued, the second document is more lik ely to be retrie ved by Local-By-V alue than by Global-By-V alue .
No w suppose that we are required to manually assign an inte gral impact between 1 and k to each distinct term of some document. We would instincti vely expect to assign a relati vely small number of high values, and a relati vely lar ge number of low values  X  in hu-man terms, a reflection of the observ ation that  X  X rdinary X  is shared among man y, while  X  X utstanding X  is less frequent.

The Local-By-V alue method, unfortunately , does not guarantee a distrib ution in which only a few things are deemed to be  X  X utstand-ing X , and pro vides little control over the distrib ution of the assigned impact values in any given document. That is, the precise value of cosine impacts is perhaps not a good basis for retrie val, and the set of impact ranks (ordered positions when sorted into order) might be a better choice, thereby ensuring that the complete range of im-pact scores is used in every document. In this alternati ve regime, the term in a document with the highest score is given an impact of k , and a group of terms with low scores is given an impact of 1 , with the assignment in between based on the relati ve position of each term in the sorted list of scores for that document.
One problem with using ranks rather than scores is the need to choose a partitioning of the n d impacts into k groups so that the number of elements in the groups gro ws in the desired way. We minimize this problem by choosing a mechanism that does not em-plo y any further tuning parameters, but does embody the  X  X an y ordinary X  principle. Taking x i to be the number of impacts of value i (where i is between 1 and k inclusi ve), we build a geo-metric sequence with x i B x i +1 . To anchor the values, we then further set x k = B 1 . The solution to these two constraints is
B = ( n d + 1) 1 =k . All of the x i values are then appropriately rounded to inte gers, with carry-forw ard of residual discrepancies. In practice, for typical values of k and n d , B is between meaning that zero, one, or two terms in each document are identi-fied as being the  X  X ost important X  ones, and assigned an impact of k . Table 1 gives a work ed example of the computation.

Use of ranks rather than numeric scores is rather liberating, and allo ws any ordering rule on terms to be applied, of which those that result in a numeric term weight are just a small subset. To create an ordering that is mapped to ranks all that is really required is  X  X ore important than X  and  X  X ess important than X  guidelines, rather  X  X his important X  numeric comparators. That is, to assign impacts, a first sorting phase orders the term list T d of document d in decreasing order of term contrib ution. Then a second mapping phase con verts each ordered position to an inte ger in the range 1 to k . As the end of the process, these inte ger values serv e as term impacts.
Once this freedom of choice is recognized, several options for ordering the terms are immediately apparent. For example, any Table 1: Computing x i , the number of impact scores of value an assumed k = 6 and n d = 100 . In this example B = 101 1 2 : 158 , and hence a tar get of B 1 1 : 158 terms are assigned an impact of 6 ; a tar get of B ( B 1) 2 : 499 are assigned an impact of 5 ; and so on. cosine formulation could be used to calculate a numeric sort key, as was proposed in the pre vious section. That method is the first one in the list belo w. In addition, other variants that step right away from numeric computation have been tested:
One slightly complication with our schemes is that of common words. In most English prose the word  X  X he X  is the most frequent, but it probably should not be assigned an impact of k in every doc-ument, since the appearance of just two or three common words in the term list for each document then means that only k 0 &lt; k dif ferent impact values are available to express the importance of the more meaningful terms. To bypass this dilemma, and ensure that the high impacts are not sub verted by common words, all stop words are given an artificial f d;t score of one during the sorting phase of the impact assignment process. The file stoplist.orig at the site http://goanna.cs.rmit.edu.au/ ~jz/ resou rces / stopping.zip pro vides a suitable list of 600 common words, and we used that list as the basis for this adjustment to the
Finally in this section, note that the number of dif ferent is high in any non-tri vial document collection. In all but the last of these four sorting rules there is only a small chance that two dif ferent terms in the same document have the same sort key value, and further tie-breaking is not critical. In the Local-By-Rank -( TF ) method, ties are handled explicitly .
The discussion and various conjectures of the pre vious section can be summarized as a multi-part hypothesis: In this section we examine these claims, and explore the beha vior of the ranking mechanism in a training environment. In particular , an appropriate value for k needs to be set. Taking k too small allo ws compact inde xes and fast evaluation, but may harm retrie val effec-tiveness. Con versely , taking k too lar ge pro vides more fidelity in the scoring process, but increases the size of the compressed inde x. The bigger hypothesis  X  that the new approach pro vides excellent retrie val effecti veness compared to the vector space model, BM25, and language model alternati ves  X  is considered in the next section, using dif ferent collections, and once the details of our document-centric qualitati ve-impact system are finalized.

The dataset used in this section is WSJ2  X  a subset of Disk2 of the TREC corpus, see trec.nist.gov . Collection WSJ2 is a homogeneous set of documents, and contains the text of the Wall Str eet Journal for the period 1990 X 1992. It has around 75 ; 000 uments totaling approximately 240 MB. To measure effecti veness, 150 short queries were formed by taking the  X  X itle X  fields only from TREC topics 051  X  200 .

We also need to specify how query impacts are computed, since the y are also part of the similarity estimation process. All of the results in this paper mak e use of the follo wing process: 1. For each t 2 q , the weight of t in q is defined as 2. The set of weights is transformed to a set of inte ger query The dif ferent procedure (to document impact computation) is re-quired because, in general, queries cannot be expected to contain suf ficiently man y terms for ranks to be sensibly emplo yed. Also worth noting is that this calculation still includes f t in an IDF fac-tor , but that it is not required until the inde x has been completed and queries are being processed.

The first set of experiment in this series, sho wn in Figure 1, com-pare the Local-By-V alue and four Local-By-Rank methods across values of k . Three standard metrics for assessing retrie val effec-tiveness are in common use. The most reliable metric is average precision [Buckle y and Voorhees, 2000]. Ho we ver, precision at 10 documents retrie ved is also included, as it is important in some searching situations. The corresponding graph for mean reciprocal rank sho wed a similar trend to the two sho wn, and is omitted.
There are several conclusions to be dra wn from the graphs in Fig-ure 1. First, three of the methods are clearly superior , except when k is small, and all three are relati vely stable when k is greater than about 6 . This compares well with the k = 32 (or b = 5 ) value noted by Anh and Mof fat [2002] to obtain stability when using globally normalized impacts. As already noted, when k is small, each inverted list has only a few blocks, and the inverted inde x is compact.

Second, Local-By-Rank -( IDF , TF ) performs poorly in all situa-tions. This outcome is also some what surprising, since at face value queries.
 Table 2: Changes in retrie val effecti veness caused by the adjust-ment in frequenc y of 600 common words, using the Local-By-Rank -( TF , IDF ) strate gy, k = 8 , and the same collection and query combination as in Figure 1. it means that the IDF factor performs only a minor role in determin-ing the contrib ution made by a term. One possible explanation for the poor performance is that IDF tak es on man y more distinct val-ues than does TF , and so there is only limited opportunity for TF to be used in a tie-breaking role. On the other hand, when the pri-mary sort key is given by TF , then IDF can be expected to be used relati vely often to break ties. That is, in Local-By-Rank -( TF , IDF ) both of TF and IDF are lik ely to influence the ordering. If this is the case, then there may be further benefit to be gained by quantiz-ing either or both of the TF and IDF components before applying the ordering step.

Third, and most satisfying, is that the methods that emplo y ranks to determine impacts are consistently slightly better than Local-By-V alue , a validation of our suggestion that ranks are the more intuiti ve assessment.

The second set of experiments then tak e the Local-By-Rank -( TF , IDF ) method with k = 8 , and explore the suitability of the process described abo ve for handling common words. Table 2 sho ws the outcomes, for the same collection and query set.
As can be seen, the stopping version has a slight (but not sig-nificant) edge in terms of effecti veness, and in the remainder of the experiments reported here we applied the f d;t = 1 stopping scheme for common words. Note that queries containing common words can still be processed, so that the decision to stop in this way is not as irre vocable as if the y were simply not inde xed at all.
In this section we compare the performance of the new retrie val mechanism to the established vector space approach using pivoted document normalization (equation 1), and the BM25 mechanism (equation 2). We have also tested the Global-By-V alue mechanism of Anh and Mof fat [2002]. All settings, including the parser used and the treatment of stop words, were identical between the three approaches; and each of these implementations was validated by comparing against other publicly-a vailable effecti veness scores in preliminary testing, to ensure that the implementations are compa-rable with those of other authors. In particular , the pivoted vector space results were compared with the BD-A CI-BCA mechanism of Zobel and Mof fat [1998]; the BM25 results with those given by Jin et al. [2002]; and the Global-By-V alue mechanism against the work of Anh and Mof fat [2002]. In all of the cases we obtained closely matching (but not quite identical) levels of effecti veness
Three datasets were emplo yed in this testing: TREC12 (com-posed of TREC disk 1 and disk 2) with  X  X itle X  queries from top-ics 051  X  200 ; TREC45 X  X R (TREC disk 4 and 5, minus the Con-gressional Record) with queries tak en from the titles of TREC top-ics 351  X  450 ; and wt10g with  X  X itle X  queries from topics These datasets were chosen because the y include some of the lar gest collections for TREC ad-hoc-style retrie val, and because each is ac-companied by a relati vely lar ge number of topics of a diverse type and length. Information about the test data appears in the various TREC overvie w papers available at trec.nist.gov .

A detailed comparison is given in Table 3. Effecti veness out-comes for the Local-By-Rank -( TF , IDF ) mechanism, for the three data sets, and for three dif ferent evaluation metrics, are presented in the first column headed  X  X core X , in all cases as averages over the set of queries. The three pairs of columns that follo w give the effec-tiveness for the three competing similarity formulations, presenting first of all a comparable average score, and then a flag to indicate whether the dif ference between the score for the new method dif-fers significantly from the score for that pre vious method. Statis-tical significance was carried out using the Wilcoxon signed-rank test over the effecti veness results from the indi vidual queries, with the 95% confidence level used. A  X + X  in any of the three columns headed  X  X ig.  X  indicates that the new method is reliably better than the corresponding pre vious technique. The lar gest value in each row of the table is highlighted in bold to assist in the comparison.
Quite remarkably , the new method outperforms (our implemen-tations of) both the pivoted vector space model, and also the BM25 scoring scheme. It also performs well compared to the Global-By-Value scheme of Anh and Mof fat [2002].

We also compared the new Local-By-Rank -( TF , IDF ) method with the published performance of the BM25J scoring regime (a BM25 implementation), the LMJ regime (a language model imple-mentation), and the TLM mechanism (the best of the  X  X itle language models X ), taking results from the work of Jin et al. [2002]. Four data collections were used in Jin et al. X  s experiments: AP2 (the AP sub-collection of TREC disk 2); AP3 (the AP sub-collection of TREC disk 3); SJM (the sub-collection SJM of TREC data), and WSJ2 (the sub-collection WSJ of TREC disk 2). For all four col-lections, TREC topics 201  X  250 are used as queries. Pivoted BM25 Global-By-V alue Score Sig. Score Sig. Score Sig.
 Collection BM25J LMJ TLM Local-By-Rank -( TF , IDF ) AP2 0.2463 0.2238 0.2667 0.3007 AP3 0.2511 0.2411 0.2711 0.2838 SJM 0.1727 0.1845 0.2081 0.2257 WSJ2 0.1719 0.1844 0.1950 0.2240 Table 4: Local-By-Rank -( TF , IDF ) versus BM25 and some Lan-guage Modeling variants. The numbers reflect the mean average precision over the query set. The last column represents the new method, Local-By-Rank -( TF , IDF ) with k = 8 ; all other values are tak en from the work of Jin et al. [2002]. Numbers in bold are the lar gest in each row.

Results from the comparison are given in Table 4. In general, the table sho ws that the new ranking method give the same or better level of effecti veness as the best version of the title language model, and is also consistently better than the two other methods tested by Jin et al.  X  a further validation of the claims made in Table 3. Note that statistical significance testing of these results is not possible, since the query-by-query details underlying the averages for the BM25J , LMJ , and TLM methods are not available to us. It should further be noted that the dif ferences between our results and those of Jin et al. might be in part or entirety a consequence of altered parsing or stemming regimes, since it is not possible for us to be certain that precisely the same experiment has been carried out.
Table 5 sho ws how the localized-ranking schemes would have fitted against two of the 2004 TREC Terabyte track runs. The two TREC runs listed were chosen by position from an ordering based on mean average precision of all 56 submitted runs for short queries, so that each row of the table reflects a single system. While it is clearly much easier to be competiti ve in arrears than at the time, we can observ e that, lik e the 2004 TREC participants, we have trained only on earlier years X  data, and simply present, with-out any further iteration, the results obtained by our system on the new data. The fact that the Local-By-Rank approach gives effec-tiveness scores well into the top quartile, and within a few positions of the top of the listing, is extremely positi ve.
 Table 5: The various Local-By-Rank versus submitted TREC runs on the 2004 Terabyte collection and queries. The row labeled  X  X REC best X  gives the effecti veness scores for the TREC run that achie ved the best score for mean average precision, over all 56 TREC short query runs; and the run labeled  X  X REC Quartile X  is the one that was 25% of the way down the same list of TREC runs when ordered by mean average precision.
There are two primary aspects to retrie val efficienc y: the speed at which queries can be resolv ed; and the volume of inde x space occupied by the compressed inverted file. In order to pro vide a baseline for measurements of these attrib utes, we mak e use of the mg system, available from www.cs.mu.oz.au/mg/ . It emplo ys a document-sorted inverted inde x storing d -gaps alternating with f d;t values, and processes queries using floating-point computa-tions based around equation 1. In particular , mg pro vided the basis for the Pivoted and BM25 results sho wn in Table 3. The mg uses a Golomb code to represent the d -gaps in the inde x, and an Elias gamma code for the f d;t values, see Witten et al. [1999] for details of these codes. While mg is by no means a state-of-the-art system in terms of retrie val effecti veness, it does pro vide a refer -ence point against which relati ve retrie val speed and inde x storage costs can be evaluated. The mg system was compiled and executed using the def ault options for rank ed querying.

The non-interlea ved nature of the impact-sorted inde xes used in the Local-By-Rank methods allo ws efficient representations to be used [Anh and Mof fat, 2005]. Impact computations are carried out strictly on inte gers, and the process of computing the highest r scoring documents is also significantly eased [Anh et al., 2001].
Table 6 sho ws, as a fraction of the collection size, the cost of Table 6: Sizes of inverted inde x, including vocab ulary files, as a percentage of the text of the collection. The column labeled  X  X e w X  is for the Local-By-Rank -( TF , IDF ) system using k = 8 the inde x stored using the carry-12 method of Anh and Mof fat [2005].
 Table 7: Query throughput rates, in queries per second measured over a sequence of 10 ; 000 random queries, using a dual 2 : 8 Intel Xeon with 2 GB RAM. All other details are as for Table 6. storing the impact-sorted inde x for some of the document collec-tions used in the experiments reported in this paper . The small dif ferences in storage cost arises because of two dif ferent factors: the mg system stores a document-ordered inde x, and changing to an impact-sorted inde x means that dif ferent data is stored; and be-cause a dif ferent inte ger representation is used. In particular , the carry-12 method used in our implementation obtains better com-pression effecti veness than the Golomb code when terms have lo-calized appearance patterns. The real message of Table 6 is that the new arrangement has no great effect on storage cost compared to what can be regarded as a benchmark system in terms of economy of storage.

Table 7 compares the same two implementations in terms of querying speed, with all experiments carried out on a dual Intel Xeon with 2 GB RAM running Debian GNU/Linux (sar ge), with a 73 GB SCSI disk for system files and twelv e 146 GB SCSI disks for data in a RAID-5 configuration. To construct the table, each of the systems executed a stream of 10 ; 000 random queries containing an average of three terms each, using a single processor . To form a query , words were chosen at random from a randomly se-lected document in the collection, with stop words (using the same stop list) not permitted in queries. Times reflect the cost of doing TREC-style experiments, in which the top 1 ; 000 matching docu-ments are identified, but not retrie ved.

The uniform gain in query throughput sho wn in Table 7 arises from three factors: the use of inte ger computations in the new method; the consequent use of a digital priority queue when ex-tracting the highest-ranking answers [Anh et al., 2001]; and the carry-12 mechanism used to represent the impact-ordered inde x, which is faster to decode than are Golomb codes [Anh and Mof fat, 2005]. As a total package, we have impro ved upon the mg system by a factor of around three.

The fact that the inde x is stored in inde x-sorted order allo ws a further benefit: dynamic pruning techniques can be easily applied to accelerate query processing. Figure 2 illustrates the benefits that are possible, by comparing the effect of three dif ferent approaches. To create the graph, the total number of document pointers involv ed in each of the queries was calculated, by summing the sizes of the inverted lists for the terms. The query was then executed multiple Figur e 2: Effecti veness of three dynamic pruning techniques, us-ing Local-By-Rank -( TF , IDF ) with k = 8 , and collection TREC12 . The three pruning techniques are: (a) ordering the processing of pointers purely by decreasing query-term impact; (b) ordering the processing of pointers purely by decreasing document-term impact; and (c) by decreasing product of query-term impact and document-term impact. times, stopping the evaluation after 0.1%, 0.2%, 0.4%, and so on, of the total pointers in the query had been incorporated into the ranking. At each partial evaluation level an overall ranking was produced, and the rele vance judgments used to assess the retrie val effecti veness.

Three dif ferent pruning strate gies were emplo yed, where a  X  X trat-egy X  is an ordering of the blocks of pointers in the inverted lists for the terms in the query . The first strate gy, denoted  X  X erm ordering X  in the graph, is the simplest. It orders blocks strictly by the query term impact score, processing pointers in decreasing order without any interlea ving of blocks from dif ferent terms. This strate gy closely (but not exactly) corresponds to the usual heuristic of processing terms in a document sorted inde x in decreasing f t order .
The second strate gy, labeled in the graph as  X  X mpact ordering X , processes pointers from terms in decreasing document impact or-der . That is, all blocks of pointers with impact k are processed, multiplied by their corresponding query impacts; then all blocks with impact k 1 ; and so on.

The third combined strate gy processes blocks of pointer in de-creasing order of the product of query impact and document impact  X  that is, in decreasing order of similarity contrib ution.
The monotonic gro wth of all three curv es in the graph sho ws that all of the available pointers contrib ute to the evaluation of queries. Ho we ver, around half of the available mean average precision score arises from the first just 1% of the pointers, suggesting that there is a definite role for dynamic pruning techniques. Of the three strate-gies, the combined one works best, in which blocks of pointers are processed in decreasing oder of the product of the document and query term impacts. When effecti veness is quantified by precision at 10 documents retrie ved, the bias towards early pointers is even more mark ed.
Local impacts, assigned as a function of the term ranks rather than as a function of their scores, have several adv antages.
The most striking of their benefits is the excellent retrie val effec-tiveness that can be obtained  X  outperforming our implementations of three competiti ve methods on a broad set of TREC ad-hoc tasks, and also outperforming published results from other authors. Their simplicity is not at the expense of quality .

In terms of querying speed, the low comple xity of the Local-By-Rank methods pays off handsomely . The fact that all calculations are performed on small inte gers, and that there is no need for query-time incorporation of document weights, allo ws very fast query processing. Inde x costs are close to what is possible with good compression schemes applied to standard document ordered in-dexes. That is, the localized by-rank methods described in this pa-per represent a genuine all-round impro vement over current meth-ods. The required inde x structures are also easy to build. There are several areas in which we still hope to mak e progress. One is in the area of pruning strate gies  X  the results in Figure 2 are encouraging, but not yet compelling, and we plan further experi-ments on long queries, with a vie w to devising impro ved dynamic pruning heuristics. A second area of future investigation is the for -mulation of the query-term impacts  X  there is still an  X  X ormula X  used in that part of the computation, and it would be interesting to try and bypass that requirement too, to mak e the entire similarity computation a qualitati ve one, rather than numeric.
 Ac knowledgment . This work was supported by the Australian Re-search Council, and by the ARC Center for Percepti ve and Intelli-gent Machines in Comple x En vironments.

