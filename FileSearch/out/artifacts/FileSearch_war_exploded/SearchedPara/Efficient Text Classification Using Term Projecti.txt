 Text classification [1, 2] is a fundamental task in text mining research area. The goal of text classification is to assign documents with pre-defined semantically meaningful labels. Traditional text classification methods always follow a supervised learning strategy: use some labeled training set and machine learning technologies, like Na X ve Bayesian, KNN, and SVM [3, 4] to build a model, and then classify the documents in test set. In general, those algorithms have demonstrated reasonable performance. 
Standard representation of text uses bag-of-word (BOW) model, with each term corresponds to a dimension. It is obvious that BOW representation will bring sparse and noisy problems, especially when the training set is relatively small. Moreover, this will also lead to curse of dimensionality issue, which is tough and common in text classification. 
A sophisticated methodology to reduce feature dimensionality is feature selection [5], such as  X  2 statistic, mutual information and information gain. In [6], they show that statistic has better performance on Chinese text dataset when the dimensionality is After feature selection, the next procedure is cluster-based text classification. The goal according to the distribution of terms with different clustering algorithm applied. Then classification. 
In this paper, we follow the similar procedure described above. First, we found that traditional  X  2 statistic method doesn't take term frequency into account. However, we argue that term frequency indeed shows relationship between terms and categories. The more a term emerges in a correspondi ng category, the stronger their relationship is. With this tiny modification, we get better performance on both English and Chinese dataset on varied dimensionality. 
Second, we do term projection according to the modified  X  2 statistics, which can be considered as a rule-based clustering algorithm. The advantage of our projection tried diverse term clustering algorithms, which bring different degrees of computa-tional costs. Moreover, they have to determine the number of clustering result, which is difficult for different datasets. 
After term projection step, we utilize the generated clusters as features to represent the documents. The benefit of using clusters as features include: (a) make most use of semantic meanings of terms. We can group similar terms into the same cluster, and cient classification speed. We can reduce the feature dimensionality by three orders of magnitude, from 60,000 to 55 in Chinese dataset, while still and maintain comparable accuracy as compared to LSA. Besides, the classification speed can be greatly accel-classification model. With the feature dimensionality greatly reduced, we need less parameter to determine the model. Furthermore, experiment result shows that this model also has better generalization capability, (d) a complement to feature selection. Feature selection aims at removing noisy features, while term clustering is good at decreasing redundant features by putting them together. In practice, we generally do feature selection first to keep meaningful features, and then condense the feature space by clustering. 
Our contributions in this paper include: (a) we modify the traditional  X  2 statistic, to the best of our knowledge, no one has take term frequency into consideration when using  X  2 statistic to do feature selection. This modification improves the performance, especially at lower dimensionalities, (b) rule-based term projection algorithm, we practical, (c) we reduce the feature dimensionality by three orders of magnitude, and still maintain comparable accuracy in comparison with LSA on homogeneous dataset. This indicates that our method require less computational cost to achieve the same performance, (d) we have observed some improvement both on classification accu-racy and speed on heterogeneous dataset using a small training corpus. 
The rest of the paper is organized as follows. In Section 2, we will review some re-transfer learning that used to solve the heterogeneous dataset problem. Then we intro-duce our term projection method in section 3. After that, projection based classification algorithms are discussed in section 4, using a rule-based manner or via SVM. Experi-ment results and discussions are shows in section 5. Section 6 concludes the whole paper and gives some future works. words in 1993, followed by a group of researchers in text classification area [7, 8, 9, 10], to establish a more sophisticated text representation than bag-of-words model via term clusters. 
Baker and McCallum [7] apply term clustering according to the distributions of class labels associated with them. Then use these learned clusters to represent the documents in a new reduced feature space. They get only a slight decrease in accu-racy; however, the cluster-based representation is significantly more efficient than BOW model. 
Bekkerman et al. [8, 9] follow the similar idea. They introduced a new information bottleneck method to generate cluster-based representations of documents. What X  X  more, combined with SVM, their experiment result outperforms other methods in both accuracy and efficiency. The shortcoming of all mentioned work is that they spend extra computational cost on term clustering, and some parameters like number of clusters should be determined. Unfortunately, this is always tough for different computational cost introduced. 
On the other hand, traditional text classification strategies always make a basic as-sumption: the training and test set are sampling from the same distribution. However, this assumption may be violated in reality. For example, it is not reasonable to assume that web-pages on the internet are homogeneous because they change frequently. New terms emerge; old terms disappear; identical terms have different meanings. Recently, application of skills and knowledge learned in one context being applied in another context. In this paper, we make use of cluster-based representations of documents to alleviate this heterogeneous problem. We gain improvement on both classification accuracy and speed, which give evidence of better generalization ability of our method. In this section, we will introduce our term projection algorithm, which is significantly more efficient than other clustering algorithms. First, we present the modified  X  2 sta-tistic formula, with term frequency taken into consideration. We also give some ex-planation and benefit of doing this. We also use the modified  X  2 statistic to do feature selection in the following experiment. Second, we straightforwardly utilize modified statistic to do term projection, which is quite efficient. 3.1 Modified  X  2 Statistic Yang et al. [5] has investigated several feature selections for text classification. They formula as follows. 
Using a two-way contingency table of term t and class label c , we can found four elements in the table, where A is the number of times that both t and c occur, B is the number of times that only t occurs, C is the number of times that only c occurs, D is the number of times that neither c nor t occurs. N is the number of documents in the training set. Statistics are performed at document level. The formula is defined to be: Ideally, t and c always occur or disappear together, which means that t and c have a strong relationship. Then once a document contains term t , maybe we are confident enough to classify it to class c . On the other hand, if t and c are completely independ-ent, then we get a value of zero in formula (1). We computed the  X  2 statistic between a particular term t and all the class labels in the training set. We use the maximum value to represent the final score of term t , which is known as  X  2 max . The formula is defined as: (m is the number of classes in the training set) We also record the  X  2 total value for future use, which is defined as: ample, a document d which belongs to class c contains two terms, t1 and t2 . Suppose ship with c than t1 has. But this term frequency information is not revealed in formula which assumes t1 and t2 have the same relation with c . 
We perform statistic at the term level to consider the missing term frequency information discussed above. Use the same annotations before; now A is the total of all terms (term t not included) outside class c , N is the total frequency of all terms in training set. Experiment results on both English and Chinese datasets show that our modified  X  2 statistic has better performance, especially at lower dimensionalities. Actually, we obtain 18.4% improvement on Chinese dataset at 200 dimensions. 3.2 Projection by  X  2 Statistic As stated above, we use modified  X  2 statistic and  X  2 max to do feature selection. In tradi-candidates. However,  X  2 max values have natural semantic meanings. For example, in our experiment dataset, the term  X   X  X  X  (director) X  gains  X  2 max in class  X   X  X  X  (movie) X , which gives evidence that  X   X  X  X   X  has the strongest semantic relationship with  X   X  X  X   X  among all the classes. So, we have every reason to use  X  2 max information to do seman-tic term projection. 
Furthermore, we record the  X  2 max values of each term and the corresponding class that it gains  X  2 max value. Then we make semantic matching between terms and classes, which can be considered as term projectio n (clustering) procedure straightforwardly after feature selection. Similar terms are projected to the same cluster. 
The benefit of our proposed projection method is: (a) make most use of semantic meanings of the dataset. Dataset is usually labeled by human, which is in high quality. parameters introduced. Other clustering algorithms always require extra parameters, such as clustering numbers, iteration convergence control parameters. It is always cantly more efficient. Unlike other clustering algorithms, all those projections are generated straightforwardly after previous feature selection step without any extra computational cost brought about. 
Meanwhile, we do some post-processing jobs to reduce noise. Certain terms may class on different datasets. Consider classifying documents into classes by individual either class. To solve this problem, we only keep the terms whose  X  2 max value makes up at least  X  % of their  X  2 total value.  X  is set as 50 in this paper. In this section, we will show how to utilize the generated clusters as features to repre-sent the documents. We reduce the feature dimensionality by three orders of magni-tude using this more sophisticated text representation. The direct benefit of this we always desire splendid processing speed as the documents on the internet accumu-late exponentially. Besides, we also gain better generalization performance using this representation on heterogeneous dataset. 
In subsection 4.2, we proposed two strategies to do classification task using clus-ter-based representation. The former performs in a rule-based manner, which classi-advantage of classification power of SVM. In our experiment, the latter method achieves better performance with a little more computational cost. While in practice, we are free to choose either method under different situation. 4.1 Cluster-Based Representation cussed before, this representation is more sophisticated than traditional BOW model with semantic meanings of terms taken into consideration. Similar terms are projected to the identical cluster. We project the documents from previous feature space to the newly created feature space, in which each dimension corresponds to a cluster. 
Then, we elaborate the concept of discriminability and a straight metric of this concept [14]. Discriminability measures how unbalanced is the distribution of term among the classes. A term is said to be have high discriminability if it appears signifi-cantly more frequent in one class c than others. Once this term appears for quite a lot of times in one document, it is reasonable to infer that this document belongs to class PR instead hereafter) to measure the discriminability of a term, where df means number of documents: discriminability of term t , which is denoted as PR max . n binary classification problem. Assume that t 1 and t 2 are projected to positive class c 1 dimension corresponds to c 2 . Corresponding feature weighting is performed as: The weighting schema is similar to traditional TF*IDF, with discriminability taken into consideration. As a result, weight i indicates the possibility that d belongs to class c . Therefore, we tend to label d as positive according to above information. 
As we can see, feature dimensionality is greatly reduced using this representation, our method has significantly more efficient training and classification speed, espe-cially when the dataset contains hundreds of thousands of class labels. 4.2 Classification In the last subsection, we have demonstrated have to use the clusters to represent the documents. The dimensionality of the new feature space is exactly the number of classes in training set, with each dimension corresponds to a class. be performed in a rule-based manner, which is quite efficient. At the meantime, ex-periment results prove this method maintain comparable accuracy. In addition, we can machine learning task especially text classification. We apply the same cluster-based model, which is used to classify documents in the test set. 5.1 Experimental Setting We carry out experiments both on Chinese and English datasets. 20Newsgroups [16] is a widely used English document collectio n. We choose this collection as a secon-dary validation case for modified  X  2 statistic. 
For Chinese document collection, we involve two datasets. One is the electronic version of Chinese Encyclopedia ( CE ). This collection contains 55 categories and homogeneous. The other is Chinese Web Documents ( CWD ) collection. It has the same taxonomy as CE, including 24016 single-labeled documents. The distributions of two Chinese text collections are diverse though under the same taxonomy, which reflects the heterogeneous problem. 
Libsvm [17] with linear kernel is used as our SVM classifier. Previous work [6] shows that Chinese character bigram has better performance than Chinese word unit at higher dimensionality. Besides, we don X  X  have to consider Chinese word segmenta-tion problem. We use bigram as our term unit. Finally, Micro-average F1-Measure is adopted as performance evaluation metric. 
In addition,  X  X raditional method X  used her eafter follows a straightforward strategy: use traditional  X  2 statistic with various dimension cutoff values to do feature selection, and then use Libsvm to train a SVM model, finally, classify the documents in test set. 5.2 Modified  X  2 Statistic quency into account indeed improves accuracy especially at lower dimensionalities. applied. 
Experiment result on CE is shown in Fig.1. X-axis represents the dimension cutoff value, and Y-axis means the corresponding F1 value. It is remarkable that we gain 17% improvement at dimensionality of 100, from 35.1% to 52.1% and 18.4% improvement at 200, from 44.4% to 62.8%. Furthermore, we can promote the per-formance on various dimensionalities to a certain extent. 
To verify the effectiveness of our method further, we also carry out the same ex-periment on 20NG dataset. Result is shown in Fig.2, which is similar to CE. This shows that our proposed modified  X  2 statistic approach has good generalization performance. On the other hand, we can infer that term frequency is helpful informa-feature selection method. 
In addition, we also obtain greater promotion on Chinese dataset compared to Eng-lish dataset, which indicates that our method is more appropriate for Chinese text classification. Therefore, we use CE (for homogenous case) and CWD (for heteroge-neous case) collections in the following experiments. 5.3 Homogeneous Dataset We first focus on homogeneous case, which means that training and test set are sam-pling from the same distribution. We split CE dataset to training and test set according to the proportion of 9:1. With 64529 documents used as training set and 7140 used as test set. statistic to do feature selection and term projection. Two parameters introduced in this step, one is the dimension cutoff value T , which determines the selected feature size, and the other is  X  , which remove the noisy term whose  X  2 max value is relatively small words, we only keep the terms whose  X  2 max value makes up at least 50% of their  X  2 total utilize all the terms in the training set, as we only need to record the projection result of them. While in traditional method, each term corresponds to a dimension, it X  X  im-possible to handle the dimension at that level. 
Then we use the term projection information to represent the documents in training and test set, following the procedure in subsection 4.1. Using this cluster-based repre-sentation, we can extremely reduce the feature dimensionality to the number of space to represent the documents in CE dataset. We can apply rule-based or SVM-based algorithms to do classification discussed in subsection 4.1. The results of both methods are shown in Fig.3 along with different values of T . 
As illustrated in Fig.3, we get better performance when T increases, which shows that our noise reducing method is effective. Generally speaking, using rule-based manner, we can achieve an acceptable and comparable performance with extremely efficient classification speed. Furthermore, we can an approximately 5% improvement by SVM compared to rule-based method with a little more computational cost. We gain the best performance when T reaches 600,000, which indicates that we almost because of dimensionality curse problem. 
Rule-based method gets F1-value of 78.3%, a comparable result with traditional method with dimension cutoff value of 2,000. While SVM-based method gains F1-value of 82.4%. Using the traditional method, we gains the same performance with dimension cutoff value of 4,000. In other words, we can get an acceptable and compa-rable result with less training efforts and greater classification efficiency. The training memory. 
As we can see clearly from the table, using SVM-based method, we can save training time by 75 percent, as well as test time by 50 percent without any lost in F1. Furthermore, rule-based method does not require model training process, and the classification step is really efficient. In fact, we spend only about 2 percent test time of traditional method, but gain a small improvement. Traditional Method 4,000 430.676s 139.873s 82.4% Traditional Method 2,000 247.805s 91.957s 77.9% 
We also do comparisons between Latent Semantic Analysis (LSA) and our method. As we all known that LSA is time consuming and computational intractable, which is not suitable for practical application. In a PC with Intel Core2 2.10GHz CPU and 3G memory, it takes about an hour to do singular value decomposition (SVD) when the dimensionality is reduced to 200. We also perform similar experiments on dimensionalities of 55 and 100. Results are shown in Table 2. On the contrary, we obtain some improvement both on classification accuracy and speed compared with LSA. Rule-based and SVM-based methods get F1-value of 78.3% and 82.4% with dimensionality reduced to 55, which gains improvements of 2.4% and 6.5% compared to LSA with the same dimensionality. 5.4 Heterogeneous Dataset To verify the generalization performance of our proposed methods, we carry out simi-lar experiments on heterogeneous datasets. As shown before, distributions of CE and CWD datasets are distinct. CE stands for a more constant distribution, while CWD reflects the characteristic of web documents which change from time to time. We use the smaller portion of CE as our training set in the following experiment, which con-tains 7140 documents. CWD with 24016 documents is used as test set. 
We follow the same steps in previous experiment. Results of rule-based and SVM-based methods on heterogeneous datasets are shown in Fig. 4. F1-value of both meth-ods increases along with larger value of T . Overall, performance of SVM-based method exceeds rule-based method by 3%. Both methods reach peak performance when T is 200,000, with F1-values of 64.4% and 68.3%. We also compare our methods with traditional one. Traditional method gains best F1-value of 64% at dimensionality of 60,000. Both of our methods get better per-formance with less training and test time. SVM-based method uses about 2 percent training time and 7 percent test time of traditional method, but gains improvement of 4.3%. Besides, Rule-based method ignores training process and uses only about 1 percent test time of traditional method, while still maintains comparable performance. Traditional Method 60,000 179.51s 462.875s 64% In this paper, we proposed an efficient text classification method based on term projection. First, we show that our modified  X  2 statistic promotes the performance especially at lower dimensionalities. Then, we project the terms to appropriate classes using the modified  X  2 statistic, this can make most use of semantic meanings of terms. We also use a more sophisticated cluster-based text representation to reduce the fea-ture dimensionality by three orders of magnitude. Finally, Rule-based and SVM-based methods are adopted to do classification. Experiment results on both homogeneous and heterogeneous datasets show that our method can greatly reduce performance than traditional method. As a result, our method is practical in the large-scale text classification tasks which require efficient classification speed. Whether our method is effective on other heterogeneous datasets is left as future work. This work is supported by the National 863 Project under Grant No. 2007AA01Z148 and the National Science Foundation of China under Grant No. 60873174. 
