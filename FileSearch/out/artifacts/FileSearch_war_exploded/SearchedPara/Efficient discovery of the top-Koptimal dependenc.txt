
Dependency analysis is a fundamental problem in all empirical science. A common form of the problem is to find dependency rules between categorical attributes. Such rules X  X  A or X  X  X  A can explain, which factors can cause or prevent other factors. Statistical dependencies do not necessarily express causal relationships, but still they give an important insight on the regularities in data.
For example, in the medical science, an important task is to search for dependencies between gene alleles, environ-mental factors, and diseases. The interesting dependencies do not necessarily have to be strong or frequent, but instead they should be statistically valid, i.e. genuine dependencies which hold also in the future data. Therefore, the signifi-cance of discoveries is tested with Fisher X  X  exact test or  X  when it is infeasible  X  with the  X  2 -test.

When the number of attributes is relatively small, it is possible to implement a brute-force search, which tests all possible dependency rules. However, the problem becomes soon intractable when the number of attributes increases. For example, if we have 9 binary attributes, there are about million possible dependencies to check. If the number of attributes is 15, there are already 15  X  10 9 possible depen-dencies. This exponential explosion is not surprising, since even a simpler problem  X  finding the best classification rule containing only positive attributes  X  is already N P -hard [1].
The traditional rule discovery algorithms, like Apriori [2], do not work either, because they are based on the frequency-based pruning. In the worst case, they find only independen-cies or spurious rules, while all significant dependencies are missed [3].

In addition, statistical dependence is not an anti-monotonic property, which means that an attribute set can produce a strong dependency rule, even if all its subsets contained only independencies. The same holds for the statistical significance, which means that a totally different kind of a search strategy is needed.

In this paper, we introduce a new algorithm called King-fisher , which is able to search for the K best, non-redundant dependency rules efficiently. We focus on the Fisher X  X  exact test ( p -value) as a search criterion, but the same algorithm is applicable to other goodness measures, like the  X  2 -measure or mutual information, as well. The requirement for non-redundancy means that a more specific rule is pruned out, if a better, but more general rule has already been found. This is an essential property, when statistical dependencies are analyzed, because non-redundant rules can obscure the real dependencies and make the rules less accurate. Compared to the nearest existing solutions, Kingfisher is significantly better scalable, without their restrictions like minimum fre-quency thresholds, restricted rule lengths, or fixed conse-quent attributes. In addition, the algorithm searches both positive and negative dependency rules, which is a more demanding task than the search for only positive rules.
The algorithm is based on the common branch-and-bound strategy, supplemented by several pruning properties. The most important new inventions are a tight lower bound for Fisher X  X  p and a new effective pruning principle called Lapis philosophorum .

According to our experiments, Kingfisher is extremely well scalable and can handle even dense and high-dimensional data sets efficiently. Compared to the  X  2 -measure, Fisher X  X  exact test is not only better scalable, but it also produces better quality rules, which are likely to hold well in the future data. This is an important result, because scientists prefer to use Fisher X  X  exact test, whenever it is feasible.
 The rest of the paper is organized as follows: In Section II, we define all basic concepts and the problem in detail. In Section III, we review the existing solutions and other related research. The new theoretical results are introduced in Section IV. Section V represent the new Kingfisher algorithm. Experimental results are reported in Section VI and the final conclusions are drawn in Section VII.
In this section, we formalize the idea of dependency rules, represent Fisher X  X  exact test and the corresponding goodness measure, which will be used to estimate the statistical significance of dependency rules, and analyze the problem of redundancy. The notations are introduced in Table I. A. Dependency rules
Definition 1 (Dependency rule): Let R be a set of binary attributes, X  X  R , and A  X  R \ X . Rule X = x  X  A = a , where | X | = l , x  X  X  0 , 1 } l and a  X  X  0 , 1 } , is a dependency rule, if P ( X = x, A = a ) 6 = P ( X = x ) P ( A = a ) . The dependency is (i) positive, if P ( X = x, A = a ) &gt; P ( X = x ) P ( A = a ) , and (ii) negative, if P ( X = x, A = a ) &lt; P ( X = x ) P ( A = a ) . Otherwise, the rule is called an independence rule.

In this paper we concentrate on rules, where all attributes in X are positive. For values A = 1 and A = 0 , we use notations A and  X  A . Now dependency rules can be expressed simply by X  X  A , X  X  X  A ,  X  X  X  A , and  X  X  X  X  A . We notice that negative dependence between X and A is the same as positive dependence between X and  X  A . Therefore, it is enough to consider only rules which express positive dependencies.
 B. Fisher X  X  exact test
Generally, the statistical significance of a dependency between X and A = a is estimated by calculating the probability p that the observed or a stronger dependency would have occurred by chance, if X and A were actually independent. The p -value can be calculated from a cumula-tive hyper-geometric distribution using Fisher X  X  exact test: p F ( X  X  A = a ) = where J = min { m ( XA 6 = a ) , m (  X  XA = a ) } .
Here, p F is used only as a goodness measure for ranking the dependency rules. We do not try solve the multiple testing problem and decide at which level the rule can be called significant in a statistical sense. Solutions for this problem can be found in [4], [3].

From the definition of p F we see that p F ( X  X  A ) = p F (  X  X  X  X  A ) and p F ( X  X  X  A ) = p F (  X  X  X  A ) . Therefore, it is enough to search for only rules of the form X  X  A or X  X  X  A . The same property holds also for the famous  X  2 -measure. It is generally assumed that the  X  2 -measure can approximate p F quite well, when all expected absolute frequencies nP ( X ) P ( A ) , nP ( X ) P (  X  A ) , nP (  X  X ) P ( A ) , nP (  X  X ) P (  X  A ) are sufficiently high (e.g.  X  5). However, according to our experiments, the ranking order can differ quite much, even if the frequencies are large. C. Redundant rules
An important task in all rule discovery is to identify redundant rules, which add no new information to remaining rules. When the objective is to find statistical dependencies, independent attributes do not add any new information on the dependency. In fact, they can rather blur the interpreta-tion of dependencies. For example, if people suffering for disease A are more likely to have a gene allele B than healthy people, then there is a positive dependency between A and B . In addition, there are a lot of gene alleles which have no effect on disease A or the occurrence of B . For any such independent allele C we can construct a dependency rule BC  X  A , which is equally strong to the original rule B  X  A , but the conclusions could be quite different. Now one could assume that the alleles B and C together cause the disease and consider only people with both alleles B and C as a potential risk group. Even a more serious error could happen, if C actually prevented the disease ( A and C were negatively dependent), but now the dependency would also be weaker than in the original rule.

Generally, adding new attributes to the antecedent of a dependency rule, can 1) have no effect, 2) weaken the dependency, or 3) strengthen the dependency. When the task is to search for only positive dependencies (concerning either A or  X  A ), the first two cases can only add redundancy. The third case is more difficult to judge, because now the extra attributes make the dependency stronger, but in the same time the rule usually becomes less frequent. If the frequency is very small, the observed improvement can well be due to chance. For example, if BC occurs on just one row  X  where A is also true  X  then the dependency is the strongest possible, but we have no guarantees that the same dependency would hold in the future data. Therefore, we need some test to decide, whether the extra attributes made the rule better or worse.

In this paper, we generalize the classical definition of redundancy (e.g. [5]) to a general goodness measure M :
Definition 2 (Redundancy): Let M be a goodness mea-sure. Rule X  X  A = a is redundant, if there exists rule Y  X  A = a such that Y ( X and M ( Y  X  A = a ) is better than M ( X  X  A = a ) . Otherwise, rule X  X  A = a is non-redundant.
 D. Search problem
Given a positive integer K , measure function M , and a data set r , the problem is to search for the K most significant dependency rules with measure function M . This problem is also known as K -optimal rule discovery problem (e.g. [6]). In our case, the default goodness measure is Fisher X  X  p F . No other restrictions like minimum frequency thresholds or maximum rule length (number of attributes in the antecedent) are allowed, because otherwise we could no more guarantee that the discovered rules are globally optimal.

The same search algorithm can be used to search for all non-redundant dependency rules, whose goodness value M exceeds some given threshold. However, in practice, the users are interested in only the most significant rules, and searching for the top 50, 100, or at most 1000 rules suffices. Searching for only the top-K rules is also more efficient, because we can use stricter conditions to prune the search space. In addition, this approach saves memory significantly.
The problem of searching for the most significant, non-redundant, positive or negative dependency rules with a statistical goodness measure can be divided into three sub-problems. First, each rule X  X  A = a should express a statistical dependency, which means that the lift should deviate from one. Second, the significance of the rule should be measured by a statistical goodness measure, which reflects the probability that such a strong dependency had occurred by chance, if X and A were independent. Third, the rules should be non-redundant, which means that a more specific rule is pruned out, if there already exists a better but more general rule.

Most of the previous research has tackled only some of the subproblems and only a few of the existing algorithms have searched for both positive and negative dependencies. As far as we know, no scalable solutions are known to the general problem.

The simplest way to solve the problem is to search for association rules [7] with a sufficiently low minimum fre-quency threshold and then select the most significant, non-redundant dependency rules in the post-processing phase. The problem of this approach is that association rule mining requires relative large minimum frequency thresholds to be feasible. With large-dimensional data sets the threshold can be 0.60-0.80, which means that the dependency should occur on at least 60 X 80% of the rows. Such rules have always low lift values and the strongest and most significant dependencies are missed. In addition, a lot of extra work is done, because most of the frequent rules do not express any significant dependencies or are redundant specializations of already discovered rules.

A better approach is to search directly for dependency rules, but the problem is still complex. Therefore, a common solution is to consider only some special cases and use some minimum frequency thresholds, restrict the maximal rule length, fix the consequent attribute, or use some other restrictions to prune out  X  X ninteresting X  rules.
The most common approach is to search for classification rules X  X  C with a fixed consequent attribute C . In [1] and [8] classification rules were searched for with the  X  2 -measure. These approaches utilized the convexity of the  X  2 -function and determined a minimum frequency threshold, which guarantees a certain minimum  X  2 -value. It was noted that the approach is quite inefficient, because the resulting minimum frequency thresholds were too low. In [9], a more efficient solution was developed for searching for the best classification rule with a closed set in the rule antecedent. The  X  2 -measure was used also in [10], in addition to minimum frequency thresholds and some other pruning heuristics, to test the goodness of rule X  X  C and whether the improvement of rule X  X  C with respect to its immediate generalizations ( Y  X  C , X = Y B for some attribute B ) was significant.

In [11], only non-redundant classification rules were searched for using different measure functions, including confidence and lift. No minimum frequency thresholds were used, but instead it was required that P ( X | A ) was larger than some predefined threshold.
Searching for general dependency rules with any conse-quent attribute is a more difficult and less studied problem. In [12], positive dependencies between just two attributes were searched for using an upper bound for Pearson X  X  correlation coefficient as a measure function. In [13], non-redundant, positive dependency rules were searched for with the z -score. No minimum frequency thresholds were required, but the notion of redundancy was more restrictive than the classical definition.

Webb X  X  MagnumOpus software [6] is able to search for, among other things, general dependency rules X  X  A with different goodness measures, including leverage and lift. In addition, it is possible to test that the rules are significantly productive. In practice, MagnumOpus tests the improvement of the confidence of rule X  X  A with respect to its immediate generalizations ( Y  X  C , X = Y B for some B ) with Fisher X  X  exact test. However, the significant productivity alone does not guarantee that the rules were non-redundant.

All of the above mentioned algorithms search for only positive dependencies, but there are a couple of approaches which have searched for also negative dependency rules. In [14], negative rules were searched for using a minimum frequency threshold, minimum confidence threshold, and Pearson X  X  correlation coefficient as a goodness measure. Since Pearson X  X  correlation coefficient for binary attributes is the same as p  X  2 /n , the algorithm should be able to search for dependencies with the  X  2 -measure, as well.
In [15], general positive and negative dependency rules ( X = x )  X  ( A = a ) , x, a  X  X  0 , 1 } , were searched for using leverage as a goodness measure. No minimum frequency thresholds were needed, but the rule length was restricted. In [16], leverage was also used as a goodness measure together with a minimum frequency threshold and a minimum threshold for the certainty factor. In [17], a heuristic algorithm was introduced, based on the (faulty) assumption that statistical dependence would be a monotonic property. However, the algorithm should be able to find positive and negative dependency rules among two attributes correctly.

In this section, we introduce the theoretical basis for the search algorithm. We describe the basic branch-and-bound search, give lower bounds for the goodness measure p F , and introduce additional pruning properties.
 A. Branch-and-bound search
The whole search space can be represented by an enumer-ation tree (Fig. 1). Given a set of attributes R , the enumera-tion tree lists all possible attributes sets X  X  X  ( R ) . In each set X , rules X \{ A i } X  A i = a i , A i  X  X , a i  X  X  0 , 1 } can be generated. We recall that this form of rules covers dependency rules X  X  A i , X  X  X  A i ,  X  X  X  A i , and  X  X  X  X  A i . If any of these rules X  X  A i = a i has a sufficiently low p F -value, p F ( X  X  A i = a i )  X  max p there are no better rules Y  X  A i = a i such that Y ( X , the rule is non-redundant and significant. If the task is to search for only the best K rules, the maximal p F -value max p is updated always, when a new K th best rule with a lower p F -value is found. On the other hand, if the task is to search for all non-redundant dependency rules, the user-defined threshold max p remains constant during the whole search.

The basic idea of the branch-and-bound search is to estimate the best possible p F -values for all possible con-sequences A i = a i in a subtree, whose root is given. If the lower bound is too high, consequence A = a i is impossible in the whole subtree.

If the path from the root of the main tree to the root of the subtree contains attributes X , we should estimate a lower bound for p F ( X \{ A i } Q  X  A i = a i ) , where Q  X  R \ X , A i /  X  Q . This covers the current antecedent X \{ A i } and any of its supersets X \{ A i } Q . The consequent attribute A i can either occur in X , be added to the subtree later, or be absent from the whole subtree. In the last case, no rules XQ  X  A i = a i are generated in the X  X  X  subtree. Instead, rule XQ  X  A i = a i can be generated in another subtree, and set XQ is needed as its parent set for evaluating the rule.

In the following subsection, we will derive lower bounds, which are required for the basic branch-and-bound search. However, the basic branch-and-bound is quite inefficient as such. Therefore, we will introduce additional pruning properties, which can prune the search space remarkably.
The search order has also an important impact on the efficiency. In principle, the attributes can be in any order in the enumeration tree and the enumeration tree can be traversed either in a breadth-first or depth-first manner, from right to left or left to right. In practice, the best approach is to order the attributes in an ascending order by their frequency, i.e. in Fig. 1, P ( A )  X  . . .  X  P ( E ) . Now the largest subtree (under A ) is likely to contain the least frequent sets and many of the sets can be totally missing (have zero frequency). The breadth-first search is also more efficient than the depth-first search, because we search for only non-redundant dependency rules. In the breadth-first search the more general rules are checked before their specializations, and it is possible to prune out the specializations without checking, if they would be redundant with respect to already discovered rules. Finally, we prefer to perform the search from left to right, because it enables an efficient extra prun-ing property called Lapis philosophorum , which is described in a subsequent subsection.
 B. Lower bounds for p F
To perform the search, we need a lower bound (best possible value) for p F ( X \{ A i } Q  X  A i = a i ) . Here we give lower bounds for three different cases: 1) for p ( X  X  A i = a i ) , when only m ( A i = a i ) is known, 2) for p F ( XQ  X  A i = a i ) , when m ( X ) and m ( A i = a known and A i /  X  X , and 3) for p F ( X \{ A i } Q  X  A i when m ( X ) , m ( XA i = a i ) , and m ( A i = a i ) are known. The first lower bound is needed in the beginning to decide, whether any rule with consequence A i = a i could be significant. The second lower bound is needed always, when A i /  X  X . This occurs for example on the first level, when the sets of single attributes are processed, and we should decide possible consequences for an attribute B or its supersets BQ . The third lower bound is needed when A i  X  X . It is tighter than the second lower bound, because now we know also m ( XA i = a i ) and therefore and upper bound for m ( XQA i = a i ) . The first two lower bounds are quite simple and likely to be known also in the previous research. However, to the best of our knowledge, the proof for the third lower bound is a new result.
 is the number of rows. For any attribute A  X  R and X  X  R \{ A } , p F ( X  X  A )  X  p abs and p F ( X  X  X  A )  X  p abs p always  X  1 , the minimum value is p abs . Case p F ( X  X  X  A ) is similar.

This means that p abs is the absolute minimum value for any rule, where either A or  X  A is the consequence. The minimum value is achieved, when m ( X ) = m ( A = a ) = m ( XA = a ) . The p abs value is lowest, when m ( A ) is closest to n 2 . On the other hand, if m ( A ) or m (  X  A ) is very low, p abs can be so large that no rule with consequence A or  X  A could be significant. On the algorithmic level this means that A and  X  A can be marked as impossible consequences and A can occur at most in the antecedent part of a significant rule. If m ( A )  X  n 2 , then A cannot occur even in the antecedent of any significant rule, and it can be pruned: Observation 1: If m ( A )  X  n 2 , then for all X  X  R , B  X  R , holds p F ( XA  X  B = b ) = p F ( B = b  X  XA )  X 
This observation can be used to determine a minimum fre-quency threshold min fr  X  0 . 5 such that no rule containing A , P ( A ) &lt; min fr , can be significant.
 The second lower bound is the following: Theorem 2: Let R be like before. For all X ( R , A  X  R \ X , a  X  X  0 , 1 } , and Q  X  R \ ( X  X  X  A } ) holds
If m ( X )  X  m ( A = a ) , then Therefore, p F  X  t J , where J = min { m ( XQA 6 = a ) , m (  X  ( XQ ) A = a ) } . Because m ( XQ )  X  m ( X )  X  m ( A = a ) , J = m ( XQA 6 = a ) and p F has a lower bound t
This result is especially useful, when m ( XA = a ) is unknown. However, it applies only, when m ( X )  X  m ( A = a ) . If m ( X ) &gt; m ( A = a ) , it is possible that there is a superset XQ such that m ( XQ ) = m ( A = a ) = m ( XQA = a ) and p F achieves its absolute minimum value p abs . When m ( XA = a ) is known, the following result gives a tighter (larger) lower bound: Theorem 3: Let R be like before. For all X ( R , A  X  R \ X , a  X  X  0 , 1 } , and Q  X  R \ ( X  X  X  A } ) holds dependency rule X  X  A = a holds m ( XA = a ) &gt;
Let us notate p F = p abs p X . Because p abs is a constant, when the consequence is fixed, it can be omitted. For clarity, we present the proof for case a = 1 . The same result is achieved for a = 0 , when A and  X  A are reversed.
If m ( X ) = m ( XA ) , then p X ( X  X  A ) contains just one term, which is equal to its lower bound. Otherwise, p
X ( X  X  A ) is a sum of several terms, but it is enough to show that for the first term t 0 holds:  X 
This is true, because for all i = 0 , . . . , m ( X  X  A )  X  1 holds m ( X  X  A )  X  i , which was true.
 C. Pruning by minimality
Rule X  X  A = a with P ( A = a | X ) = 1 is called minimal , because none of its specializations XQ  X  A = a can achieve a better p F -value. However, this kind of minimal rules can be used to prune out other rules as insignificant or redundant, too. The following observation extends a well-known result (e.g. [11]) for p F and negative consequences.
Observation 2: If P ( A = a | X ) = 1 , then all rules of the form XQA  X  B = b (where B /  X  X , B /  X  Q , B 6 = A , b  X  X  0 , 1 } ) are either insignificant or redundant. rule XQA  X  B = b cannot be significant. If P ( A | X ) = 1 , then P ( XQAB = b ) = P ( XQB = b ) and P ( XQA ) = and rule XQA  X  B = b is redundant with respect to XQ  X  B = b .
 D. The Lapis philosophorum principle
The basic branch-and-bound search prunes possible con-sequences only in the subtrees of a given node (like in [6]). However, it is also possible to prune consequences in the parents nodes, and propagate the results to other subtrees. This requires that the node is processed before any children are generated for its parents, except the immediate parent. Fig. 1 shows an example. When we proceed level by level, from left to right, set ACD is processed before any children are created for its parent sets AD and CD . If set ACD is now removed (e.g. has a zero frequency, which means that rule AD  X  X  C was minimal), then C and  X  C become impossible consequences in the node for AD and its subtrees. Similarly, if we find in the node for ACD that no rule QCD  X  A (for any Q  X  R \{ A, C, D } ) could be significant and non-redundant, then A can be marked as an impossible consequence in the node for CD and its subtrees. This simple principle performs so effective pruning that it is called Lapis Philosophorum , the legendary Philosopher X  X  stone.

Principle 1 (Lapis Philosophorum): Let q XA be a node corresponding to set XA and q X a node corresponding to set X as shown in Fig. 2. If any of the following conditions holds in node q XA , consequence A = a can be marked as impossible in node q X and all nodes q XQ in its subtrees: (i) Node q XA does not exist (i.e. no consequence was (ii) Rule X  X  A = a is minimal. (iii) Rule XQ  X  A = a would be insignificant or redun-
The principle is based on the fact that all supersets XQA lie in the subtree under q XA and q XQ is needed only as a parent node for rules XQ  X  A = a . The same principle can be applied to any goodness measure M , but for increasing goodness measures (where high values are better), the lower bound should be replaced by an upper bound, threshold max p by min M , and the inequality signs should be reversed.
 The Kingfisher algorithm is given in Alg. 1, 2, 3, 4, and 5. For simplicity, we represent the algorithm for a decreasing goodness measure M (like p F ), but increasing goodness measures can be used as well, with the above mentioned adjustments.

In each node v = N ode ( X ) , we use the following fields:  X  v.set set X ; in practice, it is enough to store just the  X  v.children table of pointers to v  X  X  child nodes.  X  v.ppossible and v.npossible bit vectors, whose j th bits  X  v.pbest and v.nbest tables for the best M -values of
The main idea of the algorithm is the following: First (Alg. 1 line 1, Alg. 2 lines 2 X 3), Observation 1 is used to determine the maximal absolute frequency threshold min fr Alg. 1 Algorithm Kingfisher ( R, r, max M , K ) Alg. 2 Algorithm check1sets ( R, r, max M , min fr ) Alg. 3 Algorithm bfs ( st, l, len ) such that even the best possible rule X  X  A = a with m ( A = a ) = m ( X ) = m ( XA = a ) &lt; min fr cannot be significant. All attributes which cannot occur in significant rules are pruned out. This step is possible only with some goodness measures like p F satisfying the conditions of Alg. 4 Algorithm checknode ( v X )
Alg. 5 Auxiliary functions. Observation 1.
 are ordered into ascending order by frequency and added to the enumeration tree. For all A i , lower bound lb 2 from Theorem 2 is used to determine possible consequences
A j = a j such that XA i  X  A j = a j can be significant.
The possible consequences are marked into tables ppossible and npossible in node N ode ( A i ) . If A j is possible, then N ode ( A i ) .ppossible [ j ] = 1 , and if  X  A j is possible, then N ode ( A i ) .npossible [ j ] = 1 .
 sets as long as new non-redundant, significant rules can be found. Alg. 3 (lines 2 X 7) creates l -sets from the existing ( l  X  1) -sets. Each new node is checked in Alg. 4. If it becomes deleted, the information is propagated to all parent nodes by the Lapis Philosophorum principle (Alg. 3 lines 11 X 13).
Alg. 4 is the following: First (lines 2 X 5), the possible consequences in node N ode ( X ) are initialized, given pos-sible consequences in its parent nodes N ode ( Y m ) , where X = Y m A m for some A m  X  X . Consequence A j = a j , A j  X  R , is possible in N ode ( X ) only if it is possible in all parent nodes N ode ( Y m ) . The best M -values are also initialized for all consequences A j = a j , where A j  X  X , using the best -values in the parent sets.

Second (lines 7 X 12), frequency m ( X ) is calculated and minimality is checked. Lower bounds lb 2 and lb 3 (from Theorems 2 and 3) are used to decide, whether any rule ( XQ ) \{ A j } X  A j = a j can be a non-redundant, significant rule.

Third (lines 13 X 22), for all possible consequences A j = a , A j  X  X , the measure value M ( X  X  A j = a j ) is calculated. If it is sufficiently good (among the best K rules and better than more general rules with consequent A j = a ), it is added to the rule collection and the corresponding best -values are updated.

Fourth (lines 23 X 28), if minimal rules were found, the redundant consequences are marked as impossible. The Lapis Philosophorum principle is used to propagate the information to parents.

The goal of the experiments was to assess the quality of discovered rules and the efficiency of the new pruning property (Lapis philosophorum) and the whole search. For testing, we used classical benchmark data sets, which are available in the FIMI repository (http://fimi.cs.helsinki.fi/ data/).
 For comparison, we implemented also the  X  2 -measure to Kingfisher algorithm. We note that there are no previous algorithms for searching for both positive and negative dependency rules with the  X  2 -measure (or p F ). With some association rule implementations, it is possible to filter the rules afterwards using the  X  2 -measure, but the rules are always positive and contain a large number of redundant rules. In addition, this alternative is very inefficient, unless large minimum frequency thresholds are used. When the  X  -measure was used with the Kingfisher, we needed low minimum frequency thresholds with data sets Accidents, Pumsb, and Retail. The thresholds are reported in Table II. With Pumsb, we also restricted the search to level 7, because no new rules were found after that. We note that the thresholds for Accidents and Pumsb are still several magnitudes smaller than those needed for association rule algorithms. In other data sets, we used only the general requirement that all rules should occur on at least five rows of data. With p F , no thresholds were needed, because minimum frequency thresholds are derived implicitly from the maximal p F -value requirement.

In the quality evaluation, the objective is that the dis-covered dependency rules should hold also in the future data. To estimate the behaviour in the future data, we used the following cross-validation scheme: Each data set was divided 10 times randomly to a training set and a test set. Because each test set should also be sufficiently large, we used 2/3 of the data for training and 1/3 for testing. For each training set, we searched for the 100 best rules, and evaluated them in the test set. For assessing how well the dependency held in the test data, we calculated the mean squared error of lift ( M SE  X  ) and leverage ( M SE  X  ). If the discovered dependency rules were equally strong in the test set, then both M SE  X  and M SE  X  would be zero. On the other hand, large M SE values indicate that the dependencies were significantly weaker or stronger in the test sets. It is not necessarily harmful, if the dependency is stronger in the test set (future data), but in some applications, one may want to ascertain that the data does not contain any strong dependencies. Because the M SE values are difficult to interpret, we report the root mean squared errors, which are in the same scale as the lift and leverage.
 The results of quality evaluation are represented in Table III. In addition to p M SE  X  and average frequency, confidence, lift, and leverage in the training sets.

In all data sets, p F produced more frequent rules, with a significantly smaller lift but larger leverage than the  X  measure. The  X  2 -measure suffered for exaggerated values (due to extremely low or high frequency rules) in all sets except Chess and Retail. This was especially clear in Mush-room, T10I4D100K, and Accidents, were the lift values held poorly in the test sets. On the other hand, the dependencies by p F held extremely well in all test sets ( p M SE  X  was about 7% from the average lift and from the average leverage, while for the  X  2 -measure, the proportions were about 122% and 11%).

Negative rules were found among the 100 best rules in sets Mushroom (6.6% with p F and 6.0% with  X  2 ), Chess (61.6% and 36.0%), Accidents (42.8% and 29.2%), and Pumsb (63.3% and 18.8%). The  X  2 -measure found less and simpler negative rules (typically 2-rules), while p F produced also more complex rules. The reason is that the  X  2 -measure gets its maximal value, whenever m ( XA = a ) = m ( X ) = m ( A = a ) . This occurs more often for simple and relatively infrequent rules, while the negative rules were typically quite frequent.

In the efficiency evaluation, we compared three versions of the Kingfisher algorithm: 1) the original Kingfisher with p , 2) Kingfisher with p F but without the Lapis Philosopho-rum principle, and 3) Kingfisher with the  X  2 -measure. With each version, we searched for the 100 best rules from the whole data set. For p F , no minimum frequency thresholds were used, but the initial ln ( max p ) -values and correspond-ing (implicit) minimum frequency thresholds are reported. When the Lapis Philosophorum principle was not used, the program often got stuck, and was halted after 20 minutes CPU time. For the  X  2 -measure, we used the same minimum frequency thresholds as for the quality evaluation along with the maximal rule length 6 for Pumsb.
 All experiments were run on 2.5 GHz AMD Opteron 8360 SE having 256 GB central memory and Linux operating system. The large memory size was beneficial for the  X  2 -measure, but with p F all experiments could be run with just 1 GB main memory.

The results are represented in Table IV. The first columns characterize the size of the traversed search space in the terms of the enumeration tree, which was generated: the maximal level, the widest level, and the number of sets on the widest level. We recall that the enumeration tree is pruned after each level, and thus the widest level is the bottle-neck. The last column gives the execution time in seconds.

With the original Kingfisher, the whole search space could be traversed, and therefore the discovered rules were globally optimal. The most demanding data set was Retail, where the number of attributes is extremely large. In ad-dition, all dependencies are relatively weak, and therefore Kingfisher could not determine any effective minimum fre-quency from the maximal p F -value requirement. Most of the execution time was spent on level 2, where the program had to determine lower bounds for over 17 million attribute combinations.

The implicit minimum frequency thresholds explain the efficiency of Kingfisher only on the first levels. After that the Lapis Philosophorum principle begins to play a more important role. When the principle was not used, the program got stuck with data sets Chess, Accidents, and Pumsb. The sparsest data sets could be handled without Lapis Philosophorum, but the enumeration tree was still significantly larger. In the densest data sets, the widest level was at least 15 X 1570 times as large as with Lapis Philosophorum principle, and none of these experiments could have been run in an ordinary desktop computer.
Chess, Accidents, Pumsb, and Retail were the most de-manding data sets for the  X  2 -measure, despite of minimum frequency thresholds. The problem is that the minimal  X  2 value requirement does not define any minimum frequency threshold, but any rule  X  even one occurring on just one row  X  can gain the maximal value. Therefore, the upper bounds used for pruning tend to be large, and the search continues deep.

Searching for statistical dependencies from high-dimensional data is a fundamental problem in all empirical science. The problem is computationally very demanding, and so far there have not been any scalable solutions to the general problem. Heuristic or sub-optimal solutions are often insufficient, because the new scientific theories should be based on valid information. In practice, the scientists want to find the most significant dependencies which hold also in the future data. For this purpose we need an ef-ficient search algorithm, which optimizes some statistical significance measure like Fisher X  X  p -value, without any other restrictions like minimum frequency requirements. Pruning redundant dependencies is also important, because scientists want to find the real causes of dependencies, without any weakening or occasional extra factors.

In this paper, we have introduced an effective solution for a special case, where dependency rules are searched for from binary data. The rules can be of the form X  X  A , X  X  X  A ,  X  X  X  A , or  X  X  X  X  A , where X is a set of positive-valued attributes, and A can be any attribute. The new Kingfisher algorithm can be used to search for either the K best, non-redundant dependency rules, or to enumerate all sufficiently good, non-redundant rules. The search algorithm itself is applicable to any statistical goodness measure, given the required upper or lower bounds, but in this paper, we have focused on the search with Fisher X  X  exact test. For this purpose, we have introduced new, tight lower bounds for the corresponding p F -value. In addition, we have in-troduced several general pruning principles, which enable to restrict the search into areas, where the most significant non-redundant dependencies are to be found.

According to our experiments, the algorithm is extremely well-scalable, even to densest and largest-dimensional data sets, when p F is used as a search criterion. No minimum frequency thresholds or other restrictions were required. Surprisingly, the  X  2 -measure turned out to be much less effective, and small minimum frequency thresholds were needed with the most demanding data sets. Another sur-prising result was the relatively poor quality of rules, when the  X  2 -measure was used. With p F , the results were very accurate, and we can conclude that Kingfisher offers a robust and efficient tool for scientists.

