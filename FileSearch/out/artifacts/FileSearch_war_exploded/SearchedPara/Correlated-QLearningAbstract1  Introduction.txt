 Amy Greenwald amy@cs.brown.edu Keith Hall kh@cs.brown.edu Recently, there have been several attempts to design a multiagent learning algorithm that learns equilib-rium policies in general-sum Markov games, just as Q -learning converges to optimal policies in Markov decision processes. Hu and Wellman [8] propose an algorithm called Nash-Q that converges to Nash equilibrium policies under certain (restrictive) con-ditions. Littman X  X  [11] friend-or-foe-Q (FF-Q ) algo-rithm always converges, but it only learns equilib-rium policies in restricted classes of games: e.g. , two-player, constant-sum Markov games, which exhibit minimax equilibria (foe-Q ); e.g. , coordination games with uniquely-valued equilibria (friend-Q ). This paper introduces Correlated-Q (CE-Q ) learning, a multiagent Q -learning algorithm based on the cor-related equilibrium solution concept [1]. CE-Q gener-alizes both Nash-Q and FF-Q : in general-sum games, the set of correlated equilibria contains the set of Nash (and thus, coordination) equilibria; in constant-sum games, where Nash and minimax equilibria coincide, the set of correlated equilibria contains the set of min-imax equilibria.
 A Nash equilibrium (NE) is a vector of independent probability distributions over actions, in which all agents optimize with respect to one another X  X  prob-abilities. A correlated equilibrium (CE) is more gen-eral than a NE, since it permits dependencies among the agents X  probability distributions, while maintain-ing the property that agents are optimizing. An ev-eryday example of a correlated equilibrium is a traffic signal. For two agents that meet at an intersection, the traffic signal translates into the joint probability distri-bution ( stop , go ) with probability 0.5 and ( go , stop ) with probability 0.5. No probability mass is assigned to ( go , go ) or ( stop , stop ). Note that it is optimal for agents to obey their respective traffic signals. The set of CE is a convex polytope; thus, unlike Nash equilibria (NE), CE can be computed easily via linear programming. Also, CE that are not NE can achieve higher rewards than NE, by avoiding positive proba-bility mass on less desirable outcomes, unlike mixed strategy Nash equilibria. Also unlike NE, to which no learning algorithm is known to converge in general, no-regret algorithms ( e.g. , Foster and Vohra [2]) converge to CE in repeated games.
 One of the difficulties in learning (Nash or correlated) equilibrium policies in general-sum Markov games stems from the fact that in general-sum games, there exist multiple equilibria with multiple payoff values. We attempt to resolve this equilibrium selection prob-lem by introducing four variants of CE-Q , based on four equilibrium selection functions. We define utili-tarian, egalitarian, republican, and libertarian CE-Q learning. This paper demonstrates empirical conver-gence to equilibrium policies for all four CE-Q variants on a testbed of Markov games. We also discuss the the-ory of stochastic stability, which could be employed to describe the convergence properties of our algorithms. Stochastic games generalize repeated games and Markov decision processes (MDPs). A stochastic game I is a set of n players, S is a set of states, A i ( s ) is the i th player X  X  set of actions at state s , P is a prob-ability transition function that describes state tran-sitions, conditioned on past states and joint actions, and R i ( s, ~a ) is the i th player X  X  reward for state s  X  S and joint actions ~a  X  A ( s ) = A 1 ( s )  X  . . .  X  A n ( s ). Stochastic games for which the probability transitions satisfy the Markov property are called Markov games : i.e. , for ~a t = ( a 1 , . . . , a n ) t , P [ s t +1 | s An MDP is a one-player Markov game. Recall Bell-man X  X  equations that characterize the optimal state-and action-values for a single agent and an MDP:
Q  X  ( s, a ) = (1  X   X  ) R ( s, a ) +  X  X or 0  X   X  &lt; 1. In words, the value Q  X  ( s, a ) is the normalized sum of the immediate reward obtained at state s for taking action a and the discounted expected value of the future rewards obtained by following the optimal policy thereafter. The value function V  X  ( s ) at state s is defined as the value that maximizes Q  X  ( s, a ) over all actions a . The actions that maximize Q  X  ( s, a ) at each state s describe the (deterministic) optimal policy  X   X  : i.e. , In Markov games, player i  X  X  Q -values are defined over states and action-vectors ~a = ( a 1 , . . . , a n ), rather than state-action pairs:
Q i ( s, ~a ) = (1  X   X  ) R i ( s, ~a ) +  X  X Intuitively, the notion of state-value function also car-ries over from MDPs to Markov games. But the obvi-ous analogue of Eq. 3, in which all players maximize their respective rewards with respect to one another X  X  actions is not adequate, since (deterministic) actions that satisfy these simultaneous equations need not ex-ist. (As a consequence, Markov games need not exhibit deterministic equilibrium policies: e.g. , Rochambeau  X  Rock-paper-scissors.) Several alternative definitions of the value function have been proposed. Littman [10] studied two-player, zero-sum Markov games and von Neumann X  X  minimax value function [14]. Let  X  i ( s ) be the probabilistic ac-tion space of player i at state s . Now
V 1 ( s ) = max where Q ( s,  X  1 , a 2 ) = P a the opposite extreme, Littman X  X  friend-Q [11] value function is suited to coordination games X  X ames for which all the players X  reward functions are equivalent X  X ith uniquely-valued equilibria: For the general case of n -player, general-sum games, Hu and Wellman [8] proposed the following definition of the value function: nash i ( X 1 , . . . , X n ) denotes the i th player X  X  reward according to some Nash equilibrium in the general-sum game determined by reward matrices X 1 , . . . , X n . Note that existence of such values, which is implied by Nash X  X  theorem [13] relies on probabilistic actions. This definition generalizes the minimax value function, since Nash equilibria and minimax strategies coincide in zero-sum games. But this value function need not be well-defined: in general, the set of Nash equilibria need not be a singleton.
 We propose an alternative definition of the value func-tion in Markov games: where ce i ( X 1 , . . . , X n ) denotes the i th player X  X  re-ward according to some correlated equilibrium in the general-sum game determined by the rewards X 1 , . . . , X n . Eq. 8 generalizes Eq. 7, since a Nash equilibrium is a correlated equilibrium that can be factored into independent distributions over each in-dividual player X  X  action space. Thus, equilibria that are consistent with Eq. 8 exist, but this value func-tion, too, need not be well-defined.
 For each choice of value function, it is necessary to establish the existence of Q -values that support equi-librium policies defined by the value function. In par-ticular, we seek a solution to the system of equations given by Eq. 4 and either Eq. 5, 6, 7, or 8: i.e. , a set of action-values Q  X  , and corresponding state-values Greenwald [5], using Kakutani X  X  and Brouwer X  X  fixed point theorems. Most of these results were known pre-viously, but new and direct proofs of these four results are presented in this recent work. 2.1. Correlated Equilibrium A Nash equilibrium (NE) is a vector of independent probability distributions over actions, in which all agents optimize with respect to one another X  X  probabil-ities. A correlated equilibrium (CE) allows for the pos-sibility of dependencies in the agents X  randomizations: a CE is a probability distribution over the joint space of actions, in which all agents optimize with respect to one another X  X  probabilities, conditioned on their own. In contrast to Nash equilibria, for which no efficient method of computation is known, correlated equilibria can be computed easily via linear programming. As an example, consider  X  X hicken X  a two-player, two-action, one-shot, general-sum game.
 The correlated equilibria in this game are described by the probability constraints  X  T L +  X  T R +  X  BL +  X  BR = 1 and  X  T L ,  X  T R ,  X  BL ,  X  BR  X  0 together with the follow-ing rationality constraints: These constraints have a natural interpretation in terms of conditional probabilities. Let  X  ( y | x ) de-note the conditional probability of y given x , and let  X  ( y ) = P x  X  ( y | x ) denote the marginal probability of Thus, the first constraint, which describes row X  X  re-wards, can be restated as  X  1  X  ( L | T )+2  X  ( R | T )  X  0, or Intuitively, the expected reward to the row player of action T is at least that of action B whenever he in fact plays action T . The other three constraints can be interpreted analogously. In principle, the generalization of dynamic program-ming and reinforcement learning from MDPs to Markov games is straightforward. A template for multiagent Q -learning, is presented in Table 1. In this generic formulation, the algorithm takes as in-put an equilibrium selection function f , which com-putes the value function V , given matrix-vector ~ Q = ( Q 1 , . . . , Q n ). Littman X  X  FF-Q algorithm computes V according to either Eq. 5 or Eq. 6, as appropriate. Hu and Wellman X  X  Nash-Q algorithm computes V accord-ing to Eq. 7. Correlated-Q computes V via Eq. 8. multiQ (MarkovGame , f,  X ,  X , S, T ) for t = 1 to T 3.1. CE-Q The difficulty in learning equilibria in Markov games stems from the equilibrium selection problem: how can multiple agents select among multiple equilibria? We introduce four variants of correlated-Q learning, based on four correlated equilibrium selection mechanisms. Each variant of CE-Q learning resolves the equilibrium selection problem with its respective choice of objec-tive function, which ensures that the equilibrium value of a game is unique, albeit not the equilibrium policy. 1. maximize the sum of the players X  rewards: 2. maximize the minimum of the players X  rewards: 3. maximize the maximum of the players X  rewards: 4. maximize the maximum of each individual player i  X  X  rewards: let  X  = Q i  X  i , where Thus, ce i ( ~ Q ( s )) = P ~a  X  A  X  ( ~a ) Q i ( s, ~a ) , where  X  satisfies either Eq. 9, 10, 11, or 12.
 We refer to these algorithms as utilitarian ( u CE-Q ), egalitarian ( e CE-Q ), republican ( r CE-Q ), and libertar-ian ( l CE-Q ) correlated Q -learning, respectively. Note that all these equilibria can be computed via linear programming by incorporating the objective function of choice into the linear programming formulation ( i.e. , the probability and rationality constraints) described in Sec. 2.1. Note also, the implementation of all four selection functions necessitates the sharing of Q -tables among agents. Hu and Wellman [8] resolve this issue by allowing all agents to observe all other agents X  ac-tions and rewards; thus, one agent can simulate an-other X  X  update procedure, thereby maintaining a copy of the other X  X  Q -table. The first set of detailed experimental results on which we report pertain to grid games [9]. We describe three grid games: grid game 1 (GG1), a multi-state coordi-nation game; grid game 2 (GG2), a stochastic version of Battle of the Sexes; and grid game 3 (GG3), a multi-state version of Chicken. In the following section, we describe experiments with grid soccer, a constant-sum Markov game that extends Matching Pennies.
 Fig. 1 depicts the initial states of the three grid games. In GG1, there are two agents and two goals. The agents X  action sets include one step in any of the four compass directions. Actions are executed simultane-ously. If both agents attempt to move into the same cell, they cannot; instead, they both lose 1 point. If ever an agent reaches its goal, it scores 100 points, and the game ends. Note that it is possible for both agents to score 100 points since actions are simultaneous. Other than the board setup, GG2 is identical to GG1. In GG2, there are two agents, one goal, and two bar-riers: if an agent attempts to move through one of the barriers, then with probability 1 / 2 this move fails. In GG3, like GG2 there is one goal, but there are no prob-abilistic transitions, and the reward structure differs: if both agents enter the goal from the side, they both earn 120; but, if one agent enters the goal through the center, while the other enters through the side, the former earns 125, while the latter earns only 100. In all three of these grid games there exist determin-istic Nash equilibrium policies for both agents. In GG1, there are several pairs of deterministic equilib-rium policies in which the agents coordinate their be-havior all of which yield equivalent rewards. In GG2, there are exactly two deterministic equilibrium poli-cies: one agent moves up the center and the other attempts to pass through the barrier, and the same again with the agents X  roles reversed. Note that these equilibria are asymmetric: the agent that moves up the center scores 100, but the agent that attempts to move through the barrier scores only 50 on average. The deterministic equilibrium policies of GG2 carry over to GG3.
 In addition, all the grid games exhibit nondetermin-istic correlated (and Nash) equilibrium policies. In GG2, there exists a continuum of symmetric, nonde-terministic, correlated equilibrium policies: i.e. , for all p  X  [0 , 1], with probability p one agent moves up the center and the other attempts to pass through the barrier, and with probability 1  X  p the agents X  roles are reversed. In GG3, there exist symmetric, non-deterministic, correlated equilibrium policies in which both agents move up the sides with high probability and each of the deterministic equilibria is played with equally low probability.
 Our experiments reveal that off-policy correlated-Q , foe-Q , friend-Q (  X   X  0 . 001 and  X  = 0 . 9.) and on-policy Q -learning ( i.e. , -greedy, with  X  0 . 001,  X   X  0 . 001, and  X  = 0 . 9) all converge empirically in the three grid games. Littman [11] proves that FF-Q converges in general-sum Markov games. Fig. 2 shows that in fact ordinary Q -learning (on-policy) and correlated-Q (off-policy) also converge in these games. The values plotted in Fig. 2 are computed as follows. The error err t i at time t for agent i is the difference between Q ( s, ~a ) at time t and Q ( s, ~a ) at time t  X  1: axis represent time, and the corresponding y -values are all t = 0 , . . . , x . This mean is converging to zero for all algorithms in all grid games. 1 4.1. Equilibrium Policies Since all the learning algorithms converge, the follow-ing question arises: to what equilibrium policies do these algorithms converge? Since the state space in the grid games is large, rather than enumerate the equilib-rium policies, we addressed this question by append-ing to the training phase an auxiliary testing phase in which the agents play according to the learned policies. Our results are depicted in Table 2.
 On-policy Q -learning is successful in grid games: it consistently converges to equilibrium policies in which the two agents coordinate their behavior perfectly. In GG1, this leads to symmetric scores, but in GG2 and ,  X  10 4 0  X  10 4 ,  X  10 4 0 GG3 their policies and their scores are asymmetric. Foe-Q learners perform poorly in GG1. Rather than progress toward the goal, they cower in the corners, avoiding collisions, and avoiding the goal. In GG2 and GG3, the principle of avoiding collisions leads both foe-Q learners straight up the sides of the grid. Al-though these policies yield reasonable scores in GG2, and Pareto optimal scores in GG3, these are not equi-librium policies. On the contrary, both agents have an incentive to deviate to the center, since the reward for using the center passage exceeds that of moving up the sides, given that one X  X  opponent moves up the side. In GG1, friend-Q learning can perform even worse than foe-Q learning. This result may appear surpris-ing at first glance, since GG1 satisfies the conditions under which friend-Q is guaranteed to converge to equilibrium values. Indeed, friend-Q learns Q -values that support equilibrium policies, but friends lack the ability to coordinate their play. Whenever one friend chooses a policy that collides with the policy of its so-called friend, both agents obtain negative scores in one never-ending game. In GG2 and GG3, friend-Q  X  X  per-formance is always poor: both friends learn to play the equilibrium policy that uses the center passage, which causes friends to collide repeatedly. 4.2. CE-Q Learning In GG1, u CE-Q , e CE-Q, and r CE-Q all learn Q -values that coincide exactly with those of friend-Q : i.e. , Q -values that support equilibrium policies. But unlike friend-Q , these variants of CE-Q always obtain posi-tive scores. In our implementation of CE-Q learning, a centralized mechanism computes a correlated equi-librium. Thus, CE-Q play is always coordinated, and u CE-Q , e CE-Q , and r CE-Q learners do not collide while playing the grid games. Were we to implement a decentralized version of CE-Q , such learners could fail to coordinate and earn negative scores.
 The libertarian operator is one way to eliminate CE-Q  X  X  dependence on a centralized mechanism. In l CE-Q , each agent solves an independent optimization problem during learning; thus, play is not necessarily coordinated. Like the other variants of CE-Q , l CE-Q converges, and its Q -values coincide exactly with those of friend-Q in GG1. Also like the other variants of CE-Q , but unlike friend-Q , l CE-Q achieved positive scores in GG1. In fact, l CE-Q learners are indifferent between multiple equilibrium policies, but in this test run both agents happened upon coordinated equilib-rium policies.
 In GG2, all variants of CE-Q learning converge to poli-cies much like ordinary Q -learners. Interestingly, tak-ing long-run rewards into account, this game does not retain its Battle of the Sexes-like structure. On the contrary, GG2 is a dominance-solvable game. The Q -table below depicts the Q -values in the initial state that were learned by u CE-Q . (The other algorithms learned similar, although possibly transposed, values.) The column player eliminates the strategy side , since it is dominated, after which the row player eliminates the strategy center . Thus, the equilibrium outcome is ( side , center ), as the scores indicate. In both GG1 and GG2, CE-Q learning is indifferent between all correlated equilibrium policies, determin-istic and nondeterministic, since they all yield equal sums of rewards. In GG3, however, u CE-Q (and e CE-Q ) learn the particular nondeterministic correlated equilibrium policies that yield symmetric scores, be-cause the sum (and the minimum) of rewards at this equilibrium exceeds that of any deterministic equilib-rium policy. Consequently, the sum of the scores of u CE-Q and e CE-Q exceed that of Q -learning. CE-Q  X  X  rewards do not exceed the sum of the foe-Q learners X  scores, however; but foe-Q learners do not behave ra-tionally. In contrast, r CE-Q converges to a pure strat-egy equilibrium policy that is among those policies that maximize the maximum of all agents X  rewards. Finally, each l CE-Q agent attempts to play the equi-librium policy that maximizes its own rewards, but this yields repeated collisions and negative scores. Like Nash-Q , correlated-Q learning generalizes friend-Q , since it converges to precisely the same Q -values as friend-Q in games where friend-Q learns equilibrium values. In the next section, we show that again like Nash-Q , correlated-Q learning also generalizes foe-Q . The grid games are general-sum games for which there exist deterministic equilibria. In this section, we con-sider soccer [10], a zero-sum game for which there do not exist deterministic equilibrium policies. The soccer field is a grid. The circle represents the ball. There are two players, whose possible actions are N, S, E, W, and stick. The players X  actions are executed in random order. If this sequence of actions causes the players to collide, then only the first moves. But if the player with the ball moves second , then the ball changes possession. 2 If the player with the ball moves into a goal, then he scores +100 if it is in fact his own goal and the other player scores  X  100, or he scores  X  100 if it is the other player X  X  goal and the other player scores +100. In either case, the game ends. In this simple soccer game, there do not exist deter-ministic equilibrium policies, since at some states there do not exist deterministic equilibria. For example, at the state depicted in Fig. 4 (hereafter, state s ), any deterministic policy for player B is subject to indef-inite blocking by player A . But if player B employs a nondeterministic policy, then player B can hope to pass player A on his next move.
 We experimented with the same set of algorithms in this soccer game as we did in the grid games. Con-sistent with the theory, FF-Q converges at all state-action pairs. All variants of correlated-Q also converge everywhere X  X n this game, all equilibria at all states have equivalent values; thus, all CE-Q operators yield identical outcomes. Moreover, CE-Q learns Q -values (and policies) that coincide exactly with those of foe-Q . But Q -learning does not converge.
 Fig. 3 presents an example of a state-action pair at which Q -learning does not converge. The values on the x -axis represent time, and the corresponding y -values The error values shown in Figs. 3(a), (b), and (c) re-flect player A  X  X  Q -values corresponding to state s , with player A taking action S and player B sticking. These three graphs depict converging sequences of error val-ues for u CE-Q , foe-Q , and friend-Q , respectively, Q -learners compute Q -values for each of their own pos-sible actions, ignoring their opponents X  actions. The error values shown in Fig. 3(d) reflect player A  X  X  Q -values, corresponding to state s and action S. In this figure, although the Q -value differences are decreasing, they are not converging. They are decreasing only be-cause the learning rate  X   X  0 . 001. At all times, the amplitude of the oscillations in error values is as great as the envelope of the learning rate.
 At state s , CE-Q and foe-Q converge to nondetermin-istic policies for both players, where each one random-izes between sticking and heading south.
 Friend-Q , however, converges to a deterministic policy for player B at state s , namely E. Learning accord-ing to friend-Q , player B (fallaciously) anticipates the following sequence of events: player A sticks at state s , and then player A takes action E. Thus, by taking action E, player B passes the ball to player A , with the intent that player A score for him. Player A is in-different among her actions, since she assumes player B plans to score a goal for her immediately.
 In this soccer game, Q -learning does not converge. In-tuitively, the rationale for this outcome is clear: Q -learning seeks deterministic optimal policies, but in this game no such policies exist. Friend-Q converges but its policies are irrational. Correlated-Q learn-ing, however, converges to the same solution as foe-Q learning X  X he Q -values learned by the two algorithms are identical. Thus, CE-Q learns minimax equilibrium policies in this two-player, zero-sum game. In MDPs, Q -learning has remarkable properties of global convergence: it provably converges to an op-timal policy, from any initial condition. Similarly, if one applies multiagent Q -learning to a two-player constant-sum Markov game, it continues to yield global convergence to the unique equilibrium value of the game [12]. However, beyond this class of games, the strong convergence properties of Q -learning cease to hold. Although Hu and Wellman [8] have iden-tified sufficient conditions for convergence of Nash-Q learning, their theorems are of limited applicability be-cause the conditions are extremely demanding. Fur-thermore, generalizing the solution concept from Nash equilibrium to allow for correlation does not necessar-ily help in this respect. Although this paper, like Hu and Wellman X  X  work, provides some empirical evidence for convergence, we, too, offer no general result. These difficulties with multiagent Q -learning in general-sum Markov games are to be expected because there are often multiple equilibria in such games, which renders the Q -learning dynamics non-ergodic. Thus, Q -learning converges to a collection of Q -values (and an equilibrium policy) that depend on initial condi-tions (or it may even exhibit lack of convergence, al-though we have not observed this outcome). If we add noise to the Q -learning system so that at each itera-tion the agents play an equilibrium with high probabil-ity, but with low probability they choose their actions arbitrarily, we arrive at the notion of stochastic sta-bility [3]. The system with noise is ergodic: it has a unique stationary distribution, which gives a pre-cise estimate of the proportion of time that the sys-tem spends at each collection of Q -values (and at each equilibrium policy) in the long run. An equilibrium is stochastically stable if Q -learning with noise picks it up with positive probability in the limit, as the amount of noise go to zero. In future work, we intend to char-acterize the stochastically stable states of this ergodic Q -learning system. The goal of this line of research is to improve the design of multiagent systems (MAS). At one extreme, MAS designers act as central planners, equipping all agents in the system with pre-specified behaviors; but such systems are rarely compatible with agents X  incentives. At the other extreme, MAS designers allow the agents to specify their own behavior; but these systems are susceptible to miscoordination. A MAS design based on the correlated equilibrium solution concept would (perhaps) rely on a central planner, but would pre-specify rational agent behavior. Such a design would not only facilitate multiagent coordination, but could generate greater rewards than any MAS design based on the Nash equilibrium solution concept.
 In this paper, we discussed algorithms for learning Q -values in Markov games, given a game-theoretic so-lution concept. In past work, we have studied al-gorithms for learning game-theoretic equilibria in re-peated games [7]. In ongoing work, we are combining these two types of learning. Specifically, we are re-placing the linear programming call in CE-Q learning with an adaptive procedure that converges to corre-lated equilibrium [2, 6]. Similarly, we are studying an adaptive version of minimax-Q that replaces its lin-ear programming call with an adaptive procedure that converges to minimax equilibrium [4]. (No adaptive algorithm is known to converge to Nash equilibrium.) This adaptive approach could lead to decentralized al-gorithms that learn correlated equilibrium policies in Markov games. The results contained in the present paper serve as the foundation for ongoing research, in which agents simultaneously learn Q -values and game-theoretic equilibria.

