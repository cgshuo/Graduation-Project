 MapReduce is an efficient programming model from Google for large-scale data processing in domains such as search engine, data mining and machine learn-ing. MapReduce is extensively used to process the join operation for large-scale dataset, and various join algorithms have been proposed to implement join oper-ation in MapReduce environment [ 3 ].
 mance degradation when handling skewed data , because they use hash parti-tioning to distribute data that can lead to partitioning skew. Partitioning skew will bring some problems. On one hand, join algorithms have to take longer time to deal with load imbalance caused by partitioning skew. On the other hand, large amounts of intermediate results have to be moved from mappers to reduc-ers over the network, thus introducing extra network overhead. As a result, join processing upon skewed data consumes more time.
 Some methods such as SAND [ 2 ] and LEEN [ 2 ], have been proposed to solve the problem of data skew in MapReduce-based join, which adopt parti-tion schemes considering the key X  X  frequency distribution. However, SAND does not take into account the network overhead. LEEN not only solves load imbal-ance but also reduces network transmission, however, it changes the internal implementation scheme of Hadoop and ignores the advantage of overlapping [ 1 ] between the shuffle and map phases.
 To overcome the above deficiency, we proposed SALA (Skew-avoiding and Locality-aware) MapReduce-based join algorithm, which uses volume/locality-aware partitioning to distribute data and does not make any modification of the MapReduce framework. Our approach firstly obtains the distribution infor-mation of key X  X  frequency and location through data sampling. Based on this distribution information, SALA is able to guarantee the uniform distribution of data even when skewed data exist, so as to effectively avoid partitioning skew. At the same time, SALA reduces the amount of data transferred over network by utilizing the data locality feature of MapReduce, i.e., assigning keys to the nodes on which most of the intermediate results are located. This significantly improves the efficiency of the whole join operation.
 In summary, we make the following major contributions:  X  A novel algorithm called SALA is proposed to handle skewed data in
MapReduce-based join. It not only achieves better load balance but reduces shuffled data over the network, thus resulting in significantly performance improvement.  X  Volume/locality-aware partitioning scheme is proposed to distribute data, which is easy to implement without any modification of the MapReduce framework.  X  We carry out extensive experiments and the results show the efficiency of SALA in the presence of data skew.
 The rest of this paper is organized as follows. Sect. 2 briefly introduces MapReduce-based join, and then we investigate the problem of data skew in Sect. 3 . Sect. 4 presents the detail of the SALA. Extensive experimental results are reported in Sect. 5 . Related work is reviewed in Sect. 6 . Finally, we conclude the paper and discuss our future work in Sect. 7 . MapReduce-based join algorithms can be classified into two categories: map-side join and reduce-side join. For map-side join, the smaller input dataset is placed on each mapper and join operation only needs to be executed in the map phase to get the final results. Instead, a reduce-side join is carried out on the reduce phase. First, the map function takes the input dataset from DFS(Distributed File System), and generates key-value pairs with the form of (Key, Value) as interme-diate results, wherein Key represents join attribute. These intermediate results are to be assigned to reducers using hash partitioning. Second, in the shuffle phase, the reducers are notified to pull partitions across the network. Finally, the reduce function performs join operation with (Key, list(Value)) pairs, wherein list(Value) is a list of values associated with the given key Key , and writes the final results to DFS. Fig. 1 shows the process of a join operation between the dataset R and S with a typical reduce-side join algorithm, which is called repar-tition join[ 3 ].
 because they produce the final results in map phase without shuffling data across the network. However, they can be used only in particular circumstances, i.e., one of the input datasets must be small enough to be buffered in memory of nodes. Reduce-side join algorithms are commonly used because they have fewer restrictions on input datasets. Therefore we focuse on the problem of data skew in reduce-side join. MapReduce-based join algorithms sometimes suffer performance degradation from partitioning skew and heavy network overhead. 3.1 Partitioning Skew Traditional join algorithms use hash partitioning to distribute data. Hash par-titioning, the default partitioning function used in MapReduce model, is based on key hashing: hash(Key) mod R , wherein R is the number of reducers, which can allow data to be distributed uniformly when there are no skewness in the input datasets. In practice, however, partitioning skew tends to occur in process-ing skewed data and cause load imbalance, which means large amounts of data are distributed on only a few nodes. Because the larger the volume of partition is, the longer time it takes to process data. In addition, the response time of a MapReduce job is dominated by the slowest reduce instance. So partitioning skew results in longer response time of MapReduce-based join on the whole. According to the process of repartition join shown in Fig. 1 , to which node the intermediate results will be distributed, is determined by the partitioning function. Therefore, the key factor to achieve load balance in MapReduce-based join operation lies in whether or not the partitioning function can guarantee uniform distribution of data. 3.2 Heavy Network Overhead Apart from partitioning skew, network overhead is another non-negligible prob-lem. Large amounts of intermediate results are produced and need to be trans-ferred across the nodes through network, which may consume a lot of network resources and result in longer execution time. For Hadoop, it runs mappers on those machines where splits of input datasets are located, so as to avoid net-work overhead. Most existing MapReduce-based join algorithms, however, does not take full advantage of such data locality feature in the reduce phase, and as a result, lots of intermediate results have to be transferred over network dur-ing the shuffle phase. In addition, transmission for skewded partitions may also introduce extra network overhead, because they have more data to transfer than non-skewed partitions. What X  X  more, the reduce phase only can start after the shuffle phase completes, so network overhead is to increase the response time of the whole join operation.
 Therefore, with evenly distribution of partitions, data transmission time tends to be equal among various partitions. In addition, applying the data local-ity feature in the reduce phase, can also reduce the amount of the transferred data and further improve the performance of join operation. To solve the above problem, we propose SALA join algorithm to handle skewed data and reduce network overhead. In this section, we first present an overview of SALA, and then present the volume/locality-aware partitioning used in SALA in detail. Also a example is discussed to compare SALA join with repartition join. Finally, we propose a cost model to analyze the performance of our algorithm. 4.1 Overview We propose SALA join algorithm to handle skewed data. The core idea of SALA join is to distribute intermediate results based on the distribution information of key X  X  frequency and location. With volume/locality-aware partitioning scheme, SALA join is able to not only handle skewed data but also reduce network overhead.
 process with SALA is shown in Fig. 2 . The main difference between the two algorithms is that SALA adds an additional MapReduce job to obtain key X  X  distribution information. The SALA join includes two phases: 1. Phase 1: sample the input dataset and pre-compute the data to get the 2. Phase 2: perform the actually join operation. The join process is similar with on phase 1, i.e., the pre-partitioning process. 4.2 The Pre-partitioning Process The pre-partitioning process is to pre-compute the sample input dataset to get K-P . and includes three phases -map phase, combine phase and reduce phase:  X  Map phase: process the sample input dataset and take the join attribute as  X  Combine phase: the combine task will count the frequency of each key on the current node and the output will be (Key, (node, sum)) , i.e., outputting the total frequency of each Key on the node.  X  Reduce phase: volume/locality-aware partitioning is employed to get the pre-partitioning result (Key, Partition) , i.e., K-P .
 Volume/locality-aware partitioning plays an important role in SALA join, so we discuss it in more detail below. 4.3 Volume/Locality-Aware Partitioning Volume/locality-aware partitioning can not only deal with partitioning skew to achieve load balance, but also reduce the data transferred over network. Assum-ing that the data volume is M (which can be represented by the rows of the input dataset) and the number of nodes is N . In order to achieve load balance, the volume of data distributed to each node should be close to data transferred over network, volme/locality-aware partitioning makes full use of data locality feature by adopting greedy selection strategy as follows: 1. Each key value is distributed in higher priority to the node on which most intermediate results of this key are located. 2. First process the key value which has larger size of intermediate results. Volume/locality-aware partitioning involves the following two steps: 1. Preparing step: (a) Compute the total rows of intermediate results of each key value in all (b) Extract all (Key, node, sum) tuples from ( Key,list ( 2. Partitioning step: (a) Traverse list L and process each tuple (Key, node, sum) .Weuse (b) Lastly, there may be some key values in K which have not been parti-Algorithm 1 in Fig. 3 formally describes volume/locality-aware partitioning. Due to the random sampling method used in the pre-partitioning process, there are some key values which may not be counted in. Therefore, when the interme-diate results are partitioned in the perform-join process, key values which have been counted in will be partitioned according to the K-P , while key values which have not been counted in will still be partitioned by hash partitioning. Given that key values which have not been counted in only involve a small part of all key values, they will have negligible impact on the data distribution. 4.4 Example Taking the following join operation for example: R there are 3 nodes in the cluster and the input data volume of each node is the same, i.e., 70, but with skewed data. Fig. 4(a) shows the intermediate results produced in the map phase, in which, each row represents one key group (Key, volume) , wherein volume is the data volume of this key value on the present node.
 Fig. 4(b) and Fig. 4(c) respectively. According to Fig. 4(b) , partitioning skew hap-pens in repartition join. Too much data are distributed to times of that distributed to Node 1 . Therefore, load imbalance appears. However, as Fig. 4(c) shows, SALA join algorithm has achieved better load balancing, and at the same time, the overall network overhead has reduced by 36% compared with repartition join. With SALA join algorithm, the volume of data distributed to each node will tend to be equal and load balance is therefore achieved. Further, because each key value is first distributed to the node on which most of its intermediate results are located, the overall volume of data to be transferred over network is remarkably reduced and the performance of join operation is improved. 4.5 Cost Model As shown in Fig. 1 , the whole processing time of the traditional reduce-side join algorithm includes three parts: processing time of map phase, transmission time of shuffle phase and processing time of reduce phase. For convenience, we use the following notations in Table 1 : Because the response time of a MapReduce job is determined by the slowest reduce instance, we can estimate the response time by the reducer which is allocated the most volume of data, represented as R . Therefore, the cost model for a traditional reduce-side join algorithm is as follows: tioning skew, however, R s tends to be: of key value k on nodes. With hash partitioning, the data locality rithm in the case of partitioning skew can further be written as: join guarantees the uniform distribution of data, but needs an additional pre-partitioning process, and the required time of pre-partitioning process is repre-sented as T pre . Therefore the cost model for SALA join is: rithm when satisfying the following condition:
T of time results in from avoiding solving the partitioning skew is greater than the time used to process pre-partitioning. We can therefore employ Eq.( 5 )in optimal query plan selection. Here, according to many experiments, to be 0 . 23  X  t m  X  M N and t r tends to be 0 . 69  X  t m in Eq.( 6 ). Also with the case of greater data skewness and the lower available bandwidth, SALA join will performs much better.
 In this section, we conduct experiments to verify the efficiency of our approach. We mainly use the response time of join operation to demonstrate performance difference in the case of data skew. We compare SALA join with the reparti-tion join algorithm [ 3 ] and SAND join algorithm [ 2 ], because repartition join is extensively used, and SAND join is a typical join algorithm to deal with skewed data. 5.1 Environmental Setup Our experiments run on AliCloud (Alibaba Cloud Computing) with a 6-node cluster running native Hadoop 2.4.1, where there are 1 master node scheduling the task and 5 slave nodes taking charge of both storage and computation. Each node has two Xeon 2.3Ghz CPUs, 4GB memory and 60GB disk drive. HDFS block size is set to be 128MB and each node is configured to run one reducer task.
 We use TPC-H to generate the input dataset and take the following query in our experiments: select * from CUSTOMER C join ORDER O on C.CUSKEY = O.CUSKEY In order to control data skewness, we randomly choose a portion of the input dataset ORDER and change its CUSKEY to the same value. For example, if the skewness is 10%, it means that we change 10% rows of the input dataset ORDER to have the same value in the join attribute CUSKEY . Finally, we generate 20 million records for query with various degree of data skewness. 5.2 Partitioning Effectiveness Firstly, our concern is whether or not SALA join can effectively solve the par-titioning skew problem. As analysis in Sect. 2 has suggested that the key factor of load balancing is uniform distribution of data, we can therefore evaluate the capability of a join algorithm to handle skewed data by the value of max-reducer-input , i.e., the maximum volume of data distributed to any reducer. According to Fig. 5(a) , as the degree of skewness increases, repartition join concentrates a large amount of data on hot nodes, while both SALA join and SAND join can guarantee the uniform distribution of data. results that do not need to be transferred over network. From Fig. 5(b) ,wecan see that the data locality for SALA is much larger than the data locality for both repartition and SAND methods. Because the larger the data locality is, the less the volume of data required to be transferred across the network, thus less data needs to be transferred in SALA join algorithm than in both repartition and SAND methods. 5.3 Response Time Fig. 6(a) shows comparison between response time used to complete the given join operation under different degree of data skewness. The performance of repar-tition join is the best in the case of no or little data skewness, and the reason is that both SALA join and SAND join require additional MapReduce job to obtain frequency distribution of key values. However, with the increase of data skewness degree, the response time of repartition join increases almost linearly. The reason is that as the degree of data skewness increases, data will concen-trates on hot nodes as Fig. 5(a) shows, which increases the completion time of the overall join operation. However, both SALA and SAND can guarantee load balance, and therefore the response time remains steady with the increase of skewness degree. Most importantly, SALA join algorithm performs better than others when skewed data exist, because SALA not only achieves load balance but also reduces network overhead, thus speeding up the join operation process with the increase of data skewness degree. when the degree of data skewness is 10%. It can be seen that as the average available bandwidth reduces, the problem of network overhead becomes promi-nent. It is because that the lower the bandwidth is, the longer time it will cost to complete network transmission. By taking full advantage of data locality fea-ture, the minimum volume of data is transferred with SALA join algorithm, and therefore SALA is preferable in the case of low bandwidth. In recent years, various approaches have been proposed to deal with skewed data has demonstrated that the default hash partitioning function in Hadoop is not efficient for the skewed data and may lead to load imbalance of reducers. The partitioning skew problem due to data skew can be solved by making a good partition scheme based on the key X  X  frequency distribution, while sampling is a common way to obtain key X  X  frequency [ 12 , 2 ]. The SAND join alogoritm [ 2 ] uses simple range partitioning for data distribution to achieve load balancing. Yujie Xu et al. proposed two partition schemes, namely cluster combination opti-mization and cluster partition combination based on random sampling results, to handle slight skew and heavy skew respectively.
 Reducing the volume of data transferred across the network is an efficient way to further improve the performance of data-intensive join operation. Based on the priori knowledge of skewed key values, PRPD join geography proposed in [ 11 ] keeps all skewed rows locally to reduce the data volume transferred among nodes over network. LEEN scheme presented in [ 6 ] partitions the intermediate results based on key X  X  frequency and the fairness score that is calculated after the shuffle phase. However, LEEN scheme changes the internal implementation of Hadoop and overlooks the advantage of overlapping between the shuffle and map.
 An alternative strategy to mitigate skew is dividing the workload into fine-grained computation tasks and then scheduling them dynamically at runtime processing time when a node in the cluster becomes idle. The unprocessed data of this unfinished task is then repartitioned in a way that fully utilizes the computing power of cluster nodes. In this paper, we propose SALA join algorithm, using volume/locality-aware partitioning to distribute intermediate results. On one hand, SALA guarantees the uniform distribution of data based on key X  X  distribution information and therefore avoids partitioning skew problem. On the other hand, SALA takes full advantage of the data locality feature to reduce the volume of data transferred across the network. Experiments show that SALA is efficient to deal with skewed data. Our future work includes further improving performance of SALA join and extending volume/locality-aware partitioning to other MapReduce applications.
