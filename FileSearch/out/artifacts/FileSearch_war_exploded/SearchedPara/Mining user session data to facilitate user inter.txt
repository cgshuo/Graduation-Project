
The design of software applications that rely heavily on interaction with users is often handicapped by lack of knowl-edge about how users behave. This fact has stimulated the development of applications that monitor user activity and attempt to learn from this activity to improve this appli-cation's effectiveness. These types of applications can be divided into two classes: (1) applications used primarily by a single person, whose idiosyncrasies can be learned over time, and (2) applications that are used by many people. In the latter case, the design choice is whether to adapt to better fit a prototypical user, or to attempt to identify users as belonging to pre-defined or learned categories with cor-responding behavior. Assuming the user can be categorized with reasonable accuracy, the last approach offers the pos-sibility of the most adapted or personalized response, and hence should be the most effective. To achieve this, the record of user interactions, or clickstream, must be mined for information about classes of user behavior. 
RightNow Web (RNW) is a complete product for online customer service. As such, it contains many features which are beyond the scope of this paper. Here we will not ad-dress the entire realm of administrative functions, including such things as problem ticket tracking, workfiow routing, and individual and company based contract management. Instead, we will focus on the end-user interactions with a knowledge base in the form of FAQs. In RNW, FAQs con-tain a title, question description, and question answer. Each FAQ is tagged with products or categories assigned by an administrator to help end users identify FAQs specific to a particular product or type of issue. Depending on the config-uration, there may also exist custom fields associated with each FAQ as defined by an administrator. In addition to businesses, educational and government entities use RNW as a means of providing information to the public; one ex-ample is the U. S. Social Security Administration help site end-user activities of browsing and searching the knowledge base are similar to those in many database-related applica-tions. 
RNW tracks users' activities as they search the knowledge base for answers. However, users are anonymous until they specifically submit a request for help. Therefore we have data for an anonymous individual visiting the site, but no way to track a particular visitor on subsequent visits. This restriction forces a reconceptualization of several interest-ing problems. Most importantly, conventional collaborative for the FAQ drops. If, instead, the visitor rates the FAQ as helpful, the ranking for the FAQ increases. Since the list of FAQs is displayed sorted in descending order of usefulness based in part on explicit rankings, those FAQs with higher ratings can be thought to float to the top of the list, while those with lower ratings will sink lower in the list, as seen in Figure 1. 
Unfortunately, only a small percentage of visitors actu-ally explicitly rate FAQ usefulness. A random sampling of sites suggests that between 0.1% and 10% of visitors ac-tuaily rate an FAQ. Implicit feedback was a method de-vised to augment the low response rate of visitors on the explicit feedback measure. With an implicit measure there is a guarantee of full participation at the potential cost of ac-curacy of the information. Thus, implicit feedback measures are always counted with less weight than explicit measures. The philosophy behind the implicit measures of usefulness is loosely based on a swarm intelligence or ant colony [1] approach. With our implicit measures we turn the tables from the standard approach with these techniques where each member of the swarm is a computer function searching for a local optimum. We use human visitors as the swarm members instead. To this end we must assume, instead of relying on explicit algorithmic instructions in the computer swarm model, that each human is searching the information space in a greedy, locally optimal way. For human visitors to an FAQ server, this means that we first assume each per-son comes to the site with a specific question to which they are attempting to find an answer. We further assume that once at the site the search approach they use is directed as opposed to random. In this case, a directed search means that the visitor will choose only those items which, based on the available information, appear to answer their question. Through the implicit measure, we increment our usefulness counter for each item an end-user visits. The last FAQ vis-ited in a session increments the counter slightly more than the other FAQs visited in that session, under the assump-tion that the session ended with the successful discovery of the answer to the visitor's question. Thus, each visited FAQ in a session is rated higher than before the session began, and the terminal FAQ is rated higher than the non-terminal FAQs in that session. 
The implicit and explicit rankings are tracked through the same mechanism, but the explicit rankings are given signif-icantly more weight in the calculation. Implicit rankings are based on assumptions of usefulness, so they are not con-sidered as reliable as the explicit rankings. Through the combination of the implicit and explicit manipulations to the usefulness counter, we can place an order on the list of FAQs that directly corresponds to the relative usefulness of each FAQ. 
At the same time as the implicit feedback is computed, links between incidents are also generated using the ses-sion history information. These links are generated by our human visitors, which we assume are operating in a non-random manner. Thus, a link is created between two FAQs or a usage counter of an existing link is incremented when an end-user visits two FAQs in sequence in the same session. Each link contains at least four characteristics, a "from" lo-cation, a "to" location, a usefulness counter, and a link type. The link type is not necessary in conceptualization, but is usefulness rankings current with respect to recent user traffic patterns. 
Conceptually the data aging process is rather simple. First, each time an end-user visits an FAQ, an access timestamp is updated on that FAQ. Periodically the access timestamps on the entire set of FAQs are analyzed, and those FAQs that have not had recent visitation have their usefulness rankings decreased. Both the amount of decrease and the frequency of aging are configurable depending on the particular de-mands of the site. 
When an unreinforced usefulness ranking drops as a result of the data aging process, the FAQ will slide down the list of information presented to the user. Once an FAQ consis-tently drops below the first few pages of information, the reduction in visitation will result in a drop in the frequency of link reinforcement. 
Data aging is an important consideration for keeping in-formation current in real-world systems. For example, Pit-ney Bowes is a large supplier of postage metering equip-ment. Recently, when the U. S. postage rates increased, the RNW installation of Pitney Bowes had a sudden spike of questions related to this change. As one might guess, those FAQs that had previously appeared at the top of the useful-ness rankings likely had no relevance to the sudden flurry of questions about the rate increase. If previously top-ranked items are no longer visited in favor of the newly preferred information, the new information will reach the top of the usefulness listing sooner if the old information is aged. In customer service applications, the faster information is pro-vided to customers, the lower the cost of support becomes. 
The system can also be configured to present the list of linked FAQs sorted by link strength instead of by solved count. As in the previous configuration, when a link is rein-forced less frequently than other links it slides down the list of information presented. As the FAQ moves down the list it will be visited less frequently resulting in the usefulness ranking being reinforced less frequently. 
It is obvious that there is a strong coupling between the reinforcement of the links and the reinforcement of the use-fulness rankings. This coupling implies a linear relationship between the strength of the usefulness ranking and the total strengths of the links. If the number of links associated with a given FAQ is high, the effect of a particular link becoming weaker or disappearing altogether will have a low effect on the usefulness ranking of that document. 
For documents with a low number of links the aging pro-cess can result in suppressed usefulness rankings and vice versa depending on the configuration of the interface. In-tuitively, these types of documents axe examples of isolated information. An RNW feature not covered in this paper is that, if desired, the aging process can be configured to change the status of isolated documents and signal admin-istrators. 
Maintainers of the knowledge base need to be aware to watch for these types of status changes and take appropri-ate action to either remove the document, alter it to be more related to stronger documents, or add new information sim-ilar to the given document in an attempt to build a group of related information. 
In this system, groups of similar FAQs will tend to form "cliques" where all members of the group have strong links to most of the other members. These groups tend to be very stable while isolated FAQs tend to have trouble stay-Table 2: Growth and pre-growth saving figures for Military.corn 300% 10% ScholarOne 100% 17% 
Specialized 10% 10% a dollar savings or a number of hours saved, instead of a reduction rate. In some notable cases RNW customers re-ported an estimated savings of more than $1.2 million per month as a result of reduced phone support volume. A re-cent study by Forrester Research [5] claims an industry wide average of $33 per phone support call, so any reduction in phone support volume can reduce costs quickly. Table 2 shows self-reports of overall savings in spite of an increase in customer base over the same time period. Businesses closely track the number of support requests as well as the average cost to respond to each request as part and parcel of their operating expenses. Thus, while these numbers are self-reports, they are likely to be fairly accurate. 
These numbers refer only to the percentage of end-users who find their answer within the entire FAQ list. The com-parison group are, effectively, the same end-users before there existed a dynamic FAQ list. Unfortunately this data does not break down the usefulness of the individual ap-proaches outlined above. However, from these numbers it is apparent that the suite of data mining and knowledge discovery approaches as described certainly aids a large per-centage of the general public to find solutions to their prob-lems faster than if the suite were unavailable. 
A more complete analysis performed by Doculabs, Inc. [8] on 3.7 million service requests to 202 companies in the first quarter of 2001 shows an 86.9% self-service rate with RNW. According to Doculabs, that equates to a savings of more than $100 million quarterly. The breakdown of self-service by industry is shown in Table 3. 
Our objective in using a links matrix and solved counts was to meet the user's demands as quickly as possible. How-ever, from the discussions in the previous sections, we know that these rely heavily on the actual usage patterns. Explicit solved count techniques require user feedback, which, when added to the user-dependence of the links matrix, could bias these metrics undesirably. The key to achieving user-independence would be to "understand" the similarities of text FAQs using relatively simple knowledge of natural lan-guage. Grouping similar FAQs using clustering techniques would not only enhance user interaction, but also provide a semantic tool to automate the process of answering user queries. 
In the release of RNW currently under development, we have made many modifications which are the topic of an-other paper. First, we incorporate a heavily modified vari-ation on the incremental and hierarchical clustering tech-nique, BIRCH [9]. This algorithm allows us to organize the FAQs in a tree-structure where each internal node of the tree stands as a representative summary of the FAQs in the their responses to best suit the situation. Previously we re-lied on word-scoring and word-stemming for this, but we have recently added the part-of-speech-tagging techniques to our repertoire. This technique complements the other knowledge discovery and data mining approaches discussed in this paper to make for a more complete software applica-tion. 
Various knowledge discovery techniques applied to user sessions have proven themselves very useful in RightNow Web, an online customer service tool used by approximately 1,200 companies, schools, and government organizations. With the knowledge discovery techniques in PdghtNow Web, human support load is significantly reduced by allowing end-users to easily find answers to their questions. Several new techniques are identified for future versions of the product, hopefully allowing even more time to spend providing a per-sonal touch for those customers requiring more involved so-lutions to their problems. [1] E. Bonabeau and G. Th~raulaz. Swarm Smarts. [2] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [3] E. Brill. Some Advance in Transformation Based Part [4] E. Brill and P. Resnik. A rule-based alJproach to [5] B. Chatham, D. M. Cooperstein, and A. A. Reinhard. [6] W. W. Cohen. Fast effective rule induction. In [7] D. Fisher. Iterative optimization and simplification of [8] J. Watson, G. Donnelly, and J. Shehab. The [9] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: 
