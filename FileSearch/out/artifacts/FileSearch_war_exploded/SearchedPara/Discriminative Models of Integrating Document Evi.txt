 Generative models such as statistical language modeling have been widely studied in the task of expert search to model the relationship between experts and their expertise indi-cated in supporting documents. On the other hand, dis-criminative models have received little attention in expert search research, although they have been shown to outper-form generative models in many other information retrieval and machine learning applications. In this paper, we propose a principled relevance-based discriminative learning frame-work for expert search and derive specific discriminative models from the framework. Compared with the state-of-the-art language models for expert search, the proposed re-search can naturally integrate various document evidence and document-candidate associations into a single model without extra modeling assumptions or effort. An extensive set of experiments have been conducted on two TREC En-terprise track corpora (i.e., W3C and CERC) to demonstrate the effectiveness and robustness of the proposed framework. H.3 [ Information Storage and Retrieval ]: H.3.3 Infor-mation Search and Retrieval; H.3.4 Systems and Software Algorithms, Design, Experimentation Expert search, enterprise search, discriminative models
With vast amount of information available within large or-ganizations, the key challenge is to harness existing knowl-edge and expertise in a timely and effective manner. In consequence, enterprise information retrieval systems are in-creasingly demanded to return people with specific knowl-edge and skills in response to a user X  X  query. A class of vertical search engines known as expert finder have emerged for enterprise organizations.

As an important IR application, expert search (also known as expert finding) has received substantial attention in the IR research community. Rapid progress has been made in modeling and evaluation since the launch of TREC Enter-prise Track in 2005 [12]. A notable observation is that prob-abilistic generative models have dominated the literature of expert search. In particular, many statistical language mod-eling techniques were proposed to model the relationship be-tween a candidate expert and a query. These models usually characterize a generative process of how a query is generated from supporting documents of an expert. The key ingredient in these methods is to determine associations between peo-ple and documents because the associations are ambiguous in the TREC scenarios as well as in many realistic settings. Previous works have investigated different metrics or a com-bination of them to measure the associations, but the way of choosing or combining them is rather often heuristic and lacks of a clear justification. Furthermore, document ev-idence such as document or expert authority information, internal and external document structures, global evidence and so on is shown to be able to significantly improve ex-pert retrieval performance, but to incorporate these features often requires many modeling assumptions and is often un-wieldy.

On the other hand, discriminative models, another impor-tant class of probabilistic models with solid statistical foun-dation, are nearly absent in the research of expert search, especially on the TREC evaluations. In fact, discriminative models have been preferred over generative models in the recent past in many machine learning applications, partly because of their attractive theoretical properties. In the do-main of IR, various discriminative models have also been applied to many retrieval problems (e.g., [23]). However, very limited research has been conducted to design discrim-inative models for expert search.

In this work, we present a relevance-based discrimina-tive learning framework for expert search and derive spe-cific discriminative models from the framework. Similar to some prominent language models, the proposed models ag-gregate document evidence and document-candidate associ-ations through supporting documents. Unlike the language models, we directly model the conditional probability of rel-evance given a query and an expert. As a result, heteroge-neous or even arbitrary features can be naturally included into a single model. The parameters associated with the features are automatically learned from training data. We report an extensive set of experiments on two TREC corpora to evaluate the effectiveness and robustness of the proposed discriminative framework.

The next section discusses related work. Section 3 intro-duces the state-of-the-art generative language models for ex-pert search. Section 4 presents our proposed approaches. In section 5, we discuss the advantages of discriminative mod-els in the context of expert search. Section 6 explains our experimental methodology and Section 7 presents the ex-perimental results. Section 8 concludes and points out some future work.
The early work on expert finding systems was initiated in the Knowledge Management community, usually in the form of yellow pages [9]. These systems relied on experts to judge and input their skills by themselves against a predefined set of keywords, and thus the task was time-consuming. More recent techniques locate experts in an automatic fashion. An overview of early automatic expert finding systems is pro-vided in [36]. The task of expert search has received a signif-icant amount of attention as the task had been included in the TREC Enterprise track from 2005 to 2008 [12, 32, 1, 7]. The TREC Enterprise tracks provided a common platform for researchers to empirically evaluate methods for expert search. They demonstrated the feasibility of expert search on heterogeneous data collections. In the TREC corpora, the relationship between documents and experts is ambigu-ous and thus to model the document-candidate associations is a key issue in expert search research.

Most of the recent work on expert search generally falls into two categories: profile-centric and document-centric ap-proaches. Balog et al. [3] formalizes the two methods by proposing two generative language models. Their Model 1 directly models the knowledge of an expert from associated documents, which is equivalent to a profile-centric approach, and their Model 2 first locates documents on the topic and then finds the associated experts, which is a document-centric approach. It has been shown in [3] that Model 2 is generally more effective than Model 1 and since then it be-comes one of the most prominent language models for expert search. In [8], a two-stage language model combining a doc-ument relevance and co-occurrence model is proposed, which is essentially equivalent to Model 2. An attempt to further improve their models is made by proposing a proximity-based document representation for incorporating sequential information in text [25]. There are many other generative probabilistic models proposed for expert finding. For exam-ple, Serdyukov and Hiemstra [30] propose an expert-centric language model. Fang and Zhai [14] derive two families of generative models by applying probability ranking principle. Probabilistic topic models are also proposed to simultane-ously model the topical distribution of expertise evidence and experts [34].

Some alternative approaches to expert search exist beyond language modeling. One effective approach is to treat the problem of ranking experts as a voting problem based on data fusion techniques [21]. Eleven different voting strate-gies were proposed to aggregate over the documents associ-ated to an expert. Another approach is to model the process of expert finding by probabilistic random walks on so-called expertise graphs [31]. Many other expert finding methods were proposed during TREC Enterprise tracks.

Besides the models, some researchers have shown that suitable features can help significantly boost the performance of expert finding. These features include document author-ity information such as the PageRank, indegree, and URL length [38], graph-based expert authority [10], internal doc-ument structures that indicate the experts X  associations with the content of documents [6], non-local evidence [2], and the evidence that can be acquired outside of an enterprise [29]. Additional evidence can be integrated by identifying home pages of candidate experts and clustering relevant docu-ments [20]. Proximity features that characterize the co-occurrence of query and expert mentions in the document are also shown indicative by the top runs in the TREC eval-uations [16]. This led to several window-based approaches including [25, 4, 20].

On the other hand, the early work of applying discrim-inative models in IR can date back to the early 1980s in which the maximum entropy approach was investigated to get around term independence assumptions in probabilis-tic generative models [11]. More recently, Nallapati [23] compared the performance of the maximum entropy model and support vector machines with that of language modeling in ad hoc retrieval and homepage finding, and argued that SVMs are preferred over language models because of their ability to learn arbitrary features automatically. Further-more, it has been shown that feature-based discriminative models can consistently and significantly outperform cur-rent state of the art retrieval models with the correct choice of features [22]. Discriminative models have received in-creasing attention in IR, as another related area, learning to rank for IR, sparked genuine interest among researchers in the community [18]. Most of the learning to rank models are discriminative in nature and they have been shown improve-ments over their generative counterparts in ad hoc retrieval. Benchmark data sets such as LETOR [19] are also available for research on learning to rank. Although valuable work has been done on discriminative models for ad hoc retrieval and other IR domains, very limited research has been conducted to design discriminative models for expert search. The only relevant work that we are aware of is [15], which addressed the issue of differentiating heterogeneous sources according to specific queries and experts by learning associated weights from data, but the work did not model document-candidate relationship nor address how to incorporate new document evidence, which are two key issues in expert search.
To predict a class  X  given an observation  X  , the desired choice of  X  is given by the conditional class probabilities  X  (  X   X   X  ). Depending on how to compute  X  (  X   X   X  ), the exist-ing classification techniques can be broadly classified into two major categories: generative models and discriminative models. In a discriminative approach, a parametric model is introduced for  X  (  X   X   X  ), and the values of the parameters are inferred from a set of labeled training data. In contrast, the generative approach attempts to capture the manner in which an observation  X  is generated from given classes  X  by specifying a prior distribution  X  (  X  ) over classes and a class-conditional distribution  X  (  X   X   X  ) over the observation. The posterior  X  (  X   X   X  ) is obtained from Bayes X  Theorem as
In the context of expert search, the task is to find out what is the probability of a candidate  X  being an expert given a query topic  X  . In other words, we want to know  X  (  X   X   X  ) in order to rank candidate  X  according to this probability. Similarly, by invoking Bayes X  Theorem, we have: where  X  (  X  ) is the prior probability of a candidate, which is generally assumed uniform. Thus, the key quantity to esti-mate in the generative models is the probability of a query given the candidate,  X  (  X   X   X  ). Many language modeling tech-niques are proposed to estimate this quantity. One of the most prominent and effective one was called document mod-els (often referred as Model 2) [3] where documents act as a hidden variable in the process which accumulates expertise evidence. Formally, it is expressed as where  X  (  X   X   X   X  ) is the probability of the document  X   X  erate the query  X  and can be calculated using a standard language model.  X  (  X   X   X   X  ) is the probability of association between the document  X   X  and the candidate  X  .  X  is the number of documents in the collection. Model 2 mimics the process one might use to find experts using a document re-trieval system. Here, relevant documents are retrieved for the expertise requested, and they are used as evidence to indicate whether the associated candidates are experts. Af-ter aggregating all such evidence, the experts can be iden-tified. As  X  (  X   X   X   X  ) is relatively easy to determine in lan-guage models, the key ingredient in this model (and also in many other language models for expert search) is to estimate the document-candidate associations:  X  (  X   X   X   X  ), or  X  (  X   X   X   X  (  X   X  ) is assumed to be uniform.  X  (  X   X   X   X  ) can be estimated by various methods. The simplest form is the boolean model where associations are binary decisions:  X  (  X   X   X   X  ) = 1 if the candidate appears in the document; otherwise,  X  (  X   X   X   X  More sophisticated methods are frequency based which con-sider the number of times that a candidate appears in the document. A set of heuristic combinations of all these met-rics are also compared and investigated in [6].
For the text-based retrieval, conventional relevance-based probabilistic models rank documents by sorting the con-ditional probability that each document would be judged relevant to the given query [17]. The underlying principle using probabilistic models for information retrieval is called probability ranking principle [26]. The Binary Independence Model (BIM) [27] is a realization of this principle. In the domain of expert search, the similar principle can be used where experts are ranked according to the descending order of the conditional probability of relevance given an expert and a query. Fang and Zhai [14] applied this principle in studying expert search problem. Both BIM and [14] X  X  mod-els are generative and they use Bayes X  theorem to reverse the original conditional probability.

We propose a discriminative learning framework to di-rectly model the conditional probability of relevance by a parametric probability function. We cast expert search into a binary classification problem that treats the relevant query-expert pairs as positive data and irrelevant pairs as negative data. Formally, we use a relevance variable  X   X  X  1 , 0 } to denote whether two entities are relevant or not and thus the conditional probability of relevance  X   X  (  X   X   X ,  X  ) represents the extent to which the expert  X  is relevant to the query  X  . In our framework,  X   X  (  X   X   X ,  X  ) can take any function form with parameter  X  that needs to estimate from training data. Based on different forms of  X   X  , the resulting discriminative models are different. Given the relevance judgment  X   X  X  X  for the training expert-query pair (  X   X  ,  X   X  ) which is assumed in-dependently generated, the conditional likelihood  X  of the training data is as follows  X  = where  X  is the number of queries and  X  is the number of experts. The parameters can then be estimated by maxi-mizing the following log likelihood function  X  = arg max
The estimated parameters can then be plugged back in  X  (  X  = 1  X   X   X  ,  X   X  ). According to the probability ranking prin-ciple, the experts are presented to users in the descending order of  X   X  (  X  = 1  X   X   X  ,  X   X  ). In the next section, we pro-pose a specific discriminative model by defining the form of  X   X  (  X  = 1  X   X   X  ,  X   X  ).
According to the previous work, Model 2 turned out to be one of the most effective formal models for expert search. The success of the model lies in its effective process to col-lect expertise evidence from documents. Our discriminative model builds on the same process in which the supporting document  X  serves as a bridge to connect expert  X  and query  X  . Given a document  X  , whether  X  and  X  are relevant depends on two factors: document evidence and document-candidate associations. More specifically, we consider: 1) whether the document  X  is relevant to the query  X  ; 2) whether the expert  X  is relevant to the document  X  . The final relevance deci-sion for (  X ,  X  ) is made by averaging over all the documents. Formally, this can be expressed as  X   X  (  X  = 1  X   X ,  X  ) = where  X  (  X  1 = 1  X   X ,  X   X  ) allows us to model the probability that a document  X   X  matches a topic  X  , which indicates the docu-ment evidence.  X  (  X  2 = 1  X   X ,  X   X  ) allows us to model the prob-ability that a supporting document  X   X  mentions a candidate  X  , which indicates the document-candidate associations. A document  X   X  with higher values on both probabilities would contribute more to the value of  X  (  X  = 1  X   X ,  X  ). The prior probability of a document,  X  (  X   X  ), is generally assumed uni-form (i.e.,  X  (  X   X  ) = 1  X  (  X  2 = 1  X   X ,  X   X  ) by logistic functions on a linear combination of features. Formally, they are parameterized as follows: where  X  (  X  ) = 1 / (1 + exp (  X   X  )) is the standard logistic func-tion.  X   X  is the weight for the  X   X  X  X  query-document feature  X  (  X ,  X   X  ) and  X   X  is the weight for the  X   X  X  X  document-candidate dence such as document retrieval scores that indicates how relevant the document is to the query.  X   X  (  X ,  X   X  ) is the feature such as the boolean associations that describe the strength of associations between a document and a candidate.  X   X  denotes the number of document evidence features and  X   X  denotes the number of document-candidate association fea-tures. The weight parameters can be learned by maximiz-ing the conditional log-likelihood of the data (i.e., Eqn. 5). Because there is no analytical solution, we use the BFGS Quasi-Newton for the optimization [13]. The method re-quires the objective function and its gradients. The partial derivatives of the log-likelihood  X  with respect to  X   X  and  X  are given as  X  X  X   X  X  X  where  X   X  ,  X   X  and  X   X  denote the probabilities of Eqn. 6, Eqn. 7, and Eqn. 8, respectively. The main computation of the gradient method is evaluating the log likelihood function and its gradients against parameters. Both of them have computational complexity of  X  tice, we only have a small number of relevance judgments for training and thus  X  is relatively small. In addition, the number of documents associated with each expert and the number of features used are also usually relatively small. Therefore, the training procedure can be efficient.
We can see that both Model 2 and this discriminative model try to aggregate document evidence and document-candidate associations through the bridge of documents, but they are different in how to estimate these two probabili-ties. In Model 2, the document evidence (i.e.,  X  (  X   X   X   X  calculated by standard language models and the document-candidate associations (i.e.,  X  (  X   X   X   X  )) are estimated by a heuris-tic combination of document-candidate association features. In our proposed discriminative model, both quantities are modeled by logistic functions with arbitrary features and the parameters are automatically determined from training data. From Eqn. 6, we can see that  X   X  (  X  = 1  X   X ,  X  ) is essen-tially the arithmetic mean of  X  (  X  = 1  X   X ,  X ,  X  ) with respect to  X  . Thus we refer the model as the arithmetic mean discrim-inative (AMD) model.
It has been shown that in certain cases geometric mean (the product rule) is better than arithmetic mean (the sum rule) in combining evidences [35]. This observation mo-tivates an alternative discriminative model which we refer as the geometric mean discriminative (GMD) model where  X  (  X  = 1  X   X ,  X  ) is modeled by the geometric mean as follows:  X  (  X  = 1  X   X ,  X  ) = 1 where  X  is the normalization factor that scales the geometric mean to be a proper probability distribution as follows  X  =
Both  X  (  X  1 = 1  X   X ,  X   X  ) and  X  (  X  2 = 1  X   X ,  X   X  ) here take the same form with Eqn. 7 and Eqn. 8. By plugging them and Eqn. 10 into Eqn. 9, we can get  X  (  X  = 1  X   X ,  X  ) = 1 where  X  =  X  =
We can notice that in Eqn. 11 there are three exponential terms in the denominator, which means that either query-document features  X   X  (  X ,  X   X  ) or document-candidate features  X  (  X ,  X   X  ) alone cannot dominate the final relevance  X  (  X  = 1  X   X ,  X  ). The parameters of the model can also be estimated by maximizing the conditional log-likelihood function using BFGS. The GMD model has the same computational com-plexity with AMD.
Some theoretical results show that discriminative mod-els tend to have a lower asymptotic error [24]. Besides the theoretical considerations, we believe there are specific rea-sons for the domain of expert search that make discrimi-native models a suitable choice. First of all, the proposed discriminative models can effortlessly incorporate features. As shown in Section 2 and prior research, expert search can benefit from including various types of features. Language modeling approaches often require many modeling assump-tions and extra modeling effort to include new features es-pecially when the heterogeneous features are present. Sec-ondly, discriminative models typically make fewer model assumptions than their generative counterparts. For ex-ample, many state-of-the-art generative models, including Model 2, the candidate-generation model [14] and the two-stage language model approach [8], assume that the query  X  and candidate  X  are independent given the document  X  , these models to overcome the assumption [4]. In contrast, our proposed discriminative models can easily get around it. For example,  X  (  X  2 = 1  X   X ,  X   X  ) in Eqn. 6 can be replaced by  X  (  X  2 = 1  X   X ,  X ,  X   X  ) where no independence assumption is made on  X  (  X  2 = 1  X   X ,  X ,  X   X  ). Thirdly, the discriminative models directly and naturally characterize the notion of rel-evance. In Model 2 and many other language models, there is no explicit reference to the class variable that denotes whether an expert is relevant or not. We use  X  (  X  = 1  X   X ,  X  ) instead of  X  (  X   X   X  ) to make it explicit that the relevance of an expert is measured with respect to a query. This explicit notion of relevance can help quantify the extent to which a user X  X  information need is satisfied.
Our experiments are carried out in the setting of the Ex-pert Search task of the TREC Enterprise tracks from 2005 to 2008. For TREC 2005 and 2006, the document collection was a crawl of the World Wide Web Consortium (W3C) [12, 32]. For TREC 2007 and 2008, a different and more realis-tic corpus was introduced, which is a crawl of the website of Commonwealth Scientific and Industrial Research Orga-nization (CSIRO). The corpus is known as the CSIRO En-terprise Research Collection (CERC) [1, 7]. Table 1 gives detailed statistics of the collections and query sets. The W3C data is supplemented with a list of 1092 candidate ex-perts represented by their full names and email addresses while the CERC data do not contain a predefined list of candidates. Based on the observation that most CSIRO em-ployees have a CSIRO email address following the pattern  X  X irstname.lastname@csiro.au X , we extract a list of candi-dates with email addresses matching this pattern from text. We also use heuristic rules to filter non-personal addresses (e.g. education.act@csiro.au). The total number of candi-dates extracted is 3,482. In 2005, 50 queries were created based on the working groups in W3C (there were 10 train-ing topics also available in 2005). In 2006, 49 queries were developed by the track participants collectively using the provided list of supporting documents for each candidate. The 50 queries used in 2007 were created with the help of CSIRO X  X  Science Communicators, while the judgments of 77 queries in 2008 were made by participants.
 To evaluate the proposed models on W3C, we use the TREC 2006 topics plus the 10 available TREC 2005 training topics for training and test the models on the TREC 2005 topics. Similarly on CERC, we use TREC 2008 topics for training and TREC 2007 topics for testing. Although differ-ent years have different ways of topic assessments, we will see in the experiments that the discriminative models can still gain significant improvements from the training data. Our decision of choosing the training and testing configura-tions is mainly based on the number of relevance judgments available. We need a reasonable amount of training data for the discriminative models and there are relatively more rel-evance judgments in 2006 for W3C and in 2008 for CERC. Because the two test collections have very different charac-teristics, we do not evaluate the models across the corpora. To obtain a balanced training set, we randomly select the same number of negative instances with the number of posi-tive instances for each training query, by following the under-sampling method in [23]. To acquire negative instances for the queries without non-relevance judgments (i.e., 10 TREC 2005 training topics), we use the Base method introduced in Section 6.1 to identify a list of unjudged/irrelevant experts for each query. Evaluation measures are mean average pre-cision (MAP), R-precision (R-Prec), mean reciprocal rank (MRR), and precision@5 (p@5) and precision@10 (p@10). Table 1: Statistics of the W3C and CERC testbeds
An extensive set of experiments were designed to address the following questions of the proposed research:
In all the sections except Section 6.4, we only use the arithmetic mean discriminative (AMD) model to assess the discriminative learning approach, since we care less about the difference between discriminative models than about the difference between generative and discriminative models.
In all our experiments, we have done minimal preprocess-ing in which both queries and documents are stemmed using Krovetz stemmer. We only use the  X  X itle X  or  X  X uery X  fields in the topics without using extra information (e.g.,  X  X arra-tive X ). No query expansion nor external resource is utilized. As shown in Section 4, each query-expert pair is character-ized by two feature vectors, i.e., document evidence  X   X  (  X ,  X  and document-candidate associations  X   X  (  X ,  X   X  ). Table 2 sum-marizes the features used in the discriminative models.
These features include the score from the standard doc-ument language model (  X  1 ), document features (  X  2  X   X  external document structure features (  X  6  X   X  9 ), basic associ-ation features (  X  1  X   X  5 ), internal document structure features (  X  6  X   X  9 ), and proximity features (  X  10  X   X  13 ). Here the ex-ternal document structure features are the boolean variables to represent whether a document (in W3C) comes from spe-cific types of documents (e.g.,  X  8 = 1 means the document is either from  X  X ww X  or  X  X sw X ). The evaluations on W3C use all the features, while the features  X  6  X   X  9 and  X  6 are not applied to CERC, as the CERC dataset does not Table 2: Features used in the discriminative models.  X  X  X  denotes the feature takes boolean values and  X  X  X  represents numerical values contain explicit document types nor many emails with in-ternal structure information useful for expert search [38]. The  X  1 feature is the document retrieval score by LM us-ing the topic as the query. The smoothing method of LM is Jelinek-Mercer with the parameter  X  = 0 . 5 (we use the same smoothing for other LMs). The  X  5 feature is the retrieval score by LM using the candidate identifier as the query [6]. The  X  X roximity X  features (  X  6  X   X  9 ) are the boolean variables indicating whether the candidate identifier co-occurs with the query term in a window with various sizes. We use 20, 50, 100 and 250 as the window sizes (in number of words), approximated to the sizes of sentence, passage, paragraph and section, respectively. The details about these features can be found in the corresponding reference. To normal-ize the features, we use query-based normalization for each feature as suggested in [19].

Many of these features have been shown useful for expert search. Because of the generative nature of language mod-els, it is difficult for them to incorporate such heterogeneous features in a unified modeling framework, but discrimina-tive models can effortless include all the features and many more. Since the focus of this study is on the probabilistic models rather than feature engineering, we do not intend to choose a complete set of features, but they are one of the most comprehensive and diverse feature sets in a single work among the existing expert search research.
In this section, we compare the proposed discriminative model with its generative counterpart: Model 2. The pro-posed model is evaluated on four different feature configu-rations, which are presented in Table 3. The Base method is the implementation of Model 2 by following [3], which includes 4 types of document-candidate associations. The R1 configuration uses these 4 association features plus  X  as document evidence. Thus, the identical information is Table 4: Comparison of the discriminative model (AMD) with the Base mehod on W3C and CERC.
 Best results on each collection are highlighted. The  X  symbol indicates statistical significance at 0.95 con-fidence interval against Base available for R1 and Base to use. The weights in Base are set by following the choice of the best run in [3]. R4 is the configuration with full applicable features for the discrimi-native model (the R4 configuration is the default setting in all the experiments except explicitly noted). Table 4 con-tains the evaluation results on the two test collections. We can see that the discriminative model consistently performs better than Base across all the feature configurations on all measures. With the full set of features (i.e., R4 vs Base), all the differences are statistically significant by two-tailed Stu-dent X  X  t-test at 0.95 confidence level. In R1 vs Base, although their differences are not significant, the discriminative model outperforms the Base method on all the evaluation metrics.
Since all the features are normalized, the weight associated with each feature can reflect the importance of the feature in some degree. Table 5 reports the top 3 features with the largest weights in  X   X  and  X   X  respectively in the learned AMD model. These features are ordered alphabetically in the ta-ble since their weights are not very distinct from each other. We find that the features listed for the two testbeds are gen-erally different with the exception of  X  1 and  X  2 , showing the importance of these two features across the corpora. An interesting observation is that the  X  8 feature when used on W3C has a large weight among all the document-candidate association features. This is intuitive in the sense that the person who is in the email cc field is likely an authoritative of the topics of the email, which is also consistent with what was reported in [5]. Another observation is that the  X  X rox-imity X  features have large weights for both testbeds (i.e.,  X  Table 5: The top 3 features with the largest weights in AMD (R4) learned from training data Figure 1: Impact of varying the number of docu-ments retrieved (  X  ) on the discriminative model. Top: impact on W3C; Bottom: impact on CERC. for W3C and  X  11 for CERC), but with different window sizes: i.e., larger size on W3C. This may come from the fact that these two collections have very different average document lengths.
Similar to Model 2, the learned discriminative model can be efficiently used on top of an existing document search engine as follows: 1) Perform a standard document retrieval run using the topic as a query and retrieve the top  X  doc-uments; 2) For each candidate associated with the rele-vant documents, calculate the probability of relevance using Eqn. 6 on these  X  documents. In this section, we aim to in-vestigate the effect of the size of documents retrieved on the performance of the discriminative model. We use LM as the document retrieval run. Figure 1 shows the MAP results by varying  X  on the two test collections. Note that the scales on the x-axis and y-axis differ per plot. From the figure, we can see that as  X  increases, the discriminative model has a similar trend with the baseline: increasing, achieving a max-imum, and then flattening. On W3C, the MAP value tops after 300 documents retrieved, fewer than what the baseline needs (i.e., 400). For CERC, both models need around 50 documents for best performance. Therefore, using a subset of documents could speed up the process of expert search as the best performers use much less documents than the whole set of relevant documents. At the same time, the retrieval performance can be improved although their differences are not found statistically significant. As shown in Section 6.1 as well as in prior work, the doc-Table 6: Evaluation of AMD with different docu-ment retrieval methods on W3C and CERC Table 7: Comparison of the geometric mean discrim-inative model with Base and AMD (R4) on W3C and CERC. The  X  symbol indicates statistical signif-icance at 0.95 confidence interval for GMD against Base ument retrieval score  X  1 is an important feature to show document evidence for expert search. In this experiment, we assess the extent to which the performance of the dis-criminative model is affected by the choice of the underlying document retrieval model. Besides LM, another two differ-ent document retrieval methods are used (i.e., BM25 [28] and Indri [33]). Specifically, the  X  1 feature is replaced by these two retrieval scores respectively in the R4 configura-tion. Table 6 shows the MAP results of the proposed model across the three retrieval models. From the table, we can see that the results are quite similar and they are all sig-nificantly better than the baseline. This indicates that the discriminative model is robust to the underlying document retrieval method.
In this section, we conduct the experiment to evaluate the alternative discriminative model (GMD). The aim is to investigate the robustness of the proposed discriminative framework with respect to the choice of specific discrimi-native models derived from the framework. Table 7 con-tains the results. From the table, we can see that all the results achieved by GMD significantly outperform the base-line. Furthermore, these results are quite similar with those achieved by the AMD (R4) model. In particular, the GMD model is generally better than AMD on CERC and worse on W3C, but the differences between GMD and AMD are not statistically significant. These results demonstrate that the proposed discriminative framework generates accurate and robust results with both types of discriminative models.
In this work, we propose a discriminative learning frame-work and derive specific models for expert search. The main advantage of the proposed approaches is their ability to integrate a variety of document evidence and document-candidate association features. The evaluations on two TREC Enterprise track testbeds have shown the effectiveness and robustness of the proposed framework.

There are several possibilities to extend the research in this paper. We chose  X  X ut-of-order X  training in the experi-ments because more training data are available in 2006 and 2008. It would be interesting to perform the  X  X n-order X  ex-periments (i.e., training on 2005 or 2007), which would allow fair comparisons with the TREC submitted runs. The rele-vance judgments in 2005 and 2007 seem also more likely to be obtained in a real enterprise. In fact, lack of training data hinders the applicability of many discriminative models. On the other hand, generative models may be able to effectively utilize abundant unlabeled data. It is desirable to develop a hybrid of discriminative and generative models to obtain the best of both for expert search. In addition, in certain scenarios, pairwise comparisons between experts might be more easily collectible than the pointwise judgment for each expert. We will explore to extend the proposed discrimi-native learning framework to handle this type of training data.
We thank the anonymous reviewers for many valuable comments. This research was partially supported by a grant from the Indiana Economic Development Company, the NSF research grant IIS-0749462, and a grant from Purdue Uni-versity. Any opinions, findings, conclusions, or recommen-dations expressed in this paper are the authors X , and do not necessarily reflect those of the sponsor.
