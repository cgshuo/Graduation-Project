 This demonstration presents New Dimensions in Testimony , the first dialogue system prototype to en-able a conversation with a real person who is not available for conversation in real time. Technology such as the telegraph, telephone and videoconfer-encing allowed people to communicate with each other across long distances with increasing fidelity, but required that the participants make themselves available for conversation at the same time. Other technologies such as writing, audio recording and video recording allowed people to send messages across time, but did not allow synchronous conver-sation. In the past two decades, embodied conversa-tional agents  X  that is, artificial characters controlled by computer programs  X  have been able to converse with users with increased complexity and natural-ness. Our system demonstrates how conversational agent technology can be used with recorded video statements from a real person to create a conversa-tion that is offset in time: the speaker recorded his statements in the past as a message to the future, and users now can interact with him and hold a conver-sation as if the speaker were present.

The New Dimensions in Testimony prototype is intended to emulate a conversation with Holocaust survivor Pinchas Gutter. Holocaust education today relies to a great extent on survivors talking to audi-ences in museums and classrooms, relating their ex-periences directly and creating an intimate connec-tion with their audiences (Bar-On, 2003). However, the youngest survivors are in their seventies today, and in a few years there will be no more survivors left to tell the story in person. The prototype will afford future generations the opportunity to engage in such conversation, talking to Pinchas Gutter and asking him questions about his life before, during and after the Holocaust. What makes our project unique is the ability to connect on a personal level with a survivor, and the history, even when that sur-vivor is not present.

The technology can have a wide range of appli-cations, such as preserving the memory of a per-son for the future (historical figures as well as ordi-nary people); enabling conversation with family and friends who are temporarily unavailable (traveling, deployed overseas, or incarcerated); allowing pop-ular speakers (leaders, celebrities) to engage with multiple people at the same time; and enabling ac-cess to expert knowledge and customer service. In the New Dimensions in Testimony prototype, users talk to a persistent representation of a Holo-caust survivor presented on a video screen, and a computer algorithm selects and plays individual video clips of the survivor in response to user utter-ances. The result is much like an ordinary conversa-tion between the user and the survivor. The system has been described in detail in previous publications, covering the proof of concept (Artstein et al., 2014), the content elicitation process (Artstein et al., 2015), the language processing (Traum et al., 2015a), the full prototype (Traum et al., 2015b), and ethical con-siderations (Artstein and Silver, 2016). Here we give a brief description of the language processing tech-nology and the system X  X  runtime components. 2.1 Language processing At the heart of the runtime computer system is a re-sponse classifier and dialogue management compo-nent called NPCEditor (Leuski and Traum, 2011), which selects a response to each user utterance. NPCEditor combines the functions of Natural Lan-guage Understanding (NLU) and Dialogue Manage-ment  X  understanding the utterance text and select-ing an appropriate response. The NLU functional-ity is a classifier trained on linked question-response pairs, which identifies the most appropriate response to new (unseen) user input. The dialogue manage-ment logic is designed to deal with instances where the classifier cannot identify a good direct response. During training, NPCEditor calculates a response threshold based on the classifier X  X  confidence in the appropriateness of selected responses; at runtime, if the confidence for a selected response falls below the predetermined threshold, that response is replaced with an  X  X ff-topic X  utterance that asks the user to re-peat the question or takes initiative and changes the topic (Leuski et al., 2006); such failure to return a direct response, also called non-understanding (Bo-hus and Rudnicky, 2005), is usually preferred over returning an inappropriate one (misunderstanding). The current system uses a five-stage off-topic se-lection algorithm which is an extension of that pre-sented in Artstein et al. (2009). Figure 1 shows a sample dialogue illustrating the handling of non-understanding.

The system has over 1700 recorded responses (a total of almost 18 hours of video), allowing it to give appropriate direct responses to about 64% of the user questions, with 20% off-topic responses and the remaining 16% being errors. This is sufficient to enable a reasonable conversation flow (Traum et al., 2015a). Between responses the system loops through short videos of idle behavior by the survivor, giving the feeling of live presence. When the user starts speaking, this changes to concentrated listen-ing behavior, adding to the feeling of engagement. 2.2 Software components The system is built on top of the components from the USC ICT Virtual Human Toolkit, which is pub-cally, we use the AcquireSpeech tool for capturing itor (Leuski and Traum, 2011) for classifying the ut-terance text and selecting the appropriate response, and a video player to deliver the selected video re-sponse. The individual components run as sepa-messaging over ActiveMQ: each component con-nects to the broker server and sends and receives messages to other components via the broker. The system setup also uses the JLogger tool for record-ing the messages, and the Launcher tool that controls starting and stopping of individual tools. Figure 2 shows the overall system architecture. A typical ses-sion on a Mac is shown in Figure 3. 2.3 System hardware A typical installation is run on a 15-inch MacBook Pro with Retina display, connected via HDMI to an external monitor or television. We have used dis-plays ranging from a basic 22-inch desktop mon-itor for personal interaction to a large theatre pro-jector screen, though our preferred display is an 80-inch high definition television in vertical orientation (Figure 4). This allows showing the speaker at ap-proximately life size, making it appropriate for one-on-one and small group interaction, as well as large group interaction in a theatre setting.

For small, informal demonstrations in a quiet set-ting, we have had good results using the MacBook Pro X  X  built-in microphone for audio capture, and the built-in trackpad as a push-to-talk button. In more challenging environments we use a Sennheiser HSP-4 headworn microphone, which works well to isolate the user X  X  speech from the background noise. The microphone is connected to a wireless transmitter-receiver pair and sent to the computer through a Focusrite Scarlett 2i2 USB recording au-dio interface. Push-to-talk functionality is provided by a wireless mouse, removing any physical con-nection with the computer and allowing the user full freedom of movement. The speaker X  X  audio is normally transmitted over HDMI together with the video, but can be routed through the Focusrite inter-face to external speakers when needed. 2.4 Mobile version The mobile version (Figure 5) is built using an Android-based virtual human software platform (Feng et al., 2015). This platform allows script-based access to speech recognition, video playback, and dialogue management services via Jerome, an implementation of the NPCEditor algorithm.

In order to accommodate the smaller display and mobile nature of a handheld device, the videos were reduced from 1080  X  1920 to 270  X  480, effectively reducing the size of the videos by a factor of 16. This results in a change in video file size from ap-proximately 1.7 gb per hour (28 mb per minute) of content to 110 mb per hour (1.75 mb per minute) of content. Frequently used videos, such as those for listening and off-topic responses are stored locally on the mobile device, while the rest are stored on a video-streaming cloud service and are retrieved on demand. Streaming videos of such size via wifi con-nection yields similar response times to playing the videos locally on the device, and greatly reduces the size of the mobile app. An additional button on the app allows the user to indicate explicitly that a given response is inappropriate to the question asked; this information is used for future classifier training.
The classification algorithm and data are pro-cessed locally on the device. Speech recognition is handled via the Android X  X  interface to Google ASR. Thus, there are three network messages for each user utterance: one to obtain the results of the ASR, an-other to retrieve the desired video if found, and a third to store the recognized question and response in a cloud-based database, for later analysis. The classifier data can be replaced through an update to the mobile app, thus allowing for easy propagation of improvements in the question/answer interaction as larger amounts of data are captured and analyzed. The demonstration will feature a live interaction be-tween participants and Pinchas Gutter, on both desk-top and mobile platforms. Depending on the par-ticipant X  X  preference, interaction will be either mod-erated (speech relayed by a demonstrator) or direct (participant operating the push-to-talk and talking into the microphone). The live conversations will highlight Mr. Gutter X  X  understanding, his ability to deal with non-understanding of user utterances, and the overall coherence of the conversation. It will also showcase many of Mr. Gutter X  X  moving personal sto-ries, and illustrate the sense of closeness and bond-ing that can form when talking to a person through a system of time-offset interaction.
 The New Dimensions in Testimony prototype is a collaboration between the USC Institute for Creative Technologies, the USC Shoah Foundation, and Con-science Display. It was made possible by generous donations from private foundations and individuals. We are extremely grateful to The Pears Foundation, Louis F. Smith, and two anonymous donors for their support. The Los Angeles Museum of the Holo-caust, the Museum of Tolerance, New Roads School in Santa Monica, and the Illinois Holocaust Museum and Education Center offered their facilities for data collection and testing. We owe special thanks to Pin-chas Gutter for sharing his story, and for his tireless efforts to educate the world about the Holocaust. This work was supported in part by the U.S. Army; statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorse-ment should be inferred.
