 SIGKDD Exploration s. Volume 4, Issue 2  X  page 99 In this paper, we ou tline the main steps leading to the develop ment of the winn ing solution for Task 2 o f KDD Cup 2002 (Yeast Gene Regulation Prediction ). Our unu sual solution was a pair of lin ear classifiers in h igh d imension al space (~14 ,000 ), develop ed with just 38 and 84 training examples, respectively, all belon ging to the target class on ly. The c lassifiers were bu ilt using the suppo rt vector machine a pp roach ou tlined in the paper.
 Suppo rt Vector Machines, One Class Learning, SVM, yeast gene. The Yeast Gene Regulation Prediction d ata, Task 2 o f KDD Cup 2002 , was heavily unb alanced, with 38 and 84  X  X arget X  class examples on ly ou t of the total of 3018 examples in the trai ning set. Most machine learning procedu res for develop ing a discrimination in such a data will requ ire some sort of re -balancing of the priors, i.e., boo sting the impact of examples from the mino rity class combined with d iminishing the impact of examples f rom the majority class. Using cross -validation on the training set we have found that the op timal solution for ou r setting is ob tained in the e xtreme ca se, when the majority class is completely eliminated. In this s ho rt paper, we present some details of o ur sub mission , includ ing specifics of data representation and classification procedu re a s well as s ome results of cross -validation tests. Each training and test gene was represented b y a vector of binary attribu tes extracted from the da ta sou rces provided. Attribu tes were e xtracted b y using on ly the e ntries from the data sou rces correspond ing to the training g enes.  X  Hierarchical information abou t fun ction , protein classes and  X  Textual information from all abstracts associated with a gene SIGKDD Exploration s. Volume 4, Issue 2  X  page 100 Figure 1: Mean AR OC +/ -std as a fun ction of balance factor Some results of cross -validation experiments aimed at  X  X p timization  X  of the bal ance factor B and the regularization con stant C are sho wn in Figures 1 and 2 . The figures s ho w the mean AROC (area und er ROC curve) with stand ard d eviation as an envelop e, where the means are c ompu ted on the validation set over 20 rando m splits of data int o 70 % : 30 %, learning : validation .
 Figure 1 sho ws the impact of the balance factor B on acc uracy. We have used 5000 = C , and the c ross -validation tests are performed u sing splits of the training data on ly. Figure 2 sho ws the e ffect of the reg ularization con stant C on AROC. Results are sho wn for 0 = B and 5 . 0 = B , with cross -validation splits of the training data on ly in Figures 2A and 2 B and combined training and test data in Figures 2C and 2 D. Based on the results in Figures 1, 2A and 2 B, the values 0 = B and 5000 = C were selected for the c ompetiti on sub mission . This s election amoun ts to training  X  X ard margin SVM X  with examples from a single (target) class on ly in the 14 ,336 d imension al f eature space .
 An add ition al po int to no te is that cross -validation estimates of AROC from the training data a nd the c ombined training + test data a re very close to eac h o ther. Thu s, in retrospect, the c ross -validation techn iqu e for mod el selection was a j ustified step. Our app roach h as a nu mber of distinct features.
 Automatic pre -processing and large nu mber of f eatures for classification : We have used a minimal do main kno wledge a nd passed a large nu mber of f eatures to the c lassifier. This follo ws from ou r previou s experience in p ractical t ext categorisation systems where labo riou s manu al i ntervention s withou t a deep do main insight often p rodu ced mediocre, if any, improvements.
 One -class learning : A po ssibilit y of on e c lass learning (with SVMs) h as been explored p reviou sly [1,2,3]. In these experiments, while 1 -class mod els performed reason ably, they were systematically ou tperformed b y mod els develop ed u sing data from bo th classes. To ou r kno wledge, the e xperiments with Yeast Gene data set repo rt ed in this paper, is the on ly case where the con trary is true.
 , Xiaojing Yuan 3 , and Zujia Xu 4 probability estimation trees, or PETs, for ranking and probability estimation. Capable of generating class membership probabilities, PETs have been shown to be highly accurate and flexible for many difficult problems, such as cost-sensitive learning and matching skewed distributions. There are a large number of PET algorithms available, and about ten of them are well-known. This large number provides an advantage, but it also creates confusion in practice. One would ask  X  X iven a new dataset, which algorithm to choose and what performance to expect and not to expect? What are the reasons to explain either good or bad performance under different situations? X  In this paper, we systematically, for the first time, answer these important questions by conducting a large-scale empirical comparison of five popular PETs by examining their AUC, MSE and error rate  X  X earning curves X  (instead of training-test split based cross-validation). Using the maximum AUC achieved by any of the evaluated probability estimation tree algorithms, we demonstrate that the preference of a probability estimation tree on different evaluation metrics can be accurately characterized by the  X  X ignal-noise separability X  of the dataset, as well as some other observable statistics of the dataset explained further in the paper. Moreover, in order to understand their relative performance, many important and previously unrevealed properties of each PET X  X  mechanism and heuristics are analyzed and evaluated. Importantly, a practical guide for choosing the most appropriate PET algorithm given a new data mining problem is provided. decision tree induction, probability estimation trees or PETs can estimate the posterior probabilities P(y|x) instead of just a predicted class label. One of the simplest approaches to compute probability from a single decision tree is to divide the number of examples belonging to class y by the number of examples in the classifying leaf node. It is then up to the data-miners to make the optimal decision y * which minimizes the expected loss or risk t based on the probability estimates and the predefined loss function L(t,y). For many problems, the true label t could be nondeterministic and different values of t may be given when x is sampled repeatedly. Typical examples of loss functions in data mining are 0-1 loss and cost-sensitive loss. For 0-1 loss function, the optimal decision is the most probable label or the label that appears the most often when x is sampled repeatedly. For cost-sensitive loss, the optimal prediction is the one that minimizes the empirical risk.
 compared to parametric or generative methods, is that the hypothesis itself is much less  X  X iased X  in the sense that both the structure of the tree and the internal statistics are estimated from the labeled training data. However, parametric models usually assume that the true and typically unknown probability distribution follows a  X  X articular form X . In other words, the formula is fixed and learning is to estimate the parameters of the formula using maximum likelihood estimation. In practice, given a new dataset with little knowledge about the true form of the target probability distributio n, PET type of methods provide a family of rather unbiased, flexible and convenient solutions, and ind eed their efficacy has been shown in various works, described below.
 The utilities of PETs have been demonstrated in previous works to be highly accurate for many difficult problems. An incomplete list of these applications includes extremely skewed distribution [19], cost-sensitive learning [4,6,7], di stributed learning [20] and concept-drifting data stream mining [21]. Many researchers have previously proposed a large number of PET algorithms, and around ten of them are relatively well-known. Although this large variety provides choice, it also imposes the inevitable confusion in practice. An important question that data miners have to confront with is: given a dataset, which algorithm to choose and what performance to expect and not to expect? Answers to this question are crucial to the successful deployment of PETs in practice. question by carrying out a large-scale empirical comparison of five popular PETs by examining their  X  X earning curves X  on AUC, MSE, and error rate. Those five algorithms include C4.5[1], C4.4[3], and confusion factor tree (CFT)[8], which are representative of single tree PETs, as well as random decision tree (RDT)[7] and probability-averaged baggingPET (instead of majority voting) [3,15] that are often used as ensemble PETs. Each chosen algorithm is exhaustiv ely examined on seventeen real-world datasets, varying in size, feature characteristic and noise level. Unlike some previous works that base algorithm comparison on a  X  X  ixed X  training set size in either cross-validation or training-test splits,  X  X earning curves X  are the fundamen tal empirical basis for comparison used in this paper. To avoid overly simplified conclusion that  X  X ethod A is always better than method B X , multiple performance metrics are being evaluated against each chosen PET. Quite different from cross-validation and training-test splits that use a  X  X ixed X  data size, learning curves represent the generalization performance of different models produced by the same learning algorithm as a functio n of the size of the training set. In other words, from each training set of a different size, a new model is constructed by the same algorithm. By examining the learning cu rves over a number of different datasets, the correlation between performance metric and training set size can be observed and possibly generalized over different datasets. Therefore, the conclusions will be unbiased on training set sizes. of the evaluated probability estimation tree algorithms, we demonstrate that the preference of a probability estimation tree on different ev aluation metrics, i.e., which algorithm is better, can be clearly characterized by the  X  X ignal-noise separability X  of the dataset. The concept of signal-noise separability, originated from signal detection theory and discussed in more detail below, provides a good analogy for two different populations present in every learning domain w ith uncertainty  X  correct identification of information of interest and some other noise factors which may inte rfere this identification. Finally, a practical guide is suggested to data miners on how to choose the most appropriate PETs in light of the problem at hand. In summary, the basic procedure is to first approximate the signal-noise separability of the dataset by applying either RDT or baggingPET, without worrying about other algorithms for the moment. If the estimated signal-noise separability of the dataset is quite high, baggingPET is preferred for ensembles and C4.5 or C4.4 is a good choice for sing le trees. Otherwise when the separability is low, depending on the preference of ensembles or single trees, either RDT or CFT should be selected. information of interest and random matches resulted from noise factors are represented by two different distributions. For example, imagine that a doctor is looking for evidence of a tumor by examining a CT image. Either there is a tumor (signal present) or not (signal absent). Based on the  X  X es X  or  X  X o X  decision made by the doctor, these two distributions are divided into four different regions (Figure 1a), hit, miss, correct reject, and false alarm. Hit (P( X  X es X  X tumor)) and correct rejection (P( X  X o X  X  tumor absent)) are the desired actions. Although relative areas of the four different out comes vary as the decision criterion changes, the separation of the two distributions does not. By changing the decision criterion, the full range of the obtained hit and false alarm (P( X  X es X  X  tumor absent)) rate can be visuali zed through the ROC (Receive Operating Characteristics) cu rve. Figure 1b shows two different ROC curves, each co rresponding to a different signal strength affected by the different overlapping degree with noise demonstrated in Figure 1a. It is evident that the more signal overlaps with noise, the smaller the area under the ROC curve is and vice versa. As a consequence, the fraction of the total area under the ROC curve  X  AUC can serve as a concise index for the separability of signal from noise with respect to the domain under consideration. The larger the AUC score can be accomplished by a model, the more likely that the signal (class of interest) can be separated from the noise (class of no interest) within the underlying domain. As such, this domain is characteri zed as high degree of signal separability. In nature, a domain of high-degree (low-degree) signal separability can be either deterministic (stochastic) or of little or no noise (rather noisy), however, which factor determines the degree of separability is actually unknown. This is typically the case for problems in real-world applications.
 simply focus on utilizing AUC score as a criterion for model comparison, while, to a great extent, the efficacy of AUC as the separability indicator of the underlying domain has been neglected. The only work that we are aware of which extensively employs this property of AUC is [14] and their main objective is to identify the preference between two differ ent types of algorithms -logistic regression and tree induction. Discriminating the superiority among the same category algorithms is obviously more difficult, and to the best of our knowledge, this is the first time that AUC utilized as the separatibilty quantifier for preference id entification solely for the probability estimation tree algo rithms. Therefore, within our context, the signal-noise separability of a dataset is indicated by the maximum AUC score achieved by any of the probability estimation trees involved in this study . typically unknown, the inductive learner looks for the best model  X  within a hypothesis space to best approximate this true but unknown distribution. The estimated probability has none-trivial dependency on  X  , so the estimated probability is best denoted as P(y|x,  X  ). In our case,  X  is each PET. Obviously, the accuracy of the estimated probability is dependant on  X  . trees -a single tree estimator and an ensemble of multiple trees. The single tree is better for comprehensibility. However, it only approximates the true target function with one particular model representation and may not consistently achieve high accuracy. The ensemble is preferred when accurate probability estimation is desired. C4.5, C4.4 and CFT are repres entatives of single PETs. BaggingPETs and RDT are examples of ensembles. membership probability from a decision tree is to use the frequency of the class at the classifying leaf node. If an unlabeled example x falls into a leaf node with M examples in total, of which, m examples belong to the positive class, then the posterior probability is simply decision tree systems, such as C4.5 and CART [2]. It has been noted [3] that frequency-based estimates of class-membership probability are not always accurate and statistically reliable. One reason is that the tree-growing algorithm searches for  X  X ure X  leaves, and tends to produce unrealistically high probability estimates, especially when the leaves cover just a few training examples. These estimates can be smoothed to mitigate these problems. As one of the simplest forms of smoothing, Laplace correction incorporates an equal prior for each class. Various experiments have showed that smoothing techniques can generally improve the performances [3,22]. Importantly, Provost et al [3] pointed out that many heuristics of decision tree induction used for improving classification accuracy and minimizing tree size actually  X  X re biased against building accurate PETs X . Pruning via error reduction is blamed as the culprit. C4.4 [3] is essentially a variation of C4.5 by replacing frequency estimati on with Laplace correction and turning off pruning and collapsing. propose the  X  X onfusion factor tree X  (CFT) by assigning a fixed confusion factor s to each internal node of the tree. This value measures the probability of errors altering the value of a tested attribute at a decision node due to noise introduced in data collection. Therefore, the probabilities from all other leaves as well as the probability of the leaf into which an example falls will contribute to the final probability estimate of the example. The contribution is determined by the number of unequal attribute values along the path leading to th e leaf. Thus, the final class probability estimate for x is a weighted average of the leaf L i and q is the number of unequal attribute values. trees, each determined random ly. To build each tree, a feature is randomly selected fr om remaining features with which to split data at a node. Along a decision path, a discrete feature can only be selected once; however, continuous features can be used multiple times, but each time with a different, randomly chosen splitting value. Features from the training data are used to construct tree structures and the data them selves to update the class probabilities that these trees represent. Th ese probabilities are simply used to keep trac k the number of samples that are classified by each node. For a test point, each tree produces a class probability . Probabilities from all the trees in the ensemble are aver aged to generate the overall class probability estimate. It is pointed out [4] that thirty trees is sufficient and fifty trees may be necessary when the distribution is skewed. On average, the depth of a random tree is about half of the number of the features, whereby distinct features are mo st likely to be selected to maximize diversity of the ensemble. variability. Bagging was introd uced by Breiman [16] to address this problem, and has been shown to often work well in practice [15]. Given a training set z with n data points, bagging generates an ensemble by creating a number of bootstrap samples through uniform sampling with replacement from z. From each bootstrap sample of size n, a tree is built. When a test point is evaluated, instead of the seminal bagging work that takes majority vote, bagging probability es timation trees uniformly average the probability of each class over all trees and the class with the highest pr obability is predicted for classification. metrics . measurement for probability  X  X an king. X  It is not a proper assessment for the  X  X ccuracy X  of probability estimation. MSE or Brier score is thus us ed to handle this problem. Given a data point x i , MSE is calculated as to be 1 if x i of class y, and 0 ot herwise. In addition, member of class y, and N is the size of a dataset. calibration and refinement [1 3]. Calibration measures the absolute precisio n of probability estimation, and refinement indicates how confid ent the estimator is in its estimates. Given two probability estimators of the same calibration capability , the one whose pr obability estimates are closer to 0 or 1 is more desirable. As a summary, calibration of the pr obability estimator can be visualized through  X  X eliability plots X .  X  X  inned X  probability estimates are plotted horizontally vers us the empirical probability calculated as the frequency of test points within the class of interest distributed in eac h bin. Empirical probability is a useful estimate for tr ue probability, since true probability is not available fo r most real-world problems. If the estimator is well-calibra ted, all points fall onto the diagonal line, indicating that the estimated probability equal to the empirical probability based on test samples. Refinement can be illustra ted through  X  X harpness histograms X  by plotting the relative frequency of the examples belonging to the target class versus the different levels of estimated probab ilities denoted by the bins. criterion for evaluating probability estimates [11]. Error rate-related results are included so that this work can be related to past and future research that may only utilize this metric. For binary classi fication problem, error rate is usually obtained by thresholding the predicted probabilities at 0.5. We begin by describing the datasets used in this study as well as the generation of learning curves and the implementation detail of each PET algorithm. reliability plots and sharpness histograms, we confine our study to binary classification problems. As shown in first column of Table 2, eighteen benchmark binary classification problems from UCI repository [10] are selected. These eighteen data sets, ranging from around one hundred to fifty thousands examples, include almost all natural occurring binary classification tasks in this database. From one dataset to another, different feature combinations and various marginal class distributions provide us a variety of domains to examine how the probability estimation tree m odels are affected by the characteristics of different da tasets. Based on the feature property of each dataset, they can be divided into three groups: continuous, categorical and mixed. Continuous datasets refer to the datasets solely of numerical features, and categorical datasets are those which only include nominal attributes. Between them is the group of mixed datasets, whose feature vector includes both categorical attributes and numerical feat ures. This categorization for each dataset is specified in th e second column of Table 2. We treat the minority class of each dataset as the class of interest. utilize all available data points, for each dataset, we first reserve 25% random examples as the test set and use the remaining 75% data points as the pool for training sets. Each training set is then formed by random sampling from this pool, and each time the size of the training set increases by 10%. This procedure is repeated ten times for each dataset. Stratified sampling is utilized to ensure each obtained sample set conforms to the original class distribution. The same training and test sets are used for all of the PETs, and reported results are averages over the ten individual runs.
 connecting the corresponding mean of the ten-time runs for each training set size. For significance test, 95% confidence interval for the mean is used as a measure of the inherent variability of each evaluation metric across different training sets of the same size, and error bars are representing M / t ) 1 M , 2 / (  X   X   X   X  , where  X  is sample standard deviation, M is sample size 10, ) 1 M , 2 / ( upper critical value of the t distribution with M-1 degrees of freedom. And,  X  is set to be 0.05 to achieve the 95% confidence interval. During the evaluation process, two trees are considered to be different for a particular training-set size if the mean of neither falls within the error bars of the other. Section 3. Our codes are based on C4.5 release 8. For C4.5, both the pruned (C4.5P) and unpruned (C4.5UP) trees with default settings are investigated. In addition, we construct FullC4.5 by disabling collapsing and pruning to make it more comparable with C4.4. Four variations of confusion factor tree are built for compar ison, pruned (CFT.LC.P) and unpruned (CFT.LC.UP) CFTs with Laplace correction, as well as pruned (CFT.FE.P) and unpruned (CFT.FE.UP) CFTs with frequency estimation. As suggested in [8], we set the confusion factor to be 0.3. In addition to random decision tree with half depth (RDTH), we also implement RDT with full depth (RDTF). Bagging procedure is applied on C4.4 and FullC4.5 by turns, resulting in baggingC4.4 and baggingFullC4.5 consequently. Each ensemble in cludes thirty trees as base learners. PETs, we first conduct a pr eliminary stud y to identify which variant performs the best for different evaluation metrics as the training size increases. The top performer of each PET algorithm is then selected as the representative for the corres ponding tree model. Due to space limits, we omit the resu lts for this pilot study and Table 1 lists the best vari ant of each PET algorithm involved in the final study with respect to different evaluation metrics. Instead of the specific variants, generic names: C4.5, C4.4, CFT, RDT and baggingPET are used in the rest of the paper. summarized in Table 2, a nd each row corresponds to a dataset. The feature characte ristic of each dataset is identified in the second column , and the third column lists the maximum AUC achieved by any of these five PETs. For each evaluation metric, the results from single PETs and ensembles are examined separately. This is because single PETs often subject to ensembles for almost all of the datasets.  X  X inner AUC X  indicates the algorithm giving the best AUC for the la rgest training set, and for most of the training set sizes this algorithm dominates its counterparts. If two algorithms are liste d, that means for most of the training set sizes, their results are statistically identical according to two-si ded paired t-test at 95% confidence level. The same notations are applied to  X  X inner MSE X  and  X  X inner ErrorRate X . Table 1 : Representatives for each PET algorithm involved in the study Noticeably, if we choose 0.9 as the threshold to distinguish the datasets of high signal separability (Max_AUC  X  0.9) from the low ones (Max_AUC &lt; 0.9), the relative performances of these PETs seem to exhibit fundamental differences in these two groups. This is particularly true when feature set characteristics of each dataset are also taken into a ccount. The datasets of low signal separability are indicated in italic font in Table 2. The Mushroom dataset is excluded from our discussion, since its max AUC is equal to 1, and all PETs quickly achieve the best statistically identical performances with respect to each evaluation metric. set sizes, baggingPET dominates RDT for AUC on categorical or mixed datasets -Chess, Tic and SickEuthyroid. On the other categorical or mixed sets, both algorithms achieve AUC scores which are statistically identical. On th e categorical dataset with a small number of feature values -Chess, RDT even fails to the single trees as shown in Figure 2 . While for continuous datasets -BreastCancer_Wisc, Spam and Spectf, RDT outperforms baggingPET. Among single trees, CFT achieves overall better AUC scores and its strength especially exhibits on datasets of continuous or mixed features. C4.5 never wins its counterparts on a single dataset for AUC. on BreastCancer_Wisc. The datasets for which both algorithms perform statistica lly identically are Spectf, BreastCancer_wdbc and Spam. Th ese four sets are solely of continuous feature values. For continuous datasets, the only exception that baggingPET outdoes RDT is Ionosphere. On categorical or mixed datasets, baggingPET is consistently better than RDT. Among single trees, CFT never wins C4.5 and C4.4 on a single dataset of high signal separability. Its degraded performance of MSE on data set Tic is illustrated in Figure 2. 
With respect to error rate, what we observed is quite similar to that for MSE. The datasets that RDT outperforms baggingPET or both perform statistically BreastCancer_wdbc and Spect f. All of them are continuous datasets, and Ionosphere is again the sole exception. BaggingPET still exhibits advantages on categorical or mixed datasets. Among single trees, C4.5 is obviously the top performer, and CFT fails to C4.5 and C4.4 on all datasets of high signal separability. phenomenon on low signal separability problems is  X  X impler X  X . On these five sets, baggingPET never outdoes RDT on any single dataset w ith respect to all three metrics. Their performances are at most statistically identical for some datasets. Similarly, among single trees, CFT shows clear advantages on these datasets, and it doesn X  X  lose to other single tree methods on any single dataset for all three evalua tion metrics. Three learning curves of different datasets are provided in Figure 3 to illustrate the favorable perfor mances of RDT or CFT on low signal separability datasets. derive the following conjectures: 1. In ensembles, RDT ex hibits overall better 2. With respect to AUC, MSE and error rate, &amp; &amp; &amp; &amp; $ # # 3. For datasets of high si gnal separability, RDT X  X  4. Among single trees, CFT achieves overall better  X  X igging X  into the mechanis ms employed by each PET algorithm. A thorough understanding can help us provide a useful heuristic to choose the most appropriate PET given a new dataset. Indeed, we provide such a heuristic in Section 8. 7.1 Why RDT and CFT Better on AUC? CFT perform the best among single trees for the AUC criteria? The explanation lie s in the two factors which play the critical roles in the computation of AUC. As is well understood, ROC curve shows the ability of an algorithm to rank positive instances as compared to negative instances. The more positive examples are ranked higher than those negative examples, the higher the AUC score is, in genera l. For a random probability estimator, its ROC points would fall on the diagonal line x = y, whereby it has an AUC value around 0.5. All of the five PET algorithms we disc ussed are better probability estimators than a random guesser. None of them generates an AUC score less th an or equal to 0.5 at any training set size for any datase ts in our study. However, this is not enough . The probability esti mates should vary from one test point to another, so that each example can be ranked differently, and the number of unique probabilities is maximized as a result. This is quite important, since only unique pairs of true positive rate and false positive rate, or  X  X lotte d point X  on ROC, resulting from thresholding by differ ent probability estimates are considered in the trapezoidal integra tion rule for AUC calculation. Descriptions on AUC computation can be found in [12]. of unique probabilities that each PET can generate at different training set sizes with respect to each dataset. As a summary, Table 3 presents the average values and standard deviations of the pair-wised Win-Loss-Tie statistics over the ten different training set sizes. For 14.9 out of 18 sets, the num ber of unique probabilities generated by RDT is signifi cantly larger than that of baggingPET. Moreover, baggingPET neither outdoes RDT on a single set nor even at a training set size. Quantitatively, the Win-Loss-Tie record of baggingPET versus RDT is 0-14.9-3.1 with a standard deviation of 0-0.9-0.9. Utilizing distinct random ization methods at different stages of tree construction for ensemble building [5,6], RDT and baggingPET essentia lly can be unified by the framework that Breiman established for random forest [9]. In terms of the strength of the individual tree (S) and the mean correlation among these trees (  X  ), the generalization error of random forest is upper-bounded by ensemble can do better than the boundary given its strength and correlation. Lowering PE * can be achieved by either minimizing  X  or increasing S. In light of this framework, the reason that RDT can generate more unique probabilities than bagg ingPET is because, in general, the member trees of RDT are less correlated with each other than those of baggingPET. Each random decision tree is used more as a data structure to group and summarize the training data rather than a clearly motivated hypothesis in trad itional decision tree learning to maximize/minimize a splitting criterion. However, compared with RDT, individu al trees of baggingPET still extensively use the splitting crite rion at each step of tree expansion. The  X  X andomization process X  itself reflects the fact that the same dataset can be summarized and partitioned in many ways. Si nce randomization does not specify any particular ways and orders on how the training data should be su mmarized, it imposes a weaker  X  X nductive or hypothesis bi as X  than splitting criterion-based tree expansion mechanisms. Statistically, each independently constructed deci sion tree is thus quite uncorrelated, as compared to the member trees of baggingPET. among single trees, followed by C4.4 and C4.5. The strength of CFT for distin ctive probability generation mainly comes from its underlying aggregation technique. By aggregating the weighted probability estimates from all of the leaves in a tree, CF T is able to a ssign different estimated probabilities to differe nt data points, even if these points are classified by the same leaf node. This capability increases the number of unique probabilities that CFT can generate, and the common observation that the number of unique probab ilities generated by a single tree can not exceed the number of leaves in a tree obviously is not true for CFT. 7.2 Why RDT Better on Low-Signal Separability Datasets? separability datasets can be tr aced back to its underlying mechanism. Compared to the other four PETs, RDT is the only algorithm which forgoes any criterion for optimal feature selection at each internal node. Rather than  X  X raining X  on the labeled examples as the other decision tree methods, RDT is more like a structure for data  X  X ummarization. X  When the separability between noise and signal is low, this property not only makes RDT avoid the massive searches or optimizations adopted by other PET algorithms, but also effectively prevents RDT from the danger of identi fying noise as signal or overfitting on noise. In additi on, according to the law of large number, RDT tends to provide an average of probability estimation which appr oaches the mean of true probabilistic values as more individual trees are added to the ensemble [5]. Th us, its estimated probabilities are less biased or less pure than those generated by baggingPET. As shown in Figure 4, when the problem is of low signal separability, RDT can clearly achieve better performance than other four PETs . On the other hand, if the domain is of high signal separability, and especially the domain involves some other charact eristics which may adversely affect the diversity among individual random trees (explained in detail below), the performance of RDT, as illustrated in Figure 5, ma y be degraded and possibly inferior to that of baggingPET. 7.3 Why High-Signal Separability Categorical Datasets with Limited Feature Values Hurt RDT? detrimental on categorical sets with a small number of feature values can be ascribed to its inherent mechanism for random feature selection. Recall from Section 3.3, RDT is a tree ensemble which builds completely randomly. Each member tree is induced by completely ignoring any feature selection criterion that is employed by the other four PET algorithms. Along a decision path, a categorical feature can only be selected once; however, continuous features can be used multiple times, but each time with a different, randomly chosen splitting value. High separability datasets with a small number of categorical values tend to li mit the degree of freedom or diversity that RDT X  X  random feature selection can explore. In other words, such datasets increase the correlation among each independently constructed single tree, and make these trees  X  X ore similar to each other X . As a consequence, the combination of these similar trees has restricted or no benefit on the reduction of variance, thereby the overall generalizati on error could be increased in terms of the upper bound derived by Breiman. Completely forgoing the cr iteria for optimal feature selection, RDT apparently aims at achieving higher tree diversity at the expenses of strength of each member tree, and the overall performance of RDT can be adversely affected if the diversity among member trees is impaired and strength of each tree is lost in the same time. To further verify our conjecture, we discretize each feature of BreastCancer_Wisc dataset into 10, 5 and 2 bins. The AUC learning curves of baggingPET and RDT for the original set of continu ous features as well as these three discretized categorical feature sets are shown in Figure 6. Obviously, as compared to the AUC learning curves for the origin al continuous feature set, the changes resulting from discretization in to 10 bins and 5 bins are trivial, which can be clearly seen in the respective plots. However, for both algorithms, the differences between the original continuous feature set and 2-bin categorical feature set are visually apparent and statistically significant. On the original continuous feature set, RDT achieves significantly higher AUC score than baggingPET across different training set sizes. However, their performances are statis tically identical on 2-bin categorical feature set. RDT has an average AUC reduction rate 5.7% which is higher than the 3.35% reduction of baggingPET. This result provides the experimental evidence that RDT favors continuous datasets. 7.4 Why CFT Favorable on Low-Signal Separability Sets? The reason that CFT performs better on datasets of low signal separability is ma inly due to the motivation and rationale of this algorithm. As stated in Section 3.2, by aggregating the estimated probabilities from all of the leaf nodes in a tree, CFT assumes that noise or error embroiled during data collection process may alter the attribute values, and the data points might have fallen into other leaves. Therefore, the estimated probability for each example should take this factor into account, and the contribution from each of the other leaves is reversely determined by the deviations between attribute values along the decision path leading to the classifying leaf node, and those along the path to the other leave nodes. Although the low signal separab ility datasets in nature can be either noisy or stochastic, this inherent ensemble-like probability aggregation mechanism undoubtedly can help rectify the error in troduced to the probability estimates simply due to the attrib ute noise, whereby CFT demonstrates good performances on this sort of datasets with respect to the three ev aluation metrics. Figure 7 presents such a situ ation in which CFT is superior to C4.4. However, if the underlying domain is of low noise or really deterministic, aggr egation of the estimated probabilities from the other irrelevant leaves will introduce undesirable error or noise into the final probability estimates, and this is typically the case when a more complicated tree model with a huge number of leaves is induced as the trai ning set size increases. As more data points are assimilated, these errors tend to accumulate since the contributions from the other non-target leaves may account for a larger proportion while the target leaf contributes relatively less, thereby its overall probability estimation accuracy is affected reversely. The discoveries in this paper obviously can help data mining practitioners choose the most appropriate PET algorithm given a new problem. The procedure is summarized in Figure 8. Specifically, the choice of which PET to use depends on th e signal-noise separability of the given problem, practical preference for either single trees or ensembles, predefined evaluation metrics, as well as feature vector characteristic s. By taking these factors into account, the reason for e ach algorithm X  X  preference is demonstrated in Section 6, and later further explained and analyzed in Section 7. For example, the superiority of RDT on low signal separability domains is first shown in Section 6.2 and then analyzed in Section 7.2. As a brief  X  X alk-through X  of the procedure, the first step is to estimate the sign al-noise separability of the given problem using all available training points. When the signal-noise separability is high, baggingPET is expected to achieve overall better performances on datasets with categorical feat ure, and RDT is preferred for datasets with continuous features. Among single trees, CFT is preferred over other methods if the criterion is to maximize AUC. While both C4.5 and C4.4 are better choices if either MSE or error rate is to be minimized. For low separability problems, e ither RDT or CFT should be selected regardless of the criterion. This straightforward heuristic is ba sed on extensive empirical studies and thorou gh performance analyses presented in previous sections. It is applicable to sixteen out of the seventeen sets used in this paper, captures the common patterns over a variety of problems, and is independent from training data size. On the utility side, this rule can save the trouble of exhaustive tests to choose the most appropriate PET algorithm for a new data mining problem. The concept of maximum AUC is first introduced in [17] to compare various learning methods on a medical dataset. Recently, Perlich et al [14] applied this criterion in a learning curve study which focuses on identifying the preference of logistic regression and decision trees according to the characteristics of data. Their results show that logistic regression perf orms better on datasets of low signal separability, while the high signal-separability situation is in favor of tree models. Independent of the marginal probability of cl ass membership, maximum AUC is claimed to be a more robust criterion for Bayes rate estimation [14], and its magnitude, as we illustrated previously, can serve as a qu antifier indicatin g the signal-noise separability of the underlying doma in. Although in general, we can not determ ine the actual nature of separabiliy of a domain, ma ximum AUC indeed provides some clues of such inherent characteristics in a sensible way. Before we used this criterion in our study, we didn X  X  expect that even the same category models can be potentially distinguished according to this criterion, and a nature way to expand our study and the work of Perlich X  X  would be: on the datasets of low signal separability, which one is better? logistic regression, RDT or CFT? In [5], error-rate based evaluation of RDT, bagging, single tree and random forest is co nducted on cross-validation based experiment. Ho wever, this pape r utilizes learning curve (that is training set size independent), us es the idea of signal-noise ratio, evalua tes error rate, AUC, MSE, reliability plots, etc, and also digs into the details of a rather exhaustive combination of situations to explain what works and why it works. In both [4] and [6], their main focus is on how PET co uld approximate the true probability, and experiments are mainly conducted on train-test splits. In [23], although several PETs are evaluated through learning-curves, they have different goals from us, and relatively limited explanation is provided on the reasons behind the results. Most recently, Davidson and Fan [24] showed that traditional bagging that uses majority voting doesn X  X  perform well in a number of situations, such as class label noise, multi-label, sample selection bias, and small training sets. It is interesting to study the performance of averaged probability version or ba ggingPET under similar situations. In this paper, we have systematically addressed and answered the important qu estions that:  X  X ithout an exhaustive test, which PET would be the most appropriate algorithm for a new problem? Why a PET perform well in one situation but not so well in another? X  Answers not only help algorithm selection, but also explain the reasons of either good or bad performance, thus serving as a guide to design better algorithms. First, using maximum AUC score achievable by the evaluated probability estima tion tree algorithms, we demonstrate that the pr eference of a probability estimation tree on several wi dely-used evaluation metrics can be effectively determin ed by the  X  X ignal-noise separability X  of the dataset. Based on this categorization, some of the important di scoveries on PETs relative performance are as follows . Between baggingPET and RDT, 1) baggingPET is expected to perform better on AUC, MSE and error rate if the dataset is either of only categorical feature values, or of both continuous and categorical values but the pr oblem has high signal-noise separability, 2) while RDT is pr eferred on eith er datasets of low signal separability regard less of feature vector type, or high-signal se parability datasets with co ntinuous features. Among single tree s, 3) CFT is expected to achieve higher AUC scores th an both C4.4 and C4.5 for most problems, 4) For MSE and error rate, CFT is preferred over C4.4 and C4.5 only on datasets of low signal separability. In order to understand the relative performance and generalize beyond these discoveries, systematic analyses have been provided to expl ain the suitability of the mechanism and heuristic employed by each algorithm under different situations (i.e., high or low signal separability, continuous or categ orical feature set, etc). We have found that, by discarding any feature selection criterion, but gaining diversit y through complete random feature selection, RDT is ex pected to perform well when the signal separability is lo w. On the contrary, by selecting features through information gain, and obtaining diversity from random manipulation of the training set, baggingPET is expected to work better than RDT when the signal-noise separability is high, so that it can benefit from the use of information gain. For single trees, CFT X  X  advantage on datasets of lo w signal separability mainly comes from its unique aggreg ation mechanism that does not rely on a singe leaf node. However, for datasets of high signal separability, its perf ormance is expected to be inferior to C4.4 or C4.5 due to this same aggregation. In addition, we have also shown that the performance of RDT can be sensitive to featur e types, eith er continuous or categorical. The main reason lies in the diversity creation method incorporated within RDT that limits the diversity of trees built from pu rely categorical features. Finally, based on the extensive empirical results and analyses using seventeen datasets, a practical guide is provided to data mining pr actitioners to choose the appropriate PET algorithm gi ven a new dataset without running into the trouble of exhaustive tests. 
