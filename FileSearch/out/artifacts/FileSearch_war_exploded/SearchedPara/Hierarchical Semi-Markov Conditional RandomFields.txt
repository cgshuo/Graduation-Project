 Modelling hierarchical aspects in complex stochastic proc esses is an important research issue in chunking, noun-phrases (NPs) and part-of-speech tags (POS ) are two layers of semantics associated to the chunker. The POS tagger takes no information of the NPs . This may not be optimal, as a desirable to jointly model and infer both the NPs and the POS tags at the same time. Many graphical models have been proposed to address this cha llenge, typically extending the flat hidden Markov models (e.g., hierarchical HMM (HHMM) [2], DB N [6]). These models are, how-hierarchical structures, such as the dynamic CRFs (DCRF) [1 0], and hierarchical CRFs [5]. These models assume predefined structures, therefore, they are no t flexible to adapt to many real-world datasets. For example, in the noun-phrase chunking problem , no prior hierarchical structures are cessfully built and learned.
 In addition, most discriminative structured models are tra ined in a completely supervised fashion Bayesian network. To address the above issues, we propose th e Hierarchical Semi-Markov Condi-undirected Markov chains and allows hierarchical decompos ition. The HSCRF is parameterised as ple, the noun-phrase chunking problem can be modeled as a two level HSCRF, where the top level represents the NP process, the bottom level the POS process. The two processes are conditioned on the sequence of words in the sentence. Each NP generally span s one or more words, each of which has a POS tag. Rich contextual information such as starting a nd ending of the phrase, the phrase same time, like the HHMM, exact inference in the HSCRFs can be performed in polynomial time in a manner similar to the Asymmetric Inside-Outside algorith m (AIO) [1].
 better performance when compared to DCRFs and flat-CRFs. Res ults for the partially observable reasonably well. We also show that observing a small amount o f labels can significantly increase the accuracy during decoding. In noun-phrase chunking, the HSCRFs can achieve higher accuracy than standard CRF-based techniques and the recent DCRFs. Ou r contributions from this paper are efficient generalised Asymmetric Inside-Outside (AIO) alg orithm for partially supervised learning and constrained inference, and iii) the applications of the proposed HSCRFs in human activities recognition, and in shallow parsing of natural language.
 necessary notations and provides a model description for th e HSCRF, followed by the discussion discussions on the implications of the HSCRF and conclusion s are given in section 6. 2.1 The Hierarchical Semi-Markov Conditional Random Fields Consider a hierarchically nested Markov process with D levels where, by convention, the top level is the dummy root level that generates all subsequent Markov chains. Then, as in the generative process of the hierarchical HMMs [2], the parent state embed s a child Markov chain whose states may in turn contain grand-child Markov chains. The relation among these nested Markov chains is For each state s d  X  S d where d 6 = D , the model also defines a set of children associated with less number of sub-states required when D is large, and thus lead to fewer parameters and possibly less training data and time complexity [1].
 at the upper level d  X  1 .
 s a child state s d +1 a new child state s d +1 is returned to the parent s d parent state s d (middle-top), initialisation and ending (middle-bottom) , and state-transition (rightmost). The HSCRF, which is a multi-level temporal graphical model o f length T with D levels, can be i , there is a node representing a state variable x d is an ending indicator e d by imposing the specific constraints on the value assignment of ending indicators: clique types (cf. Fig. 1): In the HSCRF, we are interested in the conditional setting, in which the entire state and ending variables ( x 1: D the part-of-speech tags and the phrases.
 To capture the correlation between variables and such condi tioning, we define a non-negative po-specification are described in the Section 2.2.

State persistence potential R d,s,z
State transition potential A d,s,z
State initialization potential  X  d,s,z
State ending potential E d,s,z Let V = ( x 1: D indices where e d ending indicators ( x 1: D potentials over all ending time indexes i  X  [1 , T ] and all semantic levels d  X  [1 , D ] :  X (  X , z ) = Y The conditional distribution is given as: where Z ( z ) = P 2.2 Log-linear Parameterisation In our HSCRF setting, there is a feature vector f d clique  X  , in that  X  (  X  d , z ) = exp  X  d a and b . Thus, the features are active only in the context in which th e corresponding contextual partition function Z and the potential  X ( . ) . Typical inference tasks in the HSCRF include computing the p artition function, MAP assignment ( x to the asymmetric Markov blanket , which includes the set of variable assignments  X  d,s (the covering arrowed line) containing a smaller asymmetri c blanket (the left arrowed line) and a symmetric blanket (the double-arrowed line). Denote by  X  d,s potentials falling inside the symmetric Markov blanket  X  d,s value assignments of the set of variables inside  X  d,s of products of all clique potentials falling inside the asym metric Markov blanket  X  d,s be a shorthand for  X  d,s decomposition depicted Figure 3, the following recursions arise:
As the symmetric Markov blanket  X  1 ,s partition function can be computed as Z = P MAP assignment is essentially the max-product problem, which can be solved by turning all sum-mations in (2) into corresponding maximisations.
 Parameter estimation in HSCRFs, as in other log-linear models, requires the compu tation of fea-any black-box standard numerical optimisation algorithms . As the feature expectations are rather state-persistence features where f d,s ending at j ;  X  d,s blanket  X  d,s random configuration  X  . help to improve the performance.
 In general, when we make observations, we observe some state s and some ending indicators. Let  X  the auxiliary variables such as  X  d,s these observations. For example, computing  X  d,s is an  X  x d computation of that variable.
 As an example, we consider the computation of  X  d,s they must be s , (b) if there is any observed ending indicator  X  e d indicator  X  e d observed, then  X  e d When observations are made, the first equation in (2) is thus re placed by ity recognition in Secion 5.1 and shallow parsing in Section 5.2. 5.1 Recognising Indoor Activities In this experiment, we evaluate the HSCRFs with a relatively small dataset from the domain of in-sequences of discrete locations.
 First, we examine the fully observed case where the HSCRF is c ompared against the DCRF [10] at HSCRF outperforms the DCRF.
 of 80.2% and 90.4% at levels 2 and 3, respectively.
 degraded learned models. Such degraded models (emulating n oisy training data or lack of training rate to go up considerably. 5.2 POS Tagging and Noun-Phrase Chunking In this experiment, we apply the HSCRF to the task of noun-phr ase chunking. The data is from the CoNLL-2000 shared task 3 , in which 8926 English sentences from the Wall Street Journa l corpus are used for training and 2012 sentences are for testing. Eac h word in a pre-processed sentence is labelled by two labels: the part-of-speech (POS) and the n oun-phrase (NP). There are 48 POS labels and 3 NP labels (B-NP for beginning of a noun-phrase, I -NP for inside a noun-phrase or O for others). Each noun-phrase generally has more than one wo rds. To reduce the computational our HSCRFs we do not have to explicitly indicate which node is the beginning of a segment, the NP label set can be reduced further into NP for noun-phrase, and O for anything else. HSCRF+POS and DCRF+POS mean HSCRF and DCRF with POS given at t est time, respectively. We build an HSCRF topology of 3 levels, where the root is just a dummy node, the second level has 2 NP states, and the bottom level has 5 POS states. For comp arison, we implement a DCRF, a SCRF, and a semi-Markov CRF (Semi-CRF) [8]. The DCRF has gri d structure of depth 2, one small, we are able to run exact inference in the DCRF by collap sing both the NP and POS state spaces to a combined state space of size 3  X  5 = 15 . The SCRF and Semi-CRF model only the NP process, taking the POS tags and words as input.
 Semi-CRF also include the POS tags. Words with less than 3 occ urrences are not used. This reduces rise to about 32K raw features. The model feature is factoris ed as f ( x Although both the HSCRF and the Semi-CRF are capable of model ling arbitrary segment durations, online stochastic gradient ascent method. At test time, sin ce the SCRF and the Semi-CRF are able inference. Instead, we also give the POS tags to the DCRF and H SCRF and perform constrained significantly.
 Let us look at the difference between the flat setting of SCRF a nd Semi-CRF and the the multi-level setting of DCRF and HSCRF. Let x = ( x distribution Pr( x | z ) = Pr( x by finding the maximiser of Pr( x not make use of it at test time. However, Pr( x NP distribution.
 of the noun-phrases since this data has POS tags. Without POS tags given at test time, both the HSCRF and the DCRF perform worse than the SCRF. This is not sur prising because the POS tags are always given in the case of SCRF. However, with POS tags, t he HSCRF consistently works better than all other models. The HSCRFs presented here are not a standard graphical model since the clique structures are not not available in the DBN representation of the HHMMs in [6]. T hus, the segmental nature of the HSCRF thus incorporates the recent semi-Markov CRF [8] as a s pecial case [11]. Our HSCRF is related to the conditional probabilistic conte xt-free grammar (C-PCFG) [9] in the same way that the HHMM is to the PCFG. However, the context-fr ee grammar does not limit the set of approximate inference techniques available in graph ical models.

