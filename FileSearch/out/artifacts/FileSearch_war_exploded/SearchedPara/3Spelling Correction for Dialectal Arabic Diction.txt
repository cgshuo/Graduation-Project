 C. ANTON RYTTING, DAVID M. ZAJIC, PAUL RODRIGUES, SARAH C. WAYLAND, CHRISTIAN HETTICK, TIM BUCKWALTER, and CHARLES C. BLAKE Spelling correction is an important and non-trivial task that supports word processing, search, and many natural language processing (NLP) applications. We address the problem of Arabic spelling correction for non-native speakers of Arabic, in the context of online dictionary lookup for words spoken in dialect. A number of factors influenced our approach to this task, including (1) the fundamental goal of tailoring the tool to second-language (L2) learners of dialectal Arabic, (2) the need to build the tool without access to adequate training corpora for Arabic spelling correction, (3) a requirement that it perform correction in the absence of context, and (4) a desire to appropriately rank order options.
 Section 1 will review background and related work in Arabic spelling correction. In Section 2 we provide evidence that Arabic has relatively high orthographic density compared to English. This means that on average an Arabic string will have a larger set of equally close valid Arabic words than an English string will have. We argue that in order to provide concise lists of proposed spelling corrections, we must develop finer-grained ranking methods than unweighted edit distance.

In Section 3 we describe an Arabic spelling correction system, Did You Mean...? (DYM), that uses insights into the linguistic characteristics of spelling errors made by L2 learners of dialectal Arabic to improve the ranking of proposed spelling corrections. DYM uses a weighted finite state transducer (FST) to calculate the most probable Ara-bic words in a dictionary given an Arabic query. The FST is specialized to the context of L2 learner dictionary lookup by setting the weights on different string edit oper-ations, and by augmenting the lexicon with anticipated spelling variants. Section 3 describes DYM in more detail, and Section 4 evaluates the effect of edit operation weights and lexicon augmentation on mean reciprocal rank (MRR). We show that both factors significantly improve MRR. In Section 5 we outline directions for extensions of this approach, and in Section 6 we summarize our conclusions. Arabic is a diglossic language; this is one reason it is so difficult for second language learners. Diglossic languages have two varieties, one of high prestige used in for-mal and official settings, and one of low prestige used in more familiar situations [Ferguson 1959]. The high prestige variety of Arabic, referred to as Modern Stan-dard Arabic (MSA), is based on an earlier classical form of Arabic and is taught in schools in addition to being used in newspapers, news broadcasts, and most literature. The low prestige variety of Arabic is a modern vernacular that can differ considerably from one Arab region to another. This dialectal form is spoken as a first language by native Arabic speakers, but is rarely used in written form. MSA forms the basis for the majority of Arabic foreign language courses, and most learners of Arabic in academic settings are taught little or no dialectal Arabic. The low prestige of the di-alects results in a scarcity of dialect teaching materials. Further, the general absence of orthographic standards for writing the dialects X  X oupled with linguistic variation among the dialects X  X an make it difficult for second language learners to recognize dialect-specific syntax, words, and sounds.

Extensive research has been produced on spell-checking systems designed for speak-ers of English. Damerau [1964] used an unweighted edit distance system and was able to correct words that had no more than one error. Church and Gale [1991] introduced a spell corrector that used a noisy channel model that perturbed misspellings according to a confusion matrix of weights that was created by harvesting errors from newswire text. The perturbation that had the lowest weight and was found in any of five dictio-naries was chosen as the spell correction. This system was able to correct words that had more than one error per word. Brill and Moore [2000] augmented Church and Gale [1991] by using a spelling error corpus to estimate the probabilities of string-to-string edits. Toutanova and Moore [2002] improve upon this model by using pronunciation models. All this pioneering interest in spell correction used the English language, primarily due to the size of the English-language user market. This market-based motivation resulted in extensive error corpora resources that have allowed English spell checking to get far ahead of research in less market-friendly languages, such as Arabic.
 Solutions for Arabic spelling correction have been proposed by Shaalan et al. [2003], Rachidi et al. [2003], Zribi and Ben Ahmed [2003], and Hassan et al. [2008]. How-ever, only the work of Shaalan et al. [2003] explicitly aims to correct dialectal Arabic (Egyptian); the other solutions focus on Modern Standard Arabic (MSA). We know of no previous work on Arabic spell-checking for second language (L2) users working with dialects. Our goal was to develop a spelling correction tool for dialectal Arabic with the goal of assisting L2 learners and language professionals who want to look up words in a colloquial Arabic-to-English dictionary.

Our approach addresses issues that affect nonnative speakers of Arabic; this differs from traditional approaches that are largely concerned with correcting the spelling mistakes of native speakers. Learners of a language tend to make more errors per word than a native speaker [Boyd 2008]. Many spell correction systems utilize spelling error word lists to train a noisy-channel model [e.g., Boyd 2008; Brill and Moore 2000; Church and Gale 1991]. These solutions work well, but are often only practical for resource-rich languages such as English, French, or Japanese. There are no corpora of spelling errors of sufficient size for training a spell-correction system for Arabic, let alone dialectal Arabic or non-native Arabic. For this reason, our approach cannot rely on spelling error training data.

Because the focus here is on spell correction for dictionary lookup, the forms to be corrected occur in isolation. We cannot, therefore, use a word X  X  phonological, syntactic, or semantic context to disambiguate between possible corrections. This increases the number of candidates to be displayed and ranked.

There are also language characteristics of Arabic that increase the number of candi-dates. Zribi and Ben Ahmed [2003] note the difficulty in Arabic spelling correction due to the large number of plausible candidates. One way to quantify this language char-acteristic is orthographic neighborhood density [Coltheart et al. 1977]. Orthographic neighborhood density is a measure of how many valid words in a language differ from a query word by only a single letter. Coltheart X  X  N is the standard measure of ortho-graphic neighborhood density, and is calculated by counting the number of valid words that can be created when you substitute a single letter of a word with another letter from that language. Orthographic density metrics have been used to describe phenom-ena of lexical access [e.g., Andrews 1997; Coltheart et al. 1977; Grainger 1990; Spieler and Balota 2000], and auditory word recognition [e.g., Ziegler et al. 2003]. We use an orthographic density metric to motivate the need for more sensitive ranking schemes for Arabic spelling correction.
 Researchers have followed a variety of approaches for ranking the candidates. Rachidi et al. [2003] used word frequency in a corpus to rank correction candidates, while Hassan et al. [2008] as well as Zribi and Ben Ahmed [2003] use word context to select the best candidates. Because our system must suggest alternatives in the ab-sence of context and because frequency counts are not currently available for dialectal Arabic, our rankings had to be based on a different type of data.

Levenshtein edit distance is one approach that can be used to generate and rank alternative spelling corrections. Levenshtein edit distance [Levenshtein 1965; Wagner and Fischer 1974] counts the number of string operations needed to change a word into a suggested alternative, assigning equal weights for character insertions, deletions, or substitutions.
 In this section we motivate the need for finer-grained ranking schemes for Arabic spelling correction than in English spelling correction. We present studies that show that in both Arabic and English, shorter words have higher orthographic neighborhood densities; and that Arabic has shorter average word length than English. This sug-gests that ranking schemes based on edit distance will give more tied scores for Arabic spelling correction candidate lists than for English lists. We compare the average character lengths of word lists derived from Arabic and Eng-lish dictionaries. The Arabic word list consists of the citation and inflected forms from A Dictionary of Iraqi Arabic [Woodhead and Beene 2003] with spaces and punctuation removed. A few further normalizations were also performed: stem-initial variants of alif (  X  X  X  ) were normalized to bare alif (  X  ), and  X  (  X  X lif maqsura )and  X  (final yaa X  ) were collapsed into a single representation, as were  X  ( taa X  marbuta )and  X  (final haa X  ). 1 This created a list of 32,419 words with an average length of 4.52 characters.

The English calculation used a list of all unique citation forms from the 1913 edi-tion of Webster X  X  English Dictionary [Porter 1913]. We removed punctuation and space characters from words and standardized word capitalization. Words with ligatures, umlauts, or cedillas were either standardized to current orthography or removed from the word list. This resulted in 91,751 words, with an average length of 8.64 charac-ters. We observe that Porter [1913] has many citation forms that are inflected forms. Running the experiment on only uninflected forms would provide a more direct com-parison. Short words (one to three characters in length) make up nearly one-fifth of the entries for the Arabic dictionary, whereas for English less than 0.25% of the words are this length. Figure 1 shows the distribution of word lengths for both the English and Arabic word lists. We extend the notion of orthographic neighborhood density to include insertion and deletion operations, as well as substitution operations. Recall that insertion, dele-tion, and substitution are the string edit operations used in Levenshtein edit distance. To measure the orthographic density of each language X  X  lexicon, we first determined the number of valid words (orthographic neighbors) that can be created within one Levenshtein operation of every word in a word list. These orthographic neighbor counts are then averaged together across all words in the word list to create a den-sity number for that lexicon. For the 32,419 Arabic words in the Woodhead and Beene [2003] dictionary, this yielded an average of 11.69 orthographic neighbors. For the 91,751 English words in the Porter [1913] dictionary, each word had an average of 1.76 orthographic neighbors.
 To further examine the relationship between word length and orthographic density, Levenshtein edit distance was then calculated on several subsets of the list. Each subset was based on the length of the source words. The pool of available orthographic neighbors continued to be the entire word list, and since the calculations included insertion and deletion operations, the words were compared to words of equal length, words with one character more, and one character less. The results are shown in Figure 2, where it is clearly shown that for both English and Arabic, short words have higher orthographic density.

From a usability standpoint, this presents a challenge: in order to minimize the cognitive load on a user, we wish to minimize the number of choices presented while still including the intended query. In addition, we wish to provide a principled ranking so that the words most likely to have been the user X  X  intended query appear at the top of the presented list. An added complication is that the greater the lexical density within a dictionary or wordlist, the greater the number of tied rankings. Unless some other ranking mechanism is found, this will result in a greater cognitive load on the user, who will be forced to choose from a larger list of items with less meaningful ranking distinctions.

The greater lexical density of Arabic suggests that standard Levenshtein edit dis-tance is likely to be a less effective method for ranking spell correction alternatives than it is for English, particularly for very short words (1 X 3 characters long). This suggests that it is worthwhile to pursue a more fine-grained approach to ranking the returned list of suggested corrections in an Arabic spell checker. In this section we describe our proposed ranking system, Did You Mean...? (DYM). This system uses a weighted Finite State Transducer (FST) with modules for sound-based confusions, confusions based on orthographic rules, and rules that differentiate dialectal speech from MSA. Transliteration errors and keyboard proximity errors are discussed further in Rytting et al. [2010]; while these issues are of practical impor-tance, the findings may be limited to specific transliteration systems and keyboard layouts. 2 This article abstracts away from these issues so as to examine in more detail those issues that are likely to affect all English-speaking learners of spoken Arabic, independent of the specific keyboard used. Errors based on visual similarity will be addressed in future work; the emphasis here is on spoken dialect, not written vari-eties of Arabic.

Each module is composed with a finite state machine (FSM) that accepts all strings in a lexical pool, populated from words in a dictionary. The FST calculates which words from the lexical pool have the lowest cost in edit operations when converted into a query term. The words with the lowest costs are the ones that the system proposes as spelling correction candidates, ranked by cost. Note that in a weighted FST, different operations have different costs. Our weighting scheme is based on pedagogical insights into the spelling errors made by L2 Arabic learners derived from surveys. The intuition behind the sound-based confusion module is that some substitutions are more likely (i.e., lower cost) than others because of L2 learner ability to distinguish phonemes that are outside their native language. The intuition behind the orthographic rule and dialectal speech modules is to improve performance by anticipating likely variant spellings and lowering their costs.

We compare our best DYM configuration to a baseline that gives equal cost to all edit operations (i.e., Levenshtein edit distance), and that does not anticipate any variant spellings. We find that the best DYM configuration has significantly higher Mean Reciprocal Rank (MRR) scores on a test corpus of Arabic query strings and intended words than the baseline, and that both sound-based confusions and variant spellings contribute to the improvement. English learners of Arabic have to learn how to produce and perceive new phonemes, and to phonologically distinguish several sounds that would be considered allophonic in their native language. Phonetic distinctions, like those between the voiceless uvular stop /q/ and the voiceless pharyngeal fricative / -h/ must be learned, as well as the ability to contrast them with phonemes that do appear in their L1, like the voiceless velar stop /k/ and the voiceless glottal fricative /h/. Velar fricatives such as /x/ and /  X  /havetobe learned and distinguished from each other as well as from other acoustically similar sounds such as /k/ and /g/.

While these are known to cause confusions for English listeners, quantitative data on the severity or frequency of these confusions is lacking in previous literature. As collecting a corpus of learner errors is costly, we have opted to approximate this quan-titative data via surveys. Surveys are often cheaper and less time-consuming to collect than error corpora for low-resource languages, and we argue that they provide an ad-equate basis for an initial spell-correction system, which may subsequently be refined as more data become available (see Future Work). While this survey-based approach is not motivated by theoretical phonology, it has the advantage of highlighting sounds that students and teachers are consciously aware of being problematic, and expect to receive help with.

Surveys. We created a list of anecdotally well-known confusions, and asked a native-speaking teacher of undergraduate level Arabic to rate on a five-point scale (1 = lowest; 5 = highest) how often her students struggled with each confusion. We also provided space on this survey for the native Arabic instructor to add any additional confusions not included on our original list. In addition, we also administered this survey to several current or former students of Arabic, in order to elicit their self-report of the severity for these confusions, using the same scale. 3
Method. The results of these surveys were used to create a weighted variant of stan-dard Levenshtein edit distance. As in standard edit distance, the allowable operations for changing a string consist of symbol insertion, symbol deletion, and substitution of one symbol for another.

Our system differs from standard edit distance in two ways: (1) we disallowed some operations for specific letters, and (2) weighted the remaining alternations with dif-ferent costs so as to provide a more fine-grained ranking mechanism for suggested alternate strings to the query. In standard edit distance, each of these operations are given an equal cost. In our system, the cost was assigned based on the scale provided by our survey. Confusions receiving an average rank of 5 (the highest) were given a cost of 1. Each point lower on the scale increased the cost by 0.5, until items ranked as 1 (lowest) were given a cost of 3.

In response to feedback on a prototype system, a few additional confusions were added, including /f/  X  /s/, /f/  X  /x/, and the insertion of /l/ to correct for a learner X  X  possi-ble hypercorrection of Arabic  X  X un letter X  assimilation.

The confusions presented in the survey were not assumed to be necessarily symmet-ric. However, as it seems likely that students will sometimes make hypercorrective er-rors to overcompensate for their listening errors, the reverse of each substitution was also added to the confusion matrix (if not already present), with an additional weight of 1.25. Insertions and deletions of specific letters were also inverted (as deletions and insertions, respectively).

Finally, one level of transitivity was added to the system. For every pair of opera-tions A  X  B (with cost x )andB  X  C (with cost y ), the composed operation A  X  C (with cost x + y ) was added if it was not already found (with lower cost) in the confusion matrix.

Implementation. The confusion matrix described above was implemented as a weighted finite state transducer using the AT&amp;T FST toolkit. Since the original data from our survey provided no information about context for various confusions, we as-sumed for simplicity that each operation was equally likely in any context. Thus our FST used only a single state. The final step of adding a level of transitivity was con-ducted by composing the FST resulting from the previous steps with itself. If any mul-tiple arcs were found that encoded the same operation, only the one with the lowest cost was kept. All others were removed.

The resulting confusion matrix can be found in Appendix A, and the performance of this confusion matrix (on its own) is examined in Condition 1 of our experiments (below). In addition to the acoustic (sound-based) confusions discussed above, Arabic orthog-raphy is not always predictable by pronunciation alone. One of the most common ambiguities is the final /a/ sound, which can be written  X  ( taa X  marbuta )or  X  (  X  X lif ), or (less commonly) as  X  (  X  X lif maqsura ). The complex orthography of hamza (  X  X  X  X  X  X  ) illustrates the mapping of a single phoneme, the glottal stop, to multiple graphemes.
Implementation. In order to correct for possible misspellings due to this ortho-graphic ambiguity, a variant of our confusion matrix was tested with confusions (sub-stitutions) between  X  ( taa X  marbuta )and  X  (  X  X lif ) added. Since  X  ( taa X  marbuta ) can only occur word-finally, we restricted this confusion to the end of the word. The effect of this additional rule is examined in Condition 2 of our experiments, below. The other orthographic ambiguities described above (such as representation of hamza and use of  X  X lif maqsura ) did not need to be addressed directly in this module since they were al-ready accounted for either through the normalizations used in representing dictionary entries and queries or by the sound-based confusions described in Module 1. In addition to the orthographic ambiguity between  X  (  X  X lif )and  X  ( taa X  marbuta ) (found in all written dialects of Arabic), there is an additional set of sound and or-thographic confusions specific to the Iraqi dialect.

The Iraqi dialect uses three sounds not found in Modern Standard Arabic: /p/, /g/, and /  X  /. While characters corresponding to these sounds exist in an  X  X xtended X  Perso-Arabic alphabet used in writing Farsi (and found on Farsi keyboards), we assume that users of an Iraqi Arabic dictionary will generally only have access to a standard Arabic keyboard. It is not clear how naive (non-native) users will choose to represent these sounds when typing queries.

While /p/ is unlikely to be problematic X  X e assume it will be represented with the letter  X  ( baa X  ) X  X he /g/ sound may be represented as either  X  ( qaf ), due to etymological correspondences with MSA between /g/ and /q/, or as  X  ( kaf ), due to visual similarity of the extended Farsi letter for /g/ (  X  ) with the Arabic letter  X  . Likewise, the /  X  / sound may be represented as  X  or  X  (for etymological or cross-dialectal reasons) or with  X  (due to visual similarity of the extended Farsi letter  X  for /  X  /).
Also, the Iraqi dialect often deletes certain phonemes in certain contexts that are written in MSA. For example, a word-initial vowel will typically be written with an  X  (  X  X lif ) in MSA, but is often not pronounced in the spoken Iraqi dialect.

Implementation. Unlike Sound Confusions and Complex Orthographic Rules, which we handled via rules that applied for all words with a minimum of contextual restric-tions, Dialect Based Confusions is handled by targeting words with the problematic sounds /g/ and /  X  /, generating potential alternative spellings for them, and mapping these potential alternate spellings to a single reference spelling used for indexing the form with an electronic dictionary.

For example, alternate spellings were added for words containing /  X  / so that this sound could be spelled with  X  ,  X  ,or  X  . Similarly, alternate spellings were added for words containing /g/ so that it could be spelled  X  or  X  .

In addition, a naive pronunciation-to-orthography transliterator was used to create potentially spurious spellings of words from their pronunciation fields in the Iraqi dictionary. These forms were then mapped to the reference spellings for these words (i.e., the spellings that appeared in the lexicon).

This module, like modules 1 and 2, is implemented as a weighted FST built using the lexcomplex tool from AT&amp;T X  X  lextools package, allowing for easy composition with modules 1 and 2. Weights within the interval (0,2) were assigned to all mappings from nonstandard spellings to the corresponding reference spelling listed in the dictionary. Hence, if an exact match for an alternate spelling is found for a query, the mapping to the reference spelling will be ranked above most single-character edit operations (excepting those with weights of 1 or 1.5 as shown in Appendix A). The effect of this module is examined in Condition 3 of our experiments, below. As we know of no existing spelling error corpus for Arabic, we simulated error data by generating a corpus using a noisy-channel model for error production [Shannon 1948]. For the noise model, we learned the kinds of errors a nonnative speaker could make from a corpus of transcribed Arabic speech elicited from learners of Arabic during an imitation task [Sethy et al. 2005]. The elicitations were common MSA greetings and simple conversation uttered by native speakers of Levantine Arabic and Iraqi Arabic.
Probabilities for insertions, deletions, and substitutions were captured for each er-ror by taking the number of times the error appeared between the surrounding phones and dividing by the number of times that phonetic context appeared in the corpus. For substitutions, the probability of specific sound transformation that occurred in that context was learned as well.
 The source words were randomly chosen citation forms from A Dictionary of Iraqi Arabic [Woodhead and Beene 2003]. A random error point is chosen in the word, and a weighted error operation (insertion, deletion, substitution) is applied depending on the left and right context of that point. If the error operation is an insertion or deletion, the error is applied based upon the left and right context. If the operation is a substitution, the context is defined as the specific transformations that were made between the left and right characters surrounding that point. Like the high-level error operations, the substitution operations were weighted to produce the most likely substitution errors in that context. If no error could be found in that context, the point was abandoned, and another random position attempted. If no error could be produced, the word was removed from the evaluation set.

While this is a novel way of building an error corpus for a less-resourced language, it is not the most ideal method for attaining dictionary lookup errors due to the use of mispronunciation errors (rather than isolating listening errors) and the extrapolation of errors to novel words. Furthermore, by using insertions, deletions, and substitutions as the methodology, we constructed our evaluation corpus using a similar methodology to that used to construct the FST modules for our tool. However, the evaluation corpus crucially differs from the original surveys used to build the confusion matrix in two respects: first, the evaluation corpus was based on actual errors of nonnative speakers, rather than self-or teacher-reported types of errors; second, surrounding context was taken into account in the evaluation, whereas the sound-based confusion matrix in the tool itself uses no context. Approaches to produce error corpora that isolate listening errors, and elicit errors over a larger set of word types, are discussed in the section on Future Work. Rytting et al. [2010] showed that a confusion matrix custom-designed for particular types of errors could outperform a standard Levenshtein edit distance baseline on mean reciprocal rank (MRR). For a single query, the reciprocal rank (RR) is the in-verse of the rank of the correct answer. For cases where the correct answer has a tie cost with other answers, we take the RR for the query to be the average inverse rank of all the tying answers. The RR is zero if the system does not propose the cor-rect answer as a candidate. MRR is the average RR over a set of queries. We would like to know which components contribute to the increase in MRR. In this section, we describe various tests of our confusion matrix that tease apart the contributions of various components.

All tests are run on the same evaluation corpus. The corpus was split into two parts: a development corpus of 600 words and a test corpus of 750 words. Only words where there was a spelling difference between the query and the target were tested, leaving 326 distinct words in the development set, and 434 in the test set.

Inspection of the results on the development data set showed that the weights of the sound-based confusion matrix were biased to favor query corrections with multi-ple operations (of a few types), rather than first considering corrections with just one operation. In order to correct for this bias, an additional cost of 3 was added to each operation (substitution, insertion, deletion) on the sound-based confusion matrix. We refer to the matrix with the uncorrected weighting scheme as the Sound-based Confu-sion Matrix, and the matrix with the cost correction of 3 as the Optimized Sound Based Confusion Matrix. Thus we evaluate three conditions with respect to the weighting schemes: No Weights Levenshtein, Sound-based Confusion Matrix, and Optimized Sound-based Confusion Matrix.

With respect to variant spellings, we consider three conditions: No variant spellings (i.e., only the contents of the lexicon, labeled Lex), lexicon plus variant spellings de-rived from orthographic rules (labeled Lex+OR), and lexicon plus variant spellings derived from orthographic rules and dialectal variation (labeled Lex+OR+DV).
The MRR for DYM under the different weighting schemes and variant spelling schemes are shown in Table I. We consider the No Weights Levenshtein, Lex con-dition to be the baseline. All conditions are significantly different from the baseline except the condition with Sound-based Confusion Matrix and Lex. The method for detecting significant difference is described below.

In order to extend our conclusions beyond the particular set of query terms, we use a mixed-effects model to analyze the effect of the experimental conditions on MRR. A mixed-effect model accounts for the variation due to query term, that is, that spelling correction is harder for some misspelled words than for others. The dependent vari-able for this evaluation is the RR of the correctly spelled word for a query term. The independent variables are weighting scheme and variant spellings.

We analyze the evaluation data using the lme4 and languageR modules of the R statistical computing tool. A linear mixed-effects (LME) model [Bates 2007] was used to calculate the t -values for the effect of weighting scheme and variant spellings on RR, with a random effect for adjustments conditional on query term. LME reports a t -value for the baseline condition, and for other conditions reports the effect on the t -value. For example, in Table II the test-value of the baseline is 30.046, and the effect of switching the weighting scheme to sound-based confusion matrix is -1.186. In order to determine if this is a significant difference, we used Markov chain Monte Carlo (MCMC) sampling [Andrieu et al. 2003]. MCMC is an algorithm for simulating a large sample set from a probability distribution, and it reports a p -value, that is, the probability of making a Type I error. We consider p &lt; 0.05 to indicate a statistically significant difference in t -values.
 5.2.1 Effect of Weighting Scheme Across Variant Spelling Conditions. We first consider the effect of weighting scheme under different variant spelling conditions. Table II shows the LME t -values and MCMC p -values for the values of weighting scheme under the three variant spellings conditions, using Levenshtein as the baseline. Under Lex, we see that the difference between No Weights Levenshtein and Sound-based Confusion Matrix is not significant, however the difference between the No Weights Levenshtein and Optimized Sound-based Confusion matrix is significant. Under Lex+OR, both Sound-based and Optimized Sound-based Confusion Matrix perform significantly better than Levenshtein. Under Lex+OR+DV, there is no significant difference in performance due to weighting scheme. We observe that weighting scheme has the strongest positive effect under Lex and no significant effect under Lex+OR+DV, which we interpret to mean that choice of distance metric makes the most difference when DYM does not account for likely spelling errors. 5.2.2 Effect of Variant Spelling Conditions Across Weighting Schemes. Next we consider the effect of variant spellings under different weighting scheme conditions. These results are shown in Table III. Under No Weights Levenshtein and both Sound-based Confu-sion Matrices, Lex+OR and Lex+OR+DV have significantly higher performance than Lex. Note that the weakest improvement from variant spellings is shown for the Op-timized Sound-based Confusion Matrix, which we interpret to mean that augmenting DYM with likely spelling errors makes the most difference when the poor weights or no weights are used on edit operations. We see from these results that there are two pathways to improvement on the baseline X  X ither through more fine-grained rankings based on privileging more likely errors over less likely errors (as in our proposed sound-based error ranking system) or by anticipating likely alternate spellings for words in the dictionary. The two strate-gies differ in their requirements: the first requires access to students or teachers of the language with accurate intuitions as to likely student errors. It also requires some tuning of the parameters on development data in order to obtain the best trade-off between weight ratios. 4 The second strategy requires knowledge of areas where the orthography departs from a simple phonetic representation. It also requires access to pronunciation fields and orthographic fields for each word in the dictionary, as well as ana  X   X ve phoneme-to-grapheme system whose errors correspond to those that a student is likely to make.

Across language pairs, the relative contributions of each component will likely de-pend on whether the sounds or the spelling of the language poses more difficulties and the degree to which these overlap. For English learners of Iraqi Arabic, the use of both strategies together provides the best performance, but not significantly better than using spelling variants alone. The Sethy et al. [2005] corpus provides a useful starting point for evaluation of our dictionary retrieval system on production errors created by nonnative learners of a colloquial Arabic, but we are interested in testing our system on more than the types of errors that can be observed by elicited imitation, and we are interested in testing the system without using a pronunciation-to-orthography converter. To address our concerns, we are working on the creation of a sizeable Arabic spelling error corpus through several data collection approaches. These approaches may be used to cross-validate our elicited imitation evaluation approach presented here, to evaluate and extend our existing FST, to ascertain the spelling ability of Arabic learners at various levels, and may provide data points that could be used for a cost/benefit comparison to weights elicited by our survey-based approach.

Our Arabic error corpus will consist of Arabic samples elicited using three different data collection methods. First, we are constructing a learner corpus by administering spelling tests with auditory presentation of word lists that are designed to elicit errors in all Arabic consonants. Additionally, we plan to collect native speaker errors through online typing games. Third, we plan on surveying native speakers on their stereotypes of L1 English Arabic language learners.

Additionally, we are interested in building Did You Mean...? systems for other dialects of Arabic, including MSA. A DidYouMean...? system for MSA would allow us to make direct comparisons against existing Arabic spell correctors. We have demonstrated a new spell correction system for Arabic, which we believe is the first spell correction system designed for the L2 learners of dialectal Arabic. The system is based on the particular kinds of mistakes native English speakers make while learning Arabic. It is built without spelling error corpora. This is a resource-light approach to spell correction that may be useful for other less resourced languages.
We have shown why spell correction is an inherently more difficult problem in Ara-bic than in English. We did this by comparing orthographic density as it effects lexical retrieval in dictionaries using a popular language-independent string comparison al-gorithm. We use the same algorithm as the baseline comparison for our evaluation and show our approach performs significantly better.
