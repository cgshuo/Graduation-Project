 Andr  X  asKornai (MetaCarta Inc.) Springer (Advanced information and knowledge processing series, edited by Lakhmi Jain), 2008, xiii+289 pp; ISBN 978-1-84628-985-9, $99.00 Reviewed by Richard Sproat and Roxana G X   X rju University of Illinois at Urbana-Champaign For readers of traditional textbooks such as that of Partee, ter Meulen, and Wall (1990), the term  X  X athematical linguistics X  denotes a rather narrowly circumscribed set of issues including automata theory, set theory, and lambda calculus, with maybe a little formal language theory thrown in. Kornai X  X  contribution is refreshingly different in that he treats, in this relatively compact volume, practically all areas of linguistics, phonetics, and speech and language processing.
 the central methods and concepts of linguistics that are made largely inaccessible to the mathematician, computer scientist, or engineer by the surprisingly adversarial style of argumentation ...and the proliferation of unmotivated notation and formal-ism...alltoooftenencounteredinresearchpapersandmonographsinthehumanities X  (page viii). There is no question that much of what passes for rigor (mathematical and scientific) in linguistics is a joke and that there is clearly a need for any work that can place the field on a more solid footing. It also seems likely that Kornai is the only person who could have written this book.
 lays the groundwork and identifies the potential audience, and a concluding chapter where Kornai reveals his own views on what is important in the field, which in the interests of balance he has largely suppressed throughout the book. Chapter 2 is also introductory in that it presents basic concepts of generation (via a ruleset), axioms, and string rewriting.
 and speech, starting with phonology in Chapter 3. This chapter introduces the notion of phonemes, distinctive features, autosegmental phonology, and computation using finite automata. Kornai offers many details that are of course lacking in most linguistic treatments, such as a proof that the number of well-formed association lines between two tiers of length n is asymptotically (6 + 4 tion, but also prosody (including stress assignment and moraic structure), as well as Optimality Theory and Zipf X  X  law.
 dependency frameworks, valency, and weighted models of grammar, ending with a discussion of weighted finite automata and hidden Markov models. In the context of weighted models, Kornai implies that Chomsky X  X  original notion of degree of gram-maticality fits naturally as an instance of a weighted model with a particular semiring; of course, exactly what the  X  and  X  operators of that semiring map to remain to be seen insofar as the notion  X  X egree of grammaticality X  has never been rigorously defined.
 such as the Liar, and then moves on to an overview of Montague X  X  theory, type theory, and grammatical semantics. Throughout the discussion, Kornai underscores the fun-damental limitations of theories of semantics that are based purely upon evaluation of truth conditions for artificial fragments, an important point for anyone who wants to go beyond theoretical philosophically inspired models and consider semantic interpre-tation in the real world.
 complexity, but rather deals with information theory, in particular entropy, Kolmogorov complexity, and a short section on learning, including identification in the limit and PAC learning.
 sential groundwork of linguistic pattern recognition, and Chapter 9 presenting details on speech processing and handwriting recognition. This includes feature extraction: In the case of speech recognition, Kornai reviews the frequency representation of speech signals, and defines the cepstrum. Discussion of acoustic models leads us to phonemes as hidden units, with a slight detour into the fine-grained distinctions between different levels of phonemic analysis in the once popular but now largely discredited theory of Lexical Phonology.
 to are generally quite useful as material for readers who wish to explore the issues further.

Erd  X  os number is 2. Unfortunately, neither of us can claim Kornai X  X  mathematical so-phistication or stature, but on the other hand this makes us good judges of the book X  X  potential audience; and herein lies a problem. Kornai X  X  target is  X  X nyone with suffi-cient general mathematical maturity X  with  X  X n]o prior knowledge of linguistics or lan-guages ...assumed on the part of the reader X  (page v iii). This suggests that the book is not primarily aimed at linguists, and certainly the mathematical maturity assumed puts this book well beyond the reach of most linguists, so that it could not easily be used in an introductory course on mathematical linguistics in a linguistics program. It is probably beyond the reach of many computer science students as well.
 about linguistics? The problem here is that in many cases Kornai does not give enough background (or any background) to appreciate the significance of the particular issues being discussed. For example, on page 77 Kornai gives weak crossover and heavy NP shift as examples of phenomena that have  X  X eak X  effects on grammaticality, and resumptive pronouns as examples of phenomena that are marginal in some languages (such as Eng-lish). But nowhere does he explain what these terms denote, which means that these are throw-away comments for anyone who does not already know. Section 3.2 introduces phonological features and feature geometry and sketches some of the mathematical properties of systems with features; but very little background is given on what features are supposed to represent. The short discussion of Optimality Theory (pages 67 X 69) hardly gives enough background to give a feel for the main points of that approach.
In other cases, topics are introduced but their importance to surrounding topics is hard to fathom. For example, in Section 6.1.3 a discussion of the Berry paradox leads into a digression on how to implement digit-sequence-to-number-name mappings as finite-state transducers. Apart from giving Kornai an opportunity to emphasize that this is 616 trivial to do (something that is true in principle, but less true in practice, depending upon the language), it is not clear what purpose this digression serves.
 way, which might make sense from some points of view, but not if you are trying to introduce someone to the way the field is practiced. It is odd, for instance, that prosody is introduced not in the chapter on phonology but in the one on morphology. It is also somewhat odd that Zipf X  X  law gets introduced in the morphology chapter. (And why is it that nowhere does Kornai cite Baayen X  X  excellent book on word-frequency distributions (Baayen 2001), which would be a very useful source of further information on this topic to any reader of Kornai X  X  book?) sense German has a  X  X ure SVO construction X  (page 103) in contradistinction to the normal assumption that German is verb-second. The Cypriot syllabary does not date from the 15th century BCE (page 54); Latin does not have two locative cases (page 90) X  indeed, it does not even have one locative case, so-called; the basic Hangul letter shapes (introduced on page 31 to make a point about phonetic features) are, with two excep-tions, completely incorrect X  X robably it would have been better to use a real Korean font rather than trying to imitate the jamo with L A T E X math symbols. There are of course a great many places where the discussion is useful and informative, but there are enough examples of the kinds we have outlined that the uninitiated reader should be careful. tional) linguists and others who already know the linguistic issues, have a fairly strong formal and mathematical background, and could benefit from the more-precise and more-rigorous mathematical expositions that Kornai provides.
 These range from relatively simple to major research projects. As with other aspects of this book, the distribution of topics for the exercises is somewhat erratic. Thus, on page 184, in the chapter on complexity, we are offered exercises 7.6 and 7.7 in close proximity: But variety is, after all, what keeps things interesting.
 References
 DouglasBiber,UllaConnor,andThomasA.Upton (Northern Arizona University and Indiana University X  X ndianapolis) John Benjamins Publishing (Studies in corpus linguistics, edited by Elena Tognini-Bonelli, volume 28), 2007, xii+289 pp; hardbound, ISBN 978-90-272-2302-9, $142.00,  X  105.00 Reviewed by Marina Santini University of Glasgow The study of discourse can be undertaken from different perspectives (e.g., linguistic, cognitive, or computational) with differing purposes in mind (e.g., to study language use or to analyze social practices). The aim of Discourse on the Move is to show that it is possible and profitable to join quantitative and qualitative analyses to study dis-course structures. Whereas corpus-based quantitative discourse analysis focuses on the distributional discourse patterns of a corpus as a whole with no indication of how patterns are distributed in individual texts, manual qualitative analysis is always carried out on a small number of texts and does not support large generalizations of the findings. The book proposes two methodological approaches X  X op-down and bottom-up X  X hat combine the quantitative and qualitative views into a corpus-based description of discourse organization . Such a description provides detailed analyses of individual texts and the generalization of these analyses across all the texts of a genre-specific corpus.
 researchers establish functional X  X ualitative methods to develop an analytical frame-work capable of describing the types of discourse units in a target corpus. In this approach, linguistic X  X uantitative analyses come as a later step to facilitate the interpre-tation of discourse types. In contrast, the bottom-up approach begins with a linguistic X  quantitative analysis based on the automati csegmentation of texts into dis course units on the basis of vocabulary distributional patterns. In this approach, the functional X  qualitative analysis that provides an interpretation of the discourse types is performed as a later step.
 steps, but the order of the steps in the two approaches is not the same. The steps to be followed in top-down methods are these: 1. Determination of communicative/functional categories. 2. Segmentation. 3. Classification. 4. Linguisti canalysis of ea ch unit. 5. Linguistic description of discourse categories. 6. Text structure analysis. 7. Description of discourse organizational tendencies.

In the top-down studies presented in this book, the communicative/functional cate-gories used to segment texts into meaningful units of discourse are identified through move analysis and appeals analysis . Moves segment texts according to the commu-nicative functions of texts (Swales 1981, 1990), whereas the primary role of appeals X  derived from the Aristotelian theory of persuasion and employed by Perelman (1982) to develop his theory of  X  X ew rhetoric X  X  X s to make the reader  X  X ct X . For this reason, appeals analysis is often applied to persuasive texts.
 1. Automati csegmentation. 2. Linguisti canalysis of ea ch unit. 3. Classification. 4. Linguistic description of discourse categories. 5. Determination of communicative/functional categories. 6. Text structure analysis. 7. Description of discourse organizational tendencies.

In the bottom-up studies presented in this book, the computational methods used to automatically identify vocabulary-based discourse units (VBDUs) are based on Hearst X  X  (1994, 1997) TextTiling procedure. TextTiling is a quantitative procedure that compares the words used in contiguous segments of text. If the vocabulary in two segments is very similar, the two segments are analyzed as belonging to the same discourse unit; otherwise they are analyzed as two distinct units.
 sis. In the top-down case, the units of analysis (i.e., moves and appeals) are directly interpretable by discourse analysts, whereas bottom-up VBDUs are more complex to describe. However, the bottom-up method can be easily applied to large corpora and is replicable, whereas the top-down approach is more subjective. In both approaches, the linguisti canalysis of dis course units relies on multidimensional analysis, the well-known statistical method developed by Biber to study linguistic variation. top-down and one bottom-up, described in Chapters 4 and 7, respectively X  X arried out on two different corpora, one containing biochemistry research articles, the other including articles about the more general discipline of biology. Although the expectation that both methods would reveal and underpin a similar inherent structure in the articles of both corpora is met to some extent, there are still several aspects that are problematic and have no ready explanation. The authors acknowledge that additional research is needed to shed more light on these aspects.
 divided into two parts X  X nd two appendices. Top-down methods (based on move analysis and appeals analysis) and their application to direct-mail letters, biochem-istry research articles, and fund-raising letters are described in Chapters 2 X 5. Bottom-up methods (based on VBDUs) and their application to biology research articles and spoken university lectures are presented in Chapters 6 X 8. The introductory Chapter 1 contains a synthetic overview of the several perspectives and purposes guiding dis-course analysis, a useful distinction between register and genre, the motivation for the book, and its research questions. The final Chapter 9 presents a critical comparison between the two approaches and outlines the many questions to be addressed in fur-ther studies and the directions to be explored in future research. The two appendices document the steps included in multidimensional analysis (Appendix I) and list the 106 lexico-grammatical features identified by the Biber tagger (Appendix II) and used in multidimensional analyses.
 synthesis, where several previous approaches are combined to produce more extensive and comprehensive findings. The title suggests that discourse analysis is moving for-ward. This is indeed the impression that we have when reading the last pages of the final chapter. The authors list studies, publication of which is forthcoming, that were carried out using top-down and bottom-up approaches, and possible future directions that range from the investigation of multimodal texts to the integration of  X  X ontextual analysis X  through, for example, surveys or interviews with informants.
 tween the findings returned by the top-down and bottom-up analyses on the same genre-specific corpus. This would allow us to assess whether the two methods are basically overlapping and can be used interchangeably, or whether they are comple-mentary, so that it is worth applying them both on the same corpus. Such comparisons would also reveal important details X  X or example, whether multidimensional analyses are more effective and more easily interpretable on top-down units of analysis (i.e., moves and appeals) or on bottom-up VBDUs, or whether there is a way of marshaling the descriptive labels assigned to factor interpretations. As it is now, if we have a look at the functional labels shown in Table 9.1 (pages 246 X 247), it is unclear to what extent they are comparable.
 computational linguists, NLP researchers, and language engineers who are keen on incorporating language variation and genre specificities into computational models. For instance, the identification of regular variational discourse patterns could be helpful to fine-tune parsers and automati csummarizers.
 surface X  of the corpus-based description of discourse organization. We look forward to a rapid growth of this research area.
 References
 ChengXiangZhai University of Illinois at Urbana Champaign Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst), volume 1, 2008; xiii+125 pp, Princeton, NJ; paperbound, ISBN 978-1-59829-590-0, $40.00; ebook, ISBN 978-1-59829-591-7, $30.00 or by subscription Reviewed by Eric Gaussier University Joseph Fourier &amp; LIG
The past decade has seen a steady growth of interest in statistical language models for information retrieval, and much research work has been conducted on this subject. This book by ChengXiang Zhai summarizes most of this research. It opens with an introduc-tioncoveringthebasicconceptsofinformationretrievalandstatisticallanguagemodels, presenting the intuitions behind these concepts. This introduction is then followed by a chapter providing an overview of:
This overview covers the main aspects of these models and frameworks, and allows the author to introduce notions that help position the statistical language model to be presented in the following chapters.
 language models used in information retrieval, and their application to special tasks.
Chapter 3 presents the standard, simple query likelihood retrieval model. After review-ing the basic idea behind this model (at the core of current statistical language models forinformation retrieval), theauthor presents thedifferent event models that have been considered: Multinomial, Multiple Bernoulli, and Multiple Poisson. He then explains the strategy for parameter estimation and the different smoothing techniques one can relyon.Thispresentationisfollowedbyadiscussionoftherelationbetweensmoothing and tf-idf weighting, which paves the way for the two-stage smoothing method pre-sented in the following section. This chapter is very well written and presents, in a clear yet complete way, the fundamentals of the query likelihood retrieval model. votedtoextensions ofthesimplequerylikelihood model ofChapter3.Inparticular, the author reviews document-specific smoothing methods, based on document clustering and document expansion, the use of n -gram models and Markov random fields, as well as the full Bayesian query likelihood and the translation models. The intent here is not to provide a detailed description of these elements, but rather to give an overview, and pointers to extensions and models related to the query likelihood retrieval model. fact that feedback cannot be naturally accommodated. Indeed, in this model, a query is seen as a sample from the document model; adding terms according to a completely differentprocessrendersthe X  X ampleview X  X nadequate.Inordertobetteraccommodate feedback, the Kullback X  X eibler divergence retrieval model has been introduced. In this model, a query model (associated with a word probability distribution) and a docu-ment model (also associated with a word probability distribution) are compared with the Kullback X  X eibler divergence, a form of probabilistic distance. Chapter 5, entitled
Probabilistic Distance Retrieval Model, is devoted to the presentation of this model, with a complete description of how to estimate query models and how to account for feedback (positive and negative). This chapter is slightly more technical than the previous ones. It, however, provides a very accurate and complete description of the
Kullback X  X eibler model X  X urrently the most widely used statistical language model in information retrieval.
 to special aspects of the retrieval process. In Chapter 6, the author briefly reviews several retrieval tasks: cross-lingual information retrieval, distributed information re-trieval,structureddocumentretrieval,personalizedandcontext-sensitivesearch,expert finding, passage retrieval, and sub-topic retrieval. Although some tasks are reasonably well covered (e.g., cross-lingual information retrieval), others are less detailed (e.g., distributed information retrieval), the author only providing a brief synthesis of the research conducted about the use of language models. The last chapter of the book,
Chapter 7, entitled Language Models for Latent Topic Analysis, presents two widely used topic models, namely PLSA (Probabilistic Latent Semantic Analysis) and LDA (Latent Dirichlet Allocation), and their application to information retrieval. The presen-tationofbothPLSAandLDAisclearandprecise.Asectiondevotedtotheextensionsof these models furthermore provides a lot of pointers to related studies. Although topic models fit within the general definition adopted here for language models ( probability distribution over word sequences , page 6), their goal radically differs from that of the statistical models presented in the previous chapters, so that choosing to devote a chapter to topic models may seem surprising at first. The importance of topic models and their use in various places of the retrieval process, however, justifies this choice.
A conclusion summarizing the position of statistical language models with respect to traditional information retrieval models, and synthesizing the research progress on statistical language models over the last decade, closes the book.
 initely targets researchers and Ph.D. students in information retrieval who already possess some understanding of probability theory and statistical language models.
Teachers may use it for a course on information retrieval, but only as a complement to general information retrieval books. Although most statistical approaches to informa-tion retrieval are mentioned in this book, one may regret the relatively poor treatment of the Divergence from Randomness (DFR) approach (Amati and Van Rijsbergen 2002).
Thisapproachhasbeenshowntoyieldstate-of-the-artresultsonseveralcollectionsand couldhavebeencoveredinmoredetailinChapter2(OverviewofInformationRetrieval
Models), all the more so since a formal relation between statistical language and DFR models was recently established (Clinchart and Gaussier 2009).
 trieval in a thorough and clear manner. All the aspects of such models are very well presented and all the subtleties fully addressed. Furthermore, each chapter contains many pointers to related research, as well as a summary synthesizing the main results 280 presented. The part on the application to special tasks (Chapters 6 and 7) is not covered asdeeplyastheotherparts,butdoesprovideusefulinformationandpointers.Allinall, thisisaveryinterestingbook,bothclearandcomplete,andwrittenbyaresearcherwho has been an important contributor to the field. Indeed, a substantial part of the material presented in the book was in fact developed or co-developed by the author himself. This boo kreview was edited by Pierre Isabelle.
 References

GrahamWilcock (University of Helsinki) Princeton, NJ: Morgan &amp; Claypool (Synthesis Lectures on Human Language
Technologies, edited by Graeme Hirst, volume 2, No. 1), 2009, x+149 pp; paperbound, ISBN 978-1-59829-738-6, $40.00; ebook, ISBN 978-1-59829-739-3, $30.00 or by subscription Reviewed by Udo Hahn Friedrich-Schiller-Universit  X  at Jena Looking for a book that lucidly sorts out XML, XSLT and XMI, GATE and UIMA,
WordFreak, OpenNLP, and the Stanford NLP Toolkit? Looking for meticulous guid-ance via batteries of stylesheets and shell scripts, but also keen on exploiting special-purpose rule languages such as JAPE, or pre-configured text analytics engines such as
ANNIE? Looking for a coherent set of exercises (covering different technical angles and varying in task complexity) and a series of illustrative screenshots that break down the understanding of annotation workflows into easy-to-digest atomic thematic chunks?
Preferring the hands-on  X  X ow-to X  over dry and winding theory debates and formally based methodological discussions? Interested to get in touch (again) with Shakespeare X  X  Sonnet 130 (the text example running throughout the entire book)? Well, Graham
Wilcock X  X  IntroductiontoLinguisticAnnotationandTextAnalytics might be a perfect match to enjoy all this.
 on Human Language Technologies series, consists of six chapters. The first one lays the
XML-focused meta language foundations and provides additional insights into XML parsing and validation tools, as well as format-switching XML transformation routines using XSL Transformations (XSLT). Chapter 4 continues this technical thread as it elab-orates on frameworks for interchanging annotations between different formats using
XSLT, as well as the UML-based XML Metadata Interchange (XMI) format, an emerging standard to support the interchange of annotations produced by different tools. In-troducing the WordFreak annotation tool, the second chapter sheds light on relevant linguistic annotation layers ranging from formal sentence splitting and tokenization via part-of-speech tagging, syntactic constituency parsing, and semantic predicate X  argument analysis up to discourse phenomena such as co-reference resolution. Al-though this chapter focuses primarily on manual annotation (there is also a subsection at the end where WordFreak is linked with OpenNLP), the following one features automatic statistical annotation procedures. Easy-to-plug-in OpenNLP is contrasted here with stand-alone Stanford NLP tools at all major levels of the linguistic food chain, namely, sentence and token splitting, chunking and parsing, named entity recognition, and co-reference resolution. Increasing the level of abstraction at the systems level, the author then advances to comparing GATE and UIMA, two alternative architectures for text analytics. His emphasis is on the proper configuration and task-specific customiza-tion (e.g., by introducing the JAPE rule language in GATE and the regex annotator in UIMA, both serving to improve named entity recognition). Again, practical exercises are discussed in detail running through all levels of linguistic processing (integrating gazetteers/dictionaries, POS tagging, NP chunking and full parsing, named entity recognition, co-reference resolution, etc.). The final chapter concludes with a survey of commercially available tools doing text analytics (e.g., alias-i X  X  LingPipe or IBM X  X 
LanguageWare). Furthermore, an advanced treatment of named entity recognition (for job titles) and co-reference resolution is provided using the open-source frameworks of rule-based GATE and UIMA solutions (both incorporating customized dictionaries) and statistically based OpenNLP.

XML basics will certainly be helpful. It is targeted at undergraduate level (not necessar-ily computer science) students who wish to gather experience in reusing, modifying, and customizing existing linguistic annotation tools and text analytics architectures.
Throughout the book, the author directs the reader to open-source, freely available, platform-independent, and easily downloadable software resources and repositories.
So students find themselves embedded in a stimulating playground where tooling is the message.
 in particular when students have the exercises run simultaneously on their personal machines. The author has a clearly designed didactical master plan in mind which is realized by a large number of exercises with increasing, but never particularly high, complexity (see, e.g., the fourth chapter that deals with a nice set of transformation problems involving WordFreak X  X penNLP [XML X  X lain text], GATE X  X ordFreak [XML X 
XML], and WordFreak X  X IMA [XML X  X MI] tool [language] pairs). These exercises are reasonably chosen and solutions are technically well prepared and exhaustively ex-plained with an admirable degree of clarity.
 pampering by way of overly fine-grained thematic exposition and visualization a bit excessive, perhaps even wearisome. There is, for example, also no division into easy, medium, and hard problems to offer challenging tasks at different levels of students X  understanding. If the book is used in the context of more advanced teaching, it should be complemented by much more technical standard reference textbooks providing complementary theoretical background information on empirical NLP, (supervised) machine learning methods for NLP, computational corpus linguistics, and so forth. Yet, for getting used to mapping routines, workflows, and software underlying linguistic meta data annotation, this tutorial fills certainly a gap for students who come across these topics for the first time and enjoy the all-embracing tutorial approach of the author. able (Web links were not removed from the printed version of the book).
 book. 1 This book review was edited by Pierre Isabelle.


StevenBird,EwanKlein,andEdwardLoper (University of Melbourne, University of Edinburgh, and BBN Technologies) Sebastopol, CA :O X  X eilly Media, 2009, xx+482 pp; paperbound, ISBN 978-0-596-51649-9, $44.99; on-line free of charge at nltk.org/book Reviewed by Michael Elhadad Ben-Gurion University
This book comes with  X  X atteries included X  (a reference to the phrase often used to explain the popularity of the Python programming language). It is the compan-ion book to an impressive open-source software library called the Natural Language
Toolkit (NLTK), written in Python. NLTK combines language processing tools (token-izers, stemmers, taggers, syntactic parsers, semantic analyzers) and standard data sets (corpora and tools to access the corpora in an efficient and uniform manner). Al-though the book builds on the NLTK library, it covers only a relatively small part of what can be done with it. The combination of the book with NLTK, a growing system of carefully designed, maintained, and documented code libraries, is an extra-ordinary resource that will dramatically influence the way computational linguistics is taught.
 guistics for science and engineering students; it also serves as practical documentation for the NLTK library, and it finally attempts to provide an introduction to programming and algorithm design for humanities students. I have used the book and its earlier on-line versions to teach advanced undergraduate and graduate students in computer science in the past eight years.
 comprehensive survey of the theory underlying computational linguistics. The niche for such a comprehensive review textbook in the field remains filled by Jurasky and
Martin X  X  Speech and Language Processing (2008). What the book does achieve very well is to bring the  X  X un X  in building software tools to perform practical tasks and in exploring large textual corpora.
 to the glorious family of Charniak et al. X  X  Artificial Intelligence Programming (1987),
Pereira and Shieber X  X  Prolog and Natural Language Analysis (1987), and Norvig X  X  mind-expanding Paradigms of Artificial Programming (1992). It differs from these books in its scope (CL vs. AI) and the programming language used (Python vs. Lisp or Prolog).
Another key difference is in its organization :Whereas the classical books have a strict distinction between chapters covering programming techniques and chapters introduc-ing core algorithms or linguistic concepts, the authors here attempt to systematically blend, in each section, practical programming topics with linguistic and algorithmic topics. This mixed approach works well for me.
 ago), this book is important in closing a gap. The transition of the field from a symbolic approach to data-driven/statistical methods in the mid 1990s has transformed what counts as basic education in computational linguistics. Correspondingly, textbooks ex-panded and introduced new material on probability, information theory, and machine learning. The trend started with Allen X  X  (1995) textbook, which introduced a single chapter on statistical methods. Charniak (1993) and Manning and Sch  X  utze (1999) fo-cused uniquely on statistical methods and provided thorough theoretical material X  X ut there was no corresponding focus on programming techniques. Another impediment to teaching was the lack of easy access to large data sets (corpora and lexical resources).
This made teaching statistical methods with hands-on exercises challenging. Combining statistical methods for low-level tasks with higher levels (semantic analysis, discourse analysis, pragmatics) within a one-semester course became an acrobatic exercise. linguistic concepts, low-level programming techniques, advanced algorithmic methods, and methodological principles remains challenging, this book definitely makes the life of computational linguistics students and teachers more comfortable. It is split into five sections :Chapters 1 to 4 are a hand-holding introduction to the scope of  X  X anguage technologies X  and Python programming. Chapters 5 to 7 cover low-level tasks (tagging, sequence labeling, information extraction) and introduce machine learning tools and methods (supervised learning, classifiers, evaluation metrics, error analysis). Chapters 8 and 9 cover parsing. Chapter 10 introduces Montague-like semantic analysis. Chap-ter 11 describes how to create and manage corpora X  X  nice addition that feels a bit out of place in the structure of the book. Each chapter ends with a list of 20 to 50 exercises X  ranging from clarification questions to mini-programming projects.
 a few pages, the reader is led into an interactive session in Python, exploring textual corpora, computing insightful statistics about various data sets, extracting collocations, computing a bigram model, and using it to generate random text. The presentation is fun, exciting, and immediately piques the interest of the reader.
 commonly used corpora packaged together with NLTK and Python code to read them. The corpora include the Gutenberg collection, the Brown corpus, a sample of the Penn
Treebank, CoNLL shared task collections, SemCor, and lexical resources (WordNet and 768
Verbnet). The important factor is that these resources are made thoroughly accessi-ble, easily downloaded, and easily queried and explored using an excellent Python programming interface. The NLTK Corpus Reader architecture is a brilliant piece of software that is well exploited in the rest of the book.
 loading documents from various sources (URLs, RSS feeds) and excellent practical cov-erage of regular expressions. It is typical of the book X  X  approach that regular expressions are taught by example and through useful applications, and not through an introduction to automata theory. The chapter ends with an excellent introduction to more advanced topics in sentence and word segmentation, with examples from Chinese. Overall, this chapter is technical but extremely useful as a practical basis.
 introduces some key techniques in Python (generators, higher-order functions) together with basic material (what a function is, parameter passing). In my experience teaching humanities students, the material is not sufficient for non-programmers to become suf-ficiently proficient and not focused enough to be useful for experienced programmers. field in the past 15 years. Chapter 5 covers the task of part-of-speech tagging. The linguistic concepts are clearly explained, the importance of the annotation schema is well illustrated through examples (using a simplified 15-tag tagset and a complex one with 50 or more tags). The chapter incrementally introduces taggers using dic-tionaries, morphological cues, and contextual information. Students quickly grasp the data-driven methodology :training and testing data, baseline, backoff, cross-validation, error analysis, confusion matrix, precision, recall, evaluation metrics, perplexity. The concepts are introduced through concrete examples and help the student construct and improve a practical tool. Chapter 6 goes deeper into machine learning, with supervised classifiers. The Python code that accompanies this chapter (the classifier interface and feature extractors) is wonderful. The chapter covers a wide range of tasks where the classification method brings excellent results (it reviews POS tagging, document clas-sification, sequence labeling using BIO-tags, and more). The theory behind classifiers is introduced lightly. I was impressed by the clarity of the explanations of the first mathematical concepts that appear in the book X  X he presentation of the concept of entropy , naive Bayes, and maximum entropy classifiers builds strong intuition about the methods. (Although the book does not cover them, NLTK includes excellent code for working with support vector machines and hidden Markov models.) Chapter 7 builds on the tools of the previous two chapters and develops competent chunkers and named-entity recognizers. For a graduate course, the theoretical foundations would be too superficial X  X nd one would want to complement these chapters with theoretical foundations on information theory and statistics. (I find that a few chapters from All of
Statistics [Wasserman 2010] and from Probabilistic Graphical Models [Koller and Friedman 2009] together with Chapter 6 of Foundations of Statistical NLP [Manning and Sch  X  utze 1999] on estimation methods are useful at this stage to consolidate the mathematical un-derstanding.) Readers come out of this part of the book with an operational understand-ing of supervised statistical methods, and with a feeling of empowerment :They have built robust software tools, run them on the same data sets big kids use, and measured their accuracy.
 simple parsing algorithms (recursive descent and shift-reduce). CKY-type algorithms are also covered. A short section on dependency parsing appears (Section 8.5), but
I found it too short to be useful. A very brief section is devoted to weighted CFGs.
Chapter 9 expands CFGs into feature structures and unification grammars. The au-thors take this opportunity to tackle more advanced syntax :inversion, unbounded dependency.
 ging and chunking, the book does not conclude with a robust working parser. On the conceptual side, I would have liked to see a more in-depth chapter on syntax X  X  chapter similar in depth to Chapter 21 of Paradigms of A IProgramming (Norvig 1992) or the legendary Appendix B of Language as a Cognitive Process (Winograd 1983). In my experience, students benefit from a description of clausal arguments, relative clauses, and complex nominal constructs before they can properly gauge the complexity of syntax. On the algorithmic side, there is no coverage of probabilistic CFGs. The material on PCFGs is mature enough, and there is even excellent code in NLTK to perform tree binarization (Chomsky normal form) and node annotation, which makes it possible to build a competent PCFG constituent-based parser. The connection between probabilistic independence and context-freeness is a wonderful story that is missed in the book.
Finally, I believe more could have been done with dependency parsing :transition-based parsing with perceptron learning ` a la MaltParser (Nivre et al. 2007) is also mature enough to be taught and reconstructed in didactic code in an effective manner. approach of Blackburn and Bos (2005) and covers first-order logic, lambda calculus,
Montague-like compositional analysis, and model-based inferencing. The chapter ex-tends up to Discourse Representation Theory (DRT). As usual, the presentation is backed up by impressively readable code and concrete examples. This is a very dense chapter X  X ith adequate theoretical material. It could have been connected to the ma-terial on parsing, by combining a robust parser with the semantic analysis machinery.
This would have had the benefit of creating more cohesion and illustrating the benefits of syntactic analysis for higher-level tasks.
 skills required for collecting and annotating textual material are complex, and the chapter is a unique and welcome extension to the traditional scope of CL textbooks. guistics. As a textbook for graduate courses, it should be complemented by theoretical material from other sources, but the introduction the authors give is never too simplistic.
The authors provide remarkably clear explanations on complex topics, together with concrete applications.
 Although I still use Lisp in class to present algorithms in the most concise manner,
I am happy to see how effective Python turns out to be as the main tool to convey practical CL in an exciting, interactive, modern manner. Python is a good choice for this book :It is easy to learn, open-source, portable across platforms, interactive (the authors do a brilliant job of exploiting the exploratory style that only interpreters can provide in interspersing the book with short code snippets to make complex topics alive), and it supports Unicode, libraries for graph drawing and layout, and graphical user interfaces. This allows the authors to develop interactive visualization tools that vividly demonstrate the workings of complex algorithms. The authors exploit everything this software development platform has to deliver in an extremely convincing manner. authors manage to cover a range of issues from word segmentation, tagging, chunking, parsing, to semantic analysis, and even briefly reach the world of discourse. I look forward to an expanded edition of the book that would cover probabilistic parsing, text 770 generation, summarization, and lexical semantics. I would also have liked to see some coverage of unsupervised and semi-supervised learning methods.

NLTK library, is an important milestone. No one should learn computational linguistics without it.
 References

KristiinaJokinenandMichaelMcTear (University of Helsinki, University of Ulster) Princeton, NJ: Morgan &amp; Claypool (Synthesis Lectures on Language
Technologies, edited by Graeme Hirst, volume 5), 2009, xiv+151pp; paperback, ISBN 978-1-59829-599-3, $40.00; ebook, ISBN 978-1-59829-600-6, doi 10.2200/S00204ED1V01Y200910HLT005, $30.00 or by subscription Reviewed by Mary Ellen Foster Heriot-Watt University
This book gives a short but comprehensive overview of the field of spoken dialogue systems, outlining the issues involved in building and evaluating this type of system and making liberal use of techniques and examples from a wide range of implemented systems. It provides an excellent review of the research, with particularly relevant dis-cussions of error handling and system evaluation, and is suitable both as an intro-duction to this research area and as a survey of current state-of-the-art techniques. the research area and briefly introduces the topics covered in the book. The end of the chapter consists of a list of links to tools and components that can be used for dialogue system development, which X  X lthough currently useful X  X eems likely to go out of date quickly.
 simple graph-and frame-based methods for dialogue control, and continuing with a discussion of VoiceXML. The chapter ends with an extended discussion of recent work in statistical approaches to dialogue control and modeling. It is unfortunate that the discussion of other methods such as the information state approach and plan-based models is postponed to Chapter 4, but otherwise this chapter provides a good summary of both classic and recent approaches.
 tection, error prediction (i.e., the online prediction of errors based on dialogue features), and error recovery. After surveying a range of previous approaches to these three sub-tasks, the authors go on to discuss several more recent, data-driven approaches. Error handling is both a vital component of any spoken dialogue system designed for real-world use and an active area of current research, so this compact summary of techniques and issues is welcome.
 and models. It begins with a description of the information state approach, and then moves on to discuss plan-based approaches as exemplified in the TRAINS and TRIPS projects. This is followed by a discussion of two systems that employ software agents for dialogue management: the Queen X  X  Communicator and the AthosMail system. Finally, two systems which make use of statistical models are presented: the Microsoft Bayesian receptionist, which models conversation as decision making under uncertainty, and the DIHANA system, which employs corpus-based dialogue management. The case studies in this chapter provide detailed examples of a range of techniques, along with some discussion of the advantages and disadvantages of each approach, adding to the relevance of the book for developers of future dialogue systems.
 straightforward information exchange scenarios considered in the preceding chapters. The first section covers aspects of collaborative planning along with Jokinen X  X  (2009)
Constructive Dialogue Modeling approach. The section on adaptation and user model-ing provides a brief but useful survey of approaches to this task. The discussion of mul-timodality is longer and more concrete, but concentrates almost entirely on multimodal input processing; it would have been helpful to include a similar summary of the issues involved in multimodal output generation. The final section on  X  X atural interaction X  briefly discusses two topics: non-verbal communication for embodied conversational agents and multimodal corpus annotation.
 volved in evaluating spoken dialogue systems, beginning with an historical overview.
It continues with a discussion of terminology and techniques and describes a wide range of subjective and objective evaluation measures that have been applied to the evaluation task. Next, two frameworks are presented that are designed to provide a general methodology for evaluation: PARADISE (Walker et al. 1997) and Quality of
Experience (M  X  oller 2005). This is followed by a discussion of concepts from HCI-style usability evaluations and how they can be applied to spoken dialogue systems, and then a section dealing with semiautomatic evaluation and the role of standardization.
The final section discusses challenges that arise when evaluating advanced dialogue systems incorporating multimodality and adaptivity, and when evaluating systems designed for real-world applications.
 panion/chatbot systems), whose emphasis is on social communication rather than the information exchange tasks considered for most of the book. It also addresses the relationship between commercial and academic approaches to spoken dialogue. that address aspects of spoken dialogue systems: McTear (2004) gives a comprehensive overview of the research area, including a series of hands-on tutorials on building systems using tools such as the CSLU toolkit and VoiceXML, and Jokinen (2009) pro-vides a detailed description of a particular style of dialogue management, Constructive Dialogue Modeling.
 McTear book, nor the in-depth description of a single formalism provided by Jokinen.
Instead, it concentrates on outlining the issues involved in building a spoken dialogue system and on describing a wide range of existing techniques and systems. Although the book is short, it provides an excellent starting point for researchers new to the field, and every section has a good selection of references which would easily allow the interested reader to follow up any particular topic in more depth. The chapters on error handling and evaluation provide particularly useful summaries of the issues and techniques in these active research areas.
 ties students, I suspect that readers without any mathematical background would have difficulty with some of the more formal sections. However, the necessary background knowledge is not great, and in general the book is clearly written and understandable; it is suitable as both an introduction to this research area and a survey of current state-of-the-art techniques. 782 References This book review was edited by Pierre Isabelle.

