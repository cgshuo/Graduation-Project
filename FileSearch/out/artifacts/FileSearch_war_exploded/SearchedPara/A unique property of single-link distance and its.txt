 1. Introduction types of applications. Recent research efforts include a statistical log-linear model to mining signi data points.

Typical examples include BIRCH [7] , CURE [8] , and Chameleon [9] . BIRCH geometry of arbitrary shapes and avoid the chaining effect of the single link method. Chameleon algorithm to partition the data items into several relatively small sub-clusters, and then combining these sub-clusters. The algorithm is proven to fi space.
 encounter problems in identifying complex data structures. Aghagolzadeh et al. introduced bottom which the distribution is approximated using the k-nearest neighbor estimator [11] . environment. To measure compactness, either of inter-subset or intra-subset distances alone is not suf Ward's), only single-link distance has this property.
 we compare graph-cut criteria and compactness. In Section 1.2 we brie 1.1. Graph-cut criteria and compactness the two pieces: cut A ; B  X  X  =  X  clusters into the criterion: Rcut A ; B  X  X  = cut A ; B  X  X  where assoc X ; Y  X  X  =  X  minimizing the number of connections between A and B and maximizing the number of connections within each: representing the compactness of a cluster, which is illustrated by the example in Fig. 1 , where A ={ v Intuitively A is not compact since it is as divided as A  X  paper we introduce isolation compactness to fi x the problem. 1.2. Compactness and separation metrics include single, average, and complete links. Validity indices using these metrics can be found in [18 and average diameter [21] .
 different densities or arbitrary shapes. The Davies  X  Bouldin index [22] is roughly de scatter and between-cluster separation.
 that these subsets are single-link clusters, and vice versa. 1.3. Density-based clustering algorithms region may be arbitrarily distributed. DBSCAN [23] and OPTICS [24] are two typical algorithms of this kind. in DBSCAN [23] served as a starting point for a number of density-based algorithms, aimed at ef parameters [24] , eliminating the non-intuitive parameter log n ) is obtained. DeLiClu [25] uses a similar density estimator for data grouping, but does not require the and thus produces an optimal reachability plot w.r.t. accuracy.
 algorithm is designed on the basis of the monotonic sequences. 2. Isolation compactness
We also prove that only single-link isolation-compact subsets make a dendrogram. In Section 2.2 we de on graphs and prove the equivalence between single-link isolation-compact subsets and single-link clusters. 2.1. Isolation compactness induced by linkage distances
Given a set V of n points and a distance matrix ( d ij ) n * n distances.Foralinkagedistance D and a subset S p V ,wede fi
See Fig. 2 . For consistency, we let max  X  =0 and min  X  = of its external connecting distance to its internal connecting distance: icompactness compact ,or D -icompact ,if icompactness D ( S ) N 1.
 icompact subsets A and B , A p B , B p A ,or A  X  B =  X  . Any D example. In this example we allow duplicate points. The data set V consists of 5 points: p p =(0,0), and p 4 = p 5 =(1, 0). Let A ={ p 1 , p 2 , p 3 } and B ={ p both A and B are icompact.

We prove the Isolation Property of single-link distance in Lemma 1 . Based on this property all D containment hierarchy.

Lemma 1. For any D s -icompact subsets A and B of V, A p B, B Proof. We assume otherwise: A and B are D s -icompact, A  X  ( A )  X  D s ( A  X  B , A  X  B )  X  icd Ds ( A ) b ecd Ds ( A )  X  D
B  X 
A ) b D s ( A  X  B , A  X  B ), due to the D s -icompactness of B . A contradiction is thus formed.
In the proof, D s ( A , B  X  A )  X  D s ( A  X  B , B  X  A ), because D having the isolation property. 2.2. Isolation compactness on graphs edge-weighted by a positive function w . We prove in Theorem 5 that a graph and its MST share common D the same isolation compactness. In Theorem 6 we prove the equivalence between D
A weight function w on a graph G =( V , E ) induces a distance matrix ( d otherwise. Based on this matrix we de fi ne the internal connecting distance icd icompactness Ds induced by the single-link distance, as we did in Section 2.1 . From the de that a D s -icompact subset is always connected.
 icd (  X  , G ), ecd (  X  , G ), and icompactness (  X  , G ).
 outside vertices. See Fig. 3 .
 that (1) icd ( S , G )  X  w ( e ) and (2) w ( e )  X  icd ( S , G ). (1) By de fi nition  X  A q S such that icd ( S , G )= D s ( A , S (2) We remove e from H and get two subsets A and S  X  A . Because H is a MST, D 1 implies the connectivity of S in H .

Corollary 3. For a graph G, a MST H of G, and a vertex subset S, (1) if S is connected in H, then icd(S,G)=icd(S,H) and ecd(S,G)=ecd(S,H). (2) if S is icompact in H, then icd(S,G)=icd(S,H) and ecd(S,G)=ecd(S,H). We prove that an icompact subset in G is connected in its MST.
 Lemma 4. For a graph G, a MST H of G, and a vertex subset S, if S is icompact in G, then S is connected in H. and ( u , v )  X  H } and  X  =min { w ( u , v )| u  X  S 1 , v (1) We prove T  X   X  . Otherwise,  X  a shorter edge e of G connecting S (2) Ontheotherhand, T N  X  ,because T  X  min{ w ( u , v )| u Theorem 5 follows directly from Corollary 3 and Lemma 4 .

Theorem 5. For a graph G, a MST H of G, and a vertex subset S, (1) S is icompact in G  X  S is icompact in H. (2) If S is icompact in G, icd(S,G)=icd(S,H) and ecd(S,G)=ecd(S,H). single-link clusters are icompact subsets: (1) If components c 1 and c 2 are joined into component c with an edge e , then ecd ( c Kruskal's algorithm to compute icompact subsets and record their isolation compactness.
Theorem 6. (1) icompact subsets are single-link clusters. (2) Single-link clusters are icompact subsets.
 remaining component. By de fi nition, S is a single link cluster. apply Kruskal's algorithm to compute icluster trees, as shown in Fig. 4 . layers, which can be analyzed by monotonic sequences of iclusters. See Section 3.1 . 3. Clustering takes time and space linear to the size of the input icluster tree. 3.1. Monotonic sequence c , c 2 , ... , c m such that for i =1,2, ... , and m  X  1, (1) c before it.

We de fi ne a parent-child relation on the monotonic sequences: a sequence { c based on the following observations: Fig. 7 .

A monotonic sequence tree is represented as a rooted tree, where each node has a list stores the iclusters in the sequence, and the children fi sequence tree from an icluster tree. The algorithm starts with the root icluster r , 4). Then for each child icluster c of the last icluster in R with the number of points child monotonic sequence of R (step 5).
 Algorithm 1. Compute_MonotonicSequenceTree ( r , T_size ) Input: the root r of an icluster tree, threshold T_size Output: the monotonic sequence tree referred by its root R
Begin
End 3.2. Clustering with multiple monotonic sequences
To classify a data set we discuss two cases: the monotonic sequence tree has multiple nodes or one node. In the leaf 2 monotonic sequence S i ( i =1,2, ... ,or m with m N 1) represents the kernel of a class C non-kernel points. Formally we initialize each C i with points in S with the shortest distance to C is assigned to C i if dist ( p , C )= dist ( p , C the algorithm for growing the kernels is straightforward: given a set { C points not in any kernels, and a list { e i } i =1 n  X  1 of ascendingly sorted MST edges, for each e (1) We add to C j the vertex v of e i in O , (2) We search the connected subgraph starting with v and consisting of edges in { e (3) We add to C j the other vertices in the subgraph.
 The kernel growing algorithm is illustrated in Fig. 8 .

A short trace of the algorithm is shown in Fig. 9 : 1) The data set has 127 points ( Fig. 9 a). We set T_size =19 and correspond to two classes C 1 and C 2 . 3) In Fig. 9 c, v 1  X  O is the point nearest to C 1  X  C 2 4) In Fig. 9 d, v 2  X  O is the point nearest to C 1  X  C 2 also added to C 2 . 5) In Fig. 9 e, v 3  X  O , as well as another point in its subgraph, is added to C 6) In Fig. 9 f, v 4  X  O , as well as the points in its subgraph, is added to C 7) In Fig. 9 g, v 5  X  O is added to C 1 . 8) Fig. 9 h shows the fi nal partition. 3.3. Clustering with a single monotonic sequence monotonic sequence { c i } i =1 m of iclusters, we let c k input data set. We choose c i with the highest isolation compactness among c kernel class) and points not in c i make another (the outlier class). linear to n . 3.4. Advantages and disadvantages of arbitrary shape, as justi fi ed by the following argument. We de or expect any shape or spatial relationships (except ecd N each subset, and merge the clusters detected in the subsets. 4. Experiments
We conducted comprehensive experiments to assess the proposed approach and compare it with other algorithms. Our fi x the parameter b =20%. We can use a binary search strategy to expedite the process. 4.1. Algorithms http://www.pudn.com/downloads36/sourcecode/math/detail115779.html . Chameleon has 3 parameters: k , MINSIZE, and of preset clusters.
 4.2. On 2D data
We experimented with six 2D data sets shown in Fig. 12 . Each of the and orientation, as well as noise points.
 shape from the surrounding noise. In Set3 the algorithm fi algorithm is tolerant to outlier points, and is effective in in Set1 a dense shape from a sparse environment. Only Chameleon, NCUT, and DeLiClu can identifying clusters of different densities (on Set1 and Set3). 4.3. On high dimensional data reported in Tables 2-4 .In Figs. 14 and 15 , we show a clustering accuracy comparison among different methods in
Chameleon, and DeLiClu. electrons in the ionosphere. The data are classi fi ed as type of structure in the ionosphere.  X  Bad  X  returns are those that do not. In the monotonic sequence the and the  X  bad  X  in the outlier, implying that the  X  Good  X  that the density of the  X  Good  X  class is 3.7 and the density of the classes, some of them are listed in Table 7 . Experiments on these sets con nonlinearly separated or of various densities. 5. Conclusion nonlinearly separated or of arbitrary shape and density.
 validation technique to assess the result.
 like image and video segmentation.
 Acknowledgments and Hi-Tech Research and Development Program of China (863 Program) under contract No. 2009AA01Z317.
References
