 With the explosive growth of microblogging services, short-text messages (also known as tweets) are being created and shared at an unprecedented rate. Tweets in its raw form can be incredibly informative, but also overwhelming. For both end-users and data analysts it is a nightmare to plow through millions of tweets which contain enormous noises and redundancies. In this paper, we study continuous tweet summarization as a solution to address this problem. While traditional document summarization methods focus on static and small-scale data, we aim to deal with dynamic, quickly arriving, and large-scale tweet streams. We propose a novel prototype called Sumblr (SUMmarization By stream cLus-teRing) for tweet streams. We first propose an online tweet stream clustering algorithm to cluster tweets and maintain distilled statistics called Tweet Cluster Vectors. Then we de-velop a TCV-Rank summarization technique for generating online summaries and historical summaries of arbitrary time durations. Finally, we describe a topic evolvement detection method, which consumes online and historical summaries to produce timelines automatically from tweet streams. Our experiments on large-scale real tweets demonstrate the effi-ciency and effectiveness of our approach.
 H.3.1 [ Content Analysis and Indexing ]: Abstracting methods; H.3.4 [ Systems and Software ]: Performance evaluation (efficiency and effectiveness) Tweet stream; continuous summarization; timeline
With the explosive growth of microblogging services, such as Twitter, Weibo and Tumblr, short-text messages known as tweets are being created and shared at an unprecedented
Figure 1: A timeline example for topic  X  X pple X  rate. Take Twitter for instance, receiving over 400 million tweets per day 1 , it has become an invaluable source of news, blogs, opinions, and more. Tweets in its raw form can be incredibly informative, but also overwhelming. For exam-ple, searching for a hot topic in Twitter can yield millions of tweets, which span for weeks. Even if filtering is allowed, plowing through so many tweets for interesting contents would be a nightmare, not to mention the enormous noises and redundancies that one could encounter. To make things worse, new tweets satisfying the filtering criteria may arrive continuously, at an unpredictable rate.

A possible solution to the above problem is continuous tweet summarization , which represents the massive tweets in a set of short text pieces covering the main topics (or sub-topics of course). Specifically, let us look at an example, which presumes availability of a topic-related tweet stream, for example tweets about X  X pple X . With a tweet summariza-tion system, we can (i) continuously monitor X  X pple X -related tweets arriving from the stream and produce a continuous timeline which grows by time. (ii) Suppose a user wants to learn the main happenings about  X  X pple X  from the tweets between 22 Oct 2012 and 11 Nov 2012. The range timeline during that period can be provided to her, so that she un-derstands the big picture of topic evolution in those weeks (as shown in Figure 1). (iii) After that, she may need more detailed reports for a much smaller duration (e.g. from 8 am to 11 pm on 5 Nov), which is like a drill-down summary on the duration. (iv) Alternatively, she may ask for a more con-cise report during 21 Oct to 30 Oct, in a roll-up summary . http s://blog.twitter.com/2013/celebrating-twitter7 Such application would not only facilitate easy navigation in topic-relevant tweets, but also support a range of data analysis tasks such as instant reports or historical survey.
Implementing continuous tweet stream summarization is not an easy task, as the tweets are of noisy, redundant, and social nature. More importantly, tweets arrive very quickly and are strongly correlated with their posted time. A good solution to continuous tweet stream summarization has to address the following issues: (1) Efficiency -tweet streams always have very large scales, so their summarization should be highly efficient, with only one pass over the data; (2) Flex-ibility -the ability to provide tweet summaries of arbitrary time durations. (3) Topic evolvement -to automatically de-tect sub-topic changes and the moments that they happen.
Unfortunately, the importance of continuous summariza-tion has long been overlooked by the research community. Although there exist numerous studies on document sum-marization [6, 26, 23, 13, 9, 11], these methods cannot sat-isfy our requirements, because: (1) They mainly focus on static and small-sized datasets, making it intractable to im-prove their efficiency. (2) To provide summary for arbi-trary duration, these techniques will have to perform iter-ative/recursive summarization for every possible time du-ration, which is unacceptable. (3) The summary results of these algorithms are insensitive to time. Thus it is difficult for them to detect topic evolvement.

In this paper, we introduce a novel tweet summarization prototype called Sumblr (SUMmarization By stream cLus-teRing) . To the best of our knowledge, our work is the first to study continuous tweet stream summarization. The over-all structure of the prototype is depicted in Figure 2. Sumblr consists of two main components, namely a Tweet Stream Clustering module and a High-level Summarization module. In the tweet stream clustering module, we design an efficient tweet stream clustering algorithm , an online algorithm allow-ing for effective clustering of tweets with only one pass over the data. This algorithm uses two data structures to keep important tweet information in clusters. The first one is a compressed structure called Tweet Cluster Vector (TCV) . TCVs are considered as potential sub-topic delegates and maintained dynamically in memory during stream process-ing. The second structure is the Pyramidal Time Frame (PTF) [1], which is used to store and organize cluster snap-shots at different moments, thus allowing historical tweet data to be retrieved by any arbitrary time durations.
The high-level summarization module supports the gen-eration of two kinds of summaries: online summaries and historical ones. (1) To generate online summaries, we pro-pose a TCV-Rank summarization algorithm by referring to the current clusters maintained in memory. This algorithm first computes centrality scores for tweets kept in TCVs, and selects the top-ranked ones in terms of content coverage and novelty. (2) To compute historical summaries where the user specifies an arbitrary time duration, we first retrieve two historical cluster snapshots from the PTF with respect to the two endpoints (the beginning and ending points) of the duration. Then, based on the difference between the two cluster snapshots, the TCV-Rank summarization algorithm is applied to generate summaries.

The summarization module also contains a topic evolve-ment detection algorithm, which consumes online/historical summaries to produce continuous/range timelines. A time-line is a sequence of time-stamped summaries (nodes). Both continuous and range timelines are generated by monitoring a quantity called summary-based variation , which is defined for summaries during the course of topic evolvement. A large variation at a particular moment implies a sub-topic change, leading to the addition of a new node on the timeline.
The main contributions of our work include: (1) A con-tinuous tweet stream summarization framework; (2) Novel data structures and algorithms for online summarization and historical summarization of any arbitrary time interval; (3) A topic evolvement detection scheme for continuous and range timelines; (4) Extensive experiments on real Twitter datasets, showing promising results in terms of summary quality and efficiency.

The rest of the paper is organized as follows. Section 2 reviews the related work. Section 3 explains concepts includ-ing TCV and PTF. Section 4 describes our Sumblr frame-work in detail. The experimental settings and results are presented in Section 5. In Section 6, we conclude the paper.
In this section, we review the related work including stream data clustering , traditional document summarization and mi-croblog summarization and mining .
Stream data clustering has been widely studied in the lit-erature. CluStream [1] is one of the most classic stream clustering methods. It consists of an online micro-clustering component and an offline macro-clustering component. The pyramidal time frame is also proposed in [1] to recall histor-ical micro-clusters for different time durations.
A variety of services on the Web such as news filtering, text crawling, and topic detecting etc. have posed require-ments for text stream clustering. A few algorithms have been proposed to tackle the problem [7][10][27][29]. Most of these techniques adopt partition-based approaches to en-able online clustering of stream data. As a consequence, these techniques fail to provide effective analysis on clusters formed over different time durations.

In [2], the authors propose to generate duration-based clustering results by extending CluStream for text and cat-egorical data stream. However, this algorithm relies on an online phase to generate large number of  X  X icro-clusters X , leading to inefficiency and poor storage utilization.
Document summarization techniques can be categorized into two types: extractive techniques and abstractive ones. The former selects sentences from the documents, while the latter may generate phrases and sentences that do not ap-pear in the original documents. In this paper, we focus on extractive summarization.

Extractive document summarization has received a lot of recent attention. Most of them assign salient scores to sen-tences of the documents, and select the top-ranked sentences [3][30] [19][26] [6][22]. Some works try to extract sentences without such salient scores. A method using Singular Value Decomposition (SVD) is proposed in [8] to select highly ranked sentences. Wang et al. [23] use the Symmetric Non-negative Matrix Factorization (SNMF) to cluster sentences and choose sentences in each cluster for summarization. In [11], He et al. propose to summarize documents from the perspective of data reconstruction, and select sentences that can best reconstruct the original documents. Unfortunately, all above methods neglect the significant temporal dimen-sion of documents, which is critical in our problem.
Yan et al. propose a technique called Evolutionary Time-line Summarization (ETS) [24] to compute evolution time-lines consisting of individual but correlated component sum-maries. However, the dates of component summaries are de-termined by a pre-defined timestamp set. In contrast, our solution discovers the changing dates and generate timelines dynamically during the process of continuous summariza-tion. More importantly, ETS does not focus on efficiency and scalability. Its task is formulated as an optimization problem via iterative substitution. Thus it does not meet our requirements.
While document summarization has been studied for years, microblog summarization is still in its infancy. Sharifi et al. proposed the Phrase Reinforcement algorithm to summarize multiple tweet posts on the same topic with a single tweet [20]. Later, Inouye et al. proposed a modified Hybrid TF-IDF algorithm and a Cluster-based algorithm to generate multiple post summaries [13]. In [9], Harabagiu et al. in-troduced a framework for microblog summarization which capitalizes on a combination of two relevance models: an event structure model and a user behavior model. Taka-mura et al. proposed a microblog summarization method based on the p-median problem, which takes posted time of microblogs into consideration [21].

The emergence of microblogs also motivates research on other mining tasks, including topic modeling [12], storyline generation [14] and event exploration [17]. Most of these researches focus on static datasets instead of data streams. Yang et al. studied twitter stream analysis [25], but they aim at frequent pattern mining and compression, which is also a different problem from ours.

To sum up, almost all existing document/microblog sum-marization works mainly deal with static and small datsets, and rarely pay attention to evolvement and efficiency issues. In this section, we first give a data model for tweets. Then we introduce two data structures used in our solu-tion, namely the Tweet Cluster Vector and the Pyramidal Time Frame .
Generally, a document is represented as a textual vector, where the value of each dimension is the TF-IDF score of a word. However, tweets are not only textual, but also hav-ing temporal nature -a tweet is strongly correlated with its posted time. In addition, the importance of a tweet is af-fected by the author X  X  social influence. To estimate the user influence, we build a matrix based on social relationships among users, and compute the UserRank as [5].

As a result, we define a tweet t i as a tuple: ( tv i , ts where tv i is the textual vector, ts i is the posted timestamp and w i is the UserRank value of the tweet X  X  author.
During tweet stream clustering, it is necessary to maintain statistics for tweets to facilitate summary generation. In this section, we propose a new data structure called Tweet Cluster Vector , which keeps information of tweet cluster.
Definition 1. For a cluster C containing tweets t 1 , t 2 its Tweet Cluster Vector (TCV) is defined as a tuple: T CV ( C ) = ( sum v , wsum v , ts 1 , ts 2 , n, f t set ) , wher e The form of sum v is used for ease of presentation. In fact, we only store the identifiers and sums of values of the words occurring in the cluster. The same convention is used for wsum v . To select tweets into f t set , we use cosine similarity as the distance metric.

From the definition, we can derive the vector of cluster centroid (denoted as cv ):
The definition of TCV is an extension of the cluster feature vector in [28]. Like in [28], our TCV structure can also be updated in an incremental way when new tweets arrive. We shall discuss details on updates to TCV in Section 4.1.2.
To support summarization over user-defined time dura-tions, it is crucial to store the maintained TCVs at partic-ular moments, which are called snapshots . While storing snapshots at every moment is impractical due to huge stor-age overhead, insufficient snapshots make it hard to recall historical information for different durations. This dilemma leads to the incorporation of the Pyramidal Time Frame [1]:
Definition 2. The Pyramidal Time Frame (PTF) stores snapshots at differing levels of granularity depending on the recency. Snapshots are classified into different orders which vary from 0 to log ( T ) , where T is the time elapsed since the beginning of the stream. The order of a particular class of snapshots defines the level of granularity in time at which the snapshots are maintained. The snapshots of different orders are maintained as follows: Accord ing to the definition, PTF has two properties: (1) The maximum order of any snapshot stored at T times-tamps since the beginning of the stream is log ( T ); (2) The maximum number of snapshots maintained at T is (  X  l + 1)  X  log ( T ). These properties are crucial for system per-formance. Taking more snapshots (by using a larger  X  or l ) offers better accuracy of time duration approximation, but meanwhile causes larger storage overhead. Therefore, we need to strike a balance between duration accuracy and storage space. Note that we only maintain the current clus-ters in main memory, and store all historical snapshots in the PTF on disk.

To clarify how snapshots are stored, we give an example here. Let  X  = 3 and l = 2, then there are at most 3 2 +1 = 10 snapshots stored in each order. Suppose the stream starts at timestamp 1 and the current timestamp is 86. The stored snapshots are illustrated in Table 1. Redundancy is removed by storing each snapshot only in its highest possible order.
Note that for more recent timestamps, the time interval between successive snapshots stored in PTF is smaller (finer granularity). This feature of PTF is consistent with the demand that recent summaries should be of higher quality because people usually care more about recent events.
In this section, we present the details of our Sumblr frame-work. As shown in Figure 2, our framework includes two main modules: the tweet stream clustering module and the high-level summarization module. In what follows, we will elaborate these two modules respectively.
The tweet stream clustering module maintains the online statistical data. Given a topic-based tweet stream, it is able to efficiently cluster the tweets and maintain compact cluster statistics, with only one scan on the data.
At the beginning of the stream, we collect a small number of tweets and use a k-means clustering algorithm to create the initial clusters. The value of k and the initial cluster centroids are decided via the Canopy [18] method. Once the initial clusters are established, the first set of TCVs are ini-tialized according to Definition 1. Next, the stream cluster-ing process starts to incrementally update the TCVs when a new tweet arrives.
Suppose a tweet t arrives at time ts , and there are N active clusters at that time. First, we try to absorb t into one of the current clusters. The priority is given to the cluster whose centroid is the closest to t . Specifically, we get the centroid (denoted as co ) of a cluster based on Equation (1), compute the cosine similarity between co and t , and find the cluster C p with M axSim ( co, t ).

Note that although C p is the closest to t , it does not mean t naturally belongs to C p . The reason is that t may still be Algo rithm 1: Incremental tweet stream clustering
Input : a cluster set S 1 while !stream.end() do 2 T weet t = stream.next (); 3 choose C p in S whose centroid is the closest to t ; 4 if M axSim ( co p , t ) &lt; M BS then 5 create a new cluster C new = { t } ; 6 S = S  X  C new ; 7 else 8 updat e C p with t ; 9 if T S cur rent % (  X  i ) == 0 then 10 store S in to PTF; very dis tant from C p . In such case, a new cluster will be created. The decision of whether to create a new cluster can be made with the following heuristic.

Heuristic 1. If M axSim ( co, t ) is smaller than a Mini-mum Bounding Similarity (MBS), then t is upgraded to a new cluster. Otherwise, t is added to its closest cluster.
The MBS is defined as  X   X  Sim ( co, t i ), where  X  is a bound-ing factor (0 &lt;  X  &lt; 1) and Sim ( co, t i ) is the average cosine similarity between co and tweets included in C p . According to Section 3.2, Sim ( co, t i ) can be calculated by using the information stored in TCV: Sim ( co, t i ) = 1
When ad ding a new cluster, it is hard to tell whether it is noise or a truly new sub-topic. Actually, the decision cannot be made until more tweets arrive. We discuss how noises are diminished in Section 4.4.

After applying Heuristic 1, the corresponding TCV needs to be updated. For a newly created cluster, its TCV can be initialized easily. For an existing cluster, its components in TCV can also be easily updated in an incremental manner, except the focus tweet set .

Recall that in Definition 1, f t set is the set of the closest m tweets to the cluster centroid. However, as new tweets are added to the cluster during stream processing, the clus-ter centroid would change unpredictably. Since we can not store all tweets in the cluster, it is rather difficult to main-tain exact focus tweets. For this reason, we use a heuristic strategy to choose promising candidates instead of exact fo-cus tweets: for the newly absorbed tweet and those already in f t set , we compute their cosine distances to the new cen-troid , and select the closest m tweets. The advantage of this strategy is that it gives a higher probability for fresh tweets to get into the focus set, which usually represent new statuses of the topic.

The above updating process is executed upon the arrival of each new tweet. Meanwhile, when the current timestamp is divisible by  X  i for any integer i , we store the snapshot of the current TCVs into disk and index it by PTF. Algorithm 1 gives an overview of our incremental clustering procedure.
During incremental clustering, assume there are N active clusters, the computational cost of finding the closest cluster for every new tweet is O ( N d ), where d is the vocabulary size. In addition, the complexity of computing Heuristic 1 and updating TCV is O ( d ) and O ( md ) respectively, where m is Figure 3: Probability density func. of timestamp
Figure 4: A running example of cluster merging the size of focus set. Then the total cost is O (( N + m ) d ). Because m and d are static, the computational cost depends on N . Similarly, the storage costs in disk (TCV snapshots) and memory (current TCVs) also depend on N .

Given the above analysis, we need to restrict the num-ber of active clusters. We achieve this goal via two opera-tions: deleting outdated clusters and merging similar clus-ters . Since the computational complexity of deletion is O ( N ) and that of merging is O ( N 2 ), we use the former method for periodical examination and use the latter method only when memory limit is reached.
For most events (such as news, football matches and con-certs) in tweet stream, timeliness is important because they usually do not last for a long time. Therefore it is safe to delete the tweet clusters representing these sub-topics when they are rarely discussed. To find out such clusters, an intu-itive way is to estimate the average arrival time (denoted as Avg p ) of the last p percent of tweets in a cluster. However, storing p percent of tweets for every cluster will increase memory requirements, especially when some clusters grow big. Thus, we employ an approximate method to get Avg p .
Note that the temporal statistics in TCV of a cluster C al-low us to compute the mean and standard deviation of times-tamps of tweets in C :  X  c = ts 1 /n ,  X  c =
Assumin g that the tweet timestamps are normally dis-tributed, we can obtain the arrival time of the q th percentile of the tweets. The q th percentile is the value that cuts off the first q percent of the tweet timestamps when they are sorted in ascending order. When q = 100  X  p , the q th percentile is the start timestamp of the last p percent of tweets (noted as ts p in Figure 3). Then, we can approximate Avg p using the (100  X  p/ 2)-th percentile (noted as ts p= 2 in Figure 3).
Now the problem is transformed into obtaining the value of ts p= 2 . Let x = ts p= 2 and p  X  = (100  X  p/ 2)%, we have where F ( x ) is the cumulative distribution function (CDF),  X ( x ) is the CDF of the standard normal distribution, and erf  X  1 ( z ) is the inverse error function and can be calculated using the Maclaurin series expansion [31].
 The value of ts p= 2 represents the freshness of cluster C . We empirically set a freshness threshold as 3 days (as empir-ically no bursty events would last longer) and set p = 10. If ts p= 2 is smaller than this threshold, i.e., the average times-tamp of the latest 10 percent tweets is more than 3 days old, then we regard C as an outdated cluster and remove it.
If the number of clusters keeps increasing and few of them are deleted, the system memory will be exhausted. To avoid this, we specify an upper limit for the number of clusters as N max . When the limit is reached, a merging process starts.
The process merges clusters in a greedy way until the ter-mination condition is satisfied. First, we sort all cluster pairs by their centroid similarities in a descending order. Then, beginning with the most similar pair, we try to merge two clusters in the pair. When both clusters are single clusters which have not been merged with other clusters, they are merged into a new composite cluster . When one of them be-longs to a composite cluster (it has been merged with others before), then the other is also merged into that compos-ite cluster. When both of them have been merged, if they belong to the same composite cluster, this pair is skipped; otherwise, the two composite clusters are merged together. This process continues until there are only mc percentage of the original clusters left ( mc is a merge coefficient which provides a balance between available memory space and the quality of remaining clusters). We omit the pseudo-code of the algorithm due to space limitation.

During cluster merging, each composite cluster is given an IDList which consists of ID s of the clusters merged in it. Furthermore, its TCV is obtained by the Aggregation operation to combine two TCVs.

Definition 3. ( Aggregation Operation ) Let C 1 and C 2 be two clusters, and their TCV structures be T CV ( C 1 ) and T CV ( C 2 ) . Then, when C 1 and C 2 are merged together, the composite cluster X  X  T CV ( C 1  X  C 2 ) is given by
Figure 4 shows a running example of the process. For ease of presentation, we use cluster centroids (the black solid points) to represent clusters and use Euclidean distance in-stead of cosine distance. First, we calculate distances for all cluster pairs and sorted them as: ( c 1 , c 2 ) , ( c 2 , c ( c move 10  X  (1  X  0 . 7) = 3 clusters. To start with, c 1 and c merged into a composite cluster { c 1 , c 2 } . After that, when processing the second pair ( c 2 , c 4 ), we find that c 2 merged. Hence we combine c 4 into { c 1 , c 2 } , and the compos-ite cluster becomes { c 1 , c 2 , c 4 } . For next pair, c have been merged, so this pair is skipped. Next we merge c and c 7 into another composite cluster { c 5 , c 7 } . Now that we have reduced the number of clusters by 3, the algorithm terminates. Note that since only N max  X  (1  X  mc ) (denoted as N  X  ) clusters need to be removed, we would access at most C
The high-level summarization module provides two types of summaries: online and historical summaries. The online summaries are retrieved directly from the current clusters main tained in the memory. For historical summaries, re-trieval of the required clusters is more complicated. In what follows, we shall focus on the second type.

Suppose the length of a user-defined time duration is H , and the ending timestamp of the duration is ts e . From PTF, we can retrieve two snapshots whose timestamps are either equal to or right before ts e and ts e  X  H , respectively. We denote their timestamps by ts 1 and ts 2 , and their cluster sets by S ( ts 1 ) and S ( ts 2 ). Now the original duration [ ts is approximated by [ ts 2 , ts 1 ].

Intuitively, we need to perform a cluster set subtraction between S ( ts 1 ) and S ( ts 2 ). For each cluster C in S ( ts acquire its ID (if it is a single cluster) or ID s in its IDList (a composite cluster). For each of these ID s, we find the corresponding cluster in S ( ts 2 ), and subtract its TCV from C  X  X  TCV according to:
Definition 4. ( Subtraction Operation ) Given a clus-ter C 1 in S ( ts 1 ) and its corresponding cluster C 2 in S ( ts when C 2 is subtracted from C 1 , their difference T CV ( C C ) is given by
The abo ve process eliminates the influence of clusters cre-ated before ts 2 on summary results. The final set of clusters after this process is the input for historical summarization. Given an input cluster set, we denote its corresponding TCV set as D ( c ). A tweet set T consists of all the tweets in the f t set s in D ( c ). Our tweet summarization aims to extract k tweets from T , so that they can cover as many tweet contents as possible.

We first prove it is a NP-hard problem, then we present a greedy algorithm to solve it.
 Lemma 1. The tweet summarization problem is NP-hard.
Proof. Let us first describe this problem formally. F = {
T 1 , T 2 , ..., T t } is a collection of non-empty subsets of T , where a subset T i represents a sub-topic and | T i | means the number of its related tweets. The subsets may have some tweets in common because one tweet can be related to more than one sub-topic. Suppose for each T i , there is a tweet which represents the content of T i  X  X  sub-topic. Then, select-ing k tweets is equivalent to selecting k subsets.
Now, the problem can be defined as: given a number k and a collection of sets F , find a subset F  X   X  X  , such that k and | tweets as possible). We notice that this is the Max-k-Cover problem, which is NP-hard. Therefore, our summarization problem is also NP-hard.

More gen erally, summary length are limited in terms of words (250 words). Since the number of words and that of tweets are linearly dependent, the problem is still NP-hard.
From the geometric interpretation, our summarization tends to select tweets that span the intrinsic subspace of candidate tweet space, such that it can cover most information of the whole tweet set.
 Algo rithm 2: TCV-Rank summarization Input : a cluster set D ( c )
Output : a summary set S 1 S =  X  , T = { all the tweets in f t sets of D ( c ) } ; 2 Build a similarity graph on T ; 3 Compute LexRank scores LR ; 4 T c = { tweets with the highest score in each cluster } 5 while | S | &lt; L do 6 fore ach tweet t i in T c do 7 calcu late v i according to Equation (2); 8 select t max wit h the highest v i ; 9 S = S  X  t max ; 10 wh ile | S | &lt; L do 11 fore ach tweet t  X  i in T  X  S do 12 cal culate v  X  i according to Equation (2); 13 selec t t  X  max with the highest v  X  i ; 14 S = S  X  t  X  max ; 15 re turn S ;
We des ign a greedy algorithm to select representative tweets to form summaries (Algorithm 2). First, we gather tweets from the f t set s in D ( c ) as a set T , and build a cosine simi-larity graph. The maximum size of T is N  X  m , where N is the number of clusters in D ( c ) and m is the size of f t set . It is the upper bound because f t set s of some clusters (e.g., small clusters or clusters newly created) may not be full. Next, we apply the LexRank method [6] to compute cen-trality scores for tweets. LexRank is an effective static sum-marization method and is efficient for small-sized datasets. But when datasets become large, its efficiency drops quickly (this will be shown in the experiment). The tweet set T has at most N m tweets (usually hundreds or thousands), so LexRank is suitable for our situation.

However, a potential problem of LexRank is that some top-ranked tweets may have similar contents. Fortunately, since the tweets are retrieved from TCVs, they have got inherent cluster information. Hence, we choose one tweet with the highest LexRank score from each TCV, and add tweet t into the summary according to: t = argmax where  X  (0  X   X   X  1) is a weight parameter, n t i is the size of the cluster containing t i , n max is the size of the biggest cluster, LR ( t i ) is t i  X  X  LexRank score and S is the summary set containing already chosen tweets.
 The motivation of Equation (2) is analogous to that of Maximal Marginal Relevance (MMR) [4]. In query-oriented summarization, MMR combines query relevance and infor-mation novelty. Here, we combine coverage and novelty as our criterion: the first component on the right side of the equation favors tweets which have high scores and belong to big clusters (content coverage); the second component penalizes redundant tweets with similar contents to those already chosen (novelty).

After the first round selection, if the summary length is still not reached, then we try to select tweets globally ( t T  X  S ) according to Equation (2).

The computational complexity for LexRank is O ( r | T | 2 where r is the iteration number. In tweet selection, note that for each tweet, the first component in the righthand of Equation (2) only needs to be computed once, and the sec-ond component can be updated incrementally. So the worst-case cost of tweet selection is O ( N 2 + ( | S | X  N )  X | |
S | &lt;&lt; | T | , the total cost for our algorithm is O ( r N 2 ) = O ( rm 2 N 2 ). As mentioned before, N m is always controlled at a relatively small number, hence the summa-rization procedure is very efficient.
Topic evolvment detection algorithm can produce contin-uous and range timelines in a similar way. We shall only describe the continuous case here.

As tweets arrive from the stream, online summaries are generated continuously by utilizing real-time cluster statis-tics. This allows for a continuous timeline. Generally, when an obvious variation occurs in the main contents discussed in tweets, we can expect a change of sub-topics (i.e., a time node on the timeline). To quantify the variation, we use Kullback-Leibler divergence to measure the distance between two word distributions in two successive summaries S p and S , S p is the previous summary and S c is the current one. where V is the vocabulary set and tf ( w, S ) is the term frequency for word w in S .  X  is a small positive constant for Laplace Smoothing, which is applied to avoid zero values of p ( w Furthermore, we normalize the distance into interval [0 , 1)
According to the summary-based variation, we determine if the current time is a sub-topic changing node by: where D cur is the distance between current summary and its previous neighboring summary, D avg is the average distance of all the previous successive summary pairs which do not produce time nodes, and  X  (  X   X  1) is the decision threshold.
That is, we detect topic evolvement when there is a burst in distances between successive summaries. In this way, we can automatically draw a timeline as the stream proceeds.
Handling noises The effect of clusters of noises can be diminished by two means in Sumblr. First, in tweet stream clustering, noise clusters which are not updated frequently will be deleted as outdated clusters. Second, in the summa-rization step, tweets from noise clusters are far less likely to be selected into summary, due to their small LexRank scores and cluster sizes.

Extension to multi-topic streams So far we have assumed a tweet stream of only one topic as the input to Sumblr. However, we should note that Sumblr can be easily extended for multi-topic streams. For example, when a new tweet arrives, we first decide its related topics by keyword matching. Then it is delivered into different groups of clus-ters. Clusters are grouped by their corresponding topical IDs. Consequently, Sumblr is applied within each cluster group. It is important to note that this mechanism allows for distributed system implementation.
 Datasets We construct 5 datasets to evaluate Sumblr. One is obtained by conducting keyword filtering on a Twitter dataset (from Feb. to Oct., 2009) used by [5]. The other four include more recent tweets acquired in one month via Twitter X  X  keyword tracking API 2 . As we do not have access to the respective users X  social networks for these four, we set their weights of tweets w i to the default value of 1. Details of the datasets are listed in Table 2.

Ground truth for summaries As no previous work has conducted similar study on continuous summarization, we have to build our own ground truth (reference summaries). However, manual creation of these summaries is apparently impractical due to the huge size of the datasets.
Thus, we employ a two-step method to obtain fair-quality reference summaries without tremendous human labor: 1) Given a time duration, we first retrieve the correspond-ing tweet subset, and use the following three well-recognized summarization algorithms to get three candidate summaries.
ClusterSum [13]: first clusters the tweets and then sum-marizes each cluster by picking the most weighted post ac-cording to the hybrid TF-IDF weighting described in [13].
LexRank [6]: first builds a sentence similarity graph, and then selects important sentences based on the concept of eigenvector centrality.

DSDR [11]: models the relationship among sentences us-ing linear reconstruction, and finds an optimal set of repre-sentative sentences to approximate the original documents, by minimizing the reconstruction error. 2) Next, for each subset, the final reference summary is manually extracted from three candidate summaries. Pri-ority is given to those sentences appearing in at least two candidate summaries. The length of each summary is lim-ited to 250 words.

Ground truth for timelines For the timeline gener-ation test, the reference timelines are manually produced. We choose the  X  X rsenal X  and  X  X helsea X  datasets for this ex-periment, as the ground truth for sport topics are relatively easier to build manually. Specifically, we read through all the related news during one month from news websites (Yahoo! and ESPN), and select those dates as nodes on the refer-ence timeline when important events happen, e.g., football matches, players X  signing of new contracts, etc. 3
Baseline methods Most existing summarization meth-ods are not designed to handle continuous summarization. However, they can be adapted to streaming data by using a sliding window scheme. As illustrated in Figure 8, each window contains a certain number (window size) of tweets which are summarized as a document. After that, the win-dow moves forward by a step size , so that the oldest tweets are discarded and the new ones are added into the window. http s://dev.twitter.com/docs/api/1.1/post/statuses/filter
Our datasets and ground truth are available at http://db.zju.edu.cn/s/sumblr/. In this way, we implement the sliding window version of the above three algorithms, namely ClusterSum , LexRank , and DSDR . The windows are dimensioned by number of tweets instead of time duration, because the number of tweets may vary dramatically across fixed-length durations, leading to very poor performance of the baseline algorithms.
Evaluation method We apply the popular ROUGE toolkit [15] for evaluation. Among supported metrics, ROUGE-1 has been demonstrated to be the most consistent with hu-man judgement [16]. Considering the short and informal nature of the tweet contents, we decide that ROUGE-1 is suitable for measuring tweet summaries.

To evaluate the metrics on a continuous time period [ T 0 we have to calculate the integral of the metric over the pe-riod, which is given by  X  The integral requires unaffordable number of samplings dur-ing the period. In practice, we only sample the metrics by each arrival of a certain number of new tweets. This number is called the sampling interval . Note the sampling interval must be no greater than the step size.

In our experiments, we find similar trends in the compar-ison of precision, recall and F-score between the proposed approach and the baseline methods. Therefore, we shall only report the F-score results to save space. The F-score results presented are averaged on all five datasets.
In this section, we compare the F-scores and runtime cost between Sumblr and the three baseline algorithms (sliding window version). As tweets are often produced very quickly and reach a huge volume in a short while, it is hardly mean-ingful to summarize a small number of tweets. Thus the window size should be a relatively large one. In this ex-periment, we set window size to 20000, sampling interval to 2000. The step size varies from 4000 to 20000. The metrics are averaged over the whole stream.

Figure 5 and Figure 6 present the results for different step sizes. In Figure 5, we also give a baseline Random method, which selects tweets randomly from each window. Note that Sumblr is not affected by the step size, as it supports con-tinuous summarization inherently.
 The results show that when the step size is small (4000), DSDR and LexRank achieve better summary quality than Sumblr, at the expense of much more computation. When step size  X  12000, Su mblr outperforms all the baseline meth-ods in terms of both summarization quality and computation cost. Although the efficiency of LexRank is comparable with our method when step size  X  16000, its summary quality is significantly worse.

The above results reveal a major problem with the base-line methods: These methods rely on a small step size to produce quality summaries. Unfortunately, small step size leads to very frequent and expensive computations for win-dows. In contrast, Sumblr strikes a good balance between summary quality and efficiency.

Another issue to note here is that, as the ground truth is generated using these baseline methods, the summary qual-ity is to some extent biased in favor of them.
 The scalability experiment evaluates the efficiency results of a single window, while varying the window size. It simulates the case of a large burst of tweets in a short period.
Figure 7 presents the scalability results for different meth-ods. Note that the y-axis is in the log scale. We can see that our method outperforms the others significantly. When the data size is above 15000, Sumblr is faster than LexRank by nearly an order of magnitude, and outperforms the other two by more than that. The small fluctuations in Sumblr may be caused by the cluster deletions and merges. In this section, we tune the parameters in our approach. In each of the following experiments, we vary one parameter and keep the others fixed.

Effect of  X  . In Heuristic 1 we use  X  to determine whether to create a new cluster. Figure 9(a) and Figure 9(b) show its effect on summary quality and efficiency. When  X  is small, tweets related to different sub-topics may be absorbed into the same clusters, so the input of our summarization com-ponent is of low quality. At the same time, there are many focus tweets in each cluster, thus the time cost of cluster updating and summarization is high. When  X  increases, too many clusters are created, causing damage to both quality and efficiency. A good choice is  X  = 0 . 07 as it gives more balanced results.

Effect of N max . Figure 9(c) and Figure 9(d) depicts the performance of N max . For small N max s, many merging operations are conducted, which are time-consuming and produce lots of low-quality clusters. For large values, stream clustering is slow due to large number of clusters. Note that the storage overhead (both in memory and disk) is also higher for larger N max s. A balanced value for N max is 150.
Effect of mc . Another parameter in cluster merging is mc (0 &lt; mc &lt; 1). It does not have significant impact on efficiency, so we only present its quality results (Figure 9(e)). Small values of mc result in low-quality clusters, while large ones lead to many merging operations, which in turn reduce the quality of clusters. An ideal value for mc is 0 . 7.
Effect of m . As shown in Figure 9(f), the summary quality improves when m increases. When m  X  40, the improvement is not obvious. Meanwhile, a larger m incurs more storage overhead. We choose m = 40.

Effect of  X  . Finally we check the effect of  X  (0  X   X   X  1), which is a trade-off factor between content coverage and novelty. We gradually vary  X  from 0 to 1 at the step of 0.1 to examine its effect, as shown in Figure 9(g). When  X   X  0 . 7, the extreme emphasis on coverage causes performance loss. Therefore, we set  X  = 0 . 4 as a balanced factor.
One distinguishing feature of Sumblr is the flexibility to summarize tweets over arbitrary time durations. This fea-ture is provided by incorporating the PTF. The effective-ness of PTF depends on  X  and l (Section 3.3). We fix  X  at 2 and show the results varying l . For consistency, we extract a subset of one-month period from each dataset as the input stream. The interval between two successive snap-shots (timestamp unit) is one hour. For a timestamp ts , we evaluate the results for drill-down/roll-up summaries using durations with different length len . Due to space limit, we only present the average F-score for len varying from 1 day to 10 days, and report score ( ts ) by interval of 48 hours.
Figu re 10 gives the following observations:  X 
There exists a common trend for all l s: as a time dura-tion is closer to the current time, the summary quality improves. This is because PTF has finer granularity of snapshots for more recent moments. Thus the queried durations can be better approximated.  X 
For different values of l , the summary quality is similar for recent durations, but decreases in different degrees for early durations. The reason is that for recent moments, most of their snapshots or neighborhood snapshots are still kept in PTFs regardless of l ; while for early moments, their snapshots are more likely to be removed from PTFs with smaller l s, due to smaller capacity of each order. Figure 10: Quality on time duration  X 
A larger l leads to better results but more storage cost (the numbers in the parentheses represent the amounts of snapshots in PTF). This is obvious, as a larger l en-ables PTF to store more snapshots, which results in more accurate approximation and heavier storage burden.
For different applications, Sumblr can be customized with different l values. For example, for real-time summarization, a small l is enough; while for historical review, a large l is needed.
In this section, we evaluate the effectiveness of topic evolve-ment detection. We present the precision, recall, and F-score of the timeline nodes detected by our algorithm while vary-ing the decision threshold  X  .

From Figure 11, we can see that the recall declines as  X  increases. This is expected as higher threshold would ex-clude more promising candidates i.e., produce more false negatives. The precision and F-score both reach the highest value when  X  = 1 . 012. Although the recall drops when  X  increases, precision increases further (as more false positive nodes are excluded). Surprisingly, when  X  &gt; 1 . 012, preci-sion also drops. This may result from the incompleteness of our manual timeline or noises in the datasets.

The output for  X  X rsenal X  is presented in Table 3. It is interesting to find that our summary-based timeline not only detects important events (e.g. a match is called off due to a tube strike), but also contains popular public opinions (e.g. public calling for WENGER OUT).
We proposed a prototype called Sumblr which supported continuous tweet stream summarization. Sumblr employed a tw eet stream clustering algorithm to compress tweets into TCVs and maintain them in an online fashion. Then, it used a TCV-Rank summarization algorithm for generating online summaries and historical summaries with arbitrary time durations. The topic evolvement could be detected au-tomatically, allowing Sumblr to produce dynamic timelines for tweet streams. The experimental results demonstrated the efficiency and effectiveness of our method. For future work, we aim to develop a multi-topic version of Sumblr in a distributed system, and evaluate it on more complete and large-scale datasets.
 The work is supported by the National Science Foundation of China (GrantNo. 61170034).
