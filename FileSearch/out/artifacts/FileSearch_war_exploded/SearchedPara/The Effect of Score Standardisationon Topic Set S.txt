 Given a topic-by-run score matrix from past data, topic set size design meth-ods can help test collection builders determine the number of topics for a new test collection from a statistical viewpoint [ 8 ]. These methods enable test col-lection builders such as the organisers of evaluation conferences such as TREC, CLEF and NTCIR to improve the test collection design across multiple rounds of the tracks/tasks, through accumulation of topic-by-run score matrices and computation of better variance estimates.
 In this study, we apply a recently-proposed score standardisation method called std-AB [ 7 ] to score matrices before applying topic set size design, and demonstrate its advantages. A standardised score for a particular topic means how different the system is from an  X  X verage X  system in standard deviation units, and therefore enables cross-collection comparisons [ 14 ]. For topic set size design, std-AB suppresses score variances and thereby enables test collection builders to consider realistic choices of topic set sizes, and to handle unnormalised measures in the same way as normalised measures. In addition, even discrete measures that clearly violate normality assumptions look more continuous after applying std-AB , which may make them more suitable for statistically motivated topic set size design. Our experiments cover four different tasks from the recent NTCIR-12 conference 1 : MedNLP [ 1 ], MobileClick-2 [ 4 ], STC (Short Text Conversation) [ 11 ] and QALab-2 [ 12 ], and some of the official evaluation measure scores from these tasks kindly provided by the task organisers. The present study demonstrates the advantages of our score standardisation method called std-AB [ 7 ] in the context of topic set size design, which deter-mines the number of topics to be created for a new test collection [ 8 ]. This section situates these methods in the context of related work. 2.1 Power Analysis and Topic Set Size Design Webber/Moffat/Zobel, CIKM 2008. Webber, Moffat and Zobel [ 15 ]pro-posed procedures for building a test collection based on power analysis. They recommend adding topics and conducting relevance assessments incrementally while examining the achieved statistical power (i.e., the probability of detecting a between-system difference that is real) and re-estimating the standard devi-ation  X  t of the between-system differences. They considered the comparison of two systems only and therefore adopted the t -test; they did not address the problem of the family-wise error rate [ 2 , 3 ]. Their experiments focused on Aver-age Precision (AP), a binary-relevance evaluation measure. In order to estimate  X  t (or equivalently, the variance  X  2 t ), they relied on empirical methods such as 95 %-percentile computation.
 Sakai X  X  Topic Set Size Design. Unlike the incremental approach of Webber et al. [ 15 ], Sakai X  X  topic set size design methods seek to provide a straightforward answer to the following question:  X  X  have a topic-by-run score matrix from past data and I want to build a new and statistically reliable test collection. How many topics should I create? X  [ 8 ]. His methods cover not only the paired t -test but also one-way ANOVA for comparing more than two systems at the same time, as well as confidence interval widths. The present study focusses on the ANOVA-based approach, as it has been shown that the topic set sizes based on the other two methods can be deduced from ANOVA-based results. His ANOVA-based topic set size design tool 2 requires the following as input:  X ,  X  : Probability of Type I error  X  and that of Type II error  X  . m : Number of systems that will be compared ( m  X  2). minD : Minimum detectable range [ 8 ]. That is, whenever the performance differ- X   X  : Estimated variance of a system X  X  performance, under the homoscedasticity Sakai recommends estimating within-system variances  X  2 for topic set size design using the sample residual variance V E which can easily be obtained as a by-product of one-way ANOVA; it is known that V E is an unbiased estimate of  X  .Let x ij denote the performance score for the i -th system with topic j ( i =1 ,...,m and j =1 ,...,n ); let  X  x i  X  = 1 n n j =1 and  X  x = 1 m n m i =1 n j =1 x ij (sample grand mean). Then: If there are more than one topic-by-run matrices available from past data, a pooled variance may be calculated to improve the accuracy of the variance esti-mate [ 8 ]. However, this is beyond the scope of the present study, as we are interested in obtaining a future topic set size based on a single matrix from NTCIR-12 for each measure in each task.
 The present study uses the above method with existing NTCIR test collec-tions and propose topic set sizes for the next NTCIR rounds. Sakai and Shang [ 9 ] considered the problem of topic set size design for a new task, where we can only assume the availability of a small pilot topic-by-run matrix rather than a com-plete test collection. Based on reduced versions of the NTCIR-12 STC official Chinese subtask topic-by-run matrices, they conclude that accurate variance estimates for topic set size design can be obtained if there are about n =25 topics and runs from only a few different teams. 2.2 Score Standardisation Webber/Moffat/Zobel, SIGIR 2008. Webber, Moffat and Zobel [ 14 ]pro-posed score standardization for information retrieval evaluation with multiple test collections. Given m runs and n topics, a topic-by-run raw score matrix { raw ij } ( i =1 ,...,m , j =1 ,...,n ) is computed for a given evaluation measure. For each topic, let the sample mean be mean  X  j = 1 m i raw standard deviation be sd  X  j = 1 m  X  1 i ( raw ij  X  mean  X  j score is then given by which quantifies how different a system is from the  X  X verage X  system in stan-dard deviation units. Using standardised scores, researchers can compare systems across different test collections without worrying about topic hardness (since, for every j , the mean mean  X  j across runs is subtracted from the raw score) or nor-malisation (since the standardised scores, which are in the [ later mapped to the [0 , 1] range as described below). In practice, runs that par-ticipated in the pooling process for relevance assessments ( pooled systems )can also serve as the runs for computing the standardisation factors ( mean for each topic ( standardising systems )[ 14 ]. The same standardisation factors are then used also for evaluating new runs.
 chose to employ the cumulative density function (CDF) of the standard normal distribution. The main reason appears to be that, after this transformation, a score of 0.5 means exactly  X  X verage X  and that outlier data points are suppressed. Our Method: Std-AB. Recently, we proposed to replace the aforementioned CDF transformation of Webber et al. [ 14 ] by a simple linear transformation [ 7 ]: where A and B are constants. By construction, the sample mean and the standard deviation of std ij over the known systems are 0 and 1, respectively ( j =1 ,...,n ). It then follows that the sample mean and the standard deviation of lin ij are B and A , respectively ( j =1 ,...,n ). Regardless of what distribution raw ij follows, Chebyshev X  X  inequality guarantees that at least 89 % of the trans-formed scores lin ij fall within [  X  3 A, 3 A ]. In the present study, we let B =0 . 5as we want to assign a score of 0.5 to  X  X verage X  systems, and let A =0 . 15 so that the 89 % score range will be [0 . 05 , 0 . 95]. Furthermore, in order to make sure that even outliers fall into the [0 , 1] range, we apply the following clipping step: This means that extremely good (bad) systems relative to others are all given a score of 1 (0). Note that if A is too small, the achieved range of std-AB scores would be narrower than the desired [0 , 1]; if it is too large, the above clipping would be applied to too many systems and we would not be able to distinguish among them. The above approach of using A and B with standardis-ation is quite common for comparing students X  scores in educational research: for example, SAT (Scholastic Assessment Test) and GRE (Graduate Record Exam-inations) have used A = 100 ,B = 500 [ 5 ]; the Japanese hensachi ( X  X tandard score X ) uses A =10 ,B = 50.
 CDF-based method of Webber et al. : std-AB ensures pairwise system compar-isons that are more consistent across different data sets, and is arguably more convenient for designing a new test collection from a statistical viewpoint. More specifically, using a small value of A ensures that the variance estimates  X   X  will be small, which facilitates test collection design, as we shall demonstrate later. Moreover, as score normalisation becomes redundant if we apply stan-dardisation [ 14 ], we can handle unnormalised measures (i.e., those that do not lie between 0 and 1). Furthermore, even discrete measures (i.e., those that only have a few possible values), which clearly violate the normality assumptions, look more continuous after applying std-AB . While our previous work was lim-ited to the discussion of TREC robust track data and normalised ad hoc IR evaluation measures, the present study extends the work substantially by exper-imenting with four different NTCIR tasks with a variety of evaluation measures, including unnormalised and discrete ones for the first time. The core subtask of the MedNLPDoc task is phenotyping : given a medical record, systems are expected to identify possible disease names by means of ICD (Inter-national Classification of Diseases) codes [ 1 ]. Systems are evaluated based on recall and precision of ICDs. MedNLPDoc provided us with a precision matrix with n = 78 topics (i.e., medical records) and m = 14 runs, as well as a recall matrix with n = 76 topics and m = 14 runs.
 The MobileClick-2 task evaluates search engines for smartphones. Systems are expected to output a two-layered textual summary in response to a query [ 4 ]. The basic evaluation unit is called iUnit , which is an atomic piece of factual informa-tion that is relevant to a given query. In the iUnit ranking subtask, systems are required to rank given iUnits by importance, and are evaluated by nDCG (nor-malised discounted cumulative gain) and Q-measure .Inthe iUnit summarisation subtask, systems are required to construct a two-layered summary from a given set of iUnits. The systems are expected to minimise the reading effort of users with different search intents; for this purpose the subtask employs a variant of the intent-aware U-measure [ 6 ], called M-measure [ 4 ], which is an unnormalised measure. MobileClick-2 provided us with 12 topic-by-run matrices in total: six from the English results and six from the Japanese results. While the variances of the unnormalised M-measure are too large for the topic set size design tool to handle, we demonstrate that the problem can be solved by applying std-AB . The STC (Short Text Conversation) task requires systems to return a human-like response given a tweet (a Chinese Weibo post or a Japanese twitter post) [ 11 ]. Rather than requiring systems to generate natural language responses, however, STC makes them search a repository of past responses (posted in response to some other tweet in the past) and rank them. The STC Chinese subtask provided us with three matrices, representing the official results in nG@1 (normalised gain at 1), P + (a variant of Q-measure), and nERR@10 (normalised expected reciprocal rank at 10), all of which are navigational intent measures [ 10 ]. The QALab-2 task tackles the problem of making machines solve university entrance exam questions. From the task organisers, we received two matrices based on National Center Test multiple choice questions, one for Phase-1 (where question types are provided to the system) and one for Phase-3 (where ques-tion types are not provided). As each topic is a multiple choice question, the evaluation measure is  X  X oolean X  (either 0 or 1). nG@1 for STC takes only three values: 0, 1/3 or 1 [ 10 ], and Boolean for QAlab-2 takes only two values: 0 or 1. These clearly violate the normality assumptions behind ANOVA: x ij  X  N (  X  i , X  2 ) for each system i . Thus, it should be noted that, when we apply topic set size design using the variances of these raw measures, what we get are topic set sizes for some normally distributed mea-sure M that happens to have the same variance as that discrete measure, rather than topic set sizes for that measure per se. Whereas, if we apply std-AB , these measures behave more like continuous measures, as we shall demonstrate later. 4.1 Results Overview Table 1 Columns (e) and (f) show the variance estimates obtained by applying Eq. 1 to the aforementioned topic-by-run matrices, before and after performing std-AB as defined by Eq. 3 . It can be observed that the variances are sub-stantially smaller after applying std-AB . This means that the required topic set sizes will be smaller, provided that the tasks take up the habit of using std-AB measures. For each subtask (and language), we selected the largest raw score variance, shown in bold in Column (e), and plugged into the topic set size design tool (except for the unnormalised M-measure, whose variances were too large for the tool to handle); that is, we focus on the least stable measures to obtain topic set sizes that are reliable enough for all evaluation measures. We then used the variances of the corresponding std-AB measures, shown in bold in Column (f).
 Now, how would std-AB actually affect the official results? Table 1 Column (g) compares the run rankings before and after applying std-AB in terms of Kendall X  X   X  for each evaluation measure in each subtask. The 95 % confidence intervals show that the two rankings are statistically equivalent for all cases, except for nDCG@5 in MobileClick English iUnit ranking whose 95 % CI is [.433, .993]. These results suggest that, by and large, std-AB enables cross-collection comparisons without affecting within-collection comparisons.
 Table 2 shows the recommended topic set sizes with  X  =0 . 05 , X  =0 . 20 (Cohen X  X  five-eighty convention [ 3 ]), for several values of m (i.e., number of systems to be compared) and minD (i.e., minimum detectable range), based on the variances shown in bold in Table 1 . It should be noted first, that the values of minD are not comparable across Parts (a) and (b). For example, a minD of 0.02 with raw scores and a minD of 0.02 with std-AB scores are not equivalent, because std-AB applies score standardisation (Eq. 2 ) followed by a linear transformation (Eq. 3 ). Nevertheless, it can be observed that, after applying std-AB , the choices of topic set sizes look more realistic. For example, let us consider the m = 2 row in Table 2 (I). If we want to guarantee 80 % power whenever the difference between the two systems is minD =0 . 05 (i.e., 5 % of the score range) or larger in raw recall, we would require 369 topics. Whereas, if we want to guarantee 80 % power whenever the difference between the two systems is minD =0 . 05 (i.e., 5 % of the score range) or larger in std-AB recall, we would require only 79 topics. Although the above two settings of minD mean different things, the latter is much more practical. In other words, while ensuring 80 % power for a minD of 0.05 in raw recall is not realistic, ensuring the same power for a minD of 0.05 in std-AB is.
 some of our data. Below, we discuss the effect of std-AB on recommended topic set sizes for each task in turn. 4.2 Recommendations for MedNLPDoc The effect of std-AB on the recall scores from MedNLPDoc can be observed by comparing Fig. 1 (a) and (a X ). Note that while many of the raw recall values are 0 X  X , all values are positive after applying std-AB . Moreover, there are fewer 1 X  X  after applying std-AB .
 lection would be as follows. If the task is continuing to use raw recall, then:  X  Create 100 topics: this guarantees 80 % power for comparing any m = 2 sys-tems with a minD of 0.10 (93 topics are sufficient), and for comparing any m = 50 systems with a minD of 0.20 (91 topics are sufficient);  X  Create 50 topics: this guarantees 80 % power for comparing m = 10 systems with a minD of 0.20 (48 topics are sufficient).
 Whereas, if the task adopts std-AB recall, then:  X  Create 80 topics: this guarantees 80 % power for comparing m = 2 systems with a minD of 0.05 (79 topics are sufficient), and for comparing m =50 systems with a minD of 0.10 (77 topics are sufficient).
 Note that MedNLPDoc actually had 76 X 78 topics (Table 1 (d)), and therefore that the above recommendation is quite practical. 4.3 Recommendations for MobileClick-2 The effect of std-AB on the nDCG@3 scores from MobileClick-2 iUnit ranking (English) can be observed by comparing Fig. 1 (b) and (b X ). It can be observed that, after applying std-AB , the scores are more evenly distributed within the [0 , 1] range. Similarly, the effect of std-AB on the unnormalised M-measure from MobileClick-2 iUnit summarisation (English) can be observed by comparing Fig. 1 (c) and (c X ). Note that the scale of the y -axis for Fig. 1 (c) is very different from others. Despite this, Fig. 1 (c X ) shows that std-AB transforms the scores into the [0 , 1] range without any problems. In this way, std-AB can handle any unnormalised measure. Put another way, if we take up the habit of using std-AB scores, normalisation becomes no longer necessary.
 Since MobileClick-2 is a multilingual task, let us discuss topic set sizes that work for both English and Japanese. Moreover, since the topic set is shared across the iUnit ranking and summarisation subtasks, we want topic set sizes that work across these two subtasks. From Table 2 (II) and (III), a few recommendations for future MobileClick test collections would be as follows. If the task is continuing to use raw nDCG@3, then:  X  Create 90 topics: this guarantees 80 % power for comparing any m = 10 Eng-lish iUnit ranking systems with a minD of 0.10 (82 topics are sufficient), and for comparing any m = 2 Japanese iUnit ranking systems with a minD of 0.10 (88 topics are sufficient).
 However, the above setting cannot guarantee anything for the iUnit summari-sation task, due to the use of the unnormalised M-measure. In contrast, if the tasks adopts std-AB nDCG@3 and std-AB M-measure, then:  X  Create 100 topics: this guarantees 80 % power for comparing any m =20
English iUnit ranking systems with a minD of 0.10 (89 topics are sufficient), and for comparing any m = 30 Japanese iUnit ranking systems with a minD of 0.10 (86 topics are sufficient), and for comparing any m = 10 English iUnit summarisation systems with a minD of 0.05 (91 topics are sufficient), and for comparing any m = 10 Japanese iUnit summarisation systems with a minD of 0.05 (97 topics are sufficient).
 Thus being able to handle unnormalised measures just like normalised measures seems highly convenient. Also, recall that MobileClick-2 actually had 100 topics. 4.4 Recommendations for STC The effect of std-AB on the nG@1 scores from STC (Chinese) can be observed by comparing Fig. 1 (d) and (d X ). It can be verified from Fig. 1 (d)thatnG@1 indeed take only three values: 0, 1/3 and 1. Whereas, Fig. 1 (d X ) shows that std-AB nG@1 is more continuous, and that there are fewer 1 X  X , and no 0 X  X . would be as follows. If the task is continuing to use raw nG@1, then:  X  Create 120 topics: this guarantees 80 % power for comparing any m =20 systems with a minD of 0.20 (118 topics are sufficient);  X  Create 90 topics: this guarantees 80 % power for comparing any m =10sys-tems with a minD of 0.20 (exactly 90 topics are needed).
 But note that, strictly speaking, the above recommendations are for normally distributed measures that have a variance similar to that of nG@1, since nG@1 takes only three values. Whereas, if the tasks adopts std-AB nG@1, then:  X  Create 100 topics: this guarantees 80 % power for comparing any m =30 systems with a minD of 0.10 (94 topics are sufficient).
 The STC task actually had 100 topics; this was actually a decision based on topic set size design with raw evaluation measures and pilot data [ 10 ]. 4.5 Recommendations for QALab The effect of std-AB on the Boolean scores from QALab Phase-3 can be observed by comparing Fig. 1 (e) and (e X ). It can be observed that std-AB trans-forms the raw Boolean scores (0 X  X  and 1 X  X ) into something a little more continu-ous, but that the resultant scores still fall into two distinct score ranges; hence our topic set size design results for QALab should be taken with a large grain of salt even after applying std-AB as the scores are clearly not normally dis-tributed. The reason why the std-AB scores are monotonically increasing from left to right is just that the QALab organisers sorted the topics by the number of systems that correctly answered them before providing the matrices to the present author. This is equivalent to sorting the topics by mean order, i.e., easy topics first).
 From Table 2 (V), a few recommendations for a future STC test collection would be as follows. If the task is continuing to use raw Boolean, then:  X  Create 90 topics: this guarantees 80 % power for comparing any m = 2 systems with a minD of 0.20 (82 topics are sufficient).
 Whereas, if the tasks adopts std-AB Boolean, then:  X  Create 40 topics: this guarantees 80 % power for comparing any m = 2 systems with a minD of 0.10 (32 topics are sufficient), or any m = 50 systems with a minD of 0.20 (31 topics are sufficient).
 But recall that the above recommendations are for normally distributed measures whose variances happen to be similar to those of the Boolean measures. QALab-2 Phase-3 actually had 36 topics only. Note that n = 36 is not sat-isfactory in any of the settings shown in Table 2 (V)(a); n = 36 does not even satisfy the suggested setting shown above for (a normally distributed equivalent of) std-AB Boolean. These results suggest that the QALab task should have more topics to ensure high statistical power. Using topic-by-run score matrices from the recent NTCIR-12 MedNLPDoc, MobileClick-2, STC and QALab tasks, we conducted topic set design experi-ments with and without score standardisation and demonstrated the advantages of employing std-AB in this context. It is clear from our results that std-AB suppresses score variances and thereby enables test collection builders to consider realistic choices of topic set sizes, and that it can easily handle even unnormalised measures such as M-measure. Other unnormalised measures such as Time-Biased Gain [ 13 ], U-measure [ 6 ] and those designed for diversified search may be han-dled similarly. Furthermore, we have demonstrated that discrete measures such as nG@1, which clearly violate the normality assumptions, can be  X  X moothed X  to some extent by applying std-AB . Recall that topic set size design assumes that the scores are indepent and identically distributed: that the scores for sys-tem i obey N (  X  i , X  2 ). While this is clearly a crude assumption especially for unnormalised and discrete measures, std-AB makes it a little more believable at least, as shown in the right half of Fig. 1 .
 robustness of standardisation factors mean  X  j , sd  X  j for handling unknown runs (i.e., those that contributed to neither pooling nor the computation of standard-ising factors). However, our experiments were limited to handling unknown runs from the same round of TREC. Hence, to examine the longevity of standardisa-tion factors over technological advances, we have launched a new web search task at NTCIR, which we plan to run for several years 3 . The standardisation factors obtained from the first round of this task will be compared to those obtained from the last round: will the initial standardisation factors hold up against the latest, more advanced systems?
