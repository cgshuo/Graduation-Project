 REGULAR PAPER Rajagopalan Srinivasan Abstract Agile manufacturing is the capability to prosper in a competitive envi-ronment of continuous and unpredictable changes by reacting quickly and effec-tively to the changing markets and other exogenous factors. Agility of petroleum refineries is determined by two factors  X  ability to control the process and ability to efficiently manage the supply chain. In this paper, we outline some challenges faced by refineries that seek to be lean, nimble, and proactive. These problems, which arise in supply chain management and operations management are seldom amenable to traditional, monolithic solutions. As discussed here using several ex-amples, methodologies drawn from artificial intelligence  X  software agents, pat-tern recognition, expert systems  X  have a role to play in this path toward agility. Keywords Petroleum refining  X  Supply chain management  X  Decision support  X  Enterprise-wide optimization  X  Process supervision  X  Fault diagnosis  X  Pattern recognition 1 Introduction The refining environment today is a fast changing one. Reformulated products are replacing classical distilled ones. Volatile crude prices, imbalance between refin-ing capacity and crude availability, and fluctuating product demands is the name of the game. To survive and prosper in this scenario, refineries are required to be agile. Agility is the ability to proactively respond to a changing environment through responsiveness to customers, product and process innovation, and opera-tional flexibility. Agility of petroleum refining is determined by two factors: 1. Process control: The ability to operate and control the production process to 2. Supply chain management: The capability to optimize and manage the sup-new technologies. Data is collected in real time, not only from the process but also from the business. Automation applications have been devored by the indus-try with the hope of extracting information from data. In the process operations arena, advanced control and unit optimization has become commonplace. Expert systems and neural networks are also being increasingly used. Abnormal situation management has come a long way from a buzzword to near reality. The business operations field also has not been left behind from this incessant push of tech-nology. Vendors are touting systems that integrate different parts of the business, from crude oil acquisition to terminal and depot management, thus cutting across the complex web of logistics, manufacturing, and sales. In such an environment, where it may seem that all that can be solved has been solved, we will outline two areas where artificial intelligence methodologies promise further benefits. The first, as discussed in Section 2 , relates to optimization at the enterprise level, and the second to management at the process operations level, as discussed in Sect. 3 . 2 Supply chain management Since most refineries are capable of producing a variety of products and grades, plant personnel have to answer the following questions on a regular basis: 1. What products to sell? 2. When and where should they be produced? 3. What raw materials are needed, when and from whom should they be bought? face of highly competitive markets and constant pressure to reduce lead times, en-terprises today consider SCM to be the key area where improvements can signifi-cantly impact the bottom line. The refinery supply chain spans numerous internal departments and several suppliers and customers. Decision-making is distributed across departments. Figure 1 shows the major supply chain functions in a typical chemical enterprise, and their roles and timescales. Forecasting provides the fu-ture requirements of markets or specific customers typically for the next 12 X 18 months. Based on these, Planning sets the operating targets and provides coordi-nation among the different departments X  X ales, materials management, production and distribution. The time horizon here is  X  X ong, X  typically 2 X 6 months. Various types of uncertainties including those arising from the supply and demand chains as well as from the production are aggregated at this stage. The Scheduling func-tion defines the specific Source and Make activities to be performed over a horizon of a few days to a week. Both the timing and volume of specific activities are spec-ified at this stage. The key drivers are product demand, inventory levels, as well as operational factors such as the desire to keep process units operating continuously and containment issues. The operations and control function ensures the execu-tion of the schedule and is concerned with time horizons of seconds to hours. It involves the real-time determination of selected production variables so as to hold product qualities and production rates constant (as specified by the schedul-ing function previously). Key considerations here are rejecting short-term process disturbances, insuring process safety and containment, and minimizing energy and utility costs. When the schedule dictates a switch from one raw material/product (or mix) to another, operations and control has to ensure that the process transition is achieved in an optimal way.
 individual problems such as scheduling of crude oil supply, distribution of crudes to distillation units, pipeline scheduling, jetty and storage tank scheduling, etc. We can see [ 26 ] for a review of refinery planning and scheduling. But local improve-ments do not necessarily assure that the overall supply chain is moving towards an optimum. Many times, the objectives of the departments are conflicting and decisions do not contribute positively to the overall performance of the refinery. A mere electronic integration of existing tools does not solve the problem; rather, there is a need for a unified approach to supply chain optimization and, in turn, to modeling and analysis of supply chains, one that explicitly captures the interac-tions among enterprises and within the departments of an enterprise.
 thus, are well suited for emulating supply chain entities. Julka et al. [ 14 ]usedsoft-ware agents to model the crude procurement process in a refinery. They also report a simulation-based decision support that assists in evaluating refinery policies, plant hardware configurations, and respond to external changes. Their Petroleum Refinery Integrated Supply chain Modeler and Simulator (PRISMS) models the individual departments and the business processes within the refinery: 1. Procurement: This department is the coordinator of the crude oil procurement 2. Sales: The sales department provides the present and forecasted product prices 3. Storage: The storage department is responsible for tank and jetty scheduling. 4. Operations: This department decides upon the crudes to be run through the 5. Logistics: The logistics department is responsible for arranging the tanker [ 13 ]. The tasks of each agent are specified using the Grafcet graphical language. Figure 2 shows the Grafcet model for the procurement agent. All information and material flow is emulated by the exchange of messages between the agents. The external elements in the supply chain  X  the crude oil exchange, oil suppliers, and 3PLs  X  are also similarly modeled.
 refinery by specifying the organization details, practices and policies, and process configuration. Then the supply chain model can be simulated, and key perfor-mance indices (KPIs) such as production, inventory, and crude quality as well as supply demand curves can be calculated. Figure 3 shows the evolution of one KPI  X  comparison between the target and actual production  X  over a 60-day horizon for a given set of conditions. Such stochastic simulation-based studies can be used to analyze the impact of procurement policy packet size, (fixed/variable packets), effect of market fluctuations (product demand, crude price changes), impact of configuration change (extra storage), etc, on the supply chain performance. They can also be used to identify the impacts of disruptions in the different entities of the supply chain [ 5 ].
 mathematical programming ones, is that it is more versatile and can easily capture the qualitative as well as the transactional nature of the supply chain. The entire supply chain can be modeled and an overall solution can be obtained. Also, spe-cific agents can be embedded with the detailed solution techniques used by the individual departments, e.g., LPs for tank scheduling or crude distribution to dis-tillation units [ 28 , 29 ]. Thus, it does not preclude the tools that are currently used; ery. Other benefits relate to improved visibility into the entire supply chain. The approach can also be extended to consider supply chain monitoring, exception handling, and disruption management [ 5 ]  X  an increasingly important problem [ 19 ]. Subsequently, robust supply chain designs can be identified [ 1 ], or reactive strategies can be evolved [ 2 , 3 ]. 3 Operations management Next, let us consider the processing that follows once crude has been procured. Refineries commonly operate on 10 X 20 different crude mixes. It is not uncommon for a refinery to undergo transitions 3 X 5 times a week due to crude switching alone. Crude switches are often plagued with uncertainties, since the composition (assay) of the new mixture is known only approximately. Stratification caused by poor mixing in the crude tanks, addition of slops, and change in crude properties during crude production lead to changes in the quality of crude. Operators have to take a number of complex control decisions to bring about the transition. In addition to setting throughput rates and changing the flowrates of the different side streams at the right times, they have to control qualities and diagnose distillation problems in the face of extensive dynamics and interactions between the variables. Operators therefore resort to conservative operational targets while switching to new crudes or mix ratios, and put up with less yield or poor product qualities for long durations while waiting for the process to settle down to new steady-state conditions. This period of suboptimal operation could extend to several hours resulting in a large economic impact. Refineries also operate and switch between modes aimed to maximize kerosene, naphtha, diesel, or gas oil, which lead to transitions.
 fineries. Refineries have therefore developed specialized techniques for minimiz-ing the impact of crude changes, and substantial economic benefits have been reported  X  3% increase in throughput, 1.5% increase in the yield of white prod-ucts, and 5% decrease in fuel consumption [ 10 ]. Reduction (50% or more) in the time to reach the new operating point has also been reported [ 42 ].
 plications such as control, alarm, and monitoring systems are normally configured for a single steady-state operation and do not function satisfactorily during tran-sitions. The applications themselves are unaware of the limited domain to which they are suited and continue to function, oblivious to plant state changes. As a result, during transitions, these applications signal abnormalities (such as false alarms) even when a desired change is occurring. Operators therefore commonly disable the alarm system, ignore, or override their results. The complete absence of functional alarm management or monitoring systems during transitions renders safe process operation entirely the operators X  responsibility  X  clearly a steep ex-pectation [ 24 , 25 ]. Given the numerous actions that operators have to perform dur-ing transitions, they could be caught unaware if an abnormal situation arises. This is compounded by the fact that the probability of an abnormal situation occurring because of human error is significantly higher during transitions. It is therefore imperative to monitor and manage transitions efficiently in order to avert abnor-malities. Besides these difficulties in process monitoring, process transitions also affect the functioning of advanced control and optimization strategies that make use of mode-specific models. A coherent solution to transition management is hence essential.
 transitions is to make them cognizant of the domain of their applicability. In case of alarm management, this would mean dynamic reconfiguration of alarm settings [ 12 ]. A two-tier approach (see Fig. 4 ) is suitable for extending process automa-tion applications to the transition domain. The top-tier consists of a supervisory system, which identifies the current state  X  operating mode or transition  X  of the process. The second-tier consists of the existing process automation applications like controllers, alarm management systems, monitoring systems, etc, that have been extended to be state  X  X onscious X  and are aware of the states to which they are applicable. The supervisor detects state changes and broadcasts it as a mes-sage to the applications, which then reconfigure themselves to the new state, if necessary.
 tween two classes of states  X  modes and transitions [ 36 ]. Clearly, a plant can oper-ate in several modes or undergo several transitions over a period of time. Different sections/units of the plant may exist in different states. The state of a unit relates to the values of one or more of its variables  X  input, state, or output variables  X  and relates to its operational condition. The state is not an independent variable but is manifested through the values and behavior of the variables related to the pro-cess unit. Conversely, the state of the process unit is aggregated from the state of the process variables. A mode corresponds to the continuous operation of the unit and corresponds to a fixed flowsheet configuration, i.e., no equipment is brought online or taken offline, and the unit operates in a quasi-steady state. Transitions correspond to discontinuities in the plant operation such as change of setpoint, change of equipment configuration, etc. Thus, during a transition, some of the variables would show a significant change. Recognizing this, a mode can be de-fined as the period where all process variables remain nearly constant. The process is said to undergo a transition when any variable shows a nonconstant trend. measured variable would display a constant trend. Figure 5 shows examples of modes and transitions during the startup of a FCCU preheater, along with the pro-files of some important variables. Key variables (notated as 16FC105, 16FC107, and 16TI102 in Fig. 5 ), which exhibit considerable variation within a transition, form the basis for characterizing transitions. A transition is described using the temporal evolution of the key variables. Identified modes and transitions can be compared among themselves to determine repetitions in the training data. One ad-vantage of this approach is that it can be used both in an offline fashion on histor-ical data and in an online mode during process operation. Offline analysis can aid in better understanding of the operation practices and lead to process and produc-tivity improvement. The online analysis would enable state-conscious automation as described above. In the following, we review a few of them. The methods can be broadly catego-rized into two groups  X  intelligent systems and model-based approaches as de-scribed next. 3.1 Intelligent systems Intelligent systems can be based on different techniques including qualitative trend analysis, rule-based, and signal processing methods. 3.1.1 Qualitative trend analysis Qualitative trend analysis (QTA) is a monitoring technique that is based on the hierarchical abstraction of process data into sets of trends. Monitoring is usually done on process trends that are made up of primitives, which describe the quali-tative behavior of the process variables [ 30 ]. It has been shown that regular trend analysis is not sufficient to monitor transitions adequately [ 40 ]. Classical trend analysis approaches are based on monitoring an ordered set of primitives that make up the trend of a process variable. When a fault occurs, process variables vary from their nominal values and ranges, exhibiting trends that are characteris-tic of the fault. Hence, different faults can be mapped to their characteristic trend signatures. For performing fault diagnosis, the current process state (normal or abnormal) and the specific fault in the case of an abnormal state can be detected from the trend displayed by the process variables. The same is not true during process transitions, since each variable might display a different trend during dif-ferent phases of the transition. Also, the trends during transitions are temporal, each trend only occurs at a particular time during a particular step of the operating procedure. The trends observed during different stages might be totally different or could have different meanings even if they are the same. The problem can be overcome through enhanced trends analysis [ 40 ]. Enhanced trend is composed of an ordered sequence of enhanced atoms, which consists of shape, duration of trend manifestation, and magnitude at the start and end of the trend. The enhanced trends identified during real time can be compared to the trends dictionary, com-puted offline from normal operating data set. Multiple types of matching degree, e.g., shape matching degree, magnitude matching degree, and duration matching degree have also been introduced for improved precision during transition moni-toring. The proposed approach has been successfully tested on a pilot-scale distil-lation column and a simulated FCCU, and was found to detect equipment failures, sensor problems, and human errors accurately. 3.1.2 Rule-based systems Rule-based systems, sometimes referred to as expert systems, use rules to per-form monitoring. They are best suited for situations where plant operators have good knowledge of the nuances of transitions and the underlying process. For developing an expert system, process-specific knowledge has to be first acquired from experienced plant personnel and manuals. Using these, inference rules can be developed and used for online monitoring. A rule-based expert system for au-tomating the startup of a refinery unit was reported in [ 27 ].
 tomation. A sequential control model was proposed in [ 21 ] to perform phase-based control, where Grafcets are used to describe sequential control actions and pro-vide a structural and behavioral description of how and when to perform control actions. The sequence of operations for performing the transition is represented as a sequence of Grafcet steps separated by Grafcet transitions. Each Grafcet step is a distinct subset of the standard operating procedures. The necessary process ac-tions for executing the transition operation can be carried out automatically when a step is active. Thus, the process execution can be automated thereby reducing the likelihood of operator errors. 3.1.3 Signal processing methods Signal processing methods can be used to compare the online profile of process variables with those from previously known normal runs to decide on the nor-mal/abnormal status. One of the important issues when such methods are used for monitoring transitions is that run-to-run and operator-to-operator variations are significant. Thus, an operation may be performed faster or slower in two runs, re-sulting in significant variation in the process variable profiles, but this may not be indicative of abnormality. To overcome this challenge, methods such as dynamic time warping (DTW) use dynamic programming (DP) to carry out time normal-ization between two signals. Two signals can be synchronized by stretching or compressing their time axis so as to minimize the difference between them. DTW originated from the area of speech recognition and has found application in the chemical engineering domain recently. Some applications of DTW for process monitoring can be found in [ 15 ]. Though accurate, one known issue of DTW is the expensive computational cost, which grows exponentially with the number of data points. This can be minimized by using features such as peaks or local min-ima in the signals. These features, called singular points, can be aligned across runs [ 35 ]. Singular points can be used first to decompose a long continuous signal into multiple, short, and semicontinuous ones. DTW is subsequently applied to perform monitoring during transitions [ 35 ]. 3.2 Model-based systems First-principle models, statistical models, and neural network-based approaches are grouped under model-based systems. 3.2.1 First-principle model-based techniques Monitoring techniques can be based on a quantitative model also  X  either a first-principle model or black-box model developed using input X  X utput data [ 9 ]. The model is used for residuals generation by comparing the predicted (normal) out-puts with the actual outputs. In Bhagwat et al. [ 7 ], a nonlinear model-based ap-proach is used to monitor process transitions. Estimation of process states and residuals is achieved through open-loop observers and extended Kalman filters. To address the issues arising from the discontinuous nature of transition operations, the scheme incorporates the knowledge of standard operating procedures. It is based on the structured division of transitions into phases; each phase corresponds to a model component, and different filters and observers are selected for fault de-tection in that phase. The disadvantage of this approach is that accurate models of highly complex processes operating in multiple regimes are difficult to develop. This limits the practicality of this nonlinear models-based approach. In Bhagwat et al. [ 8 ], a multilinear model-based fault detection scheme during process transi-tions is proposed to overcome this shortcoming. Under the proposed framework, nonlinear process is first decomposed into multiple local-linear regimes, where Kalman filters and open-loop observers can be used for state estimation and resid-ual generation based on linear models. Analysis of residuals using thresholds, fault tags, and logic-charts enables online detection and isolation of faults. 3.2.2 Statistical methods With the increasing availability of inexpensive sensors, the number of measured variables ranges in thousands for most industrial processes. This has lead to the popularity of statistical methods. The application of multivariate statistical anal-ysis can bring forth powerful, yet simple ways to monitor transitions. Principal components analysis (PCA) is one of several multivariate statistical techniques that is capable of compressing plant operating data, while retaining critical infor-mation of the process. PCA finds variables that describe the dominant directions of the process operation through eigenvector decomposition of the covariance or cor-relation matrix of the variables. When PCA is applied to multistate operations, it can adequately differentiate between the different modes, which would form com-pact well-separated clusters in the score plot. This was demonstrated clearly for production of different grades of polypropylene by [ 32 ]. Multiple, distinct PCA models can also be developed for each mode. The collection of PCA models can be used for monitoring the process when it is in one of the modes. This was termed as Super-PCA by [ 11 ].
 during transitions due to the nonstationary behavior of the process and the time-lagged dynamics. In order to overcome this, an extension called dynamic PCA (DPCA) has been proposed [ 18 ]. In Srinivasan et al. [ 37 ], DPCA has been used to classify process states based on historical operating data. In the proposed method-ology, process data is first segmented into modes and transitions. Steady-state modes are identified by using a moving window approach, which is capable of rejecting outliers. Other periods of operation are deemed as transitions. Next, the process states are clustered. Modes are compared using the mean operating con-ditions across all variables. A DPCA-based similarity factor is used to compare transitions. This type of domain knowledge can greatly simplify the process of fault isolation when a known (previously encountered) fault occurs. The ability to continuously update the model engenders the adaptation and learning property. 3.2.3 Neural networks Neural networks have been popular for classification and function approximation. Theoretically, artificial neural networks can approximate any well-defined non-linear function with arbitrary accuracy. But unfortunately, there is no universal criterion for selecting a specific structure for a practical application. Usually, the structure of the network is decided based on the input dimensionality and the com-plexity of the underlying classes. A typical chemical process section has hundreds of sensors, each generating thousands of observations every day. This data is noisy and contains numerous patterns from different operating states. The construction of an accurate neural classifier for such multivariate, multiclass temporal classifi-cation problem suffers from the  X  X urse of dimensionality. X  In Srinivasan et al. [ 39 ], two new neural network architectures namely one-variable-one-network (OVON), and one-class-one-network (OCON) are proposed. In both structures, the origi-nal classification problem is decomposed into a number of simpler classification problems. The OVON uses a sub-state identification layer, where a set of neural networks are used to identify simpler, univariate, and temporal patterns. A unifi-cation layer is subsequently used to infer the process state based on the substates, through multidimensional, static pattern recognition. On the other hand, a state-identification layer is used to identify the presence or absence of a temporal pattern in multidimensions for OCON; the state of the process is inferred by analyzing the static, multidimensional outputs from the state-identification layer. Comparisons with traditional networks indicate that the new neural networks architectures are simpler in structure, faster to train, and yield substantial improvement in classifi-cation accuracy.
 identification can also be addressed by suitably modifying the neural network ar-chitecture [ 38 ]. Context-dependency occurs when a pattern X  X  interpretation varies across contexts. In such situations, the resulting one-to-many mapping between patterns and their classes (states) cannot be adequately handled by traditional pat-tern recognition approaches that are designed for one-to-one or many-to-one map-pings. Features in the process measurements can be classified as primary  X  one that can be used in isolation for classification, and contextual  X  one that is not useful in isolation, but can be beneficial when combined with other primary features, and is irrelevant. Contextual features can be used to improve predictive performance of primary features. Srinivasan et al. [ 38 ] showed that process states in chemical and biological processing are context-dependent and proposed an operating state iden-tification neural network (OSINN) that recognizes the process state from online process measurements. The contextual feature is an additional input for fault iden-tification where it modifies the feature space so that the final mapping is reduced to the classical one-to-one or many-to-one situation.
 three dimensions) the operation of high-dimensional processes and thus aid oper-ators in process monitoring [ 22 , 33 ]. The self-organizing map was first proposed by [ 16 ] as a visualization tool. Here, self-organization means that the net orients and adaptively assumes a form to best describe the input vectors. SOM employs nonparametric regression of discrete, ordered reference vectors to the distribution of the input feature vectors. A finite number of reference vectors (called neurons) are adaptively placed in the input signal space to approximate the input signals. The visualization in SOM is usually done through a unified distance matrix (U-matrix), where the distances of each neuron to its neighbors are calculated and displayed through gray scale or color scale. The U-matrix thus shares similarities with the scores plot in PCA. Ng and Srinivasan [ 22 ] showed that modes in process operations correspond to tight clusters in the U-matrix while transitions lead to a well-ordered trajectory. State identification and process monitoring can be per-formed by identifying the best matching neuron for the online measurements at any point in time and tracking its temporal evolution (see Fig. 6 ). The trajectories of best matching neurons serve as state signatures and can be compared across runs to locate deviations.
 tions, such as alarm systems as well as new transition-specific automations, can be developed for the second tier. When the process undergoes a state change, these state  X  X onscious X  process automation applications reconfigure themselves as needed to remain relevant in the new state. Examples of reconfiguration in-clude turning modules on/off, changing parameter settings, changing models, etc. It is thus a generalization of the multimodel and model switching approaches for transition control and provides an efficient mechanism for extending exist-ing single-mode automation applications to multimode operation. State-specific alarm management system [ 34 ] and automated sequence control are examples of two such applications. The event-specific capabilities can also be used to develop new automation applications such as for transition monitoring. 4 Discussions and some future directions While it has become passe to note that  X  X hange is the rule, not the exception, X  there is still a lack of effective solutions to control and effectively manage change  X  change in business operation, in process operation, and in design lifecycle. Here, we have briefly explored the problems resulting from changes and touched upon some solutions that are beginning to evolve. These solutions use artificial intelli-gence methodologies as key elements of an overall solution. Further research is needed to address two related outstanding challenges, discussed next.
 4.1 Integrated optimization of process and supply chain dynamics As is clear from the above, existing literature has addressed the optimization of process dynamics in isolation and considering the supply chain dynamics induced transition as  X  X isturbances. X  To bring about the next level of agility, process transi-tions should be viewed not in isolation, but jointly with the supply chain dynamics that induce them and are affected by them [ 6 , 20 ]. Efficient and safe transition con-trol management, which is synchronized with the dynamics of the supply chain is therefore crucial. Further, dynamics in the supply chain and the process operations domains are not insulated from one another; in fact, an event that originates as a process disturbance (i.e., manufacturing dynamics) may result in supply chain dis-ruptions to the customers (i.e., supply chain dynamics) and vice versa. A recent example that brings out this interaction was the inadequate supply of Jet fuel to the Sydney airport (supply chain disruption) that arose because of quality problems (manufacturing dynamics) in the refinery. The domino effect of the manufactur-ing disturbance on the downstream supply chain including numerous businesses lead to an overall loss of USD $400 million. An integrated optimization of pro-cess operation (control) with the supply chain planning/scheduling is essential to unearth globally optimal supply chain and process operations options [ 4 , 31 , 41 ]. One possible strategy for such integration is outlined in Fig. 7 . 4.2 Integration of heterogeneous monitoring methods Another necessary development relates to the monitoring of agile processes. Each of the process monitoring approaches described in Sect. 3 has its pros and cons as summarized in Table 1 . Thus, no one method can completely address all the issues involved in agile operations management. The shortcomings of the indi-vidual methods can be overcome by effective integration of different monitoring methods (see Fig. 8 ). Recently, a number of researchers including [ 17 , 23 ]have proposed agent-based approaches as an effective solution in this direction. The key challenge here relates to consolidation of evidence (or results) from the dif-ferent agents. Techniques from voting, Bayesian, and Dempster X  X hafer theories hold promise for fusion of evidence.
 References Author Biography
