 ing robust machine learning models for classification, clustering, and other tasks. Feature section has been playing an important role in many applications since it can speed up the learning process, improve the mode generalization capability, and alleviate the effect of the curse of dimensional-ity [15]. A large number of developments on feature selection have been made in the literature and there are many recent reviews and workshops devoted to this topic, e.g. , NIPS Conference [7]. formatics where a large amount of genomic and proteomic data are produced for biological and biomedical studies. For example, in genomics, DNA microarray data measure the expression levels of thousands of genes in a single experiment. Gene expression data usually contain a large number of genes, but a small number of samples. A given disease or a biological function is usually asso-ciated with a few genes [19]. Out of several thousands of genes to select a few of relevant genes thus becomes a key problem in bioinformatics research [22]. In proteomics, high-throughput mass spectrometry (MS) screening measures the molecular weights of individual biomolecules (such as proteins and nucleic acids) and has potential to discover putative proteomic biomarkers. Each spec-trum is composed of peak amplitude measurements at approximately 15,500 features represented by a corresponding mass-to-charge value. The identification of meaningful proteomic features from MS is crucial for disease diagnosis and protein-based biomarker profiling [22]. ods [14] where the selection is independent of classifiers, (2) wrapper methods [12] where the pre-diction method is used as a black box to score subsets of features, and (3) embedded methods where the procedure of feature selection is embedded directly in the training process. In bioinformatics applications, many feature selection methods from these categories have been proposed and applied. Widely used filter-type feature selection methods include F -statistic [4], reliefF [11, 13], mRMR tor machine recursive feature elimination (SVM-RFE) [8]. They often have good performance, but their computational cost is very expensive.
 Recently sparsity regularization in dimensionality reduction has been widely investigated and also applied into feature selection studies. ` 1 -SVM was proposed to perform feature selection using the ` -norm regularization that tends to give sparse solution [3]. Because the number of selected features using ` 1 -SVM is upper bounded by the sample size, a Hybrid Huberized SVM (HHSVM) was proposed combining both ` 1 -norm and ` 2 -norm to form a more structured regularization [26]. But norm minimization on both loss function and regularization. Instead of using ` 2 -norm based loss ` 2 , 1 -norm minimization problem. We also provide the algorithm analysis and prove the convergence of our algorithm. Extensive experiments have been performed on six bioinformatics data sets and our method outperforms five other commonly used feature selection methods in statistical learning and bioinformatics. We summarize the notations and the definition of norms used in this paper. Matrices are written as its i -th row, j -th column are denoted by m i , m j respectively.
 The ` p -norm of the vector v  X  R n is defined as k v k p = v  X  R n is defined as k v k 0 = for multi-task learning [1, 18] and tensor factorization [10]. It is defined as ` 2 , 1 -norm can be generalized to ` r,p -norm Note that ` r,p -norm is a valid norm because it satisfies the three norm conditions, including the the triangle inequality ( v = k b i k r , we obtain  X  Eq. (4) is just k A k r,p + k B k r,p  X k A + B k r,p .
 k  X  v k 0 = |  X  |k v k 0 for scalar  X  . The term  X  norm  X  here is for convenience. { x least square regression solves the following optimization problem to obtain the projection matrix dimension for each data x i (1  X  i  X  n ) . Thus the problem becomes: In this paper, we use the robust loss function: where the residual k W T x i  X  y i k is not squared and thus outliers have less importance than the pure ` 1 -norm loss function does not has such desirable property [5].
 We now add a regularization term R ( W ) with parameter  X  . The problem becomes: Several regularizations are possible:
R 1 ( W ) = k W k 2 , R 2 ( W ) = R 1 ( W ) is the ridge regularization. R 2 ( W ) is the LASSO regularization. R 3 ( W ) and R 4 ( W ) penalizes all c regression coefficients corresponding to a single feature as a whole. This has the identical or approximately identical to the ` 1 -norm results under practical conditions. Surprisingly, we will show in the next section that the problem can be solved using a simple yet efficient algorithm. 4.1 Reformulation as A Constrained Problem First, the problem in Eq. (10) is equivalent to which is further equivalent to Rewriting the above problem as where I  X  R n  X  n is an identity matrix. Denote m = n + d . Let A = U = problem is much more difficult to solve than the ` 1 -norm minimization problem. Existing algorithms usually reformulate it as a second-order cone programming (SOCP) or semidefinite programming (SDP) problem, which can be solved by interior point method or the bundle method. However, solv-ing SOCP or SDP is computationally very expensive, which limits their use in practice. Recently, an efficient algorithm was proposed to solve the specific problem Eq. (14) by complicatedly refor-mulating the problem as a min-max problem and then applying the proximal method to solve it [25]. algorithm is a gradient descent type method and converges very slow. Moreover, the algorithm is de-minimization problem.
 In the next subsection, we will propose a very simple but at the same time much more efficient method to solve this problem. Theoretical analysis guarantees that the proposed method will con-verge to the global optimum. More importantly, this method is very easy to implement and can be readily used to solve other general ` 2 , 1 -norm minimization problem. 4.2 An Efficient Algorithm to Solve the Constrained Problem The Lagrangian function of the problem in Eq. (14) is Taking the derivative of L ( U ) w.r.t U , and setting the derivative to zero, we have: where D is a diagonal matrix with the i -th diagonal element as 1 Left multiplying the two sides of Eq. (16) by AD  X  1 , and using the constraint AU = Y , we have: Substitute Eq. (18) into Eq. (16), we arrive at: Since the problem in Eq. (14) is a convex problem, U is a global optimum solution to the problem if and only if the Eq. (19) is satisfied. Note that D is dependent to U and thus is also a unknown the global optimum.
 The algorithm is described in Algorithm 1. In each iteration, U is calculated with the current D , and then D is updated based on the current calculated U . The iteration procedure is repeated until the algorithm converges.
 Algorithm 1: An efficient iterative algorithm to solve the optimization problem in Eq. (14). 4.3 Algorithm Analysis The Algorithm 1 monotonically decreases the objective of the problem in Eq. (14) in each iteration. To prove it, we need the following lemma: Lemma 1. For any nonzero vectors u , u t  X  R c , the following inequality holds: Proof. Beginning with an obvious inequality ( The convergence of the Algorithm 1 is summarized in the following theorem: Theorem 1. The Algorithm 1 will monotonically decrease the objective of the problem in Eq. (14) in each iteration, and converge to the global optimum of the problem.
 Proof. It can easily verified that Eq. (19) is the solution to the following problem: Thus in the t iteration, which indicates that That is to say, where vectors u i t and u i t +1 denote the i -th row of matrices U t and U t +1 , respectively. On the other hand, according to Lemma 1, for each i we have Thus the following inequality holds: Combining Eq. (25) and Eq. (27), we arrive at That is to say, Thus the Algorithm 1 will monotonically decrease the objective of the problem in Eq. (14) in each is a convex problem, satisfying the Eq. (19) indicates that U is a global optimum solution to the problem in Eq. (14). Therefore, the Algorithm 1 will converge to the global optimum of the problem (14).
 thus D  X  1 is also diagonal with the i -th diagonal element as d  X  1 ii = 2 Therefore, the proposed method can be applied to large scale problem in practice.
 follows: The problem can be solved by solve the following problem iteratively: where D k is a diagonal matrix with the i -th diagonal element as 1 2 k ( A analysis can be used to prove that the iterative method will converge to a local minimum. If the iterative method will converge to the global minimum. Figure 1: Classification accuracy comparisons of six feature selection algorithms on 6 data sets. SVM with 5-fold cross validation is used for classification. RFS is our method. In order to validate the performance of our feature selection method, we applied our method into two bioinformatics applications, gene expression and mass spectrometry classifications. In our experi-ments, we used five publicly available microarray data sets and one Mass Spectrometry (MS) data sets: ALLAML data set [6], the malignant glioma (GLIOMA) data set [17], the human lung carcino-mas (LUNG) data set [2], Human Carcinomas (Carcinomas) data set [24, 27], Prostate Cancer gene expression (Prostate-GE) data set [23] for microarray data; and Prostate Cancer (Prostate-MS) [20] for MS data. The Support Vector Machine (SVM) classifier is employed to these data sets, using 5-fold cross-validation. 5.1 Data Sets Descriptions We give a brief description on all data sets used in our experiments as follows.
 ALLAML data set contains in total 72 samples in two classes, ALL and AML, which contain 47 and 25 samples, respectively. Every sample contains 7,129 gene expression values.
 GLIOMA data set contains in total 50 samples in four classes, cancer glioblastomas (CG), non-cancer glioblastomas (NG), cancer oligodendrogliomas (CO) and non-cancer oligodendrogliomas (NO), which have 14, 14, 7,15 samples, respectively. Each sample has 12625 genes. Genes with minimal variations across the samples were removed. For this data set, intensity thresholds were set at 20 and 16,000 units. Genes whose expression levels varied &lt; 100 units between samples, or varied &lt; 3 fold between any two samples, were excluded. After preprocessing, we obtained a data set with 50 samples and 4433 genes.
 LUNG data set contains in total 203 samples in five classes, which have 139, 21, 20, 6,17 samples, respectively. Each sample has 12600 genes. The genes with standard deviations smaller than 50 expression units were removed and we obtained a data set with 203 samples and 3312 genes. breast, colorectal, gastroesophagus, kidney, liver, ovary, pancreas, lung adenocarcinomas, and lung squamous cell carcinoma, which have 26, 8, 26, 23, 12, 11, 7, 27, 6, 14, 14 samples, respectively. are 174 samples and 9182 genes. Table 1: Classification Accuracy of SVM using 5-fold cross validation. Six feature selection meth-ods are compared. RF: ReliefF, F-s: F-score, IG: Information Gain, and RFS: our method. ALLAML 90.36 89.11 92.86 93.21 93.21 95.89 95.89 96.07 94.29 95.71 94.46 97.32 GLIOMA 50 50 56 60 62 74 54 60 58 66 66 70 LUNG 91.68 87.7 89.22 93.1 92.61 93.63 93.63 91.63 90.66 95.1 94.12 96.07 Carcinom. 79.88 65.48 49.9 85.09 78.22 91.38 90.24 83.33 68.91 89.65 87.92 93.66 Pro-GE 92.18 95.09 92.18 92.18 93.18 95.09 91.18 93.18 93.18 89.27 86.36 95.09 Pro-MS 76.41 98.89 95.56 98.89 95.42 98.89 89.93 98.89 94.44 98.89 93.14 100 Average 80.09 81.04 79.29 87.09 85.78 91.48 85.81 87.18 83.25 89.10 87 92.02 Prostate-GE data set has in total 102 samples in two classes tumor and normal, which have 52 and 50 samples, respectively. The original data set contains 12600 genes. In our experiment, intensity thresholds were set at 100 C16000 units. Then we filtered out the genes with max/min  X  5 or (max-min)  X  50 . After preprocessing, we obtained a data set with 102 samples and 5966 genes. Prostate-MS data can be obtained from the FDA-NCI Clinical Proteomics Program Databank [20]. This MS data set consists of 190 samples diagnosed as benign prostate hyperplasia, 63 samples considered as no evidence of disease, and 69 samples diagnosed as prostate cancer. The samples diagnosed as benign prostate hyperplasia as well as samples having no evidence of prostate cancer were pooled into one set making 253 control samples, whereas the other 69 samples are the cancer samples. 5.2 Classification Accuracy Comparisons All data sets are standardized to be zero-mean and normalized by standard deviation. SVM classifier kernel with the parameter C = 1 . We compare our feature selection method (called as RFS) to [11, 13], mRMR [19], t-test, and information gain [21]. Because the above data sets are for multi-class classification problem, we don X  X  compare to ` 1 -SVM, HHSVM and other methods that were designed for binary classification.
 sets. Table 1 shows the detailed experimental results using SVM. We compute the average accuracy using the top 20 and top 80 features for all feature selection approaches. Obviously our approaches outperform other methods significantly. With top 20 features, our method is around 5%-12% better than other methods all six data sets. ` joint sparsity. We provided an efficient algorithm with proved convergence. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies have been performed on two bioinformatics tasks, six data sets, to demonstrate the performance of our method. This research was funded by US NSF-CCF-0830780, 0939187, 0917274, NSF DMS-0915228, NSF CNS-0923494, 1035913.
