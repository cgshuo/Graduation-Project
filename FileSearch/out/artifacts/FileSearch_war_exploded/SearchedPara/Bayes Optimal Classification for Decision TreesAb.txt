 Siegfried Nijssen siegfried.nijssen@cs.kuleuven.be K.U. Leuven, Celestijnenlaan 200A, 3001 Leuven, Belgium We study the problem of Bayes optimal classification for density estimation trees. A density estimation tree in this context is a decision tree which has a probabil-ity density for a class attribute in each of its leaves. One can distinguish two Bayesian approaches to den-sity estimation using a space of such trees.
 In the first approach a single maximum a posteriori (MAP) density estimation tree is identified first: where X and ~y together constitute the training data. The posterior probability P ( T | X , ~y ) of a hypothesis T is usually the product of a prior and a likelihood. The MAP hypothesis can then be used to classify a test example x  X  using the densities in the leaves. The second approach is to marginalize over all possible trees, instead of preferring a single one: arg max Predictions that are performed using this second ap-proach are called Bayes optimal predictions . It has been claimed that  X  X o single tree classifier using the same prior knowledge as an optimal Bayesian classifier can obtain better performance on average X  (Mitchell, 1997). The Bayesian point of view is that Bayesian averaging cancels out the effects of overfitted models (Buntine, 1990), and  X  X olves X  overfitting problems. This claim was challenged by Domingos (2000). Domingos demonstrated experimentally that an en-semble of decision trees that are weighted according to posterior probabilities performs worse than uniformly weighted hypotheses. It was found that one overfitting tree usually dominates an ensemble.
 However, these results were obtained by sampling from the hypothesis space. Even though Domingos argued that similar issues should also occur in the truly op-timal approach, this claim could not be checked in practice as the exact computation of Bayes optimal predictions was considered to be impractical. Indeed, in (Chipman et al., 1998) it was already claimed that  X  X xhaustive evaluation ... over all trees will not be feasible, except in trivially small problems, because of the sheer number of trees X . Similar claims were made in other papers studying Bayesian tree induction (Buntine, 1992; Chipman et al., 1998; Angelopoulos &amp; Cussens, 2005; Oliver &amp; Hand, 1995), and have led to the use of sampling techniques such as Markov Chain Monte Carlo sampling.
 In this paper we present an algorithm that can be used to evaluate Domingos X  claim in a reasonable number of non-trivial settings. Our algorithm allows us to ex-actly compute the Bayes optimal predictions given pri-ors that assign non-zero probability to trees that sat-isfy certain constraints. An example of a constraint is that every leaf covers a significant number of examples; this constraint has been used very often in the liter-ature (Buntine, 1992; Quinlan, 1993; Chipman et al., 1998; Angelopoulos &amp; Cussens, 2005; Oliver &amp; Hand, 1995).
 Our algorithm is an extension of our earlier work, in which we developed the DL8 algorithm for determining one tree that maximizes accuracy (Nijssen &amp; Fromont, 2007). DL8 is based on dynamic programming on a pre-computed lattice of itemsets, and scans these itemsets decreasing in size. Its time complexity is lin-ear in the size of the lattice. In this paper we extend this algorithm to a Bayesian setting. From a technical point of view, the main contribution is that we prove that a different pass over the lattice allows us to per-form Bayes optimal predictions without increasing the asymptotic complexity of building the lattice. The task that our algorithm addresses is similar to the task addressed in (Cleary &amp; Trigg, 1998). Compared to this earlier work, we study the more common Dirich-let priors also considered in (Chipman et al., 1998; Angelopoulos &amp; Cussens, 2005); furthermore, by ex-ploiting the link to itemset mining, our algorithm is more efficient, and its results are more interpretable. The paper is organized as follows. Notation and con-cepts are introduced in Section 2. Bayes optimal clas-sification is formalized in Section 3. We show how to map this problem to the problem of finding itemsets and building a classifier with weighted rules in Sec-tion 4. Experiments are performed in Section 5. Before we are ready to formalize our problem and our proposed solution, we require some notation. We re-strict ourselves to binary data; we assume that data is converted in this form in a preprocessing step. The data is stored in binary matrix X , of which each row ~x k corresponds to one example. Every example ~x k has a class label y k out of a total number of C class labels. Class labels are collected in a vector ~y .
 We assume that the reader is familiar with the concept of decision trees (see (Breiman et al., 1984; Quinlan, 1993) for details). Essential in our work is a link be-tween decision trees and itemsets . Itemsets are a con-cept that was introduced in the data mining literature (Agrawal et al., 1996). If I is a domain of items, I  X  X  is an itemset. In our case, we assume that we have two types of items: for every attribute there is a positive item i that represents a positive value, and a negative item  X  i that represents a negative value. An example ~x can be represented as an itemset Thus, for a data matrix with n columns, we have that I = I pos  X  X  neg , where I pos = { 1 , 2 , . . . n } and I neg = { X  1 ,  X  2 , . . .  X  n } . We overload the use of the  X  operator: when I is an itemset, and ~x is an example, we use I  X  ~x to denote that I is a subset of ~x after translating ~x into an itemset.
 Every sequence of test outcomes in a decision tree, starting from the root of the tree to an arbitrary node deeper down the tree, can be represented as an itemset. For instance, a decision tree with B in the root, and A in its right-hand branch can be represented by: Every itemset in T corresponds to one node in the tree. By T we denote all subsets of 2 I that represent decision trees. A decision tree structure is an element T  X  X  . Consequently, when T is a decision tree we can write I  X  T to determine if the itemset I corresponds to a path occurring in the tree.
 An itemset is an unordered set: given an itemset in a tree, we cannot derive from this itemset in which order its tests appear in the tree. This order can only be determined by considering all itemsets in a tree T . We are not always interested in all nodes of a tree. The subset of itemsets that correspond to the leaves of a tree T will be denoted by leaves ( T ); in our example, The most common example of a decision tree is the classification tree , in which every leaf is labeled with a single class. In a density estimation tree , on the other hand, we attach a class distribution to each leaf, rep-resented by a vector ~  X  I ; for each class c this vector contains the probability  X  Ic that examples ~x  X  I be-long to class c . All the parameters of the leaves of a tree are denoted by  X  . The vectors in  X  are thus indexed by the itemsets representing leaves of the tree. For the evaluation of a tree T on a binary matrix X , it is useful to have a shorthand notation for the number of examples covered by a leaf: usually we omit the matrix X in our notation, as we assume the training data to be fixed. We call f ( I, X ) the frequency of I . Class-based frequency is given by: The frequent itemset mining problem is the problem of finding all I  X  I such that f ( I )  X   X  , for a given threshold  X  . Many algorithms for computing this set exist (Agrawal et al., 1996; Goethals &amp; Zaki, 2003). They are based on the property that the frequency constraint is anti-monotonic . A binary constraint p on itemsets is called anti-monotonic iff  X  I  X   X  I : p ( I ) = true =  X  p ( I  X  ) = true . Consequently, these algo-rithms do not need to search through all supersets I  X   X  I of an itemset I that is found to be infrequent. One application of itemsets is in the construction of rule-based classifiers (CMAR (Li et al., 2001) is an example). Many rule-based classifiers traverse rules sequentially when predicting examples. Here, we study a rule-based classifier that derives a prediction from all rules through voting. Such a classifier can be seen as a subset P  X  2 I of itemsets, each of which has a weight vector ~w ( I ). We predict an example ~x by computing where we thus pick the class that gets most votes of all rules in the ruleset; each rule votes with a certain weight on each class. The aim of this paper is to show that we can derive a set of itemsets P and weights ~w ( I ) for all I  X  X  such that the predictions of the rule-based classifier equal those of a Bayes optimal classifier. The rules in P represent all paths that can occur in trees in the hypothesis space. In this section we formalize the problem of Bayes op-timal classification for a hypotheses space of decision trees. Central in the Bayesian approach is that we first define the probability of the data given a tree structure T and parameters  X  : In Bayes optimal classification we are interested in finding for a particular example ~x  X  the class y  X  which maximizes the probability y  X  = arg max where we sum over the space of all decision trees and integrate over all possible distributions in the leaves of each tree. Applying Bayes X  rule on the second term, and observing that  X  is dependent on the tree T , we can rewrite this into X in this formula P ( T | X ) is the probability of a tree given that we have seen all data except the class labels. Our method is based on the idea that we can constrain the space of decision trees by manipulating this term. A first possibility is that we set P ( T | X ) = 0 if there is a leaf I  X  leaves ( T ) such that f ( I ) &lt;  X  , for a frequency threshold  X  . We call such leaves small leaves . The class estimates of a small leaf are often unreliable, and it is therefore common in many algorithms to consider only large leaves.
 Additionally, we can set P ( T | X ) = 0 if the depth of the decision tree exceeds a predefined threshold. Both limitations impose hard constraints on the trees that are considered to be feasible estimators. We de-note trees in T that satisfy all hard constraints by L . In the simplest case we can assume a uniform distribu-tion on the trees that satisfy the hard constraints. Ef-fectively, this would mean that we set P (  X  | T, X ) = 1 in Equation 2 for all T  X  L . However, we will study a more sophisticated prior in this paper to show the power of our method. The aim of this prior, which was proposed in (Chipman et al., 1998), is to give more weight to smaller trees; it can be seen as a soft con-straint . This prior is defined as follows.
 Here, the term P node ( I, T, X ) is defined as follows. P node ( I, T, X ) = where
P leaf ( I, X ) = and P f ( I  X  X  i )  X   X  } , which consists of all possible tests that can still be performed to split the examples covered by itemset I .
 The term  X  (1 + | I | )  X   X  makes it less likely that nodes at a higher depth are split. The term e ( I ) determines how many tests are still available if a test is to be performed. We assume that tests are apriori equally likely, independent of the order in which the previous tests on the path have been performed. An alternative could be to give more likelihood to tests that are well-balanced.
 Note that P leaf and P intern are computed for an item-set I almost independently from the tree T : we only need to know if I is a leaf or not.
 As common in Bayesian approaches, we assume that the parameters in every leaf of the tree are Dirichlet distributed with the same parameter vector ~ X  , i.e. where and  X  is the gamma function.
 Finally, it can be seen that where I ( T, ~x  X  ) is the leaf of T for which I  X  ~x  X  . We now have formalized all terms of Equation 2. An essential step in our solution strategy is the con-struction of the set which consists of all itemsets in trees that satisfy the hard constraints. Only these paths are needed when we wish to compute the posterior distribution over class labels, and are used as rules in our rule-based classifier. The weights of these rules are obtained by rewriting the Bayesian optimization criterion for a test example ~x  X  (Equation 1) as where w c ( I ) = X The idea behind this rewrite is that the set of all trees in L can be partitioned by considering in which leaf a test example ends up. An example ends in exactly one leaf in every tree, and thus every tree belongs to one partition as determined by that leaf. We sum first over all possible leaves that can contain the example, and then over all trees having that leaf. The weights of the rules in our classifier consist of the terms w c ( I ), and will be computed from the training data in the training phase; the sum of the weights w c ( I ) is computed for a test example in the classification phase.
 This rewrite shows that in the training phase we need to compute weights for all itemsets that are in P . We will discuss now how to compute these.
 In the formulation above we multiply over all leaves, including the leaf that we assumed the example ended up in. Taking this special leaf apart we obtain: w c ( I ) = W c ( I ) X where W c ( I ) = P leaf ( I, X ) and V ( I, T ) = This rewrite is correct due to the fact that we can move the integral of Equation 3 within the product over the leaves: the parameters of the leaves are independent from each other.
 Let us write the integrals in closed form. First consider W c ( I ). As the Dirichlet distribution is the conjugate prior of the binomial distribution, we have W c ( I ) = Here f  X  c  X  ( I ) = f c  X  ( I ) if c 6 = c  X  , else f  X  c  X  Similarly, we can compute V ( I, T ) as follows.
V ( I, T ) = The remaining question is now how to avoid summing all trees of Equation 4 explicitly. In the following, we will derive a dynamic programming algorithm to implicitly compute this sum. We use a variable that is defined as follows. Here we define L ( I ) as follows:
L ( I ) = {{ I  X   X  T | I  X   X  I }| all T  X  X  for which I  X  T } ; thus, L ( I ) consists of all subtrees that can be put be-low an itemset I while satisfying the hard constraints. As usual, we represent a subtree by listing all its paths. For this variable we will first prove the following. Theorem 1. The following recursive relation holds for u ( I ) : u ( I ) = V leaf ( I )+ Proof. We prove this by induction. Assume that for all itemsets | I | &gt; k our definition holds. Let us fill in our definition in the recursive formula, then we get: u ( I ) = V leaf ( I )+ This can be written as Equation 5 to prove our claim: the term for V leaf corresponds to the possibility that I is a leaf, the first sum passes over all possible tests if the node is internal, the second and third sum tra-verse all possible left-hand and right-hand subtrees; the product within the three sums is over all nodes in each resulting tree.
 We can use this formula to write w c ( I ) as follows. Theorem 2. The formula w c ( I ) can be written as: w c ( I ) = W c ( I )
X Here,  X ( I ) contains all permutations (  X  1 , . . . ,  X  the items in I for which it holds that  X  1  X  i  X  n : {  X  1 , . . . ,  X  i } , {  X  1 , . . . ,  X  i  X  1 ,  X   X  i } X  X  . Proof. The set of permutations  X ( I ) consists of all (or-dered) paths that can be constructed from the items in I and that fulfill the constraints on size and frequency. Each tree T  X  X  with I  X  T must have exactly one of these paths. Given one such path, Equation 4 requires us to sum over all trees that contain this path. Each tree in this sum consists of a particular choice of sub-trees for each sidebranch of the path. Every node in a tree T  X  X  with I  X  T is either (1) part of the path to node I or (2) part of a sidebranch; this means that we is part of Equation 4, into a product for nodes in side-branches, and a product for nodes on the path to I . The term for nodes on the path is computed by considering the side branches, u ( I ) sums over all possible subtrees below sidebranches of the path {  X  1 , . . . ,  X  n } ; using the product-of-sums rule where P m i j =1  X  ij corresponds to a u -value of a sidebranch, we can deduce that the product Q combinations of side branches.
 Given their potentially exponential number it is unde-sirable to enumerate all permutations of item orders for every itemset. To avoid this let us define v ( I ) =
X such that w c ( I ) = W c ( I ) v ( I ) .
 Theorem 3. The following recursive relation holds. v ( I ) = Proof. This can be shown by induction: if we fill in our definition of v ( I ) in the recursive formula we get Both sums together sum exactly over all possible per-mutations of the items; the product is exactly over all terms of every permutation.
 Algorithm 1 Compute Bayes Optimal Weights A summary of our algorithm is given in Algorithm 1. The main idea is to apply the recursive formulas for u ( I ) and v ( I ) to perform dynamic programming in two phases: one bottom-up phase to compute the u ( I ) val-ues, and one top-down phase to compute the v ( I ) val-ues. Given appropriate data structures to perform the look-up of sub-and supersets of itemsets I , this pro-cedure has complexity O ( |P|  X C ). As |P| = O ( n 2 m ), where n is the number of examples in the training data and m the number of attributes, this algorithm is ex-ponential in the number of attributes.
 After the run of this algorithm, for a test example we easily compute the exact class probability estimates from this: P ( y  X  = c | ~x  X  , X , ~y ) = q c ( ~x  X  ) P To compute the set P of paths in feasible trees, we can modify a frequent itemset miner (Goethals &amp; Zaki, 2003), as indicated in our earlier work (Nijssen &amp; Fromont, 2007). We replace the itemset lattice post-processing method of (Nijssen &amp; Fromont, 2007) by the algorithm for computing Bayes optimal weights. Compared to the OB1 algorithm of Cleary &amp; Trigg (1998), the main advantage of our method is its clear link to frequent itemset mining. OB1 is based on the use of option trees, which have a worst case complexity of O ( nm !) instead of O ( n 2 m ). Cleary et al. suggest that sharing subtrees in option trees could improve performance; this exactly what our approach achieves in a fundamental way. The link between weighted rule-based and Bayes optimal classification was also not made by Cleary et al., making the classification phase either more time or space complex. We can interpret predictions by our approach by listing the (maximal) itemsets that contribute most weight to a prediction. We do not perform a feasibility study here, as we did such a study in earlier work (Nijssen &amp; Fromont, 2007). We performed several experiments to determine the importance of the  X  and  X  parameters of the size prior. We found that the differences between values nificant and choose  X  = 0 . 80 and  X  = 0 . 80 as defaults. We also experimented with a uniform prior. We choose ~ X  = (1 . 0 , . . . , 1 . 0) as default for the Dirichlet prior. This setting is common in the literature.
 All comparisons were tested using a corrected, two-tailed, paired t  X  test with a 95% confidence interval. Artificial Data In our first set of experiments we use generated data. We use this data to confirm the influence of priors and the ability of the Bayes optimal classifier to recognize that data can best be represented by an ensemble of multiple trees.
 A common approach is to generate data from a model and to compute how well a learning algorithm recov-ers this original model. In our setting this approach is however far from trivial, as it is hard to generate a re-alistic lattice of itemsets: Calders (2007) showed that it is NP-hard to decide if a set of itemset frequencies can occur at all in data. Hence we used an alternative approach. The main idea is that we wish to generate data such that different trees perform best on different parts of the data. We proceed as follows: we first gen-erate n tree structures (in our experiments, all trees are complete trees of depth 7; the trees do not yet have class labels in their leaves); from these n trees we randomly generate a database of given size (4000 ex-amples with 15 binary attributes in our experiments, without class labels). We make sure that every leaf in every tree has at least  X  examples (3% of the training data in our experiments). Next, we iterate in a fixed order over these trees to assign classes to the exam-ples in one leaf of each tree; in each tree we pick the leaf which has the largest number of examples without class, and assign a class to these examples, taking care that two adjacent leaves get different majority classes. We aim for pure leaves, but these are less likely for higher numbers of generating trees.
 The results of our experiments are reported in Fig-ure 1. The accuracies in these experiments are com-puted for 20 randomly generated datasets. Each fig-ure represents a different fraction of examples used as training data; remaining examples were as test data. The learners were run using the same depth and sup-port constraints as used to generate the data. We can learn the following from these experiments. As all our datasets were created from trees with max-imal height, the prior which prefers small trees per-forms worse than the one which assigns equal weight to all trees. If the amount of training data is small, the size prior forces the learner to prefer trees which are not 100% accurate for data created from one tree. In all cases, the Bayes optimal approach is significantly more accurate than the corresponding MAP approach, except if the data was created using a single tree; in this case we observe that a single (correct) tree is dom-inating the trees in the ensembles.
 The more training data we provide, the smaller the differences between the approaches are. For the correct prior the optimal approach has a better learning curve. Additional experiments (not reported here) for other tree depths, dataset sizes and less pure leaves con-firm the results above, although sometimes less pro-nounced.
 UCI Data In our next set of experiments we de-termine the performance of our algorithm on common benchmark data, using ten-fold cross validation. The frequency and depth constraints in our prior in-fluence the efficiency of the search; too low frequency or too high depth constraints can make the search in-feasible. Default values for  X  that we considered were 4, 6 and  X  ; for  X  we considered 2, 15 and 50. We re-laxed the constraints as much as was computationally possible; experiments (not reported here) show that this usually does not worsen accuracy.
 As our algorithm requires binary data, numeric at-tributes were discretized in equifrequency bins. Only a relatively small number of 4 bins was feasible in all experiments; we used this value in all datasets to avoid drawing conclusions after parameter overfitting. Where feasible within the range of parameters used, we added results for other numbers of bins to investi-gate the influence of discretization.
 The experiments reported in Figure 1 help to provide more insight in the following questions: (Q1) Is a single tree dominating a Bayes optimal clas-(Q2) Are there significant differences between a uni-(Q3) Is the optimal approach overfitting more in (Q4) What is the influence of the 4-bin discretization? To get an indication about (Q1) we compare the opti-mal and MAP predictions. We underlined those cases where there is a significant difference between optimal and MAP predictions. We found that in many cases there is indeed no significant difference between these two settings; in particular when hard constraints im-pose a high bias, such as in the Segment and Vote data, most predictions turn out to be equal. If there is a significant difference, the optimal approach is always the most accurate.
 To answer (Q2) we highlighted in bold for each dataset the system that performs significantly better than all other systems. In many cases, the differences between the most accurate settings are not significant; how-ever, our results indicate that a uniform prior performs slightly better than a size prior in the Bayes optimal case; the situation is less clear in the MAP setting. Answering (Q3), we found not many significant dif-ferences between J48 X  X  and Bayes optimal predictions in those cases where we did not have to enforce very hard constraints to turn the search feasible. This sup-ports the claim of Domingos (2000) that Bayes optimal predictions are not really much better. However, our results also indicate that there is no higher risk of over-fitting either. The optimal learner does not perform as well as J48 in those cases where the search is only fea-sible for high frequency or low depth constraints, and thus quite unrealistic priors; in (Nijssen &amp; Fromont, 2007) we found that under the same hard constraints J48 is not able to find accurate trees either, and often finds even worse trees in terms of accuracy.
 To provide more insight in (Q4), we have added results for different discretizations. In the datasets where we used harder constraints to make the search feasible, a negative effect on accuracy is observed compared to J48. Where the same hard constraints can be used we observe similar accuracies as in J48. The experiments do not indicate that a higher number of bins leads to increased risks of overfitting. Our results indicate that instead of constructing the optimal MAP hypothesis, it is always preferable to use the Bayes optimal setting; even though we found many cases in which the claim of Domingos (2000) is confirmed and a single tree performs equally well, in those cases where there is a significant difference, the comparison is always in favor of the optimal setting. The computation of both kinds of hypothesis remains challenging if no hard constraints are applied, while incorrect constraints can have a negative impact.
