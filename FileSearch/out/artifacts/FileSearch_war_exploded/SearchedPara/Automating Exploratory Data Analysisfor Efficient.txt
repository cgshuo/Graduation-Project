 data mining does not guarantee good models, even when the size of the training data is virtually unlimited. Instead, careful data preprocessing is required, including data cleansing, handling missing values, attribute representation and encoding, and generating derived attributes. In particular, the selection of the most appropriate subset of attributes to include is a critical step in building an accurate and efficient model. We describe an automated approach to the exploration, preprocessing, and selection of the optimal attribute subset whose goal is to simplify the KDD process and dramatically shorten the time to build a model. Our implementation finds inappropriate and suspicious attributes, performs target dependency analysis, determining optimal attribute encoding, generates new derived attributes, and provides a flexible approach to attribute selection. We present results generated by an industrial KDD environment called the Accrue Decision Series on several real world Web data sets.
 Attribute selection, automation, transformation, encoding. in Data (KDD) community has centered around a variety of approaches  X  including cross validation and bootstrapping  X  for handling the situation in which there is not enough data to build a good model. Although these techniques are of interest to industrial practitioners, in practice we typically have access to a nearly limitless supply of data. This is particularly true when mining Web data sources, which seem to double in size every few months.
 opposite problem: how to effectively simplify and narrow down the scope of data used for building a model. Smaller data sets provide a number of benefits, including:  X  Reduced elapsed CPU time for building a model (training  X  Reduced elapsed CPU time for using a model (forecasting or  X  Potentially increased model accuracy  X  Increased explanatory power of the model prime consideration: getting a model into production as quickly as possible. This stems from the fact that Web time is famously seven times faster than "real" time; our client's patience is typically low as their business is constantly being reinvented overnight. In response, we have attempted to automate as many steps of the KDD process as possible, from data collection, to attribution encoding, transformation and selection, through model parameter searching, to best model selection.
 exploratory data analysis (EDA) step of the KDD process. In particular, we focus on the problem of attribute selection, narrowing the potentially hundreds of thousands of attributes (a.k.a. variables or features) down to a manageable subset without destroying the viability of the subsequent model. Our focus stems from the facts that automated attribute selection is less well understood than case (a.k.a. record or instance) selection through sampling techniques, and that attribute selection has historically been highly labor intensive and error prone.
 in the Accrue Decision Series [ADS], a highly scalable knowledge discovery workbench with seven separate predictive and descriptive mining engines. The Decision Series also contains techniques outside the scope of this paper for automating other steps in the KDD process, including sampling, parameter searching and model selection. pre-processing step for both predictive and descriptive engines, much of our focus has been on the efficient building of models using standard predictive techniques: neural networks [12,11], classification and regression decision trees [2, 25], and Bayesian learning [7]. Due to the practical limitations of commercial mining, we have tried to achieve a balance between the time spent on data exploration and the gains we get in this process. In this paper, we assume that the data consists of homogeneous cases each with fixed number of attributes . Furthermore, we confine ourselves to numeric attributes with float or integer values and to nominal
Permission to make digital or hard copies of part or all of this work or personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 categorical attributes. We call the first type of attributes continuous and the second type of attributes categorical.
 Our EDA approach involves a four-step process: Some attributes are obviously inappropriate to be used in a particular model and can be discarded automatically. Other attributes are borderline and marked by the system as suspicious. For these suspicious attributes, the modeler may choose to manually intervene by providing additional domain specific information that cause them to be removed. Otherwise, the suspicious attributes are retained for processing in subsequent steps.
 appropriate representation. This step handles outliers, missing values, and encoding. Continuous attributes are encoded by thresholding (a.k.a. discretizing) [8] the original values into a small number of value ranges. For categorical attributes, encoding merges several values (categories) together. This grouping is similar to a subset option in C4.5 [25]. As both thresholding and grouping can cause the fatal loss of some of the detail contained in original attributes, we try to intelligently balance this loss against the advantages of efficient encoding by using three association measures: mutual information, chi-squared Cramer's V, and Goodman-Kruskal index. These associations are measured between the source attributes and a specific target (a.k.a. dependent variable) to determine which encoding is optimal for a given model. This optimization is performed by an EDA function we call target dependency analysis (TDA).
 EDA attempts to create new derived attributes that may be more beneficial than the existing ones. It does so by experimenting with a variety of univariate and multivariate transformations. When all original and new derived attributes are cleansed, confirmed to be appropriate, and discretized, EDA uses two independent approaches to attribute selection [18, 13], which are both based on filter model selection [14]. Using two algorithms provides additional flexibility and increases our confidence in the results. automatically removed from further processing. EDA identifies following types of inappropriate attributes: warrant some skepticism about their appropriateness but are kept unless specified by the user. EDA identifies following types of suspicious attributes: measures of association between the source and target attributes, and to research the strength and variation of these associations. These associations measure the strength of dependency between attributes and provide a nonlinear generalization to the classic linear concept of correlation. TDA supplies three choices of association measures: mutual information, Cramer's V, and Goodman-Kruskal index.
 j=1:J and target Y with values q=1:Q , with joint distribution P jq , and marginal distributions P j . , P .q , mutual information I(X,Y) is defined as where we use base two logarithm if the information units are bits. This measure is widely used in information theory [3] and machine learning [25]. It is sometimes referred to as information gain, due to a property that it is equal to a decrease in entropy H(Y)-H(Y|X) caused by knowing X , where H(X) = -q log( P . q ) . There is also a relation between mutual information and so-called Kullback Leibler distance (KL-instance) [19]. TDA uses a normalization of mutual information I(X,Y)/H(X), which is scaled to [0,1].
 The second association measure based on chi-squared statistic where N is total number of cases. A well known normalization of chi-squared statistic scaled to [0,1], which represents the strength of association is Cramer's V [24] The third measure of association used in EDA is a Goodman-Kruskal association index . Consider a trivial classifier, which chooses as a forecast the most frequent target value. When X is available, we can do better, by choosing the most frequent forecast among all cases with the same X -value j (maximum likelihood forecast). The Goodman-Kruskal index is a difference in error rate between trivial and X conditioned forecasters. While mutual information and Cramer's V demonstrate a high level of consistency, the Goodman-Kruskal index has a drawback in that it can be zero for non binary target, even when other two measures are positive.
 in TDA to generate optimal thresholding for continuous attributes and optimal grouping for categorical attributes. This optimization must take into account both the number of groups or thresholds and their location. For a continuous attribute and a fixed number of thresholding intervals, the corresponding cut points are optimized by means of an annealing algorithm [23]. In practice, we also impose a lower bound on the number of cases per thresholding interval to ensure that the ranges are relevant. For a categorical attribute and a fixed number of groups (less than J ), we experimented with two approaches for grouping of categorical values. The first one was based on clustering of j -conditional distributions of target q -values and used the traditional K-means technique to cluster J points in Q -dimensional space. We preferred a direct information based clustering, similar to K-means, but using an information based objective function rather than L 2 norm as it explicitly relates to the I(X,Y) association measure defined above.
 continuous attributes and also the number of groups for categorical attributes, we used a simple heuristic based on the rate of flattening of the objective function. Higher dimensions produce better results but also introduce more complexity; as long as the objective function is increasing rapidly we continue to increase the number of dimensions. We also check if adding Nulls as a separate category would more than marginally affect the objective function. Choosing the best number of intervals and groups is an area of on-going research for us.
 benefit to the model and rank their predictive power with respect to the target. transformations may increase its correlation with the target. These transformations are typically only beneficial to linear regression models (such as the regression tree technique in the Decision Series) and, as such, can be disabled when using other mining engines to save computational time. Several transformations, including quadratic function, inverse function, exponent, logarithm, power function, and square root, are tried for each continuous attribute. Some of the functions have free parameters, which are optimized to find the best value. If a given transformation increases the correlation with the target beyond a specified threshold, that transformation is retained for further processing. EDA relies on the fact that the concept of correlation can be generalized to a continuous-categorical couple [26] so that these transforms can be used regardless of whether the target is continuous or categorical. attributes, including linear combinations with undefined coefficients, ratios and products. If one of these functions has a significantly higher correlation than each of the original attributes does, it becomes a new derived attribute. A good example of this from the financial community is the classic price/earning ratio, which is typically more predictive than the attributes price or earnings alone. Since this feature is computationally expensive, it is typically restricted to certain subsets of the original attributes. attributes without significantly affecting the overall quality of the resultant model. Reducing the total number of attributes used reduces computational time and memory requirements, and in many cases leads to more accurate and/or more easily explainable models. We use two independent algorithms for attribute selection to ensure the widest possible range of applicability.
 distance mentioned above and is a modification of attribute selection methodology suggested in [18]. The essence of this algorithm is the minimization of the expectation of KL-distance between the target distribution P(Y=q|X 1 =j conditioned by joint distribution of all k source attributes, and the target distribution conditioned by s selected attributes X P(Y=q|X 1 =j 1 ,..X s =j s ), s&lt;k, (for simplicity we assume that exactly the first s attributes are selected) To make this idea computationally feasible, the algorithm resorts to low dimensional Markov Blankets (MB), rather than comparing large attribute sets. The concept behind MB reflects the simple idea of information coverage. An attribute X 0 is associated with a small subset of attributes or blanket X 1 :b . If  X  (X is small, an attribute X 0 is well covered by its blanket and is a good candidate for exclusion. In practice, the implementation must address such issues as the choice of the original blankets and the exclusion criterion. The attribute with the smallest  X  -measure is not necessarily the best candidate for exclusion, since it potentially could be a member of another blanket used at some previous iteration to exclude another attribute, and its exclusion could cause a chain effect. This MB algorithm belongs to the category of backward selection algorithms. Care must be taken in setting the size of the blanket as small increases in the MB dimensions can results in large increases in computational resources; however, very modest dimensions generally result in a good selection.
 concept of Inconsistency Rate (IR), which is a generalization to many attributes of the Goodman-Kruskal index described above (see also [13]). IR is the error rate of a trivial (maximum likelihood) classifier which predicts the majority target outcome on each subset X 1 =j 1 ,..X k =j k . If the omission of a certain attribute does not affect IR, the error rate of this simple classifier remains intact without this attribute, and the attribute is a good candidate for exclusion. We use a step-wise heuristic with a major loop of backward selection, based on the iterative exclusion of the attribute that minimally affects IR. Forward steps are used to check if a previously excluded attribute can be re-included beneficially to overall monotone sequence of IR for subsequent k ,k-1,...-dimensional attribute subsets.
 stops a when user-specified minimum number of retained attributes is reached. Because all excluded attributes are ordered by the iteration count at which they have been excluded, attributes excluded at the earliest stages are eliminated until the requested number (large enough to cover minimal retained set) is reached. For example, we can request to exclude attributes with IR rates below a certain threshold. The output at each step of the  X  measures for MB algorithm and the inconsistency rates for IR algorithm provides a good heuristic of what number of attributes to request. world Web data sets. Due to confidentiality requirements of paying clients, the sources of the data have been made anonymous.
 wanted to identify repeat visitors to its site in advance of their returning. Overall the return rate was 25%; the problem is predict which visitors are most likely to return in the next 30 days from the last 90 days of Web site traffic information. The training set contained ~10,000 cases ; the verification set contained 2,500 cases. More than 300 attributes were available for modeling, including recency, frequency, duration (RFD) information, browser type, referring domain and URL, and a variety of demographic data. 50 as inappropriate and removed them from subsequent processing. As expected, many of these inappropriate attributes were Null or Near Null. A significant portion of them, however, were classified as Many Values. These pseudo-identifiers had nearly a different value for every case ; with such a high cardinality they provided little value to the modeling process. One common example of this phenomenon is the attribute that captures the Browser/IP pair. In fact, Browser/IP pairs serve as a visitor identifier at those sites which have no registration or cookie mechanism.
 also identified that the TOPLEVELDOMAIN attribute was suspicious because the value of COM covers more than the default 90% of the cases. (For those readers who may not be familiar with Internet terminology, top level domain refers to the third part of an URL, as in the com part of www.accrue.com.) The distribution of values is indeed suspicious in light of the fact that most Web sites see about 75% of their traffic come from the COM top-level domain. The open question is what the modeler should do about it. As previously mentioned, by default the Accrue Decision Series will continue to use it and ultimately decide during attribute selection whether it should be eliminated. A clue to its value is contained in the output ; a significantly higher percentage than normal (89%) of the visitors from the EDU domain do not return to the Web site.
 optimized encodings determined by the EDA module. The age attribute was a continuous attribute that was thresholded into five ranges using four cut points while the original 14 categorical values of the education attribute were replaced with five groups. On the other hand, EDA was not able to optimize the default encoding for the sex attribute; the two original categories ( M and N ) were each encoded as separate values.
 Figure 2. O ptimized encodings determined by the EDA module optimize their direct marketing campaigns by using historic data to target consumers for upcoming campaigns. Overall the response rate from previous campaigns was 4.4% with 200,000 cases split into a 9000 case 50/50 stratified training set and a 100,000 case unstratified verification set. There were 593 source attributes available, including purchase transactions, financial information, and demographics.
 EDA attribute encoding and selection. Three base models were built using all 593 attributes marked No Selection (see table 1), using 254 attributes selected by the Markov Blanket technique, and using 16 attributes selected by the Inconsistency Rate technique. These base models used default, unoptimized encodings in which continuous attributes were thresholded using an equal frequency algorithm and categorical attribute were unchanged. From these three base models, we also constructed three corresponding models with optimized encodings, using the thresholding and grouping suggestions from EDA.
 measures, lift in the top 5% quantile and ROC. We prefer these measures to the overall error rate on the verification set as they more closely reflect the actual business goals: optimizing behavior on a subset of the population as opposed to the entire audience. From the following table, we see that halving the number of attributes significantly improves the model; in fact a further 63% reduction in number of attributes to only 16 causes almost no degradation in performance with the associated saving in elapsed time. What X  X  more, using the optimized encoding suggested by EDA improves the model such that there is virtually no difference between using 254 attributes and using 16.
 increase in these measures was significant, we ran multiple models with different training and verification sets. We calculated from formulas that the top 5% lift had a standard deviation of 0.11 and the ROC metrics [20, 6] had a standard deviation of 0.0037. Second, these results were obtained using a boosted na X ve bayesian classifier; a classification tree induction technique produced analogous results. No Selection Attributes Used 593 MB Attributes Used 254 IR Attributes Used 16 pattern recognition and data mining [26]. Generic data cleansing techniques are well described in [10]. Grouping of categorical values as it relates to tree induction techniques is discussed in [25] while thresholding of continuous variables is discussed in [8]. In [9] information based thresholding of continuous attributes is augmented by the use of the minimal description length principal. A comprehensive introduction to information theory is contained in [3]. For a practical study related to deriving new attributes in context of data mining, see [1]. In linear statistical modeling, many similar approaches have been used ; most notably, principal component analysis [15]. The idea that EDA is inherently an iterative, interactive endeavor is advocated in [27]. While we agree with this philosophy in some aspect, our primary focus is on automatic process. Visualization environment for EDA is discussed in [4].
 models, filter and wrapper, exist for attribute selection and both are described in [14]. For an earlier work on attribute selection see [16]. The Markov Blanket attribute selection algorithm is a modification of the algorithm introduced in [18]. Inconsistency Rate, utilized in IR attribute selection, serves as an objective function in [13], where a Las Vegas type algorithm is used for the major iterative loop. For wrapper type attribute selection see [17,22]. data analysis step in the knowledge discovery in data. This EDA process identifies inappropriate and suspicious attributes, selects the most appropriate attribute representation, create univariate and multivariate derived attributes, and chooses an optimal subset of attributes to retain for the model. Using the resultant simplified attribute subset reduces elapsed CPU time for building and using a model, increases model accuracy, and improves the explanatory power of the model. In practice, these benefits are magnified several-fold because the process of building an effective model typically involves building multiple models with different parameters, training sets, or business objectives. while the authors were employed by NeoVista Software, Inc. which was later acquired by Accrue Software Inc. The impetus for many of the ideas was specific needs that came up during paid client modeling projects. As mentioned above, these ideas are realized in the exploratory data analysis (EDA) model of the Accrue Decision Series. The authors would like to thank the members of the knowledge discovery engineering (KDE) organization who tested early versions of the software and made several suggestions that improved the product. [1] Asker, L., and Maclin, R. Feature Engineering and Classifier [2] Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J.
 [3] Cover, T.M. , and Thomas J.A. Elements of Information [4] Derthick ,M., Kolojejchick, J., and Roth, S.F. An Interactive [5] Devaney, M., and Ram, A. Efficient Feature Selection in [6] Egan J.P. Signal Detection Theory and ROC Analysis, Series [7] Elkan C. Boosting and Na X ve Bayesian Learning. Technical [8] Fayyad U.M., and Irani K.B. Multi-interval discretization of [9] Friedman, N. Discretizing Continuous Attributes While [10] Guyon, I., Matic, N., and Vapnik, V. Discovering [12] Hertz, J., Krogh, A, and Palmer, R.G. Introduction to the [13] Huan Liu, and Setiono, R. A Probabilistic Approach to [14] John, G.H., Kohavi, R., and Pfleger, K. Irrelevant Feature [15] Jolliffe, I.T. Principle Component Analysis , Springer-Verlag, [16] Kira, K., and Rendell, L. The Feature Selection Problem: [17] Kohavi, R., and John, G.H. Wrappers for Feature Subset [18] Koller, D., and Sahami, M. Toward Optimal Feature [19] Kullback, S., and Leibler, R.A. On information and [20] Ling C.X., Li C. Data Mining for Direct Marketing: [21] Mitchel, T.M. Machine Learning . McGraw-Hill, Boston, [22] Ng, A.Y. On Feature Selection: Lerning with Exponentially [23] Otten, R.H.J.M., and van Ginneken, L.P.P.P, The Annealing [24] Press, W.H., Teukolsky, S.A., Vettering, W.T., and [25] Quinlan J.R. C4.5: Programs For Machine Learning .
 [26] Schurmann, J. Pattern Classification. A unified view of [27] Smyth, P., and Wolpert, D. Anytime Exploratory Data [28] Tukey, J.W., Exploratory Data Analysis. Addison-Wesley,
