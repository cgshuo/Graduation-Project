 Many online experiments exhibit dependence between users and items. For example, in online advertising, observations that have a user or an ad in common are likely to be associ-ated. Because of this, even in experiments involving millions of subjects, the difference in mean outcomes between con-trol and treatment conditions can have substantial variance. Previous theoretical and simulation results demonstrate that not accounting for this kind of dependence structure can re-sult in confidence intervals that are too narrow, leading to inaccurate hypothesis tests.

We develop a framework for understanding how depen-dence affects uncertainty in user X  X tem experiments and eval-uate how bootstrap methods that account for differing lev-els of dependence perform in practice. We use three real datasets describing user behaviors on Facebook  X  user re-sponses to ads, search results, and News Feed stories  X  to generate data for synthetic experiments in which there is no effect of the treatment on average by design. We then esti-mate empirical Type I error rates for each bootstrap method. Accounting for dependence within a single type of unit (i.e., within-user dependence) is often sufficient to get reasonable error rates. But when experiments have effects, as one might expect in the field, accounting for multiple units with a mul-tiway bootstrap can be necessary to get close to the adver-tised Type I error rates. This work provides guidance to practitioners evaluating large-scale experiments, and high-lights the importance of analysis of inferential methods for dependence structures common to online systems.
 G.3 [ Probability and Statistics ]: Statistical Computing causal inference, bootstrapping, field experiments, A/A tests, A/B testing, random effects, user X  X tem data
Experiments conducted on the Internet frequently involve millions to tens of billions of observations. This could lead to the perception that there is little uncertainty about ex-perimental outcomes. However, treatment effects are often very small in absolute terms, so a great number of observa-tions can be required to distinguish them from noise. Fur-thermore, many Internet-scale datasets, including those gen-erated by social media feeds, search, ads, and recommender systems, have a user X  X tem structure such that individual ob-servations are not independent; rather, there is substantial dependence between observations of the same units. For ex-ample, consider an online advertising experiment in which there 1 million ad impressions, but these only include 1,000 distinct ads and 10,000 distinct users. Clearly, the effec-tive sample size will be less than 1 million, and there can be substantial uncertainty about the difference in click-through rate (CTR) between the treatment and control.

Accounting for this dependence is important for statis-tical inference, including hypothesis testing and confidence interval estimation. Inferential procedures that neglect this dependence structure are expected to be anti-conservative: they will have higher Type I error rates than expected and, e.g.,  X 95% X  confidence intervals will include the true value less than 95% of the time.

High false positive rates have substantial managerial con-sequences. For example, experiments using one popular ex-perimentation platform at Facebook compare, on average, 3.7 non-control conditions. With four comparisons of in-dependent experiments and a nominal Type I error rate  X  =0 . 05, there should be a 1  X  (1  X  0 . 05) 4 =18 . 5% chance that at least one condition would be significant under the null hypothesis (i.e., one in 5.4 experiments with no effects may yield at least one significant condition). But if the true Type I error was considerably higher, say  X  =0 . 2, one would have a 1  X  (1  X  0 . 2) 4 = 59% chance of having falsely rejected a null hypothesis. Given that many experiments involve comparing multiple outcomes (i.e., metrics), in practice the resulting effects on decision making can be worse than this suggests: not only can there be errors in identifying effects on the primary outcome, but incorrectly rejecting the null for secondary outcomes might delay or prevent the launch of a change that is otherwise beneficial.

This paper describes sources and consequences of depen-dence in common applications of experimentation to Inter-net services. We posit a general data generating process and illustrate how experimental assignment procedures and common effects of units (e.g., users and ads) affect the true uncertainty about experimental comparisons. We then eval-uate independent, one-way, and multiway bootstrap meth-ods for computing confidence intervals using null experi-ments ( X  X /A tests X ) derived from three empirical datasets from Facebook: clicks on advertisements, search results, and content in the News Feed. We additionally modify these datasets to simulate systematic imbalance in items across conditions, as would result from changes to the underlying CTR prediction or ranking models. To examine performance under additional deviations from the null, we conduct sim-ulations using a realistic probit random effects model.
Our primary contribution is providing guidance about when accounting for dependence among observations are most im-portant: while previous work has shown that neglecting all dependence structure results in massive overconfidence, less work has examined how accounting for some sources of de-pendence, but not others, affects inference in practice. We conclude that analysts should use a inferential procedure that accounts for dependence among observations of the units assigned to conditions (e.g., users), but that whether not additionally accounting for secondary units (e.g., ads, search results, links) makes for misleading inference is more likely to depend on (partially unknown) deviations from an often implausible null hypothesis. We illustrate that when experiments have effects, accounting for secondary units may be necessary to obtain trustworthy confidence in-tervals.

The literature on routine Internet-scale experimentation stresses the importance of running  X  X /A tests X  as a valida-tion of the combination of one X  X  random assignment, data logging, and statistical inference procedures [7, 11, 12], though it is generally not stated exactly how these null experiments should be conducted and what their limitations are. We in-tend that, in addition to our results, this paper provides a blueprint for other experimenters who wish to evaluate and choose among inferential procedures in their own settings.
Many experiments allow observing the same units repeat-edly: we may observe responses from the same person many times and also observe responses to the same items many times. In this section, we examine how this affects our esti-mation of contrasts between experimental conditions, such as differences in means between treatment and control. 1
Recent work in applied econometrics has been concerned with dependence due to clustering in data. It is now routine for work in empirical economics to consider and account for dependence in observations produced by one or more types of units. 2 Concerns about such dependence have been fea-tured centrally in methodological work in the context of a growing number of field experiments in economics and other social sciences [10]. Similarly, work on two-way and ten-sordatainthecontextofrecommendersystemsandob-servational comparisons has emphasized the importance of accounting for multiway dependency [16, 17]. And in psy-
Online experiments may also exhibit other sources of de-pendence that are beyond the scope of this paper, including general equilibria in advertising auctions, peer effects, and other such  X  X pillovers. X 
For example, a recent paper by Cameron et al. [6] on deal-ing with dependence due to observing two or more types of units has been cited over 600 times as of May 2013, accord-ing to Google Scholar. chometrics [5] and psycholinguistics [2], investigators have identified problems with ignoring either of two sources of dependence.

As practitioners conducting and analyzing massive Inter-net experiments, the degree of attention given to this area suggests a need to consider the consequences of dependence for our data. We present our efforts to understand whether it would be necessary to account for multiple units caus-ing dependence in our data, or whether a single unit would suffice in order to have inferential procedures with good per-formance.
In this section we describe a simple data generating pro-cess based on random effects models to illustrate how de-pendence can affect uncertainty in experiments and moti-vate the need to evaluate inferential procedures. Random effects models provide a general way to describe data aris-ing from combinations of units, such as users and items (e.g., ads, search results). In the two-way crossed random effects model [2, 22], each observation is generated by some func-tion f of a linear combination of a grand mean,  X  , a random effect  X  i for the first unit, which (without loss of generality) we take to be the idiosyncratic deviation for user i , and a second random variable  X  j for the idiosyncratic deviation for item j . Finally, we have a error term  X  ij for each user X  X  idiosyncratic response to each item. 3 This final term could be caused by a number of factors, including how relevant the item is to the user. Thus, we have the model
Y ij = f (  X  +  X  i +  X  j +  X  ij ) Each random effect is modeled as being drawn from some distribution with zero mean and some variance. In the ho-mogeneous random effects model, this variance is the same foreachuseroritem(i.e.,  X   X ,i =  X   X  ), whereas in a heteroge-nous random effects model, each unit or group of units may have their own variances.
We generally do not observe all combinations of users and items; in fact, usually we only observe a small fraction of the possible combinations. Which items users are exposed tomaydependonuseranditemcharacteristics,andthis pattern of exposure is often subject to experimental manip-ulation. Without loss of generality, let users (rather than items) be assigned to experimental conditions, so that D i i  X  X  assignment to a condition (e.g., in the case of a binary treatment, D i = 0 is the control and D i = 1 is the treat-ment). Let Z ( d ) be a matrix of indicator variables where Z ij = 1 if and only if i is exposed to item j when i is as-signed to condition d . The pattern of exposure Z defines what outcomes can occur: if Z ( d ) ij = 1 for some user X  X tem-treatment combination ( i, j, d ), then we would observe Y  X  i  X  X  potential outcome in response to j under the treatment d . Note that since a user is only assigned to one condition, D , we cannot simultaneously observe both Z (0) ij and Z (1) nor can we observe both Y (0) ij and Y (1) ij .
For simplicity, we consider only a single observation of each user X  X tem pair. Additional error terms can be included when there are repeated observations of pairs.
We wish to estimate quantities comparing outcomes that would occur under different values of D i  X  most simply, the differenceinmeans for a binary treatment
Experiments can produce a non-zero  X  simply by chang-ing the pattern of users X  exposure to items. For exam-ple, a search ranking experiment could primarily have ef-fects by changing which items are displayed as results (and thus observed). At one extreme, it could be that the po-tential outcomes are identical under treatment and control, Y different ( Z (0) ij = Z (1) ij ), such that  X  =0.
Other experiments can produce a non-zero  X  while leav-ing the pattern of exposure identical or otherwise ignorably similar. For example, an experiment might not alter which items are displayed to particular users, but instead render items slightly differently, so that Y (0) ij = Y (1) ij for some i, j . In this case,  X  is then an average treatment effect (ATE) since it is a difference in means for the same units [19].
If for all i, j , Z (0) ij = Z (1) ij , the pattern of exposure is the same and  X  is an ATE,
For expository simplicity 4 , the remainder of this section assumes that the pattern of exposure is the same under the treatment and control: Z ij  X  Z (0) ij = Z (1) ij .
We extend the basic random effects model above to in-clude experimental conditions that may affect a user X  X  ex-posure and response to an item. We then derive expressions for the variance of the difference in mean outcomes between conditions to illustrate how repeatedly observing the same units, and which units are randomly assigned to conditions, influences estimates of experimental effects.

Here we restrict our attention to linear models with nor-mally distributed random effects. That is, the following analysis considers cases where Y is unbounded, f is the identity function, and random effects are drawn from a mul-tivariate normal distribution, so that Note that we define the random effects  X  i , etc., as vectors with a covariance matrix (e.g.,  X   X  ) so that their effects may be correlated across conditions.

The sample mean for each condition is  X  Y
The results for the more general case require introducing a model for exposure such that the random effects share com-mon causes with the missingness. The  X  terms in variance expressions are then replaced with variances conditional on data being observed. where, e.g., n ( d ) j  X  is the number of observations of item j in condition d . We then estimate the ATE using the difference in observed sample means  X   X  =  X  Y (1)  X   X  Y (0) .
Consider the case where the number of observations in the treatment and control groups are of equal size such that N = n (1)  X  X  = n (0)  X  X  . This enables simplifying the expression for  X   X  and its variance to and V[  X   X  ]= 1 The first term in (2) is the contribution of random effects of users to the variance, and the second is the contribution of the random effects of items. The covariance term  X  2  X  (0) present for items, is absent for users and user X  X tem pairs since each is only observed in either the treatment or control.
To further simplify, we can introduce coefficients measur-ing how much units are duplicated in the data. Following previous work [16, 17], we define which are the average number of observations sharing the same user (the  X  A s) or item (the  X  B s) as an observation (including itself). For the units assigned to conditions (in this case, users), either n (0) i  X  or n (1) i  X  is zero for each i ;for the non-assigned units (items), we need a measure of this between-condition duplication Under the homogeneous random effects model (1), we can then simplify (2) to The above expression makes clear that if the random effects for items in the treatment and control are correlated (as we would usually expect), then an increase in the balance of how often items appear in each condition reduces the variance of the estimated treatment effect.
Sharp null hypothesis . Under the sharp null hypothe-sis for user X  X tem experiments, the treatment has no average, interaction, or exposure effects; that is, the outcome for a particular user X  X tem pair, and whether or not the item is displayed to the user, would be the same regardless of treat-ment assignment. In the context of our model, in addition to  X  = 0, the sharp null can be defined by:
In the case of the sharp null, only random effects for items that are not balanced across conditions contribute to the variance of our difference: the contribution a single item j makes to the variance simplifies to n (0)  X  j  X  n (1)  X  j is, it depends only on the squared difference in duplication between treatment and control. It is easy to show that condition duplication of observations of items. If items, like users, also only appear in either treatment or control, then  X 
B =  X  tween users X  and items X  contributions to our uncertainty.
Non-sharp null hypothesis . Experiments may have zero average effect (  X  = 0) and still violate the sharp null. For example, when (4) does not hold, the pattern of exposure may change such that users are exposed to different items in each condition, but there are no user or item effects (i.e., (5) remains true). This can occur when exposures are missing from the layout Z ( d ) at random. That is, the change in exposure between conditions is independent of the potential
Another deviation from the sharp null is when (5) does not hold. In this case, we say that there are interaction effects of the treatment and units; for example, an experiment can positively or negatively affect particular items or user X  X tem pairs, but when outcomes are averaged over all exposures, there is zero effect.

Together these considerations highlight the ways in which field experiments without true average effects can appear to have differences between treatment and control conditions due to effects of users and items, imbalance, and treatment-item interactions. Inferential methods that do not account for these kinds of dependence structures may result in un-derstating the variance of the difference of means estimator used to produce confidence intervals. In the next section we review bootstrap methods which can account for varying levels of dependence structure.
The bootstrap [8] offers a very general method for char-acterizing the sampling distribution of a statistic (e.g., a difference in means), and can be used to produce confidence intervals for experimental comparisons for many data gen-erating processes. The bootstrap distribution of a sample statistic is the distribution of that statistic under resampling [8] or reweighting [20] of the data. In this section, we de-scribe how the bootstrap can be applied to dependent data. We focus on a version of the bootstrap that uses indepen-dent weights, rather than the resampling bootstrap, since it is suitable for use in online (i.e., streaming) computational settings [17, 18].

Bootstrap methods are attractive because they involve minimal assumptions and scale well to large datasets com-pared to other methods commonly used in practice for sta-tistical inference with dependent data. One could fit crossed random effects model to the data [2], but such models don X  X  scale to large datasets and involve specifying (likely incor-rect) assumptions not needed for the bootstrap. Other al-ternatives include cluster robust Huber X  X hite  X  X andwich X  standard errors from econometrics [6], but such methods cannot be applied in a streaming fashion and are not avail-ableforrobuststatisticsofinterest,suchastrimmedorWin-sorized means.
In order to get a confidence interval for some statistic t , we produce R replicates of the the statistic, t  X  r , computed on randomly reweighted versions of the sample. That is, for some replicate r  X  [1 ,R ], each observation Y ij is randomly reweighted with weights W ijr . These reweighted samples al-low us to estimate features of the sampling distribution of our statistic. We generally have W ijr  X  X  where G is some distribution with mean and variance 1, such as Poisson(1) and Uniform { 0 , 2 } [17,  X  3.3]. Note that each observation is reweighted independently, including other observations of the same units. Applied to two-way data, the iid bootstrap can be expected to underestimate the variance of statis-tics and thus produce confidence intervals with poor cov-erage [14].
In the one-way bootstrap, or  X  X lock X  or  X  X luster X  boot-strap, the analyst chooses a single relevant type of unit (e.g., users) and all observations from the same unit are given the same random weight when reweighting. In other words, taken i as indexing the chosen type of unit, we have W ijr = W ij r = u ir and u ir  X  X  for all j, j . When the data only has one-way dependency, this procedure produces a bootstrap distribution that gives consistent confidence in-tervals. When the data has additional dependency struc-ture, it can be anti-conservative.
When there are two or more relevant units, analysts can use a bootstrap that reweights all relevant units. Under a more general model than the one presented above, the multiway bootstrap produces variance estimates, and thus confidence intervals, that are accurate or mildly conservative [16, 17]. The two-way bootstrap has been used for analyzing large online advertising experiments [3]. With two-way data, we have W ijr = u ir v jr , where u ir  X  X  and v jr  X  X  . That is, the random weights for an observation is the product of two independently sampled weights assigned to unit i and unit j . For example, if in one replicate, user i gets weight 2anditem j gets weight 3 then all observations of the pair ( i, j )getweight2  X  3 = 6 in that replicate. Note that if either unit has a weight of 0, any combination of that unit UDQN shown relative to the grand mean. with another unit will be given weight of 0. This procedure can be generalized to cover d -way data in a straightforward fashion [17].
For any statistic t that can be computed online, the one-way bootstrap can be implemented online as follows [17, 18]:onvisitingeachobservation,useahashofanidentifier of each unit (e.g., a user ID) as the seed to a pseudoran-dom number generator for G , draw R weights, one for each bootstrap replicate r , and use these weights to update the running sufficient statistics for t  X  r .The d dimensional multi-way bootstrap can be implemented using a similar procedure using the products of weights generated from each unit.
Quantifying the uncertainty in experimental effects re-quires that we correctly estimate the variance of a difference in means. Based on our intuitions from the simple model in Section 2.1, this variance can depend critically on at least four sources of variation: effects of users and items, their duplication, how well items are balanced across conditions, and heterogeneity in treatment effects. We might expect that inferential procedures which do not account for users and items to produce inaccurate confidence intervals; we ex-plore this hypothesis with respect to these four sources in turn in using synthetic null experiments in the remaining sections.
We examine click-through rate outcomes for three core product areas at Facebook: Ads, Search, and News Feed.
Ads . We analyze ad click-through rates for one type of ad unit for a popular advertising product on Facebook. Each impression corresponds to a single delivery of the ad to a user X  X  Web browser.

Search . We analyze search click-through rates for one type of search result on Facebook. Each impression is a val-idated delivery of an item in the  X  X ypeahead X  results, and eachclickisaclickontheitem. Notethatifanitempre-sented multiple times over several query reformulations, each is considered a separate impression.

Feed . We analyze click-through rates for one type of story in the News Feed in a large country. Each impression cor-responds to a single delivery of the story to a viewer X  X  Web browser, and a click corresponds to a click on the item X  X  thumbnail or snippet.
The most basic A/A test we consider is a synthetic experi-ment that evaluates inferential methods under the sharp null by partitioning the data into multiple random segments and computing confidence intervals for the difference in mean outcomes several hundred times. To do this, we first take the identifier of the unit we wish to randomize over (the user id), concatenate it with a salt (i.e., an integer), com-pute this string X  X  MD5 hash value, and assign the unit to a segment number that is the integer representation of the first 7 digits of the hash modulo M = 100. We use a similar procedure to hash the secondary units (items). We gener-ate the confidence intervals for differences between even and odd numbered segments, yielding 50 comparisons per salt, and repeat this procedure for 10 salts, yielding K =500null experiment comparisons, illustrated in Figure 1.

The confidence intervals for each bootstrap method for each null experiment comes from R = 500 bootstrap reweight-ings of the data. To determine whether or not an experiment is significant, we compute the mean and variance of the dif-ference in means  X   X  kr over all R replicates for each of the K experiments. The distributions of  X   X  kr are asymptotically normal under the bootstrap, so we simply use normal quan-tile function to compute the central 100(1  X   X  )% interval.
To obtain the estimated true coverage for the nominal 95% confidence intervals, we compute the proportion of times the K bootstrap tests indicate a significant difference in means at  X  =0 . 05. We treat each of the K comparisons as in-dependent, and use the Wilson score interval for binomial proportions [1] to determine the uncertainty around the es-timated true coverage rate. have the advertised or conservative Type I error rate. Table 1: The amount of duplication present in our datasets for a single 1% segment of users.
A central quantity that contributes to the variance of  X   X  is the average number of observations that share the same user,  X  user , and item,  X  item . Wegivebasicsummarystatistics about the duplication in the data for a random 1% segment used in our evaluation for the three datasets in Table 1. For the restricted categories of items we consider in each dataset, there are more users exposed to ads than the search results or feed stories. While per-user duplication is similar across the three datasets, the per-item duplication for Ads is much higher than either Search or Feed. This pattern is congruent with the nature of the items: the number of businesses that are actively advertising are far fewer than the number of users, while search and News Feed stories tend to have a much longer tail of items that result in lower duplication.
Experiments often run for many days; as the number of days increase, so does the duplication. Figure 3 shows how duplication increases over time. With the exception of Feed items, this relationship is rather linear both for user and items. The behavior for Feed may be explained by the way social media feeds work: unlike ads and search results, users see and interact with very recent content, therefore limiting the average number of users that may be exposed to an item.
The sharp null variance expression (6) suggests that we might expect that the increase in user and item-level dupli-cation over time to contribute substantially to the variance of  X   X  . Not taking these units into account when computing confidence intervals may result in poor coverage. Figure 2 shows the true coverage of the different bootstrap methods for consecutively larger spans of time in each dataset. We Figure 3: Duplication (  X  ) for users and items over time relative to the first day. find that the iid confidence intervals tend to be highly anti-conservative. For example, after two weeks of data collec-tion, a search experiment that tests the difference in click-through rates between two equivalent groups of users could result in rejecting the null hypothesis nearly 50% of the time. We find that bootstrapping by the unit not being random-ized over (the item) often leads to anti-conservative intervals, and that for the sharp null, which has little item imbalance or item-treatment effects, the user-level bootstrap yields ac-curate coverage. The multiway bootstrap, however, remains conservative no matter how many days are considered.
Given how the synthetic experiments in Section 3.2 were constructed, there is approximate balance of items across conditions such that the primary contributors to the vari-ance of  X   X  are expected to come from user and residual error random effects. However, if items are systematically im-balanced across treatments (e.g., the experiment results in showing similarly relevant, but different ads), then based on our intuition from (6), item random effects can also make a substantial contribution.

To examine how such imbalance might affect the cover-age of the confidence intervals, we created imbalance by downsampling items from either condition with probabil-ity p . This type of data augmentation results in synthetic Figure 4: True coverage for nominal 95% confidence intervals for each bootstrap method applied to data with varying levels of synthetic imbalance of items across conditions for 2 weeks of data. Violations to the sharp null via imbalance do not appear to af-fect the accuracy of the true coverage for the multi-way and user-level bootstrap, while the iid and item level bootstrap become more conservative when im-balance is greatest. experiments that correspond to the non-sharp null hypothe-sis with zero mean difference, equal variances, and different patterns of potential exposure ( Z (0) = Z (1) ).
To do this, for each pair of segments ( m, m +1), we down-sample each item from either segment even or odd segments (chosen with equal probability); in the downsampled seg-ment for some item j , its user X  X tem pairs are independently removed with probability p .Thus,when p =0,wehave the sharp null hypothesis, and when p = 1, we have total imbalance (i.e., the two conditions contain disjoint sets of items).

Figure 3.4 shows the true coverage with varying censoring probabilities, p  X  X  0 . 3 , 0 . 6 , 1 . 0 } . Despite the threat that the imbalance might result in a large item-level contribution to the variance, the coverage of the user bootstrap, which neglects this variance, remains approximately as advertised. This result may be due to a number of factors. First, the most straightforward expressions for V[  X   X  ] and the expected variance estimates from the bootstrap procedures involve assuming a homogeneous random effects model, when it can actually be expected that the variances of the random effects for, e.g., frequently observed users are different than those for infrequently observed users. Second, there is a relatively high amount of variable-level duplication in the data such that there are for many users and items a small number of user X  X tem pairs observed; this duplication can cause the multiway bootstrap to be very conservative [17, Theorem 7].
For Feed and Search, the poor coverage of the iid and item bootstrap confidence intervals notably increases, though they continue to under cover. This is expected since, in addition to creating imbalance, the downsampling procedure reduces within-condition duplication.
We have seen how different bootstrap methods perform in practice under the sharp null where experiments have no effects at all, and a non-sharp null where experiments may change the pattern of users X  exposure to items. However, these two tests cannot tell us about how bootstrap proce-dures might perform in situations where experiments affect potential outcomes for specific user X  X tem pairs. For exam-ple, an ads experiment that manipulates the display of cer-tain advertising units may only affect certain items and not others [3]. To explore these circumstances, we conduct sim-ulations with a probit random effects model parameterized to mirror the kinds of outcomes described in the previous section. We use this generative model to vary the presence of an item X  X reatment interaction, a plausible source of vio-lations of the sharp null hypothesis given in (5).
We modify the model of (1) so that Y is binary and there is a single intercept common to both treatment and control, reflecting the lack of an ATE: Also reflecting the absence of an ATE, we restrict the ran-dom effect variance to be the same in treatment and control. For example, the covariance matrix for the item random ef-fects is To make realistic choices for the variances of the random effects, we fit a probit random effects models to the ads dataset from a large random sample of users in each of sev-eral small countries. This produced several estimates of  X  and  X   X  . We report on simulation results for  X   X  =0 . 3, which is close to several of the estimates. Our estimates of  X   X  often ranged from 0.2 to 0.9, so we present results for  X   X   X  X  0 . 1 , 0 . 3 , 0 . 5 , 1 . 0 } .Weset  X  so as to achieve close to 0 . 02. 5
We constructed the set of observed user X  X tem pairs used in the simulations by assigning each of 3,000 potential users and 200 potential ads to log-normally distributed scores. For each of 2 N observations, we selected a particular user and ad with probability proportional to this score. This yielded a  X  X ayout X  with 2481 unique users, 199 unique ads, and du-plication coefficients  X  A . =30 . 9and  X  B . = 6077 . 4, which is similar to the Ads dataset.
Even if the treatment has no effects on average, it can have positive effects for some users and items and negative effects for others. Given our random effects model, we know that item X  X reatment interactions can increase the contribution of duplication of items to the variance of the mean difference.
We vary item X  X reatment interactions by setting the corre-lation coefficient  X   X   X  X  0 , 0 . 25 , 0 . 5 , 0 . 75 , 1 tion  X   X  = 1 corresponds to data generated under the sharp null hypothesis, while decreasing  X   X  corresponds to an in-creasing proportion of item random effects being not shared across conditions. At the extreme of  X   X  = 0, the random effect of an item in the treatment is completely independent of its random effect in the control.
Since there is no scale to the latent variable y ij ,weachieved this by in fact choosing a fixed  X  =  X  2 and rescaling the random effect variances to sum to 1.  X  level random effects.
Figure 5 summarizes the results of 1000 simulations for each combination of parameter values. Without any item X  treatment interaction, both the user and item bootstrap have approximately correct coverage; this is attributable to the relatively low  X  A in this simulation, and is consistent with the results from a small number of days in our datasets. As the item X  X reatment interaction increases, the coverage of the user bootstrap confidence intervals drops substantially. For example, even with moderate values of  X   X  =0 . 5and  X   X  =0 . 75, a nominally 95% confidence interval has a true coverage of 87.5%. While do we not expect to observe the extremes of all item-level variance being treatment specific (i.e.,  X   X  = 0), these results demonstrate that deviations from the sharp null in the form of item X  X reatment inter-action have serious consequences for the one-way bootstrap. On the other hand, the multiway bootstrap remains mildly conservative even with large  X   X  and small  X   X  .
Despite having a large number of individual observations, many settings for online experiments involve substantial de-pendence and small effects such that statistical inference re-mains a central concern. The preceding analysis of real and simulated data makes clear that methods which neglect de-pendence structure in these large experiments can result in high Type I error rates and confidence intervals with poor coverage. In each of our three datasets, the iid bootstrap performed very poorly, such that using it (or other methods assuming iid observations) would result in reaching incor-rect conclusions about the presence, sign, and magnitude of treatment effects [9]. Furthermore, the particulars of the user X  X tem exposure layout ( Z ) provide new considerations that deserve further attention.

On the other hand, neglecting dependence among obser-vations of units not assigned to conditions (the items) gen-erally did not result in lower coverage with our data. For each of the datasets, this remained the case even when we produced imbalance of items across conditions. Given the random effects model posited in Section 2.1, one might ex-pect this imbalance to make both the user and item contri-butions to the variance necessary to account for separately. Since bootstrapping multiple units and storing these repli-cates can have substantial costs in terms of computation and infrastructure, our results suggest that experimenters should consider whether a one-way bootstrap on the experimental units may be practically sufficient, even in the presence of other clearly relevant units, such as ads and URLs.
Nonetheless, neglecting dependence among observations of non-experimental units (e.g., items) may have substantial effects on coverage when the treatment has any effects. Most treatments are expected to have some effects. Our simula-tions with item X  X reatment interaction effects demonstrate that the coverage of the user bootstrap can be extremely sensitive to the presence of these effects. This highlights that using A/A tests only serves to validate inferential pro-cedures under a narrow set of conditions (i.e., the sharp null hypothesis), but cannot detect other (potentially severe) in-ferential problems that occur in other circumstances. Given that experimenters expect treatment effects, and often want to know how large the average effects are, they should con-sider whether or not they wish to use a procedure that pro-vides a somewhat conservative measurement of uncertainty (i.e., the multiway bootstrap), or the user-level bootstrap, which correctly tests the less plausible sharp null.
A limitation of the present work is that, from the perspec-tive of experimenters such as ourselves trying to evaluate in-ferential methods in practice, there is remaining gap between what is possible to learn from straightforward perturbations of real datasets and what is possible to learn from necessar-ily simplified generative models. Future work may develop more sophisticated ways of perturbing existing data and us-ing additional parameters estimated from real experiments to produce evaluations for data that more closely resemble outcomes in the field.
This paper has been primarily concerned with Type I er-ror rates and the coverage of confidence intervals, but exper-imenters are equally concerned about Type II errors (failures to reject the null) and related errors such as incorrectly es-timating the direction or magnitude of effects. Many prin-cipled approaches to choosing how to assign units to one of many available treatments over time (e.g., solutions to multi-armed bandit problems) require correctly estimating one X  X  uncertainty about the expected payoffs of the treat-ments [21]. Therefore, we expect that addressing multiway dependence will remain important when taking these ap-proaches as well. A related point is that experimenters of-ten exert considerable effort reducing the width of CIs by increasing precision through design and adjustment [4, 13, 15]. Many of these methods could be applied in combina-tion with single or multiway bootstrapping. Finally, there may be other practical ways to reduce the width of multiway bootstrap CIs through using linear combinations of variance estimates from different bootstrap procedures [6, 17].
We would like to give many thanks to Daniel Merl, as well as Alex Deng, Wojciech Galuba, Brian Karrer, Art B. Owen, Luca Pozzi, and Daniel Ting for their thoughtful comments. [1] A. Agresti. Categorical Data Analysis .
 [2] R. H. Baayen, D. J. Davidson, and D. M. Bates. [3] E. Bakshy, D. Eckles, R. Yan, and I. Rosenn. Social [4] G.E.Box,J.S.Hunter,andW.G.Hunter. Statistics [5] R. L. Brennan, D. J. Harris, and B. A. Hanson. The [6] A. Cameron, J. Gelbach, and D. Miller. Robust [7] T. Crook, B. Frasca, R. Kohavi, and R. Longbotham. [8] B. Efron. Bootstrap methods: Another look at the [9] A. Gelman and F. Tuerlinckx. Type S error rates for [10] A. S. Gerber and D. P. Green. Field Experiments: [11] R. Kohavi, A. Deng, B. Frasca, R. Longbotham, [12] R. Kohavi, R. Longbotham, D. Sommerfield, and [13] W. Lin. Agnostic notes on regression adjustments to [14] P. McCullagh. Resampling and exchangeable arrays. [15] L. W. Miratrix, J. S. Sekhon, and B. Yu. Adjusting [16] A. B. Owen. The pigeonhole bootstrap. The Annals of [17] A. B. Owen and D. Eckles. Bootstrapping data arrays [18] N. C. Oza and S. Russell. Experimental comparisons [19] D. B. Rubin. Estimating causal effects of treatments [20] D. B. Rubin. The Bayesian bootstrap. The Annals of [21] S. L. Scott. A modern Bayesian look at the [22] S.R.Searle,G.Casella,C.E.McCulloch,etal.

