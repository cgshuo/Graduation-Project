 1. Introduction
Proximity searching is the problem of, given a data set and a similarity criterion, finding the elements of the set that are close to a given query. This problem has a vast number of applications. Some examples are:  X  Non -traditional databases . New so-called  X  X  X ultimedia X  X  data types such as images, fingerprints, audio and video cannot be meaningfully queried in the classical sense. In multimedia applications, all the queries ask for objects similar to a given one, whereas comparison for exact equality is very rare. Some example appli-cations are image, audio or video databases, face recognition, fingerprint matching, voice recognition, med-ical databases, and so on.  X  Text retrieval . Huge text databases with low quality control are emerging (being the Web the most prom-inent example), and typing, spelling or OCR (optical character recognition) errors are commonplace in both the text and the query. Documents which contain a misspelled word are no longer retrievable by a correctly written query. Thus many text search engines aim to find documents containing close variants of the query words. There exist several models of similarity among words (variants of the  X  X  X dit distance X  X  [32] ) which capture very well those kind of errors. Another related application is spelling checkers, where we look for close variants of a misspelled word in a dictionary.  X  Information retrieval . Although not considered as a multimedia data type, unstructured text retrieval poses problems similar to multimedia retrieval. This is because textual documents are in general not structured to easily provide the desired information. Although text documents may be searched for strings that are pres-ent or not, in many cases it is more useful to search them for semantic concepts of interest. The problem is basically solved by retrieving documents similar to a given query [5] , where the query can be a small set of words or even another document. Some similarity approaches are based on mapping a document to a vec-tor of real values, so that each dimension is a vocabulary word and the relevance of the word to the doc-ument (computed using some formula) is the coordinate of the document along that dimension. Similarity functions are then defined on that space. Notice, however, that the dimensionality of the space is very high (thousands of dimensions).  X  Computational biology . DNA and protein sequences are the basic objects of study in molecular biology.
They can be modeled as texts, and in this case many biological quests translate into finding local or global similarities between sequences, in order to detect homologous regions that permit predicting functionality, structure or evolutionary distance. An exact match is unlikely to occur because of measurement errors, minor differences in genetic streams with similar functionality, and evolution. The measure of similarity used is related to the probability of mutations such as reversals of pieces of the sequences and other rear-rangements (global similarity), or variants of edit distance (local similarity).  X  There are many other applications, such as machine learning and classification, where a new element must be classified according to its closest existing element; image quantization and compression, where only some vectors can be represented and those that cannot must be coded as their closest representable point; func-tion prediction, where we want to search for the most similar behavior of a function in the past so as to predict its probable future behavior; and so on.

All those applications have some common characteristics, captured under the metric space model [16] . There is a universe X of objects , and a non-negative distance function d : X X ! R
This distance satisfies the three axioms that make  X  X ; d  X  a metric space :
These properties hold for many reasonable similarity functions. The smaller the distance between two objects, the more  X  X  X imilar X  X  they are. We have a finite database U X , which is a subset of the universe of objects and can be preprocessed to build an index . Later, given a new object from the universe, a query q 2 X , we must retrieve similar elements found in the database. There are two typical queries of this kind:
Range query ( q , r ): Retrieve all elements which are within distance r to q . That is,  X  q ; r  X  X  f x 2 U ; d  X  x ; q  X  6 r g . k -Nearest neighbor query NN k ( q ): Retrieve the k elements from U closest to q . That is, NN " x 2 NN k ( q ), y 2 U NN k  X  q  X  , d ( q , x ) 6 d ( q , y ), and The distance is assumed to be expensive to compute (think, for instance, in comparing two fingerprints).
Hence, it is customary to define the complexity of the search as the number of distance evaluations performed, disregarding other components such as CPU time for side computations, and even I/O time. Given a database ture the database so as to compute much fewer distances.
 A particular case of this problem arises when the space is R paper we focus in general metric spaces, although these solutions are also well suited to D -dimensional spaces.
It is interesting to notice that the concept of  X  X  X imensionality X  X  can be translated into metric spaces as well, where it is called intrinsic dimensionality . Although there is not an accepted criterion to measure intrinsic sic dimensionality X  X hen its histogram of distances is concentrated. A high intrinsic dimension degrades the performance of any similarity search algorithm [16] .

By far, the most successful technique for searching metric spaces ever proposed is AESA [40,16] . Its main problem, however, is that it requires precomputing and storing a matrix of all the n ( n 1)/2 distances among the objects of U . This huge space requirement makes it unsuitable for most applications.
Let us now switch to graph theory and the t -spanner concept. Let G ( V , A ) be a connected graph with a non-is the one minimizing the sum of the costs of the traversed edges. Let us call d minimum sum). Shortest paths can be computed using Floyd X  X  algorithm [22] or applying j V j iterations of
Dijkstra X  X  algorithm [20] taking each vertex as the origin node [19,41] .A t -spanner is a subgraph G
E A , which permits us to compute path costs with stretch t , that is, ensuring that for any cations in distributed systems, communication networks, architecture of parallel machines, motion planning, robotics, computational geometry, and others.

Our main idea is to combine both concepts, so as to use the t -spanner as a bounded-error approximation to the full AESA distance matrix, in order to obtain a competitive space X  X ime trade-off for metric space searching.
Hence, our interest in this paper is on the design and evaluation of t -spanner algorithms that work well in metric space contexts. To achieve this goal, several algorithms to build, maintain, and search t -spanners are proposed. All these algorithms are experimentally evaluated, to show that our approach provides an excellent low-cost approximation to the performance of AESA. As an example, we apply the idea to information retrie-val, with document databases using the cosine distance [5] . If we use a 2-spanner, we need only 3.99% of the memory required by AESA for the index. To retrieve the most similar document, we need only 9% distance evaluations over AESA, and 8% to retrieve the 10 most similar documents.
 This paper is organized as follows. In Section 2 we cover related work both in metric spaces and in t -spanners.
In Section 3 we show how to use t -spanners for metric space searching. In Section 4 we present our t -spanner construction algorithms. Experimental results are shown in Section 5 . In Section 6 we discuss t -spanner updat-ing algorithms. Finally, in Section 7 we draw our conclusions and future work directions. Early versions of this work appeared in [31,30] . 2. Related work 2.1. Searching in metric spaces
There are several methods to preprocess a metric database in order to reduce the number of distance eval-uations at search time. All of them work by discarding elements using the triangle inequality. We discuss here some approaches that are relevant to our work. See [16] for a more complete survey. 2.1.1. Pivot-based algorithms
We will focus on a particular class of algorithms called pivot -based . These algorithms select a set of pivots f p pivot-based algorithms measure d ( q , p 1 ) and use the fact that, because of the triangle inequality, d ( q , u ) P j d ( q , p 1 ) d ( u , p 1 ) j , so they can discard every u 2 U such that since this implies d ( q , u )&gt; r . Once they are done with p directly compared against q .

The k distance evaluations computed between q and the pivots are known as the internal complexity of the algorithm. If there is a fixed number of pivots, this complexity has a fixed value. On the other hand, the dis-tance evaluations used to compare the query against the objects not discarded by the pivots are known as the external complexity of the algorithm. Hence, the total complexity of a pivot-based search algorithm is the sum of the internal and external complexities. Since the internal complexity increases and the external complexity decreases with k , there is an optimal k * that minimizes the total complexity. However, in practice k that one cannot store all the k * n distances, so the index uses as many pivots as memory permits.
There are many pivot-based algorithms. Among them we can find structures for discrete or continuous dis-
Fixed -Height FQT (FHQT) [4,3] , and Fixed Queries Array (FQA) [14] . In the continuous case we have: Van-tage -Point Tree (VPT) [42,17,39] , Multi -Vantage -Point Tree (MVPT) [9,8] , Excluded Middle Vantage Point
Forest (VPF) [43] , Approximating Eliminating Search Algorithm (AESA) [40] ,and Linear AESA (LAESA) [29] . For a comprehensive description of these algorithms see [16] . 2.1.2. Approximating eliminating search algorithm (AESA)
In AESA the idea of pivots is taken to the extreme k = n , that is, every element is a potential pivot and pivot to use next is chosen from the elements not yet discarded. Additionally, as it is well known that pivots closer to the query are much more effective, the pivot candidates u are ranked according to the sum of their current lower-bound distance estimations to q . That is, if we have used pivots { p as the element u minimizing for all u 2 U . Then, it chooses an object p 2C minimizing SumLB (Eq. (2) ) and removes it from C . Note that the first object p = p 1 is chosen at random. AESA measures d removes from C all the objects u that satisfy d ( u , p ) 6 2 [ d precomputed full distance matrix U U . For non-discarded objects, it updates sumLB according to Eq. (2) . These steps are repeated until C X ; . Fig. 1 depicts the algorithm.

AESA is, by far, the most efficient existing search algorithm. As a matter of fact, it has been experimentally shown to have almost constant search cost. Nevertheless, the constant hides an exponential dependence on the dimensionality of the metric space. AESA X  X  main problem is that storing O( n most applications. This has restricted an excellent algorithm to the few applications where n is very small.
Our main goal in this paper is to overcome this weakness. 2.1.3. Previous graph-based approaches
We have found only one previous metric space index based on graphs [37] . They use a graph whose nodes are the objects of the metric space and whose edges are an arbitrary collection of distances among the objects.
They compute two n  X  n matrices with upper and lower bounds to the real distances, in O( n solved using both matrices.

The greatest deficiency of [37] is that the selected distances are arbitrary and do not give any guarantee on the quality of their approximation to the real distances. In fact, the index only has good behavior when dis-tances follow a uniform distribution, which does not occur in practice. Even in R , an extremely easy-to-handle metric space, distances have a triangular distribution, whereas in general metric spaces the distance distribu-tion is usually more concentrated, far from uniform.

In our work, instead, edges are carefully chosen so as to provide an approximation guarantee to the real distance value. This yields much better results. 2.2. t-Spanner construction algorithms
It was shown [33] that, given a graph G with unitary weight edges and parameters t and m , the problem of determining whether a t -spanner of at most m edges exists is NP-complete. Hence, there is no hope for efficient construction of minimal t -spanners. Furthermore, as far as we know, only in the case t = 2 and graphs with unitary weight edges there exist polynomial-time algorithms that guarantee an approximation bound in the number of edges (or in the weight) of the resulting t -spanner [27] (the bound is log
If we do not force any guarantee on the number of edges of the resulting t -spanner, a simple O( mn shown [1,2] that these techniques produce t -spanners with n 1 More sophisticated algorithms have been proposed by Cohen in [18] , producing t -spanners with guaranteed graph. In our metric space application m = H ( n 2 ), which translates into worst case time O( n shown in Section 3 , is unsuitable for our application: We need 1 &lt; t 6 2 in most cases to obtain reasonable search performance. (Perhaps some of Cohen X  X  algorithms could be adapted to work heuristically for smaller t , but to the best of our knowledge, this has not been attempted so far.) Note that, for t = 2, Cohen X  X  algo-rithm could still be useful, but its time complexity would be rather high, O( n [38] work only for t =1,3,5, ... , which is also unsuitable for us. Parallel algorithms have been pursued in [28] , but they do not translate into new sequential algorithms.

D -dimensional space with Euclidean distance, much better results exist [21,1,2,26,25,36] , showing that one can dinate information and cannot be extended to general metric spaces.

Other related results refer to probabilistic approximations of metric spaces using tree metrics [6,12] . The idea is to build a set of trees such that their union makes up a t -spanner with high probability. However, the t values are of the form O(log n loglog n ), far from practical for our goals. 3. Simulating AESA search over a t -spanner 3.1. Relation between metric space searching and t-spanners
As we have said, AESA is the most successful technique for searching metric spaces. However, its huge memory requirement of AESA, we could use it in many practical scenarios. Towards this end, our main idea is to use the t -spanner as a bounded-error approximation to the full AESA distance matrix. This permits us trading space for query time, where the full AESA is just one extreme of the trade-off.
 Note that, given the element set U , the full AESA distance matrix can be regarded as a complete graph and its cost is the distance d ( u , v ). Thus, in order to save memory we can use a t -spanner G to store O( n 2 ) distances but only j E j edges. However, in this case we cannot apply AESA directly over the t -spanner, but we have to take into account the error introduced by stretch factor t . 3.2. Implementing AESA over a t-spanner
Given the t -spanner G 0 of G  X  U ; U U  X  , for every u ; v 2 U the following property is guaranteed: Eq. (3) permits us adapting AESA to this approximated distance. According to the stretch factor t , to simulate
AESA over a t -spanner it is enough to  X  X  X xtend X  X  the upper bound of the AESA exclusion ring with the asso-ciated decrease in the discrimination power. See Fig. 2 .

Let us return to the condition to discard an element u with a pivot p . The condition to be outside the ring, that is, Eq. (1) , can be rewritten as
Since we do not know the real distance d ( u , v ), but only the approximated distance over the t -spanner, d  X  p ; u  X  , we can use Eqs. (3) and (4) to obtain the new discarding conditions, in Eqs. (5) and (6) : u . Fig. 2 b illustrates.

Theorem 1 ( t -discarding). Given a pivot p 2 U , a query q 2 X and a radius r 2 R ( 6 ) , then d(q, u) &gt; r.
 t d  X  u ; p  X  , from Eq. (6) we obtain that t d  X  u ; p  X  P d case guarantees that d ( q , u )&gt; r . h
What we have obtained is a relaxed version of AESA, which requires less memory (O( j E j ) instead of O( n and, in exchange, discards less element per pivot. As t tends to 1, our approximation becomes better but we need more and more edges. Hence we have a space X  X ime trade-off where full AESA is just one extreme.
Let us now consider how to choose the next pivot. Since we have only an approximation to the true dis-imental fine-tuning, we have chosen a t  X  2 = t  X  1 3 , so as to rewrite Eq. (2) as follows: Our search algorithm is as follows. We start with a set of candidate nodes C , which is initially U , and set
SumLB ( u ) = 0 for all u 2 U . Then, we choose a node p 2C minimizing SumLB
C . Note that the first object p = p 1 is chosen at random. We measure d shortest path distance to p satisfies d G 0  X  v ; p  X  &gt; t  X  d increasing order, we know that all the remaining nodes will be farther away. By the t -discarding theorem, we remove from C nodes u which satisfy either Eq. (5) or (6) . For the non-discarded nodes we update sumLB according to Eq. (7) . We repeat these steps until C X ; . Fig. 3 depicts the algorithm.

The number of distance evaluations performed by AESA is in practice close to O(1), and the extra CPU time is close to O( n ) [40] (the constants hide the dimensionality of the space). In our case, however, we have the additional cost of computing shortest paths. Albeit we are interested only in the nodes belonging to C ,we need to compute distances to many others in order to obtain those we need. We remark that the shortest path algorithm works only up to the point where the next closest element found is far enough. In the worst case, if Dijkstra X  X  algorithm uses a heap data structure to find the next closest node, the total extra CPU time is
O( n p m log n ), where n p is the amount of nodes used as pivots, and m  X j E j X  n 1 complexity of a query is at most AESA CPU time cost multiplied by O  X  n O is in practice around O( n 0.13 log n ), which is rather moderate.
 Recall that we focus on applications where the cost to compute d dominates even heavy extra CPU costs.
There are many metric spaces where computing a distance is highly expensive. For instance, in the metric space of documents under the cosine distance [5] , computing a distance requires numerous disk accesses (in order to load the vector representation of the document) and hundreds of thousands of basic arithmetic oper-ations (in order to compute the angle between the vectors representing the documents). In these cases the dis-tance evaluation takes several milliseconds, which is rather expensive, even compared to Dijkstra X  X  algorithm computations we introduced by using a t -spanner as the metric database representation.

We finish with a note on the t values of interest. In most metric spaces of practical applicability the distance controlled by the parameter t . In the experimental study we achieve good results with values t 2 (1.0,2.0].
However, in some metric spaces it is possible to obtain reasonable results even using values of t beyond 2.0. 4. Practical t -spanner construction algorithms
In Section 4.1 we present a basic t -spanner construction technique. This algorithm has important deficien-cies: excessive edge-insertion cost and too high memory requirements. We seek practical algorithms that allow tage of the triangle inequality. For this sake, we propose four t -spanner construction algorithms, with the goals of decreasing CPU and memory cost, and producing t -spanners of good quality (that is, with few edges).
Our four algorithms are: (1) An optimized basic algorithm, where we limit the propagation of computations when a new edge is (2) A massive edge-insertion algorithm, where we amortize the cost of recomputing distances over many (3) An incremental algorithm, where nodes are added one by one to a well-formed growing t -spanner. (4) A recursive algorithm, combining a divide and conquer technique with a variant of the incremental Table 1 shows the worst case complexities obtained. Empirically, the time costs are around of the form
C
 X  n 2.24 , and the number of edges are around of the form m = C that good-quality t -spanners can be built in reasonable time: note that just scanning all the edges of the com-plete graph needs O( n 2 ) time.

We remark that, although the m values in each row of Table 1 are different (because they are the number of
We take no particular advantage of the metric properties of the edge weights, so our algorithms can be used on general graphs too. The only extra work needed is to precompute the shortest path among every pair of nodes, which is free when the triangle inequality holds. From now on, we use the following terms to simplify the exposition:  X  Given a pair u ; v 2 U , the t -condition is d G 0  X  u ; v  X  estimated over the graph, and d ( u , v ) is the distance in the metric space. Otherwise, it is not t -estimated .
 4.1. Basic t-spanner construction algorithm
The intuitive idea to solve this problem is iterative. We begin with an initial t -spanner that contains all the vertices and no edges, and calculate the distance estimations among all vertex pairs. These are all infinite at until all the distance estimations satisfy the t -condition.

The edges are considered in ascending cost order, so we start by sorting them. Using smaller-cost edges first is in agreement with the geometric idea of inserting edges between near neighbors and making up paths from low-cost edges in order to use few edges overall.

The algorithm uses two symmetric matrices. The first, real , contains the true distances between all the objects, and the second, estim , contains the distance estimations obtained with the t -spanner under construc-tion. The t -spanner is stored as an adjacency list.

The insertion criterion is that an edge is added to the set E only when its current estimation does not satisfy the t -condition. After inserting the edge, it is necessary to update all the distance estimations. The updating mechanism is similar to the distance calculation mechanism of Floyd X  X  algorithm [22] , except that edges, not nodes, are inserted into the set. Upon insertion of an edge e =( e now go through edge e . There are two choices for this: v the basic t -spanner construction algorithm.

Analysis . The basic t -spanner construction algorithm takes H ( n construction, H ( mn 2 ) CPU time (recall that n = j V j and m = j E j ), and H ( n deficiencies are excessive edge-insertion cost and too high memory requirements. 4.2. Optimized basic algorithm
Like the basic algorithm (Section 4.1 ), this algorithm makes use of real and estim matrices, choosing the edges in increasing weight order. The optimization focuses on the mechanism to update distance estimations.

The main idea is to control the propagation of the computation, updating only the distance estimations that can be affected by the insertion of the new edge, and disregarding those that can be proved not to be affected by the new inserted edge.

Fig. 5 illustrates the insertion of a new edge ( a 1 , a 2 only to the nodes that improve their distance estimation to a the edge that was inserted. The computation then propagates to the neighbors of the a { b 1 , b 2 , b 3 }, then to nodes { c 1 , c 2 }, and finally { d current distance estimations or when it does not have further neighbors.

The complete algorithm reviews all the edges of the graph. For each edge, it iterates until no further prop-agation is necessary. In order to control the propagation, the algorithm uses two sets, ok and check .  X  ok : The nodes that have already updated their shortest path estimations due to the inserted edge. These are the nodes that we still need to update.
 Fig. 6 depicts the optimized basic algorithm.

Analysis . The optimized basic algorithm takes H ( n 2 ) distance evaluations and H ( mk
H ( n 2 + m )= H ( n 2 ). Although this algorithm reduces the CPU time, this time is still high, and the memory requirements are also too high.

A good feature of the algorithm is that, just like the basic algorithm, it produces good-quality t -spanners (few edges). We have used its results to predict the expected number of edges per node in a t -spanner, so as to fine-tune other algorithms that rely on massive insertion of edges. We call j E intrinsic dimension of a given metric space we develop an empirical method, which will be described in Section 5.1.1 . In Section 5 ( Tables 2 and 3 ) we show some estimations obtained.
 4.3. Massive edge-insertion algorithm
This algorithm tries to reduce both CPU processing time and memory requirements. To reduce CPU time, the algorithm updates the distance estimations only after performing several edge insertions, using an
O( m log n )-time Dijkstra X  X  algorithm to update distances. To reduce the memory requirement, distances between objects are computed on the fly.

Since we insert edges less carefully than before, the resulting t -spanner could be of lower quality. Our effort aims at minimizing this effect.

The algorithm has three stages. In the first stage, it builds the t -spanner backbone by inserting whole min-imum spanning trees (MSTs), and determines the global list of not t -estimated edges ( pending ). In the second inserts all the remaining  X  X  X ard X  X  edges.
 The algorithm uses two heuristic values:
H 1 predicts the expected number of edges per node, and it is obtained from the optimized basic algorithm edge model: H 1 = j E t Spanner 1 ( n , dim , t ) j / n . With H whether or not to insert the remaining edges (those still not t -estimated) of the current node. Note that j E tion algorithm, so we can use H 1 as an expected lower bound of the number of edges per node. As it is shown by experimental results in Section 5 , in most cases the sizes of resulting t -spanners produced by any of our algorithms are rather similar, which implies that n  X  H size of the t -spanner.

H 2 is used to control the growth of the pending list size and will give a criterion to decide when to insert an additional MST into the t -spanner under construction. The maximum pending list size is H where t -Spanner is t -spanner under construction. After preliminary experiments we have fixed H values lower than 1.2 the algorithm takes more processing time without improving the number of edges, and with higher values the algorithm takes less processing time, but it inserts more edges than necessary and needs more memory to build the t -spanner.
 We describe now the stages of the algorithm.

Stage 1 . We insert successive MSTs to the t -spanner. The first MST follows the basic Prim X  X  algorithm [35] , but next MSTs are built choosing only edges that have not yet been inserted.

We make a single pass over the nodes, adding to the pending list the not t -estimated edges that are incident upon the current node under revision. Every time the size of list pending exceeds H the tighter the estimations of the distance (by using the shortest path between two objects). Thus, after an
MST is inserted, we remove the edges from pending list that become t -estimated. Additionally, if the current node has no more than H 1 /2 pending edges, we just insert them, since we only need a small set of edges in order to fix all the distance estimations for this node.

This stage continues until we pass through all the nodes. The output is the t -spanner backbone and the glo-bal list of pending edges ( pending ).
 ing edges ( pendingNodes ), from those with most to those with least pending edges. For each such node, we check which edges have to improve their t -estimation and which do not. Note that edges originally in the pend-ing list may have become well t -estimated along the process due to new added edges. We need to insert more edges in order to improve distance estimations (of edges that are incident to the current node that remain in pending ), so from those still not t -estimated edges, we insert H node.

Note that the current node may have more than H 1 /4 not t -estimated edges. However, this technique allows us to review in the first place nodes that require more attention, without concentrating all the efforts in the same node. With values lower than H 1 /4 the algorithm takes more processing time without improv-ing the number of edges considerably, and with higher values the algorithm inserts more edges than necessary.
The process considers two special cases. The first case occurs when we have inserted more than n edges overall, in which case we regenerate and re-sort list pendingNodes and restart the process (so that we resume the work on nodes with many pending edges). The second special case occurs when the pending list of the cur-rent node is so small (less than H 1 /4 edges) that we simply insert all its elements.
 estimate edges are so few that it is not worth the effort of reducing the pending list once again. We made preliminary experiments in order to fix this value, and obtained the best trade-off between CPU time and t -spanner size with n /2.

Stage 3 . The hard-to-estimate edges remain in the pending list, so we just insert the pending list into the t -Spanner to complete the t -spanner construction.
 Fig. 7 depicts the massive edge-insertion algorithm.

Analysis . The massive edge-insertion algorithm takes H ( nm ) distance evaluations, H ( nm log n ) CPU time, and H ( n + m )= H ( m ) memory. It is easy to see that the space requirement is H ( m ): the pending list is never larger than H ( m ) because at each iteration of stage 1 it grows at most by n 1, and as soon as it becomes larger than H 2  X  j t-Spanner j 6 H 2  X  m we add a new MST into t-Spanner (so m grows by n 1 as well). The distance evaluations come from running a H ( n times at stage 1 (since each run adds n 1 edges). The CPU time comes from running a H ( m log n )-time
Dijkstra X  X  algorithm once per node, and a H ( n 2 )-time Prim X  X  algorithm at most m / n times at stage 1. This adds up H ( nm log n )+ H ( mn )= H ( nm log n ) CPU time. At stage 2 we insert edges in groups of H ( m / n ), running Dijkstra X  X  algorithm after each insertion, until we have inserted j pending j n /2 = H ( m ) edges over-term.

This algorithm reduces both CPU time and memory requirements, but the amount of distance evaluations is very high ( H ( nm ) P H ( n 2 )). 4.4. Incremental node insertion algorithm
This algorithm reduces the amount of distance evaluations to just n ( n 1)/2, while preserving the amor-tized update cost idea. We insert nodes one by one, not edges. The invariant is that, for nodes 1 ... i 1, we have a well-formed t -spanner, and we want to insert the i th node to the growing t -spanner.
This algorithm, unlike the previous ones, makes a local analysis of nodes and edges, that is, it takes deci-sions before having knowledge of the whole edge set. This can affect the quality of the t -spanner.
For each new node i , the algorithm carries out two operations: the first is to connect the node i to the grow-mations satisfy the t -condition, adding some edges to node i until the invariant is restored. We repeat this process until the whole node set is inserted.

We also use the H 1 heuristic, with the difference that we recompute H size changes). We insert d  X  H 1  X  i ; dim ; t  X  = 5  X  j E time. The factor 5 in the denominator is tuned so that inserting more edges at a time obtains lower processing without significantly reducing the t -spanner size.

For the verification of distances to the new node we use an incremental Dijkstra X  X  algorithm with limited rent computed distances in an array. This is because, in Dijkstra X  X  algorithm, edges incident upon a processed node are considered only if the node has improved its current distance estimation. Furthermore, note that when the distance from node i to node j is not t -estimated, we do not really need to know how poorly esti-algorithm reuses the array previously computed, because there is no need to propagate distances from nodes whose estimations have not improved. Note that this allows the shortest path propagation to stop as soon as possible, both in the first and in the following iterations.
 Fig. 8 depicts the incremental node insertion algorithm.

Analysis . The incremental node insertion algorithm takes H ( n time, and H ( n + m )= H ( m ) memory. The CPU time comes from the fact that every node runs Dijkstra X  X  algo-4.5. Recursive algorithm
The incremental algorithm is an efficient approach to construct metric t -spanners, but it does not perform a made up of objects close to each other. Following this principle, we present a solution that divides the object set into two compact subsets, recursively builds sub-t -spanners for the subsets, and then merges them.
For the initial set division we take two objects p 1 and p tatives , and divide the set among objects nearer to p 1 and nearer to p divisions we reuse the corresponding representative, and the element farthest to it becomes the other. The recursion finishes when we have less than 3 objects.

The merging step also takes into account the spatial proximity among the objects. When we merge the sub-t -spanners, we have two node subsets V 1 and V 2 , where j V the sub-t -spanner represented by p 2 ( stsp 2 ), we choose the object u closest to p spanner represented by p 1 ( stsp 1 ) verifying that all the distances towards V is equivalent to using the incremental algorithm, where we insert u into the growing t -spanner stsp tinue with the second closest and repeat the procedure until all the nodes in stsp illustrates. Note that edges already present in stsp 2 are conserved. We also use the H we recompute H 1 at the beginning of the merging step (not upon inserting every node). We insert processing time.

This algorithm also uses an incremental Dijkstra X  X  algorithm with limited propagation, but this time we are only interested in limiting the propagation towards stsp already satisfy the t -condition). Albeit we are interested only in the nodes belonging to stsp compute distances to stsp 2 to obtain those we need. Hence, Dijkstra X  X  algorithm takes an array with precomputed distances initialized at t  X  d ( u i , u j )+ e for ( u e is a small positive constant. For the next iterations, Dijkstra X  X  algorithm reuses the previously computed array.
 Fig. 10 depicts the recursive algorithm and the auxiliary functions used to build and merge sub-t -spanners.
Analysis . The recursive algorithm requires H ( n 2 ) distance evaluations, H ( nm log n ) CPU time, and H ( n + m )= H ( m ) memory. The cost of dividing sets does not affect that of the underlying incremental construction. 5. Experimental results
We have tested our construction and search algorithms on spaces of vectors, strings and documents (these last two are of interest to Information Retrieval applications [5] ). The experiments were run on an
Intel Pentium IV of 2 GHz, with 2.0 GB of RAM, with local disk, under SuSE Linux 7.3 operating system, with kernel 2.4.10-4GB i686, using g++ compiler version 2.95.3 with optimization option O9 , and the pro-cessing time measured user time. For construction algorithms, we are interested in measuring the CPU time needed and the amount of edges generated by each algorithm (the number of distance evaluations is always n ( n 1)/2 in the competitive alternatives). For the search algorithm, we are interested in measuring the number of distance evaluations performed in the retrieval operation. For shortness we have called the optimized basic algorithm t -Spanner 1, the massive edge-insertion algorithm t -Spanner 2, the incremental algorithm t -Spanner 3, the recursive algorithm t -Spanner 4, and the simulated AESA over the t -spanner t -AESA.

The construction experiments compare t -Spanner 1, 2, 3 and 4, in order to determine which is the most appropriate to metric spaces, with t 2 [1.4,2.0]. The search experiments use t -spanners with stretch factors t 2 [1.4,2.0], and compare them against AESA. Since t -spanners offer a time X  X pace trade-off and AESA does not, we also consider pivot-based indexes with varying number of pivots. For every t value, we measure the size of the resulting t -spanner and build a pivot-based index using the same amount of memory (pivots are chosen at random). This way we compare t -spanners against the classical space X  X ime trade-off for AESA. Note that with values of t &lt; 1.4 the performance of our construction algorithms noticeably degrades, whereas the search algorithm does not improve considerably.

Since in some cases the pivots were too many compared to the average number of candidates to eliminate, we decided to stop using further pivots when the remaining candidates were fewer than the remaining pivots.
This way we try not to pay more for having more available pivots than necessary. Also, it turns out that, some-times, even the smallest number of pivots shown is beyond the optimal. In these cases we also show results with fewer pivots, until we reach the optimum. 5.1. Uniformly distributed vectors under Euclidean distance
We start our experimental study with the space of vectors uniformly distributed in a real D -dimensional cube under the Euclidean distance, that is, ([ 1,1] D , L the effect of the space dimension D on our algorithms. We have not explored larger D values because D =28is already too high-dimensional: we can only build t -spanners in reasonable time for t P 1.8, which is too large for searching. We remind that all metric space search algorithms fail for these large values of D .
In particular, for t -Spanner 1, we can obtain an edge model to implement the H compute the intrinsic dimensionality of a given metric space M , we can apply the H
M , even if M has no coordinates. For this sake, in Section 5.1.1 we show an experimental method to estimate the intrinsic dimensionality of a given metric space.

Note that we have not used the fact that the space has coordinates, but have rather treated points as abstract objects in an unknown metric space. Computing a single distance takes from 0.893 l s in the 4-dimen-sional space, to 1.479 l s in the 24-dimensional space.

In the construction experiments, we use uniform data sets of varying size n 2 [200 ... 2000]. We first com-pare the construction algorithms, in order to choose the best. Later, search experiments are carried out over the t -spanner produced by the best construction algorithm. In the search experiments, we index a uniform data set formed by 10,000 objects, and select 100 random queries not included in the index, using search radii that on average retrieve 1 object ( r = 0.137 for D =4, r = 0.570 for D =8, r = 1.035 for D = 12, r = 1.433 for
D = 16, r = 1.812 for D = 20 and r = 2.135 for D = 24). 5.1.1. Estimating the intrinsic dimensionality of a general metric space
As said in the Introduction, the higher the intrinsic dimensionality dim of a given metric space M  X  X  X ; d  X  , the more difficult to solve proximity queries. Nevertheless, there is not an accepted criterion to measure dim .
Yet, most authors agree in that the intrinsic dimensionality of a D -dimensional vector space with uniform dis-tribution is simply D [43,16] .

Hence, to experimentally estimate the intrinsic dimensionality of a given metric space M , we approximate dim with the dimensionality D of the uniformly distributed vector space that performs similarly to M for a given benchmark comparison. Later, we refine the estimation by exploring values of dim around D . Therefore, we run the experiments to estimate dim over a small subset of the objects. Next, we refine the estimation of dim using some of the massive edge-insertion algorithms ( t -spanner 2, 3 or 4).

Thus, the procedure has the following stages. The first consists in computing the edge model j E metric space of interest M and computing its edge model. Third, we find out which value D in the model for the uniform space corresponds to the performance measured in M . Finally, we test some of the massive edge-insertion algorithms using dim values around D to find which dim value produces the smallest t -spanner without increasing the CPU time considerably.
 5.1.2. Construction
Fig. 11 shows a comparison among the four algorithms on the uniform data set where we vary D 2 [4,24], for n = 1000 nodes and t = 1.4 and 1.8. Figs. 11 b and d show that t -Spanner 1 is impractically costly, but it produces the best (that is, smallest) t -spanner as shown in Figs. 11 a and c. The next slowest algorithm is t -
Spanner 2, being t -Spanner 3 and 4 very similar in performance. On the other hand, the quality of the gener-ated t -spanners is rather similar for all, being t -Spanner 3 the algorithm that produces t -spanners with most spanners are produced by t -Spanner 4 (low dimensions, D &lt;8)orby t -Spanner 2 (medium and high dimen-sions, D P 8). Note that the difference in t -spanner quality becomes less significant for higher dimensions.
Finally, it is interesting to notice that, even in dimension D = 24 and for t = 1.8, the number of edges in the resulting t -spanners is still less that 8.7% of the complete graph.
 Fig. 12 compares the four algorithms where we vary t 2 [1.4,2.0], for n = 1000 nodes and D = 12 and 24. 1.65-spanner we need 14% of edges of the complete graph in high dimensionality ( D = 24), 7% in medium dimensionality ( D = 12) and just 1.44% in low dimensionality spaces ( D = 4). Moreover, to construct a 2.0-spanner for the same values of D , we need just a 7%, 3.8% and 1% of the complete graph, respectively. Fur-thermore, t -Spanner 3 and 4 also perform well with respect to CPU time.

It is also interesting to notice that the joint effect of high dimensionality ( D &gt; 16) and small values of t ( t &lt; 1.5) produces a sharp increase both in the number of generated edges and the CPU time.
Fig. 13 shows the effect of the set size n in our algorithms. It can be seen that, in low dimensions ( D = 5), t -Spanner 2, 3 and 4 are slightly super-quadratic in CPU time. On the other hand, all the algorithms produce t -spanners slightly super-linear in the number of edges.
We conclude that the fastest construction algorithm for all dimensions D 2 [4,24] is t -Spanner 3, closely followed by t -Spanner 4. The other two are very far away in performance. However, t -Spanner 3 produces t -spanners with the worst quality (many edges), albeit all the qualities are indeed close. This result was expected, since its incremental methodology locally analyzes the insertion of edges. On the other hand, the recursive algorithm of t -Spanner 4 strongly improves the quality of the incremental algorithm, profiting from the global analysis of the edge set. In some cases, t -Spanner 4 is competitive even with the optimized basic algorithm in the number of generated edges, yet using 50 times less CPU time.
 super-linear for all our construction algorithms, and times are slightly super-quadratic for t -Spanner 2, 3 and 4. This shows that our algorithms are very competitive in practice. Remember that constructing minimal t -spanners is NP-complete [33] , whereas our algorithms build small t -spanners in polynomial time.
The analytical results of [1,2] show that the size of a t -spanner built over a general graph of n nodes is n space (which is also usually hidden in the constant of the big-O notation). We correct the dimensional effect with t , because as the value of t increases the effect of the dimensionality diminishes. The CPU time model comes from running n times a H ( m log n ) CPU time Dijkstra X  X  algorithm, where we neglect the term log n to simplify the analysis of the time models. Note that in the case of t -Spanner 1 we modify the time model rithm run over matrix estim for each edge (this time is called k
After this analysis, we select the recursive algorithm to index the metric database, because it yields the best and document spaces (see Sections 5.2.1, 5.3.1 and 5.4.1 ). 5.1.3. Searching
Fig. 14 shows search results on a set formed by 10,000 uniformly distributed vectors indexed with t -Spanner 4. The 1.5-spanner indexes the database using from 0.17% of the total matrix in dimension D = 4, to 15.8% in dimension D = 24, whereas the 2.0-spanner indexes it using 0.08% for D = 4 to 2.02% for D = 24. With respect to search time, as long as the value of t diminishes, the performance of t -AESA improves.
On the other hand, the equivalent pivot-based algorithm (using the same amount of memory to index the space than the t -spanner) has better performance than t -AESA. The reasons are analyzed next. 5.1.4. Discussion To explain the poor results of our search algorithm on uniform spaces we need to consider several factors.
The first is that the stretch factor t reduces the discrimination power of our algorithm, as we can only discard objects beyond the extended upper bound of the exclusion range. The second is that, in uniform metric spaces ([ 1,1] D , L p ), distances among objects become similar as long as D increases: the variance of distances is H ( D 2/ p 1 ) [43] , which is constant in Euclidean case. The third is that, in ([ 1,1] between two objects is H  X 
H  X  whereas in D = 24 we must use radius r = 2.135. All these consequences of the curse of dimensionality affect the pivot-based algorithm as well, but our stretch factor t makes our structure more vulnerable to them.
Nevertheless, this situation will be reverted in real-world metric spaces, where our t -AESA algorithm beats the pivot-based one and it is very competitive with AESA. We conjecture that this is basically due to the exis-tence of object clusters , which naturally occur in real-world metric spaces. Note that in a real-world metric small compared to the average distance in M , compensating the loss of discrimination power. We also have to consider that in t -AESA every object can be used as a pivot; and that the pivot selection criterion (Eq. (7) ) tends to select pivots close to the query. Then, we can expect much better results in real-world spaces than in synthetic ones.

In Section 5.2 we experimentally verify our conjecture. For this sake, we model a real-world metric space as a synthetic metric space composed by Gaussian-distributed vectors under Euclidean distance, and then we analyze the behavior of our search algorithm. Later, in Sections 5.3 and 5.4 we show results on real-world metric spaces, namely the space of strings under the edit distance and the space of documents under the cosine distance. 5.2. Gaussian-distributed vectors under Euclidean distance
Real-life metric spaces have regions called clusters , that is, compact zones of the space where similar objects accumulate. With the Gaussian vector space we attempt to simulate a real-world space. The data set is formed by points in a D -dimensional space with Gaussian distribution forming 256 clusters randomly centered in the range [ 1,1] D , for D = 20, 40 and 80. The generator of Gaussian vectors was obtained from [24] . We consider three different standard deviations to make more crisp or more fuzzy clusters ( r = 0.1, 0.3 and 0.5), and t 2 [1.4,2.0]. Of course, we have not used the fact that the space has coordinates, rather we have treated the points as abstract objects in an unknown metric space.

Computing a single distance takes 1.281, 1.957 and 3.163 l s in our machine, for D = 20, 40 and 80, respec-tively. We experimentally estimate (according to Section 5.1.1 ) that the intrinsic dimensions for r = 0.1, 0.3 and 0.5 are, respectively, 4, 13 and 18 in the 20-dimensional Gaussian space; 7, 16 and 32 in the 40-dimensional Gaussian space; and 8, 29 and 55 in the 80-dimensional Gaussian space.

In the construction experiments, we only use Gaussian data sets with clusters randomly centered in [ 1,1] of varying size n 2 [200 ... 2000]. In the search experiments, we index Gaussian datasets formed by 10,000 objects distributed in 256 clusters randomly centered in the range [ 1,1] 100 random queries not included in the index, using search radii that on average retrieve 1 and 10 objects. In the 20-dimensional Gaussian space we use r = 0.442 and 0.563 for r = 0.1; r = 1.325 and 1.690 for r = 0.3; r = 2.140 and 2.560 for r = 0.5. In the 40-dimensional Gaussian space we use r = 0.700 and 0.818 for r = 0.1; r = 2.101 and 2.455 for r = 0.3; r = 3.501 and 4.081 for r = 0.5. In the 80-dimensional Gaussian space we use r = 1.070 and 1.196 for r = 0.1; r = 3.210 and 3.589 for r = 0.3; r = 5.350 and 5.981 for r = 0.5.
We first compare the construction algorithms, in order to choose the best. Later, search experiments are carried out over the t -spanner produced by the best construction algorithm. In particular, we aim at empiri-cally verifying the conjecture that t -spanners profit from clusters more than alternative structures. 5.2.1. Construction
Figs. 15 and 16 compare the four algorithms on the 20-dimensional Gaussian dataset, where we vary the stretch t 2 [1.4,2.0] and the number of nodes n 2 [200 ... 2000], respectively, with r = 0.1, 0.3, and 0.5.
As it can be seen, all the algorithms produce t -spanners of about the same quality, although the optimized edges and then it tries to connect both inner and peripheral objects among the clusters. Since we need to connect just the peripheral objects, there are many redundant edges that do not improve other distance estimations in the resulting t -spanner.

In construction time, on the other hand, there are large differences. t -Spanner 1 is impractically costly, as expected. Also, t -Spanner 2 is still quite costly in comparison with t -Spanner 3 and 4.
The bad performance of t -Spanner 2, unlike all the others, improves instead of degrading as clusters become more fuzzy, see Figs. 16 b and d. Nevertheless, the high intrinsic dimensionality of the Gaussian space with r = 0.5 negatively impacts its performance, raising again the CPU time, see Figs. 16 d and f. Furthermore, the to the second best on fuzzy clusters. This is because, on one hand, there are less redundant edges among clus-ters, and on the other hand, on a uniform space t -Spanner 2 inserts  X  X  X etter X  X  edges since they come from MSTs (which use the shortest possible edges).

Figs. 15 and 16 show that, the lower r , the lower the number of edges and CPU time. This is because the edge selection mechanisms of our algorithms profit from the clustered structure. In practice, t -Spanner 1, 3 and 4 can make good distance approximations between the clusters by using few edges. Thus, the generated t -spanners adapt well to, and benefit from, the existence of clusters.
The incremental and recursive algorithms are quite close in both measures, being by far the fastest algo-ysis. Note that, for t as low as 1.5, we obtain t -spanners whose size is 5 X 15% of the full graph. size when we move from t = 1.5 to t = 1.4. The effect shows up for smaller values of t on crisper clusters. A spanner over both clusters. Thus, few inter-cluster edges are necessary to complete the t -spanner. However, when t is reduced below some threshold, the size of the edge set suddenly explodes as we need to add many inter-cluster edges to fulfill the t -condition from one cluster to each other.
We show in Table 3 our least squares fittings on the data using the model j E j X  a n 1 microseconds for t -Spanner 2, 3 and 4. Once again, we modify the t -Spanner 1 time model to time  X  a n c microseconds so as to consider the sub-quadratic updating of matrix estim . In this metric space the effect of r is absorbed by the constants. This model has also been chosen according to the analytical results of [1,2] .Asit can be seen, we obtain the same conclusions than in the previous section, that is, t -spanner sizes are slightly super-linear and times are slightly super-quadratic. This confirms that our construction algorithms are very competitive in practice.

This analysis also confirms the selection of the recursive algorithm to index the metric database, as it yields the best trade-off between CPU time and size of the generated t -spanner. 5.2.2. Searching
For this section we will use the datasets of 10,000 Gaussian vectors in 20, 40, and 80 dimensions. Fig. 17 shows the construction results when indexing these datasets using t -Spanner 4 varying t 2 [1.4,2]. For r = 0.1, 0.3 and 0.5, the 1.4-spanner indexes the database using 0.97%, 7.70% and 16.32% of the memory required by AESA, respectively, in dimension 20. For higher dimensions, only t = 2 is practical.

Fig. 18 shows search results in dimension 20. It can be seen that the performance of t -AESA improves as the value of t decreases, both to retrieve 1 and 10 objects. We also show the pivot-based algorithm perfor-mance when using the optimum number of pivots (found by hand).

On the other hand, the cluster diameter also influences the performance of t -AESA. On crisp clusters, the results are competitive against AESA and better than the pivot-based algorithm, since the t -spanner adapts to and benefits from the existence of clusters. For instance, with r = 0.1, 1.4-AESA retrieves 1 and 10 objects using 1.05 and 1.04 times the distance evaluations of AESA, respectively, whereas the pivot-based algorithm uses 1.51 and 1.54 times the number of evaluations of AESA, respectively. Moreover, the pivot-based algo-rithm using the optimum number of pivots takes 1.28 and 1.52 times the number of evaluations of AESA, respectively. With r = 0.3, t -AESA is still better than the pivot-based algorithm. As a matter of fact, 1.4-
AESA retrieves 1 and 10 objects by using 1.53 and 2.72 times the distance evaluation of AESA, and the opti-mum pivot-based algorithm uses 5.07 and 5.11 times the distance evaluations of AESA for 1 and 10 objects.
The situation is reverted in fuzzy clusters, as expected from Section 5.1.3 , where the objects are distributed almost uniformly and the t -spanner notoriously loses discrimination power.

Fig. 19 shows the results on higher dimensions and t = 2. For crisp clusters ( r = 0.1) t -AESA needs few more distance computations than AESA, both to retrieve 1 or 10 objects. For instance, Figs. 19 a and b show that in 40/80 dimensions t -AESA uses 1.19/1.34 times the number of evaluations of AESA, both to retrieve 1 the techniques dramatically fall down in performance, as with that r the space is almost uniformly distributed, and this makes up an intractable scenario on high dimensions.

Hence we experimentally verify the conjecture that t -spanners take advantage of the clusters, which occur naturally in real-world metric spaces. The crisper the clusters, the better the performance of t -spanner techniques. It is known that all search algorithms improve in the presence of clusters. However, t -spanner based algorithms improve more than, for example, pivot-based algorithms. 5.3. Strings under edit distance
The string metric space under the edit distance has no coordinates. The edit distance is a discrete function that, given two strings, measures the minimum number of character insertions, deletions and substitutions needed to transform one string to the other [32] . Our database is an English dictionary, where we index a sub-set of n = 24,000 randomly chosen words. On average, a distance computation takes 1.632 l s.
Since these tests are more massive, we leave out the optimized basic and the massive edge-insertion algo-rithms in the construction experiments, as they were too slow. Anyway, we performed a test with a reduced dictionary in order to validate the decision of leaving them out, and to experimentally estimate the intrinsic dimensionality of the string space as dim =8.
 In the search experiments, we select 100 queries at random from dictionary words not included in the index.
We search with radii r = 1, 2, 3, which return on average 0.0041%, 0.036% and 0.29% of the database, that is, approximately 1, 8 and 66 words of the English dictionary, respectively. 5.3.1. Construction
Fig. 20 shows that, also for strings, the number of edges generated is slightly super-linear (8 : 03 n 1 bit better than the incremental algorithm in both aspects.

Fig. 21 shows construction results when indexing the whole subset of 24000 strings varying t . The full graph of 24,000 nodes has 288 million edges, whereas a 1.4-spanner has only 8.35 million edges (2.90% of the com-plete graph). Once again, Fig. 21 a and b confirm the selection of t -Spanner 4. 5.3.2. Searching
Fig. 22 presents the results as a space/time plot. We have chosen to draw a line to represent AESA although, since it permits no space X  X ime trade-offs, a point would be the correct representation. The position of this point in the x axis would be 288  X  10 6 , about 1.7 yards from the right end of the plot.
Note that, while pivot-based algorithms have a limit where giving them more memory does not improve their performance, t -AESA always improves with more memory.

Fig. 22 a shows that, with radius 1, the pivot-based algorithm has better performance than t -AESA with t = 2.0. Furthermore, the equivalent pivot-based algorithm performs better than t -AESA for t &gt; 1.7, and it uses less memory. Only with values of t 6 1.7 (which produce an index using more memory), t -AESA works systematically better, becoming very competitive for t = 1.4, where it makes just 1.27 times the distance eval-uations of AESA.

Figs. 22 b and c show that with radii 2 and 3, t -AESA has better performance than pivots for all t 2 [1.4,2.0]. For example, 1.4-AESA uses 1.16 and 1.46 times the distance evaluations of AESA with radii 2 and 3, respectively.

In order to understand these so favorable results, we have to take into account that, in addition to the fact that the string space has small radii clusters, the edit distance is a discrete function. This means that the approximated distances are rounded down to the closest integer. This fact can be interpreted as if the effective t of the structure were lower than the nominal t , so that the discrimination power lost is less than in contin-uous metric spaces. Overall, this improves the discrimination power of t -AESA. 5.4. Documents under cosine distance In Information Retrieval, documents are usually represented as unitary vectors of high dimensionality [5] .
The idea consists in mapping the document to a vector of real values, so that each dimension is a vocabulary word and the relevance of the word to the document (computed using some formula) is the coordinate of the document along that dimension.

With this model, document similarity is assessed through the inner product between the vectors. Note that the inner product between two exactly equal documents is one, since both documents are represented by the same vector. As long as the documents are more different, the inner product between the vectors representing the documents goes to zero. As we are looking for a distance, we consider the angle between these vectors.
This way, the cosine distance is simply the arc cosine of the inner product between the vectors [5] , and this satisfies the triangle inequality. Similarity under this model is very expensive to calculate.
In the construction experiments, we use a data set formed by 1200 documents obtained from TREC-3 collection ( http://trec.nist.gov/ ), and exclude the massive edge-insertion algorithm, which was too slow (the reason, this time, is that t -Spanner 2 is the algorithm that makes, by far, most distance computa-tions), with t 2 [1.4,2.0].

In the search experiments, we select 50 queries at random from the documents not included in the index, and search with radii chosen to retrieve, on average, 1 and 10 documents per query ( r = 0.13281 and 0.16659, respectively), with t 2 [1.4,2.0].

In most of these experiments, we maintained the whole document set in memory in order to avoid the disk included an experiment with the documents held on disk in Section 5.4.2 to demonstrate the effect of such an expensive distance function. 5.4.1. Construction
Fig. 23 a shows that t -Spanner 1, 3 and 4 produce t -spanners of about the same quality. However, as shown in Fig. 23 b the optimized basic algorithm is much more expensive than the other two, which are rather similar. t -Spanner 1 uses 3 times more CPU time than t -Spanner 3 or 4, whereas in Gaussian or uniform spaces t -Span-the CPU time cost, and this is always close to n ( n 1)/2 distances. As a matter of fact, for t = 2 the CPU time of t -Spanner 1, 3 and 4 are almost equal. Note that the complete graph of 1200 nodes has about 720 thousand edges, and a 2.0-spanner has only 28.7 thousands (3.99% of the complete graph). 5.4.2. Searching
Fig. 24 shows that t -AESA can achieve better performance than the pivot-based algorithm, and it is extre-mely competitive against AESA. For example, for t = 2.0, t -AESA makes 1.09 times the distance evaluations of AESA in order to retrieve the most similar document, and 1.08 times to retrieve 10 documents. We have again chosen to draw a line to represent AESA.

When comparing to pivots, we note that pivots do better than t -AESA when the amount of memory is very tage of more memory when it is available. Actually their initial achievement (350 X 400 distance computations) are not surpassed until they use much more memory, and at that point they do not beat t -AESA either.
Up to now we have considered only number of distance evaluations at search time, in the understanding that in some metric spaces the distance is expensive enough to absorb the extra CPU time we incur with the graph traversals. There are many metric spaces where this is the case, and the document space is a good example to illustrate this situation. The documents are stored on disk and we measure the overall elapsed time to retrieve 1 and 10 documents.

Fig. 25 shows that, even considering side computations, t -AESA can achieve better performance than the pivot-based algorithm. When the overall time is considered, t -AESA has an optimum t -spanner size of around 25,000 edges. This size is rather small and is achieved using t = 2.1. It is equivalent to using 41 pivots. To achieve a similar result with pivots, one has to spend about 10 times more space. 6. t -Spanners as dynamic structures for metric space searching
In many real applications, one does not know the whole object set at index construction time, but rather objects are added/removed to/from the set along time. Thus, it is necessary to update the index to represent the current status of the data set.

Hence, in order to use the t -spanner as a metric database representation, we have to grant it the ability of allowing efficient object insertions and deletions without degrading the performance of the object retrieval operations.

We show in this section how to implement a dynamic index for metric spaces based on t -spanners. These ing its quality. 6.1. Inserting an object into the t-spanner
Assume we want to insert an object u into a t -spanner G 0
G mental algorithm (Section 4.4 ) for u .

Therefore, constructing a t -spanner by successive object insertions is equivalent to using the incremental algorithm over the whole object set. 6.2. Deleting an object from the t-spanner
Elements handled in metric spaces are in many cases very large. Thus, upon an object deletion, it is man-datory to effectively remove it. As a consequence, folklore solutions such as marking objects as eliminated without effectively removing them are not acceptable in the metric space scenario.
 We present two choices to remove elements from a t -spanner: lazy deletion and effective deletion .
Lazy deletion consists in removing the object and leaving its node and edges untouched. Those nodes that do not represent anymore an object are said to be empty . Empty nodes are treated as regular nodes at search time, except that they are not considered to be candidate nodes. As a consequence, they are never chosen as nodes.

This choice has the advantage of taking constant time and not modifying the t -spanner structure. In order to preserve the long-term quality of the graph, periodic global reconstructions should be carried out (see Sec-tion 6.3 ).
 Effective deletion consists in eliminating not only the object, but also its graph node and its incident edges.
In order to preserve the t -condition, we have to make a local t -spanner reconstruction around the deleted object by using  X  X  X emporal X  X  edges. Note that some temporal edges could be unnecessary to preserve the t -con-dition, and that for each inserted edge we need to compute one additional distance. At search time, temporal necessary to get rid of the temporal edges.

We envision two connection mechanisms to perform the local t -spanner reconstruction:  X  Clique connection : it consists in connecting all the neighbors of the deleted vertex to each other using tem-poral edges. This mechanism has the advantage of freeing the memory used by the node and its incident edges, but the disadvantage of inserting some unnecessary edges, which degrade the quality of the t -spanner.  X  Conservative connection : it consists in connecting neighbors of the deleted vertex, with temporal edges, only if the current distance t -estimation among them is greater than before the vertex deletion. This choice has the advantage of inserting only some edges of the clique, but the disadvantage of taking more CPU time per deletion.

Note that an alternative to conservative connection is local t -spanner connection , where we use temporal edges to connect neighbors of the deleted vertex whose current distance estimation is greater than the t -dis-tance allowed for them. This mechanism takes one additional distance evaluation per checked edge, even if and taking less CPU time per deletion compared to the previous mechanism. The disadvantage is that this the t -condition is locally restored among neighbors, it might be that some distant nodes get their distance not t -estimated. These distances have to be detected and patched by adding direct edges. Periodical global reviews of the t -spanner can be used both to detect these distant edges, and to remove the excess of direct edges in favor of shorter ones. Yet, the method does not guarantee that we actually have a t -spanner. 6.3. Remodeling the t-Spanner
After successive deletions (using either lazy or effective deletion) the quality of the t -spanner may be degraded. This motivates the need of a  X  X  X econstruction X  X  algorithm to be run periodically in order to maintain database.

The remodeling algorithm has two stages. First, depending on the deletion strategy, it deletes either empty nodes and their incident edges, or temporal edges. Second, it builds the t -spanner by using the recursive algo-rithm, starting with the nodes and edges that remain after the elimination. This strategy allows us to reuse previous work. 6.4. Experimental results with dynamic t-spanners
In this section we show that dynamic t -spanners are a robust technique to index a metric database upon consider the effect of periodic reconstructions. 6.4.1. Performance of effective deletion
While the performance of lazy deletion is immediate in terms of the resulting t -spanner size and deletion cost, those measures deserve experimental study for effective deletion.

We start with a dataset of n vectors in ([ 1,1] D , L 2 ) that we index with t -Spanner 4. Then we add nodes in Section 6.1 . Now we mark n max n objects at random and delete them using the conservative connection technique of Section 6.2 .
 Fig. 26 shows the results on insertion and deletion over ([ 1,1] n = 2000 vectors, and then vary n max from 2050 to 2250. Fig. 26 a shows the number of edges when we add objects to the t -spanner and then remove the same amount at random using the conservative connection tech-nique. Fig. 26 b shows the number of distance computations required by the whole process of insertions and deletions, compared to just inserting 2000 nodes.
 It can be seen that the number of edges sharply increases under effective deletion, even after a few updates.
In the best case ( D = 24, t = 1.8) the number of edges after reaching 2200 elements and then deleting 200 is more than twice that of the initial graph (before any update). If we used lazy deletion, this ratio would be below 1.11. Thus, we pay a high price in terms of edges for having removed the node. In addition, we pay shows that lazy deletion is clearly preferable over effective deletion. 6.4.2. Remodeling
We test now the performance of our t -spanner remodeling technique. We start with a dataset of n vectors in ([ 1,1] D , L 2 ), insert new nodes until reaching n max n nodes. Now, we use the remodeling technique from Section 6.3 . Note that the deletion strategy we use is irrelevant for this experiment.
Fig. 27 shows the results for D 2 [4,24]. We start by indexing n = 2000 vectors, and then vary n ing. Analogously, in Fig. 27 b we show the ratio between the reconstruction time and initial construction time. original version even after many updates take place. This shows that the combination of lazy deletion and peri-search performance are unaltered, yet nodes are not physically removed until the next remodeling takes place). 7. Conclusions and further work
We have presented a new approach to metric space searching, which is based on using a t -spanner data structure as an approximate map of the space. This permits us trading space for query time.
We started by proposing an algorithm to solve range queries over the t -spanner. It is based on simulating the successful AESA algorithm [40] with a bounded-error estimation of the distances. We show that, the more available memory, the better the performance of the search process. Note that classical pivot-based algorithms do not have this feature.

We have shown experimentally that t -spanners are competitive against existing solutions. In particular we have shown that t -spanners are especially competitive in applications of interest to Information Retrieval: strings under edit distance and documents under cosine distance. For example, in an approximate string matching scenario typical of text databases, we show that t -spanners provide better space X  X ime trade-offs than classical pivot-based solutions. Moreover, t -Spanners permit approximating AESA, which is an unbeaten index, within 1.5 times its distance evaluations using only about 3% of the space AESA requires. This becomes a feasible approximation to AESA, which in its original form cannot be implemented in practice because of its quadratic memory requirements. Furthermore, for document retrieval in a text database, we just perform 1.09 times the distance evaluations of AESA, using only 4% of its memory requirement.

To complete this approach, practical t -spanner construction algorithms are required for 1.0 &lt; t 6 2.0. To the best of our knowledge, no previous technique had been shown to work well under this scenario (complete graph, metric distances, small t , practical construction time) and no practical study had been carried out on the subject. Our algorithms not only close this gap, but they are also well suited to general graphs.
We have shown that it is possible to build good-quality t -spanners in reasonable time. We have empirically ning all the edges of the complete graph requires O( n 2 ) time. Moreover, just computing all the distances in a general graph requires O( n 3 ) time. Compared to existing algorithms, our contribution represents in practice a large improvement over the current state of the art. Note that in our case we do not provide any guarantee in the number of edges. Rather, we show that in practice we generate t -spanners with few edges with fast algo-rithms. Among the algorithms proposed, we have selected the recursive one as the most appropriate to index a metric database.

One of the most important advantages of this methodology is that the t -spanner naturally adapts, by its construction methodology, to the spatial distribution of the data set, which allows obtaining better perfor-mance both in the construction and the search phase (in particular, better than pivot-based algorithms).
We have given enough empirical evidence to support the conjecture that this good behavior holds in any clus-other real applications.

Finally, we have addressed the problem of indexing a dynamic database using t -spanners. We proposed mechanisms for object insertion and deletion, and t -spanner remodeling, that make up a robust method for maintaining the t -spanner up to date while preserving its quality. For insertions, the use of the incremental lazy deletion (where the object itself is indeed removed) and periodic remodeling of the structure.
Several lines of future work remain open, both in t -spanner construction and in its use as a search tool:  X  A first one is that, for search purposes, we do not really want the same stretch t for all the edges. Shorter edges are more important than longer ones, as Dijkstra X  X  algorithm tends to use shorter edges to build the shortest paths. Using a t value that depends on the distance to estimate may give us better space X  X ime trade-offs.  X  We can consider fully dynamic t -spanners, which means that the t -spanner allows object insertions and deletions while preserving its quality without need of periodical remodeling. This is important in real-time applications where there is no time for remodeling.  X  A weakness of our current construction algorithms is the need to have an externally computed model pre-dicting their final number of edges. We are working on versions that use the t -spanner under construction to extrapolate its final size. Preliminary experiments show that the results are as good as with external estimation.  X  Another line of work is probabilistic t -spanners, where most distances are t -estimated, so that with much fewer edges we find most of the results. relevant elements but improves the search time. The result is a probabilistic algorithm, which is a new suc-spanner, many distances are estimated better than t times the real one, so this idea seems promising. For example, a preliminary experiment in the string metric space shows that, with a 2.0-spanner and using t 0 = 1.9, we need only 53% of the original distance computations to retrieve 92% of the result. closer to the query, as the ball of candidate elements has much smaller volume. We can use the t -spanner edges to start at a random node and approach the query by neighbors.

References
