 Shared knowledge structures across multiple domains play an essential role in assisting users to transfer understanding between applications and to perform analogy based reasoning and creative thinking [3,6,9,8,10], in supporting users to perform research by analogy [4], and in assessing similarities between datasets in order to avoid negative learning transfer [16]. Motivated by the above, Dong and Han [5] studied the problem of mining knowledge structures shared by two datasets, with a focus on mining a single shared decision tree. However, pro-viding only one shared decision tree may present only a limited view of shared knowledge structures that exist across multiple domains and does not offer users a concise representative of the space of possible shared knowledge structures. Moreover, computing all possible shared decision trees is infeasible. The purpose of this paper is to overcome the above limitations by studying the problem of mining diversified sets of shared decision trees across two application domains.
In a diversified set of shared decisi on trees, each individual tree is a high quality shared decision tree in the sen se that (a) the tree has high accuracy in classifying data for each dataset and the tree has high cross-domain class-distribution similarity at all tree nodes, and (b) different trees are structurally highly different from each other in the sense that they use very different sets of attributes. The requirements in (a ) will ensure that ea ch shared decision tree captures high quality shared knowl edge structure between the two given datasets, providing the benefit that each root-to-leaf path in the tree corresponds to a similar rule (having similar support and confidence) for the two datasets and the benefit that the tree nodes describe similar data populations in the two datasets connected by similar multi-node population relationships.

Presenting too many shared decision trees to human users will imply that users will need to spend a lot of time to understand those trees in order to select the ones most appropriate for their application. Efficient algorithms solving the problem of mining di-versified sets of shared decision trees meet-ing the requirements in (b) can offer a small representative set of high quality shared de-cision trees that can be understood without spending a lot of time, hence allowing users to more effectively select the tree most ap-propriate for their situation. Figure 1 illustrates the points given above, with the six stars as the diversified represen tatives of all shared decision trees.
The main contributions of this paper include the following: (1) The paper motivates and formulates the diversifie d shared decision tree set problem. (2) It presents two effective algorithms to con struct diversified high quality shared decision tree sets. (3) It reports an extensive experimental evaluation on the diversified shared decision tree set min ing algorithms. (4) The shared decision trees reported in the experiments are min ed from high dimensional microarray datasets for cancers, which can be useful to medical researchers.

The rest of the paper is organized as follows. Section 2 summarizes related work. Section 3 formally introduces the d iversified shared decision tree set prob-lem and associated concepts. Sections 4 presents our algorithms for mining di-versified shared decision tree sets. Sectio n 5 reports our experimental evaluation. Section 6 summarizes the results and discu sses several future research problems. Limited by space, we focus on previous studies in four highly related areas. Importance of Similarity/Analogy from Psychology and Cognitive Science: Psychology/cognitive science studies indicate that analogy plays a vital role in human thinking and reasoning, including creative thinking . For example, Fauconnier [6] states that  X  Our conceptual networks are intricately structured by analogical and metaphorical mappings, which play a key role in the synchronic construction of meaning ...  X . Gentner and Colhoun [9] state that  X  Much of hu-mankind X  X  remarkable mental aptitude can be attributed to analogical ability . X  Gentner and Markman [10] suggest  X  X hat both similarity and analogy involve a process of structural alignment and mapping. X  Christie and Gentner [3] suggest, based on psychological experiments, tha t  X  X tructural alig nment processes are crucial in developing new re lational abstractions X  and forming new hypothesis . Learning Transfer: In learning transfer [16], it is typical to use available struc-ture/knowledge of an auxiliary application domain to help build better classi-fiers/clusters for a target domain where there is a lack of data with class label or other domain knowledge. The constructed classifiers are not intended to cap-ture cross-domain similarities. In contrast, our work focuses on mining (shared) knowledge structures to capture cross domain similarity; this is a key difference between learning transfer and our work. One of the intended uses of our mining results is for direct human consumption. Our mining results can also be used to assess cross domain similarity, to help avoid negative transfer where learning transfer actually leads to poorer results, since learning transfer is a process that is based on utilizing cross domain similarities that exist.
 Shared Knowledge Structure Mining: Reference [4] defined the cross do-main similarity mining (CDSM) problem, and motivated CDSM with several potential applications. CDSM has big potential in (1) supporting understand-ing transfer and (2) supporting research by analogy, since similarity is vital to understanding/meaning and to identifying analogy, and since analogy is a fun-damental approach frequently used in hypothesis generation and in research. CDSM also has big potential in (3) advancing learning transfer since cross do-main similarities can shed light on how to best adapt classifiers/clusterings across given domains and how to avoid negative transfer. CDSM can also be useful for (4) solving the schema/ontology matching problem. Reference [5] motivated and studied the shared decision tree mining problem, but that paper focused on min-ing just one shared decision tree. Severa l concepts of this paper were borrowed from [5], including shared accuracy, data distribution similarity, weight vector pool (for two factors), and information gain for two datasets. We enhance that paper by considering mining diversified sets of shared decision trees. Ensemble Diversity: Much has been done on using ensemble diversity among member classifiers [14] to improve ensemble accuracy, including data based di-versity approaches such as Bagging [2] and Boosting [7]. However, most previ-ous studies in this area focused on classification behavior diversity, in the sense that different classifiers make highly different classification predictions. Several studies used attribute usage diversity to optimize ensemble accuracy, in a less systematic manner, including the random subspace method [13] and the distinct tree root method [15]. Our work focuses on attribute usage diversity aimed at providing diversified set of shared knowledge structures between datasets. The concept of attribute usage diversity was previously used in [12], to improve clas-sifier ensemble diversity and classification accuracy for just one dataset. In this section, we introduce the problem of mining diversified sets of shared decision trees. To that end, we also need to define a quality measure on diversified sets of shared decision trees, which is based on the quality of individual shared decision trees and on the diversity mea sure for sets of shared decision trees.
As mentioned earlier, a shared decision tree (SDT) for a given dataset pair ( D 1 : D 2 ) is a decision tree that can classify both data in D 1 and data in D 2 .
We assume that D 1 and D 2 are datasets with (1) an identical set of class names and (2) an identical set 1 of attributes. Our aim is to mine a small diversified set of high quality SDTs with these properti es: (a) each tree in the set (1) is highly accurate in each D i and (2) has highly similar data distribution in D 1 and D 2 , and (b) different trees in the set are h ighly different from each other. 3.1 Shared Accuracy and Data Distribution Similarity of SDT Set The shared accuracy and data distribution similarity measures for shared deci-sion tree (SDT) sets are based on similar measures defined for individual SDTs [5], which we will review below. Let T be an SDT for a dataset pair ( D 1 : D 2 ), and let Acc D i ( T )denote T  X  X  accuracy 2 on D i .
 Definition 1. The shared accuracy of T (denoted by SA ( T )) is defined as the
The data distribution similarity of T reflects population-structure (or class distribution) similarity between the two datasets across the nodes of T .The class distribution vector of D i at a tree node V is defined by is the subset of D i for V (satisfying the conditions on the root-to-V path). The Definition 2. The data distribution similarity of an SDT T over ( D 1 : D 2 )is defined as DS ( T )= avg V DSN ( V ), where V ranges over nodes of T . We can now define SA and DS for shared decision tree sets.
 Definition 3. Let TS be a set of SDTs over dataset pair ( D 1 : D 2 ). The shared classification accuracy of TS is defined as SA ( TS )= avg T  X  TS SA ( T ), and the data distribution similarity of TS is defined as DS ( TS )= avg T  X  TS DS ( T ). 3.2 Diversity of SDT Set To define the diversity of SDT sets , we need to define tree-pair difference, and we need a way to combine the tree-pair differences for all possible SDT pairs. 3
We measure the difference between two SDTs in terms of their attribute us-age summary (AUS). Let A 1 ,A 2 ,...,A n be a fixed list enumerating all shared attributes of D 1 and D 2 .
 Definition 4. The level-normalized-count AUS ( AUS LNC )ofanSDT T over ( D 1 : D 2 ) is defined as where Cnt T ( A i ) denotes the number of occurrences of attribute A i in T ,and avgLvl T ( A i ) denotes the average level of A i s occurrences in T . The root is at level 1, the children of the root are at level 2, and so on. In the AUS LNC measure, nodes near the root have high impact since those nodes have small level number and attributes used at those nodes often have small avgLvl T .
One can also use the level-listed count ( AUS LLC ) approach for AUS .Herewe use a matrix in which each row represents the attribute usage in one tree level: Given an SDT T with L levels, for all attributes A i and integers l satisfying 1  X  l  X  L ,the( l,i ) component of AUS LLC has the value Cnt frequency count for attribute A i in the l th level of T ).
 Remark: AUS LNC pays more attention to nodes near the root, while AUS LLC gives more emphasis to levels near the leaves (there are many nodes at those levels). Definition 5. Given an attribute usage summary measure AUS  X  ,the tree pair difference ( TPD )fortwoSDTs T 1 and T 2 is defined as We can now define the SDT set diversity concept.
 Definition 6. Given an SDT set TS and an AUS measure AUS  X  ,the diversity of TS is defined as TD  X  ( TS )= avg { TPD  X  ( T i ,T j ) | T i ,T j  X  TS,andi = j } . 3.3 Diversified Shared Decision Tree Set Mining Problem To mine desirable diversified SDT sets, we need an objective function. This section defines our objective function, which combines the quality of the SDTs and the diversity among the SDTs.
 Definition 7. Given an attribute usage summary method AUS  X  , the quality score of an SDT set TS is defined as:
We also considered other definitions using e.g. average, weighted average, and the harmonic mean of the three factors. T hey were not selected, since they give smaller separation of quality scores or require parameters from users. The above formula is chosen since it allows each of SA , DS ,and TD to play a role, it is simple to compute, and it does not require any parameters from users. We now turn to defining the diversified SDT set mining problem.
 Definition 8 (Diversified Shared Decision Tree Set Mining Problem). Given a dataset pair ( D 1 : D 2 ) and a positive integer k ,the diversified shared decision tree set mining problem ( KSDT ) is to mine a diversified set of k SDTs with high SDTSQ from the dataset pair.

An example SDT set mined from two cancer datasets will be given in  X  5. This section presents two KSDT mining algorithms, for mining diversified high quality SDT sets. One is the parallel KSDT-Miner ( PKSDT-Miner ), which builds a tree set concurrently and splits one-node-per-tree in a round-robin fashion; the other one is the sequential KSDT-Miner ( SKSDT-Miner ), which mines a tree set by building one complete tree after another.

In comparison, PKSDT-Miner gives all SDTs almost equal opportunity 4 in selecting desirable attributes for use in high impact nodes near the roots of SDTs, whereas SKSDT-Miner gives SDTs built earlier more possibilities in selecting desirable attributes (even for use at low impact nodes near the leaves), which deprives the chance of later SDTs in using those attributes at high impact nodes.
Limited by space and due to the similarity in most ideas except the node split order, we present PKSDT-Miner and omit the details of SKSDT-Miner . 4.1 Overview of PKSDT-Miner PKSDT-Miner builds a set of SDTs in parallel, in a node-based round-robin manner. In each of the round-robin loop, the trees are processed in an ordered manner; for each tree, one node is selected and spilt. Figure 2 illustrates with two consecutive states in such a loop: 2(a) gives three (partial) trees (blank rectangles are nodes to be split), and 2(b) gives those trees after splitting node V 2 of T 2 . Here, PKSDT-Miner splits node V 2 in T 2 even though V 1 in T 1 can be split. PKSDT-Miner will select a node in T 3 to split next. 4.2 Aggregate Tree Difference To build highly diversified tree sets, the aggregate tree difference ( AT D )isused to measure the differences between a new/modified tree T and the set of other trees TS . One promising approach is to define AT D as the average of  X  smallest TPD  X  ( T,T )values( T  X  TS ). We define a new aggregation function called avgmin  X  ,where avgmin  X  ( S ) is the average of the  X  smallest values in a set S of numbers. (In experiments our best choice for  X  was 3.) Then the  X  - X  -minimal aggregate tree difference ( AT D  X ,avgmin X  ) is defined as the average TPD  X  between T and the  X  most similar trees in TS :
PKSDT-Miner selects an attribute and a value to split a tree node by maximiz-ing an objective function IDT that combines information gain (to be denoted by IG and defined below) and data distribution similarity ( DS ) on two datasets, and aggregate tree difference ( AT D ) between the current tree and the other trees. To tradeoff the three factors, they are combined using a weighted sum based on a weight vector w =( w IG ,w DS ,w AT D ) whose three weights are required to satisfy
Given an AUS measure  X  , an aggregation method  X   X  X  avg, min, avgmin  X  } , atree T and a tree set TS ,the  X  - X  aggregate tree difference is defined as For example, AT D  X ,min ( T,TS )=min T  X  TS ( TPD  X  ( T,T )) when  X  is min .Each variant of AT D can be used in our two SDT set mining algorithms, resulting a number of variant algorithms. For instance, the standard version of the PKSDT-Miner algorithm can be written as PKSDT-Miner ( LNC , avgmin X  )andwecan replace LNC by LLC to get PKSDT-Miner ( LLC , avgmin X  ). 4.3 IG for Two Datasets This paper uses the union-based definition of IG for two datasets of [5]. ([5] discussed other choices and the union-based way was shown to be the best by experiments.) For each attribute A and split value a , and dataset pair ( D 1 : D 2 ) (associated with a given tree node), the union-based information gain is defined the LHS is 4-ary while IG in the RHS is 3-ary.) IG ( A, a, D )isdefinedinterms of entropy, as used for decision tree node splitting.
 4.4 The Algorithm PKSDT-Miner has six inputs: Two datasets D 1 and D 2 ,aset AttrSet of candi-date attributes that can be used in shared trees, a dataset size threshold MinSize for node-splitting termination, a weight vector w ( w IG on information gain, w DS on data distribution similarity, w AT D on aggregate tree difference), and an in-teger k for the desired number of trees. PKSDT-Miner calls PKSDT-SplitNode (Function 1) to split nodes for each tree.

PKSDT-SplitNode splits the data of a node V of a tree T by picking the split attribute and split value that optimize the IDT score. Let T be the tree that we wish to split, and let TS be the other trees that we have built. Let V be T  X  X  current node to split, and A and a V be resp. a candidate splitting attribute/value. Let T ( A, a V ) be the tree obtained by splitting V using A and a V . Then the IDT scoring function is defined by: where IG ( A, a V ) is the information gain for V when split by A and a V , DSN ( A, a V ) is the average DSN value of the two children nodes of V .
Function ShouldTerminate determines if nodes splitting should terminate. (Our algorithms aim to build simple trees and avoid  X  X verfitting X .) It uses two techniques. (1) When many attribtues are available, we restrict the candidate attributes to those whose IG is ranked high in both datasets, so avoiding non-discriminative attributes that are locally discriminative at a given node. (2) We stop splitting for a given tree node when at least one dataset is small or pure. 4.5 Weight Vector Pools Different dataset pairs have different characteristics concerning IG , DS and AT D . To mine the best SDT set, we need to treat d ifferent character istics using appro-priate focus/bias. (It is open if one can determine the characteristics of a dataset pair without performing SDT set mining.) We solve the problem by using a pool of weight vectors to help mine (near) o ptimal SDTs efficiently. Such a pool is a small representative set of all possible weight vectors.

We consider two possible weight vector pools: WVP 1 contains 36 weight vectors, defined by WVP 1 = { x | x is a multiple of 0 . 1and0 &lt;x&lt; 1 } . WVP 2 = { (0.1,0.1,0.8),(0.1,0.3,0.6),(0.1,0.5,0.4),(0.1,0.7,0.2),(0.3,0.1,0.6), (0.3, 0.4, 0.3),(0.3,0.5,0.2),(0.5,0.1,0.4),(0.5,0.3,0.2),(0.7,0.2,0.1) } .So WVP 2 contains 10 representative vectors selected from WVP 1 . For each of the three factors, each pool contains some vectors where the given factor plays the dominant role. This section uses experiments to evaluate KSDT-Miner , using real-world and also (pseudo) synthetic datasets. It reports that (1) PKSDT-Miner tends to build more diversified high quality SDT sets on average, which confirms the advantages of PKSDT-Miner analyzed in Section 4, and (2) KSDT-Miner is scalable w.r.t. number of tuples/attributes /trees. It discusses (3) how KSDT-Miner performs concerning the use of weight v ectors. It also examines (4) how KSDT-Miner performs when it uses different AUS  X  and AT D  X , X  measures. Finally, it reports that KSDT-Miner outperforms SDT-Miner regarding mining one single SDT. In the experiments, we set  X  =3, MinSize =0 . 02  X  min ( | D 1 | , | D 2 | )and AttrSet = { A | rank 1 ( A )+ rank 2 ( A ) is among the smallest 20% of all shared decreasing IG order } . Experiments were conducted on a 2.20 GHz AMD Athlon with 2 GB memory running Windows XP, with codes implemented in Matlab. To save space, the tables may list results on subsets of the 15 dataset pairs on the 6 microarray datasets, although the listed averages are for all 15 pairs. 5.1 Datasets and Their Preprocessing Our experiments used six real-world microarray gene expression datasets for cancers. 5 ArrayTrack [20] was used to identify shared (equivalent) at-tributes. Two genes are shared if they represent the same gene in different gene name systems. Table 1 lists the number of shared attributes for the 15 dataset pairs.

In some dataset pairs the two datasets have very different class ra-tios. The class ratio of a dataset is likely an artifact of the data collectio n process and may not have practical im-plications. However, class ratio difference can make it hard to compare quality values for results mined from different dataset pairs. To address this, we use sampling with replacement method to rep licate tuples so that class ratios for the two datasets are nearly the same. 5.2 Example Diversified SDT Set Mined from (DH:LM) We now give 6 an example diversified set of two shared decision trees mined from real (cancer) dataset pair (DH:LM) in Figures 3 and 4. For each tree, data in two datasets have very similar distributions at tree nodes (the average DSN for each tree is about 0 . 977) and the leaf nodes are very pure with average shared classification accuracy of leaf nodes being about 0 . 963. For the diversified tree attributes, and the SDTSQ is about 0 . 944. 5.3 KSDT -Miners Mine Diversified High Quality SDT Sets Experiments show that KSDT -Miners are able to mine diversified high quality SDT sets. Table 2 lists the statistics of best SDT sets mined by either PKSDT-Miner or SKSDT-Miner from each of the 15 dataset pairs in Table 1 (using all weight vectors). For the 15 dataset pairs, PKSDT-Miner got the best SDT sets in 9 pairs, and SKSDT-Miner got the best in 6 pairs. We include the result for (BC: LM) to indicate that it is not possible to always have high quality SDTs (as expected). 5.4 Comparison between PKSDT-Miner and SKSDT-Miner Experiments show that PKSDT-Miner is better than SKSDT-Miner . Indeed, PKSDT-Miner gets SDT sets of higher quality values on average, albeit slightly, and it never gets SDT sets of lower quality values (see Table 3, which gives the average TD and SDTSQ values for best diversified tree sets mined by PKSDT-Miner and SKSDT-Miner respectively when using all w eight vectors). As noted for Table 2, PKSDT-Miner got the best SDT sets in 9, whereas SKSDT-Miner got the best in only 6, out of the 15 dataset pairs. Below we only consider PKSDT-Miner . 5.5 Comparison of AUS and ATD Variants Experimental results demonstrate that (1) AUS LNC produces better results than AUS LLC ,and(2) AT D  X ,avgmin X  outperforms AT D  X ,min (reason: when it is used a highly similar outlier may give too much influence) and AT D  X ,avg (reason: when it is used the highly dissimilar cases may give big influence). The details are omitted to save space. 5.6 Weight Vector Issues We examined the  X  X est X  and  X  X orst X  ( w IG , w DS , w AT D ) weight vectors, which produce the SDT sets with the highest and lowest SDTSQ mined by PKSDT-Miner ( LNC , avgmin X  ). (1) We observed that the average relative improvement of the  X  X est X  over the  X  X orst X  is an impressive 4.8% and the largest is 20.5%. This indicates that the choice of weight vector has significant impact on the tree set quality mined by KSDT-Miner . (2) We also saw that no single weight vector is the best weight vector for all dataset pai rs. This reflects the fact that different dataset pairs have different characteristics regarding which of IG , DS and ATD is most important.

Regarding which weight vectors may be better suited for which kinds of dataset pairs, we observed that there are three cases. (A) For some dataset pairs (e.g. (LM:PC)), weight vectors with high IG weight (and low DS weight, low ATD weight) tend to yield SDT sets with high SDTSQ . (B) For some dataset pairs (e.g. (BC:DH)), weight vectors with high DS weight tend to yield SDT sets with high SDTSQ . (C) For some dataset pairs (e.g. (BC:CN)), weight vectors with high ATD weight tend to yield SDT sets with high SDTSQ .

Experiment showed that using multiple weight vectors leads to much better performance than using a single weight vector. Moreover, SDTSQ scores of best SDT sets obtained using WVP 2 are almost identical to those obtained using WVP 1 .Since WVP 2 is smaller (having 10 weight vectors) than WVP 1 (having 36 weight vectors), WVP 2 is preferred since it requires less computation time. 5.7 KSDT-Miner Outperforms SDT-Miner on SDTQ Both KSDT-Miner and SDT-Miner can be used to mine a single high quality SDT, by having KSDT-Miner return the best tree in the SDT set it constructs. Experiments show that KSDT-Miner gives better performance than SDT-Miner . Indeed, the average relative SDTQ improvement by KSDT-Miner over SDT-Miner for all dataset pairs is 13.8%. For some dataset pairs, the relative improvement is about 45.3%. Through more detailed comparison, the average relative improve-ment on DS by KSDT-Miner over SDT-Miner for all dataset pairs is 3.2%, and on SA is 5.4%. Clearly, better single SDT can be mined when tree set diversity is considered. 5.8 Scalability of KSDT-Miner We experimented to see how KSDT-Miner  X  X  execution time changes when the number of tuples/attributes/trees incr eases. Experiments show that execution time increases roughly linearly. (The fig ure is omitted to save space.) The experi-ments used synthetic datasets obtained by replicating tuples with added random noises up to a bound given by P % of the maximum attribute value magnitude (in order to get a desired number N of tuples), and by attribute elimination. 5.9 Using Fewer Attributes Leads to Poor SDT Sets Incidently, we compared the SDTSQ of SDT sets mined from real dataset pairs using all available attributes against those obtained from projected data using fewer attributes (e.g. 100). On average, SDTSQ using all attributes is about 34.2% better than SDTSQ using only the first 100 attributes. In this paper we motivated the diversified shared decision tree set mining prob-lem, presented two algorithms of KSDT-Miner , and evaluated the algorithms us-ing real microarray gene expression data fo r cancers and using synthetic datasets. Experimental results show that KSDT-Miner can efficiently mine high quality shared decision trees. Future research d irections include mining other types of shared knowledge structures (including those capturing alignable differences, de-fined as shared knowledge structures that capture cross-domain similarities and cross-domain differences within the context of the similarities given elsewhere in the shared knowledge structures) and utilizing such mined results to solve various research and development problems in challenging domains.

