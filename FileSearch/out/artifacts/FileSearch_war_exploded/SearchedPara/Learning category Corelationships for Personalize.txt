 Identifying and understanding the relationships between commodities is a key compo-nent of many Recommending Systems(RSs). Learning which products are similar or complementary for each other, is the key to the recommendation system. Therefore, the study of heterogeneous relationships between commodities which understand the user X  X  context and recommend alternative items from the same style or generate com-patible merchandise bundles, has a very important role in the field of RSs, especially coe ffi cient relationships between products researches. In RS, products of the similar categories will have a great probability to be recommended for users, and the conse-quences of same category commodities recommending are always recommending to the user with popular merchandise or highly similar products, which causes recommen-dation very simple and ine ff ective. It is essential to study the relationships between the commodities sharing partially similar categories or the complementary among products across-categories.
 overlap between users who have clicked on / bought related items. Of more interest to us are systems that predict item-to-item relationships based on the content (e.g. images / text / metadata) of the items themselves. Various systems have been proposed to address specific settings, e.g. to identify relationships between members of urban tribes [1], tweets [2], text [3, 4], or music [5]. However, identifying the similarity may be insu ffi cient when there is substantial heterogeneity between the items being consid-ered.
 Similarly, user preferences also present hierarchical structures. While category infor-mation connecting the gap between the users and the items is ignored, categories are not only the classification of products, but also the labels of user interests. Users will pick out relevant classifications according to their own requirements and preference to narrow the selection range. The complementary relationships between products from the di ff erent categories highlighted by the users X  buying behaviors uncover the relation-ships between categories that they should be systematically complementary in some ways, but systematically irrelevant in others, eg. if there is a Polo-shirt bought by a lot of people, someones may want to find a pair of shoes in a type and someones want a other category pair of shoes to match this dress.
 the category hierarchy, and each item has a category set representing item X  X  categories in the category hierarchy. In addition, the user interests are in the form of hierarchi-cal structure. When a user selects a new item, he / she picks another product in similar categories or complementary products in a propinquity category based on his / her pref-erence. Thus we can predict rates or assessing of the products according to the user X  X  interest categories and the product categories, not just from point of view that the us-er X  X  preferences feature vector and the characteristics of the product vector make point products for the product.
 ed products, which are presented relationships between categories. The relationships of the heterogeneous merchandise in the users X  purchase records can help us find comple-mentary between the categories, and extract personal preferences for complementary relationship of categories .
 Products (named CCHP) based on BPR [6] to explore the complementary relationships of the categories and similarity between products to recommend cross-category prod-ucts to users. In conversional RSs, recommending commodities sharing the same or close categories with the queries is emphasis, but the complementary relationships be-tween cross-category products is disregarded and lost. Here we are interested in infer-ring relationships between products across to categories. We focus on making a inquiry relationships between categories: which categories will be complementary? And what kind of complementary relationship does the user prefer? Each user has their own fa-vorite categories and users will choose and match the appropriate category factors in accordance with his her favorite categories and needs. As its core, we consider users X  requirement of products their looking for and users X  tastes of complementary according with their records in the framework.Our main contributions are outlined as follows:  X  We propose a framework named CCHP to explore the relationships of products  X  We demonstrate that CCHP is e ff ective at learning notions of relatedness of com- X  We show that CCHP can e ff ectively learn multiple complex notions of complemen-the problems and notation in Section 3. The proposed framework is formulated in Sec-tion 4. The experiments are presented in Section 5. Section 6 is the conclusion. Category Hierarchy. To address prediction accuracy issues related to cold-start for new items and the problem of sparsity, taxonomy-based models were proposed. The main idea is to utilize the categorical information in a human-induced taxonomy to share s-tatistical strength between frequently purchased items and tail items. However, items in real-world recommender systems exhibit certain hierarchical structures. There has been some e ff ort to investigate taxonomy-aware recommendation models, including earlier works extending neighborhood-based methods [7, 8]and more recent endeavors to ex-tend MF using either explicit [9] or implicit [10, 11] taxonomies. This work is related to ours, though does not consider the intermediary for category labels, which supply hierarchical structure information for items and users X  preference simultaneously for RSs.
 information of potential interest based on their demographic profiles and historical data. Collaborative Filtering (CF), which only requires past user ratings to predict unknown ratings, has attracted more and more attention [13, 14]. Collaborative Filtering can be roughly categorized into memory-based [15, 16] and model-based methods [13, 17]. To extend their expressive power, various works have made use of features such as temporal dynamics [18], social influence [19], or the content of the items themselves [20]. Items in real-world recommender systems exhibit certain hierarchical structures. Sim-ilarly, user preferences also present hierarchical structures. When facing tens of mil-lions of items at Amazon, the user X  X  first conception happened in brain is to seek the preference categories of items instead of items immediately, which is ignored by RSs. Categories bind together the hierarchical structure of items and hierarchical structures of users X  preferences. Moreover, cross-category merchandise will have complementary relationships among the category labels, which can help us to enhance user experience and satisfaction. information and complementary relationships of cross-category merchandise. Letting U , I and C denote the set of users, items and categories respectively, each user u is associated with an item set I + u about which u has expressed explicit positive feedback. category hierarchy in Amazon.cn. In addition, a single image is available for each item i. Using only the above data, our objective is to generate for each user u a personalized ranking of those items about which they havent yet provided feedback (i.e. I \ I + u ). The notation we use throughout this paper is summarized in Table 1.
 4.1 preference predictor Our preference predictor is built BPR, which is state-of-the-art for rating prediction as well as modeling implicit feedback, whose basic formulation assumes the following model to predict the preference of a user u toward an item i [6]: vectors describing latent factors of user u and item i (respectively). The inner product r r i then encodes the compatibility between the user u and the item i, i.e., the extent to which the users latent  X  X references X  are aligned with the products  X  X roperties X . one major problem it su ff ers from is the existence of  X  X old X  items in the system, about which there are too few associated observations to estimate their latent dimensions. Using explicit features can alleviate this problem by providing an auxiliary signal in such situations. In particular, we propose to partition rating dimensions into category factors and latent (non-category) factors, as shown in Fig.1. Our extended predictor takes the form: where  X , X  and r are as in Eq. 1.  X  u and  X  i are newly introduced D-dimensional category factors whose inner product models the category interaction between u and i seen Fig.1, i.e., the extent to which the user u is attracted to each of D category dimensions. Note that we still use K to represent the number of latent dimensions of our model. Items in real-world recommender systems exhibit certain hierarchical structures. Similarly, user preferences also present hierarchical structures. Our experiment will show that incorpo-rating the explicit hierarchical structures of items or user preferences can improve the performance of recommender systems in the experimental section. The Categories asso-ciating the importance hierarchical structures motivate us to study category hierarchical of users and items for recommendation. 4.2 Modeling Category factors The key to the above predictor is to model the category dimensions of users opinions, which fill the gap between items X  hierarchical structures and users X  hierarchical struc-tures of preferences to recommend users cross-category heterogeneous products. How can we pick up the hierarchical structures of items or user preferences? As we all know, the merchandises are sorted by category on the e-commerce websites, in which each product is represented by a path, or more simply a set of nodes in the category struc-ture.
 Fig. 1: Diagram of our preference pre-dictor. Rating dimensions consist of la-tent factors and category factors. in Fig.3(category hierarchy structure of Books has 11 second-level categories and and 167 following levels categories.), and commodities with category information, shown in Fig.1. For products belonging to multiple categories, we will take the union of those paths. Hierarchical structures of users X  preferences are reflected from their purchase records or browsing history and the information of the categories of purchased or viewed products mirrors users X  hierarchical structures of preferences. Consequently, products and users share the same category structure and the same category dimension-s.
 structures is represented by the vector c p  X  R D , each item i is associated with a category set C + i and each user u has a a category set C + u collected from u  X  X  purchase records or browsing history. The category a ffi nity between u and i is then predicted by computing the inner product of the two concatenated category feature vectors  X  u and  X  i . And cate-gory feature vectors  X  u and  X  i are got based category features c p  X  R D , item i  X  X  category set C + i and user u X  X  category set C + u : repeated high frequency, but the low-level categories can be more representative of the fine-grained information. And the categories in heterogeneous structures are equally important, regardless of the common categories in hight-level or the fine-grained cate-gories in low-level, in Eq.3.
 we associate a weight vector  X  i  X  R P  X  1 with each item. Each element in the vector stands for a category in structure and the category. And categories of item i in higher layer have the low weight and in the lower layer of the heterogeneous structures have the greater weight in the vector  X  i , and the remaining elements equal to zero. Consequently, item i X  X  category set C + i is equational to  X  i that corresponding elements are all equal to 1. So the category feature vectors  X  i and  X  u is: 4.3 Category Complementary Relationships People have their unique preferences and tastes for di ff erent products and also have particular preferences for di ff erent complementary relationships of products. The task we are interested in is: if complementary relationships exist between product i and items concept of complement preferences of users. ), we use user X  X  complementary preference vector  X  u to make prediction about category complementary relationships of item i and user X  X  purchase history I + u , inner products between users and item factors model the compatibility between users and items.
 we predict a positive value of user toward complementary relationships between i and factors associated with each product to be useful for prediction in the sense that we are That is, we want the logistic function: where  X  ( i , u ) is a pairwise feature vector describing the product i and user u encode the complementary between the product and positive items of user u. The vector  X  u then determines which category complementary relationships should the user like best.In summary, our final prediction model is : 4.4 Model Learning Using BPR Bayesian Personalized Ranking (BPR) is a pairwise ranking optimization framework which adopts stochastic gradient ascent as the training procedure. A training set D s consists of triples of the form ( u , i , j ), where u denotes the user together with an item i about which they expressed positive feedback, and a non-observed item j . The follow-ing optimization criterion is used for personalized ranking:
L(C | r u , r i , X  u , c p ) = min X in the following: relationships between item i and the rest items in user u  X  X  purchase history. 5.1 Datasets periment on the largest datasets available. In this section, we adopt the datasets from Amazon recently. We focus on four large category tree rooted with  X  X ovies X ,  X  X lectron-ices X ,  X  X ooks X  and  X  X omen X  X  Clothing X . Statistics are shown in Tab.2. For each dataset, there is a category tree associated with each of datasets. Fig.3 demonstrates part of the hierarchy associated with Books. On this hierarchy, we have 11 second-level categories (E-Book, Teaching materials, Humanities and Social Sciences, Technology , etc.), and 167 following levels categories (e.g. Literature, Art, Humanities, Economics,etc. under the second-level category).
 cross-category recommendation. Across the entire dataset, such relationships are noisy, sparse, and not always meaningful. To address issues of noise and sparsity to some extent, its sensible to focus on the relationships within the scope of a particular hight-level category. In our experiments, to obtain ground-truth for pairs of complementary products we focus on two types from Amazon: users who bought x also bought y and users frequently bought x and y together.
 Mens Clothing) and a relationship type (also-bought). For each experiment, the rela-tionships R and a random sample of non-relationships  X  R are pairs of items connecting di ff erent subcategories of the category we are experimenting on. Note that | R | =  X  R and they share the same distribution over the items. We split our training data ( R and  X  R) into 80% training, 10%validation, 10% test, treating users with fewer than 5 positive feedbacks as the cold start. In all cases we report the error on the test set. The iterative fitting process described in Equ.7 continues until no further improvement is gained on the validation set.
 selecting for each user u a random item to be used for validation  X  u and another for testing  X  u . All remaining data is used for training. The predicted ranking is evaluated on  X  u with the widely used metric AUC (Area Under the ROC curve): that returns 1 i ff b is true.
 l(@k). Given a set of recommended items of a given user rec , and a set of known-relevant products rel (ground-truth brought) the precision is defined as: where the fraction of recommended items that were relevant. The precision@k is then the precision obtained given a fixed budget, i.e., when rec = k. This is relevant when on-ly a small number of recommendations can be surfaced to the user, where it is important that relevant products appear amongst the first few suggestions. 5.2 Baselines  X  BPR-MF: Introduced by [21], this baseline is the state-of-the-art method for per- X  Category Tree (CT): This method computes a matrix of co-occurrences between  X  Bayesian Personalized Ranking with Category Tree(BPR-CT): BPR-C makes use  X  Item-to-Item Collaborative Filtering (CF): In 2003 Amazon [23] reported that their of-the-art no category relationships method on our datasets( BPR-MF); (2) using cat-egory metadata directly (CT) results in relatively poor performance; (3) using catego-ry metadata directly and not using the complementary relationships between products (BPR-CT) results in relatively poor performance;and that (4) our proposed model is an improvement over the state-of-the-art method on our task (CF). 5.3 Performance &amp; Ranking Table 3: Test errors (1  X  AUC ) of the prediction task on all items or cold start set using category features for each cross-category heterogeneous products on datasets of the Amazon. The best performing method in each case is boldfaced. Lower is better. in Table.3 and results in terms of the average AUC on di ff erent datasets are shown in Fig.4. For experiments on also bought relationships, CCHP uses K = 50 dimensions and D = 30 dimensions, and BPR, CF and BPR-CT also use D = 30. We make a few observations to explain and understand our findings as follows: 1) CT vs BPR-MF vs CCHP: CT make the worse than other methods whether for all 2) BPR-CT vs CF : BPR-CT makes the better accurate than BPR and CT for our 3) CCHP: CCHP, the framework we propose, consistently outperforms baseline meth-each experiment, we naturally discard candidate products that appeared during training and leave only a small number of relevant products for each query item in the training. Note that CT has precision around 5  X  10 4 , indicating that only around 5 in 10000 prod-ucts are labeled as relevant in this experiment, in addition to the fact that many relevant items may not be labeled as such highlights the incredible di ffi culty of obtaining high precision scores for this task that means there are presumably thousands of pairs of substitutable pants in our training, but only 50 or so are recommended for each product. pressed for brevity), CF is one to two orders-of-magnitude more accurate than BPR-CT for all items but is the opposite in terms of cold start. while CCHP is an order of mag-nitude more accurate again than CF in all items or BPR-CT in cold start. Fig. 4: AUC with training iterations (# K = 50 and D = 30). For each dataset, we report the average AUC on the full test set  X  . Each iteration of the stochastic gradient descen-t procedure consists of sampling training triples ( u , i , j ) with the size of the training corpus.
 have the highest likelihood under the model. While conceptually simple, comparing al-l products against all others quickly becomes impractical in datasets with millions of products and there is a part of items will be noisy in training, a ff ecting the ranking per-formance. Finding the most valuable ranking is essential for us in the case. we usually make this enumeration procedure feasible by ignoring obscure products by limiting the search space by some popularity threshold and considering the hundred-thousand most popular products per-category when generating new recommendations.
 gories. It is another more appreciative strategy is to cull the search space by searching the category related; eg. when computing the score for a single candidate on a dataset with millions of products, we consider items from the relevant categories when ranking all possible recommendations. During the experiments, we will combine ignoring ob-scure products and searching the category related to cull the search space. Seen Fig.5( by searching the category related to cull the search space), we demonstrate the relat-ed dimensions used to cull the search space learned by CCHP are e ffi cacious to this procedure, i.e., which kind of characteristics the model is capturing to explain the rela-tionships among categories.
 whether the substitutes in ranking for the query are complementary to user X  X  preference and needing and demonstrate the category dimensions captured by CCHP to explain users X  preferences of complementary. A simple way to exhibit these dimensions ex-tracted by Eq.5 is to rank items highly that show maximal values for each user, seen Fig.7. We make the following observation: By using the users X  preferences for related categories we are able to learn a transition across di ff erent subcategories and CCHP not only learns the hidden taxonomy, but also discovers the most relevant underlying category dimensions and maps items and users into the uncovered space. Fig. 5: parallels of CCHP trained on Womens Clothes and Books. In this paper, we advocate a framework based on Bayesian Personalized Ranking(BPR) that integrates complementary information between categories and latent features of users and items for personalized ranking. By uncovering the category dimensions re-lated, we can alleviate cold start issues, provide recommendations fusing complemen-tary relationships more than similarity. It is designed for personalized ranking which is consistent with users preferences and needs practically. The framework is evaluated comprehensively and the experimental results illustrate that our work optimized ranking significantly (with high recommendation performance).

