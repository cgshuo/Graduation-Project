 ORIGINAL PAPER Muriel Visani  X  Oriol Ramos Terrades  X  Salvatore Tabbone Abstract Most document analysis applications rely on the extraction of shape descriptors, which may be grouped into different categories, each category having its own advan-tages and drawbacks (O.R. Terrades et al. in Proceedings of ICDAR X 07, pp. 227 X 231, 2007). In order to improve the richness of their description, many authors choose to combine multiple descriptors. Yet, most of the authors who propose a new descriptor content themselves with compar-ing its performance to the performance of a set of single state-of-the-art descriptors in a specific applicative context ( e.g. symbol recognition, symbol spotting...). This results in a proliferation of the shape descriptors proposed in the literature. In this article, we propose an innovative protocol, the originality of which is to be as independent of the final application as possible and which relies on new quantitative and qualitative measures. We introduce two types of mea-sures: while the measures of the first type are intended to characterize the descriptive power (in terms of uniqueness, distinctiveness and robustness towards noise) of a descriptor, the second type of measures characterizes the complemen-tarity between multiple descriptors. Characterizing upstream the complementarity of shape descriptors is an alternative to the usual approach where the descriptors to be combined are selected by trial and error, considering the performance char-acteristics of the overall system. To illustrate the contribution of this protocol, we performed experimental studies using a set of descriptors and a set of symbols which are widely used by the community namely ART and SC descriptors and the GREC 2003 database.
 Keywords Document analysis  X  Shape descriptors  X  Symbol description  X  Performance characterization  X  Complementarity analysis 1 Introduction Over the last decades, there has been a growing interest about performance evaluation in the domain of graphics recogni-tion. Many contests have been organized, concerning raster-to-vector conversion [ 2  X  4 ], arc segmentation [ 5 ] and symbol recognition[ 6 , 7 ].Mostsymbolrecognitionmethodsrelyona two-step procedure: (1) symbol description (representation) by extracting a feature vector with one (or more) descriptor(s) and (2) supervised classification of the symbols to recognize, based on their feature vectors. Several shape descriptors have been proposed in the literature [ 8  X  10 ] and most of them have been applied to the task of symbol recognition.

This paper is focused on the first step of symbol descrip-tion, which is a crucial step that may be used for many other tasks in the field of document analysis (symbol spotting...). Ramos et al. have introduced in [ 1 ] a taxonomy of the differ-ent shape descriptors frequently used for symbol representa-tion. The new categorization they propose is made according to the properties of the different shape descriptors, pointing out their strengths and weaknesses. One of the main objec-tives of their work is to facilitate, for a given application, the choice of the best descriptor in that context.
However, when considering a problem of symbol recogni-tion, selecting the descriptor which is best suited for a given type of symbol and/or noise can be a hard, or even an impos-sible, task. Instead, one may be interested in combining dif-ferent descriptors from different categories in order to benefit fromtheadvantagesofallthedescriptorstobecombined.The combination may be performed at the level of the descriptor (early fusion) or at the level of the classifiers (late fusion). Early fusion is usually implicitly done with powerful classifi-ers like neural networks [ 11 ], boosting classifiers [ 12 , 13 ] and Support Vector Machines [ 14 ]. In those methods, descriptors are extracted and concatenated as a single feature vector. Later on, during the training process, each classifier com-binesthefeaturesfromthedifferentdescriptors.However,for general applications where the number of classes is high and the symbols to recognize can be counted by thousands, these expert classifiers reach their limits as their performance may decrease drastically. In this case, late fusion schemes where the combination is performed at the level of the classifier are generally preferred [ 15 ]. Late fusion methods have been applied to shape descriptors for symbol recognition [ 16 , 17 ]. Even so, in these papers, the performance characteristics of the descriptors in terms of descriptive power were not eval-uated (only the performance for recognition was studied). Additionally, the complementarity of the descriptors to be combined was not investigated upstream, even though it may beveryusefulwhenchoosingthesetofdescriptorstobecom-bined and the combination scheme which is best suited to this particular set of descriptors.

To the best of our knowledge, very few works have been proposed in the literature concerning the evaluation of the performance characteristics of symbol descriptors, most evaluations being focused on the final application. A meth-odology for characterizing the performance of shape descrip-tors for symbol recognition has been proposed in [ 18 ]. This paper additionally provides a general discussion concerning the main difficulties and problems one may be faced with when setting the data, evaluation metrics and evaluation pro-tocol, to characterize the performance of a symbol recog-nition method. Delalandre et al. [ 19 ] propose a solution to generate ground-truth based on a system that builds synthetic graphical documents. In [ 20 ], two main performance char-acterization metrics have been proposed, but we will see that these measures have several drawbacks that need to be com-pleted (see Sect. 3 ). Jouili et al. [ 21 ] propose a performance evaluation for symbol recognition based on graph matching measures. This evaluation is essentially quantitative, based on precision and recall rates.

In this paper, we propose an experimental protocol and both qualitative and quantitative measures for characterizing the descriptive power and the complementarity of different shape descriptors for symbol description. This methodology is as independent of the final application (symbol spotting, recognition) as possible. Contrary to the above-mentioned performance evaluation methodologies, we do not consider any classifier; at most we consider some dissimilarity or dis-tance measure and the nearest neighbour rule, to character-ize for instance the uniqueness and distinctiveness of a given descriptor.Weintroduceaninnovativeprotocolandtwotypes of measures: while the measures of the first type are intended to characterize the descriptive power (in terms of uniqueness, distinctiveness and robustness towards noise) of a descriptor, the second type of measures characterize the complementar-ity between multiple descriptors. Concerning the measures of the first type, we first recall the definitions of confusion matrices, recognition rate, precision, recall and Cumulative Match Characteristics (CMC) curves. Even though some of these measures are already used by many researchers in our community, our contribution here is that we link them to the notions of distinctiveness and uniqueness. Second, we introduce two measures that are original in the field of document analysis. These two measures are respectively the tolerance intervals, which characterize the robustness of descriptors towards noise, and a qualitative measure based on an analogy with Doddington X  X  zoo, widely known in the field of biometrics, characterizing the symmetries in the con-fusions. Concerning the measures of the second type, we introduce original measures to characterize upstream the complementarity between multiple descriptors. These mea-sures constitute an alternative to the usual approach where the descriptors to be combined are selected by trial and error, considering the performance characteristics of the overall system. It may also be helpful when choosing the best com-bination scheme for a given set of descriptors.

To illustrate our methodology, we present a case study using two well-known statistical descriptors: the Angular Radial Transform (ART) descriptor [ 22 ], based on region pixel values and the Shape Context (SC) [ 23 ] descriptor, based on contours. We use noisy versions of the GREC 2003 database ( cf. Fig. 4 ), which is well known and widely used by researchers working in the field of document analysis. It has to be noted that with adequate dissimilarity or distance mea-sures ( e.g. edit distance) our methodology can also be applied to structural descriptors. Moreover, the proposed framework may be further used for characterizing the complexity of any symbol database.

The paper is organized as follows. In Sect. 2 , some inno-vative measures for evaluating the descriptive power of dif-ferent shape descriptors, their robustness towards noise and their complementarity are proposed. In Sect. 3 , we propose an experimental protocol and perform experimental results using ART and SC descriptors on the GREC 2003 database. These results are analysed to highlight the interest of using the proposed protocol and measures. While Sect. 4 provides a discussion about the measures we propose, Sect. 5 concludes this paper and presents the future work. 2 Evaluating the descriptive power of the descriptors and the complementarity between descriptors In many applications such as symbol spotting, symbol rec-ognition, the richness of a descriptor is related to its ability to group the different occurrences of one given symbol (uniqueness of the representation) and to discriminate them from other symbols (distinctiveness). In this direction, Valv-eny et al. have proposed in [ 20 ] two measures characterizing respectively the uniqueness and the distinctiveness of a shape descriptor: homogeneity and separability. While homogene-ity is based on the distances between the descriptions of different occurrences of one symbol, separability is based on the distances between descriptions of different symbols. In this work, a good descriptor is characterized by high values of both homogeneity and separability. These measures are generic and may be used in many applicative contexts where a distance matrix between all the elements of the database can be computed. However, they have three main drawbacks. First, it is difficult to fix the thresholds which are necessary to characterize high values of these measures, since the dis-tributions of the distances between feature vectors vary a lot from one database to another. Second, in many applications, we have a model image (which may be considered as the original symbol) and noisy versions of this model that we need to confront to all the models in the database. These two categories of images (models and noisy symbols) have to be considered separately, which is not the case with homogene-ity and separability measures. Indeed, they rely on a distance matrix computed between all the symbols in the database, whatever their type. Third, in general the confusions between symbols are not symmetric ( e.g. symbol 1 may be confused with symbol 2 and not the opposite). And neither homo-geneity nor separability characterizes the symmetry of the confusions. Therefore, homogeneity and separability, which provide a coarse characterization of the richness of the dif-ferent descriptors, have to be completed by other measures which overcome the drawbacks listed earlier.
 To conceive such measures (which will be defined in Sects. 2.3 to 2.8 ), we first need to define more precisely the concepts of uniqueness and distinctiveness (see Sect. 2.1 ) and further to propose a protocol which is independent of the final application (see Sect. 2.2 ). 2.1 Definitions Let us focus on the very conventional case where, for each symbol i in the database (with i = 1 ,..., c ), we have in the descriptor X  X  representation space a symbol model S i and n i noisy versions of this model In this case we can consider that a descriptor provides a per-fect representation of a given symbol i when:  X  the representation of i is unique, i.e. feature vectors of  X  the representation of i is distinctive, i.e. S i is closer to Let us introduce the following notation: NN (  X  S j i ) being the the 1-nearest neighbour rule). The definitions of the unique-ness (see Eq. 1 ) and distinctiveness (see Eq. 2 ) of the symbol i may be respectively reformulated as follows:  X  j = 1 ... n i , NN (  X  S j i ) = S i (3)  X  l = 1 ... cstl = i ,  X  m = 1 ... n For instance, the symbol  X  X ircle X  in Fig. 1 is characterized by both perfect uniqueness and distinctiveness, while the symbol  X  X riangle X  is perfectly unique but not distinctive.
Equations 3 and 4 illustrate the notions of uniqueness and distinctiveness. However, they are not very useful in prac-tice, because they only characterize perfect levels of unique-ness and distinctiveness. Relaxing them in order to allow the characterization of different levels of uniqueness and distinctiveness would require the introduction of additional parameters such as thresholds applied on the distance val-ues. These parameters are difficult to settle in practice. In order to characterize the uniqueness and distinctiveness of a given descriptor, we therefore need to define a specific methodology. 2.2 Protocol In order to characterize efficiently the descriptive power of different shape descriptors in terms of uniqueness and distinctiveness, we introduce the following protocol, which is independent of the final application (spotting, recogni-tion...).First,thedistancesbetweenallthenoisysymbolsand all the models are computed. Second, we apply the k nearest neighbour rule ( kNN ) in order to associate with each noisy symbol its k nearest models in the descriptor X  X  representation space. Then, we can compute, for each descriptor, measures characterizing its descriptive power and robustness towards noise. Additionally, the complementarity between multiple descriptors may be measured.

In the remaining part of this section, we present two types of measures. The measures of the first type, introduced in Sects. 2.3 to 2.7 , characterize indirectly the levels of unique-ness and/or distinctiveness (which are too parameter-depen-dent to be computed directly) of a single descriptor. We first recall in Sects. 2.3 to 2.5 the definitions of confusion matrices, recognition rate, precision, recall and Cumulative Match Characteristics (CMC) curves. Even though some of these measures are already used by many researchers in our community, our contribution here is that we link them to the notions of distinctiveness and uniqueness. Second, we introduce two measures that are original in the field of doc-ument analysis. These two measures are respectively the tolerance intervals (see Sect. 2.6 ), characterizing the robust-ness of descriptors towards noise, and a qualitative measure based on an analogy with Doddington X  X  zoo, widely known in the field of biometrics, characterizing the symmetries in the confusions (see Sect. 2.7 ). Concerning the measures of the second type, we introduce in Sect. 2.8 original measures to characterize upstream the complementarity between mul-tiple descriptors. These measures constitute an alternative to the usual approach where the descriptors to be combined are selected by trial and error, considering the performance characteristics of the overall system. It may also be helpful when choosing the best combination scheme for a given set of descriptors.
 2.3 Confusion matrix A confusion matrix M is a quantitative measure characteriz-ing the descriptive power of a given descriptor. It is a contin-gency matrix computed from the array of distances between the descriptions of the noisy symbols and the models. This matrix contains c rows and c columns, where c is the number of models (see Table 1 ).The value in the cell n il of the con-fusion matrix is the number of noisy versions  X  S j i of symbol i which nearest model is S l in the descriptor X  X  representation space: n with NN (  X  S j i ) being the model which is the nearest to ( i.e. the result of the 1 NN rule following our protocol) and  X  NN (  X  S j otherwise. If we denote by n = c i = 1 c l = 1 n il the total number of noisy symbols, the descriptor may be consid-ered as perfectly describing the database when the confusion matrix is diagonal ( i.e. trace ( M ) = n ).

Even if the confusion matrix is in general defined by using the 1-nearest neighbour rule (see Eq. 5 ), we can characterize the behaviour of the descriptor in an enlarged neighbourhood of the noisy symbol by considering confusion matrices M ( associated with higher ranks k , by defining: n ( k ) = where k NN (  X  S j i ) is the k th nearest model to  X  S j of the kNN rule following our protocol). We can note that the confusion matrix M shown in Table 1 is the same matrix as matrix M ( 1 ) , while matrix M ( k ) with k &gt; 1 may be obtained by replacing the values n il in Table 1 with the n il ( k in Eq. 6 . In the remaining part of this article, we consider by default confusion matrices at rank k = 1, unless we explic-itly specify that we consider higher ranks k &gt; 1 (for CMC curves for example, see Sect. 2.5 ). 2.4 Recognition rate, precision and recall This section is dedicated to quantitative measures character-izing the richness of a given descriptor in terms of uniqueness and/or distinctiveness.

First of all, the recognition rate (RR) provides the per-centage of noisy symbols such that their nearest model is the  X  X ood X  one. It is computed from the confusion matrix (see Table 1 and Eq. 5 ) as follows: RR = It has to be noted that we can also compute the recognition rates at any rank k &gt; 1 by using the confusion matrix M (see Eq. 6 ): RR ( k ) = Hereafter, we consider by default the recognition rate at rank k = 1, unless we explicitly specify that we consider higher ranks of k (for CMC curves for example, see Sect. 2.5 ).
While the recognition rate gives some information about the descriptive power of a descriptor on the whole database, one may be interested in the behaviour of the descriptor for a particular symbol. We can note that a symbol i which is badly described in the descriptor X  X  representation space is associated with a large number of extra-diagonal elements n il and/or n li (with l A low distinctiveness for symbol i is characterized by high values of the column cells n li . On the other hand, a low uniqueness for symbol i is characterized by high values of the row cells values n il (see the definitions of distinctive-ness and uniqueness given in Sect. 2.1 ). The level of distinc-tiveness and uniqueness for a given symbol i may therefore be respectively measured by using precision P ( i ) and recall R ( i ) , where: P ( R (
To characterize the distinctiveness and uniqueness for the whole dataset, one may consider only the scalar value cor-responding to the average precision and/or recall among all the symbols i = 1 ... c : P = 1 R = 1
We can note that, in the special case where the number n i of noisy versions of symbol i is the same for all the c sym-bols i , the mean recall equals the recognition rate (given that 2.5 Cumulative match characteristics curve In order to characterize the behaviour of the descriptor in an enlarged neighbourhood of the symbols to describe (not only considering the nearest model), we may consider the recognition rates at ranks k &gt; 1. The Cumulative Match Characteristic (CMC) curves are most widely used for eval-uating the performance characteristics of semi-automated recognition systems where N candidates are proposed to a (often human) supervisor, the role of the supervisor being to select the good candidate. Such curves are useful to quickly visualize the cumulated recognition rates at different ranks k : CMC ( k ) = where RR ( k ) is the recognition rate at rank k  X  1 (see Eq. 8 ). For an example of a CMC curve see Fig. 7 . If the CMC curve reaches a sufficiently high value at a rank k being tractable for the supervisor, then the semi-automated system is considered as effective. 2.6 Characterization of the Robustness towards noise In this section, we present the Tolerance Interval, which has been defined in the context of face recognition in [ 25 ]in order to characterize the robustness of descriptors towards noise. Tolerance Intervals may be calculated in the case where the amount of noise is controlled by one parame-ter  X  ( i.e. in general when the noise is synthetically added to the images). For example, for Gaussian white noise the parameter is the standard deviation:  X  =  X  . The recog-nition rate is computed as a function of the value of the noise parameter. A Tolerance Interval (TI) at p % may be defined as the range of values of parameter  X  such that the recognition rate RR remains greater than 1  X  p / 100. Examples of Tolerance Intervals are given in Table 3 .For afixed p , the larger is the Tolerance Interval, the more robust is the descriptor. Tolerance Intervals characterize in a compact way the robustness of a descriptor towards a given type of noise; they may be very helpful when choos-ing the descriptor which is best suited to a specific kind of noise. 2.7 The zoo qualitative characterization All the previous measures are intrinsically quantitative. In particular, we have shown how the precision and recall measures may be used at the symbol level to characterize the asymmetries in the confusions of a single descriptor for a given symbol ( e.g. symbol i may be confused with others and not the opposite). However, when trying to compare the descriptive power of multiple descriptors for a given symbol, it is difficult to consider jointly multiple precision and recall values. That is why we introduce a qualitative measure based on the definition of categories of symbols. Our categoriza-tion is inspired from Doddington et al. X  X  terminology [ 26 ]. This terminology was first introduced in the field of speaker recognition for biometrics. Figure 1 provides examples of the different categories. We give the original definitions by Doddington et al., followed by their adaptations in our context.  X   X  X n our model, sheep dominate the population and sys- X   X  X ambs, in our model, are those speakers who are partic- X   X  X olves, in our model, are those speakers who are partic- X   X  X oats tend to adversely affect the performance of sys-
While in the case of sheep the descriptive power of the descriptor may be considered as satisfactory, in the case of goats the descriptive power is low. But, for a given symbol, the behaviours of two different descriptors may differ. For instance, symbol i may be a sheep with descriptor D 1 and a wolf with descriptor D 2. At the level of the whole data-base, these behaviours may be complementary, e.g. in the case where symbol i is a sheep with descriptor D 1 and not a sheepwithdescriptor D 2,andvice-versaforsymbol j .Inthat case, the description may be improved by combining the two descriptors, instead of considering a single descriptor. That is the reason why, in the following section, we introduce measures to characterize the complementarity of different descriptors. 2.8 Ensemble measures characterizing the complementarity In this section, we introduce quantitative measures of the complementarity between different descriptors. We can note that, in most cases, confronting the confusion matrices of different descriptors does not help to characterize their com-plementarity. Indeed, while the confusion matrix provides information at the symbol level, the complementarity occurs at the level of the noisy image. For instance, when the con-fusion matrix says that, for a given symbol i , only 15 noisy versions over 30 are well described by descriptor D 1 and by descriptor D 2, these 15 well-described images may be the same (in this case the two descriptors are not complementary at all for this symbol) or totally distinct (in this case the two descriptors are perfectly complementary for this symbol), but from the confusion matrix we cannot guess which case we are dealing with. That is why we introduce the following complementarity measures that may be computed at differ-ent ranks k  X  1, with  X  S j i being a noisy version of the original model S i and kNN D (  X  S j i ) the k th nearest model of representation domain associated with the descriptor D :
The measure U ( k ) is the number of noisy images such that their k th nearest model is the  X  X ood X  one for at least one of the two descriptors D 1or D 2. The measure I ( k ) is the number of noisy images such that their k th nearest model is the  X  X ood X  one for both descriptors D 1 and D 2. We can note that, by construction, I ( k )  X  U ( k ) . The measure I the total number of noisy images such that their k th nearest model is the  X  X ood X  one for descriptor D 1 but not for D 2 (and vice-versa for I D 2 ). We can note that U ( k ) = I I images such that their k th nearest model is not the  X  X ood X  one, neither for descriptor D 1 nor for D 2. We can note that C ( k ) + U ( k ) = n , where n is the total number of noisy sym-bols in the database. Figure 8 provides a visual example of such measures with descriptors ART and SC. Let us note the followingrelationship: I ( k ) + I D ( k ) n = RR D ( k ) is the recognition rate at rank k (see Eq. 8 ) associated with descriptor D . Of course the measures given in Eqs. ( 14  X  18 ) can be directly extended with more than two descriptors.
The numbers of images which are well represented by one descriptor but not by the other one ( i.e. I D 1 and I allow us to quantify the complementarity of the two descrip-tors. In particular, the value of U ( 1 ) is the maximal number of symbols that may be well-described at rank 1 ( i.e. the objective value) when conceiving a strategy for selecting, for each symbol, the best descriptor for this symbol. The more the objective value U ( 1 ) n exceeds the maximal recog-nition rate of the two descriptors, the more complementary are these two descriptors. Characterizing the complementar-ity between descriptors is very interesting, as considering a combination of complementary descriptors may improve the richness of the description of the symbol compared to considering a single descriptor. 3 Experimental study In this section, we perform an experimental study to illustrate the effectiveness of the measures we define in Sect. 2 .The objective is to show how to use these measures for (1) com-paring multiple descriptors in terms of descriptive power and noise robustness and (2) measuring the complementarity of multiple descriptors. For this purpose, we consider two well-known shape descriptors (that will be described in Sect. 3.1 ): ART and SC. The main objective here is not to character-ize the performance of these two descriptors, but rather to illustrate the contribution brought to the community by our innovative protocol and measures. We selected ART and SC among the large variety of available shape descriptors for two main reasons. First, because of the paper size limit, we could not consider more than two descriptors. Second, ART and SC belong to different categories of shape descriptors (ART is 2D while SC is a 1D descriptor based on contours). This fact certainly makes them complementary to some extent, which is interesting to illustrate our complementarity measure.
This section is organized as follows. A brief description of the considered descriptors is given in Sect. 3.1 . Next, the databases we use and their features are detailed in Sect. 3.2 . Then, in Sect. 3.3 we compute and analyse the measures pro-posed in Sect. 2 . 3.1 Statistical shape descriptors Amongthevariousshapedescriptorsthathavebeenproposed in the literature [ 8  X  10 ], we selected in this paper two well-known statistical descriptors which are essentially different in their primitives: Angular Radial Transform (ART) [ 22 ] and Shape Context (SC) [ 23 ]. While ART is based on a 2D prim-itive (region inside the image) and provides a polar-based feature vector, SC is a 1D primitive (relying on the extrac-tion of shape contours) resulting in a histogram-based feature vector. 3.1.1 The ART descriptor The ART descriptor [ 22 ] is the result of a complex 2D trans-form defined on a unit circle using polar coordinates. More precisely, ART coefficients are defined by the projection of the original image represented in polar coordinates on a basis of orthogonal complex functions V n , m ( X ,  X  ) = A m ( X  ) ( cf. Fig. 2 ). These basis functions are defined by multiplying a radial function R n of parameter n by an angular function A m of parameter m , the pair of parameters the order of the coefficient F n , m .

Invariance to similarity transforms is achieved by (1) using an exponential functional in the angular function (to get invariance towards rotations) and (2) centring and scaling the shape image before computing the coefficients (to achieve invariance towards scale and translation).
 Finally, the distance between shapes is measured by the Manhattan ( L 1 ) distance. 3.1.2 The SC descriptor The SC descriptor [ 23 ] is based on relative spatial locations between some points extracted from the contours of the shape to analyse. Figure 3 illustrates its underlying principle. The shape to be described is represented by a discrete set of points extractedfrom theexternal andinternal contours of theshape.
This descriptor can be considered invariant to scaling if the background is not too complex, since the radial distances are normalized by the average distance between all the pairs of points of the shape. In addition, it is invariant towards translation and can easily be made invariant towards rotation. And, given that the SC descriptor provides coarse informa-tion extracted from the whole shape, it is relatively robust towards occlusions.
Even though the  X  2 distance was initially used to compare shape contexts from different symbols [ 23 ], more recently numerous authors have chosen to consider shape contexts as feature vectors and to compare them by using the L 2 (Euclidean) distance [ 24 ]. 3.2 Experimental protocol In our experiments, we consider the GREC 2003 symbol database [ 6 ]. This database contains 150 models of sym-bols, which are used to generate noisy versions by apply-ing the Kanungo algorithm [ 27 ]. The Kanungo noise is an additive noise applied to binary images; it is controlled by six parameters. Among these parameters, we chose to vary  X  and  X  , which simulate the presence of an ascending amount of noise in the image. When  X  decreases, the probability for a symbol pixel to be inverted and considered as a back-ground pixel increases (which may be seen as some kind of  X  X alt X  noise), while when  X  decreases the probability to invert a background pixel as a symbol pixel increases ( X  X epper X  noise). It has to be noted that these probabilities of inversion decrease according to the distance from the contour of the shape. Five databases, each one containing 30 random noisy versions of each of the 150 model symbols are generated for each  X  = 1 ( cf. Fig. 4 ). Five databases are constructed similarly by vary-ing parameter  X  . At the end, we obtain a database containing the 150 model images (one image per symbol model) and 10 test databases, each one containing 30  X  150 = 4 , 500 noisy symbols.

In this experiment, we compare each noisy image from the 10 noisy databases to the database containing the model images. For this purpose, we use for each descriptor its asso-ciated distance (the Manhattan distance for ART and the Euclidean distance for SC) and the protocol presented in Sect. 2.2 . 3.3 Experimental results The analysis of the experimental results is in four steps. Dur-ing the first step (Sect. 3.3.1 ), a coarse evaluation of the per-formance characteristics of the descriptors ART and SC is provided. For this coarse evaluation, we consider the usual performance measures (recognition rate, precision and recall, seeSect. 2.4 ).Thesecondstep(Sect. 3.3.2 )isacomparisonof the robustness of the two descriptors towards noise. For this comparison, we compute Tolerance Intervals (see Sect. 2.6 ) and we consider the two different types of Kanungo noise (  X   X  X alt X  noise and  X   X  X epper X  noise). Then, a subset of dat-abases (among the most noisy) are selected for the third step of the analysis (Sect. 3.3.3 ). The third step is a detailed anal-ysis relying on the confusion matrices (see Sect. 2.4 ), the CMC curves (see Sect. 2.5 ), and the qualitative measures we introduce in Sect. 2.7 . The fourth step, given in Sect. 3.3.4 , is a study of the complementarity of ART and SC, based on the complementarity measures defined in Sect. 2.8 . 3.3.1 Overview of the performance characteristics of ART The recognition rate (RR), depending on the type and amount of noise, is given in Table 2 and Fig. 5 . We can note that, as explained in Sect. 2.4 , the recognition rate equals the mean recall in our case where the number n i = 30 of noisy ver-sions is constant over all the symbols i . Table 2 also gives the mean precision (at rank k = 1 and the standard deviations of the recognition rate and mean precision. We can see that, for both descriptors and both types of noise, the quality of the description decreases when the amount of noise increases. We can also note that the decrease is more abrupt in the pres-ence of  X  noise. For levels of noise N &gt; 2, SC is superior to ART for  X  X alt X  noise (  X  -noise) while ART is superior to SC for  X  X epper X  noise (  X  -noise). When N  X  2 ART is always superior to SC, whatever kind of noise is applied.
We can easily understand why the performance of SC decreases drastically in the presence of  X  X epper X  noise: SC is based on points sampled from the contour of the symbol ( cf. Fig. 3 ). When pepper noise is added to the image, the shape contours are modified. In that case, the SC description is computed from inaccurate points and becomes imprecise. Conversely,  X  X alt X  noise has a thinning effect on the sym-bol. Therefore, when the  X  -noise increases, the amount of information available for computing ART is reduced, which makes ART description unstable (see the high standard devi-ation values in Table 2 ). 3.3.2 Comparison of the Robustnesses of ART and SC From Table 2 and Fig. 5 , we compute the Tolerance Inter-vals (TI) (described in Sect. 2.6 ) corresponding to descriptors ART and SC, towards  X  -noise and  X  -noise. The TIs at levels p = 5% and p = 20% are given in Table 3 .

The results in Table 3 are consistent with the shapes of the curves in Fig. 5 . In particular, in the presence of  X  -noise, the TI of ART is narrower than the TI of SC, which implies that SC is more robust than ART towards  X  -noise. For  X  -noise and descriptor SC, the fact that the TIs at p = 5% and p = 20% are both equal to [ 1 , 6 ] is due to a drastic drop in the recognition rates between the levels of noise N = 6 and N = 8. Table 3 shows the decrease in the quality of the description when the amount of noise is increased. In par-ticular, none of the two descriptors is tolerant at p = 5% towards a noise of level N = 10 (neither for  X  -noise nor for  X  -noise). Consequently, the rest of this section is devoted to the detailed analysis of the results for the levels of noise N = 6 and N = 8for  X  and  X  . Indeed, when applied on these databases, the behaviour of the descriptors is representative of the general case where the noise is not too strong and at least one of our two descriptors remains robust. 3.3.3 Detailed analysis of the performance characteristics To provide a more detailed analysis of the descriptors X  behav-iours, we show in Table 4 some extracts of the confusion matrices (see Sect. 2.3 ) for symbols 11, 87 and 125 (see Fig. 6 ). We consider these particular symbols because, in the presence of  X  -noise (at levels 6 and 8), they are subject to confusions. Let us now focus on the database with noise  X  at level N = 6 (Table 4 a and c). Among the 4,500 noisy symbols of the whole database, ART badly describes only 19 noisy symbols. All of these 19 poor descriptions come from confusions between symbols 87 and 125. On the other hand, the SC descriptor badly describes 165 noisy symbols over 4,500, among which 29 poor descriptions are due to confusions between symbols 11 and 87.

To go deeper into the analysis of the database with  X  -noise of level N = 6, let us consider additionally the precision and recall measures (see Sect. 2.4 ) associated with the symbols 11, 87 and 125 and the qualitative definitions introduced in Sect. 2.7 . From this analysis, we conclude that  X  symbol 11 (respectively symbol 125) is a sheep for  X  symbol 125 (respectively symbol 87) is a lamb for  X  symbol 87 (respectively symbol 11) is a wolf for descrip-
A preliminary statistical study has shown that the dat-abases are very homogeneous for both descriptors [ 20 ]. This means that the number of goats is very reduced in this con-text. Therefore, we did not look for goats, which would have required the settlement of an additional parameter  X  (seeSect. 2.7 ). Thefact that symbol 11is awolf for symbol 87 in the presence of pepper noise for descriptor SC is easily understandable. Indeed, the pepper noise located at the cen-tre of the cross in symbol 11 may be confused with the small circle at the centre of symbol 87 (see Fig. 6 ). With ART, 11 noisy occurrences of symbol 87 have the  X  X ood X  model 87 as nearest neighbour, while the remaining 19 occurrences have the model 125 as nearest neighbour. This phenomenon makes us guess that there is an overlap between the images of symbols 87 and 125 in the ART description space.
In addition, Table 4 shows that these dissymmetries in the confusion matrix increase when more pepper noise is added to the symbols (between levels N = 6 and N = 8). For instance,whenthelevelof  X  -noisereaches N = 8,symbol87 becomes a wolf for symbol 125 in the SC space, as the images of symbols 87 become closer to the model 125 than to the model 87.

From this point of view, characterizing the results of the descriptions not only at the first rank ( i.e. using the nearest model), but at higher ranks ( i.e. using the k nearest models) is very important. Indeed, in the case where there is an overlap between two symbols ( e.g. in the case of symbols 87 and 125 for ART), the  X  X ood X  model may be among the two nearest models but not necessarily the nearest one. And we can con-sider that, if the  X  X ood X  model is among the two nearest mod-els, the quality of the description is better than if the  X  X ood X  model is farther. In other words, we can consider that a wolf model which is near its lamb (see Fig. 1 ) is better described than a goat. In order to take into account higher ranks k we consider the CMC curves (see Sect. 2.5 ) shown in Fig. 7 . Figure 4 a and c shows that most of the confusions of the descriptor SC at rank 1 with noise  X  (see Table 2 ) are solved at ranks 2 or 3. This means that the descriptive power of SC in the presence of  X  -noise is relatively good, because even the noisy symbols which are badly classified by using the 1 NN rule are well classified when considering the 3 NN rule. On the contrary, we can see from Fig. 4 d that the descriptive power of SC is bad with  X  -noise of level N = 8, as the CMC curve of SC does not reach 100% recognition rate before rank k &gt; 20.

From Table 4 a and c, we can also note that, with noise  X  at level N = 6, while ART does not manage to discrimi-nate between symbol 87 and symbol 125, SC perfectly dis-criminates between these two symbols. And vice-versa for symbols 11 and 87 in the same database. Therefore, we can consider that ART and SC are complementary for symbols 11, 87 and 125 in the particular database with noise  X  at level N = 6. Which means that combining ART and SC to obtain a single descriptor may improve (on average) the quality of our description, compared to a single descriptor. Now let us expand our study of the complementarity of ART and SC to the whole dataset. 3.3.4 Analysis of the complementarity of ART and SC To end this experimental study, we measure the comple-mentarity of ART and SC by the measures defined in Eqs. ( 14  X  18 ), where D 1 = ART and D 2 = SC. The results are given in Fig. 8 .

From that figure we can see that both descriptors are really complementary on the four databases. For the two databases with a noise level N = 6, we can see that the two descrip-tors are perfectly complementary as C = 0 with the two types of noise (even if ART is superior to SC in the presence of  X  -noise while SC is superior to ART in the presence of  X  -noise). It means for instance that, for a task of recogni-tion, any query symbol from these noisy databases may be correctly classified automatically by using the ART and SC descriptors. Provided an ideal adaptive descriptor selection method that would select, for each query symbol and each type of noise, the best performing descriptor for this particu-lar symbol. With a noise level of N = 8, we can observe high values of both I ART and I SC , which means that combining ART and SC may enhance the average quality of descrip-tion of the symbols, whatever type of noise is present in the images, compared to a single descriptor. 3.3.5 Conclusion of the experimental section The results given in this section have shown that SC is superior to ART in the presence of  X  X alt X  noise while ART is superior to SC in the presence of  X  X epper X  noise. Both descriptors are robust towards a small amount of noise but their performance decreases drastically when the amount of noise increases (especially with  X  X epper X  noise). However, the SC descriptor remains more robust than ART with salt noise. We can also note that these two descriptors are comple-mentary. Therefore, combining them may enhance the qual-ity of description of a symbol compared to that of a single descriptor. 4 Discussion Well-known and widely used evaluation measures such as the recognition rate (RR) and the Precision-Recall values (see Sect. 2.4 ) are very useful to measure whether a given descriptor is adapted to a particular context. They provide performance characteristics on a set of evaluation databases which are supposed to be representative of the images the system will find in a real environment. However, when no descriptor is superior to the others on all the databases (for different types of noise for example), then the issue of the usefulness of the evaluation measures proposed in Sects. 2.5 and 2.6 arises. Thereby, the Tolerance Interval quickly gives an idea of the robustness of descriptors towards noise, while the CMC curves characterize the quality of the description in the neighbourhood of the noisy symbols.

Nevertheless, the descriptive power of a given descrip-tor may vary from one symbol to another. Indeed, in any database and for any descriptor there are symbols which are well-described and others which are not (provided a database of sufficient complexity). In this context, using the qualita-tive measures introduced in Sect. 2.7 becomes useful, since it allows us to detect the overlapping symbols. The informa-tion given by these measures is equivalent to the information given by the CMC curves, but detailed for each symbol in the database, while the CMC remains general.

The complementarity measures (see Sect. 2.8 ) provide information about the benefit we can expect from the combi-nation of multiple descriptors. Hence, the best configuration for a pair of descriptors is to be perfectly complementary, which is the case for ART and SC on the databases of noise level N = 6, for both  X  and  X  noise. Measuring upstream the complementarity of shape descriptors is an interesting alternative to the most widely used approach consisting in selecting the descriptors to be combined by trial and error, considering the performance characteristics of the overall system.

It has to be noted that the complementarity measures can also be used to characterize the complexity of a given data-base. Indeed, if we consider a well-chosen set of mutually complementary descriptors and that this set of descriptors gives poor results on a given database ( i.e. the value of C is high), we can consider that this database is highly complex. On the contrary, when (considering the same set of descrip-tors) the value of C is small ( i.e. only a small number of samples are badly represented by all the descriptors), the database can be considered as less complex. When, in addi-tion, the value of I almost equals the value of U ( i.e. all the descriptors describe correctly almost all the samples), the complexity of the database may be considered as low.
To conclude this discussion, we can note that all the mea-sures introduced in Sect. 2 may also be applied to several structural descriptors. Indeed, we have seen that what we only need in our protocol is a confusion matrix including distances or dissimilarity measures between noisy symbol descriptors and models. In this case, we can easily extend this framework to structural methods based on graph repre-sentation and graph similarity measures which quantify the effort needed to match one graph with a another [ 28  X  30 ]. 5 Conclusion In this paper, we introduced an experimental protocol and measures for characterizing the performance of descriptors in the context of symbol description. The measures we intro-duced are of two types. While the first type of measures is devoted to the descriptive power of each descriptor taken sep-arately in terms of uniqueness, distinctiveness or robustness towards noise, the second type of measures aims at evaluat-ing the complementarity of a set of descriptors. Concerning the first type of measures, we first recalled the definitions of confusion matrices, recognition rate, precision, recall and Cumulative Match Characteristics (CMC) curves. Although some of these measures are already known by many research-ers in our community, our originality is that we linked them to the notions of distinctiveness and uniqueness. Second, we introduce two measures that are new in the field of docu-ment analysis. These two measures are respectively the tol-erance intervals, characterizing the robustness towards noise, and a qualitative measure characterizing the symmetries in the confusions. Concerning the measures of the second type, we introduce original measures to characterize upstream the complementarity between multiple descriptors. These mea-sures may assist the researchers when selecting the descrip-tors to be combined, instead of selecting them by trial and error downstream.

We analysed experimentally a didactic case study (consid-ering the widely-known descriptors ART and SC), to illus-trate the effectiveness of the measures we defined. Even if the main objective of this experimental part is didactic and not directly to draw conclusions about the performance char-acteristics of ART and SC, it highlights the relevance of combining SC and ART for describing symbols.

It has to be noted that the complementarity measures may be additionally used for characterizing the complexity of a given database: the basic idea behind this is that, when a well-chosen set of mutually complementary descriptors gives poor results on a given database, we can consider that this database is highly complex. As a conclusion, our measures may there-fore be helpful for various purposes concerning performance evaluation, in the field of document description and analy-sis. We are currently working on an ambitious performance evaluation campaign relying on our protocol and measures, dedicated to symbol description by shape descriptors. References
