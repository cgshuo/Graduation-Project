 This paper focuses on an exact implementation of the linearised form of lattice minimum Bayes-risk (LMBR) decoding using general purpose weighted finite state transducer (WFST) opera-(2008) has the form  X  E = argmax where E is a lattice of translation hypotheses, N is the set of all n -grams in the lattice (typically, n = 1 . . . 4 ), and the parameters  X  are constants estimated on held-out data. The quantity p ( u |E ) we refer to as the path posterior probability of the n -gram u . This particular posterior is defined as where E set of lattice paths containing the n -gram u at least once. It is the efficient computation of these path posterior n -gram probabilities that is the primary focus of this paper. We will show how general purpose WFST algorithms can be employed to ef-ficiently compute p ( u |E ) for all u  X  X  .

Tromble et al. (2008) use Equation (1) as an approximation to the general form of statistical machine translation MBR decoder (Kumar and Byrne, 2004): The approximation replaces the sum over all paths in the lattice by a sum over lattice n -grams. Even though a lattice may have many n -grams, it is possible to extract and enumerate them exactly whereas this is often impossible for individual paths. Therefore, while the Tromble et al. (2008) linearisation of the gain function in the decision rule is an approximation, Equation (1) can be com-puted exactly even over very large lattices. The challenge is to do so efficiently.

If the quantity p ( u |E ) had the form of a condi-tional expected count it could be computed efficiently using counting transducers (Allauzen et al., 2003). The statis-tic c ( u |E ) counts the number of times an n -gram occurs on each path, accumulating the weighted count over all paths. By contrast, what is needed by the approximation in Equation (1) is to iden-tify all paths containing an n -gram and accumulate their probabilities. The accumulation of probabil-ities at the path level, rather than the n -gram level, makes the exact computation of p ( u |E ) hard.
Tromble et al. (2008) approach this problem by building a separate word sequence acceptor for each n -gram in N and intersecting this acceptor with the lattice to discard all paths that do not con-tain the n -gram; they then sum the probabilities of all paths in the filtered lattice. We refer to this as the sequential method , since p ( u |E ) is calculated separately for each u in sequence.

Allauzen et al. (2010) introduce a transducer for simultaneous calculation of p ( u |E ) for all un-igrams u  X  X  effective for finding path posterior probabilities of unigrams because there are relatively few unique unigrams in the lattice. As we will show, however, it is less efficient for higher-order n -grams.
Allauzen et al. (2010) use exact statistics for the unigram path posterior probabilities in Equa-tion (1), but use the conditional expected counts of Equation (4) for higher-order n -grams. Their hybrid MBR decoder has the form where k determines the range of n -gram orders at which the path posterior probabilities p ( u |E ) of Equation (2) and conditional expected counts c ( u |E ) of Equation (4) are used to compute the expected gain. For k &lt; 4 , Equation (5) is thus an approximation to the approximation. In many cases it will be perfectly fine, depending on how closely p ( u |E ) and c ( u |E ) agree for higher-order n -grams. Experimentally, Allauzen et al. (2010) find this approximation works well at k = 1 for MBR decoding of statistical machine translation lattices. However, there may be scenarios in which longer useful in place of the original Tromble et al. (2008) approximation.

In the following sections, we present an efficient method for simultaneous calculation of p ( u |E ) for n -grams of a fixed order. While other fast MBR approximations are possible (Kumar et al., 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementa-tion of Equation (1) for efficient MBR decoding over lattices. We make use of a trick to count higher-order n -grams. We build transducer  X  quences to n -gram sequences of order n .  X  similar form to the WFST implementation of an n -gram language model (Allauzen et al., 2003).  X  includes for each n -gram u = w n
The n -gram lattice of order n is called E found by composing E X   X  put, removing  X  -arcs, determinizing, and minimis-ing. The construction of E lattices and is memory efficient. E have more states than E due to the association of distinct n -gram histories with states. However, the counting transducer for unigrams is simpler than the corresponding counting transducer for higher-order n -grams. As a result, counting unigrams in E Associated with each E which can be used to calculate the path posterior probabilities p ( u |E ) for all u  X  X  can be used to compute path posterior probabilities over n -grams u tion to the  X  -arc matching mechanism is required even in counting higher-order n -grams since all n -grams are represented as individual symbols after application of the mapping transducer  X 
Transducer  X  L to compute the exact unigram contribution to the conditional expected gain in Equation (5). For ex-ample, in counting paths that contain u tains the first occurrence of u other symbol to  X  . This ensures that in any path containing a given u , only the first u is counted, avoiding multiple counting of paths.

We introduce an alternative path counting trans-ducer  X  R cept the last occurrence of u on any path by en-suring that any paths in composition which count earlier instances of u do not end in a final state. Multiple counting is avoided by counting only the last occurrence of each symbol u on a path.

We note that initial  X  :  X  arcs in  X  L create |N searching for the first occurrence of each u . Com-Figure 1: Path counting transducer  X  L first (left-most) occurrence of each u  X  X  Figure 2: Path counting transducer  X  R last (right-most) occurrence of each u  X  X  posing with  X  R searching for the last occurrence of u ; we find this to be much more efficient for large N
Path posterior probabilities are calculated over each E ing, projecting on the output, removing  X  -arcs, de-terminizing, minimising, and pushing weights to the initial state (Allauzen et al., 2010). Using ei-ther  X  L It has a compact form with one arc from the start state for each u 3.1 Efficient Path Posterior Calculation Although X it can be difficult to build for large N the composition E states and arcs. The log semiring  X  -removal and determinization required to sum the probabilities of paths labelled with each u can be slow.

However, if we use the proposed  X  R path in E bel u and all paths leading to a given final state share the same u . A modified forward algorithm can be used to calculate p ( u |E ) without the costly  X  -removal and determinization. The modification simply requires keeping track of which symbol u is encountered along each path to a final state. More than one final state may gather probabilities for the same u ; to compute p ( u |E ) these proba-bilities are added. The forward algorithm requires that E ing can be slow, it is still quicker than log semiring  X  -removal and determinization.

The statistics gathered by the forward algo-rithm could also be gathered under the expectation semiring (Eisner, 2002) with suitably defined fea-tures. We take the view that the full complexity of that approach is not needed here, since only one symbol is introduced per path and per exit state.
Unlike E not segregate paths by u such that there is a di-rect association between final states and symbols. The forward algorithm does not readily yield the per-symbol probabilities, although an arc weight vector indexed by symbols could be used to cor-rectly aggregate the required statistics (Riley et al., 2009). For large N tensive. The association between final states and symbols could also be found by label pushing, but we find this slow for large E In contrast to Equation (5), we use the exact values of p ( u |E ) for all u  X  X  compute where g ing the exact path posterior probabilities at each order. We make acceptors  X  assigns order n partial gain g E  X  X  .  X  n is derived from  X  n directly by assign-ing arc weight  X  u and then projecting on the input labels. For each n -gram u = w n
To apply  X  with fixed weight  X  formed as the composition E and  X  E is extracted as the maximum cost string. Lattice MBR decoding performance and effi-ciency is evaluated in the context of the NIST the hybrid decision rule of Equation (5) at 0  X  k  X  3 , and regular linearised lattice MBR (LMBR). Table 2: Time in seconds required for path posterior n -gram probability calculation and LMBR decoding using sequential method and left-most ( X  L Arabic  X  English machine translation task 3 . The development set mt0205tune is formed from the odd numbered sentences of the NIST MT02 X  MT05 testsets; the even numbered sentences form the validation set mt0205test. Performance on NIST MT08 newswire (mt08nw) and newsgroup (mt08ng) data is also reported.

First-pass translation is performed using HiFST (Iglesias et al., 2009), a hierarchical phrase-based decoder. Word alignments are generated using MTTK (Deng and Byrne, 2008) over 150M words of parallel text for the constrained NIST MT08 Arabic  X  English track. In decoding, a Shallow-1 grammar with a single level of rule nesting is used and no pruning is performed in generating first-pass lattices (Iglesias et al., 2009). The first-pass language model is a modified Kneser-Ney (Kneser and Ney, 1995) 4-gram esti-mated over the English parallel text and an 881M word subset of the GigaWord Third Edition (Graff et al., 2007). Prior to LMBR, the lattices are rescored with large stupid-backoff 5-gram lan-guage models (Brants et al., 2007) estimated over more than 6 billion words of English text.

The n -gram factors  X  to Tromble et al. (2008) using unigram precision p = 0 . 85 and average recall ratio r = 0 . 74 . Our translation decoder and MBR procedures are im-plemented using OpenFst (Allauzen et al., 2007). Lattice MBR decoding performance is shown in Table 1. Compared to the maximum likelihood translation hypotheses (row ML), LMBR gives gains of +0.8 to +1.0 BLEU for newswire data and +0.5 BLEU for newsgroup data (row LMBR).

The other rows of Table 1 show the performance of LMBR decoding using the hybrid decision rule of Equation (5) for 0  X  k  X  3 . When the condi-tional expected counts c ( u |E ) are used at all orders (i.e. k = 0 ), the hybrid decoder BLEU scores are considerably lower than even the ML scores. This poor performance is because there are many un-igrams u for which c ( u |E ) is much greater than p ( u |E ) . The consensus translation maximising the conditional expected gain is then dominated by unigram matches, significantly degrading LMBR decoding performance. Table 1 shows that for these lattices the hybrid decision rule is an ac-curate approximation to Equation (1) only when k  X  2 and the exact contribution to the gain func-tion is computed using the path posterior probabil-ities at orders n = 1 and n = 2 .
We now analyse the efficiency of lattice MBR decoding using the exact path posterior probabil-the sequential method and both simultaneous im-plementations using path counting transducers  X  L and  X  R numerical accuracy); they differ only in speed and memory usage.
 Posteriors Efficiency Computation times for the steps in LMBR are given in Table 2. In calcu-lating path posterior n -gram probabilities p ( u |E ) , we find that the use of  X  L as slow as the sequential method. This is due to the difficulty of counting higher-order n -grams in large lattices.  X  L grams, however, since there are far fewer of them. Using  X  R method. This speed difference is due to the sim-ple forward algorithm. We also observe that for higher-order n , the composition E less memory and produces a smaller machine than E occurrence of a symbol than by the first.
 Decoding Efficiency Decoding times are signif-icantly faster using  X  average decoding time is around 0.1 seconds per sentence. The total time required for lattice MBR is dominated by the calculation of the path pos-terior n -gram probabilities, and this is a func-tion of the number of n -grams in the lattice |N| . For each sentence in mt0205tune, Figure 3 plots the total LMBR time for the sequential method (marked  X  X  X ) and for probabilities computed using  X  niques on a sentence-by-sentence basis. As |N| grows, the simultaneous path counting transducer is found to be much more efficient. We have described an efficient and exact imple-mentation of the linear approximation to LMBR using general WFST operations. A simple trans-ducer was used to map words to sequences of n -grams in order to simplify the extraction of higher-order statistics. We presented a counting trans-ducer  X  R all n -grams of order n in a single composition and allows path posterior probabilities to be computed efficiently using a modified forward procedure.
We take the view that even approximate search criteria should be implemented exactly where pos-sible, so that it is clear exactly what the system is doing. For machine translation lattices, conflat-order n -grams might not be a serious problem, but in other scenarios  X  especially where symbol se-quences are repeated multiple times on the same path  X  it may be a poor approximation.

We note that since much of the time in calcula-tion is spent dealing with  X  -arcs that are ultimately removed, an optimised composition algorithm that skips over such redundant structure may lead to further improvements in time efficiency.
 This work was supported in part under the GALE program of the Defense Advanced Re-search Projects Agency, Contract No. HR0011-06-C-0022.

