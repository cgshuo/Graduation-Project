 Name disambiguation is a challenging and important prob-lem in many domains, such as digital libraries, social media management and people search systems. Traditional meth-ods, based on direct assignment using supervised machine learning techniques, seem to be the most effective, but their performances are highly dependent on the amount of train-ing data, while large data annotation can be expensive and time-consuming requiring hours of manual inspection by a domain expert. To efficiently acquire labeled data, we pro-pose a bootstrapping algorithm for the name disambiguation task based on active learning and crowdsourced labeling. We show that the proposed method can leverage the advantages of exploration and exploitation by combining two strategies, thereby improving the overall quality of the training data at minimal expense. The experimental results on two datasets DBLP and ArnetMiner demonstrate the superiority of our framework over existing methods.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Retrieval Algorithms, Experimentation Name Disambiguation, Active Learning, Crowdsourcing, Boot-strapping
Name disambiguation has been viewed as a very impor-tant problem in many domains, such as digital libraries, social media management [2] and people search systems. Given a large set of entity names, the task is to deter-mine which names are referring to the same underlying en-tity. To solve this problem, there are many approaches pro-posed in the recent years [5, 8, 7, 6]. Generally existing methods mainly fall into three categories: supervised-based, unsupervised-based and constraint-based [9]. Among all the methods, the supervised-based approaches are considered to be the most effective ones [3]. However, their effectiveness are highly dependent on the amount of training data avail-able.

For a large amount of training examples, the annota-tion work is expensive and time-consuming requiring hours of manual inspection by domain experts. In response, re-searchers have exploited active learning techniques to help with the labeling effort problems. In [3], Ferrera et al . pro-posed an active sampling strategy based on association rules to discriminate the author names. Wang et al . [9] introduced the active name disambiguation problem and presented the ADANA method to select the data for human labeling. By carefully selecting the informative samples, these approaches can largely reduce the amount of the data needed for con-structing the training set. But the annotators still may get tired when asked to go through hundreds of hard-to-label samples in the labeling process, which is very tedious and error-prone. Moreover, previous active strategies for name disambiguation either select the data based on a single un-certainty measure [9] or just select the most potentially erro-neous ones [3]. These criteria perform exploitation focuses on regions that are difficult to learn and would overlook those highly representative samples (exploration). This sam-pling bias may significantly limit the active learning perfor-mance.

On the other hand, social computing through services such as Amazon Mechanical Turk (MTurk) have made it possi-ble for researchers to acquire sufficient quantities of crowd-sourced labels. Crowdsourcing distributes problem solving to a broader community for requesting annotation and en-able acquiring labeled data at less expensive cost.
In this paper, we propose a novel active learning algo-rithm, Active Data Augmentation, to exploit crowdsourcing techniques for name disambiguation. The proposed method combines discriminative features and crowdsourced label-ing in a bootstrapping framework. Our hypothesis is that by combining the two strategies, we could balance the ex-ploration and exploitation sampling. We first formulate the name disambiguation as a graph partitioning problem, and then we propose an algorithm to actively acquire la-beled data by combining discriminative feature labeling and crowdsourcing with bootstrapping. Our approach is able to achieve both exploration and exploitation advantages, and generate high quality training data at a minimal expense. The experimental results on two real-world datasets includ-ing DBLP and ArnettMiner demonstrate the effectiveness of our approach for the author name disambiguation task.
In our approach, we first introduce a conditional pair-wise graph model for the name disambiguation problem. Then we use a graph partition based method to make the disam-biguation decisions with some training samples. The train-ing datasets are collected using our proposed active data augmentation method. We address the problem in the digital libraries domain. Given a person name, let D = { d 1 , d 2 , ..., d n } denote a col-lection of paper records which contain the name. By viewing each paper record d i as a data point, it can be represented using l disambiguation features x i = { x 1 i , x 2 i , ..., x as coauthors, paper titles, topics of articles, emails, affili-ations etc. The objective of author name disambiguation is to group D into K clusters, where each cluster contains the references to a same author. Following the approaches proposed in [7, 6], we formalize the problem as a pairwise graph partition task. We generate a set of document feature pairs X = { ( x 1 , x 2 ) , ( x 2 , x 3 ) , ..., ( x n  X  1 observable variable. Let y ij  X  Y be a hidden variable, repre-senting whether two paper records refer to the same author, i.e. if y ij = 1, then d i and d j belong to the same author, otherwise, they belong to different authors. The generated model would first learn maximum entropy for pairwise bi-nary decisions, and then combine the information from the pairwise graph model using graph partitioning based meth-ods so as to achieve a good global and consistent decision. The complete model for the conditional distribution of all binary match variables Y given all the observable data X can be expressed as: where Y = y ij :  X  i, j and Z ( x ) = X f ( x i , x j , y ij ) is a set of l feature function for the document tion to ensure globally consistent configurations. The fea-tures for a pair of papers is described in Table 1.
Now, the problem is how to maximize the conditional probabilistic model (Eq. 1) with some training samples. Following [7], the inference of the model can be formulated as the problem of finding the graph partitions in which the nodes are the papers and the edge weights are the log clique potentials on the pair nodes ( x i , x j ) involved in their edges. Some methods, such as minimizing-disagreements correla-tions clustering [1, 10], Metropolis-Hastings (MH) [9] are in-Feature Description Title similarity between titles of x i and x j Coauthors indicator whether x i and x j share same author Venue whether x i and x j published in same venue Affiliation indicator whether x i and x j share same affiliation
NumCos number of authors shared by x i and x j troduced for the graph partitioning. We use the stochastic sampling to solve this problem, which is described in [6].
Given a bunch of unlabeled data, which samples should we select to query the user? In particular, in our problem, which document pairs ( d i , d j ) should be selected? And how can we get the labels from croudsourcing and discriminative feature labeling? The first problem can be addressed by using the active selection and the second is referred as acquiring labels based on two techniques.
The main purpose of discriminative features labeling is to identify some discriminative features which can help to determine whether the publications are written by the same author. Because we have not manually confirmed the labels, we refer to them as feature labels. Our idea is similar in spirit to the work proposed in [5], which intended to find  X  X ure cluster X  X r X  X tomic cluster X  X n which publications must be correctly grouped (high precision) but might be further grouped in the process of clustering (possible low recall). For a pair of feature vectors ( x i , x j ), we define the features similarity sim ( x i , x j ) as: wher e f m ( x i , x j )  X  [0 , 1], is the normalized score of m -th co-feature for the pair ( x i , x j ), and M is the number of co-features: Coauthors, Title, Venue, Affiliation and Num-Coauthor. Table 1 summarizes this in details. In each step, the top-s pairs ( x i , x j ), with high similarity scores will be selected to labeled by feature labeling, i.e. y ij = 1. To acquire crowdsourced labels, we use the service from Amazon X  X  Mechanical Turk. Users on the service received compensation ten cents for labeling a pair of papers with a list of paper information (title, authors, author affiliation, email, venue, and year). For each of these pairs, we ask five different users from AMT to label it as yes or no, which indi-cates whether the two authors are the same or not. Further-more, we require that each crowdsourced training samples at least receive the same label by two users, thereby providing more certainty in the acquired label. This acts as a simple quality control for filtering out bad data from disinterested or exploitative users.
A straightforward solution for active selection is to select the most uncertain document pairs. According to Eq. 4, we could have the probability of two documents belonging to the same cluster, i.e., If p ( y ij = 1 | x i , x j ) = 0 . 5, we say that the disambigua-tion model is the most uncertain about the document pair. p ( y ij = 1 | x i , x j ) = 1 and p ( y ij = 1 | x i , x tively, denote that the model is confident in that the two documents should be clustered together and should not be clustered. Based on the probability, we define an confidence score for a pair ( x i , x j ) as: An uncertain document pair will have a low confidence score in this case. Therefore, we select the k most uncertain doc-ument pairs with the lowest confidence scores according to Eq. 5.
Algorithm 1 illustrates our proposed technique with boot-strapping, which iteratively perform the training and eval-uation to augment the training data. Given an unlabeled document pair set U , let L denote the set of instances which return the top-s of sim ( x i , x j ). From this result, the top-s most confident predictions with discriminative feature la-beling are added to L . We then update U by removing all instances also found in L , leaving only unlabeled examples in U . The algorithm then iterates over the following steps. The top-k uncertain predictions are crowdsourced to obtain labels and then added into L . The algorithm terminates when the maximum number of iterations is reached or a given query budget is exhausted.
 Algorithm 1 Activ e Data Augmentation Algorithm: Aug-ments Training Data with Crowdsourcing 1: Input : Unlabeled set U , parameters s and k . 2: Output : A selected dataset L from U with labels. 3: Initialize L =  X  ; 4: Select the top-s confident instances and their labels L 5: L = L S L f , U = U X  L f . 6: Do 7: select top-k instances L c with lowest confidence 8: query and get their crowdsourced labels; 9: L = L S L c , U = U X  L c ; 10: Until 11: the augment process is stopped.
In this section, we first describe the datasets and the base-line methods. Then, we compare the performance of our approach with the baseline methods. Finally we analyze the effectiveness of discriminative feature labeling.
To evaluate our active data augmentation strategy, we use two collections of references derived from DBLP and ArnetMiner. These collections contain several ambiguous groups (groups of references with ambiguous author names). The first collection, hereafter referred to as DBLP, contains Table 2: Average results on DBLP dataset with different sampling strategies Active Data Augmentation, s =30 0.868 0.831 Table 3: Average results on ArnetMiner dataset with differ-ent sampling strategies
Active Data Augmentation, s =30 0.776 0.754 4,287 references associated with 220 distinct authors. This means an average of approximately 20 papers per author. Its original version was created by Han et al . [4]. The second collection, hereafter referred to as ArnetMiner, was collected from the ArnetMiner.org [8] and labeled by more than 30 annotators. It contains 6,370 papers with 100 author names. Each paper is associated with a set of attributes: coauthor lists, title, publication venue, publication year, references, paper content, and affiliations.
We compare the proposed approach with baseline results, with seed sets generated using discriminative feature label-ing and labels from crowdsourcing. For each dataset, when evaluating the effectiveness of discriminative feature label-ing, we use some number of initial instances from discrim-inative feature labeling. When using crowdsourced labels, we requested a total of 31,000 labels on 12,000 pairs of doc-uments that were randomly chosen from each dataset, re-spectively. After employing the validation steps described in subsection 2.2.1, around 9,500 labels remained for each dataset. Our method selects the instances using the un-certainty strategy and uses 30 initial instances from fea-ture labeling. We use four baseline methods in our experi-ments: (1) Random Selection: randomly select the instances to be labeled without initial seed data; (2) Random Selec-tion, s =30: randomly select the instances to be labeled with 30 initial instances from discriminative feature labeling; (3) Active Associative Sampling: the method used in [3]; (4) Active Selection: select the instances using uncertainty mea-sure without seed data. At each iteration, the training set is augmented with a number of k instances.
In order to evaluate the effectiveness of our method, we use two evaluation metrics: accuracy and Macro-F1. Accu-racy is the proportion of correctly disambiguated references. For Macro-F1, the performances are first calculated for in-dividual author names and then averaged over all authors. In particular, we compare the results of the proposed al-gorithm with the baseline methods. Tables 2 and 3 show the results of name disambiguation on the two datasets af-Figur e 1: Performance comparison on DBLP and Arnet-Miner datasets.
Figur e 2: Effect of S on DBLP and ArnetMiner datasets. ter 30% of training data are labeled. The proposed approach achieves an increase of at least 4.6% in accuracy and 3.9% in Macro-F1. Figures 1a and 1b show the variation of Macro-F1 score with the number of queries. We make the following observations: (1) starting with some seed data, the proposed Active Data Augmentation approach outperforms all base-line methods; (2) with some initial seed data, the random selection methods improve the performance but still results in increased error; (3) the method with single uncertainty strategy does not perform well, even comparing with the random selection with seed labels; (4) the active associative sampling cannot achieve much improvement. This indicates that a selection strategy using only discriminative feature labeling is insufficient and a strategy by considering both local and global information is necessary. Likewise, using only crowdsourced labels could not improve the results of active selection either, but combining discriminative feature labeling with crowdsourcing in our approach improves the results significantly.
We now study how the choice of the parameter s affects the performance. With fixed random sampling and active learning settings, we vary s , and observe the changes in the F-1 scores after 30% of training data are labeled. As shown in Figure 2a, as s increases from 0 to 30, the F-1 performance improves in both random and active settings. Similar trends can be observed in Figure 2b with a peak value 70. This im-plies that exploiting feature labeling to analyze the reviewer expertise can improve the classification accuracy. However, after it past a threshold, the accuracy tends to decrease. This might be due to over-fitting the training data with too much discriminative feature labeling.
We have present an unified framework for active name disambiguation by combining crowdsourcing and discrimina-tive feature labeling. This bootstrapping method balances the exploration and exploitation advantages of both tech-niques, thereby improving the overall quality of the training data at minimal expense. Experimental results on two dif-ferent genres of data sets showed that our proposed method outperforms the baseline methods. [1] N. Bansal, A. Blum, and S. Chawla. Correlation [2] Y. Cheng, K. Zhang, Y. Xie, A. Agrawal, W.-k. Liao, [3] A. A. Ferreira, R. Silva, M. A. Gon  X calves, A. Veloso, [4] H. Han, L. Giles, H. Zha, C. Li, and [5] H. Han, H. Zha, and C. L. Giles. Name [6] P. Kanani, A. McCallum, and C. Pal. Improving [7] A. McCallum and B. Wellner. Conditional models of [8] J. Tang, A. Fong, B. Wang, and J. Zhang. A unified [9] X. Wang, J. Tang, H. Cheng, and P. S. Yu. Adana: [10] Y. Yang, J. Wang, and A. E. Motter. Network
