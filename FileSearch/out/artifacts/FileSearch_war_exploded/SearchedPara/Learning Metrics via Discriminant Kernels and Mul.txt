 Zhihua Zhang zhzhang@cs.ust.hk Kowloon, Hong Kong The notion of similarity or dissimilarity plays a fun-damental role in machine learning and pattern recog-nition. For example, distance-based methods such as the k -means clustering algorithm and the near-est neighbor or k -nearest neighbor classification algo-rithms have to rely on some (dis)similarity measure for quantifying the pairwise (dis)similarity between data points. The performance of a classification (or clustering) algorithm typically depends significantly on the (dis)similarity measure used. Commonly used (dis)similarity measures are based on distance metrics. For many applications, Euclidean distance in the in-put space is not a good choice. Hence, more compli-cated distance metrics such as Mahalanobis distance, geodesic distance, chi-square distance and Hausdorff distance, have to be used.
 Instead of prespecifying a metric, a promising direction to pursue is metric learning, i.e., to learn an idealized metric from data. More specifically, one wants to em-bed input points into an idealized (metric) Euclidean space, on which the Euclidean distance accurately re-flects the dissimilarity between the corresponding in-put points. The embedded Euclidean space and points in it are also referred to as a feature space and fea-ture vectors. So the metric learning process can be regarded as an approach to feature extraction . Typically, metric learning consists of two processes. The first is to define an expected distance metric in the feature space with some information from the in-put space. The second is to embed input points into the feature space with linear or nonlinear mappings. Metric learning methods can be categorized along two dimensions. The first dimension is concerned with what information is used to form dissimilarities. The second dimension is concerned with what techniques are used to calculate the Euclidean embeddings of the dissimilarities.
 Along the first dimension, the present literature gen-erally uses one of three basic approaches. 1. Neighbor Information: In the unsupervised set-2. Label Information: In the supervised setting, the 3. Side Information: In the semi-supervised setting, Along the second dimension, Koontz and Fukunaga (1972) categorized the methods into three approaches. 1. Iterative Techniques seek to directly obtain the 2. Parametric Techniques seek to obtain a parame-3. Expansion Techniques are actually a class of para-In general, iterative techniques are used together with neighbor information, while parametric techniques are used together with label information. In this paper, we seek to propose a unifying approach to the metric learning problem according to the above categoriza-tions under the supervised setting. The rest of this paper is organized as follows. In section 2, we propose the basic problem of learning metrics from data. In section 3, we define discriminant kernels and develop a family of discriminant kernels and the induced dis-similarity matrices. In section 4, we discuss Euclidean embedding via MDS techniques. In section 5, our methods are tested on a synthetic dataset and some real datasets. The last section gives some concluding remarks. 2.1. Euclideanarity and Fisher Separability First of all, we introduce the following basic definition on Euclideanarity.
 Definition 1 (Gower &amp; Legendre, 1986) An m  X  m matrix A = [ a ij ] is Euclidean if m points P i ( i = 1 , . . . , m ) can be embedded in an Euclidean space such that the Euclidean distance between P i and P j is a ij . In this paper, we also refer to A with elements a ij as being Euclidean if and only if a matrix with a 1 2 ij is Euclidean. The following theorem provides the condi-tions for matrix A to be Euclidean.
 Theorem 1 (Gower &amp; Legendre, 1986) The matrix A = [ a ij ] is Euclidean if and only if H 0 BH is pos-itive semi-definite, where B is the matrix with ele-where I is the identity matrix, 1 m is the m  X  1 vector (1 , 1 , . . . , 1) 0 and s is a vector satisfying s 0 1 m Since Euclidean distance satisfies the triangle inequal-ity, a necessary condition for A to be Euclidean is that it be metric; that this is not also a sufficient condition (Gower &amp; Legendre, 1986).
 Inspired by the Fisher discriminant criterion, we define the notion of Fisher Separability here, as follows. Definition 2 An m  X  m matrix A = [ a ij ] is Fisher separable if a ij &lt; a lk where two points P i and P j corresponding to a ij belong to the same class, and points P l and P k corresponding to a lk belong to dif-ferent classes.
 In other words, a dissimilarity matrix is Fisher sepa-rable if its between-class dissimilarity is always larger than its within-class dissimilarity. 2.2. Kernel Trick for Dissimilarity Matrices Recently, kernel-based methods (Sch  X olkopf &amp; Smola, 2002; Vapnik, 1995) are increasingly being applied to data and information processing due to their concep-tual simplicity and theoretical potentiality. Kernel methods work over the feature space F , which is re-lated to an input space I by a nonlinear map  X  . In or-der to compute inner-products of the form  X  ( a ) 0  X  ( b ), we use a kernel function k ( a , b ) to represent which allows us to compute the value of the inner-product in F without carrying out the map  X  . This is sometimes referred to as the kernel trick . In most cases, we pay much attention to positive definite ker-k is positive definite if and only if the m  X  m Gram matrix (also called the kernel matrix ) K = [ k ( a i , a is positive semi-definite.
 feature vectors  X  ( a i ) and  X  ( a j ) can be defined as where k ij = k ( a i , a j ). Let  X  = [  X  2 ij ] where  X  2 defined in (1).  X  can be expressed in matrix form as, where k = ( k 11 , . . . , k mm ) 0 . Using Theorem 1 and we have Corollary 1 The dissimilarity matrix  X  is Euclidean if and only if k is a positive definite kernel. 2.3. Basic Problem Suppose we have an input set X  X  R d and an out-labels. We are given a training set D  X  X  X T with n points { ( x 1 , t 1 ) , . . . , ( x n , t n ) } , where t i belongs to class r . 1 Here and later, by  X  x  X  R l we denote the feature vector corresponding to the input point x , and by d ij and  X  d ij we denote the distances between points i and j in the input space and in the feature space, respectively. For simplicity, from now larities and the dissimilarity matrix .
 Simply speaking, metric learning wants to obtain  X  x  X  X  such that the Euclidean distance between  X  x i and  X  x j approximates  X  d ij , and consists of two processes: defin-ing  X  D and calculating  X  x  X  X . Since the performance of metric learning methods depends severely on the  X  D , we focus our main attention on the first process of metric learning in this paper.
 Among the three techniques of obtaining  X  x  X  X , MDS techniques (Borg &amp; Groenen, 1997) are widely used. For example, we generally employ classical MDS to develop iterative techniques (Roweis &amp; Saul, 2000; Tenenbaum et al., 2000) and use metric or nonmetric MDS models to develop parametric techniques (Cox &amp; Ferry, 1993; Webb, 1995; Zhang et al., 2003). If dis-similarity matrices are Euclidean or metric, we will be able to obtain efficient implementations of the MDS methods.
 As analyzed in Section 2.2, the dissimilarity matri-ces induced from kernel matrices are Euclidean. On the other hand, Fisher separability is a very useful criterion for discriminant analysis and clustering. In most of the existent kernel methods, although the fea-ture vectors in the feature space are more likely to be linearly separable than the input points in the input space, the induced dissimilarity matrices do not nec-essarily satisfy our expected Fisher separability. In this paper, our concerned problem is on how to con-struct dissimilarity matrices with both Euclideanarity and Fisher separability. We will address this problem with the kernel trick. 2.4. Related Work Some work in the literature is related to our work. In the existent literature, the goal of using neighbor information, label information or side information is to follow Fisher separability to define  X  D = [  X  d ij ]. For example, Cox and Ferry (1993; 1995; 2003) seek to increase the between-class dissimilarity and decrease the within-class dissimilarity. However, the dissimilar-ity matrices do not still satisfy the Fisher separability. Moreover, the dissimilarity matrices are not guaran-teed to be metric and Euclidean. Zhang et al. (2003) defined the dissimilarity matrix with the Fisher sepa-rability. However, ones have not proven it to be Eu-clidean.
 Recently, Cristianini et al. (2002) considered the re-lationship between the input kernel matrix and the target kernel matrix, and presented the notion of the alignment of two kernel matrices. Based on this no-tion, some methods of learning the kernel matrix have been successively presented in the transductive or in-ductive setting (Cristianini et al., 2002; Lanckriet et al., 2002; Bousquet &amp; Herrmann, 2003; Kandola et al., 2002a; Kandola et al., 2002b). Our work differs from that of Cristianini et al. (2002) in that we de-velop a new kernel matrix via the input kernel matrix and the output kernel matrix, while they seek to mea-sure the correlation between these two kernel matrices. On the other hand, our work can also be regarded as a parametric model of learning the kernel matrix be-cause we directly obtain the coordinates of the feature vectors, and the inner products of these coordinates form an idealized kernel matrix. Compared with the above methods, the computational complexity of our model is lower. 3.1.. Tensor Product and Direct Sum Kernels There exist a number of different methods for con-structing new kernels from existing ones (Haussler, 1999). In this section, we consider two such exam-ples. Given x 1 , x 2  X  X and u 1 , u 2  X  U . If k 1 , k 2 kernels defined on X  X X and U X U respectively, then their tensor product (Wahba, 1990), k 1  X  k 2 , defined on ( X  X U )  X  ( X  X U ) as is called the tensor product kernel .
 Similarly, their direct sum, k 1  X  k 2 , defined on ( X  X  U )  X  ( X  X U ) as k 1  X  k 2 (( x 1 , u 1 ) , ( x 2 , u 2 )) = k 1 ( x 1 , x 2 is called the direct sum kernel . 3.2. Definition of Discriminant Kernels We observe the training set D  X  X  X T . In kernel methods, kernels are mostly defined on X  X  X . In this paper, our idea is to define kernels on the training set D using label information or side information. We define the respective tensor product and direct sum kernels on ( X  X T )  X  ( X  X T ) as, ( k 1  X  k 2 )(( x i , t i ) , ( x j , t j )) = k 1 ( x i , x and ( k 1  X  k 2 )(( x i , t i ) , ( x j , t j )) =  X  1 k 1 ( x i , x where  X  1 ,  X  2  X  0 are mixing weights. We call the kernels defined above discriminant kernels . We know that in the joint space containing the training set, the input points and their corresponding labels have differ-ent Euclidean characteristics. For example, the input points are generally continuous variables while the la-bels are discrete variables. Obviously, the Euclidean distance on this joint space is unexpected. Using the discriminant kernels, our goal here is to embed the joint space into a feature space, on which the Euclidean distance is idealized.
 For convenience, by ( k 1  X  k 2 ) ij and ( k 1  X  k 2 ) ij k pute the squared distances between feature vectors em-bedded with the discriminant kernels or Corollary 2 If k 1 and k 2 are positive definite kernels, or (6), is Euclidean.
 Proof. We know that both the discriminant kernels defined by (3) and (4) are positive definite since k 1 , k are positive definite. By Corollary 1,  X  D is Euclidean. 2 3.3. Construction of Discriminant Kernels Once the kernels k 1 and k 2 have been given, we can obtain the discriminant kernels using the tensor prod-uct or the direct sum. Here, we choose the Gaussian kernels or correlation kernels 2 as k 1 , as k 2 . In the above equations  X  &gt; 0 is a scaling con-stant, q is the degree of the polynomial, and  X  ( t i , t the Kronecker delta function, is defined such that  X  ( t i , t j ) = 1 when t i = t j and 0 otherwise. We set  X  1 =  X  2 = 1 2 , the discriminant kernels then become for the Gaussian kernels, and for the correlation kernels.
 From the definition of the trivial kernel on the label set T , the trivial kernel works well if we are given only the pairwise side information of the labels instead of the values of the labels. 3.4. Dissimilarity Matrices We regard the squared distances between feature vec-tors as our concerned dissimilarities. Corresponding to the discriminant kernels in (9)-(12), we obtain the dissimilarities and denote them as We can see that  X  (1) ij and  X  (2) ij are related to distances d ij  X  X  in the input space, while  X  to correlation coefficients  X  ij  X  X  in the input space. Fig-ures 1 and 2 illustrate the propositions of these dis-similarities, where  X  = 1 or q = 2. As  X  1  X   X  ij  X  1, it is easy to obtain the following theorem Theorem 2 The dissimilarity matrices  X  D ( k ) = [  X  ( k ) for k = 1 , . . . , 4 , where  X  &gt; 0 or q is a positive even number, are Euclidean and Fisher separable.
 Theorem 2 shows that the problem given in Section 2.3 has been successfully resolved by using our discrimi-nant kernels. Notice that the resultant dissimilarities  X  d ij  X  X  incorpo-rate information from both the input points ( x i  X  X ) and their corresponding class labels ( t i  X  X ). Moreover, they possess the property of Fisher separability. They are appropriate to be used in distance-based classifiers. The subsequent task is then to find the embeddings, inter-point distances are equal to (or approximative)  X  d ij  X  X  defined in the above section. Because clidean, a natural solution can be easily obtained by using the classical MDS, i.e., an iterative technique. However in the classification scenario, this may be in-tractable because for new points with unknown labels, the problem then is on how to determine  X  d ij in the first process. Fortunately, using parametric techniques (Koontz &amp; Fukunaga, 1972; Cox &amp; Ferry, 1993; Webb, 1995), we can avoid this problem. Here we employ an expansion technique proposed by Webb (1995). 4.1. Expansion Techniques Denote the mapping from the original input space R d to the embedded Euclidean space R l by f = ( f 1 , . . . , f l ) 0 . We assume that each f i is a linear com-bination of p basis functions: where W = [ w ji ] p  X  l contains the free parameters, and the basis functions  X  j ( x ) X  X  can be linear or nonlinear. The regression mapping (17) can be written in matrix form as by the regression mapping, we seek to minimize the squared error as where q ij ( W ) = k W 0 (  X  i  X   X  j ) k with  X  i =  X  ( x The so-called expansion techniques seek to minimize e 2 w.r.t. W , given the basis functions  X  ( x ). 4.2. Iterative Majorization Algorithm Here the iterative majorization algorithm (Borg &amp; Groenen, 1997) is used to address the above expan-sion model. The procedure for obtaining W can be summarized as follows: 4. Check for convergence. If not converged, set t = The details of the method can be found in (Zhang et al., 2003). The functions  X  ( x ) can be chosen to be different linear or nonlinear functions. In the first two experiments, we aim at dimensional-ity reduction and data visualization, and concern find-ing suitable low-dimensional features with which the inter-point distances approximate as well as possible the dissimilarities. We shall use the Gaussian radial basis functions as and set l = 2. In the third experiment, we apply our model to classification problems on the real data, and use l = p = q and  X  ( x ) = x . The width  X  is set to be the average distance of the labelled points to their class means. 5.1. Synthetic Data This data set consists of 300 two-dimensional points partitioned into two ring-like classes, each with 150 points (Figure 3(a)). The training subset consists of 20 points selected randomly from each class, while the remaining points form the test set. We take 10 ba-sis functions and randomly sample 5 points from each class of the training subset as centers of the basis func-tions.
 The experiments were run for 10 random initializa-tions of W (0) for the different dissimilarities  X  (1) ij  X  ij and  X   X  ij and  X  almost the same with respect to different initializa-tions. We show the results for one initialization in Figure 3, where the the feature vectors corresponding to all input points have been obtained via the above expansion technique after the iterative majorization algorithm has converged. We can see that when us-the obtain feature vectors are expected. So we feel that using the direct sum is more effective than using the tensor product for constructing the discriminant kernels. 5.2. Iris Data In this section, we test our methods on the Iris data set with 3 classes, each of which consists of 50 four-dimensional points, and use the dissimilarities  X  (2) ij and  X  ij . Firstly, we use only a subset of points to learn the metric and then perform the Euclidean embeddings of the remaining points using the above expansion tech-nique. The subset consists of 10 points randomly sam-pled from each class. Figure 4 and Figure 5 plot the decrease in e 2 versus the number of iterations in run-ning the iterative majorization procedure and the the obtained feature vectors, respectively. We chose 6 ba-sis functions and randomly sample 2 points from each class of the training subset as the centers of the ba-sis functions. As we can be see, the convergence of the iterative majorization is very fast, requiring less than 20 iterations. We also find that the feature vec-tors in two-dimensional space are more separable than the input points in the original four-dimensional space. However, our expected goal is still not achieved, i.e., we do not get that the between-class dissimilarity is larger than the within-class one.
 Secondly, we use all 150 points to learn the metric, and use classical MDS and the above expansion technique, respectively, to obtain the feature vectors. Figure 6 shows the two-dimensional feature vectors obtained ij (d) for  X  by classical MDS. Figure 7 shows the two-dimensional feature vectors obtained by the expansion technique, where we use 24 basis functions and randomly sam-ple the centers of these basis functions from the input points. Clearly, classical MDS can obtain the idealized feature vectors, whereas the expansion technique can not. This can be attributed to the approximation abil-ity of the used regression model in (17) and the local convergence of the iterative majorization algorithm. These problems are related to the second process of metric learning. Since this paper focuses mainly on the first process, here we do not give more discussions on the second process. This process will be further addressed in the future. 5.3. Applications to Real Datasets In this section, we perform experiments on five bench-ans diabetes, soybean, wine, Wisconsin breast cancer and ionosphere). The distance metric is learned using a small subset of the labelled points, whose sizes are detailed in (Zhang et al., 2003), while the remaining points are then used for testing. We apply the near-est mean (NM) and nearest neighbor (NN) classifiers on the input space and the feature space, respectively. The experiments were run for 10 random initializations of W (0) with the dissimilarities  X  (2) ij , and the average of classification results are reported in Table 1. It can be seen that the results on the feature space almost always outperform the results on the input space. In order to obtain idealized Euclidean representations of the input points, we have discussed the metric learn-ing methods by means of the kernel methods and MDS techniques. In the classification scenario, we defined discriminant kernels on the joint space of input and output spaces, and presented a specific family of the discriminant kernels. This family of the discriminant kernels is attractive because the induced metrics are Euclidean and Fisher separable.
 The author would like to thank Dit-Yan Yeung and James T. Kwok for fruitful discussions. Many thanks to the anonymous reviewers for their useful comments. Bach, F. R., &amp; Jordan, M. I. (2003). Learning graph-ical models with Mercer kernels. Advances in Neu-ral Information Processing Systems 15 . Cambridge, MA: MIT Press.
 Borg, I., &amp; Groenen, P. (1997). Modern multidimen-sional scaling . New York: Springer-Verlag.
 Bousquet, O., &amp; Herrmann, D. J. L. (2003). On the complexity of learning the kernel matrix. Advances in Neural Information Processing Systems 15 . Cam-bridge, MA: MIT Press.
 Cox, T. F., &amp; Ferry, G. (1993). Discriminant analysis using non-metric multidimensional scaling. Pattern Recognition , 26 , 145 X 153.
 Cristianini, N., Kandola, J., Elisseeff, A., &amp; Shawe-
Taylor, J. (2002). On kernel target alignment. Ad-vances in Neural Information Processing Systems 14 . Cambridge, MA: MIT Press.
 Gower, J. C., &amp; Legendre, P. (1986). Metric and Euclidean properties of dissimilarities coefficients. Journal of Classification , 3 , 5 X 48.
 Haussler, D. (1999). Convolution kernels on dis-crete structures (Technical Report UCSC-CRL-99-10). Department of Computer Science, University of California at Santa Cruz.
 Kandola, J., Shawe-Taylor, J., &amp; Cristianini, N. (2002a). On the extensions of kernel alignment (Technical Report 2002-120). NeuroCOLT.
 Kandola, J., Shawe-Taylor, J., &amp; Cristianini, N. (2002b). Optimizing kernel alignment over combina-tions of kernels (Technical Report 2002-121). Neu-roCOLT.
 Koontz, W. L. G., &amp; Fukunaga, K. (1972). A nonlinear feature extraction algorithm using distance informa-tion. IEEE Transactions on Computers , 21 , 56 X 63. Lanckriet, G. R. G., Cristianini, N., Ghaoui, L. E.,
Bartlett, P., &amp; Jordan, M. I. (2002). Learning the kernel matrix with semi-definite programming. The 19th International Conference on Machine Learn-ing .
 Roweis, S. T., &amp; Saul, L. K. (2000). Nonlinear di-mensionality reduction by locally linear embedding. Science , 290 , 2323 X 2326.
 Sch  X olkopf, B., &amp; Smola, A. J. (2002). Learning with kernels . The MIT Press.
 Tenenbaum, J. B., de Silva, V., &amp; Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. Science , 290 , 2319 X 2323. Vapnik, V. (1995). The nature of statistical learning theory . New York: Springer-Verlag.
 Wahba, G. (1990). Spline models for observational data . Philadelphia: SIAM.
 Webb, A. R. (1995). Multidimensional scaling by iter-ative majorization using radial basis functions. Pat-tern Recognition , 28 , 753 X 759.
 Xing, E. P., Ng, A. Y., Jordan, M. I., &amp; Russell, S. (2003). Distance metric learning, with application to clustering with side-information. Advances in Neu-ral Information Processing Systems 15 . Cambridge, MA: MIT Press.
 Zhang, Z., Kwok, J. T., &amp; Yeung, D. Y. (2003).
Parametric distance metric with label information (Technical Report HKUST-CS03-02). Department of Computer Science, Hong Kong University of Sci-
