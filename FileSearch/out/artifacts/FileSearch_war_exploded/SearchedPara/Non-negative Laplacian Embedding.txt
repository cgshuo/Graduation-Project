
In many real world tasks in data mining, information retrieval, and machine learning areas, data are represented in high dimensional space, which might intrinsically lie in a very low dimensional one. In addition, many data come in as a matrix of pairwise similarities, such as network data, protein interaction data. Meanwhile, unlabeled data are much easier to be obtained than labeled data. Thus, it is challenging and useful to develop unsupervised approaches to embed high dimensional data into low dimensional one.
From the data embedding point of view, there are two categories of embedding approaches. Approaches in the first category are to embed data into a linear space with linear transformation, such as principle component analysis (PCA). These approaches give out robust representations of data in a low dimension; However, they do not properly embed data which lie on non-linear manifold.

The second category approaches embed data in a non-linear manner. They include IsoMAP [25], Local Linear Embedding (LLE) [22], Local Tangent Space Alignment [27], etc. These embeddings have different purposes and objectives. But they can detect the nonlinear manifold where data lie
The above approaches all assume data points are rep-resented by feature vectors (attributes). In this paper, our emphasis is on graph embedding, i.e., the relationship among data points are represented by a matrix of pairwise simi-larities (which are viewed as edge weights of the graph). Laplacian Embedding is one of the most popular graph embedding method.

Laplacian Embedding and the related usage of eigenvec-tors of graph Laplace matrix is first developed in 1970s. It was called quadratic placement [16] of graph nodes in a space. The eigenvectors of graph Laplace matrix are used for graph partitioning and connectivity analysis [14]. This approach becomes popular in 1990s for circuit layout in VLSI community (see a review [1]), and graph partitioning [21] for domain decomposition, a key problem in distributed-memory computing.

Laplacian Embedding is now very popularly used [3] mainly due to its relation to graph clustering [15], [5], [24], [9]. In fact, the eigenvectors of Laplace matrix provides an approximation solution of the Ratio Cut clustering [5], and the generalized eigenvectors of the Laplace matrix provides an approximation solution of the Normalized Cut clustering [24] and min-max clustering [9].
 A Difficulty with Eigenvector Embedding
A main difficulty of using eigenvectors of the Laplace matrix to solve multi-way clustering problem is that the eigenvectors have mixed-sign entries while the clustering indicator vectors (that these eigenvectors approximate) are nonnegative. For two-way clustering, this is not a problem because a linear  X  -transformation[7] of the second eigen-vector (the Fiedler vector) and the first eigenvector leads to two genuine indicator vectors (vectors with positive and/or zero entries and each row has only one nonzero entry).
Because of this main difficulty, most applications resort to a two-step procedure[20] : (1) embedding the graph into the eigenvector space (Laplace embedding) and (2) clustering these embedded points using K-means clustering. This procedure provides an approximate solution to the clustering problem.
 Nonnegative Embedding Provides a Solution
In this paper, we propose a new approach. We propose to perform the Laplace embedding with nonnegative vectors, which can be directly interpreted as cluster membership indicator vectors. As a consequence, the nonnegative em-bedding also provides a more accurate solution to Ratio Cut clustering problem because the solution indicators now more resembles the desired cluster indicators. We call this new approach, the nonnegative Laplacian Embedding (NLE). NLE has the following property. It optimizes the Ratio Cut function with enforcing the nonnegative requirements rigorously. With the nonnegative representation of the cluster indicator, the embedding results can be interpreted as the posterior clustering probability. As a result, the cluster membership can be read off from the embedding coordinates immediately (see  X  4).

Second, our NLE method has soft clustering capabil-ity (see  X  IX-A) where a data point could be fractionally assigned to several clusters. This capability is especially important for many real-life data which come with much noise. For these data, not every data point clearly and uniquely belongs to one cluster (pattern). The soft clustering capability is lacking in standard spectral clustering and K-means clustering.

Our approach requires a solver which optimizes a quadratic function with both orthonormal and nonnegative constraints. The feasible domain of such optimization prob-lem is highly non-linear and non-convex. In this paper, we also propose an efficient algorithm to address this problem.
In the remainder of the paper, we first transform the minimization problems of the embedding (  X  2) and Ratio Cut (  X  3) into a maximization problem of a well-behaved positive definite function (  X  4). In order to generalize the problem definition, in  X  5, we prove the similarity matrix (graph matrix) with mixed signs can also be applied for Laplacian embedding. After that, we present the NLE algorithm (  X  6) and prove rigorously the correctness and convergence of our algorithm (  X  7) using the theory of constrained optimization.
We illustrate the NLE algorithm and capability using an example of faces images in  X  9. In  X  10, we perform extensive experiments on five UCI datasets [2] and AT&amp;T face image dataset [23] to compare our NLE algorithm to standard spectral approach. We show that our NLE algorithm consistently gives out better objective function value of Laplacian embedding and the Ratio Cut clustering objec-tive. Meanwhile, our NLE method also improves clustering accuracy over the standard spectral approach.
 Brief Summary of Major Clustering Frameworks
In essence, our line of clustering framework is to show that the Ratio Cut clustering objective can be written as an optimization of quadratic function with nonnegative constraints and orthogonality constraints; If we retain or-thogonality while ignore the nonnegativity, the solution is the standard Laplacian embedding using eigenvectors. This has been the way spectral clustering is developed so far. However, if we retain nonnegativity rigorously and enforce the orthogonality approximately, the solution is the NLE proposed in this paper.
 We note that this clustering framework is similar to the K-means clustering  X  PCA  X  Nonnegative Matrix Factoriza-tion(NMF) [17], [18] framework [8]. It has been shown [26], [7], [8] that K-means clustering objective can be written as the maximization of a quadratic function with nonnegativity and orthogonality constraints; If we retain orthogonality while ignore the nonnegativity, the solution is PCA [26], [7]. However, if we retain nonnegativity rigorously and enforce orthogonality approximately, the solution is NMF[7]. Several further developments using NMF for clustering are convex NMF [12], orthogonal NMF [13], and equivalence between NMF and probabilistic latent semantic indexing [10]. For recent surveys of NMF see [4], [19].
 We start with a brief introduction to Laplacian embedding. The input data is a matrix W of pairwise similarities among n objects. We view W as the edge weights on a graph with n nodes. The task is to embed the nodes of the graph into 1 -D space with coordinates ( x 1 ,  X  X  X  ,x n ) . The objective is that if i, j are similar ( i.e. , w ij is large), they should be adjacent in embedded space, i.e. , ( x i  X  x j ) 2 should be small. This can be achieved by minimizing [16] where D = diag( d 1 ,  X  X  X  ,d n ) and d i = j W ij .
The minimization of ij ( x i  X  x j ) 2 w ij would get x i if there is no constraint on the magnitude of the vector x Therefore, we impose the normalization i x 2 i =1 .The original objective function invariant if we replace x i by x + constant. Thus the solution is not unique. To fix this uncertainty, we can adjust the constant such that x i =0 ( x i is centered around 0). Thus x i have mixed signs. With these two constraints: the solution of minimizing the embedding objective is given by the eigenvectors of The matrix L = D  X  W is called graph Laplacian. This is because L is a discrete form of the Laplace operator In mathematical physics, a partial differential operator is not defined unless the boundary condition are specified  X  dif-ferent boundary conditions leads to different solutions. The graph Laplacian here is the discretized form of Laplacian operator with the Von Neumann boundary condition, i.e., the derivatives along the boundary are zero. (The discretized form of Laplacian operator with the Dirichlet boundary condition has slightly different form.) Because of the Von Neumann boundary condition, the solution is invariant w.r.t. an additive constant. As a consequence, the solution contains the constant eigenvector, the first eigenvector with eigen-value zero. (see [11] for details).
 Multi-dimensional Embedding
This embedding can be generalized to embedding in k -dimensional space, with coordinates r i  X  k .Let || r i  X  r be the Euclidean distance between nodes i, j . The embed-ding is obtained by optimizing In order to prevent R  X  0 , we impose the normaliza-tion constraints RR T = I . To fix the uncertainty due to the shift invariance, we further impose the constraint by eigenvectors: R =( f 1 ,  X  X  X  , f k ) T . This is called spectral Laplacian embedding (spectral means using eigenvectors). Let Q = R T  X  n  X  p , the spectral Laplacian embedding can be formally cast as an optimization problem:
The true power of Laplacian embedding is the clustering capability. Here we briefly outline the somehow often ne-glected, but fundamentally important relationship between the Ratio Cut spectral clustering [5] and Laplacian embed-ding. In fact, these two things are identical !
In clustering/partitioning a graph, the most popular ob-jective is min-cut, which cuts the graph G into A, B such that the cross-cut similarity (weight) s ( A, B )= i  X  A j  X  B w ij is minimized. Without size balancing, the mincut will often cut a very small subgraph out, leading to two highly unbalanced subgraphs. The first solution to this problem is developed in curcuit placement field by Cheng and Wei [6] who proposed to minimize the following ratio cut objective function Note | G | is a constant and drops out. Hagen and Kahng[15] later show that Fiedler vector (2nd eigenvector of the graph Laplacian) provides an effective solution. Chan et al. [5] generalized this two-way clustering to multi-way Ration Cut clustering: divide the nodes of G into K disjoint clusters {
C p } by minimizing the objective function: h k = { 0 , 1 } n be an indicator vector for cluster C k , i.e. , h ( i )=1 ,if x i belongs to the cluster C k ; h k ( i )=0 , otherwise. They show that Theorem 1: The Ratio Cut objective can be written as, where H =( h 1 / || h 1 || ,  X  X  X  , h K / || h K || ) . ratio cut problem becomes Chan et al. also discussed the embedding of this function, which is identical to the Laplacian embedding of Eq.(4) with the same orthogonality constraints. Shi and Malik [24] further developed this into normalized cut clustering. Ding et al, [9] further developed this into the min-max cut clustering.
A simple and widely adopted algorithm for solving spec-tral clustering has two steps: (1) compute the eigenvectors of L = D  X  W for Laplacian embedding; (2) do K-means clustering in the eigenspace to obtain clusters.

The second step is necessary because the eigenvector solution Q has mixed signs and the clusters cannot be identified directly. This is a generic difficulty of multi-way spectral clustering.

In all previous working on spectral clustering, the nonneg-ativity of the cluster indicator H are ignored. On the other hand, a nonnegative solution by enforcing the constraint H  X  0 has two direct benefits: (1) we can obtain cluster assignments directly. (2) we obtain more accurate solution because the nonnegative solution resembles the desired cluster indicators.
 In this paper, we propose the Nonnegative Laplacian Embedding (NLE) approach. In NLE, we rigorously enforce the nonnegativity constraint. The most important benefit of nonnegative embedding is that the cluster membership can be read off from solution Q immediately: x i belongs to the cluster C k , where k corresponds to the largest component in the i -th row of Q , In fact, we may view the i -th row of Q as the posterior probability that object i belongs to different clusters.
Formally, the optimization of Eq.(4) is identical to because the  X  term Tr Q T  X IQ =  X  Tr I =  X n is a constant. We set  X  =  X  m to be the largest eigenvalue of L = D  X  W . W  X  D +  X I is positive definite, because W  X  D +  X I = k =1 (  X   X   X  k ) v k v T k . This 2-step transformation (change min to max and makes the objective positive definite) makes the optimization as a well-behaved problem. The algorithm to solve Eq.( 9) will be provided in  X  6.

In traditional Laplacian embedding, graph matrix ( i.e. similarity matrix) is required to be non-negative. Here we show that similarity matrix with mixed sign can also be applied for Laplacian Embedding, as well as NLE.
 Let W + and W  X  be the positive and negative part of W , respectively: W = W +  X  W  X  . For the positive part, we want to minimize the embedding distance so that the instances are similar with each other, But for the negative part, we maximize the embedding distance so that the instances are dissimilar , We can combine them together by minimizing the difference, Here we show that the similarity matrix can be shifted by any constant.
 Theorem 2: If q is a non-trivial eigenvector of graph Laplacian on similarity W , then q is also an eigenvector of graph Laplacian on similarity W +  X  E , where  X  is any constant and E is a matrix with all entries 1 with proper size.

Obviously the e (a single column with all ones) is an eigenvector of any graph Laplacian. The corresponding eigenvalue is 0 . Here,  X  X y non-trivial eigenvector X , we mean those eigenvectors which are not e .
 Proof . Since q is a non-trivial eigenvector of graph Laplacian on similarity W , ( D  X  W ) q =  X q. If the similarity matrix shifts by a constant, W = W +  X  E , then the corresponding graph Laplacian becomes: Notice that all non-trivial eigenvectors are orthogonal to the trivial eigenvector e , which indicates q is also an eigenvector of L .

Theorem 2 suggests that for any mix signed similarity matrix, we can add any constant, such that the similarity matrix is nonnegative, without changing the eigenvectors ( i.e. the embedding results remain the same).

Inspired from NMF algorithms, we solve the NLE prob-lem of Eq.(9) using the similar techniques, see discussions for the relationship with NMF in  X  VIII.
 A. NLE algorithm
The algorithm starts with an initial guess Q . It then iteratively updates Q until convergence using the updating rule: where and  X  + is the positive part of  X  , and similarly for  X   X 
Notice that the feasible domain of Eq.(9) is non-convex, indicating that our algorithm can only reach local solutions. However, we show in empirical study that our algorithm yields much better Ration Cut objective than standard spec-tral clustering with a statistical analysis over a large number of random trials.
 B. Computational complexity analysis
In the typical implementation of NLE algorithm, the com-putational complexity is O ( n 2 K ) [the complexity bottleneck is the computation of  X  in Eq.(12)], which is not suitable to large scale problems. However, one can easily incorporate the approximate decompositions such as Nystr  X  om decompo-sition, to reduce the problem to O ( nK 2 ) time complexity.
In this section, we show the correctness and convergence of our algorithm.

For correctness, we mean that the update yields a correct solution at convergence; The correctness of our algorithm is assured by the following theorem.

Theorem 3: Fixed points of Eq. (11) satisfy the KKT condition of the optimization problem of Eq.(7).
 Proof . We begin with the Lagrangian
L = Tr [ Q T ( W +  X I  X  D ) Q  X   X ( Q T Q  X  I )  X   X  Q ] , (13) where the Lagrange multiplier  X  enforces the orthogonality condition Q T Q = I and the Lagrange multiplier  X  enforces the nonnegativity of Q. The KKT complementary slackness condition (  X  X / X  X  ik ) Q ik =0 becomes Clearly, a fixed point of the update rule Eq. (11) satisfies This equation is mathematically identical to Eq. (14).
From Eq. (14), summing over j , we obtain  X  ii = [ Q
T ( W +  X I  X  D ) Q ] ii . To find the off-diagonal elements of  X  , we ignore the nonnegativity requirement and setting  X  X / X  X  =0 which leads to  X  ii =[ Q T ( W +  X I  X  D ) Q ] ii By combining these two results we obtain Eq.(9).
The convergence of our algorithm is assured by the following Theorem.

Theorem 4: Under the update rule of Eq. (11), the La-grangian function increases monotonically.
 Proof of Theorem 4 . We use the auxiliary function ap-proach [18]. An auxiliary function G ( H,  X  H ) of function L ( H ) satisfies G ( H, H )= L ( H ) ,G ( H,  X  H )  X  L ( H ) . We define Then by construction, we have This proves that L ( H ( t ) ) is monotonically increasing. The key steps in the remainder of the proof are: (1) Find an appropriate auxiliary function; (2) Find the global maxima of the auxiliary function. We write Eq.(15) as L = Tr [ Q T ( W +  X I ) Q + X   X  Q T Q  X  Q T DQ  X   X  + Q T Q ] . We can show that one auxiliary function of L is
Z ( H,  X  H )= using the inequality and a generic inequality where A, B, S, S &gt; 0 ,A = A T ,B = B T . We now find the global maxima of Z ( H )= G ( H,  X  H ) . The gradient is  X  X  ( H,  X  H )
The second derivative is negative definite. Thus Z ( H ) is a concave function in H and has a unique global maximum. This maximum is obtained by setting the first derivative to zero, yielding:
According to Eq. (16), H ( t +1) = H and H ( t ) =  X  H ,we see that Eq. (22) is the update rule of Eq. (11). Thus Eq. (17) always holds.  X 
The nonnegative Laplacian Embedding is inspired from the idea of NMF. Here we show that these two methods are connected.

Theorem 5: Eq. (9) is equivalent to the following, Proof . = W  X  D +  X I 2  X  2 Tr ( W  X  D +  X I ) QQ T + QQ T 2 Since W  X  D +  X I and QQ T 2 are constant (with the constraint Q T Q = I ), Eq.(23) is equivalent to or with the same constraints, which is identical to Eq. (9).
We illustrate the nonnegative Laplacian embedding using a simple dataset of 30 images from the AT&amp;T face database [23] (see the first three row of Fig. 1). Each person has 10 images with different expressions. Using the standard way, for each image, we reshape the image to a single vector to represent the image. For this experiment, since the pixel values of the images are non-negative, we use the inner product ( w ij = x T i x j ) of two images to calculate the similarity; an advantage of inner-product similarity is that there is no adjustable parameter.

We start NLE algorithm with random matrix Q, 0  X  Q  X  1 . We show the NLE embedding results at the 1st, 10-th, 50-th and 300-th iterations (see Fig. 2). The objective function value is also shown on y -axis. For each checkpoint, we use a 3D plot to show all 30 images (each image as a point) with the first, second, and third row of Q as x-, y-, and z-axis. Because we impose both non-negative and near orthogonal constraints on Q , all the data points are near the positive part of the axis.

From Fig. 2, we notice that the clustering structure becomes more and more clear as the objective function value increases.
 A. Soft clustering capability of NLE
In traditional spectral clustering, a data point must belong to one of the clusters  X  this is hard clustering. However, such hard clustering sometimes prevents us from detecting delicate cluster structure details in complex data. For exam-ple, in Fig. 1, we may add 10 images from other persons (shown as the bottom row) to the 30 images on the top. Traditional spectral clustering will assign these 10 images into one of the 3 clusters.

However, these 10 images do not belong to three existing clusters. Ideally, the clustering solution would exhibit this fact. We now demonstrate that this fact is revealed in our NLE approach. Our NLE has the soft clustering capability, i.e. , the solution Q can be viewed as posterior probability of the object to be assigned to each cluster. The NLE solution Q =[ q 1 ,q 2 ,q 3 ] is shown in Hinton diagram (see Fig. 3). In the figure the face images index i is sorted as following: i =1  X  X  X  10 for the 10 images shown in 1st row of Fig.1, i =11  X  X  X  20 for the 10 images shown in 2nd row of Fig.1, i =21  X  X  X  30 for the 10 images shown in 3rd row of Fig.1, and i =31  X  X  X  40 for the 10 images shown in 4th row of Fig.1. We plot the elements of solution Q in rectangles, the size of which denotes the value of the corresponding elements.

We see from Fig.3 that for the first 30 images, one of q k is very pronounced and other components negligible: the cluster distribution/assignment are very clear. For the last 10 images, none of them is clearly clustered into any clusters  X  indicating the soft clustering nature for these images. These images are outliers in this dataset, and our NLE algorithm can correctly detect them. We evaluate the performance of our NLE algorithm in 4 UCI datasets [2]: Dermatology, Soybean, Vehicle, and Zoo.
In experiments, our goal is to compare with the standard spectral approach (as explained in last paragraph of  X  3). Therefore, we initialize Q using the clustering solution of the standard spectral clustering: H is set to the cluster indicator and Q 0 = H +0 . 2 as the starting point.

In evaluation, we use clustering accuracy . Suppose we have N = n 1 + n 2 +  X  X  X  + n K data objects ( n 1 are known/observed to belong to class F 1 , etc.). They are clustered into K clusters. with m k = | C k | . This forms a contingency table T =( T kl ) , where T kl denotes the number of objects from class F k and have been clustered into cluster C l . Clearly, l T kl = n k and k T kl = m l The clustering accuracy is the percentage of objects been correctly clustered:  X  = k T kk /N . In practice, matching F k to C l is obtained by running the Hungarian algorithm for the optimal bipartite matching.
 A. Evolution of NLE algorithm
In Figs 4 and 5, we show NLE evolutions of two typical runs on two UCI ( dermatology and zoo ) datasets. The initial Q are set to be results in spectral clustering as explained above.

We observe that the NLE objective function values in-crease steadily as iteration proceeds. The clustering accuracy also improves with more iterations. These facts indicate that clustering quality is improved when the objective function value increases.
 B. Comparison with spectral clustering
We perform extensive evaluation of both NLE and spectral clustering on the 5 UCI datasets and the AT&amp;T dataset (See Table 1 for experimental setup details). We note that the standard spectral clustering results on a dataset are not deter-ministic, because the results of K-means on the eigenspace (the spectral Laplacian embedding) depend sensitively on the initialization. For this reason, we perform 1024 runs of K-means clustering on the eigenspace for each dataset. We also perform 1024 NLE computations and each of them is initialized from the spectral solution. We evaluate the performance as following. Define Best ( N ) to be the lowest Ratio Cut objective among N random trials for both of approaches (spectral clustering and NLE). Clearly, Best ( N ) improves (decreases) as we increase N . The results of experiments for different N are shown in Figure 6. [At smaller N , the results are averaged with multiple N -interval runs.] The results are shown in the right of Figures 6 (a-f). We compare the clustering accuracy using the same strategy (shown in left of Figures 6 (a-f)). For Ratio Cut objective, the best (minimum) value is subtracted from the original Ratio Cut objective.

In all 6 datasets, NLE results are consistently better than spectral clustering on average, in both terms of Ratio Cup objective and clustering accuracy.

In Table 2, we show the Ratio Cut objective function value and the corresponding clustering accuracy, picking the best result of the 1024 runs (here, the best means the lowest Ratio Cut objective function value, because this is an unsupervised learning). For all 4 datasets, NLE consistently gives lower (better) Ratio Cut objective function value and higher clustering accuracy.

In this paper, we propose a Nonnegative Laplacian Em-bedding (NLE) algorithm and prove the correctness and con-vergence of the algorithm. NLE gives nonnegative embed-ding results from which clustering structures of data can be read off immediately. A computationally efficient algorithm is developed to solve proposed NLE problems. Moreover, we prove the similarity matrix ( i.e. graph matrix) with mixed signs can also be applied for Laplacian embedding.
We demonstrate the cluster assignment advantage and soft-clustering capability of NLE algorithm by illustrations on face expression data and extensive experiments on five UCI datasets and one image dataset. Our approach consis-tently outperforms spectral clustering in terms of both Ratio Cut objective and clustering accuracy.
 Acknowledgment . This work is supported partially by NSF DMS-0844497 and NSF CCF-0830780 at UTA, and NSF DMS-0844513 and IIS-0546280 at FIU.

