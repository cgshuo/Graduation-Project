 Time series forecasting has indispensa ble importance in many practical data mining applications. It is an ongoing dynamic area of research and over the years various forecasting models have been developed in literature [1,2]. A major concern in this regard is to improve the p rediction accuracy of a model without sacrificing its flexibility, robustness, simplicity and efficiency. However, this is not at all an easy task and so far no single model alone can provide best forecasting results for all kinds of time series data [3,4].

Combining forecasts from conceptually different methods is a very effective way to improve the overall forecasting p recisions. The earliest use of this prac-tice started in 1969 with the monumental work of Bates and Granger [5]. Till then, numerous forecasts combination methods have been developed in litera-ture [6,7,8]. The precious role of model combination in time series forecasting can be credited to the following facts: (a) b y an adequate ensemble technique, the forecasting strengths of the participated models aggregate and their weaknesses diminish, thus enhancing the overall forecasting accuracy to a great extent, (b) often, there is a large uncertainty about the optimal forecasting model and in such situations combination strategies are the most appropriate alternatives to use, and (c) combining multiple forecast s can efficiently reduce errors arising from faulty assumptions, bias, or mistakes in the data [3].

The simple average is the most widely used forecasts combining technique. It is easy to understand, implement and interpret. However, this method is often criticized because it does not utilize the relative performances of the contributing models and is quite sensitive to the extreme errors [1,3]. As a result, other forms of averaging, e.g. trimmed mean, Winsorized mean, median, etc. have been studied in literature [9]. Another common method is the weighted linear combination of individual forecasts in w hich the weights are determined from the past forecast errors of the contributing models. But, this method completely ignores the possible relationships between two or more participating models and hence is not so adequate for combining nonstationary and chaotic data. Various modifications of this linear combination technique have also been suggested by researchers [9,10,11].

In this paper, we propose a weighted nonlinear framework for combining mul-tiple time series models. Our approach is partially motivated by the work of Freitas and Rodrigues [12]. The proposed technique considers individual fore-casts from different methods as well as th e correlations between pairs of fore-casts for combining. We consider three m odels, viz. Autoregressive Integrated Moving Average (ARIMA), Artificial Neural Network (ANN) and Elman ANN to build up the ensemble. An efficient methodology, based on a successive valida-tion approach is formulated for finding the appropriate combination weights. The effectiveness of the proposed technique is tested on three real-world time series (one stationary and two nonstaionary financial data). The forecasting accura-cies are evaluated in terms of the error measures: Mean Absolute Error (MAE), Mean Squared Error (MSE), and Average Relative Variance (ARV).

The rest of the paper is organized as follows. Section 2 describes a number of common forecasts combination techniques. Our proposed ensemble scheme is presented in Sect. 3. In Sect. 4, we descr ibe the three time series forecasting models, which are used here to build up the ensemble. Experimental results are reported in Sect. 5 and finally Sect. 6 concludes this paper. be its forecast obtained from the i th method ( i =1 , 2 ,...,N ). Then the series obtained from linearly combining these n forecasted series is given by: where, w i is the weight assigned to the i th forecasting method. To ensure un-biasedness, sometimes it is assumed that the weights add up to unity. Different combination techniques are developed in literature which are based on different weight assignment schemes; some important among them are discussed here:  X  In the simple average , all models are assigned equal weights, i.e. w  X  In the trimmed average , individual forecasts are combined by a simple arith- X  In the Winsorized average ,the i smallest and largest forecasts are selected  X  In the error-based combining, an individual weight is chosen to be inversely  X  In the outperformance method, the weight assignments are based on the  X  In the variance-based method, the optimal weights are determined by mini-All the combination techniques, discussed above are linear in nature. The lit-erature on nonlinear forecast combinat ion methods is very limited and further research works are required in this area [10]. Our ensemble technique is an extension of the usual linear combination method in order to deal with the possible correla tions between pairs of forecasts and is partially motivated from the work of Freitas and Rodrigues [12]. 3.1 Mathematical Description For simplicity, here we describe our ensemble technique for combining forecasts from three methods; but, it can be easily generalized . Let, the actual test dataset mean and standard deviation of  X  Y ( i ) respectively. Then the combined forecast of Y is defined as:  X  Y (c) =  X  y (c) 1 ,  X  y (c) 2 ,...,  X  y (c) N In (2), the nonlinear terms are included in calculating  X  y (c) k to take into account the correlation effects between two for ecasts. It should be noted that for com-bining n methods, there will be n 2 nonlinear terms in (2). 3.2 Optimization of the Combination Weights The combined forecast defined in (2) can be written in vector form as follows: where, The weights are to be optimized by mi nimizing the forecast SSE, given by: where, Now from (  X / X  w ) (SSE) = 0 and (  X / X   X  ) (SSE) = 0, we get the following system of linear equations: By solving (5), the optimal combination weights can be obtained as: These optimal weights are determinable if and only if all the matrix inverses, involved in (6) are well-defined. 3.3 Approach for Weights Determination The optimal weights in the proposed ensemble technique solely depend on the knowledge of the forecast SSE value. But, in practical applications it is unknown in advance, since the dataset Y to be forecasted is unknown. Due to this reason, we suggest a robust mechanism for estimating the combination weights from the training data. Here, we divide the available time series into a suitable number of pairs of training and validation subsets and determine the optimal weights for each pairs; the desired co mbination weights are then calculated as the mean of all these pairwise optimal weights. In this way, the past forecasting performances of the participating models are effectiv ely utilized for weights determination. The necessary steps of our ensemble scheme are outlined in Alg. 1.
 Algorithm 1. Weighted nonlinear ensemble of multiple forecasts In this paper, we consider three popular time series forecasting methods to build up our proposed ensemble. These three methods are briefly described here. 4.1 Autoregressive Integrated Moving Average (ARIMA) The ARIMA models are the most widely used methods for time series forecasting, which are developed by Box and Jenkins in 1970 [2]. These models are based on the assumption that the successive observations of a time series are linearly generated from the past values and a random noise process. Mathematically, an ARIMA( p, d, q ) model is represented as follows: where, The terms p, d, q are the model orders, which respectively refer to the autore-gressive , degree of differencing and moving average processes ; y t is the actual time series and t is a white noise process. In this model, a nonstationary time series is transformed to a stationary one by successively ( d times) differencing it [2,4]. A single differencing is often sufficient for practical applications. The ARIMA(0 , 1 , 0), i.e. y t  X  y t  X  1 = t is the popular Random Walk (RW) model which is frequently used in forecasting financial and stock-market data [4]. 4.2 Artificial Neural Networks (ANNs) ANNs are the most efficient computational intelligence models for time series forecasting [10]. Their outstanding characteristic is the nonlinear, nonparametric, data-driven and self-adaptive nature [4,13]. The Multilayer Perceptrons (MLPs) are the most popular ANN architectures in time series forecasting. MLPs are characterized by a feedforward network of an input layer, one or more hidden layers and an output layer, as depicted in Fig. 1. Each layer contains a number of nodes which are connected to those in the immediate next layer by acyclic links. In practical applications, usually a single hidden layer is used [4,10,13].
The notation ( p, h, q ) is commonly used to refer an ANN with p input, h hidden and q output nodes. The forecasting performance of an ANN model depends on a number of factors, e.g. the selection of a proper network architecture, training algorithm, activation functions, significant time lags, etc. However, no rigorous theoretical framework is available in t his regard and often some experimental guidelines are followed [13]. In this paper, we use popular model selection criteria, e.g. Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) [13,14] for selecting suitable ANN structures. The Resilient Propagation (RP) [15,16] is applied as the network training algorithm and the logistic and identity functions are used as the hidden and output layer activation functions, respectively.
 4.3 Elman Artificial Neural Networks (EANNs) Elman networks belong to the class of recurrent neural networks in which one extra layer, known as the context layer is introduced to recognize the spatial and temporal patterns in the input data [17]. The Elman networks contain two types of connections: feedforward and feedback. At every step, the outputs of the hidden layer are again fed back to the context layer, as shown in Fig. 2. This recurrence makes the network dynamic, so that it can perform non linear time-varying mappings of the associated nodes [16,17]. Unlike MLPs, there seems to be no general model selection guidelines in literature for the Elman ANNs [10]. However, it is a well-known fact that EANNs require much more hidden nodes than the simple feedforward ANNs in order to adequately model the temporal relationships [10,16]. In this paper, we use 24 hidden nodes and the training algorithm traingdx [16] for fitting EANNs.
 To empirically examine the performan ces of our proposed ensemble technique, three important real-world time series are used in this paper. These are the Wolfs sunspots, the daily closing price of S &amp; P 500 index and the exchange rates between US Dollar (USD) and Indian Rupee (INR) time series. These time series are obtained from the Time Series Data Library (TSDL) [18], the Yahoo! Finance [19] and the Pacific FX database [20], respectively and are described in Table 1. The natural logarithms of the S &amp; P data are used in our analysis. All three time series are divided into suitable training and testing sets. The training sets are used for fitting the three forecasting models as well as to build up the proposed ensemble; the testing sets are used to evaluate the out-of-sample forecasting performances of the fitted models and the ensemble.
 The experiments in this paper are performed using MATLAB. For fitting ANN and EANN models, the neural network toolbox [16] is used. Forecasting efficacies of the models are evaluated thr ough three well-known error statistics, viz. Mean Absolute Error (MAE), Mea n Squared Error (MSE), and Average Relative Variance (ARV), which are defined below: where, y t and  X  y t are the actual and forecasted observations, respectively; N is the size and  X  is the mean of the test set. For an efficient forecasting model, the values of these error measures are expected to be as less as possible.
The sunspots series is stationary with an approximate cycle of 11 years, as can be seen from Fig. 3(a). Following Zhang [4], the ARIMA(9 , 0 , 0) (i.e. AR(9)) Sunspots The annual number of observed sunspots (1700 X 1987). S &amp; P 500 Daily closing price of S &amp; P 500 index (2 Jan. 2004 X 31 Dec. 2007) Exchange Rate USD to INR exchange rates (1 July 2009 X 16 Sept. 2011) and the (7 , 5 , 1) ANN models are fitted to this time series. The EANN model is fitted with same numbers of input and output nodes as the ANN, but with 24 hidden nodes. For combining, we take base size = 41, validation window =20 and the number of iterations k =9 .

The S &amp; P and exchange rate are nonstationary financial series and both ex-hibit quite irregular patterns which can be observed from their respective time plots in Fig. 4(a) and Fig. 5(a). The RW-ARIMA model is most suitable for these type of time series 1 . For ANN modeling, the (8 , 6 , 1) and (6 , 6 , 1) net-work structures are used for S &amp; P and ex change rate, respectively. As usual, the fitted EANN models have the same numbers of input and output nodes as the corresponding ANN models, but 24 hidden nodes. For combining, we take base size = 200, validation window = 50, k = 12 for the S &amp; P data and base size = 165, validation window = 40, k = 10 for the exchange rate data.
In Table 2, we present the forecasting performances of ARIMA, ANN, EANN, simple average and the proposed ensemble scheme for all three time series.
From Table 2, it can be seen that our ensemble technique has provided low-est forecast errors among all methods. Moreover, the proposed technique has also achieved considerably better foreca sting accuracies than the simple average combination method, for all three time series. However, we have empirically ob-served that like the simple average, the performance of our ensemble method is also quite sensitive to the extreme errors of the component models.

In this paper, we use the term Forecast Diagram to refer the graph which shows the actual and forecasted observations of a time series. In each forecast diagram, the solid and dotted line respectively r epresents the test and forecasted time series. The forecast diagrams, obtained through our proposed ensemble method for sunspots, S &amp; P and exchange rate series are depicted in Fig. 3(b), Fig. 4(b) and Fig. 5(b), respectively. Improving the accuracy of time series fo recasting is a major area of concern in many practical applications. Although numerous forecasting methods have been developed during the past few decades, but it is often quite difficult to select the best among them. It has been observ ed by many researchers that combining multiple forecasts effectively reduces the prediction errors and hence provides considerably incr eased accuracy.

In this paper, we propose a novel nonlinear weighted ensemble technique for forecasts combination. It is an extension of the common linear combina-tion scheme in order to include possible correlation effects between the partic-ipating forecasts. An effici ent successive validation mechanism is suggested for determining the appropriate combination weights. The empirical results with three real-world time series and three forecasting methods demonstrate that our proposed technique significantly outperforms each individual method in terms of obtained forecast accuracies. Moreove r, it also provides considerably better results than the classic simple average combining technique. In future works, our ensemble mechanism can be further ex plored with other diverse forecasting models as well as other varieties of time series data.
 Acknowledgments. The first author sincerely expresses his profound gratitude to the Council of Scientific and Industrial Research (CSIR) for the obtained financial support, which helped a lot in performing this research work.
