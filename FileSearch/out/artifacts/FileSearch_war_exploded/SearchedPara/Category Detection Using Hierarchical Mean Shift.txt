 Many applications in surveillance, monitoring, scientific dis-covery, and data cleaning require the identification of anoma-lies. Although many methods have been developed to iden-tify statistically significant anomalies, a more difficult task is to identify anomalies that are both interesting and statis-tically significant. Category detection is an emerging area of machine learning that can help address this issue using a  X  X uman-in-the-loop X  approach. In this interactive setting, the algorithm asks the user to label a query data point un-der an existing category or declare the query data point to belong to a previously undiscovered category. The goal of category detection is to bring to the user X  X  attention a rep-resentative data point from each category in the data in as few queries as possible. In a data set with imbalanced cate-gories, the main challenge is in identifying the rare categories or anomalies; hence, the task is often referred to as rare cate-gory detection. We present a new approach to rare category detection based on hierarchical mean shift. In our approach, a hierarchy is created by repeatedly applying mean shift with an increasing bandwidth on the data. This hierarchy allows us to identify anomalies in the data set at different scales, which are then posed as queries to the user. The main ad-vantage of this methodology over existing approaches is that it does not require any knowledge of the dataset properties such as the total number of categories or the prior probabil-ities of the categories. Results on real-world data sets show that our hierarchical mean shift approach performs consis-tently better than previous techniques.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering ; G.3 [ Probability and Statistics ]: [Nonparametric statistics] Algorithms, Experimentation, Performance clustering, anomaly detection, category detection, mean shift
Many applications in surveillance, monitoring, scientific discovery, and data cleaning require the identification of anomalies. Ideally, these anomalies correspond to data points or events of interest, such as a disease outbreak in bio-surveillance or a network attack in intrusion detection. Al-though many methods have been developed to characterize anomalies as statistically unusual events, not all statistically significant anomalies are necessarily useful. In fact, many anomalies are simply uninteresting, corresponding to known sources of noise or known combinations of features that are irrelevant to actual events of interest.

Consider the following two examples. The first example, taken from [15], involves the task of scientific discovery from the Sloan Digital Sky Survey (SDSS) [17], which is a 5 year survey of the northern skies by ground-based telescopes. Most of the images in the SDSS capture known phenomena such as stars, comets, nebulae, etc. which have already been discovered. Anomalies in the data correspond to unusual or unknown objects which could potentially lead to new scien-tific discoveries. However, the majority of anomalies in the SDSS are of no interest to astronomers. These anomalies in-clude diffraction spikes, which are artifacts of the telescope, and satellite trails. Such anomalies clearly do not lead to new discoveries in astronomy. The anomalies of interest, such as unusual galaxies, are extremely rare and constitute a miniscule 0.001% of the entire data set. A similar task exists in the analysis of network log files. The IT infrastructure of a company can contain thousands of computers and devices networked together. These components are often equipped with monitoring agents that generate log files that capture characteristics of the network traffic. Statistical anomalies in these log files often correspond to uninteresting events such as those arising from events that are already known or expected, such as events arising from maintenance upgrades. A very small fraction of the log file anomalies correspond to actual network failures or attacks of interest. Identifying these meaningful anomalies would be beneficial for the di-agnosis of faults and the prevention of malicious attacks.
Thus, a challenging new task has emerged for the field of anomaly detection  X  identifying anomalies that are not only statistically significant but also interesting. Since the X  X nter-estingness X  of an anomaly is subjectively defined, a human-in-the-loop approach is needed. Category detection [15] is an emerging area of machine learning that can help address this issue. Category detection operates on a set of unla-beled examples S = { x 1 , x 2 , ...., x n } where x i  X  X  X  m distinct categories 1 labeled y i = { 1 , 2 ..., m } . The cate-gory detection algorithm asks the user to label a query data point under an existing category or declare the query data point to belong to a previously undiscovered category. The goal of category detection is to bring to the user X  X  atten-tion at least a single instance from each category in as few queries to the user as possible. Figure 1 illustrates the in-teractive category detection process. The main challenge i n this task is discovering the rare categories, which appear a s small, dense clusters or isolated outliers in the data set. T his task becomes especially challenging if the data set is dom-inated by a handful of disproportionately large categories , which makes the rare categories become extremely difficult to discover through manual inspection. As a result, categor y detection is often referred to as rare category detection. Figure 1: The interactive category detection loop
In this paper we present a new approach to rare category detection using a Hierarchical Mean Shift procedure. Mean Shift is a nonparametric clustering technique widely used i n computer vision for segmentation. In our approach, we use Mean Shift to discover cluster modes. By repeatedly apply-ing Mean Shift with increasing bandwidths, we can create a hierarchy of clusters at different scales and thus use this information to score each cluster by how  X  X nomalous X  it is. This score can then be used to rank the representative data points of each cluster for labeling by a user. The main ad-vantage of the proposed method over previous related work [15, 8] is twofold. First, our approach dramatically reduce s the number of queries to the user needed to discover all the categories in the data. Secondly, our approach does not re-quire any prior knowledge regarding the properties of the data set, such as the total number of clusters present in the data or the prior probability of a data point belonging to a cluster.
Category detection incorporates ideas from active learnin g [11] and semi-supervised learning [19, 1, 21] but work that investigates category detection itself is relatively spar se. We briefly review the main contributions in the area of category detection.

Category detection was first proposed by Pelleg et al. [15] through their Interleave algorithm, which assumes that the data is generated by a mixture model. Interleave starts with an entirely unlabeled data set and clusters it using EM for mixture models. Using the results of EM, the Interleave al-gorithm maintains, for every mixture component, a list of data points that are  X  X ost owned X  by that mixture compo-nent. This list is sorted in increasing order of the  X  X egree of ownership X  by the mixture component. Intuitively, the algorithm queries data points that are  X  X east owned X  by the components. It cycles through all the lists in a round robin fashion and selects the data point in the first position of eac h component X  X  list for user-labeling. Once the user has pro-vided the category labels, the Interleave algorithm clamps the labeled data points to their user-supplied labels befor e EM is applied again. This process continues until the user terminates the loop.

Despite some nice properties such as being model-independent and robust to noise in the data, Inter-leave has several shortcomings. First, the algorithm is sen -sitive to initial conditions as the EM clustering converges to a local optimum. Second, the algorithm requires an initial estimate of the number of classes in the data. Although the number of classes changes as the user provides feedback to the Interleave algorithm, this initial estimate of the numb er of classes is often not known in advance and an incorrect value at the outset can adversely affect the algorithm. Fi-nally, as pointed out by [8], Interleave requires the classe s to be separable in feature space.

He et al. [8] propose a nearest-neighbor based active learn-ing for rare category detection (NNDM) to address the sep-arability assumption in Interleave. NNDM uses an unsuper-vised local density differential sampling strategy. It make s use of nearest neighbors to measure the local density around each data point. The algorithm starts with an entirely un-labeled data set. In each iteration, the algorithm selects f or labeling the data point with the largest change in local den-sity. Like Interleave, NNDM needs to know the number of classes in advance. NNDM also requires the prior probabil-ity of a data point belonging to each class. Although NNDM is effective at discovering categories that overlap each oth er, we have noticed some undesirable behavior by NNDM in which the algorithm repeatedly queries data points (with large local density differential values) from the same clust er which has already been discovered.

Fine et al. [5] abstract the rare category detection prob-lem as an output identification task in a learning model. The learning model has an unknown target function f which maps every input in  X  to one of m output values. The output identification task is to find m inputs, one for each output value. The algorithm knows that the target function f is in a given function class F . The work assumes an unknown dis-tribution over the inputs and describes algorithms for many classes of functions. The algorithms are shown to have an expected sample bound of the order of m and log (1 / X  ) where  X  is the lower bound on the probability of each output class. The work similarly reports sample bounds in special cases like binary outputs i.e. m = 2, specific distributions (the uniform distribution) and a concept class defined over the Boolean hypercube { 0 , 1 } n .
Mean Shift is a non-parametric clustering algorithm pro-posed by Fukunaga and Hostetler [6]. It is based on the concept of nonparametric estimation of probability densit y functions in which the value of a density function at a point can be estimated using the sample observations that fall within a small region around that point. The popular Parzen window technique generalized this concept for density esti -mation. The Mean Shift algorithm is commonly used in vision for image segmentation [2, 3] but it has received rel-atively little attention in the machine learning community compared to other clustering techniques. We now describe Mean Shift based on the description and notation in Co-maniciu and Meer [3].
Let ( x 1 , . . . , x n ) be n independent and identically dis-tributed d -dimensional data points. Furthermore, let H be a symmetric positive definite d -by-d bandwidth matrix. We define the kernel function with bandwidth H to be
The term K ( x ) is a d -dimensional kernel function that is non-negative and integrates to one. It satisfies the condi-tions for asymptotic unbiasedness, consistency, and unifo rm consistency of the gradient of the density estimate [6]. The multivariate kernel density estimator with bandwidth ma-trix H computed at point x is given by In order to reduce the complexity of estimation, we assume H is proportional to the identity matrix H = h 2 I where h &gt; 0. Assuming H to be proportional to the identity ma-trix, the multivariate kernel density estimator in Equatio n 2 becomes
Various multivariate kernel functions can be used for den-sity estimation using Equation 3. In our work, we use a radially symmetric kernel obtained by rotating in R d a sym-metric univariate Gaussian kernel function with mean 0 and variance 1. The kernel is of the form We can write this kernel more abstractly as: where c k,d is a normalization constant that makes K ( x ) integrate to one and k ( x ) is called the profile of kernel K ( x ). As defined by [2], the profile k ( x ) is a function k : [0 ,  X  ]  X  X  such that K ( x ) = k ( k x k 2 ). For the Gaussian kernel func-tion, the profile is given by
The density estimator expression for the multivariate nor-mal kernel is:
The density gradient estimator can be obtained by the gradient of the density estimator in Equation 7 [3]. If we define k  X  ( x ) as the derivative of the profile, the gradient of the density estimator can be calculated as:  X   X  f h,K ( x )
The second term, shown in Equation 9 is the Mean Shift which is the difference between the weighted mean, using k for the weighting, and x .
Using the normal kernel, the Mean Shift vector becomes m h,K ( x ) is proportional to the normalized density gradi-ent and always points toward the steepest ascent direction of the density function.
Mean Shift can then be used for clustering or mode de-tection by iteratively shifting each data point towards the direction of its Mean Shift vector [6]. The amount of the shift is proportional to the gradient at the data point. Each data point x i , at the i th iteration, is shifted according to Equation 11. Eventually, the points converge to a station-ary point which corresponds to a mode of the density. We distinguish between two data sets required for the Mean Shift algorithm, which we call the reference and the query data sets. The reference data set consists of the data points that are used for the density estimation. The points in the reference data set do not move. The query data set consists of data points that are shifted by every iteration of Mean Shift. When the Mean Shift algorithm converges, the query data set contains the cluster modes. The query data set is not necessarily the same as the reference data set. When the query data set is the reference data set, the process is known as blurring [2] since it progressively blur s the data set on each iteration.
The bandwidth parameter h plays a critical role in the accuracy of the density estimation during Mean Shift. If the bandwidth is too small, it produces an undersmoothed density estimate, resulting in many small clusters. If the bandwidth is too large, the density estimate will be over-smoothed, resulting in a small number of very large clusters . A variety of techniques have been developed for bandwidth selection, such as the Maximal Smoothing Principle, Likeli -hood Cross Validation, Least Squares Cross Validation and Plug-in methods [9].
Our approach to category detection is based on Hierarchi-cal Mean Shift (HMS) [12, 4], which produces a hierarchy of clusters by repeatedly blurring the data. The repeated blurring of the data is accomplished by re-running Mean Shift with increasingly larger bandwidths, using at each it -eration the cluster centers from the previous iteration. HM S is closely related to scale-space theory [12], which is a mul ti-scale data representation framework designed to model how the human visual system sees details in an image as the image is progressively smoothed.
 One of the main reasons we chose HMS is because Mean Shift is a non-parametric clustering algorithm, which per-mits clusters to take on arbitrary shapes, unlike a Gaussian Mixture Model. Initially, we created a non-parametric ver-sion of the Interleave algorithm by replacing EM with Mean Shift and measuring cluster ownership by the total distance shifted to reach the cluster mode. However, this Interleave variant (called MS-Interleave) requires estimating the ba nd-width parameter h . After trying many different bandwidth estimation techniques [9] on a variety of data sets, we found that in many cases, the bandwidth estimates tend to under-smooth the data, producing a large number of modes when MS-Interleave algorithm produces many redundant queries for the same cluster as the algorithm believes that the data points belonging to that cluster belong to different cluster s.
In order to avoid selecting a single optimal bandwidth value, we used HMS, which repeatedly blurs the data with increasing bandwidths. HMS results in a cluster hierarchy similar to a dendrogram, except the clustering algorithm used is Mean Shift and not single-linkage clustering. This hierarchy contains extremely useful information regardin g the properties of the clusters such as their outlierness, co m-pactness, and isolation, which we will define in more detail in Section 4.3. We use these properties to determine which data points, that are representative of these clusters, sho uld be brought to the attention of the user. One other benefit to our HMS-based category detection is the fact that it does not require the user to input parameters requiring knowl-edge of the data set properties, such as an initial guess as to the number of clusters in the data (in the case of Interleave) or the category priors (in the case of NNDM).

Our HMS procedure for category detection consists of 3 phases: 1) data standardization, 2) building the cluster hi -erarchy and 3) querying data points. where modes that are within some  X  of each other are merged into the same cluster but the results for category detection were still poor.
The data sets are first standardized to prevent one di-mension dominating the distance calculations used in the analysis. Sphering is perfomed on the datasets using the following transformation [13].

In the equation above,  X  x is the sample mean, the columns of Q are the eigenvectors of the sample covariance matrix,  X  is a diagonal matrix of corresponding eigenvalues and x is the i th data point in the dataset.
In the second step, HMS builds the cluster hierarchy. Be-fore we can apply Mean Shift, we first compute the initial bandwidth value by finding the minimum non-zero distance h width matrix is computed as H = h 2 min  X  I . For convenience, we will refer to the bandwidth H as a scalar value h .
The lowest level of the hierarchy consists of the individual data points. The initial iteration of HMS applies Mean Shift with bandwidth h min to these individual data points. Each data point either remains as its own cluster of size one or it i s merged into a larger cluster. On the next iteration of HMS, the centers of these clusters become the data points at the next level of the hierarchy and the bandwidth is increased by multiplying the current bandwidth scalar value by k , which is the bandwidth increment. Mean Shift, with the increased bandwidth, is applied to these data points (which are the cluster centers one level down) to produce a new set of clus-ters. Mean Shift runs faster for each iteration of HMS as the number of data points at each level of the hierarchy de-creases quickly. This entire process continues until there is only one cluster left. In effect, we are repeatedly blurring the data by summarizing the data points in terms of their cluster centers.
 The algorithm to build the cluster hierarchy is described in Algorithm 1. In each iteration of Algorithm 1, Mean Shift is applied to a list of cluster centers stored in Hierarchy [ l ] .ClusterCenters to produce a clustering of these cluster centers. Initially, each individual data point is c on-sidered a cluster center. Hierarchical Mean Shift performs some bookkeeping that is not shown in Algorithm 1. For each cluster center at hierarchy level l , the algorithm stores the total distance it was moved by Mean Shift to a new cluster center at level ( l + 1). Furthermore, the algorithm maintains two cluster membership lists for each cluster at level ( l + 1). The first cluster membership list consists of the previous cluster centers at level l . The second cluster membership list consists of the original data points in the initial query data set.
One of the advantages of Hierarchical Mean Shift is its ability to preserve  X  X he structure and integrity of the out-liers in the clustering process X  [12]. Leung et al. define different cluster validity metrics such as lifetime, compac t-ness, isolation and outlierness to measure the  X  X oodness X  o f up Mean Shift, which allows this minimum distance compu-tation to be efficient.
Algorithm 1 : Hierarchical Mean Shift -Building the clustering hierarchy Let S be the data set and h be the bandwidth parameter ;
Set h = minimum non-zero distance between any two points in S ;
Set Hierarchy[0].ClusterCenters = S; l = 0; repeat until size(Hierarchy[l].ClusterCenters) == 1 ; a cluster in the hierarchy. This section describes two types of criteria, each of which can be used with Hierarchical Mean Shift resulting in two different approaches to rare category detection. We refer to these two approaches as Hierarchical Mean Shift -Outlierness (HMS-Out) and Hierarchical Mean Shift -Compactness-Isolation (HMS-CI).

The Outlierness criterion [12] is based on the concept of the lifetime of a cluster. The lifetime of a cluster is defined as the range of logarithmic scales over which the cluster survives. Similarly, we can define the lifetime in the Hi-erarchical Mean Shift scenario as the range of logarithmic bandwidths over which the cluster survives i.e. the loga-rithmic difference between the bandwidth when the cluster is formed and the bandwidth when the cluster is merged with other clusters. The Outlierness metric helps in iden-tifying outliers in the data. The cluster which has a long lifetime and fewer points will have high Outlierness value. Intuitively, rare categories are more likely to have high Ou t-lierness value as they tend to have fewer points and longer lifetime.

The cluster hierarchy built in the first phase is traversed and the list of unique clusters is formed. The same cluster can persist at more than one level of the hierarchy and all the different occurrences are treated as one unique cluster. The Outlierness value for each cluster is calculated and the lis t is sorted in the decreasing order of their Outlierness value. T o query a cluster, we ask the user to label its representative data point, which is the data point which has moved the least in terms of its Mean Shift distance before being assigned to the cluster center.

Compactness and isolation [12] are another set of criteria that can measure the quality of a cluster. Intuitively, a clu s-ter is well-defined if the distance between the data points inside the cluster is small (ie. it is compact) and those out-side is large (ie. it is isolated). Given p i is the cluster center of C i and h is the scalar bandwidth parameter, the isolation and compactness of C i can be calculated as
The isolation and compactness metric are close to one for a good cluster. These two criteria can be combined into one single Compactness  X  Isolation criterion (CI) for the clus-ter which is simply the sum of isolation and compactness values of the cluster. This criterion is calculated for each cluster in the cluster hierarchy and stored in a list sorted i n decreasing order. The CI criterion is more computationally expensive to compute than Outlierness. The worst case run time complexity for CI is O ( n 2 ) and occurs when CI needs to be computed for all clusters at the lowest level of the cluste r hierarchy.

The list created by using one of the above two criteria is traversed in the third phase. Clusters with higher validity criterion are selected first. The representative point of th e cluster is chosen as it was in the Outlierness criterion. If this representative point has already been selected for lab el-ing then the cluster is skipped; otherwise, the representat ive point is presented to the user for labelling. The entire algo -rithm is described in Algorithm 2.

Algorithm 2 : Hierarchical Mean Shift for rare category detection Build cluster hierarchy using Hierarchichal Mean Shift. Let ht be its height.

Let L = empty for i  X  ht to 0 do end
Sort list L in decreasing order of the validity criteria values of the clusters; while not all classes have been discovered do end
The sorted list may contain cluster entries with the same criterion values. These ties in criterion values can happen for clusters at the lower levels of the hierarchy as low band-width values lessen the effect of neighboring points for a given cluster resulting in high Compactness-Isolation val ues. Ties in Outlierness also occur when small compact clusters have low lifetimes due to being near much bigger clusters. In such cases a tiebreaker condition can be applied. For each cluster with the same criterion values, the distance betwee n the cluster mode and each data point already labeled by the expert is calculated. For our highest average distance (HAD) tiebreaker, the point with the highest average of the distances is picked first for labeling. This heuristic means that data points near already queried points are left to be queried later, thus decreasing the possibility of picking d ata points near the classes that have already been discovered. In this way, tiebreakers use the labels that have been pro-vided by the user to improve performance. Computing the HAD is typically very quick when k is small as the number of clusters with the same criterion value and the number of points already labeled by the expert are far less than n .
Each iteration of Mean Shift requires computing the dis-tance between an individual data point and all other data points. This distance computation is then performed over all the data points. If there are n data points of dimension d and if Mean Shift requires M iterations to converge, then the total time complexity of Mean Shift is O ( n 2 dM ). Mean Shift can be computationally expensive when n is large, hence sev-eral techniques, including locality-sensitive hashing [7 ] and an improved fast Gauss transform [20], have been developed to speed up the Mean Shift algorithm. Another technique for speeding up Mean Shift relies on the use of kd-trees [16], which is a multi-resolution space-partitioning data struc ture that can be used to dramatically reduce the O ( n 2 ) complex-ity of an all-pairs computation. The data points falling int o a partition defined by a leaf node of the kd-tree are treated as a homogenous group, thereby allowing an approximation to be computed. Thus, rather than computing the distance between data point x i and all other data points x j , j 6 = i , one can compare x i against groups corresponding to the par-titions of the kd-tree.

In our work, we used a single kd-tree approach which is similar to the optimization used for kernel regression by Deng et al. [10]. Like Mean Shift, kernel regression require s the calculation of the weighted average of points falling wi thin a hypersphere centered at the query point, with radius spec-ified by the bandwidth. This computation is shown in Equa-tion 16, where  X  y ( x q ) is the weighted average being computed, x is the query point, and w i ( x q ) is the weight of the i th dat-apoint with respect to the query point. Equation 16 for kernel regression is similar to Equation 9 fo r meanshift.

The use of kd-trees in kernel regression is based on the idea that if we have a group with k datapoints in which we know that all the weights in the group with respect to the query point x q are close to the same value w then approxi-mate values of P w i ( x q ) x i and P w i ( x q ) can be used. This approximate value can be obtained in constant time without the need to sum the individual members of the group. Imple-mentation of this approximation is straightforward by sup-plementing a kd-tree with extra information at each node. The nodes in the kd-tree act as hyper-rectangles enclosing a ll the points of the node and are treated as groups. To compute P w i ( x q ) x i and P w i ( x q ) a top-down search of a kd-tree is performed where at each node a decision is made either to treat all the points in the node as a group (cutoff) or recur-sively continue the search of the children (recurse). Consi d-ering x q and the hyper-rectangle of the node, the minimum and the maximum distance of x q from the hyper-rectangle i.e. D min and D max can be easily computed in turn, provid-ing the maximum and minimum possible weights w min and w max of any data points owned by the node. If the values of w min and w max are close enough then the cutoff option is taken. The algorithm makes a cutoff if where  X  is a system constant.

Wang et al. [18] propose a dual-tree optimization for mean-shift in which two kd-trees are built separately, one for the query points and the other for the reference points. These trees are traversed simultaneously and each referenc e tree node X  X  weight contribution to a query tree node is re-cursively updated by comparing these two nodes and their children. Once both trees have been traversed, the dual-tre e algorithm produces a memory-efficient cache of the Mean Shift values of all the query points. However, for every iter -ation of Mean Shift, the query tree has to be rebuilt since the query points have been changed while the reference tree remains fixed. The dual-tree method is ideally suited for datasets which need a small number of Mean Shift iterations [18]. The data sets in our work involve many iterations of Mean Shift. As a result, the dual-tree Mean Shift method tends to be slower than the single kd-tree method due to the frequent rebuilding of the query tree.
We evaluate our algorithms on the Abalone, Shuttle, Op-tical Digits, Optical Letters, Statlog and Yeast data sets taken from the UCI data repository [14]. We chose data sets that had a large number of class labels and had con-tinuous feature values. All the datasets except for Abalone and Yeast are sub-sampled. This sub-sampling is done to create imbalanced data sets that suit the rare category de-tection problem where some classes dominate the data and the remaining classes have only a few records. Shuttle is randomly sub-sampled from the original dataset to produce a smaller data set with 4000 examples. The original Optical Digits, Optical Letters, and Image Segmentation (Statlog) datasets contain almost the same number of examples for each class. Following the evaluation in [15], we changed the class distributions for the Optical Digits and Image Segmen -tation datasets into a geometric series with the largest cla ss owning half of the data and each subsequent class being half as small with the smallest class containing 8 examples. The Optical Letters data set has been sampled in such a way that the two largest classes own half the data and the sub-sequent pairs of classes being half as small with the smalles t class containing 8 examples. Table 1 has a summary of their properties. The number of records listed in Table 1 is after subsampling if the data sets were subsampled. The data sets are first standardized before running the experiments.We us e a bandwidth increment of k = 1 . 1 in all our experiments.
All the algorithms are evaluated based on the total num-ber of queries presented to the user before the user sees at least one example from all the classes in the data set. In these queries, the user is assumed to provide the correct class label for the queried data point. This evaluation met-ric is the standard used in rare category detection [15, 8]. The assumption with this performance metric is that a sin-Table 2: Number of queries needed to identify all classes for the HMS methods using the Highest Av-erage Distance (HAD) tiebreaker gle example would help the expert to generalize about the class to which the example belongs. Intuitively, the metric measures the effort the expert expends in order to discover all the classes in the data set. These results are summarized in Table 2.

Table 4 illustrates the category detection curves, which show the number of classes discovered as a function of the number of queries to the user. The area under the category detection curve can be used as an alternative metric for eval -uating the algorithms. The AUC metric favors algorithms that can quickly discover the majority of the classes but are slow to discover the last few remaining classes. Table 3 sum-marizes the AUC metric for the different algorithms, with the AUC being normalized by the total area in the graph. We compare the performance of  X  X ierarchical Mean Shift -Outlierness X  (HMS-Out),  X  X ierarchical Mean Shift -Com-pactnessIsolation X  (HMS-CI),  X  X ierarchical Mean Shift -O ut-lierness + HAD Tiebreaker (HMS-CI+Out), and  X  X ierarchi-cal Mean Shift -CompactnessIsolation + HAD Tiebreaker X  (HMS-CI+HAD) against existing rare category detection techniques NNDM [8] and Interleave [15] on the 6 data sets. Since the performance of Interleave is sensitive to the star t-ing conditions for EM, we ran Interleave 10 times on each data set with different random starting conditions and re-ported the best result. We set the category priors for NNDM to be their empirically observed values in the data sets.
The results in Table 2 and Table 4 show that the HMS approaches outperform both NNDM and Interleave. With-out the tiebreaker heuristic, the HMS variants outperform Interleave and NNDM on 4 out of the 6 datasets and come within one hint of Interleave on the Shuttle dataset. The Abalone data set was a difficult data set for category Table 3: Area under the category detection curves in Table 4, normalized by the total area in the graph. detection by HMS. From the category detection curves for Abalone in Table 4, we see that the HMS-CI method finds 18 of the total 20 classes more quickly than NNDM and Interleave, but fails to find the last 2 classes. The HMS-Out method finds 17 of 20 classes more quickly than NNDM and Interleave and also fails to discover the same 2 classes as HMS-CI. Analyzing the Abalone dataset reveals that these classes are small compact clusters that are very close to mor e than one large cluster. As a result, their Outlierness lifet ime is low as they are merged into one of the larger clusters early on in the cluster hierarchy building phase, resulting in a lo w Outlierness value. Hence, HMS-Out fails to find these clus-ters quickly. Similarly, in the case of HMS-CI, the points from nearby larger clusters contribute substantially to th e denominators of the compactness and isolation criteria of the small cluster, resulting in low values for Compactness-Isolation. The end result is that for Abalone, there are sev-eral data points that are tied either in terms of Outlierness or Compactness-Isolation near the bottom of the cluster hier-archy. The NNDM algorithm performs well on the Abalone data set because it is successful at discovering changes in local density.

If we add the HAD tiebreaker heuristic, the HMS category detection methods outperform both NNDM and Interleave on all six data sets. The HAD heuristic allows the category detection algorithm to query the data points that are tied with the same criterion values using a more intelligent orde r-ing. The improvement in category detection for the Abalone data set is due to the ability of the HAD heuristic to require the queries to be further away from all of the already queried data points. Without this tiebreaker, the HMS algorithms will query the data points with identical criterion values i n some arbitrary order.

The HMS-CI methods, both with and without the HAD tiebreaker heuristic, outperform the HMS-Out methods on all data sets except for Shuttle. The CI criteria appears to be a better criterion for selecting data points for category detection but it is more computationally expensive to com-pute than Outlierness.

We also experimented with a range of values for the band-width increment k . Due to space limitations, we do not include the results here. Our empirical results show that for a range of k = 1 . 1  X  1 . 9, the HMS-CI+HAD algorithm has little variation in the total number of hints needed to discover all classes. The running times initially decrease as k increases. However, at a certain point when k becomes large, the running time starts to increase due to the cost of computing the HAD tiebreaker heuristic.
For future work, we would like to improve the use of the user feedback for category detection. Currently, the feed-back is only used in the HAD tiebreaker heuristic. We would also like explore different presentation options, such as pr e-senting the entire cluster to the user, rather than just the representative point from the cluster. In addition, we woul d like to make the HMS algorithm even more efficient. The bottleneck in the algorithm is building the cluster hierarc hy. Although the data standardization and the cluster hierar-chy construction can be completed offline while the query selection can be performed online, we would like to make the algorithm scale to extremely large data sets. Finally, w e would like to investigate the theoretical issues surroundi ng the use of HMS for category detection.
We have proposed a rare category detection method us-ing Hierarchical Mean Shift. A cluster hierarchy is built by successively running Mean Shift first on the dataset and then on the cluster centers using a series of increasing band -width values. Then, data points are chosen for querying using two different criteria: Outlierness and Compactness-Isolation. For data points with identical criterion values , we use a highest average distance heuristic as a tiebreaker. This HMS approach has a number of attractive properties. It does not require the user to provide information regard-ing the dataset properties such as the number of classes or the prior probabilities of the classes. Furthermore, the no n-parametric nature of Mean Shift removes any restrictions on the shapes of the clusters. Finally, and most importantly, the HMS approach discovers all the categories in the data sets used in our experiments in much fewer queries than ex-isting approaches such as Interleave and NNDM. [1] Mikhail Bilenko, Sugato Basu, and Raymond J.
 [2] Yizong Cheng. Mean shift, mode seeking, and [3] Dorin Comaniciu and Peter Meer. Mean shift: A [4] Daniel Dementhon. Spatio-temporal segmentation of [5] Shai Fine and Yishay Mansour. Active sampling for [6] K. Fukunaga and L. Hostetler. The estimation of the [7] Bogdan Georgescu, Ilan Shimshoni, and Peter Meer. [8] Jingrui He and Jaime Carbonell.
 [9] M. Chris Jones, James S. Marron, and Simon J.
 [10] Andrew Moore Kan Deng. Multiresolution [11] Ashish Kapoor, Kristen Grauman, Raquel Urtasun, [12] Yee Leung, Jiang-She Zhang, and Zong-Ben Xu.
 [13] Wendy L. Martinez. Exploratory Data Analysis with [14] C.L. Blake D.J. Newman and C.J. Merz. UCI [15] Dan Pelleg and Andrew Moore. Active learning for [16] Franco P. Preparata and Michael Ian Shamos.
 [17] Alexander S. Szalay. The sloan digital sky survey. [18] Ping Wang, Dongryeol Lee, Alexander Gray, and [19] Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and [20] Changjiang Yang, Ramani Duraiswami, Nail A.
 [21] Liu Yang and Rong Jin. An efficient algorithm for to that of HMS-CI+HAD.
