 Bing, Microsoft Corporation Information Sciences Institute (ISI), University of Southern California Information Sciences Institute (ISI), University of Southern California
This article presents our work on constructing a corpus of news articles in which events are annotated for estimated bounds on their duration, and automatically learning from this and implicit temporal information and to measure inter-annotator agreement for these event baseline and approaching human performance. The methods described here should be applicable to other kinds of vague but substantive information in texts. 1. Introduction Consider the sentence from a news article:
How long did the meeting last? Our first inclination is to say we have no idea. But in fact we do have some idea. We know the meeting lasted more than ten seconds and less than one year. As we guess narrower and narrower bounds, our chances of being correct go down, but if we are correct, the utility of the information goes up. Just how accurately can we make duration judgments like this? How much agreement can we expect among people? Will it be possible to extract this kind of information from text automatically? and  X  X  have lived here for three years. X  But more often, such phrases are missing, and present-day natural language applications simply have to proceed without them. 1984; Moens and Steedman 1988; Zhou and Fikes 2002; Han and Lavie 2004; Hobbs and Pan 2004) and on temporal anchoring and event ordering in text (Hitzeman, Moens, and Grover 1995; Mani and Wilson 2000; Filatova and Hovy 2001; Boguraev and Ando 2005;
Mani et al. 2006; Lapata and Lascarides 2006). The uncertainty of temporal durations has been recognized as one of the most significant issues for temporal reasoning (Allen and Ferguson 1994). Chittaro and Montanari (2000) point out by way of example that we have to know how long a battery remains charged to decide when to replace it or to predict the effects of actions which refer to the battery charge as a precondition. and learn the vague and implicit duration information in natural language, and to perform reasoning over this information. Cyc has some fuzzy duration information, although it is not generally available; Rieger (1974) discusses the issue for less than a page; there has been work in fuzzy logic on representing and reasoning with imprecise durations (Godo and Vila 1995; Fortemps 1997). But none of these efforts make an attempt to collect human judgments on such durations or to extract them automatically from text.
 encoded in the descriptions of events, relying on their knowledge of the range of usual durations of types of events. This hitherto largely unexploited information is part of our commonsense knowledge. We can estimate roughly how long events of different types last and roughly how long situations of various sorts persist. We know that government policies typically last somewhere between one and ten years, and weather conditions fairly reliably persist between three hours and one day. We are often able to decide whether two events overlap or are in sequence by accessing this information. We know that if a war started yesterday, we can be pretty sure it is still going on today. If a hurricane started last year, we can be sure it is over by now.
 automatically. Our results can have a significant impact on computational linguistics applications like event anchoring and ordering in text (Mani and Schiffman 2007), event coreference (Bejan and Harabagiu 2010), question answering (Tao et al. 2010; Harabagiu and Bejan 2005), and other intelligent systems that would benefit from such temporal commonsense knowledge, for example, temporal reasoning (Zhou and Hripcsak 2007). text automatically, and to that end we first annotated the events in news articles with bounds on their durations. The corpus that we have annotated currently contains all 48 non-Wall-Street-Journal (non-WSJ) news articles (2,132 event instances), as well as 10 WSJ articles (156 event instances), from the TimeBank corpus annotated in TimeML (Pustejovsky et al. 2003). The non-WSJ articles (mainly political and disaster news) include both print and broadcast news that are from a variety of news sources, such as ABC, AP, CNN, and VOA. All the annotated data have already been integrated into the TimeBank corpus. 1 lines, including the annotation strategy and assumptions, and the representative event 728 classes we have categorized to minimize discrepant judgments between annotators. The method for measuring inter-annotator agreement when the judgments are intervals on a scale is described in Section 3. We will discuss how to integrate our event duration annotations to TimeML in Section 4. In Section 5 we show that machine learning tech-niques applied to the annotated data considerably outperform a baseline and approach human performance. 2. Annotation Guidelines and Event Classes
Every event to be annotated was already identified in the TimeBank corpus. In our project, annotators were asked to provide lower and upper bounds on the duration of the event, and a judgment of level of confidence in those estimates on a scale from 1 to 10. An interface was built to facilitate the annotation. Graphical output is displayed to enable us to visualize quickly the level of agreement among different annotators for each event. For example, Figure 1 shows the output of the annotations (three annotators) for the  X  X inished X  event in the sentence:
Figure 1 shows that the first annotator believed that the event lasts for minutes whereas the second annotator believed it could only last for several seconds. The third annotated the event as ranging from a few seconds to a few minutes. The confidence level of the annotators is generally subjective but as all three were higher than 5, it shows reasonable confidence. A logarithmic scale is used for the output (see Section 3.1 for details). 2.1 Annotation Instructions
Annotators were asked to make their judgments as intended readers of the article, using whatever world knowledge was relevant to an understanding of the article. They were asked to identify upper and lower bounds that would include 80% of the possible cases.
For example, rainstorms of 10 seconds or of 40 days and 40 nights might occur, but they are clearly anomalous and should be excluded. There are two strategies for considering the range of possibilities: 1. Pick the most probable scenario, and annotate its upper and lower bounds. 2. Pick the set of probable scenarios, and annotate the bounds of their upper We deemed the second to be the preferred strategy.
 environment needed to be considered before annotating. For example, there is a differ-ence in the duration of the watching events in the phrases watch a movie and watch a bird fly .
 by the entire article. This meant annotators were to read the entire article before starting to annotate. One may learn in the last paragraph, for example, that the demonstration event mentioned in the first paragraph lasted for three days, and that information was to be used for annotation.
 article. For example, an article from the fall of 1990 may talk about the coming war against Iraq. Today we know exactly how long that lasted. But annotators were asked to try to put themselves in the shoes of the 1990 readers of that article, and make their judgments accordingly. This was because we wanted people X  X  estimates of typical durations of events, rather than the exact durations.
 ential descriptions of events were identified initially. Annotators were asked to give the same duration ranges for such cases. For example, in the sentence during the demonstra-tion , people chanted antigovernment slogans , annotators were to give the same durations for the  X  X emonstration X  and  X  X hanted X  events. 2.2 Analysis
When the articles were completely annotated by the three annotators, the results were analyzed and the differences were reconciled. Differences in annotation could be due to the differences in interpretations of the event; we found that the vast majority of radi-cally different judgments could be categorized into a relatively small number of classes, however. Some of these correspond to aspectual features of events, which have been investigated intensively (e.g., Vendler 1967; Dowty, 1979; Moens and Steedman 1988;
Passonneau 1988; Giorgi and Pianesi 1997; Madden and Zwaan 2003; Smith 2005). We then developed guidelines to make annotators aware of these cases and to guide them in making the judgments (see the next section). There is a residual of gross discrepancies in annotators X  judgments that result from differences of opinion, for example, about how long a government policy is typically in effect. But the number of these discrepancies was surprisingly small.
 the agreement in the test set was greater than the agreement obtained when anno-tations were performed without the guidelines. (See Section 3.3 for the experimental results.) 2.3 Event Classes
Action vs. State : Actions involve change, such as those described by words like speaking , 730 and being at peace . When we have an event in the passive tense, sometimes there is an ambiguity about whether the event is a state or an action. For example, in does the word  X  X njured X  describe an action or a state? This matters because they will have different durations. The state begins with the action and lasts until the victim is healed. In the sentence, although retired usually indicates a state, it looks here more like the action by his company of retiring Farkas.
 1967; Dowty 1979); for example, action verbs are fine in the progressive form but progressives of stative verbs are usually odd. Another test can be applied to this specific case: Imagine someone says the sentence after the action has ended but the state is still persisting. Would they use the past or present tense? In the  X  X njured X  example, it is clear we would say  X  X hree people were injured in the attack, X  whereas we would say  X  X hree people are injured from the attack. X  Similarly, we would say  X  X arkas was retired X  rather than  X  X arkas is retired. X  specify which interpretation they are giving. If the annotator feels it X  X  too ambiguous to distinguish, annotations can be given for both interpretations.

Aspectual Events: Some events are aspects of larger events, such as their start or finish. Although they may seem instantaneous, we believe they should be considered to happen across some interval (i.e., the first or last sub-event of the larger event). For example, in the  X  X inished X  event should be considered as the last sub-event of the larger event (the  X  X leaning X  event), because it actually involves opening the door of the washer, taking out the clothes, closing the door, and so on. All this takes time. This interpretation will also give us more information on typical durations than if we simply assume such events are instantaneous.
 the gathering of people marks the  X  X eginning X  of the rally, and it generally takes time for a crowd of people to get together to start a rally.

Reporting Events: These are everywhere in the news. They can be direct quotes, taking exactly as long as the sentence takes to read, or they can be summarizations of long press conferences. We need to distinguish different cases: the speaker X  X  remarks, and a short duration should be given; if it is a long, complex sentence, then it X  X  more likely to be a summary of a long discussion or press conference, and a longer duration should be given. For example, consider the duration of the  X  X aid X  event should be short. In the second sentence everything that the spokesperson (here the police) has said is compiled into a single sentence by the reporter, and it is unlikely that the spokesperson said only a single sentence with all this information. Thus, it is reasonable to give longer duration to this  X  X aid X  event.

Multiple Events: Many occurrences of verbs and other event descriptors refer to multiple events, especially, but not exclusively, if the subject or object of the verb is plural. For example, in both single (i.e., destroyed one missile) and aggregate (i.e., destroyed all missiles) events happened. This was a significant source in disagreements in our first round of annotation. Because both judgments provide useful information, our current annotation interface allows the annotator to specify the event as multiple, and give durations for both the single and aggregate events.
  X  X uilt X  and  X  X roduction X  are both multiple events. The annotators were asked to give durations for both the single (i.e., built/produce one rifle) and aggregate (i.e., built/produce 75 million copies of the rifle) events. 732
Events Involving Negation: Negated events didn X  X  happen, so it may seem strange to specify their duration. But whenever negation is used, there is a certain class of events whose occurrence is being denied. Annotators should consider this class, and make a judgment about the likely duration of the events in it. In addition, there is the interval during which the nonoccurrence of the events holds. For example, in there is the typical amount of time of  X  X eing attacked, X  namely, the duration of a single attack, and a longer period of time of  X  X ot being attacked. X  The first is probably from seconds to minutes and the second from months to years. Similarly to multiple events, annotators were asked to give durations for both the event negated and the negation of that event.

Appearance Events . Verbs like  X  X eem X  and  X  X ppear X  usually indicate appearance events. The duration of this kind of event depends on the duration of the validity or availability of the evidence that causes one to have some impression at the time. Such an event begins when enough evidence has accumulated for one to make that guess or judgment, and it ends when either the evidence is contradicted or certainty is achieved. For example, in the  X  X ppears X  event lasts from when the archaeologist discovers enough evidence to make the conjecture until the time the conjecture is refuted or confirmed.
Positive Infinite Durations: These are states which continue essentially forever once they begin, for example,
Here the state continues for an infinite amount of time, and we allow this as a possible annotation. 3. Inter-Annotator Agreement
Although the graphical output of the annotations enables us to visualize quickly the level of agreement among different annotators for each event, a quantitative measure-ment of the agreement is needed.

Eugenio and Glass 2004), which factors out the agreement that is expected by chance, has become the de facto standard to assess inter-annotator agreement. It is computed as follows:
P ( A ) is the observed agreement among the annotators, and P ( E ) is the expected agree-ment, which is the probability that the annotators agree by chance.
 P ( E ) first. But those computations are not straightforward.
 P ( A ): What should count as agreement among annotators for our task?
P ( E ): What is the probability that the annotators agree by chance for our task? 3.1 What Should Count as Agreement?
Determining what should count as agreement is not only important for assessing inter-annotator agreement, but is also crucial for later evaluation of machine learning ex-periments. For example, for a given event with a known gold-standard duration range from 1 hour to 4 hours, if a machine learning program outputs a duration of 3 hours to 5 hours, how should we evaluate this result? convert all the temporal units to seconds. However, this would not correctly capture our intuitions about the relative relations between duration ranges. For example, the differ-ence between 1 second and 20 seconds is significant, whereas the difference between 1 year 1 second and 1 year 20 seconds is negligible. Consider the range from 1 year to 5 years and the range from 1 second to 5 seconds. The distance between 1 year and 5 years in seconds would be much larger than that between 1 second and 5 seconds, but intuitively, they represent the same level of uncertainty. In order to handle this problem, we use a logarithmic scale for our data. After first converting from temporal units to seconds, we then take the natural logarithms of these values. This use of a logarithmic scale also conforms to the idea of the importance of half orders of magnitude (HOM) (Hobbs 2000; Hobbs and Kreinovich 2001), which has been shown to have utility in commonsense reasoning and in several very different linguistic contexts.
 (either in nominal scales or ordinal scales); some can handle more general data, such as data in interval scales or ratio scales (Krippendorff 1980; Carletta 1996). However, none of the techniques directly apply to our data, which involves a range of durations from a lower bound to an upper bound.
 distribution for the event, where the area between the lower bound and the upper bound covers about 80% of the entire distribution area. It is natural to assume that the most likely duration in such a distribution is the mean or average duration, and that the distribution flattens out toward the upper and lower bounds. Thus, we use the normal or Gaussian distribution to model the distribution of possible durations.
 the mean and the standard deviation. For our duration distributions with given lower and upper bounds, the mean is the average of the bounds. Under the assumption that the area between lower and upper bounds covers 80% of the entire distribution area, the lower and upper bounds are each 1.28 standard deviations from the mean.
Then the standard deviation can be computed using either the upper bound ( X the lower bound ( X lower ) as follows: 734 the overlapping area between two normal distributions. 2 The agreement among many annotations is the average overlap of all the pairwise overlapping areas. For example, for a given event, suppose the two annotations are: 1. Lower: 10 minutes; upper: 30 minutes 2. Lower: 10 minutes; upper 2 hours
After converting to seconds and to the natural logarithmic scale, they become: 1. Lower: 6.39692; upper: 7.49554 2. Lower: 6.39692; upper: 8.88184
We then compute their means and standard deviations: 1.  X  1 = 6.94623;  X  1 = 0.42861 2.  X  2 = 7.63938;  X  2 = 0.96945
The distributions and their overlap are then as in Figure 2. The overlap or agreement ( P ( A )) is 0.508706. 3.2 Expected Agreement
What is the probability that the annotators agree by chance for our task? The first quick response to this question may be 0, if we consider all the possible durations from 1 second to 1,000 years or even positive infinity.

Siegel and Castellan (1988), we assume there exists one global distribution for our task (i.e., the duration ranges for all the events), and  X  X hance X  annotations would be consistent with this distribution. Thus, the baseline will be an annotator who knows the global distribution and annotates in accordance with it, but does not read the specific article being annotated. Therefore, we must compute the global distribution of the durations, in particular, of their means and their widths. This will be of interest not only in determining expected agreement, but also in terms of what it says about the genre of news articles and about fuzzy judgments in general.
 histogram is shown in Figure 3, where the horizontal axis represents the mean values in the natural logarithmic scale and the vertical axis represents the number of annotated durations with that mean.
 scale, which corresponds to about 1.5 minutes to 30 minutes. The other is from 14 to 17 in the natural logarithmic scale, which corresponds to about 8 days to 6 months. One could speculate that this bimodal distribution is because daily newspapers report short events that happened the day before and place them in the context of larger trends. The lowest point between the two peaks occurs at 11, which roughly corresponds to one day. annotated durations, and its histogram is shown in Figure 4, where the horizontal axis represents the width in the natural logarithmic scale and the vertical axis represents the number of annotated durations with that width.
 shows that for annotated durations, the most likely uncertainty factor from a mean or average duration is 3.5: because
This is the half orders of magnitude factor that Hobbs and Kreinovich (2001) argue gives the optimal granularity; making something three to four times bigger changes the way we interact with it.
 tions, we can then compute the expected agreement, that is, the probability that the annotators agree by chance, where the chance is based on this global distribution. Two 736 approaches were used to approximate this probability, both of which use a normal distribution to approximate the global distribution.
 as the mean of the mean distribution and the standard deviation as the mean standard deviation (this can be straightforwardly computed from the width distribution). We then compute the expected agreement by averaging all the agreement scores (overlaps) between this fixed distribution and each of the annotated duration distributions. randomly generated from the mean distribution and standard deviations are randomly computed from the width distribution. We then compute the expected agreement by averaging all the agreement scores (overlaps) between these 1,000 random distributions. did not read the article but only guessed on the basis of the global distribution. As it turns out, the results of the two approaches of computing the expected agreement are very close; they differ by less than 0.01: P ( E ) 1 = 0.1439, P ( E ) results of the second approach as the baseline in the next section. 3.3 Inter-Annotator Agreement Experiments
In order to see how effective our guidelines are, we conducted experiments to compare the inter-annotator agreement before and after annotators read the guidelines. (521 events, 1,563 annotated durations) which were all political and disaster news stories from ABC, APW, CNN, PRI, and VOA. The annotators annotated independently before reading the guidelines. The annotators were only given short instructions on what to annotate and one sample article with annotations. The second set (test set) contained 5 articles (125 events, 375 annotated durations) that were also political and disaster news stories from the same news sources. The annotators annotated independently after reading the guidelines.
 in two distributions and is thus a number between 0 and 1. The graphs show the answer to the question  X  X f we set the threshold for agreement at x , counting everything above x as agreement, what is the percentage y of inter-annotator agreement? X  The horizontal axis represents the overlap thresholds, and the vertical axis represents the agreement percentage, that is, the percentage of annotated durations that agree for given overlap thresholds. There are three lines in the graph. The top one (with circles) represents the after-guidelines agreement; the middle one (with triangles) represents the before-guidelines agreement; and the lowest one (with squares) represents the expected or baseline agreement. This graph shows that, for example, if we define agreement to be a 10% overlap or better (an overlap threshold of 0.1), we can get 0.8 agreement after reading the guidelines, 0.72 agreement before reading the guidelines, and 0.36 expected agreement with only the knowledge of the global distribution. From this graph, we can see that our guidelines are indeed effective in improving the inter-annotator agreement. shows the expected or baseline agreement, the before-guidelines agreement, and the after-guidelines agreement, as well as the kappa statistic computed from the after-738 ment actually gets marginally worse when the agreement criteria is very stringent (i.e., overlap  X  0.9), which indicates there really is no consensus at that level of agreement.
The overall agreement is relatively low. Thus in this article, we mainly focus on learn-ing coarse-grained event durations with much higher inter-annotator agreement. See Sections 5.2 and 5.3 for more details.
 4. Extending TimeML with Estimated Event Durations
This section describes the event classes in TimeML and how we can integrate our annotations of estimated event durations with them. This can enrich the expressiveness of TimeML, and provide natural language applications that use TimeML with this additional implicit event duration information for temporal reasoning. 4.1 TimeML and Its Event Classes
TimeML (Pustejovsky et al. 2003) is a rich specification language for event and tem-poral expressions in natural language text. Unlike most previous attempts at event and temporal specification, TimeML separates the representation of event and temporal expressions from the anchoring or ordering dependencies that may exist in a given text.
LINK. EVENT is a cover term for situations that happen or occur, and also those predicates describing states or circumstances in which something obtains or holds true.
TIMEX3, which extends TIMEX2 (Ferro 2001), is used to mark up explicit temporal expressions, such as time, dates, and durations. SIGNAL is used to annotate sections of text, typically function words that indicate how temporal objects are related to each other (e.g.,  X  X hen X ,  X  X uring X ,  X  X efore X ). The set of LINK tags encode various relations that exist between the temporal elements of a document, including three subtypes: TLINK (temporal links), SLINK (subordination links), and ALINK (aspectual links). each event belongs to one of the seven event classes, namely, reporting, perception, aspectual, I-action, I-state, state, and occurrence. The TimeML annotation guidelines give detailed descriptions for each of the classes:
Reporting. This class describes the action of a person or an organization declaring something, narrating an event, informing about an event, and so forth (e.g., say, report, tell, explain, state).

Perception. This class includes events involving the physical perception of another event (e.g., see, watch, view, hear).

Aspectual. This class focuses on different facets of event history, that is, initiation, reinitiation, termination, culmination, continuation (e.g., begin, stop, finish, continue).
I-Action. An I-Action is an Intensional Action. It introduces an event argument (which must be in the text explicitly) describing an intensional action or situation which does not necessarily actually happen but may be only desired or possible (e.g., attempt, try, promise).
 to alternative possible worlds (e.g., believe, intend, want).

State. This class describes circumstances in which something obtains or holds true (e.g., on board, kidnapped, peace). 740
Occurrence. This class includes all the many other kinds of events describing something that happens or occurs in the world (e.g., die, crash, build, sell). 4.2 Integrating Event Duration Annotations
Our event duration annotations can be integrated into TimeML by adding two more attributes to the EVENT tag for the lower bound and upper bound duration annotations (e.g.,  X  X owerBoundDuration X  and  X  X pperBoundDuration X  attributes).
 gration, we can try to share as much as possible our event classes as described in Section 2.3 with the existing ones in TimeML.
 ing, aspectual, state, and action/occurrence. For the other three event classes that only belong to TimeML (perception, I-action, I-state), the I-action and perception classes can be treated as special subclasses of the action/occurrence class, and the I-state class as a special subclass of the state class.
 multiple, negation, and positive infinite), however. The positive infinite class can be treated as a special subclass of the state class with a special duration annotation for positive infinity.
 aggregate events. Because the single event is usually more likely to be encountered in multiple documents, and thus the duration of the single event is usually more likely to be shared and re-used, to simplify the specification we can take only the duration annotation of the single events for the multiple event class, and the single event can be assigned with one of the seven TimeML event classes. For example, the  X  X estroyed X  event in the earlier example is assigned with the occurrence class in TimeBank. negated is usually more likely to be encountered in multiple documents, we can take only the duration annotation of the negated event for this class. 4.3 Annotation Consistency Evaluation
After the two corpora are integrated, it would be useful to evaluate how consistent the temporal relationship annotations are originally in TimeBank and in the newly integrated event duration annotations. Because not all the events in TimeBank are anchored exactly on a time line, for this consistency evaluation we have only evaluated the  X  X ncludes X  /  X  is included X  TLINK relationship: If event A includes event B ,the duration of event A should be no shorter than the duration of event B . Because the event duration annotation is a range, there are three possible relationships between the two duration annotations for event A [ a 1 , a 2 ] and event B [ b 1. A strictly includes B if a 1  X  b 2 (strictly compatible) 2. A possibly includes B if a 1  X  b 2 ,but a 2  X  b 1 (softly compatible) 3. A doesn X  X  include B if a 2 &lt; b 1 (incompatible)
We call the case (i) strictly compatible, (ii) softly compatible, and (iii) incompatible. source, and for each  X  X nclude X  TLINK relationship in the article, one of the three compatibility labels is assigned based on their definitions. The result shows that out of a total of 116  X  X nclude X  relationships, 59.5% are strictly compatible, 19.8% are softly compatible, and 20.7% are incompatible. We can merge the first two categories as compatible, which accounts for 79.3%.
 lines for the two corpora. For example, TimeBank bounds most of the I-State events to the article time, whereas our annotations usually give them a much longer duration X  for example, the event  X  X ppear X  in  X  Everyone appears to believe that somehow Cuba is going to change , X  and the event  X  X ope X  in  X  The quarantine hopes to staunch the flow of Iraqi oil . X  including those where multiple interpretations are possible, for example, in  X  This is quite an extraordinary story unfolding here , X  the event  X  X nfolding X  can be interpreted as either the start of the unfolding (TimeBank), or the entire process of the unfolding (duration annotation); Sometimes even when both corpora agree on the interpretation of the event, they may not agree on its duration, for example, in  X  But with the task-force investigation just getting under way, officials have been careful not to draw any firm it last momentarily (TimeBank) or for a couple of weeks (duration annotation)? Despite the difficulty with this event class, there exists some clear cases, for example, in  X  Anew
Essex County task force began delving Thursday into the slayings of 14 black women over the event  X  X elving X  should last much longer. 5. Learning Event Durations
It is highly unlikely that the relatively small amount of data we have annotated so far could support an automatic classification task at this fine granularity. But we have identified two classification tasks at a coarser granularity that we can hope to do well on and that have some independent utility. The first exploits the distribution shown in Figure 3, a bimodal distribution of events classifying them into those lasting less than a day and those lasting more than a day. The second coarse-grained task is the approximate identification of the temporal unit most likely to be used to describe the duration of the event.
 periments. Section 5.2 describes the experiment on classifying events into those lasting more or less than a day. Section 5.3 describes the experiment on identifying the most appropriate temporal unit for the mean durations. 5.1 Features
In this section, we describe the lexical, syntactic, and semantic features that we consid-ered in learning event durations. 5.1.1 Local Context. For a given event, the local context features include a window of n tokens to its left and n tokens to its right, as well as the event itself, for n =
The best n determined via cross validation turned out to be 0, that is, the event itself 742 with no local context. But we also present results for n = 2 in Section 5.2.3 to evaluate the utility of local context.
 because they can be indicative features for learning event durations. For example, the quotation mark is a good indication of quoted reporting events, and the duration of such events most likely lasts for seconds or minutes, depending on the length of the quoted content. However, there are also cases where quotation marks are used for other purposes, such as for titles of artistic works.
 included: the original form of the token, its lemma (or root form), and its part-of-speech (POS) tag. The lemma of the token is extracted from parse trees generated by the CONTEX parser (Hermjakob and Mooney 1997), which includes rich context information in parse trees, and the Brill tagger (Brill 1992) is used for POS tagging. shown in Table 2 (with a window size n = 2). The feature vector is [signed, sign, VBD, the, the, DT, plan, plan, NN, Friday, Friday, NNP, on, on, IN].
 5.1.2 Syntactic Relations. The information in the event X  X  syntactic environment is very important in deciding the durations of events. For example, there is a difference in the durations of the  X  X atch X  events in the phrases  X  watch a movie  X  X nd X  watch abirdfly . X  from the parse trees generated by the CONTEX parser. Similarly to the local context features, for both the subject head and the object head, their original form, lemma, and POS tags are extracted as features. When there is no subject or object for an event,  X  X ULL X  is used for the feature values.
 its subject is  X  X residents X  and the head of its object is  X  X lan. X  The extracted syntactic relation features are shown in Table 3, and the feature vector is [presidents, president,
NNS, plan, plan, NN]. 5.1.3 WordNet Hypernyms. Events with the same hypernyms may have similar durations.
For example, events  X  X sk X  and  X  X alk X  both have a direct WordNet (Miller et al. 1990) hypernym of  X  X ommunicate, X  and most of the time they do have very similar durations in the corpus.
 For example,  X  X ee X  has a direct hypernym of  X  X erceive, X  whereas for  X  X bserve X  one needs to go two steps up through the hypernym hierarchy before reaching  X  X erceive. X 
Correlation between events may be lost if only the direct hypernyms of the words are extracted.
 subject and object of the event. For example, events related to a group of people or an organization usually last longer than those involving individuals, and the hypernyms can help distinguish such concepts. The direct hypernyms of nouns are not always general enough for this purpose, but a hypernym at too high a level can be too general to be useful. For our learning experiments, we use the first three levels of hypernyms from WordNet.
 local context words. For each level of hypernyms in the hierarchy, it X  X  possible to have more than one hypernym, for example,  X  X ee X  has two direct hypernyms,  X  X erceive X  and  X  X omprehend. X  For a given word, it may also have more than one sense in WordNet. In such cases, as in Gildea and Jurafsky (2002), we only take the first sense of the word and the first hypernym listed for each level of the hierarchy. A word disambiguation module might improve the learning performance. But because the features we need are the hypernyms, not the word sense itself, even if the first word sense is not the correct one, its hypernyms can still be good enough in many cases. For example, in one news article, the word  X  X ontroller X  refers to an air traffic controller, which corresponds to the second sense in WordNet, but its first sense (business controller) has the same hypernym of  X  X erson X  (three levels up) as the second sense (direct hypernym). Because we take the first three levels of hypernyms, the correct hypernym is still extracted.
 nym on the previous level is used. When there is no hypernym for a given word (e.g.,  X  X o X ), the word itself will be used as its hypernyms. Because WordNet only provides hypernyms for nouns and verbs,  X  X ULL X  is used for the feature values for a word that is not a noun or a verb.

WordNet hypernym features for the event ( X  X igned X ), its subject ( X  X residents X ), and its object ( X  X lan X ) are shown in Table 4, and the feature vector is [write, communicate, interact, corporate executive, executive, administrator, idea, content, cognition]. 744 5.2 Learning Coarse-Grained Event Durations
The distribution of the means of the annotated durations in Figure 3 is bimodal, dividing the events into those that take less than a day and those that take a day or more. Thus, in our first machine learning experiment, we have tried to learn this coarse-grained event duration information as a binary classification task. 5.2.1 Inter-Annotator Agreement, Baseline, and Upper Bound. Before evaluating the perfor-mance of different learning algorithms, we first assess the inter-annotator agreement, the baseline, and the upper bound for the learning task.
 binary event durations. The experiments were conducted on the same data sets as in
Section 3.3. Two kappa values are reported with different ways of measuring expected agreement ( P ( E )), that is, whether or not the annotators have prior knowledge of the global distribution of the task, as described in Section 3.2.
 for this binary classification task. The baseline for the learning task is always taking the most probable class. Because 59.0% of the total data is  X  X ong X  events, the baseline performance is 59.0% . 5.2.2 Data. The original annotated data was translated into a binary classification. For each event annotation, the most likely or mean duration was calculated by averaging the logs of its lower and upper bound durations. If its most likely or mean duration was less than a day (about 11.4 in the natural logarithmic scale), it was assigned to the  X  X hort X  event class, otherwise it was assigned to the  X  X ong X  event class. (Note that these labels are strictly a convenience and not an analysis of the meanings of  X  X hort X  and  X  X ong. X ) sets: a training data set with 1,705 event instances (about 80% of the total non-WSJ data) and a held-out test data set with 427 event instances (about 20% of the total non-WSJ data). The WSJ data (156 event instances) was kept for further test purposes (see Section 5.2.5). 5.2.3 Experimental Results (non-WSJ)
Learning Algorithms . Three supervised learning algorithms were evaluated for our binary classification task, namely, Support Vector Machines (SVM) (Vapnik 1995), Naive
Bayes (NB) (Duda and Hart 1973), and Decision Trees (C4.5) (Quinlan 1993). The Weka (Witten and Frank 2005) machine learning package was used for the implementation of these learning algorithms. Linear kernel is used for SVM in our experiments. for the event only condition, and 30 feature values for the local context condition, when n = 2. For SVM and C4.5, all features are converted into binary features (6,665 and 12,502 features).

Results . Ten-fold cross validation was used to train the learning models, which were then tested on the unseen held-out test set, and the performance (including the pre-cision, recall, and F-score 4 for each class) of the three learning algorithms is shown in Table 6. The significant measure is overall precision, and this is shown for the three algorithms in Figure 6, together with human agreement (the upper bound of the learning task) and the baseline.
 for each class and also the best overall precision (76.6%). Compared with the baseline (59.0%) and human agreement (87.7%), this level of performance is very encouraging, especially as the learning is from such limited training data.

Feature Evaluation . The best performing learning algorithm, SVM, was then used to examine the utility of combinations of four different feature sets (i.e., event, local context, syntactic, and WordNet hypernym features). The detailed comparison is shown in Table 7. 5
A significant improvement above that is due to the addition of information about the subject and object. Local context does not help and in fact may hurt, and hypernym information also does not seem to help. It is of interest that the most important infor-mation is that from the predicate and arguments describing the event, as our linguistic intuitions would lead us to expect. 5.2.4 Learning Performance by Event Class. It X  X  useful to compare the learning performance between different event classes, and see how they contribute to the overall learning performance. We would intuitively expect that some classes of events (e.g., reporting events) are relatively easier to learn than others. 746 event class, it also includes the total number of event instances, how many of them are as short or long events based on their annotations in the corpus, and the number of error instances.
 event classes with the highest precision are aspectual events (e.g., continue, start) and perception events (e.g., see, look), though they don X  X  contribute much to the overall performance (total instances are only 14 and 9, respectively). As we expected, reporting events perform relatively better than most other event classes, but they don X  X  actually contribute much to the overall performance either (only 39 instances out of a total of 427 instances). The biggest event class is occurrence events (240 instances), and its precision is not much lower than the overall precision (73.3% vs. 76.5%). The most difficult event class for learning their durations seems to be I-action events (e.g., try, insist). 5.2.5 Test on WSJ Data. Section 5.2.3 describes the experimental results with the learned model trained and tested on data from the same genre, that is, non-WSJ articles. In order to evaluate whether the learned model can perform well on data from different news genres, we tested it on the unseen WSJ data (156 event instances). The performance (including the precision, recall, and F-score for each class) is shown in Table 9. The pre-cision (75.0%) is very close to the test performance on the non-WSJ data, and indicates the significant generalization capacity of the learned model. 5.3 Learning the Most Likely Temporal Unit
These encouraging results prompted us to try to learn more fine-grained event duration information, namely, the most likely temporal units of event durations (cf. Rieger X  X  by averaging its lower and upper bound durations, and assigning it to one of seven classes X  X econd, minute, hour, day, week, month, and year X  X ased on the temporal unit of its most likely duration.
 understandable. An annotation of [30 minutes, 1 hour] and [35 minutes, 2 hours] will not match, even though the area of their overlap is 52.72%. 748 notators, an  X  approximate agreement X  is computed for the most likely temporal unit of events. In  X  X pproximate agreement, X  temporal units are considered to match if they are the same temporal unit or an adjacent one. For example,  X  X econd X  and  X  X inute X  match, but  X  X inute X  and  X  X ay X  do not.
 data sets as in the binary classification task were used. The only difference was that the class for each instance was now labeled with one of the seven temporal unit classes. which with its two neighbors spans the greatest amount of data. Because the  X  X eek, X   X  X onth, X  and  X  X ear X  classes together take up the largest portion (51.5%) of the data, the baseline was always taking the  X  X onth X  class, where both  X  X eek X  and  X  X ear X  were also considered a match. Table 10 shows the inter-annotator agreement results for the most likely temporal unit when using  X  X pproximate agreement X . Human agreement, the upper bound, for this task increases from 44.4% to 79.8%.
 then tested on the unseen held-out test set. The performance of the three algorithms is shown in Figure 7. The best performing learning algorithm is again SVM with 67.9% test precision. Compared with the baseline (51.5%) and human agreement (79.8%), this again is a very promising result, especially for a multi-classification task with such limited training data. It is reasonable to expect that when more annotated data become available, the learning algorithm will achieve higher performance when learning this and more fine-grained event duration information.
 computers have no idea at all whether a meeting event takes seconds or centuries, so even coarse-grained estimates would give it a useful rough sense of how long each event may take. More fine-grained duration information is definitely more desirable for temporal reasoning tasks. But coarse-grained durations to a level of temporal units can already be very useful. For example, when you want to know how long it takes to learn to use some new software, an answer of  X  X ours X  or  X  X onths X  is often enough. 6. Conclusion
In the research described in this article, we have addressed a problem X  X xtracting information about event durations encoded in event descriptions X  X hat has heretofore received very little attention in the field. It is information that can have a substantial impact on applications where the temporal placement of events is important. Moreover, it is representative of a set of problems X  X aking use of the vague information in text X  X hat has largely eluded empirical approaches in the past. We have explicated the linguistic categories of the phenomena that give rise to grossly discrepant judgments among annotators, and give guidelines for resolving these discrepancies. We have also described a method for measuring inter-annotator agreement when the judgments are intervals on a scale; this should extend from time to other scalar judgments. Inter-annotator agreement is too low on fine-grained judgments. However, for the coarse-grained judgments of more than or less than a day, and of approximate agreement on temporal unit, human agreement is acceptably high. For these cases, we have shown that machine-learning techniques achieve encouraging results.
 our most fine-grained duration annotations that are intervals on a scale. The duration interval has been modeled using the normal distribution, and the difference between two duration intervals are then the overlap between two distributions. So in order to learn this distribution, only two parameters need to be learned, namely, the mean and the standard deviation of the normal distribution, and the cost function can be defined as a function of distribution overlaps.
 Acknowledgments References 750
