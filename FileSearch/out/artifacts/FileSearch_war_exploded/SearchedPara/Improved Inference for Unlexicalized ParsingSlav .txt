 Treebank parsing comprises two problems: learn-ing , in which we must select a model given a tree-bank, and inference , in which we must select a parse for a sentence given the learned model. Pre-vious work has shown that high-quality unlexical-ized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006). In particular, we demon-strated in Petrov et al. (2006) that a hierarchically split PCFG could exceed the accuracy of lexical-ized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for infer-3. experiments on automatic splitting for lan-In Sec. 3, we present a novel coarse-to-fine pro-cessing scheme for hierarchically split PCFGs. Our method considers the splitting history of the final grammar, projecting it onto its increasingly refined prior stages. For any projection of a grammar, we give a new method for efficiently estimating the pro-jection X  X  parameters from the source PCFG itself (rather than a treebank), using techniques for infi-nite tree distributions (Corazza and Satta, 2006) and iterated fixpoint equations. We then parse with each refinement, in sequence, much along the lines of Charniak et al. (2006), except with much more com-plex and automatically derived intermediate gram-mars. Thresholds are automatically tuned on held-out data, and the final system parses up to 100 times faster than the baseline PCFG parser, with no loss in test set accuracy.

In Sec. 4, we consider the well-known issue of inference objectives in split PCFGs. As in many model families (Steedman, 2000; Vijay-Shanker and Joshi, 1985), split PCFGs have a derivation / parse distinction. The split PCFG directly describes a gen-erative model over derivations, but evaluation is sen-sitive only to the coarser treebank symbols. While the most probable parse problem is NP-complete (Sima X  X n, 1992), several approximate methods exist, including n-best reranking by parse likelihood, the labeled bracket algorithm of Goodman (1996), and a variational approximation introduced in Matsuzaki et al. (2005). We present experiments which explic-itly minimize various evaluation risks over a can-didate set using samples from the split PCFG, and relate those conditions to the existing non-sampling algorithms. We demonstrate that n-best reranking according to likelihood is superior for exact match, and that the non-reranking methods are superior for maximizing F the role of unary productions, which previous work has glossed over, but which is important in under-standing why the various methods work as they do. Finally, in Sec. 5, we learn state-split PCFGs for German and Chinese and examine out-of-domain performance for English. The learned grammars are compact and parsing is very quick in our multi-stage scheme. These grammars produce the highest test set parsing figures that we are aware of in each lan-guage, except for English for which non-local meth-ods such as feature-based discriminative reranking are available (Charniak and Johnson, 2005). We consider PCFG grammars which are derived from a raw treebank as in Petrov et al. (2006): A simple X-bar grammar is created by binarizing the treebank trees. We refer to this grammar as G From this starting point, we iteratively refine the grammar in stages, as illustrated in Fig. 1. In each stage, all symbols are split in two, for example DT might become DT-1 and DT-2 . The refined grammar is estimated using a variant of the forward-backward algorithm (Matsuzaki et al., 2005). After a split-ting stage, many splits are rolled back based on (an approximation to) their likelihood gain. This pro-cedure gives an ontogeny of grammars G G = G n is the final grammar. Empirically, the gains on the English Penn treebank level off after 6 rounds. In Petrov et al. (2006), some simple smooth-ing is also shown to be effective. It is interesting to note that these grammars capture many of the  X  X truc-tural zeros X  described by Mohri and Roark (2006) and pruning rules with probability below e  X  10 re-duces the grammar size drastically without influenc-ing parsing performance. Some of our methods and conclusions are relevant to all state-split grammars, such as Klein and Manning (2003) or Dreyer and Eisner (2006), while others apply most directly to the hierarchical case. When working with large grammars, it is standard to prune the search space in some way. In the case of lexicalized grammars, the unpruned chart often will not even fit in memory for long sentences. Several proven techniques exist. Collins (1999) combines a punctuation rule which eliminates many spans en-tirely, and then uses span-synchronous beams to prune in a bottom-up fashion. Charniak et al. (1998) introduces best-first parsing, in which a figure-of-merit prioritizes agenda processing. Most relevant to our work is Charniak and Johnson (2005) which uses a pre-parse phase to rapidly parse with a very coarse, unlexicalized treebank grammar. Any item X :[ i, j ] with sufficiently low posterior probability in the pre-parse triggers the pruning of its lexical vari-ants in a subsequent full parse. 3.1 Coarse-to-Fine Approaches Charniak et al. (2006) introduces multi-level coarse-to-fine parsing, which extends the basic pre-parsing idea by adding more rounds of pruning. In their work, the extra pruning was with grammars even coarser than the raw treebank grammar, such as a grammar in which all nonterminals are col-lapsed. We propose a novel multi-stage coarse-to-fine method which is particularly natural for our hi-erarchically split grammar, but which is, in princi-ple, applicable to any grammar. As in Charniak et al. (2006), we construct a sequence of increasingly refined grammars, reparsing with each refinement. The contributions of our method are that we derive sequences of refinements in a new way (Sec. 3.2), we consider refinements which are themselves com-plex, and, because our full grammar is not impossi-ble to parse with, we automatically tune the pruning thresholds on held-out data. 3.2 Projection In our method, which we call hierarchical coarse-to-fine parsing, we consider a sequence of PCFGs G 0 , G 1 , . . . G n = G of the preceding grammar G grammar of interest. Each grammar G G = G n by a projection  X  n  X  i or  X  i for brevity. A projection is a map from the non-terminal (including pre-terminal) symbols of G onto a reduced domain. A projection of grammar symbols induces a pro-jection of rules and therefore entire non-weighted grammars (see Fig. 1).

In our case, we also require the projections to be sequentially compatible, so that  X  That is, each projection is itself a coarsening of the previous projections. In particular, we take the pro-jection  X  bols in round i to their earlier identities in round j .
It is straightforward to take a projection  X  and map a CFG G to its induced projection  X  ( G ) . What is less obvious is how the probabilities associated with the rules of G should be mapped. In the case where  X  ( G ) is more coarse than the treebank orig-inally used to train G , and when that treebank is available, it is easy to project the treebank and di-rectly estimate, say, the maximum-likelihood pa-rameters for  X  ( G ) . This is the approach taken by Charniak et al. (2006), where they estimate what in our terms are projections of the raw treebank gram-mar from the treebank itself.

However, treebank estimation has several limita-tions. First, the treebank used to train G may not be available. Second, if the grammar G is heavily smoothed or otherwise regularized, its own distri-bution over trees may be far from that of the tree-bank. Third, the meanings of the split states can and do drift between splitting stages. Fourth, and most importantly, we may wish to project grammars for which treebank estimation is problematic, for exam-ple, grammars which are more refined than the ob-served treebank grammars. Our method effectively avoids all of these problems by rebuilding and refit-ting the pruning grammars on the fly from the final grammar. 3.2.1 Estimating Projected Grammars
Fortunately, there is a well worked-out notion of estimating a grammar from an infinite distribution over trees (Corazza and Satta, 2006). In particular, we can estimate parameters for a projected grammar  X  ( G ) from the tree distribution induced by G (which can itself be estimated in any manner). The earli-est work that we are aware of on estimating models from models in this way is that of Nederhof (2005), who considers the case of learning language mod-els from other language models. Corazza and Satta (2006) extend these methods to the case of PCFGs and tree distributions.

The generalization of maximum likelihood esti-mation is to find the estimates for  X  ( G ) with min-imum KL divergence from the tree distribution in-duced by G . Since  X  ( G ) is a grammar over coarser symbols, we fit  X  ( G ) to the distribution G induces over  X  -projected trees: P (  X  ( T ) | G ) . The proofs of the general case are given in Corazza and Satta (2006), but the resulting procedure is quite intuitive.
Given a (fully observed) treebank, the maximum-likelihood estimate for the probability of a rule X  X  Y Z would simply be the ratio of the count of X to the count of the configuration X  X  Y Z . If we wish to find the estimate which has minimum divergence to an infinite distribution P ( T ) , we use the same for-mula, but the counts become expected counts: with unaries estimated similarly. In our specific case, X, Y, and Z are symbols in  X  ( G ) , and the expectations are taken over G  X  X  distribution of  X  -projected trees, P (  X  ( T ) | G ) . We give two practical methods for obtaining these expectations below. 3.2.2 Calculating Projected Expectations
Concretely, we can now estimate the minimum divergence parameters of  X  ( G ) for any projection  X  and PCFG G if we can calculate the expecta-tions of the projected symbols and rules according to P (  X  ( T ) | G ) . The simplest option is to sample trees T from G , project the samples, and take average counts off of these samples. In the limit, the counts will converge to the desired expectations, provided the grammar is proper. However, we can exploit the structure of our projections to obtain the desired ex-pectations much more simply and efficiently.
First, consider the problem of calculating the ex-pected counts of a symbol X in a tree distribution given by a grammar G , ignoring the issue of projec-tion. These expected counts obey the following one-step equations (assuming a unique root symbol): Here,  X  ,  X  , or both can be empty, and a rule X  X   X  appears in the sum once for each X it contains. In principle, this linear system can be solved in any eratively, with the following recurrences: Note that, as in other iterative fixpoint methods, such as policy evaluation for Markov decision processes (Sutton and Barto, 1998), the quantities c a useful interpretation as the expected counts ignor-ing nodes deeper than depth k (i.e. the roots are all the root symbol, so c ments this method converged within around 25 iter-ations; this is unsurprising, since the treebank con-tains few nodes deeper than 25 and our base gram-mar G seems to have captured this property.

Once we have the expected counts of symbols in G , the expected counts of their projections X  X  =  X  ( X ) according to P (  X  ( T ) | G ) are given by mated directly using similar recurrences, or given by one-step equations:
This process very rapidly computes the estimates for a projection of a grammar (i.e. in a few seconds for our largest grammars), and is done once during initialization of the parser. 3.2.3 Hierarchical Projections
Recall that our final state-split grammars G come, by their construction process, with an ontogeny of grammars G splitting of the preceding one. This gives us a nat-ural chain of projections  X  wards along this ontogeny of grammars (see Fig. 1). Of course, training also gives us parameters for the grammars, but only the chain of projections is needed. Note that the projected estimates need not (and in general will not) recover the original param-eters exactly, nor would we want them to. Instead they take into account any smoothing, substate drift, and so on which occurred by the final grammar.
Starting from the base grammar, we run the pro-jection process for each stage in the sequence, cal-culating  X  also be possible). For the remainder of the paper, except where noted otherwise, all coarser grammars X  estimates are these reconstructions, rather than those originally learned. 3.3 Experiments As demonstrated by Charniak et al. (2006) parsing times can be greatly reduced by pruning chart items that have low posterior probability under a simpler grammar. Charniak et al. (2006) pre-parse with a se-quence of grammars which are coarser than (parent-annotated) treebank grammars. However, we also work with grammars which are already heavily split, up to half as split as the final grammar, because we found the computational cost for parsing with the simple X-bar grammar to be insignificant compared to the costs for parsing with more refined grammars.
For a final grammar G = G mates for the n projections G where G tion. Additionally we project to a grammar G which all nonterminals, except for the preterminals, have been collapsed. During parsing, we start of by exhaustively computing the inside/outside scores with G rior probability are removed from the chart, and we proceed to compute inside/outside scores with the next, more refined grammar, using the projections  X  each pass, we skip chart items whose projection into the previous stage had a probability below a stage-specific threshold, until we reach G = G seven passes in our case). For G , we do not prune but instead return the minimum risk tree, as will be described in Sec. 4.

Fig. 2 shows the (unlabeled) bracket posteriors af-ter each pass and demonstrates that most construc-tions can be ruled out by the simpler grammars, greatly reducing the amount of computation for the following passes. The pruning thresholds were em-pirically determined on a held out set by computing the most likely tree under G directly (without prun-ing) and then setting the highest pruning threshold for each stage that would not prune the optimal tree. This setting also caused no search errors on the test set. We found our projected grammar estimates to be at least equally well suited for pruning as the orig-inal grammar estimates which were learned during the hierarchical training. Tab. 1 shows the tremen-dous reduction in parsing time (all times are cumu-lative) and gives an overview over grammar sizes and parsing accuracies. In particular, in our Java im-plementation on a 3GHz processor, it is possible to parse the 1578 development set sentences (of length 40 or less) in less than 1200 seconds with an F 91.2% (no search errors), or, by pruning more, in 680 seconds at 91.1%. For comparison, the Feb. 2006 release of the Charniak and Johnson (2005) parser runs in 1150 seconds on the same machine with an F A split PCFG is a grammar G over symbols of the form X -k where X is an evaluation symbol (such as NP ) and k is some indicator of a subcategory, such as a parent annotation. G induces a deriva-tion distribution P ( T | G ) over trees T labeled with split symbols. This distribution in turn induces a parse distribution P ( T  X  | G ) = P (  X  ( T ) | G ) over (projected) trees with unsplit evaluation symbols, where P ( T  X  | G ) = P have several choices of how to select a tree given these posterior distributions over trees. In this sec-tion, we present experiments with the various op-tions and explicitly relate them to parse risk mini-mization (Titov and Henderson, 2006).

The decision-theoretic approach to parsing would be to select the parse tree which minimizes our ex-pected loss according to our beliefs: where T trees. Here, our loss is described by the function L whose first argument is the predicted parse tree and the second is the gold parse tree. Reasonable can-didates for L include zero-one loss (exact match), precision, recall, F so on. Of course, the naive version of this process is intractable: we have to loop over all (pairs of) pos-sible parses. Additionally, it requires parse likeli-hoods P ( T ial, to compute for split models. There are two op-tions: limit the predictions to a small candidate set or choose methods for which dynamic programs exist.
For arbitrary loss functions, we can approximate the minimum-risk procedure by taking the min over only a set of candidate parses T each parse X  X  expected risk can be evaluated in closed
Rule score: r ( A  X  B C , i, k, j ) = X ( A , i, j ) P ( A x  X  B y C z ) P I N ( B y , i, k ) P I N ( C
V ARIATIONAL : q ( A  X  B C , i, k, j ) = r ( A  X  B C , i, k, j ) M AX -R ULE -S UM : q ( A  X  B C , i, k, j ) = r (
A  X  B C , i, k, j ) M AX -R ULE -P RODUCT : q ( A  X  B C , i, k, j ) = r (
A  X  B C , i, k, j ) form. Exact match (likelihood) has this property. In general, however, we can approximate the expecta-tion with samples from P ( T | w, G ) . The method for sampling derivations of a PCFG is given in Finkel et al. (2006) and Johnson et al. (2007). It requires a single inside-outside computation per sentence and is then efficient per sample. Note that for split gram-mars, a posterior parse sample can be drawn by sam-pling a derivation and projecting away the substates.
Fig. 2 shows the results of the following exper-iment. We constructed 10-best lists from the full grammar G in Sec. 2 using the parser of Petrov et al. (2006). We then took the same grammar and ex-tracted 500-sample lists using the method of Finkel et al. (2006). The minimum risk parse candidate was selected for various loss functions. As can be seen, in most cases, risk minimization reduces test-set loss of the relevant quantity. Exact match is problematic, however, because 500 samples is often too few to draw a match when a sentence has a very flat poste-act match permits a non-sampled calculation of the expected risk, we show this option as well, which is substantially superior. This experiment highlights that the correct procedure for exact match is to find the most probable parse.

An alternative approach to reranking candidate parses is to work with inference criteria which ad-mit dynamic programming solutions. Fig. 3 shows three possible objective functions which use the eas-ily obtained posterior marginals of the parse tree dis-tribution. Interestingly, while they have fairly differ-ent decision theoretic motivations, their closed-form solutions are similar.
One option is to maximize likelihood in an ap-proximate distribution. Matsuzaki et al. (2005) present a V ARIATIONAL approach, which approxi-mates the true posterior over parses by a cruder, but tractable sentence-specific one. In this approximate distribution there is no derivation / parse distinction and one can therefore optimize exact match by se-lecting the most likely derivation.

Instead of approximating the tree distribution we can use an objective function that decomposes along parse posteriors. The labeled brackets algorithm of Goodman (1996) has such an objective function. In its original formulation this algorithm maximizes the number of expected correct nodes, but instead we can use it to maximize the number of correct rules (the M AX -R ULE -S UM algorithm). A worry-ing issue with this method is that it is ill-defined for grammars which allow infinite unary chains: there will be no finite minimum risk tree under recall loss (you can always reduce the risk by adding one more cycle). We implement M AX -R ULE -S UM in a CNF-like grammar family where above each binary split is exactly one unary (possibly a self-loop). With this limitation, unary chains are not a problem. As might be expected, this criterion improves bracket measures at the expense of exact match.

We found it optimal to use a third approach, in which rule posteriors are multiplied instead of added. This corresponds to choosing the tree with greatest chance of having all rules correct, under the (incorrect) assumption that the rules correct-ness are independent. This M AX -R ULE -P RODUCT algorithm does not need special treatment of infi-nite unary chains because it is optimizing a product rather than a sum. While these three methods yield very similar results (see Fig. 2), the M AX -R ULE P
RODUCT algorithm consistently outperformed the other two.

Overall, the closed-form options were superior to the reranking ones, except on exact match, where the gains from correctly calculating the risk outweigh the losses from the truncation of the candidate set. Most research on parsing has focused on English and parsing performance on other languages is gen-been some attempts to adapt parsers developed for English to other languages (Levy and Manning, 2003; Cowan and Collins, 2005). Adapting lexi-calized parsers to other languages in not a trivial task as it requires at least the specification of head rules, and has had limited success. Adapting unlexi-calized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported.

In contrast, automatically learned grammars like the one of Matsuzaki et al. (2005) and Petrov et al. (2006) require a treebank for training but no addi-tional human input. One has therefore reason to believe that their performance will generalize bet-ter across languages than the performance of parsers that have been hand tailored to English. 5.1 Experiments We trained models for English, Chinese and Ger-man using the standard corpora and splits as shown in Tab. 3. We applied our model directly to each of the treebanks, without any language dependent modifications. Specifically, the same model hyper-parameters (merging percentage and smoothing fac-tor) were used in all experiments.

Tab. 4 shows that automatically inducing latent structure is a technique that generalizes well across language boundaries and results in state of the art performance for Chinese and German. On English, the parser is outperformed only by the reranking parser of Charniak and Johnson (2005), which has access to a variety of features which cannot be cap-tured by a generative model.

Space does not permit a thorough exposition of our analysis, but as in the case of English (Petrov et al., 2006), the learned subcategories exhibit inter-esting linguistic interpretations. In German, for ex-ample, the model learns subcategories for different cases and genders. 5.2 Corpus Variation Related to cross language generalization is the gen-eralization across domains for the same language. It is well known that a model trained on the Wall Street Journal loses significantly in performance when evaluated on the Brown Corpus (see Gildea (2001) for more details and the exact setup of their experiment, which we duplicated here). Recently McClosky et al. (2006) came to the conclusion that this performance drop is not due to overfitting the WSJ data. Fig. 4 shows the performance on the Brown corpus during hierarchical training. While the F in performance after the 5th iteration, suggesting that some overfitting is occurring. The coarse-to-fine scheme presented here, in con-junction with the risk-appropriate parse selection methodology, allows fast, accurate parsing, in multi-ple languages and domains. For training, one needs only a raw context-free treebank and for decoding one needs only a final grammar, along with coars-ening maps. The final parser is publicly available at http://www.nlp.cs.berkeley.edu .
 Acknowledgments We would like to thank Eu-gene Charniak, Mark Johnson and Noah Smith for helpful discussions and comments.

