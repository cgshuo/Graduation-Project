 Sequential pattern mining (SPM) is one of the most important applications in data mining. It is widely used to analyze customer behaviors in market-basket databases. For example, online shopping websites usually collect customer pur-chasing records in databases where sequential patterns are mined to reveal buy-ing habits of consumers. However, in many real applications, events occurring in a sequence may be uncertain for many reasons. For instance, data collected by are added deliberately; data modeling techniques such as classifications may also produce indeterministic results [ 1 ].
 Example 1. Consider an online travel website. To increase sales, a large group of products. These patterns are useful in intelligenet marketing. For example, by providing a special hotel offer to a customer who booked a late-night flight, the website is able to encourage hotel purchases.
 in the session B , the customer is first attracted by a rental car and then shows how long a customer stays on a page and so on, into consideration. The website uses a database that represents each visiting session as a single sequence, also shown in Fig. 1 (a). 1.1 Problem Statement The uncertain model applied in this paper is based on possible world semantics with existential uncertain events .
 Definition 1. An uncertain event is an event e whose presence in a sequence d is defined by an existential probability P ( e  X  d )  X  (0 , 1] . Definition 2. An uncertain sequence d is an ordered list of uncertain events. An uncertain sequence database is a collection of uncertain sequences. where an itemset s i  X  s is also called an element of s .
 In certain databases, a sequential pattern s = s 1 ,...,s sequence d = e 1 ,  X  X  X  ,e m , denoted by s d , if there exists n integers 1 ...,k in an uncertain database D , the support of a pattern s is uncertain. We define probabilistic frequent sequential patterns (p-FSP) as follows: Definition 4 (Probabilistic Frequent Sequential Pattern). Asequential of being frequent is at least  X  p , denoted by P ( sup ( s ) Here sup ( s ) is the support of s in D and  X  p is the user-defined minimum con-fidence in the frequentness of a sequential pattern. We are now able to specify the uncertain SPM problem as: Given  X  s ,  X  p , find all p-FSPs in D . In an uncertain database, sequences are often assumed to be mutually inde-the overall support sup ( s )in D is a sum of uncertain supports in every single sequence. And the probabilistic support of s in an uncertain sequence d modeled by a Bernoulli random variable X i  X  B (1 ,p i ), where p the probability that d i supports s .
 the Gaussian distribution in Eq. ( 1 ), according to the central limit theory . We are now able to compute the approximate frequentness probability of s by: And s is a p-FSP if P ( sup ( s )  X   X  s )  X   X  p .
 bilities. We use possible world semantics to interpret uncertain sequences. Here a possible world is a certain sequence instantiated by generating every event according to its existential probability. Each uncertain probability P ( e derives two possible worlds per sequence: One possible world in which event e fore, the number of possible worlds increases exponentially in the number of events.
 Figure 1 (b) shows all possible worlds derived from sequence S example, in world 2, the customer in session A is first attracted by a flight product and then interested in a flight-hotel package. Afterwards, the customer has no interests towards another hotel any more.
 tions. Therefore, we can compute the existential probability of a possible world w by Eq. ( 3 ).
 For example, the existential probability of world 2 in Fig. 1 (b) is P ( w 0 . 6  X  (1  X  0 . 8) = 0 . 084.
 in large scale databases and extracting all p-FSPs. Meanwhile, in order to mine highly scalable databases, we extend the Apriori-like framework of uncertain SPM to Spark [ 12 ] which is a distributed computing platform allowing us to load data into a cluster X  X  memory and query it repeatedly. 1.2 Contribution In this paper, we propose a D istributed S equential P attern (DSP) mining algo-rithm in large scale uncertain databases. Our main contributions are summarized as follows: (1) We propose a SPM framework for mining large scale uncertain databases in Spark. (2) We present a dynamic programming method to compute support probabilities in linear time. (3) We propose a memory-efficient distrib-uted dynamic programming approach in Spark and design a new data structure to save intermediate results efficiently. (4) Extensive experiments conducted in various scales shows that our algorithm is orders of magnitude faster than both direct extension and existing works. Uncertain data mining has been an active area of research recently. Many tra-ditional database and data mining techniques have been extended to be applied to uncertain databases [ 2 ]. Specifically, Muzammal and Raman first define the frequent sequential patterns in possible world model and propose their uncertain SPM algorithms [ 13 , 14 ]. Li et al. introduce a dynamic programming approach to mine sequential patterns in a specific spatial-temporal uncertain model [ 8 ]. Wan et al. [ 11 ] propose a dynamic programming algorithm of mining frequent serial episodes within one uncertain sequence. However, all the above mentioned methods can only be executed in a single machine and may have scalability issues in mining large databases.
 Chen et al. extend the classic SPAM algorithm to its MapReduce version SPAMC [ 5 ]. Miliaraki et al. propose a gap-constraint frequent sequence min-ing algorithm in MapReduce [ 9 ]. These algorithms are applied in the context of deterministic data, while our work aims to solve large scale uncertain SPM problems. An iterative MapReduce implementation of uncertain SPM in [ 6 ]is somehow close to our work; however, it has a quadratic complexity of support probability computation, and the time cost of that in our algorithm is linear. First of all, we define the following two types of sequential pattern extension. Definition 5 (Item-extended Pattern). An item-extended pattern s is a sequential pattern s , denoted by s = { i } X  s .
 Definition 6 (Sequence-extended Pattern). A sequence-extended pattern s is a sequential pattern generated by adding a new itemset tial pattern s as its first element, denoted by s = { i } p-FSPs in Lemma 1 allows us to prune a pattern if it is extended from a pattern which is not a p-FSP. [ 14 ] Lemma 1. If s is extended from s and s is a p-FSP, then s is also a p-FSP. rithm in Spark.
 processed in parallel.
 Map. A map function is used to compute support probabilities. A set of candi-date patterns are broadcasted to all the mappers. For each candidate pattern c , the map function first computes the support probability p uncertain sequence d i , then it emits a key-value pair as c, (  X  key field here is the pattern c ; the composite value field contains both mean  X  and variance  X  2 i of the Bernoulli distributed probabilistic support X The key-value pairs are designed to be associative and commutative, so that Spark can aggregate them first in local machines to minimize network usage in shuffling.
 Reduce. Pairs with the same key are shuffled to one reducer. In a reduce func-tion, it computes the approximate frequentness probability for each candidate by Eq. ( 2 ). All candidates with P ( sup ( c )  X   X  s )  X  p-FSPs, denoted by S k .
 Self-join. we self-join all k-length p-FSPs in S k to generate a set of ( k +1)-length candidate patterns in C k +1 .Let s 1 and s 2 be two p-FSPs in S s 1 is the pattern generated by removing the first item i in s pattern generated by removing the last item of s 2 .If s 1 s , denoted by s 1 s 2 , to generate a (k+1)-length candidate c according to the following rules: If s 1 is sequence-extended, c = { i } + s c = { i } X  s 2 . For example, let s 1 = ( a )( bc ) , s 2 then s 1 s 2 = ( a )( bc )( d ) ; while s 2 s 3 = ( bc )( de ) . otherwise, C k +1 is broadcasted to all map functions for the next iteration. 4.1 Dynamic Programming in Support Probability Computation In this section, we propose a DP method to compute support probabilities. The key to our approach is to consider it in terms of sub-problems. Here we first define P i,j in Definition 7 .
 Definition 7. P i,j = P ( s n i d m j ) is the probability that s where s n i = s i ,...,s n is a subsequence of a sequential pattern s and d e ,...,e m is a subsequence of an uncertain sequence d .
 Therefore, P ( s d )= P 1 , 1 . The idea in our approach is to split the prob-s is supported by d m j +1 ;if s i  X  e j , P i,j is equal to the probability that s in Lemma 2 to compute P i,j by means of the paradigm of dynamic programming. Lemma 2 where P ( s i  X  e j ) is the probability that s i is contained in event e e )= P ( e j  X  d ) ,if s i  X  e j ; otherwise, P ( s i  X  e j Proof. Referring to the law of total probability ,wehave: P where P ( s n i d m j | s i  X  e j )= P i +1 ,j +1 is the probability that s = s is supported by sequence e j +1 ,...,e m . And similarly we have P ( s e )= P i,j +1 is the support probability of s n i in d m j +1 This dynamic schema is an adoption of the technique previously used in solving uncertain SPM [ 10 ] and frequent episode mining problems [ 11 ]. Using this dynamic programming scheme, we can compute the support probability by calculating the cells depicted in Fig. 3 . In the matrix, each cell relates to a probability P i,j , with i marked on the x -axis and j markedonthe y -axis. Referring to Lemma 2 , we can compute P i,j from P i,j +1 d ) = 1; meanwhile, P ( s d )=0if s =  X  and d =  X  . Therefore, we iterate the cells from P n +1 ,m +1 to P 1 , 1 so that we finally obtain P ( s d )= P complexity is O ( n  X  m ), as we only need to iterate each cell once. 4.2 Distribute Dynamic Programming Schema A direct extension of the dynamic programming approach in Spark needs to build a n  X  m matrix for every support probability computation, and this might slow down the entire process because of expensive garbage collection overhead in Spark. Therefore, we refine the original DP schema and design a memory-efficient distributed dynamic programming (ddp) approach here. We first define P s,j as follows.
 Definition 8. Given a sequential pattern s and an uncertain sequence d = e 1 ,...,e m , P s,j is defined to be the support probability P ( s d d j = e j ,...,e m is a subsequence of d .
 Based on the extension type of sequential pattern s , we have different dynamic programming schemas.
 Sequence-Extended. If s = { i } + s is sequence-extended from another pattern s , then we can compute the values of P s,j from P s ,j +1 according to Lemma 2 .
 where s 1 = { i } is the first element of s .
 Item-Extended. Let s = { i } X  s , then s 1 is a strict subset of s Referring to Lemma ( 2 ), we can compute P s,j by Eq. ( 7 ).
 Note that s n 2 = s n 2 = s 2 ,...,s n , then P s ,j can be computed by: ALGORITHM 1. ddpUpdate Therefore, we can derive Eq. ( 9 ) by substituting Eq. ( 8 )intoEq.( 7 ). Now we are able to compute P s,j from only the values of P P ( s  X  e Eqs. ( 5 ) and ( 9 ) constitute our distributed dynamic programming structure O ( | d | ). 4.3 Memory-Efficient Distributed SPM Algorithm Lemma 3. It is not necessary to save the value of P s,j ,if P ( s Proof. Referring to Eqs.( 5 ) and ( 9 ), if P ( s 1  X  e j e is the nearest event of e j which has k&gt;j and P ( s 1  X  Let s be a k -length sequential pattern generated by joining two ( k -1)-length patterns as s = s s . Then, the first element of s and s are identical when k  X  2. For example, let s = ( a )( bc )( d ) and s = s s , then s = ( a )( bc ) , s = ( bc )( d ) so that s 1 = s 1 =( a ).
 Suppose L 1 is a list of non-zero P ( s 1  X  e i ) values and L values with P ( s 1  X  e j ) &gt; 0. Algorithm 1 computes the values of P and L 2 . For each value of P ( s 1  X  e i ) &gt; 0, we have P ( s event e i because s 1 = s 1 . Then we search the nearest event e P s ,k &gt; 0and k&gt;i ,totherightof e i .Thus,wehave P s ,i +1 read it from L 2 ;if P ( s  X  e i ) &gt; 0, P s ,i = P s ,i +1 of P ( s  X  e i ), P s ,i +1 and P s ,i , we can compute P time complexity of Algorithm 1 is linear because both L 1 only once.
 iteration of uncertain SPM. The root of the prefix tree is the empty pattern  X  . Each edge in the tree is associated with an item. The key of a node is identified by the path from root to that node. Values are not associated with inner nodes; only leaf nodes point to a list of P s,j values. Each value of P event e j where P ( s 1  X  e j ) &gt; 0. Here s 1 is the first element of s . terns that are linked to events in an uncertain sequence. For example, s = B event e 2 and e 3 . The support probability of B is P ( s d )= P is the first event where we have P ( s 1  X  e 2 ) &gt; 0.
 dates in a centralized node; instead, we broadcast p-FSPs to all mappers and then generate candidates in parallel. In a map function applied to a sequence d , a p-FSP s is not participated in candidate generation if it is not supported in d . In Fig. 4 , suppose S 1 is a set of 1-length p-FSPs and D prune node D from the prefix tree and generate candidates by joining only three 1-length p-FSPs: A , B and C for this sequence.
 associated with s = A and s = B in the 1-length pattern tree. Then we retrieve P ( s 1  X  e 1 )=0 . 8, P s , 2 =0 . 96 and P s , 3 the 2-length prefix tree.
 all 1-length patterns that have not been extended. For instance, node with C is prune because no 2-length candidate starting with item C are potentially supported in the sequence. We implement our algorithms in Spark and evaluate the performance in large scale datasets. The uncertain SPM algorithm which directly adopts dynamic programming in Sect. 4.1 is denote by basic . We denote our distributed uncertain SPM algorithm in Sect. 4.3 by dsp . We also implement the IMRSPM algorithm [ 6 ] in Spark and name it as uspm here.
 We employ the IBM market-basket data generator [ 3 ] to generate sequence datasets in different scales by varying the parameters: (1) C :numberof sequences; (2) T : average number of events per sequence; (3) L : average number of items per event per sequence; (4) I : number of different items. An existential probability  X  is added to each event in the synthetic name a synthetic uncertain dataset by its parameters. For example, a dataset C10kT4L10I10k indicates C =10 k =10  X  1000, T =4, L =10and I =10 k = 10  X  1000.  X  =0 . 8 and frequentness probability threshold  X  p =0 . 7. In Fig. 5 (a) X (d), we set  X  s =2%  X  C ; in Fig. 5 (e) X (h), we have  X  s =1%  X  C ;and  X  evaluate the performance of DSP in different scales: (1) Figure 5 (a), (e), (i) show the running time variations of DSP when C varies from 1000 to 100 000 000, where T = 10, L =3and I =10 k . (2) Figure 5 (b), (f), (j) show the running time variations of DSP when T varies from 10 to 60, where C = 100 k , L =4, I =10 k . (3) Figure 5 (c), (g), (k) show the running time variations when L varies from 2 to 12, where C = 100 k , T =4, I =10 k . (4) Figure 5 (d), (h), (l) show the running time variations when I varies from 10 000 to 10 000 000, where C = 100 k , T =4, L =4.
 and uspm under every setting of the parameters. Specifically, dsp is orders of magnitude faster than basic and uspm in datasets with larger values of T or L . When C&gt; 10 k , basic cannot finish because of the garbage collection overhead from Spark. This indicates that directly extending dynamic programming to Spark is not workable and also proves the advantage of our refined schemas. (2) The running time increase with the increment of C , T , L . This is intuitive because increasing these parameters generates larger scale datasets. Comparing to the C scale, both dsp and uspm are more sensitive to the increment of T and L . uspm fails quickly when T or L becomes larger; however, dsp still performs well even with large T or L values. (3) The running time first drops and then arises with the increment of I . When the value of I grows, the item occurrences become more sparse, and fewer p-FSPs are mined under the same thresholds; meanwhile, the volume of data shuffled from mapper to reducer via network slows down the process when I is extremely large. Figure 6 (a) shows the number of p-FSPs in the dataset C100kT10L3I10k with uncertain level  X  =0 . 8, where we vary the value of  X  2 %; Fig. 6 (b) shows the effect of uncertain level  X  to the number of p-FSPs in This is intuitive because a larger minimal support threshold makes fewer candi-dates be probabilistically frequent. (2) With the increment of  X  , the number of p-FSPs increases, which shows the effect of uncertainty in SPM problems. When uncertain level is high (  X  is small), there are fewer precise information in the data, which makes it more difficult to find p-FSPs under the same thresholds. In this paper, we design a distributed dynamic programming method in Spark to mine sequential patterns in large scale uncertain databases. Our algorithm is on integrating constraints in large scale uncertain SPM problems.
