 1. Introduction
The past decade has witnessed the birth and explosive growth of the World Wide Web. Exponential growth in the number of web servers has extended the web from a few dozen in 1981 up to over 400 millions today ( ISC, 2007 ). Web servers can potentially host millions of pages which make the number of web pages extre-mely difficult to track. To track web pages, one has to download them, a process that can take weeks or months. Unfortunately, during this process, a considerable fraction of pages might have been modified, number of pages, Google claims that it has indexed 3 billion documents at the end of 2001, 4.28 billions on March 2004 ( Google, 2004 ) and about 8 billions pages currently ( Gulli &amp; Signorini, 2005 ). One of the most important challenging issues in any web search engine is finding high quality web pages.
Quality of pages is defined based on the user preferences. Then, the problem of ranking is to sort web pages based on users X  requests or preferences. Definitely, to make the web more interesting and productive, we need a good and efficient ranking algorithm for crawling and searching. Unfortunately, search engines do not index queries. However, current ranking algorithms have low precision and high complexity. Meanwhile, they are more popularity by time passing. Obviously, we need a solution to remedy these problems and make web ranking algorithms fairer.

In fact, web search is a subset of the general Information Retrieval (IR) problem. In Information Retrieval rithms usually work based on matching words in documents. However, the web consists of huge unstructured documents linked together which create a massive graph. This poses new challenges to IR. New algorithms use links between web pages beside words relevancy in the documents with respect to a user query. Previous stud-come from using content of other pages to rank current pages. In other words, links carry information which can be used to evaluate relative importance of pages to the user query. Instances of the connectivity-based ranking algorithms are PageRank ( Page, Brin, Motwani, &amp; Winograd, 1998 ), HITS ( Kleinberg, 1999 ) and OPIC ( Abiteboul, Preda, &amp; Cobena, 2003 ).

In this paper, we propose a ranking algorithm, called DistanceRank, based on reinforcement learning ( Sut-as the number of  X  X  X verage clicks X  X  between two pages or logarithm of a page X  X  out degree (number of output links) as defined in Matsuo, Ohsawa, and Ishizuka (2003) . In our method the main objective is to minimize the sum of received punishments (distance) by the user agent so that the page with the low distance will have a higher rank.

The distance d j of page j is computed as d j =(1 a ) * d pages that point to j and O( i ) shows out degree of i and a is the learning rate of the user.
One of major contribution of our method is modelling a user surfing the web randomly. Initially, a user browsing the web does not have any background about pages and she clicks based on the current status (page X  X  content) of each page under consideration. As time goes on, the user selects a page (clicks on a link) based on both her background and the current content of each page. As she continues, she accumulates more knowledge from the environment and other web pages, and improves her link or page selection process.
We have used University of California at Berkeley X  X  web to evaluate our algorithm. Our algorithm outper-forms other ranking algorithms like PageRank and OPIC while removing some restricted conditions and problems. The time complexity of our solution is about O( p * j E j ) where p V and V and E are number of nodes and edges (links) in the web graph while p shows number of iterations for convergence. For instance in our experiments, we found that 5 iterations are sufficient for 5 million pages. In comparison with others, our solution has a higher throughput, i.e. it finds important pages faster than others. Furthermore, it sounds more experiments and analysis in the future.

Next section discusses our solution, DistanceRank. Experimental analysis and comparison with some of the well-known algorithms come in Section 3 . Section 4 explains the convergence rate of DistanceRank. In Section 5 , ranking problems is explained. Section 6 reviews related work. Finally, our conclusion and future work come in Section 7 . 2. DistanceRank
To remedy ranking algorithms shortcomings, we propose an intelligent ranking algorithm based on rein-factor. Indeed, we use the minimum number of average clicks that a user requires to reach from page i to another page j . Since related pages have been usually linked to each other, the distanced based solution can find pages with high qualities more quickly. In other words, each page that has less average distance from others has a higher rank.

In ranking algorithms like PageRank, the rank of each page is defined as the weighted sum of ranks of all this page have higher ranks. Intuitively, these two properties should be true for the distance. A page having many input links should have low distance and if pages pointing to this page have low distance then this page should have a low distance. The following definitions clarify our idea:
Definition 1. If page i points to page j then the weight of link between i and j is equal to Log shows i  X  X  out degree (number of forward links).
 imum value) from i to j . We call this logarithmic distance and denote it with d
For example, in Fig. 1 , the weight of out-links in pages p , r and t is equal to log(2),log(3) and log(4) logarithmic distance interchangeable unless stated otherwise.

Definition 3. If d ij shows the distance between two page i and j as Definition 2 , then d distance of page j and is defined as the following where V shows number of web pages: will be set to a big value (we use log N here).

Now, the distance vector or logarithmic distances between all web pages can be computed like simple pages are sorted in the ascending order and pages with smaller average distances will have high ranking.
Experimentally, we found that PageRank ordering and the average distance ordering are similar. PageRank.

Our method is dependent on the out degree of nodes in the web graph like other algorithms. Besides, it with single length 3 from i to j like i ! k ! l ! j , then i  X  X  effect on j is (1/O( i )) words, the probability that a random surfer started from page i to reach to page j is (1/O( i )) O( l )). Lemma 1 illustrates the relation between the rank effect and the distance.

Lemma 1. Suppose d ij shows the logarithmic distance between page i and j and r shortest path from i to j, then, if d ij &lt; d ik then r
Proof. We have
Since d ik = log r ik then, when d ij &lt; d ik , we will have r is more than on k , in other words, the probability that a random surfer reach j from i is more than the prob-ability to reach k .

In average distance, problems like sinking pages ( Arasu, Cho, Garcia-Molina, Paepcke, &amp; Raghavan, eliminated. Since we model both surfer and the page creator behavior, therefore, factors likes damping factor in PageRank are not required due to fact that we always work with the real web graph.

The main problem of average distance is its complexity, O( j V j
PageRank complexity in the worst case is O( j V j * j E j ) and practically is O(100 we proposed a new definition of ranking called DistanceRank . Intuitively, to compute average distance of each page, there is a dependency between the distance of each page and its back links. For example, if page j has only one back link and it is from page i , to compute the average distance for page j , d and 3, we will have the following relation:
Since the size of the web is huge, then, V inclines to 1 and, consequently,
In general, suppose O( i ) denotes the number of forwarding (outgoing) links from page i and B ( j ) denotes the set of pages pointing to page j . The DistanceRank of page j , denoted by d
For example in Fig. 1 , the distance d q is computed as the following:
Considering this equation, we experimentally found that DistanceRank is similar to PageRank in ranking learning algorithm ( Sutton &amp; Barto, 1998 ) to compute the distance of page j ( i links to j ). d and d i other words, the distance of page j at time t + 1 depends on its previous distance, its father distance ( d have a path s ! t ! u ! v , then the effect of the distance of s on u is regulated with a c ronment, we are going to decrease sum of received punishments. Since the above equation is based on the rein-1998 ).

The value of the learning rate a comes from Eq. (5) where t shows time or iteration number and b is a static adjusted the system will convergence and reach to the stability state very fast with a high throughput. In the it exponentially to zero
From the learning point of view, the user is an agent surfing the web randomly and in each step it receives some punishments from the environment. The system goal is to minimize sum of punishments. In each state, the agent has some selections, next pages for click, and the page with the minimum received punishment (dis-tance) will be selected as the next page for visiting. Obviously, we can write Eq. (4) as follows: So d j is the total punishment that the agent receives from selecting page j .

One of the main contributions of our method is modelling the real user surfing web. Intuitively, when a user start browsing from a random page, she does not have any background about the web. Then, by surfing and web pages. By the time goes, she continuously accumulates knowledge which help the user to reach her goal web pages (environment), a = 1, and, then, by visiting more pages, the system slowly learns more information time goes on the learning rate decreases.
 As Eq. (4) shows DistanceRank is computed recursively like PageRank. The process iterates to converge.
We show in the next section that it is possible to compute distances with O( p for an acceptable ranking is sufficient.

After convergence, we will have the DistanceRank vector. Pages are sorted in the ascending order and those with low DistanceRank will have high ranking. DistanceRank is computed using power method ( Watkins, / * D is DistanceRank vector * /
D 0 { 1 , 1 , 1 , ... } itr =0;/ * iteration number * /
While d &gt; e End while 3. Experimental results We used University of California at Berkeley X  X  Web site with five millions web pages to evaluate Distance-
Rank. Two scenarios were used: crawling scheduling and rank ordering. In the crawling scheduling the goal was to find more important pages faster in the crawling process. But in the rank ordering, we compared the ordering of DistanceRank with PageRank and Google X  X  rank with and without with and without respect to a user query.

First, the crawling scheduling scenario is used for comparison between ranking algorithms. We compared our results with the following ranking algorithms: rithms start with some starting URLs as the root of crawling tree ( Najork &amp; Wiener, 2001 ). Page, 1998 ). Pages with more input links have higher ranks.

Partial PageRank: This algorithm runs the PageRank algorithm on web pages seen so far and crawl the web pages with a high PageRank first. The PageRank algorithm is now an old algorithm that is no longer repre-sentative of the results returned by Google search engine.

Partial DistanceRank: The distance vector of the current web graph seen so far is computed and the web pages with a small DistanceRank are crawled first.

OPIC: In this algorithm, all pages start with the same amount of cash ( Abiteboul et al., 2003 ). Every time a with highest cash it has received till now (the priority is cash). This method is online and fast.
First, the algorithm starts crawling the web with some starting URLs. Every time K ( K is set to 250,000) new web pages are crawled; we run one of the ranking algorithms and sort the web pages in the queue con-ing and ranking algorithm is as the following. All methods run this procedure with their own ranking criteria ( Cho et al., 1998 ): Algorithm 1. The crawling algorithm
Input: starting _ url, K = 250,000 enqueue ( url _ queue , starting _ url ) while ( not empty ( url _ queue ))
The enqueue(queue, element) function appends an element at the end of queue, dequeue(queue) removes the element at the beginning of queue and returns it and reorder_queue(queue) reorders queue using a ranking algorithm as the following: 1. breadth first 2. backlink count 3. PageRank 4. DistanceRank
For comparison, we chose PageRank as an ideal ranking mechanism. First, ranks of all web pages are com-puted using PageRank algorithm on the entire graph. The aim of crawling is to find hot pages, pages with high We define throughput as the fraction of the crawled hot pages to all hot pages that can be discovered.
We compared previously discussed algorithms with DistanceRank in Fig. 2 for five millions web pages. In this experiment, damping factor of PageRank and b are set to 0.85 and 0.1 respectively. As the figure shows,
DistanceRank outperforms other algorithms in terms of throughput. For instance, when 65% of pages are crawled, DistanceRank finds around 81% of hot pages whereas batch PageRank and OPIC find 74% and 73% respectively. In comparison to PageRank and OPIC, DistanceRank has 5% more throughput. This result is achieved compared to the ideal PageRank. In other words; DistanceRank finds high important pages faster than PageRank.

Modelling a real user browsing on the web is the main advantage of DistanceRank. A user is going to find her favourite pages faster and by clicking and visiting a new page, she accumulates more knowledge about the environment. Thus, the next clicks are done with more information and a better selection.

In reinforcement learning algorithms, the learning rate ( a ) plays an important role in convergence of the system. In Fig. 3 , we compared throughput of DistanceRank for different learning functions such as exponen-of learning deceases slowly.
We consider throughput of DistanceRank for different b in the exponential learning rate. Fig. 4 shows the ing rate to decrease more slowly and simulates better the user behavior.

The second approach for comparison is rank ordering. In the rank ordering, we measure similarity between two ordered lists of PageRank and DistanceRank.

We used Kendall X  X  s metric ( Kendall, 1970 ) for correlation between two rank lists. In Kendall X  X  s metric, egy against a list of pages ordered with PageRank on whole graph. DistanceRank has highest Kendall, i.e. 0.75. Naturally, this factor depends on a and b . In our experiments, Kendall X  X  s between DistanceRank and PageRank was between 0.55 and 0.8.

To evaluate quality of results, we compared top 20 results from DistanceRank, PageRank and Google X  X  ranking (results from Google search engine on 20 April 2007) for some queries in a subjective manner. First, all of five millions pages are indexed. Then, after query processing, results are ordered by DistanceRank and
PageRank separately. We also forced Google engine to search query only at University of Berkeley X  X  web site (by site: .berkeley.edu instruction). Table 2 shows list (index) of all titles and URLs of these three ranking algorithms for  X  X  web AND crawling  X  X  query. Table 3 Shows list of top 20 results ordered by each algorithm. and low relevant (0 value). High relevant pages are specified by p cision of each algorithm was computed as the proportion of relevant pages to all the pages retrieved (20 pages). Table 3 shows the result. In this experiment, precision of DistanceRank was better than both Page-
Rank and Google X  X  rank. Intersection of results between Google and DistanceRank is 9, while intersection between Google and PageRank is 7. In other words, results of DistanceRank are more close to Google than
PageRank. Also the intersection between DistanceRank and PageRank is 10. Kendall factor between these 10 results from PageRank and DistanceRank is about 0.60. Furthermore, this experiment has been done for  X  X  X ORBA AND DCOM X  X  query that have been shown in Tables 4 and 5 .

Above experiments show that DistanceRank is a suitable algorithm for ranking of web pages. Exact qual-ification of DistanceRank needs more experiments that remain as future work. 4. DistanceRank convergence
DistanceRank, like PageRank in the ideal state, needs V iterations to converge. However, in practice we can get the same results with very less iterations. Several measures can be used to analyze the convergence speed.
One is the norm of difference between DistanceRank vectors from successive iterations. A more useful mea-sure is looking at the ordering of pages produced by DistanceRank vector. In the rank ordering, we measure similarity between two ordered lists of DistanceRank. In many scenarios, we are only concerned with top 1999 ). To visualize how closely two ranking match on identifying top pages, we successively computed simi-larity among top n pages in each ordering.

Therefore, we use a solution like the rank ordering to find the similarity between sets produced by different ing obtained by only 5 iterations agree closely with ordering of 20 iterations for 5 million pages. In other of algorithm can be reduced to O( p * j E j ) where p V and p shows number of iterations for convergence. 5. DistanceRank and ranking problem
One of the main problems in the current search engines is  X  X  X ich-get-richer X  X  that causes young high quality pages receive less popularity. In other words, popular high rank pages have more chances to be browsed by users and, consequently, young high quality pages can be the victims.
 To sense effect of this problem, in Cho and Roy (2004) , two models on how users discover new pages, Random-Surfer and Search-Dominant model have been introduced. In Random-Surfer, a user finds new pages only by surfing randomly (without search engine) while in Search-Dominant model user finds new pages only by search engine. The authors found that it takes 60 times longer for a new page to become popular under
Search-Dominant than Random-Surfer model. In Cho et al. (2005) , a solution has been proposed which is based on the PageRank formula. The formula applies the difference between PageRank of two snapshots to PageRank algorithm.

Obviously, if page p be a young high quality page, it will receive more input links than other young low pages will gather in the future. Thus, our goal is a ranking algorithm with good prediction of future ranking.
Fairly, we have found that the  X  X  X ich-get-richer X  X  problem is less important in DistanceRank in comparison with PageRank. To illustrate this, we computed PageRank for whole graph as the base rank or future ranking that we are going to predict. To produce web graphs in the past, we changed the base graph by removing some edges randomly and computed both DistanceRank and PageRank algorithms for sub graphs as partial rank-ing. Finally, s between the base rank and partial rankings was computed. So the algorithm with suitable pre-diction will have more Kendall factor. Fig. 6 shows the results. Naturally, for complete evaluation we need some snapshots for web graph that remains as future work.

As the figure shows, when there are more changes in the graph, DistanceRank predict their future ranks closer to the base rank in comparison to PageRank. For instance, when 70% of edges are removed, s is 0.36 and 0.32 for DistanceRank and PageRank respectively. Furthermore, we might able to conclude that Dis-tanceRank is more robust than PageRank in the rich-get-richer problem. Of course, it is not a fully convinced experiment and we need more experiments.

This gain is achieved since DistanceRank is based on an intelligent random-surfer model. We proved this property in lemma 2. Intuitively, young high quality pages have more relative rank increasing 2004 ) in two snapshots of web or increasing rate of their ranks is more than others. Because DistanceRank follow Q-learning, and Q-learning considers the difference between current and previous punishment, so it will predict high quality pages faster and its sensitivity to  X  X  X ich-get-richer X  X  is less than PageBank. Lemma 2. DistanceRank is more robust than PageRank in rich-get-richer problem.

Proof. It has been proved in Cho et al. (2005) that the rich-get-richer is improved by changing the rank com-putation as the following: where PR  X  p ; t  X  and Q  X  p ; t  X  show PageRank and quality value of page p at time t respectively and D PR t = PR ( p , t ) PR ( p , t 1). We can write this formula as
From Eqs. (3 and 4) we have
Thus we have
Since in DistanceRank we try to minimize distance (punishment), but in PageRank to maximize the rank value, then, we will have a factor. From Eqs. (6) and (7) we conclude that DistanceRank is less susceptible to rich-get-richer. h 6. Background and related work Roughly speaking, there are two categories of connectivity-based ranking algorithms ( Henzinger, 2001 ),
Query-independent and Query-dependent ranking. In the Query-independent ranking, a fixed score is assigned
Marin, Rodr X  X   X  guez, &amp; Baeza-Yates, 2004 ) are instances of the Query-independent scheme. In the Query-text. One of the well-known query-dependent solutions is HITS ( Kleinberg, 1999 ).

The PageRank, an instance of the query-independent ranking, has been designed taking into account the known-relation between web pages. For example, if page p 1 of the page to others pages. Clearly, the interest degree of a page increases with the growing number of input links. Furthermore, when a web page receives links from an important page, then, this page should have a high rank. Therefore, PageRank of a web page corresponds to the weighting sum of input links.

Let pages on the web be denoted by 1,2, ... , n , and O( i ) denotes the number of forwarding (outgoing) links connected graph, SCG, meaning that every page can be reached from any other page. The PageRank of page i , denoted by r ( i ), is given by
Thus, PageRank of page i is equal to sum of the input page ranks divided by their out degree. Dividing outgoing links to every single web page.
 PageRank can be written as a linear relation r = A [ r (1), r (2), ... , r ( n )] and elements a ij of matrix A are given by a a = 0 otherwise. Our goal is to compute r that is the eigen vector of A
This mechanism is equivalent to the Random-Surfer behavior, a person who surfs the web by randomly clicking links on the visited pages. When the user reaches a page with no output links she will jump to a ran-dom page. Therefore, when a user is in a web page, with probability of d she will select one output link ran-domly or will a jump to other web pages with the probability of 1 d .

Abiteboul et al. (2003) proposed an algorithm called OPIC (On-line Page Importance Computation). In their method, each page has cash values that are distributed to all output links. This is similar to PageRank and when a page is found its cash will be distributed to its output links.

HITS (Hypertext-Induced Topic Search) proposed by Kleinberg is a query-dependent method that parti-relevant content for that topic. A hub page is a gate page linked to authority pages. So a page with a high authority is pointed by pages with a high hub score and vice versa. Each page is also assigned two values, authority and hub. Thus, hub and authority vectors will have reinforcing effects which iterate to converge to a final value.

Fortunately, these algorithms work well and are parts of the current search engines. However, there are model view to remove sinking effect that will change quality of pages since the rank value is computed for a different graph. Secondly, most of them suffer from rich-get-richer problem. 7. Conclusion and future work
We proposed a new iterative ranking algorithm for web pages called DistanceRank in this paper., This algorithm is based on reinforcement learning with the distance between pages as punishment. In other words; we are going to minimize the distance values of pages. We define the distance between two pages i and j as the logarithm of the number of i  X  X  output links when i points to j . Our algorithm models a real user surfing the web. When a user randomly browses the web, she selects the next pages based her background from the last state properly.

Naturally, the distance of each page can be computed from its inputs links. DistanceRank algorithm recur-sively iterates to converge to a static value. Finally, we have a vector as DistanceRank vector. Then, we sort the vector in the descending order and the pages with low distance will have high ranking.
The convergence speed of our algorithm is fast with a little number of iterations. We found experimentally that DistanceRank outperforms other ranking algorithm in page ranking and crawling scheduling.
In DistanceRank, it is not necessary to change the web graph for computation. Therefore, some parameters like the damping factor can be removed and we can work on the real graph. We also found partially and intu-The complete evaluation of this property will be done in the future.

We used University of California at Berkeley X  X  Web to evaluate the proposed algorithm. We will use a big-spamming ( Henzinger, Motwani, &amp; Silverstein, 2002 ) remains as future work. We are also going to combine the logarithmic distance in PageRank, HITS and OPIC algorithms which believe will yield better results. References
