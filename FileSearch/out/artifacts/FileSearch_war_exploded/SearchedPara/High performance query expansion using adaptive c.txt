 1. Introduction
Relevance feedback improves the query representation by taking feedback information into account via query expansion. A classical relevance feedback algorithm was proposed by Rocchio (1971) for the SMART retrieval system ( Salton, 1971 ). It takes a set of documents as the feedback set. Unique terms in this set are ranked in a descending order of tf.idf weights. A number of top-ranked expansion terms are then added to the query, and finally documents are returned for the expanded query.
Many other relevance feedback techniques and algorithms have been developed, mostly derived from Rocchio X  X  algorithm ( Amati, 2003; Carpineto, de Mori, Romano, &amp; Bigi, 2001; Miao, Huang, &amp; Ye, 2012; Robertson, 1990; Robertson, Walker,
Hancock-Beaulieu, Gatford, &amp; Payne, 1995; Ye, He, Huang, &amp; Lin, 2010 ). In addition, the relevance-based language model makes use of feedback information by estimating the generation probability of both the query and the feedback documents from a latent relevance model ( Lavrenko &amp; Croft, 2001 ). The feedback documents can be obtained by many possible means. In general, some methods utilize explicit evidence, such as labeled relevant documents from real users, while others use implicit evidence, such as the click-through data. Obtaining feedback information involves extra efforts, e.g. relevance judgment by real users, which is usually expensive. For every given query, the corresponding feedback information is not necessarily avail-able. An alternate solution is pseudo-relevance feedback (PRF), which uses the top-ranked documents in the initial retrieval as (QE). The basic idea of QE based on PRF is to extract expansion terms from the top-ranked documents to formulate a new query for the second round retrieval. Through QE, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Particularly, QE refers to QE based on PRF in the rest of this paper.  X 
Despite the marked improvement over the initial retrieval performance ( Amati, 2003; Robertson et al., 1995 ), QE can also fail. There have been many studies on QE X  X  effectiveness. For example, a wide range of predictors were proposed to indicate the query performance, which is usually correlated with QE X  X  effectiveness ( Amati et al., 2004; Carpineto et al., 2002; He and Ounis, 2009b ). All these previous studies have agreed on a conclusion that the quality of feedback documents is crucial to
QE X  X  performance. Since feedback documents are not assessed by real users when using QE, the quality of the feedback doc-ument set is not guaranteed. In particular, a feedback document may not be useful even if it is relevant to the query topic  X  the document could be just partially relevant, and there might be only a small part of the document that is about the query topic, while the rest of the document is irrelevant. In this case, off-topic expansion terms are added to the query, leading to degraded retrieval performance. Relevance is not enough to judge whether the document can help QE to improve the perfor-mance in such a scenario. Thus, we use  X  X  X uality X  X  instead of  X  X  X elevance X  X  to evaluate a feedback document. Particularly, a  X  X  X ood/positive X  X  quality feedback document is not only completely relevant but also effective in improving the final perfor-mance of QE. Hence, there is a crucial need for selecting good quality feedback documents ( He and Ounis, 2009b; Ye et al., 2011 ). Details about the quality of a feedback document are described in Section 6.2 .

Recently, there have been efforts in applying machine learning methods to find good feedback documents or expansion terms ( Cao et al., 2008; Lee et al., 2008; He and Ounis, 2009a ). However, the potential effectiveness of learning techniques in labeled feedback documents in our case. To solve the problem of traditional QE algorithms, which leads to overfitting be-cause of the limited amount of training data and large term space, Xu and Akella (2008) propose an online Bayesian logistic regression algorithm to select good feedback documents for QE. However, human efforts are involved to update the features in an iterative way which adds extra costs to its implementation and applications.
 In this paper, we propose to incorporate co-training into a retrieval system in an adaptive manner at the feedback stage.
The proposed method is called adaptive co-training (AdapCOT) in this paper. Co-training is an effective method for utilizing unlabeled data for classification, which we believe is suitable for finding good feedback documents with only limited training data available. In co-training, the input data is used to create two predictors that are complementary to each other. Each classifier is then used to classify unlabeled data. After that, the classified data are used to train the other complementary classifier. Typically, the complementarity is achieved either through two redundant views of the same data ( Blum and
Mitchell, 1998 ) or through different supervised learning algorithms ( Goldman and Zhou, 2000 ). In our proposed approach, after the initial search, we create a training data set by choosing the top-ranked documents from the ranked list as positive examples and the other set of documents from the bottom of the list as negative examples. The remaining documents are treated as the test set. The top-ranked documents are usually regarded as highly relevant, and have a general aboutness of the query topic throughout them. Features representing the documents are split into two exclusive sets. Then, we classify the examples in the test set, label them, and add the most confident ones to the labeled data for the next training round. Such a process proceeds for a few iterations. In addition, at each iteration, the quality of the labeled data is monitored. The co-training process stops if the learning quality on the labeled data is below a predefined threshold. Through the above described co-training process, documents similar to the positive examples, which are considered highly related to the query topic, are added to the feedback set. In contrast, documents that are only partially relevant are discarded for relevance feed-back. Finally, the resulting labeled positive documents are used for relevance feedback.

We propose an adaptive co-training method to select feedback documents for QE, which is the main contribution of this paper. Since the quality of feedback documents is crucial to QE X  X  performance, the proposed AdapCOT method markedly im-proves the effectiveness and robustness of QE. To the best of our knowledge, this is the first study that successfully applies co-training, an effective learning paradigm, for selecting feedback documents in ad hoc retrieval. Meanwhile, taking the quality of learned classifiers into account makes the QE process more adaptive, and therefore our proposed method is robust on different kinds of data. Extensive experiments on five standard TREC collections show that our proposed approach out-performs strong baselines.
 The remainder of this paper is organized as follows. Section 2 summarizes related work. Section 3 explains our proposed
AdapCOT method in details, and Section 4 introduces the classifiers used in our study. The proposed method is then evalu-ated through extensive experiments in Sections 5 and 6 . Finally, Section 7 concludes the paper and suggests future research directions. 2. Related work Co-training using unlabeled data has shown it effectiveness for reducing error rates in text classification ( Blum and
Mitchell, 1998; Goldman and Zhou, 2000; Joachims, 1999; Nigam et al., 2000; Zelikovitz and Hirsh, 2001 ). The idea of co-training is originally proposed by Blum and Mitchell for boosting the learning performance when there is only a small amount of labeled examples available ( Blum and Mitchell, 1998 ). Under the assumption that there are two redundant but not completely correlated views of an example, unlabeled data are shown to be able to augment labeled data ( Blum and
Mitchell, 1998 ). Co-training is also used for extracting knowledge from the World Wide Web ( Craven et al., 2000 ), or for email classification ( Kiritchenko et al., 2004 ). The result shows that it can reduce the classification error by a factor of two using only unlabeled data. However, the performance of co-training depends on the learning methods used ( Craven et al., 2000; Kiritchenko et al., 2004 ).
There are a number of other studies that explore the potentials of co-training in recent years. Modified versions of the co-training method have been proposed. Goldman and Zhou (2000) use two different supervised learning algorithms to label data for each other. Raskutti presents a new co-training strategy which uses two feature sets. One classifier is trained using data from original feature space, while the other is trained with new features that are derived by clustering both the labeled and unlabeled data ( Raskutti et al., 2002 ).

All the above work shows that the co-training method can be used to boost the performance of a learning algorithm when there is only a relatively small set of labeled examples given. However, this idea is based on the assumption that the dataset comes with two sets of features which are distinct in nature, but this is not the case in document retrieval. Chan et al. (2004) suggest to randomly split one single feature set into two sets for co-training. Their experimental results show that a random split of the feature set leads to comparable, and sometimes, even better results than using the natural feature sets. Huang et al. (2006) investigate the effect of Chan et al. X  X  version of co-training on the TREC HARD track data for passage retrieval by employing a dual-index model for term weighting. The main work of Huang et al. (2006) applies co-training to identify more relevant passages based on a small set of training data and use the results to re-estimate parameters of the probabi-listic weighting function, BM25.

Recently, there have been efforts in applying machine learning methods to find good feedback documents or expansion terms. Lee et al. apply a clustering-based resampling method to select good feedback documents based on the relevance model. Relevance density is utilized to evaluate the quality of feedback documents. Cao et al. use features such as the prox-imity of expansion terms to the query terms, the expansion terms and query terms co-occurrences to predict which expan-sion terms are useful ( Cao et al., 2008 ). Based on similar features, He and Ounis select good feedback documents using standard machine learning algorithms ( He and Ounis, 2009a ). All methods proposed in Lee et al. (2008), Cao et al. (2008), and He and Ounis (2009a) provide a moderate success over a traditional QE baseline.

In this paper, we propose to incorporate an adaptive co-training method for selecting feedback documents which is a sub-stantial extension of Huang et al. (2006) .In Huang et al. (2006) , an initial study of applying co-training for QE was proposed over a small TREC HARD track dataset with 23 queries for passage retrieval. The basic idea of this method is to initialize a co-training process by taking the top-ranked and bottom-ranked passages in initial retrieval as positive and negative examples, respectively. The features representing the passages are randomly split into two sets, on which two different classifiers are learned respectively to label the remaining documents. Despite the limited but encouraging improvement on the TREC HARD track dataset, the approach ( Huang et al., 2006 ) for selecting feedback documents using co-training suffers from the follow-ing issues. First, the random feature division may lead to the situation when the discriminative power of the features is com-pletely unbalanced in the two sets. In this case, one of the classifiers may not be properly learned due to the low-quality features it has. Second, the previous approach is unable to cope with queries for which the top-ranked documents in the ini-a particular number of iterations have been done. Thus, new feedback documents will be added even if the trained classifiers are of poor quality. As a result, the new feedback documents are not reliable and could bring negative effects into the QE process. In this case, we should stop the co-training process to avoid unreliable feedback documents. Hence, The AUC mea-sure ( Witten and Frank, 2005 ) is introduced for monitoring the quality of the learned classifiers. approximations of integrals. It adds up the area of all trapezoids (green this paper to keep consistent with our previous work ( Huang et al., 2006 ).

The iterative co-training process ends when AUC is lower than a given threshold. This adaptive criterion is different from that in the previous work ( Huang et al., 2006; Xu and Akella, 2008 ), whose halting condition of iterations is only determined by a predefined value. Details about how to set this threshold will be discussed in Section 6 . Compared to this preliminary work, this paper includes additional learning models, extensive experiments on more datasets, more systematic result anal-yses, more comprehensive discussion and more conclusive findings. 3. Adaptive co-training for QE
We propose an adaptive co-training method for selecting the feedback documents in this section. Section 3.1 introduces the document representation for the classification and Section 3.2 describes the proposed method in details. 3.1. Document representation
The standard co-training method assumes that an instance comes with two complementary sets of features in nature. For example, the features that describe a Web page can be the words on the page and the links that point to that page ( Blum and
Mitchell, 1998 ). However, for text retrieval, it is not obvious how to derive two complementary sets of features to represent the feedback documents, while the link information is usually not available.

There are quite a few options that we can apply to represent the feedback documents. In summary, these options can be grouped into two main categories, namely the term-based features, and the high-level features.

The term-based document representation characterizes a feedback document by its composing terms. A typical example is to represent a feedback document by a vector of the expansion term weights in the candidate feedback documents, where the weight of an expansion term is estimated by its normalized document generation probability ( Rocchio, 1971; Salton, 1971 ). In contrast to the term-based document representation, other related studies use relatively high-level features, such as the proximity of the expansion term and the original query terms, the co-occurrences of the expansion term and the ori-ginal query terms in the collection ( Cao et al., 2008; He and Ounis, 2009a ). The high-level features are usually considered having a higher descriptive power of the feedback documents than the term-based features, as they are found to be impor-high-level features for co-training in practice: it is expensive to compute the features such as the proximity of the expansion term and the original query terms, for each retrieved document. In particular, since such features are query-dependent, which means their values change from query to query, they have to be computed during retrieval time. This is infeasible for large-scale collections. Therefore, in this paper, we rather follow the term-based document representation.
For D , the set of retrieved documents for a given query, we rank unique terms in D by a descending order of their Kull-back X  X eibler Divergence (KLD) weights. KLD is a popular choice of expansion term weighting, which has been shown to be effective in many state-of-the-art QE methods ( Amati, 2003; Ye et al., 2009 ). For example, a KLD-based QE mechanism pro-vides the best statMAP over the standard feedback sets in the TREC 2009 Relevance Feedback track ( Ye et al., 2009 ). KLD measures how a term X  X  distribution in the feedback documents diverges from its distribution in the whole collection. The higher KLD is, the more informative the term is. For a unique term in D , the KLD weight is given by: the whole collection C .

The maxN terms with the highest KLD weights in D are taken as the features to represent the documents. In each docu-ment d in D , the weight of a feature term t is again given by the Kullback X  X eibler divergence of the term X  X  distribution in d from its distribution in the whole collection. maxN is a parameter, which is obtained by tuning over a set of training topics. Once the feedback documents are represented by the feature terms which belong to the set F in Fig. 2 , they are labeled by the
AdapCOT method that is described in the next section. 3.2. Adaptive co-training
In this section, we devise an adaptive mechanism to apply co-training over a small set of training data which contains a few positive and negative documents in this case. Positive documents are those of good quality, and the negative ones are on the opposite side. In the proposed iterative co-training process, the halting criterion automatically adapts to the quality of the learned classifiers. It monitors the quality of the learned classifiers at the end of each iteration using the AUC measure.
The co-training process stops when the quality of the learned classifiers is below a given threshold, which will be discussed later in this paper. The basic idea of our proposed method is to represent the documents by important terms in the retrieved set, which are highly weighted. A small number of top-ranked and bottom-ranked documents from the initial retrieval are used as positive and negative examples, respectively. The top-ranked documents are assumed to be of a high quality, and provide a good coverage on the query topic in their content. In contrast, the bottom-ranked documents are used as negative examples in which the content is assumed to be off-topic. A learning procedure is designed to label the retrieved documents so that the documents selected for QE are meant to be highly similar to the positive set, while being dissimilar to the neg-ative set.

The proposed method learns two different classifiers from two non-overlapped sets of features which represent the doc-uments. More specifically, it allows the two classifiers, C1 and C2, to label the unlabeled instances, i.e. the rest of the re-trieved documents, for each other through an iterative process until one of the halting criteria is met. The positively labeled documents are then used for feedback in QE.

Fig. 2 gives a general description of the proposed AdapCOT method. The proposed method is based on the initial retrieval results returned by the search engine. The co.p top-ranked and the co.n bottom-ranked documents are used as the labeled positive and negative examples, respectively. The rest of the retrieved documents remain unlabeled. As explained in the previous section, maxN unique terms with the highest KLD weights are used as features to represent the retrieved docu-ments. To facilitate the co-training process, these maxN terms are split into two different sets. Each term acts as one feature of the documents in our AdapCOT method. Instead of a random split suggested in Chan et al. (2004) , we rank these maxN terms in decreasing order of their KLD weights, and group them into an odd-ranked set and an even-ranked set, respec-tively. One of the advantages of our split method over the random split is that our method balances the quality of terms in the two feature sets, 4 and avoids the case that one of the classifiers is not properly learned due to an extremely unbalanced split.

Once the feature terms are split into two sets, the two classifiers are learned on the labeled documents one by one, and label the remaining unlabeled documents for each other. Such a co-training process ends when one of the halting criteria is met. In our proposed method, we introduce two halting criteria as follows. First, the co-training process ends after co.K iterations. Second, the co-training process ends if AUC, a measure used for evaluating the successfulness of the classification over the labeled examples, is below a given threshold co.AUC. The second criterion is a key step in the pro-posed method. It ensures that the learned classifiers are good enough to classify the unlabeled documents. It is particu-larly useful when the initial retrieval is of a poor quality, and the  X  X  X ad X  X  feedback documents would not be labeled as positive since the co-training process does not proceed further when the classifiers are not properly learned. Note that the
AUC value is computed using the documents labeled by the learned classifiers, and the actual relevance assessment infor-mation is not used. Weka, 5 a powerful data mining tool, is used to calculate the value of AUC for the implementation of our AdapCOT.

Furthermore, in order to guarantee the quality of the labeled positive documents for relevance feedback, we introduce the following restrictions ( He and Ounis, 2009b ):
Only the 50 top-rankeddocuments in the initial retrieval can be added to the labeled positive documentset. This is based on the empirical observation that QE is unlikely to benefit from a feedback document that is roughly ranked lower than 50.
When two labeled positive documents receive an identical confidence value from the classifier, the one ranked higher is preferred. This is also based on the empirical observation that the higher ranked documents are likely to be better feed-back documents than the lower ranked ones.

In addition, our proposed AdapCOT method automatically labels the most confidently predicted positive documents for relevance feedback. It is inexpensive to implement since the co-training process does not involve any relevance assessment information by human effort. 4. Classifiers
In our studies, we choose to apply three different learning methods to facilitate the proposed adaptive co-training meth-od. These methods are among the most popular machine learning methods applied in IR, including the low-cost Naive Bayes and Logistic Regression, and the relatively sophisticated support vector machines. Since our work in this paper is a significant have three combinations as shown in Table 2 .
 Naive Bayes (NB) is a simple statistical learning algorithm based on Bayes X  theorem with strongindependence assumptions.
Despite their naive design and simpleindependence assumptions, NB classifiershave worked quite well in manycomplexreal-world situations ( Witten and Frank, 2005 ). In this paper, we apply the popular Gaussian kernel density function as follows: where the probability P ( x j C i ) of observing an instance x in class C standard deviation r i and mean l i .
 Logistic Regression (LR) is a generalized linear model for binomial regression ( Witten and Frank, 2005 ). The basic idea of
LR is to apply a logit function to scale membership values, which may not be proper probabilities, to values between 0 and 1: where a membership value z is mapped to a value between 0 and 1.

Support vector machines (SVMs) ( Joachims, 1999 ) are widely used in text classification in recent years. Its underlying principle is structure risk minimization. Its objective is to determine a classifier or regression function which minimizes the empirical risk (i.e., the training set error) and the confidence interval (which corresponds to the generalization or test set error). Given a set of training data, an SVM determines a hyperplane in the space of possible inputs. This hyperplane will attempt to separate the positives from the negatives by maximizing the distance between the nearest positive examples and and negative examples. There are several ways to train SVMs. One particularly simple and fast method is the sequential min-imal optimization ( Platt, 1999 ) which is adopted in our study. In addition, we apply the non-linear homogeneous polynomial kernel function at degree m as follows: where x i and x j are real vectors in a p -dimensional space, and p is the number of features. The exponential parameter m is set to 1 as a default value in our study. In the following sections, the proposed co-training method is evaluated through exten-sive experiments. 5. Experimental setup
We evaluate our proposed AdapCOT method on most of the existing TREC datasets with ad hoc topics, including the in Table 1 .
Thedisk1&amp;2anddisk4&amp;5(noCongressionalRecordaccordingtotherobusttrackofTREC2004and2005)collectionscontain newswire articles from various sources, such as Associated Press (AP), Wall Street Journal (WSJ), and Financial Times (FT), which are usually considered as high-quality text data with little noise. The WT10G collection is a medium size crawl of
Web documents, which was used in the TREC 9 and 10 Web tracks. It contains 10 GB of uncompressed data. The .GOV2 collec-tion, which is 426 GB in size and contains 25 million documents, is a crawl from the .gov domain. This collection has been em-ployed in the TREC 2004, 2005 and 2006 Terabyte tracks. The ClueWeb09 collection is a very large crawl of the Web, and is currently the largest TREC test collection. We use the category B of ClueWeb09,
Web pages, and its associated topics used in the TREC 2009 Relevance Feedback track. We index all documents in the above five collections. For all five test collections used, each term is stemmed using Porter X  X  English stemmer, and stopwords are removed.
Each topic contains three topic fields, namely title, description and narrative. We only use the title topic field that con-tains very few keywords related to the topic. The title-only queries are usually as short as a realistic snapshot of real user queries in practice ( Xu and Akella, 2008; Zhai and Lafferty, 2001 ). On each collection, we evaluate our proposed model by we use nine subsets of test topics for training, and use the remaining subset for testing. The overall retrieval performance is averaged over all 10 test subsets of topics. We use the TREC official evaluation measures in our experiments, namely the stat-
MAP at 1000 on ClueWeb09 ( Aslam et al., 2006 ), and the Mean Average Precision (MAP) at 1000 on the other four collections examples for the co-training. Only the top 1000 retrieved documents are involved in the co-training process. tests are based on Wilcoxon matched-pairs signed-rank test at the 0.05 level.

In our experiments, we apply Okapi X  X  BM25 for document ranking. BM25 is a classical probabilistic model based on approximation to the assumed two-Poisson term frequency distribution ( Robertson et al., 1995 ). It assigns the relevance score for a document d with respect to a given query Q by ( Robertson et al., 1995 ): where tf is the frequency of the given term t in the document. k 1995 ). w (1) is the raw term weight, which is given by the re-written point-5 formula as follows: where N is the number of documents in the collection, and N lection. qtw is the query term weight, which is given by: where qtf is the number of occurrences of the given term in the query. k ertson et al., 1995 ). K is: K  X  k 1  X  1 b  X  X  b l a v g l length in the collection, respectively. The length refers to the number of tokens in a document. b is a hyper-parameter. In this paper, it is obtained by Simulated Annealing ( Kirkpatrick et al., 1983 ) over the training topics on the collection used ( Robertson et al., 2004 ).

We apply the KLD weighting for query expansion ( Ye et al., 2009 ). The applied QE algorithm considers the top-ranked doc-uments as the feedback set D f . An expansion weight w ( t ) is assigned to each unique term in D computed over each feedback document by Eq. (1) . Particularly, D in Eq. (1) refers to the selected feedback documents in D
The j expT j terms with the highest mean KLD weights over D terms. In our experiments, we set j expT j , the number of expansion terms to be added to the query, to 20, as it is shown to be effective in some previous studies ( B X ttcher and Clarke, 2006; Collins-Thompson, 2009; Keikha et al., 2011 ).
We mainly report on the results obtained by setting co . init . p , the initial number of top-ranked documents used as positive negative examples than the positive ones. Moreover, in the experiment conducted by Blum and Mitchell (1998) for classifying new positive and three new negative examples to L . In this paper, in order to reduce the number of parameters for tuning, we co-training process, is set to 3 according to the recommendation in Huang et al. (2006) . Thus, at most three positive and nine negative examplesare added to L .If co . K is 0, AdapCOTacts inthe same wayas traditionalQEmethods do. Other controlparam-eters, namely the number of terms used for document representation ( maxN ), and the threshold co.AUC , are obtained over the training data in the cross-validation process. The related evaluation results are presented in the next section. 6. Experimental results
We evaluate our proposed AdapCOT for selecting feedback documents against different baselines, namely the initial re-trieval using BM25, query expansion (QE) using top-ranked documents, and the baseline co-training method (baseCOT) without the adaptive quality control proposed for AdapCOT. 6.1. Comparison with initial retrieval
In the first step of our experiments, Table 2 compares AdapCOT X  X  retrieval performance with the an initial retrieval base-forms the BM25 baseline in all cases. This is not of a great surprise since AdapCOT applies QE, which usually improves the the rest of the paper, only the results obtained by using LR and SVM for co-training are presented because this combination gets four best results over all the five collections.

In addition, we examine the accuracy of the co-training method in selecting feedback documents. Regarding the  X  X  X ood-ness X  X  of a feedback document, we consider the following three different categories: (1) by using the document alone for rel-evance feedback, a  X  X  X ood X  X  feedback document leads to at least 5% improvement over AP, the average precision of the query in initial retrieval; (2) a  X  X  X ad X  X  feedback document leads to a decrease in AP by at least 5%; and (3) other feedback documents are considered  X  X  X eutral X  X .

Table 3 presents the percentage of the feedback documents selected by AdapCOT in the above three categories. Overall, a moderate accuracy of the AdapCOT method is observed. A major proportion of the feedback documents picked up by Adap-
COT, about 2 out of 3, fall into either  X  X  X ood X  X  or  X  X  X eutral X  X  categories. The remaining  X  X  X ad X  X  feedback documents may hurt the retrieval performance. However, the impact of  X  X  X ad X  X  feedback documents could be neutralized since they are to some degree outnumbered by the  X  X  X ood X  X  ones. In the rest of this section, we show that the moderate accuracy of AdapCOT indeed leads to improved retrieval performance over strong baselines. 6.2. Comparison with QE
Next, we compare our proposed AdapCOT method with the QE baseline. QE is based on the assumption that the top-ranked documents are relevant, from which the most important terms are useful for retrieving more relevant documents.
The size of the feedback document set has considerable impact on QE X  X  effectiveness. In contrast, the AdapCOT method fol-lows a slightly different assumption. It assumes the positivity of a small set of top-ranked documents, and the negativity of another small set of bottom-ranked documents. It selectively adds other retrieved documents to the positive document set through an iterative process until a halting criterion is met. Therefore, we compare the retrieval performance of AdapCOT with QE using the same number of top-ranked documents ( j D ples for co-training, is set to j D f j . Meanwhile, we use the same number of expansion terms for both AdapCOT and QE, which is of the QE baseline and AdapCOT against different settings of j D and Fig. 3 is that AdapCOT in general outperforms the QE baseline with different j D well on the large-scale .GOV2 and ClueWeb09 collections, where it achieves statistically significant improvement over the QE baseline even if j D f j is optimal for QE. AdapCOT also appears to be more robust than the QE baseline. In most cases, Adap-
COT improves the QE baseline with different j D f j settings, even if the QE baseline has very poor retrieval performance. Be-sides, in Tables 2 and 4 , optimized BM25 outperforms the QE baseline and the AdapCOT in some cases, but generally QE and AdapCOT obtain better results when j D f j is optimal. Moreover, the best result of the QE baseline on .GOV2 (MAP 0.3003) is not so good as that of BM25 (MAP 0.3041), whereas AdapCOT (MAP 0.3243) still surpasses BM25 significantly when j D f j is set to 3. This indicates the robustness of our AdapCOT method.

Furthermore, we examine whether using AdapCOT would give a higher percentage of good feedback documents than just taking the top-ranked documents in Table 5 . Although this is indeed the case on some of the collections used, it is surprising that the retrieval performance of both the QE baseline and AdapCOT is not necessarily correlated with the percentage of good feedback documents when j D f j becomes large. By further analysis on the experiments, we discover that the effectiveness of
QE depends heavily on some key documents, which have the most important contribution to the expanded query. Therefore, if these key documents are included in the feedback set, the retrieval performance is usually good as long as the good feed-back documents are not outnumbered by the bad ones. In this case, AdapCOT is particularly useful when the key documents are not highly ranked, which would not be picked up the QE baseline. A typical example is query 195 on disk1&amp;2, for which the initial average precision (AP) obtained by BM25 is 0.0273. For this query, the document with id AP891017-0231 has the most contribution to the expanded query. However, as this key document is only ranked 35th in the initial retrieval, it is not used for feedback by the QE baseline. In contrast, AdapCOT manages to automatically identify this key document and add it to the feedback document set. It is then of no surprise that the QE baseline is unable to improve over the initial retrieval, while AdapCOT provides an AP of 0.1638 with a 500% improvement. 6.3. Impact of co.maxN and co.AUC
We also study the impact of the other two control parameters, namely the threshold ( co.AUC ) and the number of terms used for the document representation ( co.maxN ). Figs. 4 and 5 plot the retrieval performance of AdapCOT against these two control parameters, respectively. It seems that the sensitivity of AdapCOT X  X  performance to the parameters depends on the collection used. Overall, AdapCOT X  X  parameter sensitivity is relatively high on the three Web collections, namely WT10G, .GOV2 and ClueWeb09, while being low on the other two collections. On average, a co . maxN value around 100 is shown very large Web collections, namely .GOV2 and ClueWeb09, we observe a relatively high variance of the retrieval performance over different co . AUC settings, while co . AUC = 0.30 leads to the best retrieval performance in average over these two very large Web collections. 6.4. Comparison with baseCOT
A major advantage of our proposed approach is to employ a thresholding strategy that carries out a quality control on the learned classifiers. Therefore, we also compare our proposed AdapCOT with the original co-training method without the introduction of the threshold, which is equivalent to the algorithm described in Fig. 2 without steps 2.(2) and 2.(8). By apply-ing this baseline co-training method (baseCOT), the co-training process proceeds for co.K iterations, regardless of the quality of the learned classifiers. As shown in Table 6 , our proposed AdapCOT method indeed improves retrieval performance over the original co-training baseline, particularly on the three Web collections. This is possibly due to the heterogeneous nature of these three Web collections, in which documents tend to be noisy, and the quality of training data becomes crucial for designing a co-training process to find good feedback documents.
 Additionally, our proposed AdapCOT method does not cost more time than the QE significantly in our experiments. The
AdapCOT method has extra computational cost because of the co-training process. However, this cost depends on the are obtained when maxN is relatively small. The number of iteration in our experiments is set to 3. Since one iteration only consumes 8 X 10 s on our workstation(P4 2.6G, 4G RAM, 1.5T HDD), the extra time cost is not extensive when compared to the
QE baseline. 6.5. Comparison with term selection method In this section, we compare our proposed AdapCOT with the state-of-the-art term selection method (TS) ( Cao et al., 2008 ).
The basic idea of TS is to select expansion terms using SVM based on a list of features, including the distance of the expansion method that applies SVM to select expansion terms. It is a strong baseline, which has shown marked (up to 28.36%) improve-ment over the KL-divergence language model ( Cao et al., 2008 ).

In order to establish a fair comparison, we apply AdapCOT on the same collections with the same test queries used in Cao et al. (2008) . Table 7 provides the related experimental results. Our AdapCOT method is shown to be generally more effective than TS. AdapCOT provides a moderate, up to 5.21% improvement over TS on the three test collections used. This indicates that AdapCOT X  X  retrieval performance is at least comparable to, if not better than, the state-of-the-art TS method for enhanc-ing QE effectiveness. 6.6. Summary
In summary, the proposed AdapCOT method outperforms both the initial retrieval and the QE baselines in our experi-ments. The settings of the control parameters co . init . p and co . AUC have a considerable impact on AdapCOT X  X  effectiveness, particularly on the large-scale Web collections. Moreover, it is indeed helpful to introduce a parameter co . AUC to monitor the soundness of the learned classifiers. The AdapCOT method improves co-training as shown in our experiments. Finally, we compare our proposed AdapCOT method to the state-of-the-art TS method. These two methods follow different ap-proaches to providing fine-grained feedback information for QE. The TS method selects expansion terms from a large number of feedback documents. Despite its effectiveness, there is still a potential in improving QE due to the fact that a majority of the feedback documents are not closely related to the query topic, and hence can be discarded. In contrast, AdapCOT refines the feedback set by removing the noisy documents, based on the idea that the partially relevant documents can actually hurt
QE. Our evaluation results have demonstrated considerable and statistically significant improvement brought by AdapCOT, showing that it is indeed crucial to distillate the good feedback documents before QE takes place. 7. Conclusions and future work
An important conclusion of this paper is the necessity to distillate good feedback documents for QE. A good feedback doc-ument should have a general interest in the query topic throughout itself, while a bad feedback document, even if it is rel-evant, may have only a passing-by interest in the query topic, and contains many off-topic terms. Therefore, there is a need for distillating the high quality feedback documents to improve QE X  X  effectiveness and robustness. To address this problem, we have proposed an adaptive co-training method, called AdapCOT, to find good feedback documents for QE. The proposed method overcomes a major problem of applying machine learning methods to QE, namely the lack of proper training data.
On five TREC collections, extensive experiments confirm our argument that QE X  X  retrieval performance can be improved by selecting high quality feedback documents. According to the experimental results over five standard TREC test collections, our proposed AdapCOT leads to considerable and statistically significant improvement over the QE baseline. In particular,
AdapCOT is very effective on the large-scale Web collections, showing that our proposed method can be applied in Web envi-ronment. Moreover, current IR systems can benefit from our proposed method by a good choice of feedback documents, since most of them offer the functionality of QE in the form of feedback based on a pseudo relevance set.

In the future, we plan to investigate ways to further improve the AdapCOT method. For example, we can employ a co-training process that makes use of two nature sets of document features, namely the term-based features ( Huang et al., 2006 ) and the high-level features ( He and Ounis, 2009a; Cao et al., 2008 ). As mentioned in Section 3.1 , a major obstacle of applying the high-level features for co-training is the associated computational cost. However, we may still examine if the use of the high-level features is workable in a laboratory setting.
 Acknowledgements This research is supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada, the Early
Researcher Award/Premiers Research Excellence Award and the IBM Shared University Research (SUR) Award. We thank anonymous reviewers for their valuable and detailed comments on this paper. We also would like to thank IBM Canada for providing IBM BladeCenter blade servers to conduct experiments reported in the paper.
 References
