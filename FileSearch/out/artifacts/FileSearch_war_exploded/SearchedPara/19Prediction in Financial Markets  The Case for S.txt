 Machine learning has seen successful published applications in a number of application areas that include the natural sciences, engineering, medicine, and business. The no-table exception is financial markets, where published research on predictive modeling is scant. Some argue that to the extent there is predictability, there is little incentive to publish research that  X  X orks X  since a good discovery can be very financially rewarding. Skeptics would argue that there is little scope for predictability since markets tend to be efficient. The reality is that there continues to be considerable interest and effort at finding structure and predictability in financial markets.

Financial time-series forecasting is difficult because of the inherently noisy nature of the domain. It is commonly known that most forecasting models do a very poor job in predicting future returns. It is also commonly known that return predictions of typical financial time-series forecasting models are usually very close to the mean because of the inherent difficulty of making accurate forecasts, particularly for the larger values of returns. Figure 1 demonstrates this phenomenon showing actual returns from the S&amp;P500 and predicted returns from a predictive model. Notice the large variance of actual returns relative to predicted returns, which means that a standard measure of error that compares actual to predicted returns (such as Mean Squared Error (MSE) or Mean Absolute Error (MAE) is dominated by the large actual values that contribute towards most of the error. This situation is particularly discouraging since it is the really the large future values that we really care about predicting well. Predictions around zero are relatively uninteresting in that we would not take any action in such instances.

Machine learning methods provide a potential solution to the problem of predictions being close to the mean because of their ability to easily partition the data into multiple sets and build a collection of models suited to each partitioning of the data. Some partitions can make large positive or negative predictions, while the others make predictions close to the mean, with the smaller partitions typically making the more extreme predictions. For example, Figure 2 shows a regression tree that recursively partitions a dataset into smaller subsets, with each subset representing the conjunction of conditions/splits that lead to it.

The dataset in this example consists of 1000 cases with mean of zero and standard deviation of 1. Notice that the rightmost partition covers only 3% of the dataset (30 cases out of 1000), but its mean is 1, indicating that on average the partition represents a significant positive return in the data. The partition represents the following rule. The independent variables are observed at the current time. These are derived from the original time series of prices. In the preceding example, one of the derived independent variables is the previous five period return ( Ret5 ), and another is the previous two pe-riod return ( Ret2 ). The prediction is the expected return in the next period, designated as Fret mean in the previous rule. We can similarly follow the other paths in the tree and identify the corresponding rules that correlate combinations of the independent variables to the dependent variable. Some rules will have more extreme values of the dependent variable, which means that their means are distant from the overall mean of the data. The nodes on the left in Figure 1 are relatively uninteresting since the expected returns associated with these conditions are close to the overall mean of zero. These situations would result in no action on the part of a mechanical decision making system since the expected return in such situations is negligible.

The obvious drawback of this recursive partitioning approach is its tendency to find data partitions that make extreme predictions, but which  X  X verfit X  to the data in that they happen by chance alone to result in large absolute values for the dependent variable. This is known as  X  X itting a model to the noise X  [Lee 1999]. The potential benefit, on the other hand, is its ability to model genuine but infrequently occurring situations as separate submodels.
 A collection of low support situations such as the one in the bottom right node of Figure 2 are referred to in the literature as  X  X mall disjuncts X  [Weiss 2000] since the learned patterns are alternative ways of making the same prediction. This article makes the case that good predictive models can be based on collections of certain types of small disjuncts.

A partitioning of the data as in Figure 2 is based on  X  X xis parallel splits, X  in that all instances of groups produced by splitting the data have identical values for one of the variables. The advantage of this method is simplicity and high level of explainability of the partitions and an efficient computational method for generating them. The dis-advantage is that for problems where the groupings can be expressed more naturally in terms of combinations of variables, the axis parallel method can use too many splits to approximate the true groupings. Other methods such as  X  X blique partitions X  can produce simpler and more accurate trees in such cases [Murthy et al. 1994]. Indeed, as one can imagine, there are other more complex splits possible at each node, includ-ing nonlinear multivariate ones, with methods such as genetic algorithms applied to discovering linear combinations of variables [Cantu-Paz and Kamath 2000] or Boolean conditions of independent variables [Dhar et al. 2000]. In this article we have limited ourselves to axis parallel splits and simple explainable decision trees for a number of reasons. First, this is probably the most widely used and best understood data mining method and tends to work well on a remarkable range of problems. Second, our ap-proach to financial prediction is to assemble many  X  X imple X  disjuncts of low complexity that are easily interpretable. Finally, our objective is to test whether this simple ap-proach to predictive modeling has promise. If it does, future research can build on it and improve predictive accuracy using potentially more complex splitting criteria.
The remainder of this article is organized as follows. We first review the literature on small disjuncts. We discuss complexity, a key parameter in machine learning al-gorithms, as it applies to small disjuncts. We them describe the data and measure of performance. This is followed by a description our learning algorithm (a standard tree induction algorithm called CART [Breiman et al. 1984]) that is applied to various in-and out-of-sample datasets within and across time, and an interpretation of the corresponding results. We also define a simple construct called yield that measures the percentage of performance that a learned model realizes relative to an ideal model. We conclude with a discussion of the merits of using ensembles of small disjuncts to financial time-series forecasting. The basic argument is that for noisy problems that do not have a central structure, a distributed approach to forecasting works well when we keep the disjuncts simple. This approach is also suited to the problem because a system takes action only when it sees interesting opportunities and is silent at all other times. Apparently, this is also a characteristic of successful human traders. A small disjunct is a pattern that covers few training examples. In other words, the  X  X overage X  or  X  X upport X  of the pattern is limited, such as the lower rightmost node in Figure 2. A collection of small disjuncts, however, can cover a large part of the data, which means that they can have a significant impact on the overall quality of a learned model. A pattern can be represented using a rule, a neural network, a support vector machine, a regression model, or other representation that relates independent variables to a dependent variable. In Figure 2, the pattern is a rule.

We are agnostic about the representation of small disjuncts although we represent them as decision rules. A rule consists of a left-and right-hand side. The left-hand side is a conjunction of Boolean conditions, where each condition is expressed in terms of variables, relational operators, and constants. An example of a condition is  X  X he 5 day return is greater than 1. X  The term  X 5 day return X  is a standard measurement of the time series, like a  X  X eature X  in standard data mining applications. The right-hand side consists of an expected return that results by  X  X pplying X  the left-hand side to the time series. This is the dependent variable we are trying to predict in the future.
Rules vary in several key ways. Support measures how much of the dataset the rule covers. Specifically, if N is the total number of examples in a dataset, then for a rule of the form A  X  B: The right-hand side, B, is based on the type of problem. For classification problems, it specifies the majority class corresponding to the cases satisfying A. For regression problems, it is a numerical value, typically the average of the cases satisfying A, and is in Figure 2.

There has been a fair amount of research on small disjuncts in the machine learning literature. Much of it suggests that they produce a disproportionately large percent-age of errors but collectively cover enough cases that they can X  X  simply be ignored or discarded [Holte et al. 1989; Quinlan 1991; Pazzani 1992; Danyluk and Provost 1993; Weiss 1995; Weiss and Hirsh 2000]. For example, Holte et al. showed that disjuncts that cover only 12% of the cases in the training set account for 57% of the errors. They noted that the  X  X enerality bias X  used by learners like ID3 works well for large disjuncts but not small ones. On the other hand, a  X  X pecificity bias X  produces lower error rates for the small disjuncts, but it increases the error rates for the large ones, making no differ-ence to overall accuracy of the learner. Ting [1994] refined this approach by combining two learners: if a case is covered by a large disjunct, it is classified by a generality bias learner, otherwise by one with a specifity bias. The drawback of this method is the difficulty of defining practically (and in advance) what is meant by a  X  X mall X  or  X  X arge X  disjunct.

Quinlan [1991] proposed an interesting method for reducing the error rates for small disjuncts. Noting that class distributions are often skewed, disjuncts predicting the minority class are likely to have higher error rates. Quinlan [1991] incorporated these prior probabilities of the target class in classifying examples. Quinlan X  X  method calcu-lated the prior probabilities only on those training examples that are  X  X lose X  to a small disjunct, meaning that they don X  X  satisfy only one of the conditions of the pattern. His method produced lower error rates than the na  X   X ve model. In a similar vein, Pazzani [1992] assigned lower  X  X eliability X  to small disjuncts, effectively lowering their impact on the overall accuracy of a classifier.

A number of researchers have tried to provide explanations for when and why small disjuncts tend to provide erroneous predictions. Danyluk and Provost [1993] pointed to the difficulty of separating systematic noise from cases that are truly exceptional. Along these lines, Weiss and Hirsh [1998] showed that as class noise increases so does the number of small disjuncts, although with very high levels of noise the errors become more concentrated in the larger disjuncts. In a later paper, Weiss and Hirsh [2000] showed that as the size of the training set increases, errors tend to occur in the small disjuncts.

It is interesting that all the research on small disjuncts has been entirely on classifica-tion problems. What is notable about financial market prediction is that it represents a regression problem, not a classification one in which the dependent variable is discrete, often binary. When comparing models, the standard statistics used to evaluate classi-fiers such as  X  X rue positives X  (which may be thought of as the proportion of winners) and  X  X alse positives X  (proportion of losers) are meaningless. The quality of a system is driven by its ability to predict large values , even at the expense of being wrong on the small ones. A system can be profitable even if it wins a third of the time, and it can be unprofitable even it wins 90% of the time. The magnitude of winners and losers can be more important than their relative percentages. This makes regression problems in-herently different from classification problems in terms of how performance of a model is calculated. We return to this issue after presenting the results.

An additional point of interest in understanding and interpreting prior literature on small disjuncts is that the problems studied, those from the UCI database and synthetic problems, have had a fair degree of known or controllable structure where for the most part the large disjuncts captured most of this structure. It is not surprising therefore that the small disjuncts would account for a larger proportion of the errors.
Last but far from least, the complexity of small disjuncts was not explicitly controlled for in prior studies. The small disjuncts typically generated had a large and variable number of conditions. Without a control on model complexity, it has been impossible to know definitively whether small disjuncts perform better if their complexity is limited and held constant. As we discuss shortly, we keep complexity as low as possible and fixed across disjuncts, making sure that the learner generates disjuncts that do not exceed a specific threshold of complexity.
 It is well known that if we let a learner partition the data without regard to the number or types of conditions it imposes on the independent variables, it will generate models that predict every example in the training set perfectly, but these models will fail miserably on data they have not encountered. In the example of Figure 2, if the left-hand side were something like we could probably regard it as too  X  X omplex X  because of the large number of conditions, in this case six, and the fact that each condition  X  X inches X  a very narrow range of values of the independent variables. Such disjuncts will memorize the data instead of generalizing from them and not perform well on future data. For this reason, it makes sense to limit the complexity to the extent possible while still allowing the disjuncts to represent interaction effects.

It is important to be able to represent interaction effects in financial time-series forecasting because these allow combinations of variables to be conditional. In Figure 2, for example, the rightmost leaf node says that high future returns can be expected when Fret5 exceeds 1, conditional on Fret2 also exceeding 1. This adds complexity to the model relative to simple additive models, but it gives it the power to model nonlinearities.

Complexity has been studied extensively in the machine learning literature from a number of angles. Linear models are considered  X  X imple X  in terms of structure com-pared to nonlinear models. The former are considered  X  X igh bias X  since they impose a simple structure to which a learned model must conform in making predictions. In contrast, nonlinear models such as trees are considered more complex or  X  X ow bias X  since they do not impose a structure on the model a priori (see Perlich et al. [2003] for a comparison of the two types of models).

Because nonlinear models allow a learner considerable degrees of freedom in fitting a model to data, a considerable amount of attention goes into specifying and control-ling for it in learned models depending on the representation. This includes the VC dimension in support vector machines [Vapnik 1995], hidden nodes in neural networks [Poggio and Girosi 1989], and the number of conditions imposed on inputs to compute the output in decision/regression trees [Arora and Barak 2009].

Disjunct complexity has not been controlled for explicitly in prior studies, making it somewhat difficult to compare the results and to assess the impact of disjunct com-plexity on predictive accuracy. As we explain more fully next, we control explicitly for complexity.

In this study, we define disjunct complexity along two dimensions. The first of these is the number of conditions, which is a standard measure for trees and rules. The second is the number of variables allowed in the disjunct. Both dimensions deal with interaction effects, but the latter allows us to control for the dimensionality of the search space that the learner can consider. The famous expression  X  X urse of dimensionality X  coined by Bellman [1957] illustrates the exponential increase in examples required in a sample for every dimension (in this case, variable) that is added to the search space.
Our objective specifically is to keep the small disjuncts as simple as possible by limiting the interaction effects allowed between variables, thereby minimizing the  X  X urse of dimensionality. X  To keep things simple, we therefore limit the small disjuncts to dyads, that is, involving two variables with axis parallel splits as shown in the example in Figure 2, with  X  X et5 &gt; 1 X  and  X  X et2 &gt; 1. X  These constraints keep the variable interaction effects as simple as possible while still giving the learner some room to model nonlinearities.

The dyad in the example in Figure 1 has one major limitation, namely, its inability to represent ranges of the form  X 0 &gt; Ret2 &gt;1. X  Considering the fact that correlations between variables can be nonlinear and hold within specific ranges, this is a severe hindrance. For this reason, we allow one other condition to be imposed in the dyad on one of the variables in terms of the standard relational operators. This representation enables us to learn rules of the form In summary, we keep disjunct complexity as low as possible, but enabling a learner to capture conditional interaction effects. Limiting ourselves to dyads with a limit of three conditions for all disjuncts also keeps model complexity below a specific fixed threshold. By controlling for model complexity we can evaluate the impact of disjunct support size on performance. Do smaller disjuncts perform better as long as complexity is low? We can answer this question by varying a parameter, namely support, and testing for predictive performance. Before we do this, let us describe the data and the measure of performance. An observation (datum) consists of a pair (Zfret, X), where X is a vector denoting the current state and Zfret is a continuous value we are trying to predict.

How should we measure the performance of a predictive model? A commonly used measure of performance in Finance is  X  X isk adjusted return, X  which is typically a return divided by the risk involved in achieving it. Risk is generally measured in terms of volatility of returns, often calculated as a standard deviation of a return series.
A suitable time interval needs to be used to compute the volatility of each instrument for normalizing returns. For each instrument, performance is measured as follows: i is the current day, the risk adjusted future period return is defined as where N is a parameter used to standardize the returns. We use N=250, which uses roughly one year of historical data to calculate the volatility of returns for the in-strument. In practice, N must be chosen to provide a representative distribution of returns.

Expressing performance in terms of risk adjusted returns makes it possible to com-pare all instruments using an identical measure. It lets us pool data across instruments. A volatile instrument, for example, would generate returns of high magnitude relative to one where volatility is low. In order to achieve the same dollar return per trade, one would therefore invest a larger dollar amount in the latter. We express performance of all instruments in terms of risk adjusted performance for learning and evaluation. The instruments chosen in this study were the most liquid global equity indices and include the S&amp;P500, Dow Jones 30, Russell 2000, S&amp;P Midcap 400, Nasdaq 100, Nikkei 225, Hang Seng, EuroStoxx 50, FTSE 100, CAC 40, IBEX 35, and the DAX 30. Daily open, high, low, and close prices were used from the date of inception of the futures contract for the instrument up to May 22, 2008. The total dataset consists of 44,473 records and can be found at the following URL. The vector X consists of a standard set of indicators that represent the  X  X tate X  of the market every day for each instrument [Kauffman 2004]. In this study, 68 indicators were included that are described in the Appendix. The properties of the indicators and criteria for selection are beyond the scope of this study and peripheral to the focus of this article. For a detailed analysis of these and other typical indicators used to describe financial time series, the reader is referred to Kauffman [2004] and Achelis [2000].
For the first set of experiments, the data were partitioned randomly into two equal parts 50 times, covering the same length of time as illustrated in Figure 3. In this dataset, end date is June 30, 2003. The start date varies by instrument, depending on when it began trading. The random partitioning of the data provided 50 in-sample and 50 out-of-sample datasets consisting of an identical number of records (14,834 each). The learning algorithm was applied to each of the 50 training sets, and performance was measured for each learned model on the corresponding out-of-sample dataset. The learning algorithm applied to the in-sample data is CART [Breiman et al. 1984] which takes as input pairs (Zfret, X) of the form described in the previous section and generates trees of the type in Figure 2. Each path through the tree to a leaf node is a rule of the form A  X  B, where A is a conjunction expressed in terms of at most two independent variables (dyads), relational operators, and constants and B is the expected value of the dependent variable corresponding to the cases covered by A. Since input was restricted to consider only dyadic combinations from among the independent variables for reasons expressed earlier, for N variables, the learner was therefore invoked N C 2 times, thereby generating N.(N-1)/2 trees.

The first constraint applied to the learner is on the depth of trees. We limit the number of splits allowed to three in order to limit disjunct complexity. Since each terminal node represents a rule, the upper bound on the number of rules per tree is 2 S , where S is the number of splits or conditions allowed. This results in an upper bound of 2 S  X  1 .N.(N-1) rules. 1 For 68 variables and the limit of three splits used in this study, this upper bound is 18,224 rules for each threshold of support.
 The second parameter was used to control the unevenness of the split permitted. Depending on the data, it is not unusual for the induction algorithm to create highly imbalanced splits, with only a single or few cases going into one partition. This is known as an  X  X nd-cut split X  [Buja and Lee 2001]. These overly uneven trees generally result in nonactionable or highly fitted rules, although it has been observed that in less pathological cases, uneven splits can result in interpretable rules that represent useful outliers in the data [Torgo 2001; Buja and Lee 2001]. While we want small disjuncts to be created, we need to ensure that the learner generates rules that satisfy a specified minimum level of support. This requires the  X  X inimum split X  parameter (which we call M ) to exceed the minimum percentage level of support (which we refer to as L ) required as defined in Eq. (1). This condition guarantees that the first split will result in partitions that exceed the minimum required level of support for a disjunct.
It should be noted that trees generated using the different support thresholds (and fixed number of splits S) can vary considerably, depending on M and L. Consider that S splits with a threshold of M will result in a minimum support of M S for a leaf node. The support threshold L in our experiments was varied between 0.25% and 10%. Going any lower would not be meaningful considering that a 0.25% coverage would cover too few buying and selling cases to generate meaningful statistics. At the other end of the spectrum, a 10% support as the upper bound also seems reasonable since performance at higher levels of coverage would begin to resemble the overall average, namely, the market.

Our ensemble approach to the prediction problem through collections of trees is similar to that of random forests [Breiman 2001] and other similar methods that try to avoid the overfitting or bias that can result from single decision trees. Random forests do this by using bootstrap samples and considering a very small subset of the independent variables for consideration for every split. In our algorithm, we restrict this consideration to two variables and consider the combination of two variables at a time exhaustively. We selected our method because we wanted to give all variables equal opportunity in the rule generation process. These variables have been noted historically to be relevant based on past correlations with future returns, so some domain knowledge has gone into the variables which we would like to see reflected in the trees generated by the learning algorithm.

Rules are extracted from the trees as shown in Figure 2, with a path from root to leaf being a rule, represented as Boolean conditions over dyads in X. Each rule has a score, namely, its expected values of Zfret which is computed from the cases that satisfy the conditions specified by the rule. The rules are ordered by in-sample performance. This focus on nodes (rules) as the unit of analysis instead of the entire tree is important. It enables us to solve two important problems simultaneously.

The first advantage of rules as the unit of analysis is that it provides a good way to deal with the problem of model selection, which is a thorny one in machine learning. The problem is one of selecting one or more models from a large set of competing models. There are several measures for doing this, such as AIC [Akaike 1974], which typically rely on a basic measure such as MSE or MAD for assessing model error. The reality, however, is that most trees produce very similar error rates when applied to the entire dataset for reasons pointed out earlier, namely, that the majority of the error comes from the larger values of the dependent variable when the model predicts close to zero (from the nodes with high support and average prediction close to the mean of zero). Because competing models (in our case, trees) produce very similar error rates, ranking them reliably is difficult. In contrast, as illustrated in Figure 2, the means of individual nodes (especially those corresponding to small disjuncts or low support) are typically quite different from the overall mean of the entire dataset and from each other. In effect, we get a wide range of scores at the node or rule level. By comparing and ranking these, we get a well discriminated ranking of rules based on in-sample performance. We can then pick a subset of these rules as our model.

Focusing on the best rules (nodes) as the decision rules from across the large set of trees produced by the learning algorithm also solves a problem that is especially relevant to financial prediction, namely, that of making a decision to buy ( X  X o long X ) or sell ( X  X o short X ) only when there is a strong signal, and being agnostic otherwise. Unlike problems where a system must decide on every case it sees, in financial forecasting it often makes sense not to act because the expected return is close to zero. This is because of the inherent noise in the problem. By selecting only the  X  X nteresting X  rules we have a decision making system that is opportunistic, acting only when the conditions are appropriate and being agnostic otherwise.

The ordered list of rules contains the largest positive scores (Zfret) at the top and the largest negative scores at the bottom. The top T rules are considered to be  X  X ong X  rules. Whenever the left-hand side for a rule is satisfied for an instrument, a trading program would buy the instrument and sell it at the end of the next period. The bottom T rules are considered to be  X  X hort X  rules. Whenever the left-hand side for a rule is satisfied for an instrument, a trading program would sell the instrument and buy it at the end of the next period.

The top T and bottom T rules make up the decision rules employed by a trading program. This type of top/bottom quantile structure is a common method in industry for how trading programs are assembled to make predictions in both directions. In our experiments T was set to 100.

We recognize that the simple selection procedure of the top T and bottom T rules is suboptimal from a portfolio optimization perspective, since it ignores correlations among the rules. If one takes correlation into account, we are faced with a massive combinatorial optimization problem. While there are several heuristics that could be used, we ignore this problem here since it introduces additional complexity into the experimentation. While this would be essential in formulating a real trading strategy, it isn X  X  necessary for the comparative analysis of interest in this article. Before running the main experiments, a baseline was established where the learner was allowed to generate rules without any limit on the complexity of rules, namely, the number of conditions used.

As expected, the result from this baseline experiment was zero correlation between rules X  in-sample and out-of-sample performance. This result is consistent with prior literature on small disjuncts: if we don X  X  control for their complexity, they overfit the data and generate large errors on out-of-sample data. In this case, their predictions are no better than random.

Next, the learning and testing was performed 50 times based on different random partitionings of the data (no duplicates in an in-sample or out-of-sample pair). For each run, the overall performance is calculated as follows. Each rule is applied to each case that satisfies its left-hand side and the return for that case is noted. The performance of a rule is the average value of the dependent variable across all cases that satisfy the rule. The performance of the set of the T long rules is the average across all the long rules. The performance of the set of the T short rules is computed similarly. Finally, the overall performance of the system of T long and T short rules is the difference of their respective averages.

Figure 4 shows the results on the in-sample and out-of-sample datasets for varying levels of support up to 10% for the 50 runs. The error bars show one standard deviation in performance for the 50 runs for each level of support.

Several things are noteworthy about the results in Figure 2. First, as expected, the in-sample performance increases with reduced support. However, it plateaus out at half a percent, suggesting that the learner derives little additional benefit from making patterns more restrictive.

Secondly, and perhaps most significantly, the out-of-sample performance mirrors the in-sample performance for all levels of support. Surprisingly, it gets better monotoni-cally as support gets smaller. We do not see the  X  X nverted U X  that one sees when model complexity is varied [Breiman et al. 1984]. Keeping the complexity low does appear to avoid gross overfitting that occurs when complexity is not controlled. This is a sig-nificant and novel result. It argues in favor of using Occam X  X  razor for constructing ensembles of small disjuncts. We shall return to the significance of this result in the discussion section of the article.

Thirdly, the results show larger absolute performance drop for smaller disjuncts, indicating that the process of statistical induction does indeed result in more overfit-ting for smaller disjuncts than the larger ones. However, even though small disjuncts degrade more than large ones on out-of-sample data, they still perform significantly better. This is what we care about from a predictive standpoint.

Finally, there is a significant difference in performance between the in-sample and out-of-sample data at all levels of support. This is not surprising. It is common knowl-edge that the process of statistical induction leads to some invariable  X  X emorization X  of the data where some of the discovered patterns are really noise in the data. What is notable here is the extent of degradation, which is an indication of the inherent noise in the prediction problem.

It is important to recognize that in the absence of special knowledge about the prob-lem, it is impossible for the machine to distinguish between the real and coincidental patterns, that is, the ones that represent memorization (or  X  X oise X ) versus those that represent generalization (or  X  X ignal X ). And even for the patterns representing  X  X ignal X  there will be considerable variance in performance at the individual pattern level. The expectation, however, is that there is some degree of generalization captured in the aggregate collection of patterns.

Since some degree of memorization is inevitable in induction, we should expect the out-of-sample performance of a collection of patterns to be lower than the in-sample performance. For classification problems, the performance of a learner is defined using a confusion matrix which counts true and false positives and true and false negatives. For regression problems, however, evaluation depends on the magnitudes of the correct and erroneous predictions. As we mentioned earlier, systems that win a majority of the time can be unprofitable while those that win a minority of the time can be profitable. Performance is driven by the sizes of the winners and losers.

The ensemble of disjuncts provides a natural way to measure the expected degra-dation on future data in percentage terms. To understand how, consider first what an  X  X deal X  learned model would look like, one where the degradation due to inductive generalization is zero. Such a learner would produce a zero intercept scatter plot as in Figure 5. The out-of-sample performance of this learned model matches its in-sample performance exactly by producing an identical value for the continuously valued de-pendent variable for both in-sample and out-of-sample data. Such a situation would occur if the model were specified correctly including all relevant variables and with the correct functional form, in which case it should perform similarly in and out-of-sample.
In practice, however, we typically do not know the perfect functional form, nor do we include all relevant variables. Accordingly, it is more common to observe scatter plots as in Figure 6. This particular plot shows the in-and out-of-sample performance of the top 100 long rules at the 1% support level. As we see, the in-sample data have a tight distribution, whereas the variance out-of-sample is much higher. Some patterns can actually do better out-of-sample, but the majority of them do worse.

Note that the regression line in Figure 6 shows the fit between in-and out-of-sample data, and its intercept is nonzero ( X 0 . 3146). This says that an in-sample performance of zero should result in a negative out-of-sample performance, of  X 0 . 3146. This can be viewed as the  X  X ias X  of the learned model. This interpretation of the regression model is clearly unnatural for certain types of extreme cases, such as in the limit when support approaches zero and no trading occurs. In such a case, when in-sample performance is zero, so should the out-of-sample performance. Consider a line going through the origin, which would represent this limit situation, with zero performance both in and out-of-sample . Forcing the line to go through the origin solves two problems simultaneously. First, we get a natural measure of degradation in percentage terms which we call yield. Specifically, if the 45 degree line passing through the origin represents a 100% yield as with the ideal learner, lower slopes represent correspondingly less than perfect fit between in-and out-of-sample performance. In effect, the slope of the line is the meaningful measure of the goodness of the learned model (in theory, it is possible for the line to be steeper than 45% where the learner has somehow managed to perform even better out-of-sample in the aggregate This should be extremely rare and correspond to a statistical fluke).

A nonzero intercept term also leads to a more serious problem with interpreting the regression that is best illustrated through an example. Specifically, imagine a scatter plot that is circular but tightly distributed around high values for both in-and out-of-sample performance. The R-square for such a line might be zero, in which case the regression line would be horizontal. But the learned model would actually be good one, measured by the high average out-of-sample performance of the disjuncts as a whole. Allowing an intercept would yield a flat line with a low R-square despite the high out-of-sample performance. In contrast, the slope of the line passing through the origin would represent the performance of the disjuncts corresponding to the scatter plot relative to the ideal, the 45 degree line.

In summary, the slope or tangent of the zero intercept provides a natural measure of yield, with flatter lines corresponding to lower yields. Figure 7 shows the line passing through the origin, corresponding to the data in Figure 6. For this ensemble, the yield is just over 32% relative to the ideal learner. 2 The R-square for this case is meaningless and does not represent fit and can therefore be ignored.

Unlike measures of model error such as MSE and MAD which are dominated by the large values and not easy to interpret, the yield measure has a clean and useful interpretation, namely, percentage degradation in relation to an ideal learner that a model can be expected to realize.

The preceding calculation of yield measures the degradation from inductive gener-alization based on the inputs provided to the learner, since the in-sample and out-of-sample data came from the same distribution. However, the same concept can be used to calculate the degradation of a model on future data which might come from a different distribution. Indeed, in time-series forecasting, one typically implements learned models for use on future data. In this situation, we should expect additional degradation in performance. The purpose of the second set of experiments was to isolate and quantify the degrada-tion arising from the fact that the future is always different from the past. This set of experiments assesses whether small disjuncts have any predictive ability.

In order to quantify this second type of loss, the learned patterns were tested not on data covering the same period as the data on which they were discovered, but on future periods. Figure 8 illustrates how the data were selected. This dataset (labeled  X  X ut-of-sample out of time X  in the figure) corresponds to the time period between July 2003 and May 2008 and consists of 14,805 records. In this experiment, we generated the out-of-time performance by applying the rules learned on the in-sample data to the  X  X ut of time X  data. The results from evaluating the learned patterns out of time are shown in Figure 9. Each level of support shows the average performance of the patterns with the corresponding error bars similar to Figure 4. In order to avoid clutter, we do not show the in sample performance again (for which the reader is referred to Figure 4), and only compare the two out-of-sample performances: in time versus out of time. (The lower line from Figure 4 is the upper line in Figure 9.)
Interestingly, the out-of-sample performance out of time also mirrors the out-of-sample performance in time. As expected, the performance out of time is consistently worse. The yield achieved out-of-sample can be measured as before, by plotting the in sample performance versus the out-of-time performance and calculating the slope of the zero intercept regression line. We do not show this plot since it adds little to the discussion, but what is of interest is the fact that the degradation on future data both in absolute and percentage terms is not worse than that due to inductive generalization. Indeed, it is lower in percentage terms, especially for the lower levels of support. This is encouraging since it suggests that the collection of small disjuncts do indeed have some predictive power that can persist over time.

In concluding this discussion on out-of-time degradation, it is natural to expect that the degradation can be severe enough to result in a negative performance in the future. Indeed, the plot in Figure 10 (the performance of the  X  X hort ensemble X  at the 1% support level) shows such a situation. In contrast to Figure 7 where the  X  X ong ensemble X  at the 1% support level had a roughly 32% yield, this time the overall yield is negative, showing that the learned patterns that make negative predictions for Zfret perform poorly in the future.

The previous result illustrates two interesting concepts that are worth mentioning briefly. First, constructing a learning system on noisy financial time-series data can result in models that perform poorly in the future. A learning algorithm will lose performance not just in the process of induction, but also if the future turns out to be sufficiently different from the past, on which the model is based. Second, the figure illustrates a peculiarity of financial equity markets, namely the difficulty of finding good  X  X hort X  models, because markets trend up most of the time, so a short seller is on average swimming against the current. This is a well known phenomenon in equities markets.
Would anyone implement a model with a potential negative yield? Normally, one would not. However, many portfolio managers are not comfortable with implementing  X  X ong only X  models, assuming that the market will go up at all times. For this reason it is typical to  X  X edge X  long exposure to the market through a short strategy which may lose money on average, but nevertheless provides protection during those times when the market drops unexpectedly.

As a final check on the generality of the preceding results, we tested the learned models on each of the individual equity indices out of time. The results are presented in Table I which shows average performance for each equity index, long minus short, as before. Not surprisingly, there is considerable variance in results, with significant difference in performance across the individual instruments. However, other than the DAX30 for which the learned models do not work at any level of support, there is a pattern of better performance for the smaller disjuncts, similar to what we observed at the overall asset class level. It is also notable that there is a considerable drop in performance above the 1% level of support.

The previous results suggest that the results at the asset class level are not driven by outliers, but from across the asset class. They bolster the earlier findings, suggesting that the patterns described by the small disjuncts exist at the individual instrument level, and not based on outliers.

It is worth concluding this section with the question we raised at the outset, namely, whether ensembles of small disjuncts are a useful predictive model for problems that have a high level of noise. Interestingly, our results run counter to previous research, which was primarily on classification problems, and where complexity was not con-trolled for explicitly. We find that their absolute performance is better than that of large disjuncts, and this difference is statistically significant. In many problem domains, there is usually a  X  X ain X  or  X  X irst order X  model that explains a bulk of the cases in the data, and the minority of cases that are hard to classify are treated as noise or exceptions that may require a  X  X econd order X  model to classify them correctly. When rule induction algorithms are applied to such problems, it is not surprising that the small disjuncts would produce a disproportionately large number of errors. In trying to explain the residual cases, these models end up modeling much of the noise as well. Previous research supports this view.

Our assertion is that no such first-order model exists when it comes to predicting future returns of asset classes in financial markets. In the absence of such a model, an approach that attempts to find a single model will perform poorly. It will model the noise and its predictions will tend to be close to the mean for all cases as shown in Figure 1. In contrast, a distributed approach to prediction using an ensemble of small disjuncts can do a better job of avoiding the inherent noise in such problems and finding the islands of structure in the data. While this will invariably model some of the noise as well, the expectation is that in the aggregate, it will find structure.

As support for the alternative approach, we have provided new evidence on the performance of  X  X imple X  small disjuncts as predictive models in financial markets. This type of model represents a new way to think about market prediction where the prediction task is distributed across a large number of simple independent models instead of a single one.

We have shown that the distributed approach works well when the complexity of the learner is low. Indeed, keeping disjuncts simple tends to partition the search space into large segments that are  X  X ninteresting, X  where a model makes no prediction, and the small  X  X nteresting X  ones where it makes predictions. This model fits naturally with financial markets compared to problems where a model must always make a decision (such as accepting or rejecting an application for credit, and so on). This property of the problem lets us focus on the interesting submodels (tree nodes) while ignoring those whose average forecast is close to the mean.

Prior research with small disjuncts has been almost entirely on classification prob-lems, where it has been demonstrated that they account for a large proportion of errors and degrade the overall performance of a learned model. In contrast, our research shows that ensembles of simple small disjuncts perform well on data they have not seen before. Indeed the ensemble of small disjuncts is the predictive model. The re-sults appear consistent with the conjecture that markets for the most part constitute noise, with infrequent opportunities presenting favorable risk-reward trade-offs. It is important to point out, however, that higher performance of small disjuncts on regres-sion problems doesn X  X  mean  X  X igher accuracy X  as in classification problems. Rather, the performance of the small disjuncts comes from their ability, on average, to predict accurately the higher values of the dependent variable. This is a subtle but important distinction.

A natural by-product of the ensemble approach, where the ensemble is a mix of signal and noise which is indistinguishable, is the quantification of  X  X ield X  of a learned model. Virtually all models induced by machine learning methods are known to degrade on future data, but other than standard measures of error, which are of limited value, and expected error bounds of learned models on data corresponding to the distribution of the training data [Vapnik 1995], no one has heretofore proposed a metric for quantifying the yield from an induced regression model relative to an ideal benchmark.

We also show that the degradation in performance of a learned model is decomposable into two distinct components: that resulting from the inevitable overfitting that occurs in the process of statistical induction, and that resulting from the fact that the future is inherently different from the past. This seems like a useful way to break down the expected degradation of a learned model. It can be applied to any problem where decision making is distributed across an ensemble of decision rules, each of which recognizes a small set of conditions under which to act. The relative magnitudes of the two degradations provide useful information to the model builder for performing cost/benefit and other types of analyses.

Financial markets are at the same time recurrent and evolutionary. Old patterns re-peat, albeit unpredictably, and new patterns emerge constantly. This makes it difficult to find a single stable predictive model. A practical benefit of our approach is that it makes it possible to find patterns on a large scale efficiently, deploy them, and move on, without investing large amounts of effort in theory building only to find the the-ory obsolete. This approach is consistent with the observation that  X  X atterns emerge before the reasons for them become apparent. X  An implication of the result is that small disjuncts provide a promising approach towards finding emerging patterns and assembling a predictive model automatically.

The evidence presented, that small disjuncts have predictive power, raises the ob-vious question as to why they are not discovered by market players and therefore disappear. The high degree of competition among players in financial markets can be expected to dissipate any obvious advantages that may occasionally arise.

The issue of efficient markets is hotly debated in the literature. The debate and its evolution is much too extensive to review in this article, however, it is worth addressing briefly the concept of  X  X arket anomalies, X  since it is possible that small disjuncts might represent a type of anomaly. It is well known that certain strategies lead to abnormal returns by exploiting market anomalies. An example of an anomaly is that stocks with low market capitalization (small stocks) have abnormally high average returns [Banz 1981]. Similarly, stocks with high ratios of book value to the market value of equity also have unusually high average returns [Rosenberg et al. 1985; Chan et al. 1991; Fama and French 1993]. Another example is that more profitable firms have higher average stock returns [Haugen and Baker 1996; Cohen et al. 2002]. Companies that deliver an earnings surprise see subsequent price movement in the direction of the surprise [Ball 1995]. Similarly,  X  X omentum investing, X  a strategy of buying high and selling even higher which can be implemented as described in the preceding paragraph, also seems to persist and generate abnormal returns [Jegadeesh and Titman 1993; Schwager 1992; Soros 1987]. Similarly, a strategy of buying a higher interest rate yielding currency and selling a lower rate yielding one, known as the  X  X arry trade, X  is another well known one. There are several other known market anomalies.

As summarized by Ball [1995], a proponent of the efficient market hypothesis,
In other words, financial markets are a complex phenomenon, not subject to a clean and simple interpretation.

A plausible explanation for the persistence of anomalies is that the excess returns realized by applying them entail taking on some sort of risk, so there is no  X  X ree lunch X  after all [Till 2001]. For example, the carry trade strategy takes on the risk that the higher yielding currency will be devalued or that a sudden shift in risk taking preference will cause participants to reverse their default positions, in which case it performs badly. Similarly, a strategy that only buys  X  X alue X  stocks (those with high book value to price ratios) takes on a business cycle risk that others don X  X  want to assume. A strategy that sells deep out of the money options (a  X  X hort-option X  strategy) makes a steady return on the small premium it makes selling catastrophic protection to others and works well until a catastrophe actually occurs (such events occurred in September and October of 2008). In summary, each of these types of strategies associated with anomalies can be viewed as taking on some sort of risk that others are unwilling to take, for which they can make superior returns than the market for significant periods of time.

Small disjuncts may have a similar interpretation. Markets are a complex social phenomenon where there is tremendous competition among participants to place a value today on future outcomes which are uncertain. This entails risk taking. But risk is not easy to calculate, nor is it static [Damodaran 2007]. During  X  X ormal X  times, the human emotions of risk and greed are balanced in the aggregate, with large numbers of participants buying and selling, and prices move smoothly. However, fear and greed often go out of balance, causing people to become more or less risk averse. Such markets can experience rapid price changes and illiquidity, driven by human emotion. Market activity in October 2008 was an example of the rapid changes in investors X  appetite for taking risk, with extreme fear quickly pervading markets, represented by a relative absence of buyers.
 Small disjuncts by definition represent  X  X nusual X  situations in financial markets. It is plausible that in these outlier-like situations represented by the small disjuncts, market participants X  propensity for risk is imbalanced. Accordingly, when fear domi-nates, the situations appear more risky than they have been historically or they would be during  X  X ormal X  situations. There could be a number of fundamental reasons for why the outlier situations might be abnormal from a risk bearing standpoint. One is that these situations occur after major market dislocations when fear dominates. When this is the case, participants are frozen into inaction. Or they may have held positions previously that they were forced to liquidate. Or they may have had stricter risk lim-its imposed on their trading activity through institutional risk managers who tend to become highly risk averse in outlier situations. Schleifer [2001] provides an extensive discussion of the reasons why markets cease to be  X  X fficient X  during time of market stress. Whatever the reasons may be for the inefficiency, small disjuncts may represent such  X  X nomalies X  similar to the ones discussed earlier. These can be accounted for by the fact that each of these anomalies represents a specific risk that is being rewarded by the market, or some  X  X esidual X  nonrisk bearing reason. In either case, taking risk when others are not should be correspondingly rewarded.

Regardless of the interpretation one might favor for the existence and persistence of small disjuncts, this research suggests that small disjuncts represent an interesting phenomenon in financial markets. Indeed, unlike previous domains in which they rep-resent much of the  X  X oise X  in the problem, our research suggests that they collectively represent  X  X he signal X  in a domain that otherwise consists largely of noise.
More generally, the approach of considering a portfolio of small disjuncts as a pre-dictive model is likely to be useful for problems that are noisy and  X  X onstationary, X  that is, where the rules for prediction shift over time. For such problems, it appears promising to construct ensembles of rules that serve as prediction models that are valid for limited periods of time, and thereby reconstructed periodically.
 There are 68 independent variables in the dataset that belong to 8 variable types that are based on 8 historical intervals. The first variable type is historical returns that indicate the  X  X rend X  of a time series over the different intervals. (Figure 2 shows  X  X et2 X  and  X  X et5 X  which measure returns over the last 2 and 5 intervals respectively. There are 6 other returns measured over different historical intervals.) If  X  X urrent X  is the end of the current time period, the N day historical return, Ret(N) is defined as Similarly, an additional variable type, volatility, measures the historical dispersion in returns and ranges for a series. The  X  X tandard X  measure of T day volatility is Two additional types of volatility are computed identically. The first is based on in-traperiod range (i.e., high minus low for a period), and the second is known as the Garman-Klass volatility, described in Garman and Glass [1980]. All variables are  X  X or-malized X  using a 1 year time window. For example, a distribution of Vol(T) is generated using a year of history, and the current Vol(T) is expressed as a Z-score using the distribution.

Four additional variables are computed that indicate  X  X osition X  within a range (com-monly referred to as  X  X tochastics X  in the trade literature). For example, if the current  X  X lose price X  of a series is the highest closing price over the last N periods, the position for it is +1 indicating the highest position, whereas if it is the lowest, its position is -1, and so on. In addition to the close, we measure this value for the open, high, and the low. Suppose we wish to compute the position of a variable (say the close) relative to the last T intervals. Let us call this variable V. This formula for computing the position of V is where
Current(V) is the last period X  X  value of the variable V, Low(V,T)) is the low of the variable V over the last T periods, High(V,T)) is the high of the variable V over the last T periods.

In addition to the 64 variables defined before (8 types over 8 intervals), the database consists of four additional variables that calculate volatility-adjusted returns over four periods. Adjusting returns by volatility, also known as the Sharpe ratio, provides an indication of the smoothness of a trend.

