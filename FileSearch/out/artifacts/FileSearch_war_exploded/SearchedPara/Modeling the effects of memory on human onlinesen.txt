
University of California, San Diego Nearly every sentence occurring in natural language can, gi ven appropriate contexts, be interpreted intepretation from among these possibilities. More formal ly, each interpretation of a sentence w can be associated with a structural description T , and to comprehend a sentence is to infer T from w  X  computing the posterior distribution P ( T | w ) or some property thereof, such as the description T with highest posterior probability. This probabilistic pe rspective has proven extremely valuable in models of human language processing [3].
 and written sentences are by and large read, from beginning t o end. There is considerable evidence that people also comprehend incrementally, making use of li nguistic input moment by moment to re-solve structural ambiguity and form expectations about fut ure inputs [4, 5]. The incremental parsing problem can, roughly, be stated as the problem of computing t he posterior distribution P ( T | w for a partial input w distribution over partial structural descriptions of w using the properties of P ( T | w sentences more difficult to comprehend than others.
 Despite their success in capturing a variety of psycholingu istic phenomena, existing rational mod-els of online sentence processing leave open a number of ques tions, both theoretical and empirical. On the theoretical side, these models assume that humans are  X  X deal comprehenders X  capable of computing P ( T | w affect language processing. For structured probabilistic formalisms in widespread use in compu-tational linguistics, such as probabilistic context-free grammars (PCFGs), incremental processing algorithms exist that allow the exact computation of the pos terior (implicitly represented) in poly-nomial time [10, 11, 12], from which k -best structures [13] or samples from the posterior [14] can be efficiently obtained. However, these algorithms are psyc hologically implausible for two reasons: (1) their run time (both worst-case and practical) is superl inear in sentence length, whereas human in these algorithms impose strict locality conditions on th e probabilistic dependence between events (extra-)linguistic context in forming incremental poster ior expectations [4, 5].
 mented by empirical data that are hard to explain purely in pr obabilistic terms. For example, one of the most compelling phenomena in psycholinguistics is that of garden-path sentences , such as: Comprehending such sentences presents a significant challe nge, and many readers fail completely on their first attempt. However, the sophisticated dynamic p rogramming algorithms typically used for incremental parsing implicitly represent all possible continuations of a sentence, and are thus in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information [1 5].
 In this paper, we explore the hypothesis that these phenomen a can be explained as the consequence of constraints on the resources available for incremental p arsing. Previous work has addressed the issues of feature locality and resource constraints by adop ting a pruning approach, in which hard locality constraints on probabilistic dependence are aban doned and only high-probability candidate structures are maintained after each step of incremental pa rsing [6, 16, 17, 18]. These approaches can be thought of as focusing on holding on to the highest post erior-probability parse as often as on approximating the posterior distribution P ( T | w Monte Carlo method commonly used for approximate probabili stic inference in an online setting, to explore how the computational resources available influenc e the comprehension of sentences. This approach builds on the strengths of rational models of onlin e sentence processing, allowing us to examine how performance degrades as the resources of the ide al comprehender decrease. Section 3 outlines how these ideas can be applied in the conte xt of incremental parsing. Section 4 illustrates the approach for the kind of garden-path senten ce given above, and Section 5 presents an experiment with human participants testing the prediction s that the resulting model makes about the digging-in effect. Section 6 concludes the paper. contexts where the amount of data available increases over t ime [19]. The canonical setting in which a particle filter would be used involves a sequence of latent v ariables z observed variables x this problem recursively, relying on the fact that the chain rule gives where we assume x Assume we know P ( z tance sampler for P ( z Then, we draw z from P ( z to give us an approximation to P ( z of this algorithm, in which a similar approximation was used to construct to P ( z from P ( z a weighted set of  X  X articles X   X  discrete values of z in the number of observations, and provides a way to explore t he influence of memory capacity (re-of limited memory capacity for online sentence processing. In this section we develop an algorithm for top-down, increm ental particle-filter parsing. We first lay out the algorithm, then consider options for representa tions and grammars. 3.1 The basic algorithm We assume that the structural descriptions of a sentence are context-free trees, as might be produced unary rewrites. A tree is generated incrementally in a seque nce of derivation operations  X  such that no word can be generated unless all the words preced ing it in the sentence have already been generated. The words of the sentence can thus be conside red observations, and the hidden state section, we outline three possible derivation orders .
 The problem of inferring a distribution over partial deriva tions from observed words can be approx-imated using particle filters as outlined in Section 2. Assum e a model that specifies a probability distribution P (  X  | ( D, S ) , w operations  X  sider a partial derivation ( D i distribution P (( D partial derivations ( D and our importance sampler involves drawing from P (( D by P ( w independent of x 3.2 Representations and grammars We now describe three possible derivation orders that can be used with our approach. For each order, a derivation operation  X  stack, choosing a derivation operation of the appropriate t ype, applying it to add some symbols to D Order 1: Expansion ( Exp ) only. D  X  consists of D with node N expanded to have daughters Order 2: Expansion and Right-Sister ( Sis ). The sequence of symbols specified by any  X 
Order 3: Expansion, Right-Sister, and Adjunction ( Adj ). The sequence of symbols specified Figure 1: Three possible derivation orders for the sentence  X  X at walked yesterday and Sally slept X . In each case, the partial derivation ( D the word  X  X alked X . The symbols ADVP, CC, and S 3 in (a) will be generated later in the derivations of (b) and (c) as right-sister operations; the symbol S 1 will be generated in (c) as an adjunction operation. During the incremental parsing of  X  X alked X  thes e partial derivations would be reweighted by P ( the partial derivation state for each order just after the ge neration of a word in mid-sentence. For each derivation operation type Op , it is necessary to define an underlying grammar and estimate the parameters of a distribution P for derivation orders 1 and 2 is unambiguous and thus supervi sed training can be used for such a model. For derivation order 3, a known tree structure still u nderspecifies the order of derivation operations, so the underlying sequence of derivation opera tions could either be canonicalized or treated as a latent variable in training. Finally, we note th at a known PCFG could be encoded in a model using any of these derivation orders; for PCFGs, the pa rtial derivation representations used in order 3 may be thought of as marginalizing over the unary ch ains on the right frontier of the representations in order 2, which in turn may be thought of as marginalizing over the extra childless the representations with more operation types could thus be expected to function as having larger we use derivation order 2 with a PCFG trained using unsmoothe d relative-frequency estimation on the parsed Brown corpus.
 This approach has several attractive features for the model ing of online human sentence comprehen-sion. The number of particles can be considered a rough estim ate of the quantity of working memory resources devoted to the sentence comprehension task; as we will show in Section 5, sentences dif-ficult to parse can become easier when more particles are used . After each word, the incremental approximate surprisal of each word  X  a quantity argued to be correlated with many typ es of process-ing difficulty in sentence comprehension [8, 9, 23]  X  is essen tially a by-product of the incremental parsing process: it is the negative log of the mean (unnormal ized) weight P ( w To provide some intuitions about our approach, we illustrat e its ability to model online disambigua-tion in sentence comprehension using the garden-path sente nce given in Example 1. In this sentence, a local structural ambiguity is introduced at the word brought due to the fact that this word could be either (i) a past-tense verb, in which case it is the main ve rb of the sentence and The woman is The woman is its recipient, and the subject of the main clause has not ye t been completed. This documented (e.g., [24]) that locally ambiguous sentences s uch as Example 1 are read more slowly at the disambiguating region when compared with unambiguou s counterparts (c.f. The woman who Figure 2: Incremental parsing of a garden-path sentence. Tr ees indicate the canonical structures for main-verb (above) and reduced-relative (below) interpret ations. Numbers above the trees indicate ( (i), many readers may fail to recover the correct reading alt ogether.
 Figure 2 illustrates the behavior of the particle filter on th e garden-path sentence in Example 1. the posterior in favor of the correct reduced-relative inte rpretation. In low-memory situations, as disambiguator, and when it succeeds the variance in particl e weights is high. An important feature distinguishing  X  X ational X  models of o nline sentence comprehension [6, 7, 8, 9] candidate interpretation tends to grow with the passage of t ime. A body of evidence exists in the psycholinguistic literature that seems to support an inter nal feedback mechanism: increasing the from than 2b and 3b [26, 27, 15]. From the perspective of exact rational inference  X  or even fo r rational pruning models such as [6] limited-memory particle-filter model. The probability of p arse failure at the disambiguating word w performed after processing each word provides another poin t at which particles representing the disfavored interpretation could be deleted. Consequently , total parse failure at the disambiguator will become more likely the greater the length of the precedi ng ambiguous region.
 word w w parser to fail is at the disambiguating verb. We can also comp are processing of these sentences with syntactically similar but unambiguous controls. Figure 3a shows, for each sentence of each type, the proporti on of runs in which the parser suc-cessfully integrated (assigned non-zero probability to) t he disambiguating verb ( was in Example 2a and ran in Example 3a), among those runs in which the sentence was suc cessfully parsed up to the preceding word. Consistent with our intuitive explanation , both the presence of local ambiguity and length of the preceding region make parse failure at the disa mbiguator more likely. of digging-in effects. The experiment provides a way to make more detailed comparisons between the model X  X  predictions and sentence acceptability. Consi stent with the predictions of the model, ratings show differences in the magnitude of digging-in eff ects associated with different types of structural ambiguities. As the working-memory resources ( i.e. number of particles) devoted to com-prehension of the sentence increase, the probability of suc cessful comprehension goes up, but local ambiguity and length of the second NP remain associated with greater comprehension difficulty. 5.1 Method Thirty-two native English speakers from the university sub ject pool completed a questionnaire cor-responding to the complexity-rating task. Forty experimen tal items were tested with four condi-tions per item, counterbalanced across questionnaires, pl us 84 fillers, with sentence order pseudo-randomized. Twenty experimental items were NP/S sentences and twenty were NP/Z sentences. We used a 2  X  2 design with ambiguity and length of the ambiguous noun phras e as factors. In NP/S sentences, structural ambiguity was manipulated by the pre sence/absence of the complementizer that , while in NP/Z sentences, structural ambiguity was manipul ated by the absence/presence of a on a scale from 0 to 10, 0 indicating  X  X ery easy X  and 10  X  X ery di fficult X . 5.2 Results and Discussion Figure 3b shows the mean complexity rating for each type of se ntences. For both NP/S and NP/Z sentences, the ambiguous long-subject (A-L) was rated the h ardest to understand, and the unam-biguous short-subject (U-S) condition was rated the easies t; these results are consistent with model predictions. Within sentence type, the ratings were subjec ted to an analysis of variance (ANOVA) with two factors: ambiguity and length. In the case of NP/S se ntences there was a main effect of Figure 3: Frequency of irrevocable garden path in particle-filter parser as a function of number of particles, and mean empirical difficulty rating, for NP/S an d NP/Z sentences. experiment thus bore out most of the model X  X  predictions, wi th ambiguity and length combining to make sentence processing more difficult. One reason that our model may underestimate the effect to be short in English, which was not captured in the grammar u sed by the model. In this paper we have presented a new incremental parsing alg orithm based on the particle filter and shown that it provides a useful foundation for modeling t he effect of memory limitations in human sentence comprehension, including a novel solution t o the problem posed by  X  X igging-in X  effects [15] for rational models. In closing, we point out tw o issues  X  both involving the problem of resampling prominent in particle filter research  X  in whic h we believe future research may help deepen our understanding of language processing.
 generating values of z (i.e., resampling) after every word from the multinomial ov er P ( z decreases monotonically with the number of observations. A nother option is to resample only when the variance in particle weights exceeds a predefined thresh old, sampling without replacement when this variance is low [22]. As Figure 2 shows, a word that resol ves a garden-path generally creates high weight variance. Our preliminary investigations indi cate that associating variance-sensitive failure approach taken in Section 5, but further investigat ion is required.
 The other issue involves how to resample . Since particle diversity can never increase, when parts of the space of possible T are missed by chance early on, they can never be recovered. As a conse-basic algorithm with additional steps such as running Marko v chain Monte Carlo on the particles in order to re-introduce diversity (e.g., [28]). Further wo rk would be required, however, to spec-ify an MCMC algorithm over trees given an input prefix. Both of these issues may help achieve a deeper understanding of the details of reanalysis in garde n-path recovery [29]. For example, the initial reaction of many readers to the sentence The horse raced past the barn fell is to wonder what a  X  X arn fell X  is. With variance-sensitive resampling, this observation could be handled by smoothing the probabilistic grammar; with diversity-introducing MC MC, it might be handled by tree-changing operations chosen during reanalysis.
 RL would like to thank Klinton Bicknell and Gabriel Doyle for useful comments and suggestions. FR and TLG were supported by grants BCS-0631518 and BCS-0704 34 from the National Science Foundation.

