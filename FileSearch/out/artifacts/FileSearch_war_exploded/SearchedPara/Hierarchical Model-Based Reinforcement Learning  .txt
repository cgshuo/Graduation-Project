 Nicholas K. Jong NKJ @ CS . UTEXAS . EDU Reinforcement Learning (RL) algorithms tackle a very challenging problem: how to find rewarding behaviors in unknown environments (Sutton &amp; Barto, 1998). An impor-tant goal of RL research is to generalize these algorithms to structured representations and to learn from limited exper i-ence. In this paper, we develop an algorithm that integrates two important branches of RL research that, despite their popularity, have rarely been studied in tandem.
 The first of these two branches is hierarchical RL. Humans cope with the extraordinary complexity of the real world in part by thinking hierarchically, and we would like to imbue our learning algorithms with the same faculty. In the RL community, this impetus has taken shape as work on tem-poral abstraction, in which temporally extended abstract actions allow agents to reason above the level of primi-tive actions (Barto &amp; Mahadevan, 2003). The advantages of such methods include the ability to incorporate prior knowledge and the creation of opportunities for state ab-straction. Recent work in the automatic discovery of hi-erarchy has focused on the ability to focus exploration in novel regions of the state space (S  X  ims  X ek &amp; Barto, 2004). The second branch is model-based RL, which directly es-timates a model of the environment and then plans with this model. Early work demonstrated that summarizing an agent X  X  experience into a model could be an efficient way to reuse data (Moore &amp; Atkeson, 1993), and later work uti-lized the uncertainty in an agent X  X  model to guide explo-ration, yielding the first (probabilistic) finite bounds on t he amount of data required to learn near-optimal behaviors in the general case (Kearns &amp; Singh, 1998; Kakade, 2003). Few RL researchers have tried to combine these two ap-proaches, despite the intuitive appeal of learning hierarc hi-cal models of the world. Prior work includes adaptations to the average-reward formulation (Seri &amp; Tadepalli, 2002) and to deterministic domains (Diuk et al., 2006). In this pa-per, we introduce an algorithm that fully integrates modern hierarchical-decomposition and model-learning methods i n the standard setting of discounted rewards and stochastic dynamics. Section 2 details how we decompose high-level models into lower-level models. Section 3 presents our al-gorithm, which joins our model decomposition with the R-MAX approach to learning primitive models. In Section 4, we formally analyze our algorithm, R-MAXQ . Section 5 describes our empirical results. In Section 6 we discuss related work more fully, and in Section 7 we conclude. We begin by describing our recursive action decomposi-tion, which defines how we plan at the high level given learned models of primitive actions. Section 3 presents a complete algorithm obtained by combining this decompo-sition with a particular way of learning primitive models. We adopt the standard semi-Markov decision process (SMDP) formalism for describing temporally extended ac-tions (Sutton et al., 1999), but we modify the notation to better reflect the recursive nature of hierarchical RL. We define an SMDP as the conjunction  X  S, A  X  of a finite state space S and a finite action set A . Each action a  X  A is associated with a transition function P a and a reward func-tion R a . For convenience, we use a multi-time model (Sut-ton et al., 1999), so P a ( s, s 0 ) = P  X  where  X   X  (0 , 1) is a discount factor and Pr( k, s 0 | s, a ) the probability that executing action a  X  A in state s  X  S will take exactly k time steps and terminate in state s 0  X  Similarly, R a ( s ) = E P  X  step reward earned during the k th time step executing a . If a  X  A is a primitive action , then it will always terminate after exactly one time step, so P s  X  S . Since we may construe a discount factor of  X  as equivalent to terminating a trajectory with probability 1  X   X  after each time step, the  X  X issing X  transition probability corresponds to the probability of termination.
 In the RL setting, each P a and R a is initially unknown, but for each a  X  A that is a composite action , we assume the agent is given a set of terminal states T a  X  S , a set of child actions A a , and a goal reward function  X  R a : T a  X  R composite action a may be invoked in any state s  X  S \ T a and upon reaching a state s 0  X  T a it terminates and earns an internal reward of  X  R a ( s 0 ) . It executes by repeatedly choos-ing child actions a 0  X  A a to invoke. The child actions may be primitive or composite. When a 0 terminates (and assuming a does not terminate), then a selects another child action. (In contrast to the original MAXQ framework, a composite action a only tests for termination upon the ter-mination of a child action a 0 .) A composite action a selects child actions to maximize the expected sum of the child Given the transition and reward functions for each of the child actions, the optimal policy for the composite action may be computed using the following system of Bellman equations, for all s  X  S and a 0  X  A a :
Q a ( s, a 0 ) = R a 0 ( s ) + X where A a ( s ) = n a 0  X  A a | primitive ( a 0 )  X  s /  X  T a Then the optimal policy  X  a : S  X  A a is, for all s  X  S : Dietterich X  X  MAXQ framework computes Q a ( s, a 0 ) by decomposing this quantity into Q a ( s, a 0 ) = R a 0 C a ( s, a 0 ) , where C a is a completion function that estimates the reward obtained after executing a 0 but before complet-ing a . It recursively queries the child action for R a 0 and learns C a locally using model-free stochastic approxima-tion. Using the learned  X  a , it simultaneously learns an ex-ternal version of C a that doesn X  X  include the internal goal rewards  X  R a , so that a can report R a to its own parents. The key idea behind our model-based approach is to as-sume that a composite action a can query a child a 0 for not Equation 1 is V a , which can be computed using standard dynamic programming methods and stored locally. To sat-isfy our assumption, each action a , whether primitive or composite, must be able to compute both R a and P a . Prior research into option models (Sutton et al., 1999) defined Bellman-like equations, for all s  X  S and x  X  T a : and for all s  X  S and x  X  S \ T a , P a ( s, x ) = 0 . Since is a multi-time model, note that P where the  X  X issing X  transition probability corresponds to the cumulative 1  X   X  probability of terminating (the entire trajectory, not just a ) marginalized over the random dura-tion of the execution of a . A key strength of our algorithm is that it takes advantage of models to solve Equations 4 and 5 directly using dynamic programming, instead of us-ing these equations to define update rules for stochastic ap-proximation, as in prior work with option models.
 Our decomposition provides a way to compute policies and therefore high-level transition and reward models give n lower-level transition and reward models. To ground out this process, models of the primitive actions must be avail-able. However, if R a and P a are available for each primi-tive action a , note that we could compute the optimal policy of the system using standard (non-hierarchical) planning algorithms. Nevertheless, we empirically demonstrate the benefit of using hierarchies in Section 5. The next section first presents our learning algorithm. Equations 1 X 5 show how to compute abstract models from primitive models, but a complete model-based RL algo-rithm must specify how to estimate the primitive models. We propose combining our hierarchical model decompo-sition, inspired by the MAXQ value function decomposi-tion, with the primitive models defined by the R-MAX al-gorithm (Brafman &amp; Tennenholtz, 2002), yielding a new algorithm we call R-MAXQ .
 R-
MAX defines the transition and reward models for prim-itive actions as follows. Let n ( s, a ) denote the number of times primitive action a has executed in state s n ( s, a, s 0 ) denote the number of times primitive action transitioned state s to state s 0 . Finally, let r ( s, a ) the cumulative one-step reward earned by each execution of primitive action a in state s . Then the primitive transi-tion and reward models are given by: tion and m is a threshold sample size. 1 Given sufficient data, R-MAX uses the maximum likelihood model, but it otherwise uses an optimistic model that predicts a high-rewards through the value function, the learned policy ef-fectively plans to visit insufficiently explored states. R-
MAXQ works in the same way, except it computes a hierarchical value function using its model decomposi-tion instead of a monolithic value function using the stan-dard MDP model. Optimistic rewards propagate not only through the value function V a at a given composite action a but also up the hierarchy, via each action X  X  computed ab-stract reward function R a . Each local policy implicitly ex-ploits or explores by choosing a child action with high ap-parent value, which combines the child X  X  actual value and possibly some optimistic bonus due to some reachable un-known states. No explicit reasoning about exploration is required at any of the composite actions in the hierarchy: as in R-MAX , the planning algorithm is oblivious to its role in balancing exploration and exploitation in a learning agent . A key advantage of R-MAXQ is that its hierarchy allows it to constrain the agent X  X  policy in a fashion that may reduce unnecessary exploratory actions, as illustrated in Sectio n 5. Algorithms 1 X 4 give the R-MAXQ algorithm in detail. All variables are global, except for the arguments a and which represent the action and state passed to each sub-routine. All global variables are initialized to 0, except t hat R a ( s ) is initialized to V max for all primitive actions states s . Algorithm 1 is the main algorithm, invoked with the root action in the hierarchy and the initial state of the system. MAXQ recursively executes an action a in the cur-rent state s , returning the resulting state s 0  X  T a . Primitive actions execute blindly; composite actions first update the ir policy and then choose a child action to execute, until some child leaves it in a terminal state.
 Algorithm 1 R-MAXQ ( a, s ) if a is primitive then else { a is composite } end if Algorithm 2 updates the policy for composite action given that the agent is in state s . It first constructs a plan-ning envelope : all the states reachable from s (at this node of the hierarchy) and thus relevant to the value of s . Once the planning envelope has been computed and all the child actions X  models have been updated on the envelope, the value function and policy could be computed using value iteration. Note that our implementation actually uses pri-oritized sweeping (Moore &amp; Atkeson, 1993) and aggres-sive memoization, not shown in our pseudocode, to ame-liorate the computational burden of propagating incremen-tal model changes throughout the hierarchy.
 Algorithm 2 COMPUTE -POLICY ( a, s ) if timestamp ( a ) &lt; t then end if while not converged do { value iteration } end while Algorithm 3 computes the planning envelope for compos-ite action a by examining the given state X  X  successors under any applicable child action X  X  transition model and recur-sively adding any new states to the envelope. This compu-tation requires that these models be updated, if necessary. Algorithm 3 PREPARE -ENVELOPE ( a, s ) if s 6 X  envelope ( a ) then end if Algorithm 4 updates the model for an action a at some state s . For composite actions, this requires recursively comput-ing the policy and then solving Equations 4 and 5. Algorithm 4 COMPUTE -MODEL ( a, s ) if a is primitive then else { a is composite } end if We now provide a very rough sketch of our main theoret-ical result: R-MAXQ probably follows an approximately optimal policy for all but a finite number of time steps. Un-fortunately, this number may be exponential in the size of the hierarchy. This section closes with a brief discussion o f the implications of this result.
 The original R-MAX algorithm achieves efficient explo-ration by using an optimistic model. Its model of any given state-action pair is optimistic until it samples that state-action m times. By computing a value function from this optimistic model, the resulting policy implicitly tra des off exploration (when the value computed for a given state includes optimistic rewards) and exploitation (when the value only includes estimates of the true rewards). Kakade (2003) bounded the sample complexity of RL by first show-ing that R-MAX probably only spends a finite number of time steps attempting to reach optimistic rewards (explor-ing). For the remaining (unbounded) number of time steps, the algorithm exploits its learned model, but its exploitat ion is near-optimal only if this model is sufficiently accurate. Kakade then bounded the values of m necessary to ensure the accuracy of the model with high probability.
 To be precise, let an MDP with finite state space S and fi-nite action space A be given. Let be a desired error bound,  X  the desired probability of failure, and  X  the discount fac-tor. Then R-MAX applied to an arbitrary initial state will probability greater than 1  X   X  thermore, there exists an m  X  O | S | L 2 that when the agent is not exploring, V  X   X  ( s where s The hierarchical decomposition used by R-MAXQ com-plicates an analysis of its sample complexity, but essen-tially the same argument that Kakade used provides a loose bound. We refer the interested reader to the proof of Kakade (2003) for the gross structure of the argument, and we merely sketch the necessary extensions here. A key lemma is Kakade X  X  -approximation condition (Lemma 8.5.4). The transition model  X  P for an action is an -approximation for the true dynamics P if for all states s  X  S , P condition states that if a model has the correct reward func-tion but only an -approximation of the transition dynamics for each action, then for all policies  X  and states s  X  S  X  Essentially, this condition relates the error bounds in the model approximation to the resulting error bounds in the computed value function. It allows the analysis of R-MAX to determine a sufficient value of m to achieve the desired degree of near optimality. We must extend this condition in two ways to adapt the overall proof to R-MAXQ . First, R-
MAXQ violates Kakade X  X  assumption of deterministic re-ward functions. Define a model reward function  X  R to be a  X  -approximation of the true reward function R if for all states s  X  S ,  X  R ( s )  X  R ( s ) &lt;  X  . Then it is straightfor-ward to adjust Kakade X  X  derivation of the -approximation condition to show that the computed value function for any given policy satisfies s  X  S ,  X  V  X  ( s )  X  V  X  ( s ) &lt; Second, for a given composite action a , we must relate error bounds in the approximations of R a 0 and P a 0 for each child a 0  X  A a to error bounds in the approxima-tions of R a and P a . Since R a is just the value func-tion for  X  a but without the goal rewards (Equation 4), we immediately obtain that the estimated R a will be an every s 0  X  T a , P a (  X  , s 0 ) can be thought of as a value func-tion estimating the expected cumulative discounted proba-bility of transitioning into s 0 . The total error in P a ( s,  X  ) be bounded by the sum of the errors for each s 0  X  T a , so it These results bound the errors that propagate up from the primitive actions in the hierarchy, but these bounds seem quite loose. In particular, these bounds can X  X  rule out the possibility that each level of the hierarchy might multiply amount of data required varies as the inverse square of , if R-
MAX requires m samples of each action at each state to achieve a certain error bound, R-MAXQ may require m at each state to achieve the same error bound at the root of the hierarchy, where T is the maximum number of reach-able terminal states for any composite action and h is the height of the hierarchy: the number of composite tasks on the longest path from the root of the hierarchy to a primitive action (not including the root itself).
 By adapting the remainder of Kakade X  X  proof, we can es-tablish that R-MAXQ will probably converge to a (recur-sively) near-optimal policy, although this guarantee re-quires exponentially more data than R-MAX in the worst case. We note that this guarantee applies to any choice of hierarchy. It remains to be seen whether it might be pos-sible to derive tighter bounds for specific classes of action hierarchies. Furthermore, as Kakade (2003) notes in his derivation, the -approximation condition is perhaps unnec-essarily stringent, since it gives the worst possible degra da-tion in approximation quality over all possible policies. In practice, implementations of R-MAX use far smaller val-ues of m than would be required to achieve useful theoreti-cal guarantees. In this vein, we note that running R-MAXQ will result in no more time spent in exploration than run-ning R-MAX with the same value for m . The hierarchical decomposition only weakens the guarantees on the near-optimality of the policy that R-MAXQ exploits. The exper-iments described in the next section show that a good hier-archy can even reduce the amount of time spent exploring, with no appreciable deterioration in solution quality. This section presents our empirical results, which show tha t R-MAXQ outperforms both of its components, R-MAX and MAXQ-Q. We discuss our findings in detail, to reveal how precisely our algorithm benefits from combining model-based learning and hierarchical decomposition.
 For our experiments, we use the familiar Taxi domain (Di-etterich, 2000). This domain consists of a 5  X  5 gridworld with four landmarks, labeled red , blue , green , and yellow , illustrated in Figure 1a. The agent is a taxi that must navigate this gridworld to pick up and deliver a pas-senger. The system has four state variables and six primi-tive actions. The first two state variables, x and y , give the coordinates of the taxi in the grid. The third, passenger , gives the current location of the passenger as one of the four landmarks or as taxi , if the passenger is inside the taxi. The final state variable, destination , denotes the land-mark where the passenger must go. Four primitive actions, north , south , east , and west , each move the taxi in the indicated direction with probability 0.8 and in each per -pendicular direction with probability 0.1. The pickup ac-tion transfers the passenger into the taxi if the taxi is at th e indicated landmark. The putdown action ends an episode if the passenger is in the taxi and the taxi is at the desired destination. Each episode begins with the taxi in a random location, the passenger at a random landmark, and a des-tination chosen randomly from the remaining landmarks. Each action incurs a  X  1 penalty, except that unsuccessful pickup and putdown actions cost  X  10 , and a successful putdown action earns a reward of 20.
 The structure of the Taxi domain makes it conducive for research into hierarchical RL. The optimal policy may be described abstractly in four steps. First, navigate to the landmark where the passenger is. Second, pick up the pas-senger. Third, navigate to the destination landmark. Fi-nally, put down the passenger. Navigation to each of the landmarks constitute reuseable subtasks that hierarchica l algorithms can exploit. Dietterich (2000) expressed this d o-main knowledge in the task hierarchy shown in Figure 1b. This hierarchy defines a navigational composite action for each of the four landmarks. These actions include the four primitive movement actions as children, and they terminate upon reaching the coordinates corresponding to the respec-tive landmark. The GET and PUT composite actions each have all four of their navigational composite actions as chi l-dren, as well as pickup or putdown , respectively. GET terminates when the passenger is in the taxi, and PUT ter-minates only when the episode does. The ROOT action only includes GET and PUT as children, and like PUT it defines no terminal states beyond those intrinsic to the domain. All goal reward functions give 0 reward; each action simply minimizes the costs earned before reaching their subgoals. In our experiments with R-MAX and R-MAXQ we set the threshold sample size at m = 5 . Preliminary experiments showed that larger values of m did not signicantly improve the final policy, although of course they led to more time spent estimating the model. The only other parameter for these algorithms is the stopping criterion for the dynamic programming steps in Algorithms 2 and 4. In all cases, we ran value iteration until the largest change was smaller tha n = 0 . 001 . We provided R-MAXQ and the original MAXQ-Q algorithm with the hierarchy shown in Figure 1b as prior knowledge. For our implementation of MAXQ-Q, we used precisely the hand-tuned parameters Dietterich (2000) opt i-mized for the initial value function, learning rates, and te m-perature decay (for Boltzmann exploration) for each action in the hierarchy. We conducted 100 independent trials of each condition of our experiments. 5.1. R-MAXQ versus R-MAX We begin by comparing the performance of R-MAXQ and R-MAX on the Taxi task. Our initial hypothesis was that R-
MAXQ would perform no better than R-MAX in the ab-sence of state abstraction, since the model-based ability t o plan to explore might subsume the exploratory role that options have played in many model-free RL implementa-tions (S  X  ims  X ek &amp; Barto, 2004; Singh et al., 2005). Figure 2 reveals that in fact the two algorithms exhibit very differe nt learning curves. In particular, although R-MAX requires many fewer episodes to converge to an optimal policy, R-MAXQ earns much greater total reward.
 We had overlooked the fact that the hierarchy used by R-MAXQ doesn X  X  so much guide exploration as it constrains it. In particular, note that the hierarchical agent can neve r attempt the putdown action except at one of the four landmark locations, since the PUT action only becomes available when the agent is already at one of these loca-tions, and the four navigational actions keep the agent in this reduced set of states. The agent thus only attempts the putdown action in 12 incorrect states, instead of the 396 explored by R-MAX . In addition, R-MAX attempts the pickup action in 100 states in which R-MAXQ doesn X  X , when the passenger is already in the car. Since the penalty for incorrect usage of these actions is -10, R-MAX 10(396  X  12 + 100) m = 24200 reward due to its wasted exploration, accounting for the difference between the two algorithms in Figure 2a. Furthermore, since the GET action cannot navigate to an arbitrary location, R-MAXQ can X  X  at-tempt the pickup action in a non-landmark location un-til some episode randomly starts the agent there. In this case the hierarchy can only postpone, not prevent, wasted exploration. This effect explains the delayed convergence relative to R-MAX : in later episodes R-MAXQ spends time on exploration that R-MAX performed more eagerly. 5.2. R-MAXQ versus MAXQ-Q Figure 2 also compares R-MAXQ with the original MAXQ-Q algorithm. Of course, this comparison isn X  X  very fair, since a primary goal of the MAXQ framework was to create opportunities for state abstraction (Dietterich, 2000), w hich we did not initially exploit. In fact, Dietterich identified the condition described in Section 5.1, which he called shield-ing, as one that permits abstraction. For a more fair com-parison, we allowed our implementation of MAXQ-Q to use all the state abstractions in the Taxi domain identified by Dietterich (2000), along with his optimized parameters. We applied Dietterich X  X  notion of max node irrelevance to allow R-MAXQ also to enjoy an explicit form of state ab-straction as prior knowledge. Each action in the hierarchy abstracts away state variables when our domain knowledge indicates that doing so would not compromise the learned model. However, whereas in MAXQ-Q an action a only reports its abstract reward function R a to its parents, in R-MAXQ it must also convey the abstract transition function P a . Thus we only allow a composite action to ignore a state variable if all of its children also ignore that state variab le. In the hierarchy shown in Figure 1b, the four primitive movement actions and the four navigational actions can abstract away the passenger and destination state variables. GET and pickup ignore destination , and PUT and putdown ignore passenger . However, ROOT cannot ignore any state variables. When a child X  X  transition function was more abstract than a parent X  X  model, the par-ent assumed a very simple dynamic Bayes network (DBN) factorization (Boutilier et al., 1995). For example, P north sets x and y (each conditional on the previous values of both variables), but passenger and destination re-main constant. Figure 3 compares the performance of the resulting algorithms. Both MAXQ-Q and R-MAXQ learn much faster with state abstraction, with the model-based nature of R-MAXQ continuing to give it an edge.
 It is worthwhile to examine more closely how the hier-archy interacts with state abstraction in the Taxi domain. Consider how MAXQ-Q learns the ROOT action. The only values stored locally are the completion functions stract representations. The latter function is always equa l to 0, since after PUT terminates there is nothing to complete, since the entire episode has terminated. Meanwhile, to evaluate C root ( s, GET ) the algorithm need only inspect the passenger and destination variables of s , since the values of these two variables before executing GET com-pletely determine the remaining cost of completing ROOT after GET terminates. Hence, MAXQ-Q only learns 16 val-ues at the ROOT node; to compute the value of a state it re-cursively queries R a and adds the appropriate completion function (Dietterich, 2000).
 R-MAXQ doesn X  X  apply any explicit state abstraction to ROOT , but note that after executing either of its two child actions, the result must be one of 12 nonterminal states: with the taxi at one of four landmarks, the passenger in the taxi, and the destination at one of the other three land-marks. Hence, the planning envelope computed in Algo-rithm 2 will always contain some subset of these 12 states plus the current state. As with MAXQ-Q, the result dis-tribution irrelevance of GET allows R-MAXQ to store only a small number of values locally. To compute the value of a state, R-MAXQ also queries one-step values from its children and then adds the appropriate successor state val-ues. In this sense, these 12 states can be thought of as the completion set of ROOT.
 Figure 3 also shows the performance of standard R-MAX with the same DBN factorization as R-MAXQ applied to most of its actions (which are all primitive). Note that in th e absence of shielding, putdown cannot safely ignore the passenger variable. The ability to abstract the primitive models does reduce the amount of exploration that R-MAX must perform, but the improvement is significantly smaller than that of the other algorithms. This result gives more support for motivating hierarchical decomposition with op -portunities for state abstraction.
 Some preliminary further experiments support the argu-ments of Jong et al. (2008), who used model-free hierarchi-cal algorithms to suggest that composite actions more reli-ably improve RL performance when they replace instead of augment primitive actions. We ran R-MAXQ with a hierar-chy in which the root X  X  children included all six primitive actions as well as the four navigational composite actions, producing learning curves indistinguishable from those of standard R-MAX in Figure 2. When the root action can ex-ecute every primitive action, the planning envelope grows to include too many states. Formalizing the properties of a composite action X  X  completion set may help us understand how hierarchies can constrain planning envelopes without sacrificing learning performance. Other algorithms have combined hierarchical RL with a model-based approach, but not in the standard framework of discounted rewards and stochastic dynamics. Diuk et al. (2006) developed a model-based MAXQ algorithm for de-terministic domains, allowing them to quickly sample the effect of a composite action recursively: every action X  X  ef -fect can be represented as a scalar reward and a single successor state. Their algorithm also uses Dietterich X  X  ap -proach to state abstraction, occasionally forcing it to re-plan, since the effect of a child action may depend on state variables not visible to the parent, making it seem nonde-terministic. In contrast, R-MAXQ does not employ explicit state abstraction, allowing it to save the value functions a nd policies computed during one time step for all future time steps. Our algorithm relies on the choice of hierarchy to yield small planning envelopes, automatically achieving a n effective reduction in the size of the state space considere d during any one time step.
 Seri and Tadepalli (2002) extended the MAXQ framework to average-reward reinforcement learning, resulting in an algorithm that learns a model to facilitate the computation of the bias for each state from the average reward of the current policy. However, the computation of the average reward itself relies on stochastic approximation techniqu es, and their algorithm does not have any formal guarantees regarding its sample complexity. The R-MAXQ algorithm combines the efficient model-based exploration of R-MAX with the hierarchical decom-position of MAXQ. Although our algorithm does not im-prove upon the known formal bounds on the sample com-plexity of RL, it retains a finite-time convergence guar-antee. An empirical evaluation demonstrates that even a relatively simple hierarchy can improve the cumulative re-ward earned by constraining the exploration that the agent performs, both within individual episodes of learning and throughout an agent X  X  experience with its environment. Even in the absence of explicit state abstraction, the struc -ture of an action hierarchy can drastically reduce the ef-fective state space seen by a given composite action dur-ing a single episode. This implicit concept of a reduced completion set, mirroring Dietterich X  X  explicitly abstra cted completion function, suggests future avenues of research, both for improving the theoretical guarantees on the sam-ple complexity of R-MAXQ and for guiding the discovery of more useful hierarchies.

