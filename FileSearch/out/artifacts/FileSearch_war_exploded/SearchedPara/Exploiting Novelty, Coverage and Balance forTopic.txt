 Novelty, coverage and balance are important requirements in topic-focused summarization, which to a large extent de-termine the quality of a summary. In this paper, we pro-pose a novel method that incorporates these requirements into a sentence ranking probability model. It differs from the existing methods in that the novelty, coverage and bal-ance requirements are all modeled w.r.t. a given topic, so that summaries are highly relevant to the topic and at the same time comply with topic-aware novelty, coverage and balance. Experimental results on the DUC 2005, 2006 and 2007 benchmark data sets demonstrate the effectiveness of our method.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Abstracting methods ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  Text analysis Algorithms, Experimentation, Performance topic-focused summarization, topic-aware novelty, topic-aware coverage and balance, relevance measure
Automatic document summarization has received great attention in recent years, due to the explosive growth of doc-uments on the World Wide Web. One specific task of docu-ment summarization is topic-focused summarization, which is to generate summaries that are relevant to a given topic.
A good summary should satisfy the following three re-quirements: novelty ([1], also called diversity in [4]), cover-age [6, 4] and balance [4]. They require that sentences in a summary should describe different contents, cover every important aspect and pay more attention to more impor-tant aspects of the documents, respectively. When a topic is introduced for topic-focused summarization, we believe that topic-aware novelty, coverage and balance are preferred. These requirements are defined as follows. Topic-aware nov-elty requires that sentences in the summary should overlap less in topic-related contents. Topic-aware coverage requires that the summary should cover different aspects of topic-related contents. Topic-aware balance requires that differ-ent aspects of topic-related contents should have the same relative importance in the summary as in the documents.
In this paper,we propose an unsupervised topic-focused multi-document summarization method. It differs from the existing methods in that the novelty, coverage and balance requirements are all modeled w.r.t. a given topic, so that summaries are obtained from sentences that are highly rel-evant to the topic and at the same time comply with topic-aware novelty, coverage and balance. In the method, the rel-evance of a sentence to a topic is modeled as a topic relevance probability that takes into account the novelty requirement, while coverage and balance are modeled by a sentence se-lection preference function. Our method is quite effective. Extensive experiments on the DUC 1 2005  X  2007 bench-mark data sets demonstrate that our method outperforms representative existing approaches in all specified evaluation measures.
We briefly mention some related summarization methods, especially the those that considers novelty, coverage and bal-ance requirements.

Li et al. [4] treats summarization as a supervised sentence ranking problem, where structural SVM is applied. Novelty, coverage and balance requirements are incorporated into the sentence ranking process. However, [4] focuses on generic summarization, with no topic-related information consid-ered. Unsupervised methods such as TextRank [5], Lex-PagePank [3] and ManifoldRank [8] rank sentences based on graph ranking algorithms. [8] uses the maximum marginal relevance (MMR) [1] like method to penalize overlapping of sentences, where all words are treated equally whether or not they are related to the topic. The method proposed in [9] is closely related to our work in that it achieves topic-aware novelty. Similarity between sentences are biased toward the topic using the proposed query-sensitive similarity in the pa-per. Thus only overlapping of the topic-related contents is http://www-nlpir.nist.gov/projects/duc/index.html penalized if the MMR method is applied. Some summariza-tion methods such as ClusterCMRW, ClusterHITS [7] and [6, 10] utilize clustering techniques. The rationale behind the clustering-based methods is that different clusters rep-resent different aspects of the documents, which should all be covered if possible.
In this section, we introduce a sentence ranking method that incorporates topic-aware novelty, coverage and balance into a unified framework.
Topic-focused multi-document summarization tasks are often viewed as sentence ranking problems. If we can es-timate the topic relevance probabilities of the sentences, we can rank sentences according to their relevance probabili-ties. We denote the relevance probability of a sentence s atopic t by P ( R t,s =1 | t, s ), where R t,s = 1 denotes that and s are relevant and R t,s = 0 otherwise.

In this section, we propose a method for estimating P ( R 1 | t, s ). We model the information need of a topic as a set of topic words t = { w 1 ,w 2 , ..., w m } . For convenience, in the following by a topic we refer to its information need. A sen-tence s is considered to be relevant to a topic t if s covers some information need of t , i.e. some w i , 1  X  i  X  m in denote by R w,s =1that s cover w and R w,s = 0 otherwise. Therefore, the topic relevance probability P ( R t,s =1 | t, s defined as: We assume that R w i ,s and R w j ,s are independent for any i = j . By this assumption, Equation (1) can be rewritten as:
As a result, to compute P ( R t,s =1 | t, s )itsufficestoes-timate P ( R w i ,s =1)(1  X  i  X  m ). A simple term match-ing method does not work here. A sentence s may provide some information about w i even if w i does not occur in s One observation is that while sentences where w i appears are likely to provide some information about w i ,othersen-tences closely related to such sentences are also likely to provide information about w i . We appeal to manifold rank-ing algorithm [11], where sentences that w i appears in are chosen as seeds (setting relevance scores to 1). Relevance scores are then propagated among the sentences. Given a topic word w k , the algorithm that computes the relevance probabilities of the sentences is given as follows. Let W be the similarity matrix, where W ij ( i = j ) is the similarity between sentences s i and s j , computed by cosine similarity, and W ii = 0. Normalize W as S = D  X  1 / 2 WD  X  1 / 2 ,where D is a diagonal matrix with ( i , i )-element equal to the sum of the i -th row of W . The manifold algorithm iterates the following equation to get the scores f w k =[ f w k 1 , ..., f the sentences. where 0 &lt; X &lt; 1and y w k =[ y w k 1 , ..., y w k n ] T Without loss of generality, we initialize f w k (1) ( t =1)to y f of s i to w k is computed by where  X &gt; 0 is a smoothing parameter. For simplicity, P (
By inserting Equation (5) into Equation (2), we obtain the topic relevance probabilities for all sentences in D .These sentences are then ranked in the order of their topic rele-vance probabilities. The K top ranked sentences might be selected to form a topic-focused summary of D if we did not consider novelty, coverage and balance of the summary.
Most previous ranking-based approaches achieve novelty by penalizing sentences using an MMR [1] like method. The limitation of this method is that it penalizes each word equally, whether the word is related to a topic or not. In this section, we present an approach that implements topic-aware novelty by extending Equation (2).

Suppose we have already had k  X  1 sentences s 1 , ..., s k  X  1 in a summary. These sentences have covered part of the topic t = { w 1 , ..., w m } . Wewanttoaddanewsentence s k to the summary such that it covers the most possible of the remaining part of t and the least possible of the part already covered by s 1 , ..., s k  X  1 . We do this by modifying the topic relevance probabilities of sentences, and use P ( R 1 s to topic t given s 1 , ..., s k  X  1 . We define
P ( R t,s k =1 | t, s 1 , ..., s k ) =
P (  X  w i  X  t, R w This definition states that given sentences s 1 , ..., s k  X  1 tence s k is relevant to a topic t if s k covers at least one topic word w i that is not covered by any s j (1  X  j  X  k  X  1). Here we assume that R w k ,s i is independent of R w t ,s any k = t or i = j . By the same way for Equation (2), expanding Equation (6) leads to
We take the logarithm of P ( R t,s k =1 | t, s 1 , ..., s novelty gain of sentence s k given s 1 , ..., s k  X  1 : Obviously, the higher the novelty gain of s k is, the more relevant to the topic s k is.

We may construct summaries from documents by selecting one by one the sentences with the highest novelty gains. Such summaries are highly relevant to the given topic and at the same time satisfy topic-aware novelty.
Similar to [6, 10], we cluster sentences for coverage con-sideration. To achieve topic-aware coverage and balance, we first remove all sentences that are not closely relevant to the topic. Since sentences irrelevant to the topic may act as noises in the clustering process, sentence pruning is an important step towards topic-aware coverage and balance. Specifically, we compute sentence relevance probabilities us-ing equation (2) for all sentences. Sentences whose relevance probabilities are lower than a threshold are discarded. After that, we cluster sentences. Then we assign to each sentence a sentence selection preference according to cluster distribu-tion and select sentences based on their selection preferences to achieve coverage and balance. The selection preference function is defined as follows.

Suppose we have built M clusters ( C 1 , ..., C M )fromthe remaining sentences by applying a clustering algorithm such as Kmeans. Denote by ( r 1 , ..., r M ) the proportions of the clusters, where r i = | C i | / j | C j | , 1  X  i  X  M ,and the number of sentences in C i . We view r i (1  X  i  X  M )as the expected proportions of important topic-relevant aspects in each summary. Suppose we have already selected k  X  1 sentences. Let r k  X  1 i = N k  X  1 i / ( k  X  1) , 1  X  i  X  M distribution of the k  X  1 sentences in the M clusters, where i is the number of sentences occurring in s 1 , ..., s k  X  1 that belong to cluster C i . r k  X  1 i , 1  X  i  X  M are the actual proportions of important topic-relevant aspects. We want i to be as close to r i as possible to meet coverage and balance requirement. Thus, we score each cluster by where f ( x ) &gt; 0 and is a strictly monotonically increasing function of x .Wetake f ( x )= exp ( x )inthispaper. We then define the selection preference for cluster C i as
The above preference shows the degree of suitability un-der coverage and balance requirements to select the next sentence from C i ,giventhat s 1 , ..., s k  X  1 have already been selected. For each sentence s k in cluster C i , we take the log-The higher the coverage and balance gain is, the more likely s is to cover aspects of the topic different from s 1 , ..., s
Suppose we have already selected the first k  X  1 sentences s , ..., s k  X  1 for a summary. By combining the novelty gain (Equation (8)) and the coverage and balance gain (Equation (10)), we obtain the following gain for selecting the next sentence s k given s 1 , ..., s k  X  1 : where  X  is a parameter used to tune the tradeoff between the novelty gain and the coverage and balance gain. Again, thehigherthegainof s k is, the better s k summarizes the documents w.r.t. the topic. Therefore, to produce the sum-mary, we select sentences that maximize the gain given in Equation (11) one by one.
We use the popular topic-focused summarization bench-mark data sets DUC 2005, 2006 and 2007 2 for our exper-iments. The automatic summarizer is expected to extract from each document collection a summary that does not ex-ceed 250 words. We use ROUGE 1.5.5 3 package for evalua-tion, which is officially adopted by DUC for evaluating auto-matic generated summaries. Parameter setting for ROUGE is the same as the official parameter setting 4 of DUC. For each document, we use the OpenNLP 5 tool to detect and tokenize sentences. A list of 707 words is used to filter stop words. The remaining words are stemmed by Snowball 6 .
In our experiments we set the parameters empirically. We set  X  in Equation (3) to 0.95 and  X  in Equation (11) to 8. As for the irrelevant sentence pruning, we keep one sixth of the total sentences for each document collection. Cluster number is set to 0.4 times the number of sentences left. We employ two popular clustering algorithms: Kmeans and ag-glomerative hierarchical clustering. For Kmeans, seeds are randomly chosen, algorithm 2 is run five times, and average performance is collected.

We denote our algorithm by NCBsum, where  X  X  X ,  X  X  X  and  X  X  X  stand for novelty, coverage and balance respectively. For simplicity, by  X  X CBsum-A X  we refer to our algorithm where agglomerative hierarchical clustering is applied to cluster sentences, and by  X  X CBsum-K X  we refer to our algorithm where Kmeans is applied.

We compare our algorithm with the following algorithms. (1) Nsum: a method that uses novelty gain (Equation 8) to select sentences. (2) Csum: a method that considers topic-aware coverage. It prunes irrelevant sentences according to Equation (2), clusters the remaining sentences and selects the most relevant sentences from different clusters. (3) Cov-erage: a baseline clustering-based method. It clusters the sentences and select the most relevant sentences from differ-ent clusters. (4) Manifold: the manifold ranking algorithm proposed by Wan et al. [8] without considering novelty, coverage and balance. (5) Auto avg: the average scores of the participating systems in DUC 2005, 2006 and 2007 re-spectively. (6) Random: a baseline algorithm that produces summaries through random sentence selection.

Tables 1, 2 and 3 show the experimental results. Scores in bold are the best scores. Scores with * mean the perfor-mance improvement over the Manifold Ranking algorithm is significant with confidence level 95%. Clearly, our algo-rithm outperforms the comparative algorithms on all the f -measures. Our algorithm NCBsum-A ranks 1st, 2nd and 4th among all the participating systems in DUC 2005, 2006 and 2007 respectively.
Refer to http://www-nlpir.nist.gov/projects/duc/data.html for a detailed description of the datasets. http://www.isi.edu/publications/licensed-sw/BE/index.html -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -d http://opennlp.sourceforge.net/ http://snowball.tartarus.org/index.php NCBsum-A 0.38868 * 0.07878 * 0.13424 * NCBsum-K 0.3857* 0.07678 0.13282* NCBsum-A 0.40869 * 0.08981 * 0.14074 * NCBsum-K 0.40515* 0.08698* 0.13972* NCBsum-A 0.4289 * 0.11139 * 0.1478 * NCBsum-K 0.42244* 0.10931* 0.1464*
We would like to compare our algorithm with Qs-MRC [9], which to the best of our knowledge, is the only algorithm that considers topic-aware novelty in summarization. We notice that only ROUGE recall scores on DUC 2005 are provided in that paper. Therefore we compare recall of our algorithm with that of Qs-MRC, which are shown in table 4. Systems 15 and 17 are the two best participating systems in DUC 2005. Obviously, NBCsum-A outperforms Qs-MRC in all recall measures.
 NCBsum-A 0.39092 0.07924 0.13794 System 15 0.3751 0.0725 0.1316 System 17 0.3697 0.0717 0.1297
Table 4: Recall-measure comparison on DUC05  X  (Equation (11)) is a parameter used to tune the trade-off between the topic relevance probability with the topic-aware novelty and topic-aware coverage and balance. We did experiments with different  X  values to see how it affects the quality of summaries. Figures 1 shows the experimental results with  X  varying from 0 to 512. On the whole, perfor-mance of the algorithm first improves and then degenerates with the increase of  X  .

This work is supported in part by NSFC grants 60970045 and 60721061, and by the National High-tech R&amp;D Program (863 Program). [1] J. Carbonell and J. Goldstein. The use of mmr, [2] C. L. Clarke, M. Kolla, G. V. Cormack, [3] G. Erkan and D. R. Radev. Lexpagerank: Prestige in [4] L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. [5] R. Mihalcea and P. Tarau. Textrank: Bringing order [6] T. Nomoto and Y. Matsumoto. A new approach to [7] X. Wan and J. Yang. Multi-document summarization [8] X. Wan, J. Yang, and J. Xiao. Manifold-ranking based [9] F. Wei, W. Li, Q. Lu, and Y. He. Query-sensitive [10] H. Zha. Generic summarization and keyphrase [11] D. Zhou, J. Weston, A. Gretton, O. Bousquet, and
