 Indian Institute of Science, Bangalore 560012, India Shivaram Kalyanakrishnan SHIVARAM @ YAHOO -INC . COM Yahoo Labs Bangalore, Bangalore 560071, India Indian Institute of Science, Bangalore 560012, India The problem of estimating class probabilities from data with binary labels is a fundamental one in machine learn-ing, and arises in several applications in practice, including for example medical diagnosis, fraud prediction, click-rate prediction in web advertising, etc. In many of these appli-cations, one class is rare compared to the other: in medi-cal diagnosis, only a few patients develop a given disease; in fraud prediction, only a few transactions turn out to be fraudulent; in web advertising, only a few ad impressions result in clicks, and so on. Yet in all these applications, it is important to accurately estimate the probability of the rare class occurring: in medical diagnosis, these probabili-ties help in deciding the right course of treatment; in fraud prediction, these probabilities help in estimating the risk of various actions; in web advertising, these probabilities help in deciding how to rank or display various ads, and so on. It is well known that classical approaches such as logis-tic regression do not perform well in such settings, espe-cially when the probability of the rare class is very small and the number of training examples is limited (Czado &amp; Santner, 1992; King &amp; Zeng, 2001; Zhang, 2004). The rea-son for this is that the logistic loss used in logistic regres-sion is symmetric in nature, i.e. it assigns equal penalty for the losses on positive and negative examples. Common fixes used in practice include under-sampling the majority class to balance the two classes before training or weight-ing losses on positive and negative examples differently, and then applying some form of correction scheme when estimating probabilities from the learned model (King &amp; Zeng, 2001; Wallace &amp; Dahabreh, 2012).
 The logistic loss can be viewed as a proper composite loss that combines the well-known logarithmic loss for bi-nary class probability estimation (CPE) with the symmet-rithmic loss (Buja et al., 2005; Reid &amp; Williamson, 2010). An alternative approach is to use an asymmetric link func-tion that helps penalize mispredictions on positive exam-ples differently from those on negative examples. Recently, Wang &amp; Dey (2010) suggested the use of a parametrized family of asymmetric link functions based on the gener-alized extreme value (GEV) distribution, which has been used for modeling rare events in statistics (Kotz &amp; Nadara-jah, 2000; Embrechts et al., 1997); a similar approach was used in (Calabrese &amp; Osmetti, 2011). These works use a probabilistic model together with maximum likeli-hood/maximum a posteriori estimation, which effectively composes the GEV links with the same logarithmic CPE loss as that used in logistic regression; unfortunately, this results in a non-convex optimization problem.
 In this paper, we use tools from the theory of proper com-posite losses, which are natural choices for CPE problems in general (Buja et al., 2005; Reid &amp; Williamson, 2010), to construct a proper composite loss with desirable proper-ties for CPE problems when one class is rare. Specifically, we derive a family of underlying CPE losses for which the GEV links form the  X  X anonical X  link. The resulting proper composite loss family, which we call the GEV-canonical loss family, can be used to adapt to the degree of rarity in the data and accordingly penalize wrong predictions on positive examples more heavily than those on negative ex-amples. In addition, due to properties of proper composite losses formed using canonical links, each loss in the GEV-canonical loss family is convex, allowing us to use an itera-tive reweighted least squares (IRLS) algorithm for its min-imization, similar to that used in common logistic regres-sion implementations. The resulting algorithm, which we term GEV-canonical regression , outperforms various base-lines in experiments with both synthetic and real data, par-ticularly when the number of training examples is limited. Related Work. In addition to the work mentioned above, there has been much interest in learning in class imbalance settings in general, with several workshops, survey articles, and editorials devoted to the topic over the years (Provost, 2000; Japkowicz, 2000; Chawla et al., 2004; Van Hulse et al., 2007; He &amp; Garcia, 2009). Much of this work fo-cuses on classification in class imbalance settings, where again approaches such as weighting the two classes dif-ferently, subsampling the majority class or over-sampling the minority class before training, and using calibration to correct predictions afterwards are widely used (Chawla et al., 2002; Drummond &amp; Holte, 2003; Van Hulse et al., 2007; Lee et al., 2012). Such techniques are also used for cost-sensitive learning (Elkan, 2001; Zadrozny et al., 2003; Masnadi-Shirazi &amp; Vasconcelos, 2010), wherein different misclassification errors are penalized differently. A differ-ent approach to cost-sensitive learning, which bears some relation to our work, is that of Guerrero-Curieses et al. (2004), who designed loss functions to predict class prob-abilities accurately around a given classification threshold in order to minimize classification errors. In this paper, our interest is primarily in CPE problems in class imbalance settings, where we wish to estimate reliably the class prob-abilities, particularly in the region of small probabilities but in other regions as well, despite the imbalance in the data. Organization. We start with preliminaries and background on proper loss functions, link functions, the GEV link family, and proper composite losses in Section 2. Sec-tion 3 derives the proper loss for which the GEV link forms the  X  X anonical X  link, constructs the GEV-canonical proper composite loss, and describes the resulting GEV-canonical regression algorithm. Section 4 gives our experimental re-sults comparing the GEV-canonical regression algorithm with several baselines on both synthetic and real data. We conclude with a brief discussion in Section 5. Notation. We denote R = (  X  X  X  ,  X  ) , R = [  X  X  X  ,  X  ] , R + = [0 ,  X  ) , and R + = [0 ,  X  ] . For z  X  R , we denote z + = max(0 ,z ) .
 Problem Setup. We consider binary CPE problems where there is an instance space X , binary label space Y = { X  1 } , and an underlying (unknown) probability distribution D on X  X { X  1 } from which both training examples and future test examples are assumed to be drawn i.i.d. Let ( X,Y ) de-note a random example drawn from D . Let p = P ( Y = 1) denote the overall probability of the positive class under D , and let  X  : X X  [0 , 1] denote the associated class probabil-ity function:  X  ( x ) = P ( Y = 1 | X = x ) . Given a training sample S = (( x 1 ,y 1 ) , ( x 2 ,y 2 ) ,  X  X  X  , ( x n ,y n goal is to learn from S a CPE model curately estimates the true class probability function  X  . We are interested in settings where one class (say the positive class) is rare, so that p 0 . 5 .
 CPE Loss Functions and Proper Losses. A CPE loss function is any loss function c : { X  1 } X  [0 , 1]  X  R + that as-signs a penalty c ( y, ability of a positive label when the true label is y  X  { X  1 } . A CPE loss c can equivalently be defined through its par-tial losses c 1 : [0 , 1]  X  R + and c  X  1 : [0 , 1]  X  R + c ( loss c log : { X  1 } X  [0 , 1]  X  R + , with partial losses given by For any CPE loss function c , define the point-wise c -risk L c : [0 , 1]  X  [0 , 1]  X  R + as follows: 1 A CPE loss c is said to be proper if the point-wise c -risk L (  X , and strictly proper if in addition this minimizer is unique. This is a desirable property for any CPE loss as it ensures that minimizing the loss yields the correct probability es-timates. Proper losses are related to proper scoring rules that have been used in the probability forecasting literature (Savage, 1971; Hendrickson &amp; Buehler, 1971; Schervish, 1989; Gneiting &amp; Raftery, 2007), and have received signif-icant interest in the machine learning community recently (Buja et al., 2005; Reid &amp; Williamson, 2009; 2010; Agar-wal, 2013). It can be verified that the logarithmic loss c defined above is strictly proper.
 Link Functions and the GEV Link Family. In practice, when learning a CPE model learns a real-valued scoring model f S : X X  X  for some V  X  R , and then maps the real-valued scores to probabil-ity estimates in [0 , 1] via a link function . A link function is any strictly increasing function  X  : [0 , 1]  X  X  ; one then uses the inverse link function  X   X  1 : V X  [0 , 1] to map scores f ( x )  X  V to probability estimates One of the most widely used link functions is the logit link  X  logit : [0 , 1]  X  R , defined as Other common links include the probit link  X  probit : [0 , 1]  X  R , defined as where  X  denotes the standard normal CDF, and the comple-mentary log-log (cloglog) link  X  cloglog : [0 , 1]  X  R , defined as The logit and probit links are both symmetric, in that they satisfy  X  ( A general method for constructing a link function is to use the inverse CDF of a continuous real-valued random vari-able, just as the probit link uses the inverse standard nor-mal CDF. Recently, Wang &amp; Dey (2010) proposed the use of the CDF of the generalized extreme value (GEV) dis-tribution, used in statistics for modeling rare events (Kotz &amp; Nadarajah, 2000; Embrechts et al., 1997), for construct-ing a parametric family of asymmetric links. Specifically, the CDF of the GEV distribution with location parameter  X  = 0 , scale parameter  X  = 1 , and shape parameter  X   X  R which we shall denote as F  X  : R  X  [0 , 1] , is defined as Clearly, this distribution has support [  X  1  X  ,  X  ) for  X  &gt; 0 ; (  X  X  X  ,  X  1  X  ] for  X  &lt; 0 ; and R for  X  = 0 ; taking the limit in the above as  X   X  0 , one recovers the Gumbel distribution: F ( v ) = exp(  X  exp(  X  v )) . Denote the extension of the above support by R  X  : The corresponding GEV link, parametrized by  X   X  R and which we denote as  X  GEV (  X  ) : [0 , 1]  X  R  X  , is then defined as  X  In the limit  X   X  0 , it leads to the standard log-log link:  X 
GEV (0) ( b  X  ) =  X  ln(  X  ln( b  X  )) . The parameter  X  can be ad-justed to yield different degrees of asymmetry in the above link, which in turn can be used to fit different degrees of rar-ity in the data. This is similar to how the GEV distribution is used traditionally, where one selects the most appropri-ate distribution in the GEV family to model the underlying data by adjusting the parameter  X  .
 Proper Composite Losses and Canonical Links. A com-mon way to learn a real-valued scoring function f S : X X  X  (for V  X  R as above) is to minimize a loss function ` : { X  1 } X V X  R + on the training sample S , i.e. to minimize P i =1 ` ( y i ,f ( x i )) over some suitable class of functions f , where ` ( y,v ) can be viewed as the penalty assigned by ` for predicting a score v  X  V when the true label is y  X  { X  1 } . Again, any such loss ` can equivalently be defined through its partial losses ` 1 : V X  R + and `  X  1 : V X  R + , given by ` ( v ) = ` ( y,v ) . A popular loss operating on scores in is the logistic loss ` logistic : { X  1 } X  R  X  R + used in logistic regression, defined as A loss function ` : { X  1 } X V X  R + is said to be proper composite (Buja et al., 2005; Reid &amp; Williamson, 2010) if it can be written as a composition of a proper CPE loss c : { X  1 } X  [0 , 1]  X  R + and a link  X  : [0 , 1]  X  X  , so that and strictly proper composite if in addition c is strictly is a strictly proper composite loss, composed of the strictly proper logarithmic CPE loss and the logit link. It is com-mon to compose the logarithmic CPE loss with other link functions as well, such as the probit or cloglog links. The approach in (Wang &amp; Dey, 2010; Calabrese &amp; Osmetti, 2011) uses the GEV link in a probabilistic model and per-forms maximum likelihood or maximum a posteriori esti-mation under a suitable prior, which also amounts to ef-fectively composing the GEV link with the logarithmic CPE loss (and possibly adding regularization to the result-ing minimization problem). Unfortunately, this results in a non-convex optimization problem.
 For any strictly proper loss, there is a unique  X  X anonical X  link for which the resulting composite loss satisfies vari-ous desirable properties, including convexity; conversely, for any link function, there is a unique  X  X anonical X  strictly proper loss (Buja et al., 2005; Reid &amp; Williamson, 2010). The logit link and logarithmic loss form a canonical pair. Below we construct a proper composite loss using the GEV link and its corresponding canonical proper loss. In this section we examine the GEV link family more closely through the lens of proper composite losses. Us-ing results of (Buja et al., 2005; Reid &amp; Williamson, 2010), we derive a parametric proper CPE loss for which the GEV link forms the canonical link. This allows us to maintain the attractive properties of the GEV link for CPE settings when one class is rare, namely flexibility of the link func-well as obtain desirable properties for the overall compos-ite loss, such as convexity in the second argument. We term the resulting proper composite loss the GEV-canonical loss (see Table 1 and Figure 1 for a summary). This loss can be minimized efficiently using an IRLS algorithm similar to that used in logistic regression implementations; we term the resulting algorithm GEV-canonical regression . 3.1. GEV-Canonical Loss As described in (Buja et al., 2005; Reid &amp; Williamson, 2010), for any link function  X  : [0 , 1]  X  X  , the strictly proper CPE loss c : { X  1 } X  [0 , 1]  X  R + that yields a canon-ical pair with  X  is given by where  X  : (0 , 1)  X  R + is a weight function given by Applying this result to the parametric GEV link, we get that for any  X   X  R , the following is the unique strictly proper CPE loss for which the GEV link with parameter  X  forms the canonical link: The resulting proper composite loss, which we refer to as the GEV-canonical loss , is given by This loss is not available in closed form, but is guaranteed to be convex on R  X  for all  X  , and moreover, as we describe below, can be minimized efficiently using an IRLS algo-rithm. Plots of the GEV-canonical loss for various values of  X  (obtained using numerical integration) are shown in Figure 1. As can be seen, different values of  X  yield dif-ferent forms of asymmetry; for larger values of  X  , the loss effectively penalizes mispredictions on positive examples more heavily than those on negative examples.
 For comparison, Figure 1 also shows plots of the logistic, probit and cloglog losses, as well as the GEV-log loss effec-tively used in (Wang &amp; Dey, 2010; Calabrese &amp; Osmetti, 2011), which is composed of the GEV link together with the logarithmic CPE loss: The GEV-log loss is non-convex for  X  /  X  [  X  1 , 0 . 1) , making its minimization prone to local minima. 3.2. IRLS Algorithm for GEV-Canonical Regression In the following, fix any  X   X  R . (In practice,  X  will be selected based on the training data, by cross-validation or by using a validation set.) For Euclidean instance spaces, we show how the GEV-canonical loss for any fixed  X   X  R can be minimized over linear functions using an IRLS algorithm; extension to non-linear functions or to non-Euclidean instance spaces via kernels is straightforward. Let X = R k for some k  X  Z + , and let S = (( x i ,y i )) (
R k  X { X  1 } ) n . Since the GEV-canonical loss is defined only for scores in V = R  X  , we consider learning a  X  X lipped X  linear function f S : X X  R  X  of the form for some  X   X  R k , where clip  X  : R  X  R  X  clips values out-side the interval R  X  to the closest endpoint of the interval: Thus we would like to minimize the following objective over  X  : vex in v over R  X  , the losses in the above sum are not always convex in  X  . In particular, when  X  &gt; 0 , we have that for only for  X  :  X  &gt; x  X  R  X  ; the reverse is true when  X  &lt; 0 . Therefore we would like to solve the following convex op-timization problem: where C While the objective above is not available in closed form, its gradient and Hessian in C  X  can be expressed in closed form:  X   X  b L  X  (  X  ) =  X   X  2  X  b L  X  (  X  ) = Algorithm 1 GEV-Canonical Regression (using IRLS) Input: Data S = (( x i ,y i )) n i =1  X  ( R k  X { X  1 } ) n
Initialize: X = [ x 1 , x 2 ,  X  X  X  , x n ] &gt;  X  R n  X  k repeat until convergence Output: Coefficient vector  X  ( t  X  1)  X  R k Given the gradient and the Hessian, one can use Newton X  X  method iteratively: where  X  is a suitable step size. It can be verified that if  X  old  X  C  X  , then Newton X  X  update with an appropriate step size will result in  X  new  X  C  X  . For simplicity, in our exper-iments, we fix  X  = 1 , although it may be worth exploring variable step size schedules in future work.
 Implementing Newton X  X  method directly as above requires inverting the Hessian, which turns out to be a costly opera-tion. Therefore we use a variant of the iterative reweighed least squares (IRLS) algorithm (Green, 1984) to implement Newton X  X  method (see Algorithm 1). To avoid overfitting, one can add L 2 -norm regularization by simply replacing the WLS step in the algorithm with the following: where  X  is the regularization parameter, X , W are as de-fined in Algorithm 1, and I is the k  X  k identity matrix. We conducted experiments with both synthetic and real data. In the case of synthetic data (Section 4.1), we model settings where  X  ( x ) is small for most x ; this might be the case, for example, in a web advertising application, where most ad impressions have fairly small probabilities of re-sulting in a click. Since here we know the true class prob-ability function  X  , we measure performance of the learned CPE model data (Section 4.2), for which we use 12 data sets from the UCI repository (Frank &amp; Asuncion, 2010), we do not have the true class probability function  X  , but only observed la-bels y , and so we measure performance of the learned CPE model In both sets of experiments, we compare our proposed GEV-canonical regression algorithm with the following al-gorithms as baselines (all implemented to learn a linear function): logistic regression, probit regression, C-loglog regression, under-sampled logistic regression with King &amp; Zeng X  X  correction to the learned  X  coefficients (King &amp; Zeng, 2001), weighted logistic regression with correction to the learned  X  coefficients (Buja et al., 2005), and mini-mization of the GEV-log loss used in (Wang &amp; Dey, 2010; Calabrese &amp; Osmetti, 2011). 2 In the case of weighted lo-gistic regression, the weights are the inverse empirical class probabilities, i.e. if training sample, then the losses on positives are multiplied by 1 / &amp; Dey, 2010; Calabrese &amp; Osmetti, 2011), the parame-ter  X  in the GEV-log loss is incorporated as a variable in the optimization problem, which adds an additional layer of non-convexity. In our experiments with both GEV-log and GEV-canonical losses, we select  X  by validation from range of  X  values covers a wide range of shapes for the GEV link, and in our experiments, is sufficient to accurately es-timate class probabilities for various degrees of rarity. 4.1. Experiments with Synthetic Data We generated synthetic data in X = R k (for k = 100 ) from three distributions for which  X  ( x ) is small for most x  X  X  , and consequently, p is small. Details of the distributions can be found in Appendix A; for the specific distributions generated, we had p = 0 . 0158 , p = 0 . 0312 , and p = 0 . 095 . For each of the three distributions, we generated training sets of increasing sizes, and tested the learned CPE mod-els on a test set containing 5000 examples drawn indepen-dently from the same distribution. Since we know the true class probability function in this case, we used the root mean squared error (RMSE) of the learned model relative to the true class probability function as the performance measure; for a test sample containing n points x 1 ,..., x the RMSE of a CPE model We also used the RMSE on a seperately generated vali-dation set containing 500 examples (drawn from the same distribution as training set) to select the GEV parameter  X  . The results, averaged over 10 random generations of train-ing samples for each distribution, are shown in Figure 2. As can be seen, the proposed GEV-canonical regression al-gorithm has better RMSE performance than most baseline algorithms, especially for small training sample sizes. 4.2. Experiments with Real Data We conducted experiments with 12 real-world data sets from the UCI repository (Frank &amp; Asuncion, 2010). Prop-erties of these data sets are summarized in Table 2. As can be seen, the data sets have varying degrees of rarity: 4 data sets have p  X  (0 , 0 . 1] ; the next 4 data sets have (roughly) p  X  (0 . 1 , 0 . 25] , and the remaining 4 data sets have p  X  (0 . 25 , 0 . 35] . We randomly split each data set into 70% for training and 30% for testing, and report average results over 10 such random splits. We used L 2 -norm regularization in all the algorithms, and for each train/test split, further used 30% of the training set as a validation set to select the GEV parameter  X  and regularization parameter  X  ; the latter is As noted earlier, in the case of real-world data, we do not know the true class probabilities, and therefore we can-not directly evaluate the learned CPE models relative to the true class probability function  X  . Instead, we use the squared error with respect to the observed binary labels y , more commonly referred to as the Brier score in the probability forecasting literature (Brier, 1950), as one per-formance measure; for a test sample containing n exam-b  X  : X X  [0 , 1] is defined as We also use the Brier score on the validation set to select select the GEV parameter  X  and regularization parameter  X  . We note that Wallace &amp; Dahabreh (2012) recently proposed measuring the Brier score on only the positive examples, without regard to the performance on negative examples; we do not consider this here as our goal is to estimate the full class probability function well.
 We also evaluate the performance of all the algorithms in terms of the calibration loss , which provides a more fine-grained analysis of the class probability estimates (Hern  X  andez-Orallo et al., 2012). Specifically, given n test examples as above, we assign each example to one of 10 bins based on whether its predicted class probability each of these 10 bins, we count the fraction of examples  X  X roxy X  true class probability for each example x i in that bin, denoted b  X  w.r.t. these bins is then defined as The results in terms of the Brier score and in terms of the calibration loss, both averaged over 10 random train-test splits for each data set, are shown in Table 3 and Table 4, respectively. As can be seen, GEV-canonical regression performs well overall compared to other approaches. In particular, even when it is not the best performer itself, it is rarely significantly worse than the best performer, indi-cating that its performance is generally close to that of the best approach. In comparison to GEV-log loss, the GEV-canonical loss is easier to optimize due to its convexity; in our experiments, the optimization for GEV-canonical re-gression also converged faster than that for GEV-log re-gression (see Appendix B for run-time comparisons). The problem of estimating class probabilities from data with binary labels, where one class is rare compared to the other, arises in several applications. We have consid-ered a principled approach to this problem, based on the notion of proper composite losses that have received sig-nificant interest recently in the context of class probability estimation problems, and have applied tools from the the-ory of these losses to construct a flexible parametric family of convex, proper composite losses based on the GEV dis-tribution that can be used to adapt to the degree of rarity in the data. Experiments with the resulting GEV-canonical re-gression algorithm on both synthetic and real data demon-strate improved class probability estimation performance as compared to a wide variety of baseline algorithms. Future directions include developing large-scale extensions of our method, and studying statistical convergence rates for GEV-canonical regression in comparison to other CPE algorithms for distributions where one class is rare. Acknowledgments This work was supported in part by a Yahoo Labs FREP grant to SA. SA also thanks DST for its support under a Ra-manujan Fellowship. AA thanks Google for a travel grant. HN is supported by a Google India PhD Fellowship.

