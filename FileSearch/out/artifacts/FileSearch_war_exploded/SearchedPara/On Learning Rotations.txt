 The problem of learning rotations finds application in many areas of signal processing and machine learning. It is an important problem since many problems can be reduced to that of learning rota-tions; for instance Euclidean motion in R n  X  1 is simply rotation in R n . A conformal embedding was more, the rotation group provides a universal representation for all Lie groups. This was established in [2] by showing that any Lie algebra can be expressed as a bivector algebra. Since the Lie algebra describes the structure of the associated Lie group completely, any Lie group can be represented as rotation group.
 The batch version of the problem was originally posed as the problem of estimating the attitude of satellites by Wahba in 1965 [3]. In psychometrics, it was presented as the orthogonal Procrustes problem [4]. It has been studied in various forms over the last few decades and finds application in many areas of computer vision [5, 6, 7], face recognition [8], robotics [9, 10], crystallography[11] and physics [12].
 While the batch version of the problem is well understood, the online learning of rotations from vector instances is challenging since the manifold associated with the rotation group is a curved space and it is not possible to form updates that are linear combinations of rotations [13]. The set of rotations about the origin in n -dimensional Euclidean space forms a compact Lie group, SO ( n ) , under the operation of composition. The manifold associated with the n -dimensional rotation group is the unit sphere S n  X  1 in n dimensional Euclidean space. 1.1 Related Work [13]. Online learning algorithms were recently presented for some matrix groups. In [14], an online problem of learning subspaces of low rank. However, the extension of these algorithms to learning rotations will require repeated projection and approximation [13]. Adaptive algorithms were also studied in [16] for optimization under unitary matrix constraint. The proposed methods are steepest descent methods on Riemannian manifolds. 1.2 Our Approach This paper presents an online algorithm for learning rotations that utilizes the Bregman matrix di-vergence with respect to the quantum relative entropy (also known as von Neumann divergence ) as a distance measure between two rotation matrices. The resulting algorithm has matrix-exponentiated gradient (MEG) updates [14]. The key ingredients of our approach are (a) von Neumann Divergence between rotation matrices [17], (b) squared error loss function and (c) matrix exponentiated gradient (MEG) updates.
 Any Lie group is also a smooth manifold and the updates in the proposed algorithm have an intuitive elementary Lie algebra concepts to provide intuitive interpretation of the updates. The development in the paper closely follows that of the matrix exponentiated gradient (MEG) updates in [14] for density matrix parameters. The form of the updates are similar to steepest descent methods of [16], but are derived for learning rotations from vector instances using an information-theoretic approach. The MEG updates are reduced to a quadratic form in the Lie algebra element corresponding to the gradient of loss function on the rotation group.
 The paper is organized as follows. The problem is formulated in Section 2. Section 3 presents mathematical preliminaries in differential geometry and Bregman matrix divergence. The matrix exponentiated gradient updates are developed in Section 4. The MEG updates are simplified in Section 5. Experimental results are discussed in Section 6. Let x t be a stream of instances of n -dimensional unit vectors. Let R  X  be an unknown n  X  n rotation estimate of the rotation needs to be updated based on the loss incurred at every instance and the objective is to develop an algorithm for learning R  X  that has a bounded regret.
 We seek adaptive updates that solve the following optimization problem at each step, is a matrix divergence that measures the discrepancy between matrices. This is a typical problem formulation in online learning where the objective comprises a loss function and a divergence term. The parameter  X  balances the trade-off between the two conflicting goals at each update: incurring small loss on the new data versus confidence in the estimate from the previously observed data. Minimizing the weighted objective therefore results in smooth updates as well as minimizes the loss function.
 In this paper, the updates are smoothed using the von Neumann divergence which is defined for matrices as matrices such that R T R = I , RR T = I and det( R ) = 1 . This section reviews some basic definitions and concepts in linear algebra and differential geometry that are utilized for the development of the updates in the next section. 3.1 Matrix Calculus matrix R  X  R n  X  n is defined to be the matrix [18], Some of the matrix derivatives that are used later in the paper are following: for a constant matrix A related concept in differential geometry is that of the space of vectors tangent to a group at the a convenient way of describing the infinitesimal structure of a topological group about the identity element and completely determines the associated group. The utility of the Lie algebra is due to the fact that it is a vector space and thus it is much easier to work with it than with the linear group. symmetric matrix (i.e. A T =  X  A ). Furthermore, for any matrix A in the Lie algebra of SO ( n ) , exp (  X  A ) is a one-parameter subgroup of the rotation group, parametrized by  X   X  R [19]. The matrix exponential and logarithm play an important role in relating a matrix Lie group G and the associated Lie algebra g . The exponential of a matrix R  X  R n  X  n is given by the following series, Given an element A  X  g , the matrix exponential exp ( A ) is the corresponding element in the group. Lie group G into the Lie algebra g . The matrix logarithm is a well-defined map since the exponential map is a local diffeomorphism between a neighborhood of the zero matrix and a neighborhood of the identity matrix [19, 20]. 3.2 Riemannian Gradient matrix R and translated to the identity (to get a Lie algebra element) is given as [16] where  X  R L t is the matrix derivative of the cost function in the Euclidean space defined in (3) at matrix R . 3.3 Von Neumann Divergence In any online learning problem, the choice of divergence between the parameters dictates the result-ing updates. This paper utilizes the von Neumann divergence which is a special case of the Bregman divergence and measures discrepancy between two matrices.
 Let F be convex differentiable function defined on a subset of R n  X  n with the gradient f ( R ) =  X 
R F ( R ) . The Bregman divergence between two matrices R 1 and R 2 is defined as The gradient of Bregman divergence with respect to R 1 is given as, Choosing the function F in the definition of Bregman divergence to be the von Neumann entropy, given as F ( R ) = tr ( R log R  X  R )) , obtain the von Neumann divergence [14, 17]: Finally, the gradient of the von Neumann entropy was shown to be f ( R ) =  X  R F ( R ) = log R in [14]. Consequently, the gradient of the von Neumann divergence can be expressed as The problem of online learning of rotations can be expressed as the optimization problem prediction of y t . The proposed adaptive updates are matrix exponentiated gradient (MEG) updates given as rotation matrix R and skew (  X  ) is the skew-symmetrization operator on the matrices, skew ( A ) = A  X  A T . The updates seem intuitive, given the following elementary facts about the Lie algebraic vector on the unit sphere, (b) a skew-symmetric matrix is an element of Lie algebra [19, 20], (c) the matrix logarithm maps a rotation matrix to the corresponding Lie algebra element, (d) composition of two elements of Lie algebra yields another Lie algebra element and (e) the matrix exponential maps a Lie algebra element to corresponding rotation matrix.
 The loss function is defined to be the squared error loss function and therefore the gradient of the 4.1 Updates Motivated by von-Neumann Divergence The optimization problem in (10) is solved using the method of Lagrange multipliers. First observe that the constraints R T R = I and RR T = I are redundant since one implies the other. Introducing the Lagrangian multiplier matrix  X  for the orthonormality constraint and Lagrangian multiplier  X  for the unity determinant constraint, the objective function can be written as Taking the gradient on both sides of equation with respect to the matrix R , get using the matrix derivatives from Section 3.1 and the Riemannian gradient for the loss function from Given that f is a bijective map, write Since the objective is convex, it is sufficient to produce a choice of Lagrange multipliers that en-forces the rotation constraint. Choosing  X  = det( R )  X  1 and  X  =  X  (1 / 2) R  X  1 T R  X  1 yields the following implicit update As noted by Tsuda et. al. in [14], the implicit updates of the form above are usually not solvable in an explicit update The next result ensures the closure property for the matrix exponentiated gradient updates in the equation above. In other words, the estimates for the rotation matrix do not steer away from the Proof. Using the properties of matrix logarithm and matrix exponential, express (18) as trace zero. Then  X  R since determinant of exponential of a matrix is equal to the exponential of the trace of the matrix. And since S is a trace zero matrix, det(  X  R t +1 ) = 1 . 4.2 Differential Geometrical Interpretation The resulting updates in (18) have nice interpretation in terms of the differential geometry of the direction at the current estimate of the rotation matrix. The Riemannian gradient is computed as  X  tion matrix which is the multiplicative update to the estimate of the rotation matrix at the previous instance. The matrix exponentiated gradient updates ensure that the estimates for the rotation matrix stay on the manifold associated with the rotation group at each iteration. However, with the matrix ex-ponentiation at each step, the updates are computationally intensive and in fact the computational complexity of the updates is comparable to other approaches that would require repeated approxima-tion and projection on to the manifold. This section discusses a fundamental complexity reduction be written as eigenvalues) [22], which allows the following simplification.
 Lemma 2. The matrix exponentiated gradient updates in eqn. (12) are equivalent to the following updates, where  X  = 2  X  ues  X  j X  .
 ity reduction follows easily from a generalization of the Rodrigues X  formula for computing matrix exponentials for skew-symmetric matrix. The proof is not presented here due to space constraints but the interested reader is referred to [23, 24]. Owing to the result above the matrix exponential reduces to a simple quadratic form involving an element from the Lie algebra of the rotation group. The pseudocode is given in Algorithm 1.
 Choose  X  Initialize R 1 = I for t = 1 , 2 ,... do end This section presents experimental results with the proposed algorithm for online learning of rota-tions. The performance of the algorithm is evaluated in terms of the Frobenius norm of the difference of the true rotation matrix and the estimate. Figure 1 shows the error plot with respect to time. The unknown rotation is a 12  X  12 dimensional matrix and changes randomly every 200 instances. The trajectories are averaged over 1000 random simulations. It is clear from the plot that the estimation error decays rapidly to zero and estimates of the rotation matrices are exact. Figure 1: Online learning of rotations: Estimate of unknown rotation is updated every time new instance of rotation is observed. The true rotation matrix is randomly changing at regular interval (N=200). The error in Frobenius norm is plotted against the instance index.
 where  X  determines the signal to noise ratio. The performance of the algorithm is studied with in
R 20 . The Frobenius norm error decays quickly to a noise floor determined by the SNR as well as clear immediately how to pick the optimal step size but a classic step size adaptation rule or Armijo rule may be followed [25, 16].
 The tracking performance of the online algorithm is compared with the batch version. In Figure 3, the unknown rotation R  X   X  SO (30) changes slightly after every 30 instances. The smoothly changing rotation is induced by composing R  X  matrix with a matrix R  X  every thirty iterations. The matrix R  X  is composed of 3  X  3 block-diagonal matrices, each corresponding to rotation about the X -axis in 3D space by  X / 360 radians. The batch version stores the last 30 instances in an 30  X  30 matrix X and corresponding rotated vectors in matrix Y . The estimate of the unknown rotation is given as YX  X  1 . The batch version achieves zero error only at time instances when all the data in X , Y correspond to the same rotation whereas the online version consistently achieves a low error and tracks the changing rotation.
 It is clear from the simulations that the Frobenius norm decreases at each iteration. It is easy to show this global stability of the updates proposed here in noise-free scenario [24]. The proposed algorithm was also applied to learning and tracking the rotations of 3D objects. Videos showing experimental results with the 3D Stanford bunny [26] are posted online at [27]. motivated using the von Neumann divergence and squared error loss function and the updates were Figure 3: Comparing the performance of tracking rotations for the batch version versus the online algorithm. The rotation matrix changes smoothly every M = 30 instances. developed in the Lie algebra of the rotation group. The resulting matrix exponentiated gradient algorithm was studied under various scenarios. Some of the future directions include identifying the proposed updates.
 Acknowledgements: The author would like to thank W. A. Sethares, M. R. Gupta and A. B. Frigyik for helpful discussions and feedback on early drafts of the paper.
 [1] Rich Wareham, Jonathan Cameron, and Joan Lasenby,  X  X pplications of conformal geometric [2] C. Doran, D. Hestenes, F. Sommen, and N. Van Acker,  X  X ie groups as spin groups, X  Journal [3] Grace Wahba,  X  X roblem 65-1, a least squares estimate of satellite attitude, X  SIAM Review , vol. [4] P. Schonemann,  X  X  generalized solution of the orthogonal Procrustes problem, X  Psychome-[5] P. Besl and N. McKay,  X  X  method for registration of 3D shapes, X  . IEEE Trans. on Pattern [6] Hannes Edvardson and  X  Orjan Smedby,  X  X ompact and efficient 3D shape description through [8] R. Sala Llonch, E. Kokiopoulou, I. Tosic, and P. Frossard,  X 3D face recognition with sparse [9] Ameesh Makadia and Kostas Daniilidis,  X  X otation recovery from spherical images without [10] Raman Arora and Harish Parthasarathy,  X  X avigation using a spherical camera, X  in Interna-[11] Philip R. Evans,  X  X otations and rotation matrices, X  Acta Cryst. , vol. D57, pp. 1355 X 1359, [12] Richard L. Liboff, Introductory Quantum Mechanics , Addison-Wesley, 2002. [13] Adam Smith and Manfred Warmuth,  X  X earning rotations, X  in Conference on Learning Theory [14] Koji Tsuda, Gunnar Ratsch, and Manfred K Warmuth,  X  X atrix exponentiated gradient updates [15] Manfred K Warmuth,  X  X innowing subspaces, X  in Proc. 24th Int. Conf. on Machine Learning , [16] T.E. Abrudan, J. Eriksson, and V. Koivunen,  X  X teepest descent algorithms for optimization [17] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information , Cam-[18] Kaare Brandt Petersen and Michael Syskind Pedersen,  X  X he matrix cookbook, X  http:// [19] Michael Artin, Algebra , Prentice Hall, 1991. [20] John A. Thorpe, Elementary topics in Differential Geometry , Springer-Verlag, 1994. [21] J. Kivinen andM. K.Warmuth,  X  X xponentiated gradient versus gradient descent for linear pre-[22] L. J. Butler, Applications of Matrix Theory to Approximation Theory , MS Thesis, Texas Tech [23] J. Gallier and D. Xu,  X  X omputing exponentials of skew-symmetric matrices and logarithms of [24] Raman Arora, Group theoretical methods in signal processing: learning similarities, trans-[25] E. Polak, Optimization: Algorithms and Consistent Approximations , Springer-Verlag, 1997. [26] Stanford University Computer Graphics Laboratory,  X  X he Stanford 3D scanning repository, X  [27] Raman Arora,  X  X racking rotations of 3D Stanford bunny, X  http://www.cae.wisc.edu/
