 Wei Chen weic@microsoft.com Microsoft Research Asia, Beijing, China Yajun Wang yajunw@microsoft.com Microsoft Research Asia, Beijing, China Yang Yuan yangyuan@cs.cornell.edu Computer Science Department, Cornell University, Ithaca, NY, USA Multi-armed bandit (MAB) is a problem extensively studied in statistics and machine learning. The classi-cal version of the problem is formulated as a system of m arms (or machines), each having an unknown distri-bution of the reward with an unknown mean. The task is to repeatedly play these arms in multiple rounds so that the total expected reward is as close to the re-ward of the optimal arm as possible. An MAB algo-rithm needs to decide which arm to play in the next round given the outcomes of the arms played in the previous rounds. The metric for measuring the effec-tiveness of an MAB algorithm is its regret , which is the difference in the total expected reward between always playing the optimal arm and playing arms according to the algorithm. The MAB problem and its solutions reflect the fundamental tradeoff between exploration and exploitation: whether one should try some arms that have not been played much (exploration) or one should stick to the arms that provide good reward so far (exploitation). Existing results show that one can achieve a regret of O (log n ) when playing arms in n rounds, and this is asymptotically the best.
 In many real-world applications, the setting is not the simple MAB one, but has a combinatorial nature among multiple arms and possibly non-linear reward functions. For example, consider the following online advertising scenario. A web site contains a set of web pages and has a set of users visiting the web site. An advertiser wants to place an advertisement on a set of selected web pages on the site, and due to his budget constraint, he can select at most k web pages. Each user visits a certain set of pages, and on each visited page has a certain click-through probability of clicking the advertisement on the page, but the advertiser does not know these probabilities. The advertiser wants to repeatedly select sets of k web pages, observe the click-through data collected to learn the click-through prob-abilities, and maximize the number of users clicking his advertisement. Another example is viral marketing in online social networks, where a marketer repeatedly selects seed nodes in a social network, observes the cascading behavior of the viral information to learn influence probabilities between individuals in the so-cial network, with the goal of maximizing the overall effectiveness of all viral cascades.
 In the above examples, page-user pairs or pairs of nodes in the social network can be viewed as arms, but they are not played one by one. Instead, these arms form certain combinatorial structures (e.g. bi-partite graphs in the online advertising scenario and directed graphs in the viral marketing scenario), and in each round, a set of arms (called a super arm ) are played together. Moreover, the reward structure is not a simple linear function of the outcomes of all played arms but takes a more complicated form. For exam-ple, in the online advertising scenario, for all page-user pairs with the same user, the collective reward of these arms is either 1 if the user clicks the advertisement on at least one of the pages and 0 if the user does not click the advertisement on any page.
 It is possible to treat every super arm as an arm and simply apply the classical MAB framework to solve the above combinatorial problems. However, such naive treatment has two issues. First, the number of su-per arms may be exponential to the problem instance size due to combinatorial explosion, and thus classi-cal MAB algorithms may need exponential number of steps just to go through all the super arms. Second, after one super arm is played, in many cases, we can observe some information regarding the outcomes of the underlying arms, which may be shared by other super arms. However, this information is discarded in the classical MAB framework, making it less effective. In this paper, we define a general framework for the combinatorial multi-armed bandit (CMAB) problem to address the above issues and cover a large class of com-binatorial online learning problems (Section 2). In the CMAB framework, a super arm is a set of underlying arms, whose outcomes follow unknown distributions. In each round one super arm is played and the out-comes of all arms in the super arm (and possibly some other triggered arms) are revealed. A CMAB algo-rithm needs these information from the past rounds to decide the super arm to play in the next round. The framework allows an arbitrary combination of arms into super arms. The reward function only needs to satisfy two mild assumptions, and thus covering a large class of nonlinear reward functions. We do not assume the direct knowledge on how super arms are formed from underlying arms or how the reward is computed. Instead, we assume the availability of an offline computation oracle that takes such knowledge as well as the expectations of outcomes of all arms as input and computes the optimal super arm with re-spect to the input. Since many combinatorial problems are computationally hard, we further allow approxima-tion oracles with failure probabilities. In particular, we relax the oracle to be an (  X , X  )-approximation oracle for some  X , X   X  1, that is, with success probability  X  , the oracle could output a super arm whose expected reward is at least  X  fraction of the optimal expected reward. As a result, our regret metric is not comparing against the expected reward of playing the optimal su-per arm each time, but against the  X  X  fraction of the optimal expected reward, since the offline oracle can only guarantee this fraction in expectation. We refer to this as the (  X , X  ) -approximation regret . For the general framework, we provide the CUCB (combinatorial upper confidence bound) algorithm (Section 3), an extension to the UCB1 algorithm for the classical MAB problem (Auer et al., 2002a). We prove that the regret of CUCB is bounded by O (log n ). Our regret analysis is tight in that when applying it to the classical MAB problem we obtain a regret bound that matches the bound of the classical MAB up to a constant factor. Our tight analysis further allows us to provide a distribution-independent regret bound that works for arbitrary distributions of underlying arms, for a large class of CMAB instances.
 We then apply our general framework and provide solutions to two new bandit applications, the proba-bilistic maximum coverage problem for advertisement placement and social influence maximization for vi-ral marketing (Section 4). The offline version of both problems are NP-hard, with constant approximation algorithms available. Both problems have nonlinear reward structures that cannot be handled by any exist-ing work. The social influence maximization problem provides an interesting instance in which playing one super arm not only reveals the outcomes of the under-lying arms it contains, but may stochastically trigger more arms to reveal their outcomes, and the reward depends on the outcomes of all revealed arms.
 We also apply our result to combinatorial bandits with linear rewards, recently studied in (Gai et al., 2012) (Section 4.3). We show that we significantly improve their regret bound, even though we are covering a much larger class of combinatorial bandit instances. In the supplementary material, besides providing the full proof of our main theorems, we further provide (a) an  X  t -greedy algorithm for CMAB, and (b) an im-proved regret analysis for CMAB where arms are clus-tered and played together, which can be applied to the two new applications studied in this paper.
 In summary, our contributions include: (a) defin-ing a general CMAB framework that encompasses a large class of nonlinear reward functions, (b) provid-ing CUCB algorithm with a tight regret analysis as a general solution to this CMAB framework, and (c) demonstrating that our general framework can be ef-fectively applied to a number of practical combina-torial bandit problems, including ones with nonlinear rewards. Moreover, our framework provides a clean separation of the online learning task and the offline computation task: the oracle takes care of the offline computation task, which uses the domain knowledge of the problem instance, while our CMAB algorithm takes care of the online learning task, and is oblivious to the domain knowledge of the problem instance. Related work. Multi-armed bandit problem has been well studied in the literature, in particular in statistics and reinforcement learning (cf. (Berry &amp; Fristedt, 1985; Sutton &amp; Barto, 1998)). Our work follows the line of research on stochastic MAB prob-lems, which is initiated by Lai and Robbins (Lai &amp; Robbins, 1985), who show that under certain condi-tions on reward distributions, one can achieve a tight asymptotic regret of  X (log n ), where n is the number of rounds played. Later, Auer et al. demonstrate that O (log n ) regret can be achieved uniformly over time rather than only asymptotically (Auer et al., 2002a). They propose several MAB algorithms, including the UCB1 algorithm, which has been widely followed and adapted in MAB research.
 For combinatorial multi-armed bandits, a few specific instances of the problem has been studied in the litera-ture. A number of studies consider simultaneous plays of k arms among m arms (e.g. (Anantharam et al., 1987a; Caro &amp; Gallien, 2007; Liu et al., 2011)). Other instances include the matching bandit (Gai et al., 2010) and the online shortest path problem (Liu &amp; Zhao, 2012).
 The work closest to ours is a recent work by Gai et al. (Gai et al., 2012), which also considers a combi-natorial bandit framework with an approximation or-acle. However, our work differs from theirs in sev-eral important aspects. Most importantly, their work only considers linear rewards while our CMAB frame-work includes a much larger class of linear and non-linear rewards. Secondly, our regret analysis is much tighter, and as the result we significantly improve their regret bound when applying our result to the linear reward case, and we are able to derive a distribution-independent regret bound while their results cannot lead to distribution-independent bounds. Moreover, we allow the approximation oracle to have a failure probability (i.e.,  X  &lt; 1), which they do not consider. In terms of types of feedbacks in combinatorial ban-dits (Audibert et al., 2011), our work belongs to the semi-bandit type, in which the player observes only the outcomes of played arms in one round of play. Other types include (a) full information , in which the player observes the outcomes of all arms, and (b) bandit , in which the player only observes the final reward but no outcome of any individual arm. More complicated feedback dependences are also considered in (Mannor &amp; Shamir, 2011).
 A different line of research considers adversarial multi-armed bandit , initiated by the work in (Auer et al., 2002b), in which an adversary controls the arms and tries to defeat the learning process. In the context of adversarial bandits, several studies also consider com-binatorial bandits (Cesa-Bianchi &amp; Lugosi, 2009; Au-dibert et al., 2011; Bubeck et al., 2012). For linear re-wards, Kakade et al. (Kakade et al., 2009) have shown how to convert an approximation oracle into an on-line algorithm with sublinear regret both in the full information setting and the bandit setting. For non-linear rewards, various online submodular optimiza-tion problems with bandit feedback are studied in the adversarial setting (Streeter &amp; Golovin, 2008; Radlin-ski et al., 2008; Streeter et al., 2009; Hazan &amp; Kale, 2009). Notice that our framework deals with stochas-tic instances and we can handle reward functions more general than the submodular ones. A CMAB problem consists of m arms associated with a set of random variables X i,t for 1  X  i  X  m and t  X  1, with bounded support on [0 , 1]. Variable X i,t indicates the random outcome of the i -th arm in its t -th trial. The set of random variables { X i,t | t  X  1 } associ-ated with arm i are independent and identically dis-tributed according to some unknown distribution with unknown expectation  X  i . Let  X  = (  X  1 , X  2 ,..., X  m ) be the vector of expectations of all arms. Random vari-ables of different arms may be dependent.
 The CMAB problem contains a constraint S  X  2 [ m ] , where 2 [ m ] is the set of all possible subsets of arms. We refer to every set of arms S  X  S as a super arm of the CMAB problem. In each round, one super arm S  X  S is played and the outcomes of arms in S are revealed. More precisely, for each arm i  X  [ m ], let T i,t denote the number of times the outcome of arm i is revealed after the first t rounds in which t super arms are played. If S  X  X  is the super arm played in round t , the outcomes of random variables X i,T i,t for all i  X  S are revealed. For some problem instances (e.g. social influence maximization in Section 4.2), the outcomes of other arms may also be revealed depending on the outcomes of arms in S .
 Let R t ( S ) be a non-negative random variable denot-ing the reward of round t when super arm S is played. The reward depends on the actual problem instance definition, the super arm S played, and the outcomes of the revealed arms in round t . The reward R t ( S ) might be as simple as a summation of the outcomes of the arms in S : R t ( S ) = P i  X  S X i,T work allows more sophisticated nonlinear rewards, as explained below.
 In this paper, we consider CMAB problems in which the expected reward of playing any super arm S in of arms S and the expectation vector  X  of all arms. For the linear reward case as given above, this is true because linear addition is commutative with the ex-pectation operator. For non-linear reward functions not commutative with the expectation operator, it is still true if we know the type of distributions and only the expectations of arm outcomes are unknown, and outcomes of different arms are independent. For ex-ample, the distribution of X i,t  X  X  are known to be 0-1 Bernoulli random variables with unknown mean  X  i . Henceforth, we denote the expected reward of playing S as r  X  ( S ) = E [ R t ( S )]. To carry out our analysis, we make the following two mild assumptions on the expected reward r  X  ( S ):  X  Monotonicity . The expected reward of playing  X  Bounded smoothness. There exists a strictly Both assumptions are natural. In particular, they hold true for all the applications we considered.
 A CMAB algorithm A is one that selects the super arm of round t to play based on the outcomes of revealed arms of previous rounds, without knowing the expec-tation vector  X  . Let S A t be the super arm selected by A in round t . Note that S A t is a random super arm that depends on the outcomes of arms in previous rounds and potential randomness in the algorithm A itself. The objective of algorithm A is to maximize the ex-pected reward of all rounds up to a round n , that is, E denotes taking expectation among all random events generating the super arms S A t  X  X  and generating re-wards R t ( S A t ) X  X , and E S denotes taking expectation only among all random events generating the super arms S A t  X  X .
 We do not assume that the learning algorithm has the direct knowledge about the problem instance, e.g. how super arms are formed from the underlying arms and how reward is defined. Instead, the algorithm has ac-cess to a computation oracle that takes the expectation vector  X  as the input, and together with the knowledge of the problem instance, computes the optimal or near-optimal super arm S . Let opt  X  = max S  X  X  r  X  ( S ) and S  X  = argmax S  X  X  r  X  ( S ). We consider the case that ex-act computation of S  X   X  may be computationally hard, and the algorithm may be randomized with a small failure probability. Thus, we resolve to the following (  X , X  ) -approximation oracle :  X  (  X , X  ) -Approximation oracle. There is an A lot of computationally hard problems do admit effi-cient approximation oracles (Vazirani , 2004). With an (  X , X  )-approximation oracle, it is no longer fair to com-pare the performance of a CMAB algorithm against the optimal reward opt  X  as the regret of the algo-rithm. Instead, we compare against the  X   X   X  fraction of the optimal reward, because only a  X  fraction of oracle computations are successful, and when success-ful the reward is only  X  -approximate of the optimal value. Formally, we define (  X , X  ) -approximation regret of a CMAB algorithm A after n rounds of play using an (  X , X  )-approximation oracle under the expectation vector  X  as
Reg A  X  , X , X  ( n ) = n  X   X   X   X   X  opt  X   X  E S 1: For each arm i , maintain: (1) variable T i as the 2: For each arm i , play an arbitrary super arm S  X  X  3: t  X  m . 4: while true do 5: t  X  t + 1. 6: For each arm i , set  X   X  i =  X   X  i + q 3 ln t 2 T 7: S = Oracle(  X   X  1 ,  X   X  2 ,...,  X   X  m ). 8: Play S and update all T i  X  X  and  X   X  i  X  X . 9: end while Algorithm 1: CUCB with computation oracle Note that the classical MAB problem is a special case of our general CMAB problem, in which (a) the con-straint S = [ m ] so that each super arm is just a simple arm; (b) the reward of a super arm S = i in its t  X  X  trial is its outcome X i,t ; (c) the monotonicity and bounded smoothness hold trivially with function f (  X  ) being the identity function; and (d) the (  X , X  )-approximation or-acle is simply the argmax function among all expecta-tion vectors, with  X  =  X  = 1. We present our CUCB algorithm in Algorithm 1. After m initialization rounds, based on previous information, we maintain an empirical mean  X   X  i for each arm i . More precisely, if arm i has been played s times by the end of round n , then the value of  X   X  i at the end of round n is ( P s j =1 X i,j ) /s . The actual expectation vector  X   X  given to the oracle contains an adjustment term for each  X   X  i , which depends on the round number t and the number of times arm i has been played (stored in variable T i ). Then we simply play the super arm returned by the oracle and update variables T i  X  X  and  X   X  i  X  X  accordingly. Note that in our model all arms have bounded support on [0 , 1], but with the adjustment  X   X  i may exceed 1. If such  X   X  i is illegal to the oracle, we simply replace it with 1. Since replacing any value larger than 1 with 1 does not violate monotonicity and bounded smoothness of the reward function, our analysis below is not affected by this artifact and we directly use the original  X   X  i . A super arm S is bad if r  X  ( S ) &lt;  X   X  opt  X  . We define S
B = { S | r  X  ( S ) &lt;  X   X  opt  X  } as the set of bad super arms. For a given underlying arm i  X  [ m ], we define  X  i min =  X   X  opt  X   X  max { r  X  ( S ) | S  X  X  B ,i  X  S } , (1)  X  i max =  X   X  opt  X   X  min { r  X  ( S ) | S  X  X  B ,i  X  S } . (2) Furthermore, define  X  max = max i  X  [ m ]  X  i max and  X  vides the regret bound of the CUCB algorithm.
 Theorem 1. The (  X , X  ) -approximation regret of the CUCB algorithm in n rounds using an (  X , X  ) -approximation oracle is at most + where f (  X  ) is the bounded smoothness function. Due to the space constraint, we prove the following regret bound simplified from Eq.(3). Proof of regret bound in Eq. (4) . We use I { X } to denote the indicator function, and I {E} = 1 if event E is true, and 0 if E is false.
 For variable T i , let T i,t be the value of T i at the end of round t , that is, T i,t is the number of times arm i is played in the first t rounds. For variable  X   X  i , let  X   X  be the value of  X   X  i after arm i is played s times, that  X   X  let  X   X  i,t be the value of  X   X  i at the end of round t . Let  X   X  t = (  X   X  1 ,t ,...,  X   X  m,t ) be the random vector fed to the oracle as the input in line 7 of Algorithm 1 at round t . In the t -th round, let F t be the event that the oracle fails to produce an  X  -approximate answer with respect to the input vector  X   X  t = (  X   X  1 ,t ,...,  X   X  m,t Pr[ F t ] = E [ I { F t } ]  X  1  X   X  .
 We maintain counter N i for each arm i after the m initialization rounds. Let N i,t be the value of N i after the t -th round and N i,m = 1. Note that P i N i,m = m . Counters { N i } m i =1 are updated as follows. For a round t &gt; m , let S t be the super arm selected in round t by the oracle (line 7 of Algorithm 1). Round t is bad if the oracle selects a bad super arm S t  X  S
B . If round t is bad, let i = argmin j  X  S t N j,t  X  1 . We increment N i by one, i.e., N i,t = N i,t  X  1 + 1. That is, we find the arm i with the smallest counter in S t and increment its counter. If i is not unique, we pick an arbitrary arm with the smallest counter in S t . On the other hand, if S t /  X  X  B , no counter will be incremented. By definition N i,t  X  T i,t . Notice that in every bad round, exactly one counter in { N i } m i =1 is incremented, so the total number of bad rounds in the first n rounds is less than or equal to P i N i,n .
 S
B is selected and counter N i of some arm i  X  S t is updated. We have
X  X   X  =  X   X  Eq.(5) holds by our rule of updat-ing the counters. We first claim that In fact, for any i  X  [ m ], =  X  where the last inequality is due to the Chernoff-Hoeffding bound. Define  X  i,t = q 3 ln t 2 T variable since T i,t  X  1 is a random variable), and event E bound on Eq.(6), Pr[  X  E t ]  X  2  X  m  X  t  X  2 . According to fine random variable  X  t = max {  X  i,t | i  X  S t } . Then the adjusted expectation vector at round t . Then, If { E t ,  X  F t ,S t  X  X  B ,  X  i  X  S t ,T i,t  X  1 &gt; ` t t , we have the following important derivation: The first inequality above is due to the strict mono-tonicity of f (  X  ) and Eq.(8); the second is due to the bounded smoothness property and Eq.(7); the third is because  X  F t implies that S t is an  X  approximation w.r.t  X   X  t ; the fourth is by the definition of opt  X   X  the last inequality is due to the monotonicity of r  X  ( S ) and Eq.(9). So we have fore, Eq. (10) contradicts the definition of  X  min the fact that S t  X  X  B . In other words,
Pr [ { E t ,  X  F t ,S t  X  X  B ,  X  i  X  S t ,T i,t  X  1 &gt; `
Pr [ { X  F t ,S t  X  X  B ,  X  i  X  S t ,T i,t  X  1 &gt; ` t } ]  X  Pr[  X  E t ]  X  2  X  m  X  t  X  2 .
 The claim thus holds. We have, E  X  Notice that each time we hit a bad super arm at time t , we incur a regret at most  X  max  X   X   X  opt  X   X  r  X  ( S Then we obtain the regret bound of Eq.(4) as follows.  X  n X  X   X  opt  X   X  n X   X  opt  X   X  E  X   X  We now briefly discuss the idea to prove Theorem 1. In the proof of Eq.(4), we essentially show that if all arms are sufficiently sampled with respect to  X  min , the probability that we hit a bad super arm is small. On the other hand, in a bad round, if the underlying arms are not sufficiently sampled with respect to  X  min , we incur a regret of  X  max . Notice that there is a discrep-ancy in the analysis, i.e., the sufficiency of sampling is defined on  X  min while the regret is counted as  X  max . Theorem 1 is based on a more refined analysis that de-fines the sufficiency of the sampling of arm i separately for each bad super arm containing i , which avoids the over charge above.
 Comparing to classical MAB. The classical MAB is a special instance of our CMAB framework in which each super arm is a simple arm, function f ( x ) = x , and  X  =  X  = 1. Notice that  X  i max =  X  i min . Thus, by Theorem 1, the regret bound of the classical MAB is where  X  i = max j  X  [ m ]  X  j  X   X  i . Comparing with the regret bound in Theorem 1 of (Auer et al., 2002a), The improvement is due to a tighter analysis, and is the reason that we obtained improved regret bound over (Gai et al., 2012) for the linear reward CMAB. Our tight analysis implies a distribution-independent regret for arbitrary distributions with support in [0 , 1] on all arms, for a large class of problem instances with a polynomial bounded smoothness function f ( x ) =  X x  X  for  X  &gt; 0 and 0 &lt;  X   X  1, as shown below. Theorem 2. Consider a CMAB problem with an (  X , X  ) -approximation oracle. If the bounded smooth-ness function f ( x ) =  X   X  x  X  for some  X  &gt; 0 and  X   X  (0 , 1] , the regret of CUCB is at most: 2  X  2  X   X  Note that when  X  = 1, which covers all applica-tions discussed in Section 4, in the simple arm set-ting, we obtain a distribution-independent bound of O (  X  rithm (Audibert &amp; Bubeck, 2009) (up to a logarithmic factor). In the linear combinatorial bandit setting, i.e., semi-bandit with L  X  assumption in (Audibert et al., 2011), our regret is O ( p m 3 n log n ), which is a factor  X  m off the optimal bound in the adversarial setting. In this section, we describe three applications that fit our CMAB framework. Notice that, the probabilistic maximum coverage bandit and social influence maxi-mization bandit are instances of the online submodular maximization problem, which can be addressed in the adversarial setting by (Streeter &amp; Golovin, 2008). 4.1. Probabilistic maximum coverage bandit The online advertisement placement application dis-cussed in the introduction can be modeled by the bandit version of the probabilistic maximum coverage (PMC) problem. PMC has as input a weighted bipar-tite graph G = ( L,R,E ) where each edge ( u,v ) has a probability p ( u,v ), and it needs to find a set S  X  L of size k that maximizes the expected number of acti-vated nodes in R , where a node v  X  R can be activated by a node u  X  S with an independent probability of p ( u,v ). In the advertisement placement scenario, L is the set of web pages, R is the set of users, and p ( u,v ) is the probability that user v clicks the advertisement on page u . PMC problem is NP-hard, since when all edge probabilities are 1, it becomes the NP-hard Max-imum Coverage problem. Using submodular set func-tion maximization technique (Nemhauser et al., 1978), it can be easily shown that there exists a determinis-tic (1  X  1 /e ) approximation algorithm for the PMC problem, which means that we have a (1  X  1 /e, 1)-approximation oracle for PMC.
 The PMC bandit problem is that edge probabilities are unknown, and one repeatedly selects k targets in L in multiple rounds, observes all edge activations and ad-justs target selection accordingly in order to maximize the total number of activated nodes over all rounds. We can formulate this problem as an instance in the CMAB framework. Each edge ( u,v )  X  E represents an arm, and each play of the arm is a 0-1 Bernoulli random variable with parameter p u,v . A super arm is the set of edges E S adjacent to a set S  X  L of size k . The reward of E S is the number of activated nodes in R , which is the number of nodes in R that are incident to at least one edge in E S with outcome 1. Note that this reward is not linear to the outcomes of arms. The monotonicity property is straightforward. The bounded smoothness function is f ( x ) = | E | X  x , i.e., increasing all probabilities of all arms in a super arm by x can increase the expected number of acti-vated nodes in V by at most | E |  X  x . Since f (  X  ) is a linear function, the integral in Eq.(3) has a closed form. In particular, by Theorem 1, we know that the (1  X  1 /e, 1)-approximation regret of our CUCB algo-rithm on PMC bandit is bounded by Notice that all edges incident to a node u  X  L are always played together. In this case, for any two edges i,j  X  E that are incident to the same node u  X  L , we have  X  i min =  X  j min , and we define it to be  X  min . The coefficient of ln n above could be written as P u . We call those arms that are always played together as clustered arms . In the supplementary material, we show how to exploit the arm clustering property to remove the d u above and obtain the following better bound: 4.2. Social influence maximization bandit In social influence maximization (Kempe et al., 2003), we are given a directed graph G = ( V,E ), where every edge ( u,v ) is associated with an unknown propagation probability p u,v . Initially, a seed set S  X  V are se-lected and activated. In each iteration of the diffusion process, each node u activated in the previous itera-tion has one chance of activating its inactive outgoing neighbor v with probability p u,v . The reward of S after the diffusion process is the total number of activated nodes in the end. Influence maximization is to find a seed set S of at most k nodes that maximize the ex-pected reward. Kempe et al. (Kempe et al., 2003) show that the problem is NP-hard and provide an algorithm with approximation ratio 1  X  1 /e  X   X  with success prob-ability (1  X  1 / | E | ) for any  X  &gt; 0. This means that we have a (1  X  1 /e  X   X , 1  X  1 / | E | )-approximation oracle. In the CMAB framework, we do not know the activa-tion probabilities of edges and want to learn them dur-ing repeated seed selections while maximizing overall reward. Similar to PMC, we can treat each edge as an arm, and a super arm is a set of outgoing edges from at most k nodes. Different from PMC, when a super arm S is played, not only arms in S reveal their outcomes, but other arms (edges) may also reveal their outcomes in the diffusion process, and the reward depends on the outcomes of all these arms. As a result, our bounded smoothness function is f ( x ) = | V | X | E | X  x . According to Theorem 1, the (1  X  1 /e  X   X , 1  X  1 / | E | )-approximation regret of the CUCB algorithm on influence maximiza-tion is bounded by: Similar to the PMC problem, we could exploit the arm clustering property and improve the regret bound. 4.3. Combinatorial bandits with linear rewards Gai et al. (Gai et al., 2012) studied the Learning with Linear Reward policy (LLR). Their formulation is close to ours except that their reward function must be lin-ear. In their setting, there are m underlying arms. Each super arm consists of a set of underlying arms S together with a set of coefficients { w i,S | i  X  S } . The reward of playing super arm S is P i  X  S w i,S  X  X i , where X i is the random outcome of arm i . The formulation can model a lot of bandit problems appeared in the lit-erature, e.g., multiple plays, shortest path, minimum spanning tree and maximum weighted matching.
 Our framework contains such linear reward problems as special cases. 2 In particular, let L = max S | S | and a max = max i,S w i,S , and we have the bounded smooth-ness function f ( x ) = a max  X  L  X  x . By applying Theo-rem 1, the regret bound is Our result significantly improves the coefficient of the leading ln n term comparing to Theorem 2 of (Gai et al., 2012) in two aspects: (a) we remove a factor is likely to be much smaller than m  X   X  max / ( X  min in (Gai et al., 2012). This demonstrates that while our framework covers a much larger class of problems, we are still able to provide much tighter analysis than the one for linear reward bandits. In this paper, we propose the first general stochastic CMAB framework that accommodates a large class of nonlinear reward functions among combinatorial and stochastic arms. We provide CUCB algorithm with tight analysis on its distribution-dependent and distribution-independent regret bounds and applica-tions to new practical combinatorial bandit problems. There are many possible future directions from this work. One may study the CMAB problems with Markovian outcome distributions on arms, or the rest-less version of CMAB, in which the states of arms con-tinue to evolve even if they are not played. Another direction is to apply CMAB to contextual bandit set-tings where arm distributions depend on the context of the play. One may also see if any technique of this work can be applied to the study of adversarial combi-natorial bandits with nonlinear rewards. Of course, an important future work is to empirically validate our al-gorithm and demonstrate its effectiveness in practice. Anantharam, V., Varaiya, P., and Walrand, J. Asymp-totically efficient allocation rules for the multiarmed bandit problem with multiple plays  X  Part I: i.i.d. rewards; Part II: Markovian rewards. IEEE Trans-actions on Automatic Control , AC-32(11):968 X 982, 1987a.
 Audibert, J.-Y., and Bubeck, S. Minimax policies for adversarial and stochastic bandits. In COLT , 2009. Audibert, J.-Y., Bubeck, S., and Lugosi, G. Mini-max policies for combinatorial prediction games. In COLT , 2011.
 Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine Learning , 47(2-3):235 X 256, 2002a.
 Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire,
R. E. The nonstochastic multiarmed bandit prob-lem. SIAM J. Comput. , 32(1):48 X 77, 2002b.
 Berry, D. and Fristedt, B. Bandit problems . Chapman and Hall, 1985.
 Bubeck, S., Cesa-Bianchi, N., and Kakade, S. M. To-wards Minimax Policies for Online Linear Optimiza-tion with Bandit Feedback. In COLT , 2012.
 Caro, F. and Gallien, J. Dynamic assortment with de-mand learning for seasonal consumer goods. Man-agement Science , 53:276 X 292, 2007.
 Cesa-Bianchi, N. and Lugosi, G. Combinatorial ban-dits.
 Gai, Y., Krishnamachari, B., and Jain, R. Learning multiuser channel allocations in cognitive radio net-works: A combinatorial multi-armed bandit formu-lation. In DySPAN , 2010.
 Gai, Y., Krishnamachari, B., and Jain, R. Combina-torial network optimization with unknown variables:
Multi-armed bandits with linear rewards and indi-vidual observations. IEEE/ACM Transactions on Networking , 20, 2012.
 Garivier, A. and Capp  X e, O. The KL-UCB algorithm for bounded stochastic bandits and beyond. In COLT , 2011.
 Hazan, E. and Kale, S. Online submodular minimiza-tion. In NIPS , 2009.
 Kakade, S. M., Kalai, A. T., and Ligett, K. Playing games with approximation algorithms. SIAM Jour-nal on Computing , 39(3):1088 X 1106, 2009.
 Kempe, D., Kleinberg, J. M., and Tardos,  X  E. Maximiz-ing the spread of influence through a social network. In KDD , 2003.
 Lai, T. L. and Robbins, H. Asymptotically effi-cient adaptive allocation rules. Advances in Applied Mathematics , 6:4 X 22, 1985.
 Liu, H., Liu, K., and Zhao, Q. Logarithmic weak re-gret of non-bayesian restless multi-armed bandit. In ICASSP , 2011.
 Liu, K. and Zhao, Q. Adaptive shortest-path rout-ing under unknown and stochastically varying link states. Arxiv preprint arXiv:1201.4906 , 2012. Mannor, S., and Shamir, O. From Bandits to Experts: On the Value of Side-Observations. In NIPS , 2011. Nemhauser, G., Wolsey, L., and Fisher, M. An analysis of the approximations for maximizing submodular set functions. Mathematical Programming , 14:265 X  294, 1978.
 Radlinski, F., Kleinberg, R., and Joachims, T. Learn-ing diverse rankings with multi-armed bandits. In ICML , 2008.
 Streeter, M., Golovin, D., and Krause, A. Online learning of assignments. In NIPS , 2009.
 Streeter, M. and Golovin, D. An online algorithm for maximizing submodular functions. In NIPS , 2008. Sutton, R. and Barto, A. Reinforcement learning, an introduction . MIT Press, 1998.
 Vazirani, V. V. Approximation Algorithms Springer,
