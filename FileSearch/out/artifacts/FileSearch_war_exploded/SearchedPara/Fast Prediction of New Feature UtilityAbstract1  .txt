 Hoyt Koepke hoytak@stat.washington.edu Department of Statistics University of Washington Seattle, WA 98107 Mikhail Bilenko mbilenko@microsoft.com Microsoft Research, Redmond, WA 12345 In many mature learning applications, training algo-rithms are advanced and well-tuned, making the dis-covery and addition of new, informative features the primary driver of error reduction. New feature de-sign strategies include addition of previously unused descriptive signal sources, as well as various methods that derive new features from the existing represen-tation. A newly proposed feature is typically evalu-ated by augmenting it to the data representation and re-running the training and validation procedures to observe the resulting difference in predictive accuracy. However, re-training carries significant costs in many real-world applications:  X  Computational costs : large-scale domains (e.g.,  X  Logistical costs : training processes for industry  X  Monetary costs : in the domains of medical and fi-These costs call for feature utility prediction meth-ods that do not rely on re-training, instead view-ing the learner as a black box constructing a best-possible predictor from a chosen model class. The black-box assumption implies that the only description of the learned predictor is provided via its evaluation on labeled data (e.g., on a hold-out set or via cross-validation), on which its outputs are compared with true target values via a task-appropriate loss func-tion. Thus, our objective is to design a computation-ally inexpensive algorithm for statistically determining whether adding a new feature can potentially reduce the expected loss, given the current predictor. To derive a principled algorithm for the problem, we prove that under mild assumptions, testing whether a feature can yield predictive accuracy gains is equiva-lent to testing its correlation with the negative loss gra-dient, against which we train a squared-loss regressor. To construct a provably consistent hypothesis test, we form a null distribution by bootstrapping the marginal distributions. The overall algorithm is easily paral-lelizable, does not require re-training, and works on subsampled datasets, making it particularly appropri-ate for large data contexts. The method is applicable to a wide variety of learning tasks, requiring only esti-mates of the functional gradient of the loss, which can be approximated even for discontinuous losses that are common in structured tasks, i.e., ranking.
 The rest of the paper is organized as follows: Section 2 describes related work, followed by Section 3 that formally defines the problem and motivates the ap-proach. Section 4 describes our method and provides theoretical analysis. Section 5 summarizes empirical evaluation of the approach, followed by discussion of future work and conclusions in Sections 6 and 7. The task addressed in this paper  X  efficient esti-mation of predictive utility for new features with-out re-training  X  is related yet distinct from three known problems: feature selection (Guyon &amp; Elisse-eff, 2003), active feature acquisition (Saar-Tsechansky et al., 2009), and feature extraction (Krupka et al., 2008). While these tasks also involve estimating mea-sures of feature importance, they have different objec-tives. Critically, many techniques for these problems rely on re-training, while our motivation is avoiding it. In contrast to our setting, where the objective is to efficiently triage new features for addition, feature selection aims to remove unnecessary existing fea-tures (Guyon &amp; Elisseeff, 2003). Representatives in-clude wrapper approaches that utilize multiple rounds of re-training with feature subsets, methods that use prediction results for instances with permuted or dis-torted feature values (Breiman, 2001; Kononenko, 1994), and filter techniques that rely on joint statis-tics of features and class labels (Song et al., 2007). Feature acquisition aims to incrementally select indi-vidual feature values for addition to the dataset via estimating their expected utility, and can be viewed as a feature-focused variant of active learning (Li-zotte et al., 2003; Saar-Tsechansky et al., 2009). Pro-posed solutions rely on expensive value-of-information computation, making them prohibitive for our setting. Feature extraction methods attempt to construct new joint features that combine individual attributes by evaluating their dependency structure (Della Pietra et al., 1997; Krupka et al., 2008). In contrast, our approach seeks to directly evaluate the possibility of improvement in prediction accuracy for new features. On the theoretical side, several approaches have used bootstrapping or permutation tests to assess predictive value of features (Fromont, 2007; Anderson &amp; Robin-son, 2001; Ojala &amp; Garriga, 2010). These methods typically utilize the tests to assess the generalizability of results obtained on the finite sample case, a well-known property (Van der Vaart &amp; Wellner, 1996). Also of note is recent work on testing for the statis-tical independence of features, a key component of our analysis (Gretton &amp; Gy  X orfi, 2010). In particular, there has been active work for kernel methods that use the Hilbert-Schmidt Independence Criteria (Sriperum-budur et al., 2010; Song et al., 2007; Gretton et al., 2008). While our approach also relies on functional analysis techniques, it provides an alternative that does not rely on kernels, instead using standard corre-lation methods, similarly in spirit to (Huang, 2010). We consider the standard inductive learning setting, where training data is a set of samples of random vari-able pairs, ( X i ,Y i ) from an unknown joint distribution function P X,Y , corresponding to data instances de-scribed by feature values X i and prediction targets Y i . Learning corresponds to finding a predictor function f 0 from some function class F X that minimizes expected loss E L ( f ( X ) ,Y ) for a given loss function L encoding the application-appropriate error measure: The new feature utility prediction problem can be posited as determining whether adding an additional random variable, X 0 , to the data representation can result in reduction of expected loss if the predictor was re-trained with it. We designate the function class for predictors on the resulting representation as F
X,X 0 = F ; it subsumes function classes F X and F X 0 which are restricted to predictors that depend only on feature sets X or X 0 respectively. Formally, Then, the new feature utility prediction problem can be formalized as the hypothesis test of: against the null hypothesis H 0 in which they are equal. In other words, we define feature utility as the capa-bility of the feature to lower the expected predictor loss in the infinite sample case (i.e., w.r.t. to the true distributions). Thus, we use the theoretical paradigm in which our  X  X est set X  is the true distribution. To motivate the approach, consider the ideal feature evaluation test: determining whether X 0  X  Y | X , i.e. if X 0 is independent of Y given X . If this is answered in the affirmative  X  the null hypothesis H 0 is true  X  then X 0 contains no additional information about Y that is not already contained in X , and hence the loss cannot be reduced. Otherwise, knowing X 0 provides information that can be exploited to construct a better predictor as long as F is sufficiently rich. However, this ideal test is expensive to perform (Huang, 2010; Song, 2009; Su &amp; White, 2008).
 Instead of the ideal conditional independence test, we consider a more restricted test that seeks to determine whether X 0  X   X  ( f 0 ( X ) ,Y ) for some function  X  capable of capturing the part of Y that could not be predicted by f 0 ( X ). If this is answered in the affirmative for an appropriate  X  , then X 0 contains no additional infor-mation about Y that is not already contained in X . Otherwise, knowing X 0 provides some new informa-tion that can be utilized to create a better predictor. In the next section, we show that for a broad class of loss functions and predictor classes, we can construct a  X  and test independence by maximizing the correla-tion between g ( X 0 ) and  X  ( f 0 ( X ) ,Y ) for g  X  X  X 0 show this test to be equivalent to testing: ( H 0 1 )  X  g  X  X  X 0 such that against the null ( H 0 0 ) in which such g does not exist. In the following sections, we develop a consistent test for ( H 0 1 ) against ( H 0 0 ) under mild regularity assumptions on the predictor class and the loss function L (  X  ,Y ). In this section, we present a theoretical description of our approach and prove, under reasonable assump-tions, that it provides an accurate test of whether a new feature X 0 can improve prediction performance. The key part of the proof, detailed in Section 4.2, is the use of the bootstrap to test for the statistical inde-pendence of the new feature to a residual function of the current predictions. To set up this test, we first list and discuss several assumptions on the predictor class and the loss function. Then, in Section 4.1, we show a sequence of equivalent formulations of our problem in the context of the true joint distribution of ( X,X 0 ,Y ). The goal, reached in statement T4 of Theorem 4.2, is a formulation that can be accurately tested in the fi-nite sample case using the bootstrap. This formula-tion, combined with a way to handle the optimization component of the bootstrap test, leads to the practical algorithm presented in Section 4.3. Loss Function Assumptions. Our assumptions on the loss minimized when searching for a predictor from F are quite weak: finiteness, a type of strict mono-tonicity, and an available direction of descent: L1. Finiteness:  X  f  X  X  , E | L ( f ( X,X 0 ) ,Y ) | &lt;  X  . L2. Weak augmenting functional convexity: L3. A functional descent direction: Condition (L2) essentially imposes a type of strict monotonicity on the function class away from the min-imum. It is much weaker than convexity; all it requires is that moving in the direction of a better optimizer gives some improvement, even if it is relatively small. Condition (L3) intuitively says there exists a direction along which improvement in the expected loss is guar-anteed  X  provided improvement is possible.
 These assumptions are quite weak and cover many non-convex, discontinuous loss functions. In the case of a convex, differentiable loss function, however, we show below that conditions (L2) and (L3) are satisfied and  X  f 0 has an easy and natural form.
 Prediction Functions Assumptions. The assump-tions on the classes of prediction functions F , F X and F X 0 needed to prove consistency are the following: F1. Closure under scaling: cf  X  X   X  f  X  X  , c  X  R + . F2. Closure under shifting: d + f  X  X   X  f  X  X  , d  X  R . F3. min F4.  X  f  X  F , f ( X,X 0 ) is bounded, or, generally, F is These conditions, while seemingly obscure, are gener-ally satisfied by most modern predictor classes. Condi-tion (F3) states that training on all features will result in a better predictor than one obtained by first train-ing on a subset of features, then  X  X atching X  it with the remaining features. This bound guarantees that the improvement in loss on the full predictor can only be greater than that obtained by a decomposed model. The assumption (F4)  X  that F is P -Donsker  X  bounds the flexibility of the class of classifiers. Intuitively, it means that when working with an asymptotically large sample, the behavior of the classifier is not inordinately dominated by a few outlier values. This assumption ensures the behavior of the bootstrap is reasonable  X  two classifiers trained on different bootstrapped sam-ples should not be wildly different. The boundedness of f  X  F , along with measurability assumptions, im-plies this (Van der Vaart &amp; Wellner, 1996), but virtu-ally all machine learning algorithms used in practice satisfy this assumption.
 Correlated Features and the XOR Problem.
 An obvious question to ask is: what about new fea-tures that are only useful in conjunction with ex-isting features (of which Y = X 1 XOR X 2 , with X 1 ,X 2 iid  X  Bernoulli 1 2 is the canonical example), wouldn X  X  assumption (L3) be violated and the ap-proach fail to predict their utility? This intuition is correct; however, when this situation occurs in prac-tice, it is rarely in the absence of other modeling in-formation indicating that it might be the case. As our method allows for multiple variables to be tested as a block, or existing features to be recycled by setting one or more dimension of X 0 to specific dimensions of X , such cases can easily be handled.
 Connection to Convex Loss Functions. In the case of a convex, differentiable loss functions, we show that (L2) and (L3) are satisfied, and direction of de-scent  X  f 0 can be defined by the distribution of the negative gradient of the loss function, making it easily computable in practice.
 Theorem 4.1: Suppose that for  X  y in the support of Y , L ( u,y ) is convex and differentiable in u and sat-isfies assumption (L1). Then (L2) and (L3) hold with  X  0 given by where  X  is defined to produce std ( X  f 0 ) = 1 . Proof. (L2)immediately follows;  X  f  X  X  X and h  X  X  X 0 ,
L ( f ( X ) ,Y )  X  L ( f ( X ) +  X h ( X 0 ) ,Y ) by the definition of convexity; taking expectations yields the result. Now, using (L1) and the differen-tiability of L , it is easy to show that the Gateaux functional derivative d  X ( f ; h ) (Van der Vaart, 2000) of the functional  X ( f ) = E L ( f ( X,X 0 ) ,Y ) is given by d  X ( f ; h ) = E [  X  ( f ( X,X 0 ) ,Y ) h ( X,X 0 ,Y )] , with  X  given in (2). Now, d  X ( f ; h ) defines a linear operator in functional space in which d  X ( f ; h ) gives the change in  X ( f ) in the direction h . (L3) and (2) immediately follow from geometry. 4.1. Equivalent Tests In this section, we show that testing ( H 0 1 ) is equivalent to testing for the existence of a feature transform pos-itively correlated with the loss gradient, which allows designing a consistent bootstrap algorithm for it. When evaluating a new feature, we are interested in finding a function in F X 0 that improves the expected loss. In light of condition (L3), define as the function that most closely aligns with a direc-tion of improvement. The following theorem connects improvement in expected loss to this function.
 Theorem 4.2: Suppose the loss function L and pre-dictor class F satisfy conditions (L1)-(L3) and (F1)-(F4). Let f 0 = argmin f  X  X   X  0 and g 0 be as defined in (L3) and Eq. (3) . Then the following are equivalent: T1.  X  g  X  X  X 0 that improves the expected loss: T2. min T3. E [ g 0 ( X 0 )  X   X  f 0 ] &gt; 0 .
 T4. E [ g 0 ( X 0 )  X   X  f 0 ]  X  E g 0 ( X 0 ) E  X  f 0 &gt; 0 . Proof. First, we show that T1 implies T3. Suppose that T1 is true. By (L2), E L ( f 0 ( X ) +  X g ( X 0 ) ,Y ) &lt; E L ( f 0 ( X ) ,Y ) for all  X   X  (0 , 1]. As  X g ( X 0 )  X  X  by (F1), E [(  X g ( X 0 ))  X   X  f 0 ] &gt; 0 for  X  &gt; 0 sufficiently small. Now, by (F1), g ( X 0 ) / std g ( X 0 )  X  X  X 0 , thus
E [ g 0 ( X 0 )  X   X  f 0 ]  X  E { [ g ( X 0 ) / std ( g ( X as g ( X 0 ) / std ( g ( X 0 )) is included in the optimization of equation (3). Now, by (L3), T3 implies that E  X  0 &gt; 0 sufficiently small. T2 follows, as min  X   X  R + E L ( f 0 ( X ) +  X g 0 ( X 0 ) ,Y ) T2 trivially implies T1, thus T1 -T3 are equivalent. For the equivalence of T3 and T4 we show that, given the closure of F under constant shifts, E  X  f 0 = 0. Suppose E  X  f 0 = c 6 = 0. Let  X  &gt; 0, and consider the function g 0 (  X  ) =  X c . Now E  X c  X  f 0 =  X c 2 &gt; 0. As f 0 (  X  ) +  X c  X  F X  X  F by (F2), (L3) implies that, for  X  sufficiently small, E L ( f 0 ( X ) +  X c,Y ) &lt; E
L ( f 0 ( X ) ,Y ). This contradicts the optimality of f 0 F
X ; thus E  X  f 0 = 0, making T3 equivalent to T4. 4.2. A Consistent Hypothesis Test The above proofs work for random variables with re-spect to their true distributions; the bridge between this and a practical algorithm is the bootstrap. As dis-cussed earlier, we are interested in assessing the perfor-mance of our predictor on the true distribution, which requires a consistent test of whether ( H 1 )  X  g  X  X  X 0 s.t. E [ g ( X against the null, where equality holds. By Theorem 4.2, we have that test ( H  X  1 ) is equivalent to ( H 0 1 In the next theorem, we define an accurate hypoth-esis test of ( H  X  1 ) and prove its consistency. The last step needed is a good estimator  X   X  f 0 of  X  f 0 generated by the minimum-risk predictor f 0  X  F X on the true distribution. This is because we show the bias in the bootstrap to be controlled by the standard deviation of  X  There are several ways to effectively control this bias. One can assume that the true  X  f 0 is known or comes from training on a much larger dataset, against which the performance is actually evaluated. This is a com-mon scenario for many large-data domains. Also, one can use methods known to asymptotically reduce the bias, such as k-fold cross validation (Cornec, 2010). Theorem 4.3: Let Let  X  X 0 D = X 0 be a bootstrap sample of X 0 , and let  X   X  from  X  X 0 . Let F be the c.d.f. of K (  X  X 0 ,  X   X  f 0 function F  X  1 ( t ) = inf n u&gt; 0 : P K (  X  X 0 ,  X   X  Fix  X   X  (0 , 1) , and set the critical point c  X  F  X  1 (  X  ) . The test that accepts the alternative hypoth-esis, ( H  X  1 ) , for values of K ( X 0 ,  X  f 0 )  X  c  X  jects otherwise, has asymptotic level  X  and bias at most max ( F ( c  X  +  X  )  X   X ,  X   X  F ( c  X   X   X  )) , where  X  =  X  Proof. Drop the subscript f 0 from  X  and  X   X  for con-venience. From Th. 4.2, we need a consistent test of where H is the joint measure of X 0 and  X   X , P , Q are the respective marginal distributions, and G = F X 0  X { I } , where I is the identity function. However, only their empirical samples H n , P n , and Q n are available. Let  X  P n and  X  Q n be the measures formed from an inde-pendent bootstrap sample of X 0 n = ( x 0 1 ,...,x 0 n  X   X  = (  X   X  1 ,...,  X   X  n ), and let  X  H n be the joint measure formed from  X  P n and  X  Q n . Then let under these conditions, we reject the null if From (Van der Vaart &amp; Wellner, 1996), this test is consistent with asymptotic level  X  . To complete the proof, note that where a = b  X  c denotes | a  X  b | X  c . Now where (4) follows from Liapunov X  X  and Jensen X  X  in-equalities, and (5) uses std( X ) = 1. Combing this result with the bootstrap completes the proof. 4.3. A Feature Evaluation Algorithm Performing the test requires obtaining a transform that maximizes the inner product between the function and the negative loss gradient. The following theorem allows doing this by via squared-error loss minimiza-tion for an appropriately weighted expectation. Theorem 4.4: Suppose F satisfies assumptions (F1) and (F2), and let  X  f 0 be defined as in (L3). Let Then f  X  2 (  X  ) a.s. = f  X  1 (  X  ) / std ( f  X  1 ( X 0 )) . Proof. Let Z = ( X  f 0  X  E  X  f 0 ). Now, consider Eq.(6). We can enforce std( g ( X 0 )) = 1 as follows: This is invariant to scaling of g , and F X 0 is closed under scaling, so g is scaled to make std( g ( X 0 )) = std( f  X  1 ( X 0 )). Furthermore, E { ( g ( X 0 )+ c ) Z } = E g ( X 0 ) Z + E cZ = E g ( X 0 ) Z , so g is invariant to shifts. As F X 0 is closed under shifts, g is shifted so that E g ( X 0 ) = 0. Thus where G = { g  X  X  X 0 : std( g ( X 0 )) = std( f  X  1 ( X 0 )) , E Thus g  X  is exactly f  X  1 , proving the theorem. The practical implication of the theorem is that for new feature values X 0 n = ( X 0 1 ,...,X 0 n ) and standard-ized gradient samples  X   X  n = (  X   X  1 ,...,  X   X  n ), K in Theo-rem 4.3 becomes which corresponds to least-squares regression regard-less of loss L . This surprising result allows reducing the new feature utility problem for a wide array of learning tasks and loss functions to a standard task for which powerful algorithms are readily accessible. Using the bootstrap, this method is turned into a rig-orous test for feature significance, summarized in Al-gorithm 1. The p -value score corresponds to rejecting or accepting the hypothesis that the new value will lead to loss reduction. The algorithm also outputs the number of null standard deviations by which the test statistic v is above the null mean (the z-score), here refered to as the utility score. As empirical evaluation demonstrates, this score provides an accurate measure Algorithm 1 : Feature Relevance Test of relative feature utility, allowing to rank features for which no null statistic t i is greater than v . The method scales well to large-data tasks as the N bootstrap + 1 evaluations of K can be easily par-allelized, X 0 is typically lower-dimensional than X , and efficient distributed algorithms for least-squares regression are well-studied (Bekkerman et al., 2012). It is important to note that training a regressor g to maximize correlation with the loss function gradient is central to AnyBoost and MART views of boosting as gradient descent in function space (Mason et al., 1999; Friedman, 2001). Analogously, our approach can be viewed as coordinate descent in function space. 5.1. Datasets We evaluate the proposed approach on three learn-ing tasks: calibrated binary classification, regression and ranking. Standard loss functions are used for each task: cross-entropy (log-loss) for calibrated clas-sification, squared loss for regression, and NDCG for ranking (J  X arvelin &amp; Kek  X al  X ainen, 2002). Despite the fact that NDCG is discontinuous, it satisfies assump-tions (L1)-(L3), and its pointwise functional gradient estimates can be approximated by aggregating pair-wise cost differentials as described in (Burges, 2010). For classification and regression, we use standard real-task benchmarks from the UCI collection, Adult and Housing . For ranking, we employ a large-scale indus-trial search engine dataset, WebRanking . While it uses thousands of individual features, they are grouped into several dozen distinct information sources. Each information source captures some document property and yields multiple numeric features derived from the property for a given query. For example, the Doc-umentBody source yields features based on the doc-ument X  X  text contents (e.g., various similarity mea-sures w.r.t. the query), while the DocumentAnchor-Text source yields analogous features based on the an-notations of the document X  X  incoming links. The op-erational setting for feature utility prediction in this domain is to triage potential new information sources considered for addition to the index, reducing the computational and logistical costs that full re-training would involve. Hence, we overload terminology and refer to each source as a multi-dimensional  X  X eature X . Table 1 summarizes the datasets and loss functions used in the experiments. We employ 10-fold cross-validation for experiments on Adult and Housing , and hence the number of instances refers to the en-tire dataset size. For WebSearch , the number of instances refers to the size of the validation fold (the training set is much larger), and the number of features refers to the number of information sources evaluated. 5.2. Methodology Accuracy of feature utility prediction is evaluated w.r.t. actual error improvements obtained via re-training with the new feature included. Experimental procedure can be summarized as follows: 1. Given dataset X comprised of d features, 2. For each feature X ( i ) , perform evaluation on an 3. Using Algorithm 1, a p -value and utility score for 4. Feature scores and p -values are compared to the In the above procedure,  X  X valuation X  in steps 1 and 2 refers to 10-fold cross-validation for UCI datasets, and training followed by testing on the validation fold for WebSearch . Gradient boosted trees were used for all tasks (Friedman, 2001), using training loss correspond-ing to each task. For ranking, the LambdaMART tree boosting algorithm that optimizes NDCG was used (Burges, 2010). Solving for optimal g (  X  ) with maximal correlation to negative loss gradient and the corresponding bootstrap trials were performed using boosted regression trees, optimizing for squared-error loss as dictated by Theorem 4.4. Bootstrapping was performed for 100 rounds. 5.3. Results and Discussion Per-dataset plots in Figure 1 illustrate predicted vs. actual utilities for each feature, reported as percent-ages of the range obtained across all features, with ac-tual utilities based on loss reduction due to feature be-ing added, and predicted utilities based on scores pro-duced by Algorithm 1. In other words, the feature with highest actual and predicted utility appears at 100% on horizontal and vertical axes, respectively. Features for which actual utility is significant at p &lt; 0 . 05 (over validation folds) are demarkated.
 As the results demonstrate, the proposed method iden-tifies the features that produce actual accuracy gains with very high recall: all features that are determined to be insignificant indeed produce no meaningful ac-curacy gains. While some of the features identified as relevant did not in fact produce sizable accuracy gains, this is expected: while a feature may have some pre-dictive value, the predictor class or learning algorithm may be unable to realize it. The practical motiva-tion for the problem is feature triage, where a feature engineer seeks to quickly prioritize features by their potential for improving prediction quality, and the re-sults demonstrate that our approach indeed provides such prioritization accurately.
 Comparison with Feature Selection Heuristics.
 We also evaluated several commonly used feature se-lection heuristics that do not rely on re-training. Fig-ure 2 illustrates the performance of  X  2 Statistic, Infor-mation Gain Ratio, and Correlation-based Feature Se-lection (CFS) (Guyon &amp; Elisseeff, 2003; Hall, 1999) for the Adult dataset. These results demonstrate that methods that compute feature utility greedily (  X  2 and Information Gain Ratio) can significantly overestimate the value of features that are not informative given others, as evidenced by the two top-scoring features that have near-zero actual utility (in top left corner of corresponding figures). CFS works better as it takes into account the new feature X  X  correlation with other features as well as the label, yet it underestimates the utility of the best feature dramatically, demonstrating the shortcoming of label-based estimates vs. utilizing losses of the current predictor used by our approach. While this paper demonstrated that the feature utility prediction problem can be solved by posing it as a hy-pothesis test in function space, it would be interesting to see alternative algorithms for the problem designed via information-theoretic formulations. Another po-tentially fruitful direction for future work is develop-ing semi-supervised methods that can utilize unlabeled data for improving new feature utility estimates, given its abundance in large-scale domains. Additionally, designing modifications of the described approach for feature selection, extraction and active feature-value acquisition could yield new efficient methods for these tasks, as the overall idea of exploiting outputs of an existing predictor is clearly relevant for these prob-lems. Finally, another attractive future work direction lies along creating new feature utility prediction algo-rithms that remove the  X  X lack-box X  assumption and utilize properties of a specific learning algorithm or predictor class, possibly yielding better performance. This paper considered the problem of predicting new feature utility without re-training the original learner. A solution was proposed based on a consistent test-ing procedure, derived by establishing a function-space relationship between loss gradient and a maximizing transform of the new features. The approach is gen-eral, supporting many common learning tasks and loss functions for which the problem is reduced to squared-error regression. This can be performed for just the new features in isolation or in conjuction with exist-ing features. The resulting algorithm allows easy par-allelization, making it appropriate for large-scale do-mains. Empirical evaluation demonstrated the accu-racy of the approach on several learning tasks. Acknowledgements: The authors thank Tom Fin-ley for help with ranking experiments and anonymous reviewers for helpful feedback. This work was done while the first author visited Microsoft Research.
