 Quality Estimation (QE) models predict the qual-ity of Machine Translation (MT) output based on the source and target texts only, without reference translations. This task is often framed as a super-vised machine learning problem using various fea-tures indicating fluency, adequacy and complexity of the source-target text pair, and annotations on translation quality given by human translators. Var-ious kernel-based regression and classification algo-rithms have been explored to learn prediction mod-els.

The application of QE we focus on here is that of guiding professional translators during the post-editing of MT output. QE models can provide trans-lators with information on how much editing/time will be necessary to fix a given segment, or on whether it is worth editing it at all, as opposed to translating it from scratch. For this application, models are learnt from quality annotations that re-flect post-editing effort, for instance, 1-5 judgements on estimated post-editing effort (Callison-Burch et al., 2012) or actual post-editing effort measured as post-editing time (Bojar et al., 2013) or edit distance between the MT output and its post-edited version (Bojar et al., 2014; Bojar et al., 2015).

One of the biggest challenges in this field is to deal with the inherent subjectivity of quality labels given by humans. Explicit judgements (e.g. the 1-5 point scale) are affected the most, with pre-vious work showing that translators X  perception of post-editing effort differs from actual effort (Kopo-nen, 2012). However, even objective annotations of actual post-editing effort are subject to natural variance. Take, for example, post-editing time as a label: Different annotators have different typing speeds and may require more or less time to deal with the same edits depending on their level of expe-rience, familiarity with the domain, etc. Post-editing distance also varies across translators as there are of-ten multiple ways of producing a good quality trans-lation from an MT output, even when strict guide-lines are given.

In order to address variance among multiple trans-lators, three strategies have been applied: (i) mod-els are built by averaging annotations from multiple translators on the same data points, as was done in the first shared task on the topic (Callison-Burch et al., 2012); (ii) models are built for individual trans-lators by collecting labelled data for each translator (Shah and Specia, 2014); and (iii) models are built using multitask learning techniques (Caruana, 1997) to put together annotations from multiple translators while keeping track of the translators X  identification to account for their individual biases (Cohn and Spe-cia, 2013; de Souza et al., 2015).

The first approach is sensible because, in the limit, the models built should reflect the  X  X verage X  strate-gies/preferences of translators. However, its cost makes it prohibitive. The second approach can lead to very accurate models but it requires sufficient training data for each translator, and that all trans-lators are known at model building time. The last approach is very attractive. It is a transfer learn-ing (a.k.a. domain-adaptation ) approach that allows the modelling of data from each individual translator while also modelling correlations between transla-tors such that  X  X imilar X  translators can mutually in-form one another. As such, it does not require mul-tiple annotations of the same data points and can be effective even if only a few data points are available for each translator. In fact, previous work on multi-task learning for quality estimation has concentrated on the problem of learning prediction models from little data provided by different annotators.
In this paper we take a step further to investigate multitask learning for quality estimation in settings where data may be abundant for some or most an-notators. We explore a multitask learning approach that provides a general, scalable and robust solution regardless of the amount of data available. By test-ing models on single translator data, we show that while building models for individual translators is a sensible decision when large amounts of data are available, the multitask learning approach can out-perform these models by learning from data by mul-tiple annotators. Additionally, besides having trans-lators as  X  X asks X , we address the problem of learning from data for multiple language pairs.
 We devise our multitaslk approach within the Bayesian non-parametric machine learning frame-work of Gaussian Processes (Rasmussen and Williams, 2006). Gaussian Processes have shown very good results for quality estimation in previous work (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013). Our datasets  X  annotated for post-editing distance  X  contain nearly 100K data points, two orders of magnitude larger than those used in previous work. To cope with scalability issues re-sulting from the size of these datasets, we apply a sparse version of Gaussian Processes. We perform extensive experiments on this large-scale data aim-ing to answer the following research questions:  X  What is the best approach to build models to  X  When large amounts of data are available, can  X  Can crosslingual data help improve model per-
In the remainder of the paper we start with an overview on related work in the area of multitask learning for quality estimation (Section 2), to then describe our approach to multitask learning in the context of Gaussian Processes (Section 3). In Sec-tion 4 we introduce our data and experimental set-tings. Finally in Sections 5 and 6 we present the results of our experiments to answer the above men-tioned questions for cross-annotator and crosslin-gual transfer, respectively. As was discussed in Section 1, the problem of vari-ance among multiple translators in QE has recently been approached in three ways. The first two ap-proaches essentially refer to preparation of the data. At WMT12, the first shared task on QE (Callison-Burch et al., 2012), the official dataset was created by collecting three 1-5 (worst-best) discrete judge-ments on  X  X erceived X  post-editing effort for each translated segment. The final score was a scaled av-erage of the three scores, and about 15% of the la-belled data was discarded as annotators diverged in their judgemetns by more than one point. While this type of data proved useful and certainly reliable in the limit of the number of annotators, it is too ex-pensive to collect.

Shah and Specia (2014) built QE models using data from n annotators by either pooling all the data together or splitting it into n datasets for n individ-ual annotator models. These models were tested in blind versus non-blind settings, where the former refers to test sets whose annotator identifiers were unknown. They observed a substantial difference in the error scores for each of the individual models. They showed that the task is much more challenging for QE models trained independently when training data for each annotator is scarce. In other words, sufficient data needs to be available to build individ-ual models for all possible translators.

The approach of using multitask learning to build models addresses the data scarcity issue and has been shown effective in previous work. Cohn and Specia (2013) first introduced multitask learning for QE. Their goal was to allow the modelling of vari-ous perspectives on the data, as given by multiple an-notators, while also recognising that they are rarely independent of one another (annotators often agree) by explicitly accounting for inter-annotator correla-tions. A set of task-specific regression models were built from data labelled with post-editing time and perceived post-editing effort (1-5).  X  X asks X  included annotators, the MT system and the actual source sen-tence, as their data included same source segments translated/edited by multiple systems/editors.
Similarly, de Souza et al. studied multitask learn-ing to deal with data coming from different train-ing/test set distributions or domains, and generally scenarios in which training data is scarce. Offline multitask (de Souza et al., 2014a) and online multi-task (de Souza et al., 2015; de Souza et al., 2014b) learning methods for QE were proposed. The later focused on continuous model learning and adapta-tion from new post-edits in a computer-aided trans-lation environment. For that, they adapted an on-line passive-aggressive algorithm (Cavallanti et al., 2010) to the multitask scenario. While their setting is interesting and could be considered more chal-lenging because of the online adaptation require-ments, ours is different as we can take advantage of already having collected large volumes of data.
Multitask learning has also been used for other classification and regression tasks in language pro-cessing, mostly for domain adaptation (Daume III, 2007; Finkel and Manning, 2009), but also more recently for tasks such as multi-emotion analysis (Beck et al., 2014), where the each emotion explain-ing a text is defined as a task. However, in all previ-ous work the focus has been on addressing task vari-ance coupled with data scarcity, which makes them different from the work we describe in this paper. Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian non-parametric machine learning framework considered the state-of-the-art for regression. GPs have been used successfully for MT quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013), among other tasks.
 GPs assume the presence of a latent function f : R F  X  R , which maps a vector x from feature space F to a scalar value. Formally, this function is drawn from a GP prior: which is parameterised by a mean function (here, 0 ) and a covariance kernel function k ( x , x 0 ) . Each response value is then generated from the function evaluated at the corresponding input, y i = f ( x i )+  X  , where  X   X  X  (0 , X  2 n ) is added white-noise.
Prediction is formulated as a Bayesian inference under the posterior: where x  X  is a test input, y  X  is the test response value and D is the training set. The predictive posterior can be solved analitically, resulting in: where k  X  = [ k ( x  X  , x 1 ) k ( x  X  , x 2 ) ...k ( x  X  , x the vector of kernel evaluations between the training set and the test input and K is the kernel matrix over the training inputs (the Gram matrix). 3.1 Multitask Learning The GP regression framework can be extended to multiple outputs by assuming f ( x ) to be a vector valued function. These models are commonly re-ferred as Intrinsic Coregionalization Models (ICM) in the GP literature (  X  Alvarez et al., 2012).
In this work, we employ a separable multitask ker-nel, similar to the one used by Bonilla et al. (2008) and Cohn and Specia (2013). Considering a set of D tasks, we define the corresponding multitask kernel as: where k data is a kernel (Radial Basis Function, in our experiments) on the input points, d and d 0 are task or metadata information for each input and task covariances. In our experiments, we first con-sider each post-editor as a different task, and then use crosslingual data to treat each combination of language and post-editor as a task.

An adequate parametrisation of the multitask ma-trix is required to perform learning process. We follow the parameterisations proposed by Cohn and Specia (2013) and Beck et al. (2014): Individual: M = I . In this setting each task Pooled: M = 1 . Here the task identity is ignored. Multitask: M =  X  H  X  H T + diag (  X  ) , where  X  H is 3.2 Sparse Gaussian Processes The performance bottleneck for GP models is the Gram matrix inversion, which is O ( n 3 ) for stan-dard GPs, with n being the number of training in-stances. For multitask settings this becomes an is-sue for large datasets as the models replicate the in-stances for each task and the resulting Gram matrix has dimensionality nd  X  nd , where d is the number of tasks.

Sparse GPs (Snelson and Ghahramani, 2006) tackle this problem by approximating the Gram ma-trix using only a subset of m inducing inputs. With-out loss of generalisation, consider these m points as the first instances in the training data. We can then expand the Gram matrix in the following way: Following the notation in (Rasmussen and Williams, 2006), we refer K m ( n  X  m ) as K mn and its transpose as K nm . The block structure of K forms the basis of the so-called Nystr  X  om approximation: which results in the following predictive posterior: where  X  G =  X  2 n K mm + K mn K nm and k m  X  is the vector of kernel evaluations between test input x  X  and the m inducing inputs. The resulting training complexity is O ( m 2 n ) .

In our experiments, the number of inducing points was set empirically by inspecting where the learning curves (in terms of Pearson X  X  correlation gains) flat-ten, as shown in Figure 1. We used 300 inducing points in experiments with all the settings (see Sec-tion 4.3). 4.1 Data Our experiments are based on data from two lan-guage pairs: English-Spanish (en-es) and English-French (en-fr). The data was collected and made available by WIPO X  X  (World Intellectual Property Organization) Brands and Design Sector. The do-main of the data is trademark applications in En-glish, using one or more of the 45 categories of the Figure 1: Number of inducing points versus Pear-son X  X  correlation and their translations into one of the two languages.
An in-house phrase-based statistical MT system was built by WIPO (Pouliquen et al., 2011), trained on domain-specific data, to translate the English seg-ments. The quality of the translations produced is considered high, with BLEU scores on a 1K-single reference test set reaching 0.71. This is partly at-tributed to the short length and relative simplicity of the segments in the sub-domains of goods and services. The post-editing was done mostly inter-nally and systematically collected between Novem-ber 2014 and August 2015. The quality label for each segment is post-editing distance, calculated as the HTER (Snover et al., 2006) between the tar-get segment and its post-edition using the TERCOM
The data was split into 75% for training and 25% for test, with each split maintaining the original data distribution by post-editor. The number of training and test &lt; source, MT output, post-edited MT, HTER score &gt; tuples for each of the post-editors (ID) and language pair is given in Table 1. There are 63,763 overlapping English source segments out of 77,656 entries for en-fr and 98,663 entries for en-es. This information is relevant for the crosslingual data ex-periments, as we discuss in Section 6.

It should be noted that the total number of seg-ments as well as the number of segments per post-editor is significantly higher than those used in pre-vious work. For example, (Cohn and Specia, 2013) used datasets of 6,762 instances (2,254 for each of three translator) and 1,624 instances (299 for each of eight translators), while (Beck et al., 2014) had ac-cess to 1000 instances annotated with six emotions. 4.2 Algorithms For all tasks we used the QuEst framework (Specia et al., 2013) to extract a set of 17 baseline black-box necessary resources for the WIPO domain. These baseline features have shown to perform well in the WMT shared tasks on QE. They include simple counts, e.g. number of tokens in source and target segments, source and target language model prob-abilities and perplexities, average number of possi-ble translations for source words, number of punc-tuation marks in source and target segments, among other features reflecting the complexity of the source segment and the fluency of the target segment. toolkit, an open source implementation of GPs writ-ten in Python. 4.3 Settings We built and tested models in the following condi-tions:  X  Setting-1: Individual models on individual test  X  Setting-2: Pooled model on individual test  X  Setting-3: Multitask model on individual test  X  Setting-4: Individual models tested on pooled  X  Setting-5: Pooled model on pooled test set:  X  Setting-6: Multitask model on pooled test set:  X  Setting-7 to 10: Similar to setting-2, 3, 5, 6 but  X  Setting-11-13: Similar to setting-9, 6, 10 re-We report results in terms of Pearson X  X  correlation between predicted and true quality labels, as was done in the WMT QE shared tasks (Bojar et al., 2015). The multitask learning models consistently led to improvement over pooled models, and over individual models in most cases. We present the comparisons of the models for various settings in the following. The bars marked with * in each com-parison are significantly better than all others with p &lt; 0 . 01 according to the Williams significance test (Williams, 1959).
 Individual, pooled and multitask models on in-dividual test sets Results for both language pairs are shown in Figure 2. As expected, in cases where a large number of instances is available from an in-dividual post-editor, individual models tested on in-dividual test sets perform better than pooled mod-els. Overall, multitask learning models show im-provement over both individual and pooled mod-els, or the same performance in cases where large amounts of data are available for an individual post-editor. For example, in en-es, for post-editors 9 and 3, which have 845 and 3,939 instances in total, re-spectively, multitask learning models are consider-ably better. The same goes for post-editor 3 in en-fr, which has only 769 instances. For very few post-editors with a large number of instances (1,2 and 4 in en-es) multitask learning models perform the same as individual or even pooled models. For all other post-editors, multitask models further improve correlation with humans. These results emphasize Figure 2: Pearson X  X  correlation with various models on individual test sets the advantages of multitask learning models, even in cases where the post-editors that will use the mod-els are known in advance (first research question): Clearly, the models for post-editors with fewer in-stances benefit from the sharing of information from the larger post-editor data sets. As for post-editors with large numbers of instances, in the worst case the performance remains the same.
 Individual, pooled and multitask models on pooled test set Here we focus on cases where models are built to be used by any post-editor (second research question). Results in Figure 3 show that when test sets for all post-editors are put together, individual models perform distinctively worse than pooled and multitask learning models. Multitask learning models are significantly better than pooled models for both languages (0.511 vs 0.469 for en-es, and 0.481 vs 0.441 for en-fr). In the case of post-editor 3 for en-fr, the correlation is negative for individual models given the very low number of instances for this post-editor, which is not sufficient to build a general enough model that also Figure 3: Pearson X  X  correlation with various models on pooled test set works for other post-editors.
 Relationship among post-editors In order to gain a better insight into the strength of the relationships among various post-editors and thus into the ex-pected benefits from joint modelling, we plot the learned Corregionalisation matrix for all against all there exist various degrees of mutual interdepen-dences among post-editors. For instance, in the case of en-es, post-editor 4 shows a strong relationship with post-editors 6 and 7, a relatively weaker rela-tionship with post-editors 1 and 9, and close to non-existing with post-editors 3, 8 and 10. In the case of en-fr, post-editor 3 shows very weak relationship with all other post-editors, especially 4. This might explain the low Pearson X  X  correlation with individual models for post-editor 3 on pooled test sets. Figure 4: Heatmap showing a learned Coregionali-sation matrix over all post-editors To address the last research question, here we present the results on crosslingual models in com-parison to single language pair models. The train-ing models contain data from both en-es and en-fr language pairs in the various settings previously de-scribed, where for the multitask settings, tasks can be annotators, languages, or both.
 Single versus crosslingual pooled and multitask models on individual test sets Figure 5 shows a performance comparison between single language versus crosslingual models on individual test sets. Figure 5: Pearson X  X  correlation with single versus crosslingual models on individual en-fr test sets Due to space constraints, we only present results for the en-fr test sets, but those for the en-es test sets fol-low the same trend. Multitask models lead to further improvements, particularly visible for post-editor 3 (the one with less training data), where the crosslin-gual multitask learning model reaches 0.201 Pear-son X  X  correlation, while the monolingual multitask learning model performs at 0.171. The performance of the pooled models with crosslingual data also im-proves on this test set over monolingual pooled mod-els, but the overall figures are lower than with mul-titask learning, showing that the benefit does not only come from adding more data, but from ade-quate modelling of the additional data. This shows the potential to learn robust prediction models from datasets with multiple languages.
 Single versus crosslingual pooled and multitask models on pooled test set Figure 6 compares single language and crosslingual models on the pooled test sets for both languages. A pooled test set with data from different languages presents a more challenging case. Simply building crosslin-gual pooled models deteriorates the performance over single pooled models, whereas multitask mod-els marginally improve the performance for en-es and keep the performance of the single language models for en-fr. This again shows that multitask learning is an effective technique for robust predic-tion models over several training and test conditions. Single versus crosslingual pooled and multi-task models on non-overlapping data on pooled test set We posited that the main reason behind the marginal or non-existing improvement of the crosslingual transfer learning shown in Figure 6 is Figure 6: Pearson X  X  correlation with single vs crosslingual models: en-es and en-fr pooled test sets Figure 7: Pearson X  X  correlation with non-overlapping language data: single vs crosslingual models on en-es and en-fr on pooled test sets the large overlap between the source segments in the datasets for the two language pairs, as mentioned in Section 4: 63,763 instances, which comprise 82% of the en-fr instances, and 65% of the en-es instances. This becomes an issue because nearly half of the quality estimation features are based on the source segments. Therefore, we conducted an experiment with only 41,930 non-overlapping segments in the two languages. This experiment is only possible with pooled test sets, as otherwise too few (if any) instances are left for some post-editors. The re-sults, shown in Figure 7, are much more promising. The Figure compares single language and crosslin-gual multitask and pooled models on the polled test sets for both languages. It is interesting to note that, while the absolute figures are lower when com-pared to models trained on all data (Figures 5 and 6), the relative improvements of multitask crosslin-gual models over multitask single language models are much larger. We investigated multitask learning with GP for QE based on large datasets with multiple annotators and language pairs. The experiments were performed with various settings for training QE models to study the cases where data is available in abundance, ver-sus cases with less data. Our results show that mul-titask learning leads to improved results in all set-tings against individual and pooled models. Individ-ual models perform reasonably well in cases where a large amount of training data for individual anno-tators is available. Yet, by learning from data by multiple annotators, multitask learning models still perform better (in most cases) or at least the same as these models. Testing models on data for individual annotators is a novel experimental setting that we explored in this paper. Another novel finding was the advantage of multitask models in crosslingual settings, where individual models performed poorly and pooled models brought little gain.
 This work was supported by the QT21 (H2020 No. 645452) and Cracker (H2020 No. 645357). We would like to thank Bruno Pouliquen and Peter Baker for providing the WIPO data, resources and revising the details in Section 4.1.

