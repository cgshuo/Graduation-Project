 Difficult multiple-choice (MC) questions can be made easy by providing a set of answer options of which most are obviously wrong. In the education literature, a plethora of instruc-tional guides exist for crafting a suitable set of wrong choices (distractors) that enable the assessment of the students X  un-derstanding. The art of MC question design thus hinges on the question-maker X  X  experience and knowledge of the poten-tial misconceptions. In contrast, we advocate a data-driven approach, where correct and incorrect options are assembled directly from the students X  own past submissions. Large-scale online classroom settings, such as massively open online courses (MOOCs), provide an opportunity to design optimal and adaptive multiple-choice questions that are maximally informative about the students X  level of understanding of the material. In this work, we (i) develop a multinomial-logit discrete choice model for the setting of MC testing, (ii) derive an optimization objective for selecting optimally discrimi-native option sets, (iii) propose an algorithm for finding a globally-optimal solution, and (iv) demonstrate the effective-ness of our approach via synthetic experiments and a user study. We finally showcase an application of our approach to crowd-sourcing tests from technical online forums.
 Adaptive Learning, Assessment, Crowdsourcing, Optimal Testing  X  X lose X  to the correct answer, and those who were clueless. In the adaptive testing literature [11, 20], the questions them-selves are selected to be at a level that is appropriate for the student, such that their responses result in the most accurate estimate of their knowledge. In this work, we pursue the same goal, but at the level of designing a single question, i.e., to select a set of options to present as potential answers. This problem is not a straightforward extension of the classic adaptive testing problem for two reasons: (i) from an applica-tion perspective, only recently with the advent of web-scale learning platforms we are able to leverage the massive num-ber of student submissions and answer click-through logs to generate rich, adaptive, and data-driven questions that exploit actual student misconceptions; (ii) from a technical level, selecting choices is inherently a batch optimization problem, i.e., all options must be considered jointly during optimization; this is in stark contrast to question selection, which typically assumes independence between questions and finds the optimal set in a greedy fashion (though test bank optimization is an exception, see Chapter 7 in [5]) . The main contributions of our work are summarized as follows:  X  We propose an objective function for selecting an optimal set of choices in a discrete choice model, given the estimated user ability, and we investigate the solutions across different regimes of student ability.  X  We propose an algorithm for finding a globally-optimal option choice set.  X  We collect and release a dataset used in our experiments: A  X  X .S. States Quiz X  dataset, where users were given an
MCQ quiz testing their knowledge of U.S. states.  X  We propose a new paradigm of data-driven test design by leveraging data from technical online forums, and showcase the applicability of this model to the task of MCQ design from StackExchange posts.
In the education literature, multiple choice testing has received significant attention, studying a broad range of aspects of MCQ design, e.g., to ensure validity (i.e. does the question measure learning outcomes?) [8, 7], to decide on the optimal number of choices [15, 9], and to design good distractors [10, 7]. In an empirical study [8], Haladyna and Downing concluded that the key in multiple choice item design was  X  X ot the number of distractors but the quality of distractors. X  They find that, almost unanimously, high-quality distractors are considered to be those that represent common student misconceptions [9]. Thissen et al. [17] developed a graphical analysis method of distractors based on the response statistics in the context of a nominal item response model, with the goal of facilitating a posteriori analysis of multiple choice items. Computational methods have been proposed for the task of multiple choice item design (i.e. designing a question and its choices), but are restricted to specific domains, such as vocabulary [3], grammar testing [2], or topic-specific comprehension [13]. For all these methods, however, distractors are generated automatically based on the structure of the problem domain. We are unaware of prior results that directly optimize for a distractor choice set based on the data of past student submissions. capturing the ability of the student (larger values indicate greater ability). One can easily verify that Property 2 and Property 3 are both satisfied by considering the limiting behavior of (1) when  X  i = 0 and  X  i =  X  respectively. Prop-erty 1 is satisfied as a result of P ( i picks option j  X  | X  ) (i.e., the probability of student i picking a correct option) being a monotone function of  X  i . As a consequence, performing optimal option subset selection under this model and these constraints will result in subsets that are most informative about the students X  abilities .

It is also important to understand the limitations and additional assumptions underlying this model. The most significant limitation is what is known as the independence of irrelevant alternatives (IIA) assumption [14]. The IIA assumption is violated whenever the two options are not inherently different. For example, in the setting of reusing student responses as potential options in a test, this would occur if the two options are either completely identical or are paraphrases of each other. We leave dealing with the problem of IIA to future work.

To place our model in the context of existing work, we compare it with two closely related models: the classical Rasch model [5] and the recent model proposed by Bachrach et al. [1].
The classical dichotomous Rasch model defines the likeli-hood of a student answering a question correctly as a function of the question X  X  difficulty and the student X  ability, i.e., it is agnostic to the actual choice made by the student in an MCQ setting. The likelihood of student i with ability  X  i getting the question j with difficulty q j correct is given by:
P ( i correctly answers j |  X  i ,q j ) = 1 To gain intuition about how our model encodes question difficulty , consider the case of only two options: the correct option with parameter  X  j  X  and the incorrect option with parameter  X  j . We can now express the likelihood of the student answering this question correctly using our model as follows:
P ( i correctly answers j |  X  i ,  X  j  X   X  j ) = 1 where  X  j  X   X  j =  X   X  j  X   X  j , which is positive by definition (since  X   X  &gt;  X  j ). By analogy with the Rasch likelihood,  X   X  1 j  X   X  j captures a similar notion of question difficulty : the farther apart are the two options in the parameter space, the  X  X asier X  is the resulting question.

When the question contains more than two options, the likelihood of the student answering the question correctly can be expressed as: P ( i right on j |  X  i , {  X  j  X   X  j } j ) = 1 where an exponential term containing the distance  X  j  X   X  j between the correct option and every remaining option now appears in the denominator. Observe that the probability of getting the question right approaches one only when the cor-rect option parameter (scaled by ability  X  i ) is well-separated from every other option (distractor). An important advantage offered by modeling individual choices is that the model X  X  estimate of the students X  abilities will not only depend on Here, f (  X  ; C ) is the likelihood function in (1). It can be shown that the solution to the above is the following combinatorial optimization problem 2 : maximize subject to x n  X  X  0 , 1 } ,  X  n  X  Q where { x n } n =1 ...N are indicator variables ( x n  X  X  0 , 1 } ) that select choices from Q to be included in C , N = | Q | (i.e., the total number of potential options) and K is the maximum permissible size of C (e.g., four options). We now investigate the nature of the optimal choice sets. Consider two limiting cases: a student with a large ability (  X  i  X  X  X  ), and a student with a low ability (  X  i  X  0).
Case  X  i  X   X  : It is straightforward to show that in the limit of  X  X nfinite ability, X  the information will go to zero. However, the rate at which it goes to zero depends on the choice set, allowing us to gain insight into the kinds of choice sets that will be  X  X referred X  for users with a large ability. The logarithm of the information function will have a linear asymptote, with the slope dominated by the largest expo-nential in the numerator and the denominator. We can show that as  X   X  X  X  , only the two choices with the largest values of  X  remain relevant (i.e. {  X  max , X  max  X  1 } ), with the optimal spacing between them,  X  max  X   X  max  X  1 , given by: Clearly, the greatest Fisher information for large values of  X  will be obtained when  X  max  X  1  X   X  max , i.e., when the distance between the two top choices approaches zero.

Case  X  i  X  0 : In the limiting case of  X   X  0, the objective reduces to: where K is the number of options we seek to display to the student and k indexes over those options. The solution to the above can be obtained by choosing a subset of the choices from Q with the smallest  X  ( X  X eft-most X  or  X  X ncorrect X  choices) and a subset of choices from Q with the largest  X  ( X  X ight-most X  or  X  X orrect X  choices) (proof omitted). The intuition behind this solution requires some explanation. It is instructive to consider the optimal solution in the case of only two choices. The optimal  X  X pacing X  between the correct choice and the distractor ( X  ij ) will lie somewhere between 0 and  X  , but where exactly depends on our prior belief about the ability of the student (  X  ). An intuitive interpretation of this solution can be gained by relying on a related notion of information gain : the expected distance (KL-divergence) between the prior and the posterior (after observing the choice) on  X  (ability). Information gain exhibits the same limiting behavior: when the two choices are infinitely far apart ( X  ij  X  X  X  ), the student will always pick the correct option regardless of their ability X  X hus, the posterior will derivation ommited due to space limitation variance of the distribution over choice parameters  X  . Recall that the variance of the distribution from which we sample the choice parameters  X  is inversely related to the difficulty of the resulting question. As we discussed in Section 3.1, the question becomes  X  X asy X  (i.e., students of lower  X  will have a high probability of getting it right) when the choice parameters are  X  X pread out X  (which is achieved when the choices are sampled from a high-variance distribution). Both Figure 2(a) and Figure 2(b) indicate that (i) more choices result in better performance (higher accuracy in identifying correct answers and higher rank correlation between the true and inferred student rankings), and (ii)  X  X asier X  questions (i.e., questions whose choice parameters are sampled from a high-variance distribution) generally result in better accuracy and rank correlation.

It is worthwhile to analyze the observation that student rank-correlation (Figure 2(b)) remains the same between the  X  X asy X  and  X  X edium X  conditions, while accuracy (Figure 2(a)) drops considerably. This can be attributed to the fact that in inferring the ability parameter of a student, the model relies jointly on the parameters of every choice in the set, i.e., not only on whether the chosen option was correct. As a result, while the ordering of the top two choices may be incorrect (resulting in an incorrect prediction of the correct answer), the remaining choices still play an important role in inferring student parameters (and thus in the quality of the ranking).
We now evaluate the choice subset selection optimization objective introduced in Section 4. We again generate a sim-ulated classroom with 50 students and 50 questions 3 . In contrast to the experiment in Section 4, here we perform parameter inference sequentially after each student answers
Student and choice parameters were sampled from uniform distributions with support (0 , 1) and (0 , 100) respectively a question, simulating an adaptive testing scenario. For every question, we sample choice sets of size 2 according to three different sampling strategies: (i) random : choices are drawn uniformly at random, (ii) OPT-individual : the optimal choice set is selected for each student according to that stu-dent X  X  estimated ability parameter, and (iii) OPT-average : the optimal choice set is selected according to the average estimated ability of the student population (i.e. choice sets are identical for each student). Figure 3 compares the per-formance across the three conditions using the student rank correlation metric introduced in Section 5. On the basis of these results, we draw the following conclusions: (i) present-ing choice sets optimized using the objective introduced in Section 4 with the inferred parameters achieves significantly better rank-correlation and with fewer questions than when the choice sets are sampled randomly; (ii) optimizing choice sets based on the individual student parameters marginally improves performance over optimizing choice-sets to the av-erage ability of the student population. Note, however, that in practice the exact gains will vary depending on the nature of the student and choice parameter distributions.
We performed a real-world study to evaluate the impor-tance of data-driven choice set selection in the context of a quiz that asks users to name states of the United States. In this setting, we considered a question to be a specific state which the person is required to identify by picking a correct choice out of a set of options (other states). This problem serves as an excellent platform for evaluating our model for two reasons: 1. Ease of evaluation: The fact that the set of possible answers to each question is finite allows us to use the raw score on a question where all 50 options are presented as the  X  X round-truth X  of the user X  X  knowledge in this domain.
Any other test based on only a subset of the options (and consequently a method used to obtain the options) can be evaluated against this  X  X round-truth X  by measuring the correlation of the two scores. 2. Large range of  X  X ood X  and  X  X ad X  choices: Not all distractors in this setting are  X  X reated equal X : intuitively we should expect that some states, like those that border the correct state, to be easily mistaken for the correct answer. This provides an opportunity for a data-driven method to excel in finding  X  X ood X  choice sets for building effective questions.
Mechanical Turk workers residing in the U.S. were solicited to a task titled  X  X ow well do you know U.S. states? X , which was briefly described as a quick quiz to test one X  X  knowledge of the U.S. states, consisting of two stages: 1. Stage I (fullMCQ) : Workers are presented with a map of the U.S. with a randomly highlighted state and 50 options, one for each state, that they are required to choose from. This selection is made for every one of the 50 states, presented in random order. Workers are not revealed the correct answer, and are discouraged from looking up the answers externally. 2. Stage II (subsetMCQ) : The same workers then repeat the test, but now with only 4 options for each of the 50 Figure 4: Within-subject correlation between raw scores attained on the subsetMCQ and fullMCQ tests separated by choice set design strategy X  X hoice sets optimized according to the proposed objective yield better within-subject score correlation than choice sets sampled randomly.
We now focus on the quality of the workers X  ranking using the raw scores obtained on the subsetMCQ test between the Optimal and Random strategies. Our hypothesis is that a test designed to elicit maximum information about the worker X  X  knowledge should result in a higher quality discrimination across workers of different levels of knowledge (abilities), and thus yield a more accurate ranking of the workers. We obtain a ranking of workers by sorting everyone according to their raw score on the subsetMCQ , and as in the within-subject analysis, evaluate it against the  X  X round-truth X  ranking obtained by ordering the students by their raw score on the fullMCQ test. We compute rank correlation by sampling a random set of 50 workers and computing Kendall Tau for the Random and Optimal conditions, repeating the process for 1000 iterations and report the statistics in Figure 5.

We observe that rank correlation in the workers given a subsetMCQ test with the Optimal choice set significantly outperforms rank correlation of the workers given a sub-setMCQ test with a Random choice set ( p -value=0 by permutation test), confirming our hypothesis: a test that optimizes information about the student X  X  ability implicitly optimizes the accuracy of the ranking of the students .
One application that we explore in this paper is to the task of generating multiple choice tests from technical forum data. Technical forums, like StackExchange, Piazza and Quora, exhibit a typical structure: (i) a user posts a question on the forum, (ii) other users propose solutions by submitting answers, and (iii) users vote on what they consider to be the best answer to the original question. Forums that follow this structure provide an opportunity to apply our model for optimal question generation where choice subsets are selected from the user submissions. The potential benefit of creating assessment content dynamically from technical forums is: for a particular answer.  X  X oting users X  are modeled the same way as the users who answer multiple choice questions in our discrete choice model, i.e., their strictly positive  X  X hoosing ability X   X  appears as a coefficient of the choice correctness in parametrizing the discrete distribution over choices 4 .  X  X on-tributing users X  are endowed with an  X  X nswering ability X  pa-rameter  X  , which parameterizes the distribution over answer correctness parameters for answers contributed by that user. This allows us to share statistical strength of  X  X ood X  and  X  X ad X  answers that are created by the same users, e.g., users that contribute poor answers in general (answers that receive few upvotes) will be informative in inferring answer parameters in other questions they answered, where the voting information may be sparse.
We formalize the above model with a Bayesian generative story shown on the right. We put normal priors on the an-swer and user parameters, and a truncated-normal prior on the voter ability, to ensure non-negativity. The high-level description of the story is as follows: users with ability  X  i contribute answers to questions whose correctness  X  ij is nor-mally distributed about the creator X  X  ability, i.e., more able users are able to create higher-quality answers. Later at some time t , a voter with ability  X  k observes a set of answers C t q (to question q ) that have been created up to time t and makes a selection according to the discrete distribution parametrized by (1), where voters with greater ability are more likely to pick the best choice. We use variational message passing for inference, a deterministic approximate posterior inference unfortunately StackExchange datasets do not reveal the identity of the  X  X oters X , thus we assume that each vote is contributed by a distinct  X  X oter X  at the task of question generation. Full end-to-end evaluation of the quality and effectiveness of the generated questions will require user-studies, which we leave for future work. Figure 7 displays posteriors over answer correctness parameters for four questions, with the highlighted and annotated answers belonging to the optimal choice set, where optimality is deter-mined by the optimality criterion introduced in Section 4. As done in Section 6.1, we optimize the choice sets for an  X  X ver-age user, X  i.e., whose ability is given by the posterior mean of  X  . Finally, in selecting choice pairs, we require that the  X  X ost correct X  choice (one with the highest posterior mean) always appears in the set, making the selection problem essentially one of finding a good distractor.

The examples in Figure 7 are given with their respective forum name and a question ID, and can be viewed in more de-tail by finding them on the StackExchange site. For example, the top left question in Figure 7 (147346), can be found at: http://physics.stackexchange.com/questions/147346. Ques-tions 14736, 14609 and 776 are examples where the distractors are all plausible incorrect answers (the correct answer in ev-ery question is marked with  X  X  X ). Question 8996, however, is a common example of a generated choice set, where the distractor is also a correct answer, yet it appeared less pop-ular for another reason, e.g., it was incomplete, had little supporting evidence, or was simply not a commonly-known answer (the case for question 8996) and therefore received significantly fewer votes. In our setting, we argue that having an explicit constraint that the distractor is wrong is not necessary X  X t is sufficient if the user can tell apart the best answer from the remaining answers. However, if the dimen-sion of quality is orthogonal to correctness, e.g., if one of the answers is better phrased or contains additional illustrations, the question will not serve its purpose in differentiating those users that know the answer from those that do not. This limitation is potentially less severe in areas where the answer is constrained to be of a particular format, e.g., if the answer is computer code like in StackOverflow, where often multiple submitted answers may be correct, but only one exhibits the best performance. We leave the full study of the application of this model to test generation from technical forums for future work.
We have proposed a method for optimal choice selection for the task of optimal test design. Our response model is closely related to a discrete choice model, where the variance parameter encodes the ability of the user. This formulation, unlike related models such as [5, 1], allows us to explicitly identify optimal choice sets, where optimality is specified in terms of estimator efficiency on the user ability parameter. We have demonstrated that the resulting choice sets are selected on the basis of how easily the choices are mistaken for one another, highlighting one of the principles of multiple choice question design: good distractors must capture common misconceptions . We also look ahead to the application of this model to data-driven crowd-sourced assessment generation from technical forums, and briefly highlight challenges and potentials of this paradigm.
 The work of I. Labutov was supported in part by a grant from the John Templeton Foundation provided through the
Claim 1. [6] For any x i ,x j  X  X  0 , 1 } we have x i x j = y ij for y ij  X  { 0 , 1 } , when the following three constraints are satisfied: y ij  X  x i , y ij  X  x j and y ij  X  x i + x j  X  1 .
Proof. If x i = 0 or x j = 0, y ij has to equal 0 as well because of the first two constraints. If x i = x j = 1 then the third constraint forces y ij to be 1.

Claim 2. After every execution of the while loop of the algorithm r best is equal to the best objective value of (4) with where the superscript  X  X fter X  indicates the values at the end of the loop.

Proof. Proof by induction on the number of executions of the while loop. The base case is when the while loop is not executed yet (0 executions of the while loop). At that moment z is the maximum objective value of (5) with B =  X  . So any solution where P i,j y ij exp (  X  [  X  i +  X  j ])  X  B eff has objective value that does not exceed z . Therefore the objective of (4) (which is equal to the quotient of objective and the constraint) cannot exceed r best , under P i,j y ij exp(  X  [  X  i +  X  j ])  X  B eff .
Induction step: Suppose the claim is true after k  X  1 execu-tions of the while loop. In iteration k , z is the maximum ob-the superscript  X  X tart X  indicates the values at the start of the loop. By the same argument as above the objective of (4) can-and P i,j y ij exp (  X  [  X  i +  X  j ])  X  B . By the induction hypoth-esis r start best is equal to the best objective value of (4) with Because r after best is set to the maximum of these values, we have proved the claim as long as there is no better solution when P B eff . Note that the choice of  X  ensures that there is no P i,j y ij exp (  X  [  X  i +  X  j ]) &lt; B upper bound on the objective value of (5) at every execution of the while loop, we know that the denominator can be constrained to be at most z/r before best before we can find an improved solution.

Corollary 1. The algorithm above finds a (globally) op-timal solution to (4).
 [1] Y. Bachrach, T. Graepel, T. Minka, and J. Guiver. [2] I. Bejar. A sentence-based automated approach to the [3] J. C. Brown, G. A. Frishkoff, and M. Eskenazi. [4] C. H. Coombs. Psychological scaling without a unit of
