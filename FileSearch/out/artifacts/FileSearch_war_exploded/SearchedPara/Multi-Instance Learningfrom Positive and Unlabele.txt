 Multi-instance learning (MIL) [1] is a special type of learning task where each ob-servation contains a bag of instances. A bag is labeled positive if one or multiple instances inside the bag are positive, and negative otherwise. The uniqueness of not requiring labels for individual instances makes multi-instance learning very suitable for applications without label information for individual instances. Because the genuine positive instance(s) inside each positive bag is unknown, the main challenge of multi-instance learning is to leverage bag labels and con-straints to derive accurate classification models. Roughly, existing MIL methods [2] can be separated into the following three categories: (1) instance-based mod-eling, which finds most positive and most negative instances from bags to derive MIL models; (2) bag-based modeling, which directly builds classification models at the bag level; and (3) hybrid approaches, which use instances and bags to confine the learning space to build classification models.

For all existing MIL methods, one prerequisite is that training data must contain both positive and negative bags to derive discriminative models. In re-ality, many applications may only have positive bags to indicate users learning interests and the remaining bags are unlabeled (which may be positive, nega-tive, or irrelevant to the learning task). For example, during an image retrieval process [3], users may click one or multiple images which are interesting to them (the clicked images can be regarded as positive bags), but majority images re-main unchecked, so we do not know whether those images do not contain users retrieval concepts ( i.e. negative bags) or users simply overlook the images. In this case, there is no negative bag but only positive and unlabeled bags are available.
When only positive and unlabeled bags are available, a straightforward solu-tion for MIL is to propagate a bag X  X  label to each instance inside the bag, so the problem can be solved by using standard Positive-Unlabeled learning [4]. Indeed, this simple solution is ineffective because not all instances inside a positive bag are positive, so some instances will be mi slabeled and deteriorate the classifica-tion accuracy. A slightly more intelligent way is to use Positive and Unlabeled learning strategy [5] to first treat all unlabeled bags as negative bags and train an MIL classifier, and then iteratively refine identified negative bags by using trained MIL classifiers. This solution is still ineffective mainly because the iden-tification of negative bags only relies on the MIL classifiers but does not take the unique MI bag constraints into consideration, so directly training MIL clas-sifiers using positive bags and identified negative bags will severely deteriorate the classification accuracy.

The above observations motivate a very practical learning task where only positive and unlabeled bags are available for multi-instance learning. In this pa-per, we formulate this problem as positive and unlabeled multi-instance learning (puMIL), where the key challenge is twofold: (1) MIL learning with unreli-able bag labels: Although it is always possible to identify some unlabeled bags as negative bags, the labels of identified negative bags are unreliable. Directly building MIL classifiers from positive and identified negative bags may result in low classification accuracy. This reality calls for new MIL learning frameworks capable of handling unreliable bag labels; and (2) Tacking uncertainty in-side positive bags: For MIL, the genuine labels of instances inside a positive bag are unknown, although at least one instance has to be positive. Finding  X  X ost positive instances X  plays a significant role for multi-instance learning. In a puMIL setting, this process is further complicated because no negative bags are available to help identify positive instance(s) in a positive bag.
In this paper, we propose a self-adaptive learning framework to tackle the above challenges for puMIL. More specifically, we assign a weight value to each bag, and use an Artificial Immune System based search process to update bag weight values and identify the most reliable negative bags in each iteration. For each bag, a  X  X ost positive instance X  (for a positive bag) or a  X  X east negative instance X  (for an identified negative bag) is selected to form a positive margin pool (PMP). A weighted kernel function is used to calculate pairwise distances between instances in the PMP, with the distance matrix being used to learn a weighted support vector machines classifie r. Experiments on real-world positive and unlabeled bags confirm the effect iveness of the proposed design.
The remainder of the paper is structured as follows. Preliminary and prob-lem statement are addressed in Section 2, followed by the overall framework in Section 3. Section 4 introduces detailed algorithms, followed by experiments in Section 5. We conclude the paper in Section 6. Denote B = { B 1 ,  X  X  X  ,B n } a bag set with n bags, and B i is the i th bag in the set. The bag set contains both positive and unlabeled bags, with B + i and B u j indi-cating a positive and an unlabeled bag, re spectively (for ease of representation, we also use B i to denote a bag). Let Y =[ y 1 ,  X  X  X  ,y n ]where y i is the label of B . In generic MIL settings, a positive a nd a negative bag X  X  label can be denoted by y i =+1and y i =  X  1, respectively. In a puMIL setting, an unlabeled bag B j  X  X  label is denoted by y j = 0. The collections of positive and unlabeled bag sets can be denoted by B + and B u , respectively. During the learning process, the algorithm may identify a set of unlabeled bags as a negative bag set, which is denoted by B  X  . It is worth noting that B + contains bags which are genuinely positive, whereas B  X  contains bags which are  X  X dentified X  as negative bags.
For each bag B i , the number of instances inside the bag is denoted by n i ,and x i,j ,j =1 , 2 , has a label but cannot be directly observed, because of the bag constraint. In order to tackle unreliable bag labels in puMIL setting (challenge # 1), we use a weight value w i to indicate the label confidence of each bag B i . So for a positive bag B + i , its weight value w i is 1 (because it is genuinely positive), whereas for an identified negative bag B j , its weight value w j  X  (0 , 1], with a higher w j value indicating that B j is more likely being a negative bag.

Given a training set B containing a handful of positive B + bags and many unlabeled B u bags, puMIL learning aims to build a prediction model from B to accurately predict previously unseen bags with maximum F-score. Figure 1 illustrates the conceptual view of the proposed puMIL framework, which includes two major steps to solve key challenges identified in Section 1. Positive Margin Pool (PMP): In order to carry out multi-instance learning with positive and unlabeled bags only, we propose to use maximum margin idea [6] to build a positive margin pool (PMP) by modeling the distributions of the positive and unlabeled bags. More specifically, we utilize bag weight values to identify some unlabeled bags as most reliable negative bags. After that, we select  X  X ost positive patterns X  from positive bags and  X  X east negative patterns X  from identified negative bags to form a positive margin pool. Because PMP contains signature patterns with respect to positive and identified negative bags, it will help differentiate decision boundaries to separate positive bags.
 Self-Adaptive Bag Weight Updating: In order to properly identify most negative bags from unlabeled bags, we assign a weight value to each bag and will employ a self-adaptive iterative process to update bag weight values to find reliable negative bags. The proposed iterative bag weight updating process is based on clonal selection theory [7] in Artificial Immune Systems. It iteratively searches different weight values, including mutations and clones of weight values from the previous iteration, to find the one which optimizes the object function ( i.e. the F-score in our case). As a result, the self-adaptive weight updating can ensure high quality negative bags are identified to form positive margin pool (PMP) for multi-instance learning without negative bags. The weight updating is detailed in Section 4.3. 4.1 Optimization Framework with Unreliable Bag Labels To handle unreliable bag labels from identified negative bags, we propose to build a Positive Margin Pool (PMP) which contains  X  X ost positive patterns X  from positive bags and  X  X east negative patterns X  from identified negative bags. Our multi-instance learning is then achieved by building an optimization framework based on the PMP, under the condition that negative bags are unreliable and therefore need to be carefully handled. I n Eq. (1), we propose our objective function in handling unreliable bag labels. where x B i denotes a  X  X ost positive pattern X  or a  X  X east negative pattern X  in PMP with m denoting the number of instances in PMP. w i denotes weight confidence of the i th instance in PMP. In our design, because only one instance is selected from each bag, the weight confidence of the instance is equal to the weight confidence value of the bag. More specifically, if the instance is from a positive bag, its weight confidence w i = 1. For instances from identified negative bags, their weight values are w j  X  (0 , 1], with a higher w j value indicating that B j is more likely being a negative bag.

The cost function in Eq. (1) is the linear SVM in the primal formulation, where  X  ( x B i ,w i y i ;  X  ) is the hinge loss function used by the SVM classifier. The above primal form can be formulated into a dual version as where  X  i is the Lagrangian multiplier. Indeed, the above optimization problem defines a nonlinearly constrained nonconvex optimization task. It contains two sub-problems: (1) learning continuous variables  X  which is equivalent to finding hyper-plane  X  , and (2) selection of instances x B for PMP which is equivalent to finding optimal weight values w . To the best of our knowledge, no direct solution exists to find its global optimal. In this paper, we derive an self-adaptive approach to find optimal bag weights for unlabeled bags, and then use generic SVM optimization to solve the above optimization problem.

In the following, we first explain the construction of the PMP, and then in-troduce detailed self-adaptive process for finding optimal bag weight values. 4.2 PMP: Positive Margin Pool The main purpose of building a Positive Margin Pool is to identify some  X  X ost positive patterns X  (for positive bags) and  X  X east negative patterns X  (for possible negative bags), so PMP can help build classifiers to differentiate positive vs. negative instances for multi-instance classification. This also provides solutions to tackle unreliable bag labels obtained from unlabeled bags.

The construction of the PMP is motivated by the margin principle, which states that samples close to the decision boundary play critical roles in improv-ing the performance of the underlying classifier. In our puMIL setting, we assign a weight confidence value w i to each bag, so it can help identify unlabeled bags which are most likely being negative as  X  X eliable negative bags X  and then ex-tract most positive patterns from positive bags and least negative patterns from identified negative bags to form PMP.

According to MIL definition, a negative bag does not contain positive in-stances, and negative instances in negat ive bags can have very general distribu-tions, we use a Weighted Kernel Density Estimator [8] (WKDE) to model the distributions of negative instances in reliable negative bags as follows. where w x denotes weight of the bag to which the instance x belongs to. x  X  i,j denotes the j th instance of the i th reliable negative bag with its size to be n  X  i , and we use an isotropic Gaussian kernel K . Depending on whether the underlying bag is positive or is identified as negative, the  X  X ost positive pattern X  or  X  X east negative pattern X  x B i in PMP can be obtained by Eq.(4), with B  X  denoting the set of all identified reliable negative bags. After that, the optimization problem in Eq. (2) can be solved through stan-dard linear SVM. In other words, once weight values are fixed, a weighted SVM classifier can be trained from PMP for classification. 4.3 Self-Adaptive Bag Weight Optimization In the proposed puMIL learning framework, a weight confidence value w i is assigned to each bag to determine whet her an unlabeled bag is likely being a reliable negative bag or not. In this section, we propose a self-adaptive process to search optimal bag weight values. Because the aim of puMIL is to propose a learning framework to maximize the p erformance measure (F-score), a good weight combination should corresponds to a high F-score, which trades off the precision P and the recall R :F-score=2  X  P  X  R/ ( P + R ). Accordingly, we drive a self-adaptive strategy to obtain optimal weights based on a clone selection theory in Artificial Immune Systems.

When pathogens invade the body, antibodies that are produced from B-cells will respond for the detection of a foreig n antigen. This response process can be explained by clonal selection theory. More specifically, immune individuals with high affinity will gradually increase during the clone and mutation process. Mean-while, some immune individuals will polarize into memory individuals, which will be evolved towards final optimal. In our solution, antigen is simulated as train-ing bag set B , while the antibody, presented by confidence weight vector  X  w will experience a form of clone and mutation. The evolving optimization process will help discover optimal bag weight values w  X  which will lead to the best affinity ( i.e. F-score). The details of the optimal weight search process are described as: Algorithm 1. puMIL: Positive and Unlabeled Multi-Instance Learning 1) Initialization: GivenanMIset B with n bags, a set of weight vec-tors  X  W = {  X  w 1 ,  X  X  X  ,  X  w L } are randomly generat ed, with each vector  X  w i = &lt;  X  w B k is a positive bag,  X  w i,k is set to 1. If B k is an unlabeled bag,  X  w i,k is a random value within range (0 , 1]. It is worth noting that each weight vector  X  w i contains initial weight value for all bags. Because there are L weight vectors, each bag will have L initial weight values. 2) Antibody Clone: For each weight vector  X  w t i in the t th generation, if we use their weight values as the bag weights as defined in Eq. (1), each  X  w t i will correspond to an SVM classifier with an affinity score ( i.e. F-score) on the unlabeled bag set B u . This score allows us to assess the quality of weight vectors and use clone selection to find optimal w eight vector. During antibody clone process, the individual  X  w t c  X   X  W t with the best affinity score will be selected as the memory antibody to be further cloned. To ensure a fixed population size in every generation,  X  w t c will be cloned under the clone factor c to replace weight vectors  X  w t j  X   X  W t with low affinity under the same rate c . 3) Antibody Mutation: In order to maintain the diversity of the weight vectors, the mutation operation should be applied. Specifically, for any  X  w t i from the t th generation, the new variation individual v t +1 i can be generated as follows: Among them, N(0,1) is a normally distributed random variable within [0,1]. F =1  X  f [  X  w t i ], as the variation factor, can be adaptively obtained according to different clones [9] where f [  X  w t i ] denotes the affinity of  X  w t i . 4) Antibody Update: This process determines w hether the variation of weight vectors (from above steps (2) and (3)) can be used to replace a target weight vector in the next generation. In our algorithm, we adopt a greedy search strategy. Only if the affinity of v t +1 i is better than that of the target weight vector  X  w t i , the new weight vector is then selected as the offspring. 4.4 puMIL Framework Algorithm 1 reports the detailed process of the proposed puMIL framework, which combines the (1) PMP construction (Section 4.2); and (2) adaptive weight optimization (Section 4.3), to iterativ ely refine unlabeled bags for learning.
During the initial process, puMIL will initialize the bag weight vectors  X  W (line 1). During each while loop, puMIL will first select the  X  X eliable negative bags X  (
B weight vector  X  w t i in  X  W t .So( B  X  i ) t consists of unlabeled bags with the largest weight values in  X  w t i (line 4). After that, puMIL will form a positive margin pool for each weight vector (lines 5-6). The clone selection theory will be employed to update the weight vectors in order to find a better weight value in the next iteration (lines 7-10). The evolutionary process will repeat until (1) the algorithm surpasses the pre-set maximum number M , or (2) the same result is obtained for a number ( e.g. T ) of consecutive iterations.

After obtaining the best weight vector w  X  c , puMIL uses the discovered optimal weight values to obtain PMP, and then builds a weighted SVM classifier H .A test bag is classified as positive if one or multiple instances inside the bag are classified as positive, and negative otherwise. To evaluate the effectiveness of our puMIL framework, we use F-score as the evaluation metric (its definition is given in Section 4.3). For benchmark data sets used in the experiments, 30% of bags are randomly selected as testing set in each run, with the remaining bags being used as training set. All results are based on the average performance over 10 repet itions. Besides, the two parameters M and T in Algorithm 1 are set to 50 and 0.001, respectively.

Because there is no existing method for positive and unlabeled multi-instance learning, for comparison purposes we implement following baseline approaches from bag-and instance-level perspectives . The former directly employs Positive-Unlabeled learning [5] at the bag level, and the latter propagates bag label to instances and transfer MIL as a generic Positive-Unlabeled learning problem.
Bag-Level Approaches: (a) MILR+MISVM-MI: This method first labels all unlabeled bags as negative bags, and then uses MI learning approach MILR [10], which outputs probability estimation, to obtain a set of identified negative bags. After that, it runs MISVM [6] iteratively on the positive set and refined nega-tive set until converges; and (b) Spy+MISVM-MI: The difference between Spy based PU learning [5] on bag-level and MILR+MISVM-MI is in its initialization. Spy+MISVM-MI randomly samples a set of positive bags as  X  X pies X , and marks them as unlabeled bags. Because spies are g enuinely positive and behave similarly to unknown positive samples the unlabeled set, adding spies allows algorithm to infer characteristics of unknown positive bags in the unlabeled set. Previous stud-ies [5] on text documents have demonstrated good performance of this PU learning strategy.
 Instance-Level Approaches: (a) NB+SVM-MI . The variation of traditional PU setting in [5,11] is used for comparison. Specifically, a Naive Bayes classifier [12] is used to obtain identified negativ e instance set from unlabeled instance set. After that, an iterative process is used to train SVM classifier from positive instances and identified negative instances; and (b) Spy+SVM-MI Similar to the approaches used in [5,11], during the initialization process, Spy+SVM-MI randomly selects a set of positive insta nces as  X  X pies X , and adds them into the unlabeled set. After that it follows the same procedure as NB+SVM-MI. For ease of understanding, we also refer to MILR+MISVM-MI and NB+SVM-MI as  X  X o-spy X  based approaches, and Spy+MISVM-MI and Spy+SVM-MI are called  X  X py X  based approaches. 5.1 Drug Activity Prediction The objective of drug activity predictio n is to predict whether a drug molecule can bind well to a target protein related to certain disease states, which are primarily determined by the shape of the molecule. Our drug activity predic-tion data set, MUSK1 [1], is a benchmark used for MI learning. It contains 476 instances grouped into 92 bags (45 inactive and 47 active), with each instance being described by a 166-dimensional feature vector. In our experiments, we use r  X  100% active bags as positive bags, and the remaining active bags and all inactive bags are treated as unlabeled bags. The results for the two types of base-lines (bag-level and instance-level) and proposed puMIL with different r values, varying from 0.1 to 0.7, are reported in Figures 2(A) and 2(B), respectively.
Overall,  X  X o-spy X  based approaches achieve competitive F-score as  X  X py X  based methods, which demonstrates that simply adding  X  X py X  to unlabeled bags may not help differentiate positive vs. negative bags. Meanwhile, the proposed puMIL achieves better performances comp ared to other methods, especially for bag-level baselines. For small r values, such as r =0 . 2 or less, puMIL demon-strates very significant performance gain, compared to other baselines. This fur-ther assets that when the number of positive bags is very limited, leveraging useful information from unlabeled bags, like puMIL does, can be very useful for positive and unlabeled multi-instance learning. 5.2 Scientific Publication Retrieval The DBLP data set consists of bibliography data in computer science 1 .Each record in DBLP contains a number of attributes such as abstract, authors, ti-tle, and references [13]. To build a puM IL task, we select papers published in Artificial Intelligence field (AI: IJCAI, AAAI ,NIPS, UAI, COLT, ACL, KR, ICML, ECML and IJCNN) as positive bags and randomly select papers from other fields, such as computer vision, multimedia, pattern recognition as unla-beled bags. A  X  X ag-of-words X  representation based on TFIDF [14] is adopted to convert the abstract of a paper to an in stance. So each paper is a bag and each instance inside the bag denotes either the paper X  X  abstract or the abstract of a reference cited in the paper.

In our experiments, we choose 200 papers in total (which correspond to 200 bags), with each paper containing 1 to 10 references. For all 200 papers, the total number of references cited ( i.e. instances) is 1136. Each instance is described by a 4497-dimensional feature vector. To vary the number of positive bags, we randomly select r  X  100% of AI bags (varying from 0.1 to 0.7) as positive bag set, and the remaining AI bags and all other bags are used as unlabeled bags. In Figures 2(C) and 2(D), we report the results with respect to different r values. The results show that  X  X py X  based approaches can achieve a slightly better performance than  X  X o-spy X  versions when r is greater than 0.4. This is possibly because that when a large number of positive bags are used, the  X  X pies X  selected from the positive bags will not reduce the role of positive bags. Meanwhile, puMIL clearly outperforms all baselines, especially when only a small portion of positive bags are labeled ( i.e. r =0.1). This suggests that puMIL is effective over a wide range of percentage of labeled positive bags. 5.3 Region-Based Image Annotation In the third experiment, we report puMIL X  X  performance for automatic image annotation tasks. The original data are color images from the Corel data set [15] that have been preprocessed and segmented using Blobworld system [16]. An image contains a set of segments (or blobs), each characterized by color, texture, and shape descriptors. In this case, each image is considered as a bag, and each region inside the image denotes an instance. Some example images from the database are shown in Figure 3. In our experiment, we use category  X  X iger X  as positive bags (100 bags with 544 instances) and randomly draw 100 photos of other animals to form unlabeled bags with 676 instances. Each instance is described by a 230-dimensional feature vector which represent color, texture, and shape of the region. To validate the performance of puMIL with respect to different number of positive bags, we randomly select r  X  100%  X  X iger X  images (varying from 0.1 to 0.7) as positive bags, and combine remaining  X  X iger X  images and all other images as unlabeled bag set.

In Figures 2(E) and 2(F), we compare the performance of puMIL with two types of baselines by using different portions of positive bags. The results show that bag-level baselines can have a high F-score than instance-level methods when the percentage of positive bag is very small ( e.g. r&lt; 0 . 2). Because instance-level approaches directly assign bag labels to all instances in positive bags, for a small number of positive bags, the mislabeled instances in positive bags will have severe impact on the classifiers trained from labeled data. As a result, the clas-sifier may not be able to accurately differentiate positive vs. negative instances. However, as the r values become sufficiently large, instance-level methods demon-strate better performance than bag-level approaches. By properly utilizing in-formation in unlabeled bags, puMIL consistently outperforms all baselines for different percentages of positive bags. In this paper, we formulated a unique multi-instance learning task, which only has positive and unlabeled bags available for multi-instance learning. This prob-lem setting is more general but significantly more challenging than traditional multi-instance learning because no negative bags exist for deriving discriminative classification models. To address the challenge, we proposed a puMIL framework which self-adaptively selects some reliable negative bags (from unlabeled bags) and further selects some representative patterns from the positive bags and iden-tified negative bags to help train SVM classifiers. The classifiers will further help refine the selection of negative bags and iteratively lead to updated classifiers for classification. Our main technical contribution, compared to existing research, is threefold: (1) a general framework to handle multi-instance learning with only positive and unlabeled bags; (2) an effective algorithm to identify reliable negative bags from unlabeled bags; and (3) an effective approach for utilizing unreliable labels derived for unlabeled bags.
 Acknowledgments. The work was supported by the Key Project of the Natu-ral Science Foundation of Hubei Provi nce, China (Grant No. 2013CFA004), and the National Scholarship for Building High Level Universities, China Scholarship Council (No. 201206410056).

