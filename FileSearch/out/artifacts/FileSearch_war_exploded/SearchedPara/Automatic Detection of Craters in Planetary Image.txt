 Identifying impact craters on planetary surfaces is one fun-damental task in planetary science. In this paper, we present an embedded framework on auto-detection of craters, us-ing feature selection and boosting strategies. The paradigm aims at building a universal and practical crater detector. This methodology addresses three issues that such a tool must possess: (i) it utilizes mathematical morphology to ef-ficiently identify the regions of an image that can potentially contain craters; only those regions, defined as crater candi-dates, are the subjects of further processing; (ii) it selects Haar-like image texture features in combination with boost-ing ensemble supervised learning algorithms to accurately classify candidates into craters and non-craters; (iii) it uses transfer learning, at a minimum additional cost, to enable maintaining an accurate auto-detection of craters on new im-ages, having morphology different from what has been cap-tured by the original training set. All three aforementioned components of the detection methodology are discussed, and the entire framework is evaluated on a large test image of 37 , 500  X  56 , 250 m 2 on Mars, showing heavily cratered Mar-tian terrain characterized by nonuniform surface morphol-ogy. Our study demonstrates that this methodology pro-vides a robust and practical tool for planetary science, in terms of both detection accuracy and efficiency.
 I.5.2 [ Design Methodology ]: [Classifier design and eval-uation; Feature evaluation and selection; Pattern analysis]; I.5.4 [ Pattern Recognition ]: Applications X  Astronomy Algorithms classification, feature selection, transfer learning, spatial data mining, planetary and space science
Impact craters, the structures formed by collisions of me-teoroids with planetary surfaces, are among the most stud-ied geomorphic features in the solar system because they yield information about the past and present geological pro-cesses and provide the only tool for measuring relative ages of observed geologic formations [7, 20]. However, advances in surveying craters present in images gathered by plane-tary probes have not kept up with advances in collection of images at ever-higher spatial resolutions. As a result, there are  X  X illions X  of craters waiting to be identified in a deluge of high resolution planetary images but no means for their efficient identification and comprehensive analysis. Today, as in the past, craters are identified using manual inspection of images. As a result, comprehensive catalogs of craters are restricted to only large craters: 42 , 283 Martian craters with diameters larger than 5 km [3], and 8 , 497 named lu-nar craters with diameters larger than a few kilometers [1]. If left to manual surveys, the fraction of cataloged craters to the craters actually present in the available and forth-coming imagery data will continue to drop precipitously. Crater auto-detection techniques are needed, especially to catalog smaller craters that are most abundant. Surveying such craters is ill-suited for visual detection, due to their shear numbers, but well-suited for an automated technique. In fact, automating the process of small crater detection is the only practical solution to a comprehensive surveying of such craters.

Some challenges [14] of designing an accurate crater de-tection algorithm are as follows: (i) Craters, as a landform formation, lack strong common features distinguishing them from other landform formations. Their sizes differ by or-ders of magnitude. Their rims have often been eroded since their formation millions of years ago, resulting in shapes supervised learning. that depart significantly from circles. They frequently over-lap, complicating the task of their separation from back-ground. (ii) Planetary images are taken at different lighting conditions, at different resolutions, and their quality varies so that even morphologically identical craters may have dif-ferent appearances in different images. (iii) Appearances of other similar landform formations exist in images. For ex-ample, volcanic cones and valleys fragments may resemble craters.

A robust and useful crater detection algorithm must ad-dress these challenges. From a design point of view such an algorithm has to successfully address the following issues: (i) How to efficiently identify crater candidates X  X egions in an image that have relatively high probability of contain-ing a crater? (ii) Given a set of crater candidates, how to accurately classify them into crater and non-crater objects? An efficient approach in feature construction and selection and a well designed supervised learning approach are de-sirable to address this issue. (iii) Given a training set of crater candidates containing positive and negative examples (craters and non-craters), how to make a classifier applica-ble to other images, where candidates have a morphological character different from what is encapsulated by the train-ing set? This is the scenario where training and testing instances are not drawn independently and identically from a same underlying distribution. Transfer learning is needed to address this issue while minimizing the overall cost of classification.

Previous research on crater detection algorithms [16, 10, 6, 2, 14, 23, 25, 5, 17] (also see [19] for a complete bibliog-raphy of research on auto-detection of craters) focused pre-dominantly on addressing issue (ii). The problem of finding crater candidates has only recently been raised [22]; and the bulk of previous work relies on inefficient exhaustive search of the entire image. This may work for finding a small num-ber of large craters in low resolution images, but not for finding a very large number of small craters in high resolu-tion images. To the best of our knowledge, the problem of transfer learning in the context of auto-detection of craters was not previously addressed. This omission renders most existing approaches impractical for planetary research as the benefit of automation decreases significantly if new training sets need to be established for every new image or even for various segments of the same image.

This paper addresses all the three design issues in con-structing a robust and practical crater detection algorithm, using an embedded framework with feature selection and boosting. The ultimate goal is to construct a robust and reliable crater auto-detectio n framework that can be widely adopted for planetary research. The flow chart indicating components of our method is shown in Fig. 1. The three key contributions are as follows:  X  Utilizing mathematical morphology [21] for efficient iden-tification of crater candidates. The key insight behind our method is that a crater can be recognized as a pair of crescent-like highlight and shadow regions in an image (see Fig. 2). The benefits of identifying crater candidates before the clas-Figure 2: (A) Diagram explaining why an image of a crater consists of crescent-like highlight and shadow regions. (B) An image of an actual 1 km crater show-ing the highlight and shadow regions. sification stage, rather than classifying pixel-based image blocks resulting from exhaust ive search of the entire image are two-fold: (i) Significant computation time is reduced at the classification stage, and (ii) the number of false posi-tives detections is reduced, as most of the image, including background, is removed from being classified.  X  Using a combination of Haar-like image texture features [24] and a modified AdaBoost ensemble supervised learning algorithm. This approach yields a highly accurate classi-fier. As an alternative, we also evaluate the use of a simpler classifier (hereafter referred to as the  X  X aive X  classifier) for distinguishing between craters and non craters.  X  In order to minimize training for application of a crater detection algorithm to a heterogeneous planetary surface, we modify the basic boosting algorithm so it incorporates transfer learning to feature selection and classification.
The entire method is evaluated on a large, high resolution image of Martian surface (37 , 500  X  56 , 250 m 2 )featuringspa-tial variability of crater morphology. Experimental results demonstrate robustness and good accuracy that validates our approach and makes it feasible to embed our algorithm into a processing pipeline for auto-creation of global,  X  X il-lion crater X  catalogs of small c raters on Mars, Mercury, and the Moon.

The rest of the paper is organized as follows. Section 2 provides a brief review on crater-detection methods. Sec-tions 3 and 4.1 explain how we construct crater candidates and Haar-like texture-based features from those candidates. Sections 4.2 and 4.3 discuss the three supervised learning algorithms used for crater detection, and Section 5 presents the result of applying our methodology to find craters in the test image. Section 6 summarizes our work and discusses future directions.
Salamuniccar et al. provide a complete and exhaustive re-view of all previous research on crater detection algorithms [19]. All existing approaches to detecting craters in plane-tary images can be divided into two general categories: un-supervised approaches and supervised approaches.

The unsupervised methods identify crater rims in an im-age as circular or elliptical features [16, 10, 6, 2, 14]. In par-ticular, the original image is preprocessed [16, 2, 14] to en-hance the edges of rims, and the actual detection is achieved by means of the Hough Transform (HT) [11] or genetic al-gorithm [10]. Unsupervised methods have the advantage of being fully autonomous but they are generally less accurate than supervised methods.

The supervised methods [5, 23, 25] take advantage of do-main knowledge in the form of labeled training sets that guide the classification algorithm. In [5, 23], a continuously scalable template-model technique is used to achieve detec-tion. In [25], a number of algorithms are tested and the Sup-port Vector Machine algorithm is shown to achieve the best rate of crater detection. More recent methods [14, 17] incor-porate techniques originally developed [24] for the purpose of face detection. These methods concentrated on the classi-fication component of crater detection and did not incorpo-rate identification of crater candidates or transfer learning, as what has been designed and implemented in this paper.
In order to reduce the load on the classification module, we first identify crater candidates X  X arts of an image that contain crescent-like pairs of shadows and highlights. Iden-tification of crater candidates is achieved using an image processing method based on mathematical morphology pro-posed by Urbach et al. on object detection in [22, 21]. Fig. 3 shows a flow diagram of the method used for identifica-tion of crater candidates. The highlight and shadow shapes are processed in parallel using inverted image to process the shadow shapes. The goal is to eliminate all the shapes that are not indicative of craters while keeping the highlight and shadow shapes. Background removal deletes shapes, such as mountains, that are too large to be part of the craters; the power filter removes shapes that lack sufficient contrast; the area filter removes shapes that are too small for reliable crater detection; the shape filter uses shape attributes that are invariant to translation, rotation, and scaling [21] to pre-serve or remove regions of an image exclusively on the basis of their shapes. Utilization of the shape filter, that requires only a singe parsing of an image, improves performance by a factor of 5 to 9 in comparison with other shape detection methods [21]. In the final step, the algorithm matches high-light and shadow regions so that each pair corresponds to a single crater candidate. This method does not have high enough accuracy to constitute a stand-alone crater detection technique, but is ideal for identification of crater candidates.
The classification stage uses supervised learning to distin-guish (with high accuracy) between craters and non craters in the set of crater candidates. The objects of classification are image blocks containing crater candidates and the classi-fication is performed on the basis of Haar-like image texture features.
We use image texture features reminiscent of Haar basis functions first proposed in [18] for detection of objects and later popularized by [24] in the context of face detection. These features can be thought of as image masks consisting of black and white sectors. The value of a feature is the difference between the sum of gray scale values in pixels located within the white sectors and the black sectors. We use nine types of features (all squares) as shown in Fig. 4. All features can be calculated very efficiently in one image scanning using the concept of integral image [24].
To represent a crater candidate in terms of Haar-like fea-tures, we first extract square image blocks around each crater candidate having size twice that of the candidate. For ex-amples of image blocks and features positioned over them see Fig. 8. The underlying texture information of each crater candidate is encoded in the set of nine features, hav-ing various granularities and positioned at finely sampled locations. Thus an image containing a crater candidate and its immediate surrounding is described by thousands of tex-ture features. Those features are not independent from each other and those over-complete features compensate the lim-Figure 4: 9 rectangle features: (A) 2 two-rectangle features, (B) 2 three-rectangle features, (C) 5 four-rectangle features. ited texture information a single rectangle feature can cap-ture. If a single simple feature can be viewed as a weak learner, that is, only using this feature to classify a crater candidate, it is a natural choice to build a strong classifier out of thousands of weak learners, using the boosting en-semble method.
To classify crater candidates into craters and non craters on the basis of texture features, we have designed and im-plemented two supervised learning algorithms. These algo-rithms simultaneously select sub-set features necessarily for accurate classification and train the final ensemble classifier based on the supplied training set. The first is the Boost algorithm, a variant of the AdaBoost algorithm inspired by the methodology of face detection [24]. The second is the Naive algorithm X  X  drastic simplification of the Boost algo-rithm without boosting iterations. The two algorithms are described in Algorithms 1 and 2.

The Boost algorithm generates a sequence of weak classi-fiers h t ( f ) and combine them through a weighted approach to build a strong classifier H ( x ): where T is the number of iterations, t =1 ,...,T , x = &lt; f ,...,f N &gt; is the feature vector that describes a crater candidate; f , f  X  X  f 1 ,...,f N } , is the single feature used to construct a weak classifier h t ( f ), and  X  t is the weight of hypothesis h t ( f ). The Boost algorithm (See Algorithm 1) iteratively selects one feature at a time and stops when reaching T iterations; note that T&lt;&lt;N . Each iteration selects one best feature and consists of three core steps: 1. Weak Classifier Learning: The construction of weak 2. Feature Selection: Calculate the weighted error sum of 3. Weight Updating: Update weights using the same method The number of craters usually is less than the number of non-craters. The initial weight of each training instances is designed to cope with imbalance data by using different group average weights in the positive and negative classes, respectively.
 Algorithm 1 Boost: A boosting algorithm for feature se-lection and classification Require: . 1: for t =1 ...T do 2: Normalize the weight, w t,i = w t,i n 3: Select the best weak classifier with respect to the 4: Define h t ( x )= h ( x, f t ,p t , X  t ), where x, f t 5: Update the weights: 6: The final strong classifier is: 7: end for
In order to reduce the computational cost of the Boost algorithm, we design a simplified greedy version of the algo-rithm and call it the Naive algorithm (see Algorithm 2). The Naive classifier uses the same weak classifier learning step and selects the top T best features using the weighted error sum in the step of feature selection as a criterion without any further iterations on weight updating .
The time complexity of the Boost algorithm is O ( TNn ), where n is the number of training examples, N is the number of total features, and T is the number of boosting iterations. In particular, each feature produces n weak classifiers, based on each feature value for every training example according to the threshold  X  ; N features produce Nn classifiers; it takes O ( Nn ) time to find the weak classifier that produces the minimum error; and it takes O ( TNn ) time to select the top T features after T boosting iterations.

The time complexity of the Naive algorithm is O ( Nn ) as no boosting iterations are p erformed. Interestingly, the Naive classifier performs well in some circumstances during our real-world case study (see Section 5).
 Algorithm 2 Naive: A naive greedy algorithm for feature selection and classification Require: . 1: Normalize the weight, w t,i = w t,i n 2: Select the best t ( t =1 ,...,T ) weak classifiers with 3: Define h t ( x )= h ( x, f t ,p t , X  t )where x, f t ,p 5: The final strong classifier is:
Boost and Naive assume that both training and testing instances are drawn independently and identically from a same underlying distribution. What if training and test in-stances are from different distributions? We have designed a transfer learning based algorithm, inspired by [8], which is capable of transferring knowledge from the old training data to the new test data. We refer to it as the TL algo-rithm. The TL algorithm (see Algorithm 3) has the same three steps as the Boost algorithm, but the weight updating step is different as it attempts to transfer knowledge from the original training set to the new test data. The Boost al-gorithm assumes that both, training and test instances are drawn independently and identically from an underlying dis-tribution and it is not expected to perform well if the test data has a different distribution from the training data; this is because the critical set of features that best serves to dis-tinguish craters in the training set may not be the same as that in the test set.

We denote the previous original training data as the diff-distribution training data; here we are uncertain about the similarity and usefulness of this data for the new task. And we denote the additional small portion of labeled test data, which is representative of the new set of crater candidates, as the same-distribution training data. During the train-ing process, we apply the Boost algorithm to the same-distribution training data to build a model; the weights of misclassified examples are increased during the next itera-tion while the weights of correctly classified examples are decreased. The key component is that we transfer knowl-edge from the old training data to the new test data by modifying the weights of misclassified examples from the diff-distribution training data. Those misclassified exam-ples are considered as the ones that are dissimilar to the same-distribution examples and should be de-emphasized. Accordingly, we decrease (not increase) the weights of those examples in order to weaken their impact. The weight-changing mechanism selects good examples (similar to the labeled test data) from the old training data to compensate the insufficient training examples in the same-distribution data.

The difference between the TL algorithm and the exist-ing algorithm TrAdaBoost [8] is that we use embedded ap-proach in feature selection. In our method, we select the best feature in each iteration while constructing a strong classifier sequentially. The key contribution of the algo-rithm is that some features contribute more in the new test data and should be transferred and emphasized, while some features provide less or no contributions at all and thus should be de-emphasized. The sub-set features best discriminates craters and non -craters in old training set is not necessarily the same sub-set features in a new unseen test set. The change of weight factor  X  = 1 1+  X  2 classified examples from diff-distribution and the threshold classifier are to assure that the average training loss on the diff-distribution converges to zero [8, 9]. We have selected a portion of the High Resolution Stereo Camera (HRSC) nadir panchromatic image h0905 [12], taken by the Mars Express spacecraft, to serve as the test set. As illustrated in Fig. 10, the selected image has the resolution of 12 . 5 meters/pixel and the size of 3 , 000 by 4 , 500 pixels (37 , 500  X  56 , 250 m 2 ). A domain expert manually marked  X  3 , 500 craters in this image to be used as the ground truth to which the results of auto-detection are compared. The image represents a significant challenge to automatic crater detection algorithms because it covers terrain having spa-tially variable morphology and because its contrast is rather poor (this is most noticeable when the image is inspected at a small spatial scale). We divide the image into three sec-tions denoted as the west region, the central region, and the east region (see Fig. 10). The central region is characterized by surface morphology that is distinct from the rest of the image. The west and east regions have similar morphology but the west region is much more heavily cratered than the east region.
In the first stage of our method, we identify 13,075 crater candidates in the image using the pipeline depicted in Fig. 3. The data set is imbalanced as the majority objects are non-crater candidates. 1,089 Haar-like features are constructed using the 9 rectangle features described in Fig. 4. The training set for the Boost and Naive algorithms consists of Algorithm 3 TL: A boosting algorithm using transfer learning for feature selection and classification Require: . 1: for t =1 ...T do 2: Normalize the weight, w t,i = w t,i n 3: Select the best weak classifier with respect to the 4: Define h t ( x )= h ( x, f t ,p t , X  t )where x, f t ,p 5: Update the weights: 6: The final strong classifier is: 7: end for 204 true craters and 292 non-crater examples selected ran-domly from amongst crater candidates located in the north-ern half of the east region. Thus, the training set uses only 3.75% of the total data set. Note that we have purposely restricted the locations of examples in a training set to a specific sector of the image in order to mimic actual plane-tary research; it is likely that i n current studies such craters are identified in a specific region and are in need of iden-tification by a supervised learning algorithm in the rest of the image. For the TL algorithm, we have constructed an additional training set (same-distribution set) consisting of 253 crater candidates (102 true craters and 153 non-craters) selected from random locations throughout the entire im-age. The ratio between the false and true examples in the same-distribution data is proportional to that in the diff-distribution data ( 153 102 292 204 ). The original training set consisting of 496 examples from the northeastern section of the image serves as the diff-distribution set.
The table shown in Fig. 5 summarizes the performance results of crater detection by the three algorithms: Boost, Naive, and TL. The ground truth of the entire image serves as an external criterion to evaluate the performance of the three algorithms on the unseen test set. Of the three algo-rithms, the number of features used to construct a strong classifier and the values of the threshold  X  are selected to maximize the performance of each classifier.

The candidate data has imbalanced class distribution and the successful detection of tru e craters is more significant than the detection of non-craters. Hence we use recall ( r = TP + FN ) and precision ( p = ation metrics. TP stands for the number of true positive detections (detected craters that are actual craters), FP stands for the number of false p ositive detections (detected craters that are not), and FN stands for the number of false negative  X  X etections X  (non-detection of real craters). F1 measures the harmonic mean between precision and re-and the best performance of each measure is highlighted in bold. A precision score of 1.0 means that every object clas-sified as a crater is indeed a crater but says nothing about the number of craters that are not recognized by classifiers as such. A recall score of 1.0 means that every true crater is classified as such but says nothing about how many other landforms were incorrectly cla ssified as craters. An F1 score scores of the three algorithms. (Best viewed in color.) of 1.0 means that all the existing craters are correctly identi-fied and all the objects classifie d as craters are true craters.
The TL classifier yields the best precision in all regions and the Naive classifier yields the worst precision in all re-gions. On the other hand, the Naive classifier has the highest recall in all regions whereas the TL classifier has the lowest value of recall, except in the east region, where the Boost classier has the lowest value of recall. Overall, the TL clas-sifier has the highest value of F1 in all regions except the west region where the Naive classifier has the highest value of F1.
We also tested three representative algorithms for the pur-pose of comparative study: AdaBoost [9] with C4.5 as the base leaner is used as an example of boosting algorithms, SVM [4, 13] with a linear kernel is used as an example of kernel-based learning algorithms, and TrAdaBoost [8] with C4.5 as the base leaner is used an example of transfer learn-ing algorithms. Using all the 1089 features, the F1 score of SVM on all regions is 0.202, AdaBoost is 0.302, TrAdaBoost is slightly better than 0.4. The huge performance gain by the three algorithms (Boost, Naive, and TL) has been achieved by the effective integration of the feature-selection with su-pervised learning.
In order to better understand the results of three proposed algorithms, it is useful to assess dissimilarity between the set of features vectors in the original training set and those in the west, central, and east regions. Fig. 6A shows such dis-similarity as measured by the Kullback-Leibler divergence [15]; Fig. 6B plots the F1 scores graphically of 3 regions. Clearly, the central region is most dissimilar to the train-ing set, whereas the east region is the most similar (since the training set was selected f rom the northeas tern portion of the image). This is why the TL classifier performs best (relatively to other classifiers) in the central region. It is ex-pected that the TL classifier would have the least advantage in the east region, as it is the region best characterized by the training set, but the results shows that the TL classifier has the smallest gain (if any) in the west region. This can be explained by the fact that the west region has a similar char-acter to the east region, but is much more heavily cratered, so in fact, relatively fewer additional training samples come from these regions resulting in no sufficient information gain to be exploited by the TL classifier.

The Naive classifier performs surprisingly well considering its simple nature and low computational cost. We took an in-depth look into the performance of the Boost and Naive classifiers on the northeastern section of the image contain-ing 1406 crater candidates of which 496 constitute a training set for both algorithms. Fig. 7 shows the precision, recall, and F1 for these classifiers as a function of the number of features selected to construct a strong classifier. The Boost classifier clearly outperforms the Naive classifier on F1 and precision measures if more than 100 features are selected. However, the recall measures of the two classifiers remain comparable regardless of the number of selected features. Thus, the Boost classifier is superior to the Naive classifier on crater candidates that closely resemble those in the train-ing set, but that disadvantage decreases and/or disappears when classifying crater candidates that are less similar to those in the training set. We link the relatively small ad-vantage (or lack of advantage) of the Boost classifier over the naive classifier to the peculiarity of image texture features in the context of crater detection. Top features (weak clas-sifiers) are actually quite strong performers by themselves capable of achieving an F1 score as high as 0.81. These features limit the advantage of the boosting algorithm that works best with an ensemble of weak classifiers.
It is instructive to compare to p features (weak classifiers) selected by each of the three classification algorithms (Naive, Boost, and TL). Fig. 8 shows six top features selected by each algorithm. The top two features selected by the three algorithms concentrate on the transition between the shadow and the highlight which best define the characteris-tics of a cater, but there are significant differences between other selected top features. Features selected by the Naive algorithm are relatively strong by themselves. Most of them utilize the transition between the shadow and the highlight to distinguish craters from no craters. While the next best feature selected by the Boost algorithm always attempts to correct mistakes done by the p revious feature. Fig. 9 il-lustrates how the second best feature selected by the Boost algorithm corrects the mistakes by the first best feature, and we can observe that this feature performs well on candidates with shifted shadow regions. Not all top features selected by the TL algorithm utilize the transition between shadows and highlights, but rather crater rims. This indicates the new test data has different characteristics on crater edges.
Fig. 10 displays the results of the TL algorithm, using top 150 features and the threshold  X  =0 . 500. Notice that the large craters  X  5000  X  meter in diameter are intentionally not detected as we set the parameters of our algorithm to target small sub-kilometer craters (large craters on Mars have already been identified manually [3]).

Our methodology is relatively fast. Using a machine with 2 CPUs, 3.16 GHZ, 8GB RAM it took 360 seconds to gener-ate crater candidates in the test image. It took, on average, 5 seconds to classify 13,075 candidates using a pre-trained classifier. The total computing time is around 365 3000  X  4500 microseconds per pixel.
The aim of this paper is to present a robust and prac-tical framework for auto-detection of small craters in high resolution images of planetary surfaces. This is one of the most challenging problems in planetary science X  X ffective and automatic crater detection from extremely large or-biter images. The framework uses an innovative method that integrates improved techniques on three key compo-nents: identification of crater candidates, embedding feature selection with supervised classification, and transfer learn-ing. First, we have demonstrated that our method identifies craters with high accuracy. The test site is an HRSC im-age of Martian scene that presents a heterogeneous region of 37 , 500  X  56 , 250 m 2 and craters in various forms which are challenging for detection using regular algorithms. Our method can achieve an F1 score of 0 . 851; an algorithm with such performance can definitively be used in planetary re-search. Second, we have demonstrated that a consistently accurate detection can be achieved with a minimum cost through transfer lea rning. Without tra nsfer learning the performance of our algorithms (Boost and Naive) decreases in the central region of the image where surface morphology differs as characterized by the training set. However, using the TL algorithm partially restores the level of performance. Third, we noticed that the Naive algorithm can perform well examples using the first best feature.
 color.) Green: True detections, Red: False detections. in the context of crater detection for a fraction of the cost of the Boost algorithm.

We contend that the robustness and practicality of our methodology make its utilization in planetary research likely. If adopted, our method has great potential to produce sur-veys of small craters over entire surfaces of planets, thus rev-olutionizing certain aspects of planetary science. Our future research will address means of efficient selection of additional training samples for construction of the same-distribution for transfer learning. The goal is to intelligently select sam-ples that exemplify differences between the existing training sets and new candidate sets. The work is partially supported by NASA grant NNX09AK86G. The authors would like to thank Dr. Xin-dong Wu of the University of Vermont for informative discus-sions and valuable feedback and comments, and Dr. Qiang Yang of the Hong Kong University of Science and Technol-ogy for constructive advice on transfer learning. [1] L. E. Andersson and E. A. Whitaker. NASA catalog of [2] L. Bandeira, J. Saraiva, and P. Pina. Impact crater [3] N. G. Barlow. Crater size-frequency distributions and [4] B.E.Boser,I.M.Guyon,andV.N.Vapnik.A [5] M. C. Burl, T. Stough, W. Colwell, E. B. Bierhaus, [6] Y.Cheng,A.E.Johnson,L.H.Matthies,andC.F.
 [7] Crater Analysis Techniques Working Group. Standard [8] W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for [9] Y. Freund and R. Schapire. A decision-theoretic [10] R. Honda, Y. Iijima, and O. Konishi. Mining of [11] P. C. Hough V. Method and means for recognizing [12] HRSC Data Browser. [13] T. Joachims. Learning to Classify Text using Support [14] J. Kim, J.-P. Muller, S. Van Gasselt, J. Morley, and [15] S. Kullback and R. Leibler. On information and [16] B. Leroy, G. Medioni, and E. J. L. Matthies. Crater [17] R. Martins, P. Pina, J. Marques, and M. Silveira. [18] C. Papageorgiou, M. Oren, and T. Poggio. A general [19] G. Salamuniccar and S. Loncaric. Open framework for [20] K. L. Tanaka. The stratigraphy of Mars. Journal of [21] E.R.Urbach,J.B.T.M.Roerdink,andM.H.F.
 [22] E. R. Urbach and T. F. Stepinski. Automatic detection [23] T. Vinogradova, M. Burl, and E. Mjolsness. Training [24] P. Viola and M. J. Jones. Robust real-time face [25] P. Wetzler, R. Honda, B. Enke, W. Merline,
