 Undirected graphical models, such as Mark ov netw orks or log-linear models, have been used in an ever-gro wing variety of applications, including computer vision, natural language, computational biology , and more. Ho we ver, as this modeling frame work is used in increasingly more comple x and less well-understood domains, the problem of selecting from the exponentially lar ge space of possible netw ork structures becomes of great importance. Including all of the possibly rele vant interactions in the model generally leads to overtting, and can also lead to dif culties in running inference over the netw ork. Moreo ver, learning a  X good X  structure can be an important task in its own right, as it can pro vide insight about the underlying structure in the domain.

Unfortunately , the problem of learning Mark ov netw orks remains a challenge. The key dif -culty is that the maximum lik elihood (ML) parameters of these netw orks have no analytic closed form; nding these parameters requires an iterati ve procedure (such as conjugate gradient [15 ] or BFGS [5]), where each iteration runs inference over the current model. This type of procedure is computationally expensi ve even for models where inference is tractable. The problem of structure learning is considerably harder . The dominant type of solution to this problem uses greedy local heuristic search, which incrementally modies the model by adding and possibly deleting features. One approach [6, 14] adds features so as to greedily impro ve the model lik elihood; once a feature is added, it is never remo ved. As the feature addition step is heuristic and greedy , this can lead to the inclusion of unnecessary features, and thereby to overly comple x structures and overtting. An alternati ve approach [1, 7] explicitly searches over the space of low-tree width models, but the utility of such models in practice is unclear; indeed, hand-designed models for real-w orld problems generally do not have low tree-width. Moreo ver, in all of the greedy heuristic search methods, the learned netw ork is (at best) a local optimum of a penalized lik elihood score.

In this paper , we propose a dif ferent approach for learning the structure of a log-linear graphi-cal model (or a Mark ov netw ork). Rather than vie wing it as a combinatorial search problem, we embed the structure selection step within the problem of parameter estimation: features that have weight zero (in log-space) are degenerate, and do not induce dependencies between the variables the y involv e. To appropriately bias the model towards sparsity , we use a technique that has become features, man y of which may be irrele vant. It has been long kno wn [21 ] that using L over the model parameters  X  optimizing a joint objecti ve that trades off t to data with the sum of the absolute values of the parameters  X  tends to lead to sparse models, where man y weights have value 0. More recently , Dudik et al. (2004) sho wed that density estimation in log-linear models using L ber of features of the log-linear model; Ng (2004) sho ws a similar result for L lar ge number of irrele vant ones. Other recent work proposes effecti ve algorithms for L tion in log-linear models encoding natural language grammars [19 ].

Surprisingly , the use of L learning in general Mark ov netw orks. In this paper , we explore this approach, and discuss issues that are important to its effecti ve application to lar ge problems. A key point is that, for a given L based model score and given candidate feature set F , we have a x ed con vex optimization problem that admits a unique optimal solution. Due to the properties of the L impractical to simply initialize the model to include all possible features: exact inference in such a model is almost invariably intractable, and approximate inference methods such as loop y belief propagation [17 ] are lik ely to give highly inaccurate estimates of the gradient, leading to poorly learned models. Thus, we propose an algorithm schema that gradually introduces features into the model, and lets the L explore the use of dif ferent approaches for feature introduction, one based on the gain-based method of Della Pietra, Della Pietra and Laf ferty [6] and one on the grafting method of Perkins, Lack er and Theiler [18 ]. We pro vide a sound termination condition for the algorithm based on the criterion proposed by Perkins et al. [18 ]; given correct estimates of the gradient, this algorithm is guaranteed to terminate only at the unique global optimum, for any reasonable feature introduction method.
We test our method on synthetic data generated from kno wn MRFs and on two real-w orld tasks: modeling the joint distrib ution of pix el values in the MNIST data [12 ], and modeling the joint distrib ution of genetic sequence variations  X  single-nucleotide polymorphisms (SNPs)  X  in the human HapMap data [3]. Our results sho w that L and pro vides an effecti ve method for learning MRF structure even in lar ge, comple x domains. We focus our presentation on the frame work of log-linear models, which forms a con venient basis for a discussion of learning. Let X = f X log-linear model is a compact representation of a probability distrib ution over assignments to X . The log-linear model is dened in terms of a set of feature functions f that denes a numerical value for each assignment x feature functions F = f f The overall distrib ution is then dened as: P ( x ) = 1 assignment to X normalized (so that all entries sum to 1). Note that this denition of features encompasses both  X standard X  features that relate unobserv ed netw ork variables (e.g., the part of speech of a word in a sentence) to observ ed elements in the data (e.g., the word itself), and structural features that encode the interaction between hidden variables in the model. A log-linear model induces a Mark ov network over X , where there is an edge between every pair of variables X in some feature f features in the obvious way. Con versely , every Mark ov netw ork can be encoded as a log-linear model by dening a feature which is an indicator function for every assignment of variables x clique X propagation [17 , 16], operate on the graph structure of the Mark ov netw ork.

The standard learning problem for MRFs is formulated as follo ws. We are given a set of IID X . Our goal is to output a log-linear model M over X , which consists of a set of features F and a parameter vector that species a weight for each f
The log-lik elihood function log P ( D j M ) has the follo wing form: where f is the vector where all of these aggre gate features have been arranged in the same order as the parameter vector , and &gt; f ( D ) is a vector dot-product operation. There is no closed-form solution using numerical optimization procedures such as conjugate gradient [15 ] or BFGS [5]. The gradient of the log-lik elihood is: the empirical data equal to their expected counts relati ve to the learned model. Note that, to compute the expected feature counts, we must perform inference relati ve to the current model. This inference step must be performed at every iteration of the gradient process. We formulate our structure learning problem as follo ws. We assume that there is a (possibly very lar ge) set of features F , from which we wish to select a subset F F for inclusion in the model. This problem is generally solv ed using a heuristic search over the combinatorial space of possible feature subsets. Our approach addresses it as a search over the possible parameter vectors 2 IR jF j .
Specically , rather than optimizing the log-lik elihood itself, we introduce a Laplacian parame-ter prior for each feature f Q lik elihood. Taking the logarithm and eliminating constant terms, we obtain the follo wing objecti ve: In most cases, the same prior is used for all features, so we have is con vex, and can be optimized efciently using methods such as conjugate gradient or BFGS, can simply optimize this objecti ve to obtain its globally optimal parameter assignment.
The objecti ve of Eq. (3) should be contrasted with the one obtained for the more standard param-eter prior used for log-linear models: the mean-zero Gaussian prior P ( gaussian prior induces a regularization term that is quadratic in much more than smaller ones. Con versely , L thereby forcing parameters to 0 . Ov erall, it is kno wn that, empirically , optimizing an L objecti ve leads to a sparse representation with a relati ve small number of non-zero parameters.
Aside from this intuiti ve argument, recent theoretical results also pro vide a formal justication for the use of L a relati vely small number of samples. Building directly on the results of Dudik et al. (2004), we can sho w the follo wing result: Cor ollary 3.1 : Let X = f X be a distrib ution. Let F be the set of indicator featur es over all subsets of variables X X of car dinality at most c , and ; ; B &gt; 0 . Let be the par ameterization over F that optimizes For a data set D , let ^ k = = p c ln(2 nd= ) = (2 m ) IID instances from P of size we have that:
In words, using the L at most worse than the log-lik elihood of the optimal Mark ov netw ork in this class whose L is at most B . The number of samples required gro ws logarithmically in the number of nodes in the netw ork, and polynomially in B . The dependence on B is quite natural, indicating that more samples are required to learn netw orks containing more  X strong X  interactions. Note, howe ver, that if we bound the magnitude of each potential in the Mark ov netw ork, then B = O (( nd ) c ) , so that a polynomial number of data instances suf ces. The abo ve discussion implicitly assumed that we can nd the global optimum of Eq. (3) by simple con vex optimization. Ho we ver, we cannot simply include all of the features in the model in adv ance, and use only parameter optimization to prune away the irrele vant ones. Recall that computing the gradient requires performing inference in the resulting model. If we have too man y features, the model may be too densely connected to allo w effecti ve inference. Ev en approximate inference algorithms, such as belief propagation, tend to degrade as the density of the netw ork increases; for example, BP algorithms are less lik ely to con verge, and the answers the y return are typically much less accurate. Thus, our approach also contains a feature introduction component, which gradually selects features to add into the model, allo wing the optimization process to search for the optimal values for their parameters. More precisely , our algorithm maintains a set of active featur es F F . An inacti ve feature f changed when optimizing the objecti ve Eq. (3).

In addition to various simple baseline methods, we explore two feature introduction methods, both of which are greedy and myopic, in that the y compute some heuristic estimate of the lik ely benet to be gained from introducing a single feature into the acti ve set.

The grafting procedure of Perkins et al. [18 ], which was developed for feature selection in stan-dard classication tasks, selects features based on the gradient of these parameters: We rst optimize deri vative of the objecti ve Eq. (3) relati ve to A more conserv ative estimate is obtained from the gain-based method of Della Pietra et al. [6]. This method was designed for the log-lik elihood objecti ve. It begins by optimizing the parameters gain of adding that feature, assuming that we could optimize its feature weight arbitrarily , but that the weights of all other features are held constant. It then introduces the feature with the greatest gain. Della Pietra et al. sho w that the gain is a conca ve objecti ve that can be computed efciently closed-form solution for the gain. Our task is to compute not the optimal gain in log-lik elihood, but from the log-lik elihood in only a linear term, is also a conca ve function that can be optimized using line search. Moreo ver, for the case of binary-v alued features, we can also pro vide a closed-form solution for the gain. The change in the objecti ve function for introducing a feature f where M is the number of training instances. If we tak e the deri vative of the abo ve function and set it to zero, we also get a closed form solution:
Both methods are heuristic, in that the y consider only the potential gain of adding a single feature in isolation, assuming all other weights are held constant. Ho we ver, the grafting method is more the gain-based approach also considers, intuiti vely , how far one can go in that direction before the gain  X peaks out X . The gain based heuristic is, in fact, a lower bound on the actual gain obtained from cost (except in the limited cases where a closed-form solution can be found).

As observ ed by Perkins et al. [18 ], the use of the L a sound stopping criterion for any incremental feature-induction algorithm. If we have that, for every inacti ve f as the overall objecti ve is a conca ve function, it has a unique global maximum. Hence, once the termination condition is achie ved, we are guaranteed that we are at the local maximum, regar dless of the featur e intr oduction method used . Thus, assuming the algorithm is run until the con vergence criterion is satised, there is no impact of the feature introduction heuristic on the nal outcome, but only on the computational comple xity .

Finally , constantly evaluating all of the non-acti ve candidate features can be computationally prohibiti ve when man y features are possible. Ev en in pairwise Mark ov netw orks, when the number of nodes is lar ge, a quadratic number of candidate edges can become unmanageable. In this case, we must generally pre-select a smaller set of candidate features, and ignore the others entirely . One very natural method for pre-selecting edges is to train an L classiers. This approach is similar to the work of Wainwright et al. [22 ] (done in parallel with our work), who proposed the use of L Mark ov netw ork structure. All of the steps in the abo ve algorithm rely on the use of inference for computing key quantities: The gradient is needed for the parameter optimization, for the grafting method, and for the termination condition, and the expression for the gradient requires the computation of mar ginal probabilities relati ve to our current model. Similarly , the computation of the gain also requires inference. As we discussed abo ve, in most of the netw orks that are useful models for real applications, exact inference is intractable. Therefore, we must resort to approximate inference, which results in errors in the gradient. While man y approximate inference methods have been proposed, one of the most commonly used is the general class of loopy belief propa gation (BP) algorithms [17 , 16, 24]. The use of an approximate inference algorithm such as BP raises several important points.

One important question issue relates to the computation of the gradient or the gain for features that are currently inacti ve. The belief propagation algorithm, when executed on a particular netw ork with a set of acti ve features F , creates a cluster for every subset of variables X a feature f in the gradient of the objecti ve (see Eq. (2)). Ho we ver, for features f inacti ve, there is no corresponding cluster in the induced Mark ov netw ork, and hence, in most cases, the necessary mar ginal probabilities over X mar ginal probability by extracting a subtree of the calibrated loop y graph that contains all of the variables in X in that all of the belief potentials must agree [23 ]. Thus, we can vie w the subtree as a calibrated clique tree, and use standard dynamic programming methods over the tree (see, e.g.. [4]) to extract an approximate joint distrib ution over X cluster graphs, but approximate otherwise, and that the choice of tree is not obvious, and affects the accurac y of the answers.

A second key issue is that the performance of BP algorithms generally degrades signicantly as typically much less accurate. Moreo ver, non-con vergen ce of the inference is more common when the for some theoretical results supporting this empirical phenomenon. Thus, it is important to keep the model amenable to approximate inference, and thereby continue to impro ve, for as long as possi-ble. This observ ation has two important consequences. First, while dif ferent feature introduction schemes achie ve the same results when using exact inference, their outcomes can vary greatly when using approximate inference, due to dif ferences in the structure of the netw orks arising during the learning process. Thus, as we shall see, better feature introduction methods, which introduce the more rele vant features rst, work much better in practice. Second, in order to keep the inference feasible for as long as possible, we utilize an annealing schedule for the regularization parameter , beginning with lar ge values of , leading to greater sparsication of the structure, and then gradually reducing , allo wing additional (weak er) features to be introduced. This method allo ws a greater part of the learning to be executed with a more rob ust model. In our experiments, we focus on binary pairwise Mark ov networks , where each feature function is an indicator function for a certain assignment to a pair of nodes. As computing the exact log-our evaluation metric on the learned netw ork. To calculate CMLL, we rst divide the variables into P calculate the average CMLL when observing only one group and hiding the rest. Note that the CMLL is dened only with respect to the mar ginals (but not the global partition function Z ( ) ), which are empirically thought to be more accurate.

We considered three feature induction schemes: (a) Gain : based on the estimated change of gain, (b) Grad : using grafting and (c) Simple : based on pairwise similarity . Under the Simple scheme, the score of a pairwise feature between X For each scheme, we varied the regularization method: (a) None : no regularization, (b) L1 : L regularization and (c) L2 : L L1 and None. Moreo ver, we used only Grad for L2, because L closed form solution for the approximate gain.
 Experiments on Synthetically Generated Data. We generated synthetic data through Gibbs sampling on a synthetic netw ork. A netw ork structure with N nodes was generated by treating each possible edge as a Bernoulli random variable and sampling the edges. We chose the parameter of Bernoulli distrib ution so that each node had K neighbors on average. In order to analyze the dependence of the performance on the size and connecti vity of a netw ork, we varied N and K . We compare our algorithm using L in three dif ferent ways. Figure 1 summarizes our results on this data sets, and includes information about the synthetic netw orks used for each experiment. The method labeled 'True' simply learns the parameters given the true model. In Figure 1(a), we measure performance using CMLL and reconstruction error as the number of training examples increases. As expected, L1 produces the biggest impro vement when the number of training instances is small, whereas L2 and None are more prone to overtting. This effect is much more pronounced when measuring the Hamming distance, the number of disagreeing edges between the learned structure and the true structure. The gure sho ws that L2 and None learn man y spurious edges. Not surprisingly , L1 sho ws sparser distrib ution on the weights, thereby it has smaller number of edges with non-ne gligible weights; the structures from None and L2 tend to have man y edges with small values. In Figure 1(b), we plot performance as a function of the density of the synthetic netw ork. As the synthetic netw ork gets denser , L1 increasingly outperforms the other algorithms. This may be because as the graph gets more dense, each node is indirectly correlated with more other nodes. Therefore, the feature induction algorithm is more lik ely to introduce an spurious edge, which L1 may later remo ve, whereas None and L2 do Figure 1(c) sho ws that the computational cost of learning the structure of the netw ork using Gain-L1 not much more than that of learning the parameters alone. Moreo ver, L1 increasingly outperforms other regularization methods as the number of nodes increases.
 Experiments on MNIST Digit Dataset. Mo ving to real data, we applied our algorithm to hand-written digits. The MNIST training set consists of 32 32 binary images of handwritten digits. In order to speed up inference and learning, we resized the image to 16 16. We trained the model where each pix el is a variable for each digit separately , using a training set consisting of 189 X 195 images per digit. For each digit, we used 50 images as training instances and the remainder as test instances.

Figure 2(a) compares CMLL of the dif ferent methods. To save space, we sho w the digits on which 5) and highest (digit 0), as well as the average performance. As mentioned earlier , the performance of the regularized algorithm should be insensiti ve to the feature induction method, assuming infer -ence is exact. Ho we ver, in practice, because inference is approximate, an induction algorithm that introduces spurious features will affect the quality of inference, and therefore the performance of the algorithm. This effect is substantiated by the poor performance of the Simple-L1 and Simple-L2 methods that introduce features based on mutual information rather than gradient (Grad-) or approximate gain (Gain-). Ne vertheless L1 still outperforms None and L2, regardless the feature induction algorithm with which it is paired. Figure 2(b) sho ws a visualization of the MRF learned when modeling digits 4 and 7. Of course, one would expect man y short-range interactions, such as the associati vity between neighboring pix els, and the algorithm does indeed capture these relation-the algorithm picks up long-range interactions, which presumably allo w the algorithm to model the variations in the size and shape of hand-written digits.
 Experiments on Human Genetic Variation Data. The Human HapMap data set 1 represents the genetic variation over human indi viduals. Six data sets contain the genotype values over 614-1,052 genetic mark ers (SNPs) from 120 indi viduals. For each data set, we learned the structure of the Mark ov netw ork whose nodes are binary valued SNPs such that it captures the structure of the human genetic variation. Figure 2(c) compares CMLLs among three methods for these data sets. For all data sets, L1 sho ws better performance than L2 and None. We have presented a simple and effecti ve method for learning the structure of Mark ov netw orks. We vie w the structure learning problem as an L solv ed using con vex optimization techniques. We sho w that the computational cost of our method is not considerably greater than pure parameter estimation for a x ed structure, suggesting that MRF structure learning is a feasible option for man y applications.
There are some important directions in which our work can be extended. Currently , our method handles each feature in the log-linear model independently , with no attempt to bias the learning towards sparsity in the structur e of the induced Mark ov network . We can extend our approach to introduce such a bias by using a variant of L together , such as the block-L
From a theoretical perspecti ve, it would be interesting to sho w that, at the lar ge sample limit, redundant features are eventually eliminated, so that the learning eventually con verges to a minimal structure consistent with the underlying distrib ution. Similar results were sho wn by Donoho [8], and can perhaps be adapted to this case.

A key limiting factor in MRF learning, and in our approach, is the fact that it requires inference over the model. While our experiments suggest that approximate inference is a viable solution, as the netw ork structure becomes dense, its performance does degrade, especially as the approximate gradient does not always mo ve the parameters to 0, diminishing the sparsifying effect of the L regularization, and rendering the inference even less precise. It would be interesting to explore inference methods whose goal is correctly estimating the direction (even if not the magnitude) of the gradient.

Finally , it would be interesting to explore the viability of the learned netw ork structures in real-world applications, both for density estimation and for kno wledge disco very , for example, in the conte xt of the HapMap data.

