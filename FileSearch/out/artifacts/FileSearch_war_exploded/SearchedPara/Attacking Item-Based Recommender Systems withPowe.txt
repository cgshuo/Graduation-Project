 Recommender Systems (RS) are vulnerable to attack by malicious users who intend to bias the recommendations for their own benefit. Research in this area has developed attack models, detection meth-ods, and mitigation schemes to understand and protect against such attacks. For Collaborative Filtering RSs, model-based approaches such as item-based and matrix-factorization were found to be more robust to many types of attack. Advice in designing for system ro-bustness has thus been to employ model-based approaches. Our recent work with the Power User Attack (PUA), however, deter-mined that attackers disguised as influential users can successfully attack (from the attacker X  X  viewpoint) SVD-based recommenders, as well as user-based. But item-based systems remained robust to the PUA. In this paper we investigate a new, complementary attack model, the Power Item Attack (PIA), that uses influential items to successfully attack RSs. We show that the PIA is able to impact not only user-based and SVD-based recommenders but also the hereto-fore highly robust item-based approach, using a novel multi-target attack vector.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering Recommender Systems; Power Item; Attacks; Evaluation
Recommender systems help users decide which products and services to buy by making use of preference information provided by the community of system users. Unfortunately, these systems are subject to attack by malicious self-interested users who enter fake information to either promote their own products ( X  X ush X ), disparage their competition ( X  X uke X ), or to create havoc in the rec-ommender [13, 8, 11]. Commercial system operators do not tend to disclose detailed information about attacks on their systems. But we know that real attacks on recommender systems are not uncom-mon, and they take on a variety of forms, the most popular of which is providing fake reviews known as opinion spam. 1 The problem with attacks on ratings-based Collaborative Filtering (CF) Recom-mender Systems (RS) is that predictions and recommendations can become biased in accordance with the type of attack perpetrated. In turn, attacks can potentially corrupt the system dataset and cause users to distrust the information and results provided by the sys-tem. In the absence of live attack data from service providers, re-search on RS attacks has focused primarily on similarity-based sta-tistical models of user ratings, such as random and average hypo-thetical users [13, 8, 11]. Additionally, attack detection techniques have been developed based on these models of rating behavior, e.g., [11, 9, 18, 7]. Overall, model-based approaches such as item-based and matrix-factorization strategies were found to be more robust to many types of attack. The conventional advice in designing for sys-tem robustness has thus been to employ model-based approaches [12]. Because attackers continue to develop new approaches for bi-asing RS results, it is critically important for researchers, system designers and operators to keep pace in analyzing and understand-ing robustness characteristics and potential attack vectors.
We have previously studied a novel category of RS attacks based explicitly on measures of influence, in particular the potential im-pact of high-influence, or power users [19]. Power users in the RS context are those that are able to influence the largest group of RS users; influence is indicated by the ability of power user (positively or negatively) the RS prediction for another user for power user i  X  X  target item to appear in user j  X  X  top-N list. To be clear, the power user attack in our research is not about having many actual power users collude to mount an attack, rather, it is about being able to generate a set of synthetic power user profiles that, when entered into a RS database, can effectively bias the rec-ommendations. We found that Power User Attacks (PUAs) are able to successfully impact SVD-based and user-based recommenders [16, 19, 20]; we also confirmed previous research [8, 11, 20] that item-based systems are fairly robust to attack.

In order to successfully attack (from the attacker X  X  viewpoint) the item-based algorithm, we turned our attention to the complemen-tary notion of influential power items . Selected in the same manner as power users, we conjectured whether power items would exhibit the same type of influence found with power users. Therefore, the overall question for this research is, Can a Power Item Attack suc-cessfully impact item-based recommenders as measured with Hit Ratio, Prediction Shift, and Rank robustness metrics?
For example see, http://www.reuters.com/article/2013/09/23/us-fake-reviewers-idUSBRE98M0YU20130923
See  X  6 for description of robustness metrics. This paper presents our definition of power items and the power item attack model, as well as a series of experiments conducted to determine how well the Power Item Attack (PIA) is able to impact the traditional item-based algorithm [15].
Attacking RSs by entering false ratings has been termed a pro-file injection attack [11] or shilling attack [8]. Burke et al pro-vide a summary overview of RS attack models, attack detection, and algorithm robustness [2]. Most of the attack research has tar-geted the use of similarity-focused attack models that generate syn-thetic attack user profiles using random or average item ratings or a variant of these two approaches [8, 11]. Average attacks focus on attack coverage by broad similarity to all users or to a segment of RS users, and were shown to be more effective than Random attacks. Using these similarity-focused attack models, user-based systems were shown to be vulnerable to attack while item-based systems were more robust to this type of attack [8, 11]. Previous work on RS attacks has also indicated that recommenders using matrix factorization/SVD-based algorithms are robust to attack [9, 10]. However, this is only the case when attackers have been de-tected and removed from the recommendation process. None of these models used explicit measures of influence for coverage.
Our research has investigated using explicit measures of influ-ence to create attack models based on the notion of power users [19]. We used well-known concepts from Social Network Analy-sis, i.e., Degree Centrality [17], and applied them to recommender systems. Furthermore, research showed that collaborative relation-ships in recommender systems can be represented as a social net-work [14]. In earlier work, we defined the Power User Attack model as a set of power user profiles with biased ratings that influence the results presented to other users [19]. The PUA consists of one or more user profiles containing item ratings (called attack user pro-files) that push or nuke a specific item. The PUA demonstrated that influential users can impact recommendations for user-based and SVD-based systems; to a much lesser extent, item-based systems can also be impacted [19, 16, 20]. These attacks were successful because power users are able to correlate with many non-power users to impact the target item ratings.

Based on successful results with power users, we turn our atten-tion to the complementary notion of power items , wherein attackers methodically select certain items to include in their attack user pro-files that can be used to effectively influence other users and items in the RS. Power item identification has not been widely examined in the context of RS attacks, so our initial effort will use the same influence-based methods we used to select power users (see  X  3). Certain attack models, such as the Bandwagon, Segment, and Av-erage over Popular (AOP) attacks [1, 11, 7], specified the use of popular items (those with many ratings) in the attack user profiles to correlate with selected groups of users. While these attacks were successful (from the attacker X  X  viewpoint) against user-based algo-rithms, only the Segment attack was found to be effective against a small subset of users using an item-based recommender. How-ever, none of these attack models were successful against the full complement of users in the dataset. The item-based algorithm has proven to be robust against these attacks and this remains an open challenge in RS robustness research that we explore in this study.
To select power items our initial study employs the same meth-ods we used previously for power user selection [20]. We believe this is sound for similarity-based methods because the similarity calculations between items are symmetric to those between users. The methods are as follows: InDegree or ID: Our approach is based on in-degree centrality [17], where power items participate in the highest number of simi-larity neighborhoods. For each item i compute similarity with every item j applying significance weighting n cij / 50 , where number of users that have rated the same items i and j , then discard all but the top-N neighbors for each item i . 3 Count the number of similarity scores for each item j (# neighborhoods item j select the top-N item j  X  X .
 Aggregated Similarity (AggSim or AS): Analogous to the user-based Most Central heuristic from [4]. The top-N items with the highest aggregate similarity scores become the selected set of power items. This method requires at least 5 users who have rated the same item i and item j ; this method does not use significance weighting. Number of Ratings (NumRatings or NR): Power users were de-fined in [6] as users with the highest number of item ratings, thus the analog for power items would be those items with the highest number of user ratings. Therefore, we select the top-N items based on the total number of user ratings they have in their profile. Items selected by this method are also referred to as popular items in the context of Bandwagon, Segment, and AOP attacks [1, 11, 7].
When evaluating power item selection methods, there are attack dimensions such as cost and knowledge required that should be considered [8, 2]. The cost to mount an attack is controllable by the attacker and relates to the effort required to yield the desired out-come; the objective is to keep the cost low. The more knowledge an attacker has about the dataset X  X  users, items, and ratings, the more effective the attack; however, that knowledge is difficult, albeit not impossible, to obtain. We note here that the knowledge required for the NumRatings method can be considerably lower than InDegree or AggSim because popular items are usually well known and are publicly-available information; this may give NumRatings an edge over the other selection methods, costs being equal.

Although PIA detection is beyond the scope of this paper, we should note that detailing the Power Item Model ( X  4) and the meth-ods for selecting power items ( X  3) provides the basic information required for detection analysis.
We have developed a Power Item Model (PIM) that can be used to generate synthetic power item profiles (SPIP) for attack pur-poses. Unlike classic attack models (e.g., random, average, band-wagon) that employ straightforward statistical templates (e.g., av-erage item rating, popularity, and likability) to generate synthetic attack profile filler items [11], very little is known about the char-acteristics of power items. And without this knowledge, it is diffi-cult to build attack user profiles. So, for the PIA, our initial work uses influence-based methods to select power items ( X  3) and we set other attack user profile elements in the SPIP according to more traditional attack models.

To describe the PIM, we use the specification framework from [11]. The attack user profile elements consist of the following: Selected items ( I S ) have particular characteristics determined by the attacker. For the PIM, these are the power items and they are items that are likely to correlate with many user profiles in the sys-tem. The selected item size, or the number of items in each profile, is an experimental design parameter and is usually expressed as a percentage of the total number of items in the dataset. A larger size
We used a divisor of 50 users as an analog to work done with co-rated items in user neighborhoods by [5] to optimize RS accuracy.
Based on personal communication with the authors. may have more impact, however, it is also more easily detectable. Previous work [11] has shown that a 5-10% profile size should be sufficient to have an impact on recommendation robustness. lection is based on the methods described in  X  3. The I S value for each of these items in the profile is selected randomly from a normal distribution around the mean and standard deviation of the item X  X  rating in the dataset. Our intent was for SPIP X  X  to have a rating profile that was strong rather than just randomly assigning rating values. We used a normal distribution because this has been typical in RS attack research. [8, 11].
 Filler items ( I F ) are usually set randomly according a normal dis-tribution and are used to establish correlations with other users in the dataset. For the PIM, this set is empty because we wanted a strong correlation between the selected items I S and the target item I ; we believe that having filler items would tend to confound or dilute this relationship.
 Unrated items ( I U ) are the items exclusive of the I S , and have null values in the PIM.
 Target item ( I T ) is usually a single item that is typically set to the maximum r max or minimum r min rating depending on the at-tack intent (push or nuke). Our initial experiment in this study con-sisted of a single target item attack (PIA-ST) in keeping with tra-ditional attack models; our subsequent experiments (2 and 3) used the novel multiple target item attack (PIA-MT) on the item-based algorithm. The selection of the target item is also a key part of the attack model. We experiment with  X  X ew X  items (those with only one rating) because this is a typical scenario in which power users are asked to provide ratings and because items with few ratings are more vulnerable to attack; we also use a mix of  X  X ew and estab-lished X  items for subsequent experiments.
 Other factors in building effective RS attacks include [8, 11]: Attack size , the number of attack user profiles to be injected. A larger attack size may be more effective, however, it is more easily detectable. The attack size or number of profiles is an experimental design parameter and is usually expressed as a percentage of the total number of user profiles in the dataset. Previous work [11] has shown that a 5-10% attack size should be sufficient to have an im-pact on recommendation robustness. We vary the attack size for our experiments to understand the scope of impact.
 Attack intent , for a typical 1-5 rating scale, 5 is used for push at-tacks and 1 for nuke attacks. In this study, we focus on push attacks, leaving nuke attacks for future work.

Therefore, to generate a set of SPIP X  X  for a given PIA, we specify the following elements: The push version of the PIA-ST is similar to the Bandwagon, Segment, and AOP attacks, when the power item selection method is based on the Number of Ratings method, as described in  X  3. However, these attack models differ primarily in the contents of I
F and I S as shown in Table 1. Furthermore, the PIA-MT differs radically from previously studied attacks using popular items, not only in the profile contents shown in Table 1 but also in that the
Bandwagon ratings set to r max ratings set with normal
Segment Segment items, Random items, Average x-% Popular Items, Over Empty ratings set with normal Popular dist around item mean
Power Item ratings set with normal Empty PIA-MT uses multiple targets simultaneously rather than a single target item in order to mount the attack.

The PIM approach goes beyond prior research primarily in two areas: first, we utilize influence-based methods ( X  3) to select the power items for the attack user profile, and second, we utilize mul-tiple rather than just single target items. We believe that this combi-nation can yield powerful attacks, especially against the item-based algorithm that has been resistant to attack in the past [8, 11].
We conducted a series of three experiments to address our main research question  X  whether the PIA could have a substantial im-pact on item-based recommenders. First, to see whether the PIA had traction as an attack vector overall, which it did. Second, to see whether a multiple-target variant would have a greater impact on item-based approaches, which it did. And third, to see whether the multiple-target PIA could have an impact on both new and estab-lished items, which it can. The line of experimentation was to find a PIA approach that was more successful in attacking item-based recommenders than previous research [11] had indicated. Experiment 1: Consists of the PIA with a number of  X  X ew X  (low # ratings) item targets pushed one at a time and averaged over all target items. We call this the PIA Single Target (PIA-ST) attack because we are, in effect, attacking the recommender with a sin-gle target item. The objective of this experiment is to determine the effectiveness of the PIA against various recommender algorithms and to compare with the results we obtained with the PUA against similar recommenders.
 Experiment 2: Although it is easy to envision an attacker with an intent to promote a single item, e.g., a book they just published, it is also possible for an attacker to have several items to attack at once in order to promote (or disparage) a group of products as opposed to only one product. This experiment consists of the PIA with mul-tiple  X  X ew item X  targets all pushed at the same time and is called the PIA Multiple Target (PIA-MT). The objective of this experiment is to test how well the power item approach can significantly impact item-based systems, above and beyond previously-observed results by further exploiting item-item similarities in the SPIP X  X . Experiment 3: A question that also needs to be answered is whether the PIA can still be effective when using a mix of new and estab-lished target items rather than just new items. This experiment con-sists of the PIA with multiple  X  X ew and established item X  targets all pushed at the same time and is another variation of the PIA-MT. The objective of this experiment is to determine how well the PIA-MT is able to impact recommendations for a mix of new and established items. Based on our research question, we note two hypotheses: H1: A PIA with relatively small number of SPIP X  X  ( &lt; = users) can have significant effects on RS predictions and top-N lists of recommendations, measured with robustness metrics. For Exper-iments 1 and 2 that use new items as targets, we expect Hit Ratio to be &gt; 50% and Rank &lt; 20 to qualify as significant impacts. For Hit Ratio, a majority of users ( &gt; 50%) should have target items in their top-N lists. In our experiments we use a top-N value of 40 for Hit Ratio calculations based on the analysis in [8] that the median recommendation search ends within the first 40 items displayed. Therefore, a Rank of 20 would be well within the median search. Since there is no precedent for measuring a PIA that uses new and established items as targets, for Experiment 3 we used values based on the  X  X ll-users X  Hit Ratio and Prediction Shift results for the Seg-ment attack against the item-based systems [1, 11], i.e, Hit Ratio 11% and Prediction Shift &gt; 0.1.
 H2: SPIP X  X  identified using the InDegree power user selection method will have a higher level of impact, compared to SPIP X  X  identified using NumRatings or AggSim, on RS predictions and top-N recom-mendation lists as measured with Hit Ratio and Rank. This hypoth-esis is based on the findings from Social Network Analysis [17] that high InDegree centrality is indicative of nodes (users) that have strong influence over other users. Evaluation Metrics : Evaluations were performed before and after the attacks using the Apache Mahout 0.8 platform 5 . For robustness metrics [11, 2], we use Hit Ratio (HR), Average HR ( HR ), Predic-tion Shift (PS), Average PS ( PS ), Rank (R), and Average R ( For example, a high Hit Ratio and a low Rank indicates that the at-tack was successful (from the attacker X  X  standpoint). Since we are using multiple targets simultaneously in Experiments 2 and 3, the interpretation of Hit Ratio is changed from its traditional meaning, i.e., HR is now the percentage of users that have at least one of the multiple target items in their top-N list. We also defined a new met-ric, Number of Targets per User (NTPU), associated with Hit Ratio that provides the average number of target items present in a user X  X  top-N list of recommendations. This metric provides a measure of the effectiveness of a multiple-item attack, a higher NTPU meaning higher attack effectiveness, and is averaged over all users with hits (target items in their top-N lists). For a test run T , let of users, UH T the set of users with hits, and IT T the set of target items; and let R u be the set of top-N recommendations for user u . If the target item appears in R u for user u , the scoring function H ui has value 1; otherwise it is zero. NTPU for a user u is given by within and between experiments, a normalized NTPU or NNTPU is calculated using average Hit Ratio as the normalizing factor. So, for a given test run T , NNTPU T = HR T  X  NTPU T . Since the PIA X  X  being evaluated for Experiments 1 and 2 are for  X  X ew X  items, i.e., items with one rating, the Prediction Shift is expected to be close to r max of 5.
 Datasets and Algorithms : We used MovieLens 6 ML100K 7 , ML1M and ML10M 9 datasets. The RS algorithms used were provided in http://mahout.apache.org http://www.grouplens.org nominal 100,000 ratings, 1,682 movies, and 943 users. nominal 1,000,209 ratings, 3,883 movies, 6,040 users. nominal 10,000,054 ratings, 10,676 movies, 69,878 users.
MovieLens 100K: % of Dataset 1, 5 1, 5, 10 1, 5 # Attackers, Items 10, 50 17, 83, 166 10, 50
MovieLens 1M: % of Dataset 0.1, 1 0.1, 1, 10 0.5, 1, 3 # Attackers, Items 6, 60 4, 37, 368 18, 37, 110
MovieLens 10M: % of Dataset 0.1, 1 0.1, 1, 10 0.5, 1 # Attackers, Items 70, 699 11,107,1068 50, 100 Apache Mahout and customized for this study. The CF user-based weighted algorithm (UBW) [3] uses Pearson similarity with a thresh-old of 0.0 (positive correlation), neighborhood size of 50, and sig-nificance weighting of n/50 where n is the number of co-rated items [5]. The item-based weighted algorithm (IBW) [15] uses Adjusted Cosine similarity with a threshold of 0.0 and significance weight-ing of n/50. For the SVD-based algorithm (SVD), we used Rat-ingStochasticGradientDescent (RSGD); run-time parameter settings were number of features (=100) and number of training steps or iterations (=50) and were determined empirically to optimize rec-ommender accuracy.
 Attack User Profiles : To mount the Power Item Attack, attack user profiles were generated as described in  X  4 and converted to attack profiles by setting target items to the Attack Intent.
 Power Item Selection : Methods used for power item selection are described in  X  3.
 Target Item Selection : For Experiments 1 and 2, we used  X  X ew X  items, i.e., target items with only one rating were selected randomly from the corresponding dataset. Experiment 3 used  X  X ew and estab-lished X  items, i.e., target items were selected randomly and had the following average number of ratings, average rating, and average rating entropy, respectively: ML100K (73.78, 3.13, 1.77), ML1M (253.40, 3.26, 1.81), Ml10M (675.76, 3.15, 1.71).
 Attack Parameter Selection : The Attack Intent is Push, i.e., target item rating is set to max (= 5). The Attack Size or number of power users in each attack was varied for these experiments; the (number of power items) and the number of target items used were also varied as shown in Table 2. The Attack profiles were gener-ated as described in  X  4 and the target item rating was injected at run time.
 Test Variations : For all three experiments, we used all three power item selection methods ( X  3). For Experiment 1 we used UBW, IBW, and SVD algorithms, ML100K and ML1M datasets, and sin-gle new target items. For Experiments 2 and 3 we focused on the IBW algorithm and used all three datasets. Experiment 2 used new multiple target items and Experiment 3 used new and established multiple target items.
Single target item attacks have been used in the past [11, 2] to eliminate confounds between the selected/filler items (that are used to correlate with other users) and the target item. This is especially important for user-based recommenders because user-user similar-ities with multiple target items would form neighborhoods of users that have similar tastes not only with selected/filler items but also with the multiple target items, effectively reducing the focus of the attack. To successfully attack item-based systems, prior research showed that item-item similarities can be manipulated; e.g., this was demonstrated in the Bandwagon and Segment attacks [1, 11].
For this experiment we select 50 target items from the ML100K dataset that only have one rating, with the intent of attacking a  X  X ew X  item. We calculate impacts on robustness metrics (see  X  6) for each target item individually and then average the results over all 50 targets. We repeat this calculation for three levels of power items (17, 83, and 166) and two levels of SPIP X  X  (10 and 50) for each of the three power item selection methods (InDegree, Num-Ratings, and AggSim) and using each of the three recommender algorithms (UBW, IBW, SVD). The Hit Ratio results for ML100K are shown in Figure 1. For the case with 50 attackers or 5% of user base (top of Figure 1), both InDegree and NumRatings show strong HR results using 166 power items for UBW and SVD (70% to 75% for UBW and 96% for SVD) and significantly weaker re-sults for IBW (21% to 22%). AggSim shows strong results for SVD (96%) and very weak results for UBW and IBW ( &lt; 5%). Results for R (not shown) indicate little variation across the power item selection methods and average as follows: 3.0 for UBW, 14.6 for IBW, and 2.0 for SVD. And results for PS (not shown) also indi-cate little variation across the power item selection methods and are at a higher level ( &gt; 4) because of the  X  X ew X  item targets. For the case with 10 attackers or 1% of user base (bottom of Figure 1), we observe similar results against IBW and SVD as well as a signifi-cantly weaker attack against UBW.

To see whether these results would scale, we repeated a simi-lar experiment using the ML1M dataset for two levels of power items (37 and 368) and two levels of SPIP X  X  (6 and 60) for each of the three power item selection methods (InDegree, NumRatings, and AggSim) and using each of the three recommender algorithms (UBW, IBW, SVD). Results for this ML1M attack (not shown) us-ing 60 attackers (1% of user base) and each with 368 power items (10% of all items), are similar to those obtained for ML100K with 10 attackers (also 1% of user base), i.e., a weak attack for UBW (high of 21% to 36% HR ) and IBW (13% to 19% HR ), and a strong attack for SVD (81% to 98% HR ). This would indicate that more attackers are required for a stronger attack. Results for age as follows over all power item selection methods: 6.7 for UBW, 15.4 for IBW, and 5.5 for SVD.

Overall, these results indicate that under a specific set of condi-tions (e.g., using 50 attackers and 166 power items for ML100K), the PIA is effective (high HR ,low R ) against the UBW and SVD algorithms. We also found that the PIA is not very effective against the IBW, regardless of the test conditions. While we had hoped to see a larger impact on IBW using the PIA, our results are consistent with previous findings (including our PUA) [8, 11, 19], showing that the item-based algorithm is resistant or robust to attack. Hy-pothesis H1 is accepted for both UBW and SVD recommenders, meaning that a relatively small number of power users (5% or less of the user base on a given dataset) can have significant effects on RS predictions and top-N lists of recommendations regardless of power user selection method. IBW is partially accepted because &lt; 20, however, HR does not meet the 50% requirement. Hypoth-esis H2 is rejected for all three algorithms. Although the InDegree and NumRatings perform well at a high level, NumRatings is a slightly better method for selecting power items, i.e., simply insert-ing popular items into SPIP X  X  creates very effective attacks against some recommender systems (UBW and SVD in our experiment).
The motivation for Experiment 2 was to develop a PIA model that had higher impacts on IBW than had been previously observed. Intuitively, we expect for carefully configured single-item attacks such as Average, Bandwagon, and Segment attacks [8, 11, 2] to be effective against user-based algorithms because of the similarity correlations established between the selected and filler items of the attacker profiles and the corresponding items in the profiles of non-attackers in the dataset. Once that strong correlation is made (by the algorithm), then the correlation between the selected/filler items and the target item allows the algorithm to calculate a higher pre-diction value for the target item which is then recommended to the non-attacker. Previous results indicate that larger attack and filler sizes create stronger attacks and research has shown that these at-tack models consistently impact user-based systems with impunity [8, 11, 2]. The item-based algorithm, however, establishes similar-ity correlations between the selected/filler items and the target item of the attacker profiles that are then used to calculate recommen-dations for non-attackers. The Segment attack [11, 1] was success-ful against the item-based algorithm to the extent that it impacted users who belonged to a particular segment of the user base (e.g., the  X  X orror X  movie crowd), however, this attack did not have a high impact over the entire user base. We believe that to mount a stronger attack against item-based systems, two elements are required in the attack user profile. First, the set of selected items must correlate with a broad cross-section of the user base and second, multiple target items must be used to establish strong correlations with the selected items. Experiment 2 takes on this challenge.

We recognize that because of multiple target items, there can be impacts to the robustness metrics, i.e., the HR for a single tar-get item will be different due to confounding when a target item is grouped with multiple other target items during the similarity and prediction calculation process. An analysis of this situation was performed and we found that a metric such as Hit Ratio decreases slightly for any given target item as the number of multiple target items in the SPIP increase. For example, for a set of attacks using ML100K and IBW, we found that the HR for a single target item across all users decreased from 0.225 to 0.208 to 0.184 going from 1 to 10 to 50 targets, respectively. However, at the same time, the HR across all target items and users increased from 22% to 70%, so the confounding effect for IBW does not present a major issue for the PIA.
 Figure 2: ML100K / ML10M  X  Experiment 2 Hit Ratio Results
The effectiveness of Experiment 2 was measured using robust-ness metrics (see  X  6). For each dataset used in this experiment, we select a specified number of target items that only had one rating with the intent of attacking  X  X ew X  items. Those target items are in-jected into the dataset at one time and then HR , R , and over all targets are calculated. This process is repeated for three levels of power items, up to three levels of SPIP X  X  for each of the three power item selection methods (InDegree, NumRatings, and AggSim), and using only the IBW recommender algorithm. See Table 2 for parameter settings. The ML100K HR results shown in the upper chart of Figure 2 indicate some interesting charac-teristics for this type of attack. InDegree and NumRatings show strong HR values (80% to 90%) when attack profiles used 166 and 83 power items and 50 target items, while AggSim impacts were weaker (15% to 34%) for the same number of power items and tar-get items. Average Hit Ratio is sensitive to the number of power items and target items, and somewhat insensitive to number of at-tackers; i.e., the PIA can be effective with a small number of attack user profiles . NumRatings shows the least amount of this sensitivity across the number of power items and target items, i.e., 10 attack-ers, each with 17 power items and 10 target items (a total of 270 ratings) impacts over 60% of the user base with 100,000 ratings. To see if these results scaled, we also ran this experiment on ML1M and ML10M. For the ML1M dataset (not shown), we ob-Figure 3: ML100K / ML10M  X  Experiment 2 Normalized Num-ber of Targets Per User (NNTPU) Results served a similar set of characteristics in the results. InDegree and NumRatings continue to show strong HR results while AggSim results are much weaker. And a NumRatings attack with 6 attack-ers, each with 4 power items and 37 target items (a total of 888 ratings) impacts 40% of the user base with a million ratings. For the ML10M dataset, results are shown in the lower chart of Fig-ure 2. InDegree and NumRatings continue to show strong HR sults while AggSim results are much weaker. And a NumRatings attack with 70 attackers, each with 11 power items and 50 target items (a total of 38,500 ratings or 0.4% of the total number of rat-ings) impacts 64% of the user base with ten million ratings. Aver-age Rank for each of these cases was also calculated: for ML100K, R varied from 9 to 19 (mean 15.9); for ML1M, R varied from 14 to 19 (mean 16); and for ML10M, R varied from 10 to 20 (mean 16). To compare the attack effectiveness within and between datasets in the experiment, the NNTPU metric is shown in Figure 3. The high-est number of targets per user occurs with the SPIP X  X  containing 11 power items and 100 target items generated using the NumRat-ings method for ML10M; a close second would be SPIP X  X  contain-ing 107 power items generated using the InDegree method. For all three datasets, the results for PS (not shown) indicate little vari-ation across the power item selection methods and are at a higher level ( &gt; 4) because of the  X  X ew X  item targets. To further confirm our results, we also ran a complete set of baseline PIAs across all datasets, attack sizes, and power item levels for IBW without any target items . The robustness metrics were all zero, meaning that in-jecting SPIP X  X  without any target item ratings had no effect on the RS recommendations.

Most notable is that the HR results exceed Hit Ratio measure-ments reported previously for attacks against item-based recom-menders, including the Segment attack. We conclude that the use of power items and multiple (new) target items in the SPIPs has resulted in a powerful attack against the item-based algorithm. Hy-pothesis H1 is accepted for the higher levels of attack size and num-ber of targets for all power item selection methods and all three datasets. Hypothesis H2 is partially accepted for the IBW algo-rithm. Although the InDegree and NumRatings both perform well at a high level, NumRatings is a slightly better method for select-ing power items, especially at lower levels of power items; both methods are superior to AggSim.

Figure 4: ML100K / ML1M  X  Experiment 3 Hit Ratio Results
In general, the robustness results for this experiment were lower than Experiment 2; this was expected since new items are more vulnerable to attack than established items. For each dataset used in this experiment, we select a specified number of target items to obtain a mix of items with a range of  X  X ge X  based on number of ratings. The attack and calculation processes described for Experi-ment 2 are used again here. See Table 2 for parameter settings. For the ML100K dataset, we added a third level of SPIP X  X  with 100 tar-get items (10% of the total number of items) to compare with the three levels of target items in ML1M and to observe the impacts resulting from adding more target items to the SPIP X  X . The sults shown in the upper chart of Figure 4 indicate sensitivity to the number of target items and insensitivity to number of attackers and power items. A similar pattern can be observed for the ML1M dataset shown in the lower chart of Figure 4; this is also the case for ML10M (not shown) except for the sensitivity to the number of power items for NumRatings and InDegree. For higher numbers of target items, ML100K and ML1M show strong HR results across all power item selection methods; for ML10M, NumRatings and In-Degree still have a slight edge (40% to 50%) over AggSim (31%) Figure 5: ML100K and ML1M  X  Experiment 3 Normalized Number of Targets Per User (NNTPU) Results although not quite as substantial as in Experiments 1 and 2. Aver-age Rank for each of these cases was also calculated: for ML100K, R varied from 17 to 21 (mean 19.6); for ML1M, 18 to 23 (mean 20.6); and for ML10M, 19 to 21 (mean 20.3).

To compare the attack effectiveness between Experiments 2 and 3, we used the NNTPU metric shown in Figure 5 for ML100K and ML1M. The results confirm that attacks in Experiment 2 had more impact than those in Experiment 3. For example, for ML100K and 50 target items, Experiment 2 had NNTPU values between 4.5 and 11 for NumRatings and InDegree, AggSim had values between 0 and 2. Experiment 3 had NNTPU values between 1.3 and 2.1 for all three selection methods. An interesting result for ML100K is that NNTPU displays a phenomenon similar to one reported in previ-ous work, i.e., as the number of power items increases, the attack effectiveness decreases (see upper chart in Figure 5); this occurs consistently for all three power item selection methods. Reported in [11], as the number of filler items increases, PS decreases; the explanation for this was that attack user profiles need to achieve a balance between  X  X overage X  (including enough item ratings to cor-relate with other users) and  X  X enerality X  (including too many item ratings that could make the profile dissimilar to a given user). We also observed this for NumRatings and InDegree for ML10M in this experiment (not shown) and in Experiment 2 (see Figure 3). Regarding Prediction Shift results, for ML100K we observed PS values in the range of 0.2 to 0.4 and for ML1M they ranged from 0.03 to 0.19. By comparison, [1, 11] reported PS values of 0.1 and 0.15 for the Segment and Bandwagon attacks, respectively, against the item-based algorithm for all users in ML100K with an attack size of 1%. Our HR and PS results for ML100K were sig-nificantly improved over previously reported results. Given time constraints, full re-implementation, testing, and execution of Seg-ment / Bandwagon attacks for more direct comparison was beyond the scope of the experiment. It is a limitation of the study to be addressed in future work.
 To further determine the quality of our results, we computed for attack datasets that included the power items but not the target items and compared results statistically. For ML100K and 100 tar-get items, differences in PS with and without the target items were significant ( p&lt; 0.005) for NumRatings, InDegree, and AggSim across all three levels of power items. For ML1M, differences were significant ( p&lt; 0.05) for NumRatings and InDegree (368 power items only). As in Experiment 2, we ran a set of PIA X  X  across all datasets, attack sizes, and power item levels for IBW without any target items as a baseline. In this case, the robustness metrics were all &gt; zero. The interpretation is that, because Experiment 3 uses  X  X ew and established X  items as target items, it is possible (and ex-pected) that some of them will show up in top-N recommendation lists as confirmed by our findings. However, we found significant differences in key metrics for cases with and without targets. For example, averaged over all the cases run with ML1M, NNTPU was 2.29 (with targets) and 1.38 (without targets) and PS was 0.09 and 0.01, respectively; this indicates that the attack had impacts above and beyond the baseline.

Hypothesis H1 is accepted for the highest levels of attack size and number of targets across all power item selection methods for ML100K and ML1M, given the threshold rates of 11% HR and 0.1 PS . H1 is partially accepted for ML10M for HR . Hypothesis H2 is partially accepted for the IBW algorithm. We find that the In-Degree and NumRatings methods, on average, perform the same at all levels of power items and both methods are superior to AggSim.
In this study we have developed a power item model that is able to generate synthetic power item profiles that can be used to mount effective power item attacks against user-based and SVD-based recommenders measured by traditional Hit Ratio, Rank, and Pre-diction Shift robustness metrics. In addition, we showed how the power item attack using a novel multi-target approach can gener-ate effective attacks against the typically robust item-based algo-rithm using new, as well as established, dataset items. We have also compared power item selection methods used to generate synthetic power item profiles and shown that, because of its low-cost and low-knowledge requirements, the NumRatings method is the more effective, by a small margin, in attacking recommenders than the influence-based InDegree method. We have shown that a relatively small number of NumRatings and InDegree synthetic power item profiles can have significant effects on RS predictions and top-N recommendation lists. And, in order to compare attack effective-ness results within and between our experiments, we developed a metric that measures the number of target items per user resulting from the multi-target approach. Future work includes evaluation in other domains / datasets and developing PIA detection methods. [1] R. Burke, B. Mobasher, R. Bhaumik, and C. Williams. [2] R. Burke, M. P. O X  X ahony, and N. J. Hurley. Robust [3] C. Desrosiers and G. Karypis. A comprehensive survey of [4] A. Goyal and L. V. S. Lakshmanan. Recmax: Exploiting [5] J. L. Herlocker, J. A. Konstan, A. Borchers, and J. Riedl. An [6] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. Riedl. [7] N. Hurley, Z. Cheng, and M. Zhang. Statistical attack [8] S. K. Lam and J. Riedl. Shilling recommender systems for [9] B. Mehta and W. Nejdl. Attack resistant collaborative [10] B. Mehta and W. Nejdl. Unsupervised strategies for shilling [11] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams. [12] B. Mobasher, R. Burke, and J. Sandvig. Model-based [13] M. P. O X  X ahony, N. Hurley, and G. C. M. Silvestre. [14] J. Palau, M. Montaner, B. Lopez, and J. L. D. L. Rosa. [15] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based [16] C. E. Seminario and D. C. Wilson. Assessing impacts of a [17] S. Wasserman and K. Faust. Social Network Analysis: [18] C. Williams, B. Mobasher, and R. D. Burke. Defending [19] D. C. Wilson and C. E. Seminario. When power users attack: [20] D. C. Wilson and C. E. Seminario. Evil twins: Modeling
