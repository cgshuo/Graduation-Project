 All state-of-the-art web search engines implement an auto-completion mechanism X  X n assistive technology enabling users to effectively formulate their search queries by predicting the next characters or words that they are likely to type. Query completions (or suggestions) are typically mined from past user interactions with the search engine, e.g., from query logs, clickthrough patterns, or query reformulations; they are ranked by some measure of query popularity, e.g., query frequency or clickthrough rate. Current query suggestion tools largely assume that the set of suggestions provided to the users is homogeneous, corresponding to a single real-world interpretation of the query. In this paper, we hypoth-esize that, in some cases, users would benefit from an al-ternative presentation of the suggestions, one where sugges-tions are not only ordered by likelihood but also organized by high-level user intent. Rich search suggestion interac-tion frameworks that reduce the user effort in identifying the set of relevant suggestions open new and promising di-rections towards improving user experience. Along these lines, we propose clustering the set of suggestions presented to a search engine user, and assigning an appropriate label to each subset of suggestions to help users quickly identify useful ones. For this, we present a variety of unsupervised clustering techniques for search suggestions, based on the information available to a large-scale web search engine. We evaluate our novel search suggestion presentation techniques on a real-world dataset of query logs. Based on a set of user studies, we show that by extending the existing assistance layer to effectively group suggestions and label them X  X hile accounting for the query popularity X  X e substantially in-crease the user X  X  satisfaction.
 H.5.0 [ Information Interfaces and Presentation ]: Gen-eral Algorithms, Experimentation
Search engines are increasingly exploring ways to reduce user efforts in performing search-related tasks. Such efforts have resulted in the widely-used auto-completion mechanism that automatically suggests possible completion of search queries while users are formulating their queries.
Example 1. Consider the case where a user initiates a search engine query by typing the character sequence,  X  X aifa X . For this query prefix, the set of suggestions presented by a major search engine could include the following ordered list:
Various factors such as, click behavior, query frequencies, or query reformulations, based on past user behavior deter-mine the set of query completions offered by a search engine. This paper extends the current query completion approach by organizing auto-complete suggestions by topic .
The above example underscores two important observa-tions which form the basis of our work. First, query comple-tion suggestions may correspond to non-identical real-world entities or facets. For instance, the suggestions at position 1, 2, and 5 relate to a popular entertainer, whereas the sugges-tions at position 3, and 6 relate to a city. Second, query sug-gestions associated with similar facets may not be grouped together and thus, suggestions may often be an unordered list from a topicality perspective. Contrast the presentation above with the following alternative presentation:
Our goal is to identify (implicit) topics or facets among a set of suggestions for a query prefix, and organize and present suggestions in a topic-aware manner as shown in the above example. For this several problems need to be ad-dressed. First, we need to identify appropriate representa-tions for each suggestion for a prefix such that they capture user perception. For instance, while  X  X almart pharmacy X  and  X  X almart careers X  may be related to a single real-world concept, some users may perceive these as related but dis-similar concepts. Second, we need to enable quick identifica-tion of desired clusters while keeping the user efforts low. For instance, in the above example, providing a representative image or textual label for each suggestion cluster can help users distinguish between the clusters and locate one that matches their needs. Third, this novel setting of presenting clusters of suggestions, instead of suggestions, introduces a new ranking challenge of deciding how to order suggestion clusters for a query prefix.

In this paper, we present an end-to-end approach for au-tomatically organizing and presenting completions for web search queries . Specifically, our contributions are: The rest of the paper is structured as follows: Section 2 dis-cusses the problem on which we focus and presents methods to characterize and cluster similar suggestions for a query prefix. Section 3 discusses our methods to identify appro-priate labels to the clusters generated for a set of sugges-tions. Section 4 discusses our method to measure user effort which, in turn, leads to a ranking algorithm to order sug-gestion clusters. Section 5 reports the results of our exper-imental evaluation. Finally, Section 6 reviews related work, and Section 7 concludes the paper.
A query prefix (or prefix ) is a sequence of characters typed by a users while formulating her query. Given a prefix p , a search engine returns an ordered set S of suggestions for completing the query that started with p .

Problem 2.1. Given a prefix p and an ordered set of sug-gestions, S = { s 1 ,s 2 ,...,s n } , our goal is to partition S into k ordered, disjoint partitions, P = { P 1 ,P 2 ,...,P k } that every s i belongs to exactly one P j , and the members of every P j are topically-coherent 1 , i.e., refer to a single topic or aspect of q . After partitioning S , we wish to assign a distinct label L to each partition such that L( P j ) describes to a user that topic which is shared by members of P j ,but not by the rest of the elements in S . Finally, we wish to rank the partitions L( P j ) as well as the suggestions within each partitions so as to maximize the utility (see Section 4) of the set S to the user.
To address this problem, we begin by examining three different approaches to the task of clustering suggestions for a query prefix. Our approaches make use of the search engine itself, with an increasing level of dependency. We start with a clustering mechanism requiring only the num-ber of documents returned by the search engine for a given query(Section 2.1); continue with a different approach utiliz-ing the full text of the search results (Section 2.2); and dis-cuss a clustering technique that employs implicit user feed-back by examining the documents clicked by users issuing the query (Section 2.3).
Many of the suggestions offered as the users types are com-pletions, treating the user input as a prefix (and, sometimes, a suffix or infix). As a result, the set S is usually already very similar at the lexical level. In general, a suggestion s be viewed as s i = p  X  c i ,where p is the user-supplied query prefix, and c i is additional context added in the particular suggestion s i . For example, consider the top suggestions for p = salsa : The different suggestions to be clustered already share p ; the terms that may be useful for identifying the cluster s belongs to are more likely in c i . We hypothesize that we can select a single term from each s i  X  X he most discriminative term X  X nd that clustering these terms only will translate to a good clustering on S itself. In the example above, the discriminative terms are recipes , dancing , dance ,andsoon. We refer to such a term as the head word of s i .

Once the clustering task is reduced from the query level to the term level, we can employ a multitude of existing ap-proaches for estimating semantic or topical word-level sim-ilarity. Commonly-used methods include those based on word contexts in a large corpus or lexical resources such as Wordnet [18, 6]. We choose PMI-IR , a simple co-occurrence technique shown to be effective in similar settings [25, 5]. Here, the similarity between two words { w i , w j } is defined as the pointwise mutual information between the words, where the probability of a single word, P ( w i ), as well as the joint probability P ( w i ,w j ) are estimated using maximum likeli-hood of occurrences in a corpus. Specifically, the similarity measure between the words in this case is defined as where hits ( x ) is the set of documents containing x and n is the corpus size. As mentioned earlier, our only requirement form the search engine for this approach is obtaining hits ( x ); we set the similarity between suggestions to be the similarity between their head words.
 Due to the short average length of web queries, c i often consists of a single term only. However, there are cases where the head word needs to be chosen from several candidates. We experiment with several simple approaches to selecting the head word:
In the context of web search, a natural approach to com-puting the similarity between two queries (or query sugges-tions) is to leverage the search results associated with each. Existing work in this direction [23, 27] represents queries using tf  X  idf -weighted term vectors of frequent terms found in the top-N search results for the query; cosine similarity between these vectors is shown to be effective as a query sim-ilarity measure. Our next proposal to clustering query sug-gestions largely follows this approach, extending it slightly by separately utilizing terms appearing in the titles of top-ranked documents, their URLs, and the content of the doc-uments themselves.
 Concretely, given a query suggestion s we obtain the set R ( s ) of the top-n documents for s returned by a search en-gine. Each document d  X  R ( s ) contains a title, a URL, and an  X  X bstract X   X  a snippet of d that is shown to the user, containing the terms in the query and a small amount of context around them. We mark these as t ( d ), u ( d ), and a ( d ), respectively. Next, we construct term vectors for each of these components in a manner similar to [23]: each doc-ument component is represented using a tf  X  idf vector of the terms appearing in it, and the result set as a whole is repre-sented using the centroid of these vectors. We concatenate the vectors formed by t ( d ), u ( d ), and a ( d ) to obtain a single vector representing the result set, tion we then use to cluster the suggestions in this method is the inner product between the vectors representing the result sets of the suggestions,
Our third approach leverages clickthrough data maintained by search engines, which contains information about urls from the search results presented to the users that were clicked. For instance, a search log may contain the following clicked urls for a query pineapple salsa , for different users:
Using the clickthrough data, we can characterize each sug-gestion for a prefix by the set of clicked urls associated with it and group suggestions with similar user click behavior. The
Figure 1: Sample bipartite clickthrough graph. intuition behind this clustering method is that non-identical queries that generate clicks on the same urls capture simi-laruserintent. Forinstance,thequery pineapple salsa for fish may also generate clicks on the one of the above urls, indicating that the two suggestions are similar.

Before representing query suggestions using clickthrough data, we discuss two main observations. First, using clicked urls as is could result in specific representations which prove to be too restrictive since, websites tend to dedicate a web page per concept. So, we generalize our suggestion repre-sentation by using base urls from the clickthrough data. For instance, the url clicked by user u 1 is generalized to www.allrecipes.com . Second, encyclopedic websites such as, www.wikipedia.org , may introduce undesired bias and lead to non-similar concepts to be placed in the same cluster. For instance, we observed the most frequently clicked base url for both gold retriever and gold rush 1849 is www.wikiped ia.org . Similarly, other websites such as, wwww.youtube.com may also introduce such bias. To address this issue, we can treat each suggestion as a  X  X ocument X  and compute an in-verse document frequency for each base url and use that as the weight when generating the representation. In our ex-periments, we employed a stop-list by eliminating top-5 urls based on their inverse document frequency, which tends to work better than using the inverse document frequency as weights.

Given a prefix p and the set S of suggestions associated with it, we define a clickthrough graph for p .

Definition 2.1. [Clickthrough graph] A clickthrough graph is a bipartite, directed graph consisting of two classes of nodes: suggestions nodes ( X  s  X  nodes) and base urls nodes ( X  u  X  nodes), and a set of directed edges E . Each suggestion in the set S is represented as an s node. To generate the u nodes, we take the union of the set of base urls associated with each suggestion and generate a node per distinct base url. An edge s  X  u between a suggestion node s and url node u indicates that url u was clicked when s was issued as a query. Each edge is assigned a weight which is the number of times u was clicked when s was issued as a query.
Using the clickthrough graph, for each suggestion s in the graph, we generate a L2-normalized feature vector of size equal to the number of url nodes in the graph, and each dimension in the vector represents a url in the graph. The value for the dimension associated with url j is computed as: f = where U is the set of urls in the clickthrough graph and w is the weight associated with edge s  X  j in the clickthrough graph. To compute the similarity between two suggestions for a prefix, we experimented with two similarity functions and picked cosine-similarity defined as: 3 So far, we discussed three different way of characterizing suggestions for a query prefix, varying in the way topics are modeled; each of these results in a different similarity mea-sure between suggestion. Once this similarity is estimated, performing the clustering itself is straightforward; we use Hierarchical Agglomerative Clustering to partition the sug-gestion set into individual clusters using the similarity.
User studies on clustered presentation of web search re-sults consistently show that users prefer clusters that are assigned a meaningful title over clusters with no label [14]. We hypothesize that the same holds for a clustered presen-tation of query suggestion. In this section, we describe sev-eral methods for assigning a meaningful label to a set of suggestions. To demonstrate these approaches over differ-ent suggestion sets, we refer to two query prefixes and their (clustered) suggestions appearing in Figure 2. Figure 2: Sample suggestion clusters; a cluster iden-tifier is shown next to each cluster.
 Most Frequent Suggestion (MFS) : One way to select a label for a cluster of query suggestions is to select the most representative suggestion in the cluster. Since every sugges-tion is a query by itself, a natural way to select the most representative suggestion is to choose the most frequent one in the search engine X  X  query log. Formally, the label assigned by MFS to the set of suggestions S is Where Freq( x ) is the number of times x is observed in a large query log. In the examples appearing in Figure 2, cluster LA.1 would be assigned the label los angeles daily news using this method; cluster NU.1 would be assigned nursing ; and so on.
 Longest Common Subsequence (LCS) : Often, a se-quence of characters is shared among all suggestions within a cluster, but not with suggestions in other clusters; for ex-ample; the user-supplied  X  X s a X  may be completed to us air-ways, us airways flights, . . . as well as to us army, us army jobs,... , and so on. We hypothesize that in some cases it is beneficial to use the longest common subsequence of the sug-gestions as the label. Formally, the label assigned by LCS to S is Where Q ( S ) is the set of subsequences of any s  X  S .For example, the label assigned by this method to clusters LA.1 through la.3 is los angeles , whereas the label assigned to cluster NU.2 is nursing home .
 Most Frequent in Result Set (MFRS) : One drawback of both MFS and LCS is that they always draw a label from the suggestions belonging to the cluster. For some clusters of suggestions, a meaningful label is not part of the cluster and has to be obtained using external resources; for example, for the cluster LA.1 in Figure 2, a useful label may be los angeles newspapers  X  a label that has only partial overlap with the suggestions in the cluster.

As with the case of performing the clustering itself, we turn to the top-ranked documents for each suggestion (when it is used as a query to a search engine) for this external knowledge. By transforming the suggestion set into a doc-ument set we can use a variety of methods developed for labeling documents, rather than queries. We adopt a stan-dard approach to labeling clusters of documents, namely, harvesting word n -ngrams from them and selecting the most frequent n -gram [9]. Formally, let R ( s )bethesetoftop-ranked results for the suggestion s ;let R ( S )=  X  s i  X  let NG ( d )bethesetofword n -ngrams contained in the doc-ument d ;andlet NG ( R ( S )) be the set of all n -grams in all top-ranked documents, NG ( R ( S )) =  X  d  X  R ( S ) NG ( d ). Then the label assigned by MFRS to S is MFRS( S )= l i : l i  X  NG(R(S)) , This method assigns, for example, the label news to cluster LA.1 ,andthelabel nursing jobs to NU.1 .
 Most Frequent in Modified Result Set (MFRS*) :Fi-nally, we return to our observation from Section 2.1: search suggestions are unique as a collection of entities to cluster in that they often have a high degree of lexical overlap. In a cluster with a long common subsequence (such as LA.2 ), the elements we are interested in labeling are sometimes best represented in those portions of the suggestions that are not shared among all elements of the cluster (e.g. public library, police department ). To this end, we propose an additional labeling mechanism, similar to MFRS, but where the queries we use are not the suggestions themselves, but the portions of the suggestions that are distinct within the cluster. For-mally, let s  X  i be the suggestion s i with the longest common subsequence of S removed, s  X  i = s i  X  LCS( S ), and let S the set of suggestions in S with the longest common subse-quence removed from all suggestions, S  X  =  X  i s  X  i , then the label assigned by MFRS* to S is For example, this method assigns the label services to the cluster LA.2 .
 Combined Labeling Strategy (Comb) : As seen in the examples in 2, clusters of suggestions have different charac-teristics, and may benefit from different labeling approaches. Figure 3: Algorithm to order clusters to minimize the expected cost.
 Our final labeling is a hybrid one, first selecting which label-ing method to use, then assigning the label itself;. We refer to this method as Comb.

The main difference between our labeling approaches is whether the label is assigned from within the cluster (MFS, LCS), or from external knowledge (MFRS, MFRS*). The Comb approach selects among these by examining the clus-ter cohesion , the degree to which the cluster elements are similar; the more compact a cluster is, the more likely it is that a good label is found in its members rather than externally. The cohesion of S is measured using the av-erage distance between the elements of S :Cohesion( S )  X   X  i,j Sim ( s i ,s j ); the label assigned by Comb is then
Our objective in clustering suggestions is to reduce the users X  effort in locating their desired query completions. Nat-urally, the order in which suggestions are presented influ-ences the amount of user effort. With this in mind, we describe a cost metric to characterize the user effort spent in locating a suggestion from among a set of clusters of sug-gestions. We then present an algorithm that minimizes the expected cost of locating suggestions.

We begin by describing a structural property of clusters of suggestions fundamental to our cost metric. By clustering (and labeling) the set of suggestions available for a prefix, we are generating a skip list of the suggestions where users will first skip between buckets (i.e., clusters) and then upon identifying a relevant bucket, the user will scan within the bucket to locate the desired suggestion. Thus, the cost of identifying a suggestion s consists of:
Consider a user who has provided a prefix p and is inter-ested in locating suggestion s from a set of clusters C 1 C ,andlet C m be the cluster than contains suggestions s ,s 2 ,  X  X  X  ,s | Cm | such that s k = s i.e., s is located at position k within C m . The cost of locating suggestion s for the user, which we denote T ( s ), is then For simplicity, we assume that the cost to read any cluster label is the same for all clusters, namely T lb . Similarly, we assume the cost to scan through suggestions within a clus-ter is T sc , the same regardless of the suggestion. T ( s )fora suggestion s at position k in cluster m then becomes simply
For a user who has entered prefix p , we would like to study the expected cost T ( p ) of locating the suggestion she is in-terested in among all the suggestions provided. If we denote by P { s | p } the probability that the user prefers suggestion s when she entered prefix p then the expected cost is: In our notation we mark T p as a function of the ranking R of our suggestion, to emphasize that the cost is very much dependent on the ranking R . P { s | p } can be estimated from the query logs based on observed user preferences when en-tering prefix p . Specifically, if f ( p ) is the number of times the prefix p was entered, and f ( s ) is the number of times suggestion s was submitted as a user query, then: Note that may have entered queries that are not among our suggestions list. We assume the cost to the user interested in a sugges-tion not present in our list to be independent of the ranking of the list of suggestions that are present, and we don X  X  count it as part of T p .

The goal of our ranking algorithm is then to order the clus-ters and suggestions such as to minimize T p ( R ). Figure 3 shows the algorithm we use to find the optimal ordering of suggestions R. Specifically, the algorithm ranks suggestions withing a cluster in nonincreasing order of their frequencies f ( s ). To rank clusters of suggestions, each cluster C is as-signed and aggregate frequency F ( C ) equal to the sum of the frequencies of all suggestions in C . We show that Al-gorithm 3 generated a ranking R that minimizes the cost T ( R ) using the following Theorem.

Theorem 4.1. Given a set of clusters S, RankClusters generates an optimal ranking R that minimizes the total ex-pected cost of finding a suggestion in S.
 Proof. By contradiction.
 Case 1: Ranking within a cluster. Assume that there exists an optimal ranking R such that a cluster C m contains suggestions s x and s y at positions x and y . Assume for con-tradiction that f ( s x ) &gt;f ( s y ), but x&gt;y . We will show that by swapping s x and s y in C m , we achieve a new ranking R with total cost is less than that of R , thereby contra-dicting the optimality of R . Based on Equation 5 we can express T p ( R )as T ( s y )  X  P { s y | p } . Similarly, T p ( R )as T ( s x )  X  Ps x | p + T ( s y )  X  P { s y | p } .First,wenotethatby swapping s x and s y , we have not altered the cost of any sug-gestion other than s x and s y . Therefore, T ( s )= T ( s )for any s != x, y . We can therefore compute T p ( R )  X  T p ( R )= T ( s x )  X  P { s x | p } + T ( s y )  X  P { s y | p } X  [ T ( s P { s y | p } ]=[ T ( s x )  X  T ( s x )]  X  P { s x | p } +[ T ( s P { s y | p } . Using 4 and 6, T p ( R )  X  T p ( R ) becomes [ m x  X  T sc  X  ( m  X  T lb + y  X  T sc )]  X  f ( s x ) /f ( p )+[ m  X  y  X 
T sc )]  X  f ( s y ) /f ( p )=1 /f ( p )  X  [ T sc  X  ( x  X  y ) x )  X  f ( s y )] = 1 /f ( p )  X  T sc  X  ( x  X  y )  X  [ f ( s x ) ing to our assumptions. This shows that T p ( R ) &gt;T p proving our contradiction.

Case 2. Ranking of clusters. Assume that there exists an optimal ranking R such that a cluster C x with aggregate frequency F ( C x ) appears at position x ,anda cluster C y with aggregate frequency F ( C y )appearsapo-sition y . Assume for the sake of contradiction that F ( C F ( C y ) but x&gt;y . We will show that by swapping C x C y we achieve a new ranking R whose total cost is less than R , thereby contradicting the optimality of R . Based on Equation 5 we can express T p ( R )as P { s | p } + ilarly, T p ( R )= P { s | p } + the cost of any suggestion not in C x or C y is not affected by swapping C x and C y and therefore T ( s )= T ( s )for any s/  X  C x ,C y . We can then compute T p ( R )  X  T p ( R )= P P { s | p } + +
P comes +
P =
P =( x  X  y )  X  T lb  X  1 /f ( p )  X  [ T that x  X  y&gt; 0and F ( C x )  X  F ( C y ) &gt; 0. This shows that T ( R ) &gt;T p ( R ), proving our contradiction. In this section, we present our experimental evaluation. We first discuss our data sets(Section 5.1) and our obser-vations from two pilot studies examining whether users can benefit by the problem solved in this paper (Section 5.2). Then, we evaluate our suggestion clustering techniques (Sec-tion 5.3), cluster labeling techniques (Section 5.4), and tech-niques to order suggestion clusters (Section 5.5). We con-clude in Section 5.6. Query logs : We collected a random sample of 100 million, fully anonymized queries sent to the Yahoo! web search engine in the first seven months of 2009, along with their frequency. We sort these queries by their frequency and split them into three quantiles, and denote the queries in the top quantile as the head queries ( HQ ), those in the second quantile as the torso queries ( TQ ), and those in the third quantile as the tail queries ( LQ ). For our experiments, we focus on the top-2 quantiles, i.e., HQ and TQ , and draw a uniform sample of 250 query prefixes from each.
 Query prefixes and suggestions : To simulate a scenario where the user has only typed part of the query and is ob-serving as-you-type query completions, we use the first 30% characters of the queries only. We experimented with other Figure 4: Distribution of number of click urls for prefixes in HQ (left) and TQ (right). methods for generating query prefixes (e.g., using words in-stead of characters, or varying the fraction of characters) observing similar results. For each prefix p , we collect the top-15 query suggestions returned by Yahoo! Search. Clickthrough data : To generate the clickthrough graph (see Section 2), we used fully anonymized query clickthrough logs for first three months of 2009. Figure 4 shows the dis-tribution of the number of clicked results for the two query sets, HQ and TQ .
 User studies : All user studies and manual annotation tasks described in this Section were performed by a group of eight professional search engine quality evaluators experienced with assessing the quality of query suggestions and search results. Usefulness of query suggestion clusters :Toexplore whether users would benefit from clustering of search sug-gestions in the first place, we conducted a small-scale user study. Our annotators were asked to look at two versions of suggestions displayed for 50 different queries: non-clustered (as currently shown on a search engine) and manually clus-tered (simulating performance of a perfect clustering algo-rithm). The annotators were asked whether they prefer the clustered on unclustered version; results are shown in Ta-ble 1, and although the sample size is relatively small they indicate that clustering has potential value.
 Table 1: User preferences for suggestion clustering in our pilot study comparing unclustered sugges-tions with clustered suggestions presentation.

Prefixes where clustering was preferred by our annotators included  X  X ar, X  (sample suggestions are,  X  X ar ache, X   X  X oogle earth, X   X  X arthquake X ),  X  X os X  (sample suggestions are  X  X osemite national park, X   X  X oshimura, X   X  X oshi X  X  X ) and  X  X ho X  (sample sug-gestions are,  X  X ayden kho, X   X  X hols, X   X  X hou 11 houston. X  Pre-fixes where clustering was not preferred included  X  X sps, X  where all the suggestions involved the entity  X  X sps X  (e.g.,  X  X sps tracking, X   X  X sps delivery confirmation, X   X  X sps rates X ), and  X  X rew, X  where a set of unrelated suggestions were presented (e.g.,  X  X ancy drew, X   X  X rew barrymore, X   X  X rew university X ). In a nutshell, this study confirms that clustering sets of sug-gestions that correspond to more than one facet (or topic) is desirable, except for the case where the number of facets (or topics) is close or equal to the number of suggestions. As we will see later, we use these observations when designing our classification and presentation methods.
 Cluster prevalence and size : Next, we explore the prop-erties of possible topical clusters in the top suggestions for a query prefix. For this, our annotators constructed a gold-set of clusters for all 500 query prefixes. Each annotator was presented with a prefix along with the top-15 suggestions associated with it. Annotators were requested to label sug-gestions that belong to similar real-life facets subject to two main constraints: (a) there must be at least one cluster with more than a single query suggestion, and (b) participants must try to minimize the number of clusters. Participants were allowed to conduct a research on the web about the query or the suggestions. Overall, this user study resulted in 7500 triplets of the form { prefix, suggestion, cluster-id Figure 5(a) shows the distribution of cluster sizes over all 500 queries; Figure 5(b) shows the average size of the clus-ters, when the number of top suggestions being clustered is between 2 and 15 per query. Note that, on average, clus-ters are relatively small  X  indicating a high level of topical ambiguity within the suggestions.
 Figure 5: Mean and standard deviation of the clus-ter size within each top-n query suggestion set.
 Usefulness and choice of suggestion cluster labels : Our second pilot study studied whether users would benefit from labeling clusters of suggestions. Our annotators were now shown two versions of 400 suggestion clusters, each con-taining more than one suggestion each: an unlabeled presen-tation and one where labels were manually generated. The annotators were asked whether they prefer the unlabeled or labeled version; overall, for 238 (60%) of the clusters a la-beled version was preferred  X  a small, yet consistent gain across different individual annotators.
After confirming the usefulness of clustering query sugges-tions for a prefix, we extensively evaluate our techniques for clustering suggestions for a prefix. We discuss our evaluation methodology and metrics before reporting the results. Evaluation methodology: To evaluate the quality of the clusters generated by our methods, we used the clustering gold-set described earlier, containing full manual clusterings of 500 query prefixes.
 Techniques to compare: We evaluate our proposed sug-gestion clustering methods from Section 2. We are unaware of any existing system for providing facet-aware search sug-gestions for search engine users. However, arguably one nat-ural extension of the current search assistance mechanism is to group suggestions by shared prefix. Specifically, given a prefix p for which we have a set of suggestions S , we split each suggestion s i into a sequence fr where f contains all the characters in s i until the first occurrence of p , including p ; r is the remainder of the characters. We group together suggestions in S that share first k characters in r , and pick k as 30% of the length of remainder characters, r .Weuse this shared-prefix method as a baseline; this results in the following methods to cluster suggestions for a prefix. Evaluation metrics: To test the performance of each clustering method, we use standard clustering evaluation metrics: two set-based measures, namely purity and inverse purity , and a pair-based measures, namely, Rand statistic , and cluster entropy [1]. We define these measures as: Purity: Purity of a set of clusters is computed by assigning each cluster ( C i ) to the most frequently labeled category ( L j ) and computing the fraction of correct assignments. Inverse purity : While purity measures the precision of a cluster, it can be trivially maximized by generating clusters of size 1. To measure how well similar suggestions were grouped together, we compute the inverse purity defined as: F-measure: Similar to the F-measure frequently used in information retrieval, we can combine purity and inverse purity to derive a single metric: Rand statistic: A clustering method can be viewed as a decision-making process where given a pair of suggestions ( s , s j ), the method decides whether they should be grouped together or not. The Rand statistic R evaluates the per-formance of the clustering method for a total of | S | X  ( | decisions and is defined as: where TP is the number of similar suggestions that were assigned to identical clusters, TN is the number of dissimilar suggestions that were assigned to non-identical clusters, FP is the number of similar suggestions that were assigned to non-identical clusters, and FN is the number of dissimilar suggestions that were assigned to identical clusters. Entropy: Entropy of a set S of clusters is defined as: where Pr { C i ,L j } is the probability of finding a suggestion from cluster C i in labeled category L j . Figure 6: Distribution of number of clusters gener-ated using Head (left) and (b) Click (right).
Table 2 reports evaluation measures for all methods; sta-tistical significance with respect to B-pre is measured using the sign test [19].
 Table 2: Performance of different clustering meth-ods over all prefixes. (  X  indicates statistical signifi-cance over baseline B-pre .)
Clearly, clustering suggestions becomes harder as more suggestions are displayed to the user. Figure 7 shows the decay in performance of two of our clustering approaches, Head and Click , when the number of suggestions being clus-tered is increased.

To further understand the nature of clusters generated by our clustering methods, we examined the distribution of the number of clusters as well as the size of the clusters gen-erated by each method. Figure 6 shows the distribution of the sizes of clusters for Click (Figure 6(b)) and Head (Fig-ure 6(a)). In general, Click tends to generate more clusters as compared to Head . We traced our observations on the evaluation metrics by examining a few examples cases of suggestion clusters. Table 3 shows a sample of clusters gen-erated by three of our methods, namely, Head (using freq ), Click ,and Search . Examples show how using using Click is influenced by the navigational behaviour of users which can, in turn, lead in smaller clusters. For instance, in case of  X  X tla, X  although suggestions,  X  X tlanta braves X  and  X  X ltanta falcons X  are associated with the same geographical location, i.e.,  X  X tlanta, X  they are placed into two different clusters due to the fact that users interested in these suggestions mostly click on dissimilary URLs. On the other hand, using Head generates a single cluster for suggestions related to  X  X tlanta X  due to their high associativity. As a counter example, con-sider the case where the prefix is  X  X p ga. X  Here the sugges-tions  X  X p gas mania X  and  X  X p gas mania game X  that corre-spond identical facets were placed in different clusters. This is due to the fact that the associativity statistics used by Head do exhibit a high similarity between the terms  X  X p gas X  and  X  X ame X  than that between  X  X p gas X  and  X  X ania. X  In con-trast, Click based on the fact that the suggestions,  X  X p gas X  Figure 7: Clustering performance on varying num-ber of suggestions: Head (top) and Click (bottom). Figure 8: Distribution of number of clusters gener-ated using Head (left) and (b) Click (right). and  X  X p gas mania X  generated a click on similar urls, could correctly cluster these suggestions.
 In conclusion, we observed that our methods Click and Head outperform our strong baseline, B-pre , for a range of evaluation metrics. We also observed that Click and Head may substantially differ in the sizes and nature of clusters they produce, with Click generating pure but smaller clus-ters and Head generating larger but homogenous clusters. The choice of clustering method naturally depends on the nature of the prefix and the general intent of users that sub-mitted a prefix. Combining the virtues of the two proposed clustering methods in a principled manner remains our fu-ture work. Evaluation methodology : Finally, we evaluate the qual-ity of the labels generated for the clusters. For this, we used all of the proposed labeling methods to assign labels to the clusters of suggestions generated for 250 query prefixes used for the previous experiments; overall, the set contained 1039 clusters. Our annotators judged each label as correct on incorrect; a label was deemed correct for a cluster if it describes the suggestions in it in a way that assists a user in understanding why these suggestions appear together. La-Table 3: Sample suggestion clusters generated by various methods for the query prefixes dji (top), bp ga (middle), atla (bottom). bels with incorrect spellings, incomplete words, or incoher-ent words are considered as incorrect ones.
 Techniques for comparison : Wecomparethedifferent approaches described in Section 3: Evaluation metrics : To test the accuracy of the labeling we measure the precision of the method  X  the percent of correct labels assigned by it. Since in many cases the correct label is assigned by more than one method, we also track, for each method, the fraction of clusters for which the correct label was produced by this method only. We refer to this latter metric as the lead of the method.

Table 4 shows the precision of the different labeling ap-proaches we tested. Examining the labeling errors, we ob-served that most stem from imperfect clustering results: the less coheseive a cluster is, the less likely it is that techniques like LCS or MFRS will result in a meaningful cluster.
Table 4: Performance of cluster labeling methods.
Earlier in Section 4 we proved the correctness of our al-gorithm to rank suggestion clusters at minimizing the user effort when formulating a search query. We put our cost metric to test by examining whether our clustering algo-rithm places desirable suggestion clusters at the top. In particular, we recruited human annotators and presented them with a set of suggestion clusters. Participants were re-quested to pick rank two suggestion clusters that they would like to see at rank 1 and 2. It is noteworthy that this is a stricter evaluation than presenting human annotators with a list of ordered clusters and requesting whether they agree or not with the presented ranking.

For a set of 50 prefixes, we observed that for 85% of the prefixes the cluster placed at rank 1 by our ranking algorithm was also picked as the top-1 cluster by the hu-man annotators. Additionally, for 13% of the prefixes our ranking algorithm placed the top-1 manually chosen clus-ter at rank 2. Interestingly, 2 of such cases were where the beginning of the prefix did not overlap with the sug-gestions in the first cluster. For instance, for the prefix,  X  X ot, X  two competing clusters were {  X  X arry potter,  X  X arry potter movie X ,  X  X arry potter and the half-blood prince X  } {  X  X otato X , X  X otato soup X  } . While our ranking algorithm ranked the first cluster higher than the second, our human annota-tors ranked the second cluster higher. The explanation for their choice as traced by comments was:  X  A searcher would type harry pot if they were interested in harry potter. X  We believe that this is a special case of query completion, i.e., not all auto-completion allow for non-overlapping prefix of each suggestion in a set, and only a relatively small set of prefixes follow this trend. Our cost-metric can be easily ex-tend to handle such cases.
In summary, both Click and Head outperform the base-line methods for clustering suggestions, with Click exhibit-ing higher values for f-measure and Head generating clusters with lower entropy. Interestingly, clustering based on terms in search results ( Search ) X  X n approach shown to be effec-tive in general query clustering X  X oes not perform as well when applied to search suggestions. On the other hand, using the search results to obtain cluster labels ( MFRS ) pro-duces high-precision labels while outperforming other meth-ods; combining these labels with labels found in the sug-gestions themselves improves accuracy futher. Finally, we verified that our cluster ranking algorithm generates an de-sirable ordering that meets the user needs. Suggestions for query completions : Predictive text gen-eration systems, offering possible completions to the user X  X  text based on the existing input, have been used since the 1970s, although their applications have mostly been restricted to assisting people with physical disabilities [13]. In recent years, these systems have been found to be beneficial to broader audiences in mobile devices [11] and in web search engine interfaces. In particular, in the case of search en-gines, the user is often not aware of the wording used in the web pages she is searching for, making the suggestive frame-work useful not only for predicting what the user is likely to type, but also for offering query forms the user does not necessarily consider, and that lead to relevant information.
Today, all major search engines offer search suggestion technologies, and similar technologies adapted in related ar-eas such as databases [17]. However, although much work exists on deriving related queries [16, 28, 3, 20], little re-search has focused on organizing X  X nd in particular, clustering X  query suggestions. Boldi et al. describe an approach for la-beling query reformulation types [4]; while this can be used for organizing query suggestions, the labels attached to the reformulations are functional in nature (e.g.  X  X eneraliza-tion X ) rather than topical as in our approach.
 Query similarity and clustering : Research on clustering of web queries is mostly concerned with large-scale clustering  X  that is, grouping an entire query log into different topics, often for the purpose of query recommendation or for anal-ysisoftopicsofinterestoveralargepopulation. Aswith most clustering tasks, the core issue addressed is estimat-ing the distance between the items to be clustered: in this case, queries. Since these tend to be short, lexical distance measures perform poorly, and the approaches developed for the task of query similarity estimation typically use informa-tion external to the query. Glance measures similarity us-ing the overlap between the sets of retrieved documents for queries [15]; Sahami and Heilman use the terms appearing in documents retrieved for a query to compute the similarity between queries [23]. Similarly, Beeferman and Berger [2] and Wen et al. [26] use the set of clicked documents for a given query to determine its cluster. Chien and Immorlica measure the distance between queries according to the simi-larity between their temporal profile [8]. Closest to our work, is that of Sadikov et al. [22] which proposes a mechanism for grouping post-submit query suggestions by performing ran-dom walks on the query reformulation graph in an offline step; however, to the best of our knowledge, there has been no published work on clustering pre-submit suggestions. Labeling clusters : In the context of web search, work on labeling clusters is mostly focused on clusters of docu-ments [12, 24, 7]. In this setting, labels are often chosen from titles of central documents, frequent terms appearing in the documents of a cluster, or recurring named entities. Targeting shorter texts, Pantel and Ravichandran propose an approach to labeling sets of concepts where representa-tive elements of the set, appearing in one of several known grammatical structures that indicate a label, are used to identify and rank candidates for a cluster label [21]. A sim-ilar approach is described by Chung et al. [10], using Word-net to expand signatures defined for each cluster of concepts. These strategies work well when the instances of the clus-ters to be labeled are concepts sharing the same semantic type, and when the cluster size is relatively large (so that representative elements can be mined. In the case of query suggestions, the clusters to label are small in size, and of-ten contain mixed semantic classes; in fact, in many cases, a suggestion will contain more than a single concept, or no concepts at all.
Query auto-completion increases the usability of a web search engine substantially; despite this, the manner in which queries are completed has remained unchanged since the in-troduction of this feature on major web search engines. In this paper, we propose an alternative presentation of as-you-type query suggestions, one where X  X or some queries X  suggestions are grouped by topic. We show that users pre-fer this suggestion mechanism over an unclustered presen-tation, and evaluate several approaches to performing the clustering, with an increasing level of knowledge available to a search engine. We also discuss several additional tasks related to the clustering: selection of the queries to cluster, assignment of a label to each cluster, ordering the clusters themselves and the suggestions within each cluster. We ac-company each of the tasks we address with rigorous evalua-tion using real-life data collected from a major search engine.
To facilitate further research into alternative presentations of search suggestions in general and their clustering in par-ticular, we plan to make our data publicly available.
