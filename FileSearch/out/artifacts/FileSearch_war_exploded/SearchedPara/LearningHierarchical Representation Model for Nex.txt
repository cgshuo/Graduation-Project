 Next basket recommendation is a crucial task in market bas-ket analysis. Given a user X  X  purchase history, usually a se-quence of transaction data, one attempts to build a recom-mender that can predict the next few items that the us-er most probably would like. Ideally, a good recommender should be able to explore the sequential behavior (i.e., buy-ing one item leads to buying another next), as well as ac-count for users X  general taste (i.e., what items a user is typ-ically interested in) for recommendation. Moreover, these two factors may interact with each other to influence users X  next purchase. To tackle the above problems, in this pa-per, we introduce a novel recommendation approach, name-ly hierarchical representation model (HRM). HRM can well capture both sequential behavior and users X  general taste by involving transaction and user representations in prediction. Meanwhile, the flexibility of applying different aggregation operations, especially nonlinear operations, on representa-tions allows us to model complicated interactions among different factors. Theoretically, we show that our model subsumes several existing methods when choosing proper aggregation operations. Empirically, we demonstrate that our model can consistently outperform the state-of-the-art baselines under different evaluation metrics on real-world transaction data.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms, Experiments, Performance, Theory Hierarchical Representation Model; Sequential Behavior; Gen-eral Taste; Next Basket Recommendation
Market basket analysis helps retailers gain a better un-derstanding of users X  purchase behavior which can lead to better decisions. One of its most important tasks is next basket recommendation [7, 8, 12, 20]. In this task, usually sequential transaction data is given per user, where a trans-action is a set/basket of items (e.g. shoes or bags) bought at one point of time. The target is to recommend items that the user probably want to buy in his/her next visit.
Typically, there are two modeling paradigms for this prob-lem. One is sequential recommender [5, 25], mostly relying on Markov chains, which explores the sequential transac-tion data by predicting the next purchase based on the last actions. A major advantage of this model is its ability to capture sequential behavior for good recommendations, e.g. for a user who has recently bought a mobile phone, it may recommend accessories that other users have bought after buying that phone. The other is general recommender [1, 23], which discards any sequential information and learns what items a user is typically interested in. One of the most successful methods in this class is the model based collabo-rative filtering (i.e. matrix factorization models). Obviously, such general recommender is good at capturing the general taste of the user by learning over the user X  X  whole purchase history.

A better solution for next basket recommendation, there-fore, is to take both sequential behavior and users X  general taste into consideration. One step towards this direction is the factorizing personalized Markov chains (FPMC) model proposed by Steffen Rendle et al. [23]. FPMC can model both sequential behavior (by interaction between items in the last transaction and that in the next basket) and users X  general taste (by interaction between the user and the item in the next basket), thus achieves better performance than either sequential or general recommender alone. However, a major problem of FPMC is that all the components are lin-early combined, indicating that it makes strong independent assumption among multiple factors (i.e. each component in-fluence users X  next purchase independently).

Unfortunately, from our analysis, we show that the inde-pendent assumption is not sufficient for good recommenda-tions.

To tackle the above problems, we introduce a novel hier-archical representation model (HRM) for next basket rec-ommendation. Specifically, HRM represents each user and item as a vector in continuous space, and employs a two-layer structure to construct a hybrid representation over user and items from last transaction: The first layer forms the trans-a ction representation by aggregating item vectors from last transaction; While the second layer builds the hybrid repre-sentation by aggregating the user vector and the transaction representation. The resulting hybrid representation is then used to predict the items in the next basket. Note here the transaction representation involved in recommendation models the sequential behavior, while the user representa-tion captures the general taste in recommendation.
HRM allows us to flexibly use different types of aggrega-tion operations at different layers. Especially, by employing nonlinear rather than linear operations, we can model more complicated interactions among different factors beyond in-dependent assumption. For example, by using a max pool-ing operation, features from each factor are compared and only those most significant are selected to form the higher level representation for future prediction. We also show that by choosing proper aggregation operations, HRM subsumes several existing methods including markov chain model, ma-trix factorization model as well as a variation of FPMC mod-el. For learning the model parameters, we employ the neg-ative sampling procedure [27] as the optimization method.
We conducted experiments over three real-world transac-tion datasets. The empirical results demonstrated the effec-tiveness of our approach as compared with the state-of-the-art baseline methods.

In total the contributions of our work are as follows:
Next basket recommendation is a typical application of recommender systems based on implicit feedback, where no explicit preferences (e.g. ratings) but only positive observa-tions (e.g. purchases or clicks) are available [2, 7]. These positive observations are usually in a form of sequential da-ta as obtained by passively tracking users X  behavior over a sequence of time, e.g. a retail store records the transactions of customers. In this section, we briefly review the related work on recommendation with implicit feedback from the following three aspects, i.e. sequential recommender, gener-al recommender, and the hybrid model.

Sequential recommender , mainly based on a Markov chain model, utilizes sequential data by predicting users X  next action given the last actions [6]. For example, Zim-dar et al. [3] propose a sequential recommender based on Markov chains, and investigate how to extract sequential patterns to learn the next state using probablistic decision-tree models. Mobasher et al. [18] study different sequential patterns for recommendation and find that contiguous se-quential patterns are more suitable for sequential prediction task than general sequential patterns. Ghim-Eng Yap et al. [29] introduce a new Competence Score measure in per-sonalized sequential pattern mining for next-items recom-mendation. Shani et al. [24] present a recommender based on Markov decision processes and show that a predictive Markov Chain model is effective for next basket prediction. Chen et al. [5] model playlists as a Markov chain, and pro-pose logistic Markov Embedding to learn the representations of songs for playlist prediction. The main difference of our work to all the previous approaches is the inclusion of users X  general taste in recommendation beyond sequential behav-ior. Besides, the previous sequential recommenders seldom address the interactions among items in sequential factors.
General recommender , in contrast, does not take se-quential behavior into account but recommends based on users X  whole purchase history. The key idea is collaborative filtering (CF) which can be further categorized into memory-based CF and model-based CF [1, 26]. The memory-based CF provides recommendations by finding k-nearest-neighbour of users or products based on certain similarity measure [16]. While the model-based CF tries to factorize the user-item correlation matrix for recommendation. For example, Lee et al. [12] treat the market basket data as a binary user-item matrix, and apply a binary logistic regression model based on principal component analysis (PCA) for recommenda-tion. Hu et al. [10] conduct the factorization on user-item pairs with least-square optimization and use pair confidence to control the importance of observations. Pan et al. [19] al-so introduce the weights to user-item pairs, and optimize the factorization with both least-square and hinge-loss criteria. Rendle et al . [22] propose a different optimization criterion, namely Bayesian personalized ranking, which directly opti-mizes for correctly ranking over item pairs instead of scoring single items. They apply this method to matrix factoriza-tion and adaptive KNN to show its effectiveness. General recommender is good at capturing users X  general taste, but can hardly adapt its recommendations directly to users X  re-cent purchases without modeling sequential behavior.
Hybrid model , tries to integrate both sequential behav-ior and users X  general taste for a better recommendation. A state-of-the-art method is the FPMC model proposed by Rendle et al. [23]. In their work, a transition cube is con-structed where each entry of the cube gives the probability of a user buying next item given he has bought a certain item in the last transaction. By factorizing this cube, they interpret this probability by three pairwise interactions a-mong user, items in the last transaction and items in the next basket. In this way, FPMC models sequential behavior by interaction between items in the last transaction and that in the next basket, as well as users X  general taste by interac-tion between the user and the item in the next basket. It has been shown that such a hybrid model can achieve better per-formance than either a sequential or general recommender alone.
Next basket recommendation is the task of predicting what a user most probably would like to buy next when his/her sequential transaction data is given. When tackling this problem, both the sequential and general recommender have their own advantages. The sequential recommender can ful-denote the recommendation scores produced by the recommender. ly explore the sequential transaction data to discover the correlation between items in consequent purchases, leading to very responsive recommendation according to users X  re-cent purchase. While the general recommender can leverage users X  whole purchase histories to learn the taste of different users, and thus achieve better personalization in recommen-dation.

As shown in previous work [23], it is better to take both sequential and general factors into account for better recom-mendation. A simple solution is to use a linear combination over these two factors. Furthermore, when modeling the sequential factor, items in the last transaction are often lin-early combined in predicting the next item [23]. Obviously, one major assumption underlying these linear combinations is the independence among multiple factors. That is, both sequential and general factor influence the next purchase in-dependently, and each item in the last transaction influence the next purchase independently as well. Here comes the question: Is the independent assumption among multiple factors sufficient for good recommendation?
To answer the above question, we first consider the in-dependent assumption between the general and sequential factors. Let us take a look at an example shown in Figure 1. Imagine a user in general buys science fiction movies like  X  X he Matrix X  and  X  X -men X . In contrast to his usual buying behavior, he recently has become fascinated in Scarlett Jo-hansson and purchased  X  X atch Point X  to watch. A sequential recommender based on recent purchase would recommend movies like  X  X ost in Translation X  (0.9) and  X  X irl with a Pearl Earring X  (0.85), which are also dramas performed by Scar-lett Johansson. (Note that the number in the parentheses denotes the recommendation score). In contrast, a gener-al recommender which mainly accounts for user X  X  general taste would recommend  X  X he Dark Knight X  (0.95) and  X  X n-ception X  (0.8) and other science fiction movies. By taking into account both factors, good recommendations for the user might be the movies like  X  X ucy X  and  X  X he Avengers X , which are science fiction movies performed by Scarlett Jo-hansson. However, if we linearly combine the two factors, i.e. independent in prediction, we may not obtain the right results as we expected. The reason lies in that a good rec-ommendation under joint consideration of the two factors may not obtain a high recommendation score when calcu-lating from each individual factor. For example, the scores of  X  X ucy X  (0.3) and  X  X he Avengers X  (0.2) in sequential rec-ommender are low since they do not match well with the genre preference (i.e. drama) based on the last purchase of the user. Their scores are also not very high in general rec-ommender since there are many better and popular movies fitting the science fiction taste. Thus the linear combination cannot boost the good recommendations to the top.
Let us take a further look at sequential factor alone, i.e. rec-ommending next items based on the last transaction. For example, people who have bought pumpkin will probably buy other vegetables like cucumber or tomato next, while people who have bought candy will probably buy other s-nacks like chocolate or chips next. However, people who have bought pumpkin and candy together will very proba-bly buy Halloween costumes next. Again, we can see that if we simply combine the recommendation results from pump-kin and candy respectively, we may not be able to obtain the right recommendations.

From the above examples, we find that models based on linear combination do have limitations in capturing com-plicated influence of multiple factors on next purchase. In other words, independent assumption among different fac-tors may not be sufficient for good recommendations. We need a model that is capable of incorporating more com-plicated interactions among multiple factors. This becomes the major motivation of our work.
In this section, we first introduce the problem formaliza-tion of next basket recommendation. We then describe the proposed HRM in detail. After that, we talk about the learning and prediction procedure of HRM. Finally, we dis-cuss the connections of HRM to existing methods.
Let U = { u 1 , u 2 , . . . , u | U | } be a set of users and I = { i , i 2 , . . . , i | I | } be a set of items, where | U | and the total number of unique users and items, respectively. For each user u , a purchase history T u of his transaction-s is given by T u := ( T u 1 , T u 2 , . . . , T u t u  X  1 t  X  [1 , t u  X  1]. The purchase history of all users is denoted is to recommend items that user u would probably buy at the next (i.e. t u -th) visit. The next basket recommendation task can then be formalized as creating a personalized total ranking &gt; u,t  X  I 2 for user u and t u -th transaction. With this ranking, we can recommend the top n items to the user. F igure 2: The HRM model architecture. A two-layer struc-ture is employed to construct a hybrid representation over user and items from last transaction, which is used to predict the next purchased items.
To solve the above recommendation problem, here we present the proposed HRM in detail. The basic idea of our work is to learn a recommendation model that can involve both sequential behavior and users X  general taste, and mean-while modeling complicated interactions among these factors in prediction.

Specifically, HRM represents each user and item as a vec-tor in a continuous space, and employs a two-layer structure to construct a hybrid representation over user and items from last transaction: The first layer forms the transaction representation by aggregating item vectors from last trans-action; While the second layer builds the hybrid represen-tation by aggregating the user vector and the transaction representation. The resulting hybrid representation is then used to predict the items in the next basket. The hierar-chical structure of HRM is depicted in Figure 2. As we can see, HRM captures the sequential behavior by modeling the consecutive purchases, i.e. constructing the representation of the last transaction from its items for predicting the next purchase. At the same time, by integrating a personalized user representation in sequential recommendation, HRM al-so models the user X  X  general taste.

More formally, let V U = {  X  X  U u  X  R n | u  X  U } denote all the user vectors and V I = {  X  X  I i  X  R n | i  X  I } denote all the item vectors. Note here V U and V I are model parameters to be learned by HRM. Given a user u and two consecutive transactions T u t  X  1 and T u t , HRM defines the probability of buying next item i given user u and his/her last transaction T  X  1 via a softmax function: where  X  X  Hybrid u,t  X  1 denotes the hybrid representation obtained from the hierarchical aggregation which is defined as follows where f 1 (  X  ) and f 2 (  X  ) denote the aggregation operation at the first and second layer, respectively.

One advantage of HRM is that we can introduce various aggregation operations in forming higher level representa-tion from lower level. In this way, we can model differ-ent interactions among multiple factors at different layers, i.e. interaction among items forming the transaction repre-sentation at the first layer, as well as interaction between user and transaction representations at the second layer. In this work, we study two typical aggregation operations as follows. Note that there are other ways to define the aggregation op-erations, e.g. top-k average pooling or Hadamard product. We may study these operations in the future work. Besides, one may also consider to introduce nonlinear hidden layers as in deep neural network [4]. However, we resort to sim-ple models since previous work has demonstrated that such models can learn accurate representations from very large data set due to low computational complexity [17, 27].
Since there are two-layer aggregations in HRM, we thus can obtain four versions of HRM based on different com-binations of operations, namely HRM AvgAvg , HRM MaxAvg , HRM AvgMax , and HRM MaxMax , where the two abbrevia-tions in subscript denote the first and second layer aggre-gation operation respectively. For example, HRM AvgMax denotes the model that employs average pooling at the first layer and max pooling at second layer.

As we can see, these four versions of HRM actually as-sume different strength of interactions among multiple fac-tors. By only using average pooling, HRM AvgAvg assume independence among all the factors. We later show that HRM AvgAvg can be viewed as some variation of FPMC. B oth HRM AvgMax and HRM MaxAvg introduce partial in-teractions, either among the items in last transaction or be-tween the user and transaction representations. Finally, by using nonlinear operations at both layers, HRM MaxMax as-sumes full interactions among all the factors. In learning, HRM maximizes the log probability defined in Equation (1) over the transaction data of all users as follows where  X  is the regularization constant and  X  are the model parameters (i.e.  X = { V U ,V I } ). As defined in Section 4.1, the goal of next basket recommendation is to derive a ranking &gt; u,t over items. HRM actually defines the ranking as and attempts to derive such ranking by maximizing the buy-ing probability of next items over the whole purchase history.
However, directly optimizing the above objective function is impractical because the cost of computing the full soft-max is proportional to the size of items | I | , which is often extremely large. Therefore, we adopt the negative sampling technique [21, 27] for efficient optimization, which approxi-mates the original objective  X  HRM with the following objec-tive function where  X  ( x ) = 1 / (1 + e  X  x ), k is the number of  X  X egative X  samples, and i  X  is the sampled item, drawn according to the noise distribution P I which is modeled by empirical unigram distribution over items. As we can see, the objective of HRM with negative sampling aims to derive the ranking &gt; u,t in a discriminative way by maximizing the probability of observed item i and meanwhile minimizing the probability of unobserved item i  X  s.

We then apply stochastic gradient descent algorithm to maximize the new objective function for learning the mod-el. Moreover, when learning the nonlinear models, we also adopt Dropout technique to avoid overfitting. In our work, we simply set a fixed drop ratio (50%) for each unit.
With the learned user and item vectors, the next basket recommendation with HRM is as follows. Given a user u and his/her last transaction T u t u  X  1 , for each candidate item i  X  I , we calculate the probability p ( i  X  I | u, T u ing to Equation (1). We than rank the items according to their probabilities, and select the top n results as the final recommendations to the user. In this section, we discuss the connection of the proposed HRM to previous work. We show that by choosing prop-er aggregation operations, HRM subsumes several existing methods including Markov chain model, matrix factoriza-tion model as well as a variation of FPMC model. To show that HRM can be reduced to a certain type of Markov chain model, we first introduce a special aggregation operation, namely select-copy operation. When aggregating a set of vector representations, the select-copy operation s-elect one of the vectors according to some criterion, and copy it as the aggregated one. Now we apply this operation to both levels of HRM. Specifically, when constructing the transaction representation from item vectors, the operation randomly selects one item vector and copies it. When com-bining the user and transaction representations, the opera-tion always selects and copies the transaction vector. We re-fer the HRM with this model architecture as HRM CopyItem . The new objective function of HRM CopyItem using negative sampling is as follows: where  X  X  I s denotes the vector of randomly selected item in last transaction.

Similar as the derivation in [21], we can show that the solution of HRM CopyItem follows that which indicates that HRM CopyItem is actually a factorized Markov chain model (FMC) [23], which factorizes a transi-tion matrix between items from two consecutive transactions with the association measured by shifted PMI (i.e. P M I ( x, y ) log k ). When k = 1, the transition matrix becomes a PMI matrix.

In fact, if we employ noise contrastive estimation [27] for optimization, the solution then follows that: which indicates the transition matrix factorized by HRM CopyItem become a (shifted) log-conditional-probability matrix.
Now we only apply the select-copy operation to the second layer (i.e. aggregation over user and transaction representa-tions), and this time we always select and copy user vector. We refer this model as HRM CopyUser . The corresponding objective function using negative sampling is as follows:
Again, we can show that HRM CopyUser has the solution in the following form:
In this way, HRM CopyUser reduces to a matrix factoriza-tion model, which factorizes a user-item matrix where the association between a user and a item is measured by shifted PMI.
FPMC conducts a tensor factorization over the transition cube constructed from the transition matrices of all users. It is optimized under the Bayesian personalized ranking (BPR) criterion and the objective function using MAP-estimator is a s follows [23]:  X  where  X  x u,t,i denotes the prediction model
To see the connection between HRM and FPMC, we now set the aggregation operation as average pooling at both layers and apply negative sampling with k = 1. We denote this model as HRM AvgAvgNEG 1 and its objective function is as follows  X  where
With Equation (3) and (5), we can rewrite Equation (4) as follows Based on the above derivations, we can see that both HRM AvgAvgNEG 1 and FPMC share the same prediction mod-el denoted by Equation (3), but optimize with slightly dif-ferent criteria. FPMC tries to maximize the pairwise rank, i.e. an observed item i ranks higher than an unobserved item i , by defining the pairwise probability using a logistic func-tion as shown in Equation (2). While HRM AvgAvgNEG 1 also optimizes this pairwise rank by maximizing the probability of item i and minimizing the probability of item i  X  , each de-fined in a logistic form as shown in Equation (6). In fact, we can also adopt BPR criterion to define the objective function of HRM AvgAvg , and obtain the same model as FPMC.
Based on all the above analysis, we can see that the pro-posed HRM is actually a very general model. By introducing different aggregation operations, we can produce multiple recommendation models well connected to existing method-s. Moreover, HRM also allows us to explore other prediction functions as well as optimization criteria, showing large flex-ibility and promising potential.
In this section, we conduct empirical experiments to demon-strate the effectiveness of our proposed HRM on next bas-ket recommendation. We first introduce the dataset, base-line methods, and the evaluation metrics employed in our experiments. Then we compare the four versions of HRM to study the effect of different combinations of aggregation operations. After that, we compare our HRM to the state-of-the-art baseline methods to demonstrate its effectiveness. Finally, we conduct some analysis on our optimization pro-cedure, i.e. negative sampling technique.
We evaluate different recommenders based on three real-world transaction datasets, i.e. two retail datasets Ta-Feng and BeiRen, and one e-commerce dataset T-Mall.
We first conduct some pre-process on these transaction datasets similar as [23]. For both Ta-Feng and BeiRen dataset, we remove all the items bought by less than 10 users and users that has bought in total less than 10 items. For the T-Mall dataset, which is relatively smaller, we remove all the items bought by less than 3 users and users that has bought in total less than 3 items. The statistics of the three datasets after pre-processing are shown in Table 1.
Finally, we split all the datasets into two non overlapping set, i.e. a training set and a testing set. The testing set contains only the last transaction of each user, while all the remaining transactions are put into the training set. h ttp://recsyswiki.com/wiki/Grocery sh opping d atasets http://www.brjt.cn/ http://102.alibaba.com/competition/addDiscovery/index.htm http://www.taobao.com
We evaluate our model by comparing with several state-of-the-art methods on next-basket recommendation:
For NMF, FPMC and our HRM 6 methods, we run several times with random initialization by setting the dimensional-ity d  X  X  50 , 100 , 150 , 200 } on Ta-Feng and BeiRen datasets, and d  X  X  10 , 15 , 20 , 25 } on T-Mall dataset. We compare the best results of different methods and demonstrate the results in the following sections. h ttp://cogsys.imm.dtu.dk/toolbox/nmf/ http://www.bigdatalab.ac.cn/benchmark/bm/bd?code=HRM
The performance is evaluated for each user u on the trans-action T u t u in the testing dataset. For each recommendation method, we generate a list of N items ( N =5) for each user u , denoted by R ( u ), where R i ( u ) stands for the item recom-mended in the i -th position. We use the following quality measures to evaluate the recommendation lists against the actual bought items.
We first empirically compare the performance of the four versions of HRM, referred to as HRM AvgAvg , HRM MaxAvg , HRM AvgMax , HRM MaxMax . The results over three datasets are shown in Table 2.

As we can see, HRM AvgAvg , which only uses average pool-ing operations in aggregation, performs the worst among the four models. It indicates that by assuming independence a-mong all the factors, we may not be able to learn a good rec-ommendation model. Both HRM MaxAvg and HRM AvgMax introduce partial interactions by using max pooling either at the first or the second layer, and obtain better results than HRM AvgAvg . Take the Ta-Feng dataset as an example, when compared with HRM AvgAvg with dimensionality set as 50, the relative performance improvement by HRM MaxAvg and HRM AvgMax is around 13 . 6% and 9 . 8%, respectively. Besides, we also find that there is no consistent dominan-t between these two partial-interaction models, indicating that interactions at different layers may both help the rec-ommendation in their own way. Finally, by applying max pooling at both layers (i.e. full interactions), HRM MaxMax can outperform the other three variations in terms of all the three evaluation measures. The results demonstrate the ad-vantage of modeling interactions among multiple factors in next basket recommendation.
We further compare our HRM model to the state-of-the-art baseline methods on next basket recommendation. Here we choose the best performed HRM MaxMax as the represen-tative for clear comparison. The performance results over Ta-Feng, BeiRen, and T-Mall are shown in Figure 3. We have the following observations from the results. (1) Overall, the Top method is the weakest. However, we find that the Top method outperforms MC on the T-Mall dataset. This might be due to the fact that the items in T-Mall dataset are actually brands. Therefore, the distributions of top popular brands on both training and testing dataset-s are very close, which accords with the assumption of the Top method and leads to better performance. (2) The NMF method outperforms the MC method in most cases. A major reason might be that the transition matrix estimated in the T-Mall.
 Table 3: Performance comparison on Ta-Feng over different user groups with dimensionality set as 50. M C method are rather sparse, and directly using it for rec-ommendation may not work well. One way to improve the performance of the MC method is to factorize the transition matrix to alleviate the sparse problem [23]. (3) By combin-ing both sequential behavior and users X  general taste, FPM-C can obtain better results than both MC and NMF. This result is quite consistent with the previous finding in [23]. (4) By further introducing the interactions among multiple factors, the proposed HRM MaxMax can consistently outper-form all the baseline methods in terms of all the measures over the three datasets. Take the Ta-Feng dataset as an ex-ample, when compared with second best performed baseline method (i.e. FPMC) with dimensionality set as 200, the rel-ative performance improvement by HRM MaxMax is around 13 . 1%, 11 . 1%, and 12 . 5% in terms of F1-score, Hit-Ratio and NDCG@5, respectively.

To further investigate the performance of different meth-ods, we split the users into three groups (i.e., inactive, medi-um and active) based on their activeness and conducted the comparisons on different user groups. Take the Ta-Feng dataset as an example, a user is taken as inactive if there are less than 5 transactions in his/her purchase history, and active if there are more than 20 transactions in the pur-chase history. The remaining users are taken as medium. In this way, the proportions of inactive, medium and active are 40 . 8%, 54 . 5%, and 4 . 7% respectively. Here we only re-port the comparison results on Ta-Feng dataset under one dimensionality (i.e. d = 50) due to the page limitation. In fact, similar conclusions can be drawn from other datasets. The results are shown in Table 3.

From the results we can see that, not surprisingly, the Top method is still the worst on all the groups. Furthermore, we find that MC works better than NMF on both inactive and medium users in terms of all the measures; While on active users, NMF can achieve better performance than MC. The results indicate that it is difficult for NMF to learn a good us-er representation with few transactions for recommendation. By combining both sequential behavior and users X  general taste linearly, FPMC obtains better performance than MC on inactive and active users, and performs better than NMF on inactive and medium users. However, we can see the im-provements are not very consistent on different user groups. Finally, HRM MaxMax can achieve the best performance on all the groups in terms of all the measures. It demonstrates that modeling interactions among multiple factors can help generate better recommendations for different types of users.
To learn the proposed HRM, we employ negative sam-pling procedure for optimization. One parameter in this procedure is the number of negative samples we draw each time, denoted by k . Here we investigate the impact of the sampling number k on the final performance. Since the item size is different over the three datasets, we tried dif-ferent ranges of k accordingly. Specifically, we tried k { on BeiRen, and k  X  X  1 , 2 , 3 , 4 , 5 , 6 } on T-Mall, respectively. We report the test performance of HRM MaxMax in terms of F1-score against the number of negative samples over the three datasets in Figure 4. Here we only show the results on one dimension over each dataset (i.e. d = 50 on Ta-Feng and BeiRen and d = 10 on T-Mall) due to the space limitation.
From the results we find that: (1) As the sampling num-ber k increases, the test performance in terms of F1-score increases too. The trending is quite consistent over the three datasets. (2) As the sampling number k increases, the per-formance gain between two consecutive trials decreases. For example, on Ta-Feng dataset, when we increase k from 20 to 25, the relative performance improvement in terms of F1-score is about 0 . 0011%. It indicates that if we continue to sample more negative samples, there will be less perfor-mance improvement but larger computational complexity. Therefore, in our performance comparison experiments, we set k as 25, 60, 6 on Ta-Feng, BeiRen and T-Mall, respec-tively.
In this paper, we propose a novel hierarchical representa-tion model (HRM) to predict what users will buy in next b asket. Our model can well capture both sequential be-havior and users X  general taste in recommendation. What is more important is that HRM allows us to model complicated interactions among multiple factors by using different aggre-gation operations over the representations of these factors. We conducted experiments on three real-world transaction datasets, and demonstrated that our approach can outper-form all the state-of-the-art baseline methods consistently under different evaluation metrics.

For the future work, we would like to try other aggrega-tion operations in our HRM. We also want to analyze what kind of interactions are really effective in next basket pre-diction. Moreover, we would like to study how to integrate other types of information into our model, e.g. the transac-tion timestamp, which may introduce even more complicat-ed interactions with the existing factors.
This research work was funded by 973 Program of Chi-na under Grant No.2014CB340406, No.2012CB316303, 863 Program of China under Grant No.2014AA015204, Project supported by the National Natural Science Foundation of China under Grant No.61472401, No.61433014, No.61425016, and No.61203298. We would like to thank the anonymous reviewers for their valuable comments.
