 Hao Su  X  1 haosu@cs.stanford.edu Adams Wei Yu  X  2 wyu@cs.hku.hk Li Fei-Fei 1 feifeili@cs.stanford.edu
The sparse-inducing norms have been powerful tools for learning robust models with limited data in high-dimensional space. By imposing such norms as the constraints to the optimization task, one could bias the model towards learning sparse solutions, which in many cases has been proven to be statistically effec-tive. Typical sparse-inducing norms include the  X  1 nor-the former encourages element-wise sparsity and the latter encourages group-wise sparsity. In a variety of contexts, the two types of sparsity pattern exist si-multaneously. For example, in the multi-task learning setting, the index set for features of each task may be sparse and there might be a large overlap of features across multiple tasks ( Jalali et al. , 2010 ). One nat-ural approach is to formalize an optimization prob-lem constrained by  X  1 norm and  X  1 ;q norm together, so that  X  1 norm induces the sparsity in features of each task and  X  1 ;q norm couples the sparsity across tasks( Friedman et al. , 2010 ).
 Projection-based algorithms such as Projected Gradi-ent ( Bertsekas , 1999 ), Nesterov X  X  optimal first-order method ( Beck &amp; Teboulle , 2009 ; Nesterov , 2007 ) and Projected Quasi-Newton ( Schmidt et al. , 2009 ) are major approaches to minimize a convex function with constraints. These algorithms minimize the objective function iteratively. By invoking the projection oper-ation, the point of each iteration is guaranteed to be in the constraint set. Therefore, the projection serves as a key building block for such type of method. In this paper, we study the problem of projecting a point in a high-dimensional space into the constraint set formed by the  X  1 and  X  1 ;q norms simultaneously, in particular, q = 2 or  X  . We choose q = 2 or  X  because these two types of norms are the most wide-ly used group-sparsity inducing norms( Bach et al. , 2011 ). Our Euclidean projection operator P 1 ; 2 (1 ;q )+1 can be formulated as P where c is the point to be projected,  X  x  X  1 ;q and  X  x  X  are the  X  1 ;q norm and  X  1 norm of x (Sec 3 ). We formalize the projection as a convex optimization problem and show that the solution can be parameter-ized by the dual variables and the parametrization has an intuitive geometrical interpretation. Since the dual problem optimizes a concave objective, intuitively we can solve the dual variables through 2-D grid search. However, this method is costly and inaccurate. Fur-ther inspection reveals that seeking the optimal dual variable can be associated with finding the unique root of a 1-D auxiliary function. As a main result of this paper, we prove that this function is piecewise smooth and strictly monotonic. Therefore, it is sufficient to adopt a bisection algorithm to handle it efficiently. We then theoretically analyze its time complexity, which are O ( n + g log g ) for q = 2 and O ( n log n ) for q = Having obtained an efficient projection algorithm, we can embed it in the projection-based optimization methods to efficiently find the  X  X imultaneous sparse X  solution to the following problem: where f ( w ) is a convex function. We illustrate this point by experimentally solving regression problems with the above constraints.
 The main contribution of this paper can be summa-rized as follows. Firstly, we are the first to propose a specific method that is highly efficient in time and memory usage for the composite norm projection prob-lem. Secondly, we derive a bound to the time com-plexity of our algorithm and theoretically show that the algorithm enjoys fast convergence rate. This re-sult is supported by the experiments using synthetic data and real data. There has been a lot of research on efficient projec-tion operators to norm balls. Projection onto the  X  2 ball is straightforward since we only need to rescale the point towards the origin. Linear time projec-tion algorithms for  X  1 norm and  X  1 ; 2 norm are pro-posed by ( Duchi et al. , 2008 ; Liu &amp; Ye , 2009 ) and m, ( Quattoni et al. , 2009 ) proposes a method with O ( n log n ) complexity and ( Sra , 2010 ) introduces a method with weak linear time complexity.
 The problem of projecting a point onto the in-tersection of convex sets has been studied since decades ago. In particular, various alternating direction methods have been proposed( Han , 1988 ; Perkins , 2002 ; Schmidt &amp; Murphy , 2010 ). For exam-ple, Dykstra X  X  algorithm ( Dykstra , 1983 ) and ADMM ( Gabay &amp; Mercier , 1976 ) are variants of the alternat-ing projection algorithm which successively projects the point onto the convex sets until convergence. An-other approach to solve the projection problem is by modeling it as a Second-Order Cone Programming (SOCP) problem and solve it by the Interior Point solver. Although these algorithms could be applied, empirical results reveal their slow convergence rate and poor scalability. Recently, ( Gong et al. , 2011 ) solves the projection onto the Intersection of Hyperplane and a Halfspace by PRF method in linear time via root finding in 1-D space. However, the problem is quite different from ours and their method could not be triv-ially applied. Consequently, a specific method tailored for our problem is needed. We start by introducing the notation and definitions that will be used throughout the paper.
 Given c  X  R n , the  X  1 ,  X  2 and  X   X  norms of c are defined as  X  c  X  1 = max 1  X  i  X  n {| c i |} respectively.
 In addition, the indices of c are divided into g disjoint groups. Thus c can be written as c = [ e c 1 ; . . . ; e where each e c i is a subvector of c . We define the  X  1 ;q norm of c as  X  c  X  1 ;q  X  ball is defined as the intersection of  X  1 -norm ball and  X  1 ;q -norm ball.
 The Euclidean projections onto the  X  1 ,  X  1 ;q and (  X  1  X  1 ;q ) norm balls are denoted as P Finally, we introduce three functions, SGN( c ) =  X  2 , MAX( c , d ) = [max( c 1 , d 1 ); . . . ; max( c and MIN( c , d ) = [min( c 1 , d 1 ); . . . ; min( c n , d c , d i is the i th element of c and d . In this section, we will introduce our approach of Eu-clidean projection onto the (  X  1 +  X  1 ; 2 )-norm ball and (  X  1 +  X  1 ;  X  )-norm ball. Due to space constraints, we leave most proofs in the appendix except Theorem 1 . 4.1. Euclidean Projection on the In this section, we first formulate the projection on the (  X  1 +  X  1 ; 2 )-norm ball as a convex optimization prob-lem (Sec 4.1.1 ). We then parameterize the solution by dual variables and provide an intuitive geometri-cal interpretation (Sec 4.1.2 ). Finally, we determine the optimal dual variable values by finding the unique zero point of a monotonic function with a bisection algorithm (Sec 4.1.3 ). 4.1.1. Problem Formulation ball can be formalized as min Using the following proposition, we can reflect the point c to the positive orthant by simply setting c i := | c | and later recover the original optimizer by set-c  X  0 , i = 1 , 2 , ..., n from now on.
 Proposition 1 Let x  X  be the optimizer of problem ( 3 ) , then x  X  i c i  X  0 , i = 1 , 2 , ..., n . 4.1.2. Parameterizing the Solution by We can parameterize the solution x  X  by the optimal ma, so that the KKT system (Sec A.1 in the appendix) is satisfied ( Bach et al. , 2011 ).
 Proposition 2 Suppose x  X  ,  X   X  1 and  X   X  2 are the primal and dual solutions respectively, then where e e k is a vector of all 1 s which has the same di-mension as e c k .
 The solution has an intuitive geometrical interpre-tation. e x  X  k is obtained by first translating e c k by ing by a factor of max(  X  MAX( e c k  X   X   X  2 e e k , e 0 k The geometrical interpretation is illustrated for the simple case where n = 2 , g = 1 in Figure 1 . Accord-ing to Proposition 1 , it is sufficient to consider the projection in the positive orthant. We divide the re-gion outside the constraint set into three sets (Region I, II and III in Figure 1 ). The projection in Region I corresponds to the degenerated case when  X   X  2 = 0 and thus x  X  = P 2 1 ( c ) ( A 1 is projected to B 1 ). The projection in Region II corresponds to the degenerat-ed case when  X   X  1 = 0, and thus x  X  = P 1 1 ; 2 ( c ) ( A projected to B 2 ). The projection in Region III corre-sponds to the case when  X   X  1 &gt; 0 and  X   X  2 &gt; 0, where this simple setting with only one group, we assume c  X   X   X  reduced to e x  X  1 = SGN( e c 1  X   X   X  2 e e 1 )  X  (  X  e c One can find that e x  X  1 ( OB 3 in Figure 1 ) is actually a contraction of the translated unit vector SGN( e c 1  X   X  e jection path is separated into two segments. The first segment is called Translation Path , which is e c 1  X   X   X  ( A 3 C 3 ) of height  X   X  2 in Figure 1 . The second segment is called Stretch Path , which is C 3 B 3 of length  X   X  1 Figure 1 . 4.1.3. Determining the Dual Variables So far, we have transformed the Euclidean projection problem into determining the optimal dual variables  X  1 and  X  the variables case by case. We first consider the trivial cases when at least one of  X   X  1 and  X   X  2 equals to zero.
Case 1:  X   X  1 = 0 and  X   X  2 = 0. This is the case when c
Case 2:  X   X  1 &gt; 0 and  X   X  2 = 0. This is the case when
Case 3:  X   X  1 = 0 and  X   X  2 &gt; 0. This is the case when Now we discuss the non-trivial case when  X   X  1 &gt; 0 and  X  2 &gt; 0 (Region III in Figure 1 ). According to the complementary-slackness condition of the KKT sys-tem (See (3) and (4) in the appendix), the solution satisfies Substitute ( 4 ) into ( 5 ) and we get the two equations of  X  1 and  X  2 : where S 1 ; 2 = { i |  X  MAX( e c i  X   X  2 e e i , e 0 i )  X   X  } and S i and ( 7 ) simultaneously. ( 6 ) implicitly defines a function  X  1 (  X  2 ) and use this fact we obtain the following equation:  X  1 (  X  2 ) = Note that ( 8 ) does not define an explicit function  X  (  X  2 ) since  X  1 also appears on the right side of ( 8 ). For a detailed proof that  X  1 is an implicit function of  X  , please check Lemma 2 and Lemma 3 in the ap-pendix.
 solving the equation system ( 6 ) and ( 7 ) is equivalent to finding the zero point of the following function: f (  X  2 ) = The following theorem states that f (  X  2 ) is continuous, piece-wise smooth and monotone. The fact immedi-ately leads to a bisection algorithm to efficiently find the zero point.
 Theorem 1 1) f is a continuous piece-wise smooth function in (0 , max { c i;j } ) ; 2) f is monotonically de-creasing and it has a unique root in (0 , max { c i;j } ) . We leave the proof of the continuity and piecewise s-mooth property in the appendix (Lemma 5 in the ap-pendix). Here we just prove the monotonicity of f (  X  2 ). Proof: Because f (  X  2 ) is continuous and piecewise s-f (  X  2 )  X  0 for  X  2  X  R + \E , where E is a set containing finite points as defined in Lemma 4 in the appendix. For such points, by Lemma 4, we can always find an interval ( a, b ) where S 1 ; 2 and S i hence we can denote S 1 = S 1 ; 2 and S i 2 = S i for simplicity.
 Denote  X   X   X  i 1 =  X   X   X  X  X  X  i f (  X  2 ) =  X  there exists one and only one root in [0 , max { c i;j } ]. Given the theorem above, it is sufficient to apply a bisection algorithm to f (  X  2 ) to find its unique root. is not a definition of the function, as we discussed be-fore. Now we introduce an algorithm FindLambda1 to tackle it. More specifically, we first sort all the group-der w.r.t their  X  2 -norms. Then we repeatedly add the group indexes one at a time to the active group set S 1 ; 2 , calculating the corresponding  X  1 and checking its validity. This process stops as soon as  X  1 ,  X  2 and S 1 ; 2 are all consistent.
 Complexity Analysis: Given the tolerance  X  , the bi-section projection algorithm converges after no more 21 in Algorithm 1 ). In each iteration, FindLambda1 dominates the complexity. In Algorithm 2 , line 4 cost-s O ( n ) flops. While additional O ( n ) flops are needed for calculating the  X  2 -norm of each group, the sort-Algorithm 1  X  1 +  X  1 ; 2 Projection ing in line 5 takes O ( g log g ) flops. Finally, the com-plexity of line 7 to line 14 is O ( g ). Therefore, the overall time complexity for the projection algorithm is  X  log 2 [max i ( c i ) / X  ]  X  X  O ( n + g log g ). 4.2. Euclidean Projection on the The projection onto the (  X  1 +  X  1 ;  X  )-norm ball could also be addressed by a bisection algorithm. We first introduce a variable d , and then give an equivalent formulation of the projection problem ( 1 ) for q =  X  as follows: Note that the formulation above differs from ( Quattoni et al. , 2009 ) in the additional term  X  x  X  1  X  . Similar to Sec 4.1.2 , given the KKT system (in the appendix), we can parameterize the solution x  X  using d  X  and optimal dual variables  X   X  Proposition 3 Suppose x  X  , d  X  and  X   X  1 ,  X   X  2 are the primal and dual solution respectively, then where S i { j | c The proposition above reveals that x  X  i;j , d  X  and  X   X  1 all be viewed as functions of  X   X  2 . We can substitute the above equations into the KKT system and show that  X  2 is the zero point of the following function h (  X  2 ) = We can prove that h (  X  2 ) is a strictly monotonically decreasing function: Theorem 2 1) h is a continuous piece-wise smooth function in (0 , max { c i;j } ) ; 2) h is monotonically de-creasing and it has a unique root in (0 , max { c i;j } ) . Complexity Analysis: Based upon the above the-orem, we can determine  X   X  2 using the bisection Algo-termined, ( Quattoni et al. , 2009 ) shows that  X   X  1 and d can be solved with time complexity O ( n log n ). There-fore, the total time complexity of the bisection algo-rithm is  X  log 2 [max i;j ( c i;j ) / X  ]  X  X  O ( n log n ). In this section, we demonstrate the efficiency and effec-tiveness of the proposed projection algorithm in exper-iments using synthetic and real-world data. Due to the space limitation, we only show the result of  X  1 +  X  1 ; 2 and the case of  X  1 +  X  1 ;  X  is shown in the appendix. 5.1. Efficiency of the Proposed Projection We first compare our methods to Interior Point (IP) method and alternating projection methods which are also applicable in solving the Euclidean projection problem. For IP, we use a highly optimized commercial software MOSEK 1 . For alternating projection method-s, as there is no algorithm specifically tailored for our problem, we compare with two widely used represen-tative algorithms  X  Dykstra X  X  algorithm and ADMM algorithm. Both algorithms generate a sequence of points whose limit is the orthogonal projection onto the intersection of convex sets 2 . Note that all methods in the comparison apply P 2 1 (  X  ) in Region I and P 1 1 ; 2 (  X  ) in Region II. Therefore, we first show the time cost for the shared modules P 1 1 ;q ( and P 2 1 (  X  ), and then compare the different projection methods in Region III. To estimate the expected run-ning time of each method, we estimate the volume of the three regions by Monte Carlo method with uniform sampling distribution.
 We generate synthetic data with different number of groups g and dimensions n . Specifically, each dimen- X  1 = 5 ,  X  is run for 10 times to calculate the average running time and standard deviation. To estimate the area of each region, we sampled 10,000 i.i.d points.
 Algorithm 2 FindLambda1 Algorithm 3  X  1 +  X  1 ;  X  Projection 1: Input: c , group Index of each c i ,  X  1 ,  X  2 ,  X  . 2: Output:  X  2 ,  X  1 , x . 3: Initialization; 4: while | h (  X  2 ) | &gt;  X  do 5: if h (  X  2 ) &gt;  X  then 6: lef t =  X  2 ; 7: else 8: right =  X  2 ; 9: end if 10:  X  2 = ( lef t + right ) / 2; 11: for each i, j do x i;j = max( c i;j  X   X  2 , 0); 12: [ x , d ,  X  2 ,  X  1 ] = P 1 (1 ;  X  ) ( x ); 13: Evaluate h (  X  2 ); 14: end while In our proposed projection method, as in each step the bisection algorithm halves the range of  X  2 , we stop when the range is smaller than 10  X  9 . For Dykstra X  X  algorithm and ADMM algorithm, we stop when the  X  2 -norm of two consecutive projected points falls below 10  X  9 . For IP, we stop it if the duality gap is below 10  X  9 . Table 1 summarizes the results for q = 2. We can observe from the table that: 1) our proposed technique is significantly faster than IP and alternating projection methods (Dykstra and ADMM) in Region III; 2) our method scales well to large problems. Compared with IP, which is the runner-up algorithm in speed, our algorithm is more memory efficient since very little extra space is needed, whereas IP introduces several groups of auxiliary variables.
 Compared with Dykstra X  X  algorithm and ADMM algo-rithm, our method takes much less iterations to con-verge. As we discussed before, the number of iterations of our algorithm is bounded by  X  log 2 [max i ( c i ) / X  ] When n = 1000 and g = 100, it can be calculated that our algorithm takes no more than 30 iterations. On the other hand, empirical study shows that Dykstra X  X  algorithm takes 692  X  247 iterations to converge in Re-gion III. A closer study also shows that Dykstra X  X  algo-rithm suffers from a high variance in iterations, which may be related to the order of projections. For exam-ple, 692  X  247 iterations are taken if we first project onto the  X  1 -norm ball, whereas 3460  X  565 iterations are taken if we first project onto the  X  1 ; 2 -norm ball. We also analyze the volume of each region by Monte Carlo sampling method. Table 2 indicates that the probability for a point falling in Region III may be very high. Because our algorithm runs much faster than competitors in Region III, its expected running time can be much shorter in general. 5.2. Efficiency of the Projection-based In this section, we show that embedded with our pro-posed projection operator, various projection-based method can efficiently optimize the composite nor-m constrained linear regression problem. These projection-based methods significantly outperform the baseline method in speed.
 We embed our projection operator into three types of projection-based optimization framework, including Projected Gradient method (PG), Nesterov X  X  optimal first-order method (Nesterov) and Projected Quasi-Newton method (PQN).
 We synthesize a small-sized data and a medium-sized data. For the small-sized data, we adopt the exper-iment setting in ( Friedman et al. , 2010 ). We create the coefficient vector w  X  R 100 divided in ten blocks of ten, where the last four blocks are set to all ze-ros and the first six blocks have 10, 8, 6, 4, 2 and 1 non-zero entries respectively. All the non-zero coeffi-cients are randomly chosen as  X  1. Then we generate N = 200 observations y  X  R 200 by setting y = X w +  X  , where X  X  R 200  X  100 denotes the synthetic data points of standard Gaussian distribution with correlation 0.2 within a group and zero otherwise and  X   X  R 200 is a Gaussian noise vector with standard deviation 4 . 0 in each entry. For the medium-sized data, the data generation process is similar except that the first six blocks have 100, 80, 60, 40, 20 and 10 non-zero entries respectively for 4000 observations.
 Since the generated w exhibits sparsity in both group level and individual level, it is natural to recover w by solving the following constrained linear regression problem ( q = 2 or  X  ): min We choose Interior Point method as the baseline to solve the above problem for its efficiency. Embedded with our efficient projection operator, all projection-based algorithms (PG, Nesterov and PQN) take much less time and memory than IP to converge to the same accuracy (10  X  9 ) (see Table 3 for time used). We note that, projection-based methods usually take much more iterations to converge than IP (see Fig-ure 2 , and the projection operator may be invoked several times per iteration. Hence, the efficiency of the projection operator greatly impact the performance. 5.3. Classification Performance in Multi-task In this experiment, we show that using our efficient projection operator, with limited additional time cost, composite norm regularizer outperforms single norm regularizer in multi-task learning. In the multiple-task learning setting, there are r &gt; 1 response variables (each corresponding to a task) and a common set of p features. We hypothesize that, if the relevant features for each task are sparse and there is a large overlap of these relevant features across tasks, combining  X  1 ;q norm and  X  1 norm will recover both the sparsity of each task and the sparsity shared across tasks. We use handwritten digits recognition as test case. The input data are features of handwritten digit-s (0-9) extracted from a collection of Dutch utility maps( Asuncion &amp; Newman , 2007 ). This dataset has been used by a number of papers as a reliable dataset for handwritten recognition algorithms( Jalali et al. , 2010 ). There are r = 10 tasks, and each sample con-sists of p = 649 features. We use logistic regression as the classifier and constrain the classifier by  X  1 nor-m,  X  1 ;q norm and  X  1 +  X  1 ;q norm, respectively. PQN method is used to optimize the objective function. We compare the running time and classification perfor-mance of each method. The classification performance is measured by the mean and standard deviation of the classification error. Results are obtained from ten ran-dom samples of training and testing data with param-eters chosen via cross-validation in all methods. Us-best classification result with similar running time to  X  1 ;q -norm (Table 4 ). We also test replacing our projec-tion algorithm by the runner-up algorithm in Table 1 , which is IP for q = 2 and Dykstra for q =  X  . Unfor-tunately, using these projection operators, PQN could not converge within 30 minutes. These results show that a more structured yet complicated regularizer is more effective in a multiple-task learning problem and our efficient projection algorithms make it feasible.
