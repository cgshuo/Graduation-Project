 1. Introduction the RF process: (a) the number of relevant documents (the more the better), and (b) the diversity of the documents (the more diverse the better).
 ranked documents contain more relevant information. In some cases, the top ranked documents are very similar to each  X  to judge for relevance.
 documents. Hence, it can increase the diversity of the documents being judged.
Sections 4 and 5 concludes the paper. 2. Related work
In this section we review some active feedback algorithms which do not use the top N values for other collections.
 rithms. One is Gapped-Top-N rf and the other one is N rf
Goldstein, 1998 ) is also described in this section. 2.1. Gapped-Top-N rf
In the Gapped-Top-N rf algorithm, instead of judging the top N example, if G = 2, the set of judged documents will have rank numbers 1, 4, 7, ... , N
G = 0, the Gapped-Top-N rf is essentially the baseline RF algorithm using the top N algorithm can be thought of clustering the top ( G +1) N rf by skipping documents with little difference in their relevance scores. 2.2. N rf -Cluster-Centroid
In order to directly capture diversity, explicit clustering is performed among the top N
N documents and the distance function is the J-Divergence measure ( Lin, 1991 ). Note that when N ter-Centroid is the same as the baseline RF algorithm. 2.3. Maximal marginal relevance (MMR) selects a document d i in each of the iterations by optimizing: where J is the set of currently judged documents in the RF process, q is a query, Sim score) given by a retrieval model, Sim 2 ( d i , d j ) is a similarity measure between two documents d (max{ Sim 2 } is low).
 document to be judged is always the one ranked the first in the retrieval list. Note that the values of Sim changed for all documents throughout the iteration process. Therefore, when k baseline RF algorithm which the top N rf ranked documents are judged by the user. are different. As a result, k MMR = 1 will not produce the same result as the baseline RF algorithm. 3. Split-list approach to relevance feedback algorithm is an iterative one which takes one document for relevance judgement in each of the iterations until N of re-ranking the documents. 3.1. Definition of document-context words are removed from all the documents in our experiments.

For a query q with s distinct terms { q 1 , q 2 , ... , q window size w . As a result, there is a total number of  X  2 s  X  C terms ( Fig. 2 ). 3.2. Ranking of document-contexts ument-contexts in each of the lists are ranked. We extract document-contexts from the top N d [ k ]= q i ,1 6 i 6 s } are inserted to the list of q i d [ k +1]= q i +1 ,1 6 i &lt; s } are inserted to the list of q d [ k ]= q i , d [ k + p ]= q j ,1 6 i 6 s ,1 6 j 6 s , i  X  j , p with distance less than w .

For example, if q 1 occurs in a document for three times, q document-contexts added to the list for q 1 , five document-contexts added to the list for q will be added to the list for the phrase or proximity.
 the available relevance information. For clarity of presentation, we only discuss the ranking for the list of q ranking for other lists are done similarly. We define the score of a context c ( d , k ) in the list of q evance of the context using the log-odds: the p th position in the context c ( d , k ) and / is the rank-equivalence relation. Denote d documents and d split_irl to be the set of judged non-relevant documents. For a term t : where f ( t , c ( d , k )) is the occurrence frequency of the term t in the context c ( d , k ), a of the scores in the list. We rank the contexts in each of the lists.

After the contexts in each of the lists are ranked, the scores of the top N sign weights to different lists, for example, the weight w gether when combining context scores.

Top N c ranked contexts in each of the lists are extracted and scores of contexts belonging to the same document are summed together. The document with the highest score is used for RF. If N ends. 3.3. Bootstrapping
During the iterative process, if the number of relevant contexts of a particular list, say the list of q major problem for poor retrieval performance. The similarity of contexts is calculated using log-odds: the  X  X  X elevant X  X  contexts of q 1 for raw frequency counting. 4. Experiments 4.1. Set up and pairs of query terms during retrieval. The number of document-contexts increases with the number of query terms. the query and a higher IDF value. We use the 418 stop-words from the Lemur Toolkit. tistical significance between the MAP measures for different algorithms.
 feedback algorithm finding more relevant documents for the user to judge will have a higher P@20.
N prf documents are ranked using the BM25 weights. Top K prf the RF process begins. In RF, N rf documents are judged. K evant documents respectively. Terms from judged relevant and irrelevant documents are interpolated together with the tween different RF algorithms is the set of N rf judged documents. The baseline RF algorithm takes the top N ments for feedback while other active feedback algorithms have their own set of N changing the values of a set of parameters can lead to a larger change in performance (e.g., 4% in MAP). 5. Experimental results have more relevant documents for the subsequent RF algorithms.
 rithm, GAPPED is the Gapped-Top-N rf algorithm, CLUSTER is the N ginal relevance algorithm, MMR-Rerank is the maximal marginal relevance algorithm with re-ranking of documents and those without re-ranking. Particularly, in TREC-2005, MMR-Rerank on average chooses 7% more relevant documents than BASELINE, and SPLIT-LIST on average chooses 13% more relevant documents than BASELINE. Generally the P@20 of MMR-tween SPLIT-LIST and BASELINE shows that SPLIT-LIST can find documents that are different from the top N hence increasing the diversity of the judged documents.

Note that the lower P@20 of GAPPED and CLUSTER in Table 4 reveals the smaller number of relevant documents chosen with high diversity although the number of relevant documents is smaller.
 We mainly compare our results (SPLIT-LIST) with those using MMR-Rerank since both of them have a re-rank step. MMR-sets of queries and collections.

Table 6 shows the number of queries in SPLIT-LIST which perform better than BASELINE/MMR-Rerank. Each of the col-while 50% or more queries in SPLIT-LIST perform better than MMR-Rerank in all tested collections. with length greater than 2.
 that only half of the documents selected by MMR-Rerank and SPLIT-LIST are the same. ments than non-relevant ones and at the same time the user does not want to sacrifice the diversity of documents,
SPLIT-LIST can be used to provide more relevant documents. 6. Summary have also experimented with different active feedback algorithms, including the Gapped-Top-N two versions of Maximal Marginal Relevance (MMR). From the experimental results, algorithms with a re-ranking step show that some active feedback algorithms (Gapped-Top-N rf and this is done more reliably than MMR-Rerank on different TREC collections. Acknowledgement na (Project No. PolyU 5259/09E). We thank Prof. W. Gao and the anonymous reviewers for providing helpful comments to improve the paper.
 References
