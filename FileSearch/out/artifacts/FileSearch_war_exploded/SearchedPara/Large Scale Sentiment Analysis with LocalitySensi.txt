 With the rapidly growing of new media data such as Twitter, Facebook, Weibo, more and more people express their opinions or attitudes towards a topic on the Internet. The large-scale text data was quite useful for a commercial company to identify the users X  attitudes with their products and services, and develop a bet-ter marketing strategy and product [ 5 ]. Besides, the administrations could find quick insights about public opinions for an event, and even have more insightful conclusions for psychology and sociology. As the volume of text data becomes larger and larger, large-scale sentiment analysis will play an important role in the field of natural language processing. Sentiment classification (i.e., whether the sentiment orientation of the text is positive/negative) is one of the most important tasks in sentiment analysis. Many machine learning approaches have been applied to this task, where the representation of a document plays a key role in classification. Bag of words, N-grams are simple and effective ways to build language models. However, these representations need large amounts of memory for large-scale classification. As the scale of text data on the Internet becomes larger, there is an emerging need to scale up sentiment analysis methods. In the filed of sentiment analysis, researchers are usually aiming at improving the classification accuracy [ 7 , 13 ], but there is little literature about reducing the storage for large-scale corpus. In this paper, we design a method which incorporates sentiment analysis with our proposed Locality Sensitive One-Bit Min-Hash (BitHash) method. BitHash compresses each data sample into a compact binary hash code while preserving the pairwise similarity of the original data. The binary code can be used as a compressed and informative representation in replacement of the original data for subsequent processing. By using the compact hash code, the storage space can be significantly reduced.
 As we know, finding the nearest neighbor and measuring the distances between the instances are fundamental steps for machine learning methods. For large scale dataset, comparing the query with each sample in the dataset is infeasible, because as for natural language processing and computer vision will also suffer from curse of dimensionality, because the words and visual descriptors might have millions of dimensions. So there exist many works about approximate nearest neighbors methods which can significantly reduce the complexity of the exact nearest neigh-bors. Min-hash is one of such methods, which is simple and has been largely used in the search engine and clustering tasks. Inspired by [ 4 ], which demonstrates a hash-ing learning method. It proves that B-Bit Min-Hash method X  X  estimators could be naturally integrated with learning algorithms such as SVM.
 In our paper, we propose a new approach BitHash, which can compress each data sample into a compact binary hash code while preserving the pairwise sim-ilarity of the original data. We rigorously analyze the variance of BitHash, show-ing that as pairwise Jaccard similarity increases, the variance ratio of BitHash over the original min-hash decreases. BitHash could easily be integrated with linear learning machine like SVM. It could significantly reduce feature dimen-sions as a new representation method, and help reduce the storage for large-scale sentiment analysis substantially.
 We have three key contributions in this paper. Firstly, we are the first to combine Locality Sensitive Hashing technique to scale up sentiment analysis; Secondly, we propose One Bit Min-Hash (BitHash) method, which provides an unbiased estimate of pairwise Jaccard similarity, and the estimator is a linear function of Hamming distance. Finally, we apply BitHash into sentiment analy-sis, which can significantly reduce the feature dimensions as a more compressed and informative representation method, and help reduce the storage for large-scale text substantially.
 the fundamental Locality Sensitive Hashing (LSH) technique Min-Hash method. After brief reviewing Min-Hash, we introduce BitHash in Sect. 3 , and rigorously analyze its variance. We conduct Jaccard similarity estimation experiment in Sect. 3.3 , to verify the variance analysis in Sect. 4 . In Sect. 5 , we describe how to Integrate Locality Sensitive BitHash representation with SVM to deal with the large-scale text sentiment classification problem. In Sect. 6 ,weshowour experimental results on IMDB movie reviews dataset. Finally, Sect. 7 concludes this paper. 2.1 Sentiment Analysis Much research about sentiment analysis has been done in the past years, and [ 5 , 9 ] provide a comprehensive overview about sentiment analysis methods, and we know that machine learning methods are widely used and achieve state-of-the-art experiment results.
 Notations. Sentiment classification could be viewed as a machine learning prob-lem. More formally, the problem can be formulated as follows, given a dataset ples, and y ( i )  X  X  +1 ,  X  1 } is the sentiment orientation label of x ( i ). We train two models: p + ( x | y =+1)for { x ( i ) subject to y ( i )=+1 entation could be classified by computing the ratio r = p  X  1)  X  p ( y =+1) /p ( y =  X  1). If r&lt; 1, then x is assigned to negative class, otherwise to the positive class.
 Representation and Classification Methods. Ngram is a simple but effec-tive language model, and easy to be combined with classifiers. [ 9 ] claimed that SVM method with Unigrams could achieve the best result for sentiment analysis, when comparing with Naive Bayes and other Ngram features. [ 7 ] compared differ-ent Ngrams patterns with SVM, which shows Unigrams + Bigrams + Trigrams achieves the best performance when comparing with less Ngram combinations. However, the Ngram representation suffers from the curse of dimensionality, which is sparse and needs large memory space. Naive Bayes Support Vector Machine (NB-SVM) [ 12 ] applied NB log-count ratio as feature with LibLinear SVM classifier. It yields strong baseline result and the output even can beat many intricate approaches.
 which has better performance than Ngram. However, For the individual model, NB-SVM with Trigram outperforms other representation and classifier [ 7 ], while the paper ensembles different model together and achieves state-of-the-art per-formance. However, those approaches focus on the effectiveness of sentiment analysis methods, and the curse of the dimensionality has not been solved when meeting with large-scale text datasets.
 For the dimensionality reduction, we think of an idea which incorporating hash technique into sentiment analysis feature representation. [ 4 ] shows that hashing technique could be used in machine learning area theoretically. In this paper, we exploit One-Bit Min-Hash (BitHash) to generate binary hash code, and apply it to sentiment analysis. By using the BitHash codes representation, the storage space for strong hash codes is largely reduced, and our method provides an unbiased estimate of pairwise Jaccard similarity, and the estimator is a linear function of Hamming distance, which is very simple. 2.2 Min-Hash Review Before introducing BitHash method, we take a brief review of Min-Hash, which is a building block of our proposed BitHash.
 Min-hash [ 1 ] is a popular hashing method for Jaccard similarity, which is the most widely-used pairwise similarity between two sets A and B , defined as For data of bag-of-words representation, e.g. texts can be represented as sets of words or Ngrams, denote the dictionary (the whole set of elements) by W of which the cardinality is | W | = d . We assign each word with a unique ID from the non-negative integer set I = { 0 , 1 , 2 ,...,d  X  1 } is represented by a subset of non-negative integers in I .
 Min-Hash outputs a hash value by first generating a random permutation  X  : I  X 
I , and then taking the smallest permuted ID, i.e. min (  X  ( S )) := min It is proven [ 1 ] that for any two non-empty sets A and B , Define random variable Then correspondingly X  X  1 ,X  X  2 ,...,X  X  K . The estimator of Min-Hash is which is an unbiased estimator of J ( A, B ), and its variance is hash integers, which couldn X  X  be adapted directly with linear learning method such as SVM. So we propose to use a new approach called BitHash to deal with this problem. 3.1 Sentiment Classification Framework with BitHash The general framework of the sentiment classification with BitHash is shown in Fig. 1 . Documents are first represented as sets of N-grams, then each N-gram is replaced by its index (an nonnegative integer) in the dictionary, which is built based on a training set of documents. Now each document is represented by a set of nonnegative integers. To get a compact and similarity-preserving representation of the set, we transform it into a binary string using BitHash. Finally, we feed the (Extended) BitHash code into a classifier which predicts sentiment orientations. 3.2 BitHash BitHash is short for One-Bit Min-Hash, which is based on Min-Hash, while producing more compact hash code. One shortage of Min-Hash is that each of its hash value is an integer represented by multiple bits (e.g. 32 bits or 64 bits in modern architecture). It would be desirable to find a family of hash functions for Jaccard similarity that produce 1 bit per hash value. Fortunately, theoretical result shows that such a family of binary hash functions exists. Charikar [ 2 ] proves that any Locality-Sensitive Hash (LSH) family H that has the following property induces a binary LSH family  X  H s.t.
 And h  X  X  and b  X  X  , where B is a pairwise independent family of hash functions that map the elements in the integer set I to { 0 , 1 } . Formally, This construction of  X  H is given in the proof of the Lemma 2 in [ 2 ]. Since Min-Hash satisfies ( 2 ), and thus it satisfies the property ( 6 ). Therefore the theory guarantees that it is possible to construct a binary hash function family for Jaccard similarity.
 Without loss of generality, we assume that the cardinality of W is even. In practice, we may construct a hash function  X  h  X   X  H using two random permuta-tions  X  and  X  in the following way:  X  Apply  X  to data and generate a Min-Hash value y , which is an integer;  X  X se  X  to map the integer y to another integer z ;  X  X f z is even, output 1, otherwise output 0.
 Formally, the construction of a BitHash function  X  h is where Generate independently  X  1 , X  2 ,..., X  K and  X  1 , X  2 ,..., X  struct K independent BitHash functions  X  h 1 ,  X  h 2 ,...,  X  h ), and thus  X  h outputs K-bits. For two data A and B , we have that where d Hamming (  X  ,  X  ) measures the Hamming distance between the two input binary strings. Therefore the estimator with respect to K functions given by BitHash is 3.3 Variance of Bit-Hash In this subsection we analyze the variance of the estimator E BitHash. The variance of d Hamming (  X  h ( A ) ,  X  h ( B )) is Therefore the variance of the estimator E 1  X  bit,K given by BitHash is Thus the variance ratio of BitHash over Min-Hash is of BitHash is around 2 times that of the original Min-Hash. The fact that the variance of BitHash is larger than that of Min-Hash is not surprising since each hash value produced by BitHash only contains a single bit, while each Min-Hash value usually has 32 or 64 bits. In fact, the variance gap can be compensated by using 2 K BitHash functions, when K hash functions are required by Min-Hash. More concretely, the variance of BitHash with 2 K hash functions is about the same as that of Min-Hash with K hash functions, when the pairwise Jaccard similarity J ( A, B ) to estimate is very close to 1, while the space for storing the hash values is reduced by a factor of 16 or 32 by using BitHash. Another advantage of BitHash is that its estimator is very simple, of which the main part involves computing a Hamming distance which can be computed very fast. This is because the computation of Hamming distance can be accomplished with a bitwise exclusive-or (XOR) operation followed by a non-zero bit counting operation, both of which can be executed very efficiently by modern CPUs. The framework of generating BitHash codes from text is shown in Fig. 1 . In this section we conduct experiments to show the accuracy of BitHash and Min-Hash in estimating pairwise Jaccard similarity. We use synthetic datasets in this experiment. The data are generated as binary strings of fixed length d = 1000. Each bit of the binary string has a constant probability p to be 1 or otherwise 0. As p approaches 1, more and more pairs of data will highly overlap and have high Jaccard similarity. For each p , we generate a set of 100 data with fixed length d , and compute the true pairwise Jaccard similarity as the ground truth. Then we apply both methods, namely, BitHash and Min-Hash, with fixed number K of hash values, to approximate the similarity using their estimators to 500. For each set of p and K , we repeat the test 100 times. We measure the estimation error by mean-squared error (MSE).
 Table 1 shows that when the pairwise Jaccard similarity is very high, the estimation MSE of BitHash is about 2 times that of Min-Hash, which verifies the variance analysis in Sect. 3.3 . Tables 1 , 2 , 3 and 4 show that as p decreases, the MSE ratio of BitHash over Min-Hash increases, because the pairwise Jaccard similarity to estimate decreases. With p fixed, as the number of hash values K gets larger, the MSEs of both methods get smaller proportionally. Machine Learning algorithms like SVM and logistic regression are extremely popular. There are a lot of open source softwares which provides the tools for us, such as LibLinear [ 3 ], Pegasos [ 10 ], etc.
 The L 2 -regularized SVM solves the following optimization problem: Given a dataset { ( x i ,y i ) } n i =1 ,x i  X  R D ,y i  X  X  X  of the dataset. C&gt; 0 is a penalty parameter to avoid overfitting. Since the computation of inner product in SVM is different from Hamming distance, BitHash code cannot be directly fed to SVM as input. To incorporate the BitHash code with SVM, after representing each data sample with BitHash code, we extend it into another binary string to feed into SVM, as inspired by [ 4 ]: if a BitHash value is 0, we extend it to 01, otherwise we extend it to 10. For example, if a BitHash code is 011001, then the extended binary code to feed into SVM is 011010010110. Table 5 shows some examples illustrating the process of transforming the BitHash codes to extended codes. There is a simple relation between the inner product of two extended binary codes and the Hamming distance of two corresponding BitHash codes: their sum equals K, the BitHash code length.
 In this section, the effectiveness of our proposed framework, linear machine learn-ing method with BitHash, is demonstrated. The experiments are conducted on a well-known IMDB movie reviews dataset [ 6 ]. 6.1 Experiment Setting Data Set and Experiment Setup. The Stanford IMDB movie reviews dataset [ 6 ] has been widely used in sentiment analysis. The dataset consists 50,000 full-length movie reviews. And their sentiment orientation have already been labelled as either positive or negative. For the supervised learning methods, the dataset needs to be splitted into training and test sets. In order to remove the uncertainty of the data split, we randomly select 30,000 reviews out of the datasets, and a five-fold validation method is applied in our experiments. Four folds are used for training and the rest one fold is for testing.
 represented by a set of its Unigram, Bigrams and Trigrams tokens. The average unique N-grams (Unigram, Bigrams and Trigrams) dictionary length (feature dimensions) is about 4.3 million. For the traditional N-gram representations, each N-gram is a float or integer number, will take 4 bytes (32 bit) space. So the average storage cost for each review is about 140 million bits. In our experiments, we test the results when using BitHash representations of various lengths K (number of bits), ranging from K = 100 to K =20 , 000. The L SVM in LibLinear [ 3 ] is used as the linear learning algorithm in our experiments. Vector Machine (NB-SVM) as our baseline. NB-SVM applies NB log-count ratio for each N-gram as feature with SVM, and the output even can beat other intricate approaches [ 7 , 12 ]. 6.2 Experiment Results The accuracy of sentiment classification and the Storage-Reduction-Ratio (SRR) are two key metrics in our experiments. We use them to make a comparison from the algorithm effectiveness and storage consumption reduction respectively. SRR is the ratio between the storage requirement of original Ngrams representation and BitHash codes.
 The experiments by using linear SVM with BitHash have been conducted. The accuracy and SRR of sentiment analysis using various length of BitHash representation are shown in Figs. 2 , 3 and Table 6 . Under different length of BitHash codes, as the length K increases, the accuracy of sentiment classification gradually grows and approaches state-of-the-art, NB-SVM, while the Storage-Reduction-Ratio decreases (shown in Table 6 ).
 Storage-Reduction-Ratio is more than 6,000, or in other words, the storage requirement of BitHash is at most 1 6000 that of Ngrams-like representation which NB-SVM is based on. If Ngram token is stored with 32 bits float or integer, then the storage for a review needs D  X  32 bits, where D is the size of dic-tionary. In our experiment, D =4 , 287 , 237, so the storage requirement for a review is 13 , 719 , 584 bits. Therefore, by using BitHash with K =20 , 000, the storage requirement is reduced by a factor of 6 , 860. Table 6 shows the Storage-Reduction-Ratio results for various BitHash code length K . In this paper, we scale up sentiment analysis with our proposed Locality Sensitive BitHash method. BitHash could compress each data sample into a compact binary hash code while preserving the pairwise similarity of the original data. The binary code can be used as a compressed and informative representation in replace of the original data, and it can be easily integrated with linear learning classifier. Our experiments results show that BitHash method reduces the storage requirement by a factor of more than 6 , 000, while achieving satisfiable accuracy result. We believe our framework may provide an inspiring insights for large-scale sentiment analysis researches.
 information to our framework, as well as make a distributed version, which is easy to be used as a fundamental work for large-scale sentiment analysis researchers.
