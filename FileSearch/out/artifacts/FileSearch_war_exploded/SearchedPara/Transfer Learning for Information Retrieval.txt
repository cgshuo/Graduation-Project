 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Information retrieval, Learning to rank, Transfer learning Learning to rank algorithms (LTR) are techniques that uti-lize machine learning to learn ranking functions for particu-lar document and topic sets. LTR models are designed and trained to optimize the effectiveness of the ranking function on the assessed topics of a document set. One challenge of applying LTR on information retrieval systems is obtaining the relevance judgments. Transfer learning is a technique that can be used to train models for new datasets ( target domain ) using related datasets ( source domain ), under the circumstances where limited training data is accessible, and thus can help training LTR models in similar situations. The aim of this research is to study the issues of transfer learn-ing techniques for information retrieval (TLTR) and develop new TLTR algorithms to help improve the effectiveness of information retrieval systems.

The idea of applying transfer learning to LTR is not new but requires deeper exploring. Transfer learning techniques have been extensively studied in machine learning commu-nity [2]. However, the study of applying transfer learning to LTR had not drawn too much attention until recent years. A few attempts have been tried. However, better under-standing of the issues is needed for future research.
To better understand TLTR and develop new TLTR algo-rithms, several research questions are to be answered. Here we list the research questions we want to address in this study. Q1. What are the factors that can affect the effec-tiveness of LTR algorithms on a dataset? Q2. Is it neces-sary to apply TLTR? Moreover, under what circumstances TLTR is needed? Q3. How should the relatedness of LTR tasks be measured? Q4. What is the difference between con-ventional transfer learning and TLTR? Q5. How to tackle so-called heterogeneous TLTR problems, where the feature spaces of datasets are different? Q6. How to measure the effectiveness of TLTR models?
We tested the generalization of LTR models by training and testing in different datasets. We validated that the per-formances of LTR models will be leveraged when applying to a different dataset. Study of the stability of LTR mod-els with respect to training data will also provide evidence that standard LTR is not sufficient to improve ranking ef-fectiveness under certain circumstances. Our study showed that training LTR models do not require too many queries, but these queries should better represent the queries in the entire collection. The finding also suggests that identifying similar queries or query groups in the source domain might be a clue to solving TLTR problems. We proposed a TLTR algorithm that trains LTR models with queries that are the most similar to source domain queries, from the source do-main. These queries are weighted with importance, calcu-lated by query similarities. This approach is similar to most instance-weighting based TLTR algorithms [1, 3], but the algorithm we proposed requires no relevance judgment from the target domain.

Our study showed that transferring LTR models between different document sets might have a more terrible effect. However, our experiments are limited by lacking a pair of test collections with the same query set but different doc-ument sets. A further step of the research is to construct two test collections with different document sets, yet keep all other factors the same. The influences of the divergence between document sets on LTR algorithms will be studied based on the datasets. As mentioned before, heterogeneous TLTR is an area yet to be well explored. The study of how to tackle heterogenous TLTR is also under the scope of this study.

