 yeshen@microsoft.com In this paper, we propose a ne w latent semant ic model that incorporates a convolutional-pooling structure over word sequences to learn low-dime nsional, semantic vector representations for search queries and Web documents. In order to capture the rich contextual structures in a query or a document, we start with each word within a te mporal context window in a word sequence to directly capture contextual features at the word n-gram level. Next, the salient word n-gram features in the word sequence are discovered by the mode l and are then aggregated to form a sentence-level feature vector. Finally, a non-linear transformation is applied to extract high-level semantic information to generate a contin uous vector representation for the full text string. The proposed convol utional latent semantic model (CLSM) is trained on clickthrough data and is evaluated on a Web document ranking task using a large-scale, real-world data set. Results show that the proposed m odel effectively captures salient semantic information in queries and documents for the task while significantly outperforming previ ous state-of-the-art semantic models. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Convolutional Neural Network; Semantic Representation; Web Search Most modern search engines resort to semantic based methods beyond lexical matching for We b document retrieval. This is partially due to the fact that the same single concept is often expressed using different vocabularies and language styles in documents and queries. For example, latent semantic models such as latent semantic analysis (LSA) are able to map a query to its relevant documents at the seman tic level where lexical matching often fails (e.g., [9][10][31]). These models address the problem of language discrepancy between Web documents and search queries by grouping different terms that occur in a similar context into the same semantic cluster. Thus, a query and a document, represented as two vectors in the low-dimensional semantic space, can still have a high similarity even if they do not share any term. Extending from LSA, probabilistic topic models such as probabilistic LSA (PLSA), Latent Dirichlet Allocation (LDA), and Bi-Lingual Topic Model (B LTM), have been proposed and successfully applied to semantic matching [19][4][16][15][39]. More recently, semantic modelin g methods based on neural networks have also been proposed for information retrieval (IR) [16][32][20]. Salakhutdinov and Hinton proposed the Semantic Hashing method based on a deep auto-encoder in [32][16]. A Deep Structured Semantic Model (DSSM) for Web search was proposed in [20], which is reported to give very strong IR performance on a large-scale web search task when clickthrough data are exploited as weakly-supervised information in training the model. In both methods, plain feed-forward neural networks are used to extract the semantic structures embedded in a query or a document. latent semantic models view a qu ery (or a document) as a bag of words. As a result, they are not effective in modeling contextual structures of a query (or a do cument). Table 1 gives several examples of document titles to illustrate the problem. For example, the word  X  office  X  in the first document refers to the popular Microsoft product, but in the second document it refers to a working space. We see that the precise search intent of the word  X  office  X  cannot be identified without context. microsoft office excel could allow remote code execution welcome to the apartment office online body fat percentage calculator online auto body repair estimates 
Table 1: Sample document titles. The text is lower-cased and punctuation removed. Th e same word, e.g.,  X  office  X , has different meanings depending on its contexts. [11][25][12][26][2][22][24]. Classical retrieval models, such as TF-IDF and BM25, use a bag-of-w ords representation and cannot effectively capture contextual information of a word. Topic models learn the topic distribution of a word by considering word occurrence information within a document or a sentence. However, the contextual informati on captured by such models is often too coarse-grained to be effective for the retrieval task. For example, the word office in  X  X ffice excel X  and  X  X partment office X , which represent two very different search intents when used in search queries, are likely to be projected to the same topic. As an alternative, retrieval methods that directly model phrases (or word n-grams) and term dependencies are proposed in [12][25][26]. For example, in [25], the Markov Ra ndom Field (MRF) is used to model dependencies among terms (e .g., term n-grams and skip-grams) of the query and the document for ranking, while in [26] a latent concept expansion (LCE) model is proposed which leverages the term-dependent information by adding n-gram and (unordered n-gram) as features in to the log-linear ranking model. In [12] a phrase-based translation model was proposed to learn the translation probability of a multi-term phrase in a query given a phrase in a document. Since the phrases capture richer contextual information than words, more precise translations can be determined. However, the phrase translation model can only score phrase-to-phrase pairs observed in the clickthrough training data and thus generalize poorly to new phrases. on the convolutional neural network with convolution-pooling structure, called the convolutional latent semantic model (CLSM), to capture the important contextual information for latent semantic modeling. Instead of using the input representation based on bag-of-words, the new model views a query or a document sequence of words with rich contex tual structure, and it retains maximal contextual information in its projected latent semantic representation. The CLSM first pr ojects each word within its context to a low-dimensional continuous feature vector, which directly captures the contextual features at the word n-gram level (detailed in section 3.3). Seco nd, instead of summing over all word n-gram features uniformly, the CLSM discovers and aggregates only the salient semantic concepts to form a sentence-level feature vector (detailed in section 3.4). Then, the sentence-level feature vector is further fed to a regular feed-forward neural network, which performs a non-linear transformation, to extract high-level semantic information of the word sequence. In training, the parameters of the CLSM is learned on clickthrough data. In modern search engines, a Web document is described by multiple fields [12][38], including title, body, anchor text etc. In our experiments, we only used th e title field of a Web document for ranking. In addition to pr oviding simplicity for fast experimentation, our decision is motivated by the observation that the title field gives better single-field retrieval result than body, although it is much shorter (as shown in Table 4). Thus it can serve as a reasonable baseline in our experiments. 
Nevertheless, our methods are not limited to the title field, and can be easily applied to the multi-field description. Although most traditional retrieval models assume the occurrences of terms to be completely independent, contextual information is crucial for detecting particular search intent of a query term. Thus, research in this area has been focusing on capturing term dependencies. Early work tries to relax the independence assumption by including phrases, in addition to single terms, as indexing units [6][36]. Phrases are defined by collocations (adjacency or proximity) and selected on the statistical ground, possibly w ith some syntactic knowledge. Unfortunately, the experiments did not provide a clear indication whether the retrieval effectiveness can be improved in this way. Recently, within the framework of language models for IR, various approaches that go beyond unigrams have been proposed to capture certain term dependencies, notably the bigram and tri-gram models [35], the depende nce model [11], and the MRF based models [25][26]. These m odels have shown benefit of capturing dependencies. However, they focus on the utilization of phrases as indexing units, rather than the phrase-to-phrase semantic relationships. extract phrase-to-phrase relations hips according to clickthrough data. Such relationships are exp ected to be more effective in bridging the gap between queries and documents. In particular, the phrase translation model learns a probability distribution over  X  X ranslations X  of multi-word phras es from documents to queries. Assuming that queries and documents are composed using two different  X  X anguages X , the phrases can be viewed as bilingual phrases (or bi-phrases in short), which are consecutive multi-term sequences that can be translated from one language to another as units. In [12], it was shown th at the phrase model is more powerful than word translation models [3] because words in the relationships are considered with some context words within a phrase. Therefore, more precise translations can be determined for phrases than for words. Recent studies show that this approach is highly effective when large am ounts of clickthrough data are available for training [12][15]. Ho wever, as discussed before, the phrase-based translation model can only score phrase pairs observed in the training data, and cannot generalize to new phrases. In contrast, the CLSM can generalize to model any context. In our experiments re ported in Section 5, we will compare the CLSM with the word-based and phrase-based translation models. The most well-known linear projection model for IR is LSA [9]. It models the whole docum ent collection using a  X  X  X  document-number of word types.  X  is first factored into the product of three matrices using singular value decomposition (SVD) as  X  X   X  X  X   X  , where the orthogonal matrices  X  and  X  are called term and document vectors, respectively, and the diagonal elements of  X  are singular values in descending order. Then, a low-rank matrix approximation of  X  is generated by retaining only the k biggest singular values in  X  . Now, a document (or a query) represented by a term vector  X  can be mapped to a low-dimensional concept vector  X   X   X  X   X   X  , where the  X  X  X  matrix  X  X  X   X   X   X   X  X  projection matrix. In document search, the relevance score between a query and a document, represented respectively by term vectors  X  and  X  , is assumed to be proportional to their cosine similarity score of the corresponding concept vectors  X  according to the projection matrix  X  . include Probabilistic Latent Sema ntic Analysis (PLSA) [19] and its extensions such as Latent Dirichlet Allocation (LDA) [4][39]. PLSA assumes that each document has a multinomial distribution over topics (called the document-topic distribution), where each of the topics is in turn of a multinomial distribution over words (called the topic-word distribution). The relevance of a query to a document is assumed to be proportional to the likelihood of generating the query by that document. Recently, topic models have been extended so that they can be trained on clickthrough data. For example, a generative model called Bi-Lingual Topic Model (BLTM) is proposed for Web search in [15], which assumes that a query and its cl icked document share the same document-topic distribut ion. It is shown that, by learning the model on clicked query-title pa irs, the BLTM gives superior performance over PLSA [15]. Deep architectures have been s hown to be highly effective in discovering from training data th e hidden structures and features at different levels of abstract ion useful for a variety of tasks [32][18][20][37][7][34]. Among them, the DSSM proposed in [20] is most relevant to our work. The DSSM uses a feed-forward neural network to map the raw term vector (i.e., with the bag-of-words representation) of a query or a document to its latent semantic vector, where the firs t layer, also known as the word hashing layer, converts the term vect or to a letter-trigram vector to scale up the training. The fina l layer X  X  neural activities form the vector representation in the semantic space. In document retrieval, the relevance score between a document and a query is the cosine similarity of their corresponding semantic concept vectors, as in Eq. (1). The DSSM is reported to give superior IR performance to other semantic models. bag of words, the fine-grained contextual structures within the query (or the document) are lost. In contrast, the CLSM is designed to capture important w ord n-gram level and sentence-level contextual structures that the DSSM does not. Specifically, the CLSM directly repre sents local contextual features at the word n-gram level; i.e., it projects each raw word n-gram to a low-dimensional feature vector where semantically similar word n-feature space. Moreover, instea d of simply summing all local word-n-gram features evenly, the CLSM performs a max pooling operation to select the highest neuron activation value across all word n-gram features at each dimens ion, so as to extract the sentence-level salient semantic concepts. Meanwhile, for any sequence of words, this operatio n forms a fixed-length sentence-level feature vector, with the same dimensionality as that of the local word n-gram features. successfully in speech, image, and natural language processing [8][41][7]. The work presented in this paper is the first successful attempt in applying the CNN-like methods to IR. One main difference from the conventional CNN is that the convolution operation in our CLSM is applied implicitly on th e letter-trigram representation space with the learned convolutional matrix  X  The explicit convolution, with the  X  X eceptive field X  of a size of three words shown in Figure 1 is accomplished by the letter-trigram matrix  X   X  which is fixed and not learned. Other deep learning approaches that are related to the CLSM include word-to-vector mapping (also known as word embedding) using deep neural networks learned on large amounts of raw text [1][27]. In [28], the vector representation of a word sequence is computed as a summation of embedding vectors of all words. An alternative approach is proposed in [34], where a parsing tree for a given sentence is extracted, which is then mapped to a fixed-length representation using recursive auto-encoders. Recently, a neural network based DeepMatch model is also proposed to directly capture the correspondence between two short texts without explicitly relying on semantic vector representations [23]. The architecture of the CLSM is illustrated in Figure 1. The model contains (1) a word-n-gram layer obtained by running a contextual sliding window over the input word sequence (i.e., a query or a document), (2) a letter-trigram layer that transforms each word-convolutional layer that extracts contextual features for each word with its neighboring words defined by a window, e.g., a word-n-gram, (4) a max-pooling layer that discovers and combines salient word-n-gram features to form a fixed-length sentence-level feature vector, and (5) a semantic layer that extracts a high-level semantic feature vector for the input word sequence. In what follows, we describe these components in detail, using the annotation illustrated in Figure 1. Figure 1: The CLSM maps a variable-length word sequence to a low-dimensional vector in a latent semantic space. A word contextual window size (i.e. the receptive field) of three is used in the illustration. Convolution over word sequence via learned matrix W  X  is performed implicitly via the earlier layer X  X  mapping with a local receptive field. The dimensionalities of the convolutional layer and the semantic layer are set to 300 and 128 in the illustration, respectively. The max operation across the sequence is applied for each of 300 feature dimens ions separately. (Only the first dimension is sh own to avoid figure clutter.) Conventionally, each word w is represented by a one-hot word vector where the dimensionality of the vector is the size of the vocabulary. However, the vocabular y size is often very large in real-world Web search tasks, and the one-hot vector word representation makes model learni ng very expensive. Therefore, we resort to a technique called word hashing proposed in [20], which represents a word by a letter-trigram vector. For example, given a word (e.g. boy ), after adding word boundary symbols (e.g. #boy# ), the word is segmented into a sequence of letter-n -grams (e.g. letter-tri-grams: #-b-o , b-o-y , o-y-# ). Then, the word is represented as a count vector of letter-tri-grams. For example, the letter-trigram representation of  X  boy  X  is: transformation from a word to it s letter-trigram count vector, which requires no learning. Even though the total number of English words may grow to be ex tremely large, the total number of distinct letter-trigr ams in English (or other similar languages) is often limited. Therefore, it can generalize to new words unseen in the training data. represent a word-n-gram by conc atenating the letter-trigram vectors of each word, e.g., for the t -th word-n-gram at the word-n-gram layer, we have:  X 1 X  X 2 X  is the size of the contextual window. In our experiment, there are about 30K unique letter-trigrams observed in the training set after the data are lower-cased and punctuation-removed. Therefore, the letter-tri gram layer has a dimensionality of  X  X 30 X  . The convolution operation can be vi ewed as sliding window based feature extraction. It is designed to capture the word-n-gram contextual features. Consider the t -th word-n-gram, the convolution matrix projects its le tter-trigram representation vector  X  to a contextual feature vector  X   X  . As shown in Figure 1,  X  computed by where  X   X  is the feature transformation matrix, as known as the convolution matrix, that are sh ared among all word n-grams.  X  X  X  X  is used as the activation function of the neurons: The output of the convolutional layer is a variable length sequence of feature vectors, whose length is proportional to the length of the input word sequence. A special  X  X adding X  word, &lt;s&gt;, is added at the beginning and the end of the input word sequence so that a full window for a word at any position in the word sequence can be formed. Figure 1 shows a convolutional layer using a 3-word contextual window. Note that like the conventional CNN, the convolution matrix used in our CLSM is shared among all n-word phrases and therefore generalizes to new word-n-grams unseen in the training set. projected to vectors that are close to each other if they are semantically similar. Table 2 presents a set of sample word-tri-grams. Considering the word  X  office  X  as the word of interest, we measure the cosine similarity between the contextual feature vector of  X  office  X  within the context  X  X icrosoft office software X  and the vector of  X  office  X  within other contexts. We can see that the similarity scores between the learned feature vector of  X  X icrosoft office software X  and those of the contexts where  X  office  X  is referred to the software are quite high, while the similarity scores between it and the features vectors where  X  office  X  has the search intent of  X  X orking space X  are significantly lower. Similarly, as shown in Table 2, the context vectors of  X  body  X  are much closer when they are of the same search intent. Table 2: Sample word n-grams and the cosine similarities between the learned word-n-gram feature vectors of  X  office  X  and  X  body  X  in different contexts after the CLSM is trained. A sequence of local contextual feature vectors is extracted at the convolutional layer, one for each word-n-gram. These local features need to be aggregated to obtain a sentence-level feature vector with a fixed size independent of the length of the input word sequence. Since many words do not have significant influence on the semantics of th e sentence, we want to suppress the non-significant local features and retain in the global feature vector only the salient features that are useful for IR. For this purpose, we use a max operation, also known as max pooling , to force the network to retain only the most useful local features produced by the convolutional layers . I.e., we select the highest neuron activation value across all local word n-gram feature vectors at each dimension. Referring to the max-pooling layer of Figure 1, we have where  X   X   X   X  is the i -th element of the max pooling layer v ,  X  the i -th element of the t -th local feature vector  X  dimensionality of the max pooling layer, which is the same as the dimensionality of the local contextual feature vectors  X  X  pooling layer of the CLSM after tr aining. For each sentence, we examine the five most active neurons at the max-pooling layer, measured by  X   X   X   X  , and highlight the words in bold who win at these five neurons in the max operation (e.g., whose local features give these five highest neuron activation values) 2 . These examples show that the important concepts , as represented by these key words, make the most significan t contribution to the overall semantic meaning of the sentence. microsoft office excel could allow remote code execution welcome to the apartment office online body fat percentage calculator online auto body repair estimates vitamin a the health benefits given by carrots calcium supplements and vitamin d discussion stop sarcoidosis Table 3: Sample document titles. We examine the five most active neurons at the max-pooling layer and highlight the words in bold who win at these five neurons in the max operation. Note that, the feature of a word is extracted from that word together with the context words around it, but only the center word is highlighted in bold. After the sentence-level feature vector is produced by the max-pooling operation, one more non-linear transformation layer is applied to extract the high-level semantic representation, denoted by  X  . As shown in Figure 1, we have where v is the global feature vector after max pooling,  X  semantic projection matrix, and y is the vector representation of the input query (or document) in the latent semantic space, with a dimensionality of L . connected semantic layer, as shown in Figure 1. The model architecture can be easily extended to using more powerful, multi-layer fully-connected d eep neural networks. Given a query and a set of docu ments to be ranked, we first compute the semantic vector representations for the query and all the documents using the CLSM as described above. Then, similar to Eq. (1), we compute the relevance score between the query and each document by measuring the cosine similarity between their semantic vectors. Formally, the semantic relevance score between a query  X  and a document  X  is defined as: One word could win at multiple neurons. where  X   X  and  X   X  are the semantic vectors of the query and the document, respectively. In Web search, given the query, the documents are ranked by their semantic relevance scores. The data for training the CLSM is the clickthrough data logged by a commercial search engine. The clickthrough data consist of a clickthrough data have been used in earlier studies, such as [12][15][20]. Similar to these work, we assume that a query is relevant to the documents that are clicked on for that query, and train the CLSM on the clickthrough data in such a way that the semantic relevance scores betw een the clicked documents given the queries are maximized. between a query and a positive document to the posterior probability of that document given the query through softmax: where  X  is a smoothing factor in the softmax function, which is set empirically on a held-out data set in our experiment.  X  denotes the set of candidate documents to be ranked. In practice, for each (query, clicked-document) pair, we denote by  X , X  X   X   X  a query and  X   X  is the clicked docum ent and approximate D by including  X   X  and  X  randomly selected unclicked documents, denote by  X  X   X   X   X  X  X 1,..., X ; . As an alternative, the model could also be trained using noise cont rastive estimation as in [29]. likelihood of the clicked documents given the queries across the training set. That is, we minimize the following loss function where  X  denotes the parameter set of the CLSM. wise loss that has been widely used for learning-to-rank [5] as a special case if we allow only one unclicked document to be sampled. This loss function is also widely used in speech recognition and other applications [ 17]. It is more flexible than pairwise loss in exploring different sampling techniques for generating unclicked do cuments for discriminative information. fitting, we divide the clickthrough data into two sets that do not overlap, called training and valida tion datasets, respectively. In our experiments, the models are trained on the training data and the training parameters are optimized on the validation data. suggested in [30]. The model is trained using mini-batch based stochastic gradient descent. Ea ch mini-batch consists of 1024 training samples. In our implementation, models are trained using an NVidia Tesla K20 GPU. We evaluated the retrieval models on a large-scale, real-world data set, called the evaluation data set henceforth. The evaluation data set contains 12,071 Englis h queries sampled from one-year query log files of a commercial search engine and labeled by human judgers. On average, each query is associated with 74 Web documents (URLs). Each query-document pair has a relevance label manually annotated on a 5-level relevance scale: bad , fair , good , excellent , and perfect , corresponding to 0 to 4, where level that the document is the most relevant to query  X  . All the queries and documents are preprocessed such that the text is white-space tokenized and lowercased, numbers are retained, and no stemming/inflection is performed. Figure 2 shows the length distributions of queries and docu ments in the evaluation data. The average lengths of the queries and the document titles are 3.01 and 7.78 words, respectively. Figure 2: The distribution of query length and document title length in the evaluation data set. The evaluation set consists of 12,071 queries and 897,770 document s. Query length is 3.01 on average. Document title length is 7.78 on average. Web document for ranking in our experiments. As shown in Table 4, the title field is very effective for document retrieval, although titles are much shorter than body texts. Body 0.275 0.310 0.375 Title 0.305  X  0.328  X  0.388  X 
Table 4 : Ranking results of two BM25 models, each uses a different single field to represen t Web documents. The superscript  X  indicates statistically significant improvements  X 0.05 X  X  X  X  over Body . hyper-parameters that must be estimated empirically. In all experiments, we have used 2-fold cross validation: A set of results on one half of the data is obtained using the parameter settings optimized on the other half, and the global retrieval results are combined from those of the two sets. been measured by mean Normalized Discounted Cumulative Gain (NDCG) [21], and we will report NDCG scores at truncation levels 1, 3, and 10. We have al so performed a significance test using the paired t -test. Differences are considered statistically significant when the p -value is lower than 0.05. training include 30 million query and clicked-title pairs sampled from one year query log files. The query-title pairs are pre-processed in the same way as the evaluation data to ensure uniformity. We test the models in ranking the documents in the evaluation data set. There is no overlap between the training set and the evaluation set. We have compared the CLSM with five sets of baseline models, as shown in Table 5. The first set includes two wide ly used lexical matching methods, BM25 and the unigram language model (ULM). The second set includes a set of state-of-the-art latent semantic models which are learned either on documen ts only in an unsupervised manner (PLSA and LDA) or on clickthrough data in a supervised way (BLTM). The third set includes a phrase-based translation model (PTM) that intends to directly model the contextual information within a multi-term phrase. This set also includes a word-based translation model (WTM) which is a special case of the phrase-based translation model. Both translation models are learned on the same clickthrough data described in Section 5.1. The fourth set includes the MRF based term-dependency model and the la tent concept expansion (LCE) model. The fifth set includes th e DSSM, which is a deep neural network based model, which is also learned on the same clickthrough data. In order to make the results comparable, we re-[11][15][20][25][26]. Details are elaborated in the following paragraphs. term vector representation fo r queries and documents. BM25 (Row 1 in Table 5) follows th e BM25 term weighting function used in the Okapi system. ULM (Row 2) is a unigram language model with Dirichlet smoothing [42]. Both ULM and BM25 are state-of-the-art document ranking models based on term matching. They have been widely used as baselines in related studies. proposed in [19], and was trained on documents only (i.e., the title side of the query/clicked-title pairs). Different from [19], our version of PLSA was learned us ing MAP estimation as in [15]. We experimented with different num bers of topics, and the results of using 100 topics and 500 topics are reported in Row 3 and 4, respectively. In our experiments, they give similar performance. query/clicked-title pairs). The L DA model is learned via Gibbs sampling. The number of topics is set to 100 and 500, respectively. LDA gives slightly better results than the PLSA, and LDA with 500 topics significantly outperforms BM25 and ULM. of the bilingual topic models described in [15]. It is trained on query-title pairs using the EM algorithm with a constraint enforcing the paired query and title to have same fractions of terms assigned to each hidden topic. The number of topics is set to 100 as in [15]. We see that us ing clickthrough data for model training leads to improvement over PLSA and LDA. BLTM also significantly outperfo rms BM25 and ULM. described in [25]. We use cross-validation method to tune the optimal parameters for the feature weights. [26]. It leverages the term-dep endent information by adding n-gram and (unordered n-gram) as features into the log-linear ranking model. In our experime nts, we re-implemented LCE following [26]. Both MRF and LCE outperform BM25 and ULM significantly. translation model described in [12], which is a special case of the phrase-based translation model, listed here for comparison. 0% 5% 10% 15% 20% 25% 30% in [12]. It is supposed to be more powerful than WTM because words in the relationships are considered with contextual words within a phrase. Therefore, more precise translations can be determined for phrases than for words. The model is trained on query-title pairs. The maximum length of a phrase is set to three. Our results are consistent with that of [11], showing that phrase models are more effective for retrieval than word models when large amounts of clickthrough data are available for training. in [20]. It includes the letter-trigram based word hashing layer, two non-linear hidden layers, each of which has 300 neurons, and an output layer that has 128 neurons. In our experiments, we found that learning two separate neural networks, one for the query and one for the document title, gives better performance than sharing the same neural network for both of the query and the document title. Therefore, we al ways use two separate neural networks in our experiments ther eafter. We have experimented with using different number of negative samples,  X  , in the training of the DSSM. Row 12 uses the setting of  X 4 , where Row uses the setting of  X 50 X  . The DSSMs are trained on the same query-title pairs described in section 5.1 3 . The results in Table 5 confirm that the DSSM (e.g., Row 13) outperforms other competing models in Rows 1 to 11 significantly. The results also show that using more negative samples in tr aining leads to better results (e.g., Row 13 vs. Row 12). Sections 3 and 4. The convoluti onal layer and max-pooling layer each has 300 neurons, and the final output layer has 128 neurons. Two separate convolutional neural networks are used in the experiments. We have also expe rimented with using different number of negative samples,  X  , in the training of the CLSM. The model is trained on the same query-title clic kthrough dataset described in section 5.1. The main results of our experime nts are summarized in Table 5. First, we observe that the CLSM (  X 50 X  ) outperforms the state-of-the-art term matching based document ranking models, BM25 and ULM, with a substantial margin of 4.3% in NDCG@1. The CLSM also outperforms the stat e-of-the-art topic model based approaches (i.e., PLSA, LDA, and BLTM) with a statistically significant margin from 3.2% to 4.0%. Further, compared to previous term-dependency models, the CLSM with the best setting outperforms MRF, LCE and PTM by a substantial improvement of 3.3%, 3.6%, a nd 2.9% NDCG@1 respectively. This demonstrates CLSM X  X  effectiveness in capturing the contextual structure useful for semantic matching. Finally, we obtain significant 2.2% to 2.3% NDCG@1 improvement of the CLSM over DSSM, a state-of-the-art neural network based model. This demonstrates the importa nce of CLSM X  X  capability of modeling fine-grained word n-gr am level and sentence-level contextual structures for IR, as the DSSM is based on the bag-of-words representation and cannot capture such information. For comparison, we re-implemented the DSSM on the current data set. The data set used in [2 0] is encoded in a bag-of-words representation format and thus not suitable for this study (personal communication). different context window sizes and present the experimental results in Table 6. In Table 6, we first observe that even with a context window size of one, the CLSM still significantly outperforms the DSSM, demonstrating that it is far more effective for IR to capture salient local features than simply summing over the contributions from all words uniformly. Then, when increasing the window size from one to three, we observe another significant improvement, attributes to the capability of modeling word-tri-gram contextual information. When the window size is increased to five, however, no significant gain is observed. Our interpretation is that because the average lengths of the queries and the document titles are only three and eight words respectively, window sizes larger than three do not provide much extra context information. Moreov er, big context windows lead to more model parameters to learn, a nd thus increase the difficulty of parameter learning. In the next subsection, we will present an in-depth analysis on the performance of the CLSM. # Models NDCG@1 NDCG@3 NDCG@10 3 PLSA (T=100) 0.305 0.335  X  0.402  X  4 PLSA (T=500) 0.308 0.337  X  0.402  X  5 LDA (T=100) 0.308 0.339  X  0.403  X  6 LDA (T=500) 0.310  X  0.339  X  0.405 7 BLTM 0.316  X  0.344  X  0.410  X  8 MRF 0.315  X  0.341  X  0.409  X  9 LCE 0.312  X  0.337  X  0.407  X  10 WTM 0.315  X  0.342 11 PTM (maxlen = 3) 0.319  X  0.347  X  0.413 12 DSSM (  X   X 4 ) 0.320  X  0.355  X   X  0.431 13 DSSM (  X   X 50 ) 0.327  X   X  0.363  X   X  0.438 14 CLSM (  X   X 4 ) 0.342  X   X   X  0.374  X   X   X  0.447 15 CLSM (  X   X  X  X  X  ) 0.348  X  X  X  0.379  X   X   X  0.449 Table 5: Comparative results with the previous state of the art approaches. BLTM, WTM, PTM, DSSM, and CLSM use the same Superscripts , X , X  and  X  indicate statistically significant improvements  X 0.05 X  X  X  X  over BM25 , PTM , and DSSM (  X   X  50 ) , respectively. # Models NDCG 1 DSSM (  X   X 50 ) 0.327 0.363 0.438 2 CLSM (  X   X 50 ) win =1 0.340  X  0.374  X  0.443 3 CLSM (  X   X 50 ) win =3 0.348  X   X  0.379  X   X  0.449 4 CLSM (  X   X 50 ) win =5 0.344  X  0.376  X  0.448
Table 6 : Comparative results of the CLSMs using different convolution window sizes. The setting of the DSSM is 300/300/128 and the setting of the CLSM is K =300, L =128.
Superscripts  X  and  X  indicate statistically significant improvements  X 0.05 X  X  X  X  over DSSM and CLSM (win=1), respectively. between models , we compare the CLSM with the DSSM query by query under their best settings, i. e., row 13 and row 15 in table 5, respectively. document retrieved by the DSSM and the CLSM, respectively. We count the number of times th at the retrieved document is in each of the five relevance category, from bad to perfect , for both models. The distributions of the relevance labels of the top-1 returned documents are plotted in Figure 3. Compared with the DSSM, overall, the CLSM returns more relevant documents, i.e., the percentages of returned documents in the bad or fair categories decrease subs tantially, and the percentage of returned documents in the good category increases substantially. The although the absolute numbers are small. Figure 3: Percentage of top-1 ranked documents in each of the five relevance categories, retrieved by the CLSM and the DSSM, respectively. Table 7 : CLSM vs DSSM on Top-1 se arch results. The three relevance categories, good , excellent , and perfect , are merged into one good+ category. DSSM is presented in table 7. We observe that for 8,152 of the total 12,071 queries, both the CLSM and the DSSM return the documents of the same quality. Ho wever, in the cases where they return documents with different qualities, the advantage of the CLSM over the DSSM can be clearly observed. For example, there are 601 queries for which the CLSM returns good or better quality Top-1 documents while th e DSSM X  X  Top-1 returns are bad , much more than the opposite cases. There are also 981 queries for which the CLSM returns good or better Top-1 documents while the DSSM returns fair documents, much more than the 631 opposite cases. show several examples selected from the CLSM result on the evaluation data set in Table 8. Each row includes a query and the title of the top 1 document ranked by CLSM. In both of the query and the document title, the words that most significantly contribute to the semantic meani ng, e.g., words contribute to the most active five neurons at the max pooling layer, are marked in bold . To further illustrate the CLSM X  X  capability for semantic matching, we trace the activatio n of neurons at the max-pooling layer for the first three examples in Table 8 and elaborate these examples in Figure 4. We first project both the query and the document title to the max-pooling layer, respectively. Then, we evaluate the activation values of neurons at the max-pooling layer, and show the indices of the neurons that have high activation values for both query and document title, e.g., the product of the activation values of the query and the document title at a neuron is larger than a threshold. After that, we trace back to the words that win these neurons in both the query and the document title. In Figure 4, we show the indices of these matching neurons and the words in the query and the document title that win them. key words  X  X arm environment arterioles X  in the query and the word  X  X hermoregulation X  in the document, they both have high activation values at a similar set of neurons, and thus lead to a query-document match in the seman tic space. Similar behavior is observed in the second example.  X  X  uto X  and  X  X alculator X  in the query and  X  X ar X  and  X  X stimates X  in the document activate similar neurons, thus leading to a query-document match in the semantic space as well. The third example is more complicated.  X  X itamin d X  is closely associated to  X  X alcium absorbing X , and  X  X xcessive calcium absorbing X  is a symptom of  X  X arcoidosis X . In Figure 4 (c), we observe that both  X  X alcium X  in the document title and  X  X  X  (with its context  X  X itamin X ) in th e query gives high activation at neuron 88, while  X  X arcoidosis X  in the document title and  X  X bsorbs X   X  X xcessive X  and  X  X itam in X  in the query have high activations at the set of neurons 90, 66, 79. Our analysis indicates that different words with related semantic meanings activate the similar set of neurons, resulting to a high overall matching score. This demonstrates the effectiveness of the CLSM in extracting the salient semantic meaning in queries and documents for Web search. In this paper, we have reported a novel deep learning architecture called the CLSM, motivated by the convolutional structure of the CNN, to extract both local contextual features at the word-n-gram level (via the convolutional layer) and global contextual features at the sentence-level (via the max-pooling layer) from text. The higher layer(s) in the overall deep architecture makes effective use 0% 10% 20% 30% 40% Query Title of the top-1 returned document retrieved by CLSM how to change font excel office 2013 change font default styles in excel 2013 most active neurons at the max-pooling layer. of the extracted context-sensitive features to generate latent semantic vector representations which facilitates semantic matching between documents and queries for Web search applications. We have carried ou t extensive experimental studies of the proposed model whereby several state-of-the-art semantic models are compared and significant performance improvement on a large-scale real-world Web search data set is observed. variations have also been de monstrated giving superior performance on a range of natural language processing tasks beyond information retrieval, in cluding machine translation [13], semantic parsing and question answering [40], entity search and online recommendation [14]. In the future, the CLSM can be further extended to automatically capture a wider variety of types of contextual features from text than our current settings. Figure 4: Illustration of semantic ma tching between a query and a document title at the max-pooling layer, after word-n-gram contextual feature extraction and the max pooling operation. The indices of the neurons at the max-pooling layer that have high activation values for both query and document titl e are shown. [1] Bengio, Y., 2009. Learning deep architectures for AI. In [2] Bendersky, M., Metzler, D., and Croft, B., 2011. [3] Berger, A., and Lafferty, J. 1999. Information retrieval as [4] Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent [5] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., [6] Buckley, D., Allan, J., and Salton, G. 1995. Automatic re-[7] Collobert, R., Weston, J., Bottou, L., Karlen, M., [8] Deng, L., Abdel-Hamid, O. , and Yu, D., 2013. A deep [9] Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T., [10] Dumais, S. T., Letsche, T. A., Littman, M. L., and Landauer, [11] Gao, J., Nie, J-Y., Wu, G. and Cao, G. 2004. Dependence [12] Gao, J., He, X., and Nie, J-Y. 2010. Clickthrough-based [13] Gao, J., He, X., Yih, W-T. , and Deng, L., 2014. Learning [14] Gao, J., Pantel, P., Gamon, M., He, X., Deng, L., and Shen, [15] Gao, J., Toutanova, K., Yih., W-T. 2011. Clickthrough-based [16] Girolami, M., and Kaban, A. 2003. On an equivalence [17] He, X., Deng, L., and Chou, W., 2008. Discriminative [18] Hinton, G., and Salakhutdinov, R., 2010. Discovering binary [19] Hofmann, T. 1999. Pr obabilistic latent sema ntic indexing. In [20] Huang, P., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. [21] Jarvelin, K. and Kekalainen, J. 2000. IR evaluation methods [22] Lavrenko, V., and Croft, B., 2001. Relevance-based language [23] Lu, Z. and Li, H. 2013. A de ep architecture for matching [24] Lv, Y., and Zhai, C., 2009. Po sitional Language Models for [25] Metzler, D. and Croft, B. 2005. A Markov random field [26] Metzler, D., and Croft, B. 2007. Latent Concept Expansion [27] Mikolov, T., Yih, W., and Zweig, G., 2013. Linguistic [28] Mikolov, T., Sutskever, I., Chen , K., Corrado, G., Dean, J. [29] Mnih A., and Kavukcuoglu, K., 2013. Learning word [30] Montavon, G., Orr, G., M X ller , K., 2012. Neural Networks: [31] Platt, J., Toutanova, K., an d Yih, W. 2010. Translingual [32] Salakhutdinov R., and Hinton, G., 2007. Semantic hashing. [33] Shen, Y., He, X., Gao, J., De ng, L., and Mesnil, G., 2014. [34] Socher, R., Huval, B., Manning, C., Ng, A., 2012. Semantic [35] Song, F. and Croft, B. 1999. A general language model for [36] Sparck-Jones, K. 1998. What is the role of NLP in text [37] Tur, G., Deng, L., Hakkani-T ur, D., and He, X., 2012. [38] Wang, K., Li, X., and Gao, J. 2010. Multi-style language [39] Wei, X., and Croft, W. B. 2006. LDA-based document [40] Yih, W-T., He, X., and Meek, C., 2014. Semantic parsing for [41] Zeiler, M., Taylor, G., a nd Fergus, R., 2011. Adaptive [42] Zhai, C. and Lafferty, J. 2001. A study of smoothing The CLSM is trained using stoc hastic gradient descent. Let  X  X  X  X  X  X  X  X  be a sample-wise loss function, the model parameters are updated by where  X   X  is the learning rate at the  X   X  X  X  iteration,  X  the model parameters at the  X   X  X  X  and the  X 1 X  X  X  X   X  X  X  respectively. For each query  X  , we denote by  X   X  the clicked document, and  X  X   X   X 1,...,4 X  X ; the unclicked document. In the following, referring to Figure 1 and Eq. (5)  X  (7) for definitions of variables, we then derive  X  X  X  X  X  X  X  X  X  X  X  First, the loss function in Eq. (7) can be written as: where  X   X   X  X   X   X , X   X   X   X , X  X  X  X  X   X   X   X  . The gradient of the loss function w.r.t. the semantic projection matrix  X   X  is where and To simplify the notation, let  X , X , X  be  X   X   X   X   X  ,  X  X /1  X   X   X   X  , respectively. With  X  X  X  X  as the activation function in our model, each term in the right-hand side of Eq. (10) can be calculated using the following formula: where  X   X  (Hadamard product). In order to compute the gradient of the loss function w.r.t. the convolution matrix,  X   X  , first we also need to calculate  X  X  X  X  for each  X   X  at the max layer. For example, each  X  in the max layer, v , can be calculated through back propagation as Then we need to trace back to the local features that win in the max operation, i.e., Then, the gradient of the loss function w.r.t. the convolution matrix,  X   X  , can be computed as where for the i -th row of  X   X  , e.g.,  X   X , X  ,  X ,..., X 1 X  , we have: 
 X  X   X  where  X  X  X  X  X  is the i -th element of  X  , and  X   X , X  and  X  window vector at the t -th position of Q or D , as defined in section 3.3, respectively. 
