 A coCluster of a m n matrix X is a submatrix determined by a subset of the rows and a subset of the columns. The problem of nding coClusters with speci c prop erties is of interest, in particular, in the analysis of microarra y exp er-imen ts. In that case the entries of the matrix X are the expression levels of m genes in eac h of n tissue samples. One goal of the analysis is to extract a subset of the sam-ples and a subset of the genes, suc h that the expression lev-els of the chosen genes beha ve similarly across the subset of the samples, presumably re ecting an underlying regulatory mec hanism governing the expression level of the genes. We prop ose to base the similarit y of the genes in a co-Cluster on a simple biological mo del, in whic h the strength of the regulatory mec hanism in sample j is H j , and the re-sponse strength of gene i to the regulatory mec hanism is G . In other words, every two genes participating in a good coCluster should have expression values in eac h of the par-ticipating samples, whose ratio is a constan t dep ending only on the two genes. Noise in the expression levels of genes is tak en into accoun t by allo wing a deviation from the mo del, measured by a relative error criterion . The sleev e-width of the coCluster re ects the exten t to whic h entry i; j in the coCluster is allo wed to deviate, relativ ely, from being ex-pressed as the pro duct G i H j .

We presen t a polynomial-time Mon te-Carlo algorithm whic h outputs a list of coClusters whose sleev e-widths do not ex-ceed a presp eci ed value. Moreo ver, we pro ve that the list includes, with xed probabilit y, a coCluster whic h is near-optimal in its dimensions. Extensiv e exp erimen tation with syn thetic data sho ws that the algorithm performs well. Categories and Sub ject Descriptors: H.2.8 Database Managemen t: Database Applications -Data Mining; I.5.3 supp orted in part by the The Lynn and William Frenk el Cen ter for Computer Sciences, and The Paul Ivanier Cen ter for Rob otics Researc h and Pro duction Managemen t y Corresp onding author.
 Computing Metho dologies: Pattern Recognition -Cluster-ing.
 General Terms: Algorithms.
 Keyw ords: clustering, coclustering, gene expression data, co-regulation.
A coCluster of a m n matrix X is a submatrix determined by a subset I of the rows and a subset J of the columns. Recen tly man y researc hers have addressed the problem of nding coClusters with speci c prop erties. One suc h direc-tion of researc h originated in mining high dimensional data, in particular pro jected clustering, [1], [2], [3], [13], [16]. An-other direction resulted from the analysis of microarra y ex-perimen ts. In that case the entries of the matrix X are the expression levels of m genes in eac h of n tissue samples. One goal of the analysis is to extract a subset of the samples and a subset of the genes, suc h that the expression levels of the chosen genes beha ve similarly across the subset of the samples, presumably re ecting an underlying co-regulation, [4], [5], [6], [7], [10], [11], [12], [14], [15], [16], [17]. For concreteness, we will in this introductory section describ e our approac h using terminology from microarra y analysis, although its applicabilit y extends beyond this domain, for example to the analysis of term-do cumen t matrices, cf. [8].
Call a sub-matrix determined by a set of genes (rows) I and a set of samples (columns) J co-v arying if for any two genes i 1 and i 2 in I the ratio of elemen ts in any sample j 2 J is a constan t indep enden t of j (but possibly dep enden t on i there is a laten t variable, H , whose value for sample j is H , suc h that the expression level of gene i in sample j is prop ortional to H j , with the constan t of prop ortionalit y dep ending only on the gene: X i;j G i H j for all ( i; j ) 2 ( I; J ).

Presumably , in suc h a co-v arying submatrix, the varia-tion in the expression levels of the genes can be attributed primarily to the in uence of one (or more) regulators, repre-sen ted by H , whereas G i is indicativ e of the strength of the regulation of gene i . We want to emphasize that it is not assumed that H j is prop ortional to the abundance of some transcription factor in sample j , even if it is the variation in the latter that is resp onsible, directly or indirectly , for the variation in the expression levels. Supp ose, for example, that the functional dep endency of the expression level of a gene on the abundance of a transcription factor, x , is of the form Gu ( x ), with G a constan t speci c to the gene and u ( x ) a function common to the up-regulated genes. In the mo del describ ed above, H j is then u ( x j ) (rather than x j ), with x the level of the transcription factor in sample j . And the ex-pression levels of two up-regulated genes 1 and 2 in sample j are G 1 H j , G 2 H j .

To allo w for noise in the expression levels of genes, we will look for G i and H j suc h that X i;j is well appro ximated by G
H j . To measure how well X i;j is appro ximated by G i H it seems natural to adopt the relative error criterion, Taking logarithms and setting A i;j = log X i;j , R i = log G C j = log H j , and " = log , the problem becomes one of nding R i ; C j suc h that for all i; j ,
De nition 1. The sleeve-width of a sub-matrix of A , de-ned by a subset I of rows and a subset J of columns, is
De nition 2. Let 0 &lt; ; &lt; 1 be xed parameters. For w &gt; 0, a coCluster of A of sleeve-width w is a pair ( I; J ), with I a subset of the rows and J a subset of the columns, that satis es the follo wing conditions.
 Size: The num ber of rows is at least a -fraction of all rows, Sleev e-width: sw ( I; J ) w , i.e. , there are R i ; i 2 I , and Thus, for any gene i in a coCluster there is a shift R i whic h places its row A i;j ; j 2 J of expression values within a sleev e of width w surrounding the row pro le of the coCluster. Remark: Clearly the pro les are not unique, e.g. , R i + C ( R i + a )+( C j a ) for all i; j and a xed a . Thus an arbitrary R i or C j can be xed at 0.

Summarizing, we prop ose to iden tify subsets of genes that are purp ortedly co-regulated in a subset of the samples, by log-transforming the expression-data matrix, and then nd-ing in it coClusters of small sleev e-width, w . Observ e that, in con trast to a singular value decomp osition, the values G and H j found by this metho d, are guaran teed to be non-negativ e.

Subsection 2.1 describ es a Mon te-Carlo algorithm for ex-tracting a list of coClusters all guaran teed to be of sleev e-width w . Then it is pro ven in subsection 2.2 that the list con tains, with xed probabilit y, a coCluster that is near-optimal in a sense describ ed there.

Section 4 explains how to compute the sleev e-width of a given coCluster. The results of exp erimen ts investigating prop erties of the algorithm and its parameters, performed on syn thetic data sets, are presen ted in section 3.
The approac h dev elop ed here was stim ulated by the work of Cheng and Churc h [6], who also sough t submatrices of A whose entries are appro ximated well by R i + C j , but mea-sured the discrepancy using the mean squar ed residue score ; they called suc h matrices biclusters. Further work in this direction includes that of Yang, Wang, Wang and Yu, [17], and of Cho, Dhillon, Guan and Sra [7]. However, the mean squared residue score has some undesirable features: the score of a bicluster can be smaller than the score of a sub-matrix of this bicluster, the score is insensitiv e to outliers, and the connection to a regulatory mo del is unclear. In ad-dition, we exp ect that our approac h will nd smaller sets of genes with tigh ter co-regulation patterns.

In another direction, our results owe much to the work of Pro copiuc, Jones, Agarw al and Murali [13] on pro jectiv e clustering, whic h also formed the basis of the work of Murali and Kasif [12] on gene expression motifs. These authors looked for coClusters in whic h all the expression values of a row of the coCluster have to t within a sleev e around the exact same row pro le, arg min C max i 2 I;j 2 J j A i;j C on the other hand, permit shifting eac h row by an individual xed amoun t, so as to place it within a sleev e around the row pro le.

In a vein similar to ours, Wang, Wang, Yang, and Yu, [16] prop osed an algorithm whic h nds coClusters with the prop erty that any pair of rows in the coCluster have sleev e-width at most w= 2. It follo ws from Theorem 2.1 that the cluster as a whole has then in fact sleev e-width w at most, in our terminology . The running time of their algorithm is, however, exp onen tial.
Algorithm RSleeve 1: loop ` R times 2: choose a set of rows D of size d uniformly at random; 3: loop ` S times 4: choose a column s; 1 s n; uniformly at random; 5: J ; ; 6: for j 1 to n do 7: if sw ( D; f s; j g ) w= 2 then add j to J ; 8: choose a row r 2 D uniformly at random; 9: I ; ; 10: for i 1 to m do 11: if sw ( f r; i g ; J ) w= 2 then add i to I ; 12: if j I j &lt; m or j J j &lt; n then discard ( I; J ) 13: else compute sw ( I; J ); 14: return a list of the best ( I; J ); Figure 1: Algorithm for nding sleev ed coClusters
Figure 1 presen ts a Mon te Carlo algorithm, RSleeve , whic h outputs a list of coClusters whic h are guaran teed to have sleev e-width not exceeding w , cf. Theorem 2.1. More-over, it will be pro ven in the next subsection that the al-gorithm possesses the follo wing near-optimalit y prop erty: with xed probabilit y at least one of the coClusters on the whic h is optimal in a sense that will be de ned formally in the next subsection. Roughly speaking, ( I ; J ) is optimal in the balance it strik es between the num ber of rows and the num ber of columns. Implemen tation notes. 1. The computation of sw ( D; f s; j g ) needed in line 7 is easy: sw ( D; f s; j g ) = 1 cf. Theorem 4.1. A similar result holds for sw ( f r; i g ; J ) in line 11. 2. Although the coClusters returned by the algorithm are guaran teed to have sleev e-width w at most, they may well have smaller sleev e-width in actual fact. For this reason line 13 computes sw ( I; J ); the algorithm for doing this is detailed in section 4.

We now pro ve that all coClusters returned by the algo-rithm do indeed have sleev e-width w .
 Theorem 2.1. Let I be a set of row indic es, and let r 2 I . If sw ( f r; i g ; J ) w= 2 for each i 2 I , then sw ( I; J ) w . Thus each coCluster ( I; J ) returne d by RSleeve has sleeve-width at most w .

Proof. Consider the de nition of sleev e-width, equation (1). Fix i 2 I . Since sw ( f r; i g ; J ) w= 2, there are R R r as well as C i j ; j 2 J , suc h that Hence j A i;j A r;j R i + R r j w= 2 for all j 2 J and eac h xed i 2 I . In fact, according to the remark after de nition 2, we can alw ays set R r = 0. It follo ws that Remark: The bound sw ( I; J ) w cannot be impro ved, since it becomes tigh t as max f m; n g ! 1 . Namely , for eac h n there exists a n n matrix suc h that every pair of rows has sleev e-width n n 1 , whereas the matrix as a whole has sleev e-width 2.
Algorithm RSleeve can be view ed as a heuristic, and the exp erimen tal evidence of section 3 sho ws it to be ecacious. This subsection pro ves that there are good theoretical rea-sons for its ecacy .

Supp ose there are sev eral coClusters of the same sleev e-width. Whic h one is to be preferred? coClusters could be con text under consideration, the num ber of rows is sev eral orders of magnitude larger than the num ber of columns. It mak es sense, then, as Pro copiuc, Jones, Agarw al and Murali [13] prop osed, to specify a trade-o between the num ber of rows and the num ber of columns in the coCluster: the inclusion of an additional column in J is worth the exclusion of at most a (1 )-fraction of the rows in I . We adopt their prop osal and rank coClusters by their measure.

De nition 3. The rating of a coCluster ( I; J ) is ( j I j ; j J j ), where ( x; y ) = x (1 = ) y , for xed .
 With this de nition our problem can be rephrased as one of nding an optimal coCluster of sleev e-width w , one with the maxim um rating.

The main result of this subsection is Theorem 2.3. Para-phrased it states that if the algorithm is run long enough, then the list returned by the algorithm con tains with xed probabilit y a coCluster ( I; J ) of sleev e-width w , whose rat-ing is at least as good as that of an optimal coCluster of sleev e-width w= 2.

The cen tral insigh t of Pro copiuc, Jones, Agarw al and Mu-rali [13], in terms of the presen t con text, is that in an optimal coCluster there must be a relativ ely small subset of rows, of size O (log n ), that determines whic h columns participate in the cluster.

De nition 4. Let ( I; J ) be a coCluster of sleev e-width w , and let s 2 J . D I is a discriminating set for ( I; J ) with resp ect to s if it satis es 1. sw ( D; f s; j g ) w for all j 2 J ; 2. sw ( D; f s; j g ) &gt; w for all j = 2 J .
 It is therefore a simple matter to determine J , once s and D are kno wn. The next lemma sho ws that discriminating sets, with resp ect to any s 2 J , are both small and plen-tiful in an optimal coCluster ( I ; J ). Consequen tly in our Algorithm RSleeve , both s and D are guessed at, and then J is deduced in line 7.

Lemma 2.2. Let ( I ; J ) be an optimal coCluster of sleeve-width w , with j I j m , and let s be a column in J . Let D be a randomly chosen subset of I of size d . Then with probability at least 1 2 , D is a discriminating set for J with respect to s , provide d d log(2 n ) = log(1 = 3 ) . Proof. Let R i ; i 2 I be a column pro le, and C j ; j 2 J a row pro le for ( I ; J ). Condition (1) of the de nition 4 of a discriminating set is certainly met, since f s; j g J , and so sw ( D; f s; j g ) sw ( D; J ) sw ( I ; J ) w .
Therefore D fails to be a discriminating set for J with re-spect to s , only if there is a j = 2 J suc h that sw ( D; f s; j g ) w . The pro of will be completed by sho wing next that the probabilit y of this happ ening for a particular j is at most (3 ) d , so that the probabilit y of it happ ening for some j is bounded by n (3 ) d 1 2 .
 By de nition, sw ( D; f s; j g ) w means that there are C ; C s , and R i ; i 2 D suc h that j A Hence j A i;j A i;s C j w for all i 2 D , and some C (= C C ). We want to sho w that there are no more than 3 j I j rows i that satisfy this inequalit y, because the coCluster is optimal.

Observ e rst of all that if j A i;j A i;s C j w then ( A Since j A i;s R i C s j w= 2 for i 2 I , it follo ws that
Now the optimalit y of ( I ; J ) implies that if there is a subset of rows, I I , and a j = 2 J suc h that j A i;j R i c j w for some c and all i 2 I , then j I j j I j ; for otherwise ( I; J ), with J = J [f j g , is a coCluster of sleev e-width w satisfying ( I; J ) &gt; ( I ; J ), con tradicting the optimalit y of ( I ; J ).

Therefore, for eac h j = 2 J and eac h of the interv als there are at most j I j rows i suc h that A i;j R i c C s lies in that interv al. Thus there are at most 3 j I j rows that satisfy j A i;j A i;s C j w .
De nition 5. Let ( I ; J ) be an optimal coCluster of sleev e-width w . An integer d will be called acceptable for ( I ; J ) if it has the follo wing prop erty. If a subset D I of size d as well as a column s 2 J are chosen at random, then D is a discriminating set for ( I ; J ) with resp ect to s , with probabilit y 1 = 2 at least.
 According to the lemma any d log(2 n ) = log(1 = 3 ) is ac-ceptable. Our exp erimen ts on syn thetic data, rep orted on in section 4, sho wed this bound to be wildly pessimistic: a random subset of size 5 of the rows of a coCluster, was found to be a discriminating set with probabilit y 1.
 Theorem 2.3. Assume that A contains an optimal co-Cluster, ( I ; J ) , of sleeve-width w= 2 , and let d be acceptable for ( I ; J ) . Then, with probability at least 1 = 2 , algorithm RSleeve returns a coCluster of sleeve-width w , ( I; J ) , such that I I and J = J , provide d ` R (2 = ) d ln 4 , and ` S 2 = .

Proof. The probabilit y that a particular choice of D in the outer loop satis es D I is at least d , since j D j = d , and j I j m . By assumption, given that D I it is with probabilit y at least 1 2 a discriminating set for J with resp ect to s . Hence the probabilit y that all ` R iterations of the outer loop fail to nd a discriminating set for J does not exceed (1 1 2 d ) ` R 1 = 4.

Similarly , since j J j m , a particular choice of s in the inner loop over columns satis es s 2 J with probabilit y at least . Therefore the probabilit y that the inner loop fails to nd an s 2 J in all its ` S iterations is at most (1 ) ` S &lt; 1 = 4.

It follo ws that RSleeve chances upon a s 2 J and a discriminating set D I with probabilit y at least 3 = 4 3 = 4 &gt; 1 = 2. When it does, it nds J = J in lines. The resulting I it then computes, necessarily satis es I I . Indeed, for any i 2 I and all j 2 J Thus i 2 I .
The inner for-lo ops tak e O ( mn ) time. The total num ber of iterations is upp er bounded by Theorem 2.3 as ` S ` R (2 = ) d 2 = . The exp erimen ts rep orted on in subsection 3.3 sho w that d can be tak en as 3, and that the num ber of iter-ations of the algorithm is alw ays much less than 16 = ( 3
We note that because of its inheren t parallelism the algo-rithm can easily bene t from special-purp ose hardw are. 1. Instead of setting w a priori, RSleeve could pick a 2. In line 11 of the algorithm, candidate rows could be
We rep ort here on the results of sim ulations using syn-thetically generated data, performed on an Intel Xeon CPU 2.40GHz dual pro cessor with 512 KB cac he size, running Lin ux operating system. The purp ose of the sim ulations was to evaluate various asp ects of the RSleeve algorithm, that can only be assessed with syn thetic data. Speci c questions we wanted to address were the follo wing. 1. How should the sleev e-width be chosen so that the 2. What is the size of the discriminating set, d , needed 3. How man y iterations are needed to nd all signi can t
In eac h sim ulation run a random matrix of m = 20 ; 000 rows and n = 100 columns was rst generated, by setting the ( i; j )-th entry of the matrix to a random integer num ber in the range [0 ; 1000].
In general, the sleev e-width approac h tends to pro duce tigh t coClusters. In our exp erience, setting w to 5% of the range of values of the matrix pro vides a good balance be-tween including most of the signi can t coClusters without introducing spurious ones due to noise. For example, in their analysis of the cell-cycle yeast data, Chen and Churc h [6] adopted a value of 300 for their parameter . This pa-rameter measures the average squared residual of a coClus-ter; therefore a coCluster of sleev e-width w has a value of whic h is close to 5% of the yeast data range, [0,600]. Table 1: Distribution of sleev e-widths. low, high and peak values are in percen tages of the range. tail refers to a sleev e-width of less than 5%.
In order to test our intuition, we selected one million sub-matrices at random, for eac h com bination of coCluster di-mensions, and computed for eac h the sleev e-width. From the accum ulated results we rep ort, in Table 1, the low, high and peak sleev e-widths found in terms of percen tage of the range. The larger the submatrix the closer to 100% of the range its sleev e-width is exp ected to be. The last column states the percen tage of cases that had a sleev e-width of 5% or less. It sho ws that there is an insigni can t probabilit y of nding a coCluster with suc h a sleev e-width in all cases of interest.
The determination of a discriminating set is a cen tral part of algorithm RSleeve . It app ears from Lemma 2.2 as if the size of this set is dep enden t on the parameter measur-ing the trade-o of imp ortance between rows and columns. Not only is specifying not something we want to burden a user with, but also the bound pro vided by Lemma 2.2 is a very coarse one. Moreo ver, the notion of a discrimi-nating set would seem not to need a tie-in to this trade-o . To test these ideas we performed the follo wing exp er-imen ts. After xing the num ber of columns at 5 ; 40 ; 60 or ter with the given num ber of columns was generated and plan ted in the random matrix. Then a subset of size d of the rows of the coCluster was chosen at random 100,000 times , for d 2f 2 ; 3 ; 4 ; 5 ; 6 ; 7 g . Figure 2: Percen tage of discriminating sets among random subsets of a coCluster vs. size of subset, for coClusters with = 0 : 05 ; 0 : 4 ; 0 : 6 ; 0 : 8 .
Figure 2 presen ts a plot of the percen tage of cases in whic h the chosen subset actually was a discriminating set, as a function of d . The imp ortan t nding from this exp erimen t is that already a random subset of size 4 is a discriminating set with probabilit y greater than 0.7. Since d app ears as an exp onen t in the estimated running time, we chose d = 3 in the follo wing exp erimen ts. The plot sho ws also that coClusters with few er columns require larger discriminating sets. The reason is that the discriminating set has to lter out more columns not belonging to the coCluster.
To test the num ber of inner iterations of the algorithm, estimated in subsection 2.3 as ((2 = ) d 2 = ), we generated random data in a manner analogous to the one discussed by Pro copiuc, Jones, Agarw al and Murali [13]. All data generated had values in the range [0, 1000] and were either cluster points or noise. Eac h data matrix had m = 20 ; 000 rows and n = 100 columns, and con tained K = 5 clusters.
After initializing the m n matrix with random data, the coClusters were generated by the follo wing steps. 1. To determine the num ber of rows m k of the coClus-2. The num ber of columns of a cluster was set as a ran-3. The values of the entries of coCluster k were gener-
The outer loops were run, with d = 3, until all coClusters had been iden ti ed. Denote by N the num ber of iterations needed to retriev e a speci c coCluster. Figure 3: Dep endence of the num ber of iterations of RSleeve needed to locate a coCluster on the per-cen tage of rows in a coCluster
Figure 3 is a plot of log 10 ( N ) as a function of log 10 The minim um least squares t to the data is N 4 = ( 2 : 9 and alw ays N 16 = ( 2 : 9 ).
In this section all of the rows and columns of A are tak en into accoun t when computing the sleev e-width. By de ni-tion, with k A k M = max i;j j A i;j j the matrix max norm. A similar notion was used in [6], [7], [11], and [17]. However, they emplo yed the Frob enius norm, In that case it is easily seen that for the best R ; C ,
R i + C j = 1
For the matrix max norm emplo yed here, there is usually no explicit form for the best R ; C , except in the special case considered in subsection 4.1.

Subsection 4.2 presen ts an algorithm for computing the sleev e-width of a general matrix; it is a discrete version of the Dilib erto-Straus algorithm [9].
Theorem 4.1. Let A be a m 2 matrix. Then An explicit solution can also be computed, as follo ws. First perm ute the rows of A so that C T = &lt; 0 ; h &gt; , and R De ne the row-midp oint and column-midp oint operators, P R ( A ) ; P C ( A ), as follo ws.
 Starting with initial R (0) ; C (0) , and E (0) i;j = A i;j the algorithm computes for k = 1 ; 2 ; : : :
E If desired the algorithm also main tains
Theorem 4.2. The DS-algorithm conver ges for any ini-tial B (0) . Mor eover, the sequenc e k E ( k ) k M decreases mono-tonic ally to 1 2 sw ( A ) , and if for some k and row pro les.
We wish to express our appreciation of a perceptiv e and helpful review er. Avraham Melkman ackno wledges with pleasure enligh tening con versations with T.M. Murali. [1] C. C. Aggarw al, J. L. Wolf, P. S. Yu, C. Pro copiuc, [2] C. C. Aggarw al and P. S. Yu. Finding generalized [3] R. Agra wal, J. Gehrk e, D. Gunopulos, and [4] A. Alizadeh, M. Eisen., and R. D. et al. Distinct types [5] Y. Chen, Z. Yakhini, A. Ben-Dor, E. Doughert y, [6] Y. Cheng and G. Churc h. Biclustering of expression [7] H. Cho, I. S. Dhillon, Y. Guan, and S. Sra. Minim um [8] I. S. Dhillon. Coclustering documen ts and words using [9] S. P. Dilib erto and E. G. Straus. On the [10] G. Getz, E. Levine, and E. Doman y. Coupled two-w ay [11] L. Lazzeroni and A. Ow en. Plaid mo dels for gene [12] T. Murali and S. Kasif. Extracting conserv ed gene [13] C. Pro copiuc, M. Jones, P. Agarw al, and T. Murali. A [14] R. Sharan and R. Shamir. Clic k: a clustering [15] P. Tama yo, D. Slonim, and J. M. et al. Interpreting [16] H. Wang, W. Wang, J. Yang, and P. Yu. Clustering by [17] J. Yang, H. Wang, W. Wang, and P. Yu. Enhanced
