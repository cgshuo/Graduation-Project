
Traditional clustering algorithms have mostly focused on creating a single good clustering solution. Data, however, often bear multiple equally reasonable clusterings, especially in high-dimensional settings, so because they can often be interpreted in different ways. This observation has led to the recent emergence of the field of alternative clustering, aiming at creating different clustering solutions that are both of high quality and distinctive from each other, thus providing users with multiple, alternative views on the data structure.

In our observation, current approaches for alternative clustering can be roughly divided into two categories: (i) the objective-function-oriented approach, in which the alter-native clustering process is guided by a diversity-awareness objective function, which drives the search away from one or multiple existing target clusterings; and (ii) the data-transformation-oriented approach, wherein the the alterna-tive clustering process is mainly guided by a data transfor-mation prior to using a regular clustering algorithm. Herein we give a brief review of approaches in these two categories: -Transformation approaches take in an existing target clustering, then transform the data so that the previous clustering is unlikely to be rediscovered, encouraging a clustering algorithm to find novel aspects of the data. An earlier related work is the distance metric learning algorithm by Xing et al. [1] in the context of constrained clustering, which transforms the data so that a number of pre-specified data pairs appear closer to each other in the transformed space. Davidson &amp; Qi [2] used distance metric learning to learn the characteristic transformation matrix of a given clustering. They then used singular value decomposition and inverted the stretcher matrix to obtain the Alternative Dis-tance Function Transformation (ADFT). ADFT transforms the data so that data points that previously appeared in the same cluster now lie far apart and vice-versa. Using a similar idea, recently Qi &amp; Davidson [3] proposed another transfor-mation approach, which transforms the data such that the new data would preserve most statistical characteristics of the original data in terms of the Kullback-Leibler divergence between the empirical distributions, while each transformed individual data point is placed further from its current cluster center, so as to encourage subsequent clustering algorithms to discover a different cluster structure. Cui et al. [4] motivated their transformation approach from the view point of orthogonalization, and proposed two orthogonal transformations, called orthogonal clustering and orthogonal subspaces. Both orthogonal transforms successively project the data to the residue space that is orthogonal to the space containing the current clustering. -Objective-function-oriented approaches: Works in this category can be further divided into semi-supervised methods, which take an existing target clustering as negative information, and unsupervised methods. Semi-supervised methods include the seminal Conditional Information Bot-tleneck (CIB) method [5], which extends the information bottleneck framework by conditioning on a known, given, existing clustering; the COALA hierarchical technique [6] which attempts to satisfy as many of the cannot-link con-straints generated from a given clustering as possible; and the recently proposed NACI [7] algorithm, which uses the mutual information to quantify both clustering quality and distinctiveness from a given clustering. Unsupervised methods can be concurrent methods such as the decorrelated K-means and convolutional-EM algorithm [8] which concur-rently fit two clusterings to the data subject to maximizing a measure of orthogonality between clusterings; and the CAMI algorithm [9] which concurrently fits two Gaussian mixture models to the data subject to minimizing the mutual infor-mation between the two clusterings. Unsupervised methods can also proceed in a sequential manner, in which clusterings are discovered one after another, such as the minCEntropy algorithm to be proposed in this paper.

Apart from the mentioned approaches, there are other hybrid approaches that make use of both data transformation and a diversity-awareness objective function. Meta clustering [10] can be regarded as such a method, though the transfor-mation is done in a random fashion via feature weighting, and an objective function is only used in the post-processing step to group the clusterings. A more recent approach is the work by Niu et al. [11] which combines data transformation with spectral clustering.

This paper proposes a novel information theoretic based, objective-function-oriented approach to generate clusterings and alternative clusterings. We employ the conditional en-tropy to quantify both clustering quality and diversity, result-ing in an analytically consistent combined objective. We pro-pose a partitional clustering algorithm, named minCEntropy, to concurrently optimize both objectives. Compared with other approaches, minCEntropy has several distinct features: (i) despite being built based upon measures of information, it does not place any assumption on the data distribution. Indeed, our method does not make use of the conventional Shannon entropy, but is based on a generalized measure of Entropy namely the Havrda-Charvat X  X   X  structural entropy. In combination with Parzen window density estimation, the final resulting algorithm is conceptually clear, simple and easy to implement. (ii) We propose a simple yet effective heuristic for setting the kernel parameter and the quality-diversity trade-off parameter. (iii) minCEntropy can be flex-ibly used in both unsupervised or semi-supervised settings. It can be used in conjunction with other data-transformation-oriented approaches, and can take in either one or multiple target existing clusterings.

The paper is planned as follows: Section II and III lay out our minCEntropy approach for generating clustering and alternative clusterings. Experimental results are presented in section IV, followed by discussions and conclusions. A. Method
Given a data set X = { x 1 ,x 2 ,...,x N } of N data items in R d , a partitional clustering C = { c 1 ,c 2 ,...,c way to divide X in to K non-overlapped subsets. Let C be the space of all possible K -cluster partitions of X . We are interested in identifying the clustering C  X  in C which maximizes the mutual information between the data and the clustering: or equivalently: since I ( C ; X ) = H ( X )  X  H ( X | C ) , and H ( X ) is constant. Such an objective function for clustering has been previ-ously motivated and used in various works [7], [12] X  X 14]. Intuitively, it says that a good clustering should maximized the information shared between the clustering and the data. This objective also has an interesting interpretation similar to the likelihood objective L ( X |  X ) in model based clustering, where  X  is the set of parameters governing the model. Let a clustering C be a hypothesized structure or model imposed on the data, then H ( X | C ) measures how well the model fits our data. A good clustering therefore minimizes the data uncertainty given the cluster label.

Without much assumption on the data distribution, it is generally hard to estimate the conditional entropy in (2). Here we will employ the original idea of Principe et al. [15], which allows us to proceed. These authors have shown that using a suitable entropy measure and a suitable distance measure, in conjunction with the Parzen window method for density estimation, the resulting distance between the probability distribution functions (pdf) admits a simple form. More specifically, they proposed using the quadratic Renyi X  X  entropy, and the squared Euclidean distance I ED between the pdf X  X . Strictly speaking, I ED is a measure of shared information, but is not the mutual information itself by definition. In this paper, we present a different approach that allows us to proceed more straightforwardly. We shall make use of the general Havrda-Charvat X  X   X  -structural entropy [16], defined as: With  X  = 2 we obtain the following quadratic Havrda-Charvat X  X  entropy (with the constant coefficient discarded for simplicity): The conditional quadratic Havrda-Charvat X  X  entropy of X given C is defined as: With this measure of entropy, our objective in (2) becomes: Now let us revisit the Parzen window method for probability density estimation, and see how this method when combined with the objective above will result in a nice and neat esti-mation. Let X  X  G (  X  ) be the Gaussian kernel in d -dimensional space: where  X  is the kernel width parameter and a is the center of the Gaussian window, then the density estimation of X is given by: Now the quadratic entropy of p ( x ) can be estimated as: wherein we have employed a nice property of the Gaussian kernel, which is that the convolution of two Gaussian remains a Gaussian:
Z Similarly we have:
H 2 ( X | C = c k ) = 1  X  where n k is the number of data items in cluster c this estimation, our objective in (6) becomes:
Note that the probability of encountering the cluster c k C is n k /N , we finally have C  X  = arg max C  X  X  CE ( C ) with being our objective function. By maximizing CE ( C ) we minimize the conditional entropy criterion.
 B. Comparison with the K-means Clustering Formulation
The K-means clustering algorithm minimizes the follow-ing total sum-of-squares objective function: objective can also be expressed in terms of the pairwise distances between data items within clusters [17]: Interestingly, this objective function looks exactly analogous to our objective in (11). While K-means aims to minimize the weighted sum average intra-cluster distances in terms of the squared Euclidean distance, the minimum conditional en-tropy criterion aims to maximize the weighted sum average intra-cluster similarity as judged by the Gaussian kernel. C. minCEntropy -a Hill Climbing Strategy to Optimize the Minimum Conditional Entropy Objective
Despite its simple looking appearance, the K-means clus-tering problem has been shown to be NP-hard even for K = 2 [18]. Therefore, although the hardness of the minimum conditional entropy optimization problem is still an open question, a heuristic approach seems to be indicated. In this section, we develop a hill climbing approach, called minCEntropy, to iteratively improve the conditional entropy objective function. The strategy resembles that of K-means: in each iteration, each data point is considered to be moved to a new cluster that is closest to it. While in K-means, the point-to-cluster closeness is judged by the distance from a point to a cluster center, in minCEntropy clustering, this is judged by the total similarity between a point and all the other data points in the clusters. Our algorithm will make use of the following three data structures:  X  S N  X  N : an N  X  N array which contains the pairwise  X  SPC N  X  K : an N  X  K array which contains the point- X  Q 1  X  K : a length-K vector which contains the clus-
Note that using these notations, CE ( C ) = P K k =1 Q k /n Now suppose that we are considering moving a data item x from its current cluster c l to cluster c k . With this change, only the qualities of c l and c k are affected, i.e. : Q Q The change in the objective value is therefore:  X  CE ( C | x i ,c l  X  c k ) = = For each datum x i , the membership re-assignment that results in the largest objective improvement is c arg max c  X  C  X  CE ( C | x i ,c l  X  c ) . If k 6 = l , the datum is moved using the following procedure, that updates the point-to-cluster similarities as well as cluster qualities in the new clustering:
Update ( x i ,c l  X  c k ) : c l  X  c l \{ x i } c k  X  c k  X  X  x i } for j = 1 to N do end for Q l  X  Q l  X  2 SPC il Q k  X  Q k + 2 SPC ik We are now ready to state the minCEntropy algorithm: Algorithm 1 minCEntropy
Input: The data set X = { x 1 ,x 2 ,...,x N } , the number of desired clusters K . Optional: A hard initial clustering C = { c 1 ,c 2 ,...,c K } on X .
 Initialization: -Calculate the pairwise similarity kernel matrix S . -Generate a random initial hard clustering C (if none provided). -For C , calculate the point-to-cluster similarity matrix
SPC , and the cluster quality vector Q . -continue  X  TRUE
Begin: while continue do end while Note: mem ( x i ) = { c l | x i  X  c l } Convergence: The algorithm loops until there is no pos-sible single membership change that strictly decreases the conditional entropy. Since the entropy is lower-bounded by zero, and since there is only a finite number of possible clusterings, it can be seen that the algorithm will eventually terminate after a finite number of steps.

Proposition 1: minCEntropy converges to a local mini-mum of H ( X | C ) .

Indeed, H ( X | C ) is a discrete function of the discrete variable C , and the problem of minimizing H ( X | C ) is a discrete optimization problem on the space of clustering C . At convergence, no single change in membership could increase the objective, meaning no neighbor clustering of C  X  is better than itself. Therefore, C  X  is a local optimum of H ( X | C ) .
 Complexity analysis: -The initialization step costs O ( N 2 d ) time since the complete kernel matrix need to be set up. In reality, only N ( N  X  1) / 2 distances need to be computed due to its symmetry. -Finding the best target cluster for each datum costs O ( K ) time. The Update procedure costs O ( N ) time. The cost of the main loop of the algorithm is therefore O ( IN ( K +  X N )) where I is the number of iterations and 0 &lt;  X  &lt; 1 is the expected ratio of data items that change membership. According to our observation, the number of membership changes is large for the first few iterations, then quickly reduces as the algorithm converges.

Overall, the time complexity of minCEntropy is domi-nated by the quadratic cost of computing the kernel matrix. With a fixed kernel, after computing the kernel matrix S , it is a good strategy to run the algorithm several times with different initialization to obtain a better optimum.
In this section, we develop an extension of minCEntropy to generate one alternative clustering. Let C 1 be a pre-provided clustering of R clusters { c 1 1 ,c 1 2 ,...,c 1 ably by either minCEntropy or any other clustering method, we would like to find a clustering C  X  which concurrently satisfies two objectives: (i) high quality; and (ii) distinc-tiveness from C 1 . As clustering distinctiveness can be well measured by means of the mutual information between two clusterings, we can formulate the following multi-objective optimization problem: where  X  is a trade-off factor used to equilibrate the two objectives. Note that I ( C,C 1 ) = H ( C 1 )  X  H ( C 1 | H ( C 1 ) is a constant, we have the following equivalent optimization problem: Using the notation of quadratic Havrda-Charvat entropy, we have: with n lk denote the number of data items shared by cluster c k in C and c 1 l in C 1 . The resulting optimization problem is: A. Objective Calibration and Optimization
Since the scale of the two objectives, quality and distinc-tiveness, is different, it is important to choose a suitable factor  X  to calibrate them. Note that for quality, for each c we have: while for distinctiveness, for each c k we have: We therefore omit the kernel normalization factor (2  X  X  )  X  d/ 2 , bringing the two objectives to a common scale. This omission does not affect our formulation, since the kernel normalization factor is essentially a weighting factor, which now becomes the sole duty of  X  . Our final optimization problem takes the form: where the new objective is a weighted combination of quality, measured by CE ( C ) as defined in (11), and diversity,
Now suppose that we are moving a single data item x from cluster c l to cluster c k in C . Assume that x i C 1 . The change in CE ( C ) is  X  CE ( C ) as in (14), while the change in DI ( C 1 | C ) can be shown to be:
In order to improve our objective we must have:  X  CE ( C | x i ,c l  X  c k )+  X   X  DI ( C 1 | C,x i ,c l  X  c The potential target cluster for each datum x therefore given by c k = arg max c  X  C {  X  CE ( C ) + operation in minCEntropy (Algorithm 1), we obtain the minCEntropy + algorithm for discovering an alternative clus-tering distinctive from a given clustering. It can be easily shown that minCEntropy + converges to a local minimum of H ( X | C )  X   X H ( C 1 | C ) , and admits a complexity of O ( IN ( KR +  X N )) for the main loop.
 B. minCEntropy ++ for Discovering Alternative Clustering that is Different from Multiple Given Clusterings minCEntropy + can be easily extended to discover an alternative clustering that is different from multiple pre-given clusterings. Using the material developed from the previous sections, this extension is pretty straightforward. Let { C 1 ,C 2 ,...,C M } be M given clusterings, each with to discover a clustering C  X  that is (i) of high quality; and (ii) distinctive from { C 1 ,C 2 ,...,C M } . That means, we would like to optimize all the CE ( C ) +  X  CE ( C CE ( C ) +  X  DI ( C 2 | C ) , . . . ,CE ( C ) +  X  CE ( C reasonable objective would therefore be: The potential target cluster for each datum x now given by c k = arg max c  X  C { M.  X  CE ( C ) + that minCEntropy ++ converges to a local minimum of H ( X | C )  X   X  P M u =1 H ( C u | C ) , and admits a complexity of O ( IN ( K P u K u +  X N )) for the main loop.
 C. Parameter Setting
Kernel width parameter  X  : There is a vast literature on kernel width parameter tuning. Herein, we adapt a simple heuristic proposed in [19]. In his work in feature extraction using transformation, Torkkola suggested using a kernel width  X  of half of the distance between two furthest data points in the output space. A tempting idea is therefore to choose  X  as half of the the distance between the furthest data point in our data space. This choice is however, susceptible to outliers. We have experimentally taken  X  as half the aver-age pairwise distances, i.e.  X  0 = 1 2 N 2 P x which seems to work fairly well as compared to some other approaches that we have implemented, such as kernel width annealing, in which one starts with a large kernel and then gradually reduces it in a deterministic annealing fashion; or variable kernel width [20] in which each datum has its own kernel width based on local density.

The trade-off parameter  X  plays an important role in cal-ibrating the two objectives, quality and diversity. As  X   X  0 , minCEntropy + / ++ behave like the original minCEntropy, while as  X   X   X  , very low quality clusterings that are maximally different from the given clustering are created. A general rule of thumb is to equalize the range of quality and diversity, and probably put slightly more weight on the quality side. We propose the following simple heuristic: at the end of the initialization step, set  X  = (1 /m )  X  with  X  0  X  M. CE ( C ) / P M u =1 | DI ( C u | C ) | (roughly meaning that we judge quality m times as important as diversity). Generally we have found that m  X  [1 , 3] works reasonably well for minCEntropy + . Note that if the algorithm is to be restarted with different initializations,  X  needs to be set only once in the beginning, so that the objective values are comparable across runs.
 A. Method
We compared the minCEntropy and minCEntropy + algo-rithms (herein referred to as minCEntropy/ + ) against several recently proposed methods: -Transformation approaches: we employed the two or-thogonal transformations proposed by Cui et al. [4], herein referred to as Ortho1 and Ortho2; the ADFT transform [2]; the transformation in [3], herein referred to as Qi X  X  transform. All authors reported good performance with the K-means algorithm. In this work, we used K-means and spectral clustering to accompany these transformations. On image data, we employed the regular K-means algorithm, while on text data, we used the spherical K-means variant (S. K-means), which works better for the text domain [21]. On image data, we employed the self-tuning spectral clustering algorithm (S.T. Spectral) [20], with a Gaussian kernel and variable kernel width for each datum, while on text data we used the regular spectral clustering algorithm with the dot product between normalized feature vectors as the similarity measure. -Objective-function-oriented approaches: We took the decorrelated K-means (Dec. K-means) algorithm for com-parison [8]. Note that Dec. K-means concurrently generates 2 clusterings, therefore it is not possible to incorporate side information, such as ground-truth for the first clustering in this case. We now digress a bit and introduce a modified, semi-supervised version of the Dec. K-means algorithm (S-Dec. K-means), that can take in a clustering as negative information. More specifically, the Dec. K-means employs gradient descent to optimize the following objective func-tion: where  X  i , X  i are respectively the mean and representative vectors of cluster c 1 i in clustering C 1 with K clusters, and  X  , X  j are respectively the mean and representative vectors of cluster c 2 j in clustering C 2 with R clusters. Now suppose that we are given a target clustering C 1 , we simply set  X  i =  X  i for every cluster in C 1 , and fix this set of mean and representative vectors during the iterations. S-Dec. K-means then optimizes the objective with respect to the second clustering only.

We gave the true number of clusters in each clustering to all algorithms. Dec. K-means generated two clusterings con-currently, which were then matched against all the ground-truth clusterings to find the best match. For sequential methods, K-means, Spectral clustering and minCEntropy first generated the first dominant clustering. Transformed data were generated with input from either the first clus-tering generated by the algorithms, or the ground-truth first clustering. Note that since the ADFT transform (with the distance metric learning algorithm) is almost three orders of magnitude slower than the other transforms, we only tested it once using the ground-truth first clustering as input. S-Dec. K-means was tested with the first ground-truth clustering given. We also tested objective-function-oriented approaches on top of transformation approaches. 1) Initialization and Parameter Setting: The parameters for minCEntropy/ + were set according to Section III-C. Unless otherwise stated, we set m = 2 for the trade-off pa-rameter. On each data set, minCEntropy/ + were executed 10 times, each with 10 different random initializations (for 100 runs in total). Note that for minCEntropy + these 10 objective values are not strictly comparable due to the difference in the trade-off factor  X  . We also ran other competitor algorithms 10 times, each with 10 different initializations. The trade-off parameters for Dec. K-means and S-Dec. K-means were set according to the authors X  suggestions [8]. 2) Quality Assessment and Report: Clustering quality was assessed using the Adjusted Mutual Information (AMI) [22], an adjusted-for-chance version of the normalized mu-tual information [23], between the clustering and the ground-truth classification. For visualization, we selected the best clustering in terms of the objective value, while for tabula-tion, we reported the AMI mean  X  standard deviation values. B. Artificial data
As a sanity check, we first tested the minCEntropy/ + / ++ clustering algorithms on several artificial data sets. minCEn-tropy discovered the first clustering. Its output was then given to minCEntropy + / ++ to discover the second/third clustering.

Set 1: We generated 6 Gaussian clusters, each with 100 points in 2 dimensions, as presented in Fig. 1(a). When the number of clusters K is set to 3, it can be seen that there are two reasonable clustering solutions, discovered sequentially by minCEntropy/ + as expected (Fig. 1 b-c). On the same data set, if we set K = 2 , minCEntropy/ + discovered the first two clusterings as in Fig. 1(d-e). Taking minCEntropy/ clusterings as input, minCEntropy ++ generated the third clusterings as in Fig. 1(f). If m was set to 3 for the trade-off parameter, then minCEntropy + and minCEntropy ++ generated the two alternative clusterings as in Fig. 1(g-h), which indeed consist of more compact clusters, since now the weight on clustering quality has been increased.
Set 2: We generated 8 Gaussian clusters of different sizes and densities, each with 100 points in 2 dimensions, as presented in Fig. 1(i). With K = 4 , minCEntropy/ discovered the two reasonable clusterings as in Fig. 1(j-k). Given K = 2 , minCEntropy/ + / ++ sequentially discovered the three clusterings as in Fig. 1(l-n).
 C. Real data
Due to the lack of real datasets with multiple good ground-truth clusterings, we only tested the minCEntropy/ and other algorithms on several datasets with two known, good ground-truth classifications. 1) The ALOI data set: The Amsterdam Library of Object Images (ALOI) [24] consists of 110,250 images of 1000 common objects. For each object, a number of photos are taken from different angles and under various lightning conditions. Based on color and texture histograms, we first extracted a dense 641-dimensional feature vector for each object, using the method described in [25]. For a test set, we chose 9 objects of different shapes and colors as exemplified in Fig. 2, for a total of 984 images. We further applied principle component analysis (PCA) to further reduce the number of dimensions to 16, which retains more than 90% variance of the original data.

It can be seen that there are 2 reasonable ways to cluster the objects: by color ( K 1 = 3 ) or by shape ( K We first ran the minCEntropy algorithm, for which the result is presented in Fig. 3(top row), where it can be seen that the objects are clustered by color, which is the dominant first clustering for this data set. We next executed the minCEntropy + algorithm on the same data set, with the minCEntropy clustering result given as C 1 . The result is presented in Fig. 3(bottom row), where it can be seen that now the objects are grouped by their shapes as expected. The full clustering results for all algorithms are presented in Table I. 2) The CMUFace data set: The CMUFace data set, drawn from the UCI repository [26], contains 624 32  X  30-images of 20 people, taken with varying pose (straight, left, right, up), expression (neutral, happy, sad, angry) and eyes (wearing sunglasses or not). We applied principle component analysis to reduce the number of dimensions to 39, which explains more than 90% of the original data variance. minCEntropy was first applied, with K 1 = 20 (for the 20 people in the database). It can be observed on Fig. 4 that the objects are clustered by person. Given the ground-truth person clustering and set the number of clusters K minCEntropy + discovered the second clustering where the objects are clustered by four different poses: straight, up, left and right as demonstrated in Fig. 5. The full results for all algorithms are presented in Table I. 3) The Reuters Corpus Volume I (RCV1) dataset: We tested the clustering algorithms on the RCV1, an archive of over 800,000 manually categorized newswire stories re-cently made available by Reuters, Ltd. for research purposes [27]. For a test set, we selected all documents in one of the three meta topics, namely Corporate/Industrial (CCAT), Economics (ECAT) and Markets (MCAT), that also contain one of the three countries names U.S., Japan and China in their titles. We preprocessed the data by removing common words and rare words, stemming, and used TF-IDF weight-ing to construct the feature vectors. The final data matrix contains 378 documents and 242 words. As per common practice for text data, the feature vector for each document was normalized to have unity norm. The two ground-truth clusterings for this data set are thus topic (ECAT, MCAT, CCAT), and country (U.S., Japan, China), with topic being the dominant clustering, discovered first by all algorithms. The full clustering results for all algorithms are presented in Table II. 4) The WebKB data set: (www.cs.cmu.edu/~webkb) con-tains html documents collected mainly from four univer-sities: Cornell, Texas Austin, Washington and Wisconsin Madison. We selected all documents from those four uni-versities that fall under one of the four page types namely course, faculty, projects and students. After a preprocessing step similar to that done on the RCV1 data set, the final data matrix contains 1041 documents and 486 words. This data set can be either clustered by the four universities or by the four page types. The full clustering results for all algorithms are presented in Table II.

Overall, minCEntropy/ + with the default parameter set-ting performed competitively. Albeit not always produc-ing the best clusterings, its results were more consistent and competitive for both clusterings across all the data sets. Our experiments also confirmed good performance of the K-means algorithm on top of a transformation approach as reported in previous works [3], [4], espe-cially the Spherical K-means which is fine-tuned for text data. Spectral clustering performance appeared to fluctuate more. Of the transformation approaches, the performance of the two orthogonal transforms Ortho1 and Ortho2 was more consistent. The ADFT and Qi X  X  transforms (using the implementation provided by the author, available at http://wwwcsif.cs.ucdavis.edu/~qiz/code/code.html) with the default parameter performed poorly on the two text datasets. Although transformations have a relatively slight effect on minCEntropy + , their effect on another objective-function-oriented approach, namely the semi-supervised Dec. K-means, was more substantial, with marked improvement on 3 over the 4 real data sets. For all sequential approaches to generating alternative clusterings, having access to the first ground-truth clustering significantly improved the quality of the second clustering.

Most objective-function-oriented approaches such as CIB [5], Dec. K-means, convolutional-EM [8], semi-supervised Dec. K-means, CAMI [9], NACI [7] and minCEntropy/ + are based on a common formulation, with a combined criterion of clustering quality and diversity. Dec. K-means quantifies clustering quality by the regular sum-of-squared-distance criterion, while diversity is quantified by the sum of squared dot product between the mean and representative vectors of clusters in the two clustering. CAMI combines the traditional likelihood criterion for clustering quality with the mutual information for clustering diversity. minCEntropy on the other hand, employs the conditional entropy for both quality and diversity, resulting in a more analytically consistent combined criterion. The closest approach to ours is the recently proposed NACI method, which uses the mutual information for both clustering quality and diversity. NACI employs a greedy hierarchical strategy to optimize its objective, and therefore presumably shares all the advantages and disadvantages with traditional hierarchical clustering ap-proaches. While greedy agglomerative approaches generally possess strong capability for handling non-convex, irregular shaped clusters, their main disadvantage is that finding even a local optimal clustering solution is not guaranteed [28]. minCEntropy on the other hand employs a partitional clus-tering approach, for which convergence to a local optimal solution is assured.

Methods for revealing alternative clusterings can proceed in a sequential manner, in which alternative clusterings are created one after another (CIB, COALA, NACI, S-Dec. Kmeans, minCEntropy); or in a concurrent manner, in which a pair of clusterings is created in parallel (Dec. K-means, convolutional-EM, CAMI). In our opinion, con-current schemes for discovering alternative clustering suffer from several disadvantages: (i) their optimization problem becomes  X  X wice X  as hard, as the number of variables is dou-bled since 2 clusterings need to be handled concurrently. As the dimension of the optimization problem increases, local optimization procedures such as gradient descent (employed by Dec. K-means) or Expectation-Maximization (employed by CAMI, convolutional-EM) become less effective (the curse of dimensionality). (ii) Extension to more than 2 clus-terings is likely to be much more involved. (iii) It is not pos-sible to combine these approaches with data-transformation-oriented approaches. (iv) Finally, side information, such as an existing classification, cannot be incorporated into these algorithms. On the other hand, sequential approaches for alternative clustering, which generally do not suffer from the mentioned shortcomings, appear to be a more flexible choice.

This paper has introduced minCEntropy/ + , an objective-function-oriented approach to sequentially generate cluster-ings and alternative clusterings, in either a semi-supervised or unsupervised manner. Being built upon the strong foun-dation of information theory, minCEntropy nevertheless re-quires no specific assumption on the density functions, but employs an efficient procedure to estimate their information quantities. minCEntropy with the recommended parame-ter setting showed consistent and competitive performance throughout our experiments. As a final note, it is possible to develop a hierarchical version of minCEntropy, which will probably handle well data with non-convex, irregular-shaped clusters. Also, a directional version of minCEntropy, which employs a directional distribution, such as the von Mises Fisher, for its density estimation will probably handle better directional data, such as text and microarray data. Our future work includes investigating these variants of minCEntropy, together with a more comprehensive evaluation against the most recently proposed methods [7], [11], which are con-current with the time of this submission.

Availability : Matlab implementation of the minCEntropy/ + algorithms will be made available via our web site.

