 G Abstract The development of precision grammars is an inherently resource-intensive process; their complexity means that changes made to one area of a grammar often introduce unexpected flow-on effects elsewhere in the grammar which may only be discovered after some time has been invested in updating numerous test suite items. In this paper, we present the browser-based G D ELTA tool, which aims to provide grammar engineers with more immediate feedback on the impact of changes made to a grammar by comparing parser output from two dif-ferent grammar versions. We describe an attribute weighting algorithm for highlighting components of the grammar that have been strongly impacted by a modification to the grammar, as well as a technique for clustering test suite items whose parsability has changed, in order to locate related groups of effects. These two techniques are used to present the grammar engineer with different views on the grammar to inform them of different aspects of change in a data-driven manner. Keywords Precision grammar Grammar engineering Grammar diagnostics Deep parsing Parse mining 1 Introduction In this paper we investigate techniques for generating feedback on the impact of changes made to precision grammars, and present the G D ELTA tool, which is intended to combat a significant drawback of linguistically accurate hand-crafted grammars X  X he consider-able amount of time and resources involved in their development. G D ELTA is intended for use during the grammar development cycle, to be presented to grammar engineers as immediate feedback on the impact of a change or set of changes made to a grammar. This feedback is intended to assist grammar engineers in more readily understanding how the grammar has been affected, thus reducing the amount of time required to track down issues introduced by changes to the grammar.

Previous work that aims to provide feedback for use in the grammar engineering process (Oepen and Carroll 2000 ; van Noord 2004 ; Waterman 2009 )hasfocusedonthe analysis of parser output from just one version of a grammar. Our approach differs in that it highlights the differences between two versions of a grammar: one from before and one from after a change made to a grammar. This work can be conceptualised as the development of a diff tool that operates over parser output from two different versions of a grammar run across the same corpus. Rather than trying to improve upon or replace existing development tools, we see this tool as being complementary, filling a gap in the grammar engineering toolchain. G D ELTA was developed specifically for use with grammars developed within the DELPH-IN community, 1 however the underlying techniques that drive G D ELTA and are described in this article are largely framework agnostic, and could be readily applied to other grammar engineering frameworks. Additionally, these techniques work cross-linguistically, as demonstrated by the application of G D ELTA to grammars of English and Japanese.

The remainder of this paper is structured as follows. Section 2 briefly expands upon the concept of a precision grammar and outlines the grammar engineering process, before presenting an overview of existing tools available to grammar engineers. Section 3 outlines our methodology, including the resources we used and the underlying techniques that G D ELTA makes use of. Section 4 provides a walkthrough of the output that G D ELTA generates and outlines how it is intended to be used. Section 5 outlines a small-scale evaluation of the techniques introduced in this paper and contains a discussion of the results. Section 6 provides a brief overview of the implementation details of G D ELTA , along with the requirements for running G D ELTA and how to access the source code. Section 7 identifies further work to be pursued in his area, and Sect. 8 concludes the paper. 2 Background 2.1 Grammar engineering Precision grammars are motivated by the desire for automated and accurate linguistic analysis of natural language. They are usually based upon or heavily influenced by formal theories of syntax developed within the field of linguistics. The syntactic theory that informs the DELPH-IN grammars used in this paper is Head-driven Phrase Structure Grammar (HPSG: Pollard and Sag 1994 ). Examples of other grammar engineering approaches include the CoreGram project (Mu  X  ller 2013 ), also based on HPSG; the ParGram project (Butt et al. 1999 , 2002 ) based on Lexical Functional Grammar (Dalrymple 2001 ); the E X TENSIBLE M ETA G RAMMAR tool (Crabbe  X  et al. 2013 ), which can be used to create tree-based grammars such as Tree Adjoining Grammar (Joshi and Schabes 1997 ) and Interaction Grammar (Guillaume and Perrier 2009 ); and also D OT CCG (Baldridge et al. 2007 ), a tool used to develop Combinatory Categorial Grammars (Steedman 2000 ).

These foundations in formal syntax mean that precision grammars all share the property that they draw a sharp distinction between sentences that are grammatical and those which are not. Compared with induced grammars X  X s are used in most treebank parsers X  X recision grammars tend to generate far more detailed analyses and are able to capture much more complex linguistic constructions (Bender et al. 2011 ). Precision grammars are also usually able to capture the underlying semantic structure of language. It is for these reasons that the use of precision grammars to perform natural language parsing is often referred to as deep parsing .

Grammar engineering is the process of developing and extending the coverage of precision grammars as well as improving the quality of existing analyses. It is a resource-intensive process, requiring grammar engineers with both a linguistic background as well as an understanding of the formalism in which the grammar has been implemented. A number of factors combine to make grammar engineering a slow and time-consuming process. One is the considerable size and complexity of these grammars, with the various components involving a high degree of interaction between each other, meaning that modifications made to one part of the grammar can often produce unexpected flow-on effects in other areas of the grammar. Another factor is the high degree of accuracy that is demanded of these grammars. Over time, as improvements are made and the grammar is expanded to handle more linguistic constructions, it is important that the validity of analyses produced by the grammar does not degrade, and that the grammar does not stop being able to parse constructions that it once could.

Just as in software engineering, the grammar engineer has a number of tools available to provide quality assurance. In grammar engineering, the practice of monitoring the quality of analyses produced by the grammar is referred to as grammar profiling . This involves the maintenance of test suites which facilitate the monitoring of various signals, providing a means of ensuring that the grammar does not regress. In the DELPH-IN community, test suites are maintained through the use of profiles , which contain sets of items and information pertaining to how the grammar performed over the items. An item consists of a string from the target language, which, depending on the purpose of the test suite, can be naturally occurring or constructed to illustrate a particular phenomenon, and can also be marked as grammatical or ungrammatical. One of the aims of the TSNLP project (Oepen et al. 1997 ), which the DELPH-IN profiling technology originates from, was to additionally annotate items with grammatical phenomena they exemplified as a means of further evaluating and debugging the performance of grammars. The complexities involved in creating and maintaining such phenomenon annotations have, however, meant that this practice has not been adopted.

The most prominent aspect of the grammar that must be monitored is the range of linguistic phenomena and lexical items that it is able to handle X  X he coverage of the grammar. This is monitored by test suites constructed to track the extent to which a grammar undergenerates . Test suites can also contain ungrammatical or negative items used to test the degree to with a grammar overgenerates . In addition to these signals, the number of analyses per item is also monitored. While it is true that ambiguity is an inherent feature of language that cannot be eliminated, it is important to remove spurious ambiguity where possible. Grammar profiling tools also provide information regarding changes in the operational performance of the grammar, such as the time taken to process test suites and the number of operations required for parsing. Just as with testing in traditional software development, unexpected changes in any of these aforementioned statistics are potential symptoms of introduced errors. For further discussion of the requirements of grammar profiling tools, see Oepen and Flickinger ( 1998 ).

A limitation of grammar profiling tools is that they are not able to detect the misanalysis of items, i.e. when the analysis of an item does not match its structure. This is of particular concern when developing precision grammars as their architecture tends to be complex, reflecting the inherent complexity of language, and just as linguistic phenomena interact with each other, so too do the implementations of their analyses. This means that a modification to a grammar pertaining to a particular phenomenon can often result in unanticipated flow-on effects in other areas of the grammar. 2
A resource that can be used to monitor the accuracy of analyses is the treebank ,a corpus annotated by selecting a good analysis from the alternatives produced by the parser. Whenever a change is made to the grammar that alters the stored analysis of a treebanked item, the analysis provided by the previous version of the grammar can be used to verify that the change has not led to a degradation in quality. When using treebanks for this purpose, a necessary step in the grammar engineering cycle is the updating of items affected by the change to the grammar. This is performed to catch errors resulting from the change, and also to periodically synchronise the treebank to the current state of the grammar. In the DELPH-IN grammar engineering pipeline, this dynamic treebanking methodology (Oepen et al. 2002 ) takes advantage of previously recorded annotations by automatically applying (where possible) the outcomes of decisions regarding the resolution of local ambiguity. This greatly reduces the number of decisions an annotator must make following a change, however it remains the case that following a significant change to a grammar, the treebanking stage can be a potentially labour intensive process. 2.2 A gap in the toolchain Grammar profiling and treebanking are both vital to the grammar engineering process, each representing opposite extremes in the level of feedback they provide: grammar profiling providing immediate but course-grained feedback on the coverage and performance of the grammar, and treebanking providing slower, fine-grained feedback on an item-by-item basis. The grammar profiling stage can locate glaring problems such as coverage levels suddenly dropping or not changing when expected to, but the detailed level of feedback provided by treebanks is crucial to catching unanticipated problems not visible in the grammar profiling stage, as well as more generally building up a picture of how the change impacted the grammar. Unfortunately, the inspection and updating of treebanks can be a slow process, and an all too real possibility for the grammar engineer is locating a serious problem after considerable time has been invested in updating the treebank. When this occurs, the grammar engineer must resolve the problem in the grammar and start the process over again.

We argue that it would be desirable for there to be a tool which offers a middle ground between these two levels of feedback, one which makes the impact of the change more transparent earlier on in the grammar engineering cycle, helping the grammar engineer more quickly gain a superficial understanding of the impact of the change over the test suites. In the next section, we outline a range of existing tools and approaches that grammar engineers could potentially use to gauge the impact of a change made to a grammar. Each approach is assessed on the basis of how well it can be used to provide the desired kind of feedback. 2.3 Previous work van Noord ( 2004 ) proposed a grammar error mining technique for detecting errors in grammars. This involves assigning a parsability score to each word n -gram in a corpus, based on the ratio of the number of times each n -gram occurs in successfully parsed items over the number of items it occurs in the whole corpus. The end result is a list of n -grams, ordered by parsability, with the lowest scoring n -grams representing those most likely to be indicative of problems in the grammar. This technique has been further improved by Sagot and de La Clergerie ( 2006 ) and de Kok et al. ( 2009 ). An advantage of this error mining approach is that the only input required is a binary success or failure value for each item, meaning that it is agnostic towards the implementation of the parser and grammar. A drawback to this approach, however, is that the results are presented in terms of surface string n -grams, whereas information pertaining to the internal components of the grammar would be more informative to the grammar engineer. This is indicative of the larger problem, which is that without an analysis from the grammar, the error mining input is limited to unstructured text.

Two error mining approaches which do utilise structured input to yield further insight are that of Goodman and Bond ( 2009 ) and Gardent and Narayan ( 2012 ). Goodman and Bond ( 2009 ) introduce E GAD , a tool which leverages the ability of precision grammars to both generate and parse in order to identify errors involved in generation. For each successful parse, they use the corresponding semantic form to generate the set of possible paraphrases, flagging a sentence as a failure if the original surface form cannot be generated. They then use a maximum entropy classifier to find n -grams over paths in the derivation tree which are predictive of generation failure, resulting in a better picture of where the underlying error in the grammar might lie. Similarly, Gardent and Narayan ( 2012 ) extend the error mining approach of de Kok et al. ( 2009 ) to develop a suspicion measure for dependency subtrees, applying this to input trees for which they are unable to generate a surface form.

These automated approaches towards error detection in grammars are useful for directing the development of a grammar X  X hey are good at locating gaps in parsing and generation coverage X  X owever they are ill-suited to providing feedback on immediate changes made to a grammar, as there is no guarantee that the problems they report will be related to changes just made to the grammar. This is due to the use of a single version of the grammar in the input: without comparison to a previous version, there is no bound on how far back the errors could have been introduced, nor are these introduced errors distinguished from known gaps in the grammar.

The tool used for grammar profiling and treebanking in the DELPH-IN community is a software package called [ INCR TSDB ()] (Oepen and Carroll 2000 ). It facilitates the construction of test suites designed to measure overgeneration and undergeneration in the grammar, as well as changes in the number of analyses produced per item. [ INCR TSDB ()] is able to be used in a comparative mode, comparing the coverage and performance of two different versions of a grammar. This makes it a valuable tool for gaining immediate feedback on the impact of a change just made to a grammar. However, it suffers from the previously-mentioned limitations of grammar profiling tools, i.e. that this information offers a coarse-grained level of feedback on the impact of the changes made.

A tool which can be used to shed more light on changes in terms of the underlying properties of the grammar, is O CEANOGRAPHY (Waterman 2009 ) from the XLE grammar development environment (Crouch et al. 2014 ). O CEANOGRAPHY works by processing parser output and collating statistics such as the counts of constructions and properties and the context in which they occurred. Dost and King ( 2009 ) describe making use of O CEANOGRAPHY to gauge the impact of modifications made to a grammar by comparing the relative frequencies of the modified constructions between the initial version of the grammar and the modified version. Unexpected changes in frequencies of constructions are deemed to be symptomatic of potential problems introduced by the change. This comparative use of O
CEANOGRAPHY between two different versions of a grammar comes the closest to meeting the desired objectives. 2.4 Filling the gap In order to try to fill this gap in the grammar engineering toolchain, we have developed G D ELTA , which, like the parse-result mining of O CEANOGRAPHY , involves processing output produced by the parser. Our approach is similar to the comparative use of O CEANOGRAPHY in Dost and King ( 2009 ), but improves upon this approach, firstly through the use of a single invocation and unified output, removing the need for manual comparison of two separate runs over two grammar versions, and secondly by performing analytics over the combined results in order to highlight significant changes between the parse results. 3 Unlike grammar error-mining techniques, G D ELTA does not assign judgements of suspicion to results, but instead attempts to provide a sufficient amount of diagnostics on the impact of the changes between the two grammar versions, from which the grammar engineer can draw their own conclusions.

Rather than trying to improve or replace any of the existing tools and approaches mentioned above, we see G D ELTA as being complementary, representing a new addition to the grammar engineer X  X  toolchain with a distinct use-case from the existing tools. More concretely, this use-case is to provide an intermediate level of feedback, allowing grammar engineers to identify unexpected and potentially detrimental consequences of their change before commencing manual inspection of changed test items. 3 Methodology For the development of G D ELTA , we used the Japanese grammar J ACY (Siegel 2000 ) and the LinGO English Resource Grammar (ERG: Flickinger ( 2002 )), both HPSG-based precision grammars developed within the DELPH-IN community. Instead of creating our own changes to these grammars from scratch, we used real changes made in the course of the development of these grammars. This ensured we would be working with sample changes of the kind that would occur during the grammar engineering process. For our test corpus, we selected test suites used in the development of the grammars. In the case of J ACY , this was the Tanaka Corpus (Tanaka 2001 ) and for the ERG, this was a selection of different corpora found in the LinGO Redwoods treebank (Oepen et al. 2002 ) as well as the WeScience treebank (Ytrest X l et al. 2009 ). 3.1 Preliminary steps For each change to a grammar being investigated, we began by taking two versions of the grammar: the initial version of the grammar ( G ) and the modified version ( G 0 ). These were then used to parse the same corpus using the PET run-time parser (Callmeier 2002 ). In order to keep the run-time of the parser within reasonable limits (with an eye to real-time feedback in live grammar engineering environ-ments), we restricted the analyses the parser returned for each item to the 10 best according to the parse selection model. Limiting the number of analyses to be found (while still being able to determine the total number of analyses in the parse forest) improves the parsing speed due to the selective unpacking (Zhang et al. 2007 ) used by PET.

For each grammar pair  X  G ; G 0  X  we then created three sets of parse results: (1) parse results for items which parse with G but not G 0 ( parse ! no parse ); (2) parse results for items which do not parse with G but do with G 0 ( no parse ! parse ); and (3) parse results for items that parse under both versions of the grammar but receive different numbers of analyses in their parse forests ( parse ! parse ). In the latter case, which contains analyses from both versions of the grammar, we subdivided this set into a further two: (1) analyses from G ( parse ! parse ); and (2) analyses from G 0 ( parse ! parse ). These four categories are intended to reveal different kinds of effects resulting from changes to the grammar and are the basis of much of
D ELTA  X  X  output. 3.2 Attribute extraction As was argued in Sect. 2 , in order to gain the kind of feedback the grammar engineer is after, it is necessary to leverage more information than just the parsability of an item. Just as O CEANOGRAPHY does, we look for further information in the output produced by the parser for successful parses. PET produces a number of different outputs for successful parses, each containing different kinds of structures built up during the parsing process. For our investigation we used the derivation trees generated by the parser, which represent the syntactic analysis of successfully parsed items.

An example derivation tree for an English phrase parsed with the ERG can be seen in Fig. 1 . In this example, just the labels of the nodes are shown, which are X  with the exception of the terminals and preterminals X  X he identifiers of the rules used to derive the subtree at each node. 4 In reality, each node stands for an elaborated attribute value matrix, containing all the constraints involved in building up the derivation for that subtree.

We extracted the rule identifiers from nodes of the derivation trees to be used as the attributes for use in G D ELTA . In the case of the leaves of the tree, extracting the surface form of the word would have led to data sparsity problems due to the infrequent occurrence of most words. This would also likely have occurred had we used the preterminal nodes, which correspond to the lexical entry for that word. Instead, we extracted the lexical type of each lexical entry, requiring a simple lexicon lookup. In accordance with the highly lexicalised nature of HPSG, it is a convention of DELPH-IN grammars that lexical entries inherit most of their constraints from a lexical type, and contain little information themselves. Figure 2 shows the lexical entry for the word dog in the ERG. It is composed of a lexical type, n_-c_le , which abstracts the constraints common to count nouns, as well as a small amount of information specific to the word dog . Figure 3 shows the derivation tree from Fig. 1 with lexical entries replaced by their lexical types.
In the context of HPSG, these two kinds of attributes (rule identifiers and lexical types) are associated with phrases and words X  X ubtypes of the sign type , which pairs constraints on both phonological form and syntactico-semantic function. The type hierarchy, which forms the backbone of DELPH-IN grammars, also includes categories of types that abstract functionality different to that of sign types. Limiting
D ELTA  X  X  input to the portion of the type hierarchy which can be readily extracted from the derivation trees has the advantage of making this approach more portable across other grammatical frameworks as well as being simpler to implement since consultation of the grammar itself is not required. In Sect. 7 we discuss potential improvements to G D ELTA that involve expanding the input to more of the type hierarchy as well as other data structures produced by the parser.

A further issue that arose in the attribute extraction process was how attributes are extracted from items that yield more than one analysis, which for non-trivial input is the norm rather than the exception. One possibility would be to extract the union of attributes across all of the recorded analyses. But since the probability assigned to some of the analyses by the parse selection model could be quite low, this may in fact result in a poorer reflection of how the parse results have been affected by the change. Since it is not clear at which point analyses should be excluded from contributing their attributes, we left this as a user-definable parameter to G D ELTA , with the default being to use only the single best analysis.
The attribute extraction step is the one component of the G D ELTA methodology which is DELPH-IN/HPSG specific. In order to apply a G D ELTA style approach to other grammatical frameworks, a procedure for extracting appropriate attributes from derivation trees X  X r indeed other data structures X  X ust be developed. The attribute ranking and clustering algorithms described in the remainder of this section are agnostic with respect to the underlying grammar framework, only requiring a list of attributes and their item counts across the parser output produced by the two versions of the grammar. 3.3 Attribute ranking Having extracted attributes from the derivation trees of parsed items, we then needed a means of highlighting attributes impacted by the changes made to the grammar. This was first attempted by ranking attributes by the relative change in the number of parsed items that contained the given attribute for each version of the grammar, sorted in either ascending or descending order, depending on whether decreases or increases were being investigated. A problem with this ranking was that attributes considered to be particularly relevant to the change were sometimes only found in a small number of items, meaning they were obscured by changes in more commonly-occurring attributes. When developing the item clustering approach outlined in Sect. 3.4 , we also noticed that using the change in item frequency for the weighting of attributes biased the results towards lower numbers of clusters. We combated these two problems by using an alternative attribute weighting function based on the difference in the inverse document frequency (IDF) of attributes. Since the IDF of attributes will be low for those that occur frequently in a set of parse results and high for those that occur infrequently, the difference in IDF is effectively the change in rarity of attributes between parse results.
The IDF of attribute i in grammar G is given by Eq. 1 . P G is the number of parse outputs for grammar G and p i ; G is the number of parse outputs that contain attribute i ; the denominator is offset by one to avoid zero-division errors when the attribute does not occur in any of the parsed items. This weighting function for an attribute i is given by Eq. 2 , where G is the initial version of the grammar and G 0 is the new version of the grammar.

An issue we discovered with this weighting function is that when the frequency of an attribute is not affected by a change, the attribute can receive a non-zero weighting due to the fact that the total number of items parsing will likely change between grammars, yielding a change in IDF. This would be confusing for the user, suggesting an impact upon some attributes where none exists. This was solved by modifying the IDF calculation to use the total number of items parsing across both versions of the grammar, which has the effect of assigning unchanged attributes a weight of zero, while preserving the relative weighting of other attributes. The modified IDF formula is given in Eq. 3 .

Armed with the item frequency and IDF-based weighting functions, rankings could then be created across the sets of attributes extracted from the four different categories of parse change. Tables 1 and 2 show attribute rankings for the categories of no parse ! parse and parse ! no parse respectively, comparing the top 10 changes across both weighting functions. These rankings were generated from a change made to J ACY during the course of its development which involved extensive changes to the handling of verb phrases. The rankings in Table 1 are sorted in descending order, while those in Table 2 are sorted in ascending order, reflecting the direction of change that is of interest for each category. It should be noted that while the set of attributes for each parse change category only includes attributes found in items from that category, their changes in item frequency and IDF are calculated globally across all items, meaning that it is possible to see, for instance, attributes in the no parse ! parse category with decreases in item frequency and IDF weightings.

Looking at Table 1 , which shows attributes extracted from items which now parse as a result of the changes made to the grammar, we see in Table 1 a that ranking by item frequency places the attribute vn-light-rule at the top of the ranking, being found in 1566 more parsing items than before the change, followed by vstem-vend-rule , with a change of 1363 parsing items. Both these attributes were directly altered by the grammar engineer, with the increases in other attributes being due to interactions with changes made to the grammar. Looking at the ranking generated using the IDF weighting in Table 1 b, we see that using a weighting function based on the change in rarity of attributes has moved vn-light-rule down into the seventh position, and vstem-vend-rule has completely dropped out of the top 10.

Comparing the rankings produced using the two different weightings, we see that attributes occurring higher in the item frequency-based ranking tend to be pushed down in the IDF-based ranking, replaced by changes in rarer attributes. One of the effects of this emphasis on subtler, more nuanced changes found in the IDF ranking is that of pushing lexical types, whose distribution exhibits more of a long tail, higher up the ranking. We see these two different types of ranking as being complementary, with the item frequency weighting allowing the grammar engineer to build up a broad picture of how the attributes were affected, and with IDF-based weighting highlighting potentially interesting attributes from the long tail of item frequency changes. Within the final G D ELTA interface, these two signals can be combined, by X  X or instance X  X sing item frequency as the primary sort key and the IDF weighting as a secondary sort key.

During the development of G D ELTA , we noticed that there were occasions where it was useful to collapse increases and decreases in attributes together. When viewing attribute rankings for the category no parse ! parse , positive changes are of particular interest, and conversely, negative changes are more interesting in the case of the parse ! no parse category. But in the case of the remaining two parse change categories, which contain items with changes in the number of parses they received, both positive and negative changes are potentially relevant. Rather than introduce an additional signal via another weighting function, we opted to modify the existing IDF weighting function to be the absolute magnitude of the change. This version of the IDF weighting function (as given in Eq. 4 ) is the one used in G D ELTA 5 and referred to throughout the remainder of this paper.
 3.4 Item clustering Information on differences in the correlation between parsability and individual types is a valuable aid in gisting how the grammar has changed. However, the interaction between attributes impacted by the grammar change is often subtle, and only truly understood relative to different combinations of attributes and the corresponding items. As a second diagnostic facet of G D ELTA , we therefore cluster items based on parsability data and the attributes present in the associated derivation tree, and present the items associated with each cluster. As with the attribute rankings, the clustering is performed separately across the different categories of parse change.

For the underlying clustering algorithm, we chose k -means clustering for its simplicity and run-time efficiency. k -means clustering is an iterative algorithm which partitions a set of observations into k clusters. In each iteration, every observation is assigned to the cluster with the closest mean, and the mean of each cluster is subsequently updated. This process of assign-and-update is repeated until the clusters converge and the assignment of observations no longer changes. We selected Euclidean distance as the metric for calculating distances between points. To generate the item clusters, each item is first converted into a weighted attribute vector in n -dimensional space, where n is the total number of attributes located across all parse results from both versions of the grammar.

As previously mentioned, we found that using the change in item frequency to weight the attributes had the effect of strongly biasing the results towards lower numbers of clusters. We suspect this was caused by large changes to frequently occurring attributes dominating other changes, resulting in item vectors lying close to each other in Euclidean space and thus not being readily separable. The IDF-based weighting function solves this problem by picking up on more nuanced changes to attributes. Item vectors were then weighted by assigning, for each attribute, the value returned by the function given in Eq. 4 if the attribute occurred in the derivation tree for the item, and zero if it did not.

A decision to be made when using k -means clustering is how the centroids of the clusters are initialised. Since k -means is an expectation-maximisation algorithm, there is no guarantee that the final solution will be the global optimum. If a poor selection of initial seeds is made then the solution may be a local optimum a long way from the global optimum. We tried to reduce the likelihood of this by selecting centroids with a good spread across the entire set of observations. This was done by pseudo-randomly selecting a point to be the centroid of the first cluster, then selecting each other initial centroid to be the furthest point from the mean of the seeds already selected, until k points are selected.

One of the drawbacks of k -means is that the value of k must be specified in advance. Since the grammar engineer does not know how many types of changes there may be in the parse results, the best value of k is unknown. This problem is commonly solved by repeatedly performing the clustering across a range of values of k and selecting the value which yields the best clustering. One way of doing this is by selecting k such that it minimises the combined sum of the squared errors across each cluster, since the goal is to minimise the distance from each observation to its centroid. This approach has the drawback of favouring higher numbers of clusters and requires the introduction of a penalty for each increment of k to offset this bias.

We chose to compare the quality of the clustering for different values of k using the Silhouette algorithm (Rousseeuw 1987 ), which provides a means of evaluating the fit of an observation within a set of clustered observations. This technique involves the calculation of a single score for each observation, which is a combined representation of the internal similarity with other observations in the same cluster and of the dissimilarity with all other observations outside the cluster. The formula for calculating the Silhouette score of an observation i is given in Eq. 5 . a  X  i  X  is defined as the mean distance from i to all other observations in the same cluster and b  X  i  X  is defined as the mean distance from i to all observations in the cluster closest to the cluster i occurs in. This produces Silhouette scores ranging from 1 to 1, with 1 indicating poor clustering and 1 indicating good clustering. The average Silhouette score for a whole cluster can then be computed and in turn, the average across all clusters, resulting in a final value representing the quality of the overall clustering. The final clustering is done by performing k -means for values of k between 2 and 6, and selecting the value which resulted in the highest Silhouette score. A maximum of 6 was selected after repeated observations across different grammars and grammar changes showed that the number of clusters selected rarely exceeded 4. The result is a set of clusters for each of the parse change categories, with each cluster intended to capture a group of related effects on the parse results. In order to characterise the nature of these effects, the closest item to the centroid of each cluster is taken as the exemplar and displayed with the cluster. In order to make more transparent the changes in attributes that the clustering has picked up on, we include in the clustering visualisations the top-ranked attributes of the exemplar example of the clustering output is provided in Sect. 4.3 , along with an explanation of the additional signals provided in the clustering visualisations. 4 The output produced by G D ELTA consists of a series of HTML pages for viewing within a web browser. All data required for displaying the output is contained within the HTML files, so it can be viewed offline and independently of the grammar versions being compared. The output includes five different pages, each intended to provide a different facet of feedback. This section contains an overview of these pages. The sample output was generated from an attempted change to the ERG intended to expand the coverage of extracted complements with a right-node-raising analysis. This change was also used as a part of the evaluation of G D ELTA outlined in Sect. 5 and is further described there. 4.1 Summary page The Summary page is intended to be the first port of call for the grammar engineer after running G D ELTA . It provides a high level overview of the impacts that the grammar changes had over the profiles. The first half of the page is shown in Fig. 4 and the second half is shown in Fig. 5 . The first half provides an overview of various grammar profiling diagnostics, a breakdown of the number of items found in each category of parse change X  X s well as the number of items from these categories that could be used for extracting attributes X  X nd a summary of the attributes with the top increases and decreases in item frequency.

The second half of the Summary page contains separate tables of attributes found in each category of parse change. Each table has columns containing the attribute X  X  change in IDF (referred to as weight ), its change in item frequency (referred to as change ), and also the number of items from this category that the attribute occurred in. This third value is provided in order to determine the relevance of an attribute to the category, since the changes in item frequency and IDF are calculated globally across all items. Each column can be interactively sorted on, allowing the grammar engineer to explore different aspects of how the parse results were affected. Clicking on an attribute in these tables navigates to that attribute X  X  position in the Attributes page described in the next section.

The parse ! no parse ranking in Fig. 5 illustrates the complementary nature of the different signals presented by the interface. When the Change column is sorted, the hd_xcmp_nom_c attribute rises to the top. However, the relatively high change in IDF of the hd-hd_rnr-nv_c attribute, which is involved in the right node raising analysis, correctly suggests that this attribute could be of more interest. 4.2 Attributes page The Attributes page aims to provide more comprehensive information pertaining to how the occurrence of attributes in the parse results was affected by the changes made to the grammar. It contains a single table listing every attribute extracted by
D ELTA from both versions of the grammar. The different columns of the table contain the following information for each attribute: the size of the change in the item frequency, the signed change in the item frequency, the IDF-based weighting, the number of items under the previous grammar containing the attribute, the number of items under the new grammar containing the attribute and lastly, a column indicating whether the attribute was present in the source files of only one or both versions of the grammar. This last column is intended to provide an indication of when an attribute has been added to or removed from the grammar, making the cause of an attribute X  X  frequency changing to or from zero clearer.

All of the columns can be used as sort keys (secondary and tertiary sort keys etc. are also supported), allowing the grammar engineer to refine the feedback they receive towards a specific aspect of change that is of interest to them. There is also a text input box which can be used to filter on the name of attributes. This is useful for exploiting type-naming conventions that are commonly employed in DELPH-IN grammars. For instance, in the ERG, filtering on the substring _le has the effect of restricting results to lexical entries. Clicking on an attribute name in the table navigates to the Items page showing only those items which contain the attribute in the parse results. 4.3 Clusters page The Clusters page provides a visualisation of the clusters produced by G D ELTA for each of the different categories of parse change. The clusters are intended to represent different patterns of effects found in the changes between parse results. Each cluster is initially presented with only the text from the exemplar item of the cluster and the cluster X  X  Silhouette score, which gives an indication of the fit of the items in the cluster, as described in Sect. 3.4 . More details are available for each cluster by clicking on the Details button. This exposes a table containing the top 5 attributes found in the exemplar item as well as a table containing all items in the cluster. Figure 6 shows the two clusters for the parse change category parse ! no parse with the first cluster expanded. For the exemplar attributes, in addition to presenting the attribute weighting, the cohesion and overlap of each attribute is also given. The cohesion of an attribute is the percentage of items occurring in the cluster that contain the attribute, and the overlap is the percentage of items occurring in all other clusters that contain the attribute. An attribute is more strongly correlated with a cluster when it has high cohesion and low overlap. These values provide a means for the grammar engineer to assess the relevance of the attribute to the cluster. An additional feature of this expanded cluster view is that hovering over an attribute from the exemplar item causes all items in the cluster that contain the attribute to be highlighted.
 4.4 Items and errors pages The Items page and Errors page contain additional information not directly related to the attribute weighting and item clustering signals, that augments the feedback provided by the previous three pages. The Items page contains a table of items from all profiles along with information pertaining to each item, such as the input text of the item, the length of the string, the profile the item originated from, an indication of any changes to the parsability of the item, and the number of readings yielded by the two versions of the grammar. As well as this table being sortable, there is also a series of text boxes supporting filtering on fields, as well as on attributes occurring in each item. All of this allows the grammar engineer to drill down quickly and identify individual items with specific properties of interest. For instance, it is trivial to filter on only those items that no longer parse after the change, and that also contain a specific attribute or set of attributes.

The Errors page is similar in structure to the Items page but lists only those items which G D ELTA encountered errors with. These mostly tend to be parse failures, such as timeouts and system memory limits being reached, but can also arise due to
D ELTA failing to extract attributes from the derivation trees of items which were successfully parsed. 5 Evaluation The evaluation of the techniques used in the creation of G D ELTA is not straightforward due to the fact that they do not attempt to produce any kind of measurable judgement of correctness. The output is instead diagnostic in nature, requiring interpreting by the grammar engineer X  X n inherently subjective process. Ultimately the best indicator of the success of G D ELTA is whether grammar engineers are able to use the tool to more rapidly gain a sense of how the grammar has been impacted by a change. While we have yet to systematically road-test G D ELTA in the context of active development of a grammar, we have performed some qualitative evaluation of the underlying techniques. This section outlines this evaluation.

This evaluation was an attempt to gauge the utility of G D ELTA style feedback by simulating contexts from the grammar development process where G D ELTA is intended to be used. We performed this evaluation using modifications to the ERG which were known to have resulted in problems that were non-trivial to isolate. Participants were charged with the role of grammar engineer and were thus required to have some experience working with the ERG. This restriction unfortunately meant that we only had three participants in the evaluation. This small group of ersatz grammar engineers were asked to hypothesise about the nature of the problem resulting from each change, based upon the attribute ranking and item clustering output from G D ELTA , along with a short description of what the change was intended to do. Participants were also asked to describe the reasoning process they used to come to these conclusions so as to provide us with insight into how G D ELTA  X  X  output was being used.
We used three separate changes to the ERG made during its development which were ultimately not incorporated into the grammar. Each change involved modifications to the grammar that resulted in problems not apparent at the grammar profiling stage, requiring the analysis of treebanked items before being identified. For each change, the profiles that the two versions of the grammar were run over were taken from the WeScience treebank. The different grammar breakages used were as follows:
Change one: an attempt to reduce the number of analyses being generated for the noun phrase three thirty . The existing problem was that one of the analyses produced by the grammar resulted in such phrases being incorrectly analysed as a determiner followed by a noun. A fix for this was attempted by the modification of the type reg_or_temp_nom_rel which then resulted in parse failures for valid noun phrases containing numeric determiners such as two days .

Change two: an attempt to improve the generality of preposition phrase modification of nominals by changing the type n_wh_pro_lexent . This failed to parse prepositional phrases modifying wh -pronouns such as in Who in this town won?
Change three: an attempt to expand the coverage of extracted complements by modifying the type extracted_comp_phrase to implement a right-node-raising analysis for examples like We relied on and hired consultants . The change introduced a regression for items that contained coordinated phrases headed by prepositions, such as We wrote books for and with children .

Given that the aim is to identify the problematic effects of these changes, the participants would ideally be able to identify parts of the grammar where the problem manifested itself, rather than just the modified components of the grammar, which the grammar engineer would already be aware of. It should also be noted that these three changes, which all involve minor changes to a single type, represent relatively simple grammar modifications. When changing an existing analysis (or extending the grammar to cover a new one) potentially complex changes could be made to multiple locations in the grammar. Changes of this kind would offer a more compelling context for evaluation, as the potential for unexpected flow-on effects that need untangling would be much greater, and would more closely match the intended use case of G D ELTA . In future work, we hope to perform a more thorough evaluation of G D ELTA in the context of active grammar development.

The results of the grammar breakage experiments were encouraging. Participants were all able to at least identify the general nature of the problem introduced, if not the specific location in the grammar. All three participants, for instance, were able to identify that the problem in the third change involved numeric determiners, with both the attribute ranking and clustering visualisations clearly indicating that the num_det_c rule was likely to be associated with the regression.

The descriptions of the reasoning processes used by participants were also informative. One finding was that the clustering visualisations were considered to be quite useful. In particular, the top-ranking attributes of the exemplar item for each cluster were cited as being a useful diagnostic aid, along with their respective cohesion and overlap values. The attributes were used to determine the nature of the change being represented by the cluster, while the cohesion and overlap of the attributes were used to determine whether this was actually an informative cluster or not. The text of the exemplar item was useful in determining the function of the attributes found in that item. 6 Obtaining and using G D ELTA D ELTA was implemented as a Python program that is run from the command line. Running G D ELTA requires Python 2.7, and installation of the SciPy module is recommended as this will yield faster clustering. Viewing G D ELTA  X  X  output requires a standards-compliant web browser. G D ELTA is freely available for use and can be found on the DELPH-IN wiki. 6
Using G D ELTA involves the user specifying the two different versions of the grammar and their respective output profiles which are to be compared. The user can specify a range of command line flags to run G D ELTA with different parameters, such as the number of top parse results from which to extract attributes, and the maximum number of clusters to be tried in the clustering stage. Once run, G D ELTA produces a directory of HTML files containing the different views outlined in Sect. 4 . This directory contains everything required to view the HTML files, meaning that G D ELTA output can be browsed offline and does not require access to the different grammar versions or the parse output. Further instructions for installation are provided with G D ELTA  X  X  source code. 7 Future work The most important avenue of work we hope to pursue is substantial in situ road-testing of G D ELTA in the context of active development of a grammar. This is needed in order to more concretely assess the purported improvements to the grammar engineering toolchain we have argued G D ELTA offers. It would also provide an opportunity for identifying improvements to the presentation of G D ELTA  X  X  output, as well as additional data points that could be incorporated to further improve the utility of the tool.

One issue in particular that would benefit from further systematic investigation is the relationship between the utility of the signal provided to grammar engineers, the size of the test suites and the range of domains represented by the test suites, and the kind of modification made to the grammar. It is certainly the case that G D ELTA will be of limited value when used to investigate changes pertaining to linguistic phenomena that are underrepresented in the test suites used as input. Blindly scaling desirable solution, as increases to parsing time will add latency to the feedback, with the possibility for negating any efficiency improvements. It would be advantageous then to have more of a sense of the scale of input data required for G D ELTA to provide helpful results.

Further work on the underlying techniques that drive G D ELTA could involve looking at the impact on other data structures produced by the parser. G D ELTA currently only makes use of syntactic output generated for successfully parsed items. Incorporating the semantic analysis returned by the parser would likely be beneficial as this is an important signal which must also be monitored for quality assurance, and it can be the case that changes to a grammar only manifest as effects on the semantic analysis. The techniques outlined in this article could also be adapted to use the data structures found in the parse chart, opening up the possibility for partially parsed items X  X hose not receiving a full spanning parse X  X o be used.
In the context of DELPH-IN grammars in particular, a specific kind of flow-on effect that can manifest itself in the diagnostic information that G D ELTA provides is changes in attributes whose corresponding grammar types were not edited by the grammar engineer but inherited from supertypes which were modified. These kinds of interactions are distinct in that they can be readily identified by consulting the type hierarchy of the grammar. Extending G D ELTA to extract inheritance relation-ships between edited types and their descendants would provide an additional signal in the visualisations, allowing the grammar engineer to identify the origin of these particular attribute changes. This signal would also be useful in the context of the item clustering, as it would allow for the identification of clusters corresponding to this kind of flow-on effect. 8 Conclusion In this article we presented G D ELTA , a tool for providing grammar engineers with improved feedback on the effects of changes made to precision grammars. Existing tools that assist with the grammar engineering process tend to provide either immediate but very high-level feedback, or very low-level incremental feedback.
D ELTA aims to fill this gap between these extremes in the grammar engineering toolchain, providing a more nuanced picture of the impact of the change earlier on in the grammar engineering cycle. It is also the case that existing tools do not explicitly focus on changes made to a grammar, instead requiring the grammar engineer to isolate effects caused by the changes manually. G D ELTA , however, provides feedback explicitly on changes between two versions of the grammar. We also outlined two techniques used to drive the visualisations G D ELTA presents to the user: an attribute weighting algorithm used to highlight grammar attributes from the parser output which were significantly impacted by the change, and a test suite item clustering technique intended to identify related groups of changes across items whose parsability was affected by a change to the grammar.

The qualitative evaluation outlined in Sect. 5 supports our belief that G D ELTA has the potential to improve the efficiency of the grammar engineering process. During the course of the evaluation we also received positive comments from grammar engineers who helped us with this investigation, indicating that they would be interested in using G D ELTA .
 References
 G Abstract The development of precision grammars is an inherently resource-intensive process; their complexity means that changes made to one area of a grammar often introduce unexpected flow-on effects elsewhere in the grammar which may only be discovered after some time has been invested in updating numerous test suite items. In this paper, we present the browser-based G D ELTA tool, which aims to provide grammar engineers with more immediate feedback on the impact of changes made to a grammar by comparing parser output from two dif-ferent grammar versions. We describe an attribute weighting algorithm for highlighting components of the grammar that have been strongly impacted by a modification to the grammar, as well as a technique for clustering test suite items whose parsability has changed, in order to locate related groups of effects. These two techniques are used to present the grammar engineer with different views on the grammar to inform them of different aspects of change in a data-driven manner. Keywords Precision grammar Grammar engineering Grammar diagnostics Deep parsing Parse mining 1 Introduction In this paper we investigate techniques for generating feedback on the impact of changes made to precision grammars, and present the G D ELTA tool, which is intended to combat a significant drawback of linguistically accurate hand-crafted grammars X  X he consider-able amount of time and resources involved in their development. G D ELTA is intended for use during the grammar development cycle, to be presented to grammar engineers as immediate feedback on the impact of a change or set of changes made to a grammar. This feedback is intended to assist grammar engineers in more readily understanding how the grammar has been affected, thus reducing the amount of time required to track down issues introduced by changes to the grammar.

Previous work that aims to provide feedback for use in the grammar engineering process (Oepen and Carroll 2000 ; van Noord 2004 ; Waterman 2009 )hasfocusedonthe analysis of parser output from just one version of a grammar. Our approach differs in that it highlights the differences between two versions of a grammar: one from before and one from after a change made to a grammar. This work can be conceptualised as the development of a diff tool that operates over parser output from two different versions of a grammar run across the same corpus. Rather than trying to improve upon or replace existing development tools, we see this tool as being complementary, filling a gap in the grammar engineering toolchain. G D ELTA was developed specifically for use with grammars developed within the DELPH-IN community, 1 however the underlying techniques that drive G D ELTA and are described in this article are largely framework agnostic, and could be readily applied to other grammar engineering frameworks. Additionally, these techniques work cross-linguistically, as demonstrated by the application of G D ELTA to grammars of English and Japanese.

The remainder of this paper is structured as follows. Section 2 briefly expands upon the concept of a precision grammar and outlines the grammar engineering process, before presenting an overview of existing tools available to grammar engineers. Section 3 outlines our methodology, including the resources we used and the underlying techniques that G D ELTA makes use of. Section 4 provides a walkthrough of the output that G D ELTA generates and outlines how it is intended to be used. Section 5 outlines a small-scale evaluation of the techniques introduced in this paper and contains a discussion of the results. Section 6 provides a brief overview of the implementation details of G D ELTA , along with the requirements for running G D ELTA and how to access the source code. Section 7 identifies further work to be pursued in his area, and Sect. 8 concludes the paper. 2 Background 2.1 Grammar engineering Precision grammars are motivated by the desire for automated and accurate linguistic analysis of natural language. They are usually based upon or heavily influenced by formal theories of syntax developed within the field of linguistics. The syntactic theory that informs the DELPH-IN grammars used in this paper is Head-driven Phrase Structure Grammar (HPSG: Pollard and Sag 1994 ). Examples of other grammar engineering approaches include the CoreGram project (Mu  X  ller 2013 ), also based on HPSG; the ParGram project (Butt et al. 1999 , 2002 ) based on Lexical Functional Grammar (Dalrymple 2001 ); the E X TENSIBLE M ETA G RAMMAR tool (Crabbe  X  et al. 2013 ), which can be used to create tree-based grammars such as Tree Adjoining Grammar (Joshi and Schabes 1997 ) and Interaction Grammar (Guillaume and Perrier 2009 ); and also D OT CCG (Baldridge et al. 2007 ), a tool used to develop Combinatory Categorial Grammars (Steedman 2000 ).

These foundations in formal syntax mean that precision grammars all share the property that they draw a sharp distinction between sentences that are grammatical and those which are not. Compared with induced grammars X  X s are used in most treebank parsers X  X recision grammars tend to generate far more detailed analyses and are able to capture much more complex linguistic constructions (Bender et al. 2011 ). Precision grammars are also usually able to capture the underlying semantic structure of language. It is for these reasons that the use of precision grammars to perform natural language parsing is often referred to as deep parsing .

Grammar engineering is the process of developing and extending the coverage of precision grammars as well as improving the quality of existing analyses. It is a resource-intensive process, requiring grammar engineers with both a linguistic background as well as an understanding of the formalism in which the grammar has been implemented. A number of factors combine to make grammar engineering a slow and time-consuming process. One is the considerable size and complexity of these grammars, with the various components involving a high degree of interaction between each other, meaning that modifications made to one part of the grammar can often produce unexpected flow-on effects in other areas of the grammar. Another factor is the high degree of accuracy that is demanded of these grammars. Over time, as improvements are made and the grammar is expanded to handle more linguistic constructions, it is important that the validity of analyses produced by the grammar does not degrade, and that the grammar does not stop being able to parse constructions that it once could.

Just as in software engineering, the grammar engineer has a number of tools available to provide quality assurance. In grammar engineering, the practice of monitoring the quality of analyses produced by the grammar is referred to as grammar profiling . This involves the maintenance of test suites which facilitate the monitoring of various signals, providing a means of ensuring that the grammar does not regress. In the DELPH-IN community, test suites are maintained through the use of profiles , which contain sets of items and information pertaining to how the grammar performed over the items. An item consists of a string from the target language, which, depending on the purpose of the test suite, can be naturally occurring or constructed to illustrate a particular phenomenon, and can also be marked as grammatical or ungrammatical. One of the aims of the TSNLP project (Oepen et al. 1997 ), which the DELPH-IN profiling technology originates from, was to additionally annotate items with grammatical phenomena they exemplified as a means of further evaluating and debugging the performance of grammars. The complexities involved in creating and maintaining such phenomenon annotations have, however, meant that this practice has not been adopted.

The most prominent aspect of the grammar that must be monitored is the range of linguistic phenomena and lexical items that it is able to handle X  X he coverage of the grammar. This is monitored by test suites constructed to track the extent to which a grammar undergenerates . Test suites can also contain ungrammatical or negative items used to test the degree to with a grammar overgenerates . In addition to these signals, the number of analyses per item is also monitored. While it is true that ambiguity is an inherent feature of language that cannot be eliminated, it is important to remove spurious ambiguity where possible. Grammar profiling tools also provide information regarding changes in the operational performance of the grammar, such as the time taken to process test suites and the number of operations required for parsing. Just as with testing in traditional software development, unexpected changes in any of these aforementioned statistics are potential symptoms of introduced errors. For further discussion of the requirements of grammar profiling tools, see Oepen and Flickinger ( 1998 ).

A limitation of grammar profiling tools is that they are not able to detect the misanalysis of items, i.e. when the analysis of an item does not match its structure. This is of particular concern when developing precision grammars as their architecture tends to be complex, reflecting the inherent complexity of language, and just as linguistic phenomena interact with each other, so too do the implementations of their analyses. This means that a modification to a grammar pertaining to a particular phenomenon can often result in unanticipated flow-on effects in other areas of the grammar. 2
A resource that can be used to monitor the accuracy of analyses is the treebank ,a corpus annotated by selecting a good analysis from the alternatives produced by the parser. Whenever a change is made to the grammar that alters the stored analysis of a treebanked item, the analysis provided by the previous version of the grammar can be used to verify that the change has not led to a degradation in quality. When using treebanks for this purpose, a necessary step in the grammar engineering cycle is the updating of items affected by the change to the grammar. This is performed to catch errors resulting from the change, and also to periodically synchronise the treebank to the current state of the grammar. In the DELPH-IN grammar engineering pipeline, this dynamic treebanking methodology (Oepen et al. 2002 ) takes advantage of previously recorded annotations by automatically applying (where possible) the outcomes of decisions regarding the resolution of local ambiguity. This greatly reduces the number of decisions an annotator must make following a change, however it remains the case that following a significant change to a grammar, the treebanking stage can be a potentially labour intensive process. 2.2 A gap in the toolchain Grammar profiling and treebanking are both vital to the grammar engineering process, each representing opposite extremes in the level of feedback they provide: grammar profiling providing immediate but course-grained feedback on the coverage and performance of the grammar, and treebanking providing slower, fine-grained feedback on an item-by-item basis. The grammar profiling stage can locate glaring problems such as coverage levels suddenly dropping or not changing when expected to, but the detailed level of feedback provided by treebanks is crucial to catching unanticipated problems not visible in the grammar profiling stage, as well as more generally building up a picture of how the change impacted the grammar. Unfortunately, the inspection and updating of treebanks can be a slow process, and an all too real possibility for the grammar engineer is locating a serious problem after considerable time has been invested in updating the treebank. When this occurs, the grammar engineer must resolve the problem in the grammar and start the process over again.

We argue that it would be desirable for there to be a tool which offers a middle ground between these two levels of feedback, one which makes the impact of the change more transparent earlier on in the grammar engineering cycle, helping the grammar engineer more quickly gain a superficial understanding of the impact of the change over the test suites. In the next section, we outline a range of existing tools and approaches that grammar engineers could potentially use to gauge the impact of a change made to a grammar. Each approach is assessed on the basis of how well it can be used to provide the desired kind of feedback. 2.3 Previous work van Noord ( 2004 ) proposed a grammar error mining technique for detecting errors in grammars. This involves assigning a parsability score to each word n -gram in a corpus, based on the ratio of the number of times each n -gram occurs in successfully parsed items over the number of items it occurs in the whole corpus. The end result is a list of n -grams, ordered by parsability, with the lowest scoring n -grams representing those most likely to be indicative of problems in the grammar. This technique has been further improved by Sagot and de La Clergerie ( 2006 ) and de Kok et al. ( 2009 ). An advantage of this error mining approach is that the only input required is a binary success or failure value for each item, meaning that it is agnostic towards the implementation of the parser and grammar. A drawback to this approach, however, is that the results are presented in terms of surface string n -grams, whereas information pertaining to the internal components of the grammar would be more informative to the grammar engineer. This is indicative of the larger problem, which is that without an analysis from the grammar, the error mining input is limited to unstructured text.

Two error mining approaches which do utilise structured input to yield further insight are that of Goodman and Bond ( 2009 ) and Gardent and Narayan ( 2012 ). Goodman and Bond ( 2009 ) introduce E GAD , a tool which leverages the ability of precision grammars to both generate and parse in order to identify errors involved in generation. For each successful parse, they use the corresponding semantic form to generate the set of possible paraphrases, flagging a sentence as a failure if the original surface form cannot be generated. They then use a maximum entropy classifier to find n -grams over paths in the derivation tree which are predictive of generation failure, resulting in a better picture of where the underlying error in the grammar might lie. Similarly, Gardent and Narayan ( 2012 ) extend the error mining approach of de Kok et al. ( 2009 ) to develop a suspicion measure for dependency subtrees, applying this to input trees for which they are unable to generate a surface form.

These automated approaches towards error detection in grammars are useful for directing the development of a grammar X  X hey are good at locating gaps in parsing and generation coverage X  X owever they are ill-suited to providing feedback on immediate changes made to a grammar, as there is no guarantee that the problems they report will be related to changes just made to the grammar. This is due to the use of a single version of the grammar in the input: without comparison to a previous version, there is no bound on how far back the errors could have been introduced, nor are these introduced errors distinguished from known gaps in the grammar.

The tool used for grammar profiling and treebanking in the DELPH-IN community is a software package called [ INCR TSDB ()] (Oepen and Carroll 2000 ). It facilitates the construction of test suites designed to measure overgeneration and undergeneration in the grammar, as well as changes in the number of analyses produced per item. [ INCR TSDB ()] is able to be used in a comparative mode, comparing the coverage and performance of two different versions of a grammar. This makes it a valuable tool for gaining immediate feedback on the impact of a change just made to a grammar. However, it suffers from the previously-mentioned limitations of grammar profiling tools, i.e. that this information offers a coarse-grained level of feedback on the impact of the changes made.

A tool which can be used to shed more light on changes in terms of the underlying properties of the grammar, is O CEANOGRAPHY (Waterman 2009 ) from the XLE grammar development environment (Crouch et al. 2014 ). O CEANOGRAPHY works by processing parser output and collating statistics such as the counts of constructions and properties and the context in which they occurred. Dost and King ( 2009 ) describe making use of O CEANOGRAPHY to gauge the impact of modifications made to a grammar by comparing the relative frequencies of the modified constructions between the initial version of the grammar and the modified version. Unexpected changes in frequencies of constructions are deemed to be symptomatic of potential problems introduced by the change. This comparative use of O
CEANOGRAPHY between two different versions of a grammar comes the closest to meeting the desired objectives. 2.4 Filling the gap In order to try to fill this gap in the grammar engineering toolchain, we have developed G D ELTA , which, like the parse-result mining of O CEANOGRAPHY , involves processing output produced by the parser. Our approach is similar to the comparative use of O CEANOGRAPHY in Dost and King ( 2009 ), but improves upon this approach, firstly through the use of a single invocation and unified output, removing the need for manual comparison of two separate runs over two grammar versions, and secondly by performing analytics over the combined results in order to highlight significant changes between the parse results. 3 Unlike grammar error-mining techniques, G D ELTA does not assign judgements of suspicion to results, but instead attempts to provide a sufficient amount of diagnostics on the impact of the changes between the two grammar versions, from which the grammar engineer can draw their own conclusions.

Rather than trying to improve or replace any of the existing tools and approaches mentioned above, we see G D ELTA as being complementary, representing a new addition to the grammar engineer X  X  toolchain with a distinct use-case from the existing tools. More concretely, this use-case is to provide an intermediate level of feedback, allowing grammar engineers to identify unexpected and potentially detrimental consequences of their change before commencing manual inspection of changed test items. 3 Methodology For the development of G D ELTA , we used the Japanese grammar J ACY (Siegel 2000 ) and the LinGO English Resource Grammar (ERG: Flickinger ( 2002 )), both HPSG-based precision grammars developed within the DELPH-IN community. Instead of creating our own changes to these grammars from scratch, we used real changes made in the course of the development of these grammars. This ensured we would be working with sample changes of the kind that would occur during the grammar engineering process. For our test corpus, we selected test suites used in the development of the grammars. In the case of J ACY , this was the Tanaka Corpus (Tanaka 2001 ) and for the ERG, this was a selection of different corpora found in the LinGO Redwoods treebank (Oepen et al. 2002 ) as well as the WeScience treebank (Ytrest X l et al. 2009 ). 3.1 Preliminary steps For each change to a grammar being investigated, we began by taking two versions of the grammar: the initial version of the grammar ( G ) and the modified version ( G 0 ). These were then used to parse the same corpus using the PET run-time parser (Callmeier 2002 ). In order to keep the run-time of the parser within reasonable limits (with an eye to real-time feedback in live grammar engineering environ-ments), we restricted the analyses the parser returned for each item to the 10 best according to the parse selection model. Limiting the number of analyses to be found (while still being able to determine the total number of analyses in the parse forest) improves the parsing speed due to the selective unpacking (Zhang et al. 2007 ) used by PET.

For each grammar pair  X  G ; G 0  X  we then created three sets of parse results: (1) parse results for items which parse with G but not G 0 ( parse ! no parse ); (2) parse results for items which do not parse with G but do with G 0 ( no parse ! parse ); and (3) parse results for items that parse under both versions of the grammar but receive different numbers of analyses in their parse forests ( parse ! parse ). In the latter case, which contains analyses from both versions of the grammar, we subdivided this set into a further two: (1) analyses from G ( parse ! parse ); and (2) analyses from G 0 ( parse ! parse ). These four categories are intended to reveal different kinds of effects resulting from changes to the grammar and are the basis of much of
D ELTA  X  X  output. 3.2 Attribute extraction As was argued in Sect. 2 , in order to gain the kind of feedback the grammar engineer is after, it is necessary to leverage more information than just the parsability of an item. Just as O CEANOGRAPHY does, we look for further information in the output produced by the parser for successful parses. PET produces a number of different outputs for successful parses, each containing different kinds of structures built up during the parsing process. For our investigation we used the derivation trees generated by the parser, which represent the syntactic analysis of successfully parsed items.

An example derivation tree for an English phrase parsed with the ERG can be seen in Fig. 1 . In this example, just the labels of the nodes are shown, which are X  with the exception of the terminals and preterminals X  X he identifiers of the rules used to derive the subtree at each node. 4 In reality, each node stands for an elaborated attribute value matrix, containing all the constraints involved in building up the derivation for that subtree.

We extracted the rule identifiers from nodes of the derivation trees to be used as the attributes for use in G D ELTA . In the case of the leaves of the tree, extracting the surface form of the word would have led to data sparsity problems due to the infrequent occurrence of most words. This would also likely have occurred had we used the preterminal nodes, which correspond to the lexical entry for that word. Instead, we extracted the lexical type of each lexical entry, requiring a simple lexicon lookup. In accordance with the highly lexicalised nature of HPSG, it is a convention of DELPH-IN grammars that lexical entries inherit most of their constraints from a lexical type, and contain little information themselves. Figure 2 shows the lexical entry for the word dog in the ERG. It is composed of a lexical type, n_-c_le , which abstracts the constraints common to count nouns, as well as a small amount of information specific to the word dog . Figure 3 shows the derivation tree from Fig. 1 with lexical entries replaced by their lexical types.
In the context of HPSG, these two kinds of attributes (rule identifiers and lexical types) are associated with phrases and words X  X ubtypes of the sign type , which pairs constraints on both phonological form and syntactico-semantic function. The type hierarchy, which forms the backbone of DELPH-IN grammars, also includes categories of types that abstract functionality different to that of sign types. Limiting
D ELTA  X  X  input to the portion of the type hierarchy which can be readily extracted from the derivation trees has the advantage of making this approach more portable across other grammatical frameworks as well as being simpler to implement since consultation of the grammar itself is not required. In Sect. 7 we discuss potential improvements to G D ELTA that involve expanding the input to more of the type hierarchy as well as other data structures produced by the parser.

A further issue that arose in the attribute extraction process was how attributes are extracted from items that yield more than one analysis, which for non-trivial input is the norm rather than the exception. One possibility would be to extract the union of attributes across all of the recorded analyses. But since the probability assigned to some of the analyses by the parse selection model could be quite low, this may in fact result in a poorer reflection of how the parse results have been affected by the change. Since it is not clear at which point analyses should be excluded from contributing their attributes, we left this as a user-definable parameter to G D ELTA , with the default being to use only the single best analysis.
The attribute extraction step is the one component of the G D ELTA methodology which is DELPH-IN/HPSG specific. In order to apply a G D ELTA style approach to other grammatical frameworks, a procedure for extracting appropriate attributes from derivation trees X  X r indeed other data structures X  X ust be developed. The attribute ranking and clustering algorithms described in the remainder of this section are agnostic with respect to the underlying grammar framework, only requiring a list of attributes and their item counts across the parser output produced by the two versions of the grammar. 3.3 Attribute ranking Having extracted attributes from the derivation trees of parsed items, we then needed a means of highlighting attributes impacted by the changes made to the grammar. This was first attempted by ranking attributes by the relative change in the number of parsed items that contained the given attribute for each version of the grammar, sorted in either ascending or descending order, depending on whether decreases or increases were being investigated. A problem with this ranking was that attributes considered to be particularly relevant to the change were sometimes only found in a small number of items, meaning they were obscured by changes in more commonly-occurring attributes. When developing the item clustering approach outlined in Sect. 3.4 , we also noticed that using the change in item frequency for the weighting of attributes biased the results towards lower numbers of clusters. We combated these two problems by using an alternative attribute weighting function based on the difference in the inverse document frequency (IDF) of attributes. Since the IDF of attributes will be low for those that occur frequently in a set of parse results and high for those that occur infrequently, the difference in IDF is effectively the change in rarity of attributes between parse results.
The IDF of attribute i in grammar G is given by Eq. 1 . P G is the number of parse outputs for grammar G and p i ; G is the number of parse outputs that contain attribute i ; the denominator is offset by one to avoid zero-division errors when the attribute does not occur in any of the parsed items. This weighting function for an attribute i is given by Eq. 2 , where G is the initial version of the grammar and G 0 is the new version of the grammar.

An issue we discovered with this weighting function is that when the frequency of an attribute is not affected by a change, the attribute can receive a non-zero weighting due to the fact that the total number of items parsing will likely change between grammars, yielding a change in IDF. This would be confusing for the user, suggesting an impact upon some attributes where none exists. This was solved by modifying the IDF calculation to use the total number of items parsing across both versions of the grammar, which has the effect of assigning unchanged attributes a weight of zero, while preserving the relative weighting of other attributes. The modified IDF formula is given in Eq. 3 .

Armed with the item frequency and IDF-based weighting functions, rankings could then be created across the sets of attributes extracted from the four different categories of parse change. Tables 1 and 2 show attribute rankings for the categories of no parse ! parse and parse ! no parse respectively, comparing the top 10 changes across both weighting functions. These rankings were generated from a change made to J ACY during the course of its development which involved extensive changes to the handling of verb phrases. The rankings in Table 1 are sorted in descending order, while those in Table 2 are sorted in ascending order, reflecting the direction of change that is of interest for each category. It should be noted that while the set of attributes for each parse change category only includes attributes found in items from that category, their changes in item frequency and IDF are calculated globally across all items, meaning that it is possible to see, for instance, attributes in the no parse ! parse category with decreases in item frequency and IDF weightings.

Looking at Table 1 , which shows attributes extracted from items which now parse as a result of the changes made to the grammar, we see in Table 1 a that ranking by item frequency places the attribute vn-light-rule at the top of the ranking, being found in 1566 more parsing items than before the change, followed by vstem-vend-rule , with a change of 1363 parsing items. Both these attributes were directly altered by the grammar engineer, with the increases in other attributes being due to interactions with changes made to the grammar. Looking at the ranking generated using the IDF weighting in Table 1 b, we see that using a weighting function based on the change in rarity of attributes has moved vn-light-rule down into the seventh position, and vstem-vend-rule has completely dropped out of the top 10.

Comparing the rankings produced using the two different weightings, we see that attributes occurring higher in the item frequency-based ranking tend to be pushed down in the IDF-based ranking, replaced by changes in rarer attributes. One of the effects of this emphasis on subtler, more nuanced changes found in the IDF ranking is that of pushing lexical types, whose distribution exhibits more of a long tail, higher up the ranking. We see these two different types of ranking as being complementary, with the item frequency weighting allowing the grammar engineer to build up a broad picture of how the attributes were affected, and with IDF-based weighting highlighting potentially interesting attributes from the long tail of item frequency changes. Within the final G D ELTA interface, these two signals can be combined, by X  X or instance X  X sing item frequency as the primary sort key and the IDF weighting as a secondary sort key.

During the development of G D ELTA , we noticed that there were occasions where it was useful to collapse increases and decreases in attributes together. When viewing attribute rankings for the category no parse ! parse , positive changes are of particular interest, and conversely, negative changes are more interesting in the case of the parse ! no parse category. But in the case of the remaining two parse change categories, which contain items with changes in the number of parses they received, both positive and negative changes are potentially relevant. Rather than introduce an additional signal via another weighting function, we opted to modify the existing IDF weighting function to be the absolute magnitude of the change. This version of the IDF weighting function (as given in Eq. 4 ) is the one used in G D ELTA 5 and referred to throughout the remainder of this paper.
 3.4 Item clustering Information on differences in the correlation between parsability and individual types is a valuable aid in gisting how the grammar has changed. However, the interaction between attributes impacted by the grammar change is often subtle, and only truly understood relative to different combinations of attributes and the corresponding items. As a second diagnostic facet of G D ELTA , we therefore cluster items based on parsability data and the attributes present in the associated derivation tree, and present the items associated with each cluster. As with the attribute rankings, the clustering is performed separately across the different categories of parse change.

For the underlying clustering algorithm, we chose k -means clustering for its simplicity and run-time efficiency. k -means clustering is an iterative algorithm which partitions a set of observations into k clusters. In each iteration, every observation is assigned to the cluster with the closest mean, and the mean of each cluster is subsequently updated. This process of assign-and-update is repeated until the clusters converge and the assignment of observations no longer changes. We selected Euclidean distance as the metric for calculating distances between points. To generate the item clusters, each item is first converted into a weighted attribute vector in n -dimensional space, where n is the total number of attributes located across all parse results from both versions of the grammar.

As previously mentioned, we found that using the change in item frequency to weight the attributes had the effect of strongly biasing the results towards lower numbers of clusters. We suspect this was caused by large changes to frequently occurring attributes dominating other changes, resulting in item vectors lying close to each other in Euclidean space and thus not being readily separable. The IDF-based weighting function solves this problem by picking up on more nuanced changes to attributes. Item vectors were then weighted by assigning, for each attribute, the value returned by the function given in Eq. 4 if the attribute occurred in the derivation tree for the item, and zero if it did not.

A decision to be made when using k -means clustering is how the centroids of the clusters are initialised. Since k -means is an expectation-maximisation algorithm, there is no guarantee that the final solution will be the global optimum. If a poor selection of initial seeds is made then the solution may be a local optimum a long way from the global optimum. We tried to reduce the likelihood of this by selecting centroids with a good spread across the entire set of observations. This was done by pseudo-randomly selecting a point to be the centroid of the first cluster, then selecting each other initial centroid to be the furthest point from the mean of the seeds already selected, until k points are selected.

One of the drawbacks of k -means is that the value of k must be specified in advance. Since the grammar engineer does not know how many types of changes there may be in the parse results, the best value of k is unknown. This problem is commonly solved by repeatedly performing the clustering across a range of values of k and selecting the value which yields the best clustering. One way of doing this is by selecting k such that it minimises the combined sum of the squared errors across each cluster, since the goal is to minimise the distance from each observation to its centroid. This approach has the drawback of favouring higher numbers of clusters and requires the introduction of a penalty for each increment of k to offset this bias.

We chose to compare the quality of the clustering for different values of k using the Silhouette algorithm (Rousseeuw 1987 ), which provides a means of evaluating the fit of an observation within a set of clustered observations. This technique involves the calculation of a single score for each observation, which is a combined representation of the internal similarity with other observations in the same cluster and of the dissimilarity with all other observations outside the cluster. The formula for calculating the Silhouette score of an observation i is given in Eq. 5 . a  X  i  X  is defined as the mean distance from i to all other observations in the same cluster and b  X  i  X  is defined as the mean distance from i to all observations in the cluster closest to the cluster i occurs in. This produces Silhouette scores ranging from 1 to 1, with 1 indicating poor clustering and 1 indicating good clustering. The average Silhouette score for a whole cluster can then be computed and in turn, the average across all clusters, resulting in a final value representing the quality of the overall clustering. The final clustering is done by performing k -means for values of k between 2 and 6, and selecting the value which resulted in the highest Silhouette score. A maximum of 6 was selected after repeated observations across different grammars and grammar changes showed that the number of clusters selected rarely exceeded 4. The result is a set of clusters for each of the parse change categories, with each cluster intended to capture a group of related effects on the parse results. In order to characterise the nature of these effects, the closest item to the centroid of each cluster is taken as the exemplar and displayed with the cluster. In order to make more transparent the changes in attributes that the clustering has picked up on, we include in the clustering visualisations the top-ranked attributes of the exemplar example of the clustering output is provided in Sect. 4.3 , along with an explanation of the additional signals provided in the clustering visualisations. 4 The output produced by G D ELTA consists of a series of HTML pages for viewing within a web browser. All data required for displaying the output is contained within the HTML files, so it can be viewed offline and independently of the grammar versions being compared. The output includes five different pages, each intended to provide a different facet of feedback. This section contains an overview of these pages. The sample output was generated from an attempted change to the ERG intended to expand the coverage of extracted complements with a right-node-raising analysis. This change was also used as a part of the evaluation of G D ELTA outlined in Sect. 5 and is further described there. 4.1 Summary page The Summary page is intended to be the first port of call for the grammar engineer after running G D ELTA . It provides a high level overview of the impacts that the grammar changes had over the profiles. The first half of the page is shown in Fig. 4 and the second half is shown in Fig. 5 . The first half provides an overview of various grammar profiling diagnostics, a breakdown of the number of items found in each category of parse change X  X s well as the number of items from these categories that could be used for extracting attributes X  X nd a summary of the attributes with the top increases and decreases in item frequency.

The second half of the Summary page contains separate tables of attributes found in each category of parse change. Each table has columns containing the attribute X  X  change in IDF (referred to as weight ), its change in item frequency (referred to as change ), and also the number of items from this category that the attribute occurred in. This third value is provided in order to determine the relevance of an attribute to the category, since the changes in item frequency and IDF are calculated globally across all items. Each column can be interactively sorted on, allowing the grammar engineer to explore different aspects of how the parse results were affected. Clicking on an attribute in these tables navigates to that attribute X  X  position in the Attributes page described in the next section.

The parse ! no parse ranking in Fig. 5 illustrates the complementary nature of the different signals presented by the interface. When the Change column is sorted, the hd_xcmp_nom_c attribute rises to the top. However, the relatively high change in IDF of the hd-hd_rnr-nv_c attribute, which is involved in the right node raising analysis, correctly suggests that this attribute could be of more interest. 4.2 Attributes page The Attributes page aims to provide more comprehensive information pertaining to how the occurrence of attributes in the parse results was affected by the changes made to the grammar. It contains a single table listing every attribute extracted by
D ELTA from both versions of the grammar. The different columns of the table contain the following information for each attribute: the size of the change in the item frequency, the signed change in the item frequency, the IDF-based weighting, the number of items under the previous grammar containing the attribute, the number of items under the new grammar containing the attribute and lastly, a column indicating whether the attribute was present in the source files of only one or both versions of the grammar. This last column is intended to provide an indication of when an attribute has been added to or removed from the grammar, making the cause of an attribute X  X  frequency changing to or from zero clearer.

All of the columns can be used as sort keys (secondary and tertiary sort keys etc. are also supported), allowing the grammar engineer to refine the feedback they receive towards a specific aspect of change that is of interest to them. There is also a text input box which can be used to filter on the name of attributes. This is useful for exploiting type-naming conventions that are commonly employed in DELPH-IN grammars. For instance, in the ERG, filtering on the substring _le has the effect of restricting results to lexical entries. Clicking on an attribute name in the table navigates to the Items page showing only those items which contain the attribute in the parse results. 4.3 Clusters page The Clusters page provides a visualisation of the clusters produced by G D ELTA for each of the different categories of parse change. The clusters are intended to represent different patterns of effects found in the changes between parse results. Each cluster is initially presented with only the text from the exemplar item of the cluster and the cluster X  X  Silhouette score, which gives an indication of the fit of the items in the cluster, as described in Sect. 3.4 . More details are available for each cluster by clicking on the Details button. This exposes a table containing the top 5 attributes found in the exemplar item as well as a table containing all items in the cluster. Figure 6 shows the two clusters for the parse change category parse ! no parse with the first cluster expanded. For the exemplar attributes, in addition to presenting the attribute weighting, the cohesion and overlap of each attribute is also given. The cohesion of an attribute is the percentage of items occurring in the cluster that contain the attribute, and the overlap is the percentage of items occurring in all other clusters that contain the attribute. An attribute is more strongly correlated with a cluster when it has high cohesion and low overlap. These values provide a means for the grammar engineer to assess the relevance of the attribute to the cluster. An additional feature of this expanded cluster view is that hovering over an attribute from the exemplar item causes all items in the cluster that contain the attribute to be highlighted.
 4.4 Items and errors pages The Items page and Errors page contain additional information not directly related to the attribute weighting and item clustering signals, that augments the feedback provided by the previous three pages. The Items page contains a table of items from all profiles along with information pertaining to each item, such as the input text of the item, the length of the string, the profile the item originated from, an indication of any changes to the parsability of the item, and the number of readings yielded by the two versions of the grammar. As well as this table being sortable, there is also a series of text boxes supporting filtering on fields, as well as on attributes occurring in each item. All of this allows the grammar engineer to drill down quickly and identify individual items with specific properties of interest. For instance, it is trivial to filter on only those items that no longer parse after the change, and that also contain a specific attribute or set of attributes.

The Errors page is similar in structure to the Items page but lists only those items which G D ELTA encountered errors with. These mostly tend to be parse failures, such as timeouts and system memory limits being reached, but can also arise due to
D ELTA failing to extract attributes from the derivation trees of items which were successfully parsed. 5 Evaluation The evaluation of the techniques used in the creation of G D ELTA is not straightforward due to the fact that they do not attempt to produce any kind of measurable judgement of correctness. The output is instead diagnostic in nature, requiring interpreting by the grammar engineer X  X n inherently subjective process. Ultimately the best indicator of the success of G D ELTA is whether grammar engineers are able to use the tool to more rapidly gain a sense of how the grammar has been impacted by a change. While we have yet to systematically road-test G D ELTA in the context of active development of a grammar, we have performed some qualitative evaluation of the underlying techniques. This section outlines this evaluation.

This evaluation was an attempt to gauge the utility of G D ELTA style feedback by simulating contexts from the grammar development process where G D ELTA is intended to be used. We performed this evaluation using modifications to the ERG which were known to have resulted in problems that were non-trivial to isolate. Participants were charged with the role of grammar engineer and were thus required to have some experience working with the ERG. This restriction unfortunately meant that we only had three participants in the evaluation. This small group of ersatz grammar engineers were asked to hypothesise about the nature of the problem resulting from each change, based upon the attribute ranking and item clustering output from G D ELTA , along with a short description of what the change was intended to do. Participants were also asked to describe the reasoning process they used to come to these conclusions so as to provide us with insight into how G D ELTA  X  X  output was being used.
We used three separate changes to the ERG made during its development which were ultimately not incorporated into the grammar. Each change involved modifications to the grammar that resulted in problems not apparent at the grammar profiling stage, requiring the analysis of treebanked items before being identified. For each change, the profiles that the two versions of the grammar were run over were taken from the WeScience treebank. The different grammar breakages used were as follows:
Change one: an attempt to reduce the number of analyses being generated for the noun phrase three thirty . The existing problem was that one of the analyses produced by the grammar resulted in such phrases being incorrectly analysed as a determiner followed by a noun. A fix for this was attempted by the modification of the type reg_or_temp_nom_rel which then resulted in parse failures for valid noun phrases containing numeric determiners such as two days .

Change two: an attempt to improve the generality of preposition phrase modification of nominals by changing the type n_wh_pro_lexent . This failed to parse prepositional phrases modifying wh -pronouns such as in Who in this town won?
Change three: an attempt to expand the coverage of extracted complements by modifying the type extracted_comp_phrase to implement a right-node-raising analysis for examples like We relied on and hired consultants . The change introduced a regression for items that contained coordinated phrases headed by prepositions, such as We wrote books for and with children .

Given that the aim is to identify the problematic effects of these changes, the participants would ideally be able to identify parts of the grammar where the problem manifested itself, rather than just the modified components of the grammar, which the grammar engineer would already be aware of. It should also be noted that these three changes, which all involve minor changes to a single type, represent relatively simple grammar modifications. When changing an existing analysis (or extending the grammar to cover a new one) potentially complex changes could be made to multiple locations in the grammar. Changes of this kind would offer a more compelling context for evaluation, as the potential for unexpected flow-on effects that need untangling would be much greater, and would more closely match the intended use case of G D ELTA . In future work, we hope to perform a more thorough evaluation of G D ELTA in the context of active grammar development.

The results of the grammar breakage experiments were encouraging. Participants were all able to at least identify the general nature of the problem introduced, if not the specific location in the grammar. All three participants, for instance, were able to identify that the problem in the third change involved numeric determiners, with both the attribute ranking and clustering visualisations clearly indicating that the num_det_c rule was likely to be associated with the regression.

The descriptions of the reasoning processes used by participants were also informative. One finding was that the clustering visualisations were considered to be quite useful. In particular, the top-ranking attributes of the exemplar item for each cluster were cited as being a useful diagnostic aid, along with their respective cohesion and overlap values. The attributes were used to determine the nature of the change being represented by the cluster, while the cohesion and overlap of the attributes were used to determine whether this was actually an informative cluster or not. The text of the exemplar item was useful in determining the function of the attributes found in that item. 6 Obtaining and using G D ELTA D ELTA was implemented as a Python program that is run from the command line. Running G D ELTA requires Python 2.7, and installation of the SciPy module is recommended as this will yield faster clustering. Viewing G D ELTA  X  X  output requires a standards-compliant web browser. G D ELTA is freely available for use and can be found on the DELPH-IN wiki. 6
Using G D ELTA involves the user specifying the two different versions of the grammar and their respective output profiles which are to be compared. The user can specify a range of command line flags to run G D ELTA with different parameters, such as the number of top parse results from which to extract attributes, and the maximum number of clusters to be tried in the clustering stage. Once run, G D ELTA produces a directory of HTML files containing the different views outlined in Sect. 4 . This directory contains everything required to view the HTML files, meaning that G D ELTA output can be browsed offline and does not require access to the different grammar versions or the parse output. Further instructions for installation are provided with G D ELTA  X  X  source code. 7 Future work The most important avenue of work we hope to pursue is substantial in situ road-testing of G D ELTA in the context of active development of a grammar. This is needed in order to more concretely assess the purported improvements to the grammar engineering toolchain we have argued G D ELTA offers. It would also provide an opportunity for identifying improvements to the presentation of G D ELTA  X  X  output, as well as additional data points that could be incorporated to further improve the utility of the tool.

One issue in particular that would benefit from further systematic investigation is the relationship between the utility of the signal provided to grammar engineers, the size of the test suites and the range of domains represented by the test suites, and the kind of modification made to the grammar. It is certainly the case that G D ELTA will be of limited value when used to investigate changes pertaining to linguistic phenomena that are underrepresented in the test suites used as input. Blindly scaling desirable solution, as increases to parsing time will add latency to the feedback, with the possibility for negating any efficiency improvements. It would be advantageous then to have more of a sense of the scale of input data required for G D ELTA to provide helpful results.

Further work on the underlying techniques that drive G D ELTA could involve looking at the impact on other data structures produced by the parser. G D ELTA currently only makes use of syntactic output generated for successfully parsed items. Incorporating the semantic analysis returned by the parser would likely be beneficial as this is an important signal which must also be monitored for quality assurance, and it can be the case that changes to a grammar only manifest as effects on the semantic analysis. The techniques outlined in this article could also be adapted to use the data structures found in the parse chart, opening up the possibility for partially parsed items X  X hose not receiving a full spanning parse X  X o be used.
In the context of DELPH-IN grammars in particular, a specific kind of flow-on effect that can manifest itself in the diagnostic information that G D ELTA provides is changes in attributes whose corresponding grammar types were not edited by the grammar engineer but inherited from supertypes which were modified. These kinds of interactions are distinct in that they can be readily identified by consulting the type hierarchy of the grammar. Extending G D ELTA to extract inheritance relation-ships between edited types and their descendants would provide an additional signal in the visualisations, allowing the grammar engineer to identify the origin of these particular attribute changes. This signal would also be useful in the context of the item clustering, as it would allow for the identification of clusters corresponding to this kind of flow-on effect. 8 Conclusion In this article we presented G D ELTA , a tool for providing grammar engineers with improved feedback on the effects of changes made to precision grammars. Existing tools that assist with the grammar engineering process tend to provide either immediate but very high-level feedback, or very low-level incremental feedback.
D ELTA aims to fill this gap between these extremes in the grammar engineering toolchain, providing a more nuanced picture of the impact of the change earlier on in the grammar engineering cycle. It is also the case that existing tools do not explicitly focus on changes made to a grammar, instead requiring the grammar engineer to isolate effects caused by the changes manually. G D ELTA , however, provides feedback explicitly on changes between two versions of the grammar. We also outlined two techniques used to drive the visualisations G D ELTA presents to the user: an attribute weighting algorithm used to highlight grammar attributes from the parser output which were significantly impacted by the change, and a test suite item clustering technique intended to identify related groups of changes across items whose parsability was affected by a change to the grammar.

The qualitative evaluation outlined in Sect. 5 supports our belief that G D ELTA has the potential to improve the efficiency of the grammar engineering process. During the course of the evaluation we also received positive comments from grammar engineers who helped us with this investigation, indicating that they would be interested in using G D ELTA .
 References
