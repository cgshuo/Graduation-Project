 Clustering is the unsupervised classificatio n of patterns (observations, data items, or feature vectors) into subgroups (clusters). It has important applications in many problem domains, such as data mining, document retrieval, image segmen-tation and pattern classification. One of the well-known methods is the k -means algorithm [3], which iteratively reassigns each data point to the cluster whose cen-ter is closest to the data point and t hen recomputes the cluster centers.
Several algorithms have been proposed previously to determine cluster number (called k ) automatically. Bischof et al. [2] use a Minimum Description Length (MDL) framework, where the description length is a measure of how well the data are fit by the model optimized by the k -means algorithm. Pelleg and Moore [4] proposed a regularization framework for learning k , which is called X-means. The algorithm searches over many values of k and scores each clustering model. X-means chooses the model with the best score on the data.
 Recently, Zhang and Liu presented the SSCL algorithm [7] based on the One Prototype Takes One Cluster (OPTOC) learning paradigm. The OPTOC-based learning strategy has the following two main advantages: 1) it can find natural
Although promising results have been obtained in some applications [7], the learning speed is slow due to that only one prototype can split at one time. This paper will introduce multiple spli tting into SSCL to accelerate the learning speed.

The remainder of this paper is organized as follows. In Section 2, the origi-nal SSCL algorithm is introduced. Section 3 will describe the details of multiple splitting and merging. Their performance in identifying Gaussian clusters is com-pared in Section 4. Finally, Sectio n 5 presents the conclusions. Clustering is an unsupervised learn ing process [1]. Given a data set of N dimen-sions, the goal is to identify groups of data points that aggregate together in some manner in an N -dimensional space. We call these groups  X  X atural clusters. X  In the Euclidean space, these groups form dense clouds, delineated by regions with sparse data points.

The OPTOC idea proposed in [7] allows one prototype to characterize only one natural cluster in data set, regardless of the number of clusters in the data. This is achieved by constructing a dynamic neighborhood using an online learning vector A i , called the Asymptotic Property Vector (APV), for the prototype P i , such that patterns inside the neighborhood of P i contribute more to its learning than those outside. Let | XY | denote the Euclidean distance from X to Y ,and assume that P i is the winning prototype for the input pattern X basedonthe minimum-distance criterion. The APV A i is updated by where  X  is a general function given by and  X  i , within the range 0 &lt; X  i  X  1, is defined as n
A i is the winning counter which is initialized to zero and is updated as follow: The winning prototype P i is then updated by where,
If the input pattern X is well outside the dynamic neighborhood of P i , i.e., |
P i X || P i A i | , it would have very little influence on the learning of P i since  X  namic neighborhood of P i ,both A i and P i would shift toward X according to Equations (1) and (5), and P i would have a large learning rate  X  i according to Equation (5). During learning, the neighborhood | P i A i | will decrea se monoton-at the center of a natural cluster in the input pattern space.

Let C i denote the center, i.e. arithmetic mean, of all the patterns that P i wins according to the minimum-distance rule. The distance | P i C i | measures the discrepancy between the prototype P i found by the OPTOC learning process and the actual cluster structure in the dataset. After the prototypes have all settled down, a large | P i C i | indicates the presence of other natural clusters in the dataset. A new prototype would be generated from the prototype with the largest distance | P i C i | when this distance exceeds a certain threshold  X  .
When cluster splitting occurs, the new prototype is initialized at the position specified by a Distant Property Vector (DPV) R i associated with the mother prototype P i . The idea is to initialize the new prototype far away from its mother prototype to avoid unnecessary competitio n between the two. Initially, the DPV is set to be equal to the prototype to which it is associated with. Then each time a new pattern X is presented, the R i of the winning prototype P i is updated as follows: where and n R i is the number of patterns associated with the prototype P i .Notethat unlike A i , R i always try to move away from P i . After a successful split, the property vectors ( A i , R i ) of every prototype P i are reset and the OPTOC learning loop is restarted. 3.1 Multiple Splitting The SSCL algorithm as it was described up to this point can only allow one prototype split when it meets the conver gence and splitting criteria. This has dramatically slowed down the learning process, especially for the data with large number of points or clusters. We proceed now to demonstrate how to efficiently search for the best number of clusters by letting more prototypes to split in two at the same time.
 How can we decide how many prototypes to split at one time? Pelleg and Moore tried to split every centroid int o two children and run the test locally to make decisions that increasing the number of centroids or not in the X-means algorithm [4]. Unfortunately, SSCL is not splitting prototypes locally. New prototype can be generated far fro m the parent prototype. The method of splitting locally and testing is not a good strategy for SSCL.
Recall that, the prototype with the largest distance | P i C i | which exceeds a certain threshold  X  will split. The new splitting strategy is that all the prototypes (MSSCL). This allows an automatic choice of whether to increase the number of prototypes by very few (the current number is very close to the true number, i.e. the end of learning process) or very many (the beginning of learning process). With this strategy, the learning pr ocess of SSCL will be accelerated. 3.2 Prototypes Merging In essence, the MSSCL algorithm starts form one prototype and continues to add prototypes where they are needed until t he stop criterion is r eached. However, sometimes more than one DPVs are attracted by the same cluster. This may cause more than one prototypes will split to a very close position. It is possible that a natural cluster in the data set would be split into two or more clusters.
One merging scheme is proposed to merg e two clusters when these two clusters are close each other to the extent that their joint Probability Density Function (pdf) form a unimodal structure [6]. Let C i be the center (i.e., mean) of cluster i and  X  i be its standard deviation. In the Multiple Self-Splitting and Merging Competitive Learning (MSSMCL), if two clusters satisfy the following condition, they should be merged into one:
Only comparing prototypes, merging is a quite efficient way to overcome the potential over clustering problem brought by multiple splitting.

We then define measure of quality for a cluster  X  : where i ranges over all input points.

During the MSSMCL learning process presented in this paper, the prototype set that achieves the best BIC score is recorded, and this is the one that is finally output. The pseudo code for the proposed MSSMCL is shown in Fig. 1. We have conducted experiments on randomly-generated data, as described in [5]. The synthetic experiments were conducted in the following manner. First, a data-set was generated using randomly-selected points (cluster centers). For each data-point, a cluster was first se lected at random. Then, the point coor-dinates were chosen independently under a Gaussian distribution with mean at the cluster center.

In our first experiment, we tested the quality of the MSSMCL solution against that of SSCL. The test data set was 3-D data with 30 clusters, within the range (0, 1) for each dimension. We compared both algorithms by the distortion of their output. The convergence and splitting thresholds are both set as the same value of deviation  X  , 0.025. The results shown on Tab. 1 are average of 30 runs. The two algorithms have achieved similar results in the mean of distortion.
As far as speed is concerned, MSSMCL scales much better than SSCL. One data set was generated as described above with 20 clusters on 2 dimensions contained different number of points, from 30000 to 1000000, respectively drawn this way. The deviation  X  equals to 0.5 and each dimension data rage is (0, 10). The SSCL and MSSMCL are running on this data-set and measured for speed. The experiment is repeated 30 times and averages are taken. Fig. 2 shows the run-times of MSSMCL and SSCL with convergence threshold  X  and splitting threshold  X  set as 0.5.

Two algorithms were also tested with the number of dimensions varied from 2 to 10 with deviation  X  =0 . 5. This experiment is repeated 30 times and averages are taken. The convergence and splitting thresholds are both set to 0 . 5. The number of clusters each algorithm requested to find is 20 and the each cluster has 1000 points. The results are shown in Fig. 3, which shows that MSSMCL runs a half time of SSCL.
 Fig. 2 and Fig. 3 also show that both SSCL and MSSMCL run faster than X-means. The run-times are shown on 2.4Ghz Pentium-4.
In terms of run-time, MSSMCL is not only faster, but increases its advantages as the number of points (see Fig. 2), or the number of clusters increases (See Fig. 4) . The total number of points are 10000 with clusters from 8 to 36. Fig. 4 illustrates the iterations needed as a function of number of clusters. We can see that the iterations needed by SSCL is at least the number of the clusters if the natural number of clusters is successfully found. MSSMCL can benefit a lot from the multiple splitting especially for a data set with large number of clusters. We have presented an efficient Multiple Self-Splitting Competitive and Merging Learning algorithm that incorporates BIC scoring and prototypes merging. It uses statistically-based criteria to maximize the model X  X  posterior probabilities. This prevents missing the better prototypes set during the learning process. Our various experimental results on random generated data show that this new algorithm can perform 5 times faster than SSCL on large data set with hight dimensions and achieves better quality of clustering. This new algorithm can be used on large size of data set with high dimensions.

