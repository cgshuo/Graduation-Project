 1. Introduction down a ranked list of documents by highlighting next relevant and novel sentence. This way the user avoids non-relevant and duplicate information and saves time moving through the document collection. When used for question answering, sen-and summarizes up-to-date medical information from trustworthy online sources according users queries. We see that  X  collections.

Methods used for sentence retrieval are usually simple adaptations of document retrieval methods where sentences are ing into account local context made of surrounding sentences or the whole document, ( Fernandez et al., 2010; Murdock, 2006 ). Attempt of improving the TF X  X SF (Term Frequency X  X nverse Sentence Frequency) method by taking into account con-text by Fernandez et al. was unsuccessful and had no statistically significant improvements ( Fernandez et al., 2010 ). able to also try to apply modifications that would use local context of sentences and would promote retrieval of longer 2. Related work
Sentence retrieval methods are usually simple adaptations of document retrieval methods where sentences are treated as adaptation of the TF X  X DF method for document retrieval. TF X  X DF is a numerical statistic descriptor which indicates how in the document and at the same time rarely appears in the collection. The sentence retrieval method TF X  X SF was shown to outperform other methods like BM25 based methods or language modeling based methods ( Allan et al., 2003; Fernandez method which is language modeling approach to the document retrieval. That method, invented by Ponte and Croft for doc-et al., 2010; Murdock, 2006 ).
 partially solved by using the local context of sentences.

The idea that  X  X  X ood X  X  sentences come from  X  X  X ood X  X  documents was proposed by Murdock (2006) . So the query likelihood method was improved using local context in form of the document the sentence came from. A mixture model which com-bines a sentence language model, document language model and collection language model was proposed. The method showed better results when compared to the query likelihood baseline ( Murdock, 2006 ). was called Three-mixture model (or 3MM). Two types of context were tested: the document that contains the sentence and surrounding sentences (previous, current and next sentence). The 3MM method, additionally extended with the estimation
A method similar to 3MM was also used for the CADIAL search engine with the difference that the unit of retrieval was not a sentence but an element of a semi-structured XML document ( Mijic  X  , Moens, &amp; Dalbelo Ba X ic  X  , 2009 ). od that uses both, a context and an element for promoting retrieval of longer documents ( p ( d | s )). tfmix method served as a baseline method that combines TF X  X SF with context.
All in all in this paper we compared our new TF X  X SF based methods with local context and component that promotes re-trieval of longer sentences with the following earlier methods: 2003; Fernandez &amp; Losada, 2009; Losada &amp; Fernandez, 2007 ); 2010 )); component that promotes the retrieval of long sentences (this method is called 3MMPDS in this paper). this paper. From the methods tested in ( Fernandez et al., 2010 ) we chose 3MM because it was also used in other papers as other best methods from ( Fernandez et al., 2010 ). 3. Adding context to the TF X  X SF ranking function
From the previous examples we saw that some sentence retrieval methods were improved by using local context of sen-introduce the new method again.
 The TF X  X SF based ranking function for sentence retrieval is ( Allan et al., 2003; Losada, 2008 ): where tf t , q is the number of occurrences of term t in question q, tf t , s is the number of occurrences of term t in sentence s, sf t number of sentences that contain term t, n number of sentences in the collection.
 where s prev ( s ) depicts previous sentence of sentence s and s
R ( s next ( s )| q ) represent the relevance of the previous and next sentence. R
In other words three recurrences are used. After that no context is used i.e. R where S = R ( s | q ), P i = R ( p i | q ), N i = R ( n i the next tree sentences of sentence s as shown in Fig. 1 .

By using the recursive function, it is also possible to take into account all the neighboring sentences of the current sentence in a document. Tests, showing if it can help to include more than three previous and three next sentences into computation of the relevance of a sentence, are left for future work.

One benefit of our new TF X  X SF variant in comparison to the tfmix method from ( Fernandez et al., 2010 ) is the explicit modeling of the relevance of the context (the previous and the next sentence of the current sentence) to the query
R related to the sentence and to the neighbor sentences ( tf future work where we want to automatically generate a document representation for sentence retrieval. Our function ( R of context. Our aim is to use such representations in a web environment where structured document representations are retrieval will be converted to a document representation 4. Adding component for promoting the retrieval of longer sentences to the TF X  X SF ranking function
We already mentioned that in ( Fernandez et al., 2010 ) the probability of generating a document given the sentence There were no significant differences among them and at the same time all of them showed better performance than the by adding a component that promotes retrieval of longer sentences. We simply assume that the relevance of a sentence is tence. The new ranking function can be defined as follows
In the Eq. (4) | s | denotes the length of sentence s , d ( s ) denotes the document that contains the sentence s , AvgSenLength ( d ( s )) denotes the average sentence length in the document that contains the sentence s .
Eq. (4) raises the probability of retrieving long sentences by giving them extra weight. Specifically, the component can now combine Eqs. (2) and (4) to get a new ranking function that at the same time uses context of sentences and pro-motes the retrieval of longer sentences as follows (Eq. (5) ): 5. Other tested sentence retrieval methods
In addition to the already presented methods (Sections 3 and 4) we also included into our tests the tfmix method ( p ( d | s )) ( Fernandez et al., 2010; Murdock, 2006 ).
 The tfmix method is defined in ( Fernandez et al., 2010 ): sentence.
 The 3MM model with p ( d | s ) is defined as follows ( Fernandez et al., 2010 ): et al., 2010 ):
In Eqs. (7) and (8) p  X  t j s  X  X  tf t ; s j s j , p  X  t j context  X  s  X  X  X  length of the sentence s , context context ( s ) and collection coll respectively. 6. Results and discussion An overview of all tested methods in this paper is shown in Table 1 .
 The origin of each method from Table 1 is illustrated in Fig. 2 .

We tested all sentence retrieval methods ( Table 1 ) using data from the TREC Novelty tracks which are series of competitions used to test novelty detection systems. There were three TREC Novelty Tracks in the years from 2003 to performance of novelty detection depends on the quality of the performance of sentence retrieval. is the term used for query at TREC) and an ordered list of documents find relevant and novel sentences. In each Track list of mostly relevant documents and a list of sentence level relevance judgments.

In TREC 2002 the topics from ad hoc Tracks (previous TREC tracks that deal with document retrieval) were used. 25 most were used. If the topic had less than 25 documents, non-relevant documents were added to reach the number of 25 docu-ments. Two files with sentence relevance judgments were provided. One with 2% of sentences judged relevant and one with 7% of sentences judged relevant. We used the file with 7% of sentences judged relevant in our tests.
In TREC 2003 topics where constructed specially for the Novelty track. For every topic 25 relevant documents were cho-sen. 37.56% of sentences were judged relevant.

In TREC 2004 between 25 and 100 documents were chosen with 25 of them relevant. 16.2% of sentences were judged relevant.
 An example of a topic from the TREC 2002 Novelty track is shown in Table 2 .

To evaluate our three new methods (TF X  X SF con ,TF X  X SF length methods 2003; Fernandez &amp; Losada, 2009; Losada &amp; Fernandez, 2007 ) (Eq. (1) ), tests in ( Fernandez et al., 2010 ) (Eq. (6) ), motes the retrieval of long sentences, that together with some other language modeling based methods showed highest performance in ( Fernandez et al., 2010 ).

In the experiments we partially used Rapidminer, 2 an open-source system for data mining, with Text Extension standard stop words were removed. Stemming was not applied. Results from Rapidminer were presented as a web service and further used in a custom program that implemented the sentence retrieval methods.
 formance using the standard precision oriented measure P@10 and recall oriented measures MAP, and R-precision. The same robust to violations of its normality assumption ( Hull, 1993 ).
 track data as follows: Training with TREC 2002 and testing with TREC 2003 and TREC 2004.
 Training with TREC 2003 and testing with TREC 2002 and TREC 2004.
 Training with TREC 2004 and testing with TREC 2002 and TREC 2003.
 we measured the performance of the system by using Mean average precision (MAP). Table 3 shows the optimal values of parameter l for the corresponding methods (TF X  X SF con , TF X  X SF Table 4 . shows optimal parameter values of methods tfmix and 3MMPDS.
 differences in comparison to the 3MMPDS method are marked with an
The tfmix method showed similar performance as in ( Fernandez et al., 2010 ) with no statistically significant improve-ments of the baseline TF X  X SF.

The 3MMPDS method showed mostly worse performance than the TF X  X SF and tfmix . Precisely, 3MMPDS showed statis-tically significant worse performance than TF X  X SF and tfmix according P@10 in 18 out of 18 cases, according MAP in 2 out of 18 cases, according R-Precision 1 out of 18 cases. 3MMPDS also showed statistically better performance than TF X  X SF and tfmix according MAP in 2 out of 18 cases.
The TF X  X SF con method showed mostly statistically significant better results according MAP and R-precision and compet-itive results according P@10 in comparison to methods TF X  X SF and tfmix . The TF X  X SF significant better results according to P@10, MAP and R-precision in comparison to 3MMPDS. R-Precision) in comparison to all methods TF X  X SF, tfmix and 3MMPDS.
 sion) in comparison to methods TF X  X SF, tfmix and 3MMPDS.
 tically significant differences in comparison to the TF X  X SF in comparison to the TF X  X SF length method are marked with an
According to the presented results ( Tables 8 X 10 and Figs. 6 X 8 ) the TF X  X SF to R-Precision in 5 out of 6 cases).

We also see from experimental results that the TF X  X SF length cases, according to MAP in 6 out of 6 cases and according to R-precision in 6 out of 6 cases). tested measures (P@10, MAP, R-precision) in comparison to the baseline TF X  X SF method. It is important to notice that the TF X  X SF con,length method also has statistically better results according to MAP and
R-precision in comparison to the methods TF X  X SF con and TF X  X SF improves the baseline according to MAP and R-precision. Method TF X  X SF
MAP and R-precision. However when we combine the modification of the baseline from TF X  X SF the modification from TF X  X SF length (i.e. promoting retrieval of longer sentences) into a new method TF X  X SF at the same time promote retrieval of longer sentences.

TF X  X SF length has statistically significant improved results in comparison to the baseline TF X  X SF method. The TF X  X SF using context and promoting retrieval of longer sentences do not overlap, on the contrary they sum up. So we conclude the following It can be useful to use context for sentence retrieval.
 It can be useful to promote retrieval of longer sentences for sentence retrieval.
 It can be useful to combine previous two.

The measures MAP and R-precision by which our new methods (TF X  X SF formance are recall oriented. We saw that the improvement according to MAP and R-precision comes from two different context promotes sentences that do not have many terms in common with the query but the context have some terms in common with the query which increases recall. When it comes to promoting the retrieval of longer sentences the reason for improvements according to MAP and R-precision may lie in the fact that relevant sentences chosen from the assessors walk down a ranked list of documents by highlighting only relevant (and novel) sentences. Having access to all relevant documents is also important for multi-document summarization ( Fernandez et al., 2010 ).
When it comes to the precision oriented measure P@10 we have better results when using methods TF X  X SF the fact that relevant sentences are on average longer than not relevant sentences. The measure P@10 is important when using sentence retrieval for tasks that require high precision like question answering. 7. Conclusion
In this paper we have implemented two improvements for TF X  X SF method for sentence retrieval that were shown useful in methods based on language modeling approach. In our earlier paper ( Doko et al., 2013 ) we successfully improved the
TF X  X SF method using local context and called the new method TF X  X SF sentence length in comparison to average sentence length in document. A second new method named TF X  X SF also proposed that combines the modifications of the previous two. All methods were compared against the state of the ments in comparison to the state of the art methods. TF X  X SF also shown that the method TF X  X SF con,length summed up the positive effects of the individual methods TF X  X SF
ISF other which was not clear in ( Fernandez et al., 2010 ).
 Taking into account the presented results we can say that the main contributions of this paper are as follows: Improvement of the TF X  X SF baseline by promoting the retrieval of longer sentences.
 not overlap. In fact they sum up and it is useful to use both of them at the same time. comparison to state of the art methods TF X  X SF, tfmix , 3MMPDS.
 References
