 Along with the increasing popularity of social web sites, users rely more on the trustworthiness information for many online activities among users. However, such social network data often suffers from severe data sparsity and are not able to provide users with enough information. Therefore, trust prediction has emerged as an impor-tant topic in social network research. Traditional approaches ex-plore the topology of trust graph. Previous research in sociology and our life experience suggest that people who are in the same social circle often exhibit similar behavior and tastes. Such an-cillary information, is often accessible and therefore could poten-tially help the trust prediction. In this paper, we address the link prediction problem by aggregating heterogeneous social networks and propose a novel joint manifold factorization (JMF) method. Our new joint learning model explores the user group level similar-ity between correlated graphs and simultaneously learns the indi-vidual graph structure, therefore t he shared structures and patterns from multiple social networks can be utilized to enhance the pre-diction tasks. As a result, we not only improve the trust prediction in the target graph, but also facilitate other information retrieval tasks in the auxiliary graphs. To optimize the objective function, we break down the proposed objective function into several man-ageable subproblems, then further establish the theoretical conver-gence with the aid of auxiliary function. Extensive experiments were conducted on real world data sets and all empirical results demonstrated the effectiveness of our method.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms,Experimentation Trust Prediction, Social Network, Transfer Learning, Nonnegative Matrix Factorization
The ever-increasing popularity of social web sites such as Face-book and LinkedIn has generated complicated social networks and corresponding data sets with enormous sizes. Among the various forms of online activities, adding (accepting) other users as friends is a primary one. With the increasing risk of exposing private pro-file to malicious users, the question of whom to trust has become an important challenge to individual users. Many online social com-munities allow users to tag other users to facilitate the trustworthi-ness evaluation. Trust tags in a social network can be represented as a trust graph G = V,E ,where V represents the collection of nodes (users) and an edge between node i and j denotes a trust vote from user i to user j . Due to the lack of diligence and privacy concern on users X  part, there are often a large number of missing values in the trust graph, making the trust link prediction a diffi-cult task. In the literature, there are a few trust prediction papers using trust propagation [8, 11]. The assumption for these meth-ods is that users tend to trust each other given a trustable common friend. However, since only a very small portion of entries in trust graph are explicitly tagged, the prospect of these approaches seems gloomy.

It has been discovered in [15], people who are in the same social circle often share similar behavior and tastes. In [5], Crandall et al. give the following two main reasons. One is that people gen-erally adopt behavior exhibited by those they interact with. Such process is called social influence. The other more distinct reason is people incline to form relationships with others who are already similar to them. Prior research works on inferring individual user X  X  interests and attributes from his or her social neighbors [1, 13, 16]. These papers show the possibility of improving the users X  attributes prediction from the trust graph. In this paper, we will explore the trust graph structure with the users X  behavior profile instead, use the ancillary informa tion to help the trust prediction. We propose a joint manifold factorization (JMF) model to predict the trust and distrust in social network by aggregating heterogeneous social net-works from both target trust domain and auxiliary information do-main. When we say two graphs are heterogenous, it implies they are from different domains and have no apparent structural similar-ity and their entries generally have different scale. Our approach is to alleviate the sparsity problem in trust graph by taking advantage of the supplementary knowledge about user behavior and discover-ing the implicit group-level similarity, which are jointly determined by the user-user trust graph and user behavior auxiliary graph. This helps us find the optimal like-minded user groups across both do-mains. Moreover, we construct the individual affinity graphs to explore the individual geometric structures of the feature manifold to improve the prediction of the missing elements.

The remainder of this paper is organized as follows. In Section 2, we describe the notations used in this paper and formulate the objective function. We will derive our optimization method and provide the algorithm in Section 3. In Section 4, we will give the outline of the convergence proof in our new algorithm. We empir-ically validate the effectiveness of our method for trust prediction in Section 5 and conclude the paper in Section 6.
In this section, we will introduce our JMF objective function that aggregates the heterogeneous social networks. Prior to this, we first reiterate our motivation and then give an example to demonstrate this.

As mentioned in the introduction, trusted users in a social net-work often display similar behavior and taste. Meanwhile, social network users become friends due to the similar background and in-terest. Therefore, the trust graph and behavior graph should share some structure similarity in spite of the apparent difference, if the coincidence of the similar ratings contributes to the trust votes. As a result, the trust prediction accuracy can be improved with the aid of behavior graph information and vice versa. In summary, we transfer the knowledge from different domains to circumvent the sparsity constraint and help predict the entries in both matri-ces, this answers the most important questions of transfer learning: what to transfer and how to transfer. Fig. 1 is a demonstration of our motivation.
We use boldface uppercase letters, such as X to denote matri-ces, X i. , X .j , X ( i, j ) (or X ij when needed) to denote the j th column and the entry located at ( i, j ) of X , respectively. In our setting, for simplicity, we only discuss two matrices G 2 case, then it is natural to extend the objective function to mul-tiple matrices case. For the ease of discussion, we further assume G 1  X  R n  X  m 1 , G 2  X  R n  X  m 2 are the trust graph and rating graph (a special category of behavior graph) respectively, where number of identical users in both domains, m 1 is the number of users who receive trust votes, m 2 is the number of different items.  X  1  X  G 1 and  X  2  X  G 2 are entries known in corresponding graphs.
Inspired by the preceding discussions, we target at the joint ma-trix factorization to find out the shared group structure between two graphs. ber of group parameter determined by user. c&gt; 0 is a scalar adjust-ing the scale inconsistency between graphs since the two graphs are from different domains. Here U is jointly determined by the trust graph and rating graph, therefore it provides the shared group struc-ture for both graphs. Since rows represent users in both graphs, we could group users based on U and then conduct the trust and rating prediction with V 1 , V 2 respectively. It can be observed that ries the knowledge of both trust graph and rating graph, such frame-work becomes especially useful since both graphs usually have data sparsity issues for real data sets.

While the above model takes into account of the common row group structure in terms of both matrices, it fails to consider the individual column information for both matrices. To overcome this drawback, we include the Laplacian regularity term [9] in the above formula. To be specific, Here  X &gt; 0 is a scalar parameter to be tuned, L 1 and L Laplacian graph based on the columns of G 1 and G 2 respectively, Tr is the trace operation which yields the sum of diagonal elements of the matrix. The detailed construction for L 1 and L 2 would be given in the next section. We impose the orthogonal constraints on V 1 and V 2 to ensure the uniqueness of the solution. Suppose 1 and V  X  2 are the solutions to Eq. (2), then for any given non-zero constant c 1 &gt; 1 , c 1 U  X  and V  X  1 /c 1 would give same value in the first term and lower value for the third term, this is true no matter U  X  and V  X  1 are local or global optimum solutions, the same argument applies to V 2 . In other words, the optimal solution to Eq. (2) is not unique without the constraint.
In the following, we will give solution to Eq. (2). As we see, minimizing Eq. (2) is with respect to U , V 1 , V 2 and c can not give a closed-form solution. We will present an alternat-ing scheme to optimize the objective, this procedure repeats until convergence.
In this paper, for any missing entry G ( i, j ) , we use mean of the available entries in the corresponding row and column, for a user-item rating matrix, such initialization combines the available in-formation for both the individual user rating habit and other users X  ratings on a particular item. For a user-user trust matrix, such ini-tialization consider both user i and user j  X  X  social circle influence.
After the missing values initial imputation, we construct the Lapla-cian graphs for both social networks. We define the edge weight matrix W as follows: where N k ( X i. ) denotes the set of k nearest neighbors of easy to see W is symmetric. Let graph Laplacian L = D  X  W where D is a diagonal matrix whose entries are column sums of W
After that, we construct V 1 and V 2 based on k -means on columns for G 1 and G 2 respectively. For i -th row of V 1 , if this row belongs to j -th cluster, then V 1 ( i, j )=1 , all other elements in 0.
 V 2 is initialized in the same manner.

Now we come to the optimization of our objective function, we iteratively solve U , V 1 , V 2 and c in an alternating manner. In other words, we will optimize the objective with respect to one variable while fixing the other variables. Such process repeats until convergence. Optimizing Eq. (2) with respect to U is equivalent to optimizing Setting  X  X  1  X  U =0 leads to the following updating formula Optimizing Eq. (2) with respect to V 1 is equivalent to optimizing For the constraint V T 1 V 1 = I , we can not get a closed-form so-lution of V 1 . Therefore we will present an iterative multiplica-tive updating algorithm. We intr oduce the Lagrangian multiplier  X   X  R l  X  l , the corresponding Lagrangian function is L ( Setting  X  X  ( V 1 )  X  V 1 =0 and use the orthogonal constrain we obtain Using the Karush-Kuhn-Tucker conditio n [3]  X   X  V 1 =0 ,where is the element-wise product operator and thereafter, we get Introduce L 1 = L + 1  X  L  X  1 , V 1 = V + 1  X  V  X  1 and U where U + ij =( | U ij | + U ij )/2 and U  X  ij =( | U ij | X  U L , V 1 defined in a similar fashion, we obtain ( G Eq. (9) leads to the following updating formula V 1 ( i, j )= V 1 ( i, j ) Optimizing Eq. (2) with respect to V 2 is equivalent to optimizing
The optimization with the above equation is almost identical to the previous subsection, so we only give formula without details.
V 2 ( i, j )= V 2 ( i, j ) Optimizing Eq. (2) with respect to c is equivalent to optimizing The above task is equivalent to This can be written as where A = Tr ( G 2 G T 2 ) , B = Tr ( UV T 2 G T 2 ) , D = It is a quadratic function in c , the solution is then
In this section, we prove the convergence of our algorithm. Since the objective function is naturally lower bounded, we just need to prove the objective function is monotonically decreasing at each iteration. The decreases in objective function for updating are obvious, so we focus on the proof for the V 1 and V 2 classic auxiliary function approach used in [12].
 D EFINITION 1. [12] Z ( h, h ) is an auxiliary function for if the conditions are satisfied.
L EMMA 1. [12] If Z is an auxiliary function for F ,then F non-increasing under the update L EMMA 2. [7] For any nonnegative matrices A  X  R n  X  n , B  X  R k  X  k , S  X  R n  X  k , S  X  R n  X  k , and A , B are symmetric, then the following inequality holds T HEOREM 3. Let Then the following function
Z ( V 1 , V 1 ) =  X  )  X  2 + is an auxiliary function for J ( V 1 ) . Furthermore, it is a convex function in V 1 and its global minimum is V 1 ( i, j )= V 1 ( i, j ) Proof We have to omit the proof due to page limit. Interested reader may contact the first author for details.

T HEOREM 4. Updating V 1 using Eq. (10) will monotonically decrease the value of the objective in Eq. (2).
 Proof By Lemma 1 and Theorem 3, we can get that J Z ( tonically decreasing.
 The monotonical decrease of the objective in Eq. (2) via updating V 2 using Eq. (12) can be proved in a similar way. Therefore it is obvious our algorithm will converge.
In this paper, we want to compare the prediction performance with other methods on both trust graph and rating graph from a real data set.
This data set was collected by Paolo Massa [14] in a 5-week crawl from Epinions.com. It consists of two parts, one is the rat-ings part, the other is the trust votes part. The Epinions data set con-sists of 49,290 users, 139,738 items, 664,824 reviews from users to items, 487,181 trust statement between users. Users express their web of trust, i.e, reviewers whose reviews and ratings they have consistently found to be valuable and offensive [14]. Therefore it is reasonable to assume most individual users tend to cast trust votes towards other users if the users have similar rating patterns towards those items. As a result, the rating matrix and trust matrix could have similar row structure given common users, in other words, the assumption of our method holds on this data set.
The competitive methods include average filling (AF), k -nearest neighbors (KNN), SimRank [10], SVD [2] and matrix completion via trace norm (MC) [4]. AF uses the average of their available row and column entries to impute the missing entries. KNN imputes the missing values based on nodes similarity using Jaccard X  X  coefficient while SimRank imputes based on path ensembles. SVD finds a low-rank matrix that approximates the target matrix and MC seeks a low-rank matrix with trace norm regularity, where trace norm of a matrix is the convex relaxation of its rank. Note that we stack the trust graph and rating graph using the common users as rows for MC method, that serves the benchmark method for transfer learn-ing.

For KNN, we search k in the list { 1 , 2 ,..., 9 } , to impute the missing value using the node with the highest Jaccard similarity score. For SimRank method, we set the parameters using the de-fault value suggested by the author. For SVD, we choose the rank from the list ( R 10 , 2 R 10 ..., R ) ,where R =min( n, m of the number of rows and columns. For MC, regularity coefficient  X  is tuned from the list { 10  X  2 , 10  X  1 , 1 , 10 } .

We design the experiments as follows: we select top 2,000 users with the highest degrees (cast and receive most votes), then we se-lect items with more than 68 ratings from the above selected users. The resulting trust graph G 1 of size 2 , 000  X  2 , 000 has 149,146 trust votes (represented by 1), which consists of 3.73 % of all pos-sible votes, those distrust or unknown votes are represented by 0. The rating graph G 2 of size 2 , 000  X  96 has 10,225 ratings (from 1 to 5), which consists of 5.33 % of all possible ratings, those miss-ing ratings are represented by 0. Among those available ratings, the number of ratings 1,2 and 3 are roughly equal, 4 is twice as many as 1 and 5 is about 4 times as many as 1, such skew distribution might be due to users X  reluctance to give low ratings for unsatisfactory items.
The evaluation of rating graph is relatively easy, it is conven-tional to use Mean Average Error (MAE) and Root Mean Square Error (RMSE).
 where r ij and  X  r ij are the true and predicted ratings respectively, | T E | is the number of test ratings.

The evaluation of trust graph is more complicated. Since the bi-nary trust votes have a very skewed distribution, precision and re-call are more suitable than receiver operating characteristic (ROC) [6]. However, since most methods described in this paper do not restrict the output to the discrete binary domain, in order to use the metric measure precision and recall, we have to convert the contin-uous predictions to the binary values using threshold values. For real data set, it is difficult (if not impossible) to choose an appropri-ate threshold, therefore it is inappropriate to compare all methods solely based on the optimal recall and precision values. As a result, we evaluate the performance of all methods based on Area Under Curve (AUC) for recall-precision curves and the optimal F1 values from all methods, where F1 is defined in Eq. (18). where TP, FN and FP are numbers of true positives, false negatives and false positives, respectively.

We randomly hide half of the available entries, conduct the pre-dictions via all methods and evaluate via 2-fold cross validation. Such procedure is repeated 10 times and we report the average re-sults in Table 1. JMF has better performance than other methods except F1 score for trust links, where AF shows some slight advan-tage. Note that it is impractical and time consuming to tune thresh-old value for real application, therefore our method still shows bet-ter performance in trust link prediction than AF method consider-ing the significant AUC advantage. We can conclude our method has the best performance in trust prediction in all the methods we listed in terms of trust links and distrust links. Table 1 also lists all methods X  optimal value in terms of MAE and RMSE for the rating graph, again JMF has the best MAE and RMSE results. We can therefore conclude that transfer learning does provide the bridge for the trust graph and rating graph to share the valuable informa-tion with each other. This helps alleviate the common data sparsity issue in social network data. On the other hand, as we have shown, naive transfer learning MC method does not work very well here, MC method fails to extract the common row structure with matrices stacked.
In this paper, we developed the joint manifold factorization (JMF) method to perform trust prediction with the ancillary rating matrix. We transfer the common group structure knowledge between two related matrices and simultaneously explore the individual matrix geometric structure. With publicly available data sets, our method shows its advantage over classical trust prediction methods for both the trust matrix and rating matrix.
The authors would like to thank the authors of Epinions for gra-ciously making data available for this study. We also appreciate the code from other authors. [1] P. Bedi, H. Kaur, and S. Marwaha. Trust based recommender [2] D. Billsus and M. J. Pazzani. Learning collaborative [3] S. Boyd and L. Vandenberghe. Convex optimization .
 [4] E. Cand X s and B. Recht. Exact matrix completion via convex [5] D. Crandall, D. Cosley, D. Huttenlocher, J. Kleinberg, and [6] J. Davis and M. Goadrich. The relationship between [7] C. Ding, T. Li, and M. Jordan. Convex and semi-nonnegative [8] R. Guha, R. Kumar, P. Raghavan, and A. Tomkins.
 [9] X. He and P. Niyogi. Locality preserving pr ojections. In [10] G. Jeh and J. Widom. A measure of structural-context [11] S. Kamvar, M. Schlosser, and H. Garcia-Molina. The [12] D. Lee and H. Seung. Algorithms for non-negative matrix [13] P. Massa and P. Avesani. Trust-aware recommender systems. [14] P. Massa and P. Avesani. Trust metrics in recommender [15] M. McPherson, L. Smith-Lovin, and J. Cook. Birds of a [16] Z. Wen and C. Lin. On the quality of inferring interests from
