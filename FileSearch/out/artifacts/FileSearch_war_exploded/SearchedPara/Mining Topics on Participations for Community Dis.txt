 Community discovery on large-scale linked document cor-pora has been a hot research topic for decades. There are two types of links. The first one, which we call d2d-link , indicates connectiveness among different documents, such as blog references and research paper citations. The other one, which we call u2u-link , represents co-occurrences or si-multaneous participations of different users in one doc-ument and typically each document from u2u-link corpus has more than one user/author. Examples of u2u-link data covers email archives and research paper co-authorship net-works. Community discovery in d2d-link data has achieved much success, while methods for that in u2u-link data either make no use of the textual content of the documents or make oversimplified assumptions about the users and the textual content. In this paper we propose a general approach of com-munity discovery for u2u-link data, i.e., multiple user data, by placing topical variables on multiple authors X  participa-tions in documents. Experiments on a research proceeding co-authorship corpus and a New York Times news corpus show the effectiveness of our model.
 H.3.3 [ Information Search and Retrieval ]: Clustering; H.3.3 [ Information Search and Retrieval ]: Retrieval models; H.2.8 [ Database Applications ]: Data mining Algorithms, Experimentation Topics on Participations, Community discovery, Nonpara-metric statistical model, Hierarchical Dirichlet Process
When we talk about linked document data, there are two different types of links. Link of the first one, which we call d2d-link , indicates connectiveness between different docu-ments. These kind of data include blog reference data and research paper citation data. Link of the other type, which we call u2u-link , represents co-occurrence or simultaneous participations of different users in one document. Usually there are more than one user in each document of this data, so we also call it Multiple User Data (MUD). Email archives and research paper co-authorship network fall into this cat-egory. Figure 1 gives a sample of a research proceeding cor-p us, where u 1 , , u 4 denote four researchers, and d 1 denote three research papers with different colors indicating different research areas. Many methods of community dis-covery in d2d-link data have been proposed and gain satis-factory results. But as to u2u-link data, rare solutions which combine link information and textual content for community discovery exist. Current approaches either make no use of the textual content of the documents or make oversimplified assumptions about the users as well as the textual content. Figure 1: Different views of u2u-link graph(best v iewed in color)
One part of traditional methods of community discovery in u2u-link data are performed solely on a u2u-link graph in which the vertices represent the users and the edges in-dicate links between pairs of authors. A community is typ-ically defined as a subset of users with better connectivity amongst its members than between its members and other users of the graph. The task of community discovery is then to recover such subsets from the given graph. Most of these approaches, including graph cut based methods[ 21 ], m odularity based methods[ 16 ], flow based methods[ 6] and s pectral based methods[ 20 , 1], typically choose an objective f unction which captures the above intuition of a community and then try to optimize the objective function[ 9]. These m ethods only model the links by assigning a certain weight Figure 2: A u2u-link graph and its corresponding p articipation graph(best viewed in color) to each link between pairs of users, as the co-authorship net-work shown in Figure 1(a); though widely applicable and a ccepted, they make no use of the textual content of the documents and thus give no clue how different authors par-ticipate in the documents and what semantics these partici-pations imply. Though some other methods take the textual content into account, they make oversimplified assumptions and thus ignore useful participation information. The Net-PLSA model[ 15 ] constructs the u2u-link graph as described i n Figure 1(a), merges all documents one user participates i n into a single document for that user. Thus NetPLSA ignores the various participation information for each user. The Author-Topic(AT) model[ 22 ] learns the topic of a doc-u ment conditioned on the mixture of authors(users) of that document, which implicitly assumes that the distribution over topics of a certain author will not change for all his/her documents. This assumption seems to suffice but it may not fit some real cases such as a researcher switches to another research area and publishes research papers with different topics. A better modeling may exists.

In particular, we propose a Topics-on-Participations(ToP) model for community discovery in u2u-link data. First, we transform the u2u-link graph to an equivalent participation graph for clarity, as illustrated in Figure 2. To better exploit t he multi-author attribute of each document in the data, we place topical variables on each participation each author get involved in. More specifically, given the u2u-link data, ToP leverages Hierarchical Dirichlet Process(HDP) to introduce hidden topical variables of the participations. Other than the use of participation structure, ToP can look into the documents of the participations in detail to recover sound communities. We first generate the hidden topic variables of the participations and then documents are generated by these according to the hidden variables. Besides, ToP is a nonparametric model, which is a natural outcome of HDP. The setting of community number is a common problem for most of the community discovery algorithms proposed so far. Due to the nonparametric property, ToP is able to au-tomatically select the proper community number according to the observed data.

We evaluate ToP on two real-world datasets, a research proceeding corpus and a New York Times news corpus. The research proceeding corpus consists of paper abstracts from 7 research conferences, i.e. ACL, ICML, SIGGRAPH, SIGIR, SIGKDD, SIGMOD, and WWW during 2005-2009. In total there are 9415 individual authors and 5308 documents. And the New York Times news corpus contains business news mentioning selected sets of companies. As the whole corpus is large, we collect a subset of the corpus, which contains 1677 companies and 2461 documents. Quantitative evalua-tion on both two datasets shows that ToP outperforms two related community discovery algorithms. Deeper analysis shows that ToP can not only identify the proper number of communities, but also reveal reasonable semantic interpre-tation for every discovered community.

The main contribution of this paper is two-fold:
The remainder of this paper is organized as follows: Sec-tion 2 introduces some related work. Section 3 gives some d efinitions and formally define the community discovery task. Section 4 presents ToP model in detail. Experimental results a s well as some case studies are presented in Section 5. At l ast, we conclude the paper and discuss about future work in Section 6.
Much work has been done on community discovery. In this section, we review two lines of work: community discovery in d2d-link data and community discovery in u2u-link data.
In the context of d2d-link data, probabilistic models, such as Latent Dirichlet Allocation(LDA)[ 3] and its variations, p lay an important role in uncovering underlying topics from textual data. Zhou et.al [ 26 ] successfully extract e-communities b ased solely on the content of communication documents. Their work gives us a strong support in mining text con-tents for community discovery. With the aim of taking into account the link information among documents, serveral methods have been proposed. Li et al. in [ 10 ] design a so-p histicated model for scalable community discovery on tex-tual data with relations. Their model explores the d2d-link graph to detect some community cores and then uses text information to improve community consistency. Cohn and Hofmann combine PLSA and PHITS together and derive a unified model from text contents and citation information of documents under the same latent space [ 4]. Erosheva et al. a pply similar mixed membership model [ 5] for soft clustering o f papers by using text content and references. Liu et al. in [12 ] propose the Topic-Link LDA model with the intuitive t hat a link between two documents is determined by both content similarity and author community membership sim-ilarity and gain improved results against previous methods. All these approaches captures several aspects of the d2d-link data, but they may be not suitable for community discovery in the u2u-link data.
In the past few decades, many models have been proposed for community discovery in the u2u-link data, but most of these methods are based solely on the u2u-link graph and m ake no use of the textual content of the documents. Note here, as these methods take into account only the link infor-mation and do not consider textual content , they can also be applied to the above mentioned d2d-link data, depending on how a  X  X ser X  is defined.

Traditionally, community discovery is performed by par-tition of an u2u-link graph with the aim of optimizing a selected objective function. Many methods have been pro-posed along this direction, e.g., graph cut based methods, flow based methods, modularity based methods, spectral clustering based direction, etc. Graph cut based methods, including NCut[ 21 ], try to find an optimal graph partition w ith the edge weight between partitions minimized or edge weight inside a partition maximized. Due to the NP-complete complexity of this method, approximate solutions have been proposed. Flake et al.[ 6] propose approximate algorithms b ased on network flow ideas to partition the network by solving maximum flow problems, where they define commu-nity as a set of entities that has small inter-community cuts and large intra-community cuts. Girvan and Newman[ 7] in-t roduce betweenness centrality to detect communities. After that, Newman and Girvan[ 16 ] introduce m odularity to mea-sure the overall quality of discovered communities. Modu-larity evaluates how entities in a community connect with other entities in that community and has been adopted by many community detection literature. Modularity can be optimized by using the eigenvectors of the modularity ma-trix which gives rise to those spectral clustering based meth-ods[ 20 , 1]. McCallum et al. in [ 14 ] study a new community d iscovery task on u2u-link data. Instead of finding densely connected entities, they seek to find out users with similar connection pattern such as similar voting patterns.
Another part of previous work analyzing u2u-link data is based on statistical models[ 18 , 8, 11 ]. In these models, a l atent variable is introduced for each entity to express its po-tential characteristic and then the models generate the links between them according to the latent variable assignments which are most likely to recover the graph structure[ 18 ]. K emp et al. in [ 8] extend these models to infinite latent c lasses. The model can automatically determine the proper latent class number based on the observed graph structure. However, it restricts each entity to only one latent class. Additionally, the discovered class/group in these models is a set of entities which have similar connectivity pattern in the graph, which is quite different from the traditional defini-tion of community, i.e., a densely connected subgraph of the whole network. Because of their general modeling property, these algorithms have been widely adopted, such as biology data analysis [ 24 , 19 ], co-authorship network analysis [ 17 ], W eb community identification [ 6]. Although there are many o ther great studies in this direction, we do not give a de-tailed review for each of them because they do not utilize the text information to enhance the community discovery.
In the context of u2u-link data, there have also been some related studies aiming to take advantages of both text con-tents and the u2u-link structure. Most of these algorithms perform community discovery on a u2u-link graph with text user profiles. In [ 15 ], Mei et al. propose a regularized model N etPLSA for discovering more smoothing topics/communities over u2u-link graph by constraining that connected docu-ments that have similar topic distributions. The Author-Topic(AT) model uncovers topics conditioned on the mix-ture of authors that composed a document[ 22 ]. McCallum et a l. modeled email archives by a generative process [ 13 ]. The p roposed Author-Recipient-Topic(ART) model gives each pair of author and recipient a topic distribution instead of a topic distribution per-author. Though their modeling set-ting shares some similarity with ours, their model is specif-ically designed for email archives. Different from the above models, our method tries to handle the setting where the text contents are usually associated with more than one user and we aim to quantify the effect of each user X  X  participation contribution to the documents X  topical distribution.
In this section, we formally give some definitions and de-fine the task of community discovery from u2u-link data.
Definition 1. (Participation Graph) : A participation graph G = ( U  X  D ,E ) is a bipartite graph where the set of users U and the set of documents D are two disjoint sets. An edge e  X  E connecting user u  X  U and document d  X  D indicates that u participates d and we denote this participa-tion with e . So we can construct an equivalent participation graph from every u2u-link data. As we see in Figure 2, we t ransform a participation graph from the sample u2u-link graph.

Traditionally, a community is a group of users with dense interactions amongst its members than between its members and the remainder of the u2u-link graph. While in our set-ting, since we have two information sources for community identification, we need to expand the community definition to incorporate both of them, i.e. the topical community.
Definition 2. (Topical Community) : A topical com-munity is a soft partition of the users with a multinomial word distribution over the vocabulary V . We denote the topical community space as  X   X  . The soft partition means that each user is assigned to each of the communities with a membership weight, i.e. a community distribution. This modeling is natural since it is always the case that a user can belong to multiple communities.

Definition 3. Task (Community Discovery) : Given an participation graph G , the task of Community Discovery is to find a set of topical communities { c 1 ,c 2 ,...,c the community number K is detected automatically and to calculate the community distribution of each user u , i.e.  X 
Briefly speaking, our model looks into the participations, place topical variables on participations and analyzes the la-tent topics of the documents involved in those participations. In this section, we will introduce our model, the inference method and some basic analysis as well.
Our Topics on Participations(ToP) model first utilizes Hi-erarchical Dirichlet Process to introduce latent topic vari-ables to each user participation of a document, which forms the modeling for the participation graph part. Then, for the document part, we specify the generation process of a doc-ument via a simple topic modeling technique. Finally, we combine the two separate modeling by assuming that they s hare the same latent topic space.
 Figure 3: Separated graphical model of ToP. (a) m odels the participation graph, (b) models the doc-uments.
 Figure 4: Combined graphical model of ToP for the s ample graph in Figure 2 (b)
P articipation Graph Modeling. From the participa-tion graph view, we may consider that each user u  X  X  partic-ipation e u,d in a document d is an evidence indicating the community for the user. In our model, we choose to make it explicit by setting a topical variable for each participation. We call these variables evidence variables . Since the commu-nity number is typically not known beforehand, e u,d should be a draw from an infinite semantic community space, which will allow the model to learn the community number through the data. Here, we utilize Hierarchical Dirichlet Process to assign evidence variables and build our model.
 Figure 3(a) shows the graphical model of our Participation G raph Modeling, where M is the number of users and | D ( i ) | is the number of documents linked to user i . The outside global Dirichlet Process provides a shared infinite number of variables for all the users X  evidence variables. This infinite space brought the non-parametric manner, which enables the model to learn topic number according to the real data. Formally, the Dirichlet process is where B is the base measure and  X  g is a positive real number called the concentration parameter .

The inside local Dirichlet Process models all participation of a single user in his/her documents. This models the clus-tering property of a user X  X  participation in documents, i.e. if most publications of a user are related to information re-trieval, the next paper of the user will most probably to be about information retrieval again. Formally, where  X  l is a positive real number, called concentration pa-rameter . Then, every user X  X  evidence variables are drawn from the infinite space  X   X  as follows: where  X  i,j h denotes the h th evidence variable of U i , and  X  is the atom function. This actually forms a two-layer Hierar-chical Dirichlet Process. The specific modeling details and reason of modeling choices can be found in [ 23 ].
D ocument Modeling. To explore the semantics of the text documents, topic modeling is a good choice like PLSA and LDA. Similarly, we set a hidden document model for each document and assume that all the words in that doc-ument are drawn from the hidden model. Moreover, since each document is generated by one author or a group of au-thors, we add a layer that chooses a certain author X  X  topic. Figure 3(b) shows the graphical model of our Document denotes the number of topic variables connected to  X  j . On the top,  X  s are the topic variables of all the authors of the document, which are draws from the infinite space  X   X  Then,  X  is the weighting vector parameters for the topic se-these variables is as where  X  ( h ) ,j denotes the h th evidence variable that connects  X  . This setting captures the interaction between the au-thors of the same document by the probability feature that given  X  j =  X  , the more  X  ( h ) ,j having value  X  , the larger the probability of selecting  X  by  X  j . In other words, the topic variables linked to  X  j tend to agree with each other. Then, we assume that all the words in that document are drawn from the topic model, as the lower part of the Figure 3(b). T he joint probability of a document model  X  j and its gen-erated words W j is: where  X  j is a draw from an infinite semantic space  X   X  .
Note that, since the modeling focus is to recover the topic of the links between users, we choose this simpler method (comparing to LDA and PLSA) to model the documents. In fact, this method has also been widely used in language model for information retrieval and naive bayes classifiers because of its light weight yet effective. Combination of Participation Graph Modeling and D ocument Modeling. Considering that the community space  X   X  and the document model space  X   X  are both se-mantic spaces, we simply unite them into a single space,  X   X  . Now, the whole modeling process is finished. To make it clearer, the whole model is shown in Figure 4. Note that t he model can not be simplified into box representation and we have to expand the variables. Figure 4 just gives a sample a ccording to the participation graph in Figure 2(b). Table 1: the symbols which will be used in model inference M the number of users N the number of documents W j the j t h observed document D s ( i ) the set of single-user documents linked to user i D m ( i ) the set of multi-user documents linked to user i U ( j ) the set of users linked to document j
T ( i ) the set of tables in restaurant i t i ,j table index of customer ( i ,j ), i  X  U ( j ), j  X  D ( i ) k t i ,j the dish serving on the table that t i ,j refers k i ,t t  X  T ( i ) , the dish on the t th table in restaurant i d j the mixture component (i.e. dish) index of multi-m i ,k number of tables in restaurant i s erving dish k n i ,t,k the number of customers in restaurant i , sitting at var . a marginal of the variable v ar , e.g. m i,  X  denotes the var the set of all v ar , e.g t denotes the table indices of var  X  s all the v ar set excluding the upper scripted one,
We employ Gibbs sampling for model inference [ 2]. Firstly, w e derive two likelihood expressions for the convenience in the inference derivation later. The conditional density of W j under mixture component k given all other observed documents is: where Mul () denotes multinomial distribution. And the selection probability distribution: We have set all  X   X  X  to be uniform so that these parameters can be omitted. Further note that we will suppress refer-ences to the variables in the condition part of a conditional probability except those will used in the probability expres-sion.

There are three sets of hidden variables that need to be sampled: table indices t of customers, mixture component indices d of documents, and dish indices k of tables. We derive the Gibbs sampling expression for each of them.
Sampling t . The variable set t should be split to two sets because they are different in sampling. Firstly, if the docu-ment j is a single user document, t i,j is sampled by combing the likelihood of generating the observed documents. p ( t i,j = t | t  X  i,j , k ) (8)  X  ( n where p ( W j | t  X  i,j ,t i,j = t new , k ) is the liklihood for t t If the sampled value of t i,j is t new , we need to obtain a Secondly, for t of multi-user documents, we have a similar sampling process but the likelihood function is replaced by the selection function in Equation 7. p ( t i ,j = t | t  X  ij , k , d ) (11)  X  n where p ( d j | t  X  i,j ,t i,j = t new , k ) is: And in the case of choosing t new :  X 
Sampling d . These variables relate only to the selection processes and the multi-user document likelihood. They are thus sampled as
Sampling k . Since changing k i,t will change the mixture components of all the t i,  X  , the sampling relates to both the selection likelihood and the document likelihood of these variables. So the Gibbs sampling expression is: p ( k i ,t = k | t , k  X  it ) (15)  X   X   X   X   X   X   X   X   X   X   X   X   X   X 
Computation Complexity . The computation burden mainly lies in the Gibbs sampling inference part. Due to the sequential sampling process for the unobserved variables t , k , and d , the computation complexity of Gibbs sampling is linear to the total number of the hidden variables. The size of d is the number of documents in C 1 = D m ( ), The size of t is the number of edges in the bipartite link graph C 2 = P N j =1 U ( j ). The size of k can be estimated by its computation complexity is about O ( C 1 + C 2 + C 3 ).
Parameter Setting Analysis . Although the model is nonparametric, there are still three parameters, two DP con-centration parameters  X  l and  X  g , and one parameter  X  in the Dirichlet distribution that is set as the base measure for the global DP. The three parameters can be divided to two groups.  X  l and  X  g control the prior probability of generating new community. Since we have no prior information about the number of communities, they are set to small values, such as 0.1  X  1 for alpha l , 1  X  10 for alpha g .  X  controls the impact from the text contents. In typical topic model-ing studies,  X  is always set to around 0.01 so that the text contents can dominate the topic modeling process. While in our setting,  X  actually controls the balance between the information of text contents and the information of graph structure. The smaller  X  is the larger the impact of text contents is. We seek to reduce some impact of the text con-tents so that the graph structure can play a more important role. Therefore, we should set  X  to a little larger than the usual setting 0.01. From the experiments, we find that 0.01  X  0.05 is a suitable interval.

Parameter Estimation . After model training, there may be two sets of mixture components. One set of mix-ture components are referred by the documents, i.e. either d ,j  X  D m ( ) or t  X  ,j ,j  X  D s ( ) will refer to them. We call them MC ref for convenience. Accordingly, the others have no referred documents. We call them MC nref . MC ref is influenced by both document contents and graph structure. While MC nref has no influence on the contents. It is orig-inated from the setting that HDP is placed solely on the graph structure, but the text contents information may only agree with the graph information on a subset of the mixture components brought in by HDP. Since MC nref contains no text information, we drop them. The mixture components in MC ref are remained as the final community space. They can be reconstructed by the posterior expectation of them as  X  And the community distribution  X  i of a user i is estimated from the community assignment variables d j ,j  X  D m ( i ) and PAPER 9415 5308 25034 2.7
NYT 1677 2461 83367 49.7 t i ,j ,j  X  D s ( i ) as
To test the effectiveness of the proposed ToP model, we apply it to community discovery on two data collections we crawled from the web, a research proceeding corpus and a New York Times news corpus. We perform parameter sen-sitivity analysis, followed by community semantic analysis. We also compare it with state-of-the-art community discov-ery algorithms, NCut and NetPLSA, in the experiments.
We mainly use the research proceeding corpus to evaluate the detailed performance of our model. The dataset contains the abstracts of all papers from 7 research conferences, i.e. ACL, ICML, SIGGRAPH, SIGIR, SIGKDD, SIGMOD, and WWW, between 2005 and 2009. In total, there are 5308 papers and 9415 individual authors. 11913 unique terms appear at least once in the dataset after we preprocess the data by removing common stop words and stemming. We call this corpus PAPER.
 ToP model is not limited to discover communities of users. The New York Times news corpus is collected to verify the model X  X  general applicability. We collect a set of companies and their news articles from New York Times. The whole c orpus is quite huge. We build a subset of the complete cor-pus for experiments. The dataset consists of all the articles that mention about at least 3 companies. And hereafter we refer to it as NYT. Table 2 shows some statistics of PAPER a nd NYT. All the above datasets are publicly available.
Given two users X  community membership distributions, we use the Categorical Clustering Distance(CCD)[ 25 ] to com-p are the quality of these two distributions. This method relates only to users X  community membership distributions. So it can be applied to evaluate different algorithms that generate user community distributions with different com-munity dimensions.

Given a test data set consisting of a set of users U T = { u i | 1  X  i  X  M } with their ideal community membership distributions {  X  i | 1  X  i  X  M } and their computed commu-nity membership distributions {  X   X  i | 1  X  i  X  M } . The CCD between  X  and  X   X  is given by all k,j , where K and J are the number of communities in  X  http://topics.nytimes.com/topics/news/business/ c ompanies/index.html and  X   X  , respectively. Linear programming is used to compute equation ( 18 ).CCD is nonnegative and equals zero if and o nly if the two distributions are identical. The smaller the value is, the closer those two distributions get to each other.
According to the definition of community in our setting, we build ideal community membership distributions for the above datasets. For the PAPER corpus, we treat each con-ference as a community and the proportion of the number of papers one author published in each conference as the ideal probability the author belongs to that community. For NYT, those companies are categorized into 10  X  X ectors X  ac-cording to New York Times, such as technology, basic mate-rial, finance etc. So we treat the categorization as the ideal community membership for those companies.

Two models are selected as baseline for comparison. One is NetPLSA and the other is NCut. Since the two algo-rithms both run on user link graph, we need to first build such a graph with weights on the links. The weight of a link connecting u i and u j is set to be the number of edges that connects u i and u j . As these two models are parametric models, the number of communities are set manually. For PAPER, the number of communities of NetPLSA and NCut are both set to be 7, i.e. the number of conferences in the corpus for the reason that these conferences focus on dif-ferent subjects. This setting method is the same as what the authors of NetPLSA did in their original work. And to NYT, we set the number of communities to be identical to the number of  X  X ectors X , i.e., 10, of those companies. For the parameter  X  in NetPLSA, we choose the lead-to-optimal-result one for 9 different settings raging from 0.1 to 0.9 with step 0.1.
We examine different combinations of  X  l ,  X  g and  X  in the experiment. For each combination of the parameters, we run the inference for 100 iterations. We present the influence of  X  ,  X  g and  X  in Figure 5.

F rom Figure 5, we see that the number of communities d etected by our model is not quite sensitive to parameter changes. The number of communities detected from both PAPER and NYT are around 8. Section 5.6 gives explana-t ions why they are not precisely equal to the ideal commu-nity number(mainly because of noise). Though not exactly perfect, the number of communities our model detects show the ability of automatically determining the proper number of communities. On the PAPER dataset, the evaluation result of NCut CCD = 1351 . 02, and we list the evaluation results of Net-PLSA regarding to its parameter  X  in Table 3. As we see f rom Table 3, the best CCD value of NetPLSA is 1408.9, w hich outperforms NCut, so from now on we choose the best evaluation result of NetPLSA as comparison baseline for ToP. In our experiments, we also test different sets of parameters of ToP with  X  l varying from 0.1 to 0.5,  X  g from 1.0 to 5.0 and  X  from 0.01 to 0.20. We show part of the de-tailed evaluation results of ToP with respect to  X  l ,  X  g  X  in Figure 6.

W ith  X  l = 0 . 2,  X  g = 5 . 0 and  X  = 0 . 05, we get the mini-mum value of CCD = 1219 . 65 giving a maximum improve-Table 4: Best CCD of NCut, NetPLSA and ToP ment over NetPLSA as
We pick the configuration of  X  l ,  X  g and  X  which lead to the lowest CCD. Under that configuration, our model detects 8 communities and we make statistics of papers from each conference as shown in Figure 7. Note here that the c ommunity membership of a paper is set to be community assignment in the last iteration of Gibbs sampling. We are unable to get this result by using NetPLSA due to its coarse grained modeling of the documents. The horizontal axis represents the communities and the vertical axis represents the percentage of papers from each conference. We also list the top 10 terms for each community discovered by ToP and NetPLSA in Table 5 and Table 6, respectively.

W hile on the NYT dataset, we only show the best CCD results of NCut, NetPLSA and ToP as shown in Table 4. T oP outperforms both NetPLSA and NCut on NYT. Be-sides, from the results on NYT, we can see that NCut even outperforms NetPLSA for the reason that it constructs a densely-connected graph on NYT and that NetPLSA ne-glects too much important network clues to discover proper community structures.

Because ToP models more detailed participation informa-tion than NetPLSA, ToP takes longer time to output the community distribution results than NetPLSA does. On the PAPER corpus, NetPLSA takes about 5  X  10s per iteration while ToP takes about 10  X  30s per iteration on a daily workstation depending on the parameters.
From Figure 7, we see that our model discovered 6 major c ommunities and 2 minor communities. We examine the 6 major communities first. Considering both the percentage of papers from each conference appearing in each community and the top 10 terms of every community listed in Table 5, i t is easy to see that c 1 well corresponds to the information retrieval community, c 2 is closely related to computer graph-ics, c 3 is mainly about data mining, c 4 covers the database community, c 5 mainly concerns about computer linguistics, c is closely related to machine learning. Note that papers from WWW scatter around several communities, which is quite reasonable because the WWW conference covers top-ics in information retrieval, data mining, data management, etc. As to the 4 minor communities, c 7 and c 8 both contain only 1 paper. After an investigation of the data, we find that the authors of the above 2 papers have no co-authorship with the rest authors in the dataset and that the contents of these papers are very dissimilar from others. It is the above two reasons that prevent our model from determining the same number of communities as the ideal one. Let us take the pa-per belonging to c 7 as an example. The Portinari project: IR helps art and culture , a demo paper authored by Jo  X ao Can-dido Portinari in SIGIR X 05, demonstrates a cultural project of collecting and distributing all works of the Brazilian artist Candido Portinari and shows how IR helps accomplish this c Table 7: Community membership of several active authors W. Bruce Croft  X   X   X   X  J amie Callan  X   X  C hengxiang Zhai  X   X   X   X  J ames Allan  X   X   X   X  A ndrew McCallum  X   X   X 
K un Zhou  X   X   X  C G: Computer Graphics, DB: Database, DM: Data Mining project and spread the Brazilian culture. The author is actu-ally an isolated point in our co-authorship network. Another example from community c 8 , The New Economy -An Engi-neer X  X  Perspective , a keynote by David Brown in WWW X 06, tells how mobile communication &amp; computing changes peo-ple X  X  daily life and the economy. Also, this author is another isolated point in our co-authorship network.

ToP figures out the community each author gets involved in. Table 7 shows the community membership of several ac-t ive authors in PAPER. A  X  indicates the probability of an author belonging to a community is above some threshold.
From the above analysis, we see that with the number of communities determined automatically, our model has the ability of discovering semantic communities.
This section gives some case studies in the PAPER dataset to show the semantic community discovery ability of ToP. We investigate the PAPER dataset by calculating all the connected components of the co-authorship network and find that there are on average 184 connected components inside the scope of each conference. Also the number of authors who publish papers in more than 2 conferences is 1187. The above two observations imply that the loosely connectd se-mantic related scenario and densely connected semantic un-related scenario is common in the PAPER dataset. Thus we show two case studies in each case respectively.
For this scenario, we take authors with similar research interests but with neither direct nor indirect co-author rela-tionships to examine our model. That is, they publish paper in the same conference but in different connected compo-nents. In this case, we have the following example.
Andrew McCallum and Thomas Hofmann are in two un-connected subgraph in our bipartite graph. Andrew Mc-Callum published nine papers covering topics on data min-ing in SIGKDD and Thomas Hofmann also contributed one SIGKDD paper Non-redundant clustering with conditional ensembles to our dataset. Results of our model successfully assign most of Andrew McCallum X  X  SIGKDD papers and Thomas Hofmann X  X  SIGKDD paper to the same community( c 3 in Figure 7, in which SIGKDD papers make a majority). T herefore, our model can successfully merge loosely con-nected semantic related communities.

In this scenario, NCut fails to recover this semantic com-munity because it takes no text information into considera-tion.
For this seenario, we take authors who have co-author re-lationships but their co-authored papers are about different research topics to examine our model and we have the fol-lowing example.

Jerry Scripps, Pang-Ning Tan and Abdol-Hossein Esfaha-nian together contributed a paper Measuring the effects of preprocessing decisions and network forces in dynamic net-work analysis in SIGKDD X 09, while Haibin Cheng and Pang-Ning Tan together contributed a paper Semi-supervised learn-ing with data calibration for long-term time series forecast-ing in SIGKDD X 08. It is necessary to mention that there are no records in the PAPER corpus concerning the above 4 authors except these two papers. Therefore, authors of the first paper are densely connected to each other and the same to the authors of the second one. These two author groups share a common vertex (Pang-Ning Tan). Results of ToP assign c 3 (Data mining) to the SIGKDD X 09 paper and c (Machine learning) to the SIGKDD X 08 paper, for the rea-son that the SIGKDD X 08 paper mainly focuses on proposing a new learning method. The above analysis shows that our model has successfully split the two densely connected com-munities whose semantics are not quite related.

While in this scenario, we are unable to recover such de-tailed community of each paper using NetPLSA due to its coarse grained modeling of the documents by simply merge the above two papers of Pang-Ning Tan into a single docu-ment.
A lthough community discovery techniques have been de-veloped for decades, there is not much work done in devel-oping general algorithms for u2u-link data while considering textual content information. This paper proposes a princi-ple solution. The main contributions of this paper can be summarized as the following. 1). The proposal of a Topics on Participations(ToP) model for community discovery in u2u-link data. ToP makes it possible for mining detailed information from each partic-ipation. Moreover, the proposed model not only captures the structure information induced from the u2u-link graph but also provides a method for automatic selecting proper number of communities. 2). The extensive evaluation on two real-world datasets, which verifies the effectiveness of the proposed model. ToP can automatically detect the proper number of communi-ties, provide a reasonable interpretation of the discovered communities, and outperform the baseline models in mining user community distribution.
 There are several potential future directions of this work. First, we will try some other document modeling e.g. PLSA and LDA. Second, we will develop a parallel solution to im-prove the scalability of our algorithm. Last, by taking tem-poral dimension into consideration, studying how communi-ties evolve over time also deserves a try. [1] R. Anderson and K. Lang. Communities from seed [2] C. Andrieun, N. de Freitas, A. Doucet, and M. I. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] D. Cohn and T. Hofmann. The missing link -a [5] E. Erosheva, S. Fienberg, and J. Lafferty.
 [6] G. W. Flake, S. Lawrence, and C. L. Giles. Efficient [7] M. Girvan and M. Newman. Community structure in [8] C. Kemp, T. L. Griffiths, and J. B. Tenenbaum. [9] J. Leskovec, K. J. Lang, and M. W. Mahoney.
 [10] H. Li, Z. Nie, W.-C. Lee, C. L. Giles, and J.-R. Wen. [11] Y. Lin, Y. Chi, S. Zhe, H. Sundaram, and B. L. [12] Y. Liu, A. Niculescu-Mizil, and W. Gryc. Topic-link [13] A. McCallum, X. Wang, and A. Corrada-Emmanuel. [14] A. McCallum, X. Wang, and N. Mohanty. Joint group [15] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic [16] M. Newman and M. Girvan. Finding and evaluating [17] M. E. Newman. Coauthorship networks and patterns [18] K. Nowicki and T. A. Snijders. Estimation and [19] G. Palla, I. Derenyi, I. Farkas, and T. Vicsek. [20] J. Ruan and W. Zhang. An efficient spectral algorithm [21] J. Shi and J. Malik. Normalized cuts and image [22] M. Steyvers, P. Smyth, M. Rosen-Zvi, and [23] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. [24] D. M. Wilkinson and B. A. Huberman. A method for [25] D. Zhou, J. Li, and H. Zha. A new mallows distance [26] D. Zhou, E. Manavoglu, J. Li, C. L. Giles, and H. Zha.
