 A large number of mainstream applications, like temporal search, event detection, and trend identification, assume knowledge of the timestamp of every document in a given textual collection. In many cases, however, the required timestamps are either unavailable or ambiguous. A charac-teristic instance of this problem emerges in the context of large repositories of old digitized documents. For such doc-uments, the timestamp may be corrupted during the digiti-zation process, or may simply be unavailable. In this paper, we study the task of approximating the timestamp of a doc-ument, so-called document dating. We propose a content-based method and use recent advances in the domain of term burstiness, which allow it to overcome the drawbacks of pre-vious document dating methods, e.g. the fix time partition strategy. We use an extensive experimental evaluation on different datasets to validate the efficacy and advantages of our methodology, showing that our method outperforms the state of the art methods on document dating.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval burstiness; language models; temporal similarity
Temporal text mining is at the core of a large number of mainstream applications. The input to such applications consists of a collection of timestamped documents. The tem-poral dimension can be used for, among others, event detec-tion [1], document search [6], and text summarization [8].
The assumption made by all such applications is that the timestamp of each document is both available and accu-rate. In practice, however, this assumption may be false. A characteristic instance of this problem emerges in the con-text of large repositories of old digitized documents. Such repositories are becoming increasingly large and abundant, due to initiatives such as The National Digital Newspaper Program 1 . For such documents, the timestamp may be cor-rupted during the digitization process, or may simply be unavailable.

In this paper, we propose a purely statistical method based on document content and burstiness for approximating the true timestamp of a given document in the context of very large timestamped document collections. We address the problem by considering two main factors, (i) lexical similar-ity and (ii) burstiness. The first factor captures the intu-ition that similar documents are more likely to discuss the same topics and events, and are thus more likely to be asso-ciated with adjacent timestamps. The second factor builds upon the intuition that bursts in term frequency capture the trends in vocabulary usage during the corresponding time-frame and can thus prove useful in document dating.
When an event takes place (e.g. an earthquake, sports finals), the event X  X  characteristic terms (e.g.  X  X arthquake X ,  X  X hooting X ,  X  X vertime X ) appear more frequently in the me-dia. In the context of document dating, our intuition is that a timeframe when many terms of the query document are bursty is more likely to overlap with the document X  X  times-tamp. Our method is the first to utilize temporal informa-tion through a burstiness-aware approach, without depend-ing on specific language rules, datasets, or meta-information.
Our contributions can be summarized as follows:
The problem of document dating has proven to be difficult for the information retrieval community. The best published http://www.loc.gov/ndnp results do not achieve more than 50% precision, when esti-mating 1-year long intervals in corpora that span 10 years, whilst using purely statistical methods [2]. The underlying reason for this, is that not all documents contain temporal information, which makes a percentage of the corpus useless for testing and training purposes.

Previous approaches addressed the problem of document dating by identifying linguistic constructs with a clear tem-poral interpretation (e.g. the mention of the date or time). However, such tokens can be very sparse as well as ambigu-ous, referring to irrelevant timeframes. Another line of work overcomes this drawback, by considering the entire vocabu-lary of a document [3]. While this is a clear improvement, these methods are limited by their static consideration of the candidate timeframes, since they pre-segment the timeline into intervals of the same fixed length. A language model is then used to select the interval that is most likely to be the temporal origin of the query document.

Kanhabua and N X rv  X ag in [5] propose a document-dating method that extends the one proposed by de Jong et al. [3]. Specifically, the authors propose the application of semantic-based preprocessing of the reference collection and apply a term-weighting scheme. Chambers proposed a discrimina-tive model, using a Maximum Entropy classifier, as well as defining rules for processing temporal linguistic features, as year mentions in documents [2]. While this model outper-formed the methods proposed by de Jong and Kanhabua, it has the limitation that it only works well for year pre-dictions, because temporal linguistic features that refer to months or days are ambiguous. Moreover, in this study there were no running time experiments, an issue that we address in the current paper. In contrast with all of the above approaches, our approach has not such requirements and can handle intervals of arbitrary length. Let D be a collection of documents spanning a timeline of T = t 1 , t 2 , ..., t n of n discrete timestamps (e.g. days). Each document d  X  D is associated with exactly one timestamp t ( d )  X  T . Given a query document q /  X  D , for which the timestamp t ( q ) is unknown, our goal is to find the most Among other things, our approach considers the burstiness of the terms in the query document q . Given a term x  X  q , by B ( x ) we represent the set of non-overlapping bursty intervals for x .
Our approach considers (i) the lexical similarity of the query document with the documents in the reference collec-tion D (ii) the burstiness of the significant terms of the query document q , e.g., top-k terms ranked by tf-idf . The use of lexical similarity captures the intuition that similar docu-ments are more likely to discuss similar topics and events, and are thus more likely to originate in the same timeframe. In practice, however, similar documents may appear on dif-ferent timestamps across the timeline. We address this, by introducing term burstiness. When an event or topic is recorded in a textual corpus, its characteristic terms exhibit atypically high frequencies. We refer to these timeframes as bursty intervals . Our algorithm is orthogonal to the ac-tual mechanism used for computing non-overlapping bursty intervals. By identifying the bursty intervals of different terms, we can identify the timeframe of relevant events, as well as relevant documents that discuss them. A conceptual view of our approach is given by the example in Figure 1. In this case, the three documents d 2 , d 3 , d 4 will be selected by our algorithm, since they are both close to each other and overlap with multiple bursty intervals of the considered terms.
 Figure 1: How BurstySimDater identifies the appro-priate timestamp for a given query document.

We refer to our algorithm as BurstySimDater . The pseu-docode is given in Algorithm 1.
 Algorithm 1 BurstySimDater Input: reference corpus D , bursty intervals B , query docu-ment q , max timeframe length ` Output: timeframe of q 1: S  X  top-k most similar documents to q from D 2: W S  X  X  X  3: for d  X  X  do 4: w d  X  0 5: Y  X  d  X  q 6: for x  X  X  do 7: w d  X  w d + |{ I  X  X  ( x ) : t ( d )  X  I }| 8: w d  X  w d / |Y| 9: W S  X  W S  X  X  w d } 10: A S  X  ( d  X  X  , W S ) 11: I  X  GetMax ( A S , ` ) 12: Return I
The input to the algorithm consists of the query document q , the reference corpus D , the set of precomputed bursty intervals B and the upper bound on the reported timeframe ` . The output is an interval of length at most ` , within the timeline T spanned by D .

First, the algorithm retrieves the top-k most similar docu-ments to q from D . In our own evaluation, we experimented, among others, with the tf-idf measure and the Jaccard sim-ilarity. We use the latter in the experimental section of this paper, since it led to the best results. We refer to the re-trieved set of the k most similar documents as S .

In steps 2-9, we assign a weight w d to each document in d  X  S , based on its overlap with the burstiness patterns of its terms. Initially, w d is set to zero. Let Y be the overlap of d  X  X  vocabulary with the vocabulary of the query document q . For each term x  X  X  , let B ( x ) be the precomputed set of bursty intervals for x . We then increment w d by the number of the intervals from B ( x ) that actually contain t ( d ). After the iteration over all terms in Y is complete, we normalize w d by dividing it by |Y| . Conceptually, the weight w d of a document d is the average number of bursty intervals that it overlaps with, computed over all the terms that it has in common with the query q . The computed weights are kept in the set W S . We want to identify the interval when the most terms from the top-k similar documents are simultaneously bursty. This period is the intersection of intervals with the maximum sum of weights. To do this, in steps 10-11, we create an array A S of size T , where cell i equals to the sum of weights w d for all documents d  X  S that were written at t i . Next, we find the interval I of length ` with the maximum sum. By tuning ` , we tune the level of desired accuracy. In order to compute the sets of bursty intervals we use the GetMax algorithm [6]. Given a discrete time series of frequency measurements, GetMax returns a set of non-overlapping bursty intervals with respect to the frequencies.
For our experimental evaluation we used three real-world news datasets, each of them being a chronologically ordered sequence of documents. Table 1 describes each dataset in detail. Datasets 1 , 2 are parts of the New York Times dataset, datasets 3 , 4 are articles from The San Francisco Call newspaper and datasets 5 , 6 , 7 are articles from the web-site Topix.com , which host news articles from 181 countries. After POS tagging and Word Filtering for all competing methods, we kept only nouns, verbs and adjectives. After carefully examining the relevant literature on document dat-ing, we chose to compare with the following methods:
MaxEnt : The algorithm proposed by Chambers in [2] trains a discriminative version of a Maximum Entropy classifier. We used the MaxEnt classifiers from the freely available Stan-ford toolkit, as was done in the original paper.

NLLR : The algorithm proposed by de Jong et al. [3] and extended by Kanhabua and N X rv  X ag [5] initially splits the timeline to segments of fixed (and equal) length (e.g. weeks). It then uses temporal language modeling to compare the vocabulary between each query document and the segments, in order to choose the most likely segment.

In BurstySimDater experiments we used k = 10 most sim-ilar documents. Changing this parameter did not result in a big difference in precision. In MaxEnt and NLLR we used all unigrams features. As proposed in the respective papers [2, 5] and was validated in our experiments, performance of MaxEnt and NLLR is best when all features are used. We evaluate all approaches on each of the available datasets via a 10-fold cross validation, omitting the known timestamp of a query document.
In this experiment we applied the three methods on vari-ous random samples of increasing size from NYT10 , which http://catalog.ldc.upenn.edu/LDC2008T19 is the largest document collection. We experimented with various timeframe lengths ` = 1 , 6 , 12 months. Figure 4 de-picts the total running time for each method as a function of the sample size. X-axis illustrates the total size of the dataset, 90% of which serves as training-and 10% as testing-set. The total running time for NLLR includes partitioning of the dataset, indexing of the documents and building the lan-guage models for each partition. The total running time for MaxEnt includes the training of the Maximum Entropy Clas-sifier. The total running time for BurstySimDater includes indexing of the documents and computation of the bursty intervals for all terms in the corpus. Moreover, all running time values include the computation of the reported inter-vals for all testing documents.

As depicted in Figure 3 BurstySimDater achieves the best precision for all dataset sample sizes and all timeframe lengths ` . More importantly, in terms of total running time BurstySim-Dater scales much better than MaxEnt and is directly com-parable to NLLR . Due to the computational complexity of MaxEnt some of the experiments did not terminate in a rea-sonable amount of time.

Figure 2 illustrates the difference in total running times of the three methods for a multiple query experiment. More specifically, for each document three intervals of respective lengths ` = 1 , 6 , 12 months were desired. The reason for the depicted running time difference is that BurstySimDater al-gorithm does not need to repeat the indexing of documents and the computation of the bursty intervals in order to eval-uate the three different queries, whereas NLLR and MaxEnt need to re-partition the timeline into segments of length ` = 1 , 6 and 12 months. This is another benefit for not pre-partitioning the timeline into fixed-length segments. Figure 2: A multiple query experiment on a 60% sample of the NYT10 dataset yields the depicted total running time for the three approaches.
In this experiment we evaluate and compare the precision values achieved by BurstySimDater , MaxEnt and NLLR in all datasets and settings. In order to measure the precision val-ues as a function of (the length of the target timeframe) ` , we experimented on NYT10 , which is our largest dataset. Both MaxEnt and NLLR algorithms require a pre-segmented timeline in intervals of length ` . Our BurstySimDater algo-rithm has no such requirement. Instead, ` is provided as an upper bound of the reported timeframe. We tune ` so that the results of the competing approaches are directly com-parable. We evaluate the approaches for `  X  [4 , 12 , 24 , 48] weeks.

Table 2 contains all achieved precision values for all meth-ods. As described above, the experiments for MaxEnt algo-Timeframe length: 1-month , 6-months and 12-months . Timeframe length: 1-month , 6-months and 12-months . Table 3: Precision for 1 month in 1 or 2 year(s) rithm did not terminate in reasonable time for target time-frames of length ` = 4 and 12 weeks. BurstySimDater not only outperforms the state of the art methods in all time-frame lengths, but also this difference in precision increases as the number of candidate time intervals becomes larger (Figure 3, e.g. for target timeframe length ` = 1 month).
Table 3 depicts all precision values for ` = 1 month for all 1 or 2 year datasets. This experiment also demonstrates the challenging nature of the problem: while some of the doc-uments discuss specific events, others simply discuss topics that are not relevant to current events and can thus be as-sociated with any timestamp. BurstySimDater algorithm outperforms NLLR in all datasets for all values of ` , while achieving similar values to MaxEnt , which in turn has the scalability problems analyzed in Section 5.1. Another in-teresting observation comes from the results on the Top-ixCanada and TopixSAfrica datasets. For this corpora, the achieved precision values were significantly higher for all methods, reaching up to 63% and 84% respectively. This verifies our intuition that spatial information can be utilized to improve the results of our document-dating algorithm.
In this paper we introduced a new approach for docu-ment dating that does not depend on temporal linguistic constructs and can report timeframes of arbitrary length. In addition it clearly outperforms the state of the art in terms of running time complexity and precision, over a variety of real datasets. As future work, we will apply our approach to a document retrieval problem in the context of temporal search and leverage the determined timestamp of documents in building the temporal profile [4, 7] of a query in order to improve the overall retrieval effectiveness.
This work has been co-financed by EU and Greek National funds of the NSRF -Research Funding Programs:  X  X er-aclitus II fellowship, ARISTEIA -MMD X , the EU funded project INSIGHT and the ERC Advanced Grant ALEXAN-DRIA.
