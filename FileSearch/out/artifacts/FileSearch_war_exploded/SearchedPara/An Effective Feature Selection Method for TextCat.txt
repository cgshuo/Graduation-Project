 Text categorization [17] is the problem of automatically classifying the natural language text to predefined categories based on their content. Text categorization plays an impor-tance role in many applications, such as information retrieval [1] and Anti-Spam Email Filtering [5], etc.

In recent years, research interest in text categorization has been growing in ma-chine learning, as well as in information retrieval, computational linguistics, and other fields. Statistical machine learning methods have shown great benefit over rule-based approaches in text categorization [20]. The top-performing learning methods include Naive Bayes [14], KNN [20], Support Vector Machines(SVM) [10] and maximum en-tropy methods(ME) [15]. However, when t hese traditional machine learning methods are applied to large-scale text categorizati on, a major characteristic, or difficulty, is the high dimensionality of the feature space.

Amongst the above machine learning methods, KNN is the most sensitive to the di-mensionality of the data because KNN suffer s from two limitations. The first limitation is that KNN requires to keep all training instances and finds the nearest neighbors by searching the whole set of training instances. Therefore, the efficiency of KNN is very low when both the size and dimensionality of the training data are huge. The second limitation is that KNN is very sensitive to noise [6]. These two limitations show that KNN is low-efficient for large-scale and high-dimensional text data. Therefore, it is highly desired to reduce the dimensionality of the data by selecting the useful features and removing the noise.

The approaches for this purpose, dimensionality reduction, have attracted much at-tention recently since it make the text categor ization task more efficient and save more storage space [21,13,11,8].

In the text domain, the popular feature extraction algorithms include Document Fre-quency (DF),  X  2 statistics (CHI), Information Gain (IG), Mutual Information (MI) , etc [21] and Orthogonal Centroid Feature Selection(OCFS) [18].

The DF is to select the features with the first p largest document frequency. The basic assumption is that rare terms are either non-informative for categorization nor influen-tial in global performance. The  X  2 statistics measures the lack of independence between t and c and can be compared to the  X  2 distribution with one degree of freedom to judge extremeness. The orthogonal centroid feature selection (OCFS) [18] selects features optimally according to the objective functio n implied by the Orthogonal Centroid algo-rithm [9].
 These methods aim to remove noninformative features according to corpus statistics. The criterion used in these methods did not re late to the classification accuracy directly [3]. Besides, they did not consider the particularity of text categorization.
In this paper, we propose an optimal feature selection method, called KNNFS, in text categorization. KNNFS has two major charact eristics. One is that cosine similarity is taken in our feature selection criterion since it is usually used as similarity measure in text categorization; another is that our method adopts a criterion from the viewpoint of KNN. We calculate a score , called KNN score, for each feature and select the features with higher scores. Our experimental results show that our approach is better than the traditional FS methods, and it is also beneficial to other classifiers, such as Support Vector Machines (SVM).

The rest of the paper is organized as follows: Section 2 gives the review and analysis of the traditional feature selection methods in text categorization. Then we describe our optimal feature selection method, KNNFS, in Section 3. Experimental evaluations of our method and the other feature selection methods are presented in Section 4. Finally, we give the conclusions in Section 5. In this section, we review the representative feature selection methods, such as docu-ment frequency [21],  X  2 statistics [21] and orthogonal centroid feature selection [18], which are used as baseline methods in our experiments. 2.1 Definitions Firstly, we give some definitions used in this paper.

In text categorization, a corpus of documents is often mathematically represented by a d  X  n document matrix X  X  R d  X  n , which is generated by the traditional TF  X  IDF indexing in Vector Space Model (VSM) [16], where n is the number of documents, and d is the number of features (terms). Each document is denoted by a column vec-1 , 2 ,  X  X  X  ,d ) . X T is used to denote the transpose of X . The set of predefined categories is  X  =[  X  1 ,  X  X  X  , X  C ] ,where C is the number of the categories.

We formulate the feature selection pr oblem as follow: given a collection of n doc-uments X = { x 1 ,x 2 ,  X  X  X  ,x n } with labels Y = { y 1 ,y 2 ,  X  X  X  ,y n } and y i  X   X , ( i = 1 ,  X  X  X  ,n ) , the task of feature selection is to find a subset of features indexed by s 1 , 2 ,  X  X  X  ,p ) such that the low dimensional representation of original data x i is denoted 2.2 Three Baseline FS Methods Document Frequency(DF) Document frequency (DF) is the simplest feature selection technique for text categorization.

Document frequency is the number of documents in which a term occurs. We cal-culate the document frequency for each unique term in the training corpus. Then we select the features with the first p largest DF or remove features whose DF is less than a predefined threshold. The basic assumption is that rare terms are either non-informative for categorization nor influ ential in global performance.  X  2 statistics(CHI). CHI is aiming at maximizing a criterion J ( W ) . It is also a greedy algorithm to save the computation cost and thus is not optimal either. To a given term t and a category  X  c , suppose A is the number of times t and  X  c co-occur, B is the number of times the t occurs without  X  c , C is the number of times  X  c occurs without t , D is the number of times neither  X  c nor t occurs. The  X  2 statistics is: We can compute the  X  2 statistics between each unique term and each category in a training corpus, and then combine the cat egory specific scores of each term into two scores: The  X  2 statistics is known not to be reliable for low-frequency terms [7]. In this paper, we choose  X  2 max as one of our feature selection baseline methods. Orthogonal Centroid Feature Selection(OCFS). Document frequency is the number of documents in which a term occurs. We calculate the document frequency for each unique term in the training corpus. Then we select the features with the first p largest DF or remove features whose DF is less than a predefined threshold. The basic as-sumption is that rare terms are either non-informative for categorization nor influential in global performance. The  X  2 statistics measures the lack of independence between t and c and can be compared to the  X  2 distribution with one degree of freedom to judge extremeness.

The orthogonal centroid feature selection (OCFS) [18] selects features optimally ac-cording to the objective function implied by the Orthogonal Centroid algorithm [9]. Orthogonal Centroid is a feature extraction method with projecting the feature space into the orthogonal subspace spanned by all the centroids of the categories. OCFS op-timizes the objective function of Orthogonal Centroid subspace learning algorithm in a discrete solution space.

OCFS computes the centroid for each category firstly. Using  X  c to represent class c, ( c =1 ,  X  X  X  ,C ) , the mean vector of the c -th category is The mean vector of all documents is
Figure 1 gives the flowchart of OCFS. 2.3 Analysis of the Traditional Feature Selection Methods in Text Categorization Although there are many other feature selection methods used in text categorization, such as information gain (IG) and mutual information (IM). It is shown that CHI, DF and OCFS give the competitive performance with IG and MI [21,18].

Since OCFS is derived from the Orthogonal Centroid algorithm, it need the strong assumption that all categories are separated well by the centroids of the classes. There-fore, OCFS fails when the centroids of the classes coincide. In this section, we give the KNN decision rule for text categorization firstly, then we derive our feature selection criterion from the KNN decision rule. 3.1 K-Nearest Neighbor Classification(KNN) for Text Categorization The KNN algorithm is quite simple: given a test document, the system finds the k near-est neighbors among the training documents, and uses the categories of the k neighbors to weight the category candidates. The s imilarity score of each neighbor document to the test document is used as the weight of the categories of the neighbor document. If several of the k nearest neighbors share a category, then the per-neighbor weights of that category are added together, and the resulting weighted sum is used as the likeli-hood score of that category with respect to the test document. By sorting the scores of candidate categories, a ranked list is obtained for the test document. By thresholding on these scores, binary category assignments are obtained. The decision rule in KNN can be written as: KNN(x) is the set of the k nearest neighbors of x ; sim ( x, x i ) is the similarity between the test document x and the training document x i and b c is the category-specific thresh-old for the binary decisions.

Usually, the cosine value of two vectors is used to measure the similarity of the two documents. Thus, the KNN similarity of the document x and category  X  c is where || x || is norm of x ( || x || = 3.2 Effective Feature Selection Criterion for KNN To improve the performance of KNN, we shoul d select the features so that the doc-uments have higher KNN similarities (Eq.( 7))with their corresponding categories and have lower KNN similarities with the other categories.

Given the training dataset X = { x i ,y i } , ( i =1 ,  X  X  X  ,n ) ,where x i is the i -th docu-ment and y i is the corresponding label, we can calculate the KNN similarity of x i and its corresponding category as and the KNN similarity of x i and the other categories is calculated as From the viewpoint of KNN, we should select the features which have importance con-tributions in KNN similarity measure so that sim + KNN ( x i ) is as large as possible and sim  X  KNN ( x i ) is as small as possible for each document x i .
 We define the KNN margin of x i as So we wish to select the features to make the KNN margin as large as possible. The larger the margin of between sim + KNN ( x i ) and sim  X  KNN ( x i ) is, the more accurate the results of categorization are.
 Thus, the total KNN margin in training set can be written as where W is a sparse symmetric n  X  n matrix with W ij .
 By simple algebra formulation, the total KNN margin can be written as: where F ( k )= X k W ( X k ) T .

For the k -th feature, the larger F ( k ) is, the more its contribution is to maximize the total KNN margin. We call F ( k ) the KNN score and use it as our criterion of feature selection.

Alg. 1 gives the flowchart of KNNFS. 3.3 An Illustrating Example To give an intuitive comparison between the OCFS [18] and KNNFS, we apply them to select two features for 2D visualization on the 3D artificial dataset. The dataset is constituted by three classes. One class is generated from single Gaussian distribution, and both the rest two classes are generated with the mixture of two Gaussian distribu-tions. The centroids of three classes coincide. Figure 2 demonstrates the results of 2D visualization by OCFS and KNNFS. KNNFS gives the perfect result, but OCFS fails since all the feature scores are zeros. In this section, we conduct our experiments on three real large scale text data sets to show the performance of KNNFS. We first describe the experiments setup, then give the experimental results, and finally discuss the results. 4.1 Datasets To demonstrate the efficacy of KNNFS, we performed experiments on three data sets: Reuters21587 [14], 20 Newsgroups [12] and WebKB [4].
 Reuters21587. The ApteMod version of Reuters21587 [14] which was obtained by eliminating unlabeled docum ents and selecting the categories which have at least one document in the training set and the test set. This process resulted in 90 categories in both the training and test sets. After eliminating documents which do not belong to any of these 90 categories, we obtained a training set of 7768 documents, a test set of 3019 documents, and a vocabulary of 23793 unique words after stemming and stop word removal. The number of categories per document is 1 . 23 on average. The category distribution is skewed; the most common category has a training set frequency of 2877 but 82% of the categories have less than 100 instances, and 33% of the categories have less than 10 instances. 20 Newsgroups. The 20 Newsgroups data consists of Usenet articles Lang collected from 20 different newsgroups [12]. Over a period of time 1000 articles were taken from each of the newsgroups, which make up of an overall number of 20000 documents in this collection. Except for a small frac tion of the articles, each document belongs to exactly one newsgroup. We use the  X  X ydate X  version of data whose training and testing data are split previously by the data provider 1 .Thereare 18941 documents and 70776 unique words after stemming and stop word removal.
 WebKB. The WebKB data set [4] contains web pages gathered from university com-puter science departments. The pages are divided into seven categories: student, faculty, staff, course, project, department and other. In this paper, we use the four most popular entity-representing categories: student, faculty, course and project, all together contain-After stemming, the resulting vocabulary has 22021 words. We use four-fold cross val-idation by training on three of the universities plus the misc collection, and testing on the pages from a fourth, held-out university. 4.2 Classifiers In order to compare the different feature selection methods, we apply them to the three categorization method: KNN with RCut [19], KNN with SCut [21,19] and SVM [2]. KNN with RCut. The RCut is a thresholding strategies [19]. In RCut, we sort cate-gories by score for each document, and assign this document to each of t top-ranking categories. RCut is parameterized by t which can be either specified manually or auto-matically tuned using a validation set. In this paper, we set t =1 since that the number of categories per document is closer to 1 in our experiments.
 KNN with SCut. The formulation of KNN with SCut is in Eq.(6). We tune the thresh-old b c for each category by a validation set of doc uments. The category-specific thresh-old b c is automatically learned using a validation set of documents. That is, we used a subset of the training documents not used the test documents to learn the optimal threshold for each category. By optimization, we mean the threshold that yield the best F 1 score on the validation documents.
 In this paper, we use leave-one-out cross validation to choose the optimal thresholds. SVM. Besides KNN classification, we also test the performance of KNNFS with SVM to see whether KNNFS is helpful for the other classifiers. For SVM classifier, we choose SVMlib [2] with multi-class classification. We use the radial basis kernel in our experi-ments. 4.3 Performance Measurement In this section, we use micro-averaging F 1 [20] to measure the performance of each result. 4.4 Experimental Results Reuters21587. The performances of the different feature selection algorithms on Reuters21587 data are reported in Table 1. From this table, we can see that the low dimensional spaces selected by KNNFS have b etter performance than its counterpart selected by the other methods(DF, CHI and OCFS). CHI gives poor results in the lower dimension, but becomes better with the increasing of the dimensionality. It is also shown that all methods are overfit whe n the dimensionality is over 1024 because the noise or redundant features could appear. Besides, the performance of SVM with KNNFS is better than the other FS methods, which i ndicates that KNNFS not only improves the performance of KNN, but is helpful for SVM. 20 Newsgroups. The performances of the different feature selection algorithms on 20 Newsgroups data are reported in Table 2. It is also shown that KNNFS has better per-formance than the other FS methods. There is not overfit for each method. The reason may be that the original feature dimensionality is very large and the selected features are just little pr oportion of the original features. Therefore, there may have the fewer redundant features in selected features.
 WebKB. The performances of the different f eature selection algorithms on WebKB data are reported in Table 3. It is also proved that KNNFS outperform the other FS methods. Besides, all methods are overfit in high dimensional space.
 4.5 Discussion of Results From the experiments we can see that the proposed KNNFS is better than DF, CHI and OCFS in most cases. From the experimental results, we can draw the conclusion that KNNFS can get significant improvements than baselines, since it is an effective feature selection approach from the KNN decision rule directly, can outperform the greedy ones.
Besides, we notice that the accuracies of KNNFS are lower than other methods when the number of selected features is 2 . This phenomenon occurs due to the overfitting when the selected feature dimension is small.
 There are two reasons why KNNFS outperforms the other methods. One is that KN-NFS selects features from the viewpoint of KNN directly. Another is that KNNFS max-imizes the KNN margin, which can reduce the generalization error potentially.
Similar with the other methods, the accuracies of KNNFS also drop when the number of selected features is enough large. The reason is that there is no measure of redun-dancy between selected features, which could destroy the performance of KNNFS. In this paper, we propose a new feature selec tion method, called KNNFS, for text catego-rization. There are two contributions in our work. One is that our method adopts a criterion from the viewpoint of KNN, which selects useful features according to the KNN decision rule in text categorization directly. Another is that cosine similarity, instead of Euclidean distance, is adopted since it is usually used as similarity measure in text categorization.
In the future, we will investigate how to learn the best number of selected features automatically based on KNN margin. In addition, we will also extend our feature selec-tion criterion by imposing the penalty to the redundant features.
 This work was (partially) funded by NSFC (No. 61003091), Shanghai Leading Aca-demic Discipline Project (No. B114), S hanghai Committee of Science and Technology (No. 10dz1500102) and 863 Program (No. 2009AA01A346).

