 NICTA and Australian National University Many recent advances in the theory and practice of numer-ical optimization have come from the recognition and ex-ploitation of structure. Perhaps the most common structure is that of finite sums. In machine learning when applying empirical risk minimization we almost always end up with an optimization problem involving the minimization of a sum with one term per data point.
 The recently developed SAG algorithm (Schmidt et al., 2013) has shown that even with this simple form of struc-ture, as long as we have sufficiently many data points we are able to do significantly better than black-box optimiza-tion techniques in expectation for smooth strongly convex problems. In practical terms the difference is often a factor of 10 or more.
 The requirement of sufficiently large datasets is fundamen-tal to these methods. We describe the precise form of this as the big data condition. Essentially, it is the requirement that the amount of data is on the same order as the condition number of the problem. The strong convexity requirement is not as onerous. Strong convexity holds in the common case where a quadratic regularizer is used together with a convex loss.
 The SAG method and the Finito method we describe in this work are similar in their form to stochastic gradient descent methods, but with one crucial difference: They store addi-tional information about each data point during optimiza-tion. Essentially, when they revisit a data point, they do not treat it as a novel piece of information every time. Methods for the minimization of finite sums have classi-cally been known as Incremental gradient methods (Bert-sekas, 2010). The proof techniques used in SAG differ fun-damentally from those used on other incremental gradient methods though. The difference hinges on the requirement that data be accessed in a randomized order. SAG does not work when data is accessed sequentially each epoch, so any proof technique which shows even non-divergence for sequential access cannot be applied.
 A remarkable property of Finito is the tightness of the theo-retical bounds compared to the practical performance of the algorithm. The practical convergence rate seen is at most twice as good as the theoretically predicted rate. This sets it apart from methods such as LBFGS where the empirical performance is often much better than the relatively weak theoretical convergence rates would suggest.
 The lack of tuning required also sets Finito apart from stochastic gradient descent (SGD). In order to get good performance out of SGD, substantial laborious tuning of multiple constants has traditionally been required. A multi-tude of heuristics have been developed to help choose these constants, or adapt them as the method progresses. Such heuristics are more complex than Finito, and do not have the same theoretical backing. SGD has application outside of convex problems of course, and we do not propose that Finito will replace SGD in those settings. Even on strongly convex problems SGD does not exhibit linear convergence like Finito does.
 There are many similarities between SAG, Finto and stochastic dual coordinate descent (SDCA) methods (Shalev-Shwartz &amp; Zhang, 2013). SDCA is only applica-ble to linear predictors. When it can be applied, it has lin-ear convergence with theoretical rates similar to SAG and Finito. We consider differentiable convex functions of the form We assume that each f i has Lipschitz continuous gradients with constant L and is strongly convex with constant s . Clearly if we allow n = 1 , virtually all smooth, strongly convex problems are included. So instead, we will restrict ourselves to problems satisfying the big data condition. Big data condition: Functions of the above form satisfy the big data condition with constant  X  if Typical values of  X  are 1 -8 . In plain language, we are con-sidering problems where the amount of data is of the same order as the condition number ( L/s ) of the problem. 2.1. Additional Notation We superscript with ( k ) to denote the value of the scripted quantity at iteration k . We omit the n superscript on sum-mations, and subscript with i with the implication that in-dexing starts at 1 . When we use separate arguments for each f i , we denote them  X  i . Let  X   X  ( k ) denote the average  X   X  pends on  X  , is denoted  X  . We use angle bracket notation for dot products  X  X  ,  X  X  . 2.2. The Finito algorithm We start with a table of known  X  (0) i values, and a table of two tables during the course of the algorithm. The step for iteration k , is as follows: Our main theoretical result is a convergence rate proof for this method.
 Theorem 1. When the big data condition holds with  X  = 2 ,  X  = 2 may be used. In that setting, if we have initialized all  X  (0) i the same, the convergence rate is:
E h f (  X   X  ( k ) ) i  X  f ( w  X  )  X  See Section 5 for the proof. In contrast, SAG achieves a 1  X  1 8 n rate when  X  = 2 . Note that on a per epoch basis, the Finito rate is 1  X  1 2 n n  X  exp(  X  1 / 2) = 0 . 606 . To put that into context, 10 epochs will see the error bound reduced by more than 148x.
 One notable feature of our method is the fixed step size. In typical machine learning problems the strong convexity constant is given by the strength constant of the quadratic regularizer used. Since this is a known quantity, as long as the big data condition holds  X  = 2 may be used without any tuning or adjustment of Finito required. This lack of tuning is a major feature of Finito.
 In cases where the big data condition does not hold, we conjecture that the step size must be reduced proportion-ally to the violation of the big data condition. In practice, the most effective step size can be found by testing a num-ber of step sizes, as is usually done with other stochastic optimisation methods.
 A simple way of satisfying the big data condition is to du-plicate your data enough times so then holds. This is not as effective in practice as just changing the step size, and of course it uses more memory. However it does fall within the current theory.
 Another difference compared to the SAG method is that we store both gradients and points  X  i . We do not actually need twice as much memory however as they can be stored summed together. In particular we store the quantities p i f (  X  i )  X   X s X  i , and use the update rule w =  X  1  X sn P i This trick does not work when step lengths are adjusted during optimization however. The storage of  X  i is also a disadvantage when the gradients f 0 i (  X  i ) are sparse but  X  are not sparse, as it can cause significant additional memory usage. We do not recommend the usage of Finito when gradients are sparse.
 The SAG algorithm differs from Finito only in the w update and step lengths: By far the most interesting aspect of the SAG and Finito methods is the random choice of index at each iteration. randomness in the problem. Yet it seems that a random-ized method is required. Neither method works in practice when the same ordering is used each pass, or in fact with any non-random access scheme we have tried. It is hard to emphasize enough the importance of randomness here. The technique of pre-permuting the data, then doing in or-der passes after that, also does not work. Reducing the step size in SAG or Finito by 1 or 2 orders of magnitude does not fix the convergence issues either.
 Other methods, such as standard SGD, have been noted by various authors to exhibit speed-ups when random ferences are not as extreme as convergence v.s. non-convergence. Perhaps the most similar problem is that of coordinate descent on smooth convex functions. Coordi-nate descent cannot diverge when non-random orderings are used, but convergence rates are substantially worse in the non-randomized setting (Nesterov 2010, Richtarik &amp; Takac 2011).
 Reducing the step size  X  by a much larger amount, namely by a factor of n , does allow for non-randomized orderings to be used. This gives an extremely slow method however. This is the case covered by the MISO (Mairal, 2013). A similar reduction in step size gives convergence under non-randomized orderings for SAG also. Convergence rates for incremental sub-gradient methods with a variety of order-ings appear in the literature also (Nedic &amp; Bertsekas, 2000). Sampling without replacement is much faster Other sampling schemes, such as sampling without re-placement, should be considered. In detail, we mean the case where each  X  X ass X  over the data is a set of sampling without replacement steps, which continue until no data re-mains, after which another  X  X ass X  starts afresh. We call this the permuted case for simplicity, as it is the same as re-permuting the data after each pass. In practice, this ap-proach does not give any speedup with SAG, however it works spectacularly well with Finito. We see speedups of up to a factor of two using this approach. This is one of the major differences in practice between SAG and Finito. We should note that we have no theory to support this case however. We are not aware of any analysis that proves faster convergence rates of any optimization method under a sampling without replacement scheme. An interesting discussion of SGD under without-replacement sampling appears in Recht &amp; Re (2012).
 The SDCA method is also sometimes used with a per-muted ordering (Shalev-Shwartz &amp; Zhang, 2013), our ex-periments in Section 7 show that this sometimes results in a large speedup over uniform random sampling, although it does not appear to be as reliable as with Finito. We now consider composite problems of the form where r is convex but not necessarily smooth or strongly convex. Such problems are often addressed using proximal algorithms, particularly when the proximal operator for r : has a closed form solution. An example would be the use of L1 regularization. We now describe the Finito update for this setting. First notice that when we set w in the Finito method, it can be interpreted as minimizing the quantity: with respect to x , for fixed  X  i . This is related to the upper bound minimized by MISO, where  X s is instead L . It is straight forward to modify this for the composite case: B  X r ( x ) =  X r ( x ) + The minimizer of the modified B  X r can be expressed using the proximal operator as: This strongly resembles the update in the standard gradient descent setting, which for a step size of 1 /L is We have not yet developed any theory supporting the proxi-mal variant of Finito, although empirical evidence suggests it has the same convergence rate as in the non-proximal case.
 We start by stating two simple lemmas. All expectations in the following are over the choice of index j at step k . Quantities without superscripts are at their values at itera-tion k .
 Lemma 1. The expected step is I.e. the w step is a gradient descent step in expectation ( for SAG.
 Proof.
 Now simplify 1 n ( w  X   X   X  ) as  X  1  X sn 2 P i f 0 i (  X  i term that remains is  X  1  X sn f 0 ( w ) .
 Lemma 2. (Decomposition of variance) We can decom-pose 1 n P i k w  X   X  i k 2 as Proof. 1 n
X = w  X   X   X  2 + = w  X   X   X  2 + = w  X   X   X  2 + Main proof Our proof proceeds by construction of a Lyapunov function T ; that is, a function that bounds a quantity of interest, and that decreases each iteration in expectation. Our Lyapunov function T = T 1 + T 2 + T 3 + T 4 is composed of the sum of the following terms, We now state how each term changes between steps k + 1 and k . Proofs are found in the appendix in the supplemen-tary material: Theorem 2. Between steps k and k +1 , if 2  X   X  1  X  2  X   X  + 0 ,  X   X  2 and  X   X  2 then Proof. We take the three lemmas above and group like terms to get Next we cancel part of the first line using 1  X n , based on B3 in the Appendix. We then pull terms occurring in  X  1  X n T together, giving E [ T ( k +1) ]  X  T  X   X  + (1  X  + (  X  + (1  X  + (1  X  Next we use the standard inequality (B5) (1  X  , which changes the bottom row to  X  (1  X  1  X  ) s 2 n w  X   X  (1  X  1  X  ) s 2 n 2 P i  X   X   X   X  i 2 . These two terms can then be grouped using Lemma 2, to give We use the following inequality (Corollary 6 in Appendix) to cancel against the P i k w  X   X  i k 2 term: 1  X  and then apply the following similar inequality (B7 in Ap-pendix) to partially cancel P i k f i (  X  i )  X  f i ( w ) k Leaving us with The remaining gradient norm term is non-positive under the conditions specified in our assumptions.
 Theorem 3. The Lyapunov function bounds f (  X   X  )  X  f ( w as follows: Proof. Consider the following function, which we will call R ( x ) : When evaluated at its minimum with respect to x , which f ( w  X  ) by strong convexity. However, we are evaluating at function. R is convex with respect to x , so by definition Therefore by the lower bounding property Now note that T  X  f (  X   X  )  X  R ( w ) . So Theorem 4. If the Finito method is initialized with all  X  the same,and the assumptions of Theorem 2 hold, then the convergence rate is:
E h f (  X   X  ( k ) ) i  X  f ( w  X  )  X  with c = 1  X  1 2  X  .
 Proof. By unrolling Theorem 2, we get Now using Theorem 3 We need to control T (0) also. Since we are assuming that all  X  0 i start the same, we have that T The theory for the class of smooth, strongly convex prob-lems with Lipschitz continuous gradients under first order optimization methods (known as S 1 , 1 s,L ) is well developed. These results require the technical condition that the di-mensionality of the input space R m is much larger than the number of iterations we will take. For simplicity we will assume this is the case in the following discussions. convergence rate is bounded by: In fact, when s and L are known in advance, this rate is achieved up to a small constant factor by several meth-ods, most notably by Nesterov X  X  accelerated gradient de-scent method (Nesterov 1988, Nesterov 1998). In order to achieve convergence rates faster than this, additional as-sumptions must be made on the class of functions consid-ered.
 Recent advances have shown that all that is required to achieve significantly faster rates is a finite sum structure, such as in our problem setup. When the big data condi-tion holds our method achieves a rate 0.6065 per epoch in expectation. This rate only depends on the condition number indirectly, through the big data condition. For ex-ample, with L/s = 1 , 000 , 000 , the fastest possible rate for a black box method is a 0 . 996 , whereas Finito achieves a rate of 0 . 6065 in expectation for n  X  4 , 000 , 000 , or 124x faster. The required amount of data is not unusual in mod-ern machine learning problems. In practice, when quasi-newton methods are used instead of accelerated methods, a speedup of 10-20x is more common. 6.1. Oracle class We now describe the (stochastic) oracle class rally fit.
 Function class: f ( w ) = 1 n P n i =1 f i ( w ) , with f S Oracle: Each query takes a point x  X  R m , and returns j , f ( w ) and f 0 j ( w ) , with j chosen uniformly at random. Accuracy: Find w such that E [ w ( k )  X  w  X  2 ]  X  . The main choice made in formulating this definition is putting the random choice in the oracle. This restricts the methods allowed quite strongly. The alternative case, where the index j is input to the oracle in addition to x , is also interesting. Assuming that the method has ac-cess to a source of true random indices, we call that class that suggests that faster rates are possible in DS 1 , 1 s,L,n It should first be noted that there is a trivial lower bound rate for f  X  SS 1 , 1 s,L, X  ( R m ) of 1  X  1 n reduction per step. Its not clear if this can be achieved for any finite  X  . Finito is only a factor of 2 off this rate, namely 1  X  1 2 n  X  = 2 , and asymptotes towards this rate for very large  X  . SDCA, while not applicable to all problems in this class, also achieves the rate asymptotically.
 Another case to consider is the smooth convex but non-strongly convex setting. We still assume Lipschitz contin-uous gradients. In this setting we will show that for suffi-ciently high dimensional input spaces, the (non-stochastic) lower complexity bound is the same for the finite sum case and cannot be better than that given by treating f as a single black box function.
 The full proof is in the Appendix, but the idea is as follows: when the f i are not strongly convex, we can choose them such that they do not interact with each other, as long as the dimensionality is much larger than k . More precisely, we may choose them so that for any x and y and any i 6 = j ,  X  f i ( x ) ,f teract, no optimization scheme may reduce the iterate error faster than by just handling each f i separately. Doing so in an in-order fashion gives the same rate as just treating f using a black box method.
 For strongly convex f i , it is not possible for them to not interact in the above sense. By definition strong convexity requires a quadratic component in each f i that acts on all dimensions. In this section we compare Finito, SAG, SDCA and LBFGS. We only consider problems where the regularizer is large enough so that the big data condition holds, as this is the case our theory supports. However, in practice our method can be used with smaller step sizes in the more general case, in much the same way as SAG.
 Since we do not know the Lipschitz constant for these prob-lems exactly, the SAG method was run for a variety of step sizes, with the one that gave the fastest rate of convergence plotted. The best step-size for SAG is usually not what the theory suggests. Schmidt et al. (2013) suggest using 1 L in-stead of the theoretical rate 1 16 L . For Finito, we find that using  X  = 2 is the fastest rate when the big data condition holds for any  X  &gt; 1 . This is the step suggested by our the-ory when  X  = 2 . Interestingly, reducing  X  to 1 does not improve the convergence rate. Instead we see no further improvement in our experiments.
 For both SAG and Finito we used a differing step rule than suggested by the theory for the first pass. For Finito, during the first pass, since we do not have derivatives for each  X  yet, we simply sum over the k terms seen so far where we process data points in index order for the first pass only. A similar trick is suggested by Schmidt et al. (2013) for SAG.
 Since SDCA only applies to linear predictors, we are re-stricted in possible test problems. We choose log loss for 3 binary classification datasets, and quadratic loss for 2 re-gression tasks. For classification, we tested on the ijcnn1 and covtype datasets 1 , as well as MNIST 2 classifying 0-4 against 5-9. For regression, we choose the two datasets from the UCI repository: the million song year regression dataset, and the slice-localization dataset. The training por-tion of the datasets are of size 5 . 3  X  10 5 , 5 . 0  X  10 4 . 7  X  10 5 and 5 . 3  X  10 4 respectively.
 Figure 6 shows the results of our experiments. Firstly we can see that LBFGS is not competitive with any of the in-cremental gradient methods considered. Secondly, the non-permuted SAG, Finito and SDCA often converge at very similar rates. The observed differences are usually down to the speed of the very first pass, where SAG and Finito are using the above mentioned trick to speed their conver-gence. After the first pass, the slopes of the line are usually comparable. When considering the methods with permu-tation each pass, we see a clear advantage for Finito. In-convergence. Traditional incremental gradient methods (Bertsekas, 2010) have the same form as SGD, but applied to finite sums. Essentially they are the non-online analogue of SGD. Applying SGD to strongly convex problems does not yield linear convergence, and in practice it is slower than the linear-converging methods we discuss in the remainder of this section.
 Besides the methods that fall under the classical Incremen-tal gradient moniker, SAG and MISO (Mairal, 2013) meth-ods are also related. MISO method falls into the class of up-per bound minimization methods, such as EM and classical gradient descent. MISO is essentially the Finito method, but with step sizes n times smaller. When using these larger step sizes, the method is no longer a upper bound minimization method. Our method can be seen as MISO, but with a step size scheme that gives neither a lower nor upper bound minimisation method. While this work was under peer review, a tech report (Mairal (2014)) was put on arXiv that establishes the convergence rate of MISO with step  X  = 1 and with  X  = 2 as 1  X  1 3 n per step. This similar but not quite as good as the 1  X  1 2 n rate we establish. Stochastic Dual Coordinate descent (Shalev-Shwartz &amp; Zhang, 2013) also gives fast convergence rates on problems for which it is applicable. It requires computing the convex conjugate of each f i , which makes it more complex to im-plement. For the best performance it has to take advantage of the structure of the losses also. For simple linear classi-fication and regression problems it can be effective. When using a sparse dataset, it is a better choice than Finito due to the memory requirements. For linear predictors, its theo-retical convergence rate of 1  X   X  (1+  X  ) n per step is a little faster than what we establish for Finito, however it does not appear to be faster in our experiments.
 We have presented a new method for minimization of finite sums of smooth strongly convex functions, when there is a sufficiently large number of terms in the summation. We additionally develop some theory for the lower complexity bounds on this class, and show the empirical performance of our method.
 Bertsekas, Dimitri P. Incremental gradient, subgradient, and proximal methods for convex optimization: A sur-vey. Technical report, 2010.
 Mairal, Julien. Optimization with first-order surrogate functions. ICML , 2013.
 Mairal, Julien. Incremental majorization-minimization op-timization with application to large-scale machine learn-ing. Technical report, INRIA Grenoble Rhne-Alpes / LJK Laboratoire Jean Kuntzmann, 2014.
 Nedic, Angelia and Bertsekas, Dimitri. Stochastic Opti-mization: Algorithms and Applications , chapter Con-vergence Rate of Incremental Subgradient Algorithms. Kluwer Academic, 2000.
 Nesterov, Yu. On an approach to the construction of op-timal methods of minimization of smooth convex func-tions. Ekonomika i Mateaticheskie Metody , 24:509 X 517, 1988.
 Nesterov, Yu. Introductory Lectures On Convex Program-ming . Springer, 1998.
 Nesterov, Yu. Efficiency of coordinate descent methods on huge-scale optimization problems. Technical report, CORE, 2010.
 Recht, Benjamin and Re, Christopher. Beneath the val-ley of the noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences.
Technical report, University of Wisconsin-Madison, 2012.
 Richtarik, Peter and Takac, Martin. Iteration complexity of randomized block-coordinate descent methods for mini-mizing a composite function. Technical report, Univer-sity of Edinburgh, 2011.
 Schmidt, Mark, Roux, Nicolas Le, and Bach, Francis. Min-imizing finite sums with the stochastic average gradient. Technical report, INRIA, 2013.
 Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual co-ordinate ascent methods for regularized loss minimiza-
