 The classical Bag-of-Word (BOW) model represents a doc-ument as a histogram of word occurrence, losing the spa-tial information that is invaluable for many text analysis tasks. In this paper, we present the Language Pyramid (LaP) model, which casts a document as a probabilistic dis-tribution over the joint semantic-spatial space and motivates a multi-scale 2D local smoothing framework for nonpara-metric text coding. LaP efficiently encodes both seman-tic and spatial contents of a document into a pyramid of matrices that are smoothed both semantically and spatially at a sequence of resolutions, providing a convenient multi-scale imagic view for natural language understanding. The LaP representation can be used in text analysis in a vari-ety of ways, among which we investigate two instantiations in the current paper: (1) multi-scale text kernels for docu-ment categorization, and (2) multi-scale language models for ad hoc text retrieval. Experimental results illustrate that: for classification, LaP outperforms BOW by (up to) 4% on moderate-length texts (RCV1 text benchmark) and 15% on short texts (Yahoo! queries); and for retrieval, LaP gains 12% MAP improvement over uni-gram language models on the OHSUMED data set.
 I.5.4 [ Pattern Recognition ]: Applications X  Text process-ing ; H.3.3 [ Information Search and Retrieval ]: Re-trieval Models; I.2.4 [ Artificial Intelligence ]: Learning Theory, Algorithms, Performance Text Spatial Contents Modeling, Multi-Scale Text Analy-sis, Bag of Word, Language Pyramid, Multi-Scale Language Models, Multi-Scale Text Kernel
Today X  X  text analysis systems (e.g., search engines, text miners, social medias) rely almost unanimously on the Bag-of-Word (BOW) model for text representation, which views a document as a loose collection of independent words and represents it as a histogram of word occurrences. Although simple, efficient, sometimes effective, and usually convenient for popular vector-space analyzing algorithms, the BOW model loses too much information during its balls-into-bins process  X  with the same set of word occurrences, one could compose tens of documents with diversified meanings. Par-ticularly, the spatial information (e.g., word correlation, prox-imity, ordering and long distance dependence) is totally lost by BOW, which, however, has been proved invaluable for many text analysis tasks [21, 10, 23, 15], especially for short text (e.g., web search queries, online ads, Twitter mini-posts) processing [11, 28, 3, 37, 29].

Substantial efforts have been made to develop techniques to (partially) encode spatial information in text representa-tions. Perhaps the first attempt is the n -gram BOW model [21], where every n adjacent words are bound together for occurrence counting. This model, however, has been plagued by the curse of dimensionality [8] since the exponential num-ber of word combinations could easily exceed any possibly available corpus size, not to mention the spatial correlations among n -grams are still missing. In information retrieval, various techniques have been developed to segment short texts (e.g., web search queries) into segments so as to enforce word proximity and ordering constraints [11, 28]. Nonethe-less, to get the right segments remains a challenge  X  any seg-mentation system could occasionally either split segments or miss segments or both; yet, the spatial correlations among segments are still at large. Similar problems exist for text representation methods based on: noun phrase (or named entity) extraction [7, 29], hidden Markov models [24], mix-ture of n-grams [10], subsequence string kernel [19], etc.
In this paper, we present the Language Pyramid (or LaP) model, a new text representation method. Without sacrifice of any spatial information, LaP characterizes a document as a binary 2D matrix (i.e., a black-and-white image), with se-mantic axis along rows and spatial axis along columns. This 2D code inspires a new perspective of text representations as probabilistic distributions over the joint semantic-spatial space, and motivates 2D nonparametric density estimation using locally weighted smooth kernels, which, in the spatial-axis, coincides the spatial filter transforms in image process-ing [4] and, in the semantic direction, resembles the smooth-ing techniques used in language models [35]. We formulate both the semantic and spatial smoothing as graph-based op-timizations and efficiently solve them by simply convolving the binary text codes with well-defined local kernels. To enhance scale robustness of this 2D local smoothing frame-work, we borrow the idea of image pyramids [4] to itera-tively apply the 2D smoothers at a sequence of scales, re-sulting in (for each document) a pyramid of matrices that are smoothed both semantically and spatially at progres-sive resolutions. This multiple-scale representation includes both the original document (at the finest scale) and its BOW representation (at the coarsest scale) as spacial cases.
We believe the LaP model creates a new dimension for text modeling:
The LaP representation could be used in text analysis tasks in a variety of ways. In this paper, we demonstrate two instantiations of LaP applications: 1. Multi-Scale Text Kernels. In the scenario of text classi-2. Multi-Scale Language Models. The LaP representa-Extensive experiments were conducted to tes t LaP against BOW counterparts, on RCV1 benchmark as well as Yahoo! query corpus for classification evaluation, and on OHSUMED data set for retrieval evaluation. Experimental results indi-cate that LaP consistently outperforms BOW in all the three tasks. Particularly, for classification, it improves the classi-fication accuracy by (up to) 15% on short texts (Yahoo! queries) and 4% on moderate length texts (RCV1); and for retrieval, it achieves up to 12% MAP improvement over the uni-gram language models.

The remaining sections of this paper is organized as fol-lows. We first present the 2D local smoothing framework in Section 2, and extend it in Section 3 to the multi-scale version, i.e., the LaP model. Section 4 instantiates two ap-plications of LaP, i.e., multi-scale kernels for text classifica-tion and multi-scale language models for ad hoc retrieval. Section 5 then conducts extensive evaluations for the pro-posed models. Finally, related topics are briefly discussed in Section 6, and conclusions are addressed in Section 7.
Let W be a document comprised of a finite sequence of words: {W = w 1 w 2 ...w N ,w j  X  X } ,where N is the doc-ument length and V = { v 1 ,v 2 ,...,v M } the vocabulary of size M . Alternatively, without any information loss, we can characterize W as a 2D M  X  N -sized binary matrix X ,the ( i, j )-th entry x ij of which indicates whether or not the i -th word of V is observed at the j -th position of W , i.e.: x ij = I ( v i ,w j ), where I ( a, b )=1if a = b and 0 otherwise. Hereafter, we will refer to the i and j directions of X as the semantic axis (or v -axis) and the spatial axis (or w -axis) respectively. Accordingly, W and V are also referred to as spatial space and semantic space respectively. Under this 2D view, a document X is equivalent to a black-and-white image except that for images both i and j directions are spatial axes.

Usually, the document length is much smaller than the vo-cabulary size, N M ,making X too sparse for statistical analyzing. Therefore, the popular BOW model represents a document as a vector of word frequencies, which is equiva-lent to project the 2D matrix X toa1Dvector: where 1 is an N -vector with all 1 entries. As the spatial axis is wiped out, the rich spatial information contained in X is totally lost by the BOW representation.

In this paper, we are motivated for text representation that encodes both semantic and spatial contents of a doc-ument. Generalizing the binary matrix representation, we have the following definition: Definition 1. The 2D text model X  X  R M  X  N is a prob-abilistic distribution over the joint semantic-spatial space : V X W X  R + ,0 x ij 1, This 2D text model defines a generative model for texts. Intuitively, we can interpret x ij as the probability of observ-ing the semantic word v i at the spatial position w j .From this point of view, the binary matrix representation  X  X (af-ter normalization) can be understood as a nonparametric estimation of X . To see this, we have: where  X  x  X  j denotes the j -thcolumnvectorof  X  X ,and  X  x th row vector. Eqn.(2) is equivalent to a Parzen estimator in the (spatial) w -direction, that is, the density at position w is expressed as an average of kernel functions that are cen-tered at each sample w n . Here the Dirac kernels  X  is used, i.e., only samples with zero distance 1 ( j = n ) contributes to the density at w j . Similarly, Eqn.(3) and (4) correspond to Dirac kernel based Parzen estimators in (semantic) v -axis
Since w, v are indices for ordered sets W and V respectively, ( w i = w j )  X  ( i = j ); similarly, ( v i = v j )  X  ( i = j ).
Figure 1: Chain graph used in spatial smoothing. and 2D ( w, v )-axes respectively. In practice, Dirac kernels lead to estimates that are too erratic and noisy. This mo-tivates us to use more well-defined kernels for reliable text coding.

A fundamental characteristic of documents is that neigh-boring words are usually high ly correlated, which is true both semantically [22] and spatially [15]. For instance, ob-serving the word  X  X ew X  indicates a high likelihood of see-ing the other word  X  X ork X  at the next position. This phe-nomenon suggests more reliable estimating of X by using locally weighted smooth kernels [4], which is, in effect, equiv-alent to smoothing  X  X from binary to grayscale matrix. The following sections derive local smoothing for the spatial and the semantic dimensions respectively, both of which can be formulated as graph-based learning problems and reduced to simple convolutions with smooth kernels.
Spatial smoothing has long been popularized in signal pro-cessing and image analysis [4], and was quite recently ex-plored in text modeling by [15]. Particularly for our task, spatial smoothing attempts to smooth the matrix X along the w -axis, based on the intuitions that: ( i )thesemantic distribution (i.e., distribution over the semantic space should be similar between two neighboring positions, i.e., x i  X  x  X  j if ( i  X  j )  X  0; and ( ii ) the degree of similar-ity should be consistent with the distance of the locations. By doing this, we are actually borrowing the presence of a word at w i to its neighboring location w j with a discount depending on the distances | i  X  j | . Formally, this implies an undirected chain graph G w (Figure 1), where two neigh-boring positions are connected with a weight depending on their distance. And spatial smoothing can be formulated as graph-based learning that seeks a balance between the fidelity to the original code X 0 and the consistency with the the graph G w : where the superscript  X 0 X  specifies the Dirichlet-smoothed version of the original binary code: x 0 ij =(  X  MN + X  x ij  X  0 is the tradeoff parameter between the two objectives, and  X  ij the connection wight between w i and w j .
It is quite interesting that the solution to Eqn.(5) turns out to be as simple as the convolution of the original code with a specific kernel 2 : where k is a kernel function defined over a T -width window t  X  X  X  T 2 ,..., T 2 } ,thevalueof k depends on  X  and  X   X  X ,
Eqn.(6) could be arrived at by taking the first-order deriva-tive of the objective in Eqn.(5) w.r.t. x and making it zero. and the summation is over the T -window. This simple result inspires us that, instead of carefully picking the right values for  X   X  X  and  X  , we can directly work with the kernel k , e.g., by using well defined kernel functions such as the Gaussian kernel k t = 1 2  X  X  exp( t 2 / 2  X  ), and convolving it with the orig-inal binary coding to obtain the spatially smoothed codes. This is equivalent to applying a Gaussian spatial filter to the original code [4].
In contrast to the w -direction spatial smoothing, semantic smoothing attempts to smooth X along the semantic v -axis, ensuring that the distributions over the spatial space W are similar between two semantically related words, i.e., if v v ,then x i  X   X  x j  X  . Intuitively, we are borrowing the presence of the word v i to a relevant word v j with a discount according to the degree of their relevance.

Semantic smoothing has been extensively explored in nat-ural language processing, e.g., for tackling the sparseness of BOW [21] or the wellness of language models (LMs) [35]. Classical smoothing methods, e.g., Laplacian, Dirichlet, and Jelinek-Mercer smoother, usually shrink the original distri-butions with a predefined reference distribution, regardless of the correlations among words. In contrast, we consider lo-cally weighted smoothing, similar to what we used in spatial smoothing, which is preferable because of their capability to capture word correlations [22].

Assume the knowledge on word correlations has been fully conveyed into an undirected graph G v ,wheretwosemanti-cally correlated words v i and v j are connected with a weight  X  ij according to the degree of their dissimilarity d ij .Se-mantic smoothing is formulated as the following graph-based learning task: where  X  0 defines the tradeoff,  X  i weights the importance of the node v i . Again, the solutions could be expressed as convolutions with locally weighted smooth kernels: Compared with spatial smoothing, semantic smoothing is, however, more challenging to work with. In Eqn.(6), be-cause  X  ij is spatially homogeneous (i.e., value depends solely on i  X  j ), there exists a kernel function k that is defined over a small window and invariant to the spatial coordinates i . This no longer holds for the case of semantic smoothing. In Eqn.(8), as  X  ij is dependent on d ij rather than i  X  j , the kernel  X  is now coupled with both t and i . Although it is possible to make  X  ij spatially homogeneous, for example, by embedding G v to one dimension (e.g., spectral embedding [2], structure preserving embedding [27]) and reordering the nodes according to the 1D coordinates, this will compromise the accuracy. To handle this problem, we define the follow-ing Gaussian kernel:  X  ( i, t )= 1 2  X  X  exp(  X  d 2 it / 2  X  ). Accord-ingly, the summation in Eqn.(8) is over all the neighboring nodes of v i , i.e.,  X  t , v t and v i are connected in G
Eqn.(8) is also equivalent to applying a Gaussian filter to the original code, except that this time the filter is specif-ically defined according to the semantic graph G v .Special care needs to be taken to the construction of G v though, as the results are typically very sensitive to it. Previous works usually build semantic graphs based on the training data. For example, in [22], the mutuel information scores estimated on training corpus were used to define connection weights. Although simple and efficient, such methods is des-tined to overfit the available training corpus. We recommend to construct semantic graphs by using more reliable knowl-edge base (e.g., WordNet , Wikipedia ), or by estimating the weights on much larger universal corpora, similar to the transfer learning framework proposed by [6].
An essential property of natural language phrases is that they exist as meaningful concepts only at the right resolu-tions. Therefore, the choice of the scale parameter  X  plays a fundamental role for the success of the 2D local smoothing framework presented in the previous section. Taken to the limit, the finest scale (  X  = 0) of our 2D-smoothed repre-sentation corresponds to the original document binary code X 0 that is too erratic, whereas the coarsest scale (  X   X  X  X  ) approaches to an overly-shrunk code, i.e., a matrix with con-stant entries that loses all the details. For the purpose of automatic modeling, there is no way to decide a priori which scale fits the best. More pessimistically, different concepts usually stands at different scales, making it impossible to capture all the right meanings with a single scale. For exam-ple, named entities usually range from unigram (e.g.,  X  X ew X ) to bigram (e.g.,  X  X ew York X ), to multigram (e.g.,  X  X ew York Times X ), and even to a whole long sequence (e.g., a song name  X  Another Lonely Night In New York X ).

To this end, we borrow the idea of pyramid representation from image processing [4, 5] to textual area and present the Language Pyramid (LaP) model. The key insight is, as the best scale is unknown a prior and no single scale works best, the only reasonable model is to simultaneously represent the document at multiple scales.

There are two main types of pyramids: the Gaussian (or lowpass) pyramid (GP), and the Laplacian (or bandpass) pyramid (LP). In our scena rio, the original code X 0 serves as the 0-level GP X 0 g , corresponding to scale 0. The whole GP is obtained by iteratively repeating the 2D local smooth-ing process several times, where each time the current level is smoothed to get the next level GP. That is, the i -level GP X i g is a smoothed version of X i  X  1 g at scale  X  i .Asthe scales increases level by level, the results are a pyramid of gradually more smoothed matrices. In contrast to GP, LP encodes the representation error of GP, i.e., the l -th level LP X i l is formed by the difference between GP codes at two successive levels: X i l = X i g  X  X i +1 g . Although Lapla-cian pyramid offers more compac t representation (relatively lower encoding entropy [4]), it also requires more effort for intuitive interpretation (difference between two distributions is not generally comprehensible). Therefore, in the current paper, we mainly focus on Gaussian pyramid.

As an intuitive validation of LaP, Figure 2 demonstrates the visualization results for a synthetic short text  X  New York Times offers free iPhone 3G as gifts for new customers in New York  X , which is selected because of its containing of en-tities of multiple resolutions (e.g.,  X  X ew X ,  X  X ew York X ,  X  X ew York Times X ). For this task, we use a vocabulary containing twelve words:  X  X ew X ,  X  X ork X ,  X  X ime X ,  X  X ift X ,  X  X ree X ,  X  X Phone X ,  X  X ustomer X ,  X  X pple X ,  X  X gg X ,  X  X ity X ,  X  X ervice X  and  X  X oupon X , where the last four words are chosen because of their strong Figure 2: LaP multi-scale visualization of the text  X  X ew York Times offers free iPhone 3G as gifts for new customers in New York X . (a) Original binary code (  X  =0 ). (b-e) 2D smoothed codes at scales  X  = 1, 2, 4 and  X  respectively. (f) Bag-of-Word code i.e., spatial-only smoothing at (  X  w =  X  ,  X  v =0 ). (a-e) forms a 5-level Gaussian pyramid. Refer to text for more details. correlations with the words a ppearing in the text. The se-mantic graph is constructed based on pairwise mutual in-formation scores estimated on the RCV1 corpus [17] as well as a large scale repository of web search queries. After re-moving stop-words (i.e., words appearing in the text but not the vocabulary) and stemming, the text is represented as a 12  X  10 binary code, which is a sharp and erratic rep-resentation encoding precisely which word appears at which position, as shown the white-and-black image in Figure 2(a). The 2D smoothed LaP 3 codes are shown in Figure 2(b-e) at scales  X  =1,2,4and  X  respectively,which,alongwith (a), form a 5-level Gaussian pyramid at a sequence of pro-gressively coarser resolutions. As a reference, (f) shows the BOW representation, which is equivalent to a 2D smoothed code at scales (  X  w =  X  , X  v = 0). Compared with BOW, LaP not only captures the spatial correlations at multiple resolutions (e.g., the named entities  X  X ew York X  and  X  X ew York Times X ), but also boosts the probability of semantical topics underlying the text, for example,  X  X Phone X   X   X  X pple X ,  X  X ustomer X   X   X  X ervice X ,  X  X ree X  and  X  X ift X   X   X  X oupon X ,  X  X ew X  and  X  X Phone X   X   X  X gg X  (probably due to the online electronics store newegg.com ),andsoon.
 Complexity At the first glance, the representation costs (e.g., computational time and storage spaces) of LaP seem-ingly discount its advantages. However, the overhead, com-pared with BOW, is actually not as much as one imag-ines. The computational complexity for each layer of LaP
In all our implementations, to allow efficient unified treat-ment, both w and v axes are smoothed with the same scale parameter  X  and window width T , which are achieved by normalizing the v -axis distances to the same order as the w -axis distances. is
O ( TMN ); and overall O ( LT M N ) for an L -layer pyra-mid. Compared with BOW, which is O ( MN ), the relative overhead, O ( LT ), is reasonably small because in practice, the smoothing width T and the number of layers L are both small numbers, e.g., in our simulation, T =5, L =5. The storage complexity of LaP is O ( LM N ) overall. Compared with BOW, which is O ( M ), LaP requires O ( LN )times spaces. However, for short texts (e.g., web search queries, social community questions and Twitter mini-posts), which span very small spatial spaces, only trivial amount of storage spaces are increased by LaP.

To further reduce the overhead, for popular text analysis tasks such as document classification and ad hoc retrieval, we show in the next section that by using kernel methods or similarity-based relevance models, the high-dimensionality of LaP is no more a concern.
LaP opens a new dimension for text modeling: ( i )itcap-tures both semantic and spatial contents of documents, en-abling more accurate text modeling; ( ii ) it provides multi-scale representations for texts, allowing texts to be analyzed in a scale-invariant fashion; ( iii ), it provides an imagic view for document, not only facilit ating natural language under-standing and visualization, but also enabling well-established image analysis tools to be applied to text processing, e.g., matching, segmentation, description, interests points detec-tion, classification. As such, LaP could be used in text min-ing tasks in a variety of ways. In this section, we explore two instantiations of LaP as it applied to multi-scale text anal-ysis. Specifically, in the scenario of supervised text mining, we describe a method to combine the matrices in LaP model to form a multi-scale kernel (MSK), upon which popular ker-nel based learning methods could be applied. In addition, we generalize the language models to a multi-scale version, and apply it in the task of document retrieval.
Suppose we are learning classification rules from a train-ing corpus D = { d k ,y k } K k =1 ,where d is a document, and y its label. Let the LaP representation for d k be { X l a pyramid contains totally L levels. The documents are in advance normalized to the same length so that the matri-ces in LaP are of the same size M  X  N ,  X  k . Each level l defines a single-scale kernel R M  X  N  X  R : k l ( d k
X R M  X  N  X  R M  X  N  X  R + , e.g., Frobenius product, Gaussian RBF similarity, Jensen-Shannon divergence [16]. The task can thus be naturally formulated as a multiple kernel learn-ing (MKL) problem, i.e.: to learn an optimal combination kernel k = L l =1  X  l k l ,where0  X  l 1and L l =1  X  2 l =1.
From a methodological point of view, MKL approaches can be divided into two categories: the Wrapper methods that learn  X   X  X  to minimize the training error of a specified type of classifiers (e.g., SVMs), and the Filter methods that learn  X   X  X  independent of any classifier. Despite the facts that Wrapper MKL provide less generic results than Filter, and that they are usually solved by very time-consuming opti-mizers (e.g., quadratic-constrained quadratic programming [1], semi-definite programming [14]), almost all the existing MKL techniques are essentially Wrapper methods as they all stick to SVM classifiers.

In this paper, both Wrapper and Filter MKL are imple-mented to build MSKs. For Wrapper, we employ the Sim-plemlk MKL toolbox [26]. For Filter, we present BEM (Bayes-Error-Minimization) MKL, a new MKL method that is orthogonal to any classifier and extremely efficient to solve. BEM-MKL is inspired by the Relief [12] feature-weighting algorithm, and is formulated as convex optimization: where m kl = l ( d k ,m k )  X  l ( d k ,h k ) is a heuristic mar-gin; h k , called  X  X earest-hit X , is the nearest neighbor of d within the same class, whereas m k , the  X  X earest-miss X , is the nearest neighbor of d k from different classes; l ( d, d )= This convex formulation is theoretically sound as it has been shown to minimize the nonparametric Bayes error on the training data [31]. In addition, it has a close-form solution that is extremely efficient to obtain: where  X  =[  X  1 ,..., X  L ] ,themarginvector  X  m =[  X  m 1 ,...,  X  m with entry  X  m l = 1 K K k =1 m kl ,and(  X  ) + denotes the positive-part operator, i.e., ( a ) + =max( a, 0).
Extracting users X  information needs from their input queries is fundamental to the success of information retrieval sys-tems, which is tremendously challenging because queries contain far poorer information than normal texts, e.g., they are usually much shorter (3 words on average), noisier (e.g., no grammatical structure) and less well-formed (e.g., typos, misspellings). Worse still, the standard BOW query model further abandons all the spatial information, making it im-possible to capture the precise search intents from queries. For higher retrieval accuracy , it is therefore necessary to go beyond bag-of-word and base search on text representa-tion methods that are able to encode richer knowledge of user submitted queries. The LaP model seems to be a good choice for this purpose.

In ad hoc text retrieval, we are concerned with ranking adocument d with response to a query q based on a rele-vance score function r ( q, d ). A standard approach to learn-ing r ( q, d ) is the (unigram) language models [25], which de-fine the probability of observing a piece of text as the prod-ucts of observing each word. In its raw form, it coincides with the BOW representation. Since LaP is motivated as nonparametric estimation of a 2D generative model, each level of the LaP code could be seen as a single-scale 2D nonparametric language model, defining the probability of observing each word at each text position. Accordingly, the LaP matrices define a set of multi-scale language models, i.e., language models capturing semantic and spatial corre-lations at different resolutions. In this section, we exploit LaP for ad hoc retrieval.
 LaP representations for query q and document d respec-tively. The l -th level relevance score r l ( q, d ) is a function R M  X  N q  X  R M  X  N d  X  R + .As q and d are of different dimen-sions, we consider the following two approaches to matching them: 1. Global Matching : Documents and queries are all nor-2. Local Matching . Instead of normalizing and matching
The overall relevance function could be obtained by aggre-gating all the single-scale relevance functions. For simplicity, assume a linear aggregation: r ( q, d )= L l a l r l ( q, d ), where a level. Consider a supervised aggregation setting, where we are given a training corpus containing a set of query q ,and for each q asetofdocuments { d q i } along with their relevance judgements. We use the following convex optimization to learn a  X  X : where d q i d q j means d q i is more relevant to q than d formulation also has an efficient close-form solution: where a =[ a 1 ,...,a L ] ,  X  h =[  X  h 1 ,...,  X  h L ] ,and relevant-irrelevant document pairs. This algorithm is refer-eed to as Bayes Error Minimization based Multi-scale Lan-guage Model ,orBEM-MLM.
In this section, we test LaP by experimenting with the two instantiations. Specifically, we evaluate the effectiveness of the multi-scale text kernels in text classification tasks, and the multi-scale language models in the scenario of ad hoc text retrieval. We aim at comparing text representation models only. To rule out other factors, LaP is compared with BOW counterparts, which are popular in existing text anal-ysis systems as the standard or the state-of-the-art methods. Figure 3: Kernel weights learned by (left) Sim-pleMKL and (right) BEM-MKL for BOW kernel as well as single-scale LaP kernels. Shown results on (top) moderate-length texts (RCV1) and (bottom) short texts (Yahoo! queries).
We evaluate multi-scale text kernels (MSKs) on two real-world text sets (Table 1). The RCV1 [17] version-2 corpus is a benchmark for text classification, which contains over 804K documents from 103 classes. Our experiments focused on 161,311 documents from the following ten leaf-node top-ics: C11, C24, C42, E211, E512, GJOB, GPRO, M12, M131 and M142 . The documents in this corpus are of moderate lengths (134 words on average, min: 8, max: 3441). The other data set is a snapshot of answers.yahoo.com crawled in early 2008, containing 205 , 280 queries from 101 classes. Texts (i.e., queries) in this set are of relatively short lengths, ranging from 3 to 27 words (9.5 on average). The basic char-acteristics of the data sets are summarized in Table 1. For both corpora, the texts are first stop-worded and stemmed. For the purpose of computational efficiency, the top 20% (approximately 10K) words with the highest DFs (docu-ment frequencies) are selected as the final vocabulary 4 ;all
According to [33] and many others, words with high DFs are usually most informative for text classification. Table 2: Testing set accuracy. Compared models: BOW the out-of-vocabulary words are discarded. Both corpora use the same semantic graph, which is constructed based on pairwise mutual information scores estimated on the whole RCV1 corpus as well as a large scale web search query logs, and is further sparsified using k -nearest neighbor method with a sufficiently large k 5 . The documents are normalized to the length of the longest one in corpus. Document nor-malization is done via interpolation, in our experiment, we use bi-linear interpolation, an efficient and standard method in image processing.
As MKL algorithms optimize the overall goodness of the combined kernels, the kernel weights learned by MKL di-rectly reflect the relative effectiveness of each component kernel. Figure 3 plots the kernel weights learned by Sim-pleMKL and BEM-MKL respectively, where the kernel pool consists of the BOW kernel as well as each LaP kernel. The experiments were conducted on all the 10 one-vs-all splits of the RCV1 corpus and the one-vs-all splits of the 20 most frequent classes in the Yahoo! query corpus, reported were results (average weights and standard deviations) of 20 ran-dom runs using Gaussian RBF kernels. The results are quite similar when using other kernel forms.

Although the two MKL methods weigh different kernels slightly differently, they agree on the following observations: 1. BOW is significantly less effective than LaP. BOW kernel 2. Scale matters . The weights of effectiveness vary among
We use k =50, attaining 0.0025% sparsity. Both observations intuitively validate the effectiveness of the LaP text representation model.
We tested the LaP MSKs vs. BOW baselines (BOW rep-resentations with TF or TFIDF features) on text classifica-tion tasks. For both RCV1 and Yahoo! query corpora, we examined the classification performances of the SVM classi-fiers. Note that SVMs with TFIDF features are the state-of-art for this task [17, 36]. We train the SVMs on the one-vs-all splits of the training subsets. Three types of kernels (i.e., linear (Frobenius), RBF Gaussian and Jensen-Shannon ker-nels) were considered in our experiments. Reported in Table 2 are the averaged results (i.e., Micro-averaged F1 measures) over the 20 random repeats of 10-fold cross validation eval-uations (i.e., 10% data as training).
 There are some interesting o bservations from Table 2. Firstly, LaP outperforms BOW counterparts significantly for all the entries (i.e., on both corpora and for all the three kernel forms). LaP MSKs achieves amazingly up to 14.57% improvements of accuracy over BOW on short texts (the Yahoo! queries corpus). For the RCV1 corpus that contains mainly moderate-length texts, LaP MSKs outper-forms BOW by up to 4.13%, which is quite encouraging because the accuracies of BOW are already very high and thus not much room for further improvements. Based on Student t -test at level 0.05, all the improvements are sig-nificant. We also note that for both corpus, MSKs learned by SimpleMKL seem to perform the best, especially when Gaussian RBF kernels are used. Yet, the performances of BEM-MKL learned MSKs are also quite competitive, con-sidering its overwhelming computational efficiency over Sim-pleMKL.

The performance of single-scale LaP code relies largely on the choice of scales. If appropriate scales are set, single-scale LaP can perform significantly better than BOW. From Ta-ble 2, we observe that single-scale LaP with optimal scale parameters outperforms BOW (both TF and TFIDF) sig-nificantly in all the 6 entries. It is worth noting that scale selection turns to be more fundamental to short texts than normal texts. As an empirical validation, in Figure 4, for each single-scale LaP kernel computed on the Yahoo! query corpus, we plot the testing accuracy as a function of scales. As references, the performances of BOW and LaP kernels are shown as constant lines. Longer texts in general are more ro-bust to scales, although the tendency is quite similar. Due to space limitation, we did not report the results for the RCV1 corpus. FromFigure4,weseethattheperformancesofsin-gle scale LaP codes vary vastly with different scale setting. At scales that are too fine or too coarse, single-scale LaP codes even perform substantially worse than BOW. By con-sidering multiple scales simultaneously, the multi-scale LaP codes, on the other hand, is not only robust to scale selection but also able to achieve consistently better performances.
In this section, we evaluate the multi-scale language mod-els on an ad hoc text retrieval task. Our simulations were conducted on the OHSUMED [9] test collection, which is a set of 348,566 medical documents, 106 queries and 16,140 Jensen-Shannon.
 Table 3: Retrieval performance comparison. Compared relevance judgements for document-query pairs on three scales: either  X  X efinitely relevant X ,  X  X ossibly relevant X  or  X  X rrelevant X .
We tested three retrieval models described in Section 4.2: the multi-scale LMs using global matching, local match-ing with sliding window and sum operator, local match-ing with sliding widow and max operator. The unigram LMs (i.e., BOW) were used as baselines. For all the mod-els, the standard Kullback-Leibler divergence were used as relevance functions. The weights for multi-scale LMs are learned from Eqn.(12) on document pairs ( X  X artially rele-vant X  documents were treated the same as  X  X elevant X  ones) from 5 randomly sampled queries. Figure 5 shows the em-pirical distributions of the weights obtained on 20 random repeats. BOW weights contribute higher than in classifi-cation tasks, probably because the texts in the OHSUMED corpus are mainly technical words with low-frequency. How-ever, even for such corpus, BOW only occupies at most 24% of the overall effectiveness, much lower than LaP.
Table 3 reports the overall retrieval performances of the three retrieval models. We use three standard IR evaluation measures: the Mean-Average-Precision (MAP), Precision at N with N=5 and 10 (P@5, P@10). We observe that, in terms of MAP, LaP multi-scale LMs outperform BOW uni-gram LMs by (up to) 12.19%. Among the three retrieval mod-els, local matching in general outperforms global matching, while local matching with a sliding window and sum oper-ator performs the best. Based on Wilcoxon test with 0.05 significance threshold, we found, out of the 9 entries in Table 2, LaP significantly improve BOW in 6 entries.
Text representation is fundamental to almost all the text analysis tasks. Although classical BOW or unigram LMs capture semantic information quite well and have gained great success in existing systems, they lose the invaluable spatial contents and thus cannot meet the requirement for more accurate text modeling, especially for short texts. Lots of efforts have been devoted to developing advanced spatial-sensitive models, most representatively, the n -gram BOW [21], n -gram LMs [35], cashed LMs [13, 10], named entity detection [7], query segmentation [11], and string kernel [19]. These models, however, achieve no significant improvement at high prices of computational resources [34, 8].
Perhaps most related to our model is the recently pro-posed Locally Weighted Bag-of-Word (LOWBOW) frame-work [15]. Using also locally weighted smooth kernels, LOW-BOW embeds a document as a spatially smoothed curve in the high-dimensional multinomial simplex. Our LaP model can be seen as an advanced extension to LOWBOW: ( i ) LaP is locally smoothed both spatially and semantically; ( ii ) LaP represents text at multiple resolutions; ( iii )LaP outputs matrices, much simpler than the functional outputs of LOWBOW.

LaP applies 2D smoothing, which, in semantic direction, resembles the LMs smoothing [35]. Classical smoothing meth-ods (e.g., Laplacian, Jelineke-Mercer, and Dirichlet smoothers [34]) essentially amount to shrinking with reference distribu-tions, neglecting the correlation relationships among words. LaP, however, uses local smoothing which conveys seman-tic correlation into graphs and employs graph-based learn-ing. Similar approaches were also exploited in [22]. Our ap-proach, however, is evidently different: ( i )Besides semantic smoothing, LaP applies also spatial smoothing; ( ii )Instead of solving the graph-based optimization directly as in [22], LaP reduces it to kernel convolutions, which is more efficient to obtain and also allows both semantic and spatial direc-tions to be handled in a unified way; ( iii ) Instead of tuning parameters by cross-validation, LaP is parameter-free, i.e., it directly uses well-defined kernels; ( iv ) We use more reliable semantic graphs, more robust to training corpus noises.
Our model is built on the pyramid representations in im-age processing [4, 5, 18], which establish a pyramid of images operators. smoothed at progressive resolutions to capture the multi-scale characteristics of an image. LaP for the first time presents the pyramid representations for natural language texts, and opens up the possibilities to apply well-established image processing tools for text analysis.
In this paper, we have presented the Language Pyramid model for multi-scale text analysis. LaP encodes both se-mantic and spatial contents of a document as a probabilistic distribution over the joint semantic-spatial space. It em-ploys nonparametric estimation with locally weighted ker-nels, resulting in text codes smoothed both semantically and spatially at multiple resolutions. We have instantiated two frameworks for applying LaP, i.e., multi-scale text kernels, and multi-scale language models. Evaluations on benchmark corpora indicate that the proposed model and algorithms are useful and effective for text modeling.
 In principal, the more scales used in LaP, the better. Taken to the limit, we can consider an infinite number of scales, leading to a functional text representation frame-work, similar to the scale-space theory in image processing [18]. We will leave such investigation for future works.
This paper employed traditional classifiers (e.g., SVMs) for text classification. Advanced classifiers, such as the prob-abilistic models presented in [32, 30] that are capable to han-dle data ambiguity (e.g., multi-label classification, paragraph-wise classification), are also applicable.

We also plan to develop appropriate local descriptors (sim-ilar to the SIFT descriptor for image [20]) as well as interest point detectors on LaP, and empirically study the interest points based local-matching retrieval model proposed in Sec-tion 4.2.
 We thank James M. Rehg, whose computer vision lecture inspired the idea of textual pyramid. Part of this work is supported by NSF #DMS-0736328, grants from Microsoft and Hewlett-Packard, and the financial support of the 111-Project (B07022) and NSFC. [1] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. [2] M. Belkin and P. Niyogi. Laplacian eigenmaps for [3] M. Bendersky and W. B. Croft. Discovering key [4] P. J. Burt and E. H. Adelson. The laplacian pyramid [5] J. Crowley and A. C. Parker. A representation for [6] C. B. Do and A. Y. Ng. Transfer learning for text [7] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu, [8] K. Hall and T. Hofmann. Learning curved [9] W.Hersh,C.Buckley,T.J.Leone,andD.Hickam.
 [10] R. Iyer and M. Ostendorf. Modeling long distance [11] R. Jones, B. Rey, O. Madani, and W. Greiner. [12] K. Kira and L. A. Rendell. A practical approach to [13] R. Kuhn and R. De Mori. A cache-based natural [14] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. E. [15] G. Lebanon, Y. Mao, and J. Dillon. The locally [16] L. Lee. Measures of distributional similarity. In ACL [17] D.D.Lewis,Y.Yang,T.G.Rose,andF.Li.Rcv1: A [18] T. Lindeberg. Scale-space for discrete signals. IEEE [19] H. Lodhi, C. Saunders, J. S. Taylor, N. Cristianini, [20] D. G. Lowe. Distinctive image features from [21] C. D. Manning and H. Schuetze. Foundations of [22] Q. Mei, D. Zhang, and C. Zhai. A general [23] D. Metzler and W. B. Croft. A markov random field [24] D. R. H. Miller, T. Leek, and R. M. Schwartz. A [25] J. M. Ponte and W. B. Croft. A language modeling [26] A. Rakotomamonjy, F. R. Bach, S. Canu, and [27] B. Shaw and T. Jebara. Structure preserving [28] B. Tan and F. Peng. Unsupervised query segmentation [29] G. Xu, S.-H. Yang, and H. Li. Named entity mining [30] S. H. Yang, J. Bian, and H. Zha. Hybrid generative / [31] S. H. Yang and B. G. Hu. Feature selection by [32] S. H. Yang, H. Zha, and B.-G. Hu. Dirichlet-bernoulli [33] Y. Yang and J. O. Pedersen. A comparative study on [34] C. Zhai. Statistical Language Models for Information [35] C. Zhai and J. Lafferty. A study of smoothing [36] D. Zhang and W. S. Lee. Question classification using [37] J. Zhao and Y. Yun. A proximity language model for
