 In this paper, we apply stacking , an ensemble learning method, to the problem of building hybrid recommendation systems. We also introduce the novel idea of using runtime metrics which rep-resent properties of the input users/items as additional meta-features, allowing us to combine component recommendation engines at runtime based on user/item characteristics. In our sys-tem, component engines are level-1 predictors, and a level-2 pre-dictor is learned to generate the final prediction of the hybrid system. The input features of th e level-2 predictor are predictions from component engines and the r untime metrics. Experimental results show that our system outperforms each single component engine as well as a static hybr id system. Our method has the additional advantage of removing restrictions on component en-gines that can be employed; any engine applicable to the target recommendation task can be easily plugged into the system. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Information filtering, Retrieval models, Selection process. I.2.6 [ Artificial Intelligence ]: Learning . Algorithms, Design. Hybrid recommender systems, engi ne hybridization, collaborative filtering, content-based recommender, machine learning, ensem-ble learning, stacking, stacked generalization, meta-feature. In the past two decades, a number of recommendation engines have been developed for a wide range of applications. Content-based recommendation engines are typically used when a user's interests can be correlated with th e description (content) of items that the user has rated. An example is the newsgroup filtering system NewsWeeder [16]. Collaborative filtering engines are another popular type which utilize users' preferences on items to define similarity among users and/ or items. An example is the GroupLens system [15]. Othe r recommendation technologies include knowledge-based approach es, utility-based filtering, etc [9]. Previous research has shown that each of these engines has pros and cons [1, 9, 21]. For example, collaborative filtering engines depend on overlap in ratings (whether implicit or explicit) across users, and perform poorly when the ratings matrix is sparse. This causes difficulty in applications su ch as news filtering, where new items are entering the system fre quently. Content-based engines are less affected by the sparsity problem, because a user's interests can be based on very few ratings , and new items can be recom-mended based on content similar ity with existing items. How-ever, content-based engines requi re additional descriptive item data, for example, descriptions for home-made video clips, which may be hard to obtain. And e xperiments have shown that, in general, collaborative filtering engines are more accurate than content-based engines [2]. Real-world recommendation systems are typically hybrid systems that combine multiple recommendation engines to improve pre-dictions (see Burke [9] for a summary of different ways that rec-ommendation engines can be combin ed). Previous research on hybridization has mostly focused on static hybridization schemes which do not change at runtime for different input users/items. For example, one widely used hybr idization scheme is a weighted linear combination of the predictions from component engines [5, 10], where the weights can be uniform or non-uniform. Pazzani [20] also proposed a voting schema to combine recommendations. In this paper, we focus on building hybrid recommendation sys-tems that exhibit the following two properties: 1. The system should adjust how component engines are com-2. The system should allow not only linear combinations but To achieve these two goals, we apply stacking , an ensemble learning method, to solve the problem of building hybrid recom-mendation systems. The main idea is to treat component engines as level-1 predictors, and to learn a level-2 predictor for generat-ing the final prediction of the hybrid system. We also introduce the novel idea of using runtime metrics as additional meta-features, allowing us to use char acteristics of the input user/item when determining how to combine the component recommenda-tion engines at runtime. These runtime metrics are properties of the input user/item that are related to the precisions of the compo-nent engines. For example, th e number of items that the input user has previously rated may indicate how well a collaborative filtering engine will perform. By employing different learning algorithms for learning the level-2 predictor, we can build sys-tems with either linear or non-lin ear combinations of predictions from component engines. We na me our method and the resulting system STREAM ( ST acking R ecommendation E ngines with A dditional M eta-features). The paper is organized as follows. In the next section, we discuss related work. Section 3 describe s our STREAM approach in de-tail. Section 4 demonstrates how to build a STREAM system for a movie recommendation applicati on and discusses how to apply the concepts to other domains. Section 5 presents experimental results. Section 6 concludes the paper with discussion. The BellKor system that won the first annual progress prize of the Netflix competition [6, 19] is a statically weighted linear combi-nation of 107 collaborative filtering engines. The weights are learned by a linear regression on the 107 engine outputs [5]. This method is actually a special case of STREAM wherein no runtime metrics are employed and the level-2 predictor is learned by linear regression. Some hybrid recommendation systems choose the  X  X est X  compo-nent engine for a particular input user/item. For example, the Daily Learner system [7] selects the recommender engine with the highest confidence level. However, this method is not gen-erally applicable for two reasons. First, not all engines generate output confidence scores for their predictions. Second, confi-dence scores from different engine s are not comparable. Scores from different recommendation engi nes typically have different meanings and may be difficult to normalize. There are also hybrid recommendation systems that use a linear combination of component engines with non-static weights. For example, the P-Tango system [10] combines a content-based en-gine and a collaborative filtering engine using a non-static user-specific weighting scheme: it initially assigns equal weight to each engine, and gradually adjusts the weights to minimize prior error as users make ratings. Th is scheme combines engines in different ways for different input users. However, the prior error of an engine may not be a sufficient indicator of the quality of its current prediction. For instance, the prior error of a collaborative filtering engine is probably lower than that of a content-based engine for a user who has rated 100 items. But if the two engines are asked to predict this user X  X  rating on a new item, the content-based engine will probably make a better prediction because the collaborative filtering engine is unable to predict ratings for new items. Another disadvantage of th is method is the rise in compu-tational cost of minimizing the prior error as ratings accumulate. There have been several research efforts to apply machine learn-ing / artificial intelligence methods to the problem of combining different recommendation technologi es (mostly content-based and collaborative filtering). These typically focus on building unified models that combine features designed for different recommenda-tion technologies. For example, Basu, Hirsh &amp; Cohen applied the inductive rule learner Ripper to the task of recommending movies using both user ratings and content features [4]. Basilico &amp; Hof-mann designed an SVM-like model with a kernel function that is based on joint features of user ra tings as well as attributes of items or users [3]. Our goal in this paper, however, is to build hybrid recommendation systems that combine the outputs of indi-vidual recommendation engines into one final recommendation. We treat the component engine s as black boxes, making no as-sumption on what underlying algorithms they implement. In the latter sections of this paper, we will show that any engine appli-cable to the target recommendation task can be easily plugged into our STREAM system. Anytime a new engine is added or an old engine is removed, all we n eed to do is re-learn the level-2 predictor. This allows system de signers to flexibly customize the hybrid recommendation system with their choice of component engines. In this section, we first introduce the stacking method in ensemble learning. We then describe how we apply it to solve the engine hybridization problem with runtime metrics as additional meta-features. Finally we demons trate our STREAM framework. Stacking (also called Stacked Generalization ) is a state-of-the-art ensemble learning method that has been widely employed in the machine learning community. The main question it addresses is: given an ensemble of classifiers learned on the same set of data, can we map the outputs of these classifiers to their true classes? The stacking method was first introduced by Wolpert in [26]. The main idea is to first learn mu ltiple level-1 (base) classifiers from the set of original training examples using different learning algorithms, then learn a level-2 (meta) classifier using the predic-tions of the level-1 classifiers as input features. The final predic-tion of the ensemble is the prediction of the level-2 classifier. Training examples for the level-2 classifier are generated by per-forming cross-validation [12] on the set of original training exam-ples. The idea of stacking classifiers was extended to stacking regressors by Breiman [8], where both level-1 predictors and the level-2 predictor are regression models that predict continuous values instead of discrete class labels. The level-2 predictor can be learned using a variety of learning algorithms. We call these learning algorithms meta-learning algorithms in order to distinguish them from the learning algo-rithms used to learn the level-1 predictors. Dzeroski &amp; Zenko [11] empirically compared stacking with several meta-learning algorithms, reaching the conclusion that the model tree learning algorithm outperforms others. They also reported that stacking with model trees outperforms a simp le voting scheme as well as a  X  X elect best X  scheme that selects the best of the level-1 classifiers by cross-validation. We are addressing the problem of combining predictions from multiple recommendation engines to generate a single prediction. To apply the stacking method to the engine hybridization prob-lem, we first define each component recommendation engine as a level-1 predictor. We treat each engine as a black box that returns a prediction given the input. Then we learn a level-2 predictor, using a meta-learning algorithm, w ith predictions of the compo-nent engines as meta-features. The level-2 predictor can be either a linear function or a non-linear function based on the meta-learning algorithm employed. This satisfies one of our two goals: support for non-linear combinations of predictions from compo-nent engines, as well as linear combinations. However, this method fails to ach ieve the other goal: we want a system that can adjust how the component engines are combined depending on the input values. For example, suppose there are two users A and B, with user A rating only 5 items, while user B rates 100 items. It is likely that the collaborative filtering engine works better for user B than for user A, while the content-based engine may work equally well for both of them. Thus, the weight on the collaborative filtering engine should be higher when the system is predicting for user B than for user A. To achieve our goal of a system that adapts to the input, we define new meta-features that indicate the expected quality of the predic-tions from the component engines. These new meta-features are properties of the input users/items that can be computed at run-time, in parallel with the predicti ons from the component engines. We call these new meta-features runtime metrics . For example, the runtime metric,  X  X he number of items the input user has previ-ously rated, X  might be applicable to the problem in the previous paragraph. In general, the r untime metrics are both application domain specific and component engi ne specific. Therefore, we cannot define a set of runtime metrics that work for all applica-tions. Instead, in the next section we will describe a set of run-time metrics defined for a movi e recommendation application and discuss general characteristics of these metrics for other applica-tions. Figure 1 illustrates our STREAM fra mework. To be concrete, we assume that the recommendation task is to predict R( u , i ), the rating of the input user u on the input item i . We call the input ( u , i ) a user-item pair. The system's background data consists of historical ratings known to the system and possibly additional information such as item content. The framework does not place restrictions on the algorithms used inside the component engines. The only requirement for an engine is that given an input user-item pair and a set of background data, it must return a predicted rating. MetricEvaluator is a component for computing the run-time metrics. The engines' predictions &lt; P 1 , P values of the runtime metrics &lt; M 1 , M 2 , ..., M m level-2 predictor f ( X ), which is a function of the engines X  predic-tions and the runtime metrics, to generate a final prediction R( u , i ). Figure 2 shows the underlying meta-learning problem in STREAM. The input vector to the level-2 predictor is in the top dotted ellipse and the output value of the level-2 predictor is in the bottom dotted ellipse. This gives us a standard machine learn-ing problem. To learn the model, we first generate a set of train-ing examples in the format (&lt; M 1 , M 2 , ..., M m , P where M i is the value of the i-th runtime metric evaluated for a user-item pair, P j is the prediction of the j-th item. We then apply an appr opriate meta-learning algorithm to learn a model from these training examples. If the ratings are ordered numbers, this meta-learni ng problem is a regression prob-lem. If the ratings are unordered categories (e.g.,  X  X uy X  /  X  X o Buy X ), this meta-learning problem is a classification problem. To generate the training examples , we perform a cross-validation on the background data. The general idea is to simulate real test-ing by splitting the original background data into two parts: cv_background data which is used as background data for the component engines and th e MetricEvaluator, and cv_testing data on which the learned model is tested. For each user-item pair in the testing data, an input vector &lt; M 1 , M 2 , ..., M can be generated by running the MetricEvaluator on the input user-item pair and by requesti ng predictions of the component engines. The true rating for this user-item pair ( P giving us a complete training example. Predicting users' movie ratings is one of the most popular bench-mark tasks for recommender systems. Ratings are typically rep-resented by numbers between 1 a nd 5, where 5 means  X  X bsolutely love it X  and 1 means  X  X ertainly not the movie for me X . There are several publicly available data se ts for this problem. To demon-strate our STREAM method, we built a movie recommendation system and evaluated it on the wi dely used MovieLens data set [18]. This data set consists of 100,000 ratings from 943 users on 1682 movies ---each user rates 4.3% of the movies on average. Each record in this da ta set is a triplet &lt; user , item , rating &gt;. The MovieLens data set contains only the title, year, and genre for each movie. This is insufficient for useful recommendations from a content-based engine. Ther efore, we augmented this data set with movie information extracted from the IMDb movie con-tent collection [14]. After augm entation, the movie contents in-cluded the title, year, genre, keywords, plot, actor, actress, direc-tor, and country. Note that not all movies contain complete in-formation; some fields are missing for some movies. Three widely-used but signifi cantly different recommendation engines were chosen for the system: a user-based collaborative filtering engine, an item-based collaborative filtering engine, and a content-based engine.
 Our user-based collaborative filte ring engine is built according to the basic algorithm described in [13]. The similarity between two users is defined by the Pearson Co rrelation. To predict the rating of the user u on the item i , this engine selects the most similar 300 users as u  X  X  neighborhood, and outputs the average of the neigh-bors' ratings on the item i weighted by the corresponding similari-ties. Our item-based collaborative filtering engine is built according to the basic algorithm described in [23]. The similarity between any two items is defined as the Pear son Correlation between the rating vectors of these two items, after normalization to the interval between 0 and 1. To predict the rating of the user u on the item i , this engine computes the similarities between item i and all items u has rated, and outputs the average rating of all items u has rated weighted by the similarities between them and item i . Our content-based engine is the same as the item-based collabora-tive filtering engine except that the item similarity is defined as the TF-IDF similarity [22] calculated from the movie contents. Apache Lucene [17] is employed to compute the TF-IDF scores. There are cases where one or more engines are unable to make predictions. For example, none of the three engines can predict for new users who do not have ratings recorded in the background data. Similarly, the two collaborative filtering engines cannot predict users' ratings on items that no one has yet rated. Our en-gines will predict the overall median rating in its background data if their underlying algorithms are unable to make predictions. The runtime metrics were designe d based on characteristics of the component recommendation engines. We sought measures that we expected to correlate well with the performance of each en-gine, and that would distinguish between them. We considered the following general characteristics of the engines: 1) the user-based collaborative filtering engine works well for users who have rated many items before but not for users who have rated few items. It also works poorly for the users who tend to rate items that no one else rates; 2) the item-based collaborative filter-ing engine works well for items th at have been rated by many users but not for items that few us ers have rated; 3) the content-based engine performs consisten tly no matter how many items the input user has rated and how ma ny users have rated the input item, but it works poorly when the content of the input item is incomplete or non-descriptive. Based on these properties, we design the runtime metrics. Table 1 shows the runtime metrics we have defined for the movie recommendation application and the three engines described runtime metrics are normalized into the range between 0 and 1 by dividing by the corresponding maximum possible value (e.g., total number of items for RM 1 ). Note that these eight runtime metrics are ones that we consider related to the performance of the th ree component engines. It is by no means a complete set, and others might define different runtime metrics, even for the same engines. On the other hand, we will show in the next section that it is unnecessary to use all eight runtime metrics. Using just a subset of these metrics, we can achieve almost the same performance as using them all. It is important to note that runtime metrics are specific to both an application domain, and to the specific engines to be hybridized. Application specificity in designing our metrics can be seen in the use of user ratings and item cont ents, integral to the movie rec-ommendation application. For ot her applications, other runtime metrics would be defined. Fo r example, in an online shopping application, one could define a binary runtime metric  X  X hether the user inputs query words X  because one might expect that the content-based engine will work better when query words are pre-sented. Engine specificity can also be seen in our runtime metrics. For example, we expect better performance of the user-based collabo-rative filtering engine as values of RM 5 rise, because this engine X  X  predictions improve when more neighbors have rated the same item. Similarly, the content-ba sed engine should perform better when RM 8 is higher, because the content similarity computed for this item has higher accuracy when the content is more descrip-tive. Some of the runtime metrics are predictive of the perform-ance of multiple engines. For example, we expect all three en-gines to perform better when RM 1 is higher, but the two collaborative filtering engines to be affected by this runtime metric more than the content-based engine. It is important to select metrics that do a good job of differentiating the engines, i.e., that show a different respons e across the range of values for each engine. There are several properties of the target application to be consid-ered when choosing meta-learning algorithms:  X  Is the final prediction numerical or unordered categorical? If  X  Are the input features (predic tions from components engines) In the movie rating application, both input features and final pre-diction are numerical (real numbers between 1 and 5). Therefore, we tested the following three learning algorithms: 1. Linear regression : learns a linear function of the input 2. Model tree [24]: learns a piece-wise linear function of input 3. Bagged model trees : bagged version of model trees. Bag-We use the implementations of these three algorithms in Weka, a widely-used open source machine learning library [25]. The size of the bagged ensemble is set to 10 for the bagged model trees algorithm. The default values in Weka are retained for other learning algorithm parameters. In this section, we evaluate the performance of our STREAM system on the MovieLens data set and compare with the perform-ance of each component engine as well as a static equal-weight hybrid system. We also compare the effectiveness of the three learning algorithms in our STREAM system, and evaluate the utilities of different sets of runtime metrics. We randomly split the entire MovieLens data set into a training set with X% ratings and a testi ng set with (100-X)% ratings. The split is performed by putting all ratings in one pool and randomly picking ratings regardless of the us ers. The training set serves as background data for all three component engines and the Met-ricEvaluator. In addition, the background data for the content-based engine includes content from all movies; this is reasonable, since movie content is available wh ether or not any given film has compare the predicted rating with the true rating using mean ab-solute error (MAE), a widely -used metric for recommendation system evaluation. Smaller MAE means better performance. We vary the value of X from 10 to 90 in order to evaluate the performance of the system under different sparsity conditions. The background data is sparser when the value of X is smaller. For each value of X, we repeat the random split 10 times and report the average performance of the system. In each experiment, the level-2 predictor is learned by individu-ally running the three meta-learning algorithms on the training examples generated by performing a 10-fold cross-validation on the training set (X% of total data). The cross-validation is per-formed as described in section 3.3. The number of training ex-amples generated is the same as the size of the background set. For the MovieLens data set, th is number is 10,000 for X=10 and 90,000 for X=90. Since the model to be learned only has 11 input features (three engine predictions plus eight runtime metrics), it is not necessary to use all the training examples. Therefore we ex-tract a random sample of 5,000 training examples for learning the level-2 predictor. Figure 3 compares the performance of the different systems. The three dotted curves correspond to the three single component engines: the user-based collaborative filtering engine, the item-based collaborative filtering engine, and the content-based engine. As anticipated, the two collaborative filtering engines perform badly when X is small due to th e sparsity problem and their per-formance improves quickly as X increases, while the content-based engine's performance is less sensitive to the value of X, yielding a much flatter curve. The  X  X qual Weight Linear Hybrid X  curve in the figure corre-sponds to a static linear hybrid of the three engines with equal better than the single engines. One possible explanation is that averaging the predictions from the three engines reduces the vari-ance of the predictions. The  X  X TREAM -linear regression X ,  X  X TREAM -model tree X  and  X  X TREAM -bagged model trees X  cu rves show the performance of our STREAM system with thr ee different meta-learning algo-rithms. All three systems are c onsistently better than the equal weight hybrid. The bagged model trees algorithm is slightly bet-ter than the model tree algorithm, and they are both better than the linear regression algorithm. Note that some of the runtime metrics are engine-specific and computationally expensive. For example, RM 5 involves a com-pute-intensive neighborhood search operation that is specific to the user-based collaborative filtering engine. We want to elimi-nate such expensive runtime metrics and find a small set that are easily computed but still provide good results. This corresponds to the feature selection problem in machine learning because the runtime metrics are employed as input features for the meta-learning problem. Therefore, we conduct experiments to compare the STREAM system with different sets of runtime metrics. We use the bagged model trees algorithm as the meta-learner, since it gave the best results in the previous experiment. Experimental results are shown in Figure 4. The  X  X TREAM  X  No Runtime Metric X  curve shows the performance of the STREAM system without any runtime metric s, using only the predictions of the three engines as input meta-features to the level-2 predictor. The curve shows consistently poorer performance than the sys-tems with runtime metrics, esp ecially when the value of X is small. We believe this results from having many users who rated few items when X is small; the runtime metrics let the system weight the content-based engine more heavily when predicting for these users. The  X  X TREAM -8 Runtime Metrics X  curve shows the perform-ance of the STREAM with all eight runtime metrics, while the  X  X TREAM -2 Runtime Metrics X  curve shows the performance with only two runtime metrics: RM 1 and RM 2 . We selected these two runtime metrics because they reflect local sparsity for the input user and item, and are easy to compute. The curve shows that using only these two metrics, the system can achieve ap-proximately the same performance as the system with all eight runtime metrics; adding additiona l metrics does not necessarily improve the performance of the system. In this paper, we introduce STREAM, a novel framework for building hybrid recommendation systems by stacking recommen-dation engines with additional meta -features. In this framework, the component engines are treated as level-1 predictors, with a level-2 predictor generating the final prediction by combining component engine predictions with runtime metrics that represent properties of the input users/items. The resultant STREAM sys-tem is a dynamic hybrid recommendation system in which the component engines are combined in different ways for different input users/items at runtime. E xperimental results show that the STREAM system outperforms each single component engine in addition to a static equal weight hybrid system. This framework has the additional advantage of placing no restrictions on compo-nent engines that can be employed; any engine applicable to the target recommendation task can be easily plugged into the system. Since the STREAM framework is about hybridizing recommen-dation engines, we do not consider the computational cost of the component engines. The only concern is the additional computa-tional cost of the STREAM system over the cost of the compo-nent engines. We identify two di fferent costs: runtime cost and offline cost. At runtime, the STREAM system incurs additional computation for the runtime metrics and for evaluation of the prediction model on the current inputs. If chosen carefully, the runtime metrics can be computed quickly. For example, the run-time metrics RM 1 and RM 2 in Table 1 can be stored in a look-up table, with a table update whenever there is a new rating. The cost of evaluating the prediction model (a linear or non-linear function) depends on the learning algorithms used. Model-based algorithms, such as the three employed in our experiments, com-pute predictions very quickly. The offline cost of learning the prediction model is high, however. Most of the time is spent gen-erating the training examples from the background data. In our experiments, the training exampl e generation is performed by 10-fold cross-validation, which suggests we need to re-learn the pre-diction model only when 10 percent or more of the data has changed 1 . In summary, the STREAM system has a low runtime overhead, while offline model lear ning is costly, but can be per-formed infrequently. Depending on the meta-learning algorithm employed, it is possi-ble for the STREAM system to make predictions without running all component engines. For exampl e, if the prediction model is a linear function, there is no need to run any engines with coeffi-Further experiments show that prediction models learned by leave-one-out and 10-fold cross-validation have approximately the same performance. Therefore, offline model learning when 10 percent or more of the data has changed is as good as online model learning for every single piece of new data. cients close to zero. For more complex models, we may create a decision process that decides at runtime for each input user/item which component engine(s) to run, taking into account both the prediction model and the values of the runtime metrics. Our ultimate goal is to enable building of application-specific hybrid recommendation systems fro m sets of individual engines by a computer engineer who is not an expert in recommender technology. However, the runtime metrics used in our experi-ments are manually defined by domain experts who have some knowledge of how properties of the input users/items are related to the quality of the engines X  predictions. Automatic or semi-automatic discovery of runtime metrics given the target applica-tion and the individual engines will be an interesting subject of future research. Finally, given a set of runtime metr ics, we want to further investi-gate how to identify the best s ubset. As shown in the experi-ments, incorporating more run time metrics does not necessarily increase the performance of the system. There is a tradeoff be-tween the system performance and the computational cost. Since the runtime metrics are employed as additional input features to the machine learning problem, we plan to apply feature selection technologies to select runtime metrics. We wish to acknowledge contributions to the ideas and ap-proaches in this paper. Ravi Konuru, Claudia Perlich, Yan Liu, Vittorio Castelli and Thomas G. Dietterich all helped to shape this research. [1] G. Adomavicius and A. Tuzhilin. Toward the next genera-[2] J. Alspector, A. Kolcz and N. Karunanithi. Feature-based [3] J. Basilico and T. Hofmann. Unifying collaborative and con-[4] C. Basu, H. Hirsh, and W. Cohen. Recommendation as clas-[5] R. Bell, Y. Koren, and C. Volinsky. The bellkor solution to [6] R. Bell, Y. Koren, and C. Volinsky. Chasing $1,000,000: [7] D. Billsus and M. J. Pazzani. User modeling for adaptive [8] L. Breiman. Stacked regressions. Machine Learning , 24:49 X  [9] R. Burke. Hybrid recommender systems: Survey and ex-[10] M. Claypool, A. Gokhale, T. Miranda, P. Murnikov, D. [11] S. Dzeroski and B. Zenko. Is combining classifiers with [12] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of [13] J. Herlocker, J. Konstan, A. Borchers, and J. Riedl. An algo-[14] IMDb. Internet movie database. downloadable at [15] J. Konstan, B. Miller, D. Maltz , J. Herlocker, L. Gordon, and [16] K. Lang. Newsweeder: Learning to filter netnews. In Pro-[17] Lucene. Apache lucene. h ttp://lucene.apache.org/, 2008. [18] MovieLens. http://www.gr ouplens.org/node/73, 1997. [19] Netflix prize, http://www.netflixprize.com/. [20] M. J. Pazzani. A framework for collaborative, content-based, [21] M. Ramezani, L. Bergman, R. Thompson, R. Burke, and B. [22] G. Salton and M. J. McGill. Introduction to Modern Infor-[23] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based [24] Y. Wang and I. H. Witten. I nducing model trees for continu-[25] I. H. Witten and E. Frank. Data Mining: Practical machine [26] D. H. Wolpert. Stacked generalization. Neural Networks , 
