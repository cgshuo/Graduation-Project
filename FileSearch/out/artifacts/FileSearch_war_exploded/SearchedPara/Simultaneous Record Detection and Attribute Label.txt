 Recent work has shown the feasib ility and promise of template-independent Web data extraction. However, existing approaches use decoupled strategies  X  attemp ting to do data record detection and attribute labeling in two separa te phases. In this paper, we show that separately extracting data records and attributes is highly ineffective and propose a probabilistic model to perform these two tasks simultaneously. In our approach, record detection can benefit from the availability of semantics required in attribute labeling and, at the same time, the accuracy of attribute labeling can be improved when data records are labeled in a collective manner. The proposed model is called Hierarchical Conditional Random Fields. It can efficiently in tegrate all useful features by learning their importance, and it can also incorporate hierarchical interactions which are very impor tant for Web data extraction. We empirically compare the proposed model with existing decoupled approaches for product information extraction, and the results show significant improveme nts in both record detection and attribute labeling. Recognition]: Models -Statistical Keywords: Data record detection, Attribute labeling, Web page segmentation, Conditional Random Fields, Hierarchical Conditional Random Fields. The World Wide Web is a vast a nd rapidly growing repository of information. There are various ki nds of objects, such as products, people, conferences, etc., embedded in both statically and dynamically generated Web pages. Recent work has shown that using template-independent appro aches to extracting meta-data for the same type of real-world objects is feasible and promising. However, existing approaches us e highly ineffective decoupled strategies -attempting to do data record detection and attribute labeling in two separate phases. This paper studies how to extend existing Web data extraction methods to achieve the mutual enhancement of record detection and attribute labeling. We begin by illustrating the problem with an example, drawn from an actual application of product information extraction. The goal of the application is to extract meta-data about real-world products from every product page on the Web. Specifically, for each crawled Web page, we first use a classifier to decide whether description of each product from detected product pages. Our statistical study on 51K ra ndomly crawled Web pages shows that about 12.6 percent are product pages. That is, there are about 1 billion product pages within a search index containing 9 billion crawled Web pages. If all of these pages or just half of them are correctly extracted, we will have a huge collection of meta-data about real-world products that could be used for further knowledge discovery and data management tasks such as comparison shopping and user intention detection. However, how to extract produc t information from Web pages generated by many (maybe tens of thousands of) different templates is non-trivial. One possible solution is that we first distinguish Web pages ge nerated by different templates, and then build an extractor for each template. We say that this type of solution is template-dependent . However, accurately identifying Web pages for each template is not a trivial task because even Web pages from the same website may be generated by dozens of templates. Even if we can di stinguish Web pages, template-dependent methods are still impractical because the learning and maintenance of so many diffe rent extractors for different templates will require substantial efforts. Fortunately, recent work in [18][31][27][33] has shown the feasibility and promise of template-independent meta-data extraction for the same type of objects, and it is possible to simply combine existing techniques to build a template-independent meta-data extractor for product pa ges. Two specific types of Web pages are treated differently by the existing extraction techniques: list pages and detail pages 1 . z List pages are web pages containing several structured data Our empirical study shows that about 35% of product pages are list pages and the rest are detail pages. *The work is done when the author is visiting Microsoft Research Asia. z Detail Pages are Web pages containing only detailed However, it is highly ineffective to use decoupled strategies  X  attempting to do data record det ection and attribute labeling in two separate phases . This is because: Error Propagation: Since record detection and attribute labeling are two separate phases, the erro rs in record detection will be propagated to attribute labeling. Thus, the overall performance is limited and upper-bounded by that of record detection. Suppose record detection and attribute la beling have precisions 0.8 and 0.9 respectively, then the overall precision will be no more than 0.8. If they also perform independently, the precision will be 0.72. Lack of Semantics in Record Detection: Human readers always take into account the semantics of the text to understand Web pages. For instance, in Figure 1(a), when claiming a block is a data record, we use the evidence that it contains a product's name, image, price and description. Thus, a more effective record detection algorithm should take into account the semantics of the text, but existing methods [18][31][27] do not consider that. Lack of Mutual Interactions in Attribute Labeling: The data records in the same page are related. They always share a common template and the elements at the same position of different records always have sim ilar features and semantics. For example, in Figure 1(a) the button "Add to Cart" appears in the same position in both records and the element on the left-top of each record is an image. So, if we label the elements of the records within the same page toge ther in a collective manner, it is easier for us to detect that the repeated elements "Add to Cart" are less informative and more likely to be noise. However, existing method [33] fails to consider that because the data records are independently labeled. First-Order Markov Assumption: For Web pages and especially for detail pages, l ong distance dependencies always exist between different attribute elements. This is because there are always many irrelevant elements (i.e. noise) appearing between the attributes of an object. For example, in Figure 1(b) there is substantial noise, such as "Add to Cart" and "Select Quantity" between the price and description. However, traditional CRFs [17][33] cannot incorporate these long distance dependencies because of its firs t-order Markov assumption (i.e. only the dependencies of neighbor ing nodes are considered and represented). In this paper, we propose a novel graphical model called Hierarchical Conditional Random Field (HCRF) model to jointly optimize record detection and attribute labeling. By using the vision-based page segmentation (VIPS) technology [4], which makes use of page layout features such as font, color, and size to construct a vision-tree for a Web page, we can get a better representation of a page co mpared with the commonly used tag-tree. Given a vision-tree, record detection can be considered as the task of locating the minimum set of elements that contain the content of a record. In this way, both record detection and attribute labeling become the task of assigning labels to the nodes on a vision-tree, so they can be integrated in one probabilistic model as in this paper. In contra st to existing decoupled strategies that perform record detection and attribute labeling as two separate phases, our approach le verages the labeling results of attribute labeling for record detection, and at the same time benefits from the incorporation of some global features based on tree-alignment for attribute labeli ng. As a conditional model [17], HCRF can efficiently incorporate any useful feature for Web data extraction. By incorporating hierarchical interactions, HCRF could incorporate long distan ce dependencies and achieve promising results on detail Web pages. To the best of our knowledge, our approach is the first to simultaneously conduct record det ection and attribute labeling for both list and detail pages. Speci fically, we make the following contributions. z A mutually beneficial integrati on of data record detection and z A novel graphical model called Hierarchical Conditional z An empirical study of our approach on the task of product The rest of the paper is organized as follows. Section 2 formally defines our problem. Section 3 discusses the proposed Hierarchical Conditional Random Fi elds. Section 4 defines label spaces for the model. Section 5 presents the features we currently used. Section 6 presents our ev aluation results and discussions. Section 7 discusses related work a nd section 8 brings this paper to a conclusion. representation format for Web pages. Good representation can make the extraction task easier and improve extraction accuracy. In most previous work, tag-tree, which is a natural representation of the tag structure, is commonl y used to represent a Web page. However, as [4] pointed out, tag-trees tend to reveal presentation structure rather than content structure, and are often not accurate enough to discriminate different semantic portions in a Web page. Moreover, since authors have di fferent styles to compose Web pages, tag-trees are often complex and diverse. To overcome these difficulties, [4] proposed a vision-based page segmentation (VIPS) approach. VI PS makes use of page layout features such as font, color, and size to construct a vision-tree for a page. It first extracts all suitable nodes from the tag-tree, and then finds the separators between these nodes. Here, separators denote the horizontal or vertical lines in a Web page that visually do not cross any node. Based on these separators, the vision-tree of the Web page is constructed. Each node on this tree represents a data region in the Web page, which is called a block . The root block represents the whole pa ge. Each inner block is the aggregation of all its child blocks. All leaf blocks are atomic units ( i.e. elements) and form a flat segmentation of the Web page. Since vision-tree can effectively keep related content together while separating semantically diffe rent blocks from one another, we use it as our data representa tion format. Figure 2 is a vision-tree for the page in Figure 1(a), where we use rectangles to denote inner blocks and use ellipses to de note leaf blocks (or elements). Due to space limitations, the blocks denoted by dotted rectangles are not fully expanded. Based on the definition of vision-tree, we can now formally define the concepts of record detection and attribute labeling. Definition 2.1 (Record detection) : Given a vision-tree, record detection is the task of locating the minimum set of blocks that contain the content of a record. For a list page containing multiple records, all the records need to be identified. In other words, record detection is e qual to assigning  X  X ata Record X  labels to the blocks in the vision-tree. For instance, for the vision-tree in Figure 2, two blocks are detected as data records. Definition 2.2 (Attribute labeling) : For each identified record, attribute labeling is the process of assigning attribute labels to the leaf blocks (or elements) within the record. In Figure 2, for the two records, attribute labels Name, Image, Price, Description (Desc) and Note are assigned to different parts within the records. We use Note to denote uninteresting elements or Web noise. It is obvious that, by sequentially combining record detection and attribute labeling algorithms, a complete algorithm can be constructed to extract records and the attributes. However, as explained in the introduction, this decoupled strategy will be highly ineffective. Therefore, in this paper, we explore an integrated approach that can simultaneously conduct record extraction and attribute labeling and achieve a joint optimization of both tasks. Based on the above definitions , both record detection and attribute labeling are the processes of assigning labels to blocks of the vision-tree for a page, although the labels they use are different. Therefore, we can use a uniform probabilistic model to deal with both tasks. Formally, we define the Web data extraction problem as: Definition 2.3 (Joint optimization of record extraction and = L be the features of all the blocks and each component x i is a feature vector of one block, and let = L be one possible label assignment of the corresponding blocks. The goal of Web data extraction is to compute maximum a posteriori (MAP) probability of y and Based on the above definition, the main difficulty is the calculation of the conditional probability (y | x) p . In this paper, we introduce a Hierarchical Conditional Random Fields algorithm to compute it, which will be describe d in detail in the next section. In this section, we first in troduce some basic concepts of Conditional Random Fields and then describe our proposed model for integrated Web data extraction, including how to learn the parameters and how to perform extraction. Conditional Random Fields (CRFs) [17] are Markov random fields that are globally conditioned on observations. Let Y and X . X are variables over the observations to be labeled and Y are variables over the corresponding labels. The random variables Y could have non-trivial structure, such as linear-chain in [17] and 2D grid in [33]. Although all the components of Y are usually assumed to range over a single label space Y , this assumption is not an innate character of CRFs and we will see that in our proposed model ther e are two label spaces for two different types of variables. Th e conditional distribution of the labels y (an instance of Y ) given the observations x (an instance of X ) has the form, where C is the set of cliques in G ; y| c are the components of y associated with the clique c ; c  X  is a potential function defined normalization factor or partition function, and has the form, The potential functions are expressed in terms of feature functions (y | , x) f and their weights k  X  : Hierarchical Conditional Random Field (HCRF) model can be easily constructed. For the page in Figure 1(a), the model is shown in Figure 3, where we also use rectangles to denote inner nodes and use ellipses to denote l eaf nodes. The dotted rectangles are for the blocks that are not fully expanded. Each node on the graph is associated with a random variable i Y . We currently model the interactions of sibling variables via a linear-chain, although more complex structure such as two-dimensional grid can be used. The observations which are globally conditioned on are omitted from this graph for simplicity. Here, we assume that every inner node contains at leas t two children. Otherwise, we replace the parent with its si ngle child. Notice that this assumption has no affect on the performance because the parent is identical to its child in this case. We made this assumption just for easy explanation and implementation. The cliques of the graph in Figure 3 are its vertices, edges and triangles. So, the conditional probability in formula (1) can be concretely expressed as, p where of cliques (i.e. vertex, edge and triangle) respectively;  X  are the corresponding weights; vV  X  , eE  X  and t is a triangle. Although the feature functi ons can take real values, here we assume they are Boolean, that is, true if the feature matches and otherwise false . Some examples could be found in section 5. Given the training data distribution (x, y) p % , the parameter learning problem is to find a set of parameters 12 12 12 {, , ;, , ;, ,}  X   X  X  X  X  X   X  = LLL that optimize the log-likelihood function: To avoid over-fitting, we also use the spherical Gaussian prior with mean 0  X  = and variance matrix 2 I  X   X = to penalize the log-likelihood. The concave function can be optimized by the techniques used in other models [17]. We use the gradient-based L-BFGS [21] for its outstanding performance over other algorithms [22]. Each element of the gradient vector is given by, where distribution, and it can be easily calculated and once for all; Ef  X  is the expectation with respect to the model distribution. For example, the expectation of where y| e is any possible label assignment of the variables on the edge e . Thus, the main computation is to compute the marginal every iteration. As the graph in Figure 3 is a chordal graph, we can efficiently do inference by using the junction tree algorithm [8]. For an undirected graph, this algorithm consists of three steps: junction tree construction, initiali zation, and belief propagation. It is straightforward to construct a junction tree for a chordal graph. Figure 4 is the junction tree of the sub-graph starting from the node 0 in Figure 3, and we will denote it by T in the rest of Figure 3. The model X  X  graph of the page in Figure 1(a). the paper. In Figure 4, we use ellipses to denote clique nodes and use rectangles to denote separators. All the variables are denoted by their subscripts. Note that all the clique nodes on the junction tree have size 3 because the maximum cliques in the original graph are triangles. Once the junction tree has been constructed, the initialization can be finished with first initializing all the potentials on T to have value unity, and then multiplying the potential of a vertex, an edge or a triangle into the potential of one of the clique nodes of T which cover the variables of the vertex, the edge or the triangle. The potential of a vertex v , an edge e and a triangle t is computed as follows respectively, (y | , x) exp (y | , x)  X  X   X  X  X  =  X  X  X  Finally, we use the two-phase schedule algorithm [14] to schedule belief propagation on T . This algorithm requires the selection of an arbitrary clique node to be th e root, and then is followed by a collection phase and a distribution phase. Once these two phases are finished, the potentials on all the cliques and separators are marginal potentials. The marginal probabilities can be directly computed with the marginal potentials by normalization. As we have described in section 2. 3, we are interested in finding the most probable configuration of Y . This can be done using the same junction tree algorithm [8] just with a simple modification of the two-phase schedule algorithm [14] by merely replacing the summation, which is an operator needed in the two-phase schedule algorithm, by maximization everywhere. After the modified algorithm has finished, the most likely assignment for each variable can be found from the potentials of any clique that contains the variable. The junc tion tree X  X  running intersection property guarantees that the most likely states of a variable found on different cliques are the same. We refer the readers to [8] for more details about the junction tree algorithm. When applying HCRF models for Web data extraction, we must determine the label spaces for variables Y . We now explain how to define these labels and also give an example of the label spaces which are used in our product information extraction. When defining the label spaces, we need to distinguish between the variables at leaf nodes and th e variables at inner nodes. For Table 1. Label spaces for product information extraction Type Label Semantic Meaning Leaf Label Space Inner Label Space variables at leaf nodes, we are interested in deciding whether a leaf block is an attribute value of the object we want to extract. However, for variables at inner nodes, our interest shifts to the understanding of whether an inner block is a data record. So, we leaf nodes and inner label space for variables at inner nodes. The leaf label space consists of all the attribute names of the object we want to extract. For example, in product information extraction, we want to extract a product X  X  name, image, price and description, so the leaf label space consists of Name , Image , Price , Desc , and Note . The label Note is for the data we are not interested in (i.e. noise). All the labels for product information extraction are listed in Table 1 with their semantic meanings. The inner label space can be partitioned into an object-type independent part and an object-type dependent part. We explain how to define these two parts in turn: Object-type Independent Labels: Since we want to extract data from Web pages, the labels Page Head , Page Tail , Nav Bar , and Info Block are naturally required to denote different parts of a Web page. The labels Data Record and Data Region are also required for detecting data records. To denote blocks that do not contain any meaningful information, such as the attributes to be extracted and the head, tail or navigation bar of a Web page, the label Note Block is also required. All these labels are general to any Web data extraction problem, and they are independent of any specific object type. Object-type Dependent Labels: Between data record blocks and leaf blocks, there are intermediate blocks on a vision-tree. So, we must define some intermediate labels between Data Record and the labels in the leaf label space. These labels are object-type dependent because these intermediate blocks contain some object-type specific attribute values, but they are still inferior to a complete data record. A natural me thod is to use the combinations of the attributes to define intermediate labels. Of course, if we use all the possible combinations, the state space could be too large. If this is the case, we can discard unimportant combinations by considering the co-occurrence frequencies of their corresponding attribute values in the training data records. The object-type dependent labels in our product information extraction are listed in Table 1 with the format Con_* . As a conditional model [17], Hierarchical Conditional Random Fields have the power to incorporate any useful feature for Web data extraction. In this section, we present the types of features that are essential for record detection and attribute labeling, and discuss how to incorporate them into our HCRF model. As we explain below, some of the features were first introduced as heuristic rules in some existing methods [31][32] to detect data records. The contribution of our work is that we provide a principled integration of them by defining feature functions and learning their importance weights. For each element, we extract both content and visual features as listed in table 2. All the features can be obtained from the vision-tree since VIPS assigns position information for each node through rendering a page. Previous work [4][32][33][27] has shown the effectiveness of visual features for Web page analysis and information extraction. After extracting the features fo r all the elements, a bottom-up procedure is taken to extract the visual and content features for inner blocks. We also compute the following distances for each block to exploit the regularity of similar data records in a page. Tree Distance Features: On a vision tree , if the blocks are visually similar, usually their sub-trees are also similar. We define the tree distance of two blocks as a measure of their structure similarity. The tree distance of two blocks is defined as the edit distance of their corresponding sub-trees. Although the time-complexity of computing this dist ance could be quite high, we can substantially reduce the computation with some rules. For example, if the depth difference of two sub-trees is too large, they are not likely to be similar and this computation is not necessary. Once we have computed the tree distances, we can use some thresholds to define Boolean -valued feature functions. For example, an f function is defined as: That is, if the tree distance of two adjacent blocks x i not more than 0.2, they are bo th likely to be data records. Image Source Image Alt-Text The alternative text of an image element Shape Distance &amp; Type Distance Features: Based on the ideas in [32], we also compute the shape distance and type distance of two blocks to exploit their similarity. For shape distance, we use the same definition of shape codes and also use the same shape distance calculation method as in [32] . Due to space limitations, we would like to refer the readers to [32] for more details.
 To compute the type distance of two blocks, we define the following types for each element: z IMAGE: the element is an image. z JPEG IMAGE: the image element that is also a jpeg picture. z CODED IMAGE: the image element whose source URL z TEXT: the element has text content. z LINK TEXT: the text element that contains an action URL. z DOLLAR TEXT: the text element that contains dollar sign. z NOTE TEXT: the text element w hose tag is  X  X nput X ,  X  X elect X  z NULL: the default type of each element. After defining each element X  X  type code, a block X  X  type code is defined as a sequence of the type co des of its children. As in [32], we also compress the multiple consecutive occurrences of each type to one occurrence and take the edit distance of type codes as the type distance of two blocks. Similar to the use of tree distance, we can easily incorporate shape distance and type distance into our model by defining other Boolean-valued feature functions with some pre-determined thresholds. Notice that our method will not be sensitive to these thresholds because the defined feature functions are softened by learning a weight for each of them. As in formula (2), each feature function contributes its weight to the probability only function is always true or false, it has no effect on the probability; and if some feature functions appear sparsely in the training set, smoothing techniques [6] can be used to avoid over-fitting. Here, we use the spherical Gaussian prior to penalize the log-likelihood function as in section 3.3. As described in the introduction, data records in the same Web page are always related and mutually constrained. Based on the work in [31], we try to align the elements of the two adjacent blocks in the same Web page and extract some global features to help the labeling of related records. For two neighboring blocks, we try to align their elements using the partial tree-alignment algorithm [31]. An alignment is discarded if most of the elements are not aligned. For successful alignments, the following feature is extracted. Repeated elements are less informative : This feature is based on the observation that repeated elements in different records are more likely to be less useful while important information such as the name of a product is not likely to repeat in multiple records in the same Web page. For example, the  X  X dd to cart X  button appears in both data records as in Figure 1(a), but each record has a unique name. Similar ideas have been exploited in [30] to remove Web noise. Currently, we just denote repeated elements in different blocks and do not take a complex information measure as in [30]. More complex measures can be easily adopted if needed in the future. An exampl e feature function is defined as: That is, if the element x i repeatedly appears in the aligned records, it will be more likely to be labeled as Note. In this section, we report empirical results by applying our HCRF model to extract structured product information from both list and detail pages. We compare our proposed model with a sequential approach that combines the state-of-the-art record detection and attribute labeling algorithms. The results show that our model achieves significant improvements in both record detection and attribute labeling on list pages, an d also performs promisingly on detail pages. We also analyze to what extent the gains come from: joint optimization of record det ection and attribute labeling, the global features, and long distance dependencies. We build the baseline methods by sequentially combining the record detection algorithm DEPTA [31] and 2D CRFs [33], and for detail pages, which the DEPTA algorithm cannot deal with, we first detect the main data bl ock using [27] and then use a 2D CRF model to do attribute labeling on the detected main block. Our method is built by combining VIPS [4], and the HCRF model. A Web page is first segmented by VIPS to construct the vision-tree, and then an HCRF model is used to label both records and attributes in the vision-tree. To see the effect of the global features as in section 5.3, we evaluate an HCRF model that does not incorporate the global features. We denote this model by HCRF_NG (without global features). Similarly, we evaluate DEPTA and two 2D CRF models in the baseline methods. As in [33], a basic 2D CRF model is setup with only the basic features (see Table 2) of the elements when labeling each detected data record. A variant model is setup with both the basic features and the global features as in section 5.3. We denote the basic model by 2D CRF and denote the variant model by 2D CRF_G . For 2D CRF_G, we first cache all the detected records from one Web page and then extract the global features as in section 5.3. Both the basic features and the extracted global features are used during model learning and attribute labeling. As there is no tree-structure here, the alignments are based on the elements X  relative positions in each record. To evaluate the separate effect of our approach on record detection and attribute labeling, we first detect data records on the parsed vision-trees using the content features and the tree distance, shape distance, and type distance features. Then we use HCRF models to label the detected records. When doing attribute labeling, we also evaluate two H CRF models with and without the global features. These two models are denoted by HCRF_S and HCRF_SNG respectively. We setup two general datasets with randomly crawled product Web pages. The list dataset ( LDST ) contains 771 list pages and the detail dataset ( DDST ) contains 450 detail pages. All of the pages are parsed by VIPS. For HCRF models, we use 200 list pages and 150 detail pages together to learn their parameters. All these pages are hierarchically labeled, that is, every block in the parsed vision-trees is labeled. We use the same 200 list pages to train a 2D CRF model for extrac tion on list pages, and use the same 150 detail pages to trai n another 2D CRF model for extraction on detail pages. The r eason for training two models for list and detail pages separately is that, for a 2D CRF model, the features and parameters for list and detail pages are quite different and a uniform model cannot work well. In the training stage, all of the algorithms converge quickl y, within 20 iterations. We use the remaining pages (571 list pages and 300 detail pages) for testing. Totally 6387 data records are found in the testing set LDST . For data record detection, we use the standard precision, recall and F1 measures to evaluate the methods. A block is considered as a correctly detected data record if it contains all the appeared attributes, otherwise it's not a data record. In our experiments, a data record could tolerate (miss or contain) some non-important information like  X  X dd to Cart X  button. For attribute labeling, we use the same measures defined in [33]. The performance on each attribute is evaluated by precision (i.e. the percentage of returned elemen ts that are correct), recall (i.e. the percentage of correct elements that are returned), and their harmonic mean F1. We also use two comprehensive evaluation criteria: We compare our approach with DEPTA [31] on LDST for data record detection. The running re sults of DEPTA on our dataset are kindly provided by its authors. DEPTA has a similarity threshold, and it is set at 60% in this experiment. Some simple heuristics are also used in DEPTA to remove some noise records. For example, a data region that is far from the centre or contains neither image nor dollar sign is removed. The evaluation results are shown in Table 3. For record detection, we can see that both HCRF and HCRF_NG significantly outperform DEPTA in recall, improved by 8.1 points, and precision, improved by 7.5 points. The improvements come from two parts: Advanced data representation and more features: in our model, we incorporate more features such as content features and the shape distance &amp; type distance features as in section 5.2 than DEPTA. We also adopt an advan ced representation of Web pages  X  vision-trees which have been shown to outperform tag-tree representation in other work [4]. As we can see from Table 3, HCRF_SNG and HCRF_S outperform DEPTA, and we gain F1. Incorporation of semantics during record detection: DEPTA just detects the blocks with regular patterns (i.e. regular tree structures) and does not take semantics into account. Thus, although some heuristics are used to remove some noise blocks, the results could still contain blocks that are not data records or just parts of data records. In co ntrast, our approach can integrate attribute labeling into block detection, and can consider the semantics during detecting data r ecords. So, the blocks detected are of better quality and are more likely to be data records. For instance, a block containing a product's name, image, price and some descriptions is almost certain to be a data record, but a block containing only irrelevant informa tion is unlikely to be a data record. The lower precisions of HCRF_SNG and HCRF_S demonstrate this. When not considering the semantics of the elements, HCRF_SNG and HCRF_S extracts more noise blocks compared with HCRF_NG or HCRF, so the precisions of record detection decrease by 5.5 points and the overall F1 measures decrease by 3.2 points. As we can see from Table 3, our HCRF model significantly outperforms the baseline approach. On list pages, HCRF_NG gains 18.7 points over 2D CRF in block instance accuracy and the achievements of HCRF are 13.9 points compared with 2D CRF_G. On detail pages, our approach ga ins about 58 points over 2D CRF performance are: Attribute labeling benefits from good quality records: One reason for this better performance is that attribute labeling can benefit from the good results of record detection. For example, if a detected record is not a data record or misses some important information such as name, the attribute labeling will fail to find the missed information back or just finds a wrong one. So, HCRF_SNG outperforms 2D CRF and HCRF_S outperforms 2D CRF_G. Of course the achievements of HCRF_SNG and HCRF_S may also come from the incorporation of long distance dependencies in hierarchical CRFs which will be discussed later. Global Features help Attribute Labeling: Another reason for the improvements in attribute labe ling is the incorporation of the global features as in section 5.3. From the results, we can see that when considering the global featur es, attribute labeling is more accurate. For example, 3.4 points are gained in block instance accuracy by HCRF compared with the basic HCRF_NG model. For two separate HCRF models, HCRF_S achieve 2.7 points in block instance accuracy compared with HCRF_SNG; for the two baseline methods, compared with 2D CRF which uses only the features of the elements in each detected record, more than 8 points are gained in block instance accuracy by 2D CRF_G which incorporates the global features. Attribute Labeling HCRF Models incorporate Long Distance Dependencies: The third reason for the better performance in attribute labeling is the incorporation of long distance de pendencies. From the results on detail pages, we can see that hierarchical models could get promising results while 2D CRFs perform poorly. For a detected record or a main block, 2D CRFs put its elements in a two-dimensional grid. Although th ey could incorporate more interactions compared with lin ear-chain CRFs [17], long distance interactions cannot be incorpor ated due to their first-order Markov assumption. In contrast, HCRF models can incorporate dependencies at various levels of granularity and thus incorporate long distance dependencies. So, HCRF models perform much better than 2D CRF models. For detail pages, as there is no record detection, HCRF_SNG and HCRF_S are not applicable here. Also there are no global features, so we just list the results of HCRF and 2D CRF on detail pages in Table 3. The quite different performance of 2D CRFs on list and detail pages also shows the effectiveness of long distance dependencies. For list pages, the inputs are data records that always contain a small number of elements. In this case, 2D CRFs can effectively model the dependencies of the attributes and achieve reasonable accuracy. However, noisy elements always exist between attribute values in a detail page. So, 2D CRFs perform very poorly for their failure in incorporating long distance dependencies. Wrapper learning approaches like [23][16] are template-dependent. They take in some manually labeled Web pages and learn some extraction rules (i.e. wrappers). Since the learned wrappers can only be used to ex tract data from similar pages, maintaining the wrappers as Web sites change will require great efforts. Furthermore, in wrappe r learning a user must provide explicit information about each template. So it will be expensive to train a system that extracts data from many Web sites as in our application. [32][10][3][5][9][1] are also template-dependent, but they do not need labeled training samples. They automatically produce wrappers from a collection of similar Web pages. [31][18] are two template-independent methods. [18] segments data on list pages using the information contained in their detail pages. The need of detail pages is a limitation because automatically identifying links that point to detail pages is non-trivial and there are also many pages that do not have detail pages behind them. [31] mines data reco rds by string matching and also incorporates some visual features to achieve better performance. However, [31] detects data records only using tree regularities and not consider semantics. Furt hermore, the data extracted by [31][18] have no semantic labels. [12][33] are two methods that treat Web information extraction as a classification problem. [12] uses a support vector machine to identify the start and end tags for a single attribute. For the task of extracting multiple attributes, this method loses the dependencies between different attributes. [33] shows that these dependencies are common among several types of objects and also proposes 2D CRFs to better incorporate these dependencies. However, it has two assumptions: (1) data records are independently and identically distributed; (2) data records have been perfectly extracted. As described in the in troduction, these assumptions will result in highly ineffective de-coupled solutions. The idea of exploring mutual benefits by integrating two tasks has been attempted in previous work. [24] attempts a mutually beneficial integration of inform ation extraction and data mining. Information extraction makes po ssible the text mining which needs to handle unstructured text documents, and data mining could provide additional clues to improve the recall of an IE system. [29] proposes an integrated model to do information extraction and coreference. Incorporating extraction uncertainty could help coreference, and leve raging the identified coreference could improve extraction accuracy. However, [29] is not a fully closed integration. Due to its model X  X  complexity, separate learning and inference for different substructures is employed in [29]. [28] proposes a factorial CRF to jointly solve two NLP tasks (noun phrase chunking and part of speech tagging) on the same observation sequence. The difference is that our data are hierarchical trees and the data in [28] are sequences. [26] uses Hierarchical Hidden Ma rkov models (HHMMs) [11] to extract relation instances from biomedical articles. Our work differs from this one in two key aspects. First, as discriminative models HCRFs have the flexibility to incorporate arbitrary and non-independent features of the observations, but HHMMs, which are generative models, must make some strong independence assumption to achieve inference tractability. This is the key idea underlying the Conditional Random Fields [17]. Second, the data in [26] are two-level representatio ns of sentences, but our data are arbitrary vision-trees. Recently, se veral hierarchical structured Conditional Random Field models [13][15][20] have been proposed in different domains. Un like [13], our HCRF model is not a simple multi-scale model because it has inter-level interactions. In [15], a two-layer structured CRF model is proposed to incorporate both local and global label interactions for robust image classification. Unlike our model which is a Conditional Random Field as a whole, each layer of [15] is a separate Conditional Random Fi eld and the two layers are coupled with directed links. In [20], a three layer Conditional Random Field model is dynamically constructed for activity recognition. In contrast, we use a layout data structure  X  X ision tree X  and our hierarchical CRF m odel is built upon it. Due to its dynamic nature, [20] must take an iterative process and the whole inference is approximate. But our HCRF model can find the optimal labeling results. Other work, such as collective information extraction [2] and Semi-Markov extraction models [7][25], could achieve higher performance in named entity extraction problems on flat text documents. However, for the integrated Web data extraction, where the data are hierarchica lly represented, the proposed hierarchical CRF model is the natural and efficient method. Hierarchical Conditional Random Fields (HCRFs) to exploit the mutually beneficial integration of data record detection and attribute labeling. In this model, record detection can benefit from the availability of semantics which are required in attribute labeling, and attribute labeling could benefit from the collective labeling of similar records in the same Web page. Parameter estimation and labeling could be efficiently performed by applying the standard junction tree algorithm. As a conditional model, HCRFs can incorporate any useful feature for Web data extraction in a principled way; as a hierarchical model, HCRFs provide a way to incorporate long distance dependencies for Web data. Empirical studies show that mutual enhancement of record detection and attribute labeling can be achieved in our joint approach, and HCRFs can perform very well on both list and detail Web pages. The authors are grateful to Yanhong Zhai and Bing Liu for providing the running results of their system DEPTA on our dataset LDST . The authors Jun Zhu and Bo Zhang are supported by National Natural Science Foundation of China (60135010, 60321002), and Chinese National Key Foundation Research and Development Plan (2004CB318108). [1] Arasu, A., and Garcia-Molina, H. Extracting Structured Data [2] Bunescu, R. C., and Mooney, R. J. Collective information [3] Buttler, D., Liu, L., and Pu, C. A Fully Automated Object [4] Cai, D., Yu, S., Wen, J.-R. and Ma, W.-Y. Block-based Web [5] Chang, C.-H., and Liu, S.-L. IEPAD: Information Extraction [6] Chen, S. F., and Rosenfeld, R. A Gaussian Prior for Smooth-[7] Cohen, W. W., and Sarawagi, S. Exploiting Dictionaries in [8] Cowell, R. G., Dawid, A. P., Lauritzen, S. L., and [9] Crescenzi, V., Mecca, G., and Merialdo, P. [10] Embley, D. W., Jiang, Y., and Ng, Y.-K. Record-Boundary [11] Fine, S., Singer Y., and Tishby, N. The hierarchical hidden [12] Finn, A., and Kushmerick, N. Multi-level boundary classi-[13] He, X., Zemel, R. S., and Carreira-Perpi X  X n, M.  X . Multi-[14] Jensen, F. V., Lauritzen, S. L., and Olesen, K. G. Bayesian [15] Kumar, S., and Hebert, M. A Hierarchical Field Framework [16] Kushmerick, N. Wrapper induction: efficiency and [17] Lafferty, J., McCallum, A., and Pereira, F. Conditional [18] Lerman, K., Getoor, L., Minton, S., and Knoblock, C. Using [19] Lerman, K., Minton, S., and Knoblock, C. Wrapper [20] Liao, L., Fox, D., and Kautz, H. Location-based activity [21] Liu, D. C., and Nocedal, J. On The Limited Memory BFGS [22] Malouf, R. A comparison of algorithms for maximum [23] Muslea, I., Minton, S., and K noblock C. A. Hierarchical [24] Nahm, U. Y., and Mooney, R. J. A Mutually Beneficial [25] Sarawagi, S., and Cohen, W. W. Semi-Markov Conditional [26] Skounakis, M., Craven, M., and Ray S. Hierarchical Hidden [27] Song, R., Liu, H., Wen, J.-R., and Ma, W-Y. Learning Block [28] Sutton, C., Rohanimanesh, K., and McCallum, A. Dynamic [29] Wellner, B., McCallum, A., Peng, F., and Hay, M. An [30] Yi, L., Liu, B., and Li, X. Eliminating Noisy Information in [31] Zhai, Y., and Liu, B. Web Data Extraction Based on Partial [32] Zhao, H., Meng, W., Wu, Z., Raghavan, V., and Yu, C. Fully [33] Zhu, J., Nie, Z., Wen, J.-R., Zhang, B., and Ma, W.-Y. 2D 
