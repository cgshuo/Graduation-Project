 Web search results often integrate content from specialized corpora known as verticals . Given a query, one important aspect of aggregated search is the selection of relevant ver-ticals from a set of candidate verticals. One drawback to previous approaches to vertical selection is that methods have not explicitly modeled user feedback. However, pro-duction search systems often record a variety of feedback in-formation. In this paper, we present algorithms for vertical selection which adapt to user feedback. We evaluate algo-rithms using a novel simulator which models performance of a vertical selector situated in realistic query traffic. H.3.3 [ Information Search and Retrieval ]: Miscella-neous Algorithms vertical selection, distributed information retrieval, resource selection, aggregated search, user feedback, simulation Traditional web search engines retrieve a ranked list of URLs in response to a user X  X  query. Increasingly, search re-sults pages include content from specialized sub-collections; this model has been referred to as aggregated search [18]. These sub-collections X  X eferred to as verticals  X  X nclude non-text media collections such as images and videos as well as genre-specific subsets of the web such as news and blogs. Given a query, relevant vertical content is often displayed  X  Work conducted at Yahoo! Labs.
 Figure 1: Aggregated content. Vertical content pre-sented in a display above general web results. as a self-contained element of the search result page (Figure 1).

Vertical selection can be performed through hand-crafted rules or through machine learned query classifiers [3, 15]. These approaches represent queries in a high-dimensional feature space. Using a set of example queries, one can train a statistical model which predicts query intent given query features. By carefully choosing features, models can gen-eralize from the training set to new queries. Features may include either lexical query features (e.g. query terms) or non-lexical query features (e.g. detected entity types).
Information retrieval systems, once in operation, can gather a large amount of user feedback which can be used to cor-rect vertical selection errors. For example, in web search, measuring user activities such as clicks, timeouts, and refor-mulations can provide valuable X  X lbeit noisy X  X eedback [20, 13]. Machine learned approaches (as discussed in the exist-ing literature) would require manually mining new training data and periodically retraining a model with mined data. Even if retrained, due to the statistical and feature-based nature of these classifiers, there is no guarantee that future occurrences of new training queries will be correctly classi-fied. As a result, system designers often need to heuristically construct  X  X ail-safe X  dictionaries.

In this paper, we present models which combine a proba-bilistic query classifier (similar to those described in existing literature) with online user feedback. We will demonstrate that our methods significantly improve accuracy of intent detection. The generality of our problem definition and approaches present opportunities for further refinement as well as application to other federated search domains. Fur-thermore, we propose a simulation framework for evaluating vertical selection using parameters grounded in commercial query logs.
Despite the increasing integration of vertical content with web search results, relatively few research papers address this topic. Previous work has addressed the integration of content from shopping [15], news [10], and job [15] verti-cals. These authors approach vertical selection as a binary classification task and evaluate performance on independent binary problems. Our work builds on these results by ex-panding the set of verticals and situating evaluation in a stream of queries with potentially multiple relevant verti-cals.

Distributed information retrieval refers to the task of gen-erating a single ranked list from multiple sub-collections [6]. One important sub-task of distributed information retrieval, resource selection , refers to deciding which sub-collections to search given a user X  X  query. If we consider verticals to be the sub-collections of interest, distributed information retrieval techniques can be directly applied to vertical selection. Our work extends resource selection by introducing user feedback and a simulated evaluation.

More generally, query classification refers to techniques for automatically matching queries to some predefined set of categories. Some query classification work has focused on classifying queries into topical categories, such as games, business, and health [23, 5, 15]. Our work extends query classification by firmly grounding categories in vertical con-tent, introducing user feedback, and providing an evaluation situated in a search task.

Detecting and exploiting user feedback has been proposed for a variety of sub-tasks of web search. Joachims proposed using click-through data in order to learn good ranking func-tions [12]. Radlinski et al. use a multi-armed bandit al-gorithm to actively adapt retrieval results to gather feed-back and improve ranking functions [19]. In the context of search advertising, approaches focus on explicitly modeling the click-through rate of query-advertisement pairs, in part because revenue is directly related to user clicks [21, 7]. Our work focuses on relevance rather than revenue and verticals rather than documents or advertisements.

Finally, our use of simulation for evaluation has precedent in document retrieval. The use of deterministic simulation underlies tasks such as relevance feedback, information fil-tering, and topic tracking; our work is more related to para-metric simulation . Although Cooper first proposed a system for randomly simulating queries and documents [8], Griffiths was the first to estimate simulation parameters from empir-ical data [11]. Tague et al. simulated documents, queries, and relevance judgments using Cranfield II and Medlars cor-pora to estimate model parameters [25]. Parametric simula-tion has also been used to evaluate information filtering sys-tems [17] and relevance feedback methods [26]. Azzopardi et al. used document language models to automatically create known-item relevance judgments [4]. Our work most closely resemble that of Acharya et al. who used a query and click simulator to evaluate web advertisement placement decisions [1]. Unlike these simulators, we ground the parameters of our model in a mix of both manually labeled data (in the spirit of deterministic simulation) and parametric simulation based on query log statistics.
Assume we have a stream of queries being submitted by users to the search engine. Our search results page always presents results from the web (i.e.  X  X en blue links X ). In addi-tion to a web index, the system also has access to k verticals. The result of issuing the query to a vertical can be embed-ded in the search results page (e.g. Figure 1). In this work, we consider the situation where up to one vertical display can be integrated above web results. Reading the page from top to bottom, the user can be satisfied by the vertical con-tent (if present) or the generic web results or neither. The user then communicates satisfaction to the system (we will elaborate on the nature of this feedback in Section 5.2). Our objective is to maximize the user satisfaction by presenting the appropriate vertical display; this includes the presenta-tion of no display when the user is best satisfied by general web results.
In this section, we will describe a model which, given a query, predicts its vertical intent. The features of this model are non-lexical, allowing it to generalize beyond train-ing queries. This model, once in operation, does not adapt to user feedback. Therefore, we refer to it as the offline model. In Section 4.2, we will describe several methods for combining user feedback and offline predictions. In Section 4.3, we present a method for incorporating information from related queries. Finally, in Section 4.4, we present a method for increasing robustness through randomization.
Our off-line model derives evidence from vertical content, vertical query logs, and the query string. Vertical content evidence includes classic distributed information retrieval and performance prediction metrics. Vertical query logs can be used to determine how similar a candidate query is to queries issued directly to the vertical. Finally, query string features include boolean evidence such as regular expressions and dictionaries believed to correlate with vertical intent. A more complete description of these features can be found in [3].

We would like to use these three classes of features in or-der to predict which vertical display X  X f any X  X o present to the user. Therefore, we have k + 1 potential vertical intents, one for each vertical and one for  X  X o relevant vertical X . Be-cause our features do not refer to the actual query tokens, we can expect new queries to be correctly predicted to the extent that there are correlations between these features and vertical intent.

Given a set of queries manually labeled with vertical in-tents, we can train a statistical model to predict the relevant verticals for new queries. In particular, we would like the model to compute the probability that a vertical is relevant given a query. We will model this as a set of k + 1 Bernoulli random variables, each modeled in turn by a separate logis-tic regression. In our case, the explanatory variables are our features. If we train k + 1 logistic regression models, then, given a query, we can predict the probability of success of each of the k + 1 trials. For query q and vertical v , let  X  the probability of that vertical being relevant as estimated by our logistic regression model. Given a query, the pre-dicted vertical is argmax v  X  v q . This is equivalent to the tech-nique of training k + 1 one-versus-all binary classifiers often used for multiclass learning problem. Although we adopt logistic regression as our base classifier, any binary classifier can be used. Algorithms in subsequent sections only require that the classifier output a probability for each vertical.
Notice that the model in the previous section makes pre-dictions only dependent on the query and does not change with user feedback. Assume we have some method for de-tecting the user satisfaction or dissatisfaction with the ver-tical display presented. In web search, satisfaction may be measured by a function of clicks or dwell time and dissat-isfaction may be measured by a function of skips, reformu-lation, or abandonment [13, 20]. In this section, we will provide algorithms for adapting model predictions in the presence of binary user feedback signals . Although we leave the treatment of non-binary signals such as probabilities or real values for future work, we will address the robustness of our models to signal unreliability and noise in our evalu-ation.
If we assume that the relevance of each vertical can be represented as a Bernoulli random variable, then the conju-gate prior is the Beta distribution. That is, we model each probability of relevance of a vertical to a query, p v q , as being sampled from a Beta distribution [10]. Mathematically, The parameters a v q and b v q control the shape of our prior distribution over the probability that vertical v is relevant. In practice, we use the offline model probability,  X  v q , in order to select these parameters, where  X  is a hyperparameter of our model. A large value for  X  will concentrate the distribution around  X  v q . A small value for  X  will  X  X pread out X  this distribution.

Assume we have positive and negative feedback informa-tion for a query-vertical pair. Let R v q be the number of positive impressions (e.g. clicks) and R v q be the number of negative impressions (e.g. skips). Then, the posterior, given data from displays presented, is also a Beta distribution with a posterior mean of, where V v q = R v q + R v q represents the number of times the vertical display v was presented for query q . For small values of  X  , the model will be very sensitive to feedback from the user. For large values, the model will rely on  X  v q more than feedback.
Alternatively, we can use a logistic normal prior to model user feedback [2]. In the context of information retrieval, this model was originally proposed for adaptation of docu-ment retrieval scores to user feedback [14]. We will briefly review the model and adapt notation for vertical selection. A more complete derivation can be found in references.
Assume that t  X  1 queries have been issued to the sys-tem and we wish to predict the vertical relevance for the next query. Define two t  X  k random matrices W and W . The elements of these matrices are sampled from a single multivariate normal, where  X  is a 2 tk  X  1 vector of means and  X  is a 2 tk  X  2 tk covariance matrix. At time t , define p v q as, The prior mean at time t can be written as a function of  X  and  X  . However, given covariances  X  we can select  X  so that the prior mean matches a target value. In this work, we will define  X  so that the prior means equal  X  v q .
The posterior mean after observing t  X  1 queries can be derived as,  X  p a b where V v t is a boolean variable indicating whether vertical v was presented at time t ; R v t and R v t are boolean variables indicating whether a vertical v received positive or negative feedback at time t .

Notice that the estimate in Equation 4 only requires two columns of the covariance matrix,  X  , those associated with W tv and W tv . We will define these covariances as, where q i is the query issued at time i . Equation 5 incorpo-rates feedback from previous displays of v for q t . Equation 6 incorporates feedback from previous displays of compet-ing verticals for q t . The parameter  X  controls the positive contribution of negative feedback on competing verticals to a vertical X  X  probability of relevance. When  X  is large, nega-tive feedback on competing verticals positively affects a ver-tical X  X  probability of relevance and vice versa . This na  X   X vely assumes that all verticals are negatively correlated, a fact which is almost certainly untrue. However, we have empir-ically noticed that queries tend to have few ( &lt; 3) relevant verticals.
We use a relatively simple covariance function here. More elaborate covariance functions may incorporate temporal prox-imity for verticals with non-stationary relevance (e.g. news). In the next section, we will use this covariance matrix to en-code information from related queries.

We can use Equations 5 and 6 in order to rewrite the exponents in Equation 4,
We have demonstrated how to use feedback for a query to estimate p v q . However, a vertical X  X  probability of being rele-vant is also likely to be related to the feedback on topically related queries. For example, consider a situation where we have seen many query events for  X  X bama inauguration X , pro-viding a reliable estimate of p v q for all v . Then, given the new query  X  X arack obama inauguration X , we have informa-tion about p v q 0 if we know that these two queries are related.
There are several methods for detecting the similarity be-tween queries. In this work, we adopt a corpus-based sim-ilarity measure using language models of retrieved results. Specifically, we create a language model by interpolating the top retrieved document language models weighted by their retrieval scores [9]. This provides a query language model for each query. Given two query language models, we compare two queries by comparing their associated language models using the Bhattacharyya correlation. The Bhattacharyya correlation ranges between 0 and 1 and is defined as, This approach was previously used for news vertical selection [10] and similar methods have been used for query sugges-tion and advertisement placement tasks [22, 16]. We note that while the Bhattacharyya similarity measure uses only P ( w |  X  Q ), other similarity measures based on time or term overlap can be used or combined with B .

How we precisely incorporate information from similar queries depends on the prior we are using. For the multiple Beta model, our assumption is that if B ( q,q 0 ) is large, then p q  X  p v q 0 . We incorporate this information into the candi-date query X  X  prior. Specifically, define the nearest-neighbor estimate of p v q as, where Z q = P q 0 B ( q,q 0 ). We then compute a compound prior for p v q as, where  X   X  [0 , 1] controls the importance of the nearest-neighbor estimate relative to the offline model estimate. No-tice that the normalization in Equation 8 discards the mag-nitude of the similarity. That is, the relative weights for k queries with constant similarity  X  q 0 : B ( q,q 0 ) = . 9 is the the absolute confidence, we define  X  q =  X   X  max q 0 B ( q,q
We incorporate similar queries in the logistic normal prior model by adding elements to  X  . Assume for q i = q i 0 , covari-ances are described in Equations 5 and 6. For cases where q 6 = q i 0 , where  X  controls the contribution from similar queries and  X  is defined in the previous section. We can use Equations 10 and 11 in order to rewrite the exponents in Equation 4,
Sometimes, we would like to present a vertical display for a query even though it is not predicted to be the display with the highest probability. The motivation for this may be to gather a small amount of feedback without devastating system performance. To this end, we can present random displays for queries with some probability, . This approach is referred to as the -greedy approach [24].

The -greedy approach aggressively presents verticals re-gardless of what we have learned about the vertical probabil-ities. We can incorporate this knowledge by exploiting the posterior means across verticals. Specifically, we will sample our decision from a multinomial over verticals derived from our estimated vertical relevance probabilities,  X  p v q . In our ex-periments, we use a Boltzmann distribution to construct the multinomial, where Z = P v exp  X   X  p v q  X   X  and  X  &gt; 0 controls the uniformity of the random vertical selection. As  X   X   X  , the vertical selection becomes more random. At  X   X  0, the vertical selection becomes greedy. In reinforcement learning, state space exploration is often performed using a Boltzmann dis-tribution [24].
Our data set consists of 25195 queries sampled from a web search query log. Human editors were provided a list of 18 verticals (Table 1) and asked to assign from zero to six rel-evant verticals for each query. Queries were automatically spell corrected and normalized by down-casing and removing punctuation. The vertical distribution is shown in Table 1. About 26% of queries were assigned  X  X o relevant vertical X , indicating that these were best served by general web con-tent. Of those assigned at least one relevant vertical, the average number of verticals assigned per query was 1 . 49. Of these, 60% were assigned one vertical, 31% were assigned Table 1: Percentage of queries assigned each verti-cal. Percentages do not sum to one because queries can be assigned more than one relevant vertical. two verticals, and 9% were assigned more than two. We trained our offline model using 10 fold cross-validation on all of the labeled queries. We used a snapshot of Wikipedia as the corpus to compute query similarity.
Because we are interested in the adaptation of an offline model to online feedback, we used our offline data to con-struct a query simulator. First, we retrospectively analyzed the query frequency of each of the queries in our data set. We then estimated the parameters of a multinomial over our 25195 queries using the relative frequency data; call this multinomial  X  Q . We found that this distribution was skewed and heavy-tailed, a property common in many real query logs.

We would like to simulate a user submitting a query and having a particular intent chosen from those listed in Ta-ble 1. In the case of  X  X o relevant vertical X , we assume that the user is satisfied by general web results and, therefore we refer to this as the  X  X eb X  vertical. In order to simulate the generation of vertical intent, for each query, q , we esti-mate a multinomial over verticals using the manual judge-ments with uniform probability over relevant verticals and zero probability for non-relevant verticals; call this multino-mial  X  q V .

The simulation first samples a query, q t , from  X  Q and then one of q t  X  X  relevant verticals from  X  q t V . We repeat this process for 10 million query samples, evaluating performance at the end of the simulation. We present average and standard deviations of metrics over 10 runs of the simulator. We found 10 runs to be sufficient to achieve statistically different performances in algorithms.

So far, our simulated user provides perfect feedback. If the relevant vertical is displayed, she provides positive feedback; if a non-relevant vertical is displayed, she provides negative feedback. In practice, measuring user feedback is imperfect and signals are noisy. If a user is satisfied, sometimes we incorrectly detect negative feedback; if a user is unsatisfied, sometimes we incorrectly detect positive feedback. At other times, the system may have predicted the correct intent but the interface may have resulted in an incorrect user response.
Noisy feedback influences two aspects of system develop-ment: adaptation and evaluation. In the case of adaptation, although we expect performance to decrease in the presence of noisy feedback, we prefer adaptation algorithms which are robust to noisy feedback. In the case of evaluation, noisy feedback can misrepresent performance. This is one drawback we see with relevance metrics based exclusively on noisily detected user feedback.

Using a simulation allows us to decouple feedback signals and the  X  X rue X  user feedback variable. Because we know the true vertical intents, we can evaluate using a noiseless signal while introducing noise only in the feedback. We introduce noise by defining a probability,  X  , of correctly detecting user feedback. When a correct display is presented, the system detects positive feedback with this probability; similarly, it detects negative feedback with this probability. The simu-lation is more formally defined in Appendix A.
Assume we have a stream of queries indexed by t where y  X  X  X  X  Web } is the user X  X  intent. The system X  X  prediction is f t  X  X   X  X  Web } . We represent the user X  X  utility from the presentation as, where 0  X   X   X  1 represents the user X  X  discounted utility by being presented a display above the desired web results. When  X  is large, a user X  X  satisfaction is not affected by in-jecting the display above the desired web results. When  X  is small, a user X  X  satisfaction is degraded. When  X  = 0, a user is so distracted by the display that there is no satisfac-tion by the web results. In our experiments, we use  X  = 1 which is similar to the discount often used in measures such as NDCG.

For an individual query, we compute the average utility as, where T q is the set of times query q was issued. Over all queries, we define the macro-averaged utility as We adopt a macro-averaged metric because a micro-averaged metric would be dominated by relatively few queries.
About 30% of our queries are labeled with more than one relevant vertical. For these queries, our simulator selects one relevant vertical from the relevant set at each step. Because we do not condition the sampling on any variable, the best a system can do is to either learn one of the relevant verticals and select that vertical every time or learn all of the relevant verticals and randomly select from them. Both approaches converge to an average utility less than one. For our col-lection, the macro-averaged utility of such a selector over all queries is expected to be 0 . 843. Therefore, we normal-ize Equation 13 by this factor. The macro-averaged utility over queries with multiple relevant verticals is expected to be 0 . 461. When evaluating only on queries with multiple intents, we normalize by this factor.
In our discussion, we will refer to the multiple Beta model as MB and the logistic normal model as LN. Superscripts indicate the prior used by the model. The prior  X  refers to the model described in Section 4.1. The prior U refers to a uniform prior which assigns  X  v q = 1 2 for all query-vertical pairs. The sub-script s indicates the use of similar queries. Finally, the prefix indicates the randomization method used, for -greedy and  X  X  X  for Boltzmann.

We begin by comparing the baseline predictor,  X  , with those that use feedback information. The static baseline achieves U macro of 0 . 618  X  0 . 001. In Table 2, we present the performance of MB  X  for various settings of  X  . We observe that, for all values of  X  , we improve over the baseline. In Table 3, we show performance of LN  X  for various values of  X  . Again, for all values of  X  , we improve over the baseline. While being able to improve using feedback is not surpris-ing, we note that LN  X  always outperforms MB  X  , especially as feedback becomes noisier. We can explain this behav-ior by inspecting the computation of the posterior mean for each prior. As evidence is introduced, the posterior for MB (Equation 1) converges toward the true mean. The posterior for LN  X  (Equation 4) converges toward 0 or 1 with relatively little feedback. The negative correlations between compet-ing verticals in LN  X  further concentrates probability in a few verticals. For example, assume there is one relevant vertical for a query. Further, assume that our prior is 0.90 for the relevant vertical and 0.85 for a non-relevant verti-cal. If  X  = 0 . 75, then our posterior using MB  X  to 0.75 for the relevant vertical. The system then has to also expend interactions to drive the non-relevant vertical X  X  posterior below 0.75. LN  X  , however, pushes queries towards extremes and therefore more gracefully handles noise when evaluating over all queries.

We performed experiments using a uniform prior (table suppressed due to space constraints). In these experiments, the best performing setting of  X  for MB U resulted in an (  X  = 0 . 90), and 0 . 669  X  0 . 001 (  X  = 0 . 75). Observing that this outperforms the baseline,  X  , at all values of  X  , we may be tempted to conclude that the training of the logistic re-gression models is unnecessary. However, the combination of feedback and the offline model significantly outperforms the uniform prior. We observed similar results for LN U . Table 5: Normalized U macro for B-MB  X  runs. This table should be compared with Table 2.

The performance improvement from using similar queries is significant but slight (Table 4). When more feedback noise is introduced, gains disappear altogether. This suggests that similar queries are only useful to the extent that their esti-mates are accurate. With very noisy feedback, we gain very little information from related queries. We observed similar results for LN  X  s .

The improvement from decision randomization depends on the model prior. Boltzmann randomization significantly improves MB  X  (Table 5) while not affecting LN  X  (Table 6). We alluded to the reason B-MB  X  improves performance over MB  X  earlier. Namely, randomization allows the system to simultaneously adjust estimates toward the true mean. LN  X  however, quickly converges toward the extreme and therefore does not require simultaneous adjustment.

We present the best performing runs for all of our algo-rithms in Table 7. We find that, across values of  X  , LN tends to perform best. However, similarity information does not appear to be useful when  X  = 0 . 75. If we use MB  X  , then randomized decision making performs best.

Something interesting happens if we inspect performance only for queries with multiple relevant verticals (Table 8). In this situation, the simulator will randomly select between relevant verticals. Notice that for MB and LN methods in-dividually, the effects are the same as those observed for all queries. However, relative performance between the two priors switches for multiple intent queries. This behavior results from the aggressive use of feedback in LN  X  which is exponential in the amount of feedback. In the cases where feedback is split between two relevant verticals, posterior distributions of relevant verticals will swing from 0 to 1 given inconsistent vertical feedback.
We have presented several algorithms for combining user Table 6: Normalized U macro for B-LN  X  runs. This table should be compared with Table 3. Table 7: Normalized U macro for all queries. Perfor-mance using optimal parameter settings. Table 8: Normalized U macro for multiple intent queries. feedback with offline classifier information. Our results demon-strate that, although feedback-only models can outperform offline-only models, combining the two results in significant improvements. We found that using a logistic normal prior outperforms using multiple beta priors across all queries. However, we also found that the multiple beta priors with randomized decision making provides stable performance for both single and multiple intent queries. Finally, we pre-sented a query simulation and vertical feedback model which decouples evaluation signals and feedback signals, allowing us to accurately measure model robustness to feedback de-tection noise.
 There are several directions in which to extend this work. First, we assumed that a search results page could only in-clude a single vertical display. In practice, multiple displays can be stacked above the web results or blended elsewhere in the ranked list. Second, we have attempted to define and evaluate our algorithms with generalization in mind. We believe there is merit in applying these algorithms to other distributed information retrieval and federated search tasks. Third, we are interested in exploring domains where our sim-ulator assumptions X  X n particular with respect to query and vertical distributions X  X o not hold. For example, we have introduced a fixed noise parameter, assuming that we detect positive and negative feedback equally well and that we de-tect feedback for different vertical displays equally well. In reality these noise rates are likely to be dependent on the type of feedback and vertical. Fourth, we also would like to modify our algorithms to deal with non-binary feedback, such as that statistically detected using implicit feedback features. Finally, although we believe that our simulation provides an attractive, noiseless evaluation, we acknowledge that the efficacy of these algorithms can only be confirmed in real query traffic. It is important, though, to be cautious about non-uniform noise in the evaluation signal, be it clicks or some other measurement. We would like to thank Jean Fran  X cois Beaumont, Daniel Boies, Hugues Bouchard, Jean Fran  X cois Crespo, Deborah Donato, Rosie Jones, Remi Kwan, Vanessa Murdock, Jian-Yun Nie, Jean Fran  X cois Paiement, and Alexandre Rochette for helpful discussions and feedback. This work was sup-ported in part by the NSF grant IIS-0841275 and a generous gift from Yahoo!. Any opinions, findings, conclusions, and recommendations expressed in this paper are the authors X  and do not necessarily reflect those of the sponsors.
Our simulator is comprised of two parts: the intent sim-ulator and the feedback detector. At time t , the intent sim-ulator (Figure 2) first selects a labeled query, q t , from the multinomial over queries,  X  Q , (Line 2) and then a relevant vertical from the query-specific multinomial over verticals,  X 
V (Line 3). The retrieval system is asked to make a vertical prediction for q t (Line 4). We evaluate the utility gain us-ing the true relevant vertical and the prediction (Lines 5-6). Unlike click-based evaluation, there is no noise in evaluation.
The feedback detector (Figure 3) allows the experimenter to introduce noise into the system X  X  feedback detection us-ing the parameter  X  . Our simulated user reads the results page (Figure 1) from top to bottom. If the user is satisfied by the displayed vertical, we provide positive feedback with probability  X  (Lines 4-6); if the user is not satisfied by the displayed vertical, we provide positive feedback with prob-ability 1  X   X  (Lines 7-9). If the system presented a vertical display and the simulated user did not provide positive feed-back on the display, then this implies negative feedback on the display (Line 10). This appeals in spirit to Joachim X  X  work in preference mining from clicks [12]. In the event that the display did not receive feedback, the simulated user potentially provides positive feedback on the general web re-sults (Lines 14-21); the logic follows the logic in Lines 3-10.
Figure 3: Noisy feedback simulator pseudocode.
