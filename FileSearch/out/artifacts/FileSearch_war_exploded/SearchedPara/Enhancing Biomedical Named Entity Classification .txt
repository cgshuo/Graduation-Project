 distinguished from named entity recognition. The former is to classify named entities that may not be in free text into right classes and the latter needs to recognize names in free text. Named entity classification can be a powerful auxiliary method for named entity recognition, for instance, building a domain specific dictionary or ontology automatically or entity disambiguation and can also be used to remove noises intro-duced by query expansion in Information Retrieval (IR) tasks. Named entity classifi-cation/recognition in biomedical domain is difficult mainly due to the vast vocabulary described all the time. The state-of-the-art performance reported by BioCreative 2004 [1] does not exceed 85%. 
For the two tasks, recently most researchers focus on supervised machine learning these systems are similar, which can be categorized into three types: Surface feature: words, context words, and regular expressions, etc. Linguistic feature: POS tags, semantic tags, and syntactic tags, etc. Dictionary feature: external dictionaries, stop words, and common word lists, etc. 
Predicting the category of Out-of-Vocabulary (OOV) terms is an essential issue in identify unknown terms, but there are limitations in each of them. For surface feature, surface lexical information must be an indicator to entity class and big size of training role such as POS tags, semantic tags, and syntactic tags, etc. However the current best performance of many of these techniques itself is low and most can only work in free texts. Dictionary feature is widely used in many domains such as [5][6], but it has two errors during building lexicon (many can be found in biomedical domain since some lexicons are generated automatically). 
In this paper, we introduce a new type of feature, called Feature Coupling Degree (FCD) features, which utilize the co-occurrence information of strong indicative fea-tures for instances and classes in labeled and unlabeled data. We also propose a method based on FCD features for biomedical named entity classification. Features are generated from discretized co-occurrence counts of a candidate named entity phrase and typical gene/protein related contexts on Web pages. We investigate its effect with different kernels and discretization methods comparing with classical features. 
This method has used both labeled and unlabeled data (Web), so it can be assumed as a semi-supervised learning approach. Semi-supervised machine learning is to im-prove the performance of supervised learning by incorporating unlabeled data. Many semi-supervised algorithms are proposed in recent years [7][8][9][10]. Most of these methods focus on how to generate  X  X irtual examples X  or estimate a better structure of prediction functions utilizing unlabeled data. While we focus on using unlabeled data for feature generation, which is similar to the idea proposed in [11]. Our method also reflects a general framework to learn a new feature representation from unlabeled data. 2.1 Feature Coupling Degree features (x1, ..., xn)  X  X  X  Rn and one label y  X  {y1, ..., ym}. Assume that they are independently generated according to some unknown probability distribution D. The known so that its prediction error with respect to D is as small as possible. While most feature generation methods are done manually. Intuitively when selecting a feature, two factors should be considered, i.e. its discrimination abilities for instances and for classes. If either of the abilities is too weak, the feature is bound to be irrelevant. Con-sidering these two factors we categorize features into two types: Instance Discrimina-tive Features (IDFs) and Class Discriminative Features (CDFs). Given an instance x , classes. Usually the conditional probability P ( y | x c ) is relative high. the joint probability of the pair of IDF and CDF. It is easy to understand that high FCD values tend to be strong indictors for classifica-tion, so they can be used as features. However, usually many of them are not available in limited amount of training data especially when the distribution of the feature space is sparse (e.g., named entity classification). Fortunately, large scale of unlabeled data can be obtained without much effort. Especially the World Wide Web (WWW) pro-vides terabyte unlabeled data for machine learning in NLP (Natural Language Proc-essing). In this context much more FCDs that do not exist in labeled data can be esti-type of feature. 
For the biomedical entity classification task, we choose the normalized term itself as IDF and strong indicative context words as CDFs, which are obtained by comput-ing Chi-square values of terms in training corpus. FCD value is approximately esti-mated by the Web page count where IDF and CDF co-occur in a phrase. Also a  X  X ea-ferent IDF into one dimension of feature. For example, given two instances  X  X F-kappaB X ,  X  X RNP gene X , and a CDF  X  X xpression of X , the feature functions will be f expression-of . The two feature values can be computed as follow: mined by the number of CDFs. In addition it has the ability to identify unknown terms. For instance, if  X  X F-kappaB X  is not in the training corpus, the feature f expression-tures by discretization method. 2.2 Feature Generation t rithm is described as follow: 1. Context terms are categorized into three types: left contexts, right contexts 2. For each type of context, calculate the Chi-square values of context terms. A 3. Select the top context terms with high Chi-square scores as CDFs. Common 4. Get the returned Web page count of the query from Google. 5. Features are obtained by discretizing the Web page counts. 2.3 Discretization Methods The range of page count is extremely large and it is impossible to use them directly as features. In our experiment we investigate five methods: non-zero value method, simple binary method, equal frequency, CAIM (Class-Attribute Interdependence Maximization) algorithm [12] and manual threshold: Non-zero value method: only non-zero counts are used as binary features. 
Simple binary method: non-zero and zero counts are selected as two binary fea-tures respectively. 
Equal frequency: zero counts are used as feature, and non-zero counts are discre-tized equally according to width or frequency. 
CAIM: zero counts are used as feature, and non-zero counts are discretized by maximizing the CAIM value iterately. Detail of the algorithm can be found in [12]. 
Manual threshold: zero counts are used as feature, and non-zero counts are divided into two intervals manually. 2.4 Classical Feature most effective features reported in two well-known challenges BioCreative 2004 and JNLPBA 2004. The features include words, normalized terms, n-grams, regular ex-pressions, and stop words list, etc. In addition, some  X  X ntity-level X  features are added (e.g., length of name and characters in a sliding window), since the entity classifica-corporate in most NER systems. 3.1 Experimental Design Training and test corpus in our experiment is derived from the data collection in Bio-Creative2004 Task 1A [1], where system is required to recognize gene/protein names from MEDLINE abstracts. Our named entity classification experiment was to classify a list of terms not considering the context. The instances were obtained by the follow-ing steps: 
First, build a lexicon of gene/protein names from four biomedical databases: Lo-cusLink, EntrezGene, BioThesaurus, and ABGene lexicon. We got a lexicon of common English words and other type of named entities. 
Then, we use maximum match scheme to search the text in training and test data set in BioCreative2004. For both training and test corpus, if the term is in the external dictionary and free text but not in the gold standard, it is treated as negative instance. selected as positive instance. Table 2 shows the information in training and test data. 
In the procedure of feature generation, we obtained 93 left context terms, 98 right context terms and 63 surrounding context terms from training data using method proposed in Section 2. Classifier is SVM-light [13]. We tested our approach in differ-ent size of training data comparing with classical features. Also we show its ability to recognize unknown terms and the performances on different kernels and discretiza-tion methods are investigated. 3.2 Performance of FCD Feature Table 3 shows the comparison of classical features, FCD features and a linear combi-nation of the two features by SVM. Here OOV terms are defined as terms in which any token is not in the training data. Surprisingly, we find that only the FCD features with OOV term identification (nearly 4 percent absolute improvement). This has proved our analysis in Section 1. In the experiment, we find that for classical features the feature using non-linear kernel. However, FCD features are able to work well in a high-order polynomial kernel. Also the number of feature and VC-dimension estimated by SVM-light is significantly reduced. The idea behind is to  X  X ompress X  features space in tera-with SVM gives an improvement (84.47% to 86.12%, and 80.15% to 84.74%), which indicates that the FCD feature does enhance the classification performance. 
Table 4 shows the comparison of different discretization methods using polynomial kernel with degree 6. It can be seen that the performance of most methods differ not surrounding context FCDs. For equal frequency method, we found that the perform-ance was the best when the number of intervals is 2. For CAIM method, all the non-methods, the non-zero Web page count of the name itself was discretized into 8 equal frequency intervals and then converted to binary features. 
From the learning curves (Fig. 1), we can see that the advantage of FCD features is even more apparent when the size of training data is small. Using around 1000 named cent absolute higher than the performance of classical features. It indicates that when depending on the scale of training data. This can be explained by the change of fea-ture distribution with the size augment of training data and in some case the im-provement produced by non-linear kernel is bigger than combination with the classi-cal features. Degree (FCD) features, and its application in biomedical named entity classification. The experimental results show that it is effective in non-linear kernel method and has better generalization ability to recognize OOV terms than classical features. The method can be viewed as a general framework for semi-supervised learning, which three advantages comparing with other semi-supervised learning methods: first, effi-Web. Second, using the statistical figure as features is most robust to deal with noise in unlabeled data. Third, the feature space can be dense and dimension can be low, so the performance can be enhanced by kernel mapping. ample, the method to generate IDFs and CDFs can be further researched. The type of believe that the idea proposed in this paper can be expanded to other problems. Also it and general way for feature generation. Acknowledgments. This work is supported by grant from the Natural Science Foun-dation of China (No.60373095 and 60673039) and the National High Tech Research and Development Plan of China (2006AA01Z151). 
