 Bag-of-visual-words (BoW) has been popular for visual clas-si X cation in recent years. In this paper, we propose a novel BoW expansion method to alleviate the e X ect of visual word correlation problem. We achieve this by di X using the weights of visual words in BoW based on visual word relatedness, which is rigorously de X ned within a visual ontology. The proposed method is tested in video indexing experiment on TRECVID-2006 video retrieval benchmark, and an improve-ment of 7% over the traditional BoW is reported.
 Categories and Subject Descriptors: H.3.1 [Informa-tion Storage and Retrieval]: Content Analysis and Indexing General Terms: Algorithms, Experimentation.
 Keywords: Bag-of-visual-words, visual relatedness, expan-sion, video indexing.
Recently, bag-of-visual-words (BoW) deriving from local keypoints have shown remarkable performance for image and video classi X cation [4, 6]. Keypoints are salient image patches containing rich local information about an image, which can be automatically detected and represented by var-ious detectors and descriptors. Keypoints are then grouped into a number of clusters and each cluster is treated as a vi-sual word. With its keypoints mapped into the visual words, an image can be represented as a feature vector according to the presence or count of each visual word, which forms the basic visual cue in the classi X cation task. This BoW rep-resentation is analogous to the bag-of-words representation for text document.

Under the textual bag-of-words model, a document will not be retrieved if it does not contain the terms that are in a query. This will result in poor recall of a retrieval system when the document contains synonymous terms to the query terms. In visual word based image/video classi X cation, the problem may be even more serious. This is due to the fact that visual words are the outputs of clustering algorithms, and can be correlated to each other due to the quantization e X ect. Motivated by the textual query expansion method which can be used to e X ectively alleviate the synonym prob-lem, we present a novel method for BoW expansion to rem-edy the e X ect of visual word correlation. Figure 1 shows an overview of our approach. Firstly, based on a vocabulary Figure 1: Overview of our approach. The visual word relatedness is rigorously de X ned in a visual on-tology and then used for BoW expansion. of visual words, a visual ontology is constructed to model the hyponym (is-a) relationship of visual words. Within the visual ontology, visual relatedness of visual words is rig-orously de X ned, similar to estimating semantic relatedness of textual words using general purpose resources such as WordNet ontology. Finally, the visual relatedness is cleverly incorporated into BoW for visual word expansion.
In this section, we describe our approach for construct-ing visual ontology and estimating visual relatedness within the ontology. Given a set of keypoints, we  X rst construct a visual vocabulary through clustering the keypoints by k -means algorithm. With the visual vocabulary, a visual on-tology is further generated by adopting agglomerative clus-tering to hierarchically group two visual words at a time in the bottom-up manner. Consequently, the visual words in the vocabulary are represented in a hierarchical tree, namely visual ontology, where the leaves are the visual words and the internal nodes are ancestors modeling the is-a relation-ship of visual words. An example of the visual ontology is shown on the upper right of Figure 1. In the visual ontology, each node is a hyperball in the keypoint feature space. The size (number of keypoints) of the hyperballs increases when traversing the tree from leaves to root.

With the visual ontology, similar with the semantic re-latedness measurements of textual words, the visual relat-edness of visual words can also be explored by considering several popular ontological factors such as path length, or information content (IC). In this paper, we adopt JCN [1] to estimate the visual relatedness. Denote v i and v j as two vi-sual words, JCN considers the ICs of their common ancestor and the two compared words, de X ned as: JCN( v i ; v j ) = 1 where LCA is the lowest common ancestor of visual words v and v j in the visual ontology. IC is quanti X ed as the negative log likelihood of the word probability. The probability is estimated by the percentage of the keypoints in a visual hyperball. For example, the top node \a" in Figure 1 has IC( a ) = 0 since p ( a ) = 1. The visual relatedness is used directly to expand BoW. Let V be a vocabulary of n visual words: V = ( v 1 ; v 2 ; : : : ; v With the vocabulary, an image I can be represented as a feature vector F I = ( w v 1 ; w v 2 ; : : : ; w v n ), where w the weight of word v i in the image. Based on the visual relatedness calculated by JCN, we perform visual word ex-pansion by di X using weight w v i of word v i to another word v : where  X  is a parameter to control the degree of in X  X ence of the JCN relatedness. The aim of the word expansion is to alleviate the problem of visual word correlation. More speci X cally, the weight of a word is di X used by the in X  X ence of other ontologically related words. The di X usion inherently results in the expansion of words in an image to facilitate the utilization of word-to-word correlation for image compari-son. For instance, assume that we have a vocabulary of only two visual words. Given two images each contains one word, say v 1 and v 2 respectively. With the traditional BoW rep-resentation, the L1 distance of the two images is w v 1 + w After applying the BoW expansion, the distance will be j w which is smaller if v 1 and v 2 are highly related to each other.
While the idea of using BoW expansion appears intuitive, expanding all the words will sacri X ce the sparse property of the original BoW representation. Hence, a compromising scheme is to  X rstly sort the JCN relatedness of all word pairs, and then only perform expansion for word pairs with higher relatedness.
To verify the performance of our approach, we conduct video indexing experiments on TRECVID-2006 dataset where the training and testing sets consist of 61,901 and 79,484 video shots respectively. The aim of video indexing is to rank the video shots according to the presence of semantic concepts. We use the 20 semantic concepts which were se-lected in TRECVID-2006 evaluation [5]. We select one key frame per shot, and the keypoints are detected by DoG [3] and described by SIFT [3]. A recent study in [2] showed that the performances of BoW are similar on this dataset for vocabulary sizes ranging from 500 to 10,000. We thus generate a vocabulary of 500 visual words for e X ciency. For all the key frames, the BoW features are calculated based on term frequency ( tf ). For each semantic concept, two SVM classi X ers are trained respectively using the original BoW features, and the new features after expansion (BoW-JCN). Predictions of the SVMs on the testing set are converted Figure 2: Performances of BoW and BoW-JCN for 20 semantic concepts. into posterior probabilities, and the performance evaluation follows TRECVID's standard using average precision (AP) computed over top 2,000 ranked shots.

In our experiments, the parameter  X  in Eq. 2 is empir-ically chosen as 0.5. Expansion is performed among word pairs with relatedness ranking among the top 1%. As a re-sult, about half of the words are involved in the expansion process. Figure 2 contrasts the average precisions of BoW and BoW-JCN. BoW-JCN can improve the performance for 15 out of the 20 concepts, while the performance of the other 5 remains no change or slightly drops. In term of mean aver-age precision (MAP) over the 20 concepts, the improvement of BoW-JCN (0.094) over BoW (0.088) is 7%. The results demonstrate that the visual words are indeed correlated to each other and using visual relatedness for BoW expansion is promising to alleviate this problem. Note that a MAP of 0.094 of a single feature approach is already comparable to the state-of-the-art results on this dataset.
We have presented a novel method for estimating visual word relatedness based on a visual ontology. We showed that the visual relatedness can be used for BoW expansion, in order to alleviate the problem of visual word correlation. Results of large-scale video indexing experiments veri X ed the facts that the visual words are correlated to each other, and using visual relatedness for BoW expansion can lead to bet-ter performance.

