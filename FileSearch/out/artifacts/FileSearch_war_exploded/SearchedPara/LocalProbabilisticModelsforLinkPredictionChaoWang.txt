
One of the cor e tasks in social network analysis is to predict the formation of links (i.e . various types of relation-ships) over time . Previous resear ch has gener ally repr e-sented the social network in the form of a graph and has lever aged topolo gical and semantic measur es of similar -ity between two nodes to evaluate the probability of link formation. Her e we intr oduce a novel local probabilistic graphical model method that can scale to lar ge graphs to estimate the joint co-occurr ence probability of two nodes. Suc h a probability measur e captur es information that is not captur ed by either topolo gical measur es or measur es of se-mantic similarity , whic h are the dominant measur es used for link prediction. We demonstr ate the effectiveness of the co-occurr ence probability featur e by using it both in isola-tion and in combination with other topolo gical and seman-tic featur es for predicting co-author ship collabor ations on thr ee real datasets.
In recent times, there has been a lot of interest in under -standing and characterizing the properties of lar ge scale net-works or graphs. Part of this interest is because of the gener -ality of the graph model: man y domains, such as social net-works, gene regulatory netw orks and the World Wide Web can be naturally thought of as graphs. An important prob-lem in this conte xt is that of link prediction. Informally link prediction is concerned with the problem of predicting the (future) existence of links among nodes in a graph. Link prediction is useful in man y application domains, ranging from recommender systems to the detection of unseen links in terrorism netw orks, from protein interaction netw orks to the prediction of collaborations among scientists, and from prediction of friendship formations to the prediction of web hyperlinks.

In this article we focus on the problem of link predic-tion particularly in the conte xt of evolving co-authorship netw orks. This has been a hotbed of recent research acti v-ity where much of the focus has been on encapsulating the topological and/or semantic information embedded in such netw orks to address the link prediction problem. In contrast in this article we explore the realm of probabilistic models deri ved from frequenc y statistics and use the resulting pre-dictions from the probabilistic models as additional features to further enhance predictions made by topological-based and semantic-based link prediction algorithms.

Specifically our probabilistic model is dri ven by two as-pects. First, given the candidate link (say between nodes X and Y ) whose probability is to be estimated, we identify the centr al neighborhood set (say W;X;Y;Z ), which are the nodes that are deemed germane to the estimation pro-cedure. The identification of the central neighborhood set is governed by the local topology of the social netw ork as vie wed from the perspecti ve of the two nodes whose link probability is to be estimated.

Second, once the central neighborhood set( W;X;Y;Z ) is identified we learn a maximum entrop y Mark ov ran-dom field model that estimates the joint probability of the nodes comprising the central neighborhood set, i.e., p ( W;X;Y;Z ) . In this conte xt one can leverage the fact that most co-authorship netw orks are computed from an event log (an event corresponding to a publication). Multi-w ay statistics (e.g. non-deri vable frequent itemsets [4] whose elements are dra wn from ( W;X;Y;Z )) on these event logs can be used to constrain and learn the model parameters efficiently [19 ]. The resulting model can then be used to estimate the link probability between X and Y which we henceforth denote as the co-occurr ence probability .
In our empirical results we demonstrate that the co-occurrence probabilities inferred from the resulting model can be computed in a scalable manner and is highly dis-criminatory for link prediction when compared with state-of-the-art topological and semantic features on several real world datasets. Moreo ver, we demonstrate that the result-ing co-occurrence probability can also be effecti vely com-bined with these other features and then one can emplo y any classification algorithm to predict if a link will be formed between two nodes. Specifically , we emplo y a simple yet novel variant of the Katz score as a topological feature, one that scales reasonably well at some cost to accurac y. Ad-ditionally we describe and use straightforw ard state-of-the-art methods to measure the semantic overlap among nodes based on the topics the y work on, to further enhance the feature vector and impro ve overall link prediction perfor -mance.
The seminal work of Liben-No well and Kleinber g [12 ] was the first comprehensi ve study on the utility of topolog-ical features deri ved from graphs for predicting links in so-cial netw orks. The y examine various topological features, including graph shortest distance, common neighbors, pref-erential attachment, Adamic-Adar , Jaccard, SimRank, hit-ting time, rooted PageRank, and Katz. The y find that topo-logical information is quite useful when compared to a ran-dom predictor . In particular , Adamic-Adar and the Katz measure appear to be more effecti ve than the other topolog-ical features. Recently , Huang [7] proposes to use another topological feature  X  generalized clustering coef ficient  X  to solv e the link prediction problem.

An important limitation of these works is that the y only use a single (topological) feature for the link prediction task. Intuiti vely , it seems that one can achie ve better performance by utilizing the other sources of information, such as the content or semantic attrib utes of the nodes. A natural way to do this would be to use the multiple sources of information as features to be fed into a classifier that is trained to dis-criminate between positi ve instances (i.e. links that form) and negati ve instances (links that do not form) by making use of all the features. This is the approach adopted by Hasan et al. [6] and O X  X adadhain et al. [14 ]. Hasan et al. [6] have used topological features (such as the short-est distance between the two nodes), aggre gated features (such as the sum of neighbors) and semantic features (such as the number of matching keyw ords). The y report keyw ord match count to be their most useful feature on one dataset, which indicates that a lot is to be gained by taking into con-sideration the semantic similarity in the publications of the two authors. O X  X adadhain et al. [14 ] also have investi-gated the use of content-based attrib utes such as the KL-divergence of the topic distrib utions of the two nodes, their geographic proximity , and similarity of journal publication patterns.

The work of Popescul et al. [16 ] is another interesting approach to inte grating dif ferent kinds of information. The y represent the data in a relational format, generate candidates for features through database join queries, select features using statistical model selection criteria and use Logistic Re gression using the selected features for classification. A potential problem with this approach is that the features so generated are simple aggre gation functions of the column values in the result set of the join queries (such as count and average). A more comple x feature such as cosine similar -ity between bag-of-w ords representations cannot be easily expressed using simple SQL aggre gation functions.
Researchers have also examined the use of probabilistic models for solving the link prediction problem. Taskar et al. [18 ] use discriminati vely trained relational Mark ov net-works to define a joint probabilistic model over the entire graph (i.e. over the links as well as the content attrib utes of the nodes). The trained model is used to collecti vely clas-sify the test data. Kashima and Abe [10 ] propose a parame-terized probabilist model of netw ork evolution and then use it for link prediction. The y assume the netw ork structure is in a stationary state and propose an EM algorithm to es-timate model parameters. The y report encouraging results on two small biological datasets. Ho we ver, both collecti ve classification and training global probabilistic models can be expensi ve to compute and typically do not scale well to medium and lar ge scale netw orks.

In related work, Rattigan and Jensen [17 ] argue that the link prediction problem is too hard to solv e because of the extreme class skew problem. Social netw orks are usually very sparse and positi ve links only hold a very small amount of all possible pairs of nodes. As an alternati ve, the y pro-pose a simpler problem  X  anomalous link disco very . Specif-ically , the y constrain their focus on the links that have been formed and infer their anomaly (surprisingness) scores from the pre vious data.
We consider three sources of information of the netw ork data for link prediction. We have a lar ge number of lo-cal events that are accumulated along time, where by lo-cal event we mean an interaction among a set of objects in the netw ork. For example, the publication of a paper would represent a local event involving all the authors of the pa-per . We refer to the collection of such local events as the event log. This is the raw format of the netw ork data and pro vides the first source of information for link prediction. Such an event log is typically con verted to a graph repre-sentation, in which nodes represent objects in the netw ork and two nodes are connected to each other if the y co-occur in at least one local event. This graph pro vides the second source of information for link prediction. Finally , we have access to other attrib utes of an object in the netw ork, such as the research areas of the authors in an author collaboration netw ork, usually referred to as content or semantic informa-tion. This semantic information pro vides the third source of information for link prediction.

It is dif ficult to capture all information from dif ferent sources with one single feature. For this reason, we ex-amine three types of features  X  co-occurrence probability features, topological features and semantic features -com-ing from the first, second and third source, respecti vely . In the text belo w, we discuss how we deri ve these features in turn.
For a pair of nodes that have never co-occurred in the event log, our aim is to estimate the chances of their co-occurring in the future, i.e. of a link forming in the fu-ture between those two nodes. In order to estimate the co-occurrence probability of the given two nodes in a princi-pled manner , we use probabilistic graphical models. Specif-ically , we emplo y undirected graphical models, also called Mark ov Random Fields (MRFs) , to model the local neigh-borhood containing the two nodes. We stress the fact that we build a local model, rather than a global model, as build-ing global models can become prohibiti vely expensi ve for lar ge scale netw orks.

There are two main stages in our approach to use graph-ical models in this conte xt -(a) determining the nodes that will be included in the local model, and (b) using fre-quent non-deri vable itemsets to determine the structure of the graphical model as well as learn the parameters of the model. Once the model is learned, we use exact inference techniques to determine the co-occurrence probability of the pair of nodes under consideration. We need not resort to ap-proximate inference, as we build a local model, leading to a low tree width (maximum clique size in the graph formed by triangulating the model minus 1) for the model. 3.1.1 Determining the Central Neighborhood Set of For a given pair of nodes, we retrie ve a small set of nodes that we belie ve to be most rele vant to estimating the co-occurrence probability of the given pair of nodes. We re-fer to this set of nodes as the centr al neighborhood set of the two nodes. At one extreme, we can include any node that lies on any path between the two nodes as belonging to the central neighborhood set. Ho we ver, this would lead to a lar ge probabilistic model, over which learning can be expensi ve. For this reason, we set a parameter size that specifies the number of nodes to be present in the central neighborhood set.

We need a mechanism of selecting the nodes to include in the central neighborhood set. Intuiti vely , the nodes that lie along paths of shorter length are more rele vant. Hence, we propose a method of enumerating simple paths length-wise, i.e. we first collect all nodes that lie on length-2 sim-ple paths, and then those on length-3 simple paths and so on. (A simple path is a path without cycles.) The algorithm for enumerating all simple paths of a given length is presented in Figure 1. Ho we ver, this order of enumerating nodes may not be enough as there may be man y nodes that lie on paths of a given length. Hence, we need a way of ordering paths of the same length. For this purpose, we define the fre-quency scor e of a path as the sum of the occurrence counts of all nodes along the paths. No w, among paths of the same length, we enumerate paths with higher frequenc y scores before paths with lower frequenc y scores. The pseudo-code of the full algorithm for selecting the central neighborhood set of a pair of nodes is presented in Figure 2.

Figure 1. En umerating all simple paths of length K between two nodes
Figure 2. Selecting the central neighborhood set for two nodes
We also use a path length threshold in the algorithm, be-cause in practice, we cannot afford to enumerate all simple paths between two nodes in a lar ge graph. In our study , we consider paths up to length 4 because we found that this threshold works well in capturing the conte xtual informa-tion between two nodes. In situations where there does not exist such paths between two nodes, we define the central neighborhood set to be the two nodes themselv es. Interest-ingly , we note that in this case, the local probabilistic model reduces to a simple independence model. 3.1.2 Lear ning Local Mark ov Random Fields For a given pair of nodes and their corresponding central neighborhood set, how should one learn a local probabilistic model for it? We adopt an approach of using non-deri vable frequent itemsets from the underlying netw ork log events data to learn local probabilistic models. The event log is essentially a transactional dataset and we apply widely-used frequent itemset mining techniques [2, 20, 5] on it to collect occurrence statistics of netw ork objects. These statistics can be leveraged afterw ard to learn local probabilistic models on the central neighborhood set. The basic idea of using a set of itemsets to learn a model is as follo w: Each itemset and its occurrence statistic can be vie wed as a constraint on the underlying unkno wn distrib ution. A model that satisfies all present occurrence constraints and in the meanwhile has the maximum entr opy ( X  X s uniform as possible X ) is used as the estimate of the underlying unkno wn distrib ution. One can verify that this maximum entrop y distrib ution specifies a Mark ov Random Field . More information about this can be found in [15 , 19].

In our study we pre-compute all frequent itemsets from the underlying log events. Social netw orks are usually very sparse  X  the proportion of formed links is very low as op-posed to the number of all possible pairs of nodes. For this reason we use a support threshold of one to collect frequent itemset patterns. As a result, all positi ve occurrence evi-dence will be captured. We note howe ver, at this support threshold level, frequent itemsets are too man y to use. For-tunately , we only need to mine and use non-derivable item-sets for model learning. Simply speaking, non-deri vable itemsets are those itemsets whose occurrence statistics can not be inferred from other itemset patterns. As such, non-deri vable itemsets pro vide non-redundant constraints and we can emplo y them to learn probabilistic models without any information loss. Calders et al. [4] propose an efficient depth-first search method to mine non-deri vable itemsets. We use their implementation to mine non-deri vable item-sets.

To predict if two nodes will be link ed, we first identify from the netw ork the central neighborhood set of two in-volv ed nodes. Then we select all itemsets that lie entirely within this set and use them as evidence to learn an MRF . The learned MRF is local in that it specifies a joint distrib u-tion over only those nodes in this set. Then we estimate the joint co-occurrence probability feature of the link through
Figure 3. Inducing co-occurrence probability feature for a pair of nodes inference over the local model. The formal algorithm is presented in Figure 3. We illustrate this whole process with a simple example presented in Figure 4. Assume we want to predict the link between nodes a and b and there are two paths connecting them in the graph: p p 2 = a ! d ! e ! b and p and b . As a result, the central neighborhood set C is given by: C = f a;b;c;d;e g . Ne xt we retrie ve all non-deri vable itemsets that lie entirely within this set. Let us assume that the itemsets retrie ved are: f a, b, c, d, e, ac, ad, bc, be, de Their occurrence statistics are collected from the log events and are presented in the figure. We emplo y all of these pat-terns and their occurrence statistics to learn a local proba-bilistic model M over C : M = P ( a;b;c;d;e ) . M specifies a joint distrib ution on all variables in C and its clique poten-tial functions are listed as follo ws (  X  X  are model parameters to be learned and I () is indicator function). The shaded area
NDI = { a: 0.04, b: 0.03, c: 0.05, d: 0.1, e: 0.02 ac: 0.02, ad: 0.03, bc: 0.01, be: 0.006, de: 0.01 }
Figure 4. An example of local model-based co-occurrence probability feature induction in Figure 4 sho ws a clique of M  X  f a,c g . Then we deri ve the co-occurrence probability of a and b by computing the mar ginal probability of p ( a = 1 ;b = 1) on M .

We use an iterati ve scaling algorithm [9] Learn MRF() to learn a local MRF for the central neighborhood set. The idea is to iterate over all itemset constraints and repeatedly update the model to force it to satisfy the current itemset constraint, until the model con verges. After the model is constructed, we do inference over it to estimate the joint co-occurrence probability of s and t. For the Infer ence() procedure in the algorithm, we can plug in exact inference algorithms for it since our model is local. In our study , we use the Junction Tree inference algorithm [11 ].
The Katz measure is a weighted sum of the number of paths in the graph that connect two nodes, with shorter paths being given the more weight. This leads to the follo wing measure:
Here p nodes s and t , while is a damping factor . It has been sho wn that Katz is among the most effecti ve topological measures for the link prediction task [12 ]. It outperforms shortest distance , hitting time and man y others. It can be verified that the matrix of Katz scores can be computed by ( I M ) 1 I , where M is the adjacenc y matrix of the graph [12 ].

Ho we ver, this method does not scale well to handle the netw ork data under the consideration since computing ma-trix inverse for lar ge graphs is very expensi ve. As such, we come up with a way to approximately compute the Katz score. Specifically , we only consider paths of length up to certain threshold to compute the Katz score. The new mea-sure is as follo ws: where p k is a new input parameter specifying the maximum path length we consider . Since the score terms damp exponen-tially with the longer length, this new measure captures the most significant portion of the exact Katz score. We find that k of 4 can give a good approximation of the Katz scores in practice. We design graph algorithms to evaluate this new measure. To this end, we follo w the similar process of iden-tifying the central neighborhood set for two nodes sho wn abo ve, enumerate all simple paths up to length k from s to and use the abo ve formula to compute an approximate Katz score. We execute breadth-first-search from s up to k levels without visiting t , while keeping track of all paths formed so far. We will denote this approximate Katz measure as aKatz throughout the rest of the paper .
The degree of semantic similarity among entities is something that can be useful to predict links that might not be captured by either topological or frequenc y-based fea-tures. For example, in the conte xt of co-authorship net-works, we use the follo wing method to compute the seman-tic similarity for two authors: 1. Collect the words in the titles of each author (remo ving 2. Deri ve a bag of words representation for each author , 3. Compute the cosine between the TFIDF feature vec-
Pre viously , Hasan et al. [6] have used keyw ord match count between two authors as a feature and have reported the feature to be the most useful feature. Our method for computing semantic similarity mak es use of the well-kno wn techniques such as TFIDF feature vector representation and the cosine measure to compute similarity -the former can weight words automatically and the latter is a widely-used and effecti ve measure for computing similarity between text documents represented in the vector space model.
Since we have three types of features -the co-occurrence probability feature, the topological similarity feature and the semantic similarity feature -we need an effecti ve way to combine these features. For this we resort to supervised learning. In order to do this, we need to come up with a way to partition the original dataset into training and testing datasets. A supervised learning approach to the link predic-tion problem has been tak en pre viously by Hasan et al. [6] and Madadhain et al. [14 ], and the two works have tak en dif ferent approaches to partitioning the dataset into training and testing sets. We find the approach tak en by Madadhain et al. [14 ] to be cleaner and follo w the same, which we de-scribe belo w. An illustration of our approach can be found in Figure 5.

We form a labeled training dataset as follo ws: we tak e all the links that are formed in the 9th year (T9 in Figure 5) and label them as positi ve training instances. Of the links that are not formed in the first 9 years, we randomly sam-ple a subset and label them as negati ve training instances. We sample 10 times as man y negati ve instances as positi ve instances. The features for each of these instances are con-structed from the first 8 years of data. A classifier is then trained on the labeled training set -any off-the-shelf classi-fier can be used, we chose to use Logistic Re gression 1 , since it is computationally efficient, and produces well-calibrated class probabilities that can be used to rank predictions.
The testing dataset is formed in a similar fashion: the links that are formed in the 10th year (T10 in Figure 5) are treated as testing instances that need to be predicted as positi ve, and we include a sample of the links that are not formed in the whole of the dataset as testing instances whose ground truth labeling is negati ve. The features that are used by the classifier trained pre viously are formed from the first 9 years of data.
In this section, we report the experimental results of our proposed approach.
Figure 5. Split of the datasets into Training and Testing data. T1, T2, .. , T10 represent the 10 time inter vals the dataset spans.

Tab le 1. Summar y of the three datasets that were constructed
We evaluated performance on three real datasets in all, described belo w. The details of the datasets are summarized in Table 1.

The Genetics and the Biochemistry datasets were gener -ated from the PubMed database. 3
First we examine the distrib ution of the features among the positi ve and negati ve examples. Figure 6a-c plot the distrib ution for three features on the Genetics dataset. The results on the other two datasets are similar and are not plot-ted here due to space constraints.

One can see that both the co-occurrence probability and the aKatz measure can discriminate among the negati ve and positi ve instances. The main dif ficulty in the link prediction task occurs because the number of links that do not form far outweighs the number of links that do form. The ratio of negati ve instances to positi ve instances in our workload is 10 : 1 .
We belie ve that the co-occurrence probability feature captures information a lar ge chunk of which is not captured by either topological metrics such as aKatz or content-based metrics such as semantic similarity . To test this conjecture, we examined the number of correct predictions that were made by the co-occurrence probability feature that were not made by either aKatz or semantic similarity . The results are sho wn in Table 2. As can be observ ed, in all three of the datasets there exists a significant percentage -up to 75% on the Genetics dataset -of correct predictions in the top 500 that are captured only by the co-occurrence probabil-ity feature and not by the other features. This confirms our hypothesis that the co-occurrence probability feature uses information about the domain that is not captured by other types of features.
We report the classification results when we vary the fea-tures used for classification. First, we report the results when the three features  X  co-occurrence probability , aKatz and semantic similarity  X  are used in isolation. Then we examine the results when we use all three features. Unless otherwise specified, we use the length threshold of 4 for the aKatz feature and 6 for central neighborhood set size of the local probabilistic model-based co-occurrence probabil-ity feature. 4.4.1 Baseline Appr oaches For the sak e of comparison, the results on two baseline ap-proaches  X  the Adamic-Adar measure [1] and Prefer ential Attac hment measure [13 ] are also presented.

The Adamic-Adar measure was originally meant for computing the similarity of two homepages, but has been adapted for computing the similarity between two nodes in a graph by [12 ]. Let ( x ) be the set of all neighbors of node x . Then the similarity between two nodes x;y is given by
The intuition behind the score is that instead of simply counting the number of neighbors shared by two nodes, we should weight the hub nodes less and rarer nodes more.
Preferential Attachment is a measure based on a genera-tive model for graphs that has been well recei ved [3]. Based on the generati ve model that specifies that new nodes are more lik ely to form edges with nodes that have a lar ge num-ber of neighbors, Ne wman [13 ] has proposed the score for two nodes x;y as score ( x;y ) = j ( x ) j : j ( y ) j . 4.4.2 Ev aluation Metrics Pre vious literature has mainly used precision of top-K pre-dictions (K is a user specified parameter -usually the num-ber of true links formed in the testing period) as a metric for evaluation. While this metric has its merits, it has some problems too. Some link prediction methods have relati vely high precisions for their top-K predictions when K is small, because the y are good at predicting the  X  X asy X  links, but the precision drops off dramatically as one increases K. It seems desirable to have an additional evaluation metric that can measure the precision of the classifier without reference to any arbitrary cut-of f point. For this reason, we also use AUC (Area Under the ROC Curv e) to compare dif ferent classifiers, which is a metric that does not need the spec-ification of arbitrary cut-of f points and is widely used to evaluate rankings output by a classifier . Huang [8] has pre-viously used this as an evaluation metric in the conte xt of link prediction. An AUC score of 1.0 represents a perfect classifier , and a score of 0.5 is a random classifier . Visu-ally , the closer the ROC curv e is to the top left corner of the graph, the better the classifier . 4.4.3 Discussion Table 3 presents the AUC scores and precision of top-K pre-dictions for dif ferent features on dif ferent datasets. Follo w-ing [12 ], the K for the precision metric has been chosen to be the number of true links in the testing period. We also plot the ROC Curv es for the dif ferent features and the ensemble method considering all the three features on the three datasets in Figures 7.

The main point to note is that the co-occurrence proba-bility feature consistently outperforms all other features on AUC scores and performs comparably or better on preci-sion. We discuss the results for each dataset in detail belo w.
One can see that on the DBLP dataset, the co-occurrence probability feature yields the best AUC score ( 0 : 8229 ) when the three features are used in isolation. The aKatz feature occurrence Probability (b) aKatz (c) Semantic Similarity not made by either aKatz or semantic similarity . c , k and the set of top 500 predicted links using semantic similarity . number of positive instances (i.e . true links) in the test dataset. yields the second best AUC score 0 : 7501 and it has the best precision for K=1500 (ho we ver the precision for this feature drops une xpectedly at some point after K=1500, leading to the lower AUC score). The semantic feature is inferior to the pre vious two features, yielding 0 : 7148 AUC score and 35 : 93% precision. We note that all three features outper -form the Adamic-Adar measure significantly in terms of both the AUC score and precision. Preferential Attachment outperforms the semantic feature but is worse than the other two features. Furthermore, when we combine the aKatz and the co-occurrence probability features, we impro ve the AUC score to 0 : 8665 as well as the precision to 56 : 06% get the best results when we use all three features together  X  0 : 8722 AUC score and 57 : 66% precision.

The results are even better on the Genetics dataset, where again the co-occurrence probability feature performs signif-icantly better than the other features. This feature alone can give 0 : 7904 AUC score and 46 : 77% precision. The other two features do not perform very well on this dataset when used in isolation, with AUCs dropping to around 0 : 58 and precisions at 32 : 21% and 16 : 79% . When we combine all the three features together , there is not much impro vement in the AUC over the co-occurrence probability alone, but there is a slight impro vement in the precision to 47 : 08% This is because the co-occurrence probability feature has been able to predict a majority of the links that were cor -rectly predicted by the other two features, and predict addi-tional links, leading to not much impro vement when using the three features together . Among the baseline methods, Preferential Attachment performs better than aKatz, seman-tic as well as Adamic-Adar , giving an AUC of 0 : 71 and a precision of 35 : 03% .

Coming to the Biochemistry dataset, we again observ e the same trend of co-occurrence probability being the most useful feature with an AUC of 0 : 83 follo wed by Preferen-tial Attachment with an AUC of 0 : 82 . Precision-wise, aKatz has a slight edge over co-occurrence probability and Pref-erential Attachment, with aKatz slightly better at 54 : 9% whereas the latter two have scores of 52 : 6% and 51 : 12% Combining aKatz and co-occurrence probability impro ves the AUC and the precision to 0 : 8526 and 56 : 4% , with ad-ditionally combining the semantic similarity giving essen-tially no impro vements. The reason the performance of Preferential Attachment is better on this dataset than Ge-netics is that the latter is a sparser dataset (it was prepared from 14 journals), which meant that it gave high scores to pairs of prolific authors even though the y happened to be in dif ferent sub-fields, whereas that would happen less on the Biochemistry dataset.
In this section we report the results on varying central neighborhood size for local probabilistic models. We use the Genetics dataset as an example for this set of exper -iments. The results on other two datasets are consistent. To better validate the use of conte xtual information for co-occurrence probability estimation, we divide the positi ve examples (true links) into dif ferent classes by their short-est distance. We first examine the case where true links are formed between nodes with shortest distance 2 , follo wed by shortest distance 3 and so on. For each class of links, we generate correspondingly negati ve examples using the same ratio ( 1 to 10 ). We train a separate classifier for each class. We examine the true links up to distance 4 .

Specifically on this dataset, we have 173 true links within testing period that are of distance 2 , 247 true links of dis-tance 3 and 382 true links of distance 4 . Table 4 presents the classification results when we use co-occurrence proba-bility feature alone. We vary the size threshold of the cen-tral neighborhood set. The lar ger threshold is, we tend to use more conte xtual information when estimating the joint probability for a link. Note that threshold 2 is essentially the independence model. From the results, one can see that overall the local probabilistic model-based approach outper -forms the independence model by taking into consideration conte xtual information of pairs of nodes. For the links of distance 2 , the AUC score can be impro ved from 0 : 8181 0 : 9621 and we can identify up to 51 more true links. For the links of distance 3 , the AUC score can be impro ved from 0 : 9361 to 0 : 9943 and we can identify up to 28 more true links. Finally for the links of distance 4 , the AUC score can be impro ved from 0 : 8877 to 0 : 9819 and we can identify up to 92 more true links. We see that the conte xtual informa-tion does indeed impro ve predictions.
No w we report the timing performance on co-occurrence probability feature induction when we vary the size of the central neighborhood set. We use the DBLP dataset as an example for this set of experiments. The results on the other two datasets are similar . Table 5 presents the average time used to induce the co-occurrence probability feature for one link. As one can see, when we increase the size of the cen-tral neighborhood set, it tak es more time to compute co-occurrence probability feature. This is expected since the cost of learning a local model is higher as we increase the size of the central neighborhood set. Ov erall, one can see that inducing the co-occurrence probability feature is com-putationally efficient.
Tab le 5. Timing results on co-occurrence probability feature induction
In this paper , we have presented a simple yet effecti ve approach of leveraging local probabilistic models for link prediction. Specifically , we use topological structure of the netw ork to identify the central neighborhood set of two nodes, and then learn a local MRF model constrained on non-deri vable frequent itemsets from this local neighbor -hood. We then infer the co-occurrence (link) probability from the resulting model and feed it as a feature into a su-pervised learning algorithm. We have sho wn that this co-occurrence feature is quite effecti ve for link prediction on real data. When used in combination with other two types of features  X  topological and semantic features, we find that the resulting classification performance impro ves. As fu-ture work, we would lik e to examine and test on additional datasets from other domains. Also, we would lik e to in-vestigate the use of temporal evolution information of these features in link prediction.

