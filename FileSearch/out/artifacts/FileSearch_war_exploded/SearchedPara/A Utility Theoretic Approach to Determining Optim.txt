 Distributed IR systems query a large number of IR servers, merge the retrieved results and display them to users. Since different servers handle collections of different sizes, have different processing and bandwidth capacities, there can be considerable heterogeneity in their response times. The broker in the distributed IR system thus has to make decisions regarding terminating searches based on perceived value of waiting  X  retrieving more documents  X  and the costs imposed on users by waiting for more responses. In this paper, we apply utility theory to formulate the broker X  X  decision problem. The problem is a stochastic nonlinear program. We use Monte Carlo simulations to demonstrate how the optimal wait time may be determined in the context of a comparison shopping engine that queries multiple store websites for price and product information. We use data gathered from 30 stores for a set of 60 books. Our research demonstrates how a broker can leverage information about past retrievals regarding distributions of server response time and relevance scores to optimize its performance. Our main contribution is the formulation of the decision model for optimal wait time and proposal of a solution method. Our results suggest that the optimal wait time is highly sensitive to the manner in which users value from a set of retrieved results differs from the sum of user value from each result evaluated independently. We also find that the optimal wait time increases with the size of the distributed collections, but only if user utility from a set of results is nearly equal to the sum of utilities from each result. H.3.3 [ Information Search and Retrieval ]: Retrieval models, Search process Algorithms, Design, Economics, Human Factors Distributed IR, Optimal wait time, Utility theory, Query termination distributed data sources to gather relevant information in response to a query. The ultimate goal of these systems is to provide a user with access to all resources on the network but give the impression of a single large IR database [6]. Examples include meta-search engines like Metacrawler, comparison shopping engines that may query multiple store websites in real time to gather price and product information, library management systems, etc. Key operational issues that must be addressed during a distributed IR task include which data sources to query, how long to wait for responses, and how to merge the retrieved results [3, 6]. received considerable attention. However, there X  X  been limited work on query termination issues. In general, it is assumed that all the servers that are queried respond in time and the results are subsequently merged and displayed. However, in many instances, it may be sub-optimal to wait until all queried servers have responded. Many of the servers are themselves IR servers and have considerable processing to do locally in response to a query. Hence, a broker can decide to terminate a search before all queried servers have responded if it believes that most relevant documents have been retrieved and user benefit from waiting and retrieving more results is outweighed by the user X  X  cost of waiting. optimal query termination times. We use a utility theoretic approach and formulate the problem as a decision problem. The decision problem is a stochastic nonlinear problem with no analytical solutions. We use Monte-Carlo simulations to solve the problem and determine the optimal wait time. This research also represents a novel application of Utility theory to IR. source selection and results merging in distributed IR. Source selection refers to the process of determining which data sources to query. It is sometimes acceptable to query all data sources. When it is very expensive to query every data source due to resource constraints, source selection techniques may be used. Callan et al [4] represent each database (or server) by its terms and document frequencies to rank order the databases. Other techniques include gGIOSS resource ranking algorithm [8] and indexing servers as a series of blocks [15]. Once results have been retrieved from different sources, they need to be merged. When the data sources are homogeneous or when they cooperate with the broker, the results can be merged based on the relevance score. When the data sources are heterogeneous, more sophisticated techniques for merging are used. Some techniques used include Bayesian models based on overlap of results [2], regression based techniques [20], etc. In terms of methodology, our approach complements studies by Fuhr on a decision-theoretic approach to source selection [6] and by Voorhees on an approach to select sources and merge results based on historic data [21]. Etzioni et al. [22] also study the optimal sequence in which to query information sources in a sequential query problem where the broker pays each information source in order to query it. software agents for information search and filtering. Software agents are programs that can act independently and autonomously to complete tasks on behalf of users. A key feature of a software agent is its ability to act autonomously based on information gathered from its environment. Research on agent application in information search and filtering include studies of agent adaptation [Doorenbos et al. 1997], proactive search [Rhodes and Maes 2000], and common sense inference of search goals by agents [Liu et al 2002]. Some shopbots (short for shopping robots) query a large number of stores in real time to provide users with price and service information for a specified product. Such shopbots are also applications of distributed IR. selection and results merging by analyzing optimal wait times in distributed IR. We integrate insights from Utility theory in order to determine the tradeoffs. Prior work in marketing has shown that consumer behavior is an important factor in information search functions. Building on this literature, Montgomery et al. [16] integrate computational considerations with user behavior considerations in search tasks. Kehoe et al. [10] have shown that consumers have significant costs associated with waiting. Thus, brokers in distributed IR need to trade-off the benefits from waiting (retrieving more results) against the costs. In Section 3, we present a decision theoretic framework to study this tradeoff. In Section 4, we demonstrate how Monte Carlo Simulations may be applied to solve the decision problem. Section 5 presents a heuristic that incorporates feedback from information retrieved in real time during a search. Section 6 concludes our study and presents future work. for the broker X  X  operational decisions. First, the broker analyzes past data on distribution of response times and relevance scores of results retrieved from various servers (Figure 1a). Next, the broker determines which servers to query in response to a user request (Figure 1b). This may be determined by looking at historic data as in Voorhees et al [21] or any other source selection techniques. Next, the broker determines how long to wait for responses. This decision is based on prior (offline) analysis of the response time of servers and the expected relevance scores for the query (or a class of queries) at various servers. Finally, the broker merges the results and displays the same to the user.
 Below, we present a decision model for the optimal wait time based on Utility theory. Let M be the total number of collections/servers queried. T denotes the broker X  X  wait time and denotes the response time of server i . The vector r records the collections retrieved, that is r i =1 if T t i  X  and r n denotes the number of documents returned by server i in response to the query if there is no query termination (i.e., when T is greater than the server response time). The total number of documents retrieved is denoted by R ( R = i i n r  X  The utility from document j retrieved from server i is denoted by U and is the sum of a deterministic ( ij U ) and stochastic component ( ij  X  ): ij ij ij U U  X  + = . The deterministic component is a weighted sum of the attributes that make up user value of information, including relevance, novelty, credibility, etc [Moenart and Souder; Larcker and Lessig]. We will assume for simplicity that value of information depends on relevance alone or that the relevance score accounts for the other dimensions of user value. The IR system (broker) sorts the retrieved results based on this deterministic component and displays the sorted list to the user. The error term represents the uncertainty associated with predicting utility. Even though a document may be ranked on top (highest ij U ), the document may in fact not be highly relevant to the user because of a large error ij  X  . We assume that these are independently and identically distributed random variables and follow a standard normal distribution (mean = 0, standard deviation = 1). ordered based on the relevance scores ij U . [ k ] is used to denote the document ranked k in this ordered list. That is, [1] denotes the most relevant and [ R ] denotes the least relevant document. In order to evaluate the benefit from the information, we need to understand the user X  X  stopping criterion when evaluating the ranked results. Some of those considered in the literature include viewing a specific number of documents, stopping after looking at a specific number of relevant documents, stopping after seeing a certain number of non-relevant documents in a row, etc [6]. Here, we assume that the user evaluates the top P results, where P is specified exogenously. Alternatively, we can incorporate a metric for the user X  X  cognitive cost (for example, one specified by Shugan [18] assumes cost increases linearly with the number of results displayed) and evaluate the value of P for a user. The user evaluates the top P results and derives a benefit (utility) from the displayed information. Finally, we assume that the user evaluates the documents sequentially in the ranked order.  X  is an aggregate measure of the amount of informational intersection among the documents. When there is no intersection in the content of the documents ( 0 =  X  ), then the user utility is simply the value of document i drops because of prior investigation of documents ranked above. The weight  X  ) 1 (  X   X  i e accounts for this decline in value. Thus,  X  measures the extent to which the user X  X  value from the retrieved set differs from the sum of user X  X  value from each result.  X  denotes the consumer X  X  disutility of waiting 1 second and T is the broker X  X  decision variable (wait time). The decision problem is: 
Sort( S ): Set obtained by sorting all elements of S in descending programming problem with no analytical solution. At the time the broker decides on how long to wait for responses, the value of information to be retrieved is not known (i.e., even the deterministic component of utility i U is not known at the time and can at best be estimated). The broker may decide to terminate a search but a highly relevant document may have been retrieved half a second later. Alternatively, the broker may choose to wait for a collection X  X  response, but find that the server ends up taking too long to respond. Or the server may in fact respond soon but the relevance of the document may end up being much lower than anticipated. Thus, it is important for the broker to have some priors regarding server response times and expected relevance scores for documents to be returned by the different servers. U nor the response time of any server is known. The broker has to determine a T* such that any potential improvement in the utility of the offers obtained by waiting more is outweighed by the cost of waiting. The marginal cost of waiting is known ( However, it is difficult to ascertain the marginal benefit of waiting an additional second due to the highly non-linear and stochastic nature of the impact of T on the utility from the offers (see equation 2). This is because r varies with T in a stochastic way. can use Monte Carlo simulations to evaluate the expected utility for given choices of T*. April et al. [1] and Glover et al. [7] provide a useful primer on the merits of combining simulation and optimization in managing the complexity and uncertainty posed by many real-world problems. In order to instantiate the simulation parameters, we consider the case of a shopbot that queries multiple stores in real time and displays retrieved information (on product price at the stores) to the user. Our choice of this context is due to availability of data from past research. However the framework is applicable to other distributed IR setups as well. depends on the offer from the corresponding store (i.e., on the price, shipping time, store brand, etc). We simulate store prices and response times based on real empirical data presented in Montgomery et al. [16]. The authors constructed automated agents and collected prices and delivery information for 60 NewYork Times bestsellers at 30 online bookstores. Table 1 presents the mean and standard deviation of book prices (normalized to the list price of the book 1 ) at the various stores. The prices of the books are normally distributed at each store. Thus, with no additional information except that the book is a New York Times bestseller, the shopbot X  X  best estimate is that Buy.com has the lowest price at 52% of the list price. 
Table 1. The mean and standard deviation of book prices 1 That is, for each book we use (Price of Book at Store)/(List the stores follow a Gamma distribution. This is consistent with the results in Montgomery et al. [16]. However, since their parameter estimates are based on data gathered in 2000 and response times have likely decreased since then, we revise their Gamma distribution parameters downwards such that a typical store has a mean res ponse time of 1.5 sec. price of $20. The price of the book at the stores and the store response times are drawn from random distributions with parameters as described above. In order to compute the utilities, we have to estimate U from the attributes of the offers returned by the stores. This can be estimated directly from previous purchases at the shopbot or be specified by the user. We use the maximum likelihood estimates reported by Smith and Brynjolfson [19]. The authors specify a user utility function as follows: Their estimates are based on an analysis of consumer choice at Dealtime (a shopbot that has since been renamed to Shopping.com) and are listed in Table 2. The table indicates that if the item price of a book increases by 1$, the utility drops by 0.194 utils and similarly, if the book is ordered from A mazon, the utility increases by 0.477 (positive impact of brand). More intuitively, the average shopbot user is willing to pay 477 . 0 = more to Amazon than a lesser branded retailer. Further, we assume that the value of time is $.01/second, time cost is that if a user makes $70,000 annually, each second is roughly worth $.01. simulations and determined the expected utility as a function of the wait time. Figure 2 plots the expected utility against the broker X  X  wait time when 0 =  X  (Utility is additive). The expected utility is maximized when T*=6.25 seconds. Figure 3 plots the optimal wait time against  X  . A high value of implies that user X  X  utility significantly discounts results lower down in the search results because part of the information has already been obtained from an earlier result. The optimal wait time decreases with  X  . This is because the value of waiting once one of the top few results has been obtained is low when high. Any good result likely to be retrieved by waiting more is anyway going to be discounted c onsiderably. Interestingly, the wait time drops in a concave manner initially but switches to a convex decline after  X  has reached a threshold value. That is, the precise value of  X  matters when it is very low. determined by a study of user preferences and by measuring similarity between document content. queries 30 servers each with 1 result to return. This is the case with a comparison shopping application. However, most distributed IR brokers including meta search engines query other IR servers and hence many documents may be returned by each search engine. To analyze the impact of increasing the number of documents per collection, we repeat the Monte Carlo simulations but assume now that each of the 30 servers will return 5 documents each. The distributional parameters are assumed to be the same. That is, each document in the collection is now assumed to have the same utility distribution as the one specified previously for the server (this does not imply that the realized values are the same because both U and  X  are drawn from the distributions). Figure 4 shows that the optimal wait time increases if the number of documents per collection goes up. The increase is highly significant when  X  is low. However, as  X  retrieving more documents is not necessarily better as long as we have a few highly relevant docu ments. Thus, there is little benefit from waiting much longer. Notice the optimal wait times are reasonably high because we have assumed a low cost of waiting and the user utility weights are derived from a shopping context (where the users may have higher benefit from the information). 
Figure 4. Impact of documents per collection on wait time that while the simulations do well on average, they do not account for information a broker may gather in real time during a specific retrieval. For example, during a particular search, a broker may have retrieved the top few documents early on but will end up waiting until T* seconds have passed (or all servers have responded). In this situation, the broker may be better off by terminating the search early si nce it knows that the top few documents have responded (while the broker may not know the top few documents precisely, it can compute the expected utility based on past data). Thus, an adaptive approach is needed that allows a broker to increase or decrease T* based on information retrieved during an actual search. 10,000 simulations and determined the wait time that would have maximized the actual consumer utility for that simulation run. We do not consider how the shopbot would obtain this optimum value but instead consider ex-post (with the knowledge of prices and response times) what choice of T* would have maximized the utility. That is, if there was an oracle that could have precisely computed the prices and response times (and eliminated the stochastic component), then the shopbot could compute the utility associated with any choice of T*. The purpose of these simulations is to characterize the true optimum and explore how we can get close enough. To evaluate the optimum wait time, we only need to evaluate the expected utility at 30 values of T (the response times of the 30 stores). To see why this is so, consider any wait time T that lies between the response times of 2 stores and 1 + i t . With this choice of T, all stores from 1... i are retrieved and the remaining stores are not retrieved (the stores are assumed to be sorted in increasing order of response time). If the shopbot reduces the wait time to i t instead, the same stores are retrieved but the wait time is reduced . This increases the utility to the consumer. Thus, the candidate values of T that would maximize the expected utility are t 1 , t 2 ,...t M evaluate the utility at M discrete values of T alone. This makes the problem of determining the optimal T* tractable. Note that the value of T* obtained from this analysis is different for each run of the simulation. While a certain T* may maximize utility in one run, that same T* may be a poor choice in the next run. Thus, we get 10,000 values of T* at the end of this simulation (one for each run). The results of the simulations are summarized in Table 3. Clearly, waiting for 5.13 seconds (average value of T*) is not optimal (expected utility is maximized at 6.25s. However, if we observe the circumstances under which the system ends the search early and those in which it takes longer, we can potentially do better than in Section 4. shopbot initially sets a wait time of 6.25 seconds (T* that maximizes expected utility) and simultaneously retrieves the table of price predictions and computes expected utility. If the s stores with the highest expected utility are retrieved before 6.25s, it ends the search prematurely. If 6.25 seconds have elapsed and any of the top t offers have not yet been retrieved, it resets the wait time to the u th percentile of T* in previous simulations (for example, additional wait, it has the top t offers, the shopbot ends the search right away. Clearly, this heuristic is not optimal but the question is whether simple heuristics can pr ovide solutions that are close enough. optimal values for s , t and u . We found that the expected utility is percentile. Next, we ran 10,000 new simulations where we evaluated user utility with both approaches  X  one that sets T*=6.25 with the goal of maximizing the expected utility and another that uses the heuristic above to readjust the optimal wait time based on real time information about the results retrieved. The summary of the results from the 10,000 simulations is displayed in table 4. The adaptive heuristic clearly outperforms the static solution along all metr ics (since the cost of waiting was low in these simulations, the percentage gain is not very significant). The adaptive heuristic can use real-time information from the environment to improve the solution for a given run rather than just stick to a solution that is optimal on average. Such a heuristic was easily obtained by analyzing the simulation results and subsequently developing appropriate heuristics. Note also that the algorithm can easily accommodate a learning com ponent by updating a lookup table, which stores the statistics regarding optimal wait time, using new data. The heuristic developed in this Section was arbitrarily selected (even though the specific parameter settings for the heuristic were calibrated). In future work, we plan to consider a number of other heuristics and evaluate their performance. optimal wait times in distributed IR based on Utility theory. Brokers in distributed IR applications can mine information from previous retrievals to determin e the distribution of relevance scores and response times of servers and leverage that to improve operational performance. We showed that the stochastic and real-time nature of the problem can be addressed by using Monte Carlo simulations. Even though the optimal wait times can be computed once the parameters are calibrated, we find that the optimal decision is highly sens itive to the utility discounting parameter  X  , especially at low values of  X  . In other words, the optimal wait time is highly sensitive to the manner in which users value from a set of retrieved results differs from the sum of user value from each result evaluated independently. We also find that the optimal wait time increases w ith the size of the distributed collections, but only if user utility from a set of results is nearly equal to the sum of utilities from each result (i.e., at low values of  X  ). Finally, we evaluated one heuristic that suggests that the static solution can be improved by incorporating real time information retrieved during a search. interested in maximizing consumer utility without any other constraints. There may be other objective functions worth exploring. It would also be useful to conduct user studies to calibrate model parameters and validate modeling assumptions regarding user utility specificati ons. These will be interesting future extensions of the work. Another avenue for future research is to evaluate the impact of different stopping criteria of users on the optimal wait time. In this paper, we only analyzed a stopping criterion that is based on the specific number of documents evaluated. Although the framework was illustrated for the case of shopbots, it is applicable to other distributed Information Retrieval systems as well where decisions about how long to wait for responses must be made. [1] April, J., F. Glover, J. Kelly and M. Laguna (2001), [2] J. A. Aslam, M. Montague. Models for Metasearch. In Proc. [3] Baeza-Yates, R., Ribeiro-Neto, B. (eds.), Modern [4] James P. Callan, Zhihong Lu, and W. Bruce Croft, [5] Doorenbos, B., O. Etzioni, D. Weld,  X  A scalable comparison [6] N. Fuhr,  X  X  decision-theoretic approach to database selection [7] Glover, Fred, J. Kelly and M. Laguna.  X  X ew Advances for [8] L. Gravano and H. Garcia-Molina. Generalizing GloSS to [9] Johnson, Eric J. and J. W. Payne (1985),  X  X ffort and [10] Kehoe, Colleen, J. Pitkow, K. Sutton, G. Agarwal and J. D. [11] Larcker, D. F. and V. P. Lessig,  X  X erceived Usefulness of [12] H. Liu, Henry Lieberman, Ted Selker, "GOOSE: A Goal-[13] McFadden, D. (1980),  X  X conometric Models of Probabilistic [14] Rudy K. Moenaert , William E. Souder, Context and [15] Moffat and J. Zobel. Information retrieval systems for large [16] Montgomery, Alan, K. Hosanagar, R. Krishnan and K. Clay [17] B. Rhodes and Pattie Maes, "Just-in-time information [18] Shugan, Steven M. (1980),  X  X he Cost of Thinking X , Journal [19] Smith, Michael D., and Erik Brynjolfsson.  X  X ustomer [20] Luo Si and Jamie Callan,  X  X sing Sampled Data and [21] Ellen Voorhees, Narendra K. Gupta, and Ben Johnson-Laird, [22] Oren Etzioni, S. Hanks, T. Jiang, R. Karp, O Madani, and O. 
