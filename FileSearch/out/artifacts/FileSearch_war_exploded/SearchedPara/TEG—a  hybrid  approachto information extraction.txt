 REGULAR PAPER Ronen Feldman  X  Benjamin Rosenfeld  X  Moshe Fresko Abstract This paper describes a hybrid statistical and knowledge-based infor-mation extraction model, able to extract entities and relations at the sentence level. The model attempts to retain and improve the high accuracy levels of knowledge-based systems while drastically reducing the amount of manual labour by relying on statistics drawn from a training corpus. The implementation of the model, called TEG (trainable extraction grammar), can be adapted to any IE domain by writing a suitable set of rules in a SCFG (stochastic context-free grammar)-based extraction language and training them using an annotated corpus. The system does not contain any purely linguistic components, such as PoS tagger or shallow parser, but allows to using external linguistic components if necessary. We demonstrate the performance of the system on several named entity extraction and relation extraction tasks. The experiments show that our hybrid approach outperforms both purely statistical and purely knowledge-based systems, while requiring orders of magnitude less manual rule writing and smaller amounts of training data. We also demonstrate the robustness of our system under conditions of poor training-data quality.
 Keywords Text mining  X  Information extraction  X  Hidden Markov models  X  Rule bases systems  X  Hybrid approaches  X  Stochastic context free grammars 1 Introduction The knowledge engineering (mostly rule-based) systems traditionally were the top performers in most IE benchmarks, such as MUC [ 5 ], ACE and the KDD CUP [ 23 ]. Recently, though, the machine learning systems became state of the art, especially for simpler tagging problems, such as named entity recognition [ 2 ] or field extraction [ 20 ].
 is focused around manually writing patterns to extract the entities and relations. The patterns are naturally accessible to human understanding and can be improved in a controllable way. Whereas improving the results of a pure machine-learning system would require providing it with additional training data. However, the im-pact of adding more data soon becomes infinitesimal while the cost of manually annotating the data grows linearly.
 the power of knowledge-based and statistical machine-learning approaches. The system is based on stochastic context-free grammars. It is called TEG, for train-able extraction grammar. The rules for the extraction grammar are written man-ually, while the probabilities are trained from an annotated corpus. The powerful disambiguation ability of PCFGs allows the knowledge engineer to write very sim-ple and naive rules while retaining their power, thus greatly reducing the required labour. In addition, the size of the needed training data is considerably smaller than the size of the training data needed for pure machine-learning systems (for achieving comparable accuracy results). Furthermore, the tasks of rule writing and corpus annotation can be balanced against each other.
 tem and then proceed to the experiments and comparison with other information-extraction systems. 2 Related work The knowledge-based IE systems are well known. We use the DIAL system de-veloped by the authors [ 9 ] as a typical example, showing both the pros and cons of the knowledge-based approach. DIAL is based on a general-purpose rule lan-guage. The system was top performing at ACE-2 after being manually prepared for the task during 2 months by a team of four people.
 tion extraction modules using machine-learning methods. The general idea is that a domain expert labels the target concepts in a set of documents. The system then learns a model of the extraction task, which can be applied to new documents au-tomatically. Approaches can be roughly divided by the type of model they use. First, there are systems generating sets of rules, very much in the flavour of hand-written rules for representing the target concepts. These approaches include meth-ods based on finite-automata [ 16 ], grammar learning[ 12 ] and ILP [1, 13, 14]. tions. Most prominently, hidden Markov models (HMM) have been used for the task of IE (e.g. [3, 10, 11, 17]). HMM are probabilistic automata for which the states themselves are hidden. Once trained, such a model can give an estimate on how probable a text fragment contains the target concept. Besides HMM, there are also other approaches based on naive Bayes [ 8 ], SVM [ 22 ] or approaches combin-ing several of these techniques [8, 13, 15].
 outperform generative HMM models on several tasks. The models are maximum entropy Markov models (MEMM) [ 20 ] and conditional random fields (CRF) [ 18 ]. several systems. Typically, a SCFG is used for syntactic parsing of sentences, as in the BBN SIFT system [ 19 ].
 parsing. Instead, semantically oriented SCFG are constructed manually. The clos-est to our approach is the system described in [ 6 ], which extracted management succession events using a semantically oriented SCFG of a special predefined form. Our system can be viewed as a generalisation and extension of this work. Our system allows grammars of arbitrary structure, extends the possibilities for leaf grammar nodes and adds the possibility of conditioning the probabilities of rules upon context. Also, in contrast with [ 6 ], our system works on real-world documents, which contain many irrelevant sentences. 3 TEG X  X ridging the gap between statistical and rule-based IE systems Although the formalisms based on probabilistic finite-state automata are quite suc-cessful for entity extraction, they have shortcomings, which make them harder to use for the more difficult task of extracting relationships.
 assignment of a tag (state label) to each token in a sequence. This is suitable for the tasks where the tagged sequences do not nest and where there are no explicit re-lations between the sequences. Part-of-speech tagging and entity extraction tasks belong to this category, and indeed the HMM-based PoS taggers and entity ex-tractors are state of the art.
 nest, and there are relations between them, which must be explicitly recognised. While it is possible to use nested automata to cope with this problem, we felt that using a more general context-free grammar formalism would allow for a greater generality and extendibility without incurring any significant performance loss. 3.1 SCFG formalism Classical definition : A stochastic context-free grammar (SCFG) is a quintuple G = ( T , N , S , R , P ) ,where T is the alphabet of terminal symbols (tokens), N is the set of nonterminals, S is the starting nonterminal, R is the set of rules, and P : R [0..1] defines their probabilities. The rules have the form n  X  s 1 s 2 ... s k ,where n is a nonterminal and each s i either token or another nonterminal. As can be seen, SCFG is a usual context-free grammar with the addition of the P function. accept a given string (sequence of tokens) if the string can be produced starting from a sequence containing just the starting symbol, S , and one by one expanding nonterminals in the sequence using the rules from the grammar. The particular way a string was generated can be naturally represented by a parse tree , with the starting symbol as a root, nonterminals as internal nodes and the tokens as leaves. n  X  s Bayesian terms, if it is known that a given sequence of tokens was generated by expanding n ,then P ( r ) is the a priori likelihood that n was expanded using the rule r . Thus, it follows that, for every nonterminal n ,thesum P ( r ) of probabilities of all rules r headed by n must equal one.
 correspond to meaningful language concepts, and the rules define the allowed syntactic relations between these concepts. For instance, in a parsing problem, the nonterminals may include S, NP, VP, etc. and the rules would define the syntax of the language. For example, S NP VP. Then, when the grammar is built, it is used for parsing new sentences. In general, grammars are ambiguous in the sense that a given string can be generated in many different ways. With nonstochastic , there is no way to compare different parse trees, so the only information we can gather for a given sentence is whether or not it is grammatical , that is, whether it can be produced by any parse. With SCFG, different parses have different probabilities; thus, it is possible to find the best one, resolving the ambiguity.
 able (for performance reasons) to perform a full syntactic parsing of all sentences in the document. Instead, a very basic parsing is employed for the bulk of a text, but within the relevant parts, the grammar is much more detailed. Thus, the ex-traction grammars can be said to define sublanguages for very specific domains. Examples of such grammars will be presented in the next section.
 given parse tree by simply multiplying the probabilities of all rules participating in it. Then the usual parsing problem is formulated as follows: given a sequence of tokens (a string , find the most probable parse tree that could generate the string. A simple generalisation of the Viterbi algorithm is able to efficiently solve this problem.
 independent. Then, the easiest way to cope with this problem while leaving most of the formalism intact is to let the probabilities P ( r be conditioned upon the con-text where the rule is applied. If the conditioning context is chosen reasonably, the Viterbi algorithm still works correctly even for this more general problem. 3.2 TEG X  X sing SCFG to perform IE In a first attempt, we created a relationship extractor based on markovian SCFG. Markovian means that every possible rule that can be formed from the available symbols has nonzero probability. Usually, all probabilities are initially set to be equal and then adjusted according to the distributions found in the training data. This strategy is a straightforward generalisation of an HMM-based entity extrac-tor. It has the benefit that all rules are created automatically, and the system only needs the tagged training corpus in order to learn the target domain. But there is also an obvious downside. If the amount of training data is insufficient, such a system performs poorly.
 particular, Markovian SCFG parsers trained on the Penn Treebank perform quite well. HMM entity extractors, which are a particularly simple case of Markovian SCFGs, also perform well [ 4 , 7 , 21 ]. But for the task of relationship extraction, it turns out to be impractical to manually tag the amount of documents that would be sufficient to adequately train such grammar. At a certain point, it becomes more productive to go back to the original hand-crafted system and write rules for it, even though it is a much more skilled labour! traction grammars), which attempts to strike a balance between the two knowl-edge engineer chores X  X riting the extraction rules and manually tagging the docu-ments. In TEG, the knowledge engineer writes SCFG rules, which are then trained on the data that is available. The powerful disambiguating ability of the SCFG makes writing rules a much simpler and cleaner task. Furthermore, the knowledge engineer has the control of the generality of the rules (s)he writes and consequently on the amount and the quality of the manually tagged training corpus the system would require. 3.3 Syntax of a TEG rulebook A TEG rulebook consists of declarations and rules. Rules basically follow the classical grammar rule syntax, with a special construction for assigning concept attributes. Notation shortcuts like [] and | can be used for easier writing. The non-terminals referred by the rules must be declared before use. Some of them can be declared as output concepts , which are the entities, events and facts that the system is designed to extract. Additionally, two classes of terminal symbols also require declaration: termlists and ngrams .
 ten explicitly or loaded from external sources. Examples of termlists are countries, cities, states, genes, proteins, people X  X  first names and job titles. Some linguistic concepts, such as lists of prepositions, can also be defined as termlists. Theoret-ically, a termlist is equivalent to a nonterminal symbol that has a rule for every term.
 to any single token. But the probability of generating a given token is not fixed in the rules, but learned from the training dataset, and may be conditioned on one or more previous tokens. Thus, ngrams is one of the ways the probabilities of TEG rules can be context dependent. The exact semantics of ngrams is explained in the next section.
 Acquirer and Acquired . Then an ngram AdjunctWord is defined, followed by a nonterminal Adjunct , which has two rules, separated by | , together defining Adjunct as a sequence of one or more AdjunctWord s. Then a termlist Ac-quireTerm is defined, expanding as the main acquisition verb phrase. Finally, the single rule for the Acquisition concept is defined X  X s Company, followed by optional Adjunct , delimited by commas, followed by AcquireTerm and a second Company .Thefirst Company is the Acquirer attribute of the output frame and the second is the Acquired attribute.
 lowing set of definitions defines the concept in a manner emulating the behaviour of a HMM entity extractor: and the special nonterminal that would match the strings that do not belong to any of the output concepts: after a very modest training. Note that the grammar is extremely ambiguous. An ngram can match any token, so Company , None and Adjunct are able to match any string. Yet, using the learned probabilities, TEG is usually able to find the correct interpretation. 3.4 TEG training Currently, there are three different classes of trainable parameters in a TEG rule-book: the probabilities of rules of nonterminals, the probabilities of different ex-pansions of ngrams and the probabilities of terms in wordclasses. All those proba-bilities are smoothed maximum likelihood estimates, calculated directly from the frequencies of the corresponding elements in the training dataset.
 finds simple person names: be 1. They can be changed using count syntax, an example of which is shown below. The numbers in parentheses at the left side are not part of the rules and are used only for reference. Let us train this rulebook on the training set containing one sentence: sented the discovery .
 rulebook, but with the constraints specified by the annotations. In our case, the constraints are satisfied by two different parses, expanding Person by rules (1) and (2), respectively. The ambiguity arises because both TLHonorific and NG-FirstName can generate the token Dr. In this case, the ambiguity is resolved in favour of the TLHonorific interpretation because, in the untrained rulebook, we have  X  P ( Dr | TLHonorific ) = 1 / 5 (choice of one term among five equiprobable  X  P ( Dr | NGFirstName )  X  1 / N ,where N is the number of all known words produces the following trained rulebook (only lines that were changed are shown). Note the count syntax: for the ngrams. It is similar in nature, but more complex, because the bigram fre-quencies, token feature frequencies and unknown word frequencies are taken into consideration. In order to understand the details of ngrams training, it is necessary to go over the details of their internal working.
 but naturally the probability of generating depends on the ngram, on the token and on the immediate preceding context of the token. This probability is calculated at the runtime using the following statistics:  X  Freq (  X  ) = total number of times the ngram was encountered in the training  X  Freq ( W ), Freq ( F ), Freq ( T ) = number of times the ngram was matched to  X  Freq ( T | T  X  Freq (  X  X  T So, assuming all those statistics are gathered, the probability of the ngram gener-ating a token T given that the preceding token is T 2 is estimated as model, the back-off unigram model and the further backoff word + feature uni-gram model. The interpolation factor was chosen to be 1 / 2, which is a natural choice. However, the experiments have shown that varying the lambdas in reason-able ranges does not significantly influence the performance.
 The fact that a token was never encountered during the training gives by itself an important clue to the token X  X  nature. In order to be able to use this clue, the separate unknown model is trained. The training set for it is created by dividing the available training data into two halves, and treating all tokens in one half, that are not present in the other half as special unknown tokens. The model trained in this way is used whenever an unknown token is encountered during runtime. 3.5 Additional features There are several additional features that improve the system and help to customise specific nonterminal can be conditioned on the previous token in a way similar to the way ngram probabilities depend on previous token. Other conditioning is, of course, possible, even to the extent of using maximal entropy for combining several conditioning events.
 simultaneously X  X ifferent ngrams may use different token feature sets. This is useful for languages other than English, as well as for special domains. For in-stance, in order to extract the names of chemical compounds or complex gene names, it may be necessary to provide a feature set based on morphological fea-tures. In addition, an external part-of-speech tagger or shallow parser may be used as a feature generator.
 This is especially true for relations. While there could be thousands of persons or organisations in a dataset, the number of acquisitions could well be less than 50. The ngrams participating in the rules for such concepts are surely to be under-trained. In order to alleviate this problem, the shrinkage technique can be used. An infrequent specific ngram can be set to shrink to another, more common and more general ngram. Then the probability of generating a token by the ngram is inter-polated with the corresponding probability for the more common parent ngram. A similar technique was used with great success for HMM [ 3 ], and we found it very useful for TEG as well. 3.6 Example of real rules Here we shall demonstrate a fragment of the TEG rules, written for the extrac-tion of the PersonAffiliation relation from a real industry corpus. The fragment will show a usage of the advanced features of the system, as well as give another glimpse of the flavour of rule writing in TEG.
 name of the organisation and position of the person in the organisation. It is de-clared as follows: Mr.Name,PositionofOrg or OrgPositionMs.Name. Almost any order of the com-ponents is possible, with commas and prepositions inserted as necessary. Also, it is common for Name, Position, or both to be conjunctions of pairs of corresponding entities: Mr.Name1 and Ms.Name2, the Position1 and Position2 of Org, or Org X  X  Position1 and Position2, Ms.Name. In order to catch those complexities, and for general simplification of the rules, we use several auxiliary nonterminals: Names , which catches one or two Names; Positions , which catches one or two Positions and Orgs , which catches Organisations and Locations, which can also be involved in PersonAffiliation, as in Bush, president of US: and PosOrg : Other instances do not conform to the patterns above in several respects. So, in order to improve the accuracy, additional rules need to be written. First, the organ-isation name is often entered into a sentence as a part of a descriptive noun phrase, as in: Ms.Name is a Position of the industry leader Org. In order to catch this in a general way, we define an OrgNP nonterm, which uses an external PoS tagger: used by ngrams via the ngram featureset declaration. The restriction clause in the ngram declaration specifies that the tokens matched by the ngram must belong to the specified feature. Altogether, the set of rules above defines an OrgNP nonterm in a way similar to the way a syntax-parsing grammar would define a noun phrase. In order to use the nonterm in the rules, we simply modify the Orgs nonterminal: noun phrase whatsoever), the way it is used is very restricted. During training, the ngrams of OrgNP learn the distributions of words for this particular use and, during the run, the probability of OrgNP generating a true organisation-related noun phrase is much greater than for any other noun phrase in the text. sonAffiliation instances, in which some irrelevant sentence fragments separate the attributes. For example, ORG ......said the company X  X  Pos ition Mr.Name. In order to catch the  X ...... X  part, we can use the None nonterm, which generates all irrelevant fragments in the text. Alternatively, we can create a separate ngram and a nonterminal for the specific use of catching irrelevant fragments inside PersonAffiliation. Both those solutions have their disadvantages. The None nonterminal is too general and doesn X  X  catch the specifics of the particular case. A specific nonterminal, on the other hand, is very much undertrained. The solution is to use a specific nonterminal but shrink its ngram to None: is already a good result for relationship extraction from a real corpus. And the process of writing rules can be continued to further improve the accuracy. 4 Experimental evaluation MUC-7, ACE-2 and an industry corpus, which we will call INC. 4.1 MUC-7 evaluation X  X omparison with HMM-based NER The MUC-7-named entity recognition corpus consists of a set of news articles related to aircraft accidents, altogether containing about 200 K words, with the named entities manually categorised into three basic categories: PERSON, OR-GANIZATION and LOCATION. Some other entities are also tagged: dates and times, monetary units, etc., but they did not take part in our evaluation. the difference in the performance between the four entity extractors: the regular HMM, its emulation using TEG, a set of hand-craft rules written in DIAL and the Full TEG system, which consists of the HMM emulation augmented by a small set of hand-crafted rules (about 50 lines of code added).
 is due to slight differences in probability conditioning methods. It is evident that the hand-crafted rules performed better than the HMM-based extractors but were inferior to the TEG extractor. Significantly, the hand-crafted rules achieved the best precision; however, their recall was far worse.
 higher than we were able to produce using our version of a HMM entity extractor. We hypothesise that the reason for the difference is the usage of additional training data in the Nymble experiments. The paper mentions using approximately 750 K words of training data, while we had only 200 K. Regardless of the reasons for the difference, the experiment clearly shows that the addition of a small number of handcrafted rules can further improve the results of a purely automatic HMM-based named entity extraction. 4.2 ACE-2 evaluation X  X xtracting relationships The ACE-2 was a follow-up to ACE-1 and included, in addition to tagged entities, also tagged relationships. The ACE-2 annotations are more complex than those supported by the current version of our system. Most significantly, the annota-tions resolve all anaphoric references, which is outside the scope of the current implementation. Therefore, it was necessary to remove annotations that contained anaphoric references. This was done automatically, using a simple Perl script. lation. The original ACE-2 annotations make finer distinctions between the differ-ent kinds of ROLE, but for the purposes of current evaluation, we felt it sufficient to just recognise the basic relationships and find their attributes.
 show the performance of the HMM entity extractor on the entities in the same dataset.
 rules is rather mediocre. However, by adding a small number of hand-crafted rules (altogether about 100 lines of code), it was raised considerably (by 15% in F1). The performances of the three systems on the named entities differ very little because it is essentially the same system. The slight improvement of the Full TEG system is due to better handling of the entities that take part in ROLEs. tion of the amount of available training data. There are three graphs in the figure, a graph that represents the accuracy of the grammar with no specific ROLE rules, a graph that represents the accuracy of the grammar with four ROLE rules and, finally, a graph that represents the accuracy of the grammar with seven ROLE rules.
 system needs 125 K of training data when using all of the specific ROLE rules, while 250 K of training data are needed when no specific rules are present. Thus, adding a small set of simple rules may save 50% of the training-data requirements. 4.3 Internal news corpus evaluation X  X xtracting real-world relationships In addition to the classic data sets, we used one large new corpus that was anno-tated internally by students (the INC Corpus). Clearly, the quality of the annotation is not as good as the professional annotators that annotated the MUC and ACE cor-puses. Based on our experiments, we estimate that about 5 X 10% of the tags in the training corpus are erroneous. The INC corpus contains about 1 million tokens of news articles, annotated with the common Person, Organisation, Location, Cur-rency entities and three common relations X  X ersonAffiliation, OrgLocation and Acquisition. OrgLocation has two attributes X  X n organisation name and its loca-tion. Acquisition also has two attributes X  X he buyer company and the acquired company. PersonAffiliation was described in one of the previous sections. the INC corpus. In the exact-match results, an instance of a relation is considered and all boundaries are correctly set. In the partial-match results, an instance accuracy results of the entity extraction from the INC corpus are summarised in Ta b l e 4 . 4.4 INC corpus evaluation X  X ffects of poor annotation quality The INC corpus is annotated by nonexperts, and the quality of annotations is sig-nificantly worse than the quality of MUC and ACE corpora. We estimate the anno-tations to be about 90 X 95% accurate. The following experiment shows that TEG can work robustly under poor training conditions, a very important trait it shares with most other probabilistic-based systems.
 the test corpus should be clean because, otherwise, the results of the evalua-tion are misleading. Therefore, we randomly selected a small set of documents (about 30 K words) from the corpus, manually double checked and cleaned pus and the development test corpus were left noisy. This is why the final test results are actually better than the development test results, as we show in Ta b l e 4 .
 results. This shows the ability of TEG to effectively ignore bad or inconsistent examples. And it shows the possibility of using TEG to improve the annotation quality.
 also showed a robust behaviour and consistently achieved about 6 X 7% F-measure less than TEG in all categories. By disabling different parts of the TEG ruleset, we analysed exactly which of TEG abilities produces the improvement over the baseline HMM. It turned out that there exist three distinctly different parts, con-tributing about equally to the overall improvement. First, there are gazetteers  X  X he lists of known entities X  X hich TEG is able to use and HMM is not. Second, there are structural rules , which define the internal structure of concepts with greater precision. And finally, there are context rules , which place entities into contexts made of words and other entities. While the gazetteers and structural rules are pos-sible to implement using FSA-based models, such as MEMM or CRF, the context rules by their nature require stronger models.
 5 Implementation issues The current version of TEG is written in C ++ and implements the agenda-based probabilistic chart parser. The worst-case performance of the parser is O ( CSN 3 ), where C is the number of nonterminals, S is the number of grammar states and N is the sequence length. For reasonable grammars, the average case performance is actually O ( N 2 ), which is much better than the worst case but still not as good as the linear O ( S 2 N performance of HMM models. However, we found that, by implementing a simple approximation, it is possible to greatly increase the per-formance without reducing the extraction accuracy. The idea of the method is to exclude a grammar edge from further consideration if its inner probability is less than a small fraction of the best probability currently achieved for the sequence spanned by the edge. By lowering the fraction constant it is possible to trade ac-curacy for performance. The graph of their interdependence for the ACE-2 exper-iment is shown in Fig. 2 . Note that the left-most values where the best accuracy is achieved are still about 15 times faster than the original nonoptimised version. The optimised version achieves performance of 600 Kb/min on a 2-GHz processor. 6 Future work The next stages of the TEG development will continue in three principal direc-tions. First, we will improve the abilities of the system by adding desirable features such as the ability to handle overlapping concepts and more refined probability conditioning. Second, we plan to develop an integrated rule writing and tagging GUI environment, which will allow the two tasks to be done in parallel, support-ing and bootstrapping each other. There is also a significant practical problem of checking the consistency of tagging and of helping the annotators find and fix the tagging errors. Finally, in order to be more useful, the system itself should be integrated into a larger information extraction architecture, which might include a knowledge base of known facts, a postprocessor resolving coreferences and an ability to combine the extracted relations into complete scenario templates. 7 Summary We have presented the TEG system, which is a novel information-extraction sys-tem based on the SCFG formalism. The TEG system takes a middle ground be-tween the knowledge-engineering approach and the machine learning-based ap-proach. Because the rules used in TEG are far simpler than the typical information extraction rules, it overcomes the knowledge acquisition bottleneck, which is the main hurdle of the knowledge-engineering approach. In addition, compared with the pure machine-learning approach, it enables getting better accuracies using a smaller number of annotated documents. This reduction in the needed number of annotated documents is crucial in real-world scenarios because it enables a much more rapid deployment of the extraction technology.
 References
