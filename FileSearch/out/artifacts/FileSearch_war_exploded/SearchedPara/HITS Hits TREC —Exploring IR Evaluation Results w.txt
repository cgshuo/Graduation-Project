 We propose a novel method of analysing data gathered from TREC or similar information retrieval evaluation experi-ments. We define two normalized versions of average pre-cision, that we use to construct a weighted bipartite graph of TREC systems and topics. We analyze the meaning of well known  X  and somewhat generalized  X  indicators from social network analysis on the Systems-Topics graph. We apply this method to an analysis of TREC 8 data; among the results, we find that authority measures systems perfor-mance, that hubness of topics reveals that some topics are better than others at distinguishing more or less effective systems, that with current measures a system that wants to be effective in TREC needs to be effective on easy topics, and that by using different effectiveness measures this is no longer the case.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Measurement, Experimentation IR evaluation, TREC, Social Network Analysis, Kleinberg X  X  HITS algorithm.
Evaluation is a primary concern in the Information Re-trieval (IR) field. TREC (Text REtrieval Conference) [12, 15] is an annual benchmarking exercise that has become a de facto standard in IR evaluation: before the actual con-ference, TREC provides to participants a collection of doc-uments and a set of topics (representations of information Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. needs). Participants use their systems to retrieve, and sub-mit to TREC, a list of documents for each topic. After the lists have been submitted and pooled, the TREC organizers employ human assessors to provide relevance judgements on the pooled set. This defines a set of relevant documents for each topic. System effectiveness is then measured by well es-tablished metrics (Mean Average Precision being the most used). Other conferences such as NTCIR, INEX, CLEF pro-vide comparable data.

Network analysis is a discipline that studies features and properties of (usually large) networks, or graphs. Of partic-ular importance is Social Network Analysis [16], that studies networks made up by links among humans (friendship, ac-quaintance, co-authorship, bibliographic citation, etc.). Network analysis and IR fruitfully meet in Web Search Engine implementation, as is already described in textbooks [3,6]. Current search engines use link analysis techniques to help rank the retrieved documents. Some indicators (and the corresponding algorithms that compute them) have been found useful in this respect, and are nowadays well known: Inlinks (the number of links to a Web page), PageRank [7], and HITS (Hyperlink-Induced Topic Search) [5]. Several extensions to these algorithms have been and are being pro-posed. These indicators and algorithms might be quite gen-eral in nature, and can be used for applications which are very different from search result ranking. One example is using HITS for stemming, as described by Agosti et al. [1].
In this paper, we propose and demonstrate a method for constructing a network, specifically a weighted complete bidirectional directed bipartite graph, on a set of TREC top-ics and participating systems. Links represent effectiveness measurements on system-topic pairs. We then apply anal-ysis methods originally developed for search applications to the resulting network. This reveals phenomena previously hidden in TREC data. In passing, we also provide a small generalization to Kleinberg X  X  HITS algorithm, as well as to Inlinks and PageRank.

The paper is organized as follows: Sect. 2 gives some mo-tivations for the work. Sect. 3 presents the basic ideas of normalizing average precision and of constructing a systems-topics graph, whose properties are analyzed in Sect. 4; Sect. 5 presents some experiments on TREC 8 data; Sect. 6 dis-cusses some issues and Sect. 7 closes the paper.
We are interested in the following hypotheses: 1. Some systems are more effective than others; 2. Some topics are easier than others; 3. Some systems are better than others at distinguishing 4. Some topics are better than others at distinguishing
The first of these hypotheses needs no further justification  X  every reported significant difference between any two sys-tems supports it. There is now also quite a lot of evidence for the second, centered on the TREC Robust Track [14]. Our primary interest is in the third and fourth. The third might be regarded as being of purely academic interest; how-ever, the fourth has the potential for being of major prac-tical importance in evaluation studies. If we could identify a relatively small number of topics which were really good at distinguishing effective and ineffective systems, we could save considerable effort in evaluating systems.

One possible direction from this point would be to attempt direct identification of such small sets of topics. However, in the present paper, we seek instead to explore the relation-ships suggested by the hypotheses, between what different topics tell us about systems and what different systems tell us about topics. We seek methods of building and analysing a matrix of system-topic normalised performances, with a view to giving insight into the issue and confirming or re-futing the third and fourth hypotheses. It turns out that the obvious symmetry implied by the above formulation of the hypotheses is a property worth investigating, and the investigation does indeed give us valuable insights.
From TREC results, one can produce an Average Preci-sion (AP) table (see Tab. 1a): each AP( s i , t j ) value mea-sures the AP of system s i on topic t j .

Besides AP values, the table shows Mean Average Pre-cision (MAP) values i.e., the mean of the AP values for a single system over all topics, and what we call Average Av-erage Precision (AAP) values i.e., the average of the AP values for a single topic over all systems: MAPs are indicators of systems performance: higher MAP means good system. AAP are indicators of the performance on a topic: higher AAP means easy topic  X  a topic on which all or most systems have good performance.
MAP is a standard, well known, and widely used IR ef-fectiveness measure. Single AP values are used too (e.g., in AP histograms). Topic difficulty is often discussed (e.g., in TREC Robust track [14]), although AAP values are not used and, to the best of our knowledge, have never been proposed (the median , not the average, of AP on a topic is used to produce TREC AP histograms [11]). However, the AP values in Tab. 1 present two limitations, which are symmetric in some respect: These two problems are a sort of breakdown of the well known high influence of topics on IR evaluation; again, our formulation makes explicit the topics / systems symmetry.
To avoid these two problems, we can normalize the AP table in two ways. The first normalization removes the in-fluence of the single topic ease on system performance. Each AP( s i , t j ) value in the table depends on both system good-ness and topic ease (the value will increase if a system is good and/or the topic is easy). By subtracting from each AP( s i , t j ) the AAP( t j ) value, we obtain  X  X ormalized X  AP values ( AP A ( s i , t j ), Normalized AP according to AAP ): that depend on system performance only (the value will in-crease only if system performance is good). See Tab. 2a.
The second normalization removes the influence of the sin-gle system effectiveness on topic ease: by subtracting from each AP( s i , t j ) the MAP( s i ) value, we obtain  X  X ormalized X  AP values ( AP M ( s i , t j ), Normalized AP according to MAP ): that depend on topic ease only (the value will increase only if the topic is easy, i.e., all systems perform well on that topic). See Tab. 2b.

In other words, AP A avoids Problem 1: AP A ( s, t ) values measure the performance of system s on topic t normalized Table 2: Normalizations: AP A and MAP : normalized AP ( AP A ) and MAP ( MAP ) (a); normalized AP ( AP M ) and AAP ( AAP ) (b); a numeric example (c) and (d) according to the ease of the topic (easy topics will not have higher AP A values). Now, if, for example, AP A ( s 1 , t AP A ( s 1 , t 1 ), we can infer that s 1 is a good system on t a bad system on t 1 (see Tab. 2c). Vice versa, AP M avoids Problem 2: AP M ( s, t ) values measure the ease of topic t according to system s , normalized according to goodness of the system (good systems will not lead to higher AP M values). If, for example, AP M ( s 2 , t 1 ) &gt; AP M ( s can infer that t 1 is considered easier by s 2 than by s 1 Tab. 2d).

On the basis of Tables 2a and 2b, we can also define two new measures of system effectiveness and topic ease, i.e., a Normalized MAP ( MAP), obtained by averaging the AP A values on one row in Tab. 2a, and a Normalized AAP ( AAP), obtained by averaging the AP M values on one column in Tab. 2b:
Thus, overall system performance can be measured, be-sides by means of MAP, also by means of MAP. Moreover, MAP is equivalent to MAP, as can be immediately proved by using Eqs. (5), (3), and (1): (and 1 n P n j =1 AAP( t j ) is the same for all systems). And, conversely, overall topic ease can be measured, besides by s . . . AP M s s . . . AP A s Figure 1: Construction of the adjacency matrix.
 AP A T is the transpose of AP A . means of AAP, also by means of AAP, and this is equivalent (the proof is analogous, and relies on Eqs. (6), (4), and (2)).
The two Tables 2a and 2b are interesting per se, and can be analyzed in several different ways. In the following we propose an analysis based on network analysis techniques, mainly Kleinberg X  X  HITS algorithm. There is a little further discussion of these normalizations in Sect. 6.
The two tables 2a and 2b can be merged into a single one with the procedure shown in Fig. 1. The obtained matrix can be interpreted as the adjacency matrix of a complete weighted bipartite graph, that we call Systems-Topics graph . Arcs and weights in the graph can be interpreted as follows:
Figs. 2c and 2d show the Systems-Topics complete weighted bipartite graph, on a toy example with 4 systems and 2 top-ics; the graph is split in two parts to have an understandable graphical representation: arcs in Fig. 2c are labeled with AP M values; arcs in Fig. 2d are labeled with AP A values.
The sum of weighted outlinks, i.e., the sum of the weights on the outgoing arcs from each node, is always zero: Figure 2: The relationships between systems and topics (a) and (b); and the Systems-Topics graph for a toy example (c) and (d). Dashed arcs correspond to negative values. . . . s . . . t
The average 1 of weighted inlinks is: Therefore, weighted inlinks measure either system effective-ness or topic ease; weighted outlinks are not meaningful. We could also apply the PageRank algorithm to the network; the meaning of the PageRank of a node is not quite so ob-vious as Inlinks and Outlinks, but it also seems a sensible measure of either system effectiveness or topic ease: if a sys-tem is effective, it will have several incoming links with high
Usually, the sum of the weights on the incoming arcs to each node is used in place of the average; since the graph is complete, it makes no difference. weights ( AP A ); if a topic is easy it will have high weights ( AP M ) on the incoming links too. We will see experimental confirmation in the following.
Let us now turn to more sophisticated indicators. Klein-berg X  X  HITS algorithm defines, for a directed graph, two indicators: hubness and authority ; we reiterate here some of the basic details of the HITS algorithm in order to empha-size both the nature of our generalization and the interpreta-tion of the HITS concepts in this context. Usually, hubness and authority are defined as h( x ) = P x  X  y a( y ) and a( x ) = P y  X  x h( y ), and described intuitively as  X  X  good hub links many good authorities; a good authority is linked from many good hubs X . As it is well known, an equivalent formulation in linear algebra terms is (see also Fig. 3): (where h is the hubness vector, with the hub values for all the nodes; a is the authority vector; A is the adjacency ma-trix of the graph; and A T its transpose). Usually, A con-tains 0s and 1s only, corresponding to presence and absence of unweighted directed arcs, but Eq. (7) can be immediately generalized to (in fact, it is already valid for) A containing any real value, i.e., to weighted graphs.

Therefore we can have a  X  X eneralized version X  (or rather a generalized interpretation, since the formulation is still the original one) of hubness and authority for all nodes in a graph. An intuitive formulation of this generalized HITS is still available, although slightly more complex:  X  X  good hub links, by means of arcs having high weights, many good authorities; a good authority is linked, by means of arcs hav-ing high weights, from many good hubs X . Since arc weights can be, in general, negative, hub and authority values can be negative, and one could speak of unhubness and unauthority ; the intuitive formulation could be completed by adding that  X  X  good hub links good unauthorities by means of links with highly negative weights; a good authority is linked by good unhubs by means of links with highly negative weights X . And, also,  X  X  good unhub links positively good unauthor-ities and negatively good authorities; a good unauthority is linked positively from good unhubs and negatively from good hubs X .

Let us now apply generalized HITS to our Systems-Topics graph. We compute a ( s ), h ( s ), a ( t ), and h ( t ). Intuitively, we expect that a ( s ) is somehow similar to Inlinks, so it should be a measure of either systems effectiveness or topic ease. Similarly, hubness should be more similar to Outlinks, thus less meaningful, although the interplay between hub and authority might lead to the discovery of something dif-ferent. Let us start by remarking that authority of topics and hubness of systems depend only on each other; similarly hubness of topics and authority of systems depend only on each other: see Figs. 2c, 2d and 3.

Thus the two graphs in Figs. 2c and 2d can be analyzed independently. In fact the entire HITS analysis could be done in one direction only, with just AP M ( s, t ) values or alternatively with just AP A ( s, t ). As discussed below, prob-ably most interest resides in the hubness of topics and the authority of systems, so the latter makes sense. However, in this paper, we pursue both analyses together, because the symmetry itself is interesting.
 Considering Fig. 2c we can state that: We can summarize this as: a ( t ) is high if AP M ( s, t ) is high for those systems with high h ( s ); h ( s ) is high if AP is high for those topics with high a ( t ). Intuitively, authority a ( t ) of a topic measures topic ease; hubness h ( s ) of a system measures system X  X   X  X apability X  to recognize easy topics. A system with high unhubness (negative hubness) would tend to regard easy topics as hard and hard ones as easy.
The situation for Fig. 2d, i.e., for a ( s ) and h ( t ), is anal-ogous. Authority a ( s ) of a system node s measures system effectiveness: it increases with the weight on the arc (i.e., AP A ( s, t j )) and the hubness of the incoming topic nodes t Hubness h ( t ) of a topic node t measures topic capability to recognize effective systems: if h ( t ) &gt; 0, it increases further if AP A ( s, t j ) increases; if h ( t ) &lt; 0, it increases if AP decreases.

Intuitively, we can state that  X  X  system has a higher au-thority if it is more effective on topics with high hubness X ; and  X  X  topic has a higher hubness if it is easier for those systems which are more effective in general X . Conversely for system hubness and topic authority:  X  X  topic has a higher authority if it is easier on systems with high hubness X ; and  X  X  system has a higher hubness if it is more effective for those topics which are easier in general X .

Therefore, for each system we have two indicators: au-thority ( a ( s )), measuring system effectiveness, and hubness ( h ( s )), measuring system capability to estimate topic ease. And for each topic, we have two indicators: authority ( a ( t )), measuring topic ease, and hubness ( h ( t )), measuring topic capability to estimate systems effectiveness. We can define them formally as a ( s ) = X a ( t ) = X
We observe that the hubness of topics may be of particular interest for evaluation studies. It may be that we can evalu-ate the effectiveness of systems efficiently by using relatively few high-hubness topics.
We now turn to discuss if these indicators are meaningful and useful in practice, and how they correlate with standard measures used in TREC. We have built the Systems-Topics graph for TREC 8 data (featuring 128 2 systems  X  actually,
Actually, TREC 8 data features 129 systems; due to some bug in our scripts, we did not include one system (8manexT3D1N0), but the results should not be affected. Figure 4: Distributions of AP , AP A , and AP M values in TREC 8 data Table 3: Correlations between network analysis measures and MAP (a) and AAP (b) runs  X  on 50 topics). This section illustrates the results ob-tained mining these data according to the method presented in previous sections.

Fig. 4 shows the distributions of AP, AP A , and AP M : whereas AP is very skewed, both AP A and AP M are much more symmetric (as it should be, since they are constructed by subtracting the mean). Tables 3a and 3b show the Pear-son X  X  correlation values between Inlinks, PageRank, Hub, Authority and, respectively, MAP or AAP (Outlinks val-ues are not shown since they are always zero, as seen in Sect. 4). As expected, Inlinks and PageRank have a perfect correlation with MAP and AAP. Authority has a very high correlation too with MAP and AAP; Hub assumes slightly lower values.

Let us analyze the correlations more in detail. The corre-lations chart in Figs. 5a and 5b demonstrate the high cor-relation between Authority and MAP or AAP. Hubness presents interesting phenomena: both Fig. 5c (correlation with MAP) and Fig. 5d (correlation with AAP) show that correlation is not exact, but neither is it random. This, given the meaning of hubness (capability in estimating topic ease and system effectiveness), means two things: (i) more ef-fective systems are better at estimating topic ease; and (ii) easier topics are better at estimating system effectiveness. Whereas the first statement is fine (there is nothing against it), the second is a bit worrying. It means that system ef-fectiveness in TREC is affected more by easy topics than by difficult topics, which is rather undesirable for quite obvious reasons: a system capable of performing well on a difficult topic, i.e., on a topic on which the other systems perform badly, would be an important result for IR effectiveness; con-(b); MAP and hub of systems (c) and AAP and hub of topics (d) versely, a system capable of performing well on easy topics is just a confirmation of the state of the art. Indeed, the cor-relation between hubness and AAP (statement (i) above) is higher than the correlation between hubness and MAP (cor-responding to statement (ii)): 0 . 92 vs. 0 . 80. However, this phenomenon is quite strong. This is also confirmed by the work being done on the TREC Robust Track [14].

In this respect, it is interesting to see what happens if we use a different measure from MAP (and AAP). The GMAP (Geometric MAP) metric is defined as the geometric mean of AP values, or equivalently as the arithmetic mean of the log-arithms of AP values [8]. GMAP has the property of giving more weight to the low end of the AP scale (i.e., to low AP values), and this seems reasonable, since, intuitively, a per-formance increase in MAP values from 0 . 01 to 0 . 02 should be more important than an increase from 0 . 81 to 0 . 82. To use GMAP in place of MAP and AAP, we only need to take the logarithms of initial AP values, i.e., those in Tab. 1a (zero values are modified into  X  = 0 . 00001). We then repeat the same normalization process (with GMAP and GAAP  X  Geometric AAP  X  replacing MAP and AAP): whereas authority values still perfectly correlate with GMAP (0 . 99) and GAAP (1 . 00), the correlation with hubness largely dis-appears (values are  X  0 . 16 and  X  0 . 09  X  slightly negative but not enough to concern us).

This is yet another confirmation that TREC effectiveness as measured by MAP depends mainly on easy topics; GMAP appears to be a more balanced measure. Note that, per-haps surprisingly, GMAP is indeed fairly well balanced, not biased in the opposite direction  X  that is, it does not over-emphasize the difficult topics.

In Sect. 6.3 below, we discuss another transformation, re-placing the log function used in GMAP with logit. This has a similar effect: the correlation of mean logitAP and average logitAP with hubness are now small positive numbers (0.23 and 0.15 respectively), still comfortably away from the high correlations with regular MAP and AAP, i.e., not presenting the problematic phenomenon (ii) above (over-dependency on easy topics).

We also observe that hub values are positive, whereas au-thority assumes, as predicted, both positive and negative values. An intuitive justification is that negative hubness would indicate a node which disagrees with the other nodes, e.g., a system which does better on difficult topics, or a topic on which bad systems do better; such systems and topics would be quite strange, and probably do not appear in TREC. Finally, although one might think that topics with several relevant documents are more important and difficult, this is not the case: there is no correlation between hub (or any other indicator) and the number of documents relevant to a topic.
There has been considerable interest in recent years in questions of statistical significance of effectiveness compar-isons between systems (e.g. [2,9]), and related questions of how many topics might be needed to establish differences (e.g. [13]). We regard some results of the present study as in some way complementary to this work, in that we make a step towards answering the question  X  Which topics are best for establishing differences? X .

The results on evaluation without relevance judgements such as [10] show that, to some extent, good systems agree on which are the good documents. We have not addressed the question of individual documents in the present analysis, but this effect is certainly analogous to our results.
At this point it is also worthwhile to analyze what would happen without the MAP-and AAP-normalizations defined in Sect. 3.3. Indeed, the process of graph construction (Sect. 3.4) is still valid: both the AP M and AP A matrices are replaced by the AP one, and then everything goes on as above. Therefore, one might think that the normalizations are unuseful in this setting.

This is not the case. From the theoretical point of view, the AP-only graph does not present the interesting proper-ties above discussed: since the AP-only graph is symmetri-cal (the weight on each incoming link is equal to the weight on the corresponding outgoing link), Inlinks and Outlinks assume the same values. There is symmetry also in comput-ing hub and authority, that assume the same value for each node since the weights on the incoming and outgoing arcs are the same. This could be stated in more precise and for-mal terms, but one might still wonder if on the overall graph there are some sort of counterbalancing effects. It is there-fore easier to look at experimental data, which confirm that the normalizations are needed: the correlations between AP, Inlinks, Outlinks, Hub, and/or Authority are all very close to one (none of them is below 0 . 98).
It might be argued that (in the case of AP A , for example) the amount we have subtracted from each AP value is topic-dependent, therefore the range of the resulting AP A value is also topic-dependent (e.g. the maximum is 1  X  AAP( t j and the minimum is  X  AAP( t j )). This suggests that the cross-topic comparisons of these values suggested in Sect. 3.3 may not be reliable. A similar issue arises for AP M and comparisons across systems.

One possible way to overcome this would be to use an unconstrained measure whose range is the full real line. Note that in applying the method to GMAP by using log AP, we avoid the problem with the lower limit but retain it for the upper limit. One way to achieve an unconstrainted range would be to use the logit function rather than the log [4,8]. We have also run this variant (as already reported in Sect. 5 above), and it appears to provide very similar re-sults to the GMAP results already given. This is not sur-prising, since in practice the two functions are very similar over most of the operative range. The normalizations thus seem reliable.
It is well known that h and a vectors are the principal left eigenvectors of AA T and A T A , respectively (this can be easily derived from Eqs. (7)), and that, in the case of ci-tation graphs, AA T and A T A represent, respectively, bib-liographic coupling and co-citations. What is the meaning, if any, of AA T and A T A in our Systems-Topics graph? It is easy to derive that:
AA T [ i, j ] =
A T A [ i, j ] = (where S is the set of indices corresponding to systems and T the set of indices corresponding to topics). Thus AA T and A
T A are block diagonal matrices, with two blocks each, one relative to systems and one relative to topics: (a) if i, j  X  S , then AA T [ i, j ] = P k  X  T AP M ( i, k )  X  AP (b) if i, j  X  T , then AA T [ i, j ] = P k  X  S AP A ( k, i )  X  AP (c) if i, j  X  S , then A T A [ i, j ] = P k  X  T AP A ( i, k )  X  AP (d) if i, j  X  T , then A T A [ i, j ] = P k  X  S AP M ( k, i )  X  AP
Therefore, these matrices are meaningful and somehow interesting. For instance, the submatrix (b) corresponds to a weighted undirected complete graph, whose nodes are the topics and whose arc weights are a measure of how much two topics agree on systems effectiveness. Two topics that are very close on this graph give the same information, and therefore one of them could be discarded without changes in TREC results. It would be interesting to cluster the topics on this graph. Furthermore, the matrix/graph (a) could be useful in TREC pool formation: systems that do not agree on topic ease would probably find different relevant docu-ments, and should therefore be complementary in pool for-mation. Note that no notion of single documents is involved in the above analysis.
As indicated, the primary contribution of this paper has been a method of analysis. However, in the course of ap-plying this method to one set of TREC results, we have achieved some insights relating to the hypotheses formulated in Sect. 2: Clearly these ideas need to be tested on other data sets. However, they reveal that the method of analysis proposed in this paper can provide valuable information.
The confirmation of Hypothesis 4 leads, as indicated, to the idea that we could do reliable system evaluation on a much smaller set of topics, provided we could select such an appropriate set. This selection may not be straightforward, however. It is possible that simply selecting the high hub-ness topics will achieve this end; however, it is also possible that there are significant interactions between topics which would render such a simple rule ineffective. This investiga-tion would therefore require serious experimentation. For this reason we have not attempted in this paper to point to the specific high hubness topics as being good for evaluation. This is left for future work.
The contribution of this paper is threefold:
More particularly, we propose Average Average Precision (AAP), a measure of topic ease, and a novel way of normal-izing the average precision measure in TREC, on the basis of both MAP (Mean Average Precision) and AAP. The normalized measures ( AP M and AP A ) are used to build a bipartite weighted Systems-Topics graph, that is then ana-lyzed by means of network analysis indicators widely known in the (social) network analysis field, but somewhat gen-eralised. We note that no such approach to TREC data analysis has been proposed so far. The analysis shows that, with current measures, a system that wants to be effective in TREC needs to be effective on easy topics. Also, it is sug-gested that a cluster analysis on topic similarity can lead to relying on a lower number of topics.

Our method of analysis, as described in this paper, can be applied only a posteriori , i.e., once we have all the top-ics and all the systems available. Adding (removing) a new system / topic would mean re-computing hubness and au-thority indicators. Moreover, we are not explicitly proposing a change to current TREC methodology, although this could be a by-product of these  X  and further  X  analyses.
This is an initial work, and further analyses could be per-formed. For instance, other effectiveness metrics could be used, in place of AP. Other centrality indicators, widely used in social network analysis, could be computed, although probably with similar results to PageRank. It would be in-teresting to compute the higher-order eigenvectors of A T and AA T . The same kind of analysis could be performed at the document level, measuring document ease. Hopefully, further analyses of the graph defined in this paper, accord-ing to the approach described, can be insightful for a better understanding of TREC or similar data.
 We would like to thank Nick Craswell for insightful discus-sions and the anonymous referees for useful remarks. Part of this research has been carried on while the first author was visiting Microsoft Research Cambridge, whose financial support is acknowledged. [1] M. Agosti, M. Bacchin, N. Ferro, and M. Melucci. [2] C. Buckley and E. Voorhees. Evaluating evaluation [3] S. Chakrabarti. Mining the Web . Morgan Kaufmann, [4] G. V. Cormack and T. R. Lynam. Statistical precision [5] J. Kleinberg. Authoritative sources in a hyperlinked [6] M. Levene. An Introduction to Search Engines and [7] L. Page, S. Brin, R. Motwani, and T. Winograd. The [8] S. Robertson. On GMAP  X  and other transformations. [9] M. Sanderson and J. Zobel. Information retrieval [10] I. Soboroff, C. Nicholas, and P. Cahan. Ranking [11] TREC Common Evaluation Measures, 2005. [12] Text REtrieval Conference (TREC). [13] E. Voorhees and C. Buckley. The effect of topic set [14] E. M. Voorhees. Overview of the TREC 2005 Robust [15] E. M. Voorhees and D. K. Harman. TREC  X  [16] S. Wasserman and K. Faust. Social Network Analysis .
