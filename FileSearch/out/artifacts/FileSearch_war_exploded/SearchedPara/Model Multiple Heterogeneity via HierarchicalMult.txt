 In many real world applications such as satellite image anal-ysis, gene function prediction, and insider threat detection, the data collected from heterogeneous sources often exhib-it multiple types of heterogeneity, such as task heterogene-ity, view heterogeneity, and label heterogeneity. To address this problem, we propose a Hierarchical Multi-Latent Space (HiMLS) learning approach to jointly model the triple types of heterogeneity. The basic idea is to learn a hierarchical multi-latent space by which we can simultaneously leverage the task relatedness, view consistency and the label correla-tions to improve the learning performance. We first propose a multi-latent space framework to model the complex het-erogeneity, which is used as a building block to stack up a multi-layer structure so as to learn the hierarchical multi-latent space. In such a way, we can gradually learn the more abstract concepts in the higher level. Then, a deep learn-ing algorithm is proposed to solve the optimization problem. The experimental results on various data sets show the ef-fectiveness of the proposed approach.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ;I.5.2[ Computing Methodologies ]: Pattern Recognition X  Design Methodology Theory, algorithm, performance, experiment Heterogeneous learning; multi-task learning; multi-view learn-ing; multi-label learning
In the era of big data, a large amount of information col-lected from heterogeneous sources are correlated with each other. It is of great importance to mine such hidden corre-lations in the presence of multiple types of heterogeneity for many real world applications, such as satellite image anal-ysis, gene function prediction, and insider threat detection. In this paper, we focus on triple types of heterogeneity, i.e., task heterogeneity, view heterogeneity, and label heterogene-ity. For example, for the satellite image analysis problems, task heterogeneity refers to the images collected from dif-ferent satellites following from different distributions; view heterogeneity refers to various types of features such as col-or histogram, edge distribution histogram, and bag of visual words; label heterogeneity refers to the multiple labels asso-ciated with each image such as sea, plane, and yacht.
The major challenge for learning with the triple types of heterogeneity is how to effectively mine the hidden corre-lations among the heterogeneous data. Such correlations should reflect the key assumptions underlying each type of heterogeneity, including the task relatedness assumption [4], the view consistency assumption [9], as well as the label cor-relation assumption [23]. To the best of our knowledge, we are the first to jointly model the triple types of heterogene-ity including the task heterogeneity, view heterogeneity and label heterogeneity.
 To address this problem, we propose a Hierarchical Multi-Latent Space (HiMLS) learning approach to jointly model the triple types of heterogeneity. The basic idea is to learn a hierarchical multi-latent space by which we can accommo-date multiple types of relationship among the instances, fea-tures and labels including the task relatedness, view consis-tency and the label correlations. We first present the multi-latent space framework to model the complex heterogeneity which is formulated as a regularized non-negative matrix triple factorization problem. It aims to minimize the recon-struction loss on the instance-feature data, classification loss on the instance-label data, together with the regularization term. Furthermore, the multi-latent space model is used as a building block to stack up a multi-layer structure so as to learn the hierarchical multi-latent space. In such a way, we can gradually learn the more abstract concepts in a higher layer. Finally, we will introduce an iterative updating al-gorithm to solve the resulting optimization problem. The proposed algorithm consists of two phases. First, each layer is pre-trained in a greedy layer-wise way. Then, it fine-tunes the weights of all the layer to reduce the total reconstruc-tion loss and the classification loss. It is worth noting that the proposed model is a generalized framework to learn from complex heterogeneity. For example, some popular methods on learning from a single heterogeneity can be viewed as the special cases of the proposed approach.

The main contributions of this paper can be summarized as follows:
The rest of the paper is organized as follows. After a brief review of the related work in Section 2, we present the proposed model and the optimization algorithm in Sec-tion 3, followed by the discussion of some special cases of the proposed approach in Section 4. Section 5 shows the experimental results. Finally, we conclude in Section 6.
In this section, we review the related work on modeling a single or dual types of heterogeneity.

In multi-view learning, the features from multiple sources form natural partitions (views). Co-Training [3] is one of the earliest algorithms for multi-view learning. More re-cent work includes: SVM-2K [9] which combined KCCA with SVM in an optimization framework; the information-theoretic framework for multi-view learning [20]; the CoMR method [19] based on a data-dependent Reproducing Ker-nel Hilbert Space (RKHS); the large-margin framework for multi-view data based on a latent space Markov network [5]; the MSL [24] model which is a convex multi-view subspace learning method that enforced conditional independence a-mong the multiple views while reducing dimensionality.
In multi-task learning, the goal is to leverage the small amount of labeled data from multiple related tasks to im-prove the learner for each task. Among others, alternat-ing structure optimization [1] decomposed the model into the task-specific and task-shared feature mapping; multi-task feature learning [2] assumed that multiple related tasks share a low-dimensional representation; clustered multi-task learning [34] assumed that multiple tasks follow a clustered structure. Some recent multi-task learning methods are able to deal with irrelevant tasks by assuming that the model can be decomposed into a shared feature structure that captures task relatedness, and a group-sparse structure that detects outliers [10].

In multi-label learning, each instance is associated with a set of labels [23, 33]. The key issue for multi-label learning is how to exploit correlations or dependencies among multi-ple labels. To name a few, first-order method such as ML-kNN [32] assumed that labels are independent and trans-formed the multi-label learning into a number of indepen-dent binary classification problems; second-order approach such as Rank-SVM [8] transformed the multi-label learn-ing into the label ranking problem; LEAD [31] employed Bayesian network to encode the conditional dependencies of the labels; subspace learning approach LS-ML [14] assumed that a common subspace is shared among multiple labels; graph-based method HG [21] constructed a hypergraph to exploit the correlation information among different labels; transductive approach TRAM [15] leveraged the information from unlabeled data to estimate the optimal label concept compositions; MLLOC [13] assumed that the label correla-tion may be shared by only a subset of instances rather than all the instances; boosting based method MAHR [12] aimed to discover the label relationship by using a hypothesis reuse mechanism; a generic empirical risk minimization framework was proposed for large-scale multi-label learning [28].
More recently, researchers begin to study problems with dual types of heterogeneity. For problems with both task and view heterogeneity, a variety of techniques have been proposed to model task relatedness in the presence of mul-tiple views, e.g., [11, 30, 25, 26]. For the problems with both label and view heterogeneity, the L 2 F method pro-posed in [27] modeled both the view consistency and the la-bel correlations in a graph-based framework. For the more complex setting with all three types of heterogeneity, these techniques cannot be readily applied without disregarding the useful information from a certain type of heterogeneity.
The basic idea of the proposed HiMLS approach is to learn a hierarchical multi-latent space by which we can take ad-vantage of multiple types of heterogeneity to improve the performance of the learning system. We first present the multi-latent space framework to model the complex hetero-geneity, which is then used as a building block to stack up a multi-layer structure so as to learn the hierarchical multi-latent space. Finally, we will introduce an iterative updating algorithm to solve the resulting optimization problem.
Before going into the details, the problem statement and the notations will be introduced. Suppose we are given the multi-label data with multiple views in different tasks. Let T be the number of tasks, V thenumberofviews, m the number of labels, respectively. Let X be an instance space, L = {L 1 , L 2 ,  X  X  X  , L m } a finite set of class labels. An instance x  X  X  is described from V views,andassociatedwithasub-set of labels L ( x )  X  2 L , which is the set of relevant labels of x . In practice, the relevant labels L ( x ) can be denoted by a binary label vector y ( x )=[ y 1 ( x ) ,y 2 ( x ) , where y k ( x )=1(1  X  k  X  m )if L k  X  X  ( x ), and 0 oth-erwise. For the i th task and j th view, denote the number of instances and features by n i and d j , respectively. Let  X  X i th task and j th view, where X ij is the training data and X is the test data. Let  X  Y i = Y i Y u label matrix for the i th task, where Y i is for the training data and Y u i is for the test data. The goal is to build a multi-label classifier f ( x )=[ f 1 ( x ) ,f 2 ( x ) ,  X  X  X  f ( x )  X  X  1 , 0 } (1  X  k  X  m ) by using the data given in multi-ple tasks and described with multiple views.
In order to leverage the triple types of heterogeneity, we propose a multi-latent space learning framework to joint-ly model the task relatedness, view consistency, and label correlations in a principled way.
Intuitively, we can do multi-way clustering on the hetero-geneous data which simultaneously clusters the instances, features and labels into the their corresponding clusters, re-spectively. Let  X  R i = R i R u cluster matrix where p is the dimensionality of instance la-tent space, R i and R u i are for training and test data, re-spectively. Let C j  X  X  d j  X  q be the feature-to-cluster matrix, C
Y  X  X  m  X  q the label-to-cluster matrix where q is the di-mensionality of feature (or label) latent space. Each row in  X  R i (or C j , C Y ) represents the coeffecients of the instance (or feature, label) associated with the instance (or feature, label) clusters. Denote M ij  X  X  p  X  q ,M iY  X  X  p  X  q as the co-latent space matrices. Note that M ij models the correlations between the instance clusters and the feature clusters, while M iY models the correlations between the instance clusters and the label clusters.

In the proposed model, we aim to learn the multi-latent s-paces from the instances and labels of multiple tasks, and the features with multiple views. The multi-latent space mod-el is formulated as a regularized non-negative matrix triple factorization problem, which simultaneously decomposes the instance-feature and instance-label matrices, while enforcing the task relatedness, view consistency, and label correlations on the data. The objective is to minimize the reconstruction loss on the instance-feature data, classification loss on the instance-label data, together with the regularization term: where  X  and  X  are the non-negative coefficients to control the importance of classification loss and regularization, re-spectively.  X  ( R, M, C ) is the regularization of R , C and M .

Various regularization techniques can be used to encode our prior knowledge about the task relatedness, view con-sistency, and the label correlations. Since the instances, fea-tures, and labels may share the latent semantic concepts, we hope the learned co-latent spaces are similar to each other. In specific, we hope that the co-latent spaces learned in the feature spaces from multiple views are as similar as possible to the ones learned from label spaces, which acts as a bridge to link the labels with the features from multiple views in the latent spaces. Therefore, Eq. 1 can be instantiated as,  X 
The non-negative constraints R&gt; 0and C&gt; 0 allow for the multi-way clustering interpretation. The multi-latent space model can be interpreted from the perspective of con-strained multi-way clustering. By constraining the multi-way clustering procedures, we model the task relatedness by requiring that the features across different tasks share the same feature clustering coefficients, enhance the view consistency by requiring that the instances share the same instance clustering coefficients across different views, mod-el the label correlations by requiring that the labels share the same label clustering coefficients across different tasks. Specifically, Eq. 2 encodes multiple types of correlations a-mong the heterogeneous data as follows:
The multi-latent space model can be used as a building block to stack up a multi-layer architecture so as to learn the hierarchical multi-latent space from complex heterogeneity. Takethetextdataasanexample. Ineachlayer,wedomulti-way clustering on the documents, features and labels. Since all of the documents, features and labels may have hierar-chical structures, they can be clustered into sub-categories, and further into high-level sub-categories, until the top cat-egories. In such a way, we can gradually learn the more abstract concepts in a higher layer.

The co-latent spaces M ij and M iY can be viewed as the compact representations for the original input data  X  X ij Y . Based on the co-latent space M ( l  X  1) ij (or M ( l  X  1) l represents the layer, we hope to further learn its own co-latent space M ( l ) ij (or M ( l ) iY ) in a higher level, i.e., Suppose L is the number of layers. Similar to Eq. 2, we can obtain the objective function for the l th (2  X  l  X  L )layer:
Based on the learned co-latent spaces M ( L ) ij and M ( L the highest layer L , we hope to recover the original input data,  X  X ij and Y i , in the first layer as accurately as possi-ble. Thus, the objective for the multi-layer architecture is as follows: any matrix A .

Prediction: The final prediction is the weighted sum of predictions resulting from each layer. For i th (1  X  i  X  T ) task, the predicted instance-label matrix is as follows: where w l controls the weight for l th layer.
Following the tactics successfully used in deep learning, we adopt a two-phase procedure to train the multi-layer mod-el. We first pre-train the weights of each layer in a greedy layer-wise manner, then fine-tune the weights of all layers to reduce the total reconstruction loss and classification loss.
In specific, we pre-train the first layer by using Eq. 2, and the following l th (2  X  l  X  L ) layer by using Eq. 3. Then, at the fine-tuning phase, we update the weights for all the layers by using Eq. 4.

To derive the multiplicative update rules for pre-training and fine-tuning, we first derive Lemma 3.1. The proof of Lemma 3.1 is a key step to prove Theorem 3.1 and Theo-rem 3.2.
 Lemma 3.1. The objective function,
J ( M )=  X  is non-increasing under the updating rule:
Proof. We make use of auxiliary function approach [16] to derive the updating rules and prove its convergence and correctness.

Similar to [7], we can derive the updating rule for M .The objective function for M is as follows:
J ( M )=  X  =  X tr Let t be the number of iteration. Next we will show that is an auxiliary function of J ( M ) due to the facts: and The minimum is obtained by setting the derivative to zero: Then,wegettheupdateruleasfollows: Since J M ( t ) = G M ( t ) ,M ( t )  X  min G M ( t +1) ,M ( t )  X  J M ( t +1) , the objective function J ( M ) is non-increasing under the above update rule.

The following Theorem 3.1 shows the multiplicative up-date rules for Eq. 2, and demonstrates its convergence and correctness. The update rules for Eq. 3 can be obtained in a similar way.

Theorem 3.1 (Convergence of Pre-training). The objective function in Eq. 2 is non-increasing under the up-dating rules: Proof. Please see Appendix A.
  X   X 
Theorem 3.2 shows the multiplicative update rules for the objective function Eq. 4, and demonstrates its convergence and correctness.
Theorem 3.2 (Convergence of Fine-tuning). The objective function in Eq. 4 is non-increasing under the up-dating rules: Proof. Please see Appendix B.

After obtaining R ( l ) i and C ( l ) j , we can update M M training phase.

Based on Theorem 3.1 and Theorem 3.2, we summarize the optimization algorithm for HiMLS in Algorithm 1. There are two training phases including pre-training and fine-tuning in Algorithm 1. As shown in Steps 1-9, the pre-training phase goes forward from the first layer to the highest layer, and each layer is trained in a greedy layer-wise manner. In contrast, the fine-tuning phase shown in Steps 10-17 moves in an opposite direction, and the weights of all the layers will be updated. The convergence of the HiMLS algorithm is guaranteed by Theorem 3.1 and Theorem 3.2.

Time complexity: Similar to other matrix factoriza-tion methods based on multiplicative update rules [16, 7], a nice property of the proposed HiMLS algorithm is that most of the computations are matrix multiplications and can be computed efficiently. Theorem 3.3 shows the complexity of the algorithm. The proof is omitted for brevity.

Theorem 3.3 (Complexity). The time complexity for the multiplicative update rules in Theorem 3.1 are as follows:
O ( R i )= O ( R u Algorithm 1 HiMLS Algorithm Input: Instance-feature matrices Output: Predicted instance-label matrices F i (1  X  i  X  T ) 1: for l =1: L do 2: Initialize  X  R ( l ) i (1  X  i  X  T ), C ( l ) Y and C 4: repeat 5: Update  X  R ( l ) i (1  X  i  X  T ) by Eq. 7 and Eq. 8; 6: Update C ( l ) j (1  X  j  X  V )and C ( l ) Y by Eq. 9 and E-7: Update M ( l ) ij and M ( l ) iY where 1  X  i  X  T, 1  X  j 8: until converged 9: end for ; 10: repeat 11: Update M ( L ) ij and M ( L ) iY where 1  X  i  X  T, 1 12: for l = L :1 do 13: Update R ( l ) i (1  X  i  X  T )and R u i by Eq. 13 and E-14: Update C ( l ) j (1  X  j  X  V )and C ( l ) Y by Eq. 15 and E-15: Update M ( l ) ij and M ( l ) iY where 1  X  i  X  T, 1  X  16: end for ; 17: until converged 18: return Predictions for the test data using Eq. 5. where 1  X  i  X  T, 1  X  j  X  V and N is the number of iteration until convergence.

Note that the dimensionalities of the latent spaces are usually far smaller than the ones in the original spaces, i.e., p n i and q d j . Theorem 3.3 shows that the mul-tiplicative update rules for pre-training are scalable to the problem sizes. Likewise, we can obtain the time complexity of the update rules for fine-tuning, which are omitted due to space limit.
The proposed model is a generalized framework for learn-ing complex heterogeneity. It is widely applicable to multi-ple types of heterogeneous learning problems.

A special case of HiMLS is to learn the common co-latent space M shared among all the tasks, view, and labels, i.e., M ij = M iY = M (1  X  i  X  T, 1  X  j  X  V ). And by using the training data only, Eq. 2 can be specialized as: It is worth noting that Eq. 19 is not a trivial special case. Theorem 4.1 shows that some popular methods for learn-ing from single heterogeneity can be viewed as the special cases of our proposed model, such as the multi-view learn-ing method MSL [24] and the multi-label learning method LS-CCA [22]. Both MSL and LS-CCA are closely related to canonical correlation analysis (CCA), while MSL is a un-supervised learning method aiming to learn the subspace from multiple views, and LS-CCA is a supervised learning method for the multi-label problem when one of the views used in CCA is derived from the labels.

Theorem 4.1. The multi-view learning method MSL [24] and the multi-label learning method LS-CCA [22] can be viewed as the special cases of HiMLS.

Proof. Consider two special cases of HiMLS for learning from a single heterogeneity as follows: 1) Unsupervised multi-view learning: By letting T =1, V =2,and  X  =0,Eq.19canberewritteninto: where X j ( j =1 , 2) is the instance-feature matrix for the j view. 2) Supervised multi-label learning: By letting T =1, V = 1, and  X  =1,Eq.19canberewritteninto: where X and Y are the instance-feature matrix and instance-label matrix, respectively.

Mathematically speaking, both Eq. 20 and Eq. 21 have the same form as follows: where H = RM and C T = C T 1 ,C T 2 . Consider the normal-ized data matrix defined as Z = ( XX T )  X  1 2 X, ( YY T ) When imposing the orthogonal constraint C T C = I ,Eq.22 can be rewritten into: Let f ( H, C ) denote the objective function for Eq. 23, which can be transformed into: When fixing C ,wehave: By substituting H = ZC into Eq. 23, we have The optimal solution for C is given by the top k eigenvectors of Z T Z . According to [24], Eq. 24 has the same optimal solution with CCA which aims to optimize: max Therefore, the first special case of HiMLS is equivalent to ap-plying CCA to the instance-feature matrices from multiple views, which is equivalent to MSL [24]. The second special case of HiMLS is equivalent to applying CCA to both the instance-feature matrix and instance-label matrix, which is equivalent to LS-CCA [22].
In this section, we verify the effectiveness of the proposed algorithm on various data sets in comparison with the state-of-the-art techniques.
Three online benchmark data sets 1 including two text data sets and one image data set are used for evaluation. The first data set is the Reuters Corpus Volume I (R-CV1V2) data set [17], which is a collection of over 800,000 newswire stories. There are three category sets of data: Top-ics (i.e. major subject of a story), Industry Codes (i.e. type of business discussed), and Regions (i. e. geographic loca-tions). Each of these category sets has a hierarchical struc-tures. It is usually common to use several subsets of this data, each containing 6000 data instances on average and with a total number of 101 class labels.

EUR-Lex [18] is a text data set containing European U-nion official laws in practice, different kinds of treaties and agreements, parliamentary journals. This data set contain-s nearly 20,000 text documents classified according to three different schemas: i) subject matter (e.g. agriculture), ii) of-ficial classification hierarchy called the directory codes (e.g. a document belonging to a class also belongs to all its par-ent classes), and iii) EUROVOC, a multilingual thesaurus maintained by the Office for Official Publications of the Eu-ropean Communities. Each of these category sets forms a hierarchical structures.

NUS-WIDE 2 [6] is the a real-world web image data set comprising over 269,000 images with over 5,000 user-provided tags, and ground-truth of 81 concepts with a hierarchical structures. There are several types of low-level visual fea-tures such as 64-D color histogram in LAB color space, 144-D color correlogram in HSV color space, 73-D edge distribu-tion histogram, and 500-D bag of visual words. We use the light version of NUS-WIDE.

Table 1 shows the properties of different data sets. La-bel cardinality is the average number of labels per instance. Accordingly, label density normalizes label cardinality by the the number of labels. Label diversity is the number of distinct label combinations observed in the data set [33].
In these data sets, the label refer to the multiple categories each instance belonging to. For the NUS-WIDE data, the view refers to different types of low-level visual feature. For either RCV1V2 or EUR-Lex data sets, similar to [29], the data are described from two views: one corresponds to the TF-IDF features; another corresponds to the latent topics obtained by applying probabilistic latent semantic analysis on the term counts. The task refers to classify the instances belonging to different sub-categories, which follow different but related distributions [11]. http://mulan.sourceforge.net/datasets-mlc.html http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm http://lear.inrialpes.fr/people/verbeek/code Table 2: Comparison among HiMLS Variants on RCV1V2 1. Table 3: Comparison among HiMLS Variants on RCV1V2 2.
In order to comprehensively investigate the performance of the proposed method, we use F 1 -score, accuracy and Hamming loss on the test data as the evaluation metrics.
F 1 -score [33] is the harmonic mean of precision and recall where precision is the proportion of predicted correct labels to the total number of actual labels, recall is the proportion of predicted correct labels to the total number of predicted labels, averaged over all instances. Note that the larger value of F 1 -score is indicating the better performance.
Accuracy [33] for each instance is defined as the propor-tion of the predicted correct labels to the total number of labels for that instance. Overall accuracy is the average across all instances. Note that the larger value of accuracy is indicating the better performance.

Hamming Loss [33] reports how many times on average, the relevance of an instance to a class label is incorrectly predicted. Therefore, hamming loss takes into account the prediction error (an incorrect label is predicted) and the missing error (a relevant label not predicted), normalized over total number of classes and total number of instances. Note that the smaller the value of Hamming loss, the better the performance of the learning algorithm.
In order to demonstrate the effectiveness of simultane-ously modeling the multiple heterogeneity in a multi-layer framework, we compare HiMLS with its four variants with only one layer: 1) multi-task multi-view variant MLS; 2) multi-task single-view variant MLS-T; 3) multi-view single-task variant MLS-V; 4) single-task single-view variant MLS-S.
 Table 4: Comparison among HiMLS Variants on RCV1V2 3. Table 5: Comparison among HiMLS Variants on RCV1V2 4.
HiMLS and MLS are input with multi-task and multi-view data. For the single-view setting, the features from all the views are concatenated into one single view. For the single-task setting, the instances in all the tasks are pooled into onesingletask.ForHiMLS,weempiricallysetthenumber of layers L = 2, and the numbers of latent topics [ p, q ]for the instances and features(or labels) to [200 , 100], [40 , 20] in the first and second layer, respectively. For all the other methods with only one layer, we set [ p, q ]=[40 , 20].
The classification performances of HiMLS and its variants on RCV1V2 data sets are shown on Tables 2-5. Based on these comparison results, we have the following findings:
In this work, we focus on improving the performance of multi-label learning by leveraging the multiple type of het-erogeneity. To the best of our knowledge, there is no previ-ous work for learning from the triple heterogeneity. There-fore, we compare HiMLS with a variety of multi-label learn-ing methods which learn from single or dual heterogeneity. The comparison approaches includes: 1) multi-view multi-label learning methods L 2 F [27]; 2) graph-based multi-label approach ML-kNN [32]; 3) multi-label method based on sub-space learning LS-ML [14]; 4) transductive multi-label learn-ing approach TRAM [15].

HiMLS is input with multi-task and multi-view data. For the other algorithms, the instances of all the tasks are pooled together. L 2 F method is given the multi-view features, whereas the other methods are given the concatenated fea-tures from all the views. The parameters are tuned for each algorithm using cross-validation on the training data. We repeat the experiments ten times for each data set and re-port the average performances and the standard deviations.
Tables 6-9 show the classification performances of different methods on RCV1V2. The performances on EUR-Lex and NUS-WIDE are shown in Tables 10-11, respectively.
From these results, we can see that HiMLS performs bet-ter than the other algorithms in most cases. For ML-kNN [32], since it ignores the correlation among multiple labels, its performance on these data sets is not comparable with the other methods. In contrast, LS-ML [14] learns a common subspace shared among multiple labels, which helps improve the learning performance for the multi-label data. However, since its objective function is non-convex, the performance of LS-ML may be limited by the local optimum problem. TRAM [15] is a tranductive multi-label learning method which tries to exploit the information from unlabeled data to estimate the optimal label concept compositions. The re-sults show that unlabeled data can provide helpful informa-tion to build the multi-label classifier. Different from these methods for learning from single heterogeneity, both HiML-Sand L 2 F [27] model the feature and label heterogeneity and gain performance improvement by enhancing the view consistency. It suggests that treating the features from dif-ferent views in a discriminative and complementary way is usually better than just concatenating all the features into one view. Likewise, treating the instances in different tasks discriminatively is usually better than just pooling all the instances together. The performance superiority of HiMLS over the comparison methods verifies the effectiveness of the proposed approach to model the complex heterogeneity in a principled framework. Another important competency of HiMLS is that its multi-layer structure helps build a robust classifier by gradually finding the more high-level concepts in the deep structures.
 TRAM performs a little better than HiMLS on NUS-WIDE data set. It indicates that NUS-WIDE may be con-sistent with the smoothness assumption, and TRAM is able to effectively leverage this assumption.
We study the parameter sensitivity on the RCV1V2 1da-ta set.  X  and  X  are tuned on the grid 10 [  X  3:1:3] . The results are shown in Figure 1(a-b).  X  is used to balance the impor-tance of classification loss. The algorithm performs worse as  X  approaches 0. When  X  = 0, it means that no label information is used for training. The optimal performance is achieved at  X  = 1. Nevertheless, the performance is quite robust over a wide range of values of  X  .  X  is used to con-trol the importance of regularization. The result shown in Figure 1(b) indicates that setting appropriate weight to the regularization term can lead to better performance. As a result, we tune the parameters,  X  and  X  , for each data set by cross-validation on the training data.

Here, we empirically study the convergence of HiMLS on the RCV1V2 1 data set. The result is shown in Figure 1(c). From this figure, we can see that HiMLS converges fast and its performance becomes stable after 50 iterations.
F1 X  X core
In this paper, we propose a multi-layer framework to joint-ly model triple heterogeneity. In each layer, we learn a multi-latent space shared among the instances and labels from multiple tasks, and features from multiple views. Then the multi-latent model is used as a building block to stack up a multi-layer structure so as to gradually learn the more ab-stract concepts. A deep learning algorithm is proposed to solve the optimization problem, which first pre-trains each layer and then fine-tunes the whole multi-layer structure by using the multiplicative update rules. The comparison ex-periments with state-of-the-art methods demonstrate the ef-fectiveness of the proposed model.

Acknowledgment This work is partially supported by the NSF (No. IIS1017415), the Army Research Laboratory (No. W911NF-09-2-0053), Region II University Transporta-tion Center (No. 49997-33 25), DARPA (No. W911NF-11-C-0200 and W911NF-12-C-0028), and NSFC (No. 61473123). [1] R. K. Ando and T. Zhang. A framework for learning [2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task [3] A. Blum and T. Mitchell. Combining labeled and [4] R. Caruana. Multitask learning. Machine Learning , [5] N. Chen, J. Zhu, and E. P. Xing. Predictive subspace [6] T.Chua,J.Tang,R.Hong,H.Li,Z.Luo,and [7] C.H.Q.Ding,T.Li,W.Peng,andH.Park.
 [8] A. Elisseeff and J. Weston. A kernel method for [9] J. D. R. Farquhar, D. R. Hardoon, H. Meng, [10] P. Gong, J. Ye, and C. Zhang. Robust multi-task [11] J. He and R. Lawrence. A graph-based framework for [12] S.-J. Huang, Y. Yu, and Z.-H. Zhou. Multi-label [13] S.-J. Huang and Z.-H. Zhou. Multi-label learning by [14] S. Ji, L. Tang, S. Yu, and J. Ye. Extracting shared [15] X. Kong, M. K. Ng, and Z.-H. Zhou. Transductive [16] D. D. Lee and H. S. Seung. Algorithms for [17] D.D.Lewis,Y.Yang,T.G.Rose,andF.Li.Rcv1:A [18] E. L. Menc  X  X a and J. F  X  urnkranz. Efficient pairwise [19] V. Sindhwani and D. S. Rosenberg. An rkhs for [20] K. Sridharan and S. M. Kakade. An information [21] L. Sun, S. Ji, and J. Ye. Hypergraph spectral learning [22] L. Sun, S. Ji, and J. Ye. Canonical correlation analysis [23] G. Tsoumakas and I. Katakis. Multi-label [24] M. White, Y. Yu, X. Zhang, and D. Schuurmans. [25] H. Yang and J. He. Learning with dual heterogeneity: [26] P. Yang, J. He, and J.-Y. Pan. Learning complex rare [27] P. Yang, J. He, H. Yang, and H. Fu. Learning from [28] H.-F. Yu, P. Jain, P. Kar, and I. S. Dhillon. [29] D. Zhang, J. He, and R. D. Lawrence. Mi2ls: [30] J. Zhang and J. Huan. Inductive multi-task learning [31] M.-L. Zhang and K. Zhang. Multi-label learning by [32] M.-L. Zhang and Z.-H. Zhou. Ml-knn: A lazy learning [33] M.-L. Zhang and Z.-H. Zhou. A review on multi-label [34] J. Zhou, J. Chen, and J. Ye. Clustered multi-task
Proof. Based on Lemma 3.1, we can derive the updat-ing rules for the objective function in Eq. 2 and prove the convergence and correctness. The detailed deduction is as follows: 1) Update rule for R i : The objective function for R i is as follows: J ( R i )= Based on Lemma 3.1, we can derive the updating rules for R i and R u i as in Eq. 7 and Eq. 8, respectively. 2) Update rule for C j : The objective function for C j is as follows: Based on Lemma 3.1, we can derive the updating rule for C j as Eq. 9. 3) Update rule for C Y : The objective function for C Y is as follows: Based on Lemma 3.1, we can derive the updating rule for C
Y as in Eq. 10. 4) Update rule for M ij : The objective function for M ij as follows: Based on Lemma 3.1, we can derive the updating rule for M ij as in Eq. 11. 5) Update rule for M iY : The objective function for M iY is as follows: Based on Lemma 3.1, we can derive the updating rule for M iY as in Eq. 12.

Proof. Based on Lemma 3.1, we can derive the updat-ing rules for the objective function in Eq. 4 and prove the convergence and correctness. The detailed deduction is as follows: 1) Update rule for R ( l ) i : The objective function for R as follows: Based on Lemma 3.1, we can derive the updating rule for R i and R 2) Update rule for C ( l ) j : The objective function for C as follows: Based on Lemma 3.1, we can derive the updating rule for C j as in Eq. 15. 3) Update rule for C ( l ) Y : The objective function for C as follows: Based on Lemma 3.1, we can derive the updating rule for C j as in Eq. 16. 4) Update rule for M ( L ) ij : The objective function for M is as follows: Based on Lemma 3.1, we can derive the updating rule for M ij as in Eq. 17. 5) Update rule for M ( L ) iY : The objective function for M is as follows: Based on Lemma 3.1, we can derive the updating rule for M iY as in Eq. 18.
