 Joint models of syntactic and semantic parsing are attractive; they can potentially avoid the error propagation that is inherent in pipelines by using semantic models to inform syntactic attachments. However, in practice, the performance of joint sys-tems for semantic role labelling (SRL) has been substantially beneath that of pipelines (Sutton and McCallum, 2005; Llu X s et al., 2009; Johansson, 2009; Titov et al., 2009; Naradowsky et al., 2012; Llu X s et al., 2013; Henderson et al., 2013). In this paper, we present the first approach to break this trend, by building on the close relationship of syntax and semantics in CCG grammars to enable both (1) a simple but highly effective joint model
Semantic dependencies can span an unbounded number of syntactic dependencies, causing signif-icant inference and sparsity challenges for joint
He refused to confirm or deny the reports Figure 1: Mismatch between syntactic and se-mantic dependencies.
 Figure 2: Dependencies produced by a CCG parse. SRL dependencies can be recovered by la-belling the edges with a semantic role or  X  . Figure 3 shows a CCG derivation for these dependencies. models. For example, in the Figure 1, the se-mantic dependency between He and deny spans three syntactic edges. This fact makes it difficult to jointly parse syntactic and semantic dependen-cies with dynamic programs, and means that de-pendency path features can be sparse. Syntactic dependencies also often have ambiguous seman-tic interpretations X  X or example in He opened the door and The door opened , the syntactic subject corresponds to different semantic roles.

We address these challenges with a new joint model of CCG syntactic parsing and semantic role labelling. The CCG formalism is particu-larly well suited; it models both short-and long-range syntactic dependencies which correspond directly to the semantic roles we aim to recover. The joint model simply involves labelling a subset of these dependencies with the appropriate roles, as seen in Figure 2. This labelling decision can be easily integrated into existing parsing algo-rithms. CCG also helps resolve cases where in-terpretation depends on the valency of the pred-icate, such as ergative verbs, by learning lexical entries that pair syntactic arguments with seman-tic roles, such as open : S \ NP ARG1 and open : ( S \ NP ARG0 ) / NP ARG1 . Figure 3 shows a de-tailed trace of how the example from Figure 2 is parsed with our model.
 model. Because CCG is strongly lexicalized, we are able to introduce a new type of extended lexi-cal entries that allows us to factor the model over words and develop effective new upper bounds on rithms have previously been developed for models with tree-structured syntactic dependencies (Klein and Manning, 2003; Auli and Lopez, 2011b), and models with no bi-lexical dependencies, includ-ing supertag-factored CCGs (Lewis and Steed-man, 2014a). We generalize these techniques to SRL-style graph-structured dependencies.

Experiments demonstrate that our model not only outperforms pipeline semantic role labelling models, but improves the quality of the syntactic parser. PropBank SRL performance is 1.6 points higher than comparable existing work, and se-mantic features improve syntactic accuracy by 1.6 CKY parsing, with no loss in accuracy. The com-coding gives an efficient, accurate, and linguisti-2.1 CCG Dependencies CCG parses define an implicit dependency graph, by associating each argument of each category with a dependency. In contrast to Stanford de-pendency trees, CCG dependency graphs can be non-projective, and words can have multiple par-ents. For example, in Figure 2, He is an argument of refuse , confirm and deny .

To create dependencies, categories are marked with headedness information X  X hich we denote with subscripts. For example, in the category ( S deny \ NP x ) / NP y , the final head for the sentence S will be deny , and that two dependencies will be introduced from deny to unspecified arguments x and y . During parsing, variables are unified with heads, creating fully specified dependencies.
Re-using variables allows dependencies to propagate. For example, the determiner category NP i / N i marks that the head of the resulting NP is equal to the head of its N argument (e.g. the head of the report is report ), as in the following parse:
The same mechanism allows long-range argu-ments to be propagated. Figure 3 shows several long-range arguments, such as how co-indexation propagates the subject of deny to he . For more de-tails, see Hockenmaier (2003). out building a complete chart (in contrast to CKY parsing). This is particularly attractive for CCG, because the formalism X  X  lexical and derivational ambiguity causes the parse charts to be very dense, in practice. Lewis and Steedman (2014a) showed used a restricted model without bi-lexical features.
In A  X  parsing, entries y are added to the chart in order of their cost f ( y ) = g ( y ) + h ( y ) , where g ( y ) is the inside score for the entry, and h ( y ) is an upper bound on the Viterbi outside score. Be-cause partial parses are built in order of increasing f ( y ) , the first complete parse added to the chart is parsing efficient is computing tight upper bounds on the outside score. In Lewis and Steedman X  X  supertag-factored model, the bound can be com-puted as the sum of the highest-scoring supertags in the outside parse.

The agenda is initialized with items represent-ing every category for every word. Then, after each item is added to the chart, the agenda is up-dated with all binary and unary rules that can be applied to the new item. For more details, see Lewis and Steedman (2014a). Our joint model of CCG and SRL parsing sim-ply involves labelling CCG syntactic dependen-cies (which are implicit in CCG parses), with SRL roles (or null). This formulation allows us to easily adapt the log-linear CCG parsing model of Clark and Curran (2007) to the joint setting by working with extended dependencies that including syntac-tic and semantic information.

More formally, we can define a notion of con-sistency to specify the labelling of syntactic de-pendencies. A set of semantic dependencies  X  is consistent with a CCG derivation d if each se-mantic dependency corresponds to a single syntac-tic dependency X  X or example, see the dependen-cies in Figure 3. The CCG derivation in Figure 3 would also be consistent with the SRL dependency refuse ARG 2  X  X  X  X  X  X  X  he , but not refuse ARG 1  X  X  X  X  X  X  X  reports (because the derivation does not produce the re-quired syntactic dependency).

Now, we can define a log-linear model over pairs of consistent CCG derivations d and SRL de-pendencies  X  : where GEN ( x ) is the space of consistent pairs for sentence x . 3.1 Dependencies Because we enforce consistency, we can work with joint dependencies that are a combination of CCG and SRL dependencies. We denote a depen-dency as a 6-tuple of functor f , lexical category c , argument number n , preposition p , argument a and semantic role r :  X  f,c,n,p,a,r  X  . The first three of these ( f , c , n ) are lexically specified, but a and r are lexically underspecified until the attachment is determined by the derivation, and a label is cho-sen by the model. For example, in Figure 3, the lexical entry for deny has two underspecified de-pendencies:  X  deny, ( S \ NP ) / NP , 1 ,  X  , ? , ?  X  and  X  deny, ( S \ NP ) / NP , 2 ,  X  , ? , ?  X  . 2 At the end of the derivation, the dependencies are specified as:  X  deny, ( S \ NP ) / NP , 1 ,  X  ,he,ARG 0  X  and  X  deny, ( S \ NP ) / NP , 2 ,  X  ,reports,ARG 1  X  . The preposition p is lexically underspecified for PP arguments, but otherwise  X  . Prepositions are marked lexically on PP /  X  categories, and then propagated, for example: In the above example, the dependency from fly to Lisbon corresponds to the tuple:  X  fly, ( S \ NP ) / PP , 2 ,to,Lisbon,ARG 1  X  . Propagating both the noun and preposition from prepositional phrases allows the model to use both for features, and improved results. 3.2 Features We use the following feature classes, which de-compose over categories, dependencies, and local rule instantiations. 3.2.1 Supertagging Features Supertagging features  X  CAT score categories for words. A single feature is used, which is the (unnormalized) score from Lewis and Steedman (2014b) X  X  supertagging model. The supertagger outputs a distribution over categories for each word independently. The model is trained on su-pertags extracted from an adaptation of CCGre-bank (Honnibal et al., 2010). The adaptation makes minor changes to better match PropBank. 3.2.2 Preposition Features Preposition features  X  PP score whether the n th argument of the word at index f with category c should take a PP argument headed by preposition p . We use features x f + p , l f + c and l f + c + n + p , where x f is the f th word, and l f is its lemma. 3.2.3 Labelling Features Labelling features  X  ROLE determine whether the n th argument of a word with lemma l , and category c should take a role r . We use l + p + r , c + n + r , l + r , h + r , where h is an indica-tor of whether l is hyphenated, and p is the prepo-sition of PP arguments. 3.2.4 Dependency Features Dependency features  X  DEP score dependencies, based on the functor index f , argument index a c f + o + r , c a + o + r , where o is an offset from  X  3 ... +3 , d is the distance between f and a , c i is a cluster or POS tag for word i , and l i is the lemma of word i . Clusters were created from pre-trained word embeddings (Levy and Goldberg (2014)) us-ing k-means, with k = 20 , 50 , 200 , 1000 , 2500 . These features only apply when the role r 6 =  X  . 3.2.5 Derivation Features Derivation features  X  DERIV score properties of the syntactic derivation. To simplify computation given in Section 5, the weights of these features are constrained to be  X  0 . For simplicity, we only use features for unary rules X  X or example, a fea-ture records when the unary rule N  X  NP con-verts a determiner-less N to a NP . In this section, we show how to factor the model into extended lexical entries . This factorization al-lows us to efficiently compute upper bounds on the algorithm described in Section 5.

The key observation is that our supertagging, labelling and dependency features can each be as-sociated with exactly one word (using the functor for the dependency features). Therefore, we can equivalently view the complete parse as a series of extended lexical entries : y = y 0 ...y N . Extended lexical entries are composed of a lexical category, and a role and attachment for each argument of the category. The set of extended lexical entries specifies the yield of the parse. For example, the parse from Figure 2 can be represented as the following extended lexical entries:
The score for a sentence x and joint SRL-CCG parse y ,  X   X   X  ( x,y ) , can be decomposed into scores of its extended lexical entries y i , plus a score for the derivation features:  X   X   X  ( x,y ) =
The space of possible extended lexical entries for word x i is defined by GENLEX ( x i ) , which is expressed with a CFG, as shown in Figure 4a.
The features defined in Section 3.2 decom-pose over the rules of GENLEX  X  X o it can be weighted with the globally trained feature weights.
Expressing GENLEX ( x i ) as a CFG allows us to efficiently compute upper bounds on the score of y i when it is only partially specified, using the Viterbi algorithm X  X hich we will make use of in Section 5. Making the attachment choice indepen-dent of the syntactic category greatly improves the efficiency of these calculations. 5 A  X  Parsing To efficiently decode our model, we introduce a ing tight upper bounds on the Viterbi outside score of partial parses, with the function h .

The intuition behind our algorithm is that we can use the lexical factorization of Section 4 to compute upper bounds for words individually, then create an upper bound for the parse as a sum of upper bounds for words. The bound is not exact, because the grammar may not allow the combina-tion of the best lexical entry for each word.
Section 5.1 gives a declarative definition of h for any partial parse, and 5.2 explains how to effi-ciently compute h during parsing. 5.1 Upper Bounds for Partial Parses This section defines the upper bound on the Viterbi outside score h ( y i,j ) for any partial parse y i,j of span i...j . For example, in the parse in Figure 3, y 3 , 5 is the partial parse of confirm or deny with category ( S \ NP ) / NP .

As explained in Section 4, a parse can be de-composed into a series of extended lexical entries. Similarly, a partial parse can be viewed as a se-ries of partially specified extended lexical entries y ...y N . For example, in Figure 3, the partial parse of the span confirm or deny reports , the ex-tended lexical entries for the words outside the span ( He , refused and to ) are completely unspeci-fied. The extended lexical entries for words inside the span have specified categories, but can contain underspecified dependencies:
Therefore, we can compute an upper bound for the outside score of a partial parse as a sum of the upper bounds of the unspecified components of each extended lexical entry. Note that because the derivation features are constrained to be  X  0 , they do not affect the calculation of the upper bound.
We can then find an upper bound for completely unspecified spans using by summing the upper bounds for the words. We can pre-compute an up-per bound for the span h i,j for every span i,j as: The max can be efficiently computed using the Viterbi algorithm on GENLEX ( x k ) (as described in Section 4).

The upper bound on the outside score of a par-tial parse is then the sum of the upper bounds of the words outside the parse, and the sum of the scores of the best possible specifications for each underspecified dependency: h ( y i,j ) = where deps ( y ) returns the underspecified depen-dencies from partial parse.

For example, in Figure 3, the upper bound for the outside score of the partial parse of confirm or deny reports is the sum of the upper bounds for the other words independently ( h 0 , 3 ) added to the score of the best attachments and roles for the subjects of confirm and deny independently. 5.2 Additive Updates During parsing, the upper bound h can be ef-ficiently computed recursively with additive up-dates. Initially h is the sum of the upper bounds for each word independently h 0 ,N . Then, when specifying categories or labelling dependencies, the score is updated.  X  When specifying a category for a word x i , the  X  When specifying a semantic role for a de- X  When a binary rule combines two partial
These are the only cases where h is updated. During training, we optimize parameters  X  for the marginal likelihood of the semantic depen-dencies  X  i given a sentence x i , treating syntactic derivations d as a latent variable and using L 2 regularization.
 where GEN ( x i ) is the set of consistent CCG and SRL parses for a sentence x i (see Section 3), and  X ( x i , X  i ) is the set of CCG derivations that are maximally consistent with gold SRL parses  X  i . More formally, if labelled ( y ) returns the set of labelled dependencies from parse y , then:
The arguments of PropBank dependencies can span multiple words, so CCG dependencies are marked as equivalent if their argument is any-where within the PropBank span.

The approach is closely related to the hy-brid dependency model (Clark and Curran, 2007). However the CCGbank dependencies used by Clark and Curran X  X  model constrain all lexical and attachment decisions (only allowing  X  X puri-ous X  derivational ambiguity) whereas our use of semantic dependencies models most of the syntac-tic parse as latent. 6.1 Hyperparameters Calculating the gradient of the loss function re-quires computing the marginal probability of the correct parses. Computing marginal probabilities exactly involves summing over all possible CCG parses, which is intractable. Instead, following Clark and Curran, we limit the size of training charts using a variable-width supertagger beam  X  , nodes, we double  X  and re-parse. For computing  X  , we use a more restrictive beam of  X  = 0 . 01 to improve the quality of the positive training ex-amples. We optimize using L-BFGS (Liu and No-cedal, 1989), with  X  2 = 0 . 06 . 6.2 Pruning To improve efficiency, we compute a number of thresholds, by aligning gold CCGbank dependen-cies with PropBank. If an argument of a cate-gory occurs with a particular semantic role less than 3 times in the aligned data, it is pruned from the training and decoding charts. We also fil-ter infrequent features before training. We count the number of times each feature occurs in the aligned data, and filter features occurring less than 3 times. During decoding, we prune lexical cate-gories whose probability under the supertagging the best category for that word. If the chart size exceeds 20000 nodes, we back off to a pipeline model (roughly 3% of sentences). Finally, we build and use a tag dictionary in the same way as Lewis and Steedman (2014a). 7.1 Experimental Setup We used PropBank Section 00 for development, Sections 02-21 for training, and Section 23 for testing. The Pipeline baseline first parses with a tic role for each CCG dependency with a log-linear classifier. The classifier uses the role and attach-ment features used by the parser. 7.2 Semantic Role Labelling We evaluate our parser as a dependency-based SRL model on PropBank, comparing with CoNLL-2008 systems (Surdeanu et al., 2008).
 Comparison systems Following T X ckstr X m et al. (2015), we compare with the best  X  X ingle parser X  SRL models, which use only a single syn-
Much recent work has evaluated using gold predicate identification (Haji  X  c et al., 2009; FitzGerald et al., 2015). This setting is particu-larly unrealistic for our joint model, where gold predicate identification would be a highly useful feature for the supertagger; we only compare with models that use automatic predicate identification. To the best of our knowledge, the best SRL re-sults with automatic predicate identification were achieved at CoNLL-2008.

A number of other models have been evaluated on CoNLL-2008 data. While we cannot com-pare directly on our metric, the best reported joint model (Johansson, 2009) scored 1.4 points lower than the Che et al. (2008) system we compare to on the CoNLL metric on PropBank. Other joint models, such as those of Titov et al. (2009) and Llu X s et al. (2013), achieve similar performance. Evaluation Metric Comparing fairly with ex-isting work is complicated by the mismatch be-tween heads found by our model and those used in other evaluations. Headedness decisions are often arbitrary X  X or example, whether a prepositional phrase is headed by the noun or preposition X  X nd different choices were made in the design of CCG-bank and the CoNLL-2008 headedness rules.

To solve this problem, we introduce a new within-constituent metric, which awards depen-Model P R F1 P R F1 Vickrey 87.3 77.3 82.0 74.0 64.5 68.9 Che 85.3 78.6 81.8 71.1 65.7 68.0 Zhao 82.4 79.8 81.1 66.6 64.9 65.7 Riedel 83.6 74.7 78.9 69.3 62.7 65.8 Pipeline 79.2 73.9 76.4 69.3 64.0 66.1 Joint 84.8 82.2 83.5 71.2 69.2 70.2 Table 1: Comparison with the best single-parser SRL models on PropBank from CoNLL-2008. The comparison models are Vickrey and Koller (2008), Che et al. (2008), Zhao and Kit (2008) and Riedel and Meza-Ruiz (2008). dencies as correct if they attach anywhere within the original PropBank-annotated argument spans. For example, if the PropBank annotates that the ARG 0 of owned is by Google , a dependency to either by or Google is judged correct. We com-pute new scores for the CoNLL-2008 submissions on our metric, filtering reference and continuation arguments (which are artifacts of the CoNLL con-version of PropBank, but not required by our met-ric), and nominal predicates based on POS tag. The ranking of the top 5 CoNLL-2008 open-track models is identical under our metric and the orig-inal one (up to statistical significance), suggesting that our metric is equally discriminative. However, perhaps interestingly, the ranking of Vickrey and Koller (2008) does improve X  X ikely due to the use of a syntactic formalism with different headedness rules. For simplicity, we do not include predicate senses in the evaluation.
 Results are given in Table 1 and show that our joint model greatly outperforms the pipeline ver-sion, demonstrating the value of joint reasoning. It also scores 1.5 points higher than the best com-parable models in-domain, and 1.3 points higher out-of-domain. To the best of our knowledge, this is the first joint syntax/SRL model to outperform strong pipeline models. 7.3 Efficiency Experiments ficient than alternatives, including both algorith-builds very small charts, the features must be pre-computed for the heuristic. In CKY parsing, fea-tures can be computed lazily. Table 2: Parser speed on PropBank Section 23
We compare with a CKY parsing over the same space. If no parse is found, or the chart size ex-ceeds 400000 nodes, we back off to the pipeline (tuned so that the backoff is used roughly as often tive supertagging (AST, Clark and Curran (2007)), which is the same except for first attempting to parse with a restrictive supertagger beam  X  = 0 . 1 . fast, but inaccurate. CKY is 5 times slower than ging trades accuracy for speed. The best previ-ing is a factor of 1.2 times faster (Auli and Lopez, alternatives in both speed and accuracy. 7.4 Syntactic Parsing We also evaluate our model for CCG parsing ac-curacy, using CCGrebank (Honnibal et al., 2010), and comparing with a C&amp;C parser model adapted to this dataset. Results are shown in Table 3. Of course, given our latent syntax, and the fact we have no model of CCGbank dependencies, we do not expect state-of-the-art accuracy. However, the 1.6 point improvement over the pipeline shows that SRL dependencies are useful for disambiguat-ing syntactic attachment decisions. Many errors were caused by disagreements between CCGbank and PropBank X  X ropBank is likely to be more ac-curate as it was hand-annotated, rather than auto-matically converted from the Penn Treebank. In effect, our latent model of syntax is successfully learning a grammar that better produces the cor-rect semantics.

Table 4 shows the syntactic dependencies which are improved most by modelling semantics. Un-surprisingly, verb arguments and adjuncts rela-tions show large improvements. However, we also see more accurate attachment of relative clauses. While we do not model relative clauses explicitly, correctly attaching them is essential for propagat-Table 3: Labelled F1 for CCGbank dependencies on CCGrebank Section 23
Dependency Type  X  F1 (( S dcl \ NP ) / PP ) / NP +12.6 (( S dcl \ NP ) / PP ) / NP +11.0 (( S b \ NP ) / PP ) / NP Verb +10.7 ( S dcl \ NP ) / PP arguments +8.1 ( S ng \ NP ) / NP +7.6 ( S pt \ NP ) / NP +6.8 (( S \ NP ) \ ( S \ NP )) / NP Verb +16.9 (( S \ NP ) \ ( S \ NP )) / S dcl adjuncts +11.9 ( N \ N ) / ( S dcl \ NP ) Relative +12.5 ( NP \ NP ) / ( S dcl \ NP ) clauses +8.5 Table 4: CCG syntactic dependencies with the largest change in F1 between the Pipeline and Joint models (of those occurring at least 100 times in the development set). ing certain verb arguments (e.g. the subject of broke in the glass on the table that broke ). 7.5 Error Analysis Table 5 gives an analysis of recall errors from the first 100 sentences of PropBank Section 00. One third of errors were syntactic attachment errors. A further 14% were triggered by cases where the parser found the correct attachment, but gave an adjunct a core argument CCG category (or vice versa). Many of these decisions are not obvious X  for example in rose sharply , PropBank considers sharply to be an argument and not an adverb. 21% of errors were caused by choosing the wrong SRL Error Percentage Attachment error 33% Correct attachment, wrong label 21% Correct attachment, unlabelled 20% Argument/adjunct distinction 14% Problematic constructions 9% Dubious annotation 4% Table 5: Error analysis of recall errors from 100 development set sentences. label. Another 20% were caused by predicates as-signing arguments the null semantic role  X  . The major cause of these errors is predicates that act syntactically like adjectives (e.g. publishing in Dutch publishing group ) where syntactic cues are weak. Finally, 9% involved long-range arguments that our current grammar is unable to project. One common case is constructions like X has a plan to buy Y , where the grammar does not propagate the subject of buy to X . Further improvements to CCGbank may help to resolve these cases. Joint syntactic and SRL models There have been many proposals for jointly parsing syntac-tic and semantic dependencies. Llu X s et al. (2013) introduce a joint arc-factored model for parsing syntactic and semantic dependencies, using dual-decomposition to maximize agreement between the models. SRL performance is slightly worse than a pipeline version. Naradowsky et al. (2012) introduce a SRL model with latent syntax repre-sentations, by modelling a latent dependency tree during training, which is marginalized out at test time. However, performance at English SRL is roughly 7 points beneath state of the art. Other notable models include those of Johansson (2009) and Titov et al. (2009).
 CCG parsing Our log-linear model is closely related to that of Clark and Curran (2007), but we model SRL dependencies instead of CCG de-pendencies. The best CCG parsing results were achieved by Auli and Lopez (2011a), who, like us, score CCG parses based jointly on supertagging and dependency model scores. Decoding their model requires dual-decomposition, to maximize agreement between the separate models. We avoid the need for this technique by using a unigram su-pertagging model, rather than a sequence model. CCG semantics Work on semantic parsing has mapped sentences onto semantic representations with latent CCGs (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2013) for restricted domains. Recent work has scaled these techniques to wide-coverage datasets (Artzi et al., 2015). Krishnamurthy and Mitchell (2014) also explore joint CCG syntactic and se-mantic parsing. They use a smaller semantic lex-icon, containing 130 predicates, rather than the 3257 PropBank verbs. In contrast to our re-sults, jointly modelling the semantics lowers their model X  X  syntactic accuracy.
 Other CCG-based SRL models haved used CCG dependencies as features for predicting se-mantic roles (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), but performance is limited by relying on 1-best parses X  X  problem we re-solved with a joint model.
 plored for less general models than ours. Klein and Manning (2003) and Auli and Lopez (2011b) pendencies. The best reported speed improvement is parsing 1.2 times faster, whereas we improve by a factor of 5. Our model also allows the more complex graph-structured dependencies required for semantic role labelling. Lewis and Steedman CCG, but cannot model dependencies. We have shown that using CCG can allow joint models of syntax and semantics to outperform pipelines, and achieve state-of-the-art results on is 5 times faster than CKY parsing, without loss of accuracy. Using latent syntax allows us to train the model purely from semantic depen-dencies, enabling future work to train against other annotations such as FrameNet (Baker et al., 1998), Ontonotes (Hovy et al., 2006) or QA-SRL (He et al., 2015). The semantic labels pro-vided by PropBank can also be integrated into wide-coverage CCG semantic parsers (Bos, 2008; Lewis and Steedman, 2013) to improve perfor-mance on downstream applications.
 This research was supported in part by the NSF (IIS-1252835), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google. We thank Nicholas Fitzgerald, Dan Gar-rette, Kenton Lee, Swabha Swayamdipta, Mark Yatskar and the anonymous reviewers for their helpful comments on earlier versions of this paper, and Matthew Honnibal for useful discussions.
