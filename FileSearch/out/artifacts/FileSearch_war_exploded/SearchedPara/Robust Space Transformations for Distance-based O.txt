 particularly in the presence of variability, correlation, out-liers, and/or differing scales. Consider a dataset with the following attributes:  X  systolic blood pressure (typical range: 100-160 mm of  X  body temperature (# = 37 degrees Celsius, with a very  X  age (range: 20-50 years of age in this example) Note that different attributes have different scales and units (e.g., mm of Hg vs. degree Celsius), and different variability (e.g., high variability for blood pressure vs. low variability for body temperature). Also, attributes may be correlated (e.g., age and blood pressure), and there may be outliers. EXAMPLE OPERATION 1 (NEAREST NEIGHBOR SEARCH). Consider a nearest neighbor search using the Euclidean dis-tance function in the original data space, i.e., the identity transformation. The results are likely to be dominated by blood pressure readings, because their variability is much higher than that of the other attributes. Consider the query point (blood pressure = 120, body temperature = 37, age = 35). Using Euclidean distance, the point (120, 40, 35) is nearer to the query point than (130, 37, 35) is. But, in terms of similarity/dissimilarity, this finding is not very meaning-ful because, intuitively, a body temperature of 40 degrees is far away from a body temperature of 37 degrees; in fact, a person with a body temperature of 40 degrees needs medical attention immediately! 
A simple fix to the above problem is to somehow weight tile various attributes. One common approach is to apply a "normalization" transformation, such as normalizing each attribute into the range [0,1]. This is usually not a satisfac-tory solution because a single outlier (e.g., blood pressure = 200) could cause virtually all other values to be contained in a small subrange, again making the nearest neighbor search produce less meaningful results. 
Another common fix is to apply a "standardization" trans-formation, such as subtracting the mean from each attribute and then dividing by its standard deviation. While this transformation is superior to the normalization transforma-tion, outliers may still be too influential in skewing the mean and the standard deviation. Equally importantly, this trans-formation does not take into account possible correlation between attributes. For example, older people tend to have 127 The Fixed-angle Donoho_Stahel Algorithm 2. For i = 1 to N, compute dl = sup 0 di(O) (i.e., Compute 3. Compute the robust multivariate centre fn = 5. Return the Douoho-Stahel estimator of location and 
Figure 1: The DSE Fixed-angle Algorithm for 2-D the Fixed-angle algorithm. 
For each 0, each point is projected onto the line corre-sponding to rotating the x-axis by 0, giving the value xi(O). Mathematically, this is given by the dot product between yl the projection vector. 
In step l(b), we compute re(O), which is the median of all the xi(O) values. MAD is an acronym for median abso-lute deviation from the median. It is a better estimator of scatter than the standard deviation in the presence of out-liars. Finally, step lid ) yields di(0), which measures how outlying the projection of yi is with respect to 0. Note that di(O) is analogous to classical standardization, where each value xi(0) is standardized to ~ a~-t-(~, with /z and a being the mean and the standard deviation of xi(O), respectively. By replacing the mean with the median, and the standard deviation with the MAD, di(O) is more robust against the in-fluence of outliers than the value obtained by classical stan-dardization. 
Robustness is achieved by first identifying outlying points, and then downweighting their influence. Step 1 computes for each point and each angle 0, the degree of outlyingness of the point with respect to 0. As a measure of how outlying each point is over all possible angles, step 2 computes, for each point, the maximum degree of outlyingness over all possible 0's. In step 3, if this maximum degree for a point is too high (our threshold is 2.5), the influence of this point is weakened by a decreasing weight function. Finally, with all points weighted accordingly, the location center/2n and the covariance matrix lea are computed. 3. KEY PROPERTIES OF THE DSE 
In this section, we examine whether the estimator is use-ful for distance-based operations in KDD applications. In Section 6, we provide experimental results showing the dif-absurd" [27]. But we do not pursue this formal approach regarding breakdown points; instead, we resort to experi-mental evaluation. 
In our experiments, we used a real dataset D and com-puted the DSE ~R(D). We then inserted or deleted tuples from D, thereby changing D to DRew. To measure stabil-ity, we compared matrix ~R(D) with ~n(D~e~o). In the numerical computation domain, there are a few heuristics for measuring the difference between matrices; but there is no universally agreed-upon metric [9]. To make our com-parison more intuitive, we instead picked a distance-based operation--outlier detection--and compared the results. Sec-tion 6 gives the details of our experiments, but in brief, we proceeded as follows: (a) We used the old estimator ~R(D) to transform the space for D~w and then found all the outliers in D~; and (b) We used the updated estimator ~(Dnew) to transform the space for D .... and then fonnd all the outliers in D~e~o. 
To measure the difference between the two sets of detected outliers, we use standard precision and recall [23], and we define: (i) the answer set as the set of outliers found by a given algorithm, and (if) the tar9et set as the "official" set of outliers that are found using a sufficiently exhaustive search (i.e., using the Fixed-angle algorithm with a relatively small angular increment). Precision is the percentage of the answer set that is actually found in the target set. Recall is the percentage of the target set that is in the answer set. Ideally, we want 100% precision and 100% recall. 
Fig. 2 shows the results when there were 25%, 50%, 75% and 100% new tuples added to D, and when 25%, 50% and 75% of the tuples in D were deleted from D. The new tu-ples were randomly chosen and followed the distribution of the tuples originally in D. The deleted tuples were ran-domly chosen from D. The second and third columns show the number of outliers found, with the third column giv-ing the "real" answer, and the second column giving the "approximated" answer using the old transformation. The fourth and fifth columns show the precision and recall. They clearly show that the DSE transformation is stable. Even a 50% change in the database does not invalidate the old transformation, and re-computation appears unnecessary. 
For the results shown in Fig. 2, the newly added tuples followed the same distribution as the tuples originally in D. For the results shown in Fig. 3, we tried a more drastic sce-nario: the newly added tuples, called junk tuples, followed a totally different distribution. This is reflected by the rel-atively higher numbers in the second and third columns of Fig. 3. Nevertheless, despite the presence of tuples from two distributions, the precision and recall figures are still close to 100%. This again shows the stability of the DSE. % Change in # of Outliers in 25% Inserts 17 50% Inserts 17 75% Inserts 32 100% Inserts 37 25% Deletes 13 50% Deletes 16 75% Deletes 15 
Figure 2: Precision and Recall: Same Distribution 130 For i = 1, 2,... r, where r is the number of projection vectors chosen, do: (a) Select a k-D projection unit vector u~ randomly (b) For j = 1 to N, do: compute xj(i) = yj -ul (c) (continue with step li b) and beyond of the Fixed-
Figure 6: DSE Pure-random Algorithm for k-D 
Using the Euclidean inner product and the Law of Cosines, loaded from sites such as the Professional Hockey Server at http://maxweU.uhh.hawaii.edu/hockey/. Since this real-life dataset is quite small, we created a number of synthetic datasets mirroring the distribution of statistics within the NHL dataset. Specifically, we determined the distribution of each attribute in the original dataset by using a 10-partition histogram. Then, we generated datasets containing up to 100,000 tuples--whose distribution mirrored that of the base dataset. As an optional preprocessing step, we applied the Box and Cox transformation to normality [8] to find appro-priate parameters p and D for the distance-based outliers implementation. Unless otherwise stated, we used a 5-D case of 100,000 tuples as our default, where the attributes are goals, assists, penalty minutes, shots on goal, and games played. 
Our tests were run on Sun Microsystems Ultra-1 proces-sor, running SunOS 5.7, and having 256 MB of main mem-ory. Of the four DSE algorithms presented, only the Fixed-angle algorithm is deterministic. The other three involve randomization, so we used the median results of several runs. Precision was almost always 100%, but recall often varied. Usefulness of Donoho-Stahel Transformation In the introduction, we motivated the usefulness of the Donoho-Stahel transformation by arguing that the identity transfor-mation (i.e., raw data), as well as the normalization and standardization transformations, may not give good results. In the experiment reported below, we show a more concrete situation based on outlier detection. Based on the 1995-96 NHL statistics, we conducted an experiment using the two attributes: penalty-minutes and goals-scored. We note that the range for penalty-minutes was [0,335], and the range for goals-scored was [0,69]. 
Fig. 8 compares the top outliers found using the identity, standardization, and Donoho-Stahel transformations. Also shown are the actual penalty-minutes and goals-scored by the identified players. With the identity transformation (i.e., no transformation), players with the highest penalty-minutes dominate. With classical standardization, the dom-inance shifts to the players with the highest goals-scored (with Matthew Barnaby appearing on both lists). How-ever, in both cases, the identified outliers are "trivial", in the sense that they are merely extreme points for some at-tribute. Barnaby, May, and Simon were all in the top-5 for penalty-minutes; Lemieux and Jagr were the top-2 for 
With the Donoho-Stahel transformation, the identified outliers are a lot more interesting and surprising. Donald Figure 8: Identified Outliers: Usefulness of Donoho-~q tah~l TrAn~forrnAtinn can give acceptable results in a short time. But, the re-call curve has a diminishing rate of return, and it may take a very long time for Subsampling to reach a high level of recall, as confirmed in Fig. 10. 
Since the Hybrid-random algorithm uses the Subsampling algorithm in its first phase (with rS~] = 24 iterations), it is expected that the Hybrid-random algorithm behaves about as well as the Subsampling algorithm, at the beginning, for mediocre levels of recall, such as 70-75% (cf: Fig. 10). But, as shown in Fig. 9(d), if the Hybrid-random algorithm is al-lowed to execute longer, it steadily and quickly improves the quality of its computation. Thus, in terms of CPU time, we start with the Subsampling curve, but quickly switch to the Pure-random curve to reap the benefits of a fast algorithm and pruned randomization. Achieving a Given Rate of Recall The above exper-iment shows how each algorithm trades off efficiency with quality. Having picked a reasonable set of parameter values for each algorithm, let us now compare the algorithms head-to-head. Specifically, for fixed recall rates, we compare the time taken for each algorithm to deliver that recall rate. Be-cause the run time of the Fixed-angle algorithm is typically several orders of magnitude above the others (for compara-ble quality), we omit the Fixed-angle algorithm results from now on. 
Fig. 10 compares the Hybrid-random algorithm with both the Pure-random and Subsampling algorithms, for higher rates of recall. In general, the Subsampling algorithm is very effective for quick, consistent results. However, to im-prove further on the quality, it can take a very long time. In contrast, when the Hybrid-random algorithm is allowed to run just a bit longer, it can deliver steady improvement on quality. As a case in point, to achieve about 90% recall in the current example, it takes the Subsampling algorithm al-most 14 hours to achieve the same level of recall produced by the Hybrid-random algorithm in about two minutes. Never-theless, we must give the Subsampling algorithm credit for giving tile Hybrid-random algorithm an excellent base from which to start its computation. 
In Fig. 10, the Pure-random algorithm significantly out-performs the Subsampling algorithm, but this is not always the case. We expect the recall rate for Pure-random to be volatile, and there are cases where the Pure-random algo-rithm returns substantially different outliers for large num-bers of iterations. The Hybrid-random algorithm tends to be more focused and consistent. Scalability in Dimensionality and Dataset Size Fig. ll(a) shows scalability of dimensionality for the Subsam-piing and Hybrid-random algorithms. We used moderate levels of recall (e.g., 75%) and 60,000 tuples for this anal-ysis. High levels of recall would favor the Hybrid-random algorithm. The results shown here are for 282 iterations for the Subsampling algorithm, and 90 Patches for the Hybrid-random algorithm. Our experience has shown that these numbers of iterations and patches are satisfactory, assuming we are satisfied with conservative levels of recall. Fig. ll(a) shows that both algorithms scale well, and this confirms our complexity analysis of Section 4. 
Fig. 11 (b) shows how the Subsampling and Hybrid-random algorithms scale with dataset size, in 5-D, for conservative levels of recall. Again, both algorithms seem to scale well, and again the Hybrid-random algorithm outperforms the We saw that distance operations which ordinarily would be inappropriate when operating on the raw data (and even on normalized or standardized data), are actually appropriate in the transformed space. Thus, the end user sees results which tend to be more intuitive or meaningful for a given application. We presented a data mining case study on the detection of outliers to support these claims. 
After considering issues such as effectiveness (as mea-sured by precision and recall, especially the latter) and ef-ficiency (as measured by scalability both in dimensionality and dataset size), we believe that the Hybrid-random al-gorithm that we have developed in this paper is an excel-lent choice among the Donoho-Stahel algorithms. In tens of seconds of CPU time, a robust estimator can be computed which not only accounts for scale, variability, correlation, and outliers, but is also able to withstand a significant num-ber of database updates (e.g., 50% of the tuples) without losing effectiveness or requiring re-computation. For many cases involving high levels of recall, the randomized algo-rithms, and in particular, the Hybrid-random algorithm can be at least an order of magnitude faster (and sometimes several orders of magnitude faster) than the alternatives. In conclusion, we believe that our results have shown that ro-bust estimation has a place in the KDD community, and can find value in many KDD applications. [1] R. Agrawal, J. Gehrke, D. Gunopulos and P. [2] H. Anton and C. Rorres. Elementary Linear Algebra: [16] E. Knorr and 1~. Ng. Algorithms for Mining [17] G. Li and Z. Chen. Projection-pursuit approach to [18] 1~. Martin and R. Zamar. Bias robust estimation of [19] R. Maronna and V. Yohai. The behaviour of the [20l R. Ng and J. Han. Efficient and Effective Clustering [21] S. Ramaswamy, R. Rastogi and K. Shim. Efficient [22] P. Rousseeuw and A. Leroy. Robust Regression and [23] G. Salton and M. McGill. Introduction to Modern [24] T. Seidl and H.-P. Kriegel. Efficient User-Adaptable [25] W. A. Stahel. Breakdown of Covariance Estimators. [26] W. Wang, J. Yang and R. Muntz. STING: A [27] V. Yohai and R. Zamar. High breakdown point 
