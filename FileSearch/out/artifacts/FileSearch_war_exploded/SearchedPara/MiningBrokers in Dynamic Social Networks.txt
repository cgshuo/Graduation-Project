 The theory of brokerage in sociology suggests if contacts between two parties are enabled through a third party, the latter occupies a strategic position of controlling information flows. Such individ-uals are called brokers and they play a key role in disseminating information. However, there is no systematic approach to identify brokers in online social networks. In this paper, we formally de-fine the problem of detecting top-k brokers given a social network and show that it is NP-hard. We develop a heuristic algorithm to find these brokers based on the weak tie theory. In order to han-dle the dynamic nature of online social networks, we design in-cremental algorithms: WeakTie-Local for unidirectional networks and WeakTie-Bi for bidirectional networks. We use two real world datasets, DBLP and Twitter, to evaluate the proposed methods. We also demonstrate how the detected brokers are useful in diffusing information across communities and propagating tweets to reach more distinct users.
 J.4 [ Social and Behavioral Sciences ]: Miscellaneous Algorithm, experimentation Broker, weak tie, social network analysis
Social networks play significant roles in spreading news and opin-ions. Online social networks accelerate and expand the diffusion of information among web users [2, 13]. Sociologists have long recognized that users situated between otherwise disconnected or remote users possess advantageous positions [3]. On one hand, such users serve as bridges and enable the interaction between their neighbors; on the other hand, they can access remote information sources and control the information flow between them. These users are called brokers . A number of empirical studies have shown the significant roles played by brokers in disseminating information [19].

The work in [19] defines different types of brokers based on the direction of information flow and whether the end users are mem-bers in the same community. [14] assumes that the set of communi-ties is known and proposes two methods to find users who connect different communities and are responsible for information diffusion across communities. They call these users structural hole spanners which is one type of brokers. In the first method MaxD , the idea is to remove one node at a time such that the removal of this node will lead to the maximum decrease in the minimum cut in the given set of communities. The second method HIS looks for nodes that are incident with  X  X pinion leaders X  in different communities. However, for most social networks, it is not easy to determine the number nor the boundaries of communities that exist in the networks.
In this paper, we examine a more general type of broker that is not defined based on communities. We define a broker as one whose removal would result in the greatest increase in the pair-wise distance among the remaining users. The intuition behind this definition is to capture users who are situated between otherwise disconnected or distant users. We propose a community indepen-dent method to find the top-k brokers in online social networks. Formally, we have
D EFINITION 1. Top-k Brokers. Let G = ( V; E ) be a directed graph where each node v 2 V denotes a user, and each edge ( u; v ) 2 E denotes that user u follows user v . The top-k bro-kers are the set of users H V , j H j = k , such that the utility function  X  is maximized: where d ( u; v ) and d  X  ( u; v ) denote the distance between u and v in G and G n H respectively. G n H is the resultant subgraph of G having the nodes in H and their incident edges removed.
We believe brokers play a special role in the dissemination of information in social networks. Existing works have used different measures to identify nodes that are important in information diffu-sion. For example, [4, 18] design PageRank based algorithms to identify users with high degrees as seed nodes for viral marketing. Other works utilize the betweenness measure [8] to identify the users who have a large number of shortest paths that pass through them. These measures focus on the big clusters of users and tend to neglect the small clusters of remote users.

Consider the example social network in Figure 1. PageRank based algorithms will select u 7 because it is incident with other high-degree nodes. Methods that utilize betweenness measure will find u 2 since it carries the shortest paths between the two clusters of nodes. However, we note that the removal of u 2 or u 7 have much impact on their neighbors as the neighbours can still reach other users without significant increase in the distance.
On the other hand, the situation is different for u 1 . We see that u enables u 4 , u 5 and u 6 to reach u 7 and its neighbours within 3 or 4 hops. However, when u 1 is removed, u 4 , u 5 and u 6 need to travel a very long distance to reach u 7 and its neighbors. We say u broker that brings remote nodes close to others. With the online social networks playing an increasingly significant role in spread-ing news and opinions, identifying such brokers is clearly advanta-geous. Information disseminated through these users have a much higher chance of reaching distant users whereas the centrality-based nodes may not reach those isolated individuals.

Finding top-k brokers is an NP-hard problem as it can be re-duced from the k -densest subgraph problem (see Section 2). In order to tackle this problem, we design a heuristic solution to find the top-k brokers based on the theory of weak ties [6]. We pro-pose a connection-aware scoring function and design an algorithm called WeakTie to find the top-k brokers. Further, to handle the highly dynamic nature of social networks, we also develop incre-mental algorithms: WeakTie-Local for unidirectional networks, and WeakTie-Bi for bidirectional networks. These algorithms are effi-cient as they recompute the scores based on the local views of these nodes.

Finally, we demonstrate how brokers can be useful in two in-formation diffusion tasks: (a) structural hole spanner detection and (b) mention recommendation. We found that our detected brokers improve the precision of spanner detection by 35% . For mention recommendation, brokers can help diffuse a message to users lo-cated far away from the author and reach 23% more distinct users. The contribution of this work is summarized as follows.
The rest of this paper is organized as follows. Section 2 proves the NP-hardness of the problem. Section 3 presents our observa-tions on weak ties and introduces our proposed approach based on the weak tie theory. Section 4 further introduces the incremental al-gorithms, WeakTie-Local and WeakTie-Bi to accelerate the compu-tation in unidirectional and bidirectional networks respectively. In Section 5 we empirically evaluate our approaches and demonstrate the usefulness in two real world applications. Section 6 presents the related work and we conclude in Section 7.
In this section, we prove that the problem of finding top-k bro-kers is NP-hard.

T HEOREM 1. Identifying the top-k brokers in a directed graph is NP-hard.

P ROOF . The decision version of the top-k broker problem is stated as follow: Is there a subset of nodes H where j H j such that the utility function  X  in Equation 1 is greater than a given value s ? To prove that the top-k broker problem is NP-hard, we first show that the decision version of top-k broker can be reduced from its function version in polynomial time and then we prove the decision version of top-k broker problem is NP-hard.

Suppose we have a graph with n nodes, the maximum distance between any pair of nodes is ( n 1) and there can be at most n 2 pairs of nodes. This implies that there are at most n 2 possible values for  X  . If we can solve the decision version in poly-nomial time, then the solution to the top-k broker problem is the set of nodes H that gives the largest s value which can be found in polynomial time.

Next, we prove that the top-k broker decision problem is NP-hard by showing that it can be reduced from the known NP-hard k -densest subgraph problem, namely, determining if there exists a k -node subgraph with at least p edges in a given graph [1].
Given an instance I = f G = ( V; E ) ; k; p g of the k-densest subgraph problem, let n = j V j , and m = j E j . We can construct a directed graph G  X  = ( V  X  ; E  X  ) of ( n + m +1) nodes and ( n +2 m ) edges as follows. We first create a terminal node t in G  X  node u in G , we create a corresponding node u  X  in G  X  , and connect u to t . For each edge e ( u; v ) in G , let u  X  and v  X  in G corresponding nodes of u and v respectively. We create a node e in G  X  , and connect e  X  to u  X  and v  X  . Let Y 1 = f e  X  Y
Figure 2 shows an example graph G and the corresponding con-structed graph G  X  . It is clear that the construction of G in polynomial time.
Next, we prove that the k-densest subgraph instance I is satisfi-able, if and only if there exists a subset H Y 2 and j H j that  X  p 2 in G  X  .

F or the only if direction, suppose the k-densest subgraph in-stance I is satisfiable, that is, we have a subgraph with k nodes and at least p edges in G . Let H Y 2 be the set of nodes in G that correspond to these k nodes. When we remove H from G there are at least p 2 Y 1 nodes in G  X  that become isolated. These isolated nodes correspond to the p edges in the k-densest subgraph solution. Recall the utility function in Equation 1 which computes the difference in the sum of the inverse pairwise distance between G  X  and G  X  n H . From the graph G  X  in Figure 2(b), we note that Clearly, the p isolated nodes will cause a difference in the pairwise distances between the remaining nodes in G  X  and G  X  n H . This is because these p nodes are able to reach the terminal node t in G 2 hops, but they can no longer reach t in G  X  n H . Hence, F or the if direction, suppose the top-k broker decision instance is satisfiable, that is, there exists H V  X  such that  X  p 2 . As mentioned above, the utility function is affected when nodes that are previously reachable in G  X  are no longer reachable in G This occurs when nodes in Y 2 are removed, leading to isolated nodes in Y 1 . In this case, the number of nodes removed from Y is j
H \ Y 2 j k . Since  X  p 2 , we have at least p isolated nodes in Y 1 . These p nodes are edges in G whose end points correspond to the nodes in H \ Y 2 . Thus, we have found a subgraph with at most k nodes and at least p edges. In other words, the k-densest subgraph instance I is satisfiable.

Gi ven that the problem of detecting top-k brokers is NP-hard, we propose a solution that is based on the weak tie theory [6].
Granovetter [6] first introduced the concept of weak ties in social networks where the relationship between two users is weak when their number of overlapping friends is small. Bakshy et al. [2] showed that weak ties are important in the diffusion of novel infor-mation between remote users in Facebook while Burt [3] found that users with many weak ties are more likely to be placed in bridging positions.

We carry out preliminary experiments on the real world DBLP dataset to investigate the correlation between weak ties and bro-kers. Here, the authors are the nodes while co-authorships form the edges. The tie strength of an edge ( author 1 ; author 2) is given by the fraction of the number of common co-authors between author 1 and author 2 to the total number of co-authors they have. An edge is a weak tie if the tie strength is below some threshold. We con-sider authors who have served as PC members in only one research area as opinion leaders, and those who have served in multiple re-search areas as brokers.

We compute the number of weak ties that are incident to each author and sort the authors according to the number of weak ties. Figure 3 shows the percentage of opinion leaders and brokers with respect to the different ranges of weak ties. We observe that as the number of weak ties increases, the proportion of brokers increases significantly. This motivates us to develop an algorithm to find brokers based on the their number of incident weak ties. Figur e 3: Percentage of opinion leaders and brokers versus weak ties in DBLP.

Although the experiment results show that a broker is correlated with the number of weak ties, simply counting the number of weak ties is not a good indicator of a node being a broker. Consider Figure 4 where a bold line denotes that two users have many over-lapping friends, i.e. they have a strong tie . u 1 has a total of 4 weak ties while u 2 has 5. We observe that even though u 2 more incident weak ties than u 1 , u 1 is connected to more sepa-rate groups and is more likely to be a broker. A closer observation reveals that many of u 2  X  X  neighbours are, in fact, friends of each other X  X  friends. Thus we need to take into account the connected-ness among friends X  friends. We first define the tie strength in a social network as follows:
D EFINITION 2. Tie Strength. Let G = ( V; E ) be a social net-work. For any edge ( u; v ) 2 E , u is a follower of v and v is a followee of u . Let F u denote the followees of u inclusive of u , and F v denote the followees of v inclusive of v . Then the tie strength of the edge ( u; v ) 2 E is given by
This implies that if the majority of u and v  X  X  followees are the same, S uv will be close to 1. The edge from u to v is a weak tie if S uv is below some threshold . Otherwise, it is a strong tie. Note that S uv = 0 if u is not a follower of v .

A path from u to v is a sequence of directed edges that starts from u and ends at v . The strength of a path ( path strength ) is defined as the minimum tie strength among all the edges in the path. Now we define a strongly connected group as follows:
D EFINITION 3. Strongly Connected Group. Given a social network G = ( V; E ) and a tie threshold , a strongly connected group is a maximal subgraph of G such that for all pairs of nodes ( u; v ) in the subgraph, there exists a path from u to v with path strength greater than .

Note that our strongly connected groups are in fact strongly con-nected components with the additional constraint on the minimum (c) Strongly connected groups path strength. We denote the set of strongly connected groups as C . Based on Definition 3, when two nodes u and v are in the same strongly connected group, there exists a path from u to v with path strength greater than and vice versa. This implies that all the nodes in the same strongly connected group form a cycle. Hence, the order of processing is irrelevant as we will still find the same set of strongly connected groups.

We define the closeness of a strongly connected group c to a node u as follows: Let N be the number of users in c whose tie strength to u is non-zero. If cl oseness ( c; u ) &lt; 0 , this implies that u does not share many common followees with the users in c . In other words, u is more likely to be a broker connecting c to the rest of the network.
With this, we can assess the potential of a node u being a broker as follows: where C C denotes the set of strongly connected groups whose closeness with u is below 0 . A large value of score ( u ) indicates that u is connected to many largely non-overlapping groups. Hence, u is in a bridging position among these groups and plays an impor-tant role in bringing the users of these groups close to each other.
Given a social network G = ( V; E ) and threshold , our pro-posed approach to find the top-k brokers in G consists of the fol-lowing main steps: 1. For each directed edge ( u; v ) 2 E , compute tie strength S 2. Find the set of strongly connected groups C in G ; 3. For each node u 2 V , compute its score ( u ) ; 4. Return the top-k nodes with the highest scores.

Step 2 utilizes the Tarjan X  X  method [20], which is designed for detecting strongly connected components , to perform a depth-first traversal of the strong ties in G to obtain the set of strongly con-nected groups .

Figure 5 illustrates the proposed approach using a small network with 10 nodes. We first compute the tie strengths for all the edges. For example, the tie strength for edge ( u 1 ; u 5 ) is 0.5 since we have F being the common followees out of 4. Figure 5(b) shows the weak and strong (bold arrows) ties in the example network when = 0 : 2 .

Next, we find all the strongly connected groups by applying Tar-jan X  X  algorithm [20] to traverse only the strong ties. Figure 5(c) shows two strongly connected groups obtained (marked in differ-ent colors). With this, we can compute the scores of all the nodes in the network. For example, node u 1 is connected to two strongly connected groups, c 1 and c 2 (see Figure 5(d)). We use Equation 2 to compute the closeness of each group to u 1 as follows: Based on the above, we conclude that c 1 is not close to u closeness is less than 0, while c 2 is close to u 1 . Finally, we have score( u 1 ) = 0.033.
Real world social networks are highly dynamic. In an evolving social network, there are many users joining (add node) and leav-ing (remove node) each day. Existing users may start new relation-ships (add edge) or they may break off their existing relationships (remove edge). These changes in relationships can all potentially affect the set of top-k brokers. Figure 6 shows the changes in the network after u 1 and u 7 start following each other. Note that when edges ( u 1 ; u 7 ) and ( u 7 ; u 1 ) are added, their tie strength is 0 : 33 and hence they are strongly connected to each other. Additionally, the tie strengths between their common followers increase and the edges ( u 7 ; u 2 ) and ( u 7 ; u 3 ) now become strong ties (see Figure 6(b)). Therefore, the entire network forms a single strongly con-nected group.

Recomputing the set of new top-k brokers when each update oc-curs is computationally expensive. Each update requires reapply-ing the Tarjan X  X  algorithm on the entire graph to obtain the strongly connected groups. This has a complexity of O ( j V j + j E clearly not a practical solution for online social networks where the volume of updates is high. This motivates us to propose incremen-tal algorithms to handle the dynamic nature of social networks. (a) 2-hop neighborhood of u 1
Updates in social networks are handled as either addition of new edges or removal of existing edges. This is because adding a node can be modelled as the addition of a list of edges between the new node and existing nodes; while removing an existing node is mod-elled as the removal of the edges associated with the removed node. Empirical investigation reveals changes to the strongly connected groups are often restricted to the 2-hop neighborhood of the af-fected nodes. This is because it is rarely the case that two users are connected by a chain of close friends and they do not know each other [9]. We design WeakTie-Local algorithm that utilizes the 2-hop neighborhood of an affected node to identify brokers. Algorithm 1 gives the details. Suppose an edge ( u; v ) is added, Algorithm WeakTie-Local first creates the respective nodes if they do not exist and updates the tie strength S uv . Since F u because of the addition/deletion of ( u; v ) , for any neighbor w of u , we update S uw or S wu according to Definition 2 (Lines 14-15).
Let G u be the 2-hop neighborhood of a node u and C u be the set of strongly connected groups of nodes in G u . From henceforth, we call C u the local groups of u . We extract the 2-hop neighborhoods G u and G v . The scores of u and v are recomputed by calling the function UpdateScore() (Lines 1-8). For u  X  X  neighbor w , we need to recompute the closeness ( c; w ) for u 2 c ^ c 2 C w and update score ( w ) (Lines 24-25). Further, adding the edge ( u; v ) may af-fect the local groups of the common neighbors of u and v . Hence, we also update the scores of these nodes (Lines 20-22). Similarly, when an edge is deleted, the corresponding 2-hop neighborhoods are extracted and we recompute the scores of the affected nodes.
Figure 7 illustrates the WeakTie-Local algorithm. Figure 7(a) is the 2-hop neighborhood of node u 1 and the tie strengths of the edges have been computed. We find all the local groups by applying Tarjan X  X  algorithm [20] on the nodes in G u 1 considering the ties among them. Figure 7(b) shows the 3 local groups for u 1 and we compute the score of u 1 . When the edge ( u 2 ; u 7 ) is added (Figure 7(c)), u 2 has gained a new followee. This changes the tie strengths S a strong tie (Figure 7(d)). We then apply the Tarjan X  X  algorithm to obtain the new set of local groups of u 1 and compute the updated score of u 1 .
For bidirectional social networks, we do not need to perform Tar-jan X  X  algorithm to find the new set of strongly connected groups whenever updates come. In most cases, as we will show soon, up-dating the score of an affected node takes linear time to the number of neighbors. The only case when we need to traverse the neighbor-hood is when the deleted edge causes a group to split into smaller groups. We present WeakTie-Bi algorithm to carefully exam the dif-ferent cases in order to further accelerate broker detection in bidi-rectional networks.

Algorithm 1: WeakTie-Local
Algorithm 2 gives the details. Lines 2-8 process the insertion of new edges. Given an edge ( p; q ) , if either p or q does not exist, we create a new node (Line 3). Lines 4-7 update the friend lists of p and q as well as their tie strengths. Line 8 calls Algorithm 3 to handle the various cases for the addition of edges. Lines 10-13 process the deletion of edges. The updating of friend lists is done in Line 10 and the tie strengths are recomputed in lines 11 and 12. Line 13 calls Algorithm 4 to handle cases for the deletion of edges. The scores of the affected nodes are updated in lines 14-18. Finally, we return the top k nodes with the highest scores.

We enumerate the cases that may arise due to the insertion of an edge. The cases are illustrated using the network in Figure 8(a). A1. New group formed.
 ; u ; u 3 )
Algorithm 2: WeakTie-Bi A2. Strengthen group.
 A3. Merge groups.
 Similarly, we enumerate the cases when an edge is removed. D1. Remove group.
 D2. Weaken group.
 D3. Split group.
 We now analyze the efficiency of WeakTie-Local and WeakTie-Bi algorithms. Let d denote the degree of a node, where 0 d j
V j . Given that real world social networks are sparse, we have d  X  X  V j for most of the nodes. In WeakTie-Local, when an edge ( u; v ) is added or deleted, it takes O ( d 2 ) to find the local groups again. Hence, the time needed to recompute the scores for u and v is O ( d 2 ) . For a neighbor w of u only, it needs to recompute the closeness between each group and update the score , and this takes O ( j C w j ) . For the common neighbors of u and v , WeakTie-
Algorithm 3: AddCases (e)
Algorithm 4: RemoveCases(e) Local needs to run the strongly connected group detection proce-dure in the neighborhood whose time complexity is O ( d 2 node. We have at most d such nodes, hence the time complex-ity for WeakTie-Local is O ( d 3 ) . In WeakTie-Bi, when ( u; v ) is added, updating the scores for u and v takes O ( j C u j ) and O ( respectively since we need to recompute the closeness with each local group. Similarly for the a friend w of u or v , each friend takes O ( j C w j ) time to update its score. This requires a time complexity of O ( d j C w j ) as we have d friends. When ( u; v ) is deleted, both cases D1 and D2 require O ( d j C j ) complexity. For case D3, the complexity is O ( d 3 ) as we have d nodes to apply depth-first search to find local groups again in their neighborhood.
In this section we evaluate the effectiveness and efficiency of the weak tie based algorithms in identifying brokers. We also study how brokers can help information diffusion tasks. All the algo-rithms are implemented in Python 2.7, and the experiments are run on a Windows 7 PC with CPU 3.4GHz and 8 GB memory.

Datasets. We use two real world datasets, DBLP 1 and Twitter The DBLP dataset has 815,946 authors which form the nodes in the network. We extract a subset of authors from 4 research areas in computer science, namely Databases (DB), Information Retrieval (IR), Artificial Intelligence (AI) and Networks and Communication (NC). The resulting dataset has 219,815 authors and 1,089,793 co-authorships. The longest distance between any two authors is 20 and the average distance is 7.06.

The Twitter dataset contains 1.8 million users, including the ac-tive users in July 2009 and their neighbors. We prune out the ac-counts that have no posts and select the largest connected com-ponent, which consists of 542,798 users and 98,380,438 follow-relationships. The diameter is 59, and its average distance is 4.65. Table 1 summarizes the basic statistics of datasets.

In this set of experiments, we evaluate the quality of the solutions given by the following methods:
We run WeakTie, PageRank  X  and Betweenness on both DBLP and Twitter datasets. Since DBLP is a bidirectional network while Twitter is a unidirectional network, we apply WeakTie-Bi on DBLP dataset and WeakTie-Local on Twitter dataset. We vary k from 100 to 700 and compute the utility function  X  values obtained (see Equation 1).

Figure 10 shows that WeakTie is able to obtain the highest  X  values for all k . WeakTie-Bi and WeakTie-Local suffer a slight decrease in quality as they only consider the local views of the af-fected nodes, and may not obtain the true picture of the connected-ness among their friends. We observe that the Betweenness method performs well when k is small. This is because when we remove the nodes that have many shortest paths passing through them, the aver-age distance between nodes will increase. However, as k increases, many of the nodes have alternate paths and the performance of the Betweennesss method decreases. Note that PageRank  X  performs worse in Twitter than in DBLP. This is because a user with many followers tends to have high PageRank  X  score. However, this user may not have any followee. As such, he/she ends up at the end of a path, and will not contribute much to the reduction of the average distance among other users.
Next, we examine the effect of on the performance of the pro-posed methods. We set k = 500. Figure 11 shows the results. We http://aminer .org/billboard/structural-hole.html http://an.kaist.ac.kr/traces/www2010.html observe the performance of the proposed algorithms is dependent on . For the Twitter dataset, both methods perform best when = 0 : 125 . For the DBLP dataset, the best performance is achieved when is around 0 : 5 . Clearly, is dependent on the overall con-nectedness of the users in the social network.

For Twitter-like network, users tend to follow a large set of users and the proportion of overlapping friends tends to be small. As a result, a small value of for the Twitter dataset is preferred. On the other hand, in the DBLP dataset, each paper has only a small number of co-authors resulting in relatively larger tie strengths, and hence a larger value of is appropriate. If is too large/small, al-most all edges will be characterized as weak/strong ties, our pro-posed algorithms will not work well as they would not be able to distinguish between weak and strong ties. In all our experiments, we set as the average tie strength in the entire social network, which is 0.172 for Twitter and 0.486 for DBLP.
We also compare the scalability of WeakTie, and incremental al-gorithms WeakTie-Local and WeakTie-Bi. We use the larger Twit-ter dataset for this experiment. Since WeakTie-Bi requires undi-rected graph, we convert each directed edge ( u; v ) in the Twitter dataset to an undirected edge by adding the edge ( v; u ) if it does not exist. We start with an initial size of 100k nodes and increase the dataset size by 100k nodes as well as their associated edges at each time step.

Figure 12 shows that WeakTie is the slowest since WeakTie has to re-run the algorithm on the entire graph when updates arrive. On the other hand, both WeakTie-Local and WeakTie-Bi only look at the local view of the affected nodes to handle the new updates, hence they run much faster than WeakTie as the dataset size in-creases. Furthermore, WeakTie-Bi eliminates the need to call Tar-jan X  X  algorithm in most cases and achieves the best performance.
In this set of experiments, we demonstrate the usefulness of bro-kers in diffusing information across communities. Lou and Tang [14] define structural hole spanners as users who have connections with different communities, and propose two models, MaxD and HIS to find the structural hole spanners. Given a set of communi-ties C , MaxD iteratively finds a node such that removing this node leads to the largest reduction in the minimum cut of the communi-ties in C . The first k nodes that are removed are the top-k spanners. In the HIS model, each user is assumed to have an importance score in each community. The structural hole score is the minimum value of a node X  X  importance score in the different communities. The k nodes with highest structural hole scores are the spanners.
We adapt our WeakTie methods to detect the top-k spanners by finding the nodes that have connections to different communities with the highest WeakTie scores. Note that the spanners are a sub-set of the brokers we identify.
In the DBLP dataset, authors who have served as program com-mittee members in conferences of different areas are considered as spanners [14]. Figure 13 shows the precision of the different ap-proaches on this dataset. We observe that WeakTie significantly outperforms MaxD (+35%). Betweenness performs poorly, indi-cating spanners do not necessarily carry large numbers of shortest paths. HIS and PageRank  X  perform similarly as they both look for nodes that are incident with authority nodes but spanner authors do not merely establish cooperation with opinion leaders in different areas. WeakTie-Local performs slightly worse than WeakTie since it utilizes only the strongly connected groups in the local view.
For the Twitter dataset, since we do not have the ground truth, we examine the information diffusion paths instead. We divide Twit-ter dataset into 4 communities using Girvan-Newman X  X  community detection method [16]. Let P be the set of paths that involve at least two users in different communities. Suppose S is the set of span-ners found by a method. Let P S P be the set of paths involving users in S . We define the coverage of a method as:
A high coverage indicates that more information is being dif-fused across communities by the detected spanners. Figure 14 shows the coverage of the various methods in the Twitter dataset. We observe that WeakTie has the best performance, with WeakTie-Local being a close runner-up. On average, WeakTie outperforms the state-of-the-art method MaxD by more than 20% .
Next, we discuss how top-k brokers can be used in mention rec-ommendation to expand the spread of tweets. By adding  X  X user-name X  in a tweet, the system will push the post to the mentioned user whose retweets could enable this post to reach more users. Note that for this mention recommendation task, since we need the retweet cascades as ground truth, we keep only tweets that have been retweeted more than 5 times. This prunes the Twitter dataset to 27,754 users, 1,285,097 edges and 35,480 paths.
The state-of-the-art mention recommendation method is WTM [22]. Given a tweet posted by a user u , WTM will rank the other users based on how well their interests match u  X  X  post, their inter-action with u and how influential they are. In order to demonstrate how brokers are also useful in mention recommendation, we incor-porate the WeakTie score into WTM X  X  model of ranking the users. We call this model WTM-WeakTie.

We evaluate the effectiveness of WTM and WTM-WeakTie by the number of users reached using the top-k ranked users. The evaluation measure in [22] sums up the number of followers of the users in a cascade initiated by a recommended user regardless of whether these followers are duplicates. Based on this measure, we found that each tweet reaches 589 users on average for our Twit-ter dataset. However, if we consider only the distinct followers, each tweet actually reaches only 364 users on average. In order to present a more accurate picture of the actual number of peo-ple reached, we define the following measure numReached which counts only the distinct followers.

Let L be the set of users who actually retweet, R u denote the set of users involved in the retweet cascades initiated by a user u , and F ollower ( w ) denote the set of w  X  X  followers. We have:
Figure 15 shows the results of WTM and WTM-WeakTie. We observe that after incorporating the WeakTie score, we have in-creased the number of users reached by 23% because brokers can help diffuse the tweet to users located far away from the author and initiate a cascade with more distinct users.

We further analyze the recommended users X  ability to reach dif-ferent parts of the network. We divide the dataset into 100 com-munities. Here we divide the dataset into smaller communities to avoid many retweets X  cascades traveling within one big commu-nity. We consider that a user is able to reach a certain community if he/she has a follower from this community. Figure 16 shows the average number of communities the users in the recommendation list can reach. We observe that when k &gt; 1 , WTM-WeakTie can reach many more communities than WTM, indicating brokers are capable of broadcasting information to a wider range.
There has been much interest in social network analysis from both the sociology and computer science research communities. Sociologists have identified users who bridge otherwise discon-nected, isolated or remote individuals as brokers and empirically verify their role in diffusing information [3, 19].

At the same time, computer scientists have tried to identify users who play significant roles in diffusing information in online social networks [4, 10]. The work in [10] analyzes different information diffusion models and identifies a set of influential users who can reach the maximum number of users. Extensions of their work include personalization [7] and scalability [13] of influence maxi-mization algorithms. Other works employ centrality measures such as PageRank [11] or betweenness [8] to identify users that act as information sources and hence play an important role in dissemi-nating information.

Tong et al. [21] finds the set of users whose removal would slow down the propagation of information in a network. They assume that information propagation follows some epidemic model in the form of a matrix whose first eigenvalue corresponds to the speed of propagation. Based on this, they iteratively remove a node at a time to minimize the first eigenvalue. Their method only applies to the specific diffusion model and does not show how these users are placed in the network.

Luo et al. [15] and Cui et al. [5] focus on identifying influential retweeters. They examine the author X  X  followers X  retweet history, active time and interest to identify influential retweeters who are able to accelerate the diffusion of information. However, the scope of influence of these retweeters is typically limited to a cluster of friends and do not try to reach remote users. In contrast, the brokers we have identified in this work are able to diffuse information to a larger pool of users, and to different parts of the network.
The work in [14] looks for structural hole spanners who can bridge the gap between different communities, thus enabling the diffusing of information across communities. However, the study in [12] shows that the best-expressed communities in real world networks are rather small, with only hundreds of nodes. Therefore, focusing only on the users between well-expressed communities is not practical for real world networks. In our work, we study the problem in a more general setting without the need of known com-munities. To the best of our knowledge, this paper is the first work that addresses the problem of finding top-k brokers in large online social networks.
In this paper, we have defined the problem of detecting brokers in social networks, and proved that the problem is NP-hard. We have also designed two incremental algorithms WeakTie-Local and WeakTie-Bi to deal with evolving social networks. Experiments on two real world datasets demonstrate the effectiveness and effi-ciency of the algorithms. We have also shown that utilizing bro-kers can improve the precision of spanner detection by 35% and help the mention recommendation task to reach 23% more distinct users. Future work includes applying the identified brokers in user recommendation and link prediction tasks. [1] Y. Asahiro, R. Hassin, and K. Iwama. Complexity of finding [2] E. Bakshy, I. Rosenn, C. Marlow, and L. Adamic. The role of [3] R. S. Burt. Secondhand brokerage: Evidence on the [4] S. Cheng, H. Shen, J. Huang, W. Chen, and X. Cheng. [5] P. Cui, F. Wang, S. Liu, M. Ou, S. Yang, and L. Sun. Who [6] M. Granovetter. The Strength of Weak Ties. The American [7] J. Guo, P. Zhang, C. Zhou, Y. Cao, and L. Guo. Personalized [8] M. Haghir Chehreghani. An efficient algorithm for [9] E. Katz. The two-step flow of communication: An up-to-date [10] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the [11] O. Kurland and L. Lee. Respect my authority!: Hits without [12] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Mahoney. [13] Q. Liu, B. Xiang, E. Chen, H. Xiong, F. Tang, and J. X. Yu. [14] T. Lou and J. Tang. Mining structural hole spanners through [15] Z. Luo, M. Osborne, J. Tang, and T. Wang. Who will retweet [16] M. E. Newman. Fast algorithm for detecting community [17] L. Page, S. Brin, R. Motwani, and T. Winograd. The [18] A. Silva, S. Guimar X es, W. Meira, Jr., and M. Zaki. [19] K. Stovel and L. Shaw. Brokerage. Annual Review of [20] R. Tarjan. Depth-First Search and Linear Graph Algorithms. [21] H. Tong, B. A. Prakash, C. Tsourakakis, T. Eliassi-Rad, [22] B. Wang, C. Wang, J. Bu, C. Chen, W. V. Zhang, D. Cai, and
