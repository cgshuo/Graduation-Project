 We propose a General Markov Framework for computing page importance. Under the framework, a Markov Skeleton Process is used to model the random walk conducted by the web surfer on a given graph. Page importance is then defined as the product of page reachability and page utility ,which can be computed from the transition probability and the mean staying time of the pages in the Markov Skeleton Pro-cess respectively. We show that this general framework can cover many existing algorithms as its special cases, and that the framework can help us define new algorithms to handle more complex problem s. In particular, we demonstrate the use of the framework with the exploitation of a new process named Mirror Semi-Markov Process . The experimental re-sults validate that the Mirror Semi-Markov Process model is more effective than previous models in several tasks. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.5.4 [ Information Interface and Presentation ]: Hypertext/Hypermedia.
 Algorithm, Experimentation, Theory Page importance, PageRank, BrowseRank, general Markov framework, mirror semi-Markov process.
Page importance plays a key role in crawling of Web pages, indexing of the crawled pages, and ranking of the indexed pages. Many effective algorithms have been proposed to compute page importance in the literature, such as PageR-ank [6], TrustRank [1], and BrowseRank [4]. However, there arestillchalle ngeswhichcannotbewellhandle dbythese algori thms.(1)Insomenewscenarios ,theassumptio nsin existing algorithms may not hold. In the mobile Web, ow-ing to the specific business model, the owner of a website tends to create more hyperlinks to the pages of his own or his partners, than those of other websites. As a result, the topological property of the mobile web graph is signif-icantly different from the general web [3]. Many links in it are not preferential attachments, but profit-oriented at-tachments. In this case, the page importance computed by algorithms like PageRank may not reflect the true impor-tance of the pages. (2) In some existing applications, the assumptions in existing algorithms may not be accurate ei-ther. In BrowseRank [4], basically it trusts the user behavior data, and estimates the page importance from it. However, when there are click frauds, the data may not be trustwor-thy. Suppose a webmaster puts an online advertisement on his homepage. In order to earn money, he may click the link of the advertisement artificially or by a robot to increase the frequency of visits. As a result, we will observe a large vol-ume of transitions from his homepage to the advertisement. If we do not distinguish the sources of the transitions when estimating the staying time on the advertisement page, the estimation may be highly biased by these fraudulent clicks. In this case, the page importance computed by BrowseRank may not be accurate.

With these challenges lying ahead, it may be necessary to develop new technologies to address the problems in both existing and new scenarios. Also, it is helpful to study whether there is a common theory behind the existing al-gorithms, and whether the theory can lead to some general guidelines for designing new algorithms. For this purpose, we propose using a general Markov framework as the unified description of the page importance computation algorithms. In the framework, we consider how to model page impor-tance from the viewpoint of random surfer, assuming that there is a web link graph or a user browsing graph available. In this setting, the importance of a page means the value that the page can eventually provide to the random surfer. It can be considered that there are two factors that affect page importance: page reachability and page utility . Markov Skeleton Process can represent these two factors with tran-sition probability and mean staying time. We can then take the product of transition probability and mean staying time as page importance. In many cases, it can be proved that the product is proportional to the stationary distribution of the Markov Skeleton Process, if the distribution exists.
Existing algorithms such as PageRank and BrowseRank can be well covered by the framework. Furthermore, the general framework also provides us a guideline of designing new algorithms. We can attain new methods by defining the graph on new data sources, employing new family members of the Markov Skeleton Process, or developing new methods to estimate transition probability and mean staying time. To demonstrate the use of this framework, we propose em-ploying a new process named Mirror Semi-Markov Process . In the new process, the staying time on a page depends on both the current page and the previous pages visited by the surfer. By doing so, we can address the aforementioned is-sues that existing algorithms suffer from. We tested the Mir-ror Semi-Markov Process and the corresponding algorithms on both the mobile web graph and the user browsing graph. The experimental results show that the new algorithms can outperform existing methods in several tasks such as top ranked page finding and spam/junk page filtering. This well validates the effectiveness of the proposed framework.
We assume that there is a web surfer performing random walk on the web graph 1 . The importance of a page can be viewed as the value that the page provides to the ran-dom surfer during her entire surfing process. Note that a visit of a page by the surfer is random, and the value which a page can offer to the surfer in one visit is also random. Therefore, there are two factors that can affect page impor-tance: page reachability represents the (average) possibility that the surfer arrives at the page, and page utility repre-sents the (average) value that the page gives to the surfer in a single visit. Page reachability is mainly determined by the structure of graph, while page utility can be affected by several things like the content of the page or the pages the surfer visited before. That is, page utility may depend on not only the current page but also other related pages. Markov Skeleton Process (MSP) is a stochastic process Z defined as follows 2 . Suppose that X is a Markov Chain with state space S and transition probability matrix P .Let x ,x 2 ,  X  X  X  ,x  X  ,  X  X  X  denote a sequence of X ,where x  X  is a state and x  X  +1 is determined by the probability distribution P ( x  X  +1 | x  X  ), (  X  =1 , 2 ,  X  X  X  ). Further suppose that Y is a stochastic process on the positive real-number set R + .Let y ,y 2 ,  X  X  X  ,y  X  ,  X  X  X  denote a sequence of Y ,where y  X  , (  X  = 1 , 2 ,  X  X  X  ) is a positive real-number. Suppose that there are S ,S 2 ,  X  X  X  ,S  X  ,  X  X  X  ,and S  X   X  S, (  X  =1 , 2 ,  X  X  X  ). Y is deter-mined by the probability distribution P ( y  X  | S  X  ) , X  =1 , 2 , Then, Markov Skeleton Process Z is a Stochastic Process based on X and Y .Asequenceof Z can be represented and y  X  denotes staying time at state x  X  ,(  X  =1 , 2 ,  X  X  X 
The web graph can be web link graph or user browsing graph. In the former, each node in the graph represents a web page, and each edge represents a hyperlink between two pages. In the latter, each node in the graph stands for a web page, and each edge stands for a transition between pages. The transition information can be obtained by aggregating behavior data of billions of web users [4].
Note that we try to provide an intuitive definition here. A more rigorous definition can be found in [2]. x  X  depends on x  X   X  1 and y  X  depends on multiple states S (  X  =1 , 2 ,  X  X  X  ).

MSP is a very general stochastic process. It even does not necessarily have a stationary probability distribution. In some of its special cases, however, stationary distribution exists. Many existing stochastic processes are special cases of MSP. Some examples are Semi-Markov Process (when y  X  only depends on x  X  and x  X  +1 according to distribution P ( y  X  | x  X  ,x  X  +1 )), Continuous-Time Markov Process (when y  X  only depends on x  X  following an exponential distribu-tion P ( y  X  | x  X  )), and Discrete-Time Markov Process (when y  X  is constant). Furthermore, Mirror Semi-Markov Process, proposed in Section 3.1, is also a special case of MSP. MSP can naturally model the random walk of web surfer. Suppose that Z is an MSP and ( X, Y ) is the two stochastic processes associated with Z . Let states in MSP correspond to web pages. The random surfer randomly chooses the next page to visit based on the current page, according to X . She further randomly decides the length of staying time on the current page based on the page, and several other pages she visited before, and/or several other pages she will visit, according to Y . Therefore, the aforementioned two factors are characterized by the two quantities in MSP. Specifically, transition probability represents page reachability, and mean staying time represents page utility.

Furthermore, we can define page importance as the prod-uct of transition probability and mean staying time. Note that there might be other forms for page importance besides the product defined above. The reason we use this definition is that we can prove the product is proportional to the sta-tionary distribution of the Markov Skeleton Process if the process has a stationary distribution.

The framework can cover many existing algorithms like [6,5,1,4]. Moreover,MSPdoesnotmakeanyspecific assumption on the distribution P ( y  X  | S  X  ). That is, staying timecanbeassumedtodependonalargenumberofother states. Therefore, there is a large room for people to develop new algorithms based on this general process.
We give a showcase of the proposed framework on dealing with new problems of page importance computation, by em-ploying new family members of the Markov Skeleton Process and using new methods to estimate the two factors.
Definition 1. In Markov Skeleton Process Z ,if Y is a stochastic process for which y  X  depends only on x  X   X  1 and x Markov Process (MSMP).
 MSMP 3 is a special case of Markov Skeleton Process. The Markov Chain X in MSMP with transition probability ma-trix P is called Embedded Markov Chain (EMC) of MSMP. MSMP is similar to Semi-Markov Process (see Section 2.1). In Semi-Markov Process, y  X  depends on the current state x  X  and the next state x  X  +1 , while in MSMP y  X  depends on the current state x  X  and the previous state x  X   X  1 .The dependencies are in two opposite directions. That is why we call the new model Mirror Semi-Markov Process. Input : Web graph and metadata.

Output : Page importance score  X  1. Generate transition probability matrix P of EMC 2. Calculate stationary distribution  X   X  of EMC using 3. For each page j , identify its inlink websites and inlink 4. For each page j , estimate parameter  X  jk from 5. Calculate mean staying time  X  t j for each page j 6. Compute page importance for web graph with (1). Let  X   X  denote the stationary distribution of EMC X .We have  X   X  =  X   X P .Here  X   X  can be calculated by the power method. We use j to represent a state ( j  X  S ) and use t to represent staying time on state j . Suppose that p ( t is the corresponding probability density function on staying time. Then the mean staying time on state j is defined as  X  t j E ( t j )=  X  0 t j p ( t j ) dt j . We further define the following distribution using  X   X  and  X  t j .
It can be proved that the stationary distribution for MSMP exists and the stationary distribution is exactly that in (1).
Given a web graph and its metadata, we build an MSMP model on the graph. We first estimate the stationary dis-tribution of EMC and view it as transition probability. We next compute the mean staying time using the metadata. Finally, we calculate the product of transition probability and mean staying time, which is actually the stationary dis-tribution of MSMP. We regard it as page importance. Ta-ble 1 gives the detailed algorithm for creating Mirror Semi-Markov Process. As the transition probability can be con-veniently computed by power method, we will focus on the staying time calculation in the next subsection. Suppose that for page j there are n j pages linked to it:  X  that the surfer comes to page j from page  X  jh ,thenwe have n j h =1 p (  X  jh ) = 1. As probability p (  X  jh for the contribution of page  X  jh to page j , we refer to it as contribution probability in this paper. Suppose that the n j inlinks of page j are from m j websites, and from web-site k ( k =1 , ..., m j )thereare n jk inlinks. Thus we have n to might also exist in the m j websites.

Suppose that the m j sites that linked to page j are:  X  j {  X  j 1 , X  j 2 , ...,  X  jm j } ,and p (  X  jk ) is the probability that the surfer comes to page j from site k , referred to as construction probability of the site. Then we have m j k =1 p (  X  jk )=1and
Let t j be the random variable of staying time on page j and p ( t j ) be the probability density function on it. Then ing time  X  t j on page j can be calculated as 4  X  t j E ( t
Here we assume that staying time t j follows an exponen-tial distribution in which the parameter is related to both page j and website k , i.e., p ( t j |  X  jk )=  X  jk e  X   X  more, we assume p ( t j |  X  jl )= p ( t j |  X  jk ) ,l =1 , ..., n staying time depends on page j and the website of inlink k , not the inlink page itself.

Combining the above discussions, we can calculate the mean staying time  X  t j as
The question then becomes how to calculate the contri-bution probabilities from different sites p (  X  jk ) and to esti-mate the parameters from different sites  X  jk .Ifwehave enough observations of staying times, we can estimate the parameters  X  jk ,k =1 , ..., m j . In other cases (insufficient ob-servations or web link graph), we can employ heuristics to calculate mean staying times. For contribution probabilities p (  X  jk ), we can also use heuristics to calculate them.
As explained, BrowseRank employs user browsing graph and Continuous-Time Markov Process. The biggest chal-lenge for the algorithm is click fraud, because it trusts the user behavior data, and directly estimates the mean stay-ing time from user behavior data. BrowseRank Plus ad-dresses the problem by using MSMP. It calculates the mean staying time of a page on the basis of its inlinked web-sites. Specifically it partitions the samples of observed stay-ing time according to inlink websites and estimate param-eters  X  jk , ( k =1 , ..., m j ). We can use the same method in BrowseRank to estimate the parameters  X  jk from par-titioned samples. Furthermore, BrowseRank Plus sets the contribution probability p (  X  jk ), also based on inlink web-mean staying time using (2). Therefore, if the inlink web-sites are different, then the mean staying times from them will also differ.
To tackle the problems described in Section 1, we pro-pose a new algorithm called MobileRank for computing page importance on mobile web using MSMP. We actually con-sider a new way of calculating mean staying time. Note that in MSMP implementation we assume that staying time depends on not only the current page but also the inlink website, that means that MSMP has the ability to repre-sent relation between websites and to utilize the informa-tion for promoting or demoting staying time (utility) of page. Specifically, if the inlink is from a partner website, then we can demote the staying time of visits from the website. Specially, we define the contribution probability p (  X  jk ) in the same way as in BrowseRank Plus. We heuris-tically calculate the parameter  X  jk . Suppose that for page j there is an  X  X bserved X  mean staying time 1  X  j .  X  jk is as-sumed to follow a partnership-based discounting function L jk , i.e., 1  X  jk = L jk ( 1  X  j ). The discounting function can have
In practice, if a page does not have any inlinks, then it can be assigned the minimum mean staying time. Table 2: Top 20 websites by different algorithms different forms for different business relations between web-sites. For example, we use a Reciprocal Discounting function we can calculate the mean staying time in MobileRank as  X  t
In order to validate the effectiveness of the proposed gen-eral framework, we conducted experiments to test the perfor-mances of the proposed algorithms (BrowseRank Plus and MobileRank) on two specific issues, important websites find-ing and spam/junk sites filtering. We used exactly the same datasets and settings as [4]. The top-20 websites ranked by using different algorithms are listed in Table 2. For ease of reference, we denote PageR-ank, TrustRank, BrowseRank, and BrowseRank Plus as PR, TR, BR, and BR+. From this table, we can see that: (1) similarly to BR, BR+ ranks Web 2.0 sites (marked in bold) high and ranks those sites low which have large number of inlinks in the link graph but small number of visits. In this regard, BR+ better reflects users X  preference than TR and PR; (2) As compared to BR, BR+ can demote the sites with high local transitions, such as google.co.th . Study shows that most of the transitions are from local websites in Thailand, and the number of transitions from websites in other places is small. In this case, BR gets a large mean staying time for this website because it does not distinguish the contributions from different inlink websites. In BR+, however, different inlink websites are treated differently, and thus the influence of large number of transitions from the same website can be effectively reduced. In this way, the rank of google.co.th is effectively demoted.

We randomly sampled 10,000 websites from 5.6 million websites and asked human experts to conduct spam judg-ments on the websites. 2,714 websites are labeled as spam and the rest are labeled as non-spam. We use the spam bucket distribution to show the performances of the algo-rithms. Given an algorithm, we sorted the 5.6-million web-sites in the descending order of their scores given by the algorithm. Then we put these sorted websites into fifteen buckets. The numbers of labeled spam websites over the buckets for different algorithms are listed in Table 3. We see that among all the algorithms, BR+ pushes the largest number of spam websites to the tail buckets.
 Table 3: Number of spam websites over buckets
We used a mobile web graph collected from a Chinese mobile search engine to conduct the experiments. It was crawled in October 2008, containing about 80% Chinese pages and 20% pages in other languages. The graph con-tains 158-million webpages and 816-million hyperlinks. We studied the basic properties of the graph and found that they have similar tendencies as those reported in [3]. We took PageRank on the graph as the baseline, and applied MobileRank (denoted by MR).

We randomly sampled 1,500 pages from the mobile web graph and asked human judges to label them as junk page or non-junk page. As a result, 441 pages are labeled as junk and the rest are non-junk pages. We also use the bucket dis-tribution to show the performances of the algorithms. The numbers of the labeled junk pages over buckets are listed in Table 4. We can see that MR can produce much better results than PR in demoting junk pages to the tail buckets.
In this paper, we have proposed a general Markov frame-work for page importance computation. In this framework, the Markov Skeleton Process is employed to model the ran-dom walk by the web surfer. The framework can cover many existing algorithms as its special cases, and can also provide us with a powerful tool in designing new algorithms.
