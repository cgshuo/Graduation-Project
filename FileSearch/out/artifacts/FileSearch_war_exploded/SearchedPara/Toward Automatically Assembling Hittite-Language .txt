 The Hittite empire, in existence for about 600 years between 1800 and 1200 BCE, left numerous histori-cal, political, and literary documents behind, written in cuneiform in clay tablets. There are a number of common problems that confront Hittite scholars in-terested in any subdiscipline of Hittitology, be it his-tory, philology, or linguistics. Horst Klengel sum-marizes the issue most crucial to this paper:
Figure 2 shows a published join in hand-copied cuneiform fragments. In this case, the fragments are not contiguous, and only the text on the two frag-ments was used to make the join.

The task then, for the purposes of this paper, is to connect unknown fragments of Hittite cuneiform tablets with larger texts. I X  X  viewing this as a text classification task, where larger, CTH-numbered texts are the categories, and small fragments are the bits of text to be assigned to these categories. Hittite cuneiform consists of a mix of syllabic writ-ing for Hittite words and logographic writing, typ-ically Sumerian ideograms, standing in for Hittite words. Most words are written out phonologically using syllabic signs, in structure mostly CV and VC, and a few CVC. Some common words are written with logograms from other Ancient Near Eastern languages, e.g. Hittite antuh  X  sa- X  X an X  is commonly written with the Sumerian-language logogram tran-scribed L  X  U. Such writings are called Sumerograms or Akkadograms, depending on the language from which the ideogram is taken.

The extant corpus of Hittite consists of more than 30,000 clay tablets and fragments excavated at sites in Turkey, Syria, and Egypt (Hoffner and Melchert, 2008, 2-3). Many of these fragments are assigned to one of the 835 texts catalogued in the CTH. A large number of prior studies on text classifica-tion have informed the progress of this study. Cat-egorization of texts into genres is very well studied (Dewdney et al., 2001). Other related text classi-fication studies have looked at classifying text by source, in contexts of speech, as in an attempt to classify some segments of speech into native and non-native speaker categories (Tomokiyo and Jones, 2001), and writing and authorship, as in the fa-mous Federalist Papers study(Mosteller and Wal-lace, 1984), and context, as in a categorization of a set of articles according to which newspaper they appeared in (Argamon-Engelson et al., 1998).
Measures of similarity among sections of a single document bear a closer relation to this project than the works above. Previous studies have examined in-ternal document similarity, using some vector-based metrics to judge whether documents maintain the same subject throughout (Nicholson, 2009).

Very little computational work on cuneiform lan-guages or texts exists. The most notable example is a study that examined grapheme distribution as a way to understand Hurrian substratal interference in the orthography of Akkadian-language cuneiform texts written in the Hurrian-speaking town of Nuzi (Smith, 2007). Smith X  X  work, though using different classifying methods and and an enormously differ-ent corpus on a language with different characteris-tics, is the most similar to this study, since both are attempts to classify cuneiform fragments into cat-egories -in Smith X  X  case, into Hurrian-influenced Nuzi Akkadian and non-Nuzi standard Akkadian. For this project, I use a corpus of neo-Hittite fragment transcriptions available from H. Craig Melchert (Melchert, ). The corpus is one large text file, divided into CTH numbered sections, which themselves are divided into fragments labeled by their publication numbers -mostly KUB, which stands for Keilschrifturkunden aus Boghazk  X  oi or KBo, Keilschrifttexte aus Boghazk  X  oi , the two major publications for Hittite text fragments.

I restricted the fragments used in this project to fragments belonging to texts known to exist in at least two copies, a choice that produces a larger number of fragments per text without requiring a judgment about what number of fragments in a text constitutes  X  X ragmented enough X  for a legitimate test of this task. This leaves 36 total CTH-numbered texts, consisting of 389 total fragments.

The fragments themselves are included as plain text, with restorations by the transcribers left intact and set off by brackets, in the manner typical of cuneiform transcription. In transcription, signs with phonemic value are written in lower case characters, while ideograms are represented in all caps. Sign boundaries are represented by a hyphen, indicating the next sign is part of the current word, by an equals sign, indicating the next sign is a clitic, or a space, indicating that the next sign is part of a new word. {KUB XXXI 25; DS 29} x splitting the data randomly into ten parts, and using 9 parts of the data as a training set and 1 part of the data as a test set. This means that each set was tested ten times, with all of the data eventually being used as part of the testing phase. Accuracy values from the classifiers using the Plain corpus, and from the corpus with the Brackets Re-moved, are presented in Tables 1 and 2, respec-tively. The measures are raw accuracy, the fraction of the test fragments that the methods categorized correctly.
 The results for the Plain Corpus show that the Naive Bayes classifier was 55% accurate with all to-kens, and 44% accurate with ideograms alone. The Maximum Entropy classifier was 61% accurate with all tokens, and 51% accurate with ideograms only.
Both classifiers performed better with the Brack-ets Removed corpus. The Naive Bayes classifier was accurate 64% of the time with all tokens and 49% of the time with ideograms only. The Maximum En-tropy classifier was 67% accurate with all tokens, and 54% accurate with ideograms only.

The predicted increase in accuracy using ideograms was not upheld by the above tests. It may be the case that Sumerograms and Akkadograms are insufficiently frequent, particularly in smaller fragments, to allow for correct categorization. Some early tests suggested occasional excellent results for this tokenization scheme, including a single random 90-10 training/test run that showed a test accuracy of .86, much higher than any larger cross-validated test included above. This suggests,
