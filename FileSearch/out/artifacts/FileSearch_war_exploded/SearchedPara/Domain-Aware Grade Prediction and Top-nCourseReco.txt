 Automated course recommendation can help deliver person-alized and e  X  ective college advising and degree planning. Nearest neighbor and matrix factorization based collabora-tive filtering approaches have been applied to student-course grade data to help students select suitable courses. How-ever, the student-course enrollment patterns exhibit group-ing structures that are tied to the student and course aca-demic features, which lead to grade data that are not miss-ing at random (NMAR). Existing approaches for dealing with NMAR data, such as Response-aware and context-aware matrix factorization, do not model NMAR data in terms of the user and item features and are not designed with the characteristics of grade data in mind. In this work we investigate how the student and course academic fea-tures influence the enrollment patterns and we use these features to define student and course groups at various lev-els of granularity. We show how these groups can be used to design grade prediction and top-n course ranking models for neighborhood-based user collaborative filtering, matrix fac-torization and popularity-based ranking approaches. These methods give lower grade prediction error and more accu-rate top-n course rankings than the other methods that do not take domain knowledge into account.
 Grade Prediction, Top-n Course Ranking, Multi-Granularity Grouping
While the flexibility of degree requirements provides col-lege students with ample choices, it can complicate course selection. From among the courses that the students are el-igible to take in the next term, they need to select the ones that they like, they are expected to perform well in, and also satisfy their degree requirements. E cient college ad-vising is essential for helping students select the right courses and thus, maintain high student retention rates and timely graduation. Automated course recommendation can help improve college advising by recommending courses that are suitable for the students degrees. Moreover, predicting stu-dent grades in the next term can help students and educators make informed decisions about course enrollments in order to produce better learning outcomes.

Collaborative filtering approaches have been previously used for grade prediction and course recommendation [23, 6, 8]. The majority of these methods rely on user-based col-laborative filtering (User-CF) [11] which makes recommen-dations by relating to the courses that were taken by similar students. Recently, techniques based on matrix factorization (MF) have been used for movie and product recommenda-tions [14] and also applied for course recommendation and grade prediction [27, 26].

The grade data has special characteristics as the student-course enrollments are influenced by the academic features (e.g., student majors, academic levels and course subjects). Consequently, the student-course grade matrix exhibits group-ing structures as students with certain majors tend to en-roll in courses of certain subjects, resulting in not missing at random (NMAR) data. Response-aware MF uses miss-ing data theory to model the NMAR user response patterns [16]. However, the response patterns are not tied to the user and item features. Features-based MF methods can incorporate the user and/or item features into the predic-tion model. However, they do not explicitly model how the features determine the grouping structures in the data.
In this paper we analyze grade data and show how the student and course academic features determine the enroll-ment patterns. We use these features to define student and course groups and show how they can be incorporated in matrix factorization, user-based collaborative filtering, and popularity-based ranking.

We investigate various ways to define the groups at multi-ple levels of granularity using di  X  erent amounts of academic features. We show that in some cases the small sample sizes associated with finer granularity groups make the prediction models prone to poor generalization, especially with matrix factorization based methods. To overcome this issue, we build multiple models using the various granularity groups. We then generate multiple grade predictions and combine them based on the sample sizes associated with the various groups.
 We tested our methods on a dataset obtained from the University of Minnesota. The dataset spans 13 academic years and includes over 1,700,000 grades. Our results show that the methods that utilize finer groups give significantly more accurate top-n course rankings than the methods that utilize coarser groups and than other methods from the liter-ature; even when domain knowledge is used to pre-filter the recommended courses. Moreover, defining the groups using the academic features gives better top-n rankings than clus-tering the students and courses using the enrollment data. For grade prediction, utilizing the finer groups gives more ac-curate predictions than the coarser groups only when they are associated with reasonable sample sizes. For the matrix factorization methods, where model training su  X  ers more from the sample-size issue, combining the various model predictions while accounting for the sample sizes associated with the model parameters gives higher prediction accuracy than the various individual MF models and than the other methods from the literature.
Let G denote the grade matrix; each row in G represents a student, denoted by s , and each column represents a course, denoted by c . The entry g s,c in G represents the grade ob-tained by student s in course c . A predicted grade is denoted
In a university setting, each student enrolls in a certain college/school and declares a certain major. Each student selects from among a variety of courses to take in order to fulfill the requirements of his major (also referred to as the degree requirements). Each course has a subject that it falls under and a level that describes its di culty. As the student takes more courses, his academic level advances and he can take higher level courses.

Students tend to enroll in courses that are related to their majors and are appropriate to their academic levels. This is illustrated in Figure 1 which shows various characteristics that were extracted from a grade dataset that was obtained from the University of Minnesota. These plots show that: (i) each course is taken by students that belong to a lim-ited number of majors; (ii) each course is mostly taken by students belonging to one academic level; and (iii) each stu-dent takes courses that cover a limited number of subjects. For example, from Figure 1-left we can see that 22% of the courses are taken by students that all come from one ma-jor, and 80% of the courses are taken by students that come from at most 24 majors. From Figure 1-middle we can see that for 50% of the courses, at least 66% of the enrolled students belonged to the same academic level. Finally, from Figure 1-right we can see 90% of the students took courses that covered less than 20 subjects.

These characteristics imply that the missing entries in the grade matrix are not missing at random. This resulted be-cause students with certain student features tend to enroll in courses with certain course features. We refer to this as the grouping structures in the grade data.
Response aware techniques [16, 17, 12] model NMAR data by utilizing a data model that is based on missing data the-ory. The method proposed in [16] modified probabilistic matrix factorization by introducing two variations to model NMAR data. The first variation assumes that the probabil-ity of observing a rating depends only on the value of the rating. The second variation assumes that the probability also depends on the user and the item latent factors. None of these methods incorporate the user and item features that influence the response patterns.

Feature-based MF methods incorporate user and/or item features within the rating prediction or the top-n ranking models. The method proposed in [2] linearly transformed the user and item features to the latent space in order to predict a user X  X  preference over a given item. Other meth-ods incorporated the features within a top-n recommenda-tion model in order to estimate user preferences or bias the recommendations based on the item features [18, 10]. None of these methods were designed to address how the user and item features determine the grouping structures in the data.
Context-aware methods make recommendations in accor-dance with the di  X  erent contexts [4, 13, 3, 25, 28, 1]. Some of these methods utilized the context information to pre-filter items. Other techniques incorporated contextual informa-tion within the model.

Methods for course recommendation applied various data mining techniques to tackle the problem. The work done in [5] applied association rule mining to recommend relevant courses. The method in [15] estimated course recommenda-tion scores by accumulating weights for subject importance within the study field, satisfied prerequisites and the ex-tent by which a course broadens the student X  X  knowledge state. Methods for course recommendation with constraints focused on satisfying the degree program requirements [20, 22, 21, 19]. They take course prerequisites into consideration in order to generate valid course recommendations. They fo-cus on finding a short path to fulfill the degree requirements and thus, reduce time to graduation.
We develop methods that model the grouping structures of the grade data by using the academic features to de-fine student and course groups. These groups are defined at various levels of granularity by utilizing various amounts of features. Then they are incorporated within the recom-mendation methods for the purpose of performing (1) grade prediction and (2) top-n course ranking.

For grade prediction, the grade of a student s in a course c should be predicted by relating to how students of the same group as s performed in c , and how s performed in courses of the same group as c . Since the groups are de-fined at di  X  erent levels of granularity, various models can be built that account for various academic features. In general, the finer groups are more homogeneous and thus, utilizing them can give more accurate predictions than utilizing the coarser groups. However, based on how the groups are in-corporated into the prediction models, some models can be a  X  ected when the finer groups have small sample sizes and they can become prone to poor generalization. Such cases are addressed by building multiple models using di  X  erent granularity groups and combining the predictions of all the models based on the group sample sizes.

For top-n course ranking, it is required to generate a list of n relevant courses for each student to consider enrolling in them. Unlike other recommendation scenarios, course rec-ommendation has special considerations. Students need to enroll in courses that they are interested in, and that ful-fill their degree requirements. Accordingly, students some-times need to take some courses in order to fulfill some de-gree requirement, regardless of their expected grade in these courses. Moreover, in the typical user-item-rating scenario, when a user likes an item, he gives that item a high rating. This is not always the case with the student-course-grade scenario where a student might like a course, but this does not necessarily mean that he will get a high grade when he takes that course. Based on that, course ranking should rely on the enrollment patterns more than relying on the ex-pected grades. In this sense, the grade matrix is considered as binary where all grades are set to 1 X  X  and the rest of the entries are considered 0 X  X . Similar to grade prediction, mul-tiple models can be built by utilizing various student and course groups. Also, utilizing finer groups can give more accurate recommendations that the coarser groups. How-ever, unlike with grade prediction, the fact that some finer groups are associated with small sample sizes is an indicator for less relevant courses and as such, should not hurt model generalization.

We next describe how to define the multi-granularity groups.
The student groups define the various student subpopu-lations that can take a course. At the coarsest level, the Figure 2: An illustrative example for defining the stu-dent(left) and course(right) multi-granularity groups. set of all students is defined without using any student aca-demic features. Then finer groups are defined by using one student feature at a time to segment the student group(s) further based on the value of that feature to give smaller and more homogeneous groups. The course groups are defined similarly using the course academic features.

One way to define the student groups is shown in Figure 2-left. The node at the top represents the group of all students. The second level segments the students based on their ma-jors and it has one node (group) for each major. The third level segments the students further based on their academic levels and it has one node for each academic level within each major. Similarly, the course groups can be defined as shown in Figure 2-Right. The node at the top represents the group of all courses. The second level segments the courses based on their subjects and it has one node (group) for each sub-ject. The third level segments the courses further based on their levels and it has one node for each course level within each subject. In the rest of the paper, we will use the groups defined in Figure 2 as an illustrative example.

We next describe how the groups are incorporated into popularity based ranking, neighborhood based user collabo-rative filtering and matrix factorization.
A popularity ranking scheme ranks the courses based on how frequently they were taken by the students. In our case, we rank the courses for a student s based on how frequently they were taken by students of the same group as s . The ranking score of course c for a given student s is computed as | ' s ! c | , where ' s ! c is the set of students in the same group as s that have taken c and |X| represents the cardinality of set X .

Utilizing a di  X  erent student group from the multi granu-larity groups gives a di  X  erent model. The various models are referred to as Grp-Pop-h ' s , where h ' s is the student group level in the multi-level groups. For example, the model that utilizes the groups at the second level of the multi-level stu-dent groups in Figure 2-left is referred to as Grp-Pop-2.
User-CF predicts a grade of a student s in a course c by relating to how students that have taken same courses as s performed in c as where  X  g s is the average grade of s , N s is the set of neigh-bor students to s , and sim( s,s 0 ) is some similarity between students s and s 0 .
For grade prediction, the neighborhood set N s is selected based on the student groups as follows. Any student in the same group as s and has taken at least n c courses that were taken by s is selected as part of N s . Moreover, the size of N s is limited so that it only contains the n n students that are most similar to s . The threshold parameters n c and n are fine-tuned using a validation set. In the case where not enough neighbors are found, the grade is then estimated as where  X  g c is the average grade for course c .

By utilizing di  X  erent student groups, di  X  erent models are built. The various models are referred to as User-CF-h ' For example, the model that utilizes the groups at the sec-ond level of the multi-level student groups in Figure 2-left is referred to as User-CF-2.
Since in this case the enrollment patterns are the main indicators and not the grade values, G is converted into a binary matrix with all grades set to 1 X  X  and other entries considered as 0 X  X . The recommendation scores are then es-timated as in Equation 1. In practice this gives better rec-ommendations than using the actual grade values. In the case where not enough neighbors are found, this indicates an irrelevant course and the course rank is set to 0.
MF predicts the grade of student s in course c as where b s and b c are the bias terms of s and c , and u s v c are the latent factor vectors of s and c , respectively.
While the literature is rich with feature-based and context-aware MF techniques that can be modified and used as a framework to implement our ideas, we choose to modify the context-aware technique in [4] as we find it most relevant. This technique accounts for context via additional bias terms that are defined for each (item, context) pair.

In our case, we use the student groups to describe the con-texts in which a course is taken, and use the course groups to describe the contexts in which a student takes a course. Considering the example in Figure 2, the third level student groups describe course-side contexts in terms of the student majors and academic levels. Similarly, the third level course groups describe student-side contexts in terms of the course subjects and levels. Accordingly, we define multiple bias terms per student and per course to account for the vari-ous student-and course-side contexts. The recommendation score of a given student s and course c is estimated as where b ' c s is some student bias that accounts for the context described by the course group of c , b ' s c is some course bias that accounts for the context described by the student group of s , and u s and u c are the latent factor vectors for s and c , respectively.
 Multiple models can be defined using the di  X  erent groups. For the example in Figure 2, considering the various student and course group combinations, we can build nine di  X  erent models. The various models are referred to as MF-h ' s -h where h ' s is the level of the student group, which defines the granularity of the course bias, and similarly h ' c is the level of the course group, which defines the granularity of the student bias. For example, considering Figure 2, MF-1-3 is used to refer to the model that uses the coarsest-grain student groups (at the 1st level) to define the coarsest-grain course biases, and uses the finest-grain course groups (at the 3rd level) to define the finest-grain student biases.
Since the finer groups are more homogeneous than the coarser groups, the MF models that utilize them can give more accurate predictions. However, the student groups are recognized through defining multiple biases for each course, and similarly with the course groups and the student bi-ases. For example, if we have 2,000 student groups and 1,000 course groups, then 2,000 biases are defined per course and 1,000 biases are defined per student. Only a handful of bi-ases for each student/course are associated with some data points. and the remaining majority of the biases are associ-ated with very few or no data points. Therefore, the models that utilize finer groups can become prone to poor gener-alization. To understand why this happens, consider the following example. Assume an Artificial Intelligence course c that is o  X  ered by the Computer Science department was taken by 47 Computer Science major students and 2 other Liberal Arts major students. If we define student groups using the major, and if we have 100 di  X  erent majors, then c will have one bias associated with 47 data points, one bias associated 2 data points and 98 biases associated with 0 data points. Obviously, the biases with 0 and 2 data points cannot be as accurately estimated as the other bias.
To overcome this problem, we build multiple models uti-lizing various groups and use them to generate multiple pre-dictions. Then the predictions are combined based on the sample sizes that are associated with the bias terms of the various models as described next.
Before discussing how the various model predictions are combined, it is worth noting that the user and item latent factors are not shared among the various models but each model has its own factors. The various predictions are com-bined while accounting for the associated sample sizes as follows. Each model MF-h ' s -h ' c has a combination weight given by where sup ( b ' c s ) is the sample size (i.e., number of training samples) associated with the bias term b ' c s . The total weight is aggregated over the individual model weights as The final prediction is then given by  X  g h c , and  X  { h ' s ,h ' c } is some global combination weight for that model. This method is referred to as INTRP-MF, the interpolative multi-granularity MF method.
Parameter estimation is done via a step-wise optimization process in which the parameters of each of the individual models are first estimated, and then the  X  { h ' s ,h ' c combination weights are estimated.

The parameters of each of the various models are esti-mated via a regularized optimization process of the form where  X  represents the model parameters, L (  X  ) is the loss function and R (  X  ) is a regularization function to avoid over-fitting. We use a squared error loss function of the form where  X  g s,c (  X  ) is given by Equation 3. This loss function is suitable as the letter grades can be transformed to numeric values. The regularization function R (  X  ) is given by
R (  X  ) = u ( || U || 2 F + || B ' C S || 2 F )+ v ( || V || where u and v are the regularization parameters and || U || is the ` -2 norm of the matrix U .

After the parameters of each model are estimated, the  X  { h ' s ,h ' c } weights are estimated by minimizing a mean squared error loss as well.
We use a learning to rank approach to generate person-alized course recommendations per student. The rank of course c for student s is estimated as in Equation 3. The model parameters of each model are estimated using a per-sonalized pair-wise ranking loss function [7] of the form where C s is the set of courses taken by student s ,  X  C the set of courses never taken by any student in the same group as s , and ( z )= e z . Although  X  g s,c (  X  ) is estimated using Equation 3, it represents a ranking score in this case and not a predicted grade because the model parameters are estimated using the ranking-based loss function. We use the same regularization function as in Equation 6 to avoid overfitting.

The ranking loss function is of order O ( n u  X  n i ), where n u and n i are the number of students and courses, respec-tively. The learning time can be reduced by sampling, for each student, from among his  X  C ' s instead of considering the whole set. If the number of samples is of order O ( C s ), the run time is reduced to O ( NNZ G ), the number of non-zero entries in the grade matrix G .
In this section we describe the dataset that is used for eval-uation, the evaluation metrics, the methods that we compare against, how the various methods are trained and how the student and course groups are defined.
The dataset used for evaluation is obtained from the Uni-versity of Minnesota. It spans 13 academic years and it has over 1.7 million letter grades that involve around 60,000 stu-dents, 10,000 courses, 10 colleges, 570 course subjects, 565 majors, 4 academic levels and 8 course levels. The grades are converted into numbers according to the 4.0 GPA stan-dard 1 . All Pass/Fail grades are removed from the dataset.
The last term in the dataset is used for testing and the rest are used for training and model selection. The last term in the training set is used for model selection and the rest is used for training. Grades of the students that have graduated before the test term are included in the training set. Grades for the new courses and the new students that first appear in the test term are excluded.
We experimented with six di  X  erent ways to define the multi-level groups, namely, H-1 up to H-6. Each one con-tains three levels of student and course groups and thus, the corresponding MF models are referred to as MF-1-1 up to MF-3-3. The various User-CF and Grp-Pop methods that are defined based on the student groups are referred to as User-CF-1 up to User-CF-3, and Grp-Pop-1 up to Grp-Pop-3, respectively. The features used to define the groups are listed in Table 1.
Methods are evaluated for (1)the accuracy of top-n course ranking and (2)the accuracy of grade prediction.

Course top-n ranking is evaluated with Recall@ n which is computed for each student s as where n s,n is the number of courses that appeared in the test set of s and in his list of n recommended courses, and n s is the number of courses in the test set of s .Recall@ n is computed by averaging over Recall@ n s for all s and for n in the range [1,10]. The relative methods performances did not change with n and so, we only report results for n =5.
Grade prediction accuracy is evaluated by computing the
See X  X ttp://www.collegeboard.com/html/academicTracker-howtoconvert.html X  for letter grade-grade point conversion. Root Mean Squared Error on the testing grades G test as
We compare the performance of our method against the following approaches: User Collaborative Filtering: User-CF with Pearson correlation for user similarity [9].
 Matrix Factorization: Typical MF as described in Equa-tion 2.
 Response-aware Matrix Factorization: The context aware response model described in [16]. We implemented RAPMFc as we think it is the most relevant because it cap-tures the probability that rating an item depend on the rat-ing value, the user and the item.
 Regression Latent Factor Models: The feature-based MF-based method described in [2], referred to as RLFM. We used libFM [24] to generate the results for grade predic-tion only as it is not a top-n ranking technique. Ensemble-based Grade Combination: We compare the predictive performance of the the interpolative multi gran-ularity method INTRP-MF against various ensembles. The Minimum, Maximum, Average and Median ensembles are considered where the minimum, maximum, average and me-dian grades are selected as the final prediction, respectively. These ensembles are referred to as MIN-En, MAX-En, AVG-En and MED-En, respectively. We also include results for the interpolative method with excluding the  X  parameters in order to show how the sample-size-based weights perform. This method is referred to as WT-MF and it does not need a secondary learning step as the  X  weights are omitted.
For training the MF models, we tried a number of latent factors in the range [1,10] and u and v in the range [1e-4,5]. The values that gave the best results were latent factors in the range [1,3], u and v in the range [0.1,3.5].
For User-CF, we have tried values for the parameters, n n and n c in the range [1,50]. The best results were obtained with values in the range [2,36].

For RAPMFc, we have tried parameter values for u , v and  X  in the range [10 3 ,10 1 ], in the range [0,1] and number of factors in the range [1,10]. The values that gave the best results were in the range [0.01, 0.1] for u , v , 1 for , 0.01 for and [7,10] for l .

For grade prediction, and top-n ranking, model selection is based on the lowest RMSE and the highest Recall@ n on the validation set, respectively.
We assess the e  X  ectiveness of the developed methods in order to answer the following questions:
Q1. Does incorporating the groups in the various methods
Q2. Does incorporating the groups in the various methods
Q3. How is the grade prediction performance of the MF Table 2: Recall@5 for the various groups. The highest Re-call@5 for each set of methods within each group (column) is underlined.

Prior to ranking the courses for each student, we apply a domain-aware pre-filtering in which courses that have never been taken by at least one student of the same major and academic level as the target student are filtered out. This ap-proach performed the best among other similar pre-filtering rules that utilize various academic features.

Table 2 shows the Recall@5 for all the methods across all groups. Notice that the typical popularity ranking, User-CF and MF schemes are equivalent to Grp-Pop-1, User-CF-1 and MF-1-1, respectively.

For the popularity methods, Grp-Pop-3 and Grp-Pop-2 outperform Grp-Pop-1. Across the six groups H-1 to H-6, Grp-Pop-3 gives the highest recall.

For the User-CF methods, User-CF-2 and User-CF-3 only outperform User-CF-1 when the groups are defined in terms of the student majors (H-1, H-2 and H-5). For these groups, User-CF-3 gives the highest recall. For the other groups, User-CF-1 gives the highest recall.

For the MF methods, the ones that utilize groups outper-form MF-1-1 by an order of magnitude. In general, defining the course biases using finer student groups gives better re-call as it is the case, for example, with MF-1-2, MF-2-2 and MF-3-2 in H-1. On the other hand, defining student biases using finer course groups does not always give better recall as it is the case with MF-2-2 and MF-2-3 in H-2, H-3 and H-4. We believe this is related to the sample sizes associated with the student biases. Since each student takes a limited number of courses, the models utilizing the finest course groups have less than 2 training points associated with their student biases on average. RAPMFc performs better than MF-1-1 but worse than all models that utilize groups across H-1 to H-6.

Across all methods, the highest recalls are given by MF-3-2 and MF-3-1 which slightly surpass MF-3-3 and Grp-Pop-3. All User-CF methods outperform RAPMFc, MF-1-1, MF-1-2 and MF-1-3 for all groups. MF models with finer student groups, like MF-3-1, MF-3-2 and MF-3-3, always outper-form all User-CF methods. The student groups that are defined in using majors (H-1, H-2 and H-5) give higher re-call than the groups defined using colleges (H-3 and H-4). The clustering-based groups give the lowest recall, indicat-ing that the clustering could not capture the groups that are defined by the student and course academic features. Table 3: RMSE for all groups. The lowest RMSE for each set of methods within each group (column) is underlined.
We first discuss the performance of the di  X  erent meth-ods, then we discuss the e  X  ect of the sample sizes on the performance of the di  X  erent MF models.
RMSE given by the di  X  erent methods across all groups are listed in Table 3. For the User-CF methods, User-CF-2 and User-CF-3 that utilize finer groups give lower RMSE than User-CF-1. User-CF-2 gives the lowest RMSE when the student sgroups are defined using the academic level.
For the MF methods, MF-1-1 gives the lowest RMSE across the di  X  erent groups. MF models that utilize finer groups tend to give higher RMSE. We believe this has to do with the e  X  ect of the sample sizes that are associated with the various groups, which is analyzed in more details in Section 7.2.2.

INTRP-MF gives lower RMSE than all the ensembles across the di  X  erent groups. That is because it only gives higher weights to the finer models as their biases are associ-ated with larger sample sizes, which indicates a better ability to generalize. INTRP-MF does only marginally better than WT-MF, which indicates that the improvement is largely due to the sample-size-based weights. The AVG-En gives the third lowest RMSE and in many cases it performs worse than MF-1-1. Among all methods and groups, INTRP-MF gives the lowest RMSE, followed by WT-MF.

RAPMFc gives higher RMSE than all MF and User-CF methods. This is consistent with the results presented in [16] since our test set represents inspected entries for courses that students have taken. RLFM gives lower RMSE than the User-CF methods but higher RMSE than the MF methods.
To understand how the sample sizes associated with the bias terms of the various MF models a  X  ect their perfor-mance, and to show why INTRP-MF works, we analyze how the RMSEs of the various models change with the number of training samples associated with their biases. To do so, we extract multiple subsets from the test set. Each subset contains test cases whose corresponding student and course biases in the finest model, MF-3-3, are associated with a minimum number of training samples referred to as  X  and . We try values for  X  and in the range [0,100] to generate various test subsets. Then for each subset we compute the RMSE for all the models and plot the RMSEs against the subset coverage (number of test cases in the subset).
Figure 3 shows the RMSE against the coverage for the various models with the H-1 groups. Each coverage point represents a test subset with that amount of test cases. As the coverage decreases, the sample sizes associated with the biases of the various models increases. For each coverage point we plot the RMSE of the various models. Subfigures (a), (b) and (c) show how the models with various course groups perform given a fixed student group. At the highest coverage of 50,000 test cases, models with the coarsest course groups and thus, coarsest student biases (MF-1-1, MF-2-1 and MF-3-1) give the lowest RMSEs. For lower coverages between 10,000 and 500 (indicating that finer models have more training examples), models with finer course groups and thus, finer student biases (MF-1-2, MF-2-2 and MF-3-2) give the lowest RMSEs. We can conclude from this that INTRP-MF manages to yield lower RMSE as it gives higher weights to the finer models when their biases are associated with more samples, i.e., when they can give lower RMSE.
In this paper we addressed the grade prediction and top-n course ranking problems. We showed how the student and course academic features determine the enrollment patterns and we defined multi-granularity student and course groups accordingly. We showed how these groups can be incorpo-rated in user collaborative filtering, matrix factorization and popularity ranking methods.

By evaluating the various methods on a large dataset, we showed that incorporating the features-based groups into the various methods leads to better grade predictions and top-n course rankings. We also showed how the grade prediction accuracy of matrix factorization methods slightly degrades when their biases are associated with small sample sizes; an issue occurring with utilizing finer groups. We showed how this can be handled by building various models utilizing various-granularity groups and combining their predictions based on the sample sizes associated with their biases. Our results also showed that the student groups defined using the majors and academic level gave the best top-n rankings and the most accurate grade predictions.

In the future we will consider special cases like ranking non-required courses while considering grades in evaluation. This work was supported in part by NSF (OCI-1048018, IIS-1247632, IIP-1414153, IIS-1447788), Army Research Of-fice (W911NF-14-1-0316), Intel Software and Services Group, and the Digital Technology Center at the University of Min-nesota. Access to research and computing facilities was pro-vided by the Digital Technology Center and the Minnesota Supercomputing Institute. [1] G. Adomavicius, R. Sankaranarayanan, S. Sen, and [2] D. Agarwal and B.-C. Chen. Regression-based latent [3] D. Agarwal, B.-C. Chen, and B. Long. Localized [4] L. Baltrunas, B. Ludwig, and F. Ricci. Matrix [5] N. Bendakir and E. Aimeur. Using association rules [6] H. Bydzovska. Are collaborative filtering methods [7] W. Chen, T. yan Liu, Y. Lan, Z. ming Ma, and H. Li. [8] T. Denley. Course recommendation system and [9] M. D. Ekstrand, J. T. Riedl, and J. A. Konstan. [10] A. Elbadrawy and G. Karypis. User-specific [11] J. L. Herlocker, J. A. Konstan, A. Borchers, and [12] J. M. Hern  X andez-Lobato, N. M. T. Houlsby, and [13] A. Karatzoglou, X. Amatriain, L. Baltrunas, and [14] Y. Koren, R. Bell, and C. Volinsky. Matrix [15] Y. Lee and J. Cho. An intelligent course [16] G. Ling, H. Yang, M. R. Lyu, and I. King. Response [17] B. M. Marlin and R. S. Zemel. Collaborative [18] X. Ning and G. Karypis. Sparse linear methods with [19] A. Parameswaran, P. Venetis, and H. Garcia-Molina. [20] A. G. Parameswaran and H. Garcia-Molina.
 [21] A. G. Parameswaran, H. Garcia-Molina, and J. D. [22] A. G. Parameswaran, G. Koutrika, B. Bercovitz, and [23] S. Ray and A. Sharma. A collaborative filtering based [24] S. Rendle. Factorization machines with libFM. ACM [25] S. Rendle, Z. Gantner, C. Freudenthaler, and [26] N. Thai-nghe, L. Drumond, and L. Schmidt-thieme. [27] N. ThaiNghe, T. Horv  X ath, and L. Schmidt-Thieme. [28] K. Yu, B. Zhang, H. Zhu, H. Cao, and J. Tian.
