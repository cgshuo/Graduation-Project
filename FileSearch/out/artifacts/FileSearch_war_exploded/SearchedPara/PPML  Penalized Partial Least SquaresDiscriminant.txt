 Multi-label classification has attracted widespread attention in machine learn-ing, and has been applied to many fields, such as gene function [1], semantic annotation of images [2], and so on [3]. Currently, many learning approaches for multi-label data have been developed. They can be roughly divided into two groups: problem transformation and algorithm adaptation [4]. The former transforms the multi-label problems into a set of single-label problems, which can be solved with the traditional classification algorithms. Binary Relevance (BR) and Label Powerset (LP) [4] are two typical problem transformation ap-proaches. As BR and LP can not work well if the label set has large numbers, Random k -labelsets (RA k EL) [5] employs LP to learn a corresponding classifier by breaking the original set of labels into a number of small random subsets. Nevertheless, RA k EL shows low efficiency when the l abel set is large and sparse. Algorithm adaptation exploits the traditional learning algorithms to handle the multi-label problems. For instance, Multi-Label k -Nearest Neighbor (ML-k NN) [6] is based on the k -Nearest Neighbor ( k NN) algorithm, which is a lazy learn-ing approach and ignores the label correlations. Backpropagation for Multi-label Learning (BP-MLL) [7] is a neural network approach based on the popular back-propagation algorithm. However, with the increase of the sample dimensions, the efficiency of the learning algorit hms becomes a serious problem.

The so-called  X  X urse of dimensionality X  resulted from the high dimensionality of data brings enormous challenges to multi-label classification [8]. The existing dimensionality reduction methods are not very appropriate for the multi-label classification problems. Principal component analysis (PCA) [9], which is a clas-sical dimensionality reduction approach and widely used in practice, ignores the label information. Linear discriminant analysis (LDA) [10], a popular supervised dimensionality reduction algorithm, does not consider the dependence between the variables and labels. Canonical correlation analysis (CCA) [11] maximizes the correlation between two blocks of varia bles, but produces a generalized eigen-value problem with higher computational expense.

The challenge has been attempted to solve in multi-label classification by re-searchers during the past few years. For example, Huang and Zhou [12] encode the local influences of label correlations in a LOC code in order to exploit label correlations locally. In [13], a new criterion named PRO LOSS is proposed, which concerns the prediction on all labels and the rankings of only relevant labels. In order to high-dimensional data, Liang Sun et al. [14] structure an equivalent least-squares formulation for CCA under a mild condition. A semi-supervised framework is obtained in [15], which performs optimization for dimension reduc-tion and multi-label inference. Bayesian network structure is used to encode the conditional dependencies of both the labels and feature sets in [16]; Nevertheless, this high-order approach may lead to high model complexities.

In this work, we present a new general framework for multi-label classification, capturing the label correlations and dimensionality reduction simultaneously. The proposed framework is named as penalized partial least squares discriminant analysis for multi-label classification (PPML). It aims at reducing the dimen-sion of muliti-label data and obtaining the latent variables between the variable and label spaces. Specifically, we adopt the technique of partial least squares discriminant analysis (PLS-DA) [17] to discover the latent variables between the variable and label spaces of data, so as to model the correlation between them. To tackle with the problem of  X  X arge p ,small n  X , a ridge penalization [18] is fur-ther performed on the object optimization function of PLS-DA. After obtaining the latent space, we build a discriminant model and use to predict the label sets for new instances in terms of the regressi on coefficient of the latent variables.
The rest of the paper is divided into the following sections: Section 2 briefly re-views the state-of-the-arts of multi-label learning. Our proposed method, PPML, is presented in Section 3. Section 4 is the p art of experimental comparative study. Finally, we conclude the paper.
 In this section, we briefly review the representative multi-label learning methods. More details can be found in good papers (e.g., [4]).

Generally, multi-label learning algorithms can be divided into two categories: problem transformation and algorithm adaptation. The problem transformation methods firstly transform the multi-label problems into a set of corresponding single label ones, which can be solved by the traditional classification approaches. The characteristic of this kind of methods is fitting the multi-label data to learn-ing algorithms. Binary relevance (BR) is a typical example of this kind meth-ods [4]. It builds a binary classifier for each label occurring in data, where the instances are considered as positive if they contain the label and negative oth-erwise. Note that BR does not take the correlation of labels into consideration.
CLR [19] and MLStacking [20] exploit the pairwise correlation between the la-bels to construct classification models. Specifically, they first take a pair of labels as a new label at each time. The instances involve only one of labels in the pair are considered as positive or negative, depending on the containing label. This kind of learning algorithms belong to the second-order transformation methods [21]. Contrastively, RA k EL [5] is a high-order transformation method, where the correlation between multiple labels is involved. However, they have relatively high complexity, and can not handle the high-dimensional data effectively. Other problem transform algorithms, such as Pruned Problem Transformation (PPT) [22] and Classifier Chains (CC) [23] have similar situations.

The second learning methods, i.e., algorithm adaptation, cope with the multi-label learning problems by extending the traditional learning algorithms directly, so as to adapt to the multi-label data. In other words, this kind of methods is fit-ting learning algorithms to data. As a representative example, ML-k NN extends the traditional k NN learning algorithm, so that it can handle the multi-label data appropriately. ML-DT [24] is another first-order adapting method, where decision tree has been revised according t o the properties of multi-label data. Ranking Support Vector Machine (Rank-SVM) [25] is a second-order method, which tries to find maximum margins within the multi-label data. A high-order method named LEAD [16] adopts Bayes learning to deal with the multi-label data. It encodes the conditional dependencies of the variables and labels simul-taneously.

How to exploit the correlation of labels and variables of multi-label data is still an open issue for multi-label learning. Although there are some multi-label learning algorithms, exploiting the label dependencies for the multi-label data, they have relatively high complexity, resulting in low robust to high dimensional data. This paper presents a novel approach named PPML. It takes both the correlation of variables and labels and dimension reduction into account simul-taneously. In this section, we first briefly give the formal concepts of multi-label learning and partial least squares discriminant analysis, and then propose a new learning framework for the multi-label data. 3.1 Multi-Label Classification Without loss of generality, let X  X  R n  X  m and Y  X  R n  X  d be the variable and the label spaces, respectively, where n is the number of instances, m is dimensions of instances and d is the number of class labels involving in instances. In multi-it corresponds to a possible multi-label set y i  X  Y ,where y i equals to 1 if the corresponding instance x is tagged with the i -th class label, otherwise y i =0. Given a data set D = { ( x i ,y i ) | 1  X  i  X  n } consisting of n multi-label instance, the purpose of multi-label learning is to build a classifier g : X  X  2 Y from D , and then use this model to predict the labels of unseen instances. From this definition, one may observe that the output of multi-label classification model is a subset of labels, not a single label. This is the distinguished difference of multi-label learning to the traditional ones. 3.2 Partial Least Squares Partial least squares regression (PLS regression) is a statistical method that finds a linear regression model by projecting the predicted variables and the indepen-dent variables to a new space. It shows some similar properties to principal components regression, which tries to find hyper-planes of minimum variance between the predicted and independent variables. PLS regression is particularly suited to the case of high dimensionality, where there is multi-collinearity among the variables. In this case, standard regression will always fail.

PLS tries to locate the fundamental relations between two matrices, i.e. a la-tent variable approach to modeling the cova riance structures in these two spaces. Assume that X  X  R n  X  m and Y  X  R n  X  d denote the independent and predicted variables. Usually, they can be decomposed by the common latent components T  X  R n  X  k as follows: are the loading vectors of X and Y , respectively. E  X  R n  X  m and F  X  R n  X  d are the residual matrixes.

According to the linear transformation, we make an assumption that the latent components T is a linear tramsformation of X as follows: where W  X  R m  X  k is the weight matrix. After T is constructed, Q T in Eq.(2) can be obtained by solving the least squares problem, i.e., where ( T T T )  X  is the Moore-Penrose inverse of T T T . Substituting (3) into (2), for Y we have its regression form: where the regression coefficient B is B = WQ T = W ( T T T )  X  T T Y .
From the definitions above, we know that a PLS model tries to find the mul-tidimensional direction in the X space that explains the maximum multidimen-sional variance direction in the Y space. Depending on the tasks, PLS have many variants. For example, partial least squares discriminant analysis (PLS-DA) is widely used when the Y is categorial.

Given a new instance X new , its categories or labels can be predicted in terms of the PLS model (i.e., Eq.(5)) as follows: PLS-DA is an efficient dimension reduction tool for handling the multi-label data. However it is not a specifical method for the cases of  X  X arge p ,small n  X  and feature selection. Besides, the results obtained by PLS-DA are often difficult to be interpreted. Just for this reason, we impose a ridge penalization on the PLS-DA model to alleviate this problem. 3.3 PLS-DA with Ridge Penalization For the high dimensionality of multi-label data, obtaining the PLS-DA model in a straightforward way becomes unfeasi ble, especially when the variables are highly collinear.

According to the Eq.(1) and (2), one may observe that PLS aims at locating the common latent variables T of X and Y , such that their covariance is maximal. That is to say, the object function of PLS-DA can be represented in a equivalent form as follows: where cov ( Xp,Yq ) is the covariance of Xp and Yq , p and q are the loading vectors of X and Y respectively.

The optimization problem of Eq.(7) can be solved through the following La-grange function: where  X  and  X  are Lagrange multipliers. After differentiating Eq.(8) with re-spect to p and q respectively, and let them equal to zero, we have the following equivalent problems: Thus, the optimization problem is now transformed into the problem of solving eigenvalues and eigenvectors. Note that the vectors p and q are the eigenvectors of X T YY T X and Y T XX T Y , respectively. An intuitive way of obtaining the eigenvectors p and q is to perform the technique of singular value decomposition (SVD) on X T Y : where the eigenvectors P  X  R m  X  r and Q T  X  R r  X  d are orthogonal,  X   X  R r  X  r is the diagonal matrix consisting of the singular values. ( p, q ) is a pair of eigenvec-tors corresponding to the eigenvalue in  X  .

It should be mentioned that Eq.(11) may be ill-posed as the dimensionality of data is larger than the number of data. Thus, it should be penalized for the consideration of numerical computing and practical applications. Here we exert a l 2 -norm penalty on Eq.(11). This is also known as ridge regularization, which is a method for solving badly conditioned linear regression problems. The benefits of ridge regularization are most striking in the presence of multi-collinearity and penalize the size of the regression coefficien ts, resulting in shrinking the regres-sion coefficients toward zero [26], [27]. Sparse property is more prefer because it can yield easily interpretable results. Moreover, with the increase of the number of labels, the label space Y is usually sparse. Thus, it is necessary to shrinking the loading vectors of Y .

After applying l 2 -norm penalty, the loading vector q of Y can be obtained by solving the constraint optimization problem as follows: where M = Y T X ,  X  is the regularization parameter for the loading vector q .For q ,when  X  is enough small, the weight coefficients of some variables compressed to zero by comparing with a threshold. Let L ( q ) be the Lagrange function of Eq.(12), we have
After taking the derivative of Eq.(13) with respect to q and setting it to zero, we can obtain q ridge as Substituting M in Eq.(14) with Eq.(11), we further have where f (  X  i ) is the shrinkage factors.

Based on the analysis above, we propose a new multi-label learning framework called PPML (Penalized Partial least squares discriminant analysis for Multi-label Learning). As the name indicates, our method employs PLS-DA with ridge regularization to handle the classification problem of high-dimensional multi-label data. Algorithm 1 presents the framework of PPML in detail. It exploits Nonlinear Iterative Partial Least Squares (NIPALS) [17] to obtain p and q .Al-ternatively, PPML can also be implemented with other forms like PLS-SB [28] and SIMPLS [29].

PPML works in a straightforward way and can be easily understood. It mainly consists of two stages, i.e., model training and result predicting. Specifically, in the training stage, two loops are nested. The major purpose of the inner loop is to get the loading vectors p and q of the variable and label spaces respectively, while the outer loop aims at yielding all loading vectors ( P and Q ), the latent components T and the coefficients W , so as to build the PPML learning model with PLS-DA. In the predicting stage, the prediction value of a new instance is a real-value vector in terms of (6). Later the output will be transformed into a vector of { 0, 1 } by comparing with a given threshold  X  , which is often empirically set as 0.5. 4.1 Data Sets In our experiments, seven public data sets from the real-world applications were adopted. They are Medical , Arts , Entertainment , Health , Recreation , Reference , and Science .The Medical data set was used in the Medical Natural Language Processing Challenge 1 in 2007. In this data set, each instance contains brief free-text summary of a patient symptom history. The last six benchmark data sets were collected from Yahoo. They cover different domains in web page catego-rization.

Table 1 summaries the general information of the benchmark data sets used in experiments, where Inst. and Var. denote the number of instances and the dimensionality of data for each data set respectively. In addition, L.Card. ,rep-resenting label cardinality, is the average number of labels per instance, while L.Dens. , standing for label density, is the fraction of the cardinality according to the number of labels. 4.2 Comparison of Algorithms To demonstrate the effectiveness of our algorithm, nine multi-label learning al-gorithms have been adopted in comparing with PPML. They are BP-MLL [7], BR k NN [30], IBLR ML [31], LP [4], ML k NN [6], PPT [22], CC [23], MLStack-ing(MLS) [20], and MAHR [33]. They are representatives of the state-of-the-art multi-label learning algorithms, and stand for different learning manners. They can deal with the multi-label problems and have relatively better performance and efficiency.
 The performance of the learning algorithms heavily relies on their parameters. In our experiments, default value was assigned for each parameter as did in the MULAN software package 2 . MULAN [32] is an open source Java library for multi-label learning. It brings many popular multi-label learning algorithms together. For the MAHR classifier, its parameters was set as recommended by the authors in the literature [33], that is, the number of boosting rounds was two times of variables for each data set. 4.3 Evaluation Metrics Multi-label classification needs more complex evaluation metrics than traditional classification. In order to roundly evaluate the performance of PPML and other algorithms, we took four commonly used evaluation metrics. They are Ranking Loss , One-Error , Coverage and Average Precision [4].

Ranking Loss (RL) indicates the mis-ordered degree of couples of labels, where an irrelevant label has higher rank than a relevant one. One-Error (OE) esti-mates how many times the top-ranked label is irrelevant to the true class labels for each instance. Coverage (Cov) obtains the number of the steps that are needed, on average, to move down the ranked list of labels, in order to cover the whole relevant labels of the instance. Average Precision (AP) evaluates the average fraction of true labels ranked above a particular label.

Since Ranking Loss , One-Error and Coverage evaluate the loss of the predic-tion results, the smaller the metric values, the better the performance of learning algorithms. On the contrary, for Average Precision , the larger value indicates the better performance. 4.4 Experimental Results In the experiments, 10-folds cross-validation was performed on each combination of classifier and data set. The experiments were carried out under the platform of MULAN. Table 2 shows the comparison results of classification performance of classifiers in terms of four evaluation metrics, where the mean value of each algo-rithm was ecorded on each data set.

From the experimental results in Tabl e 2, one can notice that PPML is promis-ing. It has better performance than others in most cases. For example, PPML achieved the best performance on five over seven data sets at the aspect of Rank-ing Loss .Evenonthe Health and Science data sets, the performance of PPML is just slightly worse than the corresponding best one, not the worst one. Similar situations also present on the One-Error and Coverage metrics, where PPML outperformed other popular multi-label learning algorithms on six and five over seven benchmark data sets res pectively. The one-error of PPML on Medical is 13.61%, which is slight higher than that of MAHR.

For the measure of Average Precision , PPML is the best in comparing with other classifiers. The performance of PPML is predominant and significantly better than the rest learning algorithms over all of the seven benchmark data sets. For example, on the Arts and Recreation data sets, the average precisions of PPML are 60.59% and 62.14% respectively, while the highest precisions of other classifiers are 50.92% and 52.01%, achieved by MLS and MAHR respectively. In this paper, a new multi-label learning framework, called PPML, is proposed to deal with the multi-label problems. It mainly exploits partial least squares discriminant analysis (PLS-DA) to achieve the purpose of performing dimension reduction and capturing the label correlations simultaneously. To cope with the multi-collinearity problem resulted from the high dimensionality of data, a ridge regularization penalty is further exer ted on the object optimization function of PLS-DA. The experimental results are encouraging and show that PPML is promising in comparison with the other state-of-the-art algorithms.

In the future, we will make an attempt to find other more efficient methods and combine them with PLS-DA to tackle with the problems of multi-label learning.
 Acknowledgements. The authors are grateful to the anonymous referees for their valuable comments and suggestions. This work is partially supported by the National NSF of China (61100119, 61170108, 61170109, 61272130, and 61272468), the NSF of Zhejiang provin ce (LY14F020012), Postdoctoral Science Foundation of China (2013M530072),and the Open Project Programof the National Laboratory of Pattern Recognition (NLPR) (201204214).

