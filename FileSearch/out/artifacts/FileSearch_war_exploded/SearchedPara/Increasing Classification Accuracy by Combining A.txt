 convex pseudo-data, boosting and arcing. Whilst the first two are simply different ver-pseudo-data and boosting have been applied independently for molecular classification [1 applied prior to employing either aggregating or adaptive sampling methods. 
This paper demonstrates how a method combining pseudo-convex data and arcing, with the aid of a maximal-antiredundancy-based feature selection technique, can im-prove the molecular classification accuracy of multiclass datasets. Moreover, the im-provement is achieved using a comparatively small predictor set size. members of S is given in section 2.3. 2.1 Existing Methods Convex Pseudo-Data (CPD). Breiman [5] suggested the use of the convex pseudo-is created as follows: 1. Pick two samples x and x' randomly from the original training set T . 2. Select a random number v from the interval [0, d ], where 0  X  d  X  1. 3. Generate a new pseudo-sample x" by linearly combining x and x' . 4. Repeat step 1 until step 3 given above M times. B is traditionally assigned values within the range [10, 200]. Arcing. Breiman [6] has also established a t echnique for adaptive sampling, arc-x4, where B classifiers are produced as follows: sample in order to form a perturbed training set of the same size, T  X  X  . 3. Train a base classifier employing features from the predictor set used, and informa-tion from perturbed training set T  X  X  . the following equation. fier. 5. Repeat step 2 until step 4 given above B times. 
Again, the range for B in case of arcing is the same as that of CPD. Modifications dicted by unweighted voting of those B classifiers. 2.2 Proposed Methods turbation methods are implemented to investigate their impact on classification accu-racy and optimal predictor set size. Fixed Mixing Parameter CPD (FMCPD). In existing CPD, the value of v is picked randomly within the range [0, d ]. We propose a modified version of CPD where the method. We call this adaptation the fixed mixing parameter CPD (FMCPD). Combining CPD or FMCPD with Arcing. Our second proposed modification advantage of both techniques. B classifiers are formed as follows: 1. Same as step 1 in Arcing . 
Then, from T  X  , use either CPD or FMCPD to construct a final perturbed training set of the same size, T  X  X  . 3. Same as step 3 in Arcing . 4. Same as step 4 in Arcing . 5. Repeat step 2 until step 4 given above B times. trained in the procedures above. 2.3 Pre-aggregating Feature Selection based predictor set score is defined to measure the goodness of predictor set S . V members of S . The BSS/WSS measure, first used in [1] for multiclass tumor classifi-Variance). U S represents the measure of antiredundancy for S . with R ( i , j ) being the Pearson product moment correlation coefficient between mem-membership of S . 
This predictor set scoring approach is employed to find the optimal predictor set of sizes P = 2, ..., 150. Existing and proposed methods were tested on a 14-class oligonucleotide microarray 144 samples and a test set of 54 samples. 
A total of 9 optimal predictor sets are co nsidered in the experiment, each predictor pared to either the standard algorithm or Max Wins, and has been shown to produce comparable accuracy to both of these algorithms [10]. 3.1 Arcing whether arcing by itself, without either CPD or FMCPD, is able to produce any sig-tained from pure arcing runs for both values of B and are shown in Tables 1 and 2. 
Arcing produced only a slight improvement of classification accuracy for 6 out of  X  = 0.1. 3.2 CPD-Arcing The CPD methods may or may not incorporate arcing for forming perturbed training sets. The methods employing exclusively CPD are labeled as CNU d ( C onvex tagged as CRU d ( C onvex pseudo-data, a R cing, U nfixed v ). 
The full set of results is presented here for only CRU d methods. The values of d set formed using  X  = 0.4 (Tables 3 and 4). SC 75.9 75.9 77.8 83.3 79.6 75.9 75.9 72.2 61.1 gives the best overall results. 3.3 FMCPD-Arcing Like CPD, FMCPD methods may or may not incorporate arcing to construct per-turbed training sets. The methods employing exclusively FMCPD are labeled as arcing are tagged as CRF v ( C onvex pseudo-data, a R cing, F ixed v ). Arcing experiments show that at v =0.5 and 0.625, the classification accuracy are im-proved (Table 5) with a minimal increase in predictor set size (Table 6) as compared nied by a slight decrease in corresponding predictor set size .
 Arcing methods over the single classifier, either in terms of accuracy increase or pre-dictor set size, are more impressive than those achieved by the CPD-Arcing methods. The results from the previous section show that while arcing improves the classifica-the FMCPD-Arcing methods increase the accuracy for all 9 predictor sets. Moreover, the increase in accuracy produced through arcing is of smaller magnitude than the im-FMCPD and arcing that is crucial for the vast improvement in classification accuracy. 
A further inspection of the results from the previous section leads us to the follow-ing two questions: 1. How much of the improvement in accuracy is the effect of fixing? FMCPD technique? CPD methods, with or without arcing is required and is presently given in Section 4.1. For the second question, a double cont emplation of FMCPD-Arcing vs. FMCPD and CPD-Arcing vs. CPD is necessary and is presented subsequently in Section 4.2. 4.1 The Effects of Fixing the Mixing Parameters in CPD CRU d methods, while the other is a comparison between CNF v and CNU d methods. The value of v which gave the best overall results in accuracy for the FMCPD-Arcing methods, 0.625; and the value of d which produced the best average of accuracies for the CPD-Arcing methods, 1.0 are used in the comparisons. 
Method 
Method 
These 4 methods (Tables 7 and 8) will be used to compare and contrast  X  the results from CRF0.625 with those from CRU1.0 to examine the effects of fix-ing v in methods incorporating the arcing technique; and  X  fixing v in methods with no arcing. 
The comparison is carried out as follows. In comparing method A to method B (A method A is said to be better than method B, and vice versa. If, however, both meth-ods give equal accuracy rate, then the method providing a smaller predictor set is con-sidered to be superior. 
For methods integrating the arcing technique, fixing v does not create any consid-erable effects, improving the accuracy in only those predictor sets which are based on  X  values of 0.2, 0.4, 0.5, 0.6 and 0.9. On the other hand, for methods with no arcing, while the 6 other predictor sets produce worse accuracy (Table 9, rows 1 and 2). ing, and in fact produces adverse effects for methods without arcing. 4.2 The Effects of Incorporating Arcing in CPD and FMCPD in order to investigate the effects of incorporating the arcing technique on classifica-tion accuracy. The first is a comparison between CRF v and CNF v methods, while the 0.625; and the value of d which produced the best average of accuracies for the CPD-Arcing methods, 1.0 are used in the comparisons. The same 4 methods (Tables 7 and 8) used in the previous comparisons will be employed, in a different combination, to compare and contrast  X  the results from CRF0.625 with those from CNF0.625 to study the effects of incorporating the classifier arcing technique into FMCPD methods; and  X  the results from CRU1.0 with those from CNU1.0 to analyze the effects of incor-porating the classifier arcing technique into original CPD methods. 
The first set of comparison indicates that FMCPD benefits greatly from the integra-tion of arcing. CRF0.625 outperforms CNF0.625 in all but one (  X  =0.8) of the predic-comparison. Combining arcing with original CPD yields mixed results, with CRU1.0 worse than CNU1.0 in the remaining 5 predictor sets (Table 9, rows 3 and 4). formance. To prove this, another comparison is made between CRF0.625 and CNU1.0 to show the benefits of simultaneously combining arcing and fixing v (Table 9, bottom-most row). The former is a combination of arcing and FMCPD at v =0.625, each of the method respectively are values that have been empirically shown to yield from 0.1 to 0.9. 
CRF0.625 performs better than CNU1.0 in 7 of the predictor sets, either by produc-ing better accuracy or similar accuracy but with a smaller predictor set in each case. dataset that is higher than previously published [7  X  9, 11], 87.0%, with corresponding predictor set size of 126 genes. 4.3 The Effects of Antiredundancy Factor (1  X   X  ) With CRF0.625, although the best accuracy of 87.0% is obtained at  X  =0.5, the predic-85.2%, also higher than any previously reported, is achieved using only a considera-bly smaller 105-gene predictor set. In a previous study [7], we found an optimal point plot in Figure 1 we can still say that the trend towards high accuracy and correspond-ingly small predictor set points to the same  X  value of 0.4. the maximum accuracy barrier of 77.8% using all of its 16000 genes in the predictor set [8, 9], an improvement of more than 7% in accuracy, employing only 105 of those 16063 genes, is significant indeed. This has been done with the aid of an appropriate feature selection technique, and an astute modification of adaptive sampling and clas-sifier aggregation methods. A precise combination of a modified CPD and arcing methods implemented together with the maximal-antiredundancy-based feature selection technique has improved the ing) behind the improved performance of the hybrid method of FMCPD-Arcing have tures). Therefore the performance of our proposed methods is most likely reproduci-ble in other multiclass datasets of less complexity. 
