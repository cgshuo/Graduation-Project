 JIANFENG GAO, HISAMI SUZUKI Microsoft Research WEI YUAN Shanghai Jiao Tong University 1. INTRODUCTION Language model (LM) adaptation attempts to adjust the parameters of a LM so that it will several LM adaptation methods for Japanese text input. In particular, we focus on the so-main (which we call the background domain) to a different domain ( adaptation domain), for which only a small amount of training data is available. representative of a MAP method [Bellagarda 2001]. The other three, including the boosting [Collins 2000] and perceptron [Collins 2002] algorithms and the minimum sample risk (MSR) method [Gao et al. 2005], are discriminative methods, each of which uses a different training algorithm. corpora whose characteristics were measured using the information-theoretic notion of cross entropy. We found that discriminative training methods outperformed the linear interpolation method in all cases, and had some additional desirable properties: they were effects, and were more robust across different training sets of different domains and sizes. In our experiments, however, none of the discriminative training methods significantly outperformed the others. experimental results and discussion. We conclude in Section 7. 2. THE LANGUAGE MODEL AND THE TASK OF IME Our study falls into the context of Asian langua ge text input (Japanese, in this study). The standard method for inputting these languages is that the users first input phonetic strings, which are then converted into appropriate word strings by software. The task of auto-matic conversion has been the subject of language-modeling research in the context of Pinyin-to-Character conversion in Chinese [Gao et al. 2002a] and Kana-Kanji conver-Editor), based on the name of the commonly used Windows-based application. (CER), which is the number of characters wrongly converted from the phonetic string divided by the number of characters in th e correct transcript. Current commercial Japanese IME systems exhibit about 5 to15% CER in converting real-world data in a wide variety of domains. been converted from A : where, given A , GEN ( A ) denotes the candidate set. phonetic string is provided directly by users. Moreover, we can assume a unique mapping P ( W ), that is, the language model probability, making IME an ideal application for evaluating LM techniques. 1 Another advantage is that it is relatively easy to convert W to A , making it possible to obtain a large amount of training data for discriminative learning, as described later. recognition does: the quality of the model depe nds heavily on the similarity between the training data and the test data. This poses a serious challenge to IME, as it is currently the therefore an imminent requirement for improving user experience, not only as we build future. 3. RELATED WORK and to correlate them to the performance of various techniques for LM adaptation, to compare their effectiveness an d robustness. This relates our work to the study of calculating domain similarities and to techniques for LM adaptation. 3.1 Measuring Domain Similarity Statistical language modeling (SLM) assumes that language is generated from underlying distributions. When we discuss different text domains, we assume that the text from each domain is generated from a different underlying distribution. Hence we consider the problem of distributional similarity. true underlying probability distribution p and another distribution q (e.g., an SLM) which attempts to model L , the cross entropy of L with respect to q is ergodic and stationary process [Manning and Sch X tze 1999], and approximate the cross entropy by calculating it for a sufficiently large n instead of calculating it for the limit. This measures how well a model approximates the language L . KL divergence is defined as The cross entropy and the KL divergence are related notions. Given the notations of L , p, and q above, Manning and Sch X tze [1999] show that In other words, the cross entropy takes into account both the similarity between two contribute to the complexity of an LM task. In this article we are interested in measuring the complexity of the LM adaptation task. We therefore define the similarity between two of the corpus to capture the in-domain diversity of a corpus, as described in Section 5.2. 2 3.2 LM Adaptation Methods In this article we investigate two major approaches to cross-domain adaptation: maxi-mum a posteriori (MAP) estimation and discriminative training methods. using adaptation data to directly minimize the errors in it made by the background model. These techniques have been applied successfu lly to language modeling in non-adaptation recognition. But most of them focused on investigating the performance of certain differences in performance produced by the various methods. In this article, we investigate the effectiveness of various discriminative methods in an IME adaptation scenario, with a special emphasis on correlati ng their performance with the characteristics of the adaptation domain. 4. LM ADAPTATION METHODS In our experiments we have implemented four methods. Linear interpolation (LI) falls into the framework of MAP, while the boosting, the perceptron, and the MSR methods fall into that of discriminative training. 4.1 The Linear Interpolation Method In MAP estimation methods, adaptation data is used to adjust the parameters of the back-ground model so as to maximize the likelihood of the adaptation data. MAP. Trigram models on background and adaptation data are separately generated, and then linearly combined as adaptation model; and the history, h, corresponds to the two preceding words. For simplicity, we chose a single  X  for all histories and tuned it on held-out data. 4.2 Discriminative Training Methods This section describes three discriminative training methods we used in this study. For a Collins [2002] for the perceptron learning al gorithm, and Gao et al. [2005] for the MSR method. general framework of linear models [Duda et al. 2001; Collins 2002]. We use the following notation, adapted from Collins [2002], in the rest of this article. The decision rule of Eq. (1) is rewritten as Equation (8) views IME as a ranking problem, where the model gives the ranking score, not probabilities. We therefore do not evaluate the LM obtained using discriminative training via perplexity. risk (SR). Discriminative training methods strive to minimize the SR by optimizing the model parameters, as defined in Eq. (9), where W i is determined by Eq. (8), function of  X  and its gradient is undefined. Therefore, discriminative methods apply dif-ferent approaches that optimize it approximate ly. As we describe below, the boosting and perceptron algorithms approximate SR(.) by loss functions that are suitable for optimiza-without resorting to an approximated loss fu nction. We now describe each of the dis-criminative methods in turn. [2000]. It uses an exponential function to approximate SR(.). We define a ranking error in conversion W R . The margin of the pair ( W R , W ) with respect to the model  X  is estimated as The rank loss function (RLoss) is then defined as where I [  X  ] = 1 if  X   X  0, and 0 otherwise. Note that RLoss takes into account all candidates in GEN( A ). Since optimizing the RLoss in (11) is NP-complete, the boosting algorithm optimizes its upper bound, i.e., the exponential loss function (ExpLoss), as Notice that ExpLoss is convex, so there is no problem with local minima when optimiz-search procedures (i.e., RankBoost and its vari ants) that converge to the right solution. are repeated N times; a feature is chosen at each ite ration and its weight is updated. Fol-lowing Collins [2000], we used the following update for the d th feature, f d : margins over the set where f d is seen in W but not in W R ;  X  is a smoothing factor (whose value is optimized on held-out data); and Z is a normalization constant. ranking error loss function RLoss to approximate SR, and approximately optimizing RLoss by minimizing its upper bound function, ExpLoss. incremental training procedur e (e.g., using stochastic approximation) that optimizes a minimum square error (MSE) loss function, which is an approximation of SR [Mitchell 1997]. It is simply half the squared difference betw een the score of the correct conversion and the MSE solution, under certain conditions, leads to approximations to the maximum likelihood (ML) solution. The quality of the approximation depends upon the form of the linear discriminant functions (e.g., Eq. (7)). Wh ile this property has a certain theoretical appeal, the discriminant function that best approximates the Bayes discriminant does not similar to ExpLoss, MSELoss is also an approximation to SR. literature, and there are many solution proced ures available. Here, we consider the delta rule , a training algorithm of an unthresholded perceptron. Below, we discuss the derivation of the delta rule, following the description in Mitchell [1997]. differentiating the loss function of Eq. (14) with respect to  X  d as minima, and thus gradient descent is not guaranteed to find the global minimum. Hence computes parameter updates after summing over all training samples, as shown in Eq. (16), the stochastic approximation method updates parameters incrementally, following the calculation of the error fo r each individual training sa mple, as shown in Eq. (17). The stochastic approximation method can be viewed as optimizing a distinct loss function, MSELoss i (  X  ) ,defined for each individual training sample I , as follows: The optimization algorithm used in our experiments is shown in Fig. 2. It takes T passes highest scoring word sequence under the curr ent model is not correct, the parameters are over all training samples, provides a reasonable approximation to descending the gradient with respect to the original loss function of Eq. (14). The algorithm is similar to the per-ceptron algorithm described in Collins [2002]. Th e key difference is that instead of using slightly better performance. Following Collins [2002], we used the averaged perceptron algorithm, a simple refinement of the algorithm in Figure 2, which had been proved more processed in pass t over the training data. Then the  X  X veraged parameters X  are defined as in Eq. (19). In short, there are two approximations embedde d in the perceptron algorithm: one is the MSE criterion that approximates the ML criterion and the other is the stochastic approximation that is introduced for parameter optimization. Though in theory these approximations could to some degree prevent the algorithm from attaining the original objective of minimizing the SR, they turn out to be an effe ctive compromise empirically, as we show in Section 5. algorithm [Gao et al. 2005] is motivated by analogy with th e feature selection procedure small subset of the features that make the largest contribution in reducing SR in a sequential manner. proach: the first direction (i.e., feature) is se lected and SR is minimized along that direc-minimum, and so on, cycling th rough the whole set of directions as many times as neces-sary, until SR stops decreasing. implementation of line search that efficientl y optimizes the function along one direction. Second, the number of candidate features is not too large, and they are not highly Eq.(9) is a step function of  X  , and thus cannot be optimized directly by regular gradient-with simple grid search: usi ng a large grid could miss the optimal solution, whereas using a fine-grained grid would lead to a very slow algorithm. Second, in the case of LM, there Below, we address these issues individually. proposed in Och [2003]. The modifications are made to deal with the efficiency issue due to the fact that there is a ve ry large number of features a nd training samples in our task, compared to only eight featur es used in Och [2003]. Unlike a simple grid search where the intervals between any two adjacent gr ids are equal and fixed, we determine a sequence of grids for eac h feature with differently sized intervals, each co rresponding to a different value of sample risk. function (i.e. Er(.)) of each training sample. Therefore, we first explain how to minimize the Er(.) of a training sample using the line search. Let  X  be the current model parameter  X  word string W  X  GEN ( A ), as in Eq. (7), can be decomposed into two terms: candidate word strings have the same feature value f d ( W ), their relative rank will remain each group have the same value of f d ( W ). In each group, we define the candidate with the highest value of as the active candidate of the group because no matter what value  X  d takes, only this candidate could be selected according to Eq. (8). can find a set of intervals for  X  d , within each of which a particular active candidate will be selected as W * . We can compute the Er(.) value of that candidate as the Er(.) value for the intervals and their corresponding Er(.) values. The optimal value  X  d * can then be found by value. This process can be extended to the whole training set, as follows. By merging the sequence. search is very efficient, even for a large trai ning set with millions of candidate features. the linear model. A large number of features lead to a large number of parameters in the resulting linear model. For a limited number of training samples, keeping the number of features sufficiently small should lead to a simpler model that is less likely to overfit the training data. where the denominator is a normalization term to ensure that E ( f )  X  [0, 1]. feature vector in the linear model. may carry rich discriminative information when treated separately, there may be very little gain if they are combined in a feature vector, if they are highly correlated with each information in the feature selection criterion. reduction of using the d-th feature on the m -th training sample, computed by Eq. (20), is and f j is estimated as 
It can be shown that C ( i , j )  X  [0, 1]. Now, similar to Theodoridis and Koutroumbas [2003], selected feature and f j denotes any candidate f eature to be selected. highest E (.) value. Let us denote this feature as f 1 . the selected feature f 1 and each of the remaining M -1 features, according to Eq. (21). reducing the sample risk, but also the correlation with the previously selected feature. It is minimization. That is, we select the next feature by taking into account its average correlation with all previously selected features. feature selection method. As show n in Eq. (22), the estimates of E (.) and C (.) need to be of features in the resulting model, respectively. According to the feature selection method, described above, we need to estimate E (.) for each of the D candidate features only once in step 1. Due to the efficiency of our line se arch algorithm, this is not very costly. Unlike the case of E (.), O( K  X  D ) estimates of C (.) are required in step 4. This is computationally expensive even for a medium-sized K . Therefore, every time a ne w feature is selected (in step 4), we only estimate the value of C (.) between each of the selected features and each of the top N remaining features with the highest value of E (.). This reduces the number of estimates of C (.) to O( K  X  N ). In our experiments we set N = 1000, which is much smaller than D . This reduces the computational cost significantly, without producing any noticeable quality loss in the resulting model. feature selection (line 2) and optimization (lines 3 to 5) steps. Readers are referred to Gao et al. [2005] for a complete description of the MSR implementation and the empirical justification for its performance. 5. EXPERIMENTAL RESULTS 5.1 Data The data used in our experiments stems from five distinct sources of text. A 36-million-word Nikkei newspaper corpus provided the background domain. We used four adapta-tion domains: Yomiuri (newspaper corpus), TuneUp (balanced corpus containing news-paper and other sources), Encarta (encyclopedia), and Shincho (a collection of novels). words from the training data of each domain, respectively (corresponding to 13K to 78K corpora used for this experiment (that is, 1M words times 5 domains), including 216,565 out-of-vocabulary (OOV) items. of 72K sentences (0.9M ~ 1.7M words) and test data of 5K sentences (65K ~ 120K words) from each adaptation domain. The first 800 and 8,000 sentences of each adaptation training data were also used to sh ow how different sizes of adaptation training data affected the performances of various adaptation methods. Another 5K-sentence subset was used as held-out data for each domain. For domain adaptation experiments, we used our baseline lexicon, consisting of 167,107 entries. 5.2 Computing Domain Characteristics The first domain characteristic we computed was the similarity between two domains for the task of LM. As discussed in Section 3, we used cross entropy as the metric: we first trained a word trigram model using the system described in Gao et al. [2002a] on the one-H ( L A , q B ) following Eq. (3). For simplicity, we denote H ( L A , q B ) as H ( A,B ). entropy is not symmetric, that is, H ( A,B ) is not necessarily the same as H ( B,A ). In order to have a representative metric of similarity between two domains, we computed the average cross entropy between two domains, shown in Table II, and used this quantity as the metric for domain similarity. Along the main diagonal in Tables I and II, we also have the cross entropy computed for which we call self entropy for convenience, is an approximation of the entropy of corpus Note that self entropy increases in the following order: Nikkei  X  Yomiuri  X  Encarta  X  Yomiuri, and Encarta are highly edited texts, following style guidelines and tend to have the corpus is more diverse; we will discuss the effect of diversity further in Section 6. 3 5.3 Results of LM Adaptation We trained our baseline trigram model on our background (Nikkei) corpus using the sys-tem described in Gao et al. [2002a]. The CER (%) of this model on each adaptation do-main is in the second column of Table III. For the LI adaptation method (the third column of Table III), we trained a word trigram m odel on the adaptation data and linearly com-bined it with the background model, as described in Eq. (6). candidate word lattice for each input phonetic string in the adaptation training set using the background trigram model mentioned above. For efficiency, we kept only the best 20 The lowest CER hypothesis in the lattice, rather than the reference transcript, was used as algorithm, we set T = 40 (in Figure 1). These settings might lead to an unfair comparison, as the perceptron algorithm will select far more features than the boosting and MSR algorithms. However, we used these settings because the model training all converged under these settings. All other parameters were tuned empirically on held-out data. (N=100 in our experiments) for each sentence in test data, and used domain-adapted models to rescore the N-best list. The oracle CERs (i.e., the minimal possible CER given the hypotheses in the list) ranged from 1.45% to 5.09%, depending on the adaptation domain. Table III summarizes the results of various adaptation methods in terms of CER (%) and CER reduction (in parentheses) over the baseline model. In the first column, the numbers in parentheses next to the domain name indicate the number of training sentences used for adaptation. 6. DISCUSSION 6.1 Domain Similarity and CER ground domain (Nikkei) increases in the following order: Yomiuri  X  TuneUp  X  Encarta  X  Shincho. This indicates that among the adaptation domains, Yomiuri is most similar to Nikkei, closely followed by TuneUp; Shincho, and Encarta are the least similar to Nikkei. pora, and TuneUp, which is a manually constructed corpus from various representative domains of text, contains newspaper articles. axis) along with CER when 8,000 training sentences were used in adaptation experiments (expressed in the bar graph, scaled on the le ft axis). The correlation between the domain domain, the better the CER results. 6.2 Domain Similarity and the Robustness of Adaptation Methods The effectiveness of an LM adaptation method is measured by the relative CER reduction over the baseline model. Figure 5 shows the CER reduction for various methods for each domain when the training data size is 8K. 5 notice is that the discriminative methods outperform LI in most cases: in fact, for all rows in Table III, MSR outperforms LI in a statistically significant manner ( p &lt; 0.01 using t -test); 6 the differences among the three discriminative methods, on the other hand, are not statistically significant in most cases. More specifically, when the adaptation domain is similar to the background domain (i.e., for Yomiuri and TuneUp corpora), the contribution of the LI model is extremely limited. This can be explained as follows: if the adaptation data is too similar to the background, the difference between the two underlying distributions is so slight that adding adaptation data leads to no or very small improvements. discriminative methods are quite effective on Yomiuri, achieving more than 20% CER reduction. We therefore conclude that di scriminative methods, unlike LI, are robust against the similarity between background and adaptations domains. system, the perceptron algorithm alone achieved better results than MAP estimation. However, the difference may only be apparent, given the various experimental settings for the two studies. We used the N-best reranking approach with the same N-best list for both MAP estimation and discri minative training--while in Bacchiani et al. [2004], two different lattices were used: the perceptron model was applied to rerank the lattice created by the background model, while the MAP adaptation model was used to produce the compatible with theirs. 6.3 Adaptation Data Size and CER Reduction Among the discriminative methods, an interesting characteristic regarding the CER re-duction and the data size is observed. Figure 6 displays the self entropy of four adaptation corpora along the X-axis, and the improvement in CER reduction when 72K-sentence duction ratio on a domain (corresponding to Yomiuri, Encarta, TuneUp, Shincho from left to right) when 90 times more adaptation data was available. of the adaptation corpus and the benefit of having more training data available ( r = 0.9 ~ collection. 6.4 Domain Characteristics and Error Ratios The results so far measure the performance of various adaptation techniques in terms of CER. However, CER is not the only metric that provides meaningful comparisons among systems. Suzuki and Gao [2005a] discuss the error ratio (ER) metric, which measures the side effects of a new model, that is, the numb er of newly introduced errors relative to the in an actual software deployment scenario : if there are two new models with the same users are more intolerant of ne wly introduced errors than errors that existed previously. In this section we compare the adaptation methods using ER. where| E A | is the number of errors found only in the new (adaptation) model, and | E B | the number of errors corrected by the new model. Intuitively, this quantity captures the cost of improvement in the adaptation model, corresponding to the number of newly introduced errors per each improvement. The sm aller the ratio, the better the model at the same CER: ER = 0 if the adapted model intr oduces no new errors; ER &lt; 1 if the adapted model makes CER improvements; ER = 1 if the CER improvement is zero (i.e., the when the adapted model has worse CER pe rformance than the baseline model. In Figure 7 the performance of various MSR models at different iterations is compared to linear interpolation models at various lambda values in four adaptation domains using the error ratio metric. In each graph, the x-axis plots the relative error rate reduction (RER, i.e., the CER difference between the backgr ound and adapted models in %), and the y-axis plots the error ratio (max = 1; min = 0) . We can see that MSR models are better than linear interpolation models in all domains, as they achieve larger CER reductions (larger values on the x-axis) at smaller ER (smaller values on the y-axis). When the models achieve similar CER reductions, as they do with Encarta and Shincho domains, the MSR models have smaller ER values. So we can c onclude that a discriminative method (in this case MSR) is superior to linear interpolation, not only in terms of CER reduction but also discriminative training, which works specifica lly to adjust feature weights so as to minimize errors. results in Table III for 8,000 training samples) for each algorithm. While the algorithms perform similarly in most cases, we can see some small differences: although the boosting and perceptron algorithms have the same CER for Yomiuri and TuneUp from consistent within the domain. 7. CONCLUSION AND FUTURE WORK In this article we have examined the performance of various LM adaptation methods in terms of domain similarity and diversity. We have found that (1) the notion of cross-domain similarity, measured by cross entropy, correlates with the CER of all models (Section 6.1); and (2) the notion of in-domain diversity, measured by self entropy, corre-lates with the utility of more adaptation training data for discriminative training methods (Section 6.3). In comparing discriminati ve methods to a MAP-based method, we have also found that (1) the former uniformly achieve better performance than the latter, not only in terms of CER reduct ion but also in having fewer side effects (Section 6.4); and (2) they are more robust ag ainst the similarity of background and adap-tation data (Section 6.2). scenario, i.e., to incrementally build models using incoming data for adaptation, taking all previously available data as a background corp us. Such a scenario is easily conceivable in the context of adapting to a user or to a newly introduced topic. We hope that the results obtained in this article serve as a starting point for this direction of research. ACKNOWLEDGEMENT This research was conducted while Wei Yu an was visiting Microsoft Research Asia. REFERENCES 
